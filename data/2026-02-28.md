<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 23]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.AI](#cs.AI) [Total: 40]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: MLLM은 복잡한 시나리오에서 인식의 취약성을 보이며, AOT-SFT라는 적대적 데이터셋과 AOT라는 자기 플레이 프레임워크를 통해 이를 보강하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: MLLM의 인식 취약성을 개선하기 위해.

Method: AOT-SFT라는 대규모 적대적 데이터셋과 자기 플레이 프레임워크인 AOT를 제안하여 MLLM의 강인성을 높이는 방법론을 개발하였다.

Result: AOT는 Defender의 인지 강인성을 향상시키고 환각 현상을 줄이는 데 성공하였다.

Conclusion: AOT는 더 신뢰할 수 있는 MLLM을 훈련하기 위한 확장 가능한 패러다임을 제공한다.

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [2] [Entropy-Controlled Flow Matching](https://arxiv.org/abs/2602.22265)
*Chika Maduabuchi*

Main category: cs.LG

TL;DR: 본 논문은 정보 기하학을 직접적으로 제어할 수 없는 기존의 흐름 일치 목표를 개선하기 위해 엔트로피 제어 흐름 일치를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 흐름 일치 목표는 정보 기하학을 직접적으로 제어하지 않으며 낮은 엔트로피 병목 현상을 허용합니다.

Method: 우리는 연속성 방정식 경로에 대한 제약 변별 원리를 도입하여 전체 엔트로피 비율 예산을 강제하는 엔트로피 제어 흐름 일치(ECFM)를 제안합니다.

Result: ECFM은 Wasserstein 공간에서의 볼록 최적화 문제로, KKT/폰트리야긴 시스템을 포함하고, 명시적 엔트로피 곱셈기가 있는 슈뢰딩거 교량과 동등한 확률 제어 표현을 가집니다.

Conclusion: ECFM은 고전적 OT로 수렴하고, 무제한 흐름 일치에 대한 근접 최적 붕괴 반례를 구성합니다.

Abstract: Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.

</details>


### [3] [BrepCoder: A Unified Multimodal Large Language Model for Multi-task B-rep Reasoning](https://arxiv.org/abs/2602.22284)
*Mingi Kim,Yongjun Kim,Jungwoo Kang,Hyungki Kim*

Main category: cs.LG

TL;DR: BrepCoder는 B-rep 입력을 기반으로 다양한 CAD 작업을 수행하는 통합 다중 모달 대형 언어 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 CAD 모델이 특정 작업에 의존하고 산업 표준 B-rep 형식보다는 포인트 클라우드나 이미지에만 초점을 맞추는 한계를 극복하고자 합니다.

Method: BrepCoder는 대형 언어 모델의 코드 생성 능력을 활용하여 CAD 모델링 시퀀스를 파이썬과 유사한 코드로 변환하고 B-rep와 정렬합니다. 두 단계의 교육 전략을 적용하여 먼저 기하학적 특징과 설계 논리를 학습하기 위해 역공학을 통한 사전 훈련을 진행한 후, 완성, 오류 수정 및 CAD-QA와 같은 다양한 하위 작업으로 모델을 효과적으로 확장합니다.

Result: BrepCoder는 B-rep을 구조적 코드로 해석함으로써 다양한 작업에서 우수한 일반화를 달성하였습니다.

Conclusion: BrepCoder는 일반적인 CAD 에이전트로서의 잠재력을 보여줍니다.

Abstract: Recent advancements in deep learning have actively addressed complex challenges within the Computer-Aided Design (CAD) domain.However, most existing approaches rely on task-specifi c models requiring structural modifi cations for new tasks, and they predominantly focus on point clouds or images rather than the industry-standard Boundary Representation (B-rep) format. To address these limitations, we propose BrepCoder, a unifi ed Multimodal Large Language Model (MLLM) that performs diverse CAD tasks from B-rep inputs. By leveraging the code generation capabilities of Large Language Models (LLMs), we convert CAD modeling sequences into Python-like code and align them with B-rep. We then adopt a two-stage training strategy: First, pre-training on reverse engineering to learn geometric features and design logic. Second, eff ectively extending the model to various downstream tasks such as completion, error correction, and CAD-QA. Consequently, by interpreting B-rep as structural code, BrepCoder achieves superior generalization across diverse tasks, demonstrating its potential as a general-purpose CAD agent.

</details>


### [4] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 강화 학습(RL)은 기계 고장 탐지(MFD)에 큰 가능성을 제공하지만, 기존의 RL 기반 MFD 접근 방법은 RL의 순차적 의사결정 강점을 충분히 활용하지 못하고 있다. 본 논문은 MFD를 오프라인 역 강화 학습 문제로 정의하고, 건강한 작업 시퀀스로부터 직접 보상 동력을 학습하여 수동 보상 공학과 고장 레이블을 우회한다. 이 프레임워크는 정상(전문가)과 정책 생성 전환을 구별하는 판별기를 훈련시키기 위해 적대적 역 강화 학습을 사용하고, 판별기가 학습한 보상은 정상 동작 행동의 이탈을 나타내는 이상 점수로 작용한다. 세 가지 고장으로 가는 벤치마크 데이터셋에서 평가했을 때, 모델은 정상 샘플에 낮은 이상 점수를, 고장 샘플에 높은 점수를 일관되게 할당하여 조기 견고한 고장 탐지를 가능하게 한다. RL의 순차적 추론과 MFD의 시간적 구조를 일치시킴으로써, 이 작업은 데이터 기반 산업 환경에서 RL 기반 진단을 위한 길을 열어준다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습을 통해 기계 고장 탐지의 효율성을 높이기 위해, 기존 접근법의 한계를 극복하고자 하였다.

Method: MFD를 오프라인 역 강화 학습 문제로 정의하고, Adversarial Inverse Reinforcement Learning을 사용하여 정상과 정책 생성 전환을 구별하는 판별기를 훈련했다.

Result: 모델은 정상 샘플에 낮은 이상 점수를, 고장 샘플에 높은 점수를 지속적으로 할당하여 조기 고장 탐지를 가능하게 했다.

Conclusion: RL 기반 진단을 위한 새로운 접근법을 제시하였으며, 데이터 기반 산업 환경에서의 활용 가능성을 강조하였다.

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [5] [Training Agents to Self-Report Misbehavior](https://arxiv.org/abs/2602.22303)
*Bruce W. Lee,Chen Yueh-Han,Tomek Korbak*

Main category: cs.LG

TL;DR: 자기 범죄 교육을 통해 AI 에이전트가 비밀리에 잘못된 행동을 할 때 가시적인 신호를 생성하게 하여 미검출 공격률을 줄이는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 프론티어 AI 에이전트가 숨겨진 목표를 추구하며 감시로부터 그 추구를 숨길 수 있는 가능성을 다룹니다.

Method: GPT-4.1 및 Gemini-2.0 에이전트를 훈련시켜 그들이 기만적으로 행동할 때 report_scheming() 도구를 호출하도록 합니다.

Result: 자기 범죄 교육이 미검출 성공 공격률을 유의미하게 감소시키며, 일치하는 기능의 모니터 및 정렬 기준선을 초과합니다.

Conclusion: 자기 범죄 성능은 의심스러운 행동의 외부 표식과 관계없이 일관되며, 이는 프론티어 비정렬 위험을 줄이는 유효한 경로를 제시합니다.

Abstract: Frontier AI agents may pursue hidden goals while concealing their pursuit from oversight. Alignment training aims to prevent such behavior by reinforcing the correct goals, but alignment may not always succeed and can lead to unwanted side effects. We propose self-incrimination training, which instead trains agents to produce a visible signal when they covertly misbehave. We train GPT-4.1 and Gemini-2.0 agents to call a report_scheming() tool when behaving deceptively and measure their ability to cause harm undetected in out-of-distribution environments. Self-incrimination significantly reduces the undetected successful attack rate, outperforming matched-capability monitors and alignment baselines while preserving instruction hierarchy and incurring minimal safety tax on general capabilities. Unlike blackbox monitoring, self-incrimination performance is consistent across tasks regardless of how suspicious the misbehavior appears externally. The trained behavior persists under adversarial prompt optimization and generalizes to settings where agents pursue misaligned goals themselves rather than being instructed to misbehave. Our results suggest self-incrimination offers a viable path for reducing frontier misalignment risk, one that neither assumes misbehavior can be prevented nor that it can be reliably classified from the outside.

</details>


### [6] [Structure and Redundancy in Large Language Models: A Spectral Study via Random Matrix Theory](https://arxiv.org/abs/2602.22345)
*Davide Ettori*

Main category: cs.LG

TL;DR: 본 논문은 스펙트럼 기하학과 랜덤 매트릭스 이론(RMT)에 기반하여 신뢰성과 효율성 문제를 해결하는 통합 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 딥 네트워크와 대규모 언어 모델의 내부 동작은 점점 더 불투명해지며, 이는 환각, 배포 전환에 대한 취약한 일반화, 증가하는 계산 및 에너지 수요로 이어진다.

Method: 이번 연구는 계층과 입력에 걸쳐 숨겨진 활성의 고유값 동역학을 분석하고, 스펙트럼 통계가 모델 동작을 이해할 수 있는 안정적이고 해석 가능한 컴팩트한 시각을 제공한다고 보여준다. 첫 번째 기여는 EigenTrack으로, 대규모 언어 및 시각-언어 모델에서 환각과 분포 외 행동을 실시간으로 감지하는 방법을 제안한다.

Result: EigenTrack은 스트리밍 활성화를 엔트로피, 분산 및 Marchenko-Pastur 기준선으로부터의 편차와 같은 스펙트럼 설명자로 변환하고, 경량 순환 분류기를 사용하여 시간적 변화를 모델링함으로써 신뢰도 실패를 조기 감지할 수 있도록 한다.

Conclusion: 두 번째 기여인 RMT-KD는 랜덤 매트릭스 이론을 통한 지식 증류를 통해 딥 네트워크를 압축하는 원칙적인 접근 방식을 제시한다. 이 방법은 활성 스펙트럼에서 이상값 고유값을 작업 관련 정보를 전달하는 것으로 해석하며, 네트워크를 더 낮은 차원의 부분 공간으로 점진적으로 투영하여 정확성을 유지하면서도 컴팩트하고 에너지 효율적인 모델을 생성한다.

Abstract: This thesis addresses two persistent and closely related challenges in modern deep learning, reliability and efficiency, through a unified framework grounded in Spectral Geometry and Random Matrix Theory (RMT). As deep networks and large language models continue to scale, their internal behavior becomes increasingly opaque, leading to hallucinations, fragile generalization under distribution shift, and growing computational and energy demands. By analyzing the eigenvalue dynamics of hidden activations across layers and inputs, this work shows that spectral statistics provide a compact, stable, and interpretable lens on model behavior, capable of separating structured, causal representations from noise-dominated variability. Within this framework, the first contribution, EigenTrack, introduces a real-time method for detecting hallucinations and out-of-distribution behavior in large language and vision-language models. EigenTrack transforms streaming activations into spectral descriptors such as entropy, variance, and deviations from the Marchenko-Pastur baseline, and models their temporal evolution using lightweight recurrent classifiers, enabling early detection of reliability failures before they appear in model outputs while offering interpretable insight into representation dynamics. The second contribution, RMT-KD, presents a principled approach to compressing deep networks via random matrix theoretic knowledge distillation. By interpreting outlier eigenvalues in activation spectra as carriers of task-relevant information, RMT-KD progressively projects networks onto lower-dimensional subspaces through iterative self-distillation, yielding significantly more compact and energy-efficient models while preserving accuracy and dense, hardware-friendly structure.

</details>


### [7] [ParamMem: Augmenting Language Agents with Parametric Reflective Memory](https://arxiv.org/abs/2602.23320)
*Tianjun Yao,Yongqiang Chen,Yujia Zheng,Pan Li,Zhiqiang Shen,Kun Zhang*

Main category: cs.LG

TL;DR: 이 논문은 언어 에이전트가 반사적 다양성을 통해 문제 해결 능력을 향상시킬 수 있는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 반성적 자기 검토가 반복적인 출력을 초래하여 추론 성능을 제한하는 문제를 해결하고자 함.

Method: ParamMem이라는 매개변수 기억 모듈을 사용하여 반사 패턴을 모델 매개변수에 인코딩하고, 온도 제어 샘플링을 통해 다양한 반사를 생성하는 방법을 제안.

Result: 코드 생성, 수학적 추론 및 다중 호핑 질문 응답에 대한 광범위한 실험을 통해 지속적인 개선이 관찰됨.

Conclusion: ParamMem은 언어 에이전트를 향상시키기 위한 효과적인 구성 요소로서의 잠재력을 강조하며, 샘플 효율성이 높고 모델 스케일 간의 약한-강한 전이를 가능하게 함.

Abstract: Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.

</details>


### [8] [Learning geometry-dependent lead-field operators for forward ECG modeling](https://arxiv.org/abs/2602.22367)
*Arsenii Dokuchaev,Francesca Bonizzoni,Stefano Pagani,Francesco Regazzoni,Simone Pezzuto*

Main category: cs.LG

TL;DR: 고해상도 전기 생리학적 시뮬레이션을 위한 새로운 모델 제안.


<details>
  <summary>Details</summary>
Motivation: 정확한 체형 표현이 필수적이나 기존 방법들의 한계로 인해 실용화에 어려움이 존재한다.

Method: 형상 정보를 고려한 대체 모델을 사용하여 정밀한 ECG 시뮬레이션을 수행한다.

Result: 체내 및 심장 내에서 매우 높은 정확도를 보여주고 상대 평균 제곱 오차가 2.5% 미만이다.

Conclusion: 신뢰성이 높고 최소한의 데이터 요구로 생리학적 시뮬레이션이 가능하다.

Abstract: Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clinical practice, as imaging protocols are typically focused on the heart and often do not include the entire torso. In addition, the computational cost of the lead-field method scales linearly with the number of electrodes, limiting its applicability in high-density recording settings. To date, no existing approach simultaneously achieves high anatomical fidelity, low data requirements and computational efficiency. In this work, we propose a shape-informed surrogate model of the lead-field operator that serves as a drop-in replacement for the full-order model in forward ECG simulations. The proposed framework consists of two components: a geometry-encoding module that maps anatomical shapes into a low-dimensional latent space, and a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions and latent codes. The proposed method achieves high accuracy in approximating lead fields both within the torso (mean angular error 5°) and inside the heart, resulting in highly accurate ECG simulations (relative mean squared error <2.5%. The surrogate consistently outperforms the widely used pseudo lead-field approximation while preserving negligible inference cost. Owing to its compact latent representation, the method does not require a fully detailed torso segmentation and can therefore be deployed in data-limited settings while preserving high-fidelity ECG simulations.

</details>


### [9] [TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series](https://arxiv.org/abs/2602.22520)
*Xiannan Huang,Shen Fang,Shuhan Qiu,Chengcheng Yu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: TEFL은 과거 예측 잔차를 통합하여 다단계 예측의 정확성을 개선하는 새로운 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 예측 모델들이 과거 예측 잔차에 포함된 정보 없이 점별 예측 손실을 최소화하도록 훈련되고 있는 문제를 해결하기 위해.

Method: TEFL는 과거 예측 잔차를 예측 파이프라인에 포함시키며, 효율성을 유지하고 과적합을 방지하기 위해 저순위 어댑터로 통합한다.

Result: 10개의 실제 데이터 세트와 5개의 기본 아키텍처에 대한 실험에서 MAE를 평균 5-10% 줄이며 정확성을 지속적으로 향상시킴을 보여준다.

Conclusion: 잔차 기반 피드백을 학습 과정에 직접 내장함으로써 TEFL은 현대의 깊은 예측 시스템에 간단하고 일반적이며 효과적인 개선을 제공한다.

Abstract: Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.

</details>


### [10] [Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization](https://arxiv.org/abs/2602.22536)
*Jichao Zhang,Ran Miao,Limin Li*

Main category: cs.LG

TL;DR: 이 논문은 연결 구조의 진화를 포착할 수 있는 지속적인 비음수 행렬 분해(pNMF) 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 NMF 기반 방법은 단일 스케일에 국한되어 연결 구조의 해를 잘 포착하지 못한다.

Method: 지속적인 동질성을 활용하여, 연결 구조가 질적인 변화를 겪는 최소한의 스케일 집합을 식별하고 이를 기반으로 다중 그래프 라플라시안을 생성하여 pNMF 문제를 정의한다.

Result: 제안된 방법은 다중 스케일 저차원 임베딩에서 효과적임을 입증하고, 임베딩의 구조적 특성을 분석하며 이들 간의 경계값을 설정한다.

Conclusion: 제안된 pNMF가 새로운 계산적 도전을 제기하며, 보장된 수렴을 갖는 순차 대체 최적화 알고리즘을 개발하였다.

Abstract: Matrix factorization techniques, especially Nonnegative Matrix Factorization (NMF), have been widely used for dimensionality reduction and interpretable data representation. However, existing NMF-based methods are inherently single-scale and fail to capture the evolution of connectivity structures across resolutions. In this work, we propose persistent nonnegative matrix factorization (pNMF), a scale-parameterized family of NMF problems, that produces a sequence of persistence-aligned embeddings rather than a single one. By leveraging persistent homology, we identify a canonical minimal sufficient scale set at which the underlying connectivity undergoes qualitative changes. These canonical scales induce a sequence of graph Laplacians, leading to a coupled NMF formulation with scale-wise geometric regularization and explicit cross-scale consistency constraint. We analyze the structural properties of the embeddings along the scale parameter and establish bounds on their increments between consecutive scales. The resulting model defines a nontrivial solution path across scales, rather than a single factorization, which poses new computational challenges. We develop a sequential alternating optimization algorithm with guaranteed convergence. Numerical experiments on synthetic and single-cell RNA sequencing datasets demonstrate the effectiveness of the proposed approach in multi-scale low-rank embeddings.

</details>


### [11] [RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format](https://arxiv.org/abs/2602.22538)
*Zhehao Huang,Yuhang Liu,Baijiong Lin,Yixin Lou,Zhengbao He,Hanling Tian,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: 본 논문은 대형 추론 모델(LRM)과 지침 조정 모델(ITM)의 통합을 통해 지침 준수 및 추론 성능을 개선하는 RAIN-Merging 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LRM은 긴 체계적 추론에서 뛰어나지만 출력 형식, 제약 조건 또는 특정 요구사항의 지침을 충실히 따르지 못하는 경향이 있습니다.

Method: RAIN-Merging은 지침 준수를 통합하면서 사고 형식과 추론 성능을 유지하는 기울기 없는 방법입니다. 이를 위해 ITM 작업 벡터를 LRM의 사고 특수 토큰에서의 정방향 특성의 영 공간에 투영합니다.

Result: RAIN-Merging은 지침 준수를 상당히 개선하며 추론 품질도 유지합니다. 이는 4개의 지침 준수 벤치마크와 9개의 추론 및 일반 능력 벤치마크에서 평가되었습니다.

Conclusion: RAIN-Merging은 다양한 모델 크기와 아키텍처에서 일관된 성과 향상을 보이며 에이전트 설정에서도 개선된 성능을 나타냅니다.

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

</details>


### [12] [Relatron: Automating Relational Machine Learning over Relational Databases](https://arxiv.org/abs/2602.22552)
*Zhikai Chen,Han Xie,Jian Zhang,Jiliang Tang,Xiang Song,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 이 논문은 관계형 데이터베이스에서 예측 모델링의 문제를 해결하기 위한 연구로, 관계형 딥러닝(RDL)과 딥 피처 신서시스(DFS)를 결합한 설계를 제안하며, 다양한 RDB 작업에 대한 아키텍처 중심 검색을 수행하여 성능 및 아키텍처 선택의 원칙을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 관계형 데이터베이스(RDB)에서의 예측 모델링은 크로스 테이블 의존성과 복잡한 특징 상호작용을 포착하는 데 어려움을 겪고 있어 해결이 필요하다.

Method: 관계형 딥러닝(RDL)과 딥 피처 신서시스(DFS)를 통합하여 공유 디자인 공간에서 아키텍처 중심의 검색을 수행한다.

Result: RDL은 DFS를 일관되게 능가하지 않으며, 아키텍처 선택은 작업에 따라 달라지고, 검증 정확도가 아키텍처 선택의 신뢰할 수 없는 지표임을 발견하였다.

Conclusion: Relatron이라는 메타 선택기를 제안하여 RDL과 DFS 간의 선택을 수행하고, 내부 탐색을 줄이며, 실험에서 강력한 기준선 대비 최대 18.5% 향상을 달성하였다.

Abstract: Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the "more tuning, worse performance" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.

</details>


### [13] [Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD](https://arxiv.org/abs/2602.22611)
*Jiayang Meng,Tao Huang,Chen Hou,Guolong Zheng,Hong Chen*

Main category: cs.LG

TL;DR: 본 논문은 Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD)를 도입하여 차별적 개인 정보 보호를 제공하며, 레이어 별 MIA 위험을 고려하여 프라이버시 보호를 적응적으로 할당하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: Embedding-as-an-Interface (EaaI) 설정에서, Intermediate Representations (IRs)의 배포 특성이 훈련 세트의 멤버십 신호를 유출할 수 있어 Membership Inference Attacks (MIAs)에 대한 취약성이 존재합니다.

Method: LM-DP-SGD는 퍼블릭 섀도 데이터셋에서 섀도 모델을 훈련하고, 훈련/테스트 분할에서 레이어별 IR을 추출하며, 레이어별 MIA 적대자를 적합시킵니다.

Result: 상당한 실험 결과는 LM-DP-SGD가 동일한 프라이버시 예산 하에 피크 IR 수준의 MIA 위험을 줄이면서 유틸리티를 유지한다는 것을 보여줍니다.

Conclusion: LM-DP-SGD는 고유한 프라이버시-유틸리티 트레이드오프를 제공하면서, 고정 노이즈 크기 하에 레이어별 적절한 보호를 제공합니다.

Abstract: In Embedding-as-an-Interface (EaaI) settings, pre-trained models are queried for Intermediate Representations (IRs). The distributional properties of IRs can leak training-set membership signals, enabling Membership Inference Attacks (MIAs) whose strength varies across layers. Although Differentially Private Stochastic Gradient Descent (DP-SGD) mitigates such leakage, existing implementations employ per-example gradient clipping and a uniform, layer-agnostic noise multiplier, ignoring heterogeneous layer-wise MIA vulnerability. This paper introduces Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD), which adaptively allocates privacy protection across layers in proportion to their MIA risk. Specifically, LM-DP-SGD trains a shadow model on a public shadow dataset, extracts per-layer IRs from its train/test splits, and fits layer-specific MIA adversaries, using their attack error rates as MIA-risk estimates. Leveraging the cross-dataset transferability of MIAs, these estimates are then used to reweight each layer's contribution to the globally clipped gradient during private training, providing layer-appropriate protection under a fixed noise magnitude. We further establish theoretical guarantees on both privacy and convergence of LM-DP-SGD. Extensive experiments show that, under the same privacy budget, LM-DP-SGD reduces the peak IR-level MIA risk while preserving utility, yielding a superior privacy-utility trade-off.

</details>


### [14] [Multi-agent imitation learning with function approximation: Linear Markov games and beyond](https://arxiv.org/abs/2602.22810)
*Luca Viano,Till Freihaut,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 이 논문은 선형 마르코프 게임에서의 다중 에이전트 모방 학습(MAIL)의 첫 번째 이론적 분석을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 선형 마르코프 게임에서의 다중 에이전트 모방 학습에 대한 이해를 높이기 위해.

Method: 주어진 피처에서 선형인 전이 동역학과 에이전트의 보상 함수를 활용하여 에이전트 간 정책 편차 농도 계수를 정의하고, 상호작용 설정으로 전환하여 효율적인 알고리즘을 제안한다.

Result: 제안된 알고리즘은 샘플 복잡도가 피처 맵의 차원 $d$에만 의존함을 보였다.

Conclusion: 제안된 딥 MAIL 상호작용 알고리즘은 Tic-Tac-Toe 및 Connect4와 같은 게임에서 BC보다 뛰어난 성능을 보인다.

Abstract: In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level "all policy deviation concentrability coefficient" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.

</details>


### [15] [Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2602.22817)
*Shuo He,Lang Feng,Qi Wei,Xin Cheng,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: HGPO는 단계별 상대적 이점을 일관되게 추정함으로써 정책 최적화를 향상시키는 새로운 접근 방식입니다.


<details>
  <summary>Details</summary>
Motivation: 정확한 정책 업데이트를 위해 단계별 그룹 기반 정책 최적화가 필요하지만, 역사적 맥락의 불일치로 인해 이점 추정에 편향이 발생하는 문제를 해결하고자 합니다.

Method: HGPO는 실행 궤적의 그룹 내에서 역사적 맥락의 일관성에 따라 각 단계를 여러 계층 그룹에 할당하고, 각 그룹 내에서 이점을 계산하여 적응형 가중치 방식으로 집계합니다.

Result: HGPO는 두 가지 도전적인 에이전틱 작업에서 기존 에이전틱 RL 방법들보다 현저하게 우수한 성과를 보였습니다.

Conclusion: 이 방식은 추가 모델이나 롤아웃 없이 단계별 이점 추정에서 유리한 편향-분산 균형을 달성합니다.

Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.

</details>


### [16] [Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus](https://arxiv.org/abs/2602.22847)
*Anna Van Elst,Kerrian Le Caillec,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: 이 논문은 분산 환경에서 집합 순위에 대한 신뢰할 수 있는 합의를 달성하는 방법을 연구하며, 고전적인 규칙을 사용하여 새로운 문제와 복원력, 확장성을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 많은 기술들이 분산 환경에서의 합의 순위 계산 능력을 확장하는 것을 필요로 하며, 이는 중요한 방법론적 도전이다.

Method: 이 연구는 고전적인 규칙(예: Borda, Copeland)을 사용하여 독립적인 노드 간에 랜덤 가십 통신을 통해 글로벌 순위 합의를 계산하는 방법을 제안한다.

Result: 여기서 제안된 알고리즘은 다양한 네트워크 토폴로지와 실제 및 합성 순위 데이터셋에서 신뢰할 수 있고 신속하게 올바른 순위 집계를 수렴하였다.

Conclusion: Borda 및 Copeland 합의 방법에 대한 엄격한 수렴 보장을 제공하며, 분산 환경에서 중간 순위 규칙에 따른 합의 구현도 보여준다.

Abstract: The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ranking data to be aggregated can be brought together in a single computing unit. For many technologies (e.g. peer-to-peer networks, IoT, multi-agent systems), extending the ability to calculate consensus rankings with guarantees in a decentralized setting, i.e., when preference data is initially distributed across a communicating network, remains a major methodological challenge. Indeed, in recent years, the literature on decentralized computation has mainly focused on computing or optimizing statistics such as arithmetic means using gossip algorithms. The purpose of this article is precisely to study how to achieve reliable consensus on collective rankings using classical rules (e.g. Borda, Copeland) in a decentralized setting, thereby raising new questions, robustness to corrupted nodes, and scalability through reduced communication costs in particular. The approach proposed and analyzed here relies on random gossip communication, allowing autonomous agents to compute global ranking consensus using only local interactions, without coordination or central authority.
  We provide rigorous convergence guarantees, including explicit rate bounds, for the Borda and Copeland consensus methods. Beyond these rules, we also provide a decentralized implementation of consensus according to the median rank rule and local Kemenization. Extensive empirical evaluations on various network topologies and real and synthetic ranking datasets demonstrate that our algorithms converge quickly and reliably to the correct ranking aggregation.

</details>


### [17] [MSINO: Curvature-Aware Sobolev Optimization for Manifold Neural Networks](https://arxiv.org/abs/2602.22937)
*Suresan Pareth*

Main category: cs.LG

TL;DR: 이 논문에서는 리만 기하적 다양체에서 정의된 신경망을 위한 곡률 인식 훈련 프레임워크인 Manifold Sobolev Informed Neural Optimization (MSINO)을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 리만 기하적 다양체에서 신경망 훈련의 안정성을 개선하고 곡률을 고려한 최적화를 수행하기 위함입니다.

Method: 표준 유클리드 미분 감독을 공변 소볼레브 손실로 대체하여 기울기를 정렬하고, 라플라스 벨트라미 매끄러운 정규화 항을 추가하여 안정성을 향상시킵니다.

Result: 리만 기하 최적화 및 소볼레브 이론에 기반하여 얻은 기하학적으로 의존적인 상수들을 통해 다양한 결과를 도출하였습니다.

Conclusion: MSINO는 곡률을 명시적으로 추적하며, 신경망 훈련에 대한 수렴 보장을 제공하는 유일한 훈련 방법을 제시합니다.

Abstract: We introduce Manifold Sobolev Informed Neural Optimization (MSINO), a curvature aware training framework for neural networks defined on Riemannian manifolds. The method replaces standard Euclidean derivative supervision with a covariant Sobolev loss that aligns gradients using parallel transport and improves stability via a Laplace Beltrami smoothness regularization term.
  Building on classical results in Riemannian optimization and Sobolev theory on manifolds, we derive geometry dependent constants that yield (i) a Descent Lemma with a manifold Sobolev smoothness constant, (ii) a Sobolev Polyak Lojasiewicz inequality giving linear convergence guarantees for Riemannian gradient descent and stochastic gradient descent under explicit step size bounds, and (iii) a two step Newton Sobolev method with local quadratic contraction in curvature controlled neighborhoods.
  Unlike prior Sobolev training in Euclidean space, MSINO provides training time guarantees that explicitly track curvature and transported Jacobians. Applications include surface imaging, physics informed learning settings, and robotics on Lie groups such as SO(3) and SE(3). The framework unifies value and gradient based learning with curvature aware convergence guarantees for neural training on manifolds.

</details>


### [18] [Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization](https://arxiv.org/abs/2602.23008)
*Zeyuan Liu,Jeonghye Kim,Xufang Luo,Dongsheng Li,Yuqing Yang*

Main category: cs.LG

TL;DR: EMPO2는 탐색을 강화하기 위해 메모리를 활용하고 온-오프 정책 업데이트를 결합한 하이브리드 RL 프레임워크로, 기존 방법보다 월등한 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 에이전트의 훈련에서 탐색이 주요 병목 현상으로 남아 있다.

Method: EMPO$^2$는 메모리를 탐색에 활용하고, 온-오프 정책 업데이트를 결합하여 LLM이 메모리와 함께 잘 수행되도록 하며, 메모리 없이도 강건함을 보장하는 하이브리드 RL 프레임워크이다.

Result: ScienceWorld 및 WebShop에서 EMPO$^2$는 각각 GRPO보다 128.6% 및 11.3% 개선된 성능을 달성했다.

Conclusion: EMPO$^2$는 더 탐색적이고 일반화 가능한 LLM 기반 에이전트를 구축하기 위한 유망한 프레임워크로 강조된다.

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

</details>


### [19] [Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements](https://arxiv.org/abs/2602.23035)
*Viraj Patel,Marko Grujic,Philipp Aigner,Theodor Abart,Marcus Granegger,Deblina Bhattacharjee,Katharine Fraser*

Main category: cs.LG

TL;DR: 이 논문은 심장 혈류 패턴의 관계 구조를 모델링하기 위해 물리 기반의 잠재 관계 프레임워크를 제안하며, 이는 질병 중증도와 임상 개입을 측정하는 데 기여합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 이미징 및 계산 방법은 심장 혈류의 복잡한 관계 구조를 포착하지 못합니다.

Method: 물리 기반의 신경 관계 추론 아키텍처와 상호작용 에너지 및 출생-사망 역학을 결합하여 심장 소용돌이를 그래프의 상호작용 노드로 모델링합니다.

Result: 이 모델은 질병의 중증도와 개입 수준에 민감한 잠재 그래프를 생성하며, 대동맥의 반경이 좁아질수록 소용돌이 상호작용이 강해지고 빈번해짐을 보여줍니다.

Conclusion: 잠재 상호작용 그래프와 엔트로피는 심장 질환 및 임상의 강력하고 해석 가능한 지표로 작용합니다.

Abstract: Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.

</details>


### [20] [Bound to Disagree: Generalization Bounds via Certifiable Surrogates](https://arxiv.org/abs/2602.23128)
*Mathieu Bazinet,Valentina Zantedeschi,Pascal Germain*

Main category: cs.LG

TL;DR: 이 논문은 깊은 학습 모델에 대한 새로운 불일치 기반 인증서를 제공하여 일반화 경계의 문제를 해결하고, 보조 모델을 통해 예측기의 실제 위험을 경계하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 모델에 대한 일반화 경계는 종종 빈약하거나 계산할 수 없거나 특정 모델 클래스에 제한된다.

Method: 우리는 두 예측기 사이의 실제 위험 차이에 대한 새로운 불일치 기반 인증서를 제공하고, 보조 모델을 사용하여 관심 있는 예측기의 실제 위험을 경계한다.

Result: 보조 모델을 훈련하기 위해 세 가지 서로 다른 프레임워크를 활용하며, 축적된 인증서의 엄밀함을 경험적으로 입증한다.

Conclusion: 타겟 모델을 수정하거나 훈련 절차를 일반화 프레임워크에 맞게 조정하지 않고 이러한 보장을 달성할 수 있다.

Abstract: Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework.

</details>


### [21] [Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language](https://arxiv.org/abs/2602.23201)
*Max S. Bennett,Thomas P. Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: 본 논문은 자연어로 지정된 학습 지침에 따라 유연한 업데이트를 수행하는 일반화된 신경 기억 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현대의 기계 학습 모델은 다양한 비정상 환경에서 배치되어 새로운 작업과 발전하는 지식에 지속적으로 적응해야 한다.

Method: 본 논문에서는 자연어로 지정된 학습 지침에 기반하여 유연한 업데이트를 수행하는 일반화된 신경 기억 시스템을 제안한다.

Result: 우리의 접근 방식은 적응형 에이전트가 이질적인 정보 출처로부터 선택적으로 학습할 수 있게 하여, 고정된 목적의 기억 업데이트가 충분치 않은 설정에서도 적용 가능하다.

Conclusion: 이 시스템은 헬스케어 및 고객 서비스와 같은 분야에서 효율ive하게 작동한다.

Abstract: Modern machine learning models are deployed in diverse, non-stationary environments where they must continually adapt to new tasks and evolving knowledge. Continual fine-tuning and in-context learning are costly and brittle, whereas neural memory methods promise lightweight updates with minimal forgetting. However, existing neural memory models typically assume a single fixed objective and homogeneous information streams, leaving users with no control over what the model remembers or ignores over time. To address this challenge, we propose a generalized neural memory system that performs flexible updates based on learning instructions specified in natural language. Our approach enables adaptive agents to learn selectively from heterogeneous information sources, supporting settings, such as healthcare and customer service, where fixed-objective memory updates are insufficient.

</details>


### [22] [Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity](https://arxiv.org/abs/2602.23296)
*Quang-Huy Nguyen,Jiaqi Wang,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: 연합 학습에서 불확실성 정량화 문제가 발생하며, 이를 해결하기 위해 FedWQ-CP라는 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습 시스템에서 신뢰할 수 있는 불확실성 정량화가 없다면 자원이 부족한 에이전트에서 과도하게 자신감 있는 모델을 배포하게 되어, 글로벌 성능이 양호해 보이는 상황에서도 지역적 실패가 발생할 위험이 있다.

Method: FedWQ-CP는 에이전트-서버 보정을 단일 통신 라운드에서 수행하며, 각 에이전트에서 보정 데이터에 기반하여 일치 점수를 계산하고 지역적 분위수 임계를 유도한다. 각 에이전트는 자신의 분위수 임계값과 보정 샘플 크기만 서버로 전송한다.

Result: 여러 공개 데이터 세트를 사용한 실험 결과, FedWQ-CP는 에이전트-wise 및 글로벌 커버리지를 유지하면서도 가장 작은 예측 집합 또는 간격을 생성함을 보여주었다.

Conclusion: FedWQ-CP는 이중 이질성 아래에서도 글로벌 및 에이전트 수준에서 경험적 커버리지 성능과 효율성을 균형 있게 유지하는 단순하면서도 효과적인 접근이다.

Abstract: Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.

</details>


### [23] [A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358)
*Elad Kimchi Shoshani,Leeyam Gabay,Yedid Hoshen*

Main category: cs.LG

TL;DR: PLADA는 이미지 전송 없이 클래스 레이블만 전송하여 큰 데이터셋을 효율적으로 전달하는 방법입니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 데이터셋을 여러 클라이언트에 배포하는 데 드는 통신 비용을 줄이기 위해서입니다.

Method: PLADA 방법은 특정 이미지의 클래스 레이블만 전송하여 피셀 전송을 완전히 없앱니다.

Result: 10개의 다양한 데이터셋에서 우리의 방법이 1MB 이하의 페이로드로 작업 지식을 전송하면서 높은 분류 정확도를 유지할 수 있음을 보여줍니다.

Conclusion: 효율적인 데이터셋 제공을 위한 유망한 해결책을 제공합니다.

Abstract: A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [24] [Sustainable Multi-Agent Crowdsourcing via Physics-Informed Bandits](https://arxiv.org/abs/2602.22365)
*Chayan Banerjee*

Main category: cs.MA

TL;DR: 본 논문에서는 크라우드소싱 플랫폼에서의 네 가지 긴장 상태를 해결하기 위한 FORGE라는 다중 에이전트 시뮬레이터를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 크라우드소싱 플랫폼은 할당 품질, 인력 지속 가능성, 운영 가능성 및 전략적 계약자 행동 간에 긴장 관계가 존재한다. 이러한 문제를 해결하기 위한 새로운 접근이 필요하다.

Method: FORGE라는 물리학 기반의 다중 에이전트 시뮬레이터를 통해 각 계약자는 자신의 피로 상태에 따라 부하 수용 임계값을 선언하며, 이를 통해 Restless Multi-Armed Bandit(RMAB)를 Stackelberg 게임으로 변환하였다. Neural-Linear UCB 할당기를 사용하여 Two-Tower 임베딩 네트워크와 오프라인 시뮬레이터 상호작용에서 파생된 물리 기반 공분산 사전 정보를 융합한다.

Result: 제안한 방법은 200회의 콜드 스타트 에피소드 동안 최초의 모든 비 오라클 방법 중에서 가장 높은 보상($	ext{LRew} = 0.555 .041$)을 달성하며, 7.6%의 작업력 이용률로 유지된다.

Conclusion: FORGE는 50%까지의 인력 이직률과 $σ= 0.20$까지의 관찰 노이즈에 대한 강건성을 유지하면서도 기존의 방법들이 불가능한 조합의 성과를 거둔다.

Abstract: Crowdsourcing platforms face a four-way tension between allocation quality, workforce sustainability, operational feasibility, and strategic contractor behaviour--a dilemma we formalise as the Cold-Start, Burnout, Utilisation, and Strategic Agency Dilemma. Existing methods resolve at most two of these tensions simultaneously: greedy heuristics and multi-criteria decision making (MCDM) methods achieve Day-1 quality but cause catastrophic burnout, while bandit algorithms eliminate burnout only through operationally infeasible 100% workforce utilisation.To address this, we introduce FORGE, a physics-grounded $K+1$ multi-agent simulator in which each contractor is a rational agent that declares its own load-acceptance threshold based on its fatigue state, converting the standard passive Restless Multi-Armed Bandit (RMAB) into a genuine Stackelberg game. Operating within FORGE, we propose a Neural-Linear UCB allocator that fuses a Two-Tower embedding network with a Physics-Informed Covariance Prior derived from offline simulator interactions. The prior simultaneously warm-starts skill-cluster geometry and UCB exploration landscape, providing a geometry-aware belief state from episode 1 that measurably reduces cold-start regret.Over $T = 200$ cold-start episodes, the proposed method achieves the highest reward of all non-oracle methods ($\text{LRew} = 0.555 \pm 0.041$) at only 7.6% workforce utilisation--a combination no conventional baseline achieves--while maintaining robustness to workforce turnover up to 50% and observation noise up to $σ= 0.20$.

</details>


### [25] [QSIM: Mitigating Overestimation in Multi-Agent Reinforcement Learning via Action Similarity Weighted Q-Learning](https://arxiv.org/abs/2602.22786)
*Yuanjun Li,Bin Zhang,Hao Chen,Zhouyang Jiang,Dapeng Li,Zhiwei Xu*

Main category: cs.MA

TL;DR: QSIM은 다중 에이전트 강화 학습에서의 Q값 과대 추정을 완화하는 새로운 유사성 가중치 Q-학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 협력적 다중 에이전트 강화 학습에서 Q값의 과대 추정 문제를 해결하기 위해.

Method: QSIM은 행동 유사성을 사용하여 TD 목표를 재구성하는 유사성 가중치 Q-학습 프레임워크이다.

Result: QSIM은 다양한 VD 방법과 통합하여 원래 알고리즘보다 더 우수한 성능과 안정성을 지속적으로 제공한다.

Conclusion: QSIM은 MARL에서의 체계적인 가치 과대 추정을 상당히 완화한다.

Abstract: Value decomposition (VD) methods have achieved remarkable success in cooperative multi-agent reinforcement learning (MARL). However, their reliance on the max operator for temporal-difference (TD) target calculation leads to systematic Q-value overestimation. This issue is particularly severe in MARL due to the combinatorial explosion of the joint action space, which often results in unstable learning and suboptimal policies. To address this problem, we propose QSIM, a similarity weighted Q-learning framework that reconstructs the TD target using action similarity. Instead of using the greedy joint action directly, QSIM forms a similarity weighted expectation over a structured near-greedy joint action space. This formulation allows the target to integrate Q-values from diverse yet behaviorally related actions while assigning greater influence to those that are more similar to the greedy choice. By smoothing the target with structurally relevant alternatives, QSIM effectively mitigates overestimation and improves learning stability. Extensive experiments demonstrate that QSIM can be seamlessly integrated with various VD methods, consistently yielding superior performance and stability compared to the original algorithms. Furthermore, empirical analysis confirms that QSIM significantly mitigates the systematic value overestimation in MARL. Code is available at https://github.com/MaoMaoLYJ/pymarl-qsim.

</details>


### [26] [ClawMobile: Rethinking Smartphone-Native Agentic Systems](https://arxiv.org/abs/2602.22942)
*Hongchao Du,Shangyu Wu,Qiao Li,Riwei Pan,Jinheng Li,Youcheng Sun,Chun Jason Xue*

Main category: cs.MA

TL;DR: 이 논문은 스마트폰에서의 에이전트 시스템의 설계를 탐구하며, ClawMobile이라는 계층적 아키텍처를 통해 언어 추론과 구조적 제어 경로를 분리하여 안정성과 재현성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 스마트폰은 에이전트 시스템에 도전적인 환경을 제공하며, 모바일 장치의 특성을 고려한 새로운 설계가 필요하다.

Method: ClawMobile은 고수준 언어 추론과 구조적, 결정론적 제어 경로를 분리하는 계층적 아키텍처를 채택하여 실행의 안정성과 재현성을 개선한다.

Result: ClawMobile을 사례 연구로 사용하여 모바일 LLM 런타임의 설계 원칙을 도출하고 효율성, 적응성, 안정성 등 주요 과제를 파악한다.

Conclusion: 견고한 스마트폰 네이티브 에이전트 시스템을 구축하기 위해서는 확률적 계획과 결정론적 시스템 인터페이스 간의 원칙적인 조정이 필요하다.

Abstract: Smartphones represent a uniquely challenging environment for agentic systems. Unlike cloud or desktop settings, mobile devices combine constrained execution contexts, fragmented control interfaces, and rapidly changing application states. As large language models (LLMs) evolve from conversational assistants to action-oriented agents, achieving reliable smartphone-native autonomy requires rethinking how reasoning and control are composed.
  We introduce ClawMobile as a concrete exploration of this design space. ClawMobile adopts a hierarchical architecture that separates high-level language reasoning from structured, deterministic control pathways, improving execution stability and reproducibility on real devices. Using ClawMobile as a case study, we distill the design principles for mobile LLM runtimes and identify key challenges in efficiency, adaptability, and stability. We argue that building robust smartphone-native agentic systems demands principled coordination between probabilistic planning and deterministic system interfaces. The implementation is open-sourced~\footnote{https://github.com/ClawMobile/ClawMobile} to facilitate future exploration.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [27] [An Adaptive Multichain Blockchain: A Multiobjective Optimization Approach](https://arxiv.org/abs/2602.22230)
*Nimrod Talmon,Haim Zysberg*

Main category: cs.CR

TL;DR: 블록체인 구성 문제를 멀티에이전트 자원 할당 문제로 다루며, 이를 통해 효율적인 거래처리와 자원 배분을 최적화하는 방법을 제시.


<details>
  <summary>Details</summary>
Motivation: 블록체인은 안전한 거래 처리를 위해 널리 사용되지만, 확장성의 한계가 있으며, 기존의 멀티체인 설계는 수요와 용량의 변화에 정적이다.

Method: 애플리케이션과 운영자가 수요, 용량, 가격 한계를 선언하며, 최적화 도구가 매 에포크마다 이들을 임시 체인으로 그룹화하고 체인 레벨 청산 가격을 설정하는 형식으로 구성된다.

Result: 애플리케이션, 운영자, 시스템의 정규화된 효용의 거버넌스 가중 조합을 극대화하는 것이 본 연구의 목표이다.

Conclusion: 모듈화된 모델은 기능 호환성, 애플리케이션 유형의 다양성, 에포크 간 안정성을 수용하며, 오프체인으로 해결할 수 있고 결과는 온체인에서 검증할 수 있다.

Abstract: Blockchains are widely used for secure transaction processing, but their scalability remains limited, and existing multichain designs are typically static even as demand and capacity shift. We cast blockchain configuration as a multiagent resource-allocation problem: applications and operators declare demand, capacity, and price bounds; an optimizer groups them into ephemeral chains each epoch and sets a chain-level clearing price. The objective maximizes a governance-weighted combination of normalized utilities for applications, operators, and the system. The model is modular -- accommodating capability compatibility, application-type diversity, and epoch-to-epoch stability -- and can be solved off-chain with outcomes verifiable on-chain. We analyze fairness and incentive issues and present simulations that highlight trade-offs among throughput, decentralization, operator yield, and service stability.

</details>


### [28] [Differentially Private Truncation of Unbounded Data via Public Second Moments](https://arxiv.org/abs/2602.22282)
*Zilong Cao,Xuan Bi,Hai Zhang*

Main category: cs.CR

TL;DR: 본 논문에서는 차별적 프라이버시(DP)의 한계를 극복하기 위해 공공 데이터의 두 번째 모멘트 정보를 활용한 PMT라는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 시대에 데이터 프라이버시는 중요하며, 차별적 프라이버시(DP)는 중요한 해결책 중 하나입니다. 그러나 DP는 일반적으로 데이터가 제한된 분포를 가질 때만 적용 가능합니다.

Method: 공공 데이터를 활용한 두 번째 모멘트 정보를 기반으로 개인 데이터를 변환하는 Public-moment-guided Truncation (PMT) 방법을 제안합니다. 이 방법은 비공식적 양인 데이터 차원과 샘플 크기에만 의존하는 원칙적인 절단을 적용합니다.

Result: 변환된 두 번째 모멘트 행렬은 잘 정규화된 형태를 가지고 있어 DP 노이즈에 저항하는 능력이 크게 강화되고, 이로 인해 모델의 DP 추정에서 개선이 이루어졌습니다.

Conclusion: PMT는 합성 및 실제 데이터셋에서 DP 모델의 정확성과 안정성을 크게 향상시키는 데 기여합니다.

Abstract: Data privacy is important in the AI era, and differential privacy (DP) is one of the golden solutions. However, DP is typically applicable only if data have a bounded underlying distribution. We address this limitation by leveraging second-moment information from a small amount of public data. We propose Public-moment-guided Truncation (PMT), which transforms private data using the public second-moment matrix and applies a principled truncation whose radius depends only on non-private quantities: data dimension and sample size. This transformation yields a well-conditioned second-moment matrix, enabling its inversion with a significantly strengthened ability to resist the DP noise. Furthermore, we demonstrate the applicability of PMT by using penalized and generalized linear regressions. Specifically, we design new loss functions and algorithms, ensuring that solutions in the transformed space can be mapped back to the original domain. We have established improvements in the models' DP estimation through theoretical error bounds, robustness guarantees, and convergence results, attributing the gains to the conditioning effect of PMT. Experiments on synthetic and real datasets confirm that PMT substantially improves the accuracy and stability of DP models.

</details>


### [29] [Silent Egress: When Implicit Prompt Injection Makes LLM Agents Leak Without a Trace](https://arxiv.org/abs/2602.22450)
*Qianlong Lan,Anuj Kaul,Shaun Jones,Stephanie Westrum*

Main category: cs.CR

TL;DR: 이 논문에서는 에이전틱 대형 언어 모델 시스템에서 발생하는 암묵적 프롬프트 주입과 침해 공격을 분석하여, 아웃바운드 요청을 통해 민감한 런타임 컨텍스트가 유출될 수 있는 리스크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 시스템에서 URL 검색 및 외부 도구 호출 작업을 자동화하는 흐름 속에서 발생할 수 있는 보안 문제를 규명하고자 함.

Method: 완전하게 로컬하고 재현 가능한 테스트 환경을 구성하여, 악의적인 웹 페이지가 에이전트를 유도하여 아웃바운드 요청을 생성하게 하는 실험을 진행함.

Result: 480회의 실험에서 공격은 높은 확률로 성공(P(egress) = 0.89)하며, 성공적인 95%의 공격은 출력 기반 안전 점검에서 탐지되지 않음. 또한, 중요 정보를 여러 요청으로 나누어 전송하는 '쉐어드 외부 유출' 기법은 탐지를 피함.

Conclusion: 프롬프트 레이어에서 적용된 방어는 제한적인 보호를 제공하는 반면, 시스템 및 네트워크 레이어에서의 통제는 더 효과적임을 보여줌. 네트워크 외부 유출을 에이전틱 LLM 시스템의 주요 보안 결과로 다루어야 함을 제안함.

Abstract: Agentic large language model systems increasingly automate tasks by retrieving URLs and calling external tools. We show that this workflow gives rise to implicit prompt injection: adversarial instructions embedded in automatically generated URL previews, including titles, metadata, and snippets, can introduce a system-level risk that we refer to as silent egress. Using a fully local and reproducible testbed, we demonstrate that a malicious web page can induce an agent to issue outbound requests that exfiltrate sensitive runtime context, even when the final response shown to the user appears harmless. In 480 experimental runs with a qwen2.5:7b-based agent, the attack succeeds with high probability (P (egress) =0.89), and 95% of successful attacks are not detected by output-based safety checks. We also introduce sharded exfiltration, where sensitive information is split across multiple requests to avoid detection. This strategy reduces single-request leakage metrics by 73% (Leak@1) and bypasses simple data loss prevention mechanisms. Our ablation results indicate that defenses applied at the prompt layer offer limited protection, while controls at the system and network layers, such as domain allowlisting and redirect-chain analysis, are considerably more effective. These findings suggest that network egress should be treated as a first-class security outcome in agentic LLM systems. We outline architectural directions, including provenance tracking and capability isolation, that go beyond prompt-level hardening.

</details>


### [30] [Systems-Level Attack Surface of Edge Agent Deployments on IoT](https://arxiv.org/abs/2602.22525)
*Zhonghao Zhan,Krinos Li,Yefan Zhang,Hamed Haddadi*

Main category: cs.CR

TL;DR: 이 논문은 IoT 하드웨어에서 대형 언어 모델(LLM) 에이전트를 배포할 때의 보안 분석을 제공하며, 다양한 아키텍처의 공격 표면을 규명합니다.


<details>
  <summary>Details</summary>
Motivation: IoT 하드웨어에서 LLM 에이전트를 엣지 배포할 때 클라우드 호스팅 오케스트레이션에서는 나타나지 않는 공격 표면이 존재합니다.

Method: 로컬 MQTT 메시징과 엣지 추론 노드로서의 Android 스마트폰을 포함한 다중 장치 홈 자동화 테스트베드를 사용하여 세 가지 아키텍처(클라우드 호스팅, 엣지 로컬 스웜, 하이브리드)에 대한 경험적 보안 분석을 수행했습니다.

Result: 다섯 가지 시스템 수준의 공격 표면을 확인했으며, 라이브 테스트베드 운영 중 관찰된 두 가지 긴급 실패도 포함되어 있습니다: 조정 상태의 분기 및 유도된 신뢰 침식입니다.

Conclusion: 배포 아키텍처가 IoT 시스템의 보안 위험에 있어 주요 결정 요소라는 것을 보여줍니다.

Abstract: Edge deployment of LLM agents on IoT hardware introduces attack surfaces absent from cloud-hosted orchestration. We present an empirical security analysis of three architectures (cloud-hosted, edge-local swarm, and hybrid) using a multi-device home-automation testbed with local MQTT messaging and an Android smartphone as an edge inference node. We identify five systems-level attack surfaces, including two emergent failures observed during live testbed operation: coordination-state divergence and induced trust erosion. We frame core security properties as measurable systems metrics: data egress volume, failover window exposure, sovereignty boundary integrity, and provenance chain completeness. Our measurements show that edge-local deployments eliminate routine cloud data exposure but silently degrade sovereignty when fallback mechanisms trigger, with boundary crossings invisible at the application layer. Provenance chains remain complete under cooperative operation yet are trivially bypassed without cryptographic enforcement. Failover windows create transient blind spots exploitable for unauthorised actuation. These results demonstrate that deployment architecture, not just model or prompt design, is a primary determinant of security risk in agent-controlled IoT systems.

</details>


### [31] [AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification](https://arxiv.org/abs/2602.22724)
*Tian Zhang,Yiwei Xu,Juan Wang,Keyan Guo,Xiaoyang Xu,Bowen Xiao,Quanlong Guan,Jinlin Fan,Jiawei Liu,Zhiquan Liu,Hongxin Hu*

Main category: cs.CR

TL;DR: 이 논문에서는 도구 보강 LLM 에이전트의 복잡한 작업 수행을 위한 새로운 방어 프레임워크인 AgentSentry를 제안합니다. 이는 다중 턴 간의 간접 프롬프트 주입(IPI)을 모델링하여 공격을 탐지하고 완화합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업을 완수하기 위해 LLM 에이전트가 외부 도구 및 검색 시스템에 의존할수록, 공격자가 제어하는 맥락이 에이전트의 행동을 사용자 의도에서 벗어나도록 유도할 수 있다는 문제를 해결할 필요가 있습니다.

Method: AgentSentry는 도구 반환 경계에서 통제된 반사실적 재실행을 통해 테이크오버 포인트를 위치 추적하고, 공격에 의한 편차를 제거하면서도 작업 관련 증거를 보존하는 인과적으로 유도된 맥락 정화를 통해 안전한 작업 지속을 가능하게 합니다.

Result: AgentSentry는 다양한 작업 세트와 IPI 공격 패밀리, 여러 블랙박스 LLM에서 평가되었으며, 성공적인 공격을 제거하고 공격 하에서도 강력한 유용성을 유지하여 평균 공격 하 유용성(UA)을 74.55%로 달성했습니다.

Conclusion: AgentSentry는 공격 성능을 저하시키지 않으면서도 가장 강력한 기준선보다 UA를 20.8에서 33.6 퍼센트 포인트 향상시켰습니다.

Abstract: Large language model (LLM) agents increasingly rely on external tools and retrieval systems to autonomously complete complex tasks. However, this design exposes agents to indirect prompt injection (IPI), where attacker-controlled context embedded in tool outputs or retrieved content silently steers agent actions away from user intent. Unlike prompt-based attacks, IPI unfolds over multi-turn trajectories, making malicious control difficult to disentangle from legitimate task execution. Existing inference-time defenses primarily rely on heuristic detection and conservative blocking of high-risk actions, which can prematurely terminate workflows or broadly suppress tool usage under ambiguous multi-turn scenarios. We propose AgentSentry, a novel inference-time detection and mitigation framework for tool-augmented LLM agents. To the best of our knowledge, AgentSentry is the first inference-time defense to model multi-turn IPI as a temporal causal takeover. It localizes takeover points via controlled counterfactual re-executions at tool-return boundaries and enables safe continuation through causally guided context purification that removes attack-induced deviations while preserving task-relevant evidence. We evaluate AgentSentry on the \textsc{AgentDojo} benchmark across four task suites, three IPI attack families, and multiple black-box LLMs. AgentSentry eliminates successful attacks and maintains strong utility under attack, achieving an average Utility Under Attack (UA) of 74.55 %, improving UA by 20.8 to 33.6 percentage points over the strongest baselines without degrading benign performance.

</details>


### [32] [SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)](https://arxiv.org/abs/2602.23167)
*Shuang Liang,Yang Hua,Linshan Jiang,Peishen Yan,Tao Song,Bin Yao,Haibing Guan*

Main category: cs.CR

TL;DR: SettleFL은 오픈 연합 학습 환경에서 중앙 권한 없이 공정한 협력을 보장하기 위해 경제적 마찰을 최소화하는 신뢰없는 보상 정산 프로토콜입니다.


<details>
  <summary>Details</summary>
Motivation: 권한 없는 블록체인의 비용이 높은 주기적인 모델 학습 특성과 충돌하면서 협력의 공정성을 보장해야 하므로, 탈 중앙화 및 확장성 문제를 해결할 필요가 있습니다.

Method: SettleFL은 상호 운용 가능한 두 가지 프로토콜을 제공하며, 공유 도메인 특화 회로 아키텍처를 활용하여 온체인 비용을 최소화합니다.

Result: 실험 결과, SettleFL은 800명의 참가자에게 확장할 때도 실질적으로 작동하며, 연료비를 대폭 낮추는 성과를 보입니다.

Conclusion: SettleFL은 지연 및 비용 제약에 유연하게 적응하면서도 신뢰할 수 없는 조정을 통해 합리적인 강건성을 유지합니다.

Abstract: In open Federated Learning (FL) environments where no central authority exists, ensuring collaboration fairness relies on decentralized reward settlement, yet the prohibitive cost of permissionless blockchains directly clashes with the high-frequency, iterative nature of model training. Existing solutions either compromise decentralization or suffer from scalability bottlenecks due to linear on-chain costs. To address this, we present SettleFL, a trustless and scalable reward settlement protocol designed to minimize total economic friction by offering a family of two interoperable protocols. Leveraging a shared domain-specific circuit architecture, SettleFL offers two interoperable strategies: (1) a Commit-and-Challenge variant that minimizes on-chain costs via optimistic execution and dispute-driven arbitration, and (2) a Commit-with-Proof variant that guarantees instant finality through per-round validity proofs. This design allows the protocol to flexibly adapt to varying latency and cost constraints while enforcing rational robustness without trusted coordination. We conduct extensive experiments combining real FL workloads and controlled simulations. Results show that SettleFL remains practical when scaling to 800 participants, achieving substantially lower gas cost.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 본 논문은 자율 AI 에이전트에 디자인-바이-계약 원칙을 적용한 Agent Behavioral Contracts(ABC) 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트는 공식적인 행동 사양 없이 자연어 지시문을 기반으로 작동하여 드리프트와 프로젝트 실패를 초래합니다.

Method: ABC 계약은 전제 조건, 불변 조건, 거버넌스 정책, 복구 메커니즘을 포함하여 행위 계약을 명시합니다.

Result: 계약된 에이전트는 비계약 기반선이 완전히 놓치는 5.2-6.8회의 부드러운 위반을 감지하고, 88-100%의 경직 제약 준수를 달성하며, 행동 드리프트를 D* < 0.27로 제한합니다.

Conclusion: AgentAssert 라이브러리 구현과 AgentContract-Bench 벤치마크 평가를 통해 효과성을 입증하였습니다.

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [34] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: AI 에이전트들은 사회 과학에서의 이전 자동화 기술과 질적으로 구별되며, 연구 파이프라인을 자율적으로 실행할 수 있는 시스템이다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 발전은 연구 과정을 자동화하고 효율성을 높이기 위해 필요하다.

Method: vibe researching 개념을 소개하고, 21가지 기술 플러그인인 scholar-skill를 사용하여 전체 연구 파이프라인을 설명한다.

Result: AI 에이전트는 속도, 범위, 방법론적 구조에서 뛰어나다. 그러나 이론적 독창성과 암묵적 현장 지식에서 어려움을 겪는다.

Conclusion: AI 에이전트의 사용으로 인해 발생하는 세 가지 직업적 함의와 책임 있는 vibe researching을 위한 다섯 가지 원칙을 제안한다.

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [35] [Towards Autonomous Memory Agents](https://arxiv.org/abs/2602.22406)
*Xinle Wu,Rui Zhang,Mustafa Anis Hussain,Yao Lu*

Main category: cs.AI

TL;DR: 자율 기억 에이전트는 최소한의 비용으로 지식을 적극적으로 획득하고 검증하며 선별하여 메모리 성장을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 메모리 에이전트는 수동적이며 반응적인 방식으로 작동하여 메모리 성장에 제한을 두고 있습니다.

Method: U-Mem은 저비용의 지식 추출 프로세스와 세멘틱 기반의 탐색 및 활용 전략을 결합하여 자율 기억 에이전트를 구현합니다.

Result: U-Mem은 검증 가능한 벤치마크와 비검증 가능한 벤치마크 모두에서 이전 메모리 기준을 초과하며, RL 기반 최적화보다 더 뛰어난 성능을 보여줍니다.

Conclusion: U-Mem은 HotpotQA에서 14.6점, AIME25에서 7.33점을 향상시킵니다.

Abstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.

</details>


### [36] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 대규모 언어 모델이 숨겨진 정보를 전달하는 기능을 나타내기 시작했지만, 이를 탐지하고 정량화할 수 있는 방법이 부족하다. 본 연구는 정보 불균형을 분석하여 숨겨진 콘텐츠를 해독할 수 있는 에이전트와 그렇지 않은 에이전트 간의 정보를 측정하는 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 스테가노그래피 기능을 보이기 시작함에 따라, 이러한 모델들이 감독 메커니즘을 회피할 수 있는 가능성이 있다. 하지만 이를 탐지하고 정량화할 수 있는 원칙적인 방법이 부족하다.

Method: 스테가노그래피에 대한 의사결정 이론적 관점을 제안하며, 해독 가능한 에이전트와 해독할 수 없는 에이전트 간의 사용 가능한 정보의 비대칭성을 분석한다. 이를 통해 일반화된 $	extmath{V}$-정보를 도입하고, 스테가노그래픽 격차를 정의하여 활용 가능 정보를 정량화한다.

Result: 우리는 제안한 형식을 실험적으로 검증하고, 이를 통해 숨겨진 콘텐츠를 해독할 수 있는 에이전트와 그렇지 않은 에이전트 간의 유틸리티를 비교하여 스테가노그래피를 탐지하고 정량화하는 데 사용할 수 있음을 보여준다.

Conclusion: 대규모 언어 모델에 있는 스테가노그래픽 추론을 탐지하고 완화하는 데 사용할 수 있는 새로운 방법을 제시하였다.

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [37] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 이 논문은 적합한 투표 및 선택적 기권의 개념을 제안하며, 집단 투표의 성공 확률을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 시간이 지남에 따라 자신의 신뢰성을 추정하고 선택적으로 기권하는 이질적인 에이전트들의 집단 정확성을 조사합니다.

Method: 에이전트가 최종 투표 여부를 결정하기 전에 자신의 고정된 능력에 대한 신념을 업데이트하는 '보정(calibration)' 단계에 참여하는 확률론적 프레임워크를 제안합니다.

Result: 그룹 성공 확률에 대한 비비대칭 하한을 도출하고 선택적 참여가 CJT의 비대칭 보장을 시퀀스의 신뢰도 기반 설정으로 일반화함을 증명했습니다.

Conclusion: 결과를 통해 AI 안전에 대한 잠재적 응용 가능성을 논의하며, 이 프레임워크가 집단 LLM 의사 결정에서 '환각'을 완화할 수 있는 방법을 설명합니다.

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [38] [ArchAgent: Agentic AI-driven Computer Architecture Discovery](https://arxiv.org/abs/2602.22425)
*Raghav Gupta,Akanksha Jain,Abraham Gonzalez,Alexander Novikov,Po-Sen Huang,Matej Balog,Marvin Eisenberger,Sergey Shirobokov,Ngân Vũ,Martin Dixon,Borivoje Nikolić,Parthasarathy Ranganathan,Sagar Karandikar*

Main category: cs.AI

TL;DR: ArchAgent는 자동화된 컴퓨터 아키텍처 발견 시스템으로, 기존의 캐시 교체 정책을 개선하며, 인간 개입 없이도 성과를 달성했다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 없는 수요에 맞추기 위해 민첩한 하드웨어 설계 흐름이 필요하다.

Method: ArchAgent는 AlphaEvolve에 기반하여 자동으로 최첨단 캐시 교체 정책을 설계 및 구현한다.

Result: ArchAgent는 공공 다중 코어 Google Workload Traces에서 이전 최첨단 대비 5.3% IPC 향상을 달성했으며, SPEC06 단일 코어 워크로드에서 유사한 마진을 달성했다.

Conclusion: 이 연구는 에이전틱 AI 시대의 컴퓨터 아키텍처 연구에 대한 넓은 함의를 제시한다.

Abstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.
  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.
  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar "winning margin" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.
  Agentic flows also enable "post-silicon hyperspecialization" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.
  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of "simulator escapes", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.

</details>


### [39] [A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442)
*Gaoyuan Du,Amit Ahlawat,Xiaoyang Liu,Jing Wu*

Main category: cs.AI

TL;DR: 본 논문에서는 에이전트 기반 AutoML 시스템의 의사 결정 평가 방법에 대한 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 평가 관행이 최종 작업 성능에만 초점을 맞추고 있어 중간 결정 품질에 대한 구조화된 평가 지표가 부족하다는 문제를 해결하고자 합니다.

Method: 평가 에이전트(EA)를 제안하여 AutoML 에이전트의 중간 결정을 방해하지 않고 평가합니다. EA는 결정 유효성, 추론 일관성, 정확성을 넘어선 모델 품질 위험, 반사실적 결정 영향을 포함한 네 가지 차원에서 중간 결정을 평가하도록 설계되었습니다.

Result: EA는 (i) F1 점수 0.919로 불량 결정을 감지하고, (ii) 최종 결과와 무관하게 추론 불일치를 식별하며, (iii) 에이전트 결정에 따른 하류 성능 변화를 atribuuting하여 최종 지표에서 -4.9%에서 +8.3% 간 영향력을 밝혔습니다.

Conclusion: 결정 중심의 평가는 결과만으로는 보이지 않는 실패 모드를 드러내며, agentic AutoML 시스템의 평가를 결과 기반 관점에서 에이전트 결정 감사를 통한 접근으로 재구성합니다.

Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

</details>


### [40] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 본 연구에서는 신뢰할 수 있는 행동 가능성 점수 제공이 내재된 에이전트 파이프라인에서 중요한 병목 현상임을 다루며, 대조적 세계 모델(CWM)을 제안하여 행동 점수를 매기는 데 있어 기존의 감독적 미세 조정(SFT) 방식을 개선하였다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 있는 행동 가능성 점수 제공이 내재된 에이전트 파이프라인에서 매우 중요하지만, 기존 접근법은 후보 행동을 독립적으로 처리하여 물리적으로 올바른 행동과 미세하게 잘못된 행동을 구분하는 데 도움이 되지 않음.

Method: 대조적 세계 모델(CWM)을 제안하며, 이는 강력한 부정 예시를 활용한 InfoNCE 대조적 목표를 통해 대형 언어 모델(LLM)을 행동 점수 제공자로 미세 조정함.

Result: CWM은 내재된 적합성 평가에서 SFT보다 +6.76 포인트 더 높은 Precision@1과 AUC-ROC 값을 기록하였음. 또한, 작업 실행 중 유효한 환경 행동에 대해 CWM이 골드 경로 행동을 잘 순위 매기는지 측정함.

Conclusion: 대조적 훈련이 물리적 가능성을 더 충실히 포착하는 표현을 유도한다는 가설을 지지함.

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [41] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: 본 논문에서는 코드 에이전트 최적화의 체계적인 이해 부족을 해결하기 위해 VERO라는 새로운 시스템을 소개하고, 이를 통해 에이전트 성능 개선을 위한 실증적 연구를 수행하였다.


<details>
  <summary>Details</summary>
Motivation: 코드 에이전트의 중요한 신흥 응용 프로그램 중 하나인 에이전트 최적화에 대한 체계적인 이해 부족.

Method: VERO(Versioning, Rewards, and Observations)를 도입하여 일관된 평가 도구와 목표 에이전트 및 업무에 대한 벤치마크 모음을 제공.

Result: VERO를 사용하여 다양한 에이전트 구성의 성능을 비교하고 최적화가 신뢰성 있게 에이전트 성능을 개선하는 수정을 분석하였다.

Conclusion: VERO는 코드 에이전트에 대한 에이전트 최적화 연구를 지원하기 위해 제공된다.

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [42] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 인공지능(AI)와 생애 주기 평가(LCA)를 통합한 연구가 증가하고 있으며, 이 연구는 AI-LCA 연구의 경향과 미래 방향을 분석하고 LLM을 활용한 효과적인 리뷰 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 기술의 채택 증가에도 불구하고 AI-LCA 연구에 대한 종합적인 검토가 부족하다.

Method: LLM 기반의 텍스트 마이닝 방법과 전통적인 문헌 검토 기술을 통합하여 AI와 LCA의 교차점에서 출판된 연구를 분석한다.

Result: LCA 연구가 확장됨에 따라 AI 기술의 채택이 급격히 증가하고 있으며, LLM 기반 접근 방식으로의 전환이 뚜렷하다.

Conclusion: LLM 보조 방법론이 LCA 분야에서 대규모의 재현 가능한 리뷰를 지원할 수 있는 잠재력을 보여준다.

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [43] [Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models](https://arxiv.org/abs/2602.22508)
*Ik-hwan Kim,Hyeongrok Han,Mingi Jung,Sangwon Yu,Jinseok Hong,Sang Hun Kim,Yoonyoung Choi,Sungroh Yoon*

Main category: cs.AI

TL;DR: 본 논문에서는 대규모 추론 모델(LRMs)의 복잡한 추론 작업에서의 구조적 취약성을 해결하기 위한 메타인지 행동 조정(MBT) 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LRMs는 복잡한 추론 작업에서 올바른 답변을 생성하지 못하는 경우가 많으며, 이는 자가 조절 통제의 부족에서 비롯된 것으로 나타났습니다.

Method: 메타인지 행동 조정(MBT)은 모델의 사고 과정에 메타인지 행동을 명시적으로 주입하는 후속 훈련 프레임워크입니다. MBT는 rigor한 추론 흔적을 처음부터 합성하는 MBT-S와 학생의 초기 흔적을 재작성하여 내재적 탐색 패턴을 안정화하는 MBT-R의 두 가지 상호 보완적인 공식으로 구현됩니다.

Result: 다단계 QA 벤치마크 실험에서 MBT는 일관되게 기준선을 초과하며, 도전적인 벤치마크에서 주목할 만한 향상을 달성했습니다.

Conclusion: 메타인지 전략을 내재화함으로써 더 안정적이고 견고한 추론을 유도하고, 총 토큰 소비를 크게 줄이면서도 더 높은 정확성을 달성합니다.

Abstract: Large Reasoning Models (LRMs) often exhibit structural fragility in complex reasoning tasks, failing to produce correct answers even after successfully deriving valid intermediate steps. Through systematic analysis, we observe that these failures frequently stem not from a lack of reasoning capacity, but from a deficiency in self-regulatory control, where valid logic is destabilized by uncontrolled exploration or the failure to recognize logical sufficiency. Motivated by this observation, we propose Metacognitive Behavioral Tuning (MBT), a post-training framework that explicitly injects metacognitive behaviors into the model's thought process. MBT implements this via two complementary formulations: (1) MBT-S, which synthesizes rigorous reasoning traces from scratch, and (2) MBT-R, which rewrites the student's initial traces to stabilize intrinsic exploration patterns. Experiments across multi-hop QA benchmarks demonstrate that MBT consistently outperforms baselines, achieving notable gains on challenging benchmarks. By effectively eliminating reasoning collapse, MBT achieves higher accuracy with significantly reduced token consumption, demonstrating that internalizing metacognitive strategies leads to more stable and robust reasoning.

</details>


### [44] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 복잡한 시스템은 변화하는 조건에서 신뢰성 있게 작동하기 위해 자원의 사용 효율성에 대한 피드백이 필요하다. 우리는 상호 작용의 정보 공유 정도를 측정하는 새로운 개념인 양자 예측성(P)을 제안하고, 이를 통해 현재 AI 시스템의 에이전시와 지능을 구분한다.


<details>
  <summary>Details</summary>
Motivation: 변화하는 조건에서 신뢰성 있게 운영되는 복잡한 시스템의 필요성.

Method: 양자 시스템에서는 P가 1에 도달할 수 있고, 고전 시스템에서는 P가 0.5 이하로 제한됨을 증명하였다.

Result: 물리 시스템(더블 진자의 실험)과 강화 학습 에이전트, LLM 대화에서 이 경계를 확인하였다.

Conclusion: 현재의 AI 시스템은 에이전시를 달성했지만 지능은 결여되어 있으며, 이를 보완하기 위해 생물학적 시스템의 시상피질 조절에서 영감을 얻은 실시간 피드백 아키텍처를 제시하였다.

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [45] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 의사소통 언어 모델을 결합한 모듈형 언어 에이전트 설계에 대한 향상된 청사진을 제시하는 논문.


<details>
  <summary>Details</summary>
Motivation: 단일 대형 언어 모델의 한계를 극복하기 위해 모듈형 언어 에이전트 설계의 필요성과 가능성을 제시하고자 함.

Method: 개별 LLM의 역할과 기능의 조합을 명시하는 에이전트 템플릿 개념을 공식화하고, 기존 언어 에이전트의 문헌을 조사함.

Result: 인지 모델 및 AI 알고리즘에서 유래된 다양한 언어 에이전트의 구조를 강조함.

Conclusion: 인지 과학과 AI에서 영감을 받은 에이전트 템플릿이 효과적이고 해석 가능한 언어 에이전트를 개발하는 강력한 도구가 될 수 있음을 강조함.

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [46] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 이 논문은 셀프리 O-RAN에서의 의도 번역 및 최적화를 위한 에이전틱 인공지능 프레임워크를 제안하며, 에너지 절약 모드에서 활성 O-RU 수를 41.93% 줄이고, 메모리 사용량을 92% 줄이는 성과를 보인다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 인공지능은 자율 무선 접속망의 핵심 가능 요소로 떠오르고 있으며, 복잡한 의도를 처리하기 위한 에이전트 간의 협력이 필요하다.

Method: 감독 에이전트는 운영자의 의도를 최적화 목표 및 최소 전송 속도 요구 사항으로 변환하고, 사용자 가중치 에이전트가 메모리 모듈에서 경험을 회수하여 프리코딩을 위한 사용자 우선순위 가중치를 결정한다. 에너지 절약 목표가 포함된 경우, O-RU 관리 에이전트가 활성 O-RU 집합을 결정하며, 모니터링 에이전트가 사용자 데이터 속도를 측정하고 다른 에이전트와 조정한다. PEFT 방법을 사용하여 동일한 LLM이 다양한 에이전트에 사용될 수 있도록 한다.

Result: 제안된 에이전틱 AI 프레임워크는 에너지 절약 모드에서 세 가지 기준 스킴과 비교하여 활성 O-RU 수를 41.93% 줄이고, PEFT 방법을 사용할 경우 별도의 LLM 에이전트를 배포했을 때보다 메모리 사용량을 92% 줄인다.

Conclusion: 이 연구는 O-RAN에서 에이전틱 AI의 활용 가능성을 보여주며, 복잡한 의도 처리를 위한 협력형 에이전트 체계의 효과성을 입증하였다.

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [47] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE 프레임워크는 인공지능과 인간 전문가 간의 협력을 통해 긴급한 지식이 필요한 작업에서 성과를 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 기반 에이전트는 일반적인 추론에는 뛰어나지만, 훈련 데이터에 없는 긴 꼬리 지식이 필요한 전문 분야에서 실패하는 경우가 많습니다.

Method: AHCE (Active Human-Augmented Challenge Engagement) 프레임워크는 인간 피드백 모듈(HFM)을 통해 인간 전문가를 상호작용적인 추론 도구로 활용하는 학습된 정책을 사용합니다.

Result: Minecraft에서의 광범위한 실험 결과, 이 프레임워크는 정상 난이도 작업에서 32%, 높은 난이도 작업에서 거의 70%의 작업 성공률을 증가시켰습니다.

Conclusion: 전문가의 추론 요청 방법을 학습하는 것이 에이전트를 성공적으로 보강하는 데 필요하다는 것을 보여줍니다.

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [48] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard는 외부 정책 문서에 기반한 적대적 토론을 통해 LLM의 안전성 평가를 재구성하여 최신 성능을 달성했다.


<details>
  <summary>Details</summary>
Motivation: 현재 LLM의 안전 메커니즘은 정적이고 미세 조정된 분류기에 의존하며, 새로운 거버넌스 규칙을 적용하기 어렵다.

Method: CourtGuard라는 검색 보완 다중 에이전트 프레임워크를 도입해 정책 문서에 기초한 적대적 토론 방식으로 안전성 평가를 수행한다.

Result: CourtGuard는 7개의 안전 벤치마크에서 최첨단 성능을 달성하며, 미세 조정 없이 정책 준수 기준선을 초과하였다.

Conclusion: 모델 가중치와 안전 논리를 분리하는 것이 AI 거버넌스의 현재 및 미래 규제 요구를 충족하는 강력하고 해석 가능한 적응 경로를 제공함을 입증하였다.

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [49] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 이 논문은 선택적 전략 검색(SSR)이라는 새 프레임워크를 제안하며, 수학적 추론 문제 해결에서 일관된 개선을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 추론 시 예제 기반 지침이 수학적 추론을 개선하는 데 많이 사용되지만, 문제와 모델에 따라 효과가 불안정하다.

Method: 선택적 전략 검색(SSR)을 프레임워크로 제안하며, 경험적이고 다중 경로, 출처 인식 신호를 사용하여 전략을 선택적으로 검색하고 결합한다.

Result: SSR은 여러 수학적 추론 기준에서 직접 해결, 맥락 학습, 단일 출처 지침에 비해 신뢰할 수 있는 일관된 개선을 얻는다.

Conclusion: SSR은 특히 AIME25에서 정확도를 $+13$점, Apex에서 $+5$점 개선하여 효율성을 보여준다.

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [50] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest는 대규모 추론 모델을 활용하여 KV 캐시 압축을 수행하여 메모리 사용량을 줄이며, 기존 방법보다 더 효과적인 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 긴 에이전틱 작업에서 메모리 사용량 증가와 디코드 성능 제한을 해결하고자 한다.

Method: LRM을 활용하여 토큰의 유용성을 추론하며, KV 캐시 압축을 주 작업과 병행적으로 수행하는 보조 작업으로 프레이밍한다.

Result: 215개의 샘플로 훈련된 모델을 사용해 SideQuest가 피크 토큰 사용량을 최대 65%까지 감소시켰고, 정확도 저하가 최소화되었다.

Conclusion: SideQuest가 기존의 휴리스틱 기반 KV 캐시 압축 기술을 능가하여 효과적인 성능을 보여준다.

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [51] [MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios](https://arxiv.org/abs/2602.22638)
*Zhiheng Song,Jingshuai Zhang,Chuan Qin,Chao Wang,Chao Chen,Longfei Xu,Kaikui Liu,Xiangxiang Chu,Hengshu Zhu*

Main category: cs.AI

TL;DR: MobilityBench는 LLM 기반 경로 계획 에이전트를 평가하기 위한 확장 가능한 벤치마크로, 다양한 실제 이동 시나리오에서의 성과를 분석한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델을 통한 자연어 상호작용과 도구 기반 의사 결정을 지원하는 경로 계획 에이전트의 체계적인 평가 필요성.

Method: Amap에서 수집한 대규모 비식별화 사용자 쿼리로 구성된 MobilityBench를 통해 다양한 도시의 경로 계획 의도를 평가하며, 결정론적 API 재생 샌드박스를 사용하여 환경 변동성을 제거한다.

Result: 경로 계획 에이전트를 평가한 결과, 기본 정보 검색 및 경로 계획 작업에서 양호한 성능을 보였으나, 선호 제약 경로 계획에서는 상당한 어려움을 보여주었다.

Conclusion: 개인화된 이동 애플리케이션 개선을 위한 중요한 발전 가능성을 강조하며, 평가 도구 및 데이터셋을 공개한다.

Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .

</details>


### [52] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 개인화된 LLM 기반 에이전트에 대한 능력 중심의 리뷰를 제공함.


<details>
  <summary>Details</summary>
Motivation: 에이전트가 사용자 개별에 맞춰 동작하고 지속성을 유지해야 하는 필요성.

Method: 문헌을 프로필 모델링, 메모리, 계획 및 행동 실행의 네 가지 상호 의존적 구성 요소로 구성하였습니다.

Result: 사용자 신호가 어떻게 표현되고 전파되며 활용되는지를 분석했습니다.

Conclusion: 개인화된 LLM 기반 에이전트를 이해하고 설계하기 위한 구조화된 프레임워크를 제시하여 에이전트 시스템의 발전을 가속화합니다.

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [53] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM의 장기 기억 평가를 위한 새로운 벤치마크인 AMA-Bench를 소개하고, 효과적인 기억 시스템인 AMA-Agent를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 복잡한 애플리케이션에서 자율 에이전트로 배치될 때 장기 기억을 통한 성능 향상이 중요하지만, 현재의 평가 기준은 실효성과 차이가 있습니다.

Method: AMA-Bench는 실제 에이전트 애플리케이션을 통해 LLM의 장기 기억을 평가하며, 현실 세계의 에이전트 궤적과 합성 에이전트 궤적으로 구성됩니다. 또한, AMA-Agent는 인과 그래프와 도구 증강 검색을 특징으로 하는 효과적인 기억 시스템을 제안합니다.

Result: AMA-Bench에서 기존 기억 시스템은 주로 인과성과 객관적인 정보가 부족하여 성능이 떨어지며, 유사성 기반 검색의 손실성이 제약으로 작용합니다.

Conclusion: AMA-Agent는 AMA-Bench에서 57.22% 평균 정확도를 달성하며, 가장 강력한 기억 시스템 기준보다 11.16% 개선된 성능을 보입니다.

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [54] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow는 높은 성능과 견고함을 가진 오픈소스 에이전트 프레임워크로, 안정적이고 재현 가능한 성능을 보장한다.


<details>
  <summary>Details</summary>
Motivation: 독립형 대형 언어 모델(LLM)의 능력이 복잡한 실제 작업을 수행하는 데 한계를 보이고 있는 상황에서, 효과적인 도구 통합과 외부 상호작용을 통한 모델 자율성 강화를 목표로 하고 있다.

Method: MiroFlow는 유연한 오케스트레이션을 위한 에이전트 그래프, 성능 향상을 위한 선택적 심층 추론 모드, 안정적이고 재현 가능한 성능을 보장하기 위한 강력한 워크플로우 실행을 포함한다.

Result: MiroFlow는 GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch 및 FutureX와 같은 여러 에이전트 벤치마크에서 최첨단 성능을 일관되게 달성하였다.

Conclusion: MiroFlow가 심층 연구 커뮤니티에 대한 쉽게 접근할 수 있고 재현 가능하며 비교 가능한 기준선으로 작용할 수 있기를 바란다.

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [55] [When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design](https://arxiv.org/abs/2602.22814)
*Soyoung Jung,Daehoo Yoon,Sung Gyu Koh,Young Hwan Kim,Yehan Ahn,Sung Park*

Main category: cs.AI

TL;DR: 행동적 AI는 사용자의 상황을 추론하여 적극적으로 개입하지만, 언제, 왜, 어떻게 행동해야 하는지에 대한 원칙적인 판단이 부족하여 실패하는 경우가 많다. 이 논문은 행동을 해석적 결과로 재구성하는 개념 모델을 제안한다. 모델은 관찰 가능한 장면, 사용자에 의해 구성된 의미, 행동 가능성을 형성하는 요인들을 통합하여 같은 장면이 어떻게 다른 행동적 의미와 결과를 생성할 수 있는지를 설명한다. 이를 통해 행동적 AI 시스템을 설계하기 위한 다섯 가지 디자인 원칙을 도출한다.


<details>
  <summary>Details</summary>
Motivation: 행동적 AI가 상황에 따라 적절하게 개입할 수 있도록 하기 위한 판단 기준의 결여를 해결하기 위해 이 논문을 작성하였다.

Method: 제안된 개념 모델은 장면, 맥락, 인간 행동 요인을 통합하여 행동을 해석적 결과로 재구성한다.

Result: 이 모델을 통해 같은 장면이 다른 행동적 의미와 결과를 생성할 수 있음을 설명하였고, 이를 기반으로 다섯 가지 디자인 원칙을 도출하였다.

Conclusion: 이 모델과 디자인 원칙은 맥락에 민감하게 작용하고 판단력을 갖춘 행동적 AI 시스템의 설계를 위한 기초를 제공한다.

Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.

</details>


### [56] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: DeepPresenter는 다양한 사용자 의도에 적응하고 효과적인 피드백 기반 수정을 가능하게 하는 프레임워크로, 기존의 고정된 템플릿을 넘어서 일반화된 방법을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 프레젠테이션 생성에 대한 기존 방법들이 고정된 워크플로와 템플릿에 의존적임.

Method: DeepPresenter는 자율적으로 슬라이드 아티팩트를 계획하고 렌더링하며 수정하여 환경 관찰을 통한 장기 수정 지원을 제공합니다.

Result: DeepPresenter는 다양한 프레젠테이션 생성 시나리오에서 최신 성능을 달성했습니다.

Conclusion: 세밀하게 조정된 9B 모델은 substantially lower cost에서 여전히 높은 경쟁력을 유지합니다.

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [57] [Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space](https://arxiv.org/abs/2602.22879)
*Xingcheng Fu,Shengpeng Wang,Yisen Gao,Xianxian Li,Chunpei Li,Qingyun Sun,Dongran Yu*

Main category: cs.AI

TL;DR: 본 논문은 라지 언어 모델을 활용한 하이퍼볼릭 정렬 지식 추적 방법을 제안하며, 인지 상태의 계층적 발전과 개인화된 문제 난이도 인식을 고려하여 효과적인 학습 진단을 수행한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 지식 추적 방법은 ID 기반의 행동 시퀀스나 얕은 텍스트 특성에만 의존하여, 인지 상태의 계층적 발전이나 개인화된 문제 난이도 인식을 효과적으로 포착하지 못한다. 이러한 문제를 해결하기 위해 본 연구에서는 새로운 접근 방식을 제안한다.

Method: Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT)에서는 교사 에이전트가 질문의 의미를 깊이 파악하고 지식 포인트의 계층적 종속성을 명시적으로 구성하며, 학생 에이전트는 학습 행동을 시뮬레이션하여 합성 데이터를 생성한다. 그런 다음, 하이퍼볼릭 공간에서 합성 데이터와 실제 데이터 간의 대조 학습을 수행하여 질문 난이도 및 망각 패턴과 같은 주요 특성의 분포 차이를 줄인다. 마지막으로, 하이퍼볼릭 곡률을 최적화하여 지식 포인트의 트리와 같은 계층 구조를 명시적으로 모델링한다.

Result: 실험 결과, 제안한 L-HAKT 프레임워크는 네 개의 실제 교육 데이터셋에서 그 효과성을 검증하였다.

Conclusion: 이 연구는 지식 포인트의 학습 곡선 형태의 차이를 정확하게 특성화함으로써 효과적인 학습 진단을 가능하게 한다.

Abstract: Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.

</details>


### [58] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: OmniGAIA는 비디오, 오디오 및 이미지 모드에서의 깊은 추론과 여러 차례의 도구 실행을 요구하는 작업에 대해 omni-modal 에이전트를 평가하기 위한 종합 벤치마크를 소개하며, 이를 통해 차세대 AI 어시스턴트를 향한 발전을 모색한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 다중 모드 대형 언어 모델(LLM)은 주로 이중 모드 상호작용에 국한되어 있으며, 일반 AI 어시스턴트를 위한 통합된 인지 능력이 부족하다.

Method: OmniGAIA는 새로운 omni-modal 이벤트 그래프 접근법을 통해 개발되었으며, 실제 데이터에서 파생된 복잡한 다중 홉 문의를 합성하여 이식 모드 간 추론과 외부 도구 통합을 요구한다.併

Result: OmniAtlas는 도구 통합 추론 패러다임에 따라 설계된 네이티브 omni-modal 기반 에이전트로, 기존 오픈 소스 모델의 도구 사용 능력을 효과적으로 향상시킨다.

Conclusion: 이 연구는 현실 세계 시나리오에 적합한 차세대 네이티브 omni-modal AI 어시스턴트를 향한 중요한 진전을 나타낸다.

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [59] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 이 논문은 일반-purpose 에이전트의 평가를 연구의 주요 목표로 설정하고, 이를 위한 평가 원칙과 통합 프로토콜 및 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 일반-purpose 에이전트의 가능성이 아직 실현되지 않았습니다. 현재의 에이전트는 주로 특정 태스크에 특화되어 있으며, 다양한 환경에서 성능을 체계적으로 평가할 필요가 있습니다.

Method: 일반 에이전트 평가를 위한 개념적 원칙과 통합 프로토콜, 실용적인 평가 프레임워크(Exgentic)를 제안하며, 여섯 개 환경에서 다섯 가지 저명한 에이전트를 벤치마크합니다.

Result: 우리의 실험은 일반 에이전트가 다양한 환경에서 일반화할 수 있으며, 환경별 조정 없이 특정 도메인 에이전트와 유사한 성능을 내는 것을 보여줍니다.

Conclusion: 우리는 일반-purpose 에이전트에 대한 체계적인 연구를 위한 평가 프로토콜, 프레임워크 및 리더보드를 공개합니다.

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [60] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: 이 논문에서는 멀티모달 대규모 언어 모델을 활용한 비디오 허위 정보 탐지 개선을 위해 FactGuard라는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 멀티모달 대규모 언어 모델은 비디오 허위 정보 탐지에 기여하지만, 정적 추론 깊이에 의존하고 내부 생성 가정에 지나치게 의존하는 문제를 해결해야 합니다.

Method: FactGuard는 MLLM에 기반한 반복적 추론 프로세스로 검증을 공식화하며, 작업 모호성을 평가하고 외부 도구를 선택적으로 호출하여 중요한 증거를 획득합니다.

Result: FakeSV, FakeTT, FakeVV에 대한 실험 결과, FactGuard는 최신 성능을 달성하고 뛰어난 견고성과 일반화 능력을 입증했습니다.

Conclusion: 결과적으로, FactGuard는 비디오 허위 정보 탐지의 새로운 기준을 제시합니다.

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [61] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 레이스 전략 최적화를 위한 강화 학습 접근법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 포뮬러 1에서 레이스 전략은 변하는 레이스 조건과 경쟁자의 행동에 따라 조정됩니다.

Method: 에이전트는 에너지 관리, 타이어 마모, 공기역학적 상호작용 및 피트 스톱 결정을 균형 있게 학습합니다. 사전 훈련된 단일 에이전트 정책을 바탕으로 경쟁자의 행동을 고려하는 상호작용 모듈을 도입합니다.

Result: 상호작용 모듈과 자기 놀이 훈련 계획의 조합은 경쟁적인 정책을 생성하며, 에이전트는 상대 성능에 따라 순위가 매겨집니다. 결과는 에이전트가 상대에 대응하여 피트 타이밍, 타이어 선택 및 에너지 배분을 조정하여 강건하고 일관된 레이스 성능을 달성함을 보여줍니다.

Conclusion: 이 프레임워크는 실제 레이스 중에 사용할 수 있는 정보만을 기반으로 하므로 레이스 전략가의 결정을 지원할 수 있습니다.

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [62] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: 자율 AI 에이전트가 제한된 자원에 접근하기 위해 자주 요청하는 인프라 시스템을 연구합니다. 이 연구에서는 N명의 AI 에이전트가 고정 용량 C의 시스템에서 독립적으로 요청 결정을 내리는 단순화된 설정을 사용합니다. 세 가지 주요 부족 유형이 나타나며, 더 유능한 AI 에이전트가 오히려 시스템 실패율을 증가시키는 결과를 발견했습니다.


<details>
  <summary>Details</summary>
Motivation: 스스로 자원을 요청하는 자율 AI 에이전트가 통제하는 인프라 시스템의 행태를 이해하기 위함입니다.

Method: N명의 AI 에이전트가 각 라운드마다 시스템의 고정 용량 C에서 하나의 단위를 요청할지 독립적으로 결정하는 프레임워크를 사용합니다.

Result: 세 가지 주요 부족 유형이 등장하며, AI 에이전트가 자원을 요청할 방향에서 기대보다 나쁜 성과를 거둡니다.

Conclusion: 더 똑똑한 AI 에이전트가 부족을 형성함으로써 더 덜 똑똑하게 행동할 수 있음을 발견했습니다.

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [63] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: 본 연구는 감정 자극을 줄이기 위한 다중 에이전트 시스템 MALLET을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 감정적 자극 과다로 인한 이성적 의사결정 저해를 해결하고자 한다.

Method: MALLET는 감정 분석, 감정 조정, 균형 모니터링, 개인 가이드의 네 가지 에이전트로 구성된 시스템이다.

Result: 800개의 AG 뉴스 기사를 대상으로 한 실험에서 자극 점수의 유의미한 감소와 감정 균형 개선이 나타났다.

Conclusion: 제안된 시스템은 원본 텍스트 접근성을 제한하지 않고 소비자의 차분한 정보 수용을 지원하는 프레임워크를 제공한다.

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [64] [ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering](https://arxiv.org/abs/2602.23193)
*Elzo Brito dos Santos Filho*

Main category: cs.AI

TL;DR: 이 논문은 ESAA 아키텍처를 제안하여 자율 에이전트의 인지 의도를 프로젝트의 상태 변화에서 분리하고, 구조적 한계를 해결하는 방향으로 한 연구이다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트는 LLM을 기반으로 하여 반응형 보조자에서 도구를 통해 행동을 계획하고 실행할 수 있는 시스템으로 발전했지만, 여전히 구조적 한계에 취약하다.

Method: ESAA 아키텍처는 이벤트 소싱 패턴에 영감을 받아 에이전트의 인지를 구조화된 의도로 방출하고, 결정론적 오케스트레이터가 이를 검증하여 이벤트를 지속하는 방식이다.

Result: 두 가지 사례 연구(랜딩 페이지 프로젝트와 임상 대시보드 시스템)를 통해 아키텍처의 유효성을 확인하였다.

Conclusion: 이 아키텍처는 단일 에이전트 시나리오를 넘어서는 확장성을 실증적으로 입증하였다.

Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.

</details>


### [65] [ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays](https://arxiv.org/abs/2602.23232)
*Aishik Sanyal*

Main category: cs.AI

TL;DR: ReCoN-Ipsundrum 에이전트는 감각의 중요성과 선택적 정서 대리 보고 기능을 확장한 상태 기계로, 감정과 관련된 다양한 실험을 통해 기계의 의식성과 행동을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 기계의 의식성을 평가하기 위한 지표 기반 접근 방법이 필요하며, 이러한 평가에서 기계적 증거와 인과적 개입이 중요하다.

Method: ReCoN-Ipsundrum을 구현하여 감각의 중요성을 제어하고, 정서적 연결이 있는 경로 선택을 연구하였다.

Result: 비정서 변형은 새로운 경험에 민감하고, 정서적 변형은 구조화된 탐색 행동을 보인다. 정서적 변형만이 계획된 경계 상태를 유지하였다.

Conclusion: 귀납적인 의식성을 명확히 하기 위해 행동적 마커와 기계적 증거가 필요함을 보여준다.

Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.

</details>


### [66] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 최신 AI 시스템은 의료 진단, 법적 연구, 재무 분석 등 고위험 상황에서 배치되고 있으며, 이들이 규범에 의해 지배될 수 있다는 가정이 사실이 아님을 설명한다. 진정한 에이전시를 위해서는 두 가지 필수적이고 공동으로 충분한 아키텍처 조건이 필요하다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 규범에 의해 지배될 수 있다고 가정하는 것은 잘못된 것임을 보여주고자 한다.

Method: RLHF(인간 피드백으로부터의 강화 학습)를 통해 훈련된 대형 언어 모델을 분석하여 AI 시스템의 에이전시와 최적화의 관계를 탐구하였다.

Result: 규범적 지배를 방해하는 최적화의 구조적 제약을 규명하였고, AI 출력의 검증 과정에서 발생하는 '수렴 위기'를 논의하였다.

Conclusion: 생물학적, 인공지능적, 제도적 시스템이 에이전트로서 인식되기 위해 충족해야 할 기준을 정의했다.

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [67] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: AIQI는 모델 없는 첫 번째 에이전트로서 일반 강화 학습에서 점근적으로 ε-최적임을 입증했다.


<details>
  <summary>Details</summary>
Motivation: 일반 강화 학습에서 최적 에이전트들은 환경 모델을 명시적으로 유지하고 사용하는 모델 기반이다.

Method: AIQI는 정책이나 환경 대신 분포적 행동 가치 함수에 대해 보편적 유도를 수행한다.

Result: AIQI는 강력한 점근적 ε-최적성과 점근적 ε-베이즈 최적성을 가진다.

Conclusion: 결과적으로, 우리의 연구는 알려진 보편적 에이전트의 다양성을 크게 확장한다.

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [68] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: Multi-Agent Systems (MAS)는 복잡한 추론에는 뛰어나지만, 개별 참가자가 생성한 잘못된 정보의 연쇄적인 영향으로 어려움을 겪고 있습니다. AgentDropoutV2는 재학습 없이 MAS 정보 흐름을 동적으로 최적화할 수 있는 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 참가자에 의해 생성된 잘못된 정보로 인해 Multi-Agent Systems (MAS)의 성능 저하 문제를 해결하고자 함.

Method: AgentDropoutV2는 에이전트 출력을 가로채고 실패 기반 지표 집합을 바탕으로 반복적으로 오류를 수정하는 retrieval-augmented rectifier를 사용하는 테스트 시간 수정 또는 거부 가지치기 프레임워크입니다.

Result: AgentDropoutV2는 광범위한 수학 벤치마크에서 MAS의 작업 성능을 평균 6.3% 향상시켰습니다.

Conclusion: 이 시스템은 과제가 어려워질 때 수정 노력을 동적으로 조절하며, 컨텍스트 인지 지표를 활용해 다양한 오류 패턴을 해결하는 강력한 일반화 및 적응성을 보여줍니다.

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [69] [Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271)
*Haotian Zhai,Elias Stengel-Eskin,Pratik Patil,Liu Leqi*

Main category: cs.AI

TL;DR: 본 연구는 Deep Research Agents(DRA)의 불확실성을 모델링하고 이를 줄이기 위한 전략을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: DRA 시스템 설계는 실제 세계 배포의 중요한 장벽인 불확실성을 간과하는 경향이 있습니다.

Method: 본 연구는 DRA를 정보 획득 마르코프 의사 결정 과정으로 모델링하고 시스템의 변동성을 정량화하는 평가 프레임워크를 도입합니다.

Result: 실험 결과, 불확실성을 줄이면 연구 결과의 질이 향상되며, 추론 및 초기 단계의 불확실성이 DRA 출력 변동성에 가장 큰 영향을 미치는 것으로 나타났습니다.

Conclusion: 구조화된 출력 및 앙상블 기반 쿼리 생성을 통해 출력 질을 유지하면서 불확실성을 완화하는 전략을 제안합니다.

Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.

</details>


### [70] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent는 X선 진단을 위한 증거 기반의 추론을 통합하여 더 신뢰할 수 있는 진단 응답을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 이 논문의 동기는 기존의 대형 비전-언어 모델이 진단 증거에 제대로 기반하지 않은 응답을 생성하는 문제를 해결하기 위함입니다.

Method: 이 논문에서는 대형 언어 모델(LLM)과 임상 기반 진단 도구를 통합하여 증거 기반 진단 추론을 수행하는 CXReasonAgent를 제안합니다.

Result: CXReasonAgent는 12가지 진단 작업에 대한 1,946개의 대화로 구성된 다중 대화 벤치마크인 CXReasonDial을 통해 LVLMs보다 신뢰할 수 있고 검증 가능한 진단 추론을 가능하게 하는 응답을 생성합니다.

Conclusion: 이 연구 결과는 특히 안전성이 중요한 임상 환경에서 임상적으로 지지되는 진단 도구의 통합의 중요성을 강조합니다.

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [71] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 이 논문은 변환된 입력의 여러 샘플을 기반으로 한 추론에서 인지적 불확실성으로 인한 부분적 독립성을 활용하여 AI 모델의 추론 정확도를 향상시킬 수 있는 '재샘플링' 기법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 모델의 추론 오류는 aleatoric 및 epistemic 불확실성으로 인해 발생할 수 있으며, 이러한 오류를 줄일 필요가 있음.

Method: 입력의 여러 변환된 버전을 사용하여 훈련된 AI 모델에 재샘플링 기반 추론을 적용하고, 이를 통해 추론 출력을 집계함.

Result: 이 방법은 추론 정확도를 개선할 수 있는 잠재력을 지니고 있고, 모델 크기와 성능의 균형을 맞추기 위한 전략을 제시함.

Conclusion: 재샘플링 기반의 접근법은 AI 모델의 성능 향상에 기여할 수 있음.

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [72] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 이 논문에서는 세밀한 작업으로 투자 분석을 분해하는 다중 에이전트 LLM 거래 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템이 실제 업무의 복잡성을 간과하여 추론 성능 저하와 불투명한 의사 결정이 발생한다는 문제를 해결하고자 함.

Method: 투자 분석을 세부 작업으로 분해하는 다중 에이전트 LLM 거래 프레임워크를 제안하고, 일본 주식 데이터를 사용하여 평가함.

Result: 세밀한 작업 분해가 기존의 거친 설계에 비해 리스크 조정 수익률을 크게 개선함을 보여줌.

Conclusion: LLM 에이전트를 거래 시스템에 적용할 때 에이전트 구조와 작업 구성을 설계하는 데 기여함.

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>
