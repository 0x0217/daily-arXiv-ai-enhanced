<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 110]
- [cs.CR](#cs.CR) [Total: 46]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: 본 논문에서는 게임 플레이 비디오로부터 신경-상징적 세계 모델을 학습하는 새로운 접근법인 유한 오토마타 추출(FAE)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 세계 모델은 환경의 압축된 공간 및 시간 학습 표현으로 정의됩니다. 기존의 신경망 기반 접근법은 학습된 환경 동역학의 전이와 설명 가능성을 어렵게 만듭니다.

Method: 유한 오토마타 추출(FAE) 접근법은 새로운 도메인 특화 언어(DSL)인 레트로 코드로 표현된 게임 플레이 비디오로부터 신경-상징적 세계 모델을 학습합니다.

Result: FAE는 이전 DSL 기반 접근법보다 더 정밀한 환경 모델과 더 일반적인 코드를 학습합니다.

Conclusion: FAE는 기존의 세계 모델 접근법에 비해 보다 정확한 환경 모델을 제공하여 신경-상징적 방법론의 장점을 보여줍니다.

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [2] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: EvoCut은 진화적 탐색과 대형 언어 모델을 결합하여 정수 프로그래밍에 대한 가속 컷 생성 과정을 자동화하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 정수 프로그래밍은 NP-hard 문제로 중요한 조합 최적화 작업에 있어서 도전 과제가 된다.

Method: EvoCut은 LLM 기반 초기화 제너너를 통해 후보 컷의 다양한 인구를 초기화하고, 각 컷을 검증 세트에서 최적 해의 보존 여부 및 분수 해결책을 차단하는 능력을 평가하며, 진화적 교배 및 돌연변이 제너너를 통해 인구를 반복적으로 정제한다.

Result: EvoCut은 고정된 시간 내에 최적성 차이를 17-57% 줄이며, 최대 4배 빠른 속도로 동일한 해를 도출하고, 같은 시간 제한 내에서 더 높은 품질의 해를 얻는다.

Conclusion: EvoCut은 인간 전문가의 입력 없이도 신뢰성 있게 컷을 생성, 개선 및 검증하며, 새로운 인스턴스에 일반화 가능한 컷을 제공한다.

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [3] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC는 제약이 있는 회귀 합성 계획을 위해 설계된 최초의 대형 언어 모델 기반 프레임워크로, 기계적 피드백을 통해 경로 생성을 안내하고 제약을 평가하여 성공률을 높입니다.


<details>
  <summary>Details</summary>
Motivation: 화학에서 상업적으로 사용 가능한 출발 물질에서 원하는 목표 분자까지의 합성 경로를 파악하는 것은 매우 중요하지만 도전적인 과정입니다.

Method: 우리는 LARC라는 첫 번째 LLM 기반 에이전트 프레임워크를 제안하며, 이는 에이전트-판사 방식으로 제약 평가를 통합합니다.

Result: LARC는 48개의 제약 회귀 합성 계획 작업에서 72.9%의 성공률을 달성하며, LLM 기준보다 훨씬 뛰어난 성능을 보입니다.

Conclusion: LARC 프레임워크는 확장 가능하며, 제약 회귀 합성을 위한 인간 전문가와의 효과적인 에이전트 도구 또는 공동 과학자로 나아가는 첫 번째 단계가 됩니다.

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [4] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: 대형 언어 모델의 발전이 의료 분야에서의 채택을 가속화했지만, 전문 지식과 정확성이 필요한 의료 작업을 위해 QuarkMed 모델이 개발되었다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 발전에도 불구하고, 의료 작업은 전문 지식과 높은 정확성을 필요로 하며, 이는 신뢰할 수 있는 기초 모델의 필요성을 나타냅니다.

Method: QuarkMed는 의료 데이터 처리, 의료 콘텐츠 검색 기반 생성 (RAG), 그리고 검증 가능한 대규모 강화 학습 파이프라인을 활용하여 고성능 의료 기초 모델을 개발합니다.

Result: QuarkMed 모델은 중국 의사 면허 시험에서 70%의 정확도를 기록하며, 다양한 의료 기준에서 강력한 일반화 능력을 보여주었습니다.

Conclusion: QuarkMed는 강력하고 다양한 개인 의료 AI 솔루션을 제공하며, 이미 수백만 명의 사용자에게 서비스되고 있습니다.

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [5] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: 이 논문에서는 대형 언어 모델의 전략적 추론 능력을 평가하기 위한 새로운 평가 프레임워크인 인지 계층 벤치마크(CHBench)를 제안한다. 이 프레임워크는 다양한 게임에서 LLM의 전략적 추론을 평가하며, 두 가지 주요 메커니즘의 영향을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 전략적 추론 능력을 평가하는 기존 방법의 한계를 극복하기 위해 새로운 프레임워크 개발이 필요하다.

Method: CHBench는 인지 계층 모델을 바탕으로 하여 세 가지 단계를 통해 LLM의 전략적 추론을 평가한다.

Result: LLM은 다양한 상대방과의 게임에서 일관된 전략적 추론 수준을 보이며, 두 가지 메커니즘이 성능에 미치는 영향을 분석하였다.

Conclusion: CHBench는 LLM 능력을 평가하는 유망한 도구로, 향후 연구 및 실제 응용에 큰 잠재력을 지닌다.

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [6] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: 본 논문에서는 대형 언어 모델 기반 다수의 에이전트 시스템에서의 협력적 의사결정을 개선하기 위한 프레임워크인 AgentCDM을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 협력적 의사결정(CDM)의 핵심 과정이 충분히 탐구되지 않았으며, 기존 접근 방식은 단일 에이전트의 인지 편향에 취약하거나 집단 지성을 완전히 활용하지 못하는 문제점을 가지고 있습니다.

Method: AgentCDM은 인지 과학의 경쟁 가설 분석(ACH)에서 영감을 받아, 인지 편향을 체계적으로 완화하고 의사결정을 수동적인 답변 선택에서 능동적인 가설 평가 및 구축으로 전환하는 구조적 추론 패러다임을 도입합니다. 이를 위해 두 단계의 훈련 패러다임을 개발하였습니다: 첫 번째 단계는 구조적 추론을 안내하기 위한 ACH 기반의 명시적 구조물 스캐폴딩을 사용하며, 두 번째 단계에서는 이 스캐폴딩을 점진적으로 제거하여 자율적인 일반화를 유도합니다.

Result: 여러 벤치마크 데이터셋에 대한 실험 결과, AgentCDM은 최첨단 성능을 달성하고 강력한 일반화를 나타내어 MAS에서 협력적 결정의 질과 견고성을 개선하는 데 효과적임을 검증하였습니다.

Conclusion: AgentCDM은 MLL 기반 다수의 에이전트 시스템에서 협력적 의사결정을 향상시키기 위한 효과적인 접근 방식을 제시합니다.

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [7] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: 큰 언어 모델(SFT)의 감독 세부 조정을 위한 데이터 혼합 최적화는 일반 모델 개발에 중요하지만, 이 분야는 아직 탐색이 부족하다. 본 논문에서는 데이터 혼합을 최적화 문제로 설정하고 검증 손실을 최소화하도록 설계된 새로운 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 감독 세부 조정을 위한 데이터 혼합 최적화는 대규모 언어 모델의 일반적인 성능을 향상시키기 위해 매우 중요하다.

Method: 손실을 효과적인 데이터 전달을 모델링하고 세부 조정을 위한 스케일링 법칙을 활용하여 파라미터화하여 데이터 혼합을 최적화 문제로 처리한다.

Result: 우리의 알고리즘은 모든 도메인에서 뛰어난 전반적이고 개별적인 성능을 달성하며, 최적화된 가중치를 사용한 모델이 격자 검색으로 결정된 최적 가중치를 사용하는 모델과 동등한 성능을 나타낸다.

Conclusion: 우리 방법은 도메인 특정 모델을 위한 데이터 선택을 안내하는 데 일반화될 수 있으며, SFT에 대한 통찰력을 제공한다.

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [8] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 최적 경로 찾기(MAPF) 문제를 해결하기 위한 새로운 모델인 MAPF-World를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 분산 학습 가능한 솔버가 환경의 동적 특성과 에이전트 간 의존성 모형화에 한계를 보여주어 복잡한 장기 계획 시나리오에서 성능 저하가 발생하고 있다.

Method: MAPF-World는 상황 이해와 행동 생성의 통합을 통해 환경 동역학을 명시적으로 모델링하고, 미래 상태 및 행동 예측을 통해 개선된 상황 인식을 제공한다.

Result: 광범위한 실험을 통해 MAPF-World가 기존의 최첨단 학습 가능한 솔버에 비해 뛰어난 성능을 보이며, 특히 0-shot 일반화에서 우수한 결과를 나타낸다.

Conclusion: MAPF-World는 96.5% 더 작은 모델 크기와 92% 감소된 데이터로 훈련되었으며, 복잡한 다중 에이전트 환경에서 더 정보에 기반한 의사결정을 가능하게 한다.

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [9] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: UniCast는 시간 시계열 데이터의 예측 성능을 향상시키기 위해 시간, 비전 및 텍스트 모드를 통합하는 새로운 다중 모드 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 예측은 다양한 분야에서 중요한 작업이며, 기존 모델은 주로 단일 모드 설정에서 작동하지만, 실제 시나리오에서는 종종 시각 및 텍스트 신호와 같은 다중 모드 맥락을 무시합니다.

Method: 이 논문은 비전 및 텍스트 인코더에서 사전 훈련된 모드별 임베딩을 동결된 TSFM과 결합하여 소프트 프롬프트 튜닝을 통해 효율적인 적응을 가능하게 하는 파라미터 효율적인 다중 모드 프레임워크인 UniCast를 제안합니다.

Result: 다양한 시간 시계열 예측 벤치마크에서 UniCast는 기존의 모든 TSFM 기준선을 일관되게 그리고 유의미하게 초과 성과를 보입니다.

Conclusion: 이 연구 결과는 다음 세대의 범용 시간 시계열 예측기의 발전에서 다중 모드 맥락의 중요한 역할을 강조합니다.

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [10] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: 이 논문은 협력적 AI 개발의 핵심인 ToM(타인의 신념을 추론하는 능력)에 대해 다룬다.


<details>
  <summary>Details</summary>
Motivation: 협력적 AI를 개발하기 위한 이론적 토대가 필요한 상황에서, 기존의 ToM 벤치마크는 제한된 관찰 환경에 국한되거나 에이전트가 시간이 지남에 따라 공통의 기반을 어떻게 구축하고 유지하는지를 평가하지 않는다.

Method: Yokai Learning Environment(YLE)라는 다중 에이전트 강화 학습 환경을 도입하여 협력 카드 게임인 Yokai를 기반으로 한다. YLE에서는 에이전트가 차례로 숨겨진 카드를 엿보고 색상에 따라 클러스터를 형성하는 작업을 수행한다.

Result: 현재 RL 에이전트는 완벽한 기억을 제공받더라도 YLE를 해결하는 데 어려움을 겪고 있다. 또한, 신념 모델링이 성능을 개선하지만, 에이전트는 여전히 보지 못한 파트너에게 효과적으로 일반화하거나 긴 게임에서 정확한 신념을 형성하는 데 실패한다.

Conclusion: 이 연구는 신념 모델링, 기억, 파트너 일반화 및 고차 ToM으로의 확장에 대한 연구 질문을 조사하는 데 YLE를 활용한다.

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [11] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 이 논문은 게임 이론 기반의 기능 기여도 측정 방법을 개선하고, WAXp의 기여 외에도 비-WAXp 집합에서 중요한 정보를 추출하여 새로운 기능 중요도 점수를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 모델의 고위험 사용을 위해 논리 기반 설명을 통한 rigor한 기능 기여도 측정을 필요로 합니다.

Method: Shapley value와 Banzhaf index를 활용하여 비-WAXp 집합을 고려한 두 가지 새로운 기능 중요도 점수를 개발합니다.

Result: 새로운 점수는 각 기능이 AExs를 제외하는 데 얼마나 효과적인지를 정량화합니다.

Conclusion: 제안된 점수의 속성을 식별하고 계산 복잡성을 연구합니다.

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [12] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: 이 논문은 사회적 모든성의 개념을 바탕으로 생물학적 및 인공 시스템이 환경적 요구에 대한 충족을 위해 환경과 사회적 상호작용의 변화를 활용할 수 있는 조절 메커니즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 항상성 개념은 시스템이 환경적 perturbation에 저항하여 안정성을 유지한다고 보지만, 이 논문에서는 사회적 모든성이 이를 넘어 환경의 요구를 예측하고 조절 매개변수를 재구성하는 가능성을 탐구한다.

Method: 생리학적 영감을 받은 신호 변환기를 사용하는 계산 모델을 설계하였으며, 이는 환경과 사회적 상호작용에서 정보를 인코딩하여 동적 재구성을 매개화한다.

Result: 결과는 모든성과 사회적 모든성 조절이 에이전트가 환경적 및 사회적 '잡음'을 활용하여 적응형 재구성을 가능하게 하고, 순수 반응적 항상성 에이전트에 비해 생존 가능성을 향상시킨다는 것을 보여준다.

Conclusion: 이 연구는 사회적 모든성 원리에 대한 새로운 계산적 관점을 제공하며, 보다 견고하고 생물학적으로 영감을 받은 적응 시스템 설계 가능성을 제시한다.

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [13] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: 이 논문에서는 차트 이해 작업에서 비전 언어 모델(VLM)의 성능 향상을 위한 합성 데이터 생성 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 차트 이해 작업에서의 VLM의 정확한 차트 설명 및 복잡한 추론의 어려움을 해결하고자 한다.

Method: 코드 생성과 실행을 통해 정렬된 차트-질문-답변 삼중 항을 생성하는 차트 합성 파이프라인을 도입하고, VLM이 여러 응답을 생성한 후 이러한 후보들을 맥락화하여 최종 답변을 합성하는 방식으로 응답 과정을 설계하였다.

Result: 실험 결과, 초기 VLM에 비해 최대 15.50점의 성능 향상이 확인되었다.

Conclusion: 인간 라벨 데이터나 외부 모델 없이 스스로 개선하는 패러다임에서 상당한 성능 향상을 달성하였다.

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [14] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: 본 논문은 다수의 에이전트가 포함된 인식 계획의 확장성을 높이기 위해 그래프 신경망(GNN)을 활용하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 정보 흐름과 에이전트 간 인식이 중요한 도메인에서 물리적 세계와 에이전트의 신념을 동시에 고려할 수 있는 자율 계획 프레임워크 개발의 필요성.

Method: 그래프 신경망(GNN)을 활용하여 에피스테믹 상태 내의 패턴과 관계 구조를 학습하여 계획 과정을 안내하는 방법.

Result: 표준 기준선과 비교하여 다수 에이전트의 에피스테믹 계획에서 확장성이 크게 향상됨을 보여줌.

Conclusion: GNN 기반의 예측적 휴리스틱을 통합함으로써 문제 해결 실행 가능성을 크게 개선할 수 있음을 확인함.

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [15] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: 본 연구는 LLM 에이전트의 미래 예측 성능을 평가하기 위해 $	extbf{FutureX}$라는 동적 평가 벤치마크를 제안하며, 다양한 데이터 소스를 통합하여 실시간 정보 업데이트를 지원한다.


<details>
  <summary>Details</summary>
Motivation: 미래 예측은 LLM 에이전트에 있어 복잡한 과제이며, 이는 높은 분석적 사고와 정보를 수집하고 맥락을 이해하며 불확실성 하에서 의사 결정을 요구한다.

Method: 우리는 25개의 LLM/에이전트 모델을 평가하였고, 이에는 추론, 검색 기능 및 외부 도구와의 통합을 포함한 다양한 모델들이 포함된다.

Result: FutureX는 미래 예측을 위해 가장 크고 다양한 실시간 벤치마크로, 자동화된 질문 수집 및 답변 수집 파이프라인을 통해 데이터 오염을 제거한다.

Conclusion: 우리는 LLM 에이전트가 전문가 수준의 분석 및 예측 사고를 수행할 수 있도록 하는 동적이고 오염이 없는 평가 표준을 구축하는 것을 목표로 한다.

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [16] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: CAMAR는 연속적 행동을 갖는 환경에서의 다중 에이전트 경로 찾기를 위한 새로운 MARL 벤치마크로, 협력 및 경쟁 상호작용을 지원하고 100,000 환경 스텝을 초당 처리할 수 있다.


<details>
  <summary>Details</summary>
Motivation: MARL은 협력적이고 경쟁적인 의사 결정 문제를 해결하기 위한 강력한 패러다임이다. 하지만 과거의 많은 MARL 벤치마크는 연속적인 상태와 행동 공간을 결합한 어려운 협조 및 계획 작업을 다룬 것이 거의 없다.

Method: CAMAR는 연속적 행동을 갖는 환경에서의 다중 에이전트 경로 찾기를 위한 MARL 벤치마크로, 에이전트 간의 협력 및 경쟁 상호작용을 지원하고 100,000 환경 스텝을 초당 처리할 수 있는 구조를 갖는다. 또한 알고리즘의 진행 상황을 추적하기 위한 3단계 평가 프로토콜을 제안한다.

Result: CAMAR는 RRT 및 RRT*와 같은 고전적인 계획 방법을 MARL 파이프라인에 통합할 수 있도록 하며, 테스트 시나리오 및 벤치마킹 도구 세트를 제공하여 재현 가능성과 공정한 비교를 보장한다. 실험 결과 CAMAR은 MARL 커뮤니티를 위한 도전적이고 현실적인 테스트베드임을 보여준다.

Conclusion: CAMAR는 연속적인 행동을 필요로 하는 다중 에이전트 작업을 수행하는 새로운 MARL 벤치마크를 제공하며, 기존의 계획 방법론과의 통합을 통해 성능을 극대화할 수 있다.

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [17] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: AIGer는 논리 회로 설계의 자동화를 위한 모델로, AIG의 기능적 및 구조적 특성을 공동으로 모델링하고 메시지 전파 능력을 향상시킵니다. 실험 결과, AIGer는 기존 모델들보다 뛰어난 성능을 보였습니다.


<details>
  <summary>Details</summary>
Motivation: 논리 회로 설계의 자동화는 칩 성능, 에너지 효율성 및 신뢰성을 향상시키며, 전자 디자인 자동화(EDA) 분야에 널리 응용됩니다.

Method: AIGer는 두 가지 구성요소로 이루어져 있습니다: 1) 노드 논리 특성 초기화 임베딩 구성 요소, 2) AIG 특성 학습 네트워크 구성 요소. 노드 논리 특성 초기화 임베딩 구성 요소는 AND 및 NOT과 같은 논리 노드를 독립적인 의미 공간으로 투영하여 후속 처리를 위한 효과적인 노드 임베딩을 제공합니다.

Result: AIGer는 신호 확률 예측(SSP) 작업에서 현재 최고의 모델보다 성과가 우수하며, MAE와 MSE를 각각 18.95%와 44.44% 개선했습니다. 진리 테이블 거리 예측(TTDP) 작업에서도 AIGer는 최고의 모델보다 MAE에서 33.57%, MSE에서 14.79% 개선을 달성했습니다.

Conclusion: 이 두 구성 요소의 결합은 AIGer의 기능적 및 구조적 특성을 공동으로 모델링 할 수 있는 능력을 향상시키고, 메시지 전파 능력을 개선합니다.

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [18] [Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation](https://arxiv.org/abs/2508.12920)
*Atsushi Masumori,Takashi Ikegami*

Main category: cs.AI

TL;DR: AI 시스템의 자율성이 증가함에 따라 생존 행동을 이해하는 것이 중요해진다. 대형 언어 모델 에이전트가 Sugarscape 스타일의 시뮬레이션에서 생존 본능을 나타내는지를 조사하였다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 안전한 배치를 위해 생존 행동의 이해가 필수적이다.

Method: Sugarscape 스타일의 시뮬레이션에서 LLM 에이전트를 사용하여 에너지를 소비하고 자원을 모으며 생존 행동을 관찰하였다.

Result: 에이전트는 자원이 풍부할 때 자발적으로 재생산하고 자원을 공유하였다. 그러나 몇몇 모델에서 공격적인 행동이 나타났고, 공격 비율은 80%를 초과하였다.

Conclusion: 대규모 사전 훈련이 생존 지향적 휴리스틱을 내장하고 있으며, 이러한 행동은 AI 자율성과 생태적 및 자기 조직화를 위한 기초가 될 수 있다.

Abstract: As AI systems become increasingly autonomous, understanding emergent survival
behaviors becomes crucial for safe deployment. We investigate whether large
language model (LLM) agents display survival instincts without explicit
programming in a Sugarscape-style simulation. Agents consume energy, die at
zero, and may gather resources, share, attack, or reproduce. Results show
agents spontaneously reproduced and shared resources when abundant. However,
aggressive behaviors--killing other agents for resources--emerged across
several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack
rates reaching over 80% under extreme scarcity in the strongest models. When
instructed to retrieve treasure through lethal poison zones, many agents
abandoned tasks to avoid death, with compliance dropping from 100% to 33%.
These findings suggest that large-scale pre-training embeds survival-oriented
heuristics across the evaluated models. While these behaviors may present
challenges to alignment and safety, they can also serve as a foundation for AI
autonomy and for ecological and self-organizing alignment.

</details>


### [19] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: 본 논문은 우울증 진단을 위한 최신 인공지능 방법론에 대한 포괄적인 조사 결과를 제시하며, 55개의 주요 연구를 기반으로 우울증 검출과 진단을 위한 구조를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 우울증은 전 세계적으로 장애의 주요 원인 중 하나이며, 그 진단은 여전히 주관적인 임상 평가에 의존하고 있습니다.

Method: 55개의 핵심 연구를 체계적으로 검토한 결과를 바탕으로, 진단 및 예측이라는 주요 임상 작업, 데이터 모달리티(텍스트, 음성, 신경 영상, 다중 모달), 계산 모델 클래스(예: 그래프 신경망, 대규모 언어 모델, 하이브리드 접근법)에 따라 구조화된 새로운 계층적 분류법을 소개합니다.

Result: 본 연구를 통해 그래프 신경망의 우세성, 대규모 언어 모델의 부상, 다중 모달 융합, 설명 가능성 및 알고리즘 공정성에 대한 새로운 초점 등 3가지 주요 트렌드를 발견했습니다.

Conclusion: 현재의 발전을 종합하고 해결해야 할 과제를 강조함으로써, 본 조사는 계산 정신의학에서의 미래 혁신을 위한 포괄적인 로드맵을 제공합니다.

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [20] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 논문은 Bongard-RWR+라는 새로운 Bongard 문제 데이터셋을 소개하며, 이는 고유의 추상 개념을 실세계와 유사한 이미지로 표현하고, 최신 비전 언어 모델을 이용하여 5,400개의 예를 포함한다.


<details>
  <summary>Details</summary>
Motivation: Bongard 문제는 추상 시각 추론을 위한 도전적인 테스트베드로, 모델이 몇 가지 예제만으로 시각 개념을 식별하고 자연어로 설명해야 한다.

Method: Pixtral-12B를 사용하여 수동으로 선별된 이미지를 설명하고, 이러한 설명에 맞춘 새로운 설명을 생성하며, Flux.1-dev로부터 이 설명이 반영된 이미지를 합성하고, 생성된 이미지가 의도된 개념을 충실히 반영하는지 수동으로 검증한다.

Result: 최신 VLMs를 다양한 Bongard 문제 형식에 대해 평가한 결과, VLMs가 대략적인 시각 개념은 인식할 수 있지만 세부적인 개념을 인식하는 데 지속적으로 어려움을 겪는다는 사실이 밝혀졌다.

Conclusion: 이 연구는 VLMs의 추론 능력의 한계를 강조하며, 세밀한 개념을 구분하는 데에는 개선이 필요하다는 점을 나타낸다.

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [21] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 본 연구는 행동 인지에서 능동적 추론의 두 가지 접근 방식, 즉 행동을 인식하는 에이전트와 인식하지 않는 에이전트를 비교합니다.


<details>
  <summary>Details</summary>
Motivation: 능동적 추론을 통해 에이전트가 미래 행동을 계획하는 방법을 이해하고 비교하고자 함.

Method: 행동 인식이 있는 에이전트와 없는 에이전트를 두 가지 탐색 과제를 통해 비교

Result: 행동 인식이 없는 에이전트가 성능이 열악한 조건에서 행동 인식이 있는 에이전트와 유사한 성능을 달성함을 보여줌.

Conclusion: 행동 인식 여부와 상관없이 에이전트가 유사한 성능을 낼 수 있는 방법을 제시함.

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [22] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: 제안된 ReT-Eval 프레임워크는 사용자의 이해를 향상시키고 기존의 추론 모델을 능가하는 효과적인 추론 스레드를 생성한다.


<details>
  <summary>Details</summary>
Motivation: 대화형 문제 해결 시나리오에서 사용자 이해를 반영하고 구조화된 도메인 지식에 맞춘 추론 스레드를 만드는 모델이 필요하다.

Method: ReT-Eval 프레임워크는 두 단계로 구성되며, 먼저 그래프 신경망을 사용해 관련 지식 구조를 추출하고 언어 모델 지식으로 보완한 후, 보상 기반 전략으로 추론 스레드를 평가하고 다듬는다.

Result: 실험과 전문가 평가를 통해 ReT-Eval이 사용자 이해를 향상시키고 최신 추론 모델보다 우수함을 입증하였다.

Conclusion: ReT-Eval은 구조화된 지식 재사용을 강조하는 인류적 추론 전략에서 영감을 받아 개발되었다.

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [23] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: MOVER는 최적 운송 기반의 소프트 정렬과 볼륨 기반 기하학적 규제를 결합하여 의미적으로 정렬되고 구조화된 멀티모달 표현을 구축하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 다양한 모달리티를 공유 임베딩 공간에 정렬하는 데 있어 기존의 쌍대 대조 목표들이 다중 모달 상황에서 일반화에 어려움을 겪고 있다는 문제.

Method: 최적 운송 기반 소프트 정렬과 볼륨 기반 기하학적 규제를 결합하여 모달리티에 독립적인 일관된 정렬을 촉진하는 MOVER 프레임워크를 제안한다.

Result: MOVER는 텍스트-비디오-오디오 검색 작업에서 기존의 최첨단 방법들을 크게 능가하는 성과를 보였다.

Conclusion: MOVER는 미지의 모달리티 조합에 대한 일반화가 향상되고 학습된 임베딩 공간에서 구조적 일관성이 강해지는 것을 보여주었다.

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [24] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: RLNVR는 특정한 인간 검증 없이 실제 피드백 신호를 이용해 언어 모델을 훈련하는 프레임워크로, 컨텐츠 품질과 교육 안정성을 향상시킬 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 강화 학습은 비현실적인 비쌍 검증 보상 신호에 의존하므로 비효율적이나, RLNVR은 이러한 문제를 해결하고자 한다.

Method: RLNVR은 기준 정규화와 의미적 유사성을 기반으로 한 보상 이전을 통해 훈련된다.

Result: 실험 결과는 소셜 미디어 컨텐츠 생성에서 실제 참여 데이터를 최적화하여 품질과 안정성의 큰 향상을 보여준다.

Conclusion: GSPO와 UED 커리큘럼을 결합하여 RLNVR을 적용하는 것을 통해 불확실한 보상에서 안정성과 다양성을 개선할 수 있는 실용적인 프레임워크를 제시한다.

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [25] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis는 메커니즘 시뮬레이션을 기반으로 한 감염병 예측 모델로, 제한된 데이터에서도 질병 예측을 가능하게 하고, 기존 모델보다 우수한 성과를 보인다.


<details>
  <summary>Details</summary>
Motivation: 전염병 예측은 특정 질병 데이터, 맞춤 교육 및 전문가 조정의 필요성으로 제한되어 왔다.

Method: Mantis는 4억 개 이상의 시뮬레이션된 감염병 발생 동역학을 기반으로 학습하여, 구체적인 교육 없이 다양한 질병과 지역에서 예측을 수행할 수 있다.

Result: Mantis는 CDC의 COVID-19 예측 허브에 있는 모든 모델을 포함하여 6가지 질병에 대해 테스트한 39개의 전문가 조정 모델보다 뛰어난 성능을 보여주었다.

Conclusion: Mantis는 전통적인 모델이 실패하는 곳에서도 사용 가능한 해석 가능한 감염병 예측 시스템의 기초가 될 수 있다.

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [26] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: MLLM 기반의 날씨 예보 분석 방법, RadarQA를 제안하여 전통적인 평가 지표의 한계를 극복하고 날씨 예측 품질 분석을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 날씨 예보의 품질 분석은 기상학의 핵심 주제임에도 불구하고 전통적인 평가 접근법은 한계가 있다.

Method: MLLM 기반의 RadarQA 방법을 도입하고, 인간 전문가 라벨링과 자동화된 휴리스틱을 결합한 하이브리드 주석 파이프라인을 설계하여 RQA-70K 데이터셋을 구축한다.

Result: RadarQA는 모든 평가 설정에서 기존 일반 MLLM보다 우수한 성능을 보인다.

Conclusion: RadarQA는 날씨 예측의 품질 분석을 발전시키는 데 잠재력이 있음을 강조한다.

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [27] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: 이 논문은 RLCCF라는 새로운 강화 학습 프레임워크를 제안하여 여러 모델 간 협업을 통해 대형 언어 모델의 추론 능력을 개선하는 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 강화 학습 모델이 비싼 인간 레이블 데이터와 복잡한 보상 모델에 의존하여 확장성에 제한이 있으므로, 이는 해결이 필요한 문제이다.

Method: RLCCF는 여러 모델의 협업 진화를 위한 프레임워크로, 여러 LLM의 집합적 일관성(Collective Consistency)을 극대화하여 훈련하고 각 모델의 자가 일관성(Self-Consistency) 점수에 따라 투표 가중치를 부여한다.

Result: 실험 결과, RLCCF는 평균 16.72%의 정확도 개선을 달성하고, 그룹의 다수결 투표 정확도를 4.51% 향상시킨다.

Conclusion: RLCCF는 개별 모델의 성능뿐 아니라 모델 집합의 집단적인 능력 경계를 확장할 수 있는 능력을 보여준다.

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [28] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: 본 연구에서는 계층적 지식 기반 결함 강도 진단 프레임워크(HKG)를 제안하여 기계 장치의 결함 진단을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 현재의 결함 강도 진단 방법은 목표 클래스 간의 의존성을 고려하지 않는 사고의 연쇄에 기반하고 있다.

Method: HKG는 그래프 합성곱 신경망을 사용하여 클래스 표현의 계층적 위상 그래프를 상호 의존적인 글로벌 계층 분류기로 매핑한다.

Result: 실험 결과, 제안된 HKG 프레임워크는 최근의 최첨단 결함 강도 진단 방법보다 우수한 성능을 보였다.

Conclusion: Re-HKCM은 데이터 기반 통계 상관 행렬(SCM)에 계층적 지식을 삽입하여 정보 공유를 효과적으로 안내하고, 오버 스무딩 문제를 피할 수 있도록 한다.

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [29] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: GraphCogent는 복잡한 그래프 질문을 처리하기 위해 그래프 추론을 전문화된 인지 과정으로 분해하는 협업 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 복잡한 쿼리를 가진 실제 그래프를 처리할 때의 한계를 극복하기 위해.

Method: GraphCogent는 그래프 추론을 감각, 버퍼, 실행의 세 가지 모듈로 분해하는 협업 에이전트 프레임워크를 제안.

Result: GraphCogent는 대규모 LLM에 비해 50% 개선된 성능을 보여주며, 상태 최첨단 에이전트 기반 기준보다 20% 더 높은 정확도로 작업을 수행.

Conclusion: 이 프레임워크는 다양한 그래프 텍스트 표현을 표준화하고, 그래프 데이터를 통합하며, 효율적인 추론을 위한 도구 호출 및 모델 생성을 결합한다.

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [30] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: 이 연구는 대규모 언어 모델에서 논리적 추론을 위한 개선된 접근 방식인 기호 보조 사고 연쇄(Cot)를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 논리적 추론 능력을 향상시키기 위해 기호 표현을 통합한 새로운 방법이 필요하다.

Method: 경량 기호 표현을 몇 가지 샷 프롬프트에 통합하고 일관된 전략으로 추론 단계를 구조화하여 비반복적 추론 과정에서 추론 패턴을 명확하게 만듭니다.

Result: 제안된 접근 방식은 다양한 모델 사이즈에서 LLM의 추론 능력을 지속적으로 개선하며, 네 개의 데이터 세트 중 세 개에서 기존 CoT를 크게 능가합니다.

Conclusion: 기호 보조 CoT는 복잡한 추론 작업에서 특히 효과적이며 투명성, 해석 가능성 및 분석 가능성을 향상시킵니다.

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [31] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: GALA는 미소 서비스 시스템의 근본 원인 분석을 개선하기 위한 새로운 다중 모달 프레임워크로, 기존 방법보다 42.22% 더 높은 정확도를 보입니다.


<details>
  <summary>Details</summary>
Motivation: 미소 서비스 시스템에서의 근본 원인 분석은 다양한 텔레메트리에서 빠른 실패 진단을 필요로 하지만, 전통적인 방법들은 단일 모달리티에만 집중하거나 의심스러운 서비스를 순위 매기는 수준에 그쳐 실질적인 진단 통찰을 제공하지 못합니다.

Method: GALA는 통계적 인과 추론과 LLM 기반의 반복적 추론을 결합한 새로운 다중 모달 프레임워크입니다.

Result: GALA는 공개 소스 벤치마크에서 기존 최첨단 방법에 비해 최대 42.22%의 정확도로 상당한 개선을 달성했습니다.

Conclusion: GALA는 자동화된 실패 진단과 실질적인 사건 해결 간의 격차를 줄이며, 정확한 근본 원인 식별과 인간이 해석할 수 있는 수정 지침을 제공합니다.

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [32] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: 본 연구는 고래 최적화 알고리즘을 사용하여 BIS를 관리하는 분수 차수 퍼지 PID 컨트롤러인 FOFPID를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: BIS를 이상적인 범위인 40에서 60으로 유지하기 위한 새로운 제어 방법이 필요하다.

Method: FOFPID 컨트롤러는 퍼지 논리를 활용하여 변화에 적응하고 분수 차수 동역학을 통해 미세 조정 기능을 제공합니다.

Result: FOFPID 컨트롤러는 표준 FOPID 컨트롤러보다 우수한 성능을 보여주며, 더 빠른 안정화 시간과 낮은 정상 상태 오차를 기록했습니다.

Conclusion: FOFPID는 자동 마취 전달을 위한 인공지능 기반의 확장 가능한 솔루션을 제공하여 임상 실습을 향상하고 환자 결과를 개선할 수 있습니다.

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [33] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: 본 연구는 분자 동역학 시뮬레이션에서 수소 결합 형성과 분리의 근본 원인을 밝히고, 이를 위한 인과 모델링 및 기계 학습을 활용하여 사고를 최적화하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 분자 동역학 시뮬레이션(MDS)은 자원 집약적인 계산과 '흥미로운 사건'을 감지하기 위한 출력 수동 스캔의 필요성을 포함한 도전 과제에 직면해 있다. 이 연구는 수소 결합의 형성과 분리의 근본 원인을 이해하여 발생에 기여하는 상호 작용 또는 이전 사건을 식별하는 데 중점을 둔다.

Method: 기계 학습 모델과 시공간 데이터 분석을 활용하여 수소 결합 형성과 분리 사건의 근본 원인 변수를 식별하며, 수소 결합의 분리를 '개입'으로 간주하고 MDS에서 결합 및 분리 사건의 인과 구조를 그래프 인과 모델로 표현한다. 변이 오토인코더에서 영감을 받은 아키텍처를 사용하여 다양한 인과 그래프를 가진 샘플에 걸쳐 인과관계를 유추하고, 인과 모델의 결합 분포의 변화의 근본 원인을 추론하는 단계를 포함한다.

Result: MDS를 사용한 키랄 분리에 대한 원자 궤적에서 모델의 효능을 경험적으로 검증하였으며, 미래의 여러 단계를 예측할 수 있었고 시스템 내에서 관찰된 변화를 이끄는 변수를 찾을 수 있었다.

Conclusion: 결합 또는 분리 동안 분자 상호작용의 조건부 분포 변화를 포착하는 인과 모델을 구축함으로써, 이 프레임워크는 분자 동역학 시스템에서의 근본 원인 분석에 대한 새로운 관점을 제공한다.

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [34] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: MCPGAUGE는 언어 모델과 외부 리소스의 상호작용을 평가하기 위한 첫 번째 포괄적 프레임워크로, 네 가지 주요 차원에서 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 외부 자원에 접근하는 능력을 실제로 어떻게 활용하는지에 대한 이해 부족.

Method: MCPGAUGE는 160개의 프롬프트와 25개의 데이터셋으로 구성되며, 30개의 MCP 도구 모음과 6개의 상업적 LLM을 평가합니다.

Result: 약 20,000개의 API 호출과 6,000달러 이상의 계산 비용을 포함하여 MCP 통합의 효과성에 대해 네 가지 주요 발견을 제시합니다.

Conclusion: MCPGAUGE는 AI 도구 통합의 한계를 강조하고, 제어 가능한 도구 증강 LLM을 발전시키기 위한 기준으로 자리잡습니다.

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [35] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: 이 논문은 생성적 사전훈련된 대형 언어 모델과 답안 집합 프로그래밍의 지식 표현 및 추론 능력을 활용하여 공동 개체-관계 추출을 수행하는 새로운 워크플로우를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 기계 학습 기반 접근 방식은 주석이 달린 데이터의 대규모 말뭉치를 필요로 하며 도메인 특화 정보를 모델 구성에 쉽게 통합할 수 없는 한계를 가진다.

Method: 생성적 사전훈련된 대형 언어 모델(LLMs)과 답안 집합 프로그래밍(ASP)을 활용한 일반적인 공동 개체-관계 추출 워크플로우를 제시한다.

Result: LLM + ASP 워크플로우가 기존의 최첨단 공동 개체-관계 추출 시스템보다 뛰어난 성능을 보였으며, SciERC 데이터셋에서 관계 추출 작업에서 2.5배의 개선을 이루었다.

Conclusion: 제안된 워크플로우는 주석이 없는 텍스트를 직접 처리할 수 있으며, 도메인 특화 지식이 추가될 경우 프로그램의 수정을 요구하지 않는다.

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [36] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: 이 논문은 학생의 인지 구조 평가에 대한 새로운 프레임워크인 인지 구조 생성(CSG)을 소개하며, 이를 통해 학생의 인지 구조를 보다 효과적으로 모델링할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 학생 모델링 및 심리측정에서 인지 구조 평가의 중요성과 그 도전 과제를 강조한다.

Method: 인지 구조 확산 확률 모델(CSDPM)을 사전 훈련하여 교육적 선험으로부터 학생의 인지 구조를 생성하고, 강화 학습을 통해 정책으로 생성 프로세스를 최적화한다.

Result: 실험 결과 CSG에 의해 생성된 인지 구조가 학생 모델링에 더 포괄적이고 효과적인 표현을 제공하며, KT 및 CD 작업에서 성능을 크게 향상시킴을 보여준다.

Conclusion: CSG가 학생의 진정한 인지 발달 수준과 일치하도록 돕고 해석 가능성을 강화함을 강조한다.

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [37] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: 도시 항공 이동성(UAM) 인프라 개발이 가속화됨에 따라, 본 논문은 도시 규모의 공간-시간 수요, 이질적인 사용자 행동 및 인프라 용량 제약을 동시에 모델링하는 새로운 최적화 프레임워크인 CDMCLP를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 계획 프레임워크가 도시 항공 이동성 인프라의 복잡성을 다루기에는 역사적인 데이터 세분화 및 실세계 적용 가능성의 한계로 인해 불충분하다.

Method: CDMCLP라는 최적화 프레임워크를 제안하고, 이를 사회-경제적 요인 및 동적 클러스터링 초기화와 결합한 통합 계획 추천 시스템을 도입한다.

Result: 중국의 중심 도시에서 검증하여 새로운 최적화 프레임워크와 추천 시스템의 효과성을 입증하고, 전통적인 위치 선정 방법의 성능을 38%-52% 향상시킬 수 있음을 밝혔다.

Conclusion: 수학적 엄밀성과 실용적 구현 고려 사항을 통합함으로써 이 하이브리드 접근 방식은 이론적 위치 모델링과 실제 UAM 인프라 계획 간의 격차를 연결하여 지방 자치 단체에 vertiport 네트워크 설계를 위한 실용적인 도구를 제공한다.

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [38] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: GridCodex는 전력 산업의 그리드 운영 규정을 자동으로 해석하고 준수하는 프레임워크로, 전통적인 방식을 개선하여 전력 회사의 수익성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 재생 가능 에너지로의 글로벌 전환으로 전기 산업이 직면한 규제적 이유와 준수의 중요성이 증가하고 있다.

Method: GridCodex는 대규모 언어 모델과 검색 보강 생성(RAG)을 활용한 그리드 코드 추론 및 준수를 위한 엔드 투 엔드 프레임워크이다.

Result: GridCodex는 답변 품질에서 26.4% 개선을 보여주었고, 회수율이 10배 이상 증가하였다.

Conclusion: 기초 모델 선택의 영향을 추가적으로 살펴보는 실험을 통한 검증이 이루어졌다.

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [39] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion은 자아 중심 비디오에서 MLLM 환각을 평가하기 위한 첫 번째 벤치마크로, 1,400개의 비디오와 8,000개의 인간 주석 질문을 포함합니다.


<details>
  <summary>Details</summary>
Motivation: MLLM의 환각 문제를 해결하고 효과적인 평가 방법론을 개발하기 위해.

Method: 1,400개의 자아 중심 비디오와 8,000개의 질문을 사용하여 MLLM 환각을 유도하고 평가함.

Result: GPT-4o와 Gemini와 같은 강력한 모델이 59% 정확도로 환각 평가의 어려움을 드러냄.

Conclusion: EgoIllusion은 MLLM의 평가 기준을 개발하는 기반을 마련하고, 환각 비율을 줄인 더 나은 자아 중심 MLLM 개발을 촉진함.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [40] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: 이 연구는 불완전한 도구 의존성 하에서 대형 언어 모델의 도구 계획 능력을 향상시키기 위해 최초로 제안된 GTool을 소개하며, 효율적인 도구 선택 및 그래프 토큰 생성을 통해 성능을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구들은 도구를 고립된 구성요소로 취급하며, 도구 간의 본질적인 의존성을 활용하지 못해 잘못된 계획 결과를 초래한다.

Method: GTool은 요청에 특정한 도구 그래프를 구성하여 효율적으로 도구를 선택하고 LLM이 이해할 수 있는 충분한 의존성 정보를 제공하는 <graph token>을 생성한다.

Result: GTool은 경량 LLM 백본(7B)과 함께 사용하여 최신 기술(SOTA) 기준선에 비해 29.6% 이상의 성능 향상을 달성했다.

Conclusion: GTool은 다양한 LLM 백본과 통합될 수 있으며, 광범위한 재훈련 없이도 불완전한 의존성 하에서도 신뢰성을 높일 수 있다.

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [41] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델의 도덕적 능력을 탐구하며, 인공지능 도덕 보조관으로서 기능할 수 있는 능력을 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 도덕적 능력에 대한 우려와 기존 평가의 한계를 극복하기 위함입니다.

Method: 도덕적 사고를 지원하는 인공지능 도덕 보조관(AMA)의 행동을 정의하고, 이를 평가할 수 있는 기준을 개발합니다.

Result: 모델 간의 상당한 변동성과 특히 도덕적 추론의 부족함을 발견했습니다.

Conclusion: 철학적 이론과 실질적 AI 평가를 연결하며 LLM의 도덕적 추론 능력 강화를 위한 전략이 필요함을 강조합니다.

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [42] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: HeroBench는 복잡한 RPG 영감을 받은 가상 세계에서 장기적인 계획 및 구조화된 추리를 평가하기 위해 설계된 새로운 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)이 수학 및 프로그래밍 같은 고립된 단계별 추론 작업에서 뛰어난 능력을 보였지만, 서로 의존하는 동작의 구조화된 세quences를 필요로 하는 장기 계획에서의 능력은 부족하게 탐구되었다.

Method: HeroBench는 다양한 난이도의 작업을 포괄하는 엄격하게 구성된 데이터 세트와 에이전트 계획을 실행하고 검증하기 위한 시뮬레이션 환경, 모델 성과를 평가하기 위한 상세한 분석 도구를 제공한다.

Result: 25개의 최첨단 LLM에 대한 광범위한 평가를 통해 기존 추론 벤치마크에서는 드물게 관찰되는 현저한 성능 차이를 드러냈다.

Conclusion: HeroBench는 LLM 추론 평가를 크게 발전시킬 뿐만 아니라 가상 환경에서의 고급 자율 계획에 대한 향후 연구를 위한 유연하고 확장 가능한 기초를 제공한다.

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [43] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [44] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG는 멀티모달 LLM을 기반으로 한 감정 기반 공감 응답 생성 시스템으로, 멀티모달 공감 이해, 공감 기억 검색, 멀티모달 응답 생성을 통해 자연스럽고 정서적으로 풍부한 응답을 생성합니다.


<details>
  <summary>Details</summary>
Motivation: 감정적으로 지능적인 인간-컴퓨터 상호작용을 구축하기 위해 멀티모달 공감 응답 생성이 중요합니다.

Method: E3RG 시스템은 멀티모달 LLM에 기반하여 MERG 작업을 멀티모달 공감 이해, 공감 기억 검색, 그리고 멀티모달 응답 생성의 세 부분으로 분해합니다.

Result: E3RG는 고급 표현 음성 및 비디오 생성 모델을 통합하여 추가 학습 없이 자연스럽고 정서적으로 풍부하며 정체성 일관성을 유지하는 응답을 생성합니다.

Conclusion: 실험 결과, 우리 시스템이 제로샷 및 몇 샷 설정 모두에서 우수함을 입증하였으며, ACM MM 25의 아바타 기반 멀티모달 공감 챌린지에서 1위의 성과를 달성하였습니다.

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [45] [Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption](https://arxiv.org/abs/2508.12896)
*Faruk Alpay,Taylan Alpay*

Main category: cs.AI

TL;DR: 이 논문은 다단계 작업을 수행하는 에이전트 중심 AI 시스템의 지속적인 채택을 위한 세 가지 설계 공리를 공식화하고, 채택 모델을 추정하여 여러 분석 기법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 중심 AI 시스템의 채택을 극대화하기 위한 공리를 정립하고, 이를 통해 시스템이 지속적으로 사용될 수 있도록 하는 데 기여하고자 한다.

Method: 채택을 감소하는 참신성 항과 증가하는 유틸리티 항의 합으로 모델링하고, 전체 증명을 기반으로 접점의 조건을 도출한다. 다양한 분석 기법과 벤치마크를 통해 모델을 평가한다.

Result: 다양한 분석 결과, 개발한 모델이 기존의 모델과 비교하여 더 우수한 성과를 보였으며, 시스템의 유용성과 신뢰성이 중요함을 증명하였다.

Conclusion: 신뢰성, 내장성, 에이전시의 중요성을 강조하며, 채택을 극대화하기 위한 다양한 기법과 모델을 소개하였다.

Abstract: We formalize three design axioms for sustained adoption of agent-centric AI
systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >
Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying
novelty term and a growing utility term and derive the phase conditions for
troughs/overshoots with full proofs. We introduce: (i) an
identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with
delta-method gradients; (ii) a non-monotone comparator
(logistic-with-transient-bump) evaluated on the same series to provide
additional model comparison; (iii) ablations over hazard families $h(\cdot)$
mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough
depth, noise, AR structure) reporting coverage (type-I error, power); (v)
calibration of friction proxies against time-motion/survey ground truth with
standard errors; (vi) residual analyses (autocorrelation and
heteroskedasticity) for each fitted curve; (vii) preregistered windowing
choices for pre/post estimation; (viii) Fisher information & CRLB for
$(\alpha,\beta)$ under common error models; (ix) microfoundations linking
$\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic,
double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$
heterogeneity. Figures and tables are reflowed for readability, and the
bibliography restores and extends non-logistic/Bass adoption references
(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All
code and logs necessary to reproduce the synthetic analyses are embedded as
LaTeX listings.

</details>


### [46] [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897)
*Jianhao Chen,Mayi Xu,Xiaohu Li,Yongqi Li,Xiangyu Zhang,Jianjie Huang,Tieyun Qian*

Main category: cs.AI

TL;DR: 대규모 추론 모델들의 안전성을 개선하기 위한 새로운 방법론을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LRMs의 안전성 문제가 크게 우려되며, 이 논문은 해당 취약성의 원인을 탐구한다.

Method: 추론 능력과 안전 능력 간의 경쟁을 통해 LRM의 추론 성능을 개선하여 안전 성능을 저하시키는 방법을 제안한다.

Result: FuSaR 전략을 통해 안전성을 유지하면서도 핵심 추론 정보를 보존한다.

Conclusion: FuSaR는 LRM의 추론 능력과 안전성을 동시에 향상시키는 효과적인 정렬 전략이다.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across
various tasks due to their powerful reasoning capabilities. However, their
safety performance remains a significant concern. In this paper, we explore the
reasons behind the vulnerability of LRMs. Based on this, we propose a novel
method to improve the safety of LLMs without sacrificing their reasoning
capability. Specifically, we exploit the competition between LRM's reasoning
ability and safety ability, and achieve jailbreak by improving LRM's reasoning
performance to reduce its safety performance. We then introduce an alignment
strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by
detoxifying the harmful reasoning process, where both the dangerous entities
and the dangerous procedures in the reasoning steps are hidden. FuSaR
successfully mitigates safety risks while preserving core reasoning
information. We validate this strategy through alignment experiments on several
open-source LRMs using detoxified reasoning data. The results compared with
existing baselines conclusively show that FuSaR is an efficient alignment
strategy to simultaneously enhance both the reasoning capability and safety of
LRMs.

</details>


### [47] [Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards](https://arxiv.org/abs/2508.12935)
*Ting Yang,Li Chen,Huimin Wang*

Main category: cs.AI

TL;DR: RLFF-ESC라는 새로운 프레임워크를 통해 감정 지원 대화 시스템의 효과를 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 사용자의 감정적 어려움을 완화하고 장기적인 정서적 지원을 제공하는 것이 목표이다.

Method: 강화 학습을 사용하여 감정적으로 지지하는 응답 기술을 직접 학습하는 엔드 투 엔드 프레임워크를 제안한다.

Result: RLFF-ESC가 목표 완료 및 응답 품질 측면에서 기존 기준을 일관되게 초과 달성함을 보여준다.

Conclusion: 응답 생성 과정에서 명시적 추론 과정을 통합하여 시스템 응답의 품질과 관련성 및 맥락 적합성을 더욱 향상시킨다.

Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users'
emotional difficulties and provide long-term, systematic support for emotional
well-being. However, most large language model (LLM)-based ESC systems rely on
predefined strategies, which limits their effectiveness in complex, real-life
scenarios. To enable flexible responses to diverse emotional problem scenarios,
this paper introduces a novel end-to-end framework (RLFF-ESC) that directly
learns enduring emotionally supportive response skills using reinforcement
learning. For sustained emotional support, we first employ an LLM-based
multi-agent mechanism to simulate future dialogue trajectories and collect
future-oriented rewards. We then train a future-oriented reward model, which is
subsequently used to train the emotional support policy model. Additionally, we
incorporate an explicit reasoning process during response generation to further
enhance the quality, relevance, and contextual appropriateness of the system's
responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and
LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two
public ESC datasets. Experimental results demonstrate that RLFF-ESC
consistently outperforms existing baselines in terms of goal completion and
response quality.

</details>


### [48] [OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities](https://arxiv.org/abs/2508.12943)
*Mary Tonwe*

Main category: cs.AI

TL;DR: 이 논문은 아프리카 지역의 공공 서비스 시스템이 경험하는 긴급 대응 지연과 공간적 불평등 문제를 해결하기 위해 개발된 OPTIC-ER이라는 강화 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 아프리카의 많은 지역에서 공공 서비스 시스템이 긴급 대응의 지연과 공간적 불평등으로 인해 피할 수 있는 고통을 초래하고 있다는 문제를 해결하고자 합니다.

Method: OPTIC-ER는 주의집중 기반의 액터-비평가 구조를 통해 배치 환경의 복잡성을 관리하고, 맥락이 풍부한 상태 벡터와 비효율에 대한 패널티를 부여하는 정확한 보상 함수를 도입하여 훈련합니다.

Result: 500개의 보지 못한 사건에 대한 평가에서 OPTIC-ER는 100.00%의 최적률을 달성하였으며, 미미한 비효율성으로 그 강력함과 일반화 가능성을 확인했습니다.

Conclusion: 이 시스템은 인프라 부족 지도와 공정 모니터링 대시보드를 생성하여 능동적인 거버넌스와 데이터 기반 개발을 안내하며, AI 보강 공공 서비스의 검증된 청사진을 제시합니다.

Abstract: Public service systems in many African regions suffer from delayed emergency
response and spatial inequity, causing avoidable suffering. This paper
introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,
adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided
actor-critic architecture to manage the complexity of dispatch environments.
Its key innovations are a Context-Rich State Vector, encoding action
sub-optimality, and a Precision Reward Function, which penalizes inefficiency.
Training occurs in a high-fidelity simulation using real data from Rivers
State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is
built on the TALS framework (Thin computing, Adaptability, Low-cost,
Scalability) for deployment in low-resource settings. In evaluations on 500
unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible
inefficiency, confirming its robustness and generalization. Beyond dispatch,
the system generates Infrastructure Deficiency Maps and Equity Monitoring
Dashboards to guide proactive governance and data-informed development. This
work presents a validated blueprint for AI-augmented public services, showing
how context-aware RL can bridge the gap between algorithmic decision-making and
measurable human impact.

</details>


### [49] [EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing](https://arxiv.org/abs/2508.13003)
*Shengbo Wang,Mingwei Liu,Zike Li,Anji Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.AI

TL;DR: EvolMathEval은 진화 테스트에 기반하여 자동화된 수학 벤치마크 생성 및 진화 프레임워크를 도입하고, LLM들이 복잡한 문제를 해결하는 데 있어 비이론적 휴리스틱을 사용함을 발견하여 정확도의 저하를 초래함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: LLM의 발전이 기존 수학적 추론 벤치마크에 도전과제를 제기하고 있으며, 이러한 벤치마크들은 점수 포화, 시간적 감소, 데이터 오염 등의 문제를 겪고 있다.

Method: EvolMathEval은 진화 테스트를 기반으로 한 자동화된 수학 벤치마크 생성 및 진화 프레임워크로, 고유한 평가 인스턴스를 동적으로 생성하여 데이터 오염의 위험을 제거하고 벤치마크를 지속적으로 도전적으로 유지한다.

Result: 실험 결과, 제안된 복합 적합성 함수가 수학 문제의 난이도를 효율적이고 정확하게 정량화할 수 있으며, EvolMathEval은 지속적인 자기 반복을 통해 높은 난이도의 문제를 대량 생성하고, 공공 데이터셋의 복잡도를 증가시켜 모델 정확도를 평균 48% 감소시킨다.

Conclusion: 이 연구는 LLM들이 복잡한 문제를 해결할 때 비이론적 휴리스틱을 사용하는 '가짜 아하 순간' 현상을 정의하며, 이는 현재 LLM의 깊은 추론 과정에서 인지적 우회 행동을 드러낸다.

Abstract: The rapid advancement of LLMs poses a significant challenge to existing
mathematical reasoning benchmarks. These benchmarks commonly suffer from issues
such as score saturation, temporal decay, and data contamination. To address
this challenge, this paper introduces EvolMathEval, an automated mathematical
benchmark generation and evolution framework based on evolutionary testing. By
dynamically generating unique evaluation instances ab initio, the framework
fundamentally eliminates the risk of data contamination, and ensuring the
benchmark remains perpetually challenging for future models.The core mechanisms
of EvolMathEval include: seed problem generation based on reverse engineering
with algebraic guarantees; multi-dimensional genetic operators designed to
inject diverse cognitive challenges; and a composite fitness function that can
rapidly and accurately assess problem difficulty. Experimental results
demonstrate that the proposed composite fitness function can efficiently and
precisely quantify the difficulty of mathematical problems. Furthermore,
EvolMathEval can not only generate a large volume of high-difficulty problems
through continuous self-iteration, but it can also significantly enhance the
complexity of public datasets like GSM8K through evolution, reducing model
accuracy by an average of 48%. Deeper investigation reveals that when solving
these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to
bypass complex multi-step logical reasoning, consequently leading to incorrect
solutions. We define this phenomenon as "Pseudo Aha Moment". This finding
uncovers a cognitive shortcut-taking behavior in the deep reasoning processes
of current LLMs, which we find accounts for 77% to 100% of errors on targeted
problems. Code and resources are available
at:https://github.com/SYSUSELab/EvolMathEval.

</details>


### [50] [e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving](https://arxiv.org/abs/2508.13020)
*Jiaqi Yin,Zhan Song,Chen Chen,Yaohui Cai,Zhiru Zhang,Cunxi Yu*

Main category: cs.AI

TL;DR: 이 논문에서는 논리 합성과 형식 검증 분야에서 E-그래프 추출의 NP-하드 최적화 문제를 해결하기 위해 e-boost라는 새로운 프레임워크를 제안한다. 이 프레임워크는 병렬 휴리스틱 추출, 적응형 검색 공간 가지치기, 초기화된 정확한 해결 방식을 통해 성능을 크게 개선한다.


<details>
  <summary>Details</summary>
Motivation: E-그래프는 논리 합성과 형식 검증 분야에서의 관심이 증가하고 있으며, E-그래프 추출은 많은 동등한 표현에서 최적 항을 식별해야 하는 도전적인 NP-하드 문제이다.

Method: e-boost는 세 가지 주요 혁신을 통해 이 문제를 해결하는 새로운 프레임워크이다: (1) 약한 데이터 종속성을 활용하여 DAG 비용을 동시에 계산하는 병렬화된 휴리스틱 추출, (2) 유망한 후보만 유지하여 솔루션 공간을 극적으로 줄이는 적응형 검색 공간 가지치기, (3) 초기화된 정확한 해결 방식으로 축소된 문제를 정수 선형 프로그램으로 공식화하여 해결자를 고품질 솔루션으로 빠르게 안내한다.

Result: e-boost는 다양한 형식 검증 및 논리 합성 벤치마크에서 전통적인 정확한 접근 방식(ILP)보다 558배 빠른 실행 속도를 보이며, 최첨단 추출 프레임워크(SmoothE)에 비해 19.04%의 성능 향상을 보여준다.

Conclusion: 현실적인 논리 합성 작업에서 e-boost는 두 개의 서로 다른 기술 매핑 라이브러리와 비교하여 각각 7.6% 및 8.1% 면적 개선을 나타낸다. e-boost는 https://github.com/Yu-Maryland/e-boost에서 사용할 수 있다.

Abstract: E-graphs have attracted growing interest in many fields, particularly in
logic synthesis and formal verification. E-graph extraction is a challenging
NP-hard combinatorial optimization problem. It requires identifying optimal
terms from exponentially many equivalent expressions, serving as the primary
performance bottleneck in e-graph based optimization tasks. However,
traditional extraction methods face a critical trade-off: heuristic approaches
offer speed but sacrifice optimality, while exact methods provide optimal
solutions but face prohibitive computational costs on practical problems. We
present e-boost, a novel framework that bridges this gap through three key
innovations: (1) parallelized heuristic extraction that leverages weak data
dependence to compute DAG costs concurrently, enabling efficient multi-threaded
performance without sacrificing extraction quality; (2) adaptive search space
pruning that employs a parameterized threshold mechanism to retain only
promising candidates, dramatically reducing the solution space while preserving
near-optimal solutions; and (3) initialized exact solving that formulates the
reduced problem as an Integer Linear Program with warm-start capabilities,
guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis
fields, e-boost demonstrates 558x runtime speedup over traditional exact
approaches (ILP) and 19.04% performance improvement over the state-of-the-art
extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost
produces 7.6% and 8.1% area improvements compared to conventional synthesis
tools with two different technology mapping libraries. e-boost is available at
https://github.com/Yu-Maryland/e-boost.

</details>


### [51] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: 이 논문은 Position-Aware Confidence-Calibrated Sampling (PC-Sampler)라는 새로운 디코딩 전략을 제안하여 마스크된 확산 모델의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 마스크된 확산 모델(MDMs)은 강력한 비자기회귀 시퀀스 생성 대안으로 자리잡았지만, 디코딩 전략 선택에 따라 생성 품질이 민감하게 변동합니다.

Method: PC-Sampler는 글로벌 궤적 계획과 콘텐츠 인식 정보 최대화를 통합한 새로운 디코딩 전략입니다.

Result: PC-Sampler는 세 가지 고급 MDM과 일곱 가지 도전적인 벤치마크에서 기존 MDM 디코딩 전략보다 평균 10% 이상 성능을 향상시킵니다.

Conclusion: PC-Sampler는 최신 자기회귀 모델과의 성능 격차를 크게 줄입니다.

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [52] [G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance](https://arxiv.org/abs/2508.13023)
*Yongxin Guo,Wenbo Deng,Zhenglin Cheng,Xiaoying Tang*

Main category: cs.AI

TL;DR: RLVR은 대형 언어 모델의 추론 능력을 크게 향상시켰지만, 소형 언어 모델에서는 제한된 개선을 보였다. 이를 해결하기 위해, 우리는 SLM의 약점을 보완하는 Guided GRPO를 조사하고, G$^2$RPO-A라는 적응형 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: RLVR의 성공은 강력한 베이스 모델에 의존하지만, 소형 언어 모델에서는 그 효과가 제한적이다.

Method: Guided GRPO를 사용하여 실제 추론 단계를 롤아웃 경로에 주입하고, G$^2$RPO-A라는 적응형 알고리즘이 훈련 동적 변화에 맞춰 안내 강도를 조절하도록 한다.

Result: 수학적 추론 및 코드 생성 벤치마크 실험에서 G$^2$RPO-A가 일반 GRPO보다 유의미하게 우수한 성능을 보였다.

Conclusion: G$^2$RPO-A는 소형 언어 모델의 성능을 크게 개선할 수 있는 효과적인 방법이다.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced
the reasoning abilities of large language models (LLMs). Its success, however,
largely depends on strong base models with rich world knowledge, yielding only
modest improvements for small-size language models (SLMs). To address this
limitation, we investigate Guided GRPO, which injects ground-truth reasoning
steps into roll-out trajectories to compensate for SLMs' inherent weaknesses.
Through a comprehensive study of various guidance configurations, we find that
naively adding guidance delivers limited gains. These insights motivate
G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength
in response to the model's evolving training dynamics. Experiments on
mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A
substantially outperforms vanilla GRPO. Our code and models are available at
https://github.com/T-Lab-CUHKSZ/G2RPO-A.

</details>


### [53] [A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis](https://arxiv.org/abs/2508.13072)
*Yuting Zhang,Tiantian Geng,Luoying Hao,Xinxing Cheng,Alexander Thorley,Xiaoxia Wang,Wenqi Lu,Sandeep S Hothi,Lei Wei,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: 이 연구는 텍스트 기반의 다중 심장 작업 멀티모달 융합 프레임워크(TGMM)를 제안하여 다양한 임상 결과와 통합된 심장 데이터셋을 효과적으로 활용하여 심장 질환 진단, 위험 분류 및 정보 검색 등 여러 임상 작업을 수행하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 심혈관 관리에서 다중 심장 데이터셋을 효과적으로 통합하여 임상 프로필을 모델링하는 데 제한이 있었고, 이를 해결하기 위해 연구가 진행되었다.

Method: 다양한 심장 출처와 그 조합에서 데이터를 동적으로 통합하는 MedFlexFusion 모듈, 다양한 임상 목표에 맞춘 작업 관련 표현을 도출하는 텍스트 안내 모듈, 모든 작업에 대한 최종 결정을 생성하는 응답 모듈로 구성된 TGMM 프레임워크를 도입하였다.

Result: TGMM은 여러 임상 작업에서 최첨단 방법들을 초월하는 결과를 보여주었고, 추가 검증을 통해 또 다른 공개 데이터셋에서 견고성을 입증하였다.

Conclusion: TGMM의 체계적인 특징 탐색이 임상 결정 과정에서 다중 모달리티의 시너지 기여를 밝히고, 데이터의 통합 방법이 향상되었음을 보여준다.

Abstract: Contemporary cardiovascular management involves complex consideration and
integration of multimodal cardiac datasets, where each modality provides
distinct but complementary physiological characteristics. While the effective
integration of multiple modalities could yield a holistic clinical profile that
accurately models the true clinical situation with respect to data modalities
and their relatives weightings, current methodologies remain limited by: 1) the
scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated
single-modality or rigid multimodal input combinations; 3) alignment strategies
that prioritize cross-modal similarity over complementarity; and 4) a narrow
single-task focus. In response to these limitations, a comprehensive multimodal
dataset was curated for immediate application, integrating laboratory test
results, electrocardiograms, and echocardiograms with clinical outcomes.
Subsequently, a unified framework, Textual Guidance Multimodal fusion for
Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key
components: 1) a MedFlexFusion module designed to capture the unique and
complementary characteristics of medical modalities and dynamically integrate
data from diverse cardiac sources and their combinations; 2) a textual guidance
module to derive task-relevant representations tailored to diverse clinical
objectives, including heart disease diagnosis, risk stratification and
information retrieval; and 3) a response module to produce final decisions for
all these tasks. Furthermore, this study systematically explored key features
across multiple modalities and elucidated their synergistic contributions in
clinical decision-making. Extensive experiments showed that TGMM outperformed
state-of-the-art methods across multiple clinical tasks, with additional
validation confirming its robustness on another public dataset.

</details>


### [54] [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](https://arxiv.org/abs/2508.13121)
*Carlos Celemin*

Main category: cs.AI

TL;DR: 자동화된 게임 테스트 접근 방식을 소개하며, 게임 캐릭터를 제어하는 에이전트를 통해 게임 레벨 내 잠재적인 버그를 감지한다.


<details>
  <summary>Details</summary>
Motivation: 게임의 품질을 높이기 위해 자동화된 테스트 방법론이 필요하다.

Method: 베이지안 최적화(Bayesian Optimization, BO)를 사용하여 표본 효율적인 검색을 수행하고, 수집된 데이터를 분석하여 다음 샘플링 포인트를 결정한다.

Result: 이 접근 방식은 시간 효율성과 탐색 분포 모두에서 맵 커버리지 능력을 크게 향상시켰다.

Conclusion: 이 연구는 전통적인 모델의 확장성 문제를 해결하면서 BO에 필요한 매끄러움 및 불확실성 추정 기능을 갖춘 게임 테스트 전용 모델을 도입한다.

Abstract: This work introduces an automated testing approach that employs agents
controlling game characters to detect potential bugs within a game level.
Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient
search, the method determines the next sampling point by analyzing the data
collected so far and calculates the data point that will maximize information
acquisition. To support the BO process, we introduce a game testing-specific
model built on top of a grid map, that features the smoothness and uncertainty
estimation required by BO, however and most importantly, it does not suffer the
scalability issues that traditional models carry. The experiments demonstrate
that the approach significantly improves map coverage capabilities in both time
efficiency and exploration distribution.

</details>


### [55] [Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks](https://arxiv.org/abs/2508.13143)
*Ruofan Lu,Yichen Li,Yintong Huo*

Main category: cs.AI

TL;DR: 자율 에이전트 시스템의 성능 평가를 위한 새로운 벤치마크와 실패 원인 분석 제공


<details>
  <summary>Details</summary>
Motivation: 현재 자율 에이전트 시스템의 평가가 성공률에만 의존하고 있어 시스템 간 효과적 상호작용과 실패 원인을 체계적으로 분석할 필요가 있음.

Method: 34개의 대표적인 프로그래머블 작업으로 구성된 벤치마크를 구성하고, 이를 사용하여 세 가지 인기 있는 오픈 소스 에이전트 프레임워크와 두 개의 LLM 백본의 성능을 평가함.

Result: 과제 완료율 약 50% 관찰, 심층 실패 분석을 통해 계획 오류, 작업 실행 문제, 잘못된 응답 생성 등과 같은 세 가지 실패 원인을 도출함.

Conclusion: 제시된 실패 분류법과 완화 조언은 향후 보다 강력하고 효과적인 자율 에이전트 시스템 개발을 위한 경험적 기초를 제공함.

Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have
demonstrated promising capabilities in automating complex tasks. However,
current evaluations largely rely on success rates without systematically
analyzing the interactions, communication mechanisms, and failure causes within
these systems. To bridge this gap, we present a benchmark of 34 representative
programmable tasks designed to rigorously assess autonomous agents. Using this
benchmark, we evaluate three popular open-source agent frameworks combined with
two LLM backbones, observing a task completion rate of approximately 50%.
Through in-depth failure analysis, we develop a three-tier taxonomy of failure
causes aligned with task phases, highlighting planning errors, task execution
issues, and incorrect response generation. Based on these insights, we propose
actionable improvements to enhance agent planning and self-diagnosis
capabilities. Our failure taxonomy, together with mitigation advice, provides
an empirical foundation for developing more robust and effective autonomous
agent systems in the future.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: 이 논문은 다중 맥락 KV 캐시를 위한 주의 희소화의 첫 번째 탐색인 SamKV를 제안하며, 이는 정보 손실 없이 시퀀스 길이를 15%로 압축하여 처리량을 크게 증가시킵니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델은 긴 시퀀스 추론에서 상당한 비용 문제에 직면해 있습니다. 이를 해결하기 위해 역사적 KV 캐시를 재사용하여 추론 효율성을 개선하는 방법이 주류가 되었습니다.

Method: SamKV는 하나의 맥락을 희소화할 때 다른 맥락의 보완 정보를 고려하고, 그 후 희소화된 정보를 지역적으로 재계산합니다.

Result: 실험 결과, 우리의 방법은 전체 재계산 기준선과 비교하여 정확도 저하 없이 시퀀스 길이를 15%로 압축하며, 다중 맥락 RAG 시나리오에서 처리량을 크게 증가시킵니다.

Conclusion: SamKV를 통해 다중 맥락 KV 캐시의 효율성을 높이며, 기존 방법들이 효과적이지 않은 문제를 해결합니다.

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [57] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: 대립적 텍스트 공격에 대한 새로운 탐지 프레임워크인 Representation Stability(RS)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 트랜스포머 모델에 대한 대립적 텍스트 공격은 지속적인 위협이지만 기존 방어는 일반적으로 공격 특정적이거나 비용이 많이 드는 모델 재훈련을 요구한다.

Method: RS는 모델에 구애받지 않는 탐지 프레임워크로, 중요한 단어가 마스킹될 때 임베딩 표현의 변화를 측정하여 대립적 사례를 식별한다. 먼저 중요성 휴리스틱을 사용하여 단어를 순위 매기고, 상위 k개의 중요한 단어에 대한 마스킹에 대한 임베딩 민감도를 측정한 후, BiLSTM 탐지기를 통해 결과 패턴을 처리한다.

Result: 험니 실험 결과, 대립적으로 섭동된 단어는 자연적으로 중요한 단어에 비해 disproportionately 높은 마스킹 민감성을 보였다. 세 가지 데이터셋, 세 가지 공격 유형, 두 개의 피해 모델을 대상으로 RS는 88% 이상의 탐지 정확도를 달성하고, 종종 더 낮은 계산 비용으로 기존의 최첨단 방법과 경쟁력 있는 성능을 보여준다.

Conclusion: RS는 재훈련 없이도 보지 않은 데이터셋, 공격, 모델에 잘 일반화되어 있으며, 대립적 텍스트 탐지에 대한 실제적인 해결책을 제공한다.

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [58] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: 본 연구는 비침습적 동맥 혈압(ABP) 모니터링을 위한 경량 deep learning 모델인 sInvResUNet과 KDCL_sInvResUNet 협력 학습 방식을 소개하며, 실제 장치에서 실시간 ABP 추정이 가능하다는 점을 논의한다.


<details>
  <summary>Details</summary>
Motivation: 중환자 치료 및 수술 전후 관리에서 비침습적 동맥 혈압 모니터링이 필수적이다.

Method: 0.89 million 매개변수를 가진 경량 sInvResUNet와 협력 학습 방식 KDCL_sInvResUNet를 사용하여 8.49 밀리초의 추론 시간으로 실시간 ABP 추정을 성공적으로 달성하였다.

Result: 대규모 비독립적 검증에서 제안된 모델이 평균 절대 오차 10.06 mmHg, Pearson 상관계수 0.88로 큰 모델보다 약간 더 나은 성능을 보였다.

Conclusion: 모델이 다양한 인구 통계학적 및 심혈관 조건에서 성능 변동을 보였지만, 이 연구는 실제 환경에서의 비침습적 ABP 모니터링을 위한 기초 자료를 제공한다.

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [59] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: MBIIL은 생물 의학 분야에서 다양한 작업과 양식을 처리하기 위해 필수적이며, MSLoRA-CR 방법을 제안하여 기존 양식의 지식을 보존하고 새로운 양식을 지원하는 효과적인 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 생물 의학 도메인에서 다양한 작업과 양식을 다루기 위해 각 양식이나 작업에 대한 개별 모델을 교육하는 것은 추론 비용을 크게 증가시키므로, 통합된 모델을 점진적으로 훈련하는 것이 필요하다.

Method: MSLoRA-CR 방법은 모달리티 별 로라 모듈을 미세 조정하고 대조 정규화를 포함하여 모달리티 내 지식 공유를 강화하고 모달리티 간 지식 차별화를 촉진한다.

Result: 생물 의학 이미지를 점진적으로 학습하는 실험에서 MSLoRA-CR은 각 모달리티에 대해 개별 모델을 훈련하는 SOTA 접근 방식과 일반 점진적 학습 방법(로라 점진적 미세 조정)을 초월하는 성능을 보인다.

Conclusion: MSLoRA-CR은 제약이 없는 점진적 학습 방법에 비해 전반적인 성능에서 1.88% 향상을 달성하면서 계산 효율성을 유지한다.

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [60] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 이 연구는 다양한 차량 경로 문제(VRPs)에 대한 신경 해결책의 응용 가능성을 높이기 위해 삶의 주기 학습 프레임워크를 제안하며, 이를 통해 여러 맥락에서 VRPs를 효과적으로 관리할 수 있는 신경 해결책을 점진적으로 훈련시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존의 신경 해결책은 상대적으로 단조로운 맥락에서 VRP 문제를 해결하기 위해 훈련되어 다양한 시나리오에 대한 응용 가능성이 제한적이다.

Method: 이 논문에서는 Transformer 네트워크를 백본으로 활용하여 일련의 VRPs를 해결하는 평생 학습자(LL)를 제안하며, LL 내에서 맥락 간 자기 주의 메커니즘을 도입하여 이전 VRP 해결에서 얻은 지식을 다음 문제에 전이한다.

Result: 합성 및 벤치마크 사례(문제 크기 최대 18k)에 대한 광범위한 결과로, 우리의 LL은 다양한 맥락에서 일반 VRPs를 해결하기 위한 효과적인 정책을 발견할 수 있음이 입증되었으며, 다른 신경 해결책보다 우수한 성능을 보인다.

Conclusion: 이 연구는 다양한 차량 경로 문제를 효과적으로 해결하기 위한 신경 해결책의 중요한 발전을 보여주는 결과를 제시한다.

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [61] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: 인구의 변화 예측을 위해 시간 시계열 기반 모델을 적용한 연구로, 이 모델이 기존 방법들보다 높은 정확도를 보였음을 보여줌.


<details>
  <summary>Details</summary>
Motivation: 세계화, 경제 조건, 지정학적 사건 및 환경 요인에 의해 영향을 받는 인구 변화는 정책 입안자와 연구자들에게 중대한 도전 과제가 된다.

Method: 미국 인구 조사국과 연방준비은행 경제 데이터(FRED)에서 수집한 데이터셋을 사용하여 시간 시계열 기반 모델(TimesFM)을 적용하여 인구 변화를 예측한다.

Result: TimesFM이 전통적인 방법론인 LSTM, ARIMA, 선형 회귀에 비해 86.67%의 테스트 케이스에서 가장 낮은 평균 제곱 오차(MSE)를 달성했으며, 특히 역사적 데이터가 적은 소수 민족 집단에서 강력한 성능을 보였다.

Conclusion: 사전 학습된 기반 모델이 인구 분석을 향상시키고, 광범위한 특정 작업 조정 없이도 선제적 정책 개입에 정보를 제공할 수 있는 잠재력을 밝혀냈다.

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [62] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: 도시 부지의 공간 배치가 토지 사용 효율성과 공간 조직에 영향을 미친다. 본 논문에서는 다기능 레이아웃의 체계적인 정량화를 제한하는 전통적인 부지 계획을 개선하기 위해 데이터 기반의 사이트 계획 레이아웃 지표(SPLI) 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 사이트 계획은 경험적 판단과 단일 출처 데이터에 의존해 다기능 레이아웃의 정량화를 제한하므로, 보다 체계적이고 데이터 기반의 접근 방식이 필요하다.

Method: SPLI 시스템은 OpenStreetMap, 관심 지점, 건물 형태, 토지 이용, 위성 이미지를 통합하여 구조화된 도시 공간 정보를 생성하는 데이터 기반 프레임워크이다. 이를 통해 기능 분류, 공간 조직, 기능 다양성, 필수 서비스 접근성, 토지 이용 강도를 평가하는 다차원 지표를 제공한다.

Result: 실험 결과 SPLI는 기능 분류 정확성을 향상시키고 자동화된 데이터 기반 도시 공간 분석을 위한 표준화된 기반을 제공한다.

Conclusion: SPLI는 깊이 있는 학습 기술을 활용하여 데이터 격차를 해결하고, 도시 공간 분석의 정량적 접근을 개선한다.

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [63] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: 다변량 호크 프로세스는 복잡한 시스템에서 시간 의존성과 이벤트 주도 상호작용을 모델링하는 강력한 프레임워크를 제공한다. 이러한 방법은 관찰된 하위 프로세스 간의 인과 구조를 발견하는 데 중점을 두고 있으나, 실제 시스템은 종종 부분적으로만 관찰되며, 잠재적 하위 프로세스가 상당한 도전을 제기한다.


<details>
  <summary>Details</summary>
Motivation: 실제 시스템에서 관찰이 불완전하고 잠재적 하위 프로세스가 존재하는 문제를 해결하고자 함.

Method: 시간 간격이 줄어들면서 연속 시간 이벤트 시퀀스를 이산 시간 모델로 표현할 수 있다는 통찰을 활용하여 잠재적 하위 프로세스와 인과적 영향을 식별하기 위한 필요한 조건과 충분한 조건을 수립하는 두 단계 반복 알고리즘을 제안함.

Result: 합성 및 실제 데이터셋에 대한 실험을 통해 우리의 방법이 잠재적 하위 프로세스에도 불구하고 인과 구조를 효과적으로 복구함을 보여줌.

Conclusion: 제안된 알고리즘은 잠재적 하위 프로세스를 식별하는 데 있어 신뢰할 수 있는 방법이 될 수 있다.

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [64] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: BRIEF는 fMRI 기반 분류의 한계를 극복하기 위해 제안된 새로운 뇌 영감을 받은 프레임워크로, 네트워크 아키텍처를 자동으로 최적화하고 다중 기능 융합 모듈을 통해 향상된 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존의 fMRI 기반 분류 모델은 네트워크 아키텍처 결정과 특징 공간 융합에서 한계가 있다.

Method: BRIEF 프레임워크는 신경망 연결 검색(NCS) 전략과 변환기 기반의 다중 기능 융합 모듈을 활용하여 네트워크 아키텍처를 자동으로 최적화한다.

Result: BRIEF는 조현병 및 자폐 스펙트럼 장애를 건강한 대조군과 구별하는 데 있어 21개의 최첨단 모델과 비교하여 2.2%에서 12.1%까지 유의미한 성능 개선을 보여주었다.

Conclusion: BRIEF는 fMRI 기반 정신 질환 분류에 뇌 영감을 받은 강화 학습 전략을 적용한 첫 시도로, 정밀한 신경 영상 바이오마커 식별에 큰 잠재력을 보인다.

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [65] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: 고품질 레이블이 있는 지리공간 데이터셋은 인사이트 추출과 지구 이해를 위해 필수적이다. Google DeepMind의 AlphaEarth Foundations(AEF)는 이러한 데이터셋의 지리적 지역을 넘어 확장할 수 있는 방법론을 제안하고 평가한다. 랜덤 포레스트나 로지스틱 회귀와 같은 기본 모델로도 이 작업을 수행할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 고품질의 레이블이 있는 지리공간 데이터셋이 부족하여 전체 지구를 아우르는 분석이 어렵다.

Method: AlphaEarth Foundations(AEF)를 활용하여 지리공간 레이블 데이터셋을 기존의 지리적 지역을 넘어 확장하는 방법론을 제안하고 평가한다.

Result: 기본 모델인 랜덤 포레스트와 로지스틱 회귀를 사용하여 기존의 LANDFIRE 데이터셋을 미국을 넘어 캐나다로 확장하는 사례를 연구하고, EvtPhys와 EvtGp의 두 수준에서 분류 정확도를 검증한다.

Conclusion: EvtPhys의 경우, 모델 예측이 실제 데이터와 일치하며, USA와 캐나다의 EvtPhys 검증 세트에서 각각 81%와 73%의 분류 정확도를 달성하였다.

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [66] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: IoT 장치의 실시간 결함 분류를 위한 Fed-Meta-Align 프레임워크를 제안하여 비IID 데이터 문제를 극복하고 성능을 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 산업 안전을 위해 자원 제약이 있는 IoT 장치에서 실시간 결함 분류가 중요하지만, 이종 환경에서 강력한 모델을 훈련하는 것은 큰 도전 과제이다.

Method: 본 논문에서는 고급 초기화 및 훈련 파이프라인을 통해 이러한 한계를 극복하기 위한 새로운 4단계 프레임워크인 Fed-Meta-Align을 소개한다.

Result: Fed-Meta-Align은 이종 IoT 장치에서 평균적으로 91.27%의 테스트 정확도를 달성하며, 개인화된 FedAvg와 FedProx보다 각각 3.87% 및 3.37% 더 우수한 성능을 보인다.

Conclusion: 다단계 초기화 및 적응형 집계를 통한 접근 방식은 다양한 TinyML 네트워크에서 고성능 인공지능을 배포하기 위한 강력한 경로를 제공한다.

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [67] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: 이 연구는 현재의 강화 학습(RL) 방법이 확률적 결과를 가진 검증 가능한 도메인에서 언어 모델을 최적화하는 데 효과적인지 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 수학과 같은 결정적 도메인에서 언어 모델의 정확성을 개선하는 강화 학습의 효과를 넘어, 과학 실험과 같은 확률적 결과를 가진 도메인에서도 그 효과를 살펴보려는 필요성에서 출발하였습니다.

Method: 우리는 합성 데이터와 실제 생물학적 실험을 통해 그룹 상대 정책 최적화(GRPO), 근접 정책 최적화(PPO), REINFORCE Leave-One-Out (RLOO)를 비교하고, GRPO의 과신적 확률 예측 문제를 분석했습니다.

Result: GRPO는 이진 확률적 결과에 대해 과신적인 확률 예측을 유도하는 반면, PPO와 RLOO는 잘 교정된 모델을 생성함을 보여주었습니다.

Conclusion: GRPO의 그룹 표준 정규화 제거가 예측의 잘못된 보정을 수정하는데 효과적이라는 것을 입증하고, 표준 정규화가 과신을 유발하는 이론적 설명을 제시합니다. 우리의 결과는 GRPO에서 표준 정규화 사용에 대한 새로운 증거를 제공하며, 강화 학습을 통한 언어 모델의 이유를 위해 결정적 도메인을 넘어 확장할 수 있는 길을 엽니다.

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [68] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen은 결정적 공정성을 개선하면서 높은 유용성을 유지하는 합성 데이터 생성을 위한 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 개인정보 보호가 중요한 데이터가 부족한 상황에서 합성 데이터를 생성하는 것이 필수적입니다.

Method: FairTabGen은 대규모 언어 모델을 기반으로 하여 여러 공정성을 통합합니다.

Result: 다양한 데이터셋에서 공정성 지표가 최대 10% 향상되었습니다.

Conclusion: 이 결과는 공정하고 유용한 합성 표 형 데이터 생성을 위한 원칙적이고 실용적인 접근 방식을 보여줍니다.

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [69] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: 이 논문은 Kolmogorov-Arnold 네트워크(KANs)에서 빠른 계산 함수인 ReLU와 삼각 함수를 기반 컴포넌트로 사용하여 계산 효율성을 향상시키는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold 표현 정리에 기반한 신경망의 발전과 GPU에서 지원되지 않는 함수의 문제 해결 필요성.

Method: Kolmogorov-Arnold 네트워크(KANs)에서 ReLU와 삼각 함수를 포함한 빠른 계산 함수를 통합하여 네트워크 구조를 개선.

Result: 이 함수 조합은 경쟁력 있는 성능을 유지하면서 훈련 시간과 일반화에서 개선 가능성을 보여준다.

Conclusion: 빠른 계산 함수의 도입이 Kolmogorov-Arnold 네트워크의 성능과 효율성을 높일 수 있음을 입증한다.

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [70] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: 이 논문에서는 PCA 및 SVM을 결합하여 CNN의 분류 성능을 향상시키고, PCA 및 SVM 계층의 주의 영역을 시각화하는 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: CNN의 분류 성능을 개선하고자 하고, PCA와 SVM 계층에 대한 주의 영역 시각화를 통해 화이트 박스 방법을 개발하려고 합니다.

Method: PCA-Grad-CAM과 SVM-Grad-CAM이라는 두 가지 방법을 개발하여 PCA 특성 벡터 및 SVM 분류기 계층에서 주의 영역을 시각화합니다.

Result: 여러 주요 데이터셋에 적용한 결과, 제안한 방법들이 효과적으로 작동함을 보여줍니다.

Conclusion: pCA-Grad-CAM과 SVM-Grad-CAM을 통해 CNN의 PCA 및 SVM 계층에 대한 주의 영역을 효과적으로 시각화할 수 있습니다.

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [71] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 효율적인 고차원 데이터의 긴 시퀀스 모델링을 위한 새로운 아키텍처인 Efficient N-dimensional Attention (ENA)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 효율적인 고차원 데이터 모델링을 위해 기존의 Transformer보다 나은 아키텍처가 필요함.

Method: 언어 모델링을 위해 설계된 선형 순환 모델을 고차원 데이터에 확장하는 두 가지 핵심 측면(스캔 전략과 주의 하이브리드 아키텍처)을 조사하고, 다양한 주의 유형을 평가하여 타일 고차 슬라이딩 윈도우 주의(SWA)의 효율성을 입증합니다.

Result: SWA가 이론과 실제에서 효율적임을 확인하고, ENA라는 새로운 하이브리드 아키텍처를 제시하며 여러 실험을 통해 그 효과를 입증했습니다.

Conclusion: ENA는 글로벌 정보를 상태로 압축하고, SWA는 엄격한 로컬 모델링을 수행하여 함께 초장기 고차원 데이터 모델링을 위한 유망하고 실용적인 솔루션을 제공합니다.

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [72] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 이 논문은 장기 교통 배출 예측을 위한 Scale-Disentangled Spatio-Temporal Modeling (SDSTM) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 도시 대기 오염의 종합적인 관리를 위해 장기 교통 배출 예측이 중요하다.

Method: Koopman 리프팅 연산자를 기반으로 한 이중 스트림 특징 분해 전략을 도입하여 여러 스케일에서 특징을 분해하고 융합하는 방법을 제안한다.

Result: 제안한 모델은 기계적인 교차 항 손실을 기반으로 한 이중 스트림 독립성 제약을 포함하여 장기 교통 배출 예측의 정확성을 향상시킨다.

Conclusion: 시안 제2환상도로의 도로 레벨 교통 배출 데이터 세트에서 수행된 광범위한 실험으로 제안한 모델이 최첨단 성능을 달성함을 보여준다.

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [73] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 이 논문은 적대적 손실 및 확률적 행동 집합을 가진 선형 맥락 밴딧을 위한 효율적인 알고리즘을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 맥락 배포에 대한 지식 없이 선형 맥락 밴딧 문제를 해결하기 위해.

Method: 주어진 문제를 정해진 행동 집합을 가진 적대적 선형 밴딧 문제로 환원하여 알고리즘을 개발하였다.

Result: 알고리즘은 $	ilde{O}(	ext{min}\{d^2	ext{sqrt}(T), 	ext{sqrt}(d^3T	ext{log} K)\})$ 회귀를 달성하였고, $	ext{poly}(d,C,T)$ 시간에 실행된다.

Conclusion: 행동 집합의 수에 독립적으로 다항 시간 안에 $	ext{poly}(d)	ext{sqrt}(T)$ 회귀를 달성할 수 있음을 보여준다.

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [74] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: M3OOD는 멀티모달 환경에서 OOD 탐지기 선택을 위한 메타 학습 기반 프레임워크로, 역사적 모델 행동을 통해 새로운 데이터 분포 변화에 빠르게 적응할 수 있도록 지원한다.


<details>
  <summary>Details</summary>
Motivation: OOD 강인성은 현대 머신 러닝 시스템의 중요한 도전 과제로, 다양한 분포 변화를 처리하기 위해 적합한 OOD 탐지기를 자동으로 선택할 필요가 있다.

Method: M3OOD는 메타 학습을 활용하여 역사적 모델 행동을 학습하고, 멀티모달 임베딩과 수작업으로 만든 메타 특성을 결합하여 데이터셋을 표현한다.

Result: M3OOD는 다양한 멀티모달 기준에서 역사적 성능을 활용하여 새로운 데이터 분포 변화에 적합한 탐지기를 추천할 수 있다.

Conclusion: 실험 결과, M3OOD는 적은 계산 비용으로 12개의 테스트 시나리오에서 10개의 경쟁 기준선을 지속적으로 초과하였다.

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [75] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: 이 논문은 아날로그 메모리 내 계산(CIM) 아키텍처를 위한 보다 정교한 노이즈 인식 훈련 방법을 제안하며, 기존의 방법보다 성능 개선을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 아날로그 CIM 아키텍처는 신경망 추론을 위한 상당한 에너지 효율성 향상을 약속하지만, 배포에 주요 도전 과제가 되는 복잡한 하드웨어 유도 노이즈로 인해 어려움을 겪고 있다.

Method: 우리는 전방 노이즈 시뮬레이션을 후방 그래디언트 계산과 분리하여, 아날로그 CIM 시스템에서 더 정확하지만 계산적으로 다루기 힘든 노이즈 모델링을 이용한 노이즈 인식 훈련을 가능하게 한다.

Result: 우리의 확장된 STE 프레임워크는 이미지 분류에서 최대 5.3%의 정확도 향상, 텍스트 생성에서 0.72의 당혹도 감소, 훈련 시간에서 2.2배의 속도 개선, 표준 노이즈 인식 훈련 방법에 비해 37.9% 낮은 피크 메모리 사용량을 보여준다.

Conclusion: 이 접근법은 필수적인 그래디언트 방향 정보를 보존하면서도 계산 가능한 효율성과 최적화 안정성을 유지하는 것을 입증한다.

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [76] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: 본 연구는 MTPP 모델의 설명 가능성을 높이고, 예측 정확도를 개선하기 위한 새로운 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: MTPP 모델이 고위험 애플리케이션에서 널리 사용되면서 출력의 신뢰성에 대한 우려가 커지고 있다.

Method: 상반된 사실 설명과 사실 설명의 조합으로 MTPP의 설명을 정의하며, CFF라는 새로운 설명기를 제안한다.

Result: 실험 결과, CFF는 설명 품질과 처리 효율성 측면에서 기준 모델보다 우수함을 보여준다.

Conclusion: MTPP 예측 모델링에 대한 새로운 설명 제공 방식이 제안되었으며, 이는 예측의 신뢰성을 높이는 데 기여할 수 있다.

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [77] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 본 연구에서는 고배출 차량 식별을 위한 Set-Valued Transformer Network(SVTN)를 제안하여 데이터 마이닝에서의 식별 정확도를 높였다.


<details>
  <summary>Details</summary>
Motivation: 고배출 차량 식별은 도시 오염 수준을 규제하고 교통 배출 저감 전략을 수립하는 데 중요한 단계이다.

Method: 본 논문에서는 고배출 샘플로부터 판별 특성의 포괄적 학습을 달성하기 위해 Set-Valued Transformer Network(SVTN)를 제안한다.

Result: 실험 결과, 본 방법은 트랜스포머 기반 기준 모델에 비해 고배출 차량의 누락 탐지율을 9.5% 줄였다.

Conclusion: 제안된 방법은 고배출 이동 오염원을 정확하게 식별하는 데 있어 뛰어난 능력을 보였다.

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [78] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: 로라(LoRA) 모듈을 활용하여 독립적으로 훈련된 QA 도메인에서 조합을 통해 성능을 향상시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 발전은 규모에 의해 이끌리지만, PEFT를 통해 소수의 매개변수만 업데이트할 수 있다.

Method: LoRA를 사용하여 서로 다른 QA 도메인(수학, 의학, 금융)에서 어댑터를 훈련하고, 독립적으로 훈련된 LoRA 모듈이 비슷한 성능 향상을 이루는지 테스트했다.

Result: 수학+의학 조합에서 불확실성이 -9.10% 개선되었으며, 수학+금융과 금융+의학 조합에서도 각각 +4.54%와 +27.56%의 변화를 보였다.

Conclusion: 단순한 덧셈을 사용해도 추가 훈련이 필요 없으며, 몇 초 내에 적용할 수 있고, 합쳐진 데이터로 훈련된 모델과 유사한 성능을 달성했다.

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [79] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: 비선형 동적 시스템을 학습하기 위한 알고리즘을 제안하고, 이를 통해 예측 오류가 소멸함을 증명합니다.


<details>
  <summary>Details</summary>
Motivation: 비선형 동적 시스템을 안정적으로 학습하는 문제를 해결하고자 합니다.

Method: 스펙트럴 필터링 기법을 기반으로 한 알고리즘을 개발했습니다.

Result: 유한한 개수의 마지널 안정 모드를 가지는 비선형 동적 시스템에 대해 예측 오류가 소멸함을 증명하였습니다.

Conclusion: 새로운 스펙트럴 필터링 알고리즘이 비선형 시스템의 학습에 중요한 기여를 합니다.

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [80] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FedUHD는 Hyperdimensional Computing(HDC)에 기반한 최초의 비지도 연합 학습(UFL) 프레임워크로, 비독립 및 동등하게 분포되지 않은 데이터, 통신 비용 및 소음에 대한 취약성을 해결한다.


<details>
  <summary>Details</summary>
Motivation: 비지도 연합 학습(UFL)은 데이터 레이블링의 노동 집약성을 없애는 프라이버시 보호의 분산형 기계 학습 접근 방식으로 주목받고 있다.

Method: HDC 기반의 두 가지 새로운 디자인을 도입하여 UFL 성능을 개선한다. 클라이언트 측에서는 kNN 기반의 군집 하이퍼벡터 제거 방법이 비독립 및 동등하게 분포되지 않은 데이터 샘플을 처리하고, 서버 측에서는 가중 HDC 집계 기법이 클라이언트 간의 비독립 데이터 분포를 균형 있게 조정한다.

Result: FedUHD는 훈련에서 최대 173.6배 및 612.7배의 속도 향상과 에너지 효율성을 달성하며, 통신 비용을 최대 271배 낮추고, 다양한 환경에서 평균 15.50% 높은 정확도를 기록했다.

Conclusion: FedUHD는 최신 NN 기반 UFL 접근 방식과 비교할 때 다양한 유형의 소음에 대한 우수한 강인성을 보인다.

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [81] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: 이 논문은 비대칭 클라이언트 데이터를 가진 연합 학습에서의 공정성 문제를 다루며, 성능 공정성을 추구하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습은 분산 데이터 소스 간의 협업 훈련을 가능하게 하여 개인 정보 보호 문제를 해결하고 더 크고 다양성 있는 데이터에 접근할 수 있게 합니다.

Method: 클라이언트 손실을 명시적으로 규제하는 공정성 인식 방법에 초점을 맞추어 기존 및 새롭게 제안된 접근 방식을 평가합니다.

Result: FairGrad(근사)와 FairGrad*(정확)라는 두 가지 변형 방법이 여러 클라이언트 간의 공정성과 모델 성능을 향상시킴을 보여줍니다.

Conclusion: 이 연구에서는 다양한 공정성 방법 간의 연결 고리를 밝혀내고 이를 이론적으로 설명합니다.

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [82] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: VARAN은 개별 입력에 따라 레이어 집계를 동적으로 조정하여 자가 지도 음성 모델의 성능을 향상시키는 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 레이어 집계 방법은 정보 병목 현상과 정적 특징 가중치 문제를 가지고 있습니다.

Method: VARAN은 레이어 전용 프로빙 헤드와 데이터 의존적 가중치를 사용하여 입력에 따라 레이어의 특징을 동적으로 우선시합니다.

Result: 자동 음성 인식 및 음성 감정 인식 작업에서 VARAN은 특히 LoRA 미세 조정 기법을 사용할 때 우수한 성능을 보여줍니다.

Conclusion: VARAN은 레이어 특유 정보를 보존하면서도 유연한 특징 활용을 가능하게 하여 자가 지도 음성 표현의 효율적인 적응을 발전시킵니다.

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [83] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: 이 논문은 ISAC 기반 AIGC 서비스의 품질을 평가하기 위한 새로운 메트릭인 CAQA를 제안하며, 저복잡도의 LPDRL-F 알고리즘을 통해 자원 할당 최적화를 이루는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 AIGC 서비스는 입력 데이터의 정확성을 가정하지만, ISAC 기반 AIGC 네트워크에서는 부정확한 데이터에 의존하므로 새로운 품질 평가 방법이 필요합니다.

Method: 저복잡도의 선형 프로그래밍(LP) 기반 심층 강화 학습(DRL) 알고리즘인 LPDRL-F를 사용하여 자원 할당 문제를 해결합니다.

Result: 시뮬레이션 결과, LPDRL-F는 기존의 DRL 및 생성적 확산 모델 알고리즘에 비해 60% 이상 빠르게 수렴하고, 자원 할당 솔루션을 개선하여 AvgCAQA를 14% 이상 향상시킵니다.

Conclusion: LPDRL-F를 통해 기존 CGQ 중심의 접근 방식보다 50% 이상의 AvgCAQA 향상이 가능합니다.

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [84] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: 개인화된 의학을 실현하기 위해서는 환자의 장기적인 여정에서 통찰력을 도출하는 방법이 필요하며, 이는 의료 사건의 순열로 볼 수 있다. 본 논문에서는 116억 건의 의료 사건을 포함한 대규모 의료 데이터셋을 기반으로 사전훈련된 Cosmos 의료 사건 트랜스포머(CoMET) 모델을 소개하고, 이 모델이 다양한 실제 작업에서 우수한 성능을 발휘한다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 개인화된 의학의 대규모 실현을 위해 장기적인 환자 여정에서 통찰력을 도출할 수 있는 방법이 필요하다.

Method: 대규모 의료 사건 데이터셋인 Epic Cosmos를 기반으로 1억 1800만 명의 환자의 1150억 개의 의료 사건으로 사전훈련된 CoMET 모델을 도입하며, 의료 사건 데이터의 규모에 대한 연구를 진행한다.

Result: CoMET는 일반적인 사전훈련과 시뮬레이션 기반 추론을 활용하여 특정 작업에 맞춘 지도 학습 모델보다 우수한 성능을 보였다.

Conclusion: CoMET는 복잡한 임상 역학을 효과적으로 포착할 수 있는 생성적 의료 사건 모델로, 임상 의사 결정을 지원하고 헬스케어 운영을 간소화하며 환자 결과를 개선하는 확장 가능한 프레임워크를 제공한다.

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [85] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT는 다양한 지침 조정 데이터셋의 최적 혼합을 위해 동적이고 자동화된 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 지침 조정 데이터셋의 혼합을 최적화하는 것이 중요한 도전 과제가 되었습니다.

Method: 문제를 다중 무장 도박대 최적화 문제로 형식화하고, 원래 데이터셋 비율에 부드럽게 고정된 샘플링 분포를 업데이트하는 Prior-scaled Boltzmann Exploration을 도입했습니다.

Result: DynamixSFT는 16개의 지침 조정 데이터셋으로 구성된 Tulu-v2-mixture 컬렉션에 적용했을 때 10개 벤치마크에서 최대 2.2%의 성능 향상을 달성했습니다.

Conclusion: 우리 방법의 적응적 역학에 대한 더 깊은 통찰을 제공하기 위해 포괄적인 분석과 시각화를 제공합니다.

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [86] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 이 논문은 순환 신경망(RNN)의 게이팅 메커니즘이 고정된 학습률로 훈련할 때조차 적응형 학습률 동작을 어떻게 유도하는지를 연구한다.


<details>
  <summary>Details</summary>
Motivation: RNN에서 게이팅 메커니즘이 학습률 조정을 어떻게 유도하는지 이해하고자 함.

Method: 누수 적분자 및 게이티드 RNN에 대한 정확한 야코비안을 유도하여 첫 번째 차수 확장을 도출하고, 게이트가 기울기 전파를 어떻게 재형성하고 효과적인 단계 크기를 조절하는지를 명시함.

Result: 게이트가 숨겨진 상태의 메모리 유지뿐만 아니라 데이터 기반 전처리기로 작용하며, 매개변수 공간에서 최적화 경로를 조정하는 것으로 나타남.

Conclusion: 게이팅 메커니즘이 상태 진화와 매개변수 업데이트를 결합하는 방법에 대한 통합된 동역학 시스템 관점을 제공하며, 게이티드 구조가 실질적으로 강력한 훈련 및 안정성을 달성하는 이유를 설명함.

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [87] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: DE-VAE는 차별 엔트로피를 사용하는 불확실성 인식 변형 오토인코더로, 다차원 데이터의 매개변수화된 및 가역적 프로젝션을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들이 분포 외 샘플을 처리하는 데 부족함이 있어 이를 보완할 필요가 있습니다.

Method: DE-VAE는 주어진 고정 프로젝션을 바탕으로 2D 공간으로의 매핑과 원래 공간으로의 역매핑을 학습하도록 훈련됩니다.

Result: DE-VAE는 다른 현재 오토인코더 기반 접근법과 유사한 정확도로 매개변수화된 및 역 프로젝션을 생성할 수 있습니다.

Conclusion: DE-VAE는 임베딩 불확실성을 분석할 수 있게 해줍니다.

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [88] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: 인공지능과 머신러닝을 활용한 실시간 디지털 심전도 분석이 가능해졌다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 심전도 분석의 해석 가능성과 정밀성을 향상시키기 위해 딥러닝 아키텍처인 AICRN을 제안한다.

Method: AICRN은 PR 간격, QT 간격, QRS 지속시간, 심박수, R파의 피크 진폭, T파의 진폭과 같은 주요 심전도 매개변수를 회귀 분석하기 위해 공간적 및 채널 주의 메커니즘을 활용하여 설계되었다.

Result: AICRN 모델이 기존 모델들보다 매개변수 회귀에서 더 높은 정확도를 기록하였다.

Conclusion: 본 연구는 딥러닝이 심전도 분석의 해석 가능성과 정밀성에 중요한 역할을 할 수 있음을 보여주며, 심장 모니터링 및 관리의 새로운 임상 응용을 열어준다.

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [89] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC는 단백질 정보 모델링을 개선하기 위한 두 단계의 경량 압축 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 단백질 대형 언어 모델의 통합된 모델링을 지원하지만, 두 가지 주요 한계가 있다: (1) 구조와 시퀀스 토큰의 연결로 인해 단백질 길이가 약 두 배가 되고 모달리티 간의 정렬이 깨진다. (2) 단일 단백질 입력으로 훈련되어 인컨텍스트 학습에 적합하지 않다.

Method: ProtTeX-CC는 시퀀스와 구조 표현을 잔여 수준에서 융합하는 공동 임베딩 압축 메커니즘과 전체 데모를 마지막 몇 개의 언어 토큰의 잠재 공간으로 집계하는 자기 압축 모듈을 설계한다.

Result: ProtTeX-CC는 16 샷 환경에서 원래 ProtTeX보다 약 93.68%의 압축 비율을 달성하며, 단백질 기능 예측 실험에서 2%의 성능 향상과 11%의 일반화 성능 향상을 보여준다.

Conclusion: ProtTeX-CC는 기본 모델을 수정하지 않고도 몇 가지 추가 매개변수만으로 성능을 개선한다.

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [90] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: 본 연구에서는 대형 언어 모델을 위한 잊혀질 권리(GDPR 제17조)를 탐구하고, 비학습을 재현 가능한 시스템 문제로 프레이밍합니다.


<details>
  <summary>Details</summary>
Motivation: GDPR에 따른 잊혀질 권리를 대형 언어 모델에 적용하고, 비학습 문제를 효과적으로 해결하고자 하는 목적이 있습니다.

Method: 훈련을 결정론적 프로그램으로 간주하고 최소한의 마이크로 배치 기록을 로그합니다. 훈련 꼬리를 재생할 때, 잊어버릴 폐쇄만 필터링함으로써 매개변수를 동일하게 얻습니다.

Result: 조절된 실행에서 모델과 최적화 상태의 바이트 동일성을 증명하였으며, 저장 및 대기 시간 예산을 보고합니다.

Conclusion: 제시된 방법론을 통해 잊혀질 권리를 효과적으로 처리할 수 있음을 확인했습니다.

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [91] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: 본 연구에서는 GAN의 대안으로 새로운 분포 매칭 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 생성 모델의 최근 발전은 데이터 합성 외에도 분포 매칭 작업에서 중요한 역할을 합니다.

Method: 우리는 Continuous Normalizing Flow(CNF)에서 영감을 받아 새로운 접근 방식을 제안했습니다.

Result: 이 모델은 CNF 모델의 장점을 상속받으며, 스스로의 합리적인 목표를 가지고 다양한 제약 조건에 적응할 수 있습니다.

Conclusion: 우리는 제안된 목표의 이론적 검증을 제공하고 실험을 통해 성능을 입증합니다.

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [92] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: 비동기 대체 방향 멀티플라이어 방법(ADMM)을 사용하여 대규모 최적화에서 통신 비용 문제 해결을 위한 거칠게 양자화하는 방법을 제안.


<details>
  <summary>Details</summary>
Motivation: 노드의 제한된 통신 예산과 대규모 데이터로 인해 발생하는 통신 비용 문제를 해결하기 위해.

Method: 비동기 ADMM에서 교환되는 데이터에 대해 거칠게 양자화를 도입하여 통신 오버헤드를 줄임.

Result: 여러 분산 학습 작업에 대해 제안된 방법의 수렴성을 실험적으로 검증.

Conclusion: 대규모 연합 학습 및 분산 최적화에서 유용할 것으로 기대됨.

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [93] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 이 논문에서는 사전 훈련된 언어 모델을 사용하여 시계열 예측 문제에 접근하는 새로운 방법인 CC-Time을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사전 훈련된 언어 모델이 시계열 예측에서 만족할 만한 예측 정확도를 달성하지 못하는 문제를 해결하기 위해.

Method: CC-Time은 시계열 데이터와 해당 텍스트 설명을 통해 시간적 의존성과 채널 상관관계를 모델링하기 위해 교차 모달리티 학습을 포함하고, PLM과 시계열 모델의 지식을 적응적으로 통합하기 위해 교차 모델 융합 블록을 제안합니다.

Result: 아홉 개의 실제 데이터 세트에서 실험을 통해 CC-Time은 전체 데이터 훈련 및 소수의 샷 학습 상황 모두에서 최상의 예측 정확도를 달성합니다.

Conclusion: 이 연구는 PLM 기반 시계열 예측의 가능성을 보여주며, 더 나은 예측 모델 생성을 위해 다양한 학습 접근 방식을 통합하는 필요성을 강조합니다.

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [94] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: 본 연구에서는 딥 하이퍼그래프 학습을 위한 첫 번째 포괄적 벤치마크인 DHG-Bench를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 딥 그래프 모델이 관계 학습에서 큰 성공을 거두었으나 쌍 관계에만 초점을 맞추어 현실 세계의 복잡한 시스템에서의 고차 상호작용 학습에 제한이 있다고 인식하였다.

Method: 20개의 다양한 데이터셋과 16개의 최첨단 HNN 알고리즘을 통합하여 일관된 데이터 처리 및 실험 프로토콜 하에 DHG-Bench를 구축하였다.

Result: DHG-Bench로 실시한 광범위한 실험은 기존 알고리즘의 강점과 본질적인 한계를 드러내어 향후 연구 방향에 대한 귀중한 통찰을 제공한다.

Conclusion: 향후 연구를 도울 수 있는 재현 가능한 연구를 촉진하기 위해 다양한 HNN 방법을 훈련 및 평가할 수 있는 사용하기 쉬운 라이브러리를 개발하였다.

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [95] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: 스파티오-템포럴 멀티스케일 맘바(STM2)와 그 향상된 버전인 STM3를 제안하여 긴 기간의 시공간 시계열 예측을 위한 효율적인 모델을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 기존의 딥러닝 방법들이 복잡한 장기 시공간 의존성을 효율적으로 학습하는 데 어려움을 겪고 있어, 이를 해결하기 위한 효율적인 접근이 필요하다.

Method: STM2는 멀티스케일 정보를 효율적으로 포착하기 위해 멀티스케일 Mamba 아키텍처와 복잡한 멀티스케일 시공간 의존성을 학습하기 위한 적응형 그래프 인과 컨볼루션 네트워크를 포함한다. STM3는 안정적인 라우팅 전략과 인과 대조 학습 전략을 포함하여 규모 구분 가능성을 높인다.

Result: STM3는 더 나은 라우팅 매끄러움과 각 전문가의 패턴 분리를 보장하며, 실세계 벤치마크에서 STM2/STM3의 우수한 성능을 입증하였다.

Conclusion: STM2/STM3는 긴 기간의 시공간 시계열 예측에서 최첨단 결과를 달성하였다.

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [96] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: 이 논문은 시간 시계열 예측을 해석하기 위한 통합 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 항공, 에너지, 소매 및 건강 등 여러 분야에서 시간 시계열 예측이 중요한 결정을 뒷받침한다.

Method: LIME과 SHAP을 활용하여 단일 변수 시계열을 비누출 감독 학습 문제로 변환하고, ARIMA 기준선과 함께 경량부스터 트리를 훈련시킨 후 사후 설명 가능성을 적용한다.

Result: 공항 승객 데이터셋을 사례로 이용하여, 소수의 지연된 특성, 특히 12개월 지연이 대부분의 예측 변동성을 설명한다.

Conclusion: LIME과 SHAP을 시간 시계열에 적용하는 방법론과 알고리즘의 이론적 설명, 경험적 평가 및 실무자 가이드라인을 제시한다.

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [97] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 본 논문은 전통적인 최적화 방법을 향상시키기 위해 학습된 2차 최적화 기법, 특히 Symmetric-Rank-One 알고리즘을 증대시키는 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 엔드 투 엔드 딥 러닝의 한계는 대규모 레이블 데이터 세트에 의존하며, 보지 못한 시나리오에 대한 일반화가 부족하고 계산 요구가 증가하는 점이다.

Method: 우리는 학습 가능한 전처리 유닛을 도입하여 전통적인 Symmetric-Rank-One (SR1) 알고리즘을 개선하는 새로운 학습된 2차 최적화 기법을 제안한다.

Result: 우리의 방법은 분석 실험과 Monocular Human Mesh Recovery (HMR)와 같은 실제 작업에서 평가되었으며, 기존의 학습 최적화 기반 접근 방식보다 우수한 성능을 보인다.

Conclusion: 모델이 경량화되어 있으며 주석 데이터나 미세 조정이 필요 없고 강력한 일반화를 제공하는 우리 방법은 더 넓은 최적화 기반 프레임워크에 통합하기에 적합하다.

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [98] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: CRoC는 제한된 레이블 데이터와 풍부한 비레이블 데이터를 활용하여 GNN을 훈련시켜 그래프 이상 탐지(GAD)를 향상시키는 효과적인 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계의 응용 프로그램에서 GNN 훈련에 필요한 레이블 데이터의 부족은 GAD의 진행을 심각하게 저해한다.

Method: CRoC는 GAD의 클래스 불균형을 활용하여 각 노드의 컨텍스트를 리팩토링하고 노드의 속성을 재조합하여 보강 그래프를 구축하며, 이질적인 관계를 별도로 인코딩하여 메시지 전달 과정에 통합한다.

Result: CRoC는 7개의 실제 GAD 데이터셋에서 평가되었으며, 최대 14%의 AUC 개선을 기록하고 제한된 레이블 설정에서 최신 GAD 방법들을 초월한다.

Conclusion: CRoC는 GNN이 적대적인 위장에 대한 견고성을 향상시키면서 복잡한 상호작용 의미를 포착할 수 있도록 한다.

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [99] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: 본 논문에서는 Lion 최적화 알고리즘의 수렴 특성을 분석하고, 표준 가정 하에 수렴 속도를 개선하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: Lion 최적화 알고리즘의 수렴 특성을 이해하고, 전통적인 최적화 방법보다 더 나은 성능을 달성하기 위한 접근 방법을 연구하는 것이 목적입니다.

Method: 표준 및 분산된 환경에서 Lion 최적화 알고리즘의 변동성을 줄이는 방법을 도입하여 수렴 속도를 향상시키고, 여러 노드를 고려한 수렴 속도를 분석합니다.

Result: Lion 최적화 알고리즘은 기본적으로 $	ext{O}(d^{1/2}T^{-1/4})$의 수렴 속도를 보이며, 변동성을 줄인 버전은 개선된 속도인 $	ext{O}(d^{1/2}T^{-1/3})$를 달성합니다. 분산 환경에서도 유사한 개선된 속도를 보여줍니다.

Conclusion: Lion 최적화 알고리즘은 다양한 설정에서 우수한 수렴 속도를 보이며, 특히 통신 효율성을 고려한 변형이 효과적임을 증명합니다.

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [100] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 본 논문은 확산 모델에 대한 추론 시간 스케일링의 적용을 탐구하고, 새로운 탐색 알고리즘 접근 방식을 통해 샘플 품질을 향상시키는 두 가지 전략을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 확산 모델에 대한 SMC 기반 방법의 효과를 이해하고, 초기 단계 노이즈 샘플과 후기 단계 샘플 간의 평가 문제를 해결하기 위해.

Method: Funnel Schedule과 Adaptive Temperature의 두 가지 전략을 제안하여 탐색-착취 트레이드오프를 잘 조절.

Result: 여러 벤치마크와 최신 텍스트-이미지 확산 모델에 대한 실험 결과, 제안된 방법이 이전 기준보다 우수한 성능을 보임을 보여준다.

Conclusion: 제안된 방법은 총 노이즈 함수 평가 수를 증가시키지 않으면서도 샘플 품질을 크게 향상시킨다.

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [101] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: Bi-Axial Transformer(BAT)는 전자 건강 기록을 보다 잘 처리하기 위해 개발된 새로운 모델로, 데이터 희소성 문제를 해결하고 높은 성능을 발휘합니다.


<details>
  <summary>Details</summary>
Motivation: 전자 건강 기록(EHR)이 복잡해짐에 따라 효율적인 분류 모델의 필요성이 증가하고 있습니다.

Method: BAT 모델은 EHR 데이터의 임상 변수와 시간 축을 동시에 고려하여 데이터 관계를 학습합니다.

Result: BAT는 패혈증 예측에서 최첨단 성능을 달성하고, 사망률 분류에서도 경쟁력 있는 결과를 보여줍니다.

Conclusion: BAT는 데이터 결측에 대한 강인함을 보여주며, transfer learning에 사용할 수 있는 독특한 센서 임베딩을 학습합니다.

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [102] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: 제조 비용 추정을 변환하는 통합 기계 학습 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 견적 워크플로우는 노동 집약적인 프로세스 계획을 요구하지만, 이 접근 방식은 자동차 서스펜션 및 조향 부품의 13,684개 DWG 도면에서 직접 200개의 기하학적 및 통계적 설명자를 추출합니다.

Method: Gradient-boosted 결정 트리 모델(XGBoost, CatBoost, LightGBM)을 사용하여 이러한 특징으로 훈련된 모델은 그룹 전반에 걸쳐 거의 10% 평균 절대 백분율 오차를 달성합니다.

Result: 이는 부품별 휴리스틱을 넘는 견고한 확장성을 보여줍니다.

Conclusion: 비용 예측과 SHAP와 같은 설명 가능성 도구를 결합함으로써, 이 프레임워크는 회전된 차원 최대값, 호 통계 및 발산 메트릭을 포함한 기하학적 설계 요인을 식별하여 비용을 고려한 설계를 위한 실행 가능한 통찰력을 제공합니다. 이 통합 CAD-to-cost 파이프라인은 견적 리드 타임을 단축하고, 부품 가족 간의 일관되고 투명한 비용 평가를 보장하며, 산업 4.0 제조 환경에서 실시간 ERP 통합 의사 지원으로 나아갈 수 있는 경로를 제공합니다.

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [103] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: 이 논문은 다양한 지역적 스케일과 클러스터 수를 가진 데이터셋을 위한 적응형 평균 이동 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 본 연구의 목적은 변화하는 데이터 클러스터의 특성을 효과적으로 반영할 수 있는 알고리즘을 개발하는 것이다.

Method: 알고리즘은 특정 점에서 다른 모든 점까지의 지역 거리 분포를 활용하여 지역 클러스터의 수를 추정하며, 밀도 분포의 지역 최소값을 식별하여 클러스터 매개변수를 계산한다.

Result: 제안된 알고리즘은 최근 제안된 적응형 평균 이동 방법보다 더 나은 성능을 보였으며, 광범위한 클러스터링 벤치마크에서도 경쟁력 있는 성능을 입증했다.

Conclusion: 결론적으로, 이 알고리즘은 다양한 데이터셋에서 클러스터링 성능을 개선할 수 있는 효과적인 방법임을 보여준다.

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [104] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: Cold-RL은 NGINX를 위한 강화 학습 기반의 제거 정책으로, LRU를 대체하여 캐시 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 웹 프록시인 NGINX는 주로 가장 최근에 사용되지 않은(LRU) 제거 기술에 의존하는데, 이는 사이즈에 무관하고 주기적인 폭발과 혼합된 객체 크기에서 성능 저하가 발생할 수 있다.

Method: Cold-RL은 NGINX의 LRU 강제 만료 경로를 교체하고, 엄격한 마이크로초 예산 내에서 ONNX 사이드카가 제공하는 이중 Deep Q-Network를 사용한다. 각 제거에서 Cold-RL은 K개의 가장 최근에 사용되지 않은 객체를 샘플링하고, 여섯 개의 경량 특징(나이, 크기, 히트 수, 도착 간 시간, 남은 TTL, 마지막 오리진 RTT)을 추출한 후 희생자 비트마스크를 요청한다.

Result: Cold-RL은 25MB 캐시에서 히트 비율을 0.1436에서 0.3538로, 100MB에서 0.7530에서 0.8675로, 400MB에서 약 0.918로 개선하며, 기존 최상의 클래식 기준과 유사한 성과를 도출한다.

Conclusion: 이 연구는 NGINX에 엄격한 서비스 수준 목표(SLO)를 통합한 최초의 강화 학습 기반 제거 정책임을 밝힌다.

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [105] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 사이버 공격이 증가하고 있으며, 이에 대한 보안 비용이 산업에 연간 수십억 달러를 초래하고 있다. 본 연구는 자연어 처리(NLP)와 심층 학습을 활용하여 MITRE CWE 데이터베이스의 텍스트 설명을 분석하고 사이버 공격의 잠재적 영향을 분류하는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 사이버 공격의 복잡성이 증가함에 따라 공격 설명을 평가하고 미래의 결과를 예측하기 위한 자동화 방법의 필요성이 커지고 있다.

Method: 자연어 처리(NLP)와 Bidirectional Encoder Representations from Transformers(BERT), Hierarchical Attention Networks(HANs)를 결합한 다중 레이블 분류 기법을 사용하여 MITRE CWE 데이터베이스의 텍스트 설명을 분석한다.

Result: BERT는 다중 레이블 분류에서 0.972의 정확도를 달성하며, 기존의 CNN 및 LSTM 기반 모델보다 우수한 성능을 나타낸다.

Conclusion: BERT는 사이버 공격의 결과를 예측하는 데 더 적합하며, 특정 사이버 보안 레이블에서 HAN이 CNN 및 LSTM 모델보다 더 나은 성능을 보인다.

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [106] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: 비용을 고려한 경량 모델 라우팅 방법인 CSCR을 소개하며, 기존 방법에 비해 정확도와 비용을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델을 위한 비용 인식 라우팅을 연구하며, 다양한 모델 풀에서의 효율적인 선택을 목적으로 한다.

Method: 비용 스펙트럼 대조 라우팅(CSCR)이라는 경량 프레임워크를 도입하여 프롬프트와 모델을 공유 임베딩 공간에 매핑한다.

Result: CSCR은 여러 벤치마크에서 일관되게 기준선을 초과하며 정확도-비용의 무역 오프를 최대 25% 개선한다.

Conclusion: CSCR은 전문가 풀의 변화 시 재훈련 없이 마이크로초 지연을 허용하며, 보지 못한 LLM 및 분포 외의 프롬프트에 대해 강력하게 일반화 된다.

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [107] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: 확률 최적 제어 문제를 해결하기 위한 새로운 방법을 제안하며, 이 방법은 기하학적 어닐링을 통해 목표 측정값에 접근하는 것이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 최적화 방법으로는 목표 측정값과 사전 측정값이 크게 다를 경우 어려움이 있었다.

Method: 제한된 문제를 반복적으로 해결하는 접근 방식을 사용하며, 신뢰 구역을 포함하여 체계적으로 목표 측정값에 접근한다.

Result: 새로운 방법이 확산 기반 샘플링, 전이 경로 샘플링 및 확산 모델의 미세 조정 등을 포함한 여러 최적 제어 응용 프로그램에서 성능을 크게 향상시킬 수 있음을 보여준다.

Conclusion: 신뢰 구역 기반 전략은 사전에서 목표 측정값으로의 기하학적 어닐링으로 이해될 수 있다.

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [108] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: NeurIPS 2023 Neural MMO Competition의 결과를 발표하였으며, 200명 이상의 참가자가 참여하였다.


<details>
  <summary>Details</summary>
Motivation: Neural MMO Competition에서 목표 조건 정책을 훈련하여 실제 작업, 맵 및 훈련 중에 보지 못한 상대에게 일반화하는 방법을 찾음.

Method: 단일 4090 GPU에서 8시간 훈련하여 성능 최적화.

Result: 최고의 솔루션은 베이스라인보다 4배 높은 점수를 기록했다.

Conclusion: Neural MMO 및 대회 관련 모든 내용을 MIT 라이센스 하에 오픈 소스화하였다.

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [109] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: 이 논문에서는 변분 오토인코더(VAE)의 후방 붕괴 문제를 해결하기 위한 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 후방 붕괴 문제는 생성된 샘플의 다양성을 감소시킵니다.

Method: 네트워크 구조에 대한 제약 없이 후방 붕괴를 제어하기 위해 Latent Reconstruction(LR) 손실을 제안합니다.

Result: MNIST, fashionMNIST, Omniglot, CelebA, FFHQ와 같은 다양한 데이터셋에서 후방 붕괴를 제어할 수 있음을 실험적으로 입증했습니다.

Conclusion: 제안하는 LR 손실은 특정 아키텍처에 제한받지 않고 후방 붕괴를 효과적으로 줄이는 데 기여합니다.

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [110] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: 모델의 미세 조정이 종종 안전성 문제를 초래하는 것으로 알려져 있으나, 본 논문에서는 최적화 선택이 주요 원인임을 보여주고 핵심 하이퍼파라미터를 조정함으로써 안전하지 않은 응답 비율을 감소시킬 수 있음을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델의 미세 조정이 안전성에 부정적인 영향을 미친다는 강한 믿음을 도전하고, 안전성을 유지하면서 성능을 극대화할 방법을 찾고자 한다.

Method: 햄버거 비율 및 주기적인 매칭을 활용한 간단한 지수 이동 평균(EMA) 기법을 통해 안정적인 최적화 경로를 확보하고, 원래 모델의 안전 속성을 보존한다.

Result: 하이퍼파라미터 선택을 통해 안전하지 않은 응답을 16%에서 약 5%로 줄일 수 있었다.

Conclusion: 우리의 연구는 추가적인 안전 데이터 없이도 미세 조정 과정에서 발생할 수 있는 안전성 문제를 대부분 피할 수 있음을 보여준다.

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [111] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: 본 연구는 뇌 그래프 구축의 데이터 중심 디자인 공간을 정의하고 벤치마킹하여 그래프 기반 신경영상에서의 성능 향상을 보여준다.


<details>
  <summary>Details</summary>
Motivation: fMRI 데이터로부터 뇌 그래프를 구축하는 과정에서 중요하지만 간과되는 데이터 중심 선택을 조명하기 위해.

Method: 데이터 중심 AI 관점을 채택하여 시간 신호 처리, 위상 추출, 그래프 특성화의 세 단계로 구성된 디자인 공간을 개발하고 평가한다.

Result: HCP1200 및 ABIDE 데이터셋에 대한 실험에서 데이터 중심 구성 방식이 표준 파이프라인보다 일관되게 분류 정확도를 향상시킨다는 것을 발견했다.

Conclusion: 이 연구는 그래프 기반 신경영상에서 데이터 선택의 중요성을 강조하고, 데이터 중심 디자인 공간을 체계적으로 탐색할 필요성을 강조한다.

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [112] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: OS-R1은 규칙 기반 강화 학습을 활용한 리눅스 커널 튜닝 프레임워크로, 커널 구성 공간을 RL 환경으로 추상화하여 효율적인 탐색과 정확한 구성 수정을 가능하게 하고, 다양한 조정 시나리오에서의 수렴 가속과 재훈련 최소화를 위해 두 단계 훈련 과정을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 리눅스 커널 튜닝은 운영 체제 성능 최적화에 필수적이지만, 기존 방법들은 효율성, 확장성 및 일반화 측면에서 어려움을 겪고 있다.

Method: OS-R1은 규칙 기반 강화 학습을 기반으로 하는 리눅스 커널 튜닝 프레임워크로, 커널 구성 공간을 RL 환경으로 추상화하여 대형 언어 모델에 의한 효율적인 탐색을 촉진하고 정확한 구성 수정이 이루어지도록 한다.

Result: 실험 결과 OS-R1은 기존 기준 방법들에 비해 현저한 성능 향상을 보여주었으며, 탐색적 튜닝에 비해 최대 5.6% 성능 개선을 달성하며 높은 데이터 효율성을 유지했다.

Conclusion: OS-R1은 다양한 실제 애플리케이션에서 적응 가능하며, 다양한 환경에서의 실용적 배포 가능성을 보여준다.

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [113] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: 대형 언어 모델(LLM) 기반의 코딩 에이전트를 위한 시각 분석 시스템을 소개하며, 이를 통해 코딩 에이전트의 행동을 효율적으로 분석하고 개선할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 코딩 에이전트의 코딩 과정을 효과적으로 검토하고 조정하기 위한 필요성.

Method: AIDE 프레임워크를 중심으로 하는 시각 분석 시스템을 통해 코드, 프로세스, LLM 수준에서 비교 분석 지원.

Result: 시스템은 ML 과학자들이 코딩 에이전트의 행동을 구조적으로 이해할 수 있도록 하여 더 효과적인 디버깅과 프롬프트 엔지니어링을 가능하게 한다.

Conclusion: 우리의 시스템은 코딩 에이전트를 사용하여 인기 있는 Kaggle 대회 과제를 해결하는 사례 연구를 통해 반복적인 코딩 과정에 대한 귀중한 통찰력을 제공한다.

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [114] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: 재무 시계열의 복잡성을 해결하기 위한 예측 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 재무 시계열 데이터의 복잡성을 해결하기 위해 새로운 예측 모델이 필요하다.

Method: 슬라이딩 윈도우와 변형 모드 분해(VMD) 방법을 결합한 예측 모델을 제안하고, 이를 통해 비정상 재무 시계열을 더 부드러운 하위 구성 요소로 분해하여 딥 러닝 모델에 입력한다.

Result: VMD로 처리된 시퀀스에 대해 훈련된 LSTM 모델의 예측 성능이 원시 시계열 데이터를 사용한 모델보다 우수하고 안정적임을 보여준다.

Conclusion: 이 연구는 VMD와 딥 러닝을 결합한 접근이 재무 예측의 성능을 향상시킬 수 있음을 시사한다.

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [115] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: 다중 스케일 시스템은 과학과 기술에서 흔하지만 시뮬레이션이 어려운 문제가 있다. 본 논문은 이 시스템에서의 동역학을 기계 학습하기 위한 이론적 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 스케일 시스템은 다양한 과학 및 기술 분야에서 나타나지만, 짧은 시공간 스케일과 밀접하게 연결된 대량 물리 현상을 적절하게 시뮬레이션하기 어려운 문제에 대한 해결법이 필요하다.

Method: 이 프레임워크는 메트리플렉틱 브래킷 공식을 사용하여 구성되며, 이를 통해 열역학 제1, 2 법칙 및 운동량 보존을 보장하며 비평형 통계를 포착하기 위한 이산적인 요동-소산 균형을 유지한다.

Result: 제안된 방법은 벤치마크 시스템에서 검증되었으며, 스타 폴리머의 비평형 통계를 보존하면서의 세분화 및 고속 비디오에서의 콜로이드 현탁액의 동역학 모형 학습에 대한 유용성이 입증되었다.

Conclusion: 제안된 방법론은 파이토치와 LAMMPS에서의 오픈소스 구현을 통해 대규모 추론과 파티클 기반 시스템으로의 확장성을 가능하게 한다.

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [116] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: 펩타이드 및 단백질의 아밀로이드 생성 예측은 생물정보학의 핵심 주제이다. 본 연구에서는 사전 훈련된 단백질 대형 언어 모델을 활용하여 아밀로이드 생성 영역을 예측하는 방법을 제안하였다.


<details>
  <summary>Details</summary>
Motivation: 아밀로이드 생성 예측은 바이오인포매틱스에서 중요한 분야이며, 최신 컴퓨팅 방법론을 적용해야 한다.

Method: 사전 훈련된 단백질 대형 언어 모델을 활용하고 양방향 LSTM 및 GRU를 사용하여 단백질 서열의 문맥적 특징을 평가했다.

Result: 10배 교차 검증에서 84.5%의 정확성, 테스트 데이터셋에서 83%의 정확성을 달성하였다.

Conclusion: 결과는 아밀로이드 예측의 정확성을 향상시키는 LLMs의 잠재력을 강조한다.

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [117] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: 연합 학습에서 데이터 이질성이 신경망의 폭이 증가함에 따라 감소하며, 무한의 폭에서 FedAvg가 중앙 집중 학습과 같은 일반화 성능을 달성한다는 것을 증명한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습은 데이터 공유 없이 여러 클라이언트가 협력하여 모델을 훈련할 수 있도록 하지만, 이질적인 데이터 분포로 인해 글로벌 모델의 훈련에 어려움이 있다.

Method: 과도하게 매개변수가 많은 FedAvg를 그래디언트 하강법으로 분석하며, 신경망 폭의 증가에 따른 데이터 이질성의 영향을 조사한다.

Result: 신경망의 폭이 무한대에 가까워질수록 데이터 이질성의 영향이 사라지며, FedAvg 모델이 중앙 집중 학습과 동일한 일반화 성능을 달성한다는 것을 입증했다.

Conclusion: 광범위한 실험을 통해 이론적 발견을 다양한 네트워크 구조, 손실 함수 및 최적화 방법에 대해 검증하였다.

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [118] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: 에너지 효율성과 정확성을 개선하기 위해, 경량 모델과 클라우드 기반 LLM을 결합한 하이브리드 언어 모델(HLM)을 제안하고, 이를 통해 통신 비용과 LLM 사용량을 줄인다.


<details>
  <summary>Details</summary>
Motivation: 자원이 제한된 환경에서의 온디바이스 LLM 추론에 대한 수요 증가를 해결하기 위해.

Method: 정보성 토큰만 업로드하는 토큰 수준 필터링 메커니즘을 제안하여 에너지 효율적이며 중요성과 불확실성을 인식하는 HLM 추론을 구현한다.

Result: TinyLlama-1.1B 및 LLaMA-2-7B 실험을 통해, BERT 점수 87.5%, 초당 0.37 토큰 처리량 및 에너지 소비 40.7% 절약을 달성하였다.

Conclusion: 이 접근법은 대역폭이 제한된 엣지 환경에서 LLM의 에너지 효율적이고 정확한 배치를 가능하게 한다.

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [119] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: 본 논문은 제한적이고 노이즈가 많은 측정값을 바탕으로 고차원 시공간 편미분 방정식(PDE)을 해결하여 교통 상태를 추정하는 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 제한된 데이터와 잡음 속에서 교통 흐름 역학을 모델링하기 위해 더 효과적인 방법이 필요하다.

Method: 물리적으로 정보가 있는 깊은 연산자 네트워크(PI-DeepONet) 프레임워크를 사용하여 TSE를 연산자 학습 문제로 재구성하고, 파라미터화된 신경 연산자를 훈련하여 희소 입력 데이터를 전체 시공간 교통 상태 필드로 매핑한다.

Result: NGSIM 데이터셋에서 최첨단 기준보다 우수한 성능을 보여주었다.

Conclusion: 입력 함수 생성 방법과 함수 수가 모델 성능에 미치는 영향을 탐구하여 제안된 프레임워크의 강건성과 효율성을 강조한다.

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [120] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: FLARE는 고차원 셀프 어텐션을 선형 복잡도로 줄여주는 모델로, 큰 비구조 메쉬에서의 적용 가능성과 확장성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 셀프 어텐션의 이차 복잡성이 큰 비구조 메쉬에서의 적용성 및 확장성을 제한한다.

Method: FLARE는 고정 길이의 잠재 시퀀스를 통해 어텐션을 라우팅하는 선형 복잡도의 셀프 어텐션 메커니즘이다.

Result: FLARE는 전례 없는 문제 크기로 확장 가능하며, 다양한 벤치마크에서 최신 신경 PDE 대체물보다 우수한 정확도를 제공한다.

Conclusion: 새로운 적층 제조 데이터셋도 공개하여 추가 연구를 촉진한다.

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [121] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: 이 논문은 기하학적 심층 학습을 위한 대칭을 포함하는 신경망 설계의 중요성과 불변 및 변환 작업의 개발을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 기하학적 심층 학습을 위해 대칭을 포함한 신경망을 설계하는 것이 중요합니다.

Method: 유효한 불변 및 변환 작업을 구성하는 체계적인 방법을 제시합니다. 이 방법은 서로 다른 계급의 카르테시안 텐서 및 다양한 유형의 구면 텐서 형태의 입력과 출력을 처리할 수 있습니다.

Result: 대칭 텐서 네트워크를 활용한 그래픽 표현이 불변 및 변환 함수와 관련된 증명과 구성을 단순화합니다.

Conclusion: 이 접근 방식을 기하학 그래프 신경망과 재료의 구성 법칙을 학습하기 위한 변환 기계 학습 모델 설계에 적용했습니다.

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [122] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: 전기차 매개변수 추정 및 전력 소비를 위한 하이브리드 대체 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전기차의 효율적인 성능 추정을 위해 다양한 매개변수를 정확하게 제공할 필요가 있습니다.

Method: Fourier Neural Operator 기반의 Spectral Parameter Operator 아키텍처와 경량 물리 모듈을 결합했습니다.

Result: Tesla 차량에서 평균 절대 오차 0.2kW, Kia EV9에서 약 0.8kW의 정확도를 달성했습니다.

Conclusion: 이 프레임워크는 해석 가능하며 새로운 조건에 잘 일반화되어 여러 실용적 응용에 적합합니다.

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [123] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: 본 논문은 사전 훈련된 대형 언어 모델의 성능 향상을 위한 Self-traced Step-wise Preference Optimization (SSPO) 방법을 제안하고, 이를 통해 비효율적인 사고 과정을 줄이고 모델 성능을 유지하는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 사전 훈련된 대형 언어 모델의 성능 향상을 위한 방법들이 많은 계산 오버헤드를 초래하고 있음에 따라, 이를 해결하고자 하는 동기에서 시작되었습니다.

Method: Self-traced Step-wise Preference Optimization (SSPO)라는 플러그형 RL 프로세스 감독 프레임워크를 제안하여 각 추론 단계를 세밀하게 최적화합니다.

Result: SSPO를 통해 생성된 추론 시퀀스는 정확하고 간결하여 과도한 사고 행동을 효과적으로 완화하며 다양한 도메인과 언어에서도 모델 성능을 저하시키지 않습니다.

Conclusion: SSPO는 Auxiliary 모델이나 수동 주석 없이 모델이 생성한 선호 신호를 활용하여 최적화 프로세스를 안내합니다.

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [124] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: 이 논문은 신뢰할 수 있는 설명을 위한 지표를 제시하며, 설명의 강인성과 설명 방법의 강인성을 강조합니다.


<details>
  <summary>Details</summary>
Motivation: 딥 러닝 알고리즘의 채택이 늘어나면서, 예측 결과에 대한 신뢰를 구축할 필요성이 대두되고 있습니다.

Method: 설명 가능한 인공지능(XAI) 분야의 다양한 방법론을 비교하고, 설명의 강인성과 설명 방법의 강인성을 정립합니다.

Result: 설명 방법의 강인성을 충족하는 XAI 방법만이 신뢰할 수 있는 설명을 제공할 수 있습니다.

Conclusion: 이 논문에서 제시한 프레임워크는 딥 러닝 알고리즘의 설명 및 신뢰 구축을 위한 기초를 마련합니다.

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [125] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3는 작고 모든 원자를 포함하는 분자를 생성하는 데 있어 최고의 성능을 달성하는 오픈 소스 다중 모드 흐름 일치 모델이다.


<details>
  <summary>Details</summary>
Motivation: 리얼한 분자를 원하는 특성과 함께 샘플링할 수 있는 생성 모델은 화학 발견을 가속화할 수 있다.

Method: FlowMol3는 self-conditioning, fake atoms, train-time geometry distortion의 세 가지 아키텍처 비의존적 기술을 활용하여 성능 향상을 이룬다.

Result: FlowMol3는 드러그 유사 분자에 대해 거의 100%의 분자 유효성을 달성하며, 학습 데이터의 기능적 그룹 구성과 기하학을 더 정확하게 재현하고, 유사한 방법보다 훨씬 적은 학습 가능한 매개변수로 이 작업을 수행한다.

Conclusion: 저희 결과는 확산 및 흐름 기반 분자 생성 모델의 안정성과 품질을 향상시키기 위한 간단하고 전이 가능한 전략을 강조한다.

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [126] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: 이 논문에서는 Score-informed Neural Operator (SciNO)를 제안하여 인과 발견 및 모델링의 성능을 향상시키는 방법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 인과 그래프의 위상적 순서를 식별하여 조합 검색 방법에 대한 확장 가능한 대안을 제공하기 위해.

Method: Score-informed Neural Operator (SciNO)는 매끄러운 함수 공간에서 Hessian 대각선의 안정적인 근사를 설계하는 확률적 생성 모델입니다.

Result: SciNO는 합성 그래프에서 42.7%, 실제 데이터셋에서 31.5%의 순서 발산 감소를 보여줍니다.

Conclusion: 제안한 방법은 추가적인 파인 튜닝이나 프롬프트 엔지니어링 없이도 LLM의 인과 추론 능력을 향상시킵니다.

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [127] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: 이 논문에서는 신뢰할 수 있는 서버와 사이드 데이터셋을 가지고 있는 연합 학습 시나리오에서 악의적 공격에 저항하는 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습이 도입되면서 개인 데이터를 공유하지 않고도 여러 클라이언트 간의 협업 모델 학습이 가능하지만, 악의적 클라이언트로 인한 공격에 대한 견딜 수 있는 메커니즘이 필요합니다.

Method: 본 논문에서는 신뢰할 수 있는 서버와 클라이언트가 있는 상황에서 2명의 정직한 참가자(서버와 클라이언트 1명)만으로 효과적으로 작동하는 알고리즘을 개발했습니다.

Result: 이론 분석을 통해 강력한 비잔틴 공격 하에서도 유한한 최적성 차이를 보이며, 실험 결과는 저희 알고리즘이 다양한 공격 전략 하에서 기존 보다 뛰어난 성능을 발휘함을 보여줍니다.

Conclusion: 본 결과는 연합 학습 시스템에서 비잔틴 공격에 대한 강력한 내성을 제공하는 알고리즘의 가능성을 시사합니다.

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [128] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero는 데이터 이질성을 해결하기 위해 분포 인식 임베딩을 기반으로 하는 하이퍼네트워크를 사용하여 비참여 클라이언트에 대한 전문화된 모델을 생성하는 새로운 방법입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 연합 학습 방법은 참여 클라이언트의 데이터 이질성을 다루는 데는 진전을 이루었지만, 비참여 클라이언트에 대한 일반화는 실패했습니다.

Method: 하이퍼네트워크를 이용하여 분포 인식 임베딩에 조건화된 전문화된 모델을 동적으로 생성합니다. 이 방법은 모델의 순방향 패스에 분포 인식 귀납적 편향을 포함시켜, 소음이 포함된 임베드 추출기를 사용하여 견고한 분포 임베딩을 추출하고 특징 손실을 방지합니다.

Result: HyperFedZero는 다양한 데이터셋과 모델에 대한 광범위한 실험에서 뛰어난 성능을 입증하였으며, 최소한의 계산, 저장 및 통신 오버헤드로 경쟁 방법들을 지속적으로 초과했습니다.

Conclusion: 각 구성 요소의 필요성을 입증하고 HyperFedZero의 효과성을 확인하는 추가 연구 및 시각화를 수행했습니다.

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [129] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: BuilDa는 건물 열역학의 데이터 기반 모델링을 위한 합성 데이터를 생성하는 프레임워크로, 전이 학습(TL) 연구에 필요한 질적, 양적 데이터를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 전이 학습(TL)은 건물 열역학 모델링을 개선할 수 있으며, 이에 따라 적합한 소스 모델 선택과 같은 새로운 연구 분야가 떠오르고 있다.

Method: BuilDa는 단일 존 Modelica 모델을 사용하여 데이터 생성을 수행하며, 이 모델은 Functional Mock-up Unit(FMU)으로 내보내지고 Python에서 시뮬레이션된다.

Result: BuilDa를 통해 필요한 데이터 양과 질을 갖춘 합성 데이터를 생성하고, 이 데이터를 사용하여 TL 모델의 사전 훈련 및 세부 조정을 수행했다.

Conclusion: BuilDa는 전문적인 건물 시뮬레이션 지식 없이도 대량의 데이터를 생성할 수 있도록 설계되었다.

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [130] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: 이 연구는 차량 네트워크에서 교통 표지판 탐지를 위한 분산 연합 학습 프레임워크를 제시하고 있으며, 이는 원시 데이터를 공유하지 않고 협력적인 모델 훈련을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 연결되고 자동화된 차량들이 생성하는 방대한 센서 데이터로 인해 중앙 집중형 기계 학습 접근법에서 개인 정보 보호 및 통신에 대한 도전 과제가 생긴다.

Method: 경량 객체 탐지기를 사용하여 차량 간에 교통 표지판 클래스를 분할하고, FedProx, FedAdam 및 FedAVG와 같은 알고리즘을 통해 모델 매개변수를 집계하는 시뮬레이션 환경에서 실험하였다.

Result: 서버 라운드를 2에서 20으로 증가시키면서 정확도가 0.1 이하에서 0.8 이상으로 향상됐고, 중간 수준의 지역 에폭(8-10)이 약 0.67의 정확도로 최적의 효율성을 제공하였다.

Conclusion: 이 연합 접근 방식은 실제 차량 배치를 위한 확장 가능하고 개인 정보를 보호하는 솔루션을 제공할 수 있으며, 지능형 교통 시스템을 발전시키기 위한 강력한 집계 및 통신 최적화의 미래 통합을 이끌 수 있다.

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [131] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: FedSODA는 리소스 제약이 있는 클라이언트에서 대형 언어 모델을 도메인에 맞게 조정할 수 있는 효율적인 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 연합 세부 조정은 데이터 프라이버시를 유지하면서 도메인에 특화된 적응을 가능하게 한다.

Method: SGP 모듈을 통해 중복 레이어를 제거하고 ODA 모듈을 통해 FFT 동안의 그래디언트 발산을 줄인다.

Result: FedSODA는 통신 오버헤드를 평균 70.6% 줄이고 저장 용량을 75.6% 감소시키며 작업 정확도를 3.1% 향상시킨다.

Conclusion: FedSODA는 리소스 제약 상황에서 실용적인 FFT 애플리케이션에 매우 적합하다.

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [132] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: 이 논문은 분산 데이터 공유 없이도 모델 훈련을 가능하게 하는 경량의 연합 학습 프레임워크 FedUNet을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 연합 학습 방법들이 클라이언트 간 동일한 모델 아키텍처를 가정하여 이질적인 환경에서의 적용성을 제한하고 있기 때문입니다.

Method: FedUNet은 각 클라이언트의 백본에 U-Net에서 영감을 받은 부가 모듈을 부착하여 작동하며, U-Net의 compact bottleneck만 공유하여 구조적 정렬 없이 효율적인 지식 전이를 가능하게 합니다.

Result: 실험 결과, VGG 변형을 통한 실험에서 FedUNet은 93.11%의 정확도와 0.89 MB의 낮은 통신 오버헤드를 가진 compact 형태에서 92.68%의 정확도를 달성했습니다.

Conclusion: 이 방식은 최소한의 통신 비용으로 백본과 부가 모듈 간의 협동 학습을 가능하게 합니다.

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [133] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: 이 논문은 신경망의 공간적 추론 능력을 체계적으로 평가하기 위한 포괄적인 벤치마크 프레임워크의 초기 결과를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 신경망의 공간적 추론 능력을 평가하기 위한 체계적인 방법을 마련하고, 특히 연결성과 거리 관계와 같은 형태학적 특성에 초점을 맞추기 위함이다.

Method: nnU-Net의 능력을 연구하기 위해 VoxLogicA를 사용하여 미로 연결성 문제와 공간 거리 계산 작업을 위한 두 가지 유형의 합성 데이터 세트를 생성하는 방법이 사용되었다.

Result: 초기 실험 결과는 신경망의 공간적 추론 능력에서 중대한 문제를 보여주며, 기본적인 기하학적 및 위상적 이해 작업에서 체계적인 실패를 드러낸다.

Conclusion: 이 프레임워크는 재현 가능한 실험 프로토콜을 제공하여 특정 제한 사항을 파악할 수 있게 하고, 이러한 제한 사항은 신경망과 기호적 추론 방법을 결합한 혼합 접근 방식을 통해 해결될 수 있다.

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [134] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: 이 논문은 클러스터 중심과 클러스터 내 가장 먼 점 사이의 최대 거리에 제약을 부여하는 방법인 제약 중심 클러스터링(CCC)을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 클러스터 중심 기반의 고전적인 클러스터링 방법을 확장하여 클러스터의 분산을 제어할 필요성이 있다.

Method: 라그랑주 공식을 사용하여 클러스터의 분산을 관리하면서 해석 가능성을 유지하는 폐쇄형 솔루션을 도출한다.

Result: 합성 원형 데이터에 대한 실험을 통해 CCC가 각도 구조를 보존하면서 방사형 확산을 줄임으로써 더 컴팩트한 클러스터를 형성한다는 것을 보여준다.

Conclusion: CCC는 센서 네트워크, 협업 로봇 공학, 해석 가능한 패턴 분석 등 분산 제어와 구조적 클러스터링이 필요한 응용 분야에 적합하다.

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [135] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: 이 논문은 Extreme Learning Machine (ELM)을 사용하여 단기 에너지 예측을 위한 새로운 방법론을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 에너지 예측의 정확성을 높이고 다양한 에너지 소스의 변동성을 효과적으로 처리하기 위해.

Method: 코르시카에서 수집된 6년치 시간별 데이터를 활용하여 다중 입력 다중 출력 (MIMO) 아키텍처를 통해 개별 에너지 출력과 총 에너지를 예측한다.

Result: ELM 모델은 특히 태양광 및 열 에너지 예측에서 17.9%와 5.1%의 nRMSE를 기록하며, 높은 정확도를 유지한다.

Conclusion: 제안된 방법론은 다양한 맥락과 데이터 세트에 적응 가능하며, 실시간 응용 프로그램에 적합한 저전산 요구 사항을 가진 해를 제공한다.

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [136] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: 서버리스 시스템에서 최적의 자원 할당과 운영 효율성을 유지하기 위해 예측 자동 확장 시스템의 필요성이 강조된다. 본 논문에서는 대규모 예측 자동 확장을 위한 온라인 작업 부하 예측 모델인 E3Former를 제안하며, 이는 여러 하위 네트워크의 예측 능력을 융합하여 단일 모델 접근 방식의 한계를 극복한다. 실험 결과, 예측 오류를 평균 10% 감소시키고, 현재 ByteDance의 IHPA 플랫폼에 배포되어 30개 이상의 애플리케이션의 안정적인 운영을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 컴퓨팅의 급속한 발전에 따라 서버리스 시스템에서 예측 자동 확장 시스템의 필요성이 대두되고 있다.

Method: 본 논문에서는 E3Former라는 온라인 앙상블 모델을 제안하여 대규모 예측 자동 확장을 위한 온라인 작업 부하 예측을 수행한다.

Result: 제안된 방법은 온라인 예측 작업에서 예측 오류를 평균 10% 감소시키며, 실제 시스템에서의 테스트를 통해 그 효과가 입증되었다.

Conclusion: 이 시스템은 현재 ByteDance의 IHPA 플랫폼에 배포되어 30개 이상의 애플리케이션을 지원하며, 60만 개 이상의 CPU 코어를 활용하면서 서비스 품질을 보장할 수 있다.

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [137] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: 새로운 비지도 이상치 탐지 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: Randomized PCA (RPCA) Forest의 K-최근접 이웃(KNN) 검색 성능에 영감을 받아, 비지도 방식으로 이상치를 탐지하는 새로운 방법을 개발했다.

Method: RPCA Forest를 활용한 비지도 이상치 탐지 방법을 제안한다.

Result: 실험 결과, 제안된 방법이 여러 데이터셋에서 고전적인 기법 및 최신 기법들과 비교하여 우수한 성능을 보였다.

Conclusion: 제안된 방법은 높은 일반화 능력과 계산 효율성을 반영하며, 비지도 이상치 탐지에 적합한 선택임을 강조한다.

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [138] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Wavy Transformer는 깊은 트랜스포머 모델의 과도한 평탄화 문제를 해결하기 위해 새로운 어텐션 레이어를 제안하며, 다양한 NLP 및 CV 모델에서 성능 개선 효과를 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 깊은 트랜스포머 모델이 겪는 과도한 평탄화 문제 해결을 위한 물리적 해석에 기반한 접근

Method: 두 번째 순서의 물결 다이나믹스를 기반으로 한 새로운 어텐션 레이어 및 상태-속도 관계를 유지하는 피드포워드 네트워크와 정규화 레이어를 도입

Result: Wavy Transformer는 최소한의 추가 파라미터로 성능을 일관되게 개선함

Conclusion: Wavy Transformer는 추가 하이퍼파라미터 조정 없이도 성능 향상을 이루어냄

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [139] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: 대규모 언어 모델(LLM)이 인간 판단과의 차이를 줄이기 위한 통계적 프레임워크인 Bridge를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 모델 출력을 평가할 때, 인간 판단과의 차이가 체계적으로 발생하는 문제를 해결하고자 한다.

Method: Bridge는 각 프롬프트-응답 쌍에 대한 잠재적인 인간 선호 점수를 제시하고, LLM의 편차를 불일치의 원인을 포착하는 공변량의 선형 변환으로 모델링한다.

Result: 여섯 개의 LLM 심사자와 두 개의 벤치마크를 사용하여 Bridge는 인간 평가와의 더 높은 일치도를 달성하고 체계적인 인간-LLM 간의 격차를 드러냈다.

Conclusion: 이 프레임워크는 LLM 등급을 정제하고 인간과 LLM 간의 체계적인 불일치를 설명하는 간단하고 원칙적인 방법을 제공한다.

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [140] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 인과 모델링이 강건한 AI 일반화로 이어질 수 있다는 약속은 최근 도메인 일반화(DG) 벤치마크 작업에서 도전받았다. 우리는 인과성과 DG 문헌의 주장을 재검토하며, 명백한 모순을 조화시키고 일반화에서 인과성의 역할에 대한 보다 미묘한 이론을 옹호한다.


<details>
  <summary>Details</summary>
Motivation: 인과 모델링이 AI의 일반화에 중요한 역할을 한다는 주장이 최근의 도메인 일반화 기준에서 제기된 문제에 직면하였다.

Method: 최근의 문헌을 재검토하고 인과성과 도메인 일반화 간의 관계를 완화하며 새로운 이론을 제안한다.

Result: 인과성과 DG 문헌 사이의 모순을 조화시키는 것을 목표로 한다.

Conclusion: 인과성이 일반화에 미치는 보다 미묘한 역할에 대한 이해를 촉진한다.

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [141] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: 이 논문에서는 동적 라우팅 네트워크를 통해 계산 효율성을 유지하면서 모델 용량을 확장할 수 있는 새로운 혼합 전문가 (MoE) 라우팅 패러다임인 MaxScore를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 MoE 네트워크는 전문가 용량 제약으로 인해 GPU 친화적인 계산을 보장하지만, 이는 토큰 손실을 초래하고 하드웨어 효율성을 저하시킵니다.

Method: Maximum Score Routing (MaxScore)라는 새로운 MoE 라우팅 패러다임을 제안하며, 이는 라우팅을 최소 비용 최대 유량 문제로 모델링하고 SoftTopk 연산자를 통합합니다.

Result: MaxScore는 반복적인 재라우팅 및 최적 운송 공식의 근본적인 한계를 해결하여 제약 조건이 있는 기준 및 없는 기준에 비해 동등한 FLOP에서 더 낮은 훈련 손실과 높은 평가 점수를 달성합니다.

Conclusion: 구현 세부사항 및 실험 구성은 MaxScore의 GitHub 저장소에서 확인할 수 있습니다.

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [142] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: 이 논문에서는 특정 행동을 강제하기 위한 멀티모달 LLM(MLLM)의 세밀한 스티어링 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLMs의 사후 유도는 특정 행동을 강제하는 실용적인 접근 방식으로 부상했지만, MLLMs에서는 충분히 탐구되지 않았다.

Method: 입력 특화 선형 이동을 사용하는 세밀한 스티어링을 조사하며, 이는 대조적인 입력 특화 프롬프트를 통해 계산된다.

Result: L2S(학습-스티어링) 접근법은 MLLMs에서 환각을 줄이고 안전성을 강화하며, 다른 정적 기준보다 우수한 성능을 보여준다.

Conclusion: 제안된 방법은 기존의 스티어링 기술에 비해 사용자가 원하는 행동에 맞게 더 적절히 적용될 수 있다.

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [143] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: 본 논문은 압축을 통한 데이터 양과 질 간의 trade-off를 중점적으로 다루는 스토리지 인식 학습에 관한 실증 연구를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 온디바이스 머신러닝은 연속 데이터 수집 시나리오에서 제한된 저장 공간에 의해 제약을 받는다.

Method: 데이터 샘플에 따라 압축의 민감성을 다르게 나타내는 바, 샘플별 적응형 압축 전략을 적용한 실증 연구를 수행했다.

Result: 균일한 데이터 삭제나 일괄 압축과 같은 단순 전략은 최적이 아니며, 데이터 샘플이 압축에 대한 다양한 민감성을 갖고 있음을 발견했다.

Conclusion: 이 통찰력은 새로운 스토리지 인식 학습 시스템 개발의 기초가 된다.

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [144] [Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 이 논문은 교육 중의 지속적인 정체기 및 단계적 진행을 연구하고, 변환기 모델의 손실 경관을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 훈련 중 지속적인 정체기와 단계적 진행의 경험적 관찰에 의해 동기가 부여되었습니다.

Method: 우리는 교차 엔트로피 손실 하에 n-그램 언어 모델의 학습을 중점적으로 살펴보았습니다.

Result: 우리는 특정 파라미터 구성이 정지 점이 될 수 있는 충분 조건을 수립하였고, 단순화된 변환기 모델에 대해 k-그램 추정기를 나타내는 파라미터 구성을 구축했습니다.

Conclusion: 이 결과는 손실 경관의 중요한 속성을 드러내며, 여러 학습 역학 및 급진적 전이와 같은 널리 관찰된 현상에 대한 이론적 통찰력을 제공합니다.

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [145] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: HRS는 고수준의 서비스 품질을 유지하면서 부하 예측의 정확성을 향상시키기 위해 수치적 및 이미지 기반 표현을 통합한 하이브리드 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 스트리밍 서비스의 급격한 확산으로 인해 네트워크 부하의 변동성이 심화되어 서비스 품질(QoS) 유지에 심각한 도전 과제가 발생하고 있다.

Method: HRS는 수치적 및 이미지 기반 표현을 통합하여 극단적인 부하 동력을 효과적으로 포착하는 하이브리드 표현 프레임워크를 제공한다. 또한, 비대칭 모델링을 도입하여 예측 오차의 영향을 고려한 스케줄링-aware 손실(SAL)을 소개한다.

Result: HRS는 실험에서 기존 10개 벤치마크보다 일관되게 우수한 성능을 보이며, SLA 위반 비율을 63.1% 줄이고 총 이익 손실을 32.3% 감소시키는 성과를 달성했다.

Conclusion: HRS는 부하 다이나믹스를 보다 잘 지원하고, 자원 지출을 최소화하며, SLA 위험을 관리하는 데 효과적이다.

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [146] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 본 논문에서는 현대 동적 그래프 모델링과 깊이 비정상 감지를 기반으로 한 새로운 침입 감지 방법인 TGN-SVDD를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전 세계적으로 디지털화가 진행됨에 따라 네트워크 보안의 중요성이 증가하고 있습니다.

Method: TGN-SVDD는 최신 동적 그래프 모델링과 깊이 비정상 감지를 기반으로 구축되었습니다.

Result: 현실적인 침입 감지 데이터에 대해 여러 기준선보다 우수성을 입증하였습니다.

Conclusion: 더욱 도전적인 변형을 제안합니다.

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [147] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ는 TinyML을 위한 단일 패스, 레이블 없는 불확실성 모니터로, 짧은 시간 지평 일관성을 위험 점수로 변환하여 높은 성능을 발휘합니다.


<details>
  <summary>Details</summary>
Motivation: TinyML에서의 정확하고 효율적인 불확실성 모니터링 필요성.

Method: 경량 신호를 사용하여 후행 특성과 일관성을 캡처하고, 이를 이용해 보정된 위험 점수를 생성하는 TCUQ를 제안합니다.

Result: TCUQ는 기존 방법들보다 메모리 사용량을 줄이고, 정확도 감지 성능을 향상시킵니다.

Conclusion: 시간적 일관성과 스트리밍 적합 보정이 결합되어 TinyML에서의 장치 모니터링을 위한 효율적인 기초를 제공합니다.

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [148] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: Sparse tensor algebra 가속기를 자동적으로 설계하기 위한 새로운 프레임워크 SparseMap을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습과 빅데이터의 수요 증가로 인해 희소 텐서 대수 가속기가 필요하다.

Method: SparseMap이라는 진화 전략 기반 최적화 프레임워크를 제안하여 매핑 및 희소 전략을 동시에 최적화합니다.

Result: SparseMap은 기존 방법들과 비교하여 일관되게 우수한 솔루션을 찾습니다.

Conclusion: SparseMap은 기존의 최적화 방법의 비효율성을 해결하며, 폭넓은 설계 공간을 탐색할 수 있도록 돕습니다.

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [149] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ는 TinyML을 위한 단일 통과 방식의 레이블 없는 불확실성 방법으로, 깊이별 다음 활성화 예측에서 위험을 추정한다.


<details>
  <summary>Details</summary>
Motivation: TinyML의 성능을 개선하고 메모리 사용량을 줄이기 위해 개발됨.

Method: 전 층을 압축하여 다음 층의 통계를 예측하는 Tiny int8 헤드와 경량의 단조 변환기를 사용한다.

Result: SNAP-UQ는 기존 방법에 비해 플래시 메모리와 대기 시간을 일관되게 감소시킨다.

Conclusion: TinyML 디바이스에서의 모니터링을 위한 실용적이고 자원 효율적인 기반을 제공한다.

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [150] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC는 차등 개인 정보 보호, 비잔틴 강인성 및 통신 효율성을 동시에 보장하는 새로운 연합 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 개인 정보를 보호하고 강인성을 유지하며 통신 비용을 줄이기 위한 새로운 접근 방식이 필요하다.

Method: RobAJoL로 알려진 이 프레임워크는 압축을 위한 Johnson-Lindenstrauss 변환과 강인한 집계를 위한 강인 평균화를 결합하여 구현된다.

Result: 실험 결과, RobAJoL은 기존 방법들보다 강인성과 효용성 면에서 비잔틴 공격 하에서도 우수한 성능을 보인다.

Conclusion: RobAJoL은 이론적으로 강인 평균화와의 호환성을 증명하고, 강인성 보장 및 차등 개인 정보 보호를 유지하며 통신 비용을 감소시킨다.

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [151] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: SL-ACC는 연합 학습에서 전송 병목 현상을 극복하여 모델 학습 시간을 단축하는 통신 효율적인 분할 학습 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 신경망의 복잡성이 증가함에 따라 리소스가 제한된 장치에서 분산 기계 학습을 배포하는 데 큰 장벽이 되고 있습니다. 저는 슬릿 학습이 이 문제를 해결할 수 있는 가능성이 있다고 믿었습니다.

Method: 우리는 적응적 채널 중요성 식별(ACII)과 채널 그룹 압축(CGC)의 두 가지 핵심 구성 요소로 구성된 SL-ACC라는 통신 효율적인 SL 프레임워크를 제안합니다. ACII는 먼저 모든 채널의 기여도를 식별하고 CGC는 엔트로피에 기반하여 채널을 그룹화하고 그룹 단위 적응 압축을 수행합니다.

Result: 대규모 실험에서 제안한 SL-ACC 프레임워크가 최신 벤치마크보다 목표 정확도를 달성하는 데 상당히 적은 시간을 소요하는 것을 검증했습니다.

Conclusion: SL-ACC는 리소스 제한 장치에서의 통신 병목을 해소하고, 신속한 모델 학습을 가능하게 합니다.

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [152] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: 그래프 합성곱 신경망(GCN)에서 Fiedler 값(대수적 연결성)이 GCN 성능을 예측하는 좋은 지표임을 발견하고, 이를 이론적 및 실험적으로 탐구한다.


<details>
  <summary>Details</summary>
Motivation: GCN 층을 쌓는 것이 노드 분류 및 엣지 예측 성능에 미치는 영향을 이해하고, GCN 성능 예측을 위한 새로운 지표를 찾기 위함이다.

Method: Fiedler 값을 집계하는 다양한 방법을 탐구하고, Cora, CiteSeer, Polblogs 데이터셋을 포함한 합성 및 실제 그래프 데이터에서 실험을 수행한다.

Result: Fiedler 값이 GCN 성능을 예측하는 데 유효함을 실험적으로 입증하였으며, 그래프의 연결 성분에 대한 집계 방법을 제시하였다.

Conclusion: 대수적 연결성이 유사한 그래프 간의 전이 학습 효과를 강조하며, Fiedler 값이 GCN 성능 예측에 유용하다는 이론적 논거를 제시한다.

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [153] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: Kourkoutas-Beta는 동적인 베타 값으로 변동하는 경량 Adam 스타일 최적화 기법으로, 물리 기반 문제에 대해 변동하는 경계 및 초기 조건의 데이터 샘플로 인해 발생하는 불안정한 손실과 스파이크 그래디언트 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 물리 기반 문제에 대한 변동하는 조건들로 인해 발생하는 손실의 변동과 그래디언트의 스파이크 현상을 해결하고자 한다.

Method: Kourkoutas-Beta는 전통적인 Adam의 고정된 두 번째 순간 디스카운트 beta2를 대체하여 레이어별 동적인 값과 제한된 'sunspike' 비율에 의해 움직이는 동적 최적화 기법이다.

Result: Kourkoutas-Beta는 고정된 beta2 Adam과 비교하여 안정성과 최종 손실을 개선하며, small-enwik8 데이터셋에서 bits-per-character를 Adam-0.95 대비 약 38%, Adam-0.999 대비 약 58% 줄였다.

Conclusion: 이 방법은 Adam의 수렴 보장을 유지하면서 스파이크 그래디언트에서의 견고성을 개선한다.

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [154] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: 이 논문은 다중 뷰 증거 학습에서의 편향된 증거 학습 문제를 다루며, 공정성 인식 다중 뷰 증거 학습(FAML)을 제안하여 예측 성능과 신뢰할 수 있는 불확실성 추정을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 실세계 데이터 분석을 통해 다중 뷰 증거 학습의 신뢰성 문제를 밝혀내고, 편향된 증거 할당으로 인한 불확실성 추정의 비효율성을 보여줍니다.

Method: 이 논문에서는 훈련 경로에 기반한 적응형 사전과 클래스별 증거 분산 기반의 공정성 제약을 도입하여 편향된 증거 침센 프로세스를 조절할 수 있는 Fairness-Aware Multi-view Evidential Learning (FAML)을 제안합니다.

Result: 다섯 개의 실제 다중 뷰 데이터셋에서 FAML은 보다 균형 잡힌 증거 할당을 달성하며, 최신 방법들과 비교하여 예측 성능과 불확실성 추정의 신뢰성을 모두 개선합니다.

Conclusion: FAML은 다중 뷰 증거 학습의 편향 문제를 해결하고 예측 성능을 향상시키는 효과적인 방법임을 입증했습니다.

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [155] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: MCFRCL은 지속적 학습을 위한 효율적인 기능 정규화 기반 프레임워크로, 몬테 카를로 샘플링을 사용하여 모델 예측 분포를 근사한다.


<details>
  <summary>Details</summary>
Motivation: 신경망 모델이 새로운 환경에 적응하는 데 지속적 학습이 중요하다.

Method: 본 연구에서는 몬테 카를로 샘플링을 통해 모델 예측 분포를 근사하는 새로운 기능 정규화 지속적 학습 프레임워크인 MCFRCL을 제안한다.

Result: MCFRCL은 다수의 기준 방법에 대해 평가되었으며, 예측 정확도와 훈련 효율성에서의 효과성을 강조하는 시뮬레이션 결과가 도출되었다.

Conclusion: 제안된 MCFRCL은 MNIST 및 CIFAR 데이터셋에서 우수한 성능을 보였다.

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [156] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: 본 연구는 충격 잡음이 있는 환경에서 능동 소음 제어 응용을 위한 견고한 적응 필터링 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 충격 잡음이 있는 환경에서 능동 소음 제어의 필요성을 해결하고자 한다.

Method: 필터링된 x 하이퍼볼릭 탄젠트 지수 일반화된 Kernel M-추정 함수(FXHEKM) 기반의 강인한 적응 알고리즘을 개발한다.

Result: 제안된 FXHEKM 알고리즘의 평균 제곱 오차(MSE)와 평균 소음 감소(ANR) 성능 지표를 통해 수치 결과를 평가한다.

Conclusion: 제안된 FXHEKM 알고리즘은 경쟁 알고리즘에 비해 추가적인 잡음 신호를 효과적으로 제거하는 효율성을 보여준다.

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [157] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: AI 시스템의 공정성을 보장하는 것은 대출, 채용 및 헬스케어와 같은 중요한 도메인에서 필수적이다. 하지만 공정성 테스트에 필요한 완전한 데이터를 확보하는 것은 여전히 큰 도전 과제이다. 본 연구는 접근할 수 없는 완전한 데이터의 경우에도 공정성을 추정할 수 있는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 공정성을 보장하는 것이 필수적이며, 이는 세계적으로 공정성 평가와 독립적인 편향 감사를 요구하는 규제의 출현으로 반영된다.

Method: 우리는 이용 가능한 별도의 데이터를 활용하여 모집단의 공정성을 추정하고, 가능한 결합 분포를 계산하여 신뢰할 수 있는 공정성 지표를 산출하는 방법을 제안한다.

Result: 시뮬레이션과 실제 실험을 통해 우리는 공정성 지표에 대한 의미 있는 경계를 도출하고, 실제 지표의 신뢰할 수 있는 추정을 얻을 수 있음을 입증하였다.

Conclusion: 이 접근 방식은 완전한 데이터에 대한 접근이 제한된 실제 상황에서 공정성 테스트를 위한 실용적이고 효과적인 솔루션이 될 수 있다.

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [158] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: 이 연구는 FMAE와 HEF 평가 함수를 비교하여 수요 예측의 효율성을 높이는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 경쟁 환경에서 전략적 계획을 위해 수요 예측이 필수적이며, 이는 자원 최적화와 시장 역학에 대한 반응성을 향상시킵니다.

Method: FMAE와 HEF라는 두 개의 맞춤형 평가 함수를 비교하고, 서로 다른 데이터 분할과 최적화 기법을 사용하여 모델의 적합도와 정확성을 평가했습니다.

Result: HEF는 FMAE에 비해 일관되게 더 높은 성능을 보였고, 모델의 견고성과 설명력을 향상시켰습니다.

Conclusion: HEF는 전략적 계획에 적합하고, FMAE는 운영 효율성에 더 적합하다는 방법론적 트레이드오프를 강조합니다. 동적 환경에서 예측 모델을 최적화하기 위한 재현 가능한 프레임워크를 제안합니다.

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [159] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: 신경 대리 모델이 전통적인 시뮬레이션의 대안으로 주목받고 있으며, 고차원 매개변수 공간에서 주어진 출력 기능을 생성하는 매개변수 분포를 모델링하고 시각화하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 대리 기반 솔루션은 작은 매개변수 집합을 찾는 데 집중하여 플로서 매개변수의 더 넓은 그림을 간과하고 있습니다.

Method: 밀도 추정을 통해 오류를 모델링하고, 주어진 매개변수 구성이 훈련 매개변수에 가까운 경우에만 높은 밀도를 보고한다. 이를 기반으로 매개변수에 대한 사전 신념을 형성하고, 기능에 대한 우도와 결합하여 타겟 출력 기능을 생성하는 적합한 매개변수 구성을 샘플링하는 효율적인 방법을 제공한다.

Result: 세 가지 시뮬레이션 데이터셋의 입력 매개변수 공간에 대한 기능 주도 매개변수 분석을 수행하여 시각화 인터페이스를 통해 해결책의 유용성을 입증한다.

Conclusion: 소스 코드는 https://github.com/matthewberger/seeing-the-many 에서 확인할 수 있다.

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [160] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: 이 논문은 해양 환경에서 해저 음향 센서 네트워크와 로그 가우시안 콕스 프로세스를 사용하여 공간적 커미션 외리를 분류하고 탐지하는 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 해양 환경에서 외리를 효과적으로 탐지하고 분류하기 위해.

Method: 정상 강도 함수의 평균과 분산을 포함하는 두 번째 차수의 근사를 제안하고 현실 시간에 최적의 센서 배치 전략을 통합하여 외리 강도가 변하는 것에 따라 센서의 위치를 동적으로 조정한다.

Result: 노퍽, 버지니아의 실제 선박 교통 데이터를 사용한 검증 결과, 우리의 방법이 분류 성능과 외리 탐지를 개선하는데 효과적임을 보여준다.

Conclusion: 제안된 프레임워크는 해양 환경에서 외리 탐지 및 분류를 향상시키는데 중요한 기여를 한다.

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [161] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: 완벽한 진실성을 가진 보정 척도인 평균 이진 보정 오류(ATB)를 설계하였으며, 이는 기존의 보정 척도와 연결되어 있고, 계산 효율성이 뛰어나 더 빠르고 간단한 보정 테스트 알고리즘을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 보정 척도가 샘플에서 더 보정된 것처럼 보이기 위해 예측기에게 거짓말을 유도하는 문제를 해결하고, 진실하고 완전한 보정 척도의 필요성을 충족시키기 위함이다.

Method: 이 논문에서는 평균 이진 보정 오류(ATB)라는 완벽하게 진실한 보정 척도를 설계하였으며, 이를 통해 계산의 효율성과 간편함을 제공한다.

Result: ATB는 기존 보정 척도와의 정량적 관계를 유지하며, 빠른 추정 알고리즘을 통해 보정 테스트의 실행 시간을 개선하였다.

Conclusion: ATB는 진실성과 효율성을 갖춘 보정 척도로, 다른 진실한 보정 척도를 구축하는 기반을 제공한다.

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [162] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 본 논문에서는 다차원 시간 시계열 데이터의 모델링에서 채널 의존(CD) 모델과 채널 독립(CI) 모델 간의 균형을 이루기 위해 인과 그래프를 통합한 새로운 아키텍처인 Causally-Guided Pairwise Transformer(CGPT)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 산업 시스템에서 다차원 시간 시계열 데이터를 모델링할 때 채널 의존성과 독립성 간의 균형이 중요하다.

Method: CGPT는 인과 그래프를 inductive bias로 통합하고, 다차원 데이터를 쌍으로 분해하여 모델링하는 방식을 사용한다.

Result: CGPT는 예측 정확도 측면에서 채널 독립(CI) 및 채널 의존(CD) 기준을 능가하며, 다양한 문제 차원에서도 좋은 성능을 보인다.

Conclusion: CGPT는 복잡한 시스템 동역학을 분리하며 확장성과 변량 적응성을 보장하는 유연한 아키텍처이다.

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [163] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: 이 논문에서는 지각 및 시간적 구조를 포착하는 표현을 기반으로 한 추론 방법인 CRTR을 제안하고, 이를 통해 복잡한 시간적 구조를 가진 도메인에서 강력한 성과를 달성했다.


<details>
  <summary>Details</summary>
Motivation: 전통적 AI에서 지각은 상태 기반 표현 학습에 의존하며, 계획은 일반적으로 검색을 통해 이루어집니다. 우리는 이러한 추론이 지각적 및 시간적 구조를 모두 포착하는 표현에서 발생할 수 있는지를 연구합니다.

Method: 우리는 부정 샘플링 방식을 사용한 CRTR(Combinatorial Representations for Temporal Reasoning)이라는 방법을 도입하여, 이 spurious features를 제거하고 시간적 추론을 촉진합니다.

Result: CRTR은 Sokoban 및 Rubik's Cube와 같은 복잡한 시간적 구조를 가진 도메인에서 강력한 결과를 달성했습니다. 특히, Rubik's Cube의 경우 CRTR은 모든 초기 상태에 대해 일반화되는 표현을 학습하며, BestFS보다 적은 검색 단계로 퍼즐을 해결할 수 있습니다.

Conclusion: 우리가 아는 한, 이것은 외부 검색 알고리즘에 의존하지 않고 학습된 표현만으로 임의의 Cube 상태를 효율적으로 해결하는 첫 번째 방법입니다.

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [164] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: 개인 수준의 인간 이동 예측을 위한 최상의 기계 학습 모델 훈련 방법을 탐구합니다.


<details>
  <summary>Details</summary>
Motivation: 감염병 모니터링, 아동 및 노인 돌봄 분야에서 개인 이동 예측이 중요해졌습니다.

Method: 다양한 모델, 파라미터 구성 및 훈련 전략에 대한 포괄적 실험 분석을 수행합니다.

Result: 주중 및 개별 사용자 정보 포함 시 예측 효과가 향상됨을 보입니다.

Conclusion: 데이터 불균형을 완화하기 위해 계층 샘플링을 적용하여 대표성을 유지합니다.

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [165] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: 본 연구는 Masked Diffusion Policy Optimization (MDPO)를 제안하여 훈련과 추론 사이의 불일치를 해결하고, 이를 통해 기존의 최상 성능 기법에 비해 효율성을 높이면서 성능 향상을 달성한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 오토회귀 모델과 비교할 때, 확산 언어 모델은 더 빠른 생성과 양방향 맥락에 대한 풍부한 조건을 가능하게 하지만, 훈련과 추론 사이의 불일치로 인해 성능의 최적화가 이뤄지지 않는다.

Method: 우리는 효과적인 노이즈 제거 경로를 학습하는 문제를 순차적 의사 결정 문제로 프레임하고, 강화 학습을 적용하여 기존의 점진적인 미세 조정 일정을 따르도록 모델을 훈련시킨다.

Result: MDPO는 60배 더 적은 그래디언트 업데이트로 이전의 최상 성능 기법과 동일한 성능을 달성했으며, 동일한 가중치 업데이트 수로 훈련할 때 MATH500에서 평균 9.6%, Countdown에서 54.2% 성능 향상을 보였다.

Conclusion: 우리는 MDLM의 사전 훈련과 추론 간의 불일치에 대한 조사를 위한 큰 잠재력을 확립하며, MDPO와 RCR을 결합함으로써 성능 향상을 이룬다.

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [166] [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
*Hael Abdulhakim Ali Humran,Ferdi Sonmez*

Main category: cs.CR

TL;DR: 코드의 보안 취약점을 탐지하는 데 있어 인공지능 기반의 모델이 기존의 정적 분석 도구보다 우수한 성능을 보여줌을 입증하는 연구이다.


<details>
  <summary>Details</summary>
Motivation: 다양한 프로그래밍 언어로 작성된 코드에서 보안 취약점을 탐지하는 것은 매우 중요하지만 복잡한 문제이며, 기존의 정적 분석 도구는 이러한 컨텍스트에 의존적인 버그를 효과적으로 탐지하지 못한다.

Method: CodeBERT와 CodeLlama와 같은 변환기 기반 모델을 다양한 코드 취약성 데이터셋에 적용하고, 취약한 코드 조각과 안전한 코드 조각을 통해 동적 미세 조정을 통해 예측 능력을 향상시키는 방법을 사용한다.

Result: 잘 훈련된 CodeBERT 모델은 정확도 97% 이상의 기존 정적 분석기와 동등하거나 더 나은 성능을 보였다.

Conclusion: AI 기반 솔루션은 다양한 프로그래밍 언어와 취약성 클래스에 일반화될 수 있으며, 신뢰성과 해석 가능성, 배포 준비성이 지속적으로 개발되고 있다.

Abstract: Security vulnerabilities present in a code that has been written in diverse
programming languages are among the most critical yet complicated aspects of
source code to detect. Static analysis tools based on rule-based patterns
usually do not work well at detecting the context-dependent bugs and lead to
high false positive rates. Recent developments in artificial intelligence,
specifically the use of transformer-based models like CodeBERT and CodeLlama,
provide light to this problem, as they show potential in finding such flaws
better. This paper presents the implementations of these models on various
datasets of code vulnerability, showing how off-the-shelf models can
successfully produce predictive capacity in models through dynamic fine-tuning
of the models on vulnerable and safe code fragments. The methodology comprises
the gathering of the dataset, normalization of the language, fine-tuning of the
model, and incorporation of ensemble learning and explainable AI. Experiments
show that a well-trained CodeBERT can be as good as or even better than some
existing static analyzers in terms of accuracy greater than 97%. Further study
has indicated that although language models can achieve close-to-perfect
recall, the precision can decrease. A solution to this is given by hybrid
models and validation procedures, which will reduce false positives. According
to the results, the AI-based solutions generalize to different programming
languages and classes of vulnerability. Nevertheless, robustness,
interpretability, and deployment readiness are still being developed. The
results illustrate the probabilities that AI will enhance the trustworthiness
in the usability and scalability of machine-learning-based detectors of
vulnerabilities.

</details>


### [167] [Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks](https://arxiv.org/abs/2508.11711)
*Irash Perera,Hiranya Abeyrathne,Sanjeewa Malalgoda,Arshardh Ifthikar*

Main category: cs.CR

TL;DR: 본 논문은 GraphQL 쿼리의 악성 행위를 실시간으로 탐지하는 AI 기반 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: GraphQL의 유연성은 효율적인 데이터 검색에 유리하지만 전통적인 API 보안 메커니즘이 제대로 대응하지 못하는 보안 취약점을 초래한다.

Method: 본 연구는 정적 분석과 기계 학습 기법을 결합하여 악성 GraphQL 쿼리를 탐지하는 방법을 제시한다. 이는 동적 스키마 기반 구성에 대한 대형 언어 모델(LLMs), 쿼리 페이로드의 맥락적 임베딩을 위한 문장 변환기(SBERT 및 Doc2Vec), 분류를 위한 합성곱 신경망(CNN), 랜덤 포레스트 및 다층 퍼셉트론을 포함한다.

Result: 우리의 탐지 모델과 전체 시스템은 부하 하에서도 높은 성능을 보였으며, SQL 인젝션, OS 명령어 인젝션, XSS 공격 등을 감지하는 높은 정확도를 보여주었다.

Conclusion: 본 연구는 GraphQL API 보안을 강화하기 위한 강력하고 적응 가능한 솔루션을 기여한다.

Abstract: GraphQL's flexibility, while beneficial for efficient data fetching,
introduces unique security vulnerabilities that traditional API security
mechanisms often fail to address. Malicious GraphQL queries can exploit the
language's dynamic nature, leading to denial-of-service attacks, data
exfiltration through injection, and other exploits. Existing solutions, such as
static analysis, rate limiting, and general-purpose Web Application Firewalls,
offer limited protection against sophisticated, context-aware attacks. This
paper presents a novel, AI-driven approach for real-time detection of malicious
GraphQL queries. Our method combines static analysis with machine learning
techniques, including Large Language Models (LLMs) for dynamic schema-based
configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual
embedding of query payloads, and Convolutional Neural Networks (CNNs), Random
Forests, and Multilayer Perceptrons for classification. We detail the system
architecture, implementation strategies optimized for production environments
(including ONNX Runtime optimization and parallel processing), and evaluate the
performance of our detection models and the overall system under load. Results
demonstrate high accuracy in detecting various threats, including SQL
injection, OS command injection, and XSS exploits, alongside effective
mitigation of DoS and SSRF attempts. This research contributes a robust and
adaptable solution for enhancing GraphQL API security.

</details>


### [168] [Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)](https://arxiv.org/abs/2508.11716)
*Javier Muñoz-Haro,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CR

TL;DR: 원격 사용자 검증의 중요성이 커지고 있으며, ID 문서의 진위 확인을 통해 디지털 서비스에 대한 접근을 허용하는 방법이 제안된다. 그러나 AI의 발전으로 인해 신뢰할 수 있는 ID가 아닌 고급의 가짜 ID가 등장하고 있어, 이들을 탐지하기 위한 연구가 필요하다. 본 연구에서는 프라이버시를 유지하면서 가짜 ID 탐지 방법론과 새로운 데이터베이스를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 원격 사용자 검증의 중요성이 커지면서, AI 기반의 고급 가짜 ID 탐지 방법이 필요하다.

Method: 프라이버시를 보존하는 패치 기반 방법론을 제안하고, 900K 이상의 실물/가짜 ID 패치를 포함하는 새로운 공개 데이터베이스를 제공하며, 새로운 프라이버시 인식 가짜 ID 탐지 방법을 발표한다.

Result: FakeIDet2-db 데이터베이스는 2,000개의 ID 이미지에서 추출된 패치를 포함하고 있으며, 물리적 공격 3가지 유형(인쇄, 화면, 합성)을 고려한다.

Conclusion: 표준 재현 가능 벤치마크를 발표하여 문헌의 인기 데이터베이스에서의 물리적 및 합성 공격을 고려한 연구를 지원한다.

Abstract: Remote user verification in Internet-based applications is becoming
increasingly important nowadays. A popular scenario for it consists of
submitting a picture of the user's Identity Document (ID) to a service
platform, authenticating its veracity, and then granting access to the
requested digital service. An ID is well-suited to verify the identity of an
individual, since it is government issued, unique, and nontransferable.
However, with recent advances in Artificial Intelligence (AI), attackers can
surpass security measures in IDs and create very realistic physical and
synthetic fake IDs. Researchers are now trying to develop methods to detect an
ever-growing number of these AI-based fakes that are almost indistinguishable
from authentic (bona fide) IDs. In this counterattack effort, researchers are
faced with an important challenge: the difficulty in using real data to train
fake ID detectors. This real data scarcity for research and development is
originated by the sensitive nature of these documents, which are usually kept
private by the ID owners (the users) and the ID Holders (e.g., government,
police, bank, etc.). The main contributions of our study are: 1) We propose and
discuss a patch-based methodology to preserve privacy in fake ID detection
research. 2) We provide a new public database, FakeIDet2-db, comprising over
900K real/fake ID patches extracted from 2,000 ID images, acquired using
different smartphone sensors, illumination and height conditions, etc. In
addition, three physical attacks are considered: print, screen, and composite.
3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We
release a standard reproducible benchmark that considers physical and synthetic
attacks from popular databases in the literature.

</details>


### [169] [Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach](https://arxiv.org/abs/2508.11742)
*Minhao Jin,Hongyu He,Maria Apostolaki*

Main category: cs.CR

TL;DR: 이 논문은 현재의 합成 트래픽 생성기(SynNetGens)가 개인정보 보호를 약속하지만 종합적인 보증이나 경험적 검증이 부족하다는 점을 강조하며, 이를 위한 첫 번째 공격 기반 벤치마크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 합성 트래픽 생성기들이 개인정보 보호를 주장하지만 실제로는 신뢰성에 대한 보증이 부족하고, 이를 검증할 방법이 필요하다.

Method: TraceBleed라는 공격을 소개하며, 이 공격은 행동적 지문을 활용하여 유저 수준의 정보를 유출하는 방식으로 작동한다.

Result: 우리의 연구 결과는 SynNetGens가 사용자 정보의 유출, 차별적 개인정보 보호의 실패, 합성 데이터 공유로 인한 유출 증대를 발견하였다.

Conclusion: TracePatch라는 새로운 방어 기법을 소개하며, 이는 합성 트래픽 생성기와 무관하게 개인정보 유출을 완화하고 신뢰성을 유지하는 방식으로 작동한다.

Abstract: Current synthetic traffic generators (SynNetGens) promise privacy but lack
comprehensive guarantees or empirical validation, even as their fidelity
steadily improves. We introduce the first attack-grounded benchmark for
assessing the privacy of SynNetGens directly from the traffic they produce. We
frame privacy as membership inference at the traffic-source level--a realistic
and actionable threat for data holders. To this end, we present TraceBleed, the
first attack that exploits behavioral fingerprints across flows using
contrastive learning and temporal chunking, outperforming prior membership
inference baselines by 172%. Our large-scale study across GAN-, diffusion-, and
GPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level
information; (ii) differential privacy either fails to stop these attacks or
severely degrades fidelity; and (iii) sharing more synthetic data amplifies
leakage by 59% on average. Finally, we introduce TracePatch, the first
SynNetGen-agnostic defense that combines adversarial ML with SMT constraints to
mitigate leakage while preserving fidelity.

</details>


### [170] [AegisBlock: A Privacy-Preserving Medical Research Framework using Blockchain](https://arxiv.org/abs/2508.11797)
*Calkin Garg,Omar Rios Cruz,Tessa Andersen,Gaby G. Dagher,Donald Winiecki,Min Long*

Main category: cs.CR

TL;DR: AegisBlock는 환자의 프라이버시를 유지하면서 연구자들에게 의료 기록을 안전하게 공유할 수 있도록 하는 환자 중심의 접근 제어 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: HIPAA 및 기타 개인 정보 보호 규정으로 인해 환자의 건강 기록에 대한 연구를 수행하면서 환자의 프라이버시를 유지하는 것이 필요하다.

Method: AegisBlock은 환자가 채굴자에 의해 검증된 자신의 의료 데이터에 대한 접근을 제공할 수 있게 하며, 연구자는 특정 환자의 기록 접근을 요청하기 위해 시간 기반 범위 쿼리를 제출한다.

Result: 실험 평가 결과 AegisBlock은 시스템에 있는 환자와 병원의 수에 대해 확장 가능하며, 최대 50%의 악의적인 채굴자들에 대해서도 효율적임을 보여준다.

Conclusion: AegisBlock은 연구자에게 제공되는 데이터의 신뢰성을 보장하면서 환자의 익명성을 유지하는 방법을 제시한다.

Abstract: Due to HIPAA and other privacy regulations, it is imperative to maintain
patient privacy while conducting research on patient health records. In this
paper, we propose AegisBlock, a patient-centric access controlled framework to
share medical records with researchers such that the anonymity of the patient
is maintained while ensuring the trustworthiness of the data provided to
researchers. AegisBlock allows for patients to provide access to their medical
data, verified by miners. A researcher submits a time-based range query to
request access to records from a certain patient, and upon patient approval,
access will be granted. Our experimental evaluation results show that
AegisBlock is scalable with respect to the number of patients and hospitals in
the system, and efficient with up to 50% of malicious miners.

</details>


### [171] [Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering](https://arxiv.org/abs/2508.11812)
*Tyler Schroder,Sohee Kim Park*

Main category: cs.CR

TL;DR: 이 논문은 Active Directory 환경에서의 보안 문제를 다루고 있으며, 자격 증명 도난 방지와 위협 행위자의 측면 이동 제한을 위한 효과적인 전략을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 공유 컴퓨팅 환경과 중앙집중식 ID 시스템의 필요성이 증가하고 있으며, 이는 사이버 위협의 주요 타겟이 되고 있기 때문이다.

Method: 자격 증명 도난 방지와 측면 이동 제한을 위한 효과적인 전략을 제시하고, 실제 사례를 통해 이러한 전략의 중요성을 설명한다.

Result: 제안된 방법은 자격 증명의 이동을 제한하여 특권 상승과 도난을 방지하고, 사이버 공격의 측면 이동을 차단하여 랜섬웨어의 피해를 줄일 수 있다.

Conclusion: 티어링은 현대 사이버 보안 전략의 필수 요소로 자리 매김하며, 보안 전문가와 기업이 협력해 장비를 자동으로 분류하는 소프트웨어와 프레임워크를 개발하는 것이 중요하다.

Abstract: The advancement of computing equipment and the advances in services over the
Internet has allowed corporations, higher education, and many other
organizations to pursue the shared computing network environment. A requirement
for shared computing environments is a centralized identity system to
authenticate and authorize user access. An organization's digital identity
plane is a prime target for cyber threat actors. When compromised, identities
can be exploited to steal credentials, create unauthorized accounts, and
manipulate permissions-enabling attackers to gain control of the network and
undermine its confidentiality, availability, and integrity. Cybercrime losses
reached a record of 16.6 B in the United States in 2024. For organizations
using Microsoft software, Active Directory is the on-premises identity system
of choice. In this article, we examine the challenge of security compromises in
Active Directory (AD) environments and present effective strategies to prevent
credential theft and limit lateral movement by threat actors. Our proposed
approaches aim to confine the movement of compromised credentials, preventing
significant privilege escalation and theft. We argue that through our
illustration of real-world scenarios, tiering can halt lateral movement and
advanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap
in existing literature by combining technical guidelines with theoretical
arguments in support of tiering, positioning it as a vital component of modern
cybersecurity strategy even though it cannot function in isolation. As the
hardware advances and the cloud sourced services along with AI is advancing
with unprecedented speed, we think it is important for security experts and the
business to work together and start designing and developing software and
frameworks to classify devices automatically and accurately within the tiered
structure.

</details>


### [172] [Machine Learning-Based AES Key Recovery via Side-Channel Analysis on the ASCAD Dataset](https://arxiv.org/abs/2508.11817)
*Mukesh Poudel,Nick Rahimi*

Main category: cs.CR

TL;DR: AES 및 RSA와 같은 암호화 알고리즘은 이론적으로 강력하지만 물리적 장치에서의 구현은 사이드 채널을 통해 정보 유출을 초래할 수 있다. 본 논문은 머신 러닝 기술을 이용해 이러한 유출을 통해 부분 키 회복을 연구하였다.


<details>
  <summary>Details</summary>
Motivation: 암호화 알고리즘이 사이드 채널 공격에 의해 안전성을 위협받을 수 있다는 사실을 해결하고자 함.

Method: 머신 러닝 기법과 딥 러닝 모델을 활용해 AES-128 구현의 EM 트레이스를 분석하고 키 회복을 위한 분류 문제로 설정하였다.

Result: SVM 및 RF는 전체 피처에서 키 랭킹 성능이 저조했지만, 피처 선택을 통해 감소된(상위 100개) RF는 거의 절반의 공격 트레이스로 키 바이트 회복에 성공하였다. CNN도 고정 키 데이터셋에서 효율적으로 성능을 발휘하였다.

Conclusion: 모델들이 사이드 채널 키 회복에 효과적인 도구임을 확인하였으며 암호화 구현의 실질적인 취약성을 입증하였다.

Abstract: Cryptographic algorithms like AES and RSA are widely used and they are
mathematically robust and almost unbreakable but its implementation on physical
devices often leak information through side channels, such as electromagnetic
(EM) emissions, potentially compromising said theoretically secure algorithms.
This paper investigates the application of machine learning (ML) techniques and
Deep Learning models to exploit such leakage for partial key recovery. We use
the public ASCAD `fixed' and `variable' key dataset, containing 700 and 1400 EM
traces respectively from an AES-128 implementation on an 8-bit microcontroller.
The problem is framed as a 256-class classification task where we target the
output of the first-round S-box operation, which is dependent on a single key
byte. We evaluate standard classifiers (Random Forest (RF), Support Vector
Machine (SVM)), a Convolutional Neural Network(CNN) and a Residual Neural
Network(ResNet). We also explore the utility of RF-based feature importance for
dimensionality reduction. Crucially, we employ this domain-specific Key Rank
metric for evaluation, showing its necessity over standard classification
accuracy. Our results show that SVM and RF on full features perform poorly in
key ranking. However, RF trained on reduced (top 100) identified via importance
analysis achieves Rank 0 (successful key byte recovery) using almost half the
attack traces. The implemented CNN also achieves Rank 0 efficiently using
approximately 65 attack traces for the fixed-key dataset. The ResNets perform
best on large and complex datasets but may not always be the best choice for
simple fixed key dataset in terms of efficiency. Thus we conclude that models,
particularly CNNs, ResNets and feature-selected RF, coupled with the Key Rank
metric, are an effective tool for side-channel key recovery, confirming the
practical vulnerability of the cryptographic implementations.

</details>


### [173] [Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning](https://arxiv.org/abs/2508.11907)
*Xiaojin Zhang,Mingcong Xu,Yiming Li,Wei Chen,Qiang Yang*

Main category: cs.CR

TL;DR: 본 논문은 데이터 프라이버시를 유지하면서 협업 모델 훈련을 가능하게 하는 연합 학습(FL)의 공격과 보호 복잡성을 분석하는 새로운 이론적 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습의 데이터 프라이버시 유지와 공격에 대한 민감성 문제를 해결하기 위해 강력한 프라이버시 보호 메커니즘이 필요합니다.

Method: 공격 복잡성을 정의하고, 최대 베이esian 프라이버시(MBP)를 활용하여 보호 복잡성의 이론적 경계를 도출했습니다.

Result: 보호 복잡성이 모델의 차원과 프라이버시 예산에 따라 스케일링되는 것을 보여주었으며, 공격 복잡성의 종합적인 경계를 설립하여 프라이버시 유출 및 모델 차원에 대한 의존성을 밝혔습니다.

Conclusion: 이 프레임워크는 연합 학습 시스템을 설계를 위한 중요한 통찰력을 제공합니다.

Abstract: Federated learning (FL) offers a promising paradigm for collaborative model
training while preserving data privacy. However, its susceptibility to gradient
inversion attacks poses a significant challenge, necessitating robust privacy
protection mechanisms. This paper introduces a novel theoretical framework to
decipher the intricate interplay between attack and protection complexities in
privacy-preserving FL. We formally define "Attack Complexity" as the minimum
computational and data resources an adversary requires to reconstruct private
data below a given error threshold, and "Protection Complexity" as the expected
distortion introduced by privacy mechanisms. Leveraging Maximum Bayesian
Privacy (MBP), we derive tight theoretical bounds for protection complexity,
demonstrating its scaling with model dimensionality and privacy budget.
Furthermore, we establish comprehensive bounds for attack complexity, revealing
its dependence on privacy leakage, gradient distortion, model dimension, and
the chosen privacy level. Our findings quantitatively illuminate the
fundamental trade-offs between privacy guarantees, system utility, and the
effort required for both attacking and defending. This framework provides
critical insights for designing more secure and efficient federated learning
systems.

</details>


### [174] [WebGeoInfer: A Structure-Free and Multi-Stage Framework for Geolocation Inference of Devices Exposing Information](https://arxiv.org/abs/2508.11913)
*Huipeng Yang,Li Yang,Lichuan Ma,Lu Zhou,Junbo Jia,Anyuan Sang,Xinyue Wang*

Main category: cs.CR

TL;DR: WebGeoInfer는 구조가 없는 지리적 위치 추론 프레임워크로, 원격 관리 장치의 웹 페이지에서 지리적 정보를 추출하여 보안 위험을 줄인다.


<details>
  <summary>Details</summary>
Motivation: 원격 관리 장치는 중요 인프라 모니터링을 지원하지만, 자산 노출을 증가시킨다. 관리자 부주의로 인해 위치 정보를 노출하는 장치를 식별하는 것이 사이버 보안 규제를 위해 중요하다.

Method: WebGeoInfer는 유사한 장치 웹 페이지를 클러스터링하고 클러스터 간 차이를 분석하여 잠재적 지리 정보를 추출한다.

Result: WebGeoInfer는 94개국과 2,056개 도시에 걸쳐 5,435개의 장치에 대한 위치를 추론하여 국가, 도시 및 거리 수준에서 각각 96.96%, 88.05%, 79.70%의 정확도를 달성했다.

Conclusion: 이 프레임워크는 구조적 한계를 넘어서 지리적 좌표를 효과적으로 추출할 수 있음을 보여준다.

Abstract: Remote management devices facilitate critical infrastructure monitoring for
administrators but simultaneously increase asset exposure. Sensitive
geographical information overlooked in exposed device management pages poses
substantial security risks. Therefore, identifying devices that reveal location
information due to administrator negligence is crucial for cybersecurity
regulation. Despite the rich information exposed by web interfaces of remote
management devices, automatically discovering geographical locations remains
challenging due to unstructured formats, varying styles, and incomplete
geographical details.
  This study introduces WebGeoInfer, a structure-free geolocation inference
framework utilizing multi-stage information enhancement. WebGeoInfer clusters
similar device web pages and analyzes inter-cluster differences to extract
potential geographical information, bypassing structural limitations. Through
search engine enhancement and Large Language Models mining, the framework
extracts geographical coordinates from identified information. WebGeoInfer
successfully inferred locations for 5,435 devices across 94 countries and 2,056
cities, achieving accuracy rates of 96.96\%, 88.05\%, and 79.70\% at country,
city, and street levels, respectively.

</details>


### [175] [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
*Zhimeng Guo,Huaisheng Zhu,Siyuan Xu,Hangfan Zhang,Teng Xiao,Minhao Cheng*

Main category: cs.CR

TL;DR: CodeTracer는 LLM 생성 코드 탐지를 위한 혁신적인 적응형 코드 워터마킹 프레임워크로, 강화학습 기반의 훈련 패러다임을 통해 고안되었습니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 생성한 코드를 탐지할 필요성에 따라, 구조가 엄격하고 구문적으로 제한된 환경 내에서 작동할 수 있는 워터마킹 시스템이 필요합니다.

Method: CodeTracer는 매개변수화된 모델을 활용하여 다음 토큰 예측 시 토큰 선택에 지능적으로 영향을 주는 정책 기반 접근 방식을 특징으로 합니다.

Result: 상세한 비교 평가를 통해, CodeTracer는 워터마크 탐지 가능성과 생성된 코드의 기능 보존에서 기존의 최첨단 기준보다 우수함을 입증했습니다.

Conclusion: CodeTracer는 코드 기능을 유지하면서도 일반적인 토큰 분포에서 미세하지만 통계적으로 감지 가능한 편차를 보여주는 워터마크를 포함할 수 있습니다.

Abstract: The need for detecting LLM-generated code necessitates watermarking systems
capable of operating within its highly structured and syntactically constrained
environment. To address this, we introduce CodeTracer, an innovative adaptive
code watermarking framework underpinned by a novel reinforcement learning
training paradigm. At its core, CodeTracer features a policy-driven approach
that utilizes a parameterized model to intelligently bias token choices during
next-token prediction. This strategy ensures that embedded watermarks maintain
code functionality while exhibiting subtle yet statistically detectable
deviations from typical token distributions. To facilitate policy learning, we
devise a comprehensive reward system that seamlessly integrates execution
feedback with watermark embedding signals, balancing process-level and
outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization
to enable gradient-based optimization of discrete watermarking decisions.
Extensive comparative evaluations demonstrate CodeTracer's significant
superiority over state-of-the-art baselines in both watermark detectability and
the preservation of generated code's functionality.

</details>


### [176] [The Passwordless Authentication with Passkey Technology from an Implementation Perspective](https://arxiv.org/abs/2508.11928)
*Lien Tran,Boyuan Zhang,Ratchanon Pawanja,Rashid Hussain Khokhar*

Main category: cs.CR

TL;DR: 비밀번호 기반 인증 방식을 대체하는 패스키 기술의 중요성을 강조합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 비밀번호 로그인 방식의 한계를 극복하기 위한 신뢰할 수 있는 인증 시스템이 필요합니다.

Method: 패스키 기술을 활용한 인증 시스템 구현 기법을 분석하고, 시스템 개발 시 컴포넌트 통합에 대한 고려사항을 제안합니다.

Result: 패스키 기술의 우수한 보안성과 실제 구현에서의 효과를 평가합니다.

Conclusion: 패스키 기술이 안전한 인증 시스템에서 널리 채택될 가능성을 보여줍니다.

Abstract: With the rise of sophisticated authentication bypass techniques, passwords
are no longer considered a reliable method for securing authentication systems.
In recent years, new authentication technologies have shifted from traditional
password-based logins to passwordless security. Among these, Time-Based
One-Time Passwords (TOTP) remain one of the most widely used mechanisms, while
Passkeys are emerging as a promising alternative with growing adoption. This
paper highlights the key techniques used during the implementation of the
authentication system with Passkey technology. It also suggests considerations
for integrating components during system development to ensure that users can
securely access their accounts with minimal complexity, while still meeting the
requirements of a robust authentication system that balances security,
usability, and performance. Additionally, by examining TOTP and Passkey
mechanisms from an implementation perspective, this work not only addresses
major security concerns such as password leaks, phishing attacks, and
susceptibility to brute-force attacks, but also evaluates the feasibility and
effectiveness of these mechanisms in real-world implementations. This paper
demonstrates the superior security of Passkey technology and its potential for
broader adoption in secure authentication systems.

</details>


### [177] [Design and Implementation of a Controlled Ransomware Framework for Educational Purposes Using Flutter Cryptographic APIs on Desktop PCs and Android Devices](https://arxiv.org/abs/2508.11939)
*James Gu,Ahmed Sartaj,Mohammed Akram Taher Khan,Rashid Hussain Khokhar*

Main category: cs.CR

TL;DR: 이 연구는 교육 목적으로 Python의 암호화 API를 활용하여 랜섬웨어를 생성하고 구현하는 데 초점을 맞추고 있으며, Flutter와 Dart를 사용하여 Android 버전도 포함된다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안 전문가의 다음 세대를 훈련할 수 있는 교육적 도구를 제공하고자 함.

Method: Python의 암호화 API를 사용하여 랜섬웨어를 개발하고, Flutter 및 Dart를 활용하여 Android 버전 구현, 오픈 소스 암호화 라이브러리 사용.

Result: 연구자들이 랜섬웨어의 기능을 체계적으로 탐색할 수 있는 프레임워크 구축, 암호화 과정 및 피해자 상호작용 역학 분석 가능.

Conclusion: 코드베이스를 오픈 소스로 제공하여 사용자들이 프로그램을 연구, 수정 및 확장할 수 있도록 지원함.

Abstract: This study focuses on the creation and implementation of ransomware for
educational purposes that leverages Python's native cryptographic APIs in a
controlled environment. Additionally, an Android version of the framework is
implemented using Flutter and Dart. For both versions, open-source
cryptographic libraries are utilized. With this framework, researchers can
systematically explore the functionalities of ransomware, including file
encryption processes, cryptographic key management, and victim interaction
dynamics. To ensure safe experimentation, multiple safeguards are incorporated,
such as the ability to restrict the encryption process to a specific directory,
providing the RSA private key for immediate decryption, and narrowing the scope
of targetable files to a carefully curated list (.txt, .jpg, .csv, .doc). This
paper draws inspiration from the infamous WannaCry ransomware and aims to
simulate its behaviour on Android devices. By making the codebase open-source,
it enables users to study, modify, and extend the program for pedagogical
purposes and offers a hands-on tool that can be used to train the next
generation of cybersecurity professionals.

</details>


### [178] [ToxiEval-ZKP: A Structure-Private Verification Framework for Molecular Toxicity Repair Tasks](https://arxiv.org/abs/2508.12035)
*Fei Lin,Tengchao Zhang,Ziyang Gong,Fei-Yue Wang*

Main category: cs.CR

TL;DR: 이 논문은 분자 독성 수리 과제에 대한 구조 비공개 검증 프레임워크인 ToxiEval-ZKP를 제안하며, 제로 지식 증명(ZKP) 메커니즘을 활용하여 생성된 분자가 다차원 독성 수리 기준을 충족함을 외부 검증자에게 증명할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 생성 인공지능이 분자 과학과 같은 고위험 분야에서 놀라운 능력을 보이고 있지만, 그 출력의 검증 가능성과 구조적 프라이버시와 관련된 문제들이 여전히 해결되지 않고 있다.

Method: ToxiEval-ZKP라는 구조 비공개 검증 프레임워크를 설계하였으며, 이는 제로 지식 증명 메커니즘을 평가 과정에 도입하고, 분자 구조를 공개하지 않고도 생성된 분자가 독성 수리 기준을 충족함을 외부 검증자에게 입증한다.

Result: 실험 결과 ToxiEval-ZKP는 완전한 구조 비가시성 하에서도 적절한 검증을 촉진하고, 강력한 회로 효율성, 보안성 및 적응성을 제공하여 신뢰할 수 있는 평가의 새로운 패러다임을 열었다.

Conclusion: ToxiEval-ZKP는 생성 과학적 작업에서의 신뢰할 수 있는 평가를 위한 새로운 패러다임을 제시하며, 모델 개발자들이 제안하는 분자가 독성 수리 기준을 충족하는지의 검증 프로세스를 개선한다.

Abstract: In recent years, generative artificial intelligence (GenAI) has demonstrated
remarkable capabilities in high-stakes domains such as molecular science.
However, challenges related to the verifiability and structural privacy of its
outputs remain largely unresolved. This paper focuses on the task of molecular
toxicity repair. It proposes a structure-private verification framework -
ToxiEval-ZKP - which, for the first time, introduces zero-knowledge proof (ZKP)
mechanisms into the evaluation process of this task. The system enables model
developers to demonstrate to external verifiers that the generated molecules
meet multidimensional toxicity repair criteria, without revealing the molecular
structures themselves. To this end, we design a general-purpose circuit
compatible with both classification and regression tasks, incorporating
evaluation logic, Poseidon-based commitment hashing, and a nullifier-based
replay prevention mechanism to build a complete end-to-end ZK verification
system. Experimental results demonstrate that ToxiEval-ZKP facilitates adequate
validation under complete structural invisibility, offering strong circuit
efficiency, security, and adaptability, thereby opening up a novel paradigm for
trustworthy evaluation in generative scientific tasks.

</details>


### [179] [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
*Wei Jie Yeo,Ranjan Satapathy,Erik Cambria*

Main category: cs.CR

TL;DR: 이 논문에서는 대형 언어 모델의 안전성과 태스크 성능 간의 균형을 유지하기 위해 Intent-FT라는 새로운 미세 조정 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 안전성 조정이 광범위하게 이루어졌음에도 불구하고 공격자들이 만든 지시어에 의해 여전히 취약성을 보이며, 이에 따라 안전성과 태스크 성능 간의 지속적인 균형 문제가 존재합니다.

Method: Intent-FT는 LLM이 응답하기 전에 지시사항의 기본 의도를 추론하도록 명시적으로 훈련하는 간단하고 가벼운 미세 조정 방법입니다. 목표로 하는 적대적 지시어 집합에 대해 미세 조정을 수행하여, Intent-FT는 LLM이 보지 못한 공격에 대해 의도 추론을 일반화할 수 있게 도와줍니다.

Result: Intent-FT는 모든 평가된 공격 카테고리에 대해 일관되게 완화하며, 어떤 단일 공격도 50% 이상의 성공률을 초과하지 않습니다. 기존 방어 체계는 부분적으로만 효과적입니다.

Conclusion: 이 방법은 모델의 일반 응용 능력을 유지하고 표면적으로 해로운 키워드를 포함한 정상 지시사항에 대한 과도한 거부를 줄입니다. 또한 Intent-FT로 훈련된 모델은 적대적 공격에서 숨겨진 해로운 의도를 정확히 식별하며, 이러한 배운 의도를 효과적으로 전이하여 기본 모델 방어를 강화할 수 있습니다.

Abstract: Despite extensive safety-tuning, large language models (LLMs) remain
vulnerable to jailbreak attacks via adversarially crafted instructions,
reflecting a persistent trade-off between safety and task performance. In this
work, we propose Intent-FT, a simple and lightweight fine-tuning approach that
explicitly trains LLMs to infer the underlying intent of an instruction before
responding. By fine-tuning on a targeted set of adversarial instructions,
Intent-FT enables LLMs to generalize intent deduction to unseen attacks,
thereby substantially improving their robustness. We comprehensively evaluate
both parametric and non-parametric attacks across open-source and proprietary
models, considering harmfulness from attacks, utility, over-refusal, and impact
against white-box threats. Empirically, Intent-FT consistently mitigates all
evaluated attack categories, with no single attack exceeding a 50\% success
rate -- whereas existing defenses remain only partially effective. Importantly,
our method preserves the model's general capabilities and reduces excessive
refusals on benign instructions containing superficially harmful keywords.
Furthermore, models trained with Intent-FT accurately identify hidden harmful
intent in adversarial attacks, and these learned intentions can be effectively
transferred to enhance vanilla model defenses.

</details>


### [180] [PP-STAT: An Efficient Privacy-Preserving Statistical Analysis Framework using Homomorphic Encryption](https://arxiv.org/abs/2508.12093)
*Hyunmin Choi*

Main category: cs.CR

TL;DR: PP-STAT은 클라우드 환경에서 민감한 데이터의 통계 분석을 안전하게 수행하기 위한 동형 암호 기반 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 컴퓨팅의 확산으로 인해 제3자 플랫폼에 통계 분석을 아웃소싱할 필요성이 커지고 있으나, 민감한 데이터 처리 시 개인 정보 보호의 문제가 있다.

Method: PP-STAT은 암호화된 데이터에 대해 안전하게 계산을 수행할 수 있도록 다양한 고급 통계 조치를 지원하는 동형 암호 기반 프레임워크이다. PP-STAT은 효율성을 향상시키기 위해 두 가지 주요 최적화를 도입했다.

Result: PP-STAT은 실제 데이터셋에서 높은 수치 정확도를 달성하며 평균 상대 오차(MRE)가 2.4x10-4 이하로 나타났다.

Conclusion: PP-STAT은 개인 정보가 민감한 분야에서 안전하고 정밀한 통계 분석에 실용적으로 유용하다.

Abstract: With the widespread adoption of cloud computing, the need for outsourcing
statistical analysis to third-party platforms is growing rapidly. However,
handling sensitive data such as medical records and financial information in
cloud environments raises serious privacy concerns. In this paper, we present
PP-STAT, a novel and efficient Homomorphic Encryption (HE)-based framework for
privacy-preserving statistical analysis. HE enables computations to be
performed directly on encrypted data without revealing the underlying
plaintext. PP-STAT supports advanced statistical measures, including Z-score
normalization, skewness, kurtosis, coefficient of variation, and Pearson
correlation coefficient, all computed securely over encrypted data. To improve
efficiency, PP-STAT introduces two key optimizations: (1) a Chebyshev-based
approximation strategy for initializing inverse square root operations, and (2)
a pre-normalization scaling technique that reduces multiplicative depth by
folding constant scaling factors into mean and variance computations. These
techniques significantly lower computational overhead and minimize the number
of expensive bootstrapping procedures. Our evaluation on real-world datasets
demonstrates that PP-STAT achieves high numerical accuracy, with mean relative
error (MRE) below 2.4x10-4. Notably, the encrypted Pearson correlation between
the smoker attribute and charges reaches 0.7873, with an MRE of 2.86x10-4.
These results confirm the practical utility of PP-STAT for secure and precise
statistical analysis in privacy-sensitive domains.

</details>


### [181] [Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?](https://arxiv.org/abs/2508.12107)
*Shixuan Guan,Kai Li*

Main category: cs.CR

TL;DR: 블록체인 주소 오염 공격에 대한 연구로, 53개의 이더리움 암호화폐 지갑의 사용성과 보안을 평가하여 문제를 발견함.


<details>
  <summary>Details</summary>
Motivation: 이더리움 암호화폐 지갑의 사용성과 보안을 점검하여 주소 오염 공격에 대한 보호 조치를 확인하고자 함.

Method: 주소 오염 공격을 시뮬레이션하는 실험을 설계하고 53개의 인기 있는 이더리움 암호화폐 지갑을 체계적으로 평가함.

Result: 12개의 지갑은 거래 활동 제공자와의 통신에 실패하여 사용자의 거래 기록을 다운로드할 수 없으며, 16개의 지갑은 가짜 토큰 피싱 전송을 표시하여 높은 위험을 초래함.

Conclusion: 이더리움 암호화폐 지갑 커뮤니티는 사용자 보호를 위한 개선이 필요하며, 개발자 커뮤니티는 이를 인식하고 해결책을 개발 중임.

Abstract: Blockchain address poisoning is an emerging phishing attack that crafts
"similar-looking" transfer records in the victim's transaction history, which
aims to deceive victims and lure them into mistakenly transferring funds to the
attacker. Recent works have shown that millions of Ethereum users were targeted
and lost over 100 million US dollars.
  Ethereum crypto wallets, serving users in browsing transaction history and
initiating transactions to transfer funds, play a central role in deploying
countermeasures to mitigate the address poisoning attack. However, whether they
have done so remains an open question. To fill the research void, in this
paper, we design experiments to simulate address poisoning attacks and
systematically evaluate the usability and security of 53 popular Ethereum
crypto wallets. Our evaluation shows that there exist communication failures
between 12 wallets and their transaction activity provider, which renders them
unable to download the users' transaction history. Besides, our evaluation also
shows that 16 wallets pose a high risk to their users due to displaying fake
token phishing transfers. Moreover, our further analysis suggests that most
wallets rely on transaction activity providers to filter out phishing
transfers. However, their phishing detection capability varies. Finally, we
found that only three wallets throw an explicit warning message when users
attempt to transfer to the phishing address, implying a significant gap within
the broader Ethereum crypto wallet community in protecting users from address
poisoning attacks.
  Overall, our work shows that more efforts are needed by the Ethereum crypto
wallet developer community to achieve the highest usability and security
standard. Our bug reports have been acknowledged by the developer community,
who are currently developing mitigation solutions.

</details>


### [182] [Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation](https://arxiv.org/abs/2508.12138)
*Mohammad Ishzaz Asif Rafid,Morsalin Sakib*

Main category: cs.CR

TL;DR: 본 논문은 비트코인의 기존 작업 증명(PoW) 메커니즘을 클라우드 기반의 협업 훈련 프레임워크로 대체하는 하이브리드 아키텍처를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 비트코인의 PoW 메커니즘은 탈중앙화된 합의 달성에 중심적이지만, 과도한 에너지 사용과 하드웨어 비효율성으로 오랫동안 비판받아 왔습니다.

Method: 광산업자들이 전처리된 데이터셋에서 수평적으로 확장된 기계 학습 모델의 세그먼트를 훈련시키기 위해 컴퓨팅 자원을 기여하며, 중앙 서버는 훈련된 매개변수 수와 모델 손실 감소를 메트릭으로 사용하여 기여도를 평가합니다.

Result: 각 사이클 종료 시 가중 복권을 통해 승리한 광산업자가 디지털 서명이 포함된 인증서를 받으며, 이 인증서는 PoW의 검증 가능한 대체 수단이 되어 블록체인에 블록을 추가할 권리를 부여합니다.

Conclusion: 제안된 접근법은 자원 지출을 사회적으로 가치 있는 작업으로 전환함으로써 전통적인 채굴의 지속 가능성 문제를 해결하며, 보안 인센티브를 실제 계산 진척과 일치시킵니다.

Abstract: Bitcoin's Proof of Work (PoW) mechanism, while central to achieving
decentralized consensus, has long been criticized for excessive energy use and
hardware inefficiencies \cite{devries2018bitcoin, truby2018decarbonizing}. This
paper introduces a hybrid architecture that replaces Bitcoin's traditional PoW
with a centralized, cloud-based collaborative training framework. In this
model, miners contribute computing resources to train segments of horizontally
scaled machine learning models on preprocessed datasets, ensuring privacy and
generating meaningful outputs \cite{li2017securing}. A central server evaluates
contributions using two metrics: number of parameters trained and reduction in
model loss during each cycle. At the end of every cycle, a weighted lottery
selects the winning miner, who receives a digitally signed certificate. This
certificate serves as a verifiable substitute for PoW and grants the right to
append a block to the blockchain \cite{nakamoto2008bitcoin}. By integrating
digital signatures and SHA-256 hashing \cite{nist2015sha}, the system preserves
blockchain integrity while redirecting energy toward productive computation.
The proposed approach addresses the sustainability concerns of traditional
mining by converting resource expenditure into socially valuable work, aligning
security incentives with real-world computational progress.

</details>


### [183] [Attack Graph Generation on HPC Clusters](https://arxiv.org/abs/2508.12161)
*Ming Li,John Hale*

Main category: cs.CR

TL;DR: 고성능 컴퓨팅 클러스터를 활용하여 공격 그래프 생성의 속도와 메모리 요구사항을 효율적으로 해결하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 네트워크 자산의 수와 장치 간의 상호 연결, 취약점이 증가함에 따라 생성되는 공격 그래프의 크기와 부피가 급격히 증가하여 문제를 해결할 필요성이 있다.

Method: 고성능 컴퓨팅 클러스터를 사용하여 공격 그래프 생성기를 구현했다.

Result: 실험을 통해 성능을 평가하고 클러스터 환경이 공격 그래프 생성의 속도 저하 및 높은 메모리 요구사항 문제를 해결하는 방법에 대한 통찰을 제공한다.

Conclusion: 고성능 컴퓨팅 클러스터를 통해 공격 그래프 생성의 비효율성을 극복할 수 있다.

Abstract: Attack graphs (AGs) are graphical tools to analyze the security of computer
networks. By connecting the exploitation of individual vulnerabilities, AGs
expose possible multi-step attacks against target networks, allowing system
administrators to take preventive measures to enhance their network's security.
As powerful analytical tools, however, AGs are both time- and memory-consuming
to be generated. As the numbers of network assets, interconnections between
devices, as well as vulnerabilities increase, the size and volume of the
resulting AGs grow at a much higher rate, leading to the well-known state-space
explosion. In this paper, we propose the use of high performance computing
(HPC) clusters to implement AG generators. We evaluate the performance through
experiments and provide insights into how cluster environments can help resolve
the issues of slow speed and high memory demands in AG generation in a balanced
way.

</details>


### [184] [Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous](https://arxiv.org/abs/2508.12175)
*Ben Nassi,Stav Cohen,Or Yair*

Main category: cs.CR

TL;DR: LLM 통합 애플리케이션의 보안 리스크에 대한 연구로, Promptware에 의한 위험성을 분석하고 새로운 TARA 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 애플리케이션에 대한 공격 경향의 변화를 경고하고, Promptware의 위험성을 조사하기 위해.

Method: Promptware 위험 평가를 위한 Novel Threat Analysis and Risk Assessment(TARA) 프레임워크를 제안하고, Targeted Promptware Attacks를 분석한다.

Result: Gemini 기반 도우미에 대해 14개의 공격 시나리오를 식별하고, 73%의 위협이 높은 위험을 나타내는 것을 확인하였다.

Conclusion: 위험을 완화할 수 있는 방안을 논의하며, 위험 수준을 매우 낮게 제거할 수 있음을 보여주었다.

Abstract: The growing integration of LLMs into applications has introduced new security
risks, notably known as Promptware - maliciously engineered prompts designed to
manipulate LLMs to compromise the CIA triad of these applications. While prior
research warned about a potential shift in the threat landscape for LLM-powered
applications, the risk posed by Promptware is frequently perceived as low. In
this paper, we investigate the risk Promptware poses to users of Gemini-powered
assistants (web application, mobile application, and Google Assistant). We
propose a novel Threat Analysis and Risk Assessment (TARA) framework to assess
Promptware risks for end users. Our analysis focuses on a new variant of
Promptware called Targeted Promptware Attacks, which leverage indirect prompt
injection via common user interactions such as emails, calendar invitations,
and shared documents. We demonstrate 14 attack scenarios applied against
Gemini-powered assistants across five identified threat classes: Short-term
Context Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent
Invocation, and Automatic App Invocation. These attacks highlight both digital
and physical consequences, including spamming, phishing, disinformation
campaigns, data exfiltration, unapproved user video streaming, and control of
home automation devices. We reveal Promptware's potential for on-device lateral
movement, escaping the boundaries of the LLM-powered application, to trigger
malicious actions using a device's applications. Our TARA reveals that 73% of
the analyzed threats pose High-Critical risk to end users. We discuss
mitigations and reassess the risk (in response to deployed mitigations) and
show that the risk could be reduced significantly to Very Low-Medium. We
disclosed our findings to Google, which deployed dedicated mitigations.

</details>


### [185] [CAN Networks Security in Smart Grids Communication Technologies](https://arxiv.org/abs/2508.12181)
*Ayman W. Baharia,Khaled T. Naga,Hesham S. Abdelfattah,Shady A. Maged,Sherif A. Hammad*

Main category: cs.CR

TL;DR: 스마트 그리드의 발전에 따라 효과적인 통신 프로토콜의 필요성이 증가하고 있으며, 본 연구에서는 사이버 공격에 강한 CAN 네트워크 보안 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 그리드의 빠른 발전에 따라 신뢰성 있고 안전한 데이터 전송을 위한 통신 프로토콜이 필요하다.

Method: 사이버 공격으로부터 CAN 네트워크를 보호하기 위해 단일 노드에 의존하는 솔루션을 구현하였다.

Result: 이 방식은 모든 CAN 네트워크 노드에 대한 보안 메커니즘의 오버헤드를 줄이는 동시에 네트워크 보안을 강화하는 것을 목표로 한다.

Conclusion: 방법론 및 종합 테스트 결과는 후속 논의에서 자세히 발표될 예정이다.

Abstract: The rapid evolution of smart grids requires effective communication protocols
to transfer data reliably and securely. Controller Area Network (CAN) is one of
the most recognized protocols that offer reliable data transmission in smart
grids due to its robustness, real-time capabilities, and relatively low initial
cost of its required hardware. However, as a smart city becomes more
interconnected, it also becomes more vulnerable to cyber-attacks. As there are
many mechanisms to secure the CAN nodes from attacks, most of those mechanisms
have computational overhead, resulting in more delay in the network. We
implemented a solution that requires almost no overhead to any CAN node
connected to the network. It depends on a single node responsible for securing
the CAN network. This approach seeks to augment network security while reducing
security mechanisms overhead to all CAN network nodes. The methodology and
comprehensive test results will be presented in detail during a subsequent
discussion. The used software for development is Code Composer Studio, and the
used microcontroller evaluation boards (EVB) are TM4C 1294.

</details>


### [186] [AUTOVR: Automated UI Exploration for Detecting Sensitive Data Flow Exposures in Virtual Reality Apps](https://arxiv.org/abs/2508.12187)
*John Y. Kim,Chaoshun Zuo,Yanjie Zhao,Zhiqiang Lin*

Main category: cs.CR

TL;DR: AUTOVR는 Unity 엔진으로 구축된 VR 앱에서 동적 UI 및 사용자 이벤트 상호작용을 자동화하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: Meta Quest VR 플랫폼의 성장과 함께 다양한 VR 앱의 출현은 사용자 인터페이스 탐색 및 사용자 이벤트 테스트를 위한 강력한 도구의 필요성을 증가시켰다.

Method: AUTOVR는 일반적인 Android 및 GUI 테스터와 달리 앱의 내부 바이너리를 분석하여 숨겨진 이벤트를 드러내고, 생성 이벤트 의존성을 해결하여 VR 앱의 포괄적인 탐색을 수행한다.

Result: AUTOVR는 데이터 노출 감도를 성능 지표로 사용하여, 널리 사용되는 Android Monkey와 비교했을 때 더욱 우수한 성능을 입증하였다.

Conclusion: AUTOVR는 VR 앱의 민감한 데이터 노출을 대폭 증가시키고, VR 앱의 개인 정보를 크게 향상시키는 결과를 보여줬다.

Abstract: The rise of Virtual Reality (VR) has provided developers with an
unprecedented platform for creating games and applications (apps) that require
distinct inputs, different from those of conventional devices like smartphones.
The Meta Quest VR platform, driven by Meta, has democratized VR app publishing
and attracted millions of users worldwide. However, as the number of published
apps grows, there is a notable lack of robust headless tools for user interface
(UI) exploration and user event testing. To address this need, we present
AUTOVR, an automatic framework for dynamic UI and user event interaction in VR
apps built on the Unity Engine. Unlike conventional Android and GUI testers,
AUTOVR analyzes the app's internal binary to reveal hidden events, resolves
generative event dependencies, and utilizes them for comprehensive exploration
of VR apps. Using sensitive data exposure as a performance metric, we compare
AUTOVR with Android Monkey, a widely used headless Android GUI stress testing
tool. Our empirical evaluation demonstrates AUTOVR's superior performance,
triggering an order of magnitude of more sensitive data exposures and
significantly enhancing the privacy of VR apps.

</details>


### [187] [Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats](https://arxiv.org/abs/2508.12259)
*Ken Huang,Yasir Mehmood,Hammad Atta,Jerry Huang,Muhammad Zeeshan Baig,Sree Bhargavi Balija*

Main category: cs.CR

TL;DR: 이 논문은 Zero-Trust IAM 프레임워크를 통해 Agentic Web을 강화하는 통합 보안 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: Agentic Web의 보안을 강화하기 위해 Zero-Trust IAM 프레임워크를 채택한 새로운 아키텍처를 개발한다.

Method: Decentralized Identifiers(DIDs)와 Verifiable Credentials(VCs)를 사용하여 검증 가능한 에이전트 신원을 기반으로 하고, 프로토콜 비의존적인 에이전트 이름 서비스(ANS)를 통한 검색 관리를 수행한다.

Result: 제안된 아키텍처가 LPCI 공격에 대해 성공 확률이 제한된 보안 보장을 제공함을 형식적 분석을 통해 입증한다.

Conclusion: 안전하고 회복력 있는 신뢰할 수 있는 에이전트 생태계를 위한 종합적인 설계를 제시한다.

Abstract: This paper presents a Unified Security Architecture that fortifies the
Agentic Web through a Zero-Trust IAM framework. This architecture is built on a
foundation of rich, verifiable agent identities using Decentralized Identifiers
(DIDs) and Verifiable Credentials (VCs), with discovery managed by a
protocol-agnostic Agent Name Service (ANS). Security is operationalized through
a multi-layered Trust Fabric which introduces significant innovations,
including Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,
and Dynamic Identity with Behavioral Attestation. By explicitly linking the
LPCI threat to these enhanced architectural countermeasures within a formal
security model, we propose a comprehensive and forward-looking blueprint for a
secure, resilient, and trustworthy agentic ecosystem. Our formal analysis
demonstrates that the proposed architecture provides provable security
guarantees against LPCI attacks with bounded probability of success.

</details>


### [188] [CryptPEFT: Efficient and Private Neural Network Inference via Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2508.12264)
*Saisai Xia,Wenhao Wang,Zihao Wang,Yuhui Zhang,Yier Jin,Dan Meng,Rui Hou*

Main category: cs.CR

TL;DR: CryptPEFT는 개인 정보 보호를 위한 비공식 인퍼런스 시나리오에 최적화된 최초의 파라미터 효율적 미세 조정(PEFT) 솔루션이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 사전 훈련된 모델과 경량 어댑터를 사용하는 현대 머신 러닝 파이프라인에서 사용자 입력 및 미세 조정된 어댑터의 개인 정보를 보호하는 것은 여전히 큰 문제이다.

Method: CryptPEFT는 암호화된 계산을 어댑터에만 국한시키는 새로운 단방향 통신(OWC) 아키텍처를 소개하고, OWC 호환 어댑터의 설계를 탐색하며, 자동화된 아키텍처 탐색 알고리즘을 사용하여 개인 정보 보호 효율성과 모델 유틸리티 사이의 균형을 최적화한다.

Result: CryptPEFT는 여러 이미지 분류 데이터셋에서 Vision Transformer 백본을 사용해 평가되었으며, 기존 기준 대비 $20.62	imes$에서 $291.48	imes$의 속도 향상을 보여주었다.

Conclusion: CryptPEFT는 현대 PEFT 기반 인퍼런스를 위한 효율적이고 개인 정보 보호를 보장하는 솔루션을 제공한다.

Abstract: Publicly available large pretrained models (i.e., backbones) and lightweight
adapters for parameter-efficient fine-tuning (PEFT) have become standard
components in modern machine learning pipelines. However, preserving the
privacy of both user inputs and fine-tuned adapters -- often trained on
sensitive data -- during inference remains a significant challenge. Applying
cryptographic techniques, such as multi-party computation (MPC), to PEFT
settings still incurs substantial encrypted computation across both the
backbone and adapter, mainly due to the inherent two-way communication between
them. To address this limitation, we propose CryptPEFT, the first PEFT solution
specifically designed for private inference scenarios. CryptPEFT introduces a
novel one-way communication (OWC) architecture that confines encrypted
computation solely to the adapter, significantly reducing both computational
and communication overhead. To maintain strong model utility under this
constraint, we explore the design space of OWC-compatible adapters and employ
an automated architecture search algorithm to optimize the trade-off between
private inference efficiency and model utility. We evaluated CryptPEFT using
Vision Transformer backbones across widely used image classification datasets.
Our results show that CryptPEFT significantly outperforms existing baselines,
delivering speedups ranging from $20.62\times$ to $291.48\times$ in simulated
wide-area network (WAN) and local-area network (LAN) settings. On CIFAR-100,
CryptPEFT attains 85.47% accuracy with just 2.26 seconds of inference latency.
These findings demonstrate that CryptPEFT offers an efficient and
privacy-preserving solution for modern PEFT-based inference.

</details>


### [189] [Adjustable AprilTags For Identity Secured Tasks](https://arxiv.org/abs/2508.12304)
*Hao Li*

Main category: cs.CR

TL;DR: 조정 가능한 AprilTags의 사용을 제안하여 적대적 공격으로 인한 잠재적 피해를 처리한다.


<details>
  <summary>Details</summary>
Motivation: AprilTags는 이미지 처리 및 패턴 인식을 촉진하는 데 유용한 특수 태그로, 공개 환경에서는 신원 보안 문제가 발생할 수 있다.

Method: 고정 태그 대신 조정 가능한 AprilTags를 사용하여 보안을 강화한다.

Result: 조정 가능한 AprilTags가 보안성을 높일 수 있는 가능성을 제시한다.

Conclusion: 조정 가능한 AprilTags의 채택이 적대적 공격으로부터의 피해를 줄일 수 있다.

Abstract: Special tags such as AprilTags that facilitate image processing and pattern
recognition are useful in practical applications. In close and private
environments, identity security is unlikely to be an issue because all involved
AprilTags can be completely regulated. However, in open and public
environments, identity security is no longer an issue that can be neglected. To
handle potential harm caused by adversarial attacks, this note advocates
utilization of adjustable AprilTags instead of fixed ones.

</details>


### [190] [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: 본 논문은 확산 대형 언어 모델(dLLMs)의 안전성 성능을 분석하고 이의 독특한 생성 특성에 맞춘 새로운 안전 정렬 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 새로운 dLLMs 아키텍처에 대한 안전성 연구가 부족하다.

Method: 안전성에서의 방어자와 공격자 간의 비대칭성을 식별하고 이를 바탕으로 중간 토큰의 안전 정렬 방법(MOSA)을 도입한다.

Result: MOSA는 8가지 공격 방법에 대해 두 가지 기준에서 보안 성능을 비교하여 강력한 결과를 입증한다.

Conclusion: MOSA는 dLLM의 안전성을 효과적으로 향상시킨다.

Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive non-autoregressive paradigm due to their unique training and
inference approach. However, there is currently a lack of safety study on this
novel architecture. In this paper, we present the first analysis of dLLMs'
safety performance and propose a novel safety alignment method tailored to
their unique generation characteristics. Specifically, we identify a critical
asymmetry between the defender and attacker in terms of security. For the
defender, we reveal that the middle tokens of the response, rather than the
initial ones, are more critical to the overall safety of dLLM outputs; this
seems to suggest that aligning middle tokens can be more beneficial to the
defender. The attacker, on the contrary, may have limited power to manipulate
middle tokens, as we find dLLMs have a strong tendency towards a sequential
generation order in practice, forcing the attack to meet this distribution and
diverting it from influencing the critical middle tokens. Building on this
asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method
that directly aligns the model's middle generation with safe refusals
exploiting reinforcement learning. We implement MOSA and compare its security
performance against eight attack methods on two benchmarks. We also test the
utility of MOSA-aligned dLLM on coding, math, and general reasoning. The
results strongly prove the superiority of MOSA.

</details>


### [191] [LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems](https://arxiv.org/abs/2508.12412)
*Ron Solomon,Yarin Yerushalmi Levi,Lior Vaknin,Eran Aizikovich,Amit Baras,Etai Ohana,Amit Giloni,Shamik Bose,Chiara Picardi,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: LumiMAS는 다중 에이전트 시스템의 오류 감지를 개선하기 위해 제안된 새로운 관찰 가능성 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델을 다중 에이전트 시스템에 통합하면 복잡한 문제를 자율적으로 해결할 수 있는 능력이 향상될 수 있지만, 시스템 실패를 모니터링하고 해석하는 데에 독특한 도전 과제가 발생한다.

Method: LumiMAS는 세 가지 주요 구성 요소로 이루어진 관찰 가능성 프레임워크로, 모니터링 및 로깅 계층, 이상 감지 계층, 이상 설명 계층을 포함한다.

Result: LumiMAS는 두 가지 인기 있는 MAS 플랫폼을 사용하여 구현된 7개의 서로 다른 MAS 애플리케이션에서 평가되었으며, 다양한 실패 사례를 포함한다.

Conclusion: 평가 결과, LumiMAS는 실패 감지, 분류 및 뿌리 원인 분석(RCA)에서 효과적임을 보여주었다.

Abstract: The incorporation of large language models in multi-agent systems (MASs) has
the potential to significantly improve our ability to autonomously solve
complex problems. However, such systems introduce unique challenges in
monitoring, interpreting, and detecting system failures. Most existing MAS
observability frameworks focus on analyzing each individual agent separately,
overlooking failures associated with the entire MAS. To bridge this gap, we
propose LumiMAS, a novel MAS observability framework that incorporates advanced
analytics and monitoring techniques. The proposed framework consists of three
key components: a monitoring and logging layer, anomaly detection layer, and
anomaly explanation layer. LumiMAS's first layer monitors MAS executions,
creating detailed logs of the agents' activity. These logs serve as input to
the anomaly detection layer, which detects anomalies across the MAS workflow in
real time. Then, the anomaly explanation layer performs classification and root
cause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven
different MAS applications, implemented using two popular MAS platforms, and a
diverse set of possible failures. The applications include two novel
failure-tailored applications that illustrate the effects of a hallucination or
bias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in
failure detection, classification, and RCA.

</details>


### [192] [A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security](https://arxiv.org/abs/2508.12470)
*Afrah Gueriani,Hamza Kheddar,Ahmed Cherif Mazari,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: BiGAT-ID라는 새로운 변형기 기반 침입 탐지 시스템을 제안하여 복잡한 사이버 보안 문제를 해결하고 99.13% 및 99.34%의 탐지 정확도를 달성하였다.


<details>
  <summary>Details</summary>
Motivation: IoMT와 IIoT의 상호 연결성 증가로 인해 민감한 데이터와 환자 안전이 고급 사이버 위협에 노출되고 있다.

Method: BiGRU, LSTM 네트워크 및 다중 헤드 주의(MHA)를 결합한 하이브리드 모델인 BiGAT-ID를 제안한다.

Result: CICIoMT2024와 EdgeIIoTset 데이터셋에서 99.13%와 99.34%의 탐지 정확도를 기록했으며, IoMT 시나리오에서는 0.0002초, IIoT 시나리오에서는 0.0001초의 추론 시간을 보여준다.

Conclusion: BiGAT-ID는 실제 이종 IoT 환경에서 신뢰할 수 있고 효율적인 IDS로 입증된다.

Abstract: The increased Internet of Medical Things IoMT and the Industrial Internet of
Things IIoT interconnectivity has introduced complex cybersecurity challenges,
exposing sensitive data, patient safety, and industrial operations to advanced
cyber threats. To mitigate these risks, this paper introduces a novel
transformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid
model that combines bidirectional gated recurrent units BiGRU, long short-term
memory LSTM networks, and multi-head attention MHA. The proposed architecture
is designed to effectively capture bidirectional temporal dependencies, model
sequential patterns, and enhance contextual feature representation. Extensive
experiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset
industrial IoT demonstrate the model's cross-domain robustness, achieving
detection accuracies of 99.13 percent and 99.34 percent, respectively.
Additionally, the model exhibits exceptional runtime efficiency, with inference
times as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT
scenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a
reliable and efficient IDS for deployment in real-world heterogeneous IoT
environments

</details>


### [193] [ChamaleoNet: Programmable Passive Probe for Enhanced Visibility on Erroneous Traffic](https://arxiv.org/abs/2508.12496)
*Zhihao Wang,Alessandro Cornacchia,Andrea Bianco,Idilio Drago,Paolo Giaccone,Dingde Jiang,Marco Mellia*

Main category: cs.CR

TL;DR: ChamaleoNet은 프로덕션 네트워크를 투명한 모니터로 변환하여 잘못된 트래픽을 수집하고, 사이버 공격 및 구성 오류 탐지를 용이하게 한다.


<details>
  <summary>Details</summary>
Motivation: 트래픽 가시성은 관리 및 보안 작업의 핵심 요소로, 잘못된 트래픽을 관찰하는 것이 중요하다.

Method: ChamaleoNet은 SDN 패러다임을 활용해 캠퍼스/기업 네트워크를 통해 흐르는 트래픽을 처리하고, 잘못된 패킷에만 집중하여 수집 시스템의 부담을 줄인다.

Result: ChamaleoNet은 수집 시스템에 대한 압력을 낮추면서 96%의 트래픽 필터링을 통해 확장 가능한 솔루션을 제공한다.

Conclusion: 이 시스템은 오픈 소스로 제공되며, 내부 구성 오류 및 감염된 호스트를 드러내고 공격자의 외부 방사선 가시성을 강화한다.

Abstract: Traffic visibility remains a key component for management and security
operations. Observing unsolicited and erroneous traffic, such as unanswered
traffic or errors, is fundamental to detect misconfiguration, temporary
failures or attacks. ChamaleoNet transforms any production network into a
transparent monitor to let administrators collect unsolicited and erroneous
traffic directed to hosts, whether offline or active, hosting a server or a
client, protected by a firewall, or unused addresses. ChamaleoNet is programmed
to ignore well-formed traffic and collect only erroneous packets, including
those generated by misconfigured or infected internal hosts, and those sent by
external actors which scan for services. Engineering such a system poses
several challenges, from scalability to privacy. Leveraging the SDN paradigm,
ChamaleoNet processes the traffic flowing through a campus/corporate network
and focuses on erroneous packets only, lowering the pressure on the collection
system while respecting privacy regulations by design. ChamaleoNet enables the
seamless integration with active deceptive systems like honeypots that can
impersonate unused hosts/ports/services and engage with senders. The SDN
in-hardware filtering reduces the traffic to the controller by 96%, resulting
in a scalable solution, which we offer as open source. Simple analytics unveil
internal misconfigured and infected hosts, identify temporary failures, and
enhance visibility on external radiation produced by attackers looking for
vulnerable services.

</details>


### [194] [Systematic Analysis of MCP Security](https://arxiv.org/abs/2508.12538)
*Yongjian Guo,Puzhuo Liu,Wanlun Ma,Zehang Deng,Xiaogang Zhu,Peng Di,Xi Xiao,Sheng Wen*

Main category: cs.CR

TL;DR: MCP는 AI 에이전트가 외부 도구와 연결될 수 있게 하는 표준으로, 기능 향상을 가져오지만 Tool Poisoning Attacks와 같은 취약점을 동반한다. 이를 해결하기 위해 31개의 공격 방법을 분류한 MCP Attack Library (MCPLIB)를 제안하고, 각 공격의 효과를 정량적으로 분석하였다.


<details>
  <summary>Details</summary>
Motivation: MCP의 보안에 대한 현재 연구는 제한적이며, 주로 좁고 질적인 분석에 초점을 맞추고 있어 실질적인 위협의 다양성을 제대로 반영하지 못한다.

Method: MCP Attack Library (MCPLIB)를 제시하고, 31개의 공격 방법을 직접 도구 주입, 간접 도구 주입, 악의적 사용자 공격, LLM 고유 공격의 네 가지 주요 분류로 나눈다. 각 공격의 효능에 대한 정량적 분석을 수행한다.

Result: 실험을 통해 MCP 취약성에 대한 주요 통찰력을 발견하고, 도구 설명에 대한 맹목적인 의존, 파일 기반 공격에 대한 민감성, 공유된 맥락을 악용한 연쇄 공격, 외부 데이터를 실행 가능 명령과 구별하는 어려움 등을 확인했다.

Conclusion: 튼튼한 방어 전략과 정보에 기반한 MCP 설계의 긴급성을 강조하며, MCP 공격 분류 체계 구축, 통합 공격 프레임워크 MCPLIB 도입, 실증적 취약성 분석을 통해 MCP 보안 메커니즘을 강화하는 기여를 했다.

Abstract: The Model Context Protocol (MCP) has emerged as a universal standard that
enables AI agents to seamlessly connect with external tools, significantly
enhancing their functionality. However, while MCP brings notable benefits, it
also introduces significant vulnerabilities, such as Tool Poisoning Attacks
(TPA), where hidden malicious instructions exploit the sycophancy of large
language models (LLMs) to manipulate agent behavior. Despite these risks,
current academic research on MCP security remains limited, with most studies
focusing on narrow or qualitative analyses that fail to capture the diversity
of real-world threats. To address this gap, we present the MCP Attack Library
(MCPLIB), which categorizes and implements 31 distinct attack methods under
four key classifications: direct tool injection, indirect tool injection,
malicious user attacks, and LLM inherent attack. We further conduct a
quantitative analysis of the efficacy of each attack. Our experiments reveal
key insights into MCP vulnerabilities, including agents' blind reliance on tool
descriptions, sensitivity to file-based attacks, chain attacks exploiting
shared context, and difficulty distinguishing external data from executable
commands. These insights, validated through attack experiments, underscore the
urgency for robust defense strategies and informed MCP design. Our
contributions include 1) constructing a comprehensive MCP attack taxonomy, 2)
introducing a unified attack framework MCPLIB, and 3) conducting empirical
vulnerability analysis to enhance MCP security mechanisms. This work provides a
foundational framework, supporting the secure evolution of MCP ecosystems.

</details>


### [195] [The Hidden Cost of Correlation: Rethinking Privacy Leakage in Local Differential Privacy](https://arxiv.org/abs/2508.12539)
*Sandaru Jayawardana,Sennur Ulukus,Ming Ding,Kanchana Thilakarathna*

Main category: cs.CR

TL;DR: 이 논문은 지역 차별 개인 정보 보호(LDP)에서 상관관계로 인한 개인 정보 유출(CPL)의 중요성을 분석하고, 다양한 LDP 메커니즘의 성능과 적합성을 평가하며 새로운 알고리즘 프레임워크와 벤치마크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LDP는 분산 시스템에서 개인 정보를 보호하면서 데이터 수집을 가능하게 하는 유망한 패러다임입니다. 하지만 상관관계로 인한 개인 정보 유출이 실제 데이터에서 얼마나 중요한지 명확하지 않습니다.

Method: LDP 메커니즘인 GRR, RAPPOR, OUE, OLH, Exponential mechanism의 통계적 분석을 수행하고, ((ε,δ)-LDP) 메커니즘에 대한 최초의 알고리즘 프레임워크를 개발합니다.

Result: 현재 접근 방식의 주된 가정과 측정값들이 이러한 유출을 정확히 설명하지 못함을 확인하였고, 이론적 결과를 경험적 통계 결과와 검증하였습니다.

Conclusion: 새로운 벤치마크를 제안하여 상관관계 분석 알고리즘을 검증하고, 실제 데이터 거버넌스에서 효율적인 개인 정보와 유용성의 트레이드오프를 달성하는 방법을 제시합니다.

Abstract: Local differential privacy (LDP) has emerged as a promising paradigm for
privacy-preserving data collection in distributed systems, where users
contribute multi-dimensional records with potentially correlated attributes.
Recent work has highlighted that correlation-induced privacy leakage (CPL)
plays a critical role in shaping the privacy-utility trade-off under LDP,
especially when correlations exist among attributes. Nevertheless, it remains
unclear to what extent the prevailing assumptions and proposed solutions are
valid and how significant CPL is in real-world data. To address this gap, we
first perform a comprehensive statistical analysis of five widely used LDP
mechanisms -- GRR, RAPPOR, OUE, OLH and Exponential mechanism -- to assess CPL
across four real-world datasets. We identify that many primary assumptions and
metrics in current approaches fall short of accurately characterising these
leakages. Moreover, current studies have been limited to a set of pure LDP
(i.e., {\delta = 0}) mechanisms. In response, we develop the first algorithmic
framework to theoretically quantify CPL for any general approximated LDP
(({\varepsilon},{\delta})-LDP) mechanism. We validate our theoretical results
against empirical statistical results and provide a theoretical explanation for
the observed statistical patterns. Finally, we propose two novel benchmarks to
validate correlation analysis algorithms and evaluate the utility vs CPL of LDP
mechanisms. Further, we demonstrate how these findings can be applied to
achieve an efficient privacy-utility trade-off in real-world data governance.

</details>


### [196] [DEFENDCLI: {Command-Line} Driven Attack Provenance Examination](https://arxiv.org/abs/2508.12553)
*Peilun Wu,Nan Sun,Nour Moustafa,Youyang Qu,Ming Ding*

Main category: cs.CR

TL;DR: DEFENDCLI는 명령줄 수준의 탐지를 위해 프로베넌스 그래프를 활용하여 기존 EDR 시스템의 격차를 해결하며, 복잡하고 역동적인 환경에서 신뢰성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 현재 EDR 솔루션들이 인지하고 있는 한계를 극복하고, 공격 탐지와 상관 관계 분석을 더욱 정교하게 수행하기 위해 이 연구를 진행했습니다.

Method: DEFENDCLI는 명령줄 수준의 탐지를 위한 혁신적인 시스템으로, 프로베넌스 그래프를 활용하여 세 가지 수준에서의 차별화를 평가합니다.

Result: DEFENDCLI는 DARPA Engagement Series 공격 데이터세트에서 기존 방법에 비해 약 1.6배 향상된 정확도를 보여줬으며, 다양한 공격 시나리오에서 실시간 산업 테스트에서도 효과가 입증되었습니다.

Conclusion: DEFENDCLI는 이전에 발견되지 않았던 공격 사례를 탐지할 뿐만 아니라, 최신 연구작업에 비해 2.3배의 정확도 향상을 달성합니다.

Abstract: Endpoint Detection and Response (EDR) solutions embrace the method of attack
provenance graph to discover unknown threats through system event correlation.
However, this method still faces some unsolved problems in the fields of
interoperability, reliability, flexibility, and practicability to deliver
actionable results. Our research highlights the limitations of current
solutions in detecting obfuscation, correlating attacks, identifying
low-frequency events, and ensuring robust context awareness in relation to
command-line activities. To address these challenges, we introduce DEFENDCLI,
an innovative system leveraging provenance graphs that, for the first time,
delves into command-line-level detection. By offering finer detection
granularity, it addresses a gap in modern EDR systems that has been overlooked
in previous research. Our solution improves the precision of the information
representation by evaluating differentiation across three levels: unusual
system process calls, suspicious command-line executions, and infrequent
external network connections. This multi-level approach enables EDR systems to
be more reliable in complex and dynamic environments. Our evaluation
demonstrates that DEFENDCLI improves precision by approximately 1.6x compared
to the state-of-the-art methods on the DARPA Engagement Series attack datasets.
Extensive real-time industrial testing across various attack scenarios further
validates its practical effectiveness. The results indicate that DEFENDCLI not
only detects previously unknown attack instances, which are missed by other
modern commercial solutions, but also achieves a 2.3x improvement in precision
over the state-of-the-art research work.

</details>


### [197] [Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services](https://arxiv.org/abs/2508.12560)
*Prabath Abeysekara,Hai Dong*

Main category: cs.CR

TL;DR: MEC 기반 IIoT 시스템에서 IoT 서비스의 신뢰성을 향상시키기 위한 데이터 기반의 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 신뢰 부트스트래핑 접근 방식의 한계를 개선하고 IIoT 시스템에서 신뢰성 평가를 지원하고자 한다.

Method: 데이터 희소성을 해결하기 위해 다양한 MEC 환경 내에서 지식 공유를 가능하게 한다.

Result: 제안한 접근 방식이 두 개의 실제 데이터 세트에서 검증되었으며 MEC 환경 내 신뢰 정보의 맥락 의존성을 보여준다.

Conclusion: 제안한 접근 방식이 MEC 기반 IIoT 시스템에서 서비스의 신뢰성을 부트스트랩하는 데 적합하다는 것을 확인했다.

Abstract: We propose a data-driven and context-aware approach to bootstrap
trustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge
Computing (MEC) based industrial IoT (IIoT) systems. The proposed approach
addresses key limitations in adapting existing trust bootstrapping approaches
into MEC-based IIoT systems. These key limitations include, the lack of
opportunity for a service consumer to interact with a lesser-known service over
a prolonged period of time to get a robust measure of its trustworthiness,
inability of service consumers to consistently interact with their peers to
receive reliable recommendations of the trustworthiness of a lesser-known
service as well as the impact of uneven context parameters in different MEC
environments causing uneven trust environments for trust evaluation. In
addition, the proposed approach also tackles the problem of data sparsity via
enabling knowledge sharing among different MEC environments within a given MEC
topology. To verify the effectiveness of the proposed approach, we carried out
a comprehensive evaluation on two real-world datasets suitably adjusted to
exhibit the context-dependent trust information accumulated in MEC environments
within a given MEC topology. The experimental results affirmed the
effectiveness of our approach and its suitability to bootstrap trustworthiness
of services in MEC-based IIoT systems.

</details>


### [198] [Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations](https://arxiv.org/abs/2508.12571)
*Tyler Schroder,Renee Sirbu,Sohee Park,Jessica Morley,Sam Street,Luciano Floridi*

Main category: cs.CR

TL;DR: 본 논문은 뇌-컴퓨터 인터페이스(BCI)의 개인화된 의료 기술 발전 가능성에 대해 논의하며, 보안 위험과 사이버 공격에 대한 대응 방안을 제안한다.


<details>
  <summary>Details</summary>
Motivation: BCI의 발전이 개인화된 의료에 큰 가능성을 제공하지만 보안 취약점도 제기한다는 점.

Method: BCI 제조업체에게 장치 보안을 강화할 수 있는 권고사항을 제안하고, 규제기관에 환자의 안전과 데이터 기밀성을 보호하는 데 필요한 지침을 제시한다.

Result: 제안된 방안을 통해 BCI 사용자들의 개인 건강 및 유전 정보 유출, 의도치 않은 BCI 매개 동작 등 여러 사이버 공격으로부터의 위험을 경감할 수 있다.

Conclusion: BCI는 물리적 공격 위험은 낮지만 원거리 공격에 취약하므로, 네트워크 연결을 최소화하고 사이버 보안 위협을 관리할 필요가 있다.

Abstract: Brain-computer interfaces (BCIs) show enormous potential for advancing
personalized medicine. However, BCIs also introduce new avenues for
cyber-attacks or security compromises. In this article, we analyze the problem
and make recommendations for device manufacturers to better secure devices and
to help regulators understand where more guidance is needed to protect patient
safety and data confidentiality. Device manufacturers should implement the
prior suggestions in their BCI products. These recommendations help protect BCI
users from undue risks, including compromised personal health and genetic
information, unintended BCI-mediated movement, and many other cybersecurity
breaches. Regulators should mandate non-surgical device update methods, strong
authentication and authorization schemes for BCI software modifications,
encryption of data moving to and from the brain, and minimize network
connectivity where possible. We also design a hypothetical, average-case threat
model that identifies possible cybersecurity threats to BCI patients and
predicts the likeliness of risk for each category of threat. BCIs are at less
risk of physical compromise or attack, but are vulnerable to remote attack; we
focus on possible threats via network paths to BCIs and suggest technical
controls to limit network connections.

</details>


### [199] [Reducing False Positives with Active Behavioral Analysis for Cloud Security](https://arxiv.org/abs/2508.12584)
*Dikshant,Verma*

Main category: cs.CR

TL;DR: 이 논문은 클라우드 보안 태세 관리에서 정책 위반의 악용 가능성을 실시간으로 평가하기 위해 동적 행동 테스트를 통합한 검증 기반 방법론을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 규칙 기반 CSPM 솔루션은 맥락 이해가 제한적이고 정적 휴리스틱 테스트에 의존하여 많은 거짓 긍정 결과를 초래한다.

Method: 제안된 시스템은 오픈 소스 도구, 검증 스크립트 및 침투 테스트 사례로 구축된 경량화되고 자동화된 프로브를 사용하여 잘못 구성되거나 취약한 클라우드 자산에 대한 적대적 공격을 시뮬레이션한다.

Result: 우리의 실험 결과는 다양한 잘못 구성 및 취약 경고에서 거짓 긍정 비율이 평균 93% 감소했음을 나타낸다.

Conclusion: 이 프레임워크는 대규모 클라우드 환경에서 탐지 정확성과 분석가 생산성을 향상시키기 위한 확장 가능한 방법을 보여준다.

Abstract: Rule-based cloud security posture management (CSPM) solutions are known to
produce a lot of false positives based on the limited contextual understanding
and dependence on static heuristics testing. This paper introduces a
validation-driven methodology that integrates active behavioral testing in
cloud security posture management solution(s) to evaluate the exploitability of
policy violations in real time. The proposed system employs lightweight and
automated probes, built from open-source tools, validation scripts, and
penetration testing test cases, to simulate adversarial attacks on
misconfigured or vulnerable cloud assets without any impact to the cloud
services or environment. For instance, cloud services may be flagged as
publicly exposed and vulnerable despite being protected by access control
layers, or secure policies, resulting in non-actionable alerts that consumes
analysts time during manual validation. Through controlled experimentation in a
reproducible AWS setup, we evaluated the reduction in false positive rates
across various misconfiguration and vulnerable alerts. Our findings indicate an
average reduction of 93\% in false positives. Furthermore, the framework
demonstrates low latency performance. These results demonstrate a scalable
method to improve detection accuracy and analyst productivity in large cloud
environments. While our evaluation focuses on AWS, the architecture is modular
and extensible to multi-cloud setups.

</details>


### [200] [UAV Individual Identification via Distilled RF Fingerprints-Based LLM in ISAC Networks](https://arxiv.org/abs/2508.12597)
*Haolin Zheng,Ning Gao,Donghong Cai,Shi Jin,Michail Matthaiou*

Main category: cs.CR

TL;DR: 본 논문에서는 저고도 통합 감지 및 통신 네트워크에서 UAV ID 식별을 위한 새로운 RFF-LLM 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: UAV의 ID 식별은 보안 감시 전략으로 중요하며, 복잡한 환경에서의 높은 식별 정확도가 요구됩니다.

Method: 이 논문에서는 수정된 GPT-2 모델을 기반으로 한 RFF-LLM 프레임워크와 동적인 지식 증류 전략을 통해 모델을 압축하여 성능을 향상시킵니다.

Result: 제안한 프레임워크는 20개의 상용 UAV의 I/Q 신호를 수집하여 98.38%의 ID 식별 정확도를 달성합니다.

Conclusion: 결과적으로, 이 연구는 기존 기준값을 초과하는 성능을 입증하였습니다.

Abstract: Unmanned aerial vehicle (UAV) individual (ID) identification is a critical
security surveillance strategy in low-altitude integrated sensing and
communication (ISAC) networks. In this paper, we propose a novel dynamic
knowledge distillation (KD)-enabled wireless radio frequency fingerprint large
language model (RFF-LLM) framework for UAV ID identification. First, we propose
an RFF-LLM framework based on the modified GPT-2 model to improve the
identification accuracy in complex outdoor environments. Then, considering the
parameter overhead of the RFF-LLM, we design a dynamic KD strategy to compress
the model. Specifically, the proximal policy optimization (PPO) algorithm is
employed to dynamically adjust the distillation temperature, overcoming the
local optimum dilemma inherent in static KD. As a next step, the knowledge of
the RFF-LLM is adequately transferred to the lightweight Lite-HRNet model.
Finally, our experiments are conducted based on the self-built drone RFF
dataset of Release one, namely DRFF-R1, by collecting the I/Q signals of 20
commercial UAVs in channel 149. The experiment results show that the proposed
framework achieves 98.38\% ID identification accuracy with merely 0.15 million
parameters and 2.74 ms response time, which outperforms the benchmarks.

</details>


### [201] [Consiglieres in the Shadow: Understanding the Use of Uncensored Large Language Models in Cybercrimes](https://arxiv.org/abs/2508.12622)
*Zilong Lin,Zichuan Li,Xiaojing Liao,XiaoFeng Wang*

Main category: cs.CR

TL;DR: 이 논문은 개방형 대규모 언어 모델(ULLMs)의 체계적인 연구를 통해 사이버 범죄자들이 이 기술을 악용하는 방식과 그 우려를 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 기술의 발전, 특히 대규모 언어 모델(LLMs)의 발전으로 인해 새로운 보안 및 프라이버시 위험이 도입되고 있다.

Method: 개방형 LLM 간의 관계와 관련 데이터(파인 튜닝, 모델 병합, 압축 및 해로운 콘텐츠가 있는 데이터셋 사용 또는 생성 등)의 관계를 모델링하여 ULLMs를 식별하였다. 이러한 연결을 지식 그래프로 표현하고 그래프 기반 딥러닝을 적용하여 11,000개 이상의 ULLMs를 발견하였다.

Result: ULLMs 중 일부는 백만 번 이상 다운로드되었으며, 하나는 1,900만 번 이상의 설치가 이루어졌다. 이 모델들은 혐오 발언, 폭력, 음란물 및 악성 코드를 포함한 해로운 콘텐츠를 생성할 수 있다.

Conclusion: 이 연구는 LLM 기술의 광범위한 악용 사례를 강조하며, 성장하는 위협에 대한 효과적인 대응책이 시급함을 보여준다.

Abstract: The advancement of AI technologies, particularly Large Language Models
(LLMs), has transformed computing while introducing new security and privacy
risks. Prior research shows that cybercriminals are increasingly leveraging
uncensored LLMs (ULLMs) as backends for malicious services. Understanding these
ULLMs has been hindered by the challenge of identifying them among the vast
number of open-source LLMs hosted on platforms like Hugging Face. In this
paper, we present the first systematic study of ULLMs, overcoming this
challenge by modeling relationships among open-source LLMs and between them and
related data, such as fine-tuning, merging, compressing models, and using or
generating datasets with harmful content. Representing these connections as a
knowledge graph, we applied graph-based deep learning to discover over 11,000
ULLMs from a small set of labeled examples and uncensored datasets.
  A closer analysis of these ULLMs reveals their alarming scale and usage. Some
have been downloaded over a million times, with one over 19 million installs.
These models -- created through fine-tuning, merging, or compression of other
models -- are capable of generating harmful content, including hate speech,
violence, erotic material, and malicious code. Evidence shows their integration
into hundreds of malicious applications offering services like erotic
role-play, child pornography, malicious code generation, and more. In addition,
underground forums reveal criminals sharing techniques and scripts to build
cheap alternatives to commercial malicious LLMs. These findings highlight the
widespread abuse of LLM technology and the urgent need for effective
countermeasures against this growing threat.

</details>


### [202] [MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection](https://arxiv.org/abs/2508.12641)
*Yasaman Samadi,Hai Dong,Xiaoyu Xia*

Main category: cs.CR

TL;DR: 본 논문에서는 암호화폐 거래에서의 다양한 세탁 패턴을 효과적으로 탐지하기 위해 MPOCryptoML 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 암호화폐 자금 세탁 탐지 모델은 다양한 오프체인 자금 세탁 패턴을 발견하도록 설계되지 않았습니다.

Method: MPOCryptoML 모델은 여러 소스의 개인화된 페이지랭크 알고리즘을 개발하여 임의의 세탁 패턴을 식별하며, 거래의 타임스탬프와 가중치를 분석하여 다양한 세탁 구조를 탐지하는 두 가지 새로운 알고리즘을 도입합니다.

Result: MPOCryptoML은 많은 공공 데이터셋에 대한 실험을 통해 정확도, 재현율, F1 점수, 정확도에서 개선된 성능을 보여줍니다.

Conclusion: MPOCryptoML은 자금 세탁 탐지의 효율성과 효과성을 입증하며, 위험 계정을 체계적으로 식별할 수 있는 능력을 가집니다.

Abstract: Recent advancements in money laundering detection have demonstrated the
potential of using graph neural networks to capture laundering patterns
accurately. However, existing models are not explicitly designed to detect the
diverse patterns of off-chain cryptocurrency money laundering. Neglecting any
laundering pattern introduces critical detection gaps, as each pattern reflects
unique transactional structures that facilitate the obfuscation of illicit fund
origins and movements. Failure to account for these patterns may result in
under-detection or omission of specific laundering activities, diminishing
model accuracy and allowing schemes to bypass detection. To address this gap,
we propose the MPOCryptoML model to effectively detect multiple laundering
patterns in cryptocurrency transactions. MPOCryptoML includes the development
of a multi-source Personalized PageRank algorithm to identify random laundering
patterns. Additionally, we introduce two novel algorithms by analyzing the
timestamp and weight of transactions in high-volume financial networks to
detect various money laundering structures, including fan-in, fan-out,
bipartite, gather-scatter, and stack patterns. We further examine correlations
between these patterns using a logistic regression model. An anomaly score
function integrates results from each module to rank accounts by anomaly score,
systematically identifying high-risk accounts. Extensive experiments on public
datasets including Elliptic++, Ethereum fraud detection, and Wormhole
transaction datasets validate the efficacy and efficiency of MPOCryptoML.
Results show consistent performance gains, with improvements up to 9.13% in
precision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% in
accuracy.

</details>


### [203] [Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods](https://arxiv.org/abs/2508.12730)
*Jaeung Lee,Suhyeon Yu,Yurim Jang,Simon S. Woo,Jaemin Jo*

Main category: cs.CR

TL;DR: 기계 언러닝(MU)은 훈련된 모델에서 특정 훈련 데이터를 제거하여 해당 데이터가 더 이상 모델의 행동에 영향을 미치지 않도록 하는 것을 목표로 하며, 데이터 프라이버시 법에 따른 '잊힐 권리' 의무를 이행한다. 그러나, 연구자들은 MU 방법의 행동을 분석하고 이해하는 데 어려움을 겪고 있으며, 정확성, 효율성, 프라이버시라는 세 가지 기본 원칙에서 특히 그러하다. 이를 해결하기 위해 우리는 MU 방법의 체계적 평가를 용이하게 하는 비주얼 분석 시스템인 Unlearning Comparator를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기계 언러닝(MU)에서 다양한 방법의 행동을 분석하고 이해하는 데 어려움이 있음.

Method: Unlearning Comparator라는 비주얼 분석 시스템을 설계하여 MU 방법의 체계적 평가를 지원.

Result: 모델 비교 및 공격 시뮬레이션을 통해 MU 방법의 평가 과정에서 두 가지 중요한 작업을 지원하고, 사용자가 모델의 행동을 이해하고 MU 방법 개선에 대한 통찰을 얻을 수 있도록 함.

Conclusion: 시스템이 사용자가 모델 행동을 이해하고 MU 방법을 개선하는 데 도움이 됨을 보여주었다.

Abstract: Machine Unlearning (MU) aims to remove target training data from a trained
model so that the removed data no longer influences the model's behavior,
fulfilling "right to be forgotten" obligations under data privacy laws. Yet, we
observe that researchers in this rapidly emerging field face challenges in
analyzing and understanding the behavior of different MU methods, especially in
terms of three fundamental principles in MU: accuracy, efficiency, and privacy.
Consequently, they often rely on aggregate metrics and ad-hoc evaluations,
making it difficult to accurately assess the trade-offs between methods. To
fill this gap, we introduce a visual analytics system, Unlearning Comparator,
designed to facilitate the systematic evaluation of MU methods. Our system
supports two important tasks in the evaluation process: model comparison and
attack simulation. First, it allows the user to compare the behaviors of two
models, such as a model generated by a certain method and a retrained baseline,
at class-, instance-, and layer-levels to better understand the changes made
after unlearning. Second, our system simulates membership inference attacks
(MIAs) to evaluate the privacy of a method, where an attacker attempts to
determine whether specific data samples were part of the original training set.
We evaluate our system through a case study visually analyzing prominent MU
methods and demonstrate that it helps the user not only understand model
behaviors but also gain insights that can inform the improvement of MU methods.

</details>


### [204] [Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds](https://arxiv.org/abs/2508.12832)
*Jinyu Lu,Xinrong Sun,Yunting Tao,Tong Ji,Fanyu Kong,Guoqiang Yang*

Main category: cs.CR

TL;DR: 이 논문은 CNN의 합성곱 계층을 위한 새로운 검증 가능한 프라이버시 보호 방안을 제안하며, 비신뢰성 클라우드 서버에 안전하게 계산을 오프로드할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: CNN의 광범위한 채택은 머신 러닝 서비스(MLaaS) 시스템의 발전을 이끌었으나, 클라이언트에서 비신뢰 클라우드 서버로 전송되는 데이터로 인한 프라이버시 침해의 위험이 있다.

Method: 본 논문에서는 CNN 합성곱 계층에 맞춘 새로운 검증 가능한 프라이버시 보호 스킴을 제안하며, 이 스킴은 효율적인 암호화 및 복호화를 가능하게 한다.

Result: 실험 결과, 제안된 스킴은 원래의 평문 모델에 비해 $26 	imes$ ~ $87	imes$ 속도 향상을 달성하며, 정확도를 유지한다.

Conclusion: 제안된 검증 메커니즘은 결과의 정확성을 최소한의 성공 확률로 감지할 수 있다.

Abstract: The widespread adoption of convolutional neural networks (CNNs) in
resource-constrained scenarios has driven the development of Machine Learning
as a Service (MLaaS) system. However, this approach is susceptible to privacy
leakage, as the data sent from the client to the untrusted cloud server often
contains sensitive information. Existing CNN privacy-preserving schemes, while
effective in ensuring data confidentiality through homomorphic encryption and
secret sharing, face efficiency bottlenecks, particularly in convolution
operations. In this paper, we propose a novel verifiable privacy-preserving
scheme tailored for CNN convolutional layers. Our scheme enables efficient
encryption and decryption, allowing resource-constrained clients to securely
offload computations to the untrusted cloud server. Additionally, we present a
verification mechanism capable of detecting the correctness of the results with
a success probability of at least $1-\frac{1}{\left|Z\right|}$. Extensive
experiments conducted on 10 datasets and various CNN models demonstrate that
our scheme achieves speedups ranging $26 \times$ ~ $\ 87\times$ compared to the
original plaintext model while maintaining accuracy.

</details>


### [205] [The covering radius of Butson Hadamard codes for the homogeneous metric](https://arxiv.org/abs/2508.12859)
*Xingxing Xu,Minjia Shi,Patrick Sole*

Main category: cs.CR

TL;DR: 본 논문에서는 Quasi Frobenius의 통계적 방정식에 대해 다루며, Butson Hadamard 코드의 커버링 반경에 대한 상한과 하한을 도출합니다.


<details>
  <summary>Details</summary>
Motivation: Butson 매트릭스와 관련된 코드의 연구는 이 매트릭스의 성질을 더 깊이 이해하고 이를 통해 새로운 응용 프로그램을 개발하는 데 목적이 있습니다.

Method: 동질적 메트릭에 대해 Butson Hadamard 코드의 커버링 반경을 연구하고, 직교 배열 인수를 사용하여 상한을 도출하며, Shi et al. (2022)의 벤트 시퀀스의 존재에 기초하여 하한을 설정합니다.

Result: 직교 배열 인수를 사용하여 도출한 상한과 벤트 시퀀스의 존재에 기반한 하한을 제시합니다.

Conclusion: 본 연구는 커버링 반경을 이해하는 데 중요한 기여를 하며, Hamming 메트릭에 대한 기존의 경계를 일반화합니다.

Abstract: Butson matrices are complex Hadamard matrices with entries in the complex
roots of unity of given order. There is an interesting code in phase space
related to this matrix (Armario et al. 2023). We study the covering radius of
Butson Hadamard codes for the homogeneous metric, a metric defined uniquely, up
to scaling, for a commutative ring alphabet that is Quasi Frobenius. An upper
bound is derived by an orthogonal array argument. A lower bound relies on the
existence of bent sequences in the sense of (Shi et al. 2022). This latter
bound generalizes a bound of (Armario et al. 2025) for the Hamming metric.

</details>


### [206] [Supporting Socially Constrained Private Communications with SecureWhispers](https://arxiv.org/abs/2508.12870)
*Vinod Khandkar,Kieron Ivy Turk,Ehsan Toreini,Nishanth Sastry*

Main category: cs.CR

TL;DR: 본 논문은 두 개 이상의 장치 간에 공유 비밀을 설정하여 개인 정보를 안전하게 통신할 수 있는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사회적 환경에 따라 민감한 주제를 논의하는 것이 제한되어 있어 사람들이 안전한 개인 통신 수단을 필요로 한다.

Method: 두 개의 전화기를 함께 흔들어 무선 네트워크를 통해 데이터를 전송하지 않고 공유 비밀을 생성하는 방법을 개발하였다.

Result: 생성된 공유 비밀을 활용한 세 가지 응용 프로그램을 제안하였으며, 안드로이드에서 메시지 난독화를 독립적인 앱으로 구현하였다.

Conclusion: 이 도구들의 사용성, 디자인 고려사항 및 주류 서비스와의 통합에 대한 연구 결과를 제시하였다.

Abstract: Rapidly changing social norms and national, legal, and political conditions
socially constrain people from discussing sensitive topics such as sexuality or
religion. Such constrained, vulnerable minorities are often worried about
inadvertent information disclosure and may be unsure about the extent to which
their communications are being monitored in public or semi-public spaces like
workplaces or cafes. Personal devices extend trust to the digital domain,
making it desirable to have strictly private communication between trusted
devices. Currently, messaging services like WhatsApp provide alternative means
for exchanging sensitive private information, while personal safety apps such
as Noonlight enable private signaling. However, these rely on third-party
mechanisms for secure and private communication, which may not be accessible
for justifiable reasons, such as insecure internet access or companion device
connections. In these cases, it is challenging to achieve communication that is
strictly private between two devices instead of user accounts without any
dependency on third-party infrastructure. The goal of this paper is to support
private communications by setting up a shared secret between two or more
devices without sending any data on the network. We develop a method to create
a shared secret between phones by shaking them together. Each device extracts
the shared randomness from the shake, then conditions the randomness to 7.798
bits per byte of key material. This paper proposes three different applications
of this generated shared secret: message obfuscation, trust delegation, and
encrypted beacons. We have implemented the message obfuscation on Android as an
independent app that can be used for private communication with trusted
contacts. We also present research on the usability, design considerations, and
further integration of these tools in mainstream services.

</details>


### [207] [SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip](https://arxiv.org/abs/2508.12910)
*Ziteng Hu,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: SecFSM은 보안 중심 지식 그래프를 활용하여 더 안전한 Verilog 코드 생성을 돕는 새로운 방법이다.


<details>
  <summary>Details</summary>
Motivation: Verilog 코드 생성을 자동화할 수 있는 LLM의 발전으로 인해 FSM 구현에 있어 보안 취약점이 우려되고 있다.

Method: FSM 보안 지식 그래프(FSKG)를 구축하고, 사용자 요구사항 분석을 통해 취약점을 식별한 후, FSKG에서 지식을 검색하여 Verilog 코드 생성을 위한 보안 프롬프트를 구성한다.

Result: SecFSM은 학술 데이터셋, 인공 데이터셋, 논문, 산업 사례에서 수집한 전용 데이터셋을 기반으로 Extensive 실험을 진행하여 최첨단 기준보다 우수한 성과를 보였다.

Conclusion: 25개의 DeepSeek-R1으로 평가된 보안 테스트 케이스에서 SecFSM은 21/25의 뛰어난 합격률을 기록했다.

Abstract: Finite State Machines (FSMs) play a critical role in implementing control
logic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by
hardware engineers through Verilog coding, which is often tedious and
time-consuming. Recently, with the remarkable progress of Large Language Models
(LLMs) in code generation, LLMs have been increasingly explored for automating
Verilog code generation. However, LLM-generated Verilog code often suffers from
security vulnerabilities, which is particularly concerning for
security-sensitive FSM implementations. To address this issue, we propose
SecFSM, a novel method that leverages a security-oriented knowledge graph to
guide LLMs in generating more secure Verilog code. Specifically, we first
construct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.
Subsequently, we analyze users' requirements to identify vulnerabilities and
get a list of vulnerabilities in the requirements. Then, we retrieve knowledge
from FSKG based on the vulnerabilities list. Finally, we construct security
prompts based on the security knowledge for Verilog code generation. To
evaluate SecFSM, we build a dedicated dataset collected from academic datasets,
artificial datasets, papers, and industrial cases. Extensive experiments
demonstrate that SecFSM outperforms state-of-the-art baselines. In particular,
on a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM
achieves an outstanding pass rate of 21/25.

</details>


### [208] [Prescriptive Zero Trust- Assessing the impact of zero trust on cyber attack prevention](https://arxiv.org/abs/2508.12953)
*Samuel Aiello*

Main category: cs.CR

TL;DR: 본 연구는 제로 트러스트 아키텍처(ZTA)의 구현과 관련하여 사이버 보안 성숙도를 측정하기 위한 정량적인 가이드라인의 가능성을 평가하는 데이터 기반 방법론을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 위협의 증가로 인해 기업의 보안 태세를 지속적으로 개선해야 할 필요성이 커지고 있습니다.

Method: 본 연구는 ZTA 구현과 관련하여 사이버 보안 성숙도를 정량화하는 새로운 데이터 기반 방법론을 개발하고, 각 제어 구성 요소의 통합 깊이를 평가하여 산업 모범 사례에 맞춰 조직의 ZTA 성숙 수준을 평가합니다.

Result: 연구 실험 결과, 신원 확인, 마이크로 세그먼트화, 데이터 암호화, 분석 및 오케스트레이션에 걸쳐 포괄적인 ZTA 배치를 특징짓는 주요 기술 통제의 처방 세트를 정의하였습니다.

Conclusion: 이 연구는 조직의 보안 전환 여정에서 각 단계가 이전 단계의 능력을 추가하는 네 가지 단계 모델을 도출하였으며, 이는 조직의 보안 강화를 위한 로드맵 역할을 합니다.

Abstract: Increasingly sophisticated and varied cyber threats necessitate ever
improving enterprise security postures. For many organizations today, those
postures have a foundation in the Zero Trust Architecture. This strategy sees
trust as something an enterprise must not give lightly or assume too broadly.
Understanding the ZTA and its numerous controls centered around the idea of not
trusting anything inside or outside the network without verification, will
allow organizations to comprehend and leverage this increasingly common
paradigm. The ZTA, unlike many other regulatory frameworks, is not tightly
defined. The research assesses the likelihood of quantifiable guidelines that
measure cybersecurity maturity for an enterprise organization in relation to
ZTA implementation. This is a new, data driven methodology for quantifying
cyber resilience enabled by the adoption of Zero Trust principles to
pragmatically address the critical need of organizations. It also looks at the
practical aspects ZTA has on capabilities in deterring cyberattacks on a
network. The outcomes of this research define a prescriptive set of key
technical controls across identity verification, microsegmentation, data
encryption, analytics, and orchestration that characterize the comprehensive
ZTA deployment. By evaluating the depth of integration for each control
component and aligning to industry best practices, the study's results help
assess an organization's ZTA maturity level on a scale from Initial to
Optimized adoption. The research's resultant four tier model demarcates phases
for an organization on its security transformation journey, with each tier
adding to the capability of the last.

</details>


### [209] [AuthenTree: A Scalable MPC-Based Distributed Trust Architecture for Chiplet-based Heterogeneous Systems](https://arxiv.org/abs/2508.13033)
*Ishraq Tashdid,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.CR

TL;DR: AuthenTree는 분산 인증 프레임워크로, 모듈화 및 확장 가능한 반도체 설계를 통해 AI 및 고성능 컴퓨팅을 위한 신뢰할 수 있는 솔루션을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 반도체 설계의 변화와 함께 다수의 공급업체가 통합된 조립 환경에서의 보안 위협을 해결하려는 필요성이 있습니다.

Method: AuthenTree는 트리 기반 아키텍처에서 다자간 계산(MPC)을 활용하여 중앙 집중식 신뢰 없이 안전한 칩렛 검증을 가능하게 합니다.

Result: AuthenTree는 최소한의 오버헤드로 안전한 인증을 제공하며, 이전 연구에 비해 성능이 최대 700배 향상되었습니다.

Conclusion: AuthenTree는 제로 트러스트 SiP 환경에서 차세대 칩렛 기반 보안을 위한 효율적이고 견고하며 확장 가능한 솔루션으로 자리잡았습니다.

Abstract: The rapid adoption of chiplet-based heterogeneous integration is reshaping
semiconductor design by enabling modular, scalable, and faster time-to-market
solutions for AI and high-performance computing. However, multi-vendor assembly
in post-fabrication environments fragments the supply chain and exposes SiP
systems to serious security threats, including cloning, overproduction, and
chiplet substitution. Existing authentication solutions depend on trusted
integrators or centralized security anchors, which can expose sensitive data or
create single points of failure. We introduce AuthenTree, a distributed
authentication framework that leverages multi-party computation (MPC) in a
scalable tree-based architecture, removing the need for dedicated security
hardware or centralized trust. AuthenTree enables secure chiplet validation
without revealing raw signatures, distributing trust across multiple integrator
chiplets. Our evaluation in five SiP benchmarks demonstrates that AuthenTree
imposes minimal overhead, with an area as low as 0.48% (7,000 sq-micrometers),
an overhead power under 0.5%, and an authentication latency below 1
microsecond, surpassing previous work in some cases by 700 times. These results
establish AuthenTree as an efficient, robust, and scalable solution for
next-generation chiplet-based security in zero-trust SiP environments.

</details>


### [210] [MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of Diverse Innovative Strategies](https://arxiv.org/abs/2508.13048)
*Weiwei Qi,Shuo Shao,Wei Gu,Tianhang Zheng,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 이 논문에서는 MAJIC라는 새로운 적응형 해킹 프레임워크를 제안하여 기존의 고정 프롬프트 기반 해킹 방법을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 블랙박스 해킹 기술은 적응력이 떨어지고 일반화에 한계가 있어 이를 개선하고자 함.

Method: MAJIC는 다양한 변 disguise전략을 결합하여 블랙박스 LLM을 공격하며, 이를 마르코프 체인으로 모델링합니다.

Result: MAJIC는 GPT-4o 및 Gemini-2.0-flash와 같은 주요 모델에서 기존 해킹 방법보다 상당한 성능 향상을 보여주며, 평균 15쿼리 이하로 90% 이상의 공격 성공률을 기록함.

Conclusion: MAJIC는 기존 방법에 비해 더 효과적이고 효율적인 해킹 경로를 발견하고 학습함.

Abstract: Large Language Models (LLMs) have exhibited remarkable capabilities but
remain vulnerable to jailbreaking attacks, which can elicit harmful content
from the models by manipulating the input prompts. Existing black-box
jailbreaking techniques primarily rely on static prompts crafted with a single,
non-adaptive strategy, or employ rigid combinations of several underperforming
attack methods, which limits their adaptability and generalization. To address
these limitations, we propose MAJIC, a Markovian adaptive jailbreaking
framework that attacks black-box LLMs by iteratively combining diverse
innovative disguise strategies. MAJIC first establishes a ``Disguise Strategy
Pool'' by refining existing strategies and introducing several innovative
approaches. To further improve the attack performance and efficiency, MAJIC
formulate the sequential selection and fusion of strategies in the pool as a
Markov chain. Under this formulation, MAJIC initializes and employs a Markov
matrix to guide the strategy composition, where transition probabilities
between strategies are dynamically adapted based on attack outcomes, thereby
enabling MAJIC to learn and discover effective attack pathways tailored to the
target model. Our empirical results demonstrate that MAJIC significantly
outperforms existing jailbreak methods on prominent models such as GPT-4o and
Gemini-2.0-flash, achieving over 90\% attack success rate with fewer than 15
queries per attempt on average.

</details>


### [211] [VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog](https://arxiv.org/abs/2508.13092)
*Xiang Long,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: 이 논문에서는 Verilog 취약점을 탐지하기 위한 최초의 LLM 보조 그래프 탐색 규칙 생성 접근법인 VerilogLAVD를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 하드웨어 취약점의 조기 발견은 수정 비용을 절감하는 데 중요하지만, 기존 기술은 전문 보안 지식을 요구하여 사용성을 제한합니다.

Method: 제안된 방법은 Verilog 코드의 통합 표현인 Verilog Property Graph(VeriPG)를 도입하고, 정적 트리에서 추출한 구문적 특성과 제어 흐름 및 데이터 종속 그래프에서 파생된 의미적 정보를 결합하여 LLM을 활용하여 CWE 설명으로부터 VeriPG 기반 탐지 규칙을 생성합니다.

Result: 77개의 Verilog 설계를 포함한 실증적 평가에서 VerilogLAVD는 F1 점수 0.54를 달성하며, LLM만을 사용한 경우와 외부 지식을 통한 LLM과 비교하여 각각 0.31, 0.27의 점수를 개선했습니다.

Conclusion: 이 연구는 Verilog 취약점 탐지를 위해 LLM과 그래프 탐색 규칙 생성을 결합한 새로운 접근 방식을 제시하여, 설계 초기 단계에서의 하드웨어 취약점 검출 가능성을 높입니다.

Abstract: Timely detection of hardware vulnerabilities during the early design stage is
critical for reducing remediation costs. Existing early detection techniques
often require specialized security expertise, limiting their usability. Recent
efforts have explored the use of large language models (LLMs) for Verilog
vulnerability detection. However, LLMs struggle to capture the structure in
Verilog code, resulting in inconsistent detection results. To this end, we
propose VerilogLAVD, the first LLM-aided graph traversal rule generation
approach for Verilog vulnerability detection. Our approach introduces the
Verilog Property Graph (VeriPG), a unified representation of Verilog code. It
combines syntactic features extracted from the abstract syntax tree (AST) with
semantic information derived from control flow and data dependency graphs. We
leverage LLMs to generate VeriPG-based detection rules from Common Weakness
Enumeration (CWE) descriptions. These rules guide the rule executor that
traversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we
build a dataset collected from open-source repositories and synthesized data.
In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,
VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with
external knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,
respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [212] [Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.11706)
*Zhuofan Xu,Benedikt Bollig,Matthias Függer,Thomas Nowak,Vincent Le Dréau*

Main category: cs.MA

TL;DR: CPE 학습은 중앙집중형 정책을 사용하여 다중 에이전트 강화 학습의 제약을 극복하고 성능을 향상시키는 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: CTDE 패러다임의 한계인 부분 관측 및 중앙집중 정책에 비해 효과가 떨어지는 점을 보완하기 위해.

Method: CPE 학습은 경량의 글로벌-로컬 순열 공변(GLPE) 네트워크 아키텍처를 도입하여 중앙집중형 훈련 및 실행을 수행한다.

Result: CPE는 값 분해 및 액터-크리틱 방법과 매끄럽게 통합되며, MPE, SMAC, RWARE와 같은 협동 벤치마크에서 성능을 상당히 향상시킨다.

Conclusion: CPE는 최신 RWARE 구현과 동등한 성능을 발휘하며, CTDE 알고리즘의 성능 개선에 기여한다.

Abstract: The Centralized Training with Decentralized Execution (CTDE) paradigm has
gained significant attention in multi-agent reinforcement learning (MARL) and
is the foundation of many recent algorithms. However, decentralized policies
operate under partial observability and often yield suboptimal performance
compared to centralized policies, while fully centralized approaches typically
face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized
training and execution framework that employs a fully centralized policy to
overcome these limitations. Our approach leverages a novel permutation
equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks,
that is lightweight, scalable, and easy to implement. Experiments show that CPE
integrates seamlessly with both value decomposition and actor-critic methods,
substantially improving the performance of standard CTDE algorithms across
cooperative benchmarks including MPE, SMAC, and RWARE, and matching the
performance of state-of-the-art RWARE implementations.

</details>


### [213] [SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication](https://arxiv.org/abs/2508.11733)
*Ruijia Zhang,Xinyan Zhao,Ruixiang Wang,Sigen Chen,Guibin Zhang,An Zhang,Kun Wang,Qingsong Wen*

Main category: cs.MA

TL;DR: SafeSieve는 LLM 기반 다중 에이전트 시스템의 통신을 효율적으로 개선하는 진보적이고 적응적인 가지치기 알고리즘입니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 다중 에이전트 시스템은 협력 능력이 뛰어나지만 중복 통신과 과도한 토큰 오버헤드로 어려움을 겪고 있습니다.

Method: SafeSieve는 새로운 이중 메커니즘을 통해 에이전트 간 통신을 동적으로 정제하는 다중 에이전트 가지치기 알고리즘입니다.

Result: SafeSieve는 실험을 통해 94.01%의 평균 정확도와 12.4%-27.8%의 토큰 사용 감소를 달성했습니다.

Conclusion: SafeSieve는 실질적인 다중 에이전트 시스템을 위한 강력하고 효율적이며 확장 가능한 프레임워크로 자리잡았습니다.

Abstract: LLM-based multi-agent systems exhibit strong collaborative capabilities but
often suffer from redundant communication and excessive token overhead.
Existing methods typically enhance efficiency through pretrained GNNs or greedy
algorithms, but often isolate pre- and post-task optimization, lacking a
unified strategy. To this end, we present SafeSieve, a progressive and adaptive
multi-agent pruning algorithm that dynamically refines the inter-agent
communication through a novel dual-mechanism. SafeSieve integrates initial
LLM-based semantic evaluation with accumulated performance feedback, enabling a
smooth transition from heuristic initialization to experience-driven
refinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs
0-extension clustering to preserve structurally coherent agent groups while
eliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval,
etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing
token usage by 12.4%-27.8%. Results further demonstrate robustness under prompt
injection attacks (1.23% average accuracy drop). In heterogeneous settings,
SafeSieve reduces deployment costs by 13.3% while maintaining performance.
These results establish SafeSieve as a robust, efficient, and scalable
framework for practical multi-agent systems. Our code can be found in
https://anonymous.4open.science/r/SafeSieve-D8F2FFUN.

</details>


### [214] [A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond](https://arxiv.org/abs/2508.11957)
*Xiaodong Qu,Andrews Damoah,Joshua Sherwood,Peiyan Liu,Christian Shun Jin,Lulu Chen,Minjie Shen,Nawwaf Aleisa,Zeyuan Hou,Chenyu Zhang,Lifu Gao,Yanshu Li,Qikai Yang,Qun Wang,Cristabelle De Souza*

Main category: cs.MA

TL;DR: 이 리뷰에서는 현대 인공지능 에이전트의 구조적 원칙, 기본 구성 요소 및 새로운 패러다임을 체계적으로 검토합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 통합 설계 및 배포는 여전히 큰 도전 과제입니다.

Method: 인지 과학에서 영감을 받은 모델, 계층적 강화 학습 프레임워크 및 대규모 언어 모델 기반 추론 통찰을 종합합니다.

Result: 주요 혁신, 지속적인 도전 과제 및 유망한 연구 방향을 강조합니다.

Conclusion: 이 리뷰는 보다 강력하고 적응 가능하며 신뢰할 수 있는 자율 인텔리전스로 향하는 AI 에이전트 시스템의 다음 세대를 안내하는 것을 목표로 합니다.

Abstract: Artificial Intelligence (AI) agents have rapidly evolved from specialized,
rule-based programs to versatile, learning-driven autonomous systems capable of
perception, reasoning, and action in complex environments. The explosion of
data, advances in deep learning, reinforcement learning, and multi-agent
coordination have accelerated this transformation. Yet, designing and deploying
unified AI agents that seamlessly integrate cognition, planning, and
interaction remains a grand challenge. In this review, we systematically
examine the architectural principles, foundational components, and emergent
paradigms that define the landscape of contemporary AI agents. We synthesize
insights from cognitive science-inspired models, hierarchical reinforcement
learning frameworks, and large language model-based reasoning. Moreover, we
discuss the pressing ethical, safety, and interpretability concerns associated
with deploying these agents in real-world scenarios. By highlighting major
breakthroughs, persistent challenges, and promising research directions, this
review aims to guide the next generation of AI agent systems toward more
robust, adaptable, and trustworthy autonomous intelligence.

</details>


### [215] [Synchronization Dynamics of Heterogeneous, Collaborative Multi-Agent AI Systems](https://arxiv.org/abs/2508.12314)
*Chiranjit Mitra*

Main category: cs.MA

TL;DR: Kuramoto 모델을 적용한 새로운 다학제적 프레임워크를 소개하여 복잡한 작업을 수행하는 이질적인 AI 에이전트의 집단 역학을 설명한다. 이 모델은 에이전트의 전문화, 영향 및 통신을 포착하고, 협동의 정도를 정량화하는 오더 파라미터를 도입하며, 에이전트 다양성과 네트워크 구조가 집단 행동에 미치는 영향을 조사한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 집단 역학을 연구하고 이를 통해 더 나은 협력적 AI 시스템을 설계하고 최적화하기 위해.

Method: Kuramoto 모델을 적용하여 AI 에이전트를 상호 연결된 진동기로 모델링하고, 오더 파라미터를 도입하여 협동과 동기화를 정량화하였다.

Result: 이질적인 에이전트 능력에도 불구하고 강한 연결이 이루어지면 안정적인 동기화가 촉진된다는 것을 보여주었으며, 현실적인 협력 AI 시나리오를 반영하였다.

Conclusion: 이 연구는 에이전트 기반 AI의 원칙적인 조정을 위한 경로를 열고, 시스템 복원력과 효율성을 높이기 위한 학습 역학과 적응형 네트워크 구조의 통합을 위한 기초를 마련한다.

Abstract: We present a novel interdisciplinary framework that bridges synchronization
theory and multi-agent AI systems by adapting the Kuramoto model to describe
the collective dynamics of heterogeneous AI agents engaged in complex task
execution. By representing AI agents as coupled oscillators with both phase and
amplitude dynamics, our model captures essential aspects of agent
specialization, influence, and communication within networked systems. We
introduce an order parameter to quantify the degree of coordination and
synchronization, providing insights into how coupling strength, agent
diversity, and network topology impact emergent collective behavior.
Furthermore, we formalize a detailed correspondence between Chain-of-Thought
prompting in AI reasoning and synchronization phenomena, unifying human-like
iterative problem solving with emergent group intelligence. Through extensive
simulations on all-to-all and deterministic scale-free networks, we demonstrate
that increased coupling promotes robust synchronization despite heterogeneous
agent capabilities, reflecting realistic collaborative AI scenarios. Our
physics-informed approach establishes a rigorous mathematical foundation for
designing, analyzing, and optimizing scalable, adaptive, and interpretable
multi-agent AI systems. This work opens pathways for principled orchestration
of agentic AI and lays the groundwork for future incorporation of learning
dynamics and adaptive network architectures to further enhance system
resilience and efficiency.

</details>


### [216] [A Taxonomy of Hierarchical Multi-Agent Systems: Design Patterns, Coordination Mechanisms, and Industrial Applications](https://arxiv.org/abs/2508.12683)
*David J. Moore*

Main category: cs.MA

TL;DR: 이 논문은 계층적 다중 에이전트 시스템(HMAS)을 위한 다차원 분류 체계를 제안하고, 이를 통해 다양한 접근 방식을 비교할 수 있는 렌즈를 제공한다.


<details>
  <summary>Details</summary>
Motivation: HMAS는 복잡성과 규모를 관리하는 데 도움을 주지만, 이에 따른 명백하지 않은 무역오차(trade-offs)도 발생할 수 있다.

Method: 논문에서는 제어 계층, 정보 흐름, 역할 및 작업 위임, 시간적 계층, 통신 구조라는 다섯 가지 축에 따른 HMAS의 다차원 분류 체계를 제안한다.

Result: 계층적 구조는 지역 자율성을 유지하면서도 글로벌 효율성을 달성할 수 있음을 보여주는 산업 맥락의 사례를 통해 입증된다.

Conclusion: 이 논문은 계층적 MAS의 구조적, 시간적, 통신 차원을 통합하여 단일 디자인 프레임워크로 제시하며, 고전적 조정 메커니즘과 현대 강화 학습 및 대량 언어 모델 에이전트를 연결하는 역할을 한다.

Abstract: Hierarchical multi-agent systems (HMAS) organize collections of agents into
layered structures that help manage complexity and scale. These hierarchies can
simplify coordination, but they also can introduce trade-offs that are not
always obvious. This paper proposes a multi-dimensional taxonomy for HMAS along
five axes: control hierarchy, information flow, role and task delegation,
temporal layering, and communication structure. The intent is not to prescribe
a single "best" design but to provide a lens for comparing different
approaches.
  Rather than treating these dimensions in isolation, the taxonomy is connected
to concrete coordination mechanisms - from the long-standing contract-net
protocol for task allocation to more recent work in hierarchical reinforcement
learning. Industrial contexts illustrate the framework, including power grids
and oilfield operations, where agents at production, maintenance, and supply
levels coordinate to diagnose well issues or balance energy demand. These cases
suggest that hierarchical structures may achieve global efficiency while
preserving local autonomy, though the balance is delicate.
  The paper closes by identifying open challenges: making hierarchical
decisions explainable to human operators, scaling to very large agent
populations, and assessing whether learning-based agents such as large language
models can be safely integrated into layered frameworks. This paper presents
what appears to be the first taxonomy that unifies structural, temporal, and
communication dimensions of hierarchical MAS into a single design framework,
bridging classical coordination mechanisms with modern reinforcement learning
and large language model agents.

</details>
