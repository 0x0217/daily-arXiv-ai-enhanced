<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 25]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis](https://arxiv.org/abs/2509.12212)
*Xinyu He,Chenhan Xiao,Haoran Li,Ruizhong Qiu,Zhe Xu,Yang Weng,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: PowerGrow는 복잡한 전력망 구조와 동적 부하 프로필을 효율적으로 생성하는 프레임워크로, 운영 타당성을 유지하면서 계산 오버헤드를 크게 줄여준다.


<details>
  <summary>Details</summary>
Motivation: 재생 가능 에너지 변동성, 전기차 채택 및 능동적 그리드 재구성 등으로 인해 현대 전력 시스템은 점점 더 동적인 환경이 되고 있다. 하지만, 보안 문제와 실질 시스템의 익명화에 필요한 많은 노력으로 인해 공개 테스트 케이스는 제한적이다.

Method: PowerGrow는 의존성 분해를 통해 복잡한 결합 분포를 실현 가능 그리드 구조, 시계열 버스 부하 및 다른 시스템 속성에 대한 조건부 분포의 체인으로 분해하고, 각 단계에서 생성 프로세스를 제약하여 구조 합성을 위해 계층적 그래프 베타 확산 프로세스를 구현한다.

Result: PowerGrow는 충실도와 다양성에서 이전 확산 모델을 초월하며, 98.9	d의 전력 흐름 수렴률과 개선된 N-1 비상 대응 능력을 달성한다.

Conclusion: PowerGrow는 운영적으로 유효하고 현실적인 전력망 시나리오를 생성할 수 있는 능력을 입증한다.

Abstract: Modern power systems are becoming increasingly dynamic, with changing
topologies and time-varying loads driven by renewable energy variability,
electric vehicle adoption, and active grid reconfiguration. Despite these
changes, publicly available test cases remain scarce, due to security concerns
and the significant effort required to anonymize real systems. Such limitations
call for generative tools that can jointly synthesize grid structure and nodal
dynamics. However, modeling the joint distribution of network topology, branch
attributes, bus properties, and dynamic load profiles remains a major
challenge, while preserving physical feasibility and avoiding prohibitive
computational costs. We present PowerGrow, a co-generative framework that
significantly reduces computational overhead while maintaining operational
validity. The core idea is dependence decomposition: the complex joint
distribution is factorized into a chain of conditional distributions over
feasible grid topologies, time-series bus loads, and other system attributes,
leveraging their mutual dependencies. By constraining the generation process at
each stage, we implement a hierarchical graph beta-diffusion process for
structural synthesis, paired with a temporal autoencoder that embeds
time-series data into a compact latent space, improving both training stability
and sample fidelity. Experiments across benchmark settings show that PowerGrow
not only outperforms prior diffusion models in fidelity and diversity but also
achieves a 98.9\% power flow convergence rate and improved N-1 contingency
resilience. This demonstrates its ability to generate operationally valid and
realistic power grid scenarios.

</details>


### [2] [TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks](https://arxiv.org/abs/2509.12224)
*Parsa Vatani,Mohamed Elrefaie,Farhad Nazarpour,Faez Ahmed*

Main category: cs.LG

TL;DR: 전통적인 CFD 기반 공기역학적 형상 최적화의 계산 비용이 설계 공간 탐색을 제한한다. 본 논문에서는 차량 포인트 클라우드 데이터에서 빠른 공기역학 분석 및 형상 최적화를 위한 완전 미분 가능한 딥 러닝 프레임워크인 TripOptimizer를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 CFD 기반의 공기역학적 형상 최적화는 계산 비용이 높아 설계 공간 탐색을 심각하게 제한하고 있다.

Method: TripOptimizer는 고충실도 3D 기하학 복원 및 항력 계수 예측을 위한 트리플레인 기반의 암묵적 신경 표현을 특징으로 하는 변분 오토인코더를 사용한다.

Result: 최적화된 설계가 최대 11.8%의 항력 계수 감소를 달성했다.

Conclusion: 프레임워크는 초기 설계 단계에서 컴퓨터 밀도가 높은 CFD 시뮬레이션에 대한 의존도를 줄이면서 보다 민첩한 공기역학적 형상 최적화 워크플로를 가능하게 한다.

Abstract: The computational cost of traditional Computational Fluid Dynamics-based
Aerodynamic Shape Optimization severely restricts design space exploration.
This paper introduces TripOptimizer, a fully differentiable deep learning
framework for rapid aerodynamic analysis and shape optimization directly from
vehicle point cloud data. TripOptimizer employs a Variational Autoencoder
featuring a triplane-based implicit neural representation for high-fidelity 3D
geometry reconstruction and a drag coefficient prediction head. Trained on
DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with
corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes
simulations, the model learns a latent representation that encodes
aerodynamically salient geometric features. We propose an optimization strategy
that modifies a subset of the encoder parameters to steer an initial geometry
towards a target drag value, and demonstrate its efficacy in case studies where
optimized designs achieved drag coefficient reductions up to 11.8\%. These
results were subsequently validated by using independent, high-fidelity
Computational Fluid Dynamics simulations with more than 150 million cells. A
key advantage of the implicit representation is its inherent robustness to
geometric imperfections, enabling optimization of non-watertight meshes, a
significant challenge for traditional adjoint-based methods. The framework
enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance
on computationally intensive CFD simulations, especially during early design
stages.

</details>


### [3] [Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2509.12234)
*Benjamin Burns,Yuan Xue,Douglas W. Scharre,Xia Ning*

Main category: cs.LG

TL;DR: 본 논문에서는 알츠하이머 병의 진행 예측을 개선하기 위해 PerM-MoE라는 새로운 희소 전문가 혼합 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 알츠하이머 병의 인지 저하 진행 예측은 여러 신경 영상 기법을 결합하여 이루어지지만, 기존의 다중 모달 모델은 진료 환경에서 모달이 결여될 경우 정확한 예측을 하지 못한다.

Method: PerM-MoE는 기존의 단일 라우터 대신 각 모달리티에 대해 독립적인 라우터를 사용하는 새로운 희소 전문가 혼합 방법이다.

Result: PerM-MoE는 다양한 모달 결여 수준 하에서 Clinical Dementia Rating-Sum of Boxes (CDR-SB) 점수의 2년 변화를 예측하는 데 있어 대부분의 모달 결여 변형에서 최첨단 Flex-MoE보다 우수한 성능을 보였다.

Conclusion: PerM-MoE는 Flex-MoE보다 전문가의 활용도를 더 효과적으로 입증하였다.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

</details>


### [4] [Prediction of Stocks Index Price using Quantum GANs](https://arxiv.org/abs/2509.12286)
*Sangram Deshpande,Gopal Ramesh Dahale,Sai Nandan Morapakula,Uday Wad*

Main category: cs.LG

TL;DR: 이 논문은 주가 예측을 위한 양자 생성적 적대 신경망(QGAN)의 적용을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 금융 시장은 본질적으로 복잡하며 높은 변동성과 전통적인 모델이 종종 포착하지 못하는 복잡한 패턴을 특징으로 한다.

Method: 양자 컴퓨팅의 힘을 활용하여 생성적 모델과 양자 기계 학습 기술의 강점을 결합한 QGAN 모델을 구현하였다.

Result: QGAN은 실제 시장 행동을 밀접하게 닮은 합성 데이터를 생성할 수 있으며, 예측 정확성을 향상시킨다.

Conclusion: 이 연구는 금융 예측에 양자 컴퓨팅을 통합하는 중요한 단계를 나타내며, 전통적인 방법에 비해 속도와 정밀도의 잠재적 이점을 제공한다.

Abstract: This paper investigates the application of Quantum Generative Adversarial
Networks (QGANs) for stock price prediction. Financial markets are inherently
complex, marked by high volatility and intricate patterns that traditional
models often fail to capture. QGANs, leveraging the power of quantum computing,
offer a novel approach by combining the strengths of generative models with
quantum machine learning techniques. We implement a QGAN model tailored for
stock price prediction and evaluate its performance using historical stock
market data. Our results demonstrate that QGANs can generate synthetic data
closely resembling actual market behavior, leading to enhanced prediction
accuracy. The experiment was conducted using the Stocks index price data and
the AWS Braket SV1 simulator for training the QGAN circuits. The
quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and
GAN models in terms of convergence speed and prediction accuracy. This research
represents a key step toward integrating quantum computing in financial
forecasting, offering potential advantages in speed and precision over
traditional methods. The findings suggest important implications for traders,
financial analysts, and researchers seeking advanced tools for market analysis.

</details>


### [5] [FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning](https://arxiv.org/abs/2509.12344)
*Arth Sojitra,Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: Fourier-embedded DeepONet는 다양한 편미분 방정식에 대한 예측 정확성을 극대화하는 강력한 데이터 기반 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: DeepONets는 비선형 연산자를 학습하기 위한 유망한 프레임워크지만 전통적인 구조는 복잡한 공간 구조를 잘 포착하지 못한다.

Method: 우리는 DeepONet 아키텍처 내에 랜덤 푸리에 피쳐 매핑을 활용한 푸리에-임베디드 트렁크 네트워크를 도입한다.

Result: 제안된 FEDONet은 다양한 PDE 데이터셋에 대해 전통적인 DeepONet보다 우수한 성능을 보여준다.

Conclusion: 푸리에 임베딩이 신경 연산자 학습을 향상시키는 데 효과적이며, PDE 서그릿 모델링을 위한 견고하고 광범위하게 적용 가능한 방법론을 제공한다.

Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful
data-driven frameworks for learning nonlinear operators, particularly suited
for approximating solutions to partial differential equations (PDEs). Despite
their promising capabilities, the standard implementation of DeepONets, which
typically employs fully connected linear layers in the trunk network, can
encounter limitations in capturing complex spatial structures inherent to
various PDEs. To address this, we introduce Fourier-embedded trunk networks
within the DeepONet architecture, leveraging random Fourier feature mappings to
enrich spatial representation capabilities. Our proposed Fourier-embedded
DeepONet, FEDONet demonstrates superior performance compared to the traditional
DeepONet across a comprehensive suite of PDE-driven datasets, including the
two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic
system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation,
and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show
significant improvements in solution reconstruction accuracy, with average
relative L2 performance gains ranging between 2-3x compared to the DeepONet
baseline. This study highlights the effectiveness of Fourier embeddings in
enhancing neural operator learning, offering a robust and broadly applicable
methodology for PDE surrogate modeling.

</details>


### [6] [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)
*Ritesh Janga,Rushit Dave*

Main category: cs.LG

TL;DR: 이 논문은 농업 부문의 변화와 함께 민감한 농장 데이터를 보호하면서도 농작물 질병 탐지를 위한 효과적인 연합 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 농업에서 데이터 기반 해석에 대한 수요가 증가하면서 농장 데이터의 프라이버시 우려가 존재합니다.

Method: 미네소타 농장에서 데이터 수집, 로컬 딥러닝 알고리즘 적용, 전이 학습, 모델 정제를 위한 중앙 집계 서버를 포함하는 방법론을 제시합니다.

Result: 농작물 질병 분류의 높은 정확성 달성, 농업 시나리오에 대한 일반화, 통신 및 훈련 시간 비용 절감, 질병의 조기 식별 및 개입이 가능합니다.

Conclusion: 이 연구는 스마트 농업 시스템을 혁신하고 데이터 기밀성을 유지하면서 지역 농업 문제를 해결할 수 있는 안전하고 효율적인 질병 탐지 방법을 제공합니다.

Abstract: The agricultural sector is undergoing a transformation with the integration
of advanced technologies, particularly in data-driven decision-making. This
work proposes a federated learning framework for smart farming, aiming to
develop a scalable, efficient, and secure solution for crop disease detection
tailored to the environmental and operational conditions of Minnesota farms. By
maintaining sensitive farm data locally and enabling collaborative model
updates, our proposed framework seeks to achieve high accuracy in crop disease
classification without compromising data privacy. We outline a methodology
involving data collection from Minnesota farms, application of local deep
learning algorithms, transfer learning, and a central aggregation server for
model refinement, aiming to achieve improved accuracy in disease detection,
good generalization across agricultural scenarios, lower costs in communication
and training time, and earlier identification and intervention against diseases
in future implementations. We outline a methodology and anticipated outcomes,
setting the stage for empirical validation in subsequent studies. This work
comes in a context where more and more demand for data-driven interpretations
in agriculture has to be weighed with concerns about privacy from farms that
are hesitant to share their operational data. This will be important to provide
a secure and efficient disease detection method that can finally revolutionize
smart farming systems and solve local agricultural problems with data
confidentiality. In doing so, this paper bridges the gap between advanced
machine learning techniques and the practical, privacy-sensitive needs of
farmers in Minnesota and beyond, leveraging the benefits of federated learning.

</details>


### [7] [Surrogate Representation Inference for Noisy Text and Image Annotations](https://arxiv.org/abs/2509.12416)
*Kentaro Nakamura*

Main category: cs.LG

TL;DR: 이 논문은 Surrogate Representation Inference(SRI)라는 새로운 방법론을 제시하며, 이는 비정형 데이터가 인간 주석과 구조화된 변수 간의 관계를 완전히 매개한다고 가정한다.


<details>
  <summary>Details</summary>
Motivation: 비정형 데이터를 주석 처리하는 데 있어 기계 학습 모델과 LLMs의 의존도가 높아져, 후속 통계 분석에서의 편향을 수정할 수 있는 다양한 접근법이 필요하다.

Method: SRI는 인간 주석이 비정형 데이터에만 의존할 때 장치적으로 보장되는 가정을 바탕으로, 비정형 데이터의 저차원 표현을 학습하는 신경망 구조를 제안한다.

Result: SRI는 기계 학습 예측 정확도가 적당할 때 표준 오류를 50% 이상 줄이며, 인간 주석이 비차별 측정 오류를 포함하고 있어도 유효한 추론을 제공한다.

Conclusion: SRI는 비정형 데이터의 저차원 표현을 학습하고 활용할 수 있도록 해주는 식별 조건과 비모수적 효율 추정 전략을 정립한다.

Abstract: As researchers increasingly rely on machine learning models and LLMs to
annotate unstructured data, such as texts or images, various approaches have
been proposed to correct bias in downstream statistical analysis. However,
existing methods tend to yield large standard errors and require some
error-free human annotation. In this paper, I introduce Surrogate
Representation Inference (SRI), which assumes that unstructured data fully
mediate the relationship between human annotations and structured variables.
The assumption is guaranteed by design provided that human coders rely only on
unstructured data for annotation. Under this setting, I propose a neural
network architecture that learns a low-dimensional representation of
unstructured data such that the surrogate assumption remains to be satisfied.
When multiple human annotations are available, SRI can further correct
non-differential measurement errors that may exist in human annotations.
Focusing on text-as-outcome settings, I formally establish the identification
conditions and semiparametric efficient estimation strategies that enable
learning and leveraging such a low-dimensional representation. Simulation
studies and a real-world application demonstrate that SRI reduces standard
errors by over 50% when machine learning prediction accuracy is moderate and
provides valid inference even when human annotations contain non-differential
measurement errors.

</details>


### [8] [Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures](https://arxiv.org/abs/2509.12484)
*Ruimeng Hu,Jihao Long,Haosheng Zhou*

Main category: cs.LG

TL;DR: 이 논문에서는 그래프에서 확률적 차분 게임의 내시 균형을 계산하기 위한 새로운 신경망 아키텍처인 비학습 수정(NTM)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 확률적 차분 게임은 금융, 로봇공학, 에너지 및 사회 역학에서 발생하는 그래프 구조의 다중 에이전트 시스템을 모델링하며, 에이전트는 불확실성 아래에서 지역적으로 상호 작용합니다.

Method: NTM 아키텍처는 피드포워드 신경망에 그래프 기반의 희소성을 부여하며, 고정된 비학습 구성 요소를 포함해 기본 그래프 위상과 정렬됩니다.

Result: 이론적으로, 우리는 그래프에서의 정적 게임에 대한 NTM의 보편적 근사성 질점을 확립하고, 감독 학습 작업을 통해 그 표현력과 강인성을 수치적으로 검증합니다.

Conclusion: 세 가지 다양한 그래프 구조에서의 세 가지 SDG에 대한 수치 실험은 NTM 기반 방법이 완전히 학습 가능한 대응물과 유사한 성능을 달성하면서 계산 효율성을 향상시킨다는 것을 보여줍니다.

Abstract: We propose a novel neural network architecture, called Non-Trainable
Modification (NTM), for computing Nash equilibria in stochastic differential
games (SDGs) on graphs. These games model a broad class of graph-structured
multi-agent systems arising in finance, robotics, energy, and social dynamics,
where agents interact locally under uncertainty. The NTM architecture imposes a
graph-guided sparsification on feedforward neural networks, embedding fixed,
non-trainable components aligned with the underlying graph topology. This
design enhances interpretability and stability, while significantly reducing
the number of trainable parameters in large-scale, sparse settings. We
theoretically establish a universal approximation property for NTM in static
games on graphs and numerically validate its expressivity and robustness
through supervised learning tasks. Building on this foundation, we incorporate
NTM into two state-of-the-art game solvers, Direct Parameterization and Deep
BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical
experiments on three SDGs across various graph structures demonstrate that
NTM-based methods achieve performance comparable to their fully trainable
counterparts, while offering improved computational efficiency.

</details>


### [9] [Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model](https://arxiv.org/abs/2509.12497)
*Alessandro Crimi,Andrea Brovelli*

Main category: cs.LG

TL;DR: 본 연구는 기초 모델이 전통적인 방법에 비해 뇌 신호 예측 및 인과 관계 분석에 효과적인지를 평가하고, 제로샷 환경에서의 적용 가능성을 검토하였다.


<details>
  <summary>Details</summary>
Motivation: 뇌 활동 예측 및 신경 집단과 회로 간 인과 관계 식별은 인지 및 질병의 메커니즘을 이해하는 데 중요하다.

Method: 기초 모델을 사용하여 인간의 기능적 자기 공명 영상(fMRI)으로 측정된 자발적인 뇌 활동으로부터 방향성 상호작용을 추론하는 고전적 방법과 비교하였다. 제로샷 및 미세 조정된 설정에서 기초 모델의 예측 능력을 평가하고, 모델의 그랜저 유사 추정을 표준 그랜저 인과관계와 비교하였다.

Result: 기초 모델은 대조군에서 평균 절대 백분율 오차 0.55, 환자에서 0.27의 경쟁력 있는 제로샷 예측 성능을 발휘하였다.

Conclusion: 기초 모델은 시간 시계열 데이터의 예측 및 인과 발견에 대한 다양성, 강력한 제로샷 성능, 잠재적인 유용성을 제공한다.

Abstract: Time-series forecasting and causal discovery are central in neuroscience, as
predicting brain activity and identifying causal relationships between neural
populations and circuits can shed light on the mechanisms underlying cognition
and disease. With the rise of foundation models, an open question is how they
compare to traditional methods for brain signal forecasting and causality
analysis, and whether they can be applied in a zero-shot setting. In this work,
we evaluate a foundation model against classical methods for inferring
directional interactions from spontaneous brain activity measured with
functional magnetic resonance imaging (fMRI) in humans. Traditional approaches
often rely on Wiener-Granger causality. We tested the forecasting ability of
the foundation model in both zero-shot and fine-tuned settings, and assessed
causality by comparing Granger-like estimates from the model with standard
Granger causality. We validated the approach using synthetic time series
generated from ground-truth causal models, including logistic map coupling and
Ornstein-Uhlenbeck processes. The foundation model achieved competitive
zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55
in controls and 0.27 in patients). Although standard Granger causality did not
show clear quantitative differences between models, the foundation model
provided a more precise detection of causal interactions.
  Overall, these findings suggest that foundation models offer versatility,
strong zero-shot performance, and potential utility for forecasting and causal
discovery in time-series data.

</details>


### [10] [Exploring Training Data Attribution under Limited Access Constraints](https://arxiv.org/abs/2509.12581)
*Shiyuan Zhang,Junwei Deng,Juhan Bae,Jiaqi Ma*

Main category: cs.LG

TL;DR: 본 논문은 제한된 자원과 접근 조건 하에서의 데이터 기여도 평가(TDA) 방법의 체계적인 연구를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 상업 모델이 공개되지 않고 계산 자원이 제한된 실제 시나리오에서 TDA의 보다 넓은 채택에 대한 도전 과제가 존재합니다.

Method: 우리는 적절하게 설계된 솔루션인 프록시 모델을 활용하여 다양한 접근 제한 하에서 TDA를 수행하는 가능성을 조사합니다.

Result: 대상 데이터셋에 대한 사전 훈련 없이 얻은 기여도 점수가 여러 작업에서 여전히 유용하다는 것을 보여줍니다.

Conclusion: 우리의 발견은 제한된 접근 하에서도 TDA를 실제 환경에 배치하기 위한 실용적인 지침을 제공합니다.

Abstract: Training data attribution (TDA) plays a critical role in understanding the
influence of individual training data points on model predictions.
Gradient-based TDA methods, popularized by \textit{influence function} for
their superior performance, have been widely applied in data selection, data
cleaning, data economics, and fact tracing. However, in real-world scenarios
where commercial models are not publicly accessible and computational resources
are limited, existing TDA methods are often constrained by their reliance on
full model access and high computational costs. This poses significant
challenges to the broader adoption of TDA in practical applications.
  In this work, we present a systematic study of TDA methods under various
access and resource constraints. We investigate the feasibility of performing
TDA under varying levels of access constraints by leveraging appropriately
designed solutions such as proxy models. Besides, we demonstrate that
attribution scores obtained from models without prior training on the target
dataset remain informative across a range of tasks, which is useful for
scenarios where computational resources are limited. Our findings provide
practical guidance for deploying TDA in real-world environments, aiming to
improve feasibility and efficiency under limited access.

</details>


### [11] [A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction](https://arxiv.org/abs/2509.12600)
*Huajun Zhou,Fengtao Zhou,Jiabo Ma,Yingxue Xu,Xi Wang,Xiuming Zhang,Li Liang,Zhenhui Li,Hao Chen*

Main category: cs.LG

TL;DR: MICE는 다중 모드 데이터를 이용해 암 예후 예측을 개선하는 모델로, 다양한 전문가 네트워크를 통해 통합적 인사이트를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 정보로 구성된 다중 모드 데이터의 활용을 극대화하여 종양 미세환경에 대한 포괄적인 이해를 향상시키고자 함.

Method: MICE는 병리 이미지, 임상 보고서, 유전체 데이터를 통합하여 암 유형별 및 전이암 예후 예측을 수행합니다.

Result: MICE는 전통적인 단일모드 및 최신 다중 전문가 기반 모델을 초월하여 C-index에서 3.8%에서 11.2%까지 개선되었습니다.

Conclusion: MICE는 개인 맞춤형 치료를 가능하게 하는 뛰어난 일반화 능력과 데이터 효율성을 갖춘 암 예후 예측의 효과적이고 확장 가능한 기초를 제공합니다.

Abstract: Multimodal data provides heterogeneous information for a holistic
understanding of the tumor microenvironment. However, existing AI models often
struggle to harness the rich information within multimodal data and extract
poorly generalizable representations. Here we present MICE (Multimodal data
Integration via Collaborative Experts), a multimodal foundation model that
effectively integrates pathology images, clinical reports, and genomics data
for precise pan-cancer prognosis prediction. Instead of conventional
multi-expert modules, MICE employs multiple functionally diverse experts to
comprehensively capture both cross-cancer and cancer-specific insights.
Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's
generalizability by coupling contrastive and supervised learning. MICE
outperformed both unimodal and state-of-the-art multi-expert-based multimodal
models, demonstrating substantial improvements in C-index ranging from 3.8% to
11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,
respectively. Moreover, it exhibited remarkable data efficiency across diverse
clinical scenarios. With its enhanced generalizability and data efficiency,
MICE establishes an effective and scalable foundation for pan-cancer prognosis
prediction, holding strong potential to personalize tailored therapies and
improve treatment outcomes.

</details>


### [12] [High-Energy Concentration for Federated Learning in Frequency Domain](https://arxiv.org/abs/2509.12630)
*Haozhi Shi,Weiying Xie,Hangyu Ye,Daixun Li,Jitao Ma,Leyuan Fang*

Main category: cs.LG

TL;DR: 본 논문은 데이터 공유 없이 협업 최적화를 위한 연합 학습 프레임워크인 FedFD를 제안하며, 통신 비용을 줄이고 성능을 최적화한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습의 데이터 비공유 특성을 활용하여 실 데이터의 프라이버시를 보호하고 데이터 이질성을 완화하는 방법이 필요하다.

Method: FedFD는 이산 코사인 변환의 에너지를 특정 영역으로 분포시키는 특성을 이용하여, 저에너지 고주파 성분을 필터링하여 통신 비용을 줄이고 성능을 최적화하는 방법이다.

Result: 다섯 개의 이미지 및 음성 데이터세트에서 FedFD는 최신 방법들보다 우수한 성능을 달성하면서 통신 비용을 줄였다. 예를 들어, CIFAR-10 데이터셋에서는 37.78%의 통신 비용 절감과 10.88%의 성능 향상을 기록했다.

Conclusion: FedFD는 연합 학습에서 통신 비용을 줄이는 동시에 성능 최적화를 달성하는 혁신적인 방법이다.

Abstract: Federated Learning (FL) presents significant potential for collaborative
optimization without data sharing. Since synthetic data is sent to the server,
leveraging the popular concept of dataset distillation, this FL framework
protects real data privacy while alleviating data heterogeneity. However, such
methods are still challenged by the redundant information and noise in entire
spatial-domain designs, which inevitably increases the communication burden. In
this paper, we propose a novel Frequency-Domain aware FL method with
high-energy concentration (FedFD) to address this problem. Our FedFD is
inspired by the discovery that the discrete cosine transform predominantly
distributes energy to specific regions, referred to as high-energy
concentration. The principle behind FedFD is that low-energy like
high-frequency components usually contain redundant information and noise, thus
filtering them helps reduce communication costs and optimize performance. Our
FedFD is mathematically formulated to preserve the low-frequency components
using a binary mask, facilitating an optimal solution through frequency-domain
distribution alignment. In particular, real data-driven synthetic
classification is imposed into the loss to enhance the quality of the
low-frequency components. On five image and speech datasets, FedFD achieves
superior performance than state-of-the-art methods while reducing communication
costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha
= 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication
cost, while attaining a 10.88\% performance gain.

</details>


### [13] [Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection](https://arxiv.org/abs/2509.12650)
*Chan Sik Han,Keon Myung Lee*

Main category: cs.LG

TL;DR: TimeRep는 시간 시계열 데이터에서 이상 징후를 감지하기 위해 중간 층의 표현을 활용하는 새로운 접근법입니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 데이터에서 이상 징후를 감지하는 것은 많은 실제 시스템의 신뢰성 있는 운영을 위해 필수적입니다.

Method: TimeRep는 사전 훈련된 TSFM을 사용하여 중간 층 및 패치-토큰 위치를 선택하고, 훈련 데이터에서 중간 표현의 참조 집합을 구성하여 거리 기반의 이상 점수를 계산합니다.

Result: 우리는 UCR Anomaly Archive에서 폭넓은 실험을 수행했으며, TimeRep는 비 딥러닝, 딥러닝 및 기초 모델 기반 방법을 포함하여 다양한 최신 기법들을 꾸준히 능가했습니다.

Conclusion: TimeRep은 중간 표현을 이용한 새로운 방법으로 이상 감지에 효과적입니다.

Abstract: Detecting anomalies in time series data is essential for the reliable
operation of many real-world systems. Recently, time series foundation models
(TSFMs) have emerged as a powerful tool for anomaly detection. However,
existing methods typically rely on the final layer's representations of TSFMs,
computing the anomaly score as a reconstruction or forecasting error via a
task-specific head. Instead, we propose TimeRep, a novel anomaly detection
approach that leverages the intermediate layer's representations of TSFMs,
computing the anomaly score as the distance between these representations.
Given a pre-trained TSFM, TimeRep selects the intermediate layer and
patch-token position that yield the most informative representation. TimeRep
forms a reference collection of intermediate representations from the training
data and applies a core-set strategy to reduce its size while maintaining
distributional coverage. During inference, TimeRep computes the anomaly score
for incoming data by measuring the distance between its intermediate
representations and those of the collection. To address concept drift, TimeRep
integrates an adaptation mechanism that, at inference time, augments the
collection exclusively with non-redundant intermediate representations from
incoming data. We conducted extensive experiments on the UCR Anomaly Archive,
which contains 250 univariate time series. TimeRep consistently outperforms a
broad spectrum of state-of-the-art baselines, including non-DL, DL, and
foundation model-based methods.

</details>


### [14] [ZTree: A Subgroup Identification Based Decision Tree Learning Framework](https://arxiv.org/abs/2509.12688)
*Eric Cheng,Jie Cheng*

Main category: cs.LG

TL;DR: ZTree는 CART의 전통적인 순도 기반 분할 대신 통계적으로 원칙이 있는 하위 그룹 식별을 사용하는 새로운 결정 트리 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 결정 트리는 해석 가능성과 다재다능성으로 가치 있는 머신러닝 모델의 일반적인 클래스이다.

Method: ZTree는 각 노드에서 가설 검정(예: z-test, t-test, Mann-Whitney U, log-rank)을 적용하여 후보 하위 그룹이 보완과 의미 있게 다르는지를 평가하며, 다중 테스트의 복잡성을 조정하기 위해 교차 검증 기반 접근 방식을 사용한다.

Result: ZTree는 저 데이터 조건에서도 강력한 성능을 지속적으로 보여주며, CART와 비교했을 때 성능을 희생하지 않으면서 더 단순한 트리를 성장시키는 경향이 있다.

Conclusion: ZTree는 가설 검정과 다중 테스트 수정을 위한 교차 검증 접근 방식을 활용하여 전통적인 결정 트리 분할에 대한 통계적으로 기반이 있는 대안을 제공하여 효율적이고 유연한 프레임워크를 만든다.

Abstract: Decision trees are a commonly used class of machine learning models valued
for their interpretability and versatility, capable of both classification and
regression. We propose ZTree, a novel decision tree learning framework that
replaces CART's traditional purity based splitting with statistically
principled subgroup identification. At each node, ZTree applies hypothesis
testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a
candidate subgroup differs meaningfully from the complement. To adjust for the
complication of multiple testing, we employ a cross-validation-based approach
to determine if further node splitting is needed. This robust stopping
criterion eliminates the need for post-pruning and makes the test threshold
(z-threshold) the only parameter for controlling tree complexity. Because of
the simplicity of the tree growing procedure, once a detailed tree is learned
using the most lenient z-threshold, all simpler trees can be derived by simply
removing nodes that do not meet the larger z-thresholds. This makes parameter
tuning intuitive and efficient. Furthermore, this z-threshold is essentially a
p-value, allowing users to easily plug in appropriate statistical tests into
our framework without adjusting the range of parameter search. Empirical
evaluation on five large-scale UCI datasets demonstrates that ZTree
consistently delivers strong performance, especially at low data regimes.
Compared to CART, ZTree also tends to grow simpler trees without sacrificing
performance. ZTree introduces a statistically grounded alternative to
traditional decision tree splitting by leveraging hypothesis testing and a
cross-validation approach to multiple testing correction, resulting in an
efficient and flexible framework.

</details>


### [15] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: Tool-R1은 대형 언어 모델이 다단계 도구 사용을 수행할 수 있도록 돕는 강화 학습 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계 작업에 필요한 최신 지식, 정확한 운영 또는 전문 도구 사용에 제한된 대형 언어 모델(LLM)의 한계를 극복하기 위해 개발되었다.

Method: Tool-R1은 실행 가능한 Python 코드를 생성하여 일반적이고 조합적이며 다단계 도구 사용을 가능하게 하는 강화 학습 프레임워크이다. 사용자 정의 도구와 표준 라이브러리의 통합을 지원하며, 일관된 워크플로우 구성을 위한 단계 간 변수 공유 기능을 갖춘다.

Result: GAIA 기준에서의 실험 결과, Tool-R1은 정확성과 견고성을 크게 향상시켜 강력한 기준에 비해 약 10%의 향상을 달성했으며, 복잡한 다단계 작업에서는 더 큰 개선이 이루어졌다.

Conclusion: Tool-R1은 실제 애플리케이션에서 신뢰할 수 있고 효율적인 도구 보강 추론을 가능하게 하는 잠재력을 보여준다.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [16] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 대형 언어 모델의 정렬을 위한 통합 평가 프레임워크를 제안하고, 여러 정렬 방법을 비교하여 상충하는 목표 간의 균형을 이해한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 정렬은 사실성, 안전성, 간결성, 적극성 및 다양성과 같은 상충하는 목표 간의 균형을 맞추기 위해 신중해야 한다.

Method: 기존의 여러 정렬 방법(PPO, DPO, ORPO, KTO)을 다룬 통합 평가 프레임워크를 사용하여, 다양한 데이터셋에서 이 방법들을 비교하였다.

Result: DPO와 KTO는 사실 정확성에서 우수하며, PPO와 DPO는 안전성에서 선두를 차지하고, PPO는 간결성과 적극성의 균형을 가장 잘 맞춘다.

Conclusion: 우리의 연구 결과는 공통적인 정렬 방법의 상충 관계에 대한 통찰력을 제공하며, 보다 균형 있고 신뢰할 수 있는 대형 언어 모델의 개발을 안내한다.

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [17] [Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories](https://arxiv.org/abs/2509.12953)
*Jaume Banus,Augustin C. Ogier,Roger Hullin,Philippe Meyer,Ruud B. van Heeswijk,Jonas Richiardi*

Main category: cs.LG

TL;DR: 본 연구에서는 희소 관측값으로부터 구조화된 시공간 동역학을 모델링하기 위한 확률론적 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 심장 운동에 대한 깊은 이해와 정확한 모형화 필요성으로 인해 이 연구가 시작되었습니다.

Method: 본 연구는 신경 일차 미분 방정식(NODEs), 그래프 신경망(GNNs), 신경 프로세스를 통합하여 불확실성, 시간적 연속성 및 해부학적 구조를 포착하는 통합 모형을 개발했습니다.

Result: 세 가지 합성 동역학 시스템(결합된 진자, 로렌츠 아트랙터, 쿠라마토 오실레이터)과 두 개의 실제 심장 이미징 데이터셋(ACDC 및 UK Biobank)에서 우리 방법의 유효성을 검증하였습니다.

Conclusion: 이 연구는 심장 운동 분석을 위한 유연한 접근을 도입하고 구조화된 생물 의학 시계열 데이터에서 그래프 기반 학습의 기초를 제공합니다.

Abstract: We present a probabilistic framework for modeling structured spatiotemporal
dynamics from sparse observations, focusing on cardiac motion. Our approach
integrates neural ordinary differential equations (NODEs), graph neural
networks (GNNs), and neural processes into a unified model that captures
uncertainty, temporal continuity, and anatomical structure. We represent
dynamic systems as spatiotemporal multiplex graphs and model their latent
trajectories using a GNN-parameterized vector field. Given the sparse context
observations at node and edge levels, the model infers a distribution over
latent initial states and control variables, enabling both interpolation and
extrapolation of trajectories. We validate the method on three synthetic
dynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto
oscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK
Biobank (N=526) - demonstrating accurate reconstruction, extrapolation, and
disease classification capabilities. The model accurately reconstructs
trajectories and extrapolates future cardiac cycles from a single observed
cycle. It achieves state-of-the-art results on the ACDC classification task (up
to 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with
competitive performance (up to 67% accuracy). This work introduces a flexible
approach for analyzing cardiac motion and offers a foundation for graph-based
learning in structured biomedical spatiotemporal time-series data.

</details>


### [18] [Causal Discovery via Quantile Partial Effect](https://arxiv.org/abs/2509.12981)
*Yikang Chen,Xingzhe Sun,Dehui Du*

Main category: cs.LG

TL;DR: QPE는 조건부 분위 회귀와 관련된 통계로, 공변량의 효과를 다양한 수준에서 측정합니다. 이 논문에서는 QPE가 유한한 선형 스팬에 있다고 가정할 때 원인과 결과를 관찰 분포에서 식별할 수 있다고 설명합니다.


<details>
  <summary>Details</summary>
Motivation: QPE를 이용하여 원인과 결과를 식별할 수 있는 방법을 제시하고, 기존의 기능적 인과 모델(FCMs)을 일반화할 수 있는 가능성을 탐구합니다.

Method: QPE를 관찰 수준에서 직접 적용하고, 기초 함수 테스트를 수행하여 인과 방향성을 구별하며, Fisher 정보를 이용하여 다변량 인과 발견에서 인과 순서를 결정합니다.

Result: QPE에 대한 기초 함수 테스트는 인과 방향을 효과적으로 구별했으며, Fisher 정보는 다변량 인과 발견에서 인과 순서를 결정하는 데 충분한 통계적 측정으로 입증되었습니다.

Conclusion: Fisher 정보를 이용하여 다수의 합성 및 실제 다변량 인과 발견 데이터셋에서 인과 순서를 식별하는 것이 가능함을 검증하였습니다.

Abstract: Quantile Partial Effect (QPE) is a statistic associated with conditional
quantile regression, measuring the effect of covariates at different levels.
Our theory demonstrates that when the QPE of cause on effect is assumed to lie
in a finite linear span, cause and effect are identifiable from their
observational distribution. This generalizes previous identifiability results
based on Functional Causal Models (FCMs) with additive, heteroscedastic noise,
etc. Meanwhile, since QPE resides entirely at the observational level, this
parametric assumption does not require considering mechanisms, noise, or even
the Markov assumption, but rather directly utilizes the asymmetry of shape
characteristics in the observational distribution. By performing basis function
tests on the estimated QPE, causal directions can be distinguished, which is
empirically shown to be effective in experiments on a large number of bivariate
causal discovery datasets. For multivariate causal discovery, leveraging the
close connection between QPE and score functions, we find that Fisher
Information is sufficient as a statistical measure to determine causal order
when assumptions are made about the second moment of QPE. We validate the
feasibility of using Fisher Information to identify causal order on multiple
synthetic and real-world multivariate causal discovery datasets.

</details>


### [19] [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)
*Lorenzo Pes,Bojian Yin,Sander Stuijk,Federico Corradi*

Main category: cs.LG

TL;DR: 이 논문에서는 메모리 효율적이며 확장 가능한 Spiking Neural Networks(SNN) 훈련을 위한 Traces Propagation(TP) 학습 규칙을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: SNN은 생물학적 신경 시스템의 학습 원리를 조사하는 효율적인 프레임워크이지만, 훈련 과정에서의 공간 및 시간 크레딧 할당이 주요 과제입니다.

Method: TP는 자격 추적을 사용하여 공간 크레딧 할당을 해결하고, 보조 레이어-와이즈 매트릭스를 요구하지 않으면서 레이어-와이즈 대조 손실과 결합합니다.

Result: TP는 NMNIST 및 SHD 데이터셋에서 다른 완전히 지역적인 학습 규칙보다 우수한 성능을 보이며, DVS-GESTURE 및 DVS-CIFAR10과 같은 복잡한 데이터셋에서도 경쟁력 있는 성능을 나타냅니다.

Conclusion: TP는 Google Speech Commands 데이터셋을 통한 키워드 스포팅과 같은 실제 미세 조정 작업에 적합하며, 엣지에서 효율적인 학습을 위한 길을 열어줍니다.

Abstract: Spiking Neural Networks (SNNs) provide an efficient framework for processing
dynamic spatio-temporal signals and for investigating the learning principles
underlying biological neural systems. A key challenge in training SNNs is to
solve both spatial and temporal credit assignment. The dominant approach for
training SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.
However, BPTT is in stark contrast with the spatial and temporal locality
observed in biological neural systems and leads to high computational and
memory demands, limiting efficient training strategies and on-device learning.
Although existing local learning rules achieve local temporal credit assignment
by leveraging eligibility traces, they fail to address the spatial credit
assignment without resorting to auxiliary layer-wise matrices, which increase
memory overhead and hinder scalability, especially on embedded devices. In this
work, we propose Traces Propagation (TP), a forward-only, memory-efficient,
scalable, and fully local learning rule that combines eligibility traces with a
layer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP
outperforms other fully local learning rules on NMNIST and SHD datasets. On
more complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases
competitive performance and scales effectively to deeper SNN architectures such
as VGG-9, while providing favorable memory scaling compared to prior fully
local scalable rules, for datasets with a significant number of classes.
Finally, we show that TP is well suited for practical fine-tuning tasks, such
as keyword spotting on the Google Speech Commands dataset, thus paving the way
for efficient learning at the edge.

</details>


### [20] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: FinSearchComp는 리얼리티와 오픈 도메인을 갖춘 재무 검색 및 추론을 위한 첫 번째 완전 오픈 소스 에이전트 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 핵심 인프라로 검색이 대두되었으며, 이는 일반 지능으로의 경로에서 중요하게 여겨진다. 재무 분야는 복잡하고 다단계 검색을 수행해야 하는 분석가들이 존재하기 때문에 특히 도전적인 시험장이다.

Method: FinSearchComp는 시간 민감 데이터 검색, 단순 역사 조회, 복잡 역사 조사라는 세 가지 작업으로 구성되며, 실제 재무 분석가의 작업 흐름을 밀접하게 재현한다.

Result: FinSearchComp에는 글로벌 및 대중화 시장을 포괄하는 635개의 질문이 포함되며, 21개 모델의 성능을 평가하였다. Grok 4는 글로벌 하위 집합에서 전문가 수준의 정확도를 달성하였다.

Conclusion: FinSearchComp는 복잡한 재무 검색 및 추론을 위한 전문적이고 높은 난이도의 테스트 베드로 제공된다.

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [21] [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)
*Eric Nuertey Coleman,Luigi Quarantiello,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: 계속적인 학습은 인간 인지의 필수 능력이지만, 현재의 깊은 학습 모델에 상당한 도전을 제기한다. 이 논문은 작업 간의 어댑터를 동적으로 결합하는 새로운 접근법인 계층적 어댑터 병합(HAM)을 제안하며, 이 방법이 여러 작업을 효율적으로 관리하고 성능을 크게 향상시킬 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 계속적인 학습은 지식의 습득과 유지를 요구하지만, 새로운 지식이 이전 정보를 방해하여 모델이 잊어버리는 현상(파국적 망각)이 발생한다. 이러한 문제를 해결하기 위한 새로운 방법을 모색할 필요가 있다.

Method: HAM은 훈련 중에 서로 다른 작업의 어댑터를 동적으로 결합하는 프레임워크이다. 각 작업에 대해 저차원 어댑터와 중요도 스칼라를 훈련하고, 어댑터 유사성에 기반하여 작업들을 동적으로 그룹화한다.

Result: HAM은 대조군보다 더 많은 작업을 관리할 수 있으며, 효율성이 향상되었다. 세 가지 비전 벤치마크에서 HAM은 특히 작업 수가 증가할수록 최첨단 방법들과 비교해 우수한 성능을 보였다.

Conclusion: HAM은 복잡성을 줄이면서도 작업 간 전이 학습을 촉진할 수 있는 효과적인 방법이다.

Abstract: Continual learning is an essential capability of human cognition, yet it
poses significant challenges for current deep learning models. The primary
issue is that new knowledge can interfere with previously learned information,
causing the model to forget earlier knowledge in favor of the new, a phenomenon
known as catastrophic forgetting. Although large pre-trained models can
partially mitigate forgetting by leveraging their existing knowledge and
over-parameterization, they often struggle when confronted with novel data
distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
enable efficient adaptation to new knowledge. However, they still face
challenges in scaling to dynamic learning scenarios and long sequences of
tasks, as maintaining one adapter per task introduces complexity and increases
the potential for interference. In this paper, we introduce Hierarchical
Adapters Merging (HAM), a novel framework that dynamically combines adapters
from different tasks during training. This approach enables HAM to scale
effectively, allowing it to manage more tasks than competing baselines with
improved efficiency. To achieve this, HAM maintains a fixed set of groups that
hierarchically consolidate new adapters. For each task, HAM trains a low-rank
adapter along with an importance scalar, then dynamically groups tasks based on
adapter similarity. Within each group, adapters are pruned, scaled and merge,
facilitating transfer learning between related tasks. Extensive experiments on
three vision benchmarks show that HAM significantly outperforms
state-of-the-art methods, particularly as the number of tasks increases.

</details>


### [22] [FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data](https://arxiv.org/abs/2509.13218)
*J. Cha,J. Lee,J. Cho,J. Shin*

Main category: cs.LG

TL;DR: FOSSIL은 불균형 데이터 문제를 해결하기 위한 통합 가중치 프레임워크로, 다양한 기술을 하나의 공식을 통해 통합하여 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 희귀 질병 이미징, 유전체학, 재난 대응과 같은 분야에서는 레이블이 있는 샘플이 부족하여 데이터 불균형과 작은 데이터 얼라이먼트가 일반적입니다.

Method: FOSSIL은 클래스 불균형 교정, 난이도 인식 커리큘럼, 증강 패널티 및 워밍업 동력을 단일 해석 가능한 공식으로 통합하는 유연한 최적화 프레임워크입니다.

Result: 제안된 프레임워크는 이론적 보장과 함께 ERM, 커리큘럼 및 메타 가중치 기반선에 대해 일관된 경험적 이득을 달성했습니다.

Conclusion: FOSSIL은 구조적 변경 없이도 이러한 성과를 달성함으로써 데이터 불균형 문제 해결에 대한 새로운 접근 방식을 제시합니다.

Abstract: Imbalanced and small data regimes are pervasive in domains such as rare
disease imaging, genomics, and disaster response, where labeled samples are
scarce and naive augmentation often introduces artifacts. Existing solutions
such as oversampling, focal loss, or meta-weighting address isolated aspects of
this challenge but remain fragile or complex. We introduce FOSSIL (Flexible
Optimization via Sample Sensitive Importance Learning), a unified weighting
framework that seamlessly integrates class imbalance correction,
difficulty-aware curricula, augmentation penalties, and warmup dynamics into a
single interpretable formula. Unlike prior heuristics, the proposed framework
provides regret-based theoretical guarantees and achieves consistent empirical
gains over ERM, curriculum, and meta-weighting baselines on synthetic and
real-world datasets, while requiring no architectural changes.

</details>


### [23] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델(LLM)이 반복되는 추론 조각을 간결하고 재사용 가능한 '행동'으로 변환하여 다단계 문제 해결을 개선하는 메커니즘을 연구한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 다단계 문제 해결 시 동일한 중간 단계를 반복적으로 유도하여 토큰 사용량과 지연 시간이 증가하는 문제를 해결하고자 한다.

Method: 모델의 메타인지 분석을 통해 반복되는 추론 조각을 '행동'으로 변환하고, 이를 '행동 핸드북'에 저장하여 추론 시 활용하거나 감독된 미세 조정을 통해 매개변수로 증류한다.

Result: 행동 조건부 추론, 행동 유도 자기 개선, 행동 조건부 SFT 세 가지 설정에서 유의미한 성과를 달성하였다.

Conclusion: 느린 유도 과정을 빠른 절차적 힌트로 변환함으로써 LLM이 무엇을 결론내릴지뿐만 아니라 어떻게 추론할지를 기억하게 한다.

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


### [24] [JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks](https://arxiv.org/abs/2509.13266)
*Jiahao Zhang,Xiaobing Pei,Zhaokun Zhong,Wenqiang Hao,Zhenghao Tang*

Main category: cs.LG

TL;DR: 본 논문에서는 GNN을 대상으로 한 노드 주입 공격의 효율성과 은밀성을 향상시키기 위한 JANUS라는 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: GNN은 여러 응용 프로그램에서 뛰어난 성능을 보이지만, 노드 주입 공격과 같은 정교한 적대적 공격에 취약하다.

Method: 우리는 이중 제약식 스텔시 노드 주입 프레임워크인 JANUS를 제안한다. 지역 수준에서 특징 매니폴드 정렬 전략을 도입하고, 전역 수준에서 구조화된 잠재 변수를 포함하여 상호 정보를 최대화한다.

Result: 실험 결과, JANUS 프레임워크는 공격의 효과성과 은밀성 측면에서 기존 방법보다 우수함을 입증했다.

Conclusion: 본 연구는 GNN에 대한 공격 방어 전략을 개선할 수 있는 가능성을 보여준다.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
various applications, yet they are vulnerable to sophisticated adversarial
attacks, particularly node injection attacks. The success of such attacks
heavily relies on their stealthiness, the ability to blend in with the original
graph and evade detection. However, existing methods often achieve stealthiness
by relying on indirect proxy metrics, lacking consideration for the fundamental
characteristics of the injected content, or focusing only on imitating local
structures, which leads to the problem of local myopia. To overcome these
limitations, we propose a dual-constraint stealthy node injection framework,
called Joint Alignment of Nodal and Universal Structures (JANUS). At the local
level, we introduce a local feature manifold alignment strategy to achieve
geometric consistency in the feature space. At the global level, we incorporate
structured latent variables and maximize the mutual information with the
generated structures, ensuring the injected structures are consistent with the
semantic patterns of the original graph. We model the injection attack as a
sequential decision process, which is optimized by a reinforcement learning
agent. Experiments on multiple standard datasets demonstrate that the JANUS
framework significantly outperforms existing methods in terms of both attack
effectiveness and stealthiness.

</details>


### [25] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: 본 논문에서는 인간의 인지 한계를 초월하는 LLM 훈련의 중요성을 다루고, WebSailor라는 방법론을 통해 정보 탐색 역량을 강화하는 방법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 인간의 인지 한계를 초월하는 것은 LLM 훈련에서 중요한 경계입니다.

Method: WebSailor는 구조화된 샘플링과 정보 혼동, RFT 콜드 스타트, Duplicating Sampling Policy Optimization(DUPO)와 같은 방법을 통해 새로운 고불확실성 작업을 생성하고, 이러한 역량을 강화하는 완전한 포스트 훈련 방법론입니다.

Result: WebSailor는 복잡한 정보 탐색 작업에서 모든 오픈 소스 에이전트를 현저히 초월하며, 독점 에이전트의 성능과 일치하고 능력 간의 격차를 줄입니다.

Conclusion: WebSailor는 정보 탐색 기술에서의 성능 격차를 해소하는 성공적인 접근법입니다.

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: 이 논문은 베트남 고등학생들이 국가 고등학교 졸업 수학 시험을 준비하는 데 도움을 주기 위한 자율형 프레임워크 V-Math를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 베트남 고등학생들이 국가 고등학교 졸업 수학 시험을 효과적으로 준비할 수 있도록 지원한다.

Method: 세 가지 전문 AI 에이전트를 통합한 V-Math 프레임워크를 사용: 질문 생성기, 문제 해결기/설명기, 개인화된 튜터.

Result: V-Math는 높은 정확도의 솔루션을 가진 매트릭스 정렬 시험을 생성하고, 일관성 있는 설명을 제공하며, 다양한 연습 자료를 향상시켰다.

Conclusion: V-Math는 국가 표준에 맞춘 공정한 수학 준비를 지원하고, 교사를 AI 지원 시험 생성 도구로 강화할 수 있는 잠재력을 강조한다.

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [27] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: 대규모 다중모달 모델(LMM)은 훈련 중 관찰된 보편적인 물리 법칙을 매개변수 지식으로 인코딩한다. 그러나 매개변수 지식은 훈련 중 본 물리 법칙만 포함하므로 이 법칙을 위반하는 경우의 추론에는 부족하다. 우리는 이를 해결하기 위해 인덕티브 물리 추론을 사용하는 첫 번째 시각적 질문 응답 벤치마크인 InPhyRe를 제안하며, 이를 통해 LMM의 물리적 추론 능력을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 다중모달 모델(LMM)은 안전-critical한 응용 프로그램에서 인간 대리인을 대체하기 위해 인간처럼 보지 않은 물리 학습 환경에 적응할 수 있는 능력이 필요하다.

Method: InPhyRe는 알고리즘적으로 생성된 합성 충돌 비디오에서 충돌 사건의 결과를 예측하는 LMM의 능력을 평가하며, 시각적 입력에 대한 응답을 테스트한다.

Result: 13개의 LMM을 조사한 결과 LMM은 보편적인 물리 법칙에 대한 제한된 매개변수 지식을 적용하는 데 어려움을 겪고, LMM의 인덕티브 물리 추론은 보편적인 물리 법칙을 위반한 시연 샘플에서 약하다. 또한 인덕티브 물리 추론은 언어 편향의 영향을 받으며 시각적 입력을 대체로 무시하는 경향이 있다.

Conclusion: 이는 LMM의 신뢰성을 의심하게 하며, 시각적 입력에 대한 처리 문제를 보여준다.

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [28] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 이 논문은 LLM-Assisted 길 찾기 시스템을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 자연어 기반 경로 계획 연구의 필요성과 기존 접근 방식의 한계 극복.

Method: LLM을 파서로 사용하여 자연어 이해, 작업 식별, 사용자 선호도 추출 및 작업 의존성 인식, 이를 바탕으로 MSGS 알고리즘으로 최적 경로 탐색.

Result: 1,000개의 다양한 경로 요청을 통해 향상된 성능과 여러 제약 조건 하에서도 우수한 결과를 확인.

Conclusion: 제안된 시스템은 POI 품질과 작업 완료율을 극대화하면서 경로 거리를 최소화하는 다목적 최적화 접근 방식을 효과적으로 활용한다.

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [29] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant는 과학적 워크플로우 생성을 단순화하기 위한 인간-인공지능 협업 프레임워크로, 초안 작성 효율성과 주제 일관성을 향상시킵니다. 그러나 여전히 인간-인공지능 협력은 사실 정확성과 방법론적 적합성을 유지하는 데 중요합니다.


<details>
  <summary>Details</summary>
Motivation: AI 지원 연구의 발전이 문헌 검색, 가설 생성, 실험 및 원고 준비를 위한 강력한 도구를 도입했지만 시스템이 단편적이고 인간 중심의 워크플로가 부족한 문제를 해결하기 위해 이 연구를 진행했습니다.

Method: AIssistant는 과학적 워크플로우의 끝에서 끝까지 생성을 단순화하기 위해 설계된 에이전틱한 오픈 소스 프레임워크로, 문헌 합성, 섹션별 실험, 인용 관리 및 자동 LaTeX 논문 텍스트 생성을 위한 모듈식 도구와 에이전트를 통합합니다.

Result: AIssistant는 초안 작성 효율성과 주제 일관성을 향상시키며, NeurIPS 이중 블라인드 기준에 따라 독립적인 인간 검토, GPT-5를 활용한 자동화된 LLM 검토, 전체 검토 과정을 감독하는 프로그램의 의장 감독을 통해 포괄적인 평가를 수행했습니다.

Conclusion: 그럼에도 불구하고 사실 정확성, 방법론적 타당성 및 윤리적 준수를 유지하기 위해 인간-인공지능의 협력이 여전히 필수적이며, 주요 한계로는 환각된 인용, 동적 논문 구조에 대한 적응의 어려움, 다중 모달 콘텐츠의 불완전한 통합이 있습니다.

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [30] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: UI 상호작용 경로로부터 사용자 의도를 이해하는 것은 지능형 에이전트 개발의 중요한 과제이다.


<details>
  <summary>Details</summary>
Motivation: 사용자 의도를 정확하게 추론하는 것이 필수적인데, 특히 개인 정보 보호와 비용, 지연을 고려할 때 더 작은 모델이 필요하다.

Method: 구조화된 상호작용 요약을 수행하여 사용자의 주요 행동 정보를 포착한 후, 집계된 요약을 기반으로 세밀하게 조정된 모델을 사용하여 의도 추출을 수행한다.

Result: 이 방법은 자원이 제한된 모델에서 사용자 의도를 이해하는 데 도움을 주며 대형 MLLM의 기본 성능을 초월한다.

Conclusion: 우리의 접근 방식은 낮은 비용과 지연으로 사용자 경험을 제공하면서도 사용자의 의도 이해를 향상시킨다.

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [31] [HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making](https://arxiv.org/abs/2509.12927)
*Xingxing Hong,Yungong Wang,Dexin Jin,Ye Yuan,Ximing Huang,Zijian Wu,Wenxin Li*

Main category: cs.AI

TL;DR: HLSMAC은 고수준 전략적 의사결정 능력을 평가하기 위한 새로운 협동형 MARL 벤치마크로, 12개의 StarCraft II 시나리오를 통해 다양한 전략 요소를 도전하게 한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 MARL 벤치마크는 미세 관리에 중점을 두어 고수준 전략 지능 평가를 제한하고 있다.

Method: HLSMAC은 36계략의 고전적 계략에 기반한 12개의 StarCraft II 시나리오로 구성되며, 다양한 전략 요소를 포함하도록 설계되었다.

Result: HLSMAC는 큐리타에 대한 평가를 위해 새로운 지표를 제안하고, 현대 MARL 알고리즘과 LLM 기반 에이전트를 통합하여 종합 실험을 수행하였다.

Conclusion: HLSMAC는 다중 에이전트 전략적 의사결정 발전을 위한 강력한 테스트베드로 나타났다.

Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning
(MARL) algorithms. While StarCraft II-related environments have driven
significant advances in MARL, existing benchmarks like SMAC focus primarily on
micromanagement, limiting comprehensive evaluation of high-level strategic
intelligence. To address this, we introduce HLSMAC, a new cooperative MARL
benchmark with 12 carefully designed StarCraft II scenarios based on classical
stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a
specific stratagem and is designed to challenge agents with diverse strategic
elements, including tactical maneuvering, timing coordination, and deception,
thereby opening up avenues for evaluating high-level strategic decision-making
capabilities. We also propose novel metrics across multiple dimensions beyond
conventional win rate, such as ability utilization and advancement efficiency,
to assess agents' overall performance within the HLSMAC environment. We
integrate state-of-the-art MARL algorithms and LLM-based agents with our
benchmark and conduct comprehensive experiments. The results demonstrate that
HLSMAC serves as a robust testbed for advancing multi-agent strategic
decision-making.

</details>


### [32] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM)의 소프트웨어 엔지니어링 문제 해결을 위한 새로운 접근 방식인 \\sys를 제안하여, 다중 턴의 도구 보조 환경에 적합한 선호 최적화 알고리즘을 보완하는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 소프트웨어 엔지니어링 문제는 LLM이 대규모 코드베이스를 처리하고 도구를 협력적으로 사용해야 하므로 복잡하고 다단계의 도전 과제를 제시합니다.

Method: 우리는 선호 최적화 알고리즘을 다중 턴 도구 지원 환경에 맞게 조정하는 엔트로피 향상 프레임워크인 \\sys를 소개하고, 정책 엔트로피를 보존하는 방향으로 목적을 확장하며 다중 턴 상호작용을 최적화합니다.

Result: 우리는 다양한 모델을 조정하고 TTS의 성능 향상을 위한 하이브리드 최적 경로 선택 기법을 제안하여, \\swebench 리더보드에서 새로운 최첨단 결과를 달성했습니다.

Conclusion: 결과적으로, \\sys를 사용하여 훈련된 30B 매개변수 모델이 공개 가중치 리더보드에서 1위와 4위를 차지하며, 10배 이상 많은 매개변수를 가진 모델에만 뒤처졌습니다.

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [33] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 재정 범죄 컴플라이언스(FCC)의 비용과 복잡성이 계속 증가하고 있으며, 이로 인해 효과성 향상은 미비하다. 본 논문은 디지털 네이티브 금융 플랫폼을 위한 에이전틱 AI 시스템의 설계 및 배포를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 재정 범죄 컴플라이언스(FCC)의 비용과 복잡성을 조절하면서 효과성을 높이는 방안이 필요하다.

Method: 핀테크 기업 및 규제 이해관계자와 함께 실시한 행동 설계 연구(ADR) 과정을 통해 시스템을 개발하였다.

Result: 시스템은 온보딩, 모니터링, 조사 및 보고를 자동화하며, 설명 가능성, 추적 가능성, 설계에 의한 컴플라이언스를 강조한다.

Conclusion: 에이전틱 AI는 규제 제약 하에서 FCC 워크플로우를 재구성하는 데 기여하며, 자동화가 책임 있는 거버넌스 구조에 내재될 경우 투명성과 제도적 신뢰를 지원할 수 있음을 보여준다.

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [34] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: 고객 경험(CX) 향상을 위한 Agentic AI 시스템을 소개하며, 이를 통해 에이전트의 인지 부하를 줄이고 고객 지원을 개선할 수 있다.


<details>
  <summary>Details</summary>
Motivation: AI가 발전했음에도 불구하고, 고객 경험(CX)은 여전히 높은 평균 처리 시간(AHT), 낮은 첫 통화 해결률, 그리고 낮은 고객 만족도(CSAT) 때문에 고통받고 있다.

Method: Agentic AI는 고객의 의도를 식별하고 모듈화된 워크플로를 촉발하며, 진화하는 맥락을 유지하고 대화 상태에 동적으로 적응하는 목표 지향적이고 자율적인 도구 사용 시스템이다.

Result: 실시간 고객 지원에 배포된 Minerva CQ의 사례 연구를 통해, Agentic AI가 에이전트의 효율성과 고객 경험을 개선하는데 측정 가능한 성과를 거두었다.

Conclusion: Minerva CQ는 실시간 전사, 의도 및 감정 탐지, 엔티티 인식, 맥락 검색 및 동적 고객 프로파일링 등의 기능을 통합하여 선제적 워크플로와 지속적인 맥락 구축을 가능하게 한다.

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [35] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: Match Chat는 경기 관련 질문에 대한 신속하고 정확한 응답을 제공하여 테니스 팬 경험을 향상시키기 위해 설계된 실시간 에이전트 기반 비서이다.


<details>
  <summary>Details</summary>
Motivation: 테니스 팬들에게 실시간으로 경기 관련 정보를 제공하여 향상된 경험을 선사하기 위해 본 시스템을 개발하였다.

Method: Generative Artificial Intelligence(GenAI)와 Generative Computing(GenComp) 기술을 결합하여 라이브 테니스 단식 경기 중 핵심 인사이트를 합성하는 구조로 설계되었다.

Result: Match Chat 시스템은 120건의 요청을 초당으로 처리하면서 92.83%의 응답 정확도와 6.25초의 평균 응답 시간을 기록하였다.

Conclusion: 이 연구는 속도, 정확성 및 사용성을 강조하는 실시간 소비자 대상 AI 시스템을 위한 주요 설계 패턴을 도입하였으며, 역동적인 환경에서 성능 높은 에이전트 시스템을 배치하기 위한 실질적인 경로를 제시한다.

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [36] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: GBV-SQL이라는 새로운 다중 에이전트 프레임워크를 제안하여 Text2SQL 생성에서의 의미적 간극을 해소하며, 벤치마크의 품질 문제를 지적하고 이를 개선하기 위한 방법론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 Text2SQL 생성에서 상당한 진전을 이루었지만, 구문적으로 유효한 쿼리가 종종 사용자 의도를 잘못 해석하는 중요한 의미적 간극이 남아 있다.

Method: GBV-SQL은 SQL2Text 역번역 검증을 통한 안내 생성을 도입하는 새로운 다중 에이전트 프레임워크이다. 이 메커니즘은 생성된 SQL을 자연어로 다시 번역하여 원래 질문과의 논리적 정렬을 검증하는 전문화된 에이전트를 사용한다.

Result: BIRD 벤치마크에서 GBV-SQL은 63.23%의 실행 정확도를 달성하며, 이는 5.8%의 절대 개선을 보여준다. 결함 있는 예제를 제거한 후, GBV-SQL은 Spider 벤치마크에서 96.5%(개발) 및 97.6%(테스트)의 실행 정확도를 달성한다.

Conclusion: 우리의 작업은 의미적 검증을 위한 견고한 프레임워크와 벤치마크 무결성에 대한 비판적인 관점을 제공하며, 보다 철저한 데이터셋 관리의 필요성을 강조한다.

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [37] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: 계층적 메모리 아키텍처를 통해 다중 작업 상황에서의 지식 전이를 개선하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 접근법은 경험과 지식을 단일한 단위로 취급하여 비효율적인 지식 전이를 초래한다.

Method: 고급 계획 메모리와 저급 실행 메모리를 분리하여 세분화된 지식 전이를 가능하게 하는 계층적 메모리 아키텍처와 과거 상호작용에서 재사용 가능한 지식을 추출하는 H$^2$R 메커니즘을 소개한다.

Result: H$^2$R은 고급 및 저급 메모리를 별도로 검색하여 새로운 작업에 필요한 지식을 효율적으로 활용하도록 한다.

Conclusion: 실험 결과, H$^2$R은 일반화 및 의사결정 성능을 향상시켜 이전의 기준선을 초과한다.

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [38] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: 이 논문은 복잡한 작업을 수행할 수 있는 자율 에이전트로서 대형 언어 모델(LLMs)의 한계를 극복하는 Planning Copilot을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: LLMs는 복잡한 작업을 수행할 수 있지만, 신뢰할 수 있는 장기 계획 수행 능력이 부족합니다.

Method: Planning Copilot은 여러 계획 도구를 통합하고 사용자에게 자연어로 지시하여 이를 호출할 수 있게 해줍니다. 모델 컨텍스트 프로토콜(MCP)을 활용하여 LLM과 외부 도구 및 시스템을 연결합니다.

Result: Planning Copilot은 LLM 도구 없이 동일한 LLM을 사용할 때보다 성능이 크게 향상됩니다.

Conclusion: 전용 계획 도구는 LLM이 계획 작업을 수행하는 데 효과적일 수 있음을 시사합니다.

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [39] [A Visualized Framework for Event Cooperation with Generative Agents](https://arxiv.org/abs/2509.13011)
*Yuyang Tian,Shunqiang Mao,Wenchang Gao,Lanlan Qiu,Tianxing He*

Main category: cs.AI

TL;DR: 본 연구는 MiniAgentPro라는 시각화 플랫폼을 개발하여 에이전트 사회의 시뮬레이션을 개선하고, 다양한 이벤트 시나리오를 통해 에이전트의 능력을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)의 발전에 따라 에이전트 사회 시뮬레이션이 혁신되었지만, 기존 프레임워크는 이벤트 조직에 대한 체계적인 평가를 간과하고 있으며, 현실적으로 물리적 환경과 통합되지 않은 점이 문제로 지적된다.

Method: MiniAgentPro라는 사용자 친화적인 맵 편집기와 매끄러운 애니메이션을 지원하는 시뮬레이션 플레이어를 포함한 시각화 플랫폼을 개발하였다. 이 도구를 바탕으로 기본 및 난이도 높은 변형을 포함한 여덟 가지 다양한 이벤트 시나리오로 구성된 포괄적인 테스트 세트를 도입했다.

Result: GPT-4o를 활용한 평가에서 기본 설정에서 강력한 성능을 보였으나, 난이도 높은 변형에서는 조정 문제가 드러났다.

Conclusion: 에이전트의 능력을 평가하는 데 있어, 시뮬레이션 환경의 시각화와 이벤트 조직의 중요성을 강조한다.

Abstract: Large Language Models (LLMs) have revolutionized the simulation of agent
societies, enabling autonomous planning, memory formation, and social
interactions. However, existing frameworks often overlook systematic
evaluations for event organization and lack visualized integration with
physically grounded environments, limiting agents' ability to navigate spaces
and interact with items realistically. We develop MiniAgentPro, a visualization
platform featuring an intuitive map editor for customizing environments and a
simulation player with smooth animations. Based on this tool, we introduce a
comprehensive test set comprising eight diverse event scenarios with basic and
hard variants to assess agents' ability. Evaluations using GPT-4o demonstrate
strong performance in basic settings but highlight coordination challenges in
hard variants.

</details>


### [40] [A Scenario-Driven Cognitive Approach to Next-Generation AI Memory](https://arxiv.org/abs/2509.13235)
*Linyue Cai,Yuyang Cheng,Xiaoding Shao,Huiming Wang,Yong Zhao,Wei Zhang,Kang Li*

Main category: cs.AI

TL;DR: 이 논문은 인공지능이 일반 인공지능(AGI)으로 발전함에 따라 인상적인 기억 시스템의 필요성을 강조하며, 이를 위해 시나리오 기반 방법론과 새로운 기억 아키텍처를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 인공지능이 AGI로 발전하면서 강력하고 인간과 유사한 기억 시스템의 필요성이 증가하고 있습니다.

Method: 대표적인 인지 시나리오에서 필수 기능 요구 사항을 추출하는 시나리오 기반 방법론을 제안합니다. 이 접근 방식을 바탕으로 COLMA라는 새로운 프레임워크를 소개합니다.

Result: COLMA는 인지 시나리오, 기억 프로세스 및 저장 메커니즘을 통합하여 일관된 설계를 제공합니다.

Conclusion: COLMA는 평생 학습 및 인간과 유사한 추론이 가능한 AI 시스템 개발을 위한 구조적 기초를 제공하며 AGI의 실용적 발전에 기여합니다.

Abstract: As artificial intelligence advances toward artificial general intelligence
(AGI), the need for robust and human-like memory systems has become
increasingly evident. Current memory architectures often suffer from limited
adaptability, insufficient multimodal integration, and an inability to support
continuous learning. To address these limitations, we propose a scenario-driven
methodology that extracts essential functional requirements from representative
cognitive scenarios, leading to a unified set of design principles for
next-generation AI memory systems. Based on this approach, we introduce the
\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that
integrates cognitive scenarios, memory processes, and storage mechanisms into a
cohesive design. COLMA provides a structured foundation for developing AI
systems capable of lifelong learning and human-like reasoning, thereby
contributing to the pragmatic development of AGI.

</details>


### [41] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: Shapes of cognition는 언어가 부여된 지능형 에이전트(LEIA)의 계산적 인지 모델링을 위한 새로운 개념적 패러다임이다. 이 패러다임은 복잡한 실제 상황을 다루기 위한 에이전트의 방법론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 실제 상황을 효율적으로 처리할 수 있는 지능형 에이전트를 개발하기 위해.

Method: 감각적, 언어적, 개념적, 사건적 및 절차적 지식을 활용하여 에이전트가 전형적인 상황을 기대하고, 패턴을 인식하며, 습관적으로 행동하고, 유추에 의존하며, 인지 부하를 최소화하도록 모델링한다.

Result: Shapes 기반 회복 방법을 통해 atypical outcomes를 처리하며, 이는 실시간 학습, 인간 파트너에게 도움 요청, 상황 이해 탐색 등의 프로세스를 포함한다.

Conclusion: 이 모델링 접근법은 설명 가능하고, 확장 가능하며, 신뢰할 수 있는 유용한 에이전트 시스템을 구축하는 데 필요하다.

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [42] [PromptSculptor: Multi-Agent Based Text-to-Image Prompt Optimization](https://arxiv.org/abs/2509.12446)
*Dawei Xiang,Wenyan Xu,Kexin Chu,Zixu Shen,Tianqi Ding,Wei Zhang*

Main category: cs.MA

TL;DR: PromptSculptor는 사용자가 생성한 짧고 모호한 프롬프트를 보다 포괄적이고 정제된 프롬프트로 자동으로 변환하는 다중 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 생성 AI의 발전으로 강력한 도구에 대한 접근이 민주화되었지만, 고품질 이미지를 생성하기 위해서는 여전히 사용자들이 세부적인 프롬프트를 작성해야 한다.

Method: PromptSculptor는 네 개의 전문 에이전트가 협력하여 사용자 프롬프트를 정제하는 과정을 자동화한다. Chain-of-Thought 추론을 활용하여 숨겨진 문맥을 추론하고 장면 및 배경 세부 사항을 풍부하게 한다.

Result: 실험 결과 PromptSculptor는 출력 품질을 크게 향상시키고 사용자 만족도를 위한 반복 횟수를 줄이는 데 효과적이라는 것을 보여준다.

Conclusion: 모델 agnostic 설계를 통해 다양한 T2I 모델과 원활하게 통합할 수 있으며, 산업적 응용 가능성을 넓힌다.

Abstract: The rapid advancement of generative AI has democratized access to powerful
tools such as Text-to-Image models. However, to generate high-quality images,
users must still craft detailed prompts specifying scene, style, and
context-often through multiple rounds of refinement. We propose PromptSculptor,
a novel multi-agent framework that automates this iterative prompt optimization
process. Our system decomposes the task into four specialized agents that work
collaboratively to transform a short, vague user prompt into a comprehensive,
refined prompt. By leveraging Chain-of-Thought reasoning, our framework
effectively infers hidden context and enriches scene and background details. To
iteratively refine the prompt, a self-evaluation agent aligns the modified
prompt with the original input, while a feedback-tuning agent incorporates user
feedback for further refinement. Experimental results demonstrate that
PromptSculptor significantly enhances output quality and reduces the number of
iterations needed for user satisfaction. Moreover, its model-agnostic design
allows seamless integration with various T2I models, paving the way for
industrial applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [43] [Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics](https://arxiv.org/abs/2509.12233)
*Meryem Malak Dif,Mouhamed Amine Bouchiha,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: IoEV을 위해 특별히 설계된 에이전틱 인공지능(AAI) 프레임워크를 소개하며, 보안 및 예측 정확도에서 상당한 개선을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 전기차, 충전 인프라 및 전력망 서비스의 밀접한 생태계인 IoEV는 사이버 공격, 신뢰할 수 없는 배터리 상태 예측 및 불투명한 의사 결정 과정에 취약하다.

Method: 전문화된 에이전트가 협력하여 사이버 위협 탐지 및 대응, 실시간 충전 상태 추정, 상태 이상 탐지를 수행하며, 공유되고 설명 가능한 추론 계층을 통해 조정되는 AAI 아키텍처를 설계한다.

Result: 범위가 다양한 IoEV 시나리오에 대해 포괄적인 실험을 통해 우리의 프레임워크의 유효성을 검증하며, 보안 및 예측 정확도에서 유의미한 개선을 입증한다.

Conclusion: 모든 데이터셋, 모델 및 코드는 공개될 예정이다.

Abstract: The Internet of Electric Vehicles (IoEV) envisions a tightly coupled
ecosystem of electric vehicles (EVs), charging infrastructure, and grid
services, yet it remains vulnerable to cyberattacks, unreliable battery-state
predictions, and opaque decision processes that erode trust and performance. To
address these challenges, we introduce a novel Agentic Artificial Intelligence
(AAI) framework tailored for IoEV, where specialized agents collaborate to
deliver autonomous threat mitigation, robust analytics, and interpretable
decision support. Specifically, we design an AAI architecture comprising
dedicated agents for cyber-threat detection and response at charging stations,
real-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly
detection, all coordinated through a shared, explainable reasoning layer;
develop interpretable threat-mitigation mechanisms that proactively identify
and neutralize attacks on both physical charging points and learning
components; propose resilient SoC and SoH models that leverage continuous and
adversarial-aware learning to produce accurate, uncertainty-aware forecasts
with human-readable explanations; and implement a three-agent pipeline, where
each agent uses LLM-driven reasoning and dynamic tool invocation to interpret
intent, contextualize tasks, and execute formal optimizations for user-centric
assistance. Finally, we validate our framework through comprehensive
experiments across diverse IoEV scenarios, demonstrating significant
improvements in security and prediction accuracy. All datasets, models, and
code will be released publicly.

</details>


### [44] [Redefining Website Fingerprinting Attacks With Multiagent LLMs](https://arxiv.org/abs/2509.12462)
*Chuxu Song,Dheekshith Dev Manohar Mekala,Hao Wang,Richard Martin*

Main category: cs.CR

TL;DR: 현대 웹 환경에 대한 일반화가 어려운 기존의 웹사이트 지문 인식(WFP) 기술 문제를 해결하기 위해, 제안된 새로운 패러다임은 세션 경계를 없애고 대형 언어 모델(LLM) 에이전트를 활용한 데이터 생성 파이프라인을 개발하여 현실적인 사용자의 행동을 시뮬레이션합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 WFP 방법들이 현대의 웹 환경에 효과적으로 일반화되지 못하고 있어, 새로운 데이터 세트와 방법론의 필요성을 제기합니다.

Method: 세션 경계를 제거하고 연속적인 트래픽 세그먼트를 사용하며, LLM 에이전트를 활용하여 규모있는 데이터 생성 파이프라인을 개발합니다.

Result: LLM 생성 트래픽의 경우 정확도가 80%대에 도달하였으며, 도출된 모델들은 데이터 품질에 의해 성능이 제한받고 있음을 알 수 있습니다.

Conclusion: 데이터 품질이 모델 성능에 중대한 영향을 미친다는 것이 드러났으며, 현실적인 사용자 행동을 포착하기 위한 규모 있는 합성 트래픽 확보가 필수적입니다.

Abstract: Website Fingerprinting (WFP) uses deep learning models to classify encrypted
network traffic to infer visited websites. While historically effective, prior
methods fail to generalize to modern web environments. Single-page applications
(SPAs) eliminate the paradigm of websites as sets of discrete pages,
undermining page-based classification, and traffic from scripted browsers lacks
the behavioral richness seen in real user sessions. Our study reveals that
users exhibit highly diverse behaviors even on the same website, producing
traffic patterns that vary significantly across individuals. This behavioral
entropy makes WFP a harder problem than previously assumed and highlights the
need for larger, more diverse, and representative datasets to achieve robust
performance. To address this, we propose a new paradigm: we drop
session-boundaries in favor of contiguous traffic segments and develop a
scalable data generation pipeline using large language models (LLM) agents.
These multi-agent systems coordinate decision-making and browser interaction to
simulate realistic, persona-driven browsing behavior at 3--5x lower cost than
human collection. We evaluate nine state-of-the-art WFP models on traffic from
20 modern websites browsed by 30 real users, and compare training performance
across human, scripted, and LLM-generated datasets. All models achieve under
10\% accuracy when trained on scripted traffic and tested on human data. In
contrast, LLM-generated traffic boosts accuracy into the 80\% range,
demonstrating strong generalization to real-world traces. Our findings indicate
that for modern WFP, model performance is increasingly bottlenecked by data
quality, and that scalable, semantically grounded synthetic traffic is
essential for capturing the complexity of real user behavior.

</details>


### [45] [xOffense: An AI-driven autonomous penetration testing framework with offensive knowledge-enhanced LLMs and multi agent systems](https://arxiv.org/abs/2509.13021)
*Phung Duc Luong,Le Tran Gia Bao,Nguyen Vu Khai Tam,Dong Huu Nguyen Khoa,Nguyen Huu Quyen,Van-Hau Pham,Phan The Duy*

Main category: cs.CR

TL;DR: xOffense는 AI 기반의 다중 에이전트 침투 테스트 프레임워크로, 수작업에서 자동화된 워크플로우로의 전환을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 침투 테스트의 수작업에서의 비용과 시간을 줄이기 위해 자동화된 솔루션을 개발하고자 했다.

Method: xOffense는 Qwen3-32B이라는 중규모 오픈 소스 LLM을 활용하여 침투 테스트의 추론 및 의사 결정을 수행하며, 특정 작업에 따라 전문 에이전트를 배정하고 오케스트레이션 계층을 두어 원활한 조정을 보장한다.

Result: xOffense는 AutoPenBench와 AI-Pentest-Benchmark 두 가지 엄격한 벤치마크에서 평가되었으며, 79.17%의 하위 작업 완료율을 달성하면서 VulnBot과 PentestGPT와 같은 기존 시스템들을 초월했다.

Conclusion: 도메인에 적합한 중규모 LLM이 구조화된 다중 에이전트 오케스트레이션 내에 통합될 때, 자율 침투 테스트를 위한 우수하고 비용 효율적인 솔루션을 제공할 수 있다는 가능성을 보여준다.

Abstract: This work introduces xOffense, an AI-driven, multi-agent penetration testing
framework that shifts the process from labor-intensive, expert-driven manual
efforts to fully automated, machine-executable workflows capable of scaling
seamlessly with computational infrastructure. At its core, xOffense leverages a
fine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and
decision-making in penetration testing. The framework assigns specialized
agents to reconnaissance, vulnerability scanning, and exploitation, with an
orchestration layer ensuring seamless coordination across phases. Fine-tuning
on Chain-of-Thought penetration testing data further enables the model to
generate precise tool commands and perform consistent multi-step reasoning. We
evaluate xOffense on two rigorous benchmarks: AutoPenBench and
AI-Pentest-Benchmark. The results demonstrate that xOffense consistently
outperforms contemporary methods, achieving a sub-task completion rate of
79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.
These findings highlight the potential of domain-adapted mid-scale LLMs, when
embedded within structured multi-agent orchestration, to deliver superior,
cost-efficient, and reproducible solutions for autonomous penetration testing.

</details>


### [46] [SLasH-DSA: Breaking SLH-DSA Using an Extensible End-To-End Rowhammer Framework](https://arxiv.org/abs/2509.13048)
*Jeremy Boy,Antoon Purnal,Anna Pätschke,Luca Wilke,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 이 논문에서는 SLH-DSA에 대한 첫 번째 소프트웨어 전용 보편적 위조 공격을 제시하며, Rowhammer로 유도된 비트 플립을 활용하여 내부 상태를 손상시키고 서명을 위조하는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨팅의 발전에 따라, PQC 방식이 고전 알고리즘을 대체하기 위해 채택되고 있습니다. 이 연구는 SLH-DSA의 보안을 위협하는 새로운 공격 방법을 제시합니다.

Method: Rowhammer로 유도된 비트 플립을 활용하여 SLH-DSA의 내부 상태를 손상시키고 서명을 위조하는 소프트웨어 전용 공격을 구현했습니다. 공격 프레임워크인 Swage를 도입하여 Rowhammer 기반 오류 공격을 지원합니다.

Result: OpenSSL 3.5.1에서 SLH-DSA의 모든 보안 수준에 대한 완전한 종료 공격을 수행했으며, 최고 보안 수준에서 8시간의 해머링과 36초의 후처리로 보편적 위조를 달성했습니다.

Conclusion: 이 연구 결과는 이론적으로 안전한 PQC 방식조차도 실제 환경에서 실패할 수 있음을 강조하며, Rowhammer에 대한 추가적인 구현 강화나 하드웨어 방어가 필요함을 보여줍니다.

Abstract: As quantum computing advances, PQC schemes are adopted to replace classical
algorithms. Among them is the SLH-DSA that was recently standardized by NIST
and is favored for its conservative security foundations.
  In this work, we present the first software-only universal forgery attack on
SLH-DSA, leveraging Rowhammer-induced bit flips to corrupt the internal state
and forge signatures. While prior work targeted embedded systems and required
physical access, our attack is software-only, targeting commodity desktop and
server hardware, significantly broadening the threat model. We demonstrate a
full end-to-end attack against all security levels of SLH-DSA in OpenSSL 3.5.1,
achieving universal forgery for the highest security level after eight hours of
hammering and 36 seconds of post-processing. Our post-processing is informed by
a novel complexity analysis that, given a concrete set of faulty signatures,
identifies the most promising computational path to pursue.
  To enable the attack, we introduce Swage, a modular and extensible framework
for implementing end-to-end Rowhammer-based fault attacks. Swage abstracts and
automates key components of practical Rowhammer attacks. Unlike prior tooling,
Swage is untangled from the attacked code, making it reusable and suitable for
frictionless analysis of different targets. Our findings highlight that even
theoretically sound PQC schemes can fail under real-world conditions,
underscoring the need for additional implementation hardening or hardware
defenses against Rowhammer.

</details>
