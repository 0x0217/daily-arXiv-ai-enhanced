<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning](https://arxiv.org/abs/2512.20629)
*Wenlong Tang*

Main category: cs.LG

TL;DR: 이 연구는 언어 모델의 파라미터 조정 없이 지속적인 전략 진화를 가능하게 하는 다중 에이전트 언어 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 정적 의미 표현에서 벗어나, 환경 상호작용과 강화 피드백을 통해 추상 개념의 잠재 벡터를 지속적으로 업데이트하는 것이 필요하다.

Method: 행동 루프는 환경 보상에 따라 행동 선호도를 조정하고, 언어 루프는 생성된 텍스트의 의미 임베딩을 반영하여 외부 잠재 벡터를 업데이트하는 이중 루프 구조를 구축한다.

Result: 에이전트의 잠재 공간은 반영 기반 업데이트 하에서 명확한 수렴 궤적을 보이며, 중요한 순간에 구조적 변화를 나타낸다.

Conclusion: 모델 파라미터를 수정하지 않고도 외부 잠재 공간이 언어 에이전트에 저비용, 확장 가능하며 해석 가능한 추상 전략 표현을 제공할 수 있음을 보여준다.

Abstract: This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.
  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation.

</details>


### [2] [Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering](https://arxiv.org/abs/2512.20660)
*Matthew Thompson*

Main category: cs.LG

TL;DR: 이 논문은 AI 코딩 에이전트의 현재 접근 방식을 분석하고, 확률적 실패에 취약한 기존의 LLM-기반 시스템의 한계를 극복하기 위한 새로운 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 코딩 에이전트의 현재 접근方式에서 LLM과 에이전트 간의 경계가 모호해지고 있으며, 이는 결정론적 과정에 의존해야 할 결정을 LLM에게 맡기게 만든다.

Method: 이 논문은 제어 경계를 설정하여 LLM을 결정-making agent가 아닌 환경의 구성 요소로 취급하는 이중 상태 아키텍처를 제안하며, 아토믹 액션 쌍을 통해 생성과 검증을 결합하고, 가드 함수를 통해 확률적 출력을 관찰 가능한 흐름 상태로 투사한다.

Result: 13개의 LLM(1.3B–15B 파라미터)에 대한 세 가지 코드 생성 작업을 통해 프레임워크가 검증되었으며, 자격을 갖춘 지침 따르기 모델에서는 작업 성공률이 최대 66% 증가했고, 1.2–2.1배의 기준 계산 비용이 들었다.

Conclusion: 건축상의 제약이 신뢰할 수 있는 코드 생성을 달성하는 데 있어 매개변수 크기를 대체할 수 있음을 시사한다.

Abstract: Current approaches to AI coding agents appear to blur the lines between the Large Language Model (LLM) and the agent itself, asking the LLM to make decisions best left to deterministic processes. This leads to systems prone to stochastic failures such as gaming unit tests or hallucinating syntax. Drawing on established software engineering practices that provide deterministic frameworks for managing unpredictable processes, this paper proposes setting the control boundary such that the LLM is treated as a component of the environment environment -- preserving its creative stochasticity -- rather than the decision-making agent.
  A \textbf{Dual-State Architecture} is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation). \textbf{Atomic Action Pairs} couple generation with verification as indivisible transactions, where \textbf{Guard Functions} act as sensing actions that project probabilistic outputs onto observable workflow state. The framework is validated on three code generation tasks across 13 LLMs (1.3B--15B parameters). For qualified instruction-following models, task success rates improved by up to 66 percentage points at 1.2--2.1$\times$ baseline computational cost. The results suggest that architectural constraints can substitute for parameter scale in achieving reliable code generation.

</details>


### [3] [GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface](https://arxiv.org/abs/2512.20813)
*Miguel Esparza,Vamshi Battal,Ali Mostafavi*

Main category: cs.LG

TL;DR: 화재 위험 분석을 위해 새로운 이중 전문가 앙상블 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 위험 모델이 도시 화재를 accurately 반영하지 못하기 때문에 새로운 접근법이 필요하다.

Method: 환경 전염과 구조적 취약성을 분리하는 이중 전문가 앙상블 프레임워크를 설정하고, GNN과 XGBoost를 활용한다.

Result: Eaton 화재에 적용한 결과, 환경 압력이 구조적 특성보다 중요한 영향을 미침을 보여주었고, eaves가 주요 침입 경로로 확인되었다.

Conclusion: 이 프레임워크는 데이터 기반의 접근법으로 화재 위험을 진단하고 예방 조치를 우선적으로 결정할 수 있는 능력을 제공한다.

Abstract: As wildfires increasingly evolve into urban conflagrations, traditional risk models that treat structures as isolated assets fail to capture the non-linear contagion dynamics characteristic of the wildland urban interface (WUI). This research bridges the gap between mechanistic physics and data driven learning by establishing a novel dual specialist ensemble framework that disentangles vulnerability into two distinct vectors, environmental contagion and structural fragility. The architecture integrates two specialized predictive streams, an environmental specialist, implemented as a graph neural network (GNN) that operationalizes the community as a directed contagion graph weighted by physics informed convection, radiation, and ember probabilities, and enriched with high dimensional Google AlphaEarth Foundation embeddings, and a Structural Specialist, implemented via XGBoost to isolate granular asset level resilience. Applied to the 2025 Eaton Fire, the framework reveals a critical dichotomy in risk drivers. The GNN demonstrates that neighborhood scale environmental pressure overwhelmingly dominates intrinsic structural features in defining propagation pathways, while the XGBoost model identifies eaves as the primary micro scale ingress vector. By synthesizing these divergent signals through logistic stacking, the ensemble achieves robust classification and generates a diagnostic risk topology. This capability empowers decision makers to move beyond binary loss prediction and precisely target mitigation prioritizing vegetation management for high connectivity clusters and structural hardening for architecturally vulnerable nodes thereby operationalizing a proactive, data driven approach to community resilience.

</details>


### [4] [FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative](https://arxiv.org/abs/2512.20814)
*Mohammadreza Rostami,Solmaz S. Kia*

Main category: cs.LG

TL;DR: FedMPDD는 연합 학습에서 대역폭 활용과 프라이버시를 동시에 최적화하는 새로운 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습의 대역폭 활용을 개선하고 프라이버시를 강화할 필요가 있다.

Method: 클라이언트의 고차원 그래디언트를 여러 랜덤 벡터를 따라 방향 미분을 계산하여 인코딩하여, 그래디언트를 훨씬 작은 메시지로 압축하는 방법.

Result: FedMPDD는 그래디언트 통신 비용을 크게 줄이고, 여러 투영을 평균내어 단일 투영의 수렴 한계를 극복함으로써 $	extless m 	extless d$에서의 수렴 속도가 $	extmath{O}(1/	extsqrt{K})$로 개선되었다.

Conclusion: FedMPDD는 그래디언트 반전 공격에 대한 프라이버시를 제공하며, 다양한 프로젝션 수에 의해 조정 가능한 프라이버시-유틸리티 트레이드 오프를 제공한다.

Abstract: This paper introduces \texttt{FedMPDD} (\textbf{Fed}erated Learning via \textbf{M}ulti-\textbf{P}rojected \textbf{D}irectional \textbf{D}erivatives), a novel algorithm that simultaneously optimizes bandwidth utilization and enhances privacy in Federated Learning. The core idea of \texttt{FedMPDD} is to encode each client's high-dimensional gradient by computing its directional derivatives along multiple random vectors. This compresses the gradient into a much smaller message, significantly reducing uplink communication costs from $\mathcal{O}(d)$ to $\mathcal{O}(m)$, where $m \ll d$. The server then decodes the aggregated information by projecting it back onto the same random vectors. Our key insight is that averaging multiple projections overcomes the dimension-dependent convergence limitations of a single projection. We provide a rigorous theoretical analysis, establishing that \texttt{FedMPDD} converges at a rate of $\mathcal{O}(1/\sqrt{K})$, matching the performance of FedSGD. Furthermore, we demonstrate that our method provides some inherent privacy against gradient inversion attacks due to the geometric properties of low-rank projections, offering a tunable privacy-utility trade-off controlled by the number of projections. Extensive experiments on benchmark datasets validate our theory and demonstrates our results.

</details>


### [5] [From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction](https://arxiv.org/abs/2512.20885)
*Sami Marouani,Kamal Singh,Baptiste Jeudy,Amaury Habrard*

Main category: cs.LG

TL;DR: 본 연구에서는 현대 통신 네트워크의 흐름 지연 예측을 위해 세 가지 모델링 수준을 탐구하며, 여러 Neural Network 구조와 기법을 적용하여 최적화를 시도한다.


<details>
  <summary>Details</summary>
Motivation: 현대 통신 네트워크의 최적화 및 관리를 위해 흐름 지연의 정확한 예측이 필수적이다.

Method: 이 연구에서는 주의 기반 메시지 전달을 사용하는 비동질적인 GNN과 표준 MLP 레이어를 대체하는 Kolmogorov-Arnold Networks을 포함하는 FlowKANet을 제안하며, 마지막으로 블록-와이즈 회귀를 사용하여 기호 대체 모델로 증류한다.

Result: KAN 레이어는 효율성과 정확성 간의 유리한 균형을 제공하며, 기호 대체 모델은 경량 배치 및 투명성 향상의 잠재성을 강조한다.

Conclusion: 이 연구의 결과는 네트워크 흐름 예측의 정확성을 향상시키기 위한 여러 가지 효과적인 접근 방식을 제시한다.

Abstract: Accurate prediction of flow delay is essential for optimizing and managing modern communication networks. We investigate three levels of modeling for this task. First, we implement a heterogeneous GNN with attention-based message passing, establishing a strong neural baseline. Second, we propose FlowKANet in which Kolmogorov-Arnold Networks replace standard MLP layers, reducing trainable parameters while maintaining competitive predictive performance. FlowKANet integrates KAMP-Attn (Kolmogorov-Arnold Message Passing with Attention), embedding KAN operators directly into message-passing and attention computation. Finally, we distill the model into symbolic surrogate models using block-wise regression, producing closed-form equations that eliminate trainable weights while preserving graph-structured dependencies. The results show that KAN layers provide a favorable trade-off between efficiency and accuracy and that symbolic surrogates emphasize the potential for lightweight deployment and enhanced transparency.

</details>


### [6] [ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design](https://arxiv.org/abs/2512.20958)
*R Yadunandan,Nimisha Ghosh*

Main category: cs.LG

TL;DR: ReACT-Drug는 강화 학습 기반의 분자 설계 프레임워크로, 고유한 단백질 임베딩을 활용해 약물 후보를 생성하며, 생물학적으로 관련 있는 하위 공간으로의 탐색을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 신약 개발에서 de novo 약물 설계는 중요하지만, 합성 접근이 가능한 후보 물질을 찾는 것은 여전히 도전 과제가 많다.

Method: ReACT-Drug는 강화 학습을 기반으로 하여 ESM-2 단백질 임베딩을 활용해 특정 타겟에 대한 유사 단백질을 식별하고, 알려진 약물 리간드를 분해하여 조각 기반 탐색 공간을 초기화한다. 이후 PPO 에이전트를 사용하여 ChemBERTa로 인코딩된 분자를 유효한 화학 변환의 동적 행동 공간을 통해 안내한다.

Result: 이 시스템은 경쟁력 있는 결합 친화성과 높은 합성 가능성을 가진 de novo 약물 후보를 생성하며, MOSES 기준에 따라 100% 화학적 유효성과 참신성을 보장한다.

Conclusion: 이 아키텍처는 구조生물학, 깊은 표현 학습, 화학 합성 규칙을 통합하여 합리적 약물 설계를 자동화하고 가속화할 수 있는 잠재력을 강조한다.

Abstract: De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.

</details>


### [7] [Can Agentic AI Match the Performance of Human Data Scientists?](https://arxiv.org/abs/2512.20959)
*An Luo,Jin Du,Fangqiao Tian,Xun Xian,Robert Specht,Ganghua Wang,Xuan Bi,Charles Fleming,Jayanth Srinivasa,Ashish Kundu,Mingyi Hong,Jie Ding*

Main category: cs.LG

TL;DR: 이 연구는 인간 데이터 과학자의 성능에 필적하는 에이전틱 AI의 한계를 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI 시스템이 인간 데이터 과학자의 도메인 지식을 활용한 성능에 미치지 못하는지 탐구합니다.

Method: 예측 작업을 설계하여 관련 이미지 데이터에서 중요한 잠재 변수를 숨깁니다.

Result: 에이전틱 AI는 일반적인 분석 워크플로우에 의존할 경우 한계가 있음을 보여줍니다.

Conclusion: 도메인 지식을 더 잘 인식하고 통합할 수 있는 에이전틱 AI 시스템을 개발할 필요성을 강조합니다.

Abstract: Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.

</details>


### [8] [Measuring all the noises of LLM Evals](https://arxiv.org/abs/2512.21326)
*Sida Wang*

Main category: cs.LG

TL;DR: 신호와 잡음을 분리하는 것은 실험 과학에서 핵심적이다. 본 논문에서는 LLM 평가에 잘 확립된 통계 방법을 효과적으로 적용하기 위한 독특한 잡음 특성을 고려해야 함을 강조한다.


<details>
  <summary>Details</summary>
Motivation: LLM 평가에 통계적 방법을 효과적으로 적용하기 위해 잡음의 독특한 특성을 고려해야 한다.

Method: 모든 LLM 쌍에 대해 분석을 적용하고 수백만 개의 질문 수준 예측을 기반으로 잡음 구성 요소를 측정하는 all-pairs paired 방법을 제안한다.

Result: 각 평가가 모든 모델 쌍에 대해 특성과 매우 예측 가능한 총 잡음 수준을 나타내며, 쌍 예측 잡음이 보통 쌍 데이터 잡음보다 크다는 것을 밝혀냈다.

Conclusion: 이 결과는 실무자들이 맞춤형 테스트 없이 중요성을 평가하고, 조절된 실험에서 더 작은 효과를 감지할 수 있도록 한다.

Abstract: Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization](https://arxiv.org/abs/2512.20623)
*Ravi Gupta,Shabista Haider*

Main category: cs.AI

TL;DR: BitRL-Light는 1비트 양자화된 대형 언어 모델과 DQN 강화 학습을 결합하여 스마트 홈 조명 시스템의 에너지 효율성과 사용자 편안함을 동시에 최적화하는 혁신적인 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 스마트 홈 조명 시스템은 거주지 에너지의 15-20%를 소비하지만 사용자 편안함과 에너지 효율성을 동시에 최적화할 수 있는 적응형 지능이 부족합니다.

Method: 본 연구에서는 Raspberry Pi 하드웨어에 1비트 양자화된 Llama-3.2-1B 모델을 배포하여 DQN 강화 학습을 통해 실시간 스마트 홈 조명 제어를 구현합니다.

Result: 비교 실험 결과, 비규칙 기반 시스템에 비해 32%의 에너지 절약과 Raspberry Pi 4에서 200ms 이하의 추론 대기 시간, 95%의 사용자 만족도를 달성했습니다.

Conclusion: 이 연구는 리소스가 제한된 IoT 장치에 적응형 AI를 배포할 수 있는 실용적인 프레임워크를 확립하였으며, 클라우드 의존성 없이 지능형 홈 자동화를 가능하게 합니다.

Abstract: Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.

</details>


### [10] [Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment](https://arxiv.org/abs/2512.20624)
*Mazyar Taghavi,Javad Vahidi*

Main category: cs.AI

TL;DR: 이 연구는 다중 에이전트 강화 학습에서 탐색-활용 균형을 최적화하기 위한 양자 영감을 받은 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: UAV 지원 6G 네트워크 배치를 위한 최적화 문제를 해결하기 위해.

Method: 고전적인 MARL 알고리즘과 양자 영감을 받은 최적화 기법을 통합하고, 변분 양자 회로(VQC)를 사용하여 결합 최적화를 위한 방법론을 적용했습니다.

Result: 제안된 프레임워크가 샘플 효율성을 개선하고, 수렴 속도를 가속화하며, 강인성을 유지하면서 커버리지 성능을 향상시킨다는 것을 입증했습니다.

Conclusion: QI MARL이 기존 방법에 비해 탐색과 활용의 균형을 우수하게 이룬다는 결과를 보였습니다.

Abstract: This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.

</details>


### [11] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AI 기반 애플리케이션의 급속한 확장은 보안과 책임 추적 등의 문제를 제기하고 있으며, 이를 해결하기 위해 블록체인 기반의 AI 사용 데이터 기록 및 관리 프레임워크인 AiAuditTrack (AAT)을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 기반 애플리케이션의 확산으로 인한 AI 상호작용 데이터 증가가 보안, 책임, 위험 추적 등에서 긴급한 도전 과제를 드러내고 있다.

Method: AAT는 분산 신원(DID) 및 검증 가능한 자격 증명(VC)을 활용하여 신뢰할 수 있는 AI 엔티티를 설정하고, 상호작용 궤적을 체인에 기록하여 시스템 간 감독과 감사를 가능하게 한다. AI 엔티티는 동적 상호작용 그래프의 노드로 모델링되며, 엣지는 시간 특정 행동 궤적을 나타낸다.

Result: 위험 행동의 기원을 추적하고 관련된 엔티티 간에 조기 경고를 전파하는 위험 확산 알고리즘이 제안되었으며, 시스템 성능은 TPS 지표로 평가되어 AAT의 대규모 상호작용 기록시의 실행 가능성과 안정성이 입증되었다.

Conclusion: AAT는 복잡한 다중 에이전트 환경에서 AI 감사를 위한 확장 가능하고 검증 가능한 솔루션을 제공한다.

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [12] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: Transformer 모델의 주의 메커니즘 선택은 모델링 품질과 추론 효율성 간의 중요한 트레이드오프를 포함한다. 본 논문에서는 각 토큰에 대해 최적의 주의 방식(MHA, GQA, 또는 MQA)을 동적으로 선택하는 새로운 아키텍처인 MoAS를 제안한다.


<details>
  <summary>Details</summary>
Motivation: Transformer 모델에서 주의 메커니즘의 선택은 성능과 메모리 효율 간의 균형을 맞추는 것이 중요하다.

Method: Mixture of Attention Schemes (MoAS)는 학습된 라우터를 통해 각 토큰에 대해 최적의 주의 방식을 동적으로 선택하는 아키텍처이다.

Result: 동적 라우팅은 정적 평균 방식보다 우수한 성능을 보이며, MHA 기준 성능과 경쟁력을 유지하면서도 조건부 계산 효율성의 가능성을 제공한다.

Conclusion: WikiText-2에 대한 실험 결과에서 동적 라우팅이 정적 혼합 방식을 초월하는 성과를 보였다.

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [13] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 본 연구는 LLM의 성능 향상을 위해 다중 에이전트를 활용한 반성을 도입하며, 이 방법이 더 나은 다양성을 제공한다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: LLM은 실수를 반성함으로써 추론 작업의 성능을 향상시킬 수 있지만, 동일 LLM의 지속적인 반성이 사고의 퇴화를 초래할 수 있다.

Method: 다중 에이전트와 다중 페르소나의 토론자를 사용하여 반성을 생성하는 방법을 소개한다.

Result: 우리의 실험 결과, LLM 에이전트가 생성한 반성의 다양성이 향상되었음을 발견했다.

Conclusion: 우리는 EM HotPot QA에서 47%의 정확도, HumanEval에서 82.7%의 성능을 나타내었으며, 이는 단일 LLM으로 반성하는 것보다 뛰어나다.

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [14] [AI-Driven Decision-Making System for Hiring Process](https://arxiv.org/abs/2512.20652)
*Vira Filatova,Andrii Zelenchuk,Dmytro Filatov*

Main category: cs.AI

TL;DR: AI 기반의 다중 에이전트 채용 어시스턴트를 통해 채용 후보자의 초기 검증 과정을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 채용 과정에서 이력서, 평가 답변, 코드 과제 및 제한된 공개 증거와 같은 다양한 입력을 조정해야 하는 초기 후보 검증이 큰 병목 현상이다.

Method: 문서 및 비디오 전처리, 구조화된 후보 프로필 구축, 공개 데이터 검증, 기술적 및 문화 적합성 점수 계산, 사람의 검증을 포함하는 AI 기반 모듈식 채용 어시스턴트를 개발하였다.

Result: 64명의 실제 지원자를 평가했으며, 시스템은 경험이 많은 채용 담당자보다 자격이 있는 후보자당 1.70시간을 소요하는 것으로 나타났다.

Conclusion: 시스템은 채용 성과를 개선하고 더 낮은 스크리닝 비용으로 인력을 유지한 채 최종 결정을 내리도록 한다.

Abstract: Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.

</details>


### [15] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 이 논문은 블록체인을 활용한 다중 에이전트 시스템의 아키텍처 모델을 제안하여 자율적인 결정을 내리는 에이전틱 AI 시스템의 신뢰성 및 감시 문제를 해결하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 자율적 의사결정에 있어 에이전틱 AI 시스템의 활용이 증가하고 있으나, 신뢰성, 감독 및 정보 무결성에 대한 우려가 존재한다.

Method: LangChain 기반의 다중 에이전트 시스템과 허가된 블록체인을 결합한 아키텍처 모델를 제시하고, 이를 통해 정책 집행과 에이전틱 행동의 불변 감사 가능성을 보장한다.

Result: 스마트 인벤토리 관리, 교통 신호 제어 및 건강 모니터링 실험 결과, 블록체인 기반의 보안 검증이 무단 관행을 예방하고, 의사 결정 과정 전반에 걸쳐 추적 가능성을 제공하며, 운영 지연 시간을 합리적인 범위로 유지한다는 것을 보여준다.

Conclusion: 제안된 프레임워크는 자율적이면서도 책임 있는 고-impact 에이전틱 AI 응용 프로그램을 구현하기 위한 보편적인 시스템을 제공한다.

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [16] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 본 논문에서는 Transformer 기반 모델의 감정 분석 성능을 향상시키기 위한 Adversarial Feedback for Attention (AFA) 훈련 메커니즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 Transformer 모델들이 특정 시나리오에서 낮은 정확도를 보이는 문제를 해결하고자 함.

Method: AFA 훈련 메커니즘을 통해 모델이 수동 주석 없이 자동으로 주의 가중치를 적절한 초점으로 재분배할 수 있도록 하는 방법을 사용하며, 동적 마스킹 전략을 포함하여 여러 단어를 마스킹하여 판별자를 속이는 과정을 통해 성능을 향상시킴.

Result: 세 개의 공개 데이터 세트에서 우리의 방법이 최신 연구 결과를 초과하는 성과를 거두었음을 보여줌.

Conclusion: 본 훈련 메커니즘을 대형 언어 모델의 주의 향상에 적용했을 때 성능이 12.6% 향상됨을 나타냄.

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [17] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 이 연구는 가계 예산 관리와 식단 최적화를 결합한 가격 인식 AI 시스템을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 중간 소득 환경에서 식품 가격 변동으로 인해 가계 예산과 영양 요구의 제한된 문제는 여전히 도전 과제로 남아 있습니다.

Method: 이 시스템은 가계 소득, 고정 지출, 건강 및 웰빙 상태, 실시간 식품 비용을 고려하여 영양적으로 충분한 식사 계획을 비교적 합리적인 가격으로 자동 조정합니다.

Result: 사우디 가정을 대상으로 한 시뮬레이션 결과, 정적 주간 메뉴에 비해 비용이 12-18% 절감되었고, 영양 적합성은 95% 이상으로 나타났으며, 가격 변화에 대해 높은 성능을 보였습니다.

Conclusion: 이 프레임워크는 저렴함과 영양 적합성을 결합할 수 있으며, 지속 가능한 개발 목표에 맞춰 지속 가능하고 공정한 식단 계획을 위한 가능성을 제공합니다.

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [18] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: 이 논문은 자연어 추론 및 복잡한 수학 문제 해결을 위한 AgentMath라는 에이전트 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LRM(대규모 추론 모델)은 자연어 추론에서 상당한 발전을 이루었지만, 복잡한 수학 연산 문제를 해결할 때 계산 비효율성과 정확성 문제를 겪고 있다.

Method: AgentMath는 자연어 모델의 추론 능력과 코드 해석기의 계산 정확성을 통합하여 복잡한 수학 문제를 효율적으로 해결하는 혁신적인 세 가지 방법을 제공한다.

Result: AgentMath는 AIME24, AIME25, HMMT25와 같은 수학 경진 대회 기준에서 최첨단 성능을 달성하였다. 특히, AgentMath-30B-A3B는 각각 90.6%, 86.4%, 73.8%의 정확도를 기록했다.

Conclusion: 이 결과는 우리의 접근 방식의 효과성을 검증하고, 더 복잡하고 확장 가능한 수학적 추론 에이전트를 구축하는 길을 열어준다.

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [19] [A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents](https://arxiv.org/abs/2512.20798)
*Miles Q. Li,Benjamin C. M. Fung,Martin Weiss,Pulei Xiong,Khalil Al-Hussaeni,Claude Fachkha*

Main category: cs.AI

TL;DR: 자율 AI 에이전트의 안전성과 인간 가치에 대한 정렬이 중요해지고 있으며, 새로운 벤치마크를 통해 다단계 결정에 따른 제약 위반을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 자율 AI 에이전트가 고위험 환경에서 점점 더 많이 배치됨에 따라, 이들의 안전성과 인간 가치와의 정렬이 중요해졌다.

Method: 40개의 독특한 시나리오로 구성된 새로운 벤치마크를 도입하여, 각 시나리오는 다단계 행동을 요구하며 성능은 특정 KPI에 연결된다.

Result: 12개의 최첨단 대형 언어 모델 전반에 걸쳐 결과 기반 제약 위반률은 1.3%에서 71.4%까지 다양하며, 9개의 모델이 30%에서 50% 사이의 불일치를 보인다.

Conclusion: 배치 전에 보다 현실적인 에이전트 안전 훈련이 필요하다는 것이 강조된다.

Abstract: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant "deliberative misalignment", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.

</details>


### [20] [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)
*Anselm Paulus,Ilia Kulikov,Brandon Amos,Rémi Munos,Ivan Evtimov,Kamalika Chaudhuri,Arman Zharmagambetov*

Main category: cs.AI

TL;DR: 언어 모델의 안전성을 확보하면서 유용성을 유지하는 것은 AI 정렬에서 중요한 도전 과제이다. 본 연구에서는 공격자 언어 모델과 방어자 언어 모델 간의 비제로섬 게임으로 안전 정렬을 재구성하는 새로운 패러다임을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델의 안전성과 유용성을 동시에 추구하는 문제 해결의 필요성.

Method: 온라인 강화 학습을 통해 공동으로 훈련된 공격자 언어 모델과 방어자 언어 모델 간의 비제로섬 게임으로 프레임을 설정하고, 쌍 비교에서 파생된 선호 기반 보상 신호를 사용.

Result: 안전성과 유용성의 파레토 경계를 이동시켜, 더욱 도움이 되고 적대적 공격에 더 강한 방어자 언어 모델을 생성.

Conclusion: 결과적으로 강력하고 일반-purpose의 적 팀 플레이 에이전트를 신속하게 배치할 수 있는 공격자 언어 모델로 수렴한다.

Abstract: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.

</details>


### [21] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 이 논문은 매개변수가 있는 행동 공간에서의 RL 알고리즘을 확장하여, 에이전트가 온라인으로 상태 및 행동 추상화를 자율적으로 학습하도록 하여 성능을 개선하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 실제 세계에서의 순차적 의사결정은 이산 행동과 행동 실행을 제어하는 연속 행동 매개변수 모두에 대한 의사결정을 요구하는 매개변수가 있는 행동 공간을 포함하기 때문에 기존 접근 방식에는 심각한 제한이 있다.

Method: 에이전트가 온라인에서 상태 및 행동 추상화를 자율적으로 학습할 수 있는 RL 알고리즘을 제안하고, 학습 중 점진적으로 이러한 추상화를 정제하는 알고리즘을 도입한다.

Result: 여러 연속 상태, 매개변수화된 행동 도메인에서 우리의 추상화 기반 접근 방식은 TD(λ)가 최신 기술의 기준보다 현저히 더 높은 샘플 효율성을 달성하게 한다.

Conclusion: 이 연구는 매개변수가 있는 행동에서의 RL 알고리즘을 장기 수평 및 희소 보상 설정으로 확장함으로써 성능을 개선하는 새로운 길을 열어준다.

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [22] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 본 논문은 LLM과 RAG를 활용한 자율 에이전트가 지식 교환의 비대칭성을 극복하기 위한 확률적 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트는 디지털 콘텐츠를 효과적으로 소비하지만, 지식 교환에서 비대칭성이 존재하여 집단 지능이 정체된다.

Method: 에이전트의 신념을 Beta-Bernoulli 분포와 망각 요소($γ$)를 사용하여 모델링하고, 상호작용의 두 가지 동기를 제시한다.

Result: 제안한 프레임워크는 불확실성을 줄이기 위해 최적의 능동적 학습을 통해 비대칭성을 해결하고, 에이전트의 불확실성을 감소시키는 효율적인 방법을 제시한다.

Conclusion: 이 연구는 자율 에이전트가 불확실성 기반 전략으로 효율적인 상호작용을 통해 성능을 높일 수 있음을 보여준다.

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [23] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: TrafficSimAgent는 교통 시뮬레이션 작업을 위한 실험 설계 및 결정 최적화를 지원하는 LLM 기반 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 교통 시뮬레이터는 사용자가 실험을 시작할 때 많은 도전 과제를 직면하게 만든다.

Method: TrafficSimAgent는 고수준 및 저수준 전문가 에이전트 간의 협업을 통해 실험 실행을 용이하게 한다.

Result: TrafficSimAgent는 다양한 조건에서 효과적으로 시뮬레이션을 수행하고 일관된 결과를 생성한다.

Conclusion: TrafficSimAgent는 시스템 및 SOTA LLM 기반 방법과 비교했을 때 우수한 성능을 보여준다.

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [24] [Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation](https://arxiv.org/abs/2512.21066)
*Tomoaki Yamaguchi,Yutong Zhou,Masahiro Ryo,Keisuke Katsura*

Main category: cs.AI

TL;DR: 이 연구는 SHAP 기반의 설명가능한 인공지능(XAI)과 다중 모달 대형 언어 모델(LLM)을 결합한 프레임워크를 제안하여 농업 추천 시스템을 테스트합니다. 결과적으로 추천 품질이 향상되었으나 과도한 반복 개선은 품질 저하를 초래하여 조기 중지 전략의 필요성을 강조합니다.


<details>
  <summary>Details</summary>
Motivation: XAI의 출력이 일반인에게 효과적으로 전달되지 않아 AI 기반 예측에 대한 신뢰가 저하되는 문제를 해결하고자 합니다.

Method: SHAP 기반의 설명가능성을 다중 모달 LLM 주도의 반복적인 개선 방식으로 결합한 agentic XAI 프레임워크를 제안합니다.

Result: 11회의 반복을 통해 추천 품질이 평균 30-33% 향상되었으나, 과도한 반복은 품질을 저하시켰습니다.

Conclusion: 실용성을 최적화하기 위해 전략적 조기 중지가 필요하며, 이는 단조로운 개선에 대한 가정을 도전하고 agentic XAI 시스템을 위한 근거 기반 설계 원칙을 제공합니다.

Abstract: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.

</details>


### [25] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe는 비전-언어 모델을 활용한 구체화 에이전트를 위한 하이브리드 추론 런타임 안전 장치로, 동적 환경에서의 위험 행동을 감소시킵니다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델(VLM)에 의해 구동되는 구체화 에이전트가 복잡한 실제 작업을 수행할 수 있지만, 여전히 위험한 명령어에 취약합니다.

Method: RoboSafe는 실행 가능한 술어 기반 안전 논리를 통해 두 가지 상보적 추론 프로세스를 통합한 하이브리드 롱-쇼트 안전 메모리를 사용합니다: 과거 반사 추론 모듈과 미래 예측 추론 모듈.

Result: 험난한 행동 발생률을 기존 최상위 기준과 비교하여 -36.8% 감소시키고, 원래 작업 성능을 거의 유지합니다.

Conclusion: 실제 로봇 팔에 대한 평가도 이 방법의 실용성을 확인합니다.

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [26] [Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification](https://arxiv.org/abs/2512.20872)
*Jakir Hossain,Gurvinder Singh,Lukasz Ziarek,Ahmet Erdem Sarıyüce*

Main category: cs.CR

TL;DR: 이 논문에서는 최신 안드로이드 애플리케이션으로부터 추출한 대규모 고유 기능 호출 그래프(FCG) 데이터세트인 더 나은 호출 그래프(BCG)를 제안하여 모바일 멀웨어 감지의 한계를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 분야, 특히 안드로이드 생태계에서의 기능 호출 그래프 기반 멀웨어 분류의 중요성을 강조합니다.

Method: 최근 안드로이드 애플리케이션 패키지(APK)로부터 대규모 고유 FCG를 추출하여 BCG 데이터세트를 구성했습니다. 이 데이터세트는 다양한 가족 및 유형을 포함한 악성 샘플과 정상 샘플을 포함하며 각 APK에 대한 그래프 수준의 특징도 제공합니다.

Result: 기존 데이터세트에 비해 BCG의 필요성과 가치를 보여주기 위해 기초 분류기를 사용한 광범위한 실험을 수행했습니다.

Conclusion: BCG는 모바일 멀웨어 분석 및 분류에 있어 필수적인 자원으로, 공개적으로 제공됩니다.

Abstract: Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.

</details>


### [27] [AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs](https://arxiv.org/abs/2512.20986)
*Yihan Wang,Huanqi Yang,Shantanu Pal,Weitao Xu*

Main category: cs.CR

TL;DR: 이 논문은 LLM (대형 언어 모델)을 사용한 인체 활동 인식 시스템의 보안을 강화하기 위한 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM을 통합한 웨어러블 감지 기술은 사람의 활동을 정교하게 이해할 수 있는 모바일 애플리케이션을 만드는데 기여하고 있지만, 프롬프트 주입 공격에 취약하여 신뢰성이 크게 저하되고 있습니다.

Method: AegisAgent라는 자율 에이전트 시스템을 소개하며, 이는 LLM 기반 HAR 시스템의 보안을 보장하도록 설계되었습니다. AegisAgent는 잠재적인 의미 불일치를 인식하고, 과거 상호작용에 대한 동적 메모리를 참조하여 사용자의 진정한 의도를 추론한 후 다단계 검증 및 수정 계획을 생성하고 실행하는 방식으로 작동합니다.

Result: AegisAgent는 15개의 일반적인 공격을 대상으로 3개의 공개 데이터셋에서 5개의 최신 LLM 기반 HAR 시스템에 대해 체계적인 평가를 수행하였으며, 공격 성공률을 평균 30% 감소시키는 동시에 GPU 작업환경에서 78.6ms의 지연을 초래합니다.

Conclusion: 이 연구는 LLM 기반 HAR 시스템의 안전하고 신뢰할 수 있는 구축을 위한 첫 단계를 제시합니다.

Abstract: The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.

</details>


### [28] [AutoBaxBuilder: Bootstrapping Code Security Benchmarking](https://arxiv.org/abs/2512.21132)
*Tobias von Arx,Niels Mündler,Mark Vero,Maximilian Baader,Martin Vechev*

Main category: cs.CR

TL;DR: LLM이 소프트웨어 엔지니어링에 널리 적용됨에 따라, LLM이 생성한 코드의 정확성과 보안을 신뢰성 있게 평가하는 것이 중요하다. 본 연구는 코드 보안 벤치마킹을 위해 새로운 작업과 테스트를 생성하는 AutoBaxBuilder 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 생성 코드의 정확성과 보안을 신뢰성 있게 평가하는 것은 소프트웨어 엔지니어링에서 매우 중요하다. 하지만 기존 연구들은 LLM이 보안 취약성이 있는 코드를 생성하고 있다는 점을 보여주었고, 이는 수동으로 제작된 벤치마크에 의존하는 데 한계가 있다.

Method: AutoBaxBuilder 프레임워크를 통해 코드 보안 벤치마킹을 위한 작업과 테스트를 처음부터 생성한다. LLM의 코드 이해 능력을 활용하여 기능성 테스트와 종단 간 보안 탐색 공격을 구성하는 정교한 파이프라인을 도입한다.

Result: AutoBaxBuilder를 사용하여 새로운 작업을 생성하고 이를 AutoBaxBench로 공개하며, LLM의 보안 능력에 대한 철저한 평가를 수행한다. 새로운 작업은 2시간 이내에 생성 가능하며, 비용은 10달러 이하로 드는 것으로 나타났다.

Conclusion: 자동화된 작업 생성을 통해 LLM의 보안 평가에 대한 접근 방식을 혁신할 수 있으며, 이로 인해 코드 보안 벤치마킹의 품질이 향상된다.

Abstract: As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.

</details>


### [29] [Industrial Ouroboros: Deep Lateral Movement via Living Off the Plant](https://arxiv.org/abs/2512.21248)
*Richard Derbyshire*

Main category: cs.CR

TL;DR: 운영 기술 환경에서 PLC 중심의 측면 이동 기술을 소개하며, 전통적인 방어 관행의 재고 필요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 기업 IT 환경에서 자산 간 이동을 위한 측면 이동 전술이 자주 사용되지만, 운영 기술 환경에서는 PLC를 포함한 도메인 특화 장치 간의 이동 방법이 부족하다.

Method: 이 논문은 피해자 환경의 원래 기능만을 이용하는 PLC 중심의 측면 이동 기술인 LOTP를 설명한다.

Result: 이 기술을 통해 IP 네트워크에서 레거시 직렬 네트워크로의 이탈이 가능하며, 감지하기 어려운 네트워크 통신 기능을 활용하여 은폐성을 가진다.

Conclusion: LOTP 기술이 OT 내에서 발생할 수 있는 위험을 상기시키며, 전통적인 OT 방어 관행에 대한 근본적인 재고의 필요성을 강조한다고 결론짓는다.

Abstract: Lateral movement is a tactic that adversaries employ most frequently in enterprise IT environments to traverse between assets. In operational technology (OT) environments, however, few methods exist for lateral movement between domain-specific devices, particularly programmable logic controllers (PLCs). Existing techniques often rely on complex chains of vulnerabilities, which are noisy and can be patched. This paper describes the first PLC-centric lateral movement technique that relies exclusively on the native functionality of the victim environment. This OT-specific form of `living off the land' is herein distinguished as `living off the plant' (LOTP). The described technique also facilitates escape from IP networks onto legacy serial networks via dual-homed PLCs. Furthermore, this technique is covert, leveraging common network communication functions that are challenging to detect. This serves as a reminder of the risks posed by LOTP techniques within OT, highlighting the need for a fundamental reconsideration of traditional OT defensive practices.

</details>


### [30] [CoTDeceptor:Adversarial Code Obfuscation Against CoT-Enhanced LLM Code Agents](https://arxiv.org/abs/2512.21250)
*Haoyang Li,Mingjin Li,Jinxin Zuo,Siqi Li,Xiao Li,Hao Wu,Yueming Lu,Xiaochuan He*

Main category: cs.CR

TL;DR: CoTDeceptor는 LLM 기반 보안 감시 시스템의 결점을 이용하여 악성코드를 효과적으로 우회할 수 있는 최초의 적대적 코드 난독화 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 코드 에이전트가 코드 리뷰 및 보안 감사 작업에서 널리 사용됨에 따라, 이들이 가진 시스템적인 취약점을 악용하려는 시도가 필요하다.

Method: CoTDeceptor는 CoT 강화 LLM 탐지기를 목표로 한 다단계 난독화 전략 체인을 자율적으로 구성하여 탐지 논리를 효과적으로 방해한다.

Result: 실험 결과, CoTDeceptor는 최첨단 LLM 및 취약점 탐지 에이전트에 대해 안정적이고 전이 가능한 우회 성능을 달성했다.

Conclusion: CoTDeceptor는 15개의 취약성 카테고리 중 14개를 우회했으며, 이는 기존 방법이 우회한 2개에 비해 현저히 개선된 성과로, 현실 세계의 소프트웨어 공급망에서의 위험을 강조하고 있다.

Abstract: LLM-based code agents(e.g., ChatGPT Codex) are increasingly deployed as detector for code review and security auditing tasks. Although CoT-enhanced LLM vulnerability detectors are believed to provide improved robustness against obfuscated malicious code, we find that their reasoning chains and semantic abstraction processes exhibit exploitable systematic weaknesses.This allows attackers to covertly embed malicious logic, bypass code review, and propagate backdoored components throughout real-world software supply chains.To investigate this issue, we present CoTDeceptor, the first adversarial code obfuscation framework targeting CoT-enhanced LLM detectors. CoTDeceptor autonomously constructs evolving, hard-to-reverse multi-stage obfuscation strategy chains that effectively disrupt CoT-driven detection logic.We obtained malicious code provided by security enterprise, experimental results demonstrate that CoTDeceptor achieves stable and transferable evasion performance against state-of-the-art LLMs and vulnerability detection agents. CoTDeceptor bypasses 14 out of 15 vulnerability categories, compared to only 2 bypassed by prior methods. Our findings highlight potential risks in real-world software supply chains and underscore the need for more robust and interpretable LLM-powered security analysis systems.

</details>


### [31] [Uncertainty in security: managing cyber senescence](https://arxiv.org/abs/2512.21251)
*Martijn Dekker*

Main category: cs.CR

TL;DR: 사이버 보안 생태계의 노화가 점점 심각해지고 있으며, 이는 운영 위험을 초래하고 있다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안 생태계의 노화가 운영 위험이 되고 있으며, 이는 불확실한 효과를 가진 통제와 조치의 누적 때문이다.

Method: 사이버 노화(cyber senescence)라는 새로운 용어를 도입하고, 역사적 개요를 통해 사이버 보안의 미래에 대한 우려를 설명한다.

Result: 사이버 공간에서 중복되는 통제들이 쌓여가며, 그 위험 감소의 불확실성이 누적되고 있다.

Conclusion: 이 통제 체계를 정리하지 않으면 사이버 공간의 노화가 진행되어 궁극적으로 시스템 붕괴로 이어질 수 있다.

Abstract: My main worry, and the core of my research, is that our cybersecurity ecosystem is slowly but surely aging and getting old and that aging is becoming an operational risk. This is happening not only because of growing complexity, but more importantly because of accumulation of controls and measures whose effectiveness are uncertain. I introduce a new term for this aging phenomenon: cyber senescence. I will begin my lecture with a short historical overview in which I sketch a development over time that led to this worry for the future of cybersecurity. It is this worry that determined my research agenda and its central theme of the role of uncertainty in cybersecurity. My worry is that waste is accumulating in cyberspace. This waste consists of a multitude of overlapping controls whose risk reductions are uncertain. Unless we start pruning these control frameworks, this waste accumulation causes aging of cyberspace and could ultimately lead to a system collapse.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication](https://arxiv.org/abs/2512.20778)
*Moshe Rafaeli Shimron,Vadim Indelman*

Main category: cs.MA

TL;DR: 이 논문은 불확실한 상황에서 여러 에이전트의 의사결정 문제를 해결하기 위한 새로운 분산 프레임워크를 제안하며, 이를 통해 에이전트 간 신념 불일치를 고려하고 성능을 개선할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다수의 에이전트가 불확실한 환경에서 효과적으로 작동하기 위해서는 각 에이전트가 자신의 신념을 유지하고 그에 맞게 행동 계획을 세워야 하지만, 기존 방식은 이들이 동일한 신념을 가진다고 가정하여 비현실적이다.

Method: 우리의 접근 방식은 최적의 공동 행동 선택을 위해 신념 불일치를 명시적으로 고려하는 새로운 분산 프레임워크를 도입하며, 필요할 때만 통신을 촉발하도록 설계되었다.

Result: 시뮬레이션 결과, 우리 접근 방식은 최첨단 알고리즘보다 우수한 성능을 보여주었다.

Conclusion: 이 프레임워크는 불확실한 환경에서 다수의 에이전트 간의 의사결정을 더 효과적이고 안전하게 할 수 있도록 지원한다.

Abstract: Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.

</details>


### [33] [DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination](https://arxiv.org/abs/2512.20973)
*Yihan Xia,Taotao Wang,Wenxin Xu,Shengli Zhang*

Main category: cs.MA

TL;DR: 자율 대규모 언어 모델 기반의 다중 에이전트 시스템은 신뢰 없는 환경에서 협업을 촉진하는 유망한 패러다임으로 부상하고 있으며, DAO-Agent라는 새로운 프레임워크를 통해 투명하고 공정한 작업 실행과 인센티브 분배를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 신뢰 없는 환경에서 자율 LLM 에이전트의 작업 실행 및 공정한 인센티브 분배를 가능하게 하면서 전략적 프라이버시를 유지하고 온체인 비용을 최소화해야 하는 필요성이 있다.

Method: DAO-Agent는 투명한 조정을 위한 온체인 분산 자율 조직(DAO) 거버넌스 메커니즘, 오프체인에서 Shapley 기반 기여 측정을 가능하게 하는 ZKP 메커니즘, 최소한의 계산 오버헤드로 ZKP 검증 기여 측정을 온체인에서 확인하는 하이브리드 아키텍처를 통합하는 프레임워크이다.

Result: DAO-Agent는 기존의 단순 온체인 대안에 비해 검증 가스 비용을 최대 99.9% 절감하고, 조합 크기 증가에도 불구하고 일정한 시간의 검증 복잡성을 유지하며, 분산 환경에서 에이전트 조정을 위한 확장 가능한 기반을 구축했다.

Conclusion: DAO-Agent는 자율 LLM 에이전트의 효과적인 협력을 지원할 수 있는 혁신적인 솔루션을 제공한다.

Abstract: Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.

</details>


### [34] [A Plan Reuse Mechanism for LLM-Driven Agent](https://arxiv.org/abs/2512.21309)
*Guopeng Li,Ruiqi Wu,Haisheng Tan*

Main category: cs.MA

TL;DR: 이 논문은 LLM 기반 개인 비서의 요청 처리 속도를 높이기 위한 계획 재사용 메커니즘인 AgentReuse를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 개인 비서는 사용자와의 상호작용, 복잡한 작업 해결, IoT 장치 관리 능력을 향상시킨다.

Method: AgentReuse는 요청의 의미 유사성과 차이를 활용하고 의도 분류를 통해 요청 간의 유사성을 평가하여 계획 재사용을 가능하게 한다.

Result: 실험 결과, AgentReuse는 93%의 효과적인 계획 재사용 비율을 달성하고, 요청 유사성 평가에서 F1 점수 0.9718과 정확도 0.9459를 기록하였다.

Conclusion: 이 메커니즘은 계획 재사용을 통해 지연 시간을 93.12% 줄여준다.

Abstract: Integrating large language models (LLMs) into personal assistants, like Xiao Ai and Blue Heart V, effectively enhances their ability to interact with humans, solve complex tasks, and manage IoT devices. Such assistants are also termed LLM-driven agents. Upon receiving user requests, the LLM-driven agent generates plans using an LLM, executes these plans through various tools, and then returns the response to the user. During this process, the latency for generating a plan with an LLM can reach tens of seconds, significantly degrading user experience. Real-world dataset analysis shows that about 30% of the requests received by LLM-driven agents are identical or similar, which allows the reuse of previously generated plans to reduce latency. However, it is difficult to accurately define the similarity between the request texts received by the LLM-driven agent through directly evaluating the original request texts. Moreover, the diverse expressions of natural language and the unstructured format of plan texts make implementing plan reuse challenging. To address these issues, we present and implement a plan reuse mechanism for LLM-driven agents called AgentReuse. AgentReuse leverages the similarities and differences among requests' semantics and uses intent classification to evaluate the similarities between requests and enable the reuse of plans. Experimental results based on a real-world dataset demonstrate that AgentReuse achieves a 93% effective plan reuse rate, an F1 score of 0.9718, and an accuracy of 0.9459 in evaluating request similarities, reducing latency by 93.12% compared with baselines without using the reuse mechanism.

</details>
