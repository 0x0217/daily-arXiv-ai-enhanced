{"id": "2507.23229", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.23229", "abs": "https://arxiv.org/abs/2507.23229", "authors": ["Yufei Chen", "Yao Wang", "Haibin Zhang", "Tao Gu"], "title": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.", "AI": {"tldr": "RAG \uc2dc\uc2a4\ud15c\uc758 \uac1c\uc778\uc815\ubcf4 \ubcf4\ud638 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \uc0c8\ub85c\uc6b4 \ube14\ub799\ubc15\uc2a4 \uacf5\uaca9 \ud504\ub808\uc784\uc6cc\ud06c \uc81c\uc548.", "motivation": "RAG \uc2dc\uc2a4\ud15c\uc774 \uc678\ubd80 \uc9c0\uc2dd \uae30\ubc18\uc744 \ud1b5\ud569\ud558\uc5ec LLM\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uc9c0\ub9cc, \uac1c\uc778\uc815\ubcf4 \ubcf4\ud638\uc5d0 \ub300\ud55c \uc0c1\ub2f9\ud55c \uc704\ud5d8\uc774 \uc874\uc7ac\ud55c\ub2e4.", "method": "\uc9c0\uc2dd \ube44\ub300\uce6d\uc744 \uc774\uc6a9\ud55c \ube14\ub799\ubc15\uc2a4 \uacf5\uaca9 \ud504\ub808\uc784\uc6cc\ud06c\uc640 \uc801\uc751\ud615 \ud504\ub86c\ud504\ud2b8 \uc0dd\uc131 \uc804\ub7b5\uc744 \ud1b5\ud574 \uac1c\uc778 \uc815\ubcf4 \ucd94\ucd9c\uc744 \uc815\ubc00\ud558\uac8c \uc218\ud589\ud55c\ub2e4.", "result": "\ub2e8\uc77c \ub3c4\uba54\uc778\uc5d0\uc11c 91%, \ub2e4\uc911 \ub3c4\uba54\uc778\uc5d0\uc11c 83%\uc758 \uac1c\uc778\uc815\ubcf4 \ucd94\ucd9c\uc728\uc744 \ub2ec\uc131\ud588\uc73c\uba70, \ubbfc\uac10\ud55c \ubb38\uc7a5 \ub178\ucd9c\uc744 65% \uc774\uc0c1 \uac10\uc18c\uc2dc\ucf30\ub2e4.", "conclusion": "RAG \uc2dc\uc2a4\ud15c\uc758 \uacf5\uaca9\uacfc \ubc29\uc5b4 \uac04\uc758 \uac04\uadf9\uc744 \uba54\uc6b0\uace0, \uac1c\uc778\uc815\ubcf4\ub97c \uc815\ubc00\ud558\uac8c \ucd94\ucd9c\ud558\uace0 \uc801\uc751\ud615 \uc644\ud654\ub97c \uc704\ud55c \uae30\ucd08\ub97c \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23453", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23453", "abs": "https://arxiv.org/abs/2507.23453", "authors": ["Lijia Liu", "Takumi Kondo", "Kyohei Atarashi", "Koh Takeuchi", "Jiyi Li", "Shigeru Saito", "Hisashi Kashima"], "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems", "comment": null, "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 LLM \uae30\ubc18 \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc758 \ud504\ub86c\ud504\ud2b8 \uc8fc\uc785 \uacf5\uaca9\uc5d0 \ub300\ud55c \ubc29\uc5b4\ucc45\uc744 \uc5f0\uad6c\ud55c\ub2e4.", "motivation": "\ud504\ub86c\ud504\ud2b8 \uc8fc\uc785 \uacf5\uaca9\uc740 \ud3c9\uac00\uc790\uac00 \uc9c4\uc815\ud55c \ub2f5\ubcc0\uacfc \ub3c5\ub9bd\uc801\uc73c\ub85c \ud6c4\ubcf4 \ub2f5\ubcc0\uc744 \ub9cc\ub4e4\uc5b4\ub0b4\uc5b4 \uc18d\uc774\ub294 \ubc29\uc2dd\uc73c\ub85c, \uc774\ub7ec\ud55c \uacf5\uaca9\uc5d0 \ub300\ud55c \ubc29\uc5b4\uac00 \ud544\uc694\ud558\ub2e4.", "method": "\ud45c\uc900 \ud3c9\uac00(Standard Evaluation)\uc640 \ubc18\uc0ac\uc2e4 \ud3c9\uac00(Counterfactual Evaluation)\ub97c \uacb0\ud569\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud558\uc5ec, \uac70\uc9d3 \uae30\uc900 \ub2f5\ubcc0\uc5d0 \ub300\ud574 \uc81c\ucd9c\ubb3c\uc744 \ub2e4\uc2dc \ud3c9\uac00\ud55c\ub2e4.", "result": "\uc2e4\ud5d8 \uacb0\uacfc, \ud45c\uc900 \ud3c9\uac00\ub294 \ub192\uc740 \ucde8\uc57d\uc131\uc744 \ubcf4\uc600\uc9c0\ub9cc, SE+CFE \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uacf5\uaca9 \ud0d0\uc9c0\ub97c \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ucf1c \uc131\ub2a5 \uc800\ud558\ub294 \ucd5c\uc18c\ud654\ud558\uc600\ub2e4.", "conclusion": "\uc774 \uc5f0\uad6c\ub294 LLM \uae30\ubc18\uc758 \ud3c9\uac00 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \ud504\ub86c\ud504\ud2b8 \uc8fc\uc785 \uacf5\uaca9\uc5d0 \ub300\ud55c \ud6a8\uacfc\uc801\uc778 \ubc29\uc5b4\ucc45\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23611", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23611", "abs": "https://arxiv.org/abs/2507.23611", "authors": ["Estelle Ruellan", "Eric Clay", "Nicholas Ascoli"], "title": "LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora", "comment": null, "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \uac10\uc5fc \uc2a4\ud06c\ub9b0\uc0f7\uc744 \ubd84\uc11d\ud558\uc5ec \uc545\uc131 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 \uac10\uc5fc \ubca1\ud130\uc640 \ucea0\ud398\uc778\uc744 \ucd94\uc801\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc811\uadfc \ubc29\uc2dd\uc744 \uc81c\uc2dc\ud569\ub2c8\ub2e4.", "motivation": "\uac10\uc5fc \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc911\uc694\ud55c \ub370\uc774\ud130\ub97c \ud0c8\ucde8\ud558\ub294 Infostealers\uc758 \ubb38\uc81c\ub97c \ub2e4\ub8e8\uace0, \uc218\uc791\uc5c5 \ubd84\uc11d\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\uace0\uc790 \ud568.", "method": "gpt-4o-mini\uc640 \uac19\uc740 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \ud65c\uc6a9\ud558\uc5ec \uac10\uc5fc \uc2a4\ud06c\ub9b0\uc0f7\uc744 \ubd84\uc11d\ud558\uace0, \uc7a0\uc7ac\uc801 \uc704\ud611 \uc9c0\ud45c\ub97c \ucd94\ucd9c\ud558\ub294 \ubc29\ubc95.", "result": "1000\uac1c\uc758 \uc2a4\ud06c\ub9b0\uc0f7\uc5d0\uc11c 337\uac1c\uc758 \uc2e4\ud589 \uac00\ub2a5\ud55c URL\uacfc 246\uac1c\uc758 \uad00\ub828 \ud30c\uc77c\uc744 \ucd94\ucd9c\ud558\uba70, 3\uac1c\uc758 \ub69c\ub837\ud55c \uc545\uc131 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \ucea0\ud398\uc778\uc744 \uc2dd\ubcc4\ud568.", "conclusion": "\uac10\uc5fc \uc2a4\ud06c\ub9b0\uc0f7\uc744 \ud1b5\ud55c \ubc18\uc751\uc801, \uc544\ud2f0\ud329\ud2b8 \uae30\ubc18 \ubd84\uc11d\uc73c\ub85c \uc870\uae30\uc5d0 \uac10\uc5fc \ubca1\ud130\ub97c \uc2dd\ubcc4\ud558\uace0 \uc704\ud611 \uc815\ubcf4\ub97c \uac15\ud654\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud568."}}
{"id": "2507.23641", "categories": ["cs.CR", "11T71, 94A60"], "pdf": "https://arxiv.org/pdf/2507.23641", "abs": "https://arxiv.org/abs/2507.23641", "authors": ["Michael Schaller"], "title": "Polynomial Lattices for the BIKE Cryptosystem", "comment": null, "summary": "In this paper we introduce a rank $2$ lattice over a polynomial ring arising\nfrom the public key of the BIKE cryptosystem \\cite{aragon2022bike}. The secret\nkey is a sparse vector in this lattice. We study properties of this lattice and\ngeneralize the recovery of weak keys from \\cite{BardetDLO16}. In particular, we\nshow that they implicitly solved a shortest vector problem in the lattice we\nconstructed. Rather than finding only a shortest vector, we obtain a reduced\nbasis of the lattice which makes it possible to check for more weak keys.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 BIKE \uc554\ud638 \uc2dc\uc2a4\ud15c\uc5d0 \uae30\ubc18\ud55c \ub2e4\ud56d\uc2dd \ub9c1 \uc704\uc758 2\uacc4 \uaca9\uc790\ub97c \ub3c4\uc785\ud558\uace0, \uc774 \uaca9\uc790\uc758 \uc18d\uc131\uc744 \uc5f0\uad6c\ud558\uc5ec \uc57d\ud55c \ud0a4\ub97c \ubcf5\uad6c\ud558\ub294 \ubc29\ubc95\uc744 \uc77c\ubc18\ud654\ud558\uc600\ub2e4.", "motivation": "BIKE \uc554\ud638 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \ube44\ubc00 \ud0a4\ub85c \uc0ac\uc6a9\ub418\ub294 Sparse Vector\uc758 \ud2b9\uc131\uc744 \uc774\ud574\ud558\uace0, \uc774\ub97c \ud1b5\ud574 \uc554\ud638\uc758 \uc548\uc804\uc131\uc744 \ud3c9\uac00\ud558\uace0\uc790 \ud568.", "method": "BIKE \uc554\ud638 \uc2dc\uc2a4\ud15c\uc758 \uacf5\uac1c \ud0a4\uc5d0\uc11c \uc720\ub3c4\ub41c 2\uacc4 \uaca9\uc790\ub97c \uc5f0\uad6c\ud558\uba70, \uaca9\uc790\uc758 \ucd5c\ub2e8 \ubca1\ud130 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud568.", "result": "\uc8fc\uc5b4\uc9c4 \uaca9\uc790\ub294 \uc57d\ud55c \ud0a4\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \ucd95\uc18c\ub41c \uae30\ubc18\uc744 \uc81c\uacf5\ud568\uc73c\ub85c\uc368, \uaca9\uc790 \uc774\ub860\uc758 \uc0c8\ub85c\uc6b4 \uc801\uc6a9 \uac00\ub2a5\uc131\uc744 \ubcf4\uc5ec\uc90c.", "conclusion": "\uc774 \uc5f0\uad6c\ub294 BIKE \uc554\ud638 \uc2dc\uc2a4\ud15c\uc758 \uc548\uc804\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\ub294 \uae30\ubc18\uc744 \ub9c8\ub828\ud558\uba70, \uac15\ud654\ub41c \ud0a4 \ubcf5\uad6c \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4."}}
{"id": "2507.22954", "categories": ["cs.LG", "eess.IV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.22954", "abs": "https://arxiv.org/abs/2507.22954", "authors": ["Ridvan Yesiloglu", "Wei Peng", "Md Tauhidul Islam", "Ehsan Adeli"], "title": "Neural Autoregressive Modeling of Brain Aging", "comment": "Accepted at Deep Generative Models Workshop @ MICCAI 2025", "summary": "Brain aging synthesis is a critical task with broad applications in clinical\nand computational neuroscience. The ability to predict the future structural\nevolution of a subject's brain from an earlier MRI scan provides valuable\ninsights into aging trajectories. Yet, the high-dimensionality of data, subtle\nchanges of structure across ages, and subject-specific patterns constitute\nchallenges in the synthesis of the aging brain. To overcome these challenges,\nwe propose NeuroAR, a novel brain aging simulation model based on generative\nautoregressive transformers. NeuroAR synthesizes the aging brain by\nautoregressively estimating the discrete token maps of a future scan from a\nconvenient space of concatenated token embeddings of a previous and future\nscan. To guide the generation, it concatenates into each scale the subject's\nprevious scan, and uses its acquisition age and the target age at each block\nvia cross-attention. We evaluate our approach on both the elderly population\nand adolescent subjects, demonstrating superior performance over\nstate-of-the-art generative models, including latent diffusion models (LDM) and\ngenerative adversarial networks, in terms of image fidelity. Furthermore, we\nemploy a pre-trained age predictor to further validate the consistency and\nrealism of the synthesized images with respect to expected aging patterns.\nNeuroAR significantly outperforms key models, including LDM, demonstrating its\nability to model subject-specific brain aging trajectories with high fidelity.", "AI": {"tldr": "NeuroAR\ub294 \uc784\uc0c1 \ubc0f \uacc4\uc0b0 \uc2e0\uacbd\uacfc\ud559\uc5d0\uc11c \uc911\uc694\ud55c \ub1cc \ub178\ud654 \uc2dc\ubbac\ub808\uc774\uc158 \ubaa8\ub378\ub85c, \uace0\ud574\uc0c1\ub3c4 MRI \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud574 \ub098\uc774\uc758 \ubcc0\ud654\ub97c \uc608\uce21\ud558\uace0 \uc7ac\uad6c\uc131\ud55c\ub2e4.", "motivation": "\ub1cc \ub178\ud654 \ubd84\uc11d\uc740 \uc784\uc0c1\uc801 \ubc0f \uacc4\uc0b0\uc801 \uc2e0\uacbd\uacfc\ud559\uc5d0 \uc911\uc694\ud55c \uc8fc\uc81c\ub85c, MRI \ub370\uc774\ud130\ub97c \ud1b5\ud574 \ub098\uc774 \uad00\ub828 \uad6c\uc870\uc801 \ubcc0\ud654\ub97c \uc608\uce21\ud560 \uc218 \uc788\ub294 \ubaa8\ub378\uc774 \ud544\uc694\ud558\ub2e4.", "method": "NeuroAR\ub294 \uc0dd\uc131 \uc624\ud1a0\ud68c\uadc0 \ud2b8\ub79c\uc2a4\ud3ec\uba38\ub97c \uae30\ubc18\uc73c\ub85c \ud558\uc5ec \uc774\uc804 \ubc0f \ubbf8\ub798 \uc2a4\uce94\uc758 \ud1a0\ud070 \uc784\ubca0\ub529\uc744 \uc5f0\uacb0\ud558\uc5ec \ubbf8\ub798 \uc2a4\uce94\uc758 \ud1a0\ud070 \ub9f5\uc744 \ucd94\uc815\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc791\ub3d9\ud55c\ub2e4.", "result": "NeuroAR\uc740 \ub178\uc778 \ubc0f \uccad\uc18c\ub144 \uc9d1\ub2e8\uc5d0\uc11c\uc758 \ud3c9\uac00\uc5d0\uc11c \uae30\uc874\uc758 \uc0dd\uc131 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \uc774\ubbf8\uc9c0 \ucda9\uc2e4\ub3c4 \uba74\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4.", "conclusion": "NeuroAR\uc740 \uc8fc\uc694 \ubaa8\ub378\ub4e4\ubcf4\ub2e4 \ub69c\ub837\ud558\uac8c \uac1c\uc120\ub41c \uc131\ub2a5\uc744 \uc81c\uacf5\ud558\uba70, \uac1c\uc778\ubcc4 \ub1cc \ub178\ud654 \uacbd\ub85c\ub97c \ub192\uc740 \ucda9\uc2e4\ub3c4\ub85c \ubaa8\ub378\ub9c1\ud558\ub294 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc900\ub2e4."}}
{"id": "2507.22956", "categories": ["cs.LG", "cs.HC", "K.3.1"], "pdf": "https://arxiv.org/pdf/2507.22956", "abs": "https://arxiv.org/abs/2507.22956", "authors": ["Dong Hyun Roh", "Rajesh Kumar", "An Ngo"], "title": "LLM-Assisted Cheating Detection in Korean Language via Keystrokes", "comment": "This paper has 11 pages, 6 figures, 2 tables, and has been accepted\n  for publication at IEEE-IJCB 2025", "summary": "This paper presents a keystroke-based framework for detecting LLM-assisted\ncheating in Korean, addressing key gaps in prior research regarding language\ncoverage, cognitive context, and the granularity of LLM involvement. Our\nproposed dataset includes 69 participants who completed writing tasks under\nthree conditions: Bona fide writing, paraphrasing ChatGPT responses, and\ntranscribing ChatGPT responses. Each task spans six cognitive processes defined\nin Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and\ncreate). We extract interpretable temporal and rhythmic features and evaluate\nmultiple classifiers under both Cognition-Aware and Cognition-Unaware settings.\nTemporal features perform well under Cognition-Aware evaluation scenarios,\nwhile rhythmic features generalize better under cross-cognition scenarios.\nMoreover, detecting bona fide and transcribed responses was easier than\nparaphrased ones for both the proposed models and human evaluators, with the\nmodels significantly outperforming the humans. Our findings affirm that\nkeystroke dynamics facilitate reliable detection of LLM-assisted writing across\nvarying cognitive demands and writing strategies, including paraphrasing and\ntranscribing LLM-generated responses.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \ud55c\uad6d\uc5b4\uc5d0\uc11c LLM \ubcf4\uc870 \ubd80\uc815\ud589\uc704\ub97c \ud0d0\uc9c0\ud558\uae30 \uc704\ud55c \ud0a4\uc2a4\ud2b8\ub85c\ud06c \uae30\ubc18\uc758 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc2dc\ud55c\ub2e4.", "motivation": "\uae30\uc874 \uc5f0\uad6c\uc758 \uc5b8\uc5b4 \ubc94\uc704, \uc778\uc9c0\uc801 \ub9e5\ub77d, LLM \ucc38\uc5ec\uc758 \uc138\ubd80\uc0ac\ud56d\uc5d0\uc11c\uc758 \uc8fc\uc694 \uc5f0\uad6c \uacf5\ubc31\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574\uc11c\ub2e4.", "method": "69\uba85\uc758 \ucc38\uac00\uc790\uac00 \uc138 \uac00\uc9c0 \uc870\uac74(\uc815\uc0c1 \uc791\uc131, ChatGPT \uc751\ub2f5 \ud328\ub7ec\ud504\ub808\uc774\uc9d5, ChatGPT \uc751\ub2f5 \uc804\uc0ac)\uc5d0\uc11c \uc791\uc131 \uacfc\uc81c\ub97c \uc218\ud589\ud558\uace0, Bloom\uc758 \uc138\ubd84\ud654\ub41c \uc778\uc9c0 \uacfc\uc815\uc5d0 \ub530\ub77c \uc791\uc5c5\uc744 \ud3c9\uac00\ud55c\ub2e4. \ud574\uc11d \uac00\ub2a5\ud55c \uc2dc\uac04\uc801 \ubc0f \ub9ac\ub4dc\ubbf8\uceec\ud55c \ud2b9\uc131\uc744 \ucd94\ucd9c\ud558\uace0 \uc5ec\ub7ec \ubd84\ub958\uae30\ub97c \ud3c9\uac00\ud55c\ub2e4.", "result": "\uc2dc\uac04\uc801 \ud2b9\uc131\uc740 \uc778\uc2dd \uc778\uc9c0 \ud3c9\uac00 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uace0, \ub9ac\ub4dc\ubbf8\uceec\ud55c \ud2b9\uc131\uc740 \uad50\ucc28 \uc778\uc9c0 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \ub354 \uc798 \uc77c\ubc18\ud654\ub418\uc5c8\ub2e4.", "conclusion": "\ud0a4\uc2a4\ud2b8\ub85c\ud06c \uc5ed\ud559\uc774 \ub2e4\uc591\ud55c \uc778\uc9c0 \uc694\uad6c\uc640 \uc791\uc131 \uc804\ub7b5\uc744 \ud1b5\ud574 LLM \ubcf4\uc870 \uc791\ubb38\uc744 \uc2e0\ub8b0\uc131 \uc788\uac8c \ud0d0\uc9c0\ud560 \uc218 \uc788\uc74c\uc744 \ud655\uc778\ud588\ub2e4."}}
{"id": "2507.22959", "categories": ["cs.LG", "cs.CE", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.22959", "abs": "https://arxiv.org/abs/2507.22959", "authors": ["Salah A. Faroughi", "Farinaz Mostajeran", "Amin Hamed Mashhadzadeh", "Shirko Faroughi"], "title": "Scientific Machine Learning with Kolmogorov-Arnold Networks", "comment": null, "summary": "The field of scientific machine learning, which originally utilized\nmultilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold\nNetworks (KANs) for data encoding. This shift is driven by the limitations of\nMLPs, including poor interpretability, fixed activation functions, and\ndifficulty capturing localized or high-frequency features. KANs address these\nissues with enhanced interpretability and flexibility, enabling more efficient\nmodeling of complex nonlinear interactions and effectively overcoming the\nconstraints associated with conventional MLP architectures. This review\ncategorizes recent progress in KAN-based models across three distinct\nperspectives: (i) data-driven learning, (ii) physics-informed modeling, and\n(iii) deep operator learning. Each perspective is examined through the lens of\narchitectural design, training strategies, application efficacy, and\ncomparative evaluation against MLP-based counterparts. By benchmarking KANs\nagainst MLPs, we highlight consistent improvements in accuracy, convergence,\nand spectral representation, clarifying KANs' advantages in capturing complex\ndynamics while learning more effectively. Finally, this review identifies\ncritical challenges and open research questions in KAN development,\nparticularly regarding computational efficiency, theoretical guarantees,\nhyperparameter tuning, and algorithm complexity. We also outline future\nresearch directions aimed at improving the robustness, scalability, and\nphysical consistency of KAN-based frameworks.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \uacfc\ud559\uc801 \uae30\uacc4 \ud559\uc2b5\uc5d0\uc11c \ub2e4\uce35 \ud37c\uc149\ud2b8\ub860(MLP) \ub300\uc2e0 \ucf5c\ubaa8\uace0\ub85c\ud504-\uc544\ub180\ub4dc \ub124\ud2b8\uc6cc\ud06c(KAN)\uc758 \ucc44\ud0dd\uc774 \uc99d\uac00\ud558\uace0 \uc788\uc74c\uc744 \ub17c\uc758\ud558\uba70, KAN\uc758 \uc7a5\uc810\uacfc \ucd5c\uadfc \ubc1c\uc804\uc744 \uc138 \uac00\uc9c0 \uad00\uc810\uc5d0\uc11c \ubd84\ub958\ud55c\ub2e4.", "motivation": "MLP\uc758 \ud574\uc11d \uac00\ub2a5\uc131 \ubd80\uc871, \uace0\uc815\ub41c \ud65c\uc131\ud654 \ud568\uc218, \uad6d\uc18c\ud654\ub41c \ub610\ub294 \uace0\uc8fc\ud30c \ud2b9\uc131 \ucea1\ucc98\uc758 \uc5b4\ub824\uc6c0\uacfc \uac19\uc740 \ud55c\uacc4\ub85c \uc778\ud574 KAN\uc73c\ub85c\uc758 \uc804\ud658\uc774 \ud544\uc694\ud558\ub2e4.", "method": "KAN \uae30\ubc18 \ubaa8\ub378\uc758 \ucd5c\uadfc \ubc1c\uc804\uc744 \ub370\uc774\ud130 \uae30\ubc18 \ud559\uc2b5, \ubb3c\ub9ac \uc815\ubcf4 \ubaa8\ub378\ub9c1, \uc2ec\uce35 \uc5f0\uc0b0\uc790 \ud559\uc2b5\uc758 \uc138 \uac00\uc9c0 \uad00\uc810\uc5d0\uc11c \ubd84\ub958\ud558\uace0, \uac01 \uad00\uc810\uc744 \uad6c\uc870 \uc124\uacc4, \ud6c8\ub828 \uc804\ub7b5, \uc751\uc6a9 \ud6a8\uacfc \ubc0f MLP\uc640\uc758 \ube44\uad50 \ud3c9\uac00\ub97c \ud1b5\ud574 \ubd84\uc11d\ud55c\ub2e4.", "result": "KAN\uc744 MLP\uc640 \ube44\uad50\ud558\uc5ec \uc815\ud655\uc131, \uc218\ub834 \uc18d\ub3c4 \ubc0f \uc2a4\ud399\ud2b8\ub7fc \ud45c\ud604\uc5d0\uc11c \uc77c\uad00\ub41c \uac1c\uc120\uc744 \uac15\uc870\ud558\uba70, \ubcf5\uc7a1\ud55c \ub3d9\uc5ed\ud559\uc744 \ucea1\ucc98\ud558\uace0 \ud559\uc2b5\uc758 \ud6a8\uc728\uc131\uc744 \ub192\uc774\ub294 KAN\uc758 \uc7a5\uc810\uc744 \uba85\ud655\ud788 \ud55c\ub2e4.", "conclusion": "KAN \uac1c\ubc1c\uc758 \uc8fc\uc694 \ub3c4\uc804 \uacfc\uc81c\uc640 \uc5f4\ub9b0 \uc5f0\uad6c \uc9c8\ubb38\uc744 \uc2dd\ubcc4\ud558\uace0, KAN \uae30\ubc18 \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uacac\uace0\uc131, \ud655\uc7a5\uc131 \ubc0f \ubb3c\ub9ac\uc801 \uc77c\uad00\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud55c \ud5a5\ud6c4 \uc5f0\uad6c \ubc29\ud5a5\uc744 \uc81c\uc2dc\ud55c\ub2e4."}}
{"id": "2507.22962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22962", "abs": "https://arxiv.org/abs/2507.22962", "authors": ["Boyuan Zheng", "Victor W. Chu"], "title": "Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations", "comment": "Pre-print v0.8 2025-07-30", "summary": "Climate extremes present escalating risks to agriculture intensifying the\nneed for reliable multi-hazard early warning systems (EWS). The situation is\nevolving due to climate change and hence such systems should have the\nintelligent to continue to learn from recent climate behaviours. However,\ntraditional single-hazard forecasting methods fall short in capturing complex\ninteractions among concurrent climatic events. To address this deficiency, in\nthis paper, we combine sequential deep learning models and advanced Explainable\nArtificial Intelligence (XAI) techniques to introduce a multi-hazard\nforecasting framework for agriculture. In our experiments, we utilize\nmeteorological data from four prominent agricultural regions in the United\nStates (between 2010 and 2023) to validate the predictive accuracy of our\nframework on multiple severe event types, which are extreme cold, floods,\nfrost, hail, heatwaves, and heavy rainfall, with tailored models for each area.\nThe framework uniquely integrates attention mechanisms with TimeSHAP (a\nrecurrent XAI explainer for time series) to provide comprehensive temporal\nexplanations revealing not only which climatic features are influential but\nprecisely when their impacts occur. Our results demonstrate strong predictive\naccuracy, particularly with the BiLSTM architecture, and highlight the system's\ncapacity to inform nuanced, proactive risk management strategies. This research\nsignificantly advances the explainability and applicability of multi-hazard\nEWS, fostering interdisciplinary trust and effective decision-making process\nfor climate risk management in the agricultural industry.", "AI": {"tldr": "\uc774 \uc5f0\uad6c\ub294 \ub18d\uc5c5\uc744 \uc704\ud55c \ub2e4\uc911 \uc704\ud5d8 \uc870\uae30 \uacbd\uace0 \uc2dc\uc2a4\ud15c\uc744 \uac1c\ubc1c\ud558\uae30 \uc704\ud55c \ub525\ub7ec\ub2dd \ubc0f \uc124\uba85 \uac00\ub2a5\ud55c \uc778\uacf5\uc9c0\ub2a5 \uae30\ubc95\uc744 \uacb0\ud569\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud55c\ub2e4.", "motivation": "\uae30\ud6c4 \uadf9\ub2e8 \ud604\uc0c1\uc774 \ub18d\uc5c5\uc5d0 \ubbf8\uce58\ub294 \uc704\ud5d8\uc774 \uc99d\uac00\ud558\uace0 \uc788\uc73c\uba70, \uae30\ud6c4 \ubcc0\ud654\uc5d0 \ub300\uc751\ud558\uae30 \uc704\ud574 \uc2e0\ub8b0\uc131 \uc788\ub294 \uc870\uae30 \uacbd\uace0 \uc2dc\uc2a4\ud15c\uc758 \ud544\uc694\uc131\uc774 \ucee4\uc9c0\uace0 \uc788\ub2e4.", "method": "\uc21c\ucc28\uc801 \ub525\ub7ec\ub2dd \ubaa8\ub378\uacfc \uace0\uae09 \uc124\uba85 \uac00\ub2a5\ud55c \uc778\uacf5\uc9c0\ub2a5 \uae30\ubc95\uc744 \uacb0\ud569\ud558\uc5ec \ub2e4\uc911 \uc704\ud5d8 \uc608\uce21 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ub3c4\uc785\ud558\uc600\ub2e4.", "result": "\ubbf8\uad6d\uc758 \ub124 \uac1c \ub18d\uc5c5 \uc9c0\uc5ed\uc5d0\uc11c \ubc1c\uc0dd\ud55c \ub2e4\uc591\ud55c \uae30\ud6c4 \uc0ac\uac74\uc5d0 \ub300\ud55c \uc608\uce21 \uc815\ud655\uc131\uc744 \uac80\uc99d\ud558\uc600\ub2e4.", "conclusion": "\uc774 \uc5f0\uad6c\ub294 \ub2e4\uc911 \uc704\ud5d8 \uc870\uae30 \uacbd\uace0 \uc2dc\uc2a4\ud15c\uc758 \uc124\uba85 \uac00\ub2a5\uc131\uacfc \uc801\uc6a9 \uac00\ub2a5\uc131\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ucf1c \ub18d\uc5c5 \uae30\ud6c4 \ub9ac\uc2a4\ud06c \uad00\ub9ac\uc5d0 \uae30\uc5ec\ud55c\ub2e4."}}
{"id": "2507.22963", "categories": ["cs.LG", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2507.22963", "abs": "https://arxiv.org/abs/2507.22963", "authors": ["Abdelrhman Gaber", "Hassan Abd-Eltawab", "John Elgallab", "Youssif Abuzied", "Dineo Mpanya", "Turgay Celik", "Swarun Kumar", "Tamer ElBatt"], "title": "FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization", "comment": null, "summary": "Cardiovascular diseases (CVD) cause over 17 million deaths annually\nworldwide, highlighting the urgent need for privacy-preserving predictive\nsystems. We introduce FedCVD++, an enhanced federated learning (FL) framework\nthat integrates both parametric models (logistic regression, SVM, neural\nnetworks) and non-parametric models (Random Forest, XGBoost) for coronary heart\ndisease risk prediction. To address key FL challenges, we propose: (1)\ntree-subset sampling that reduces Random Forest communication overhead by 70%,\n(2) XGBoost-based feature extraction enabling lightweight federated ensembles,\nand (3) federated SMOTE synchronization for resolving cross-institutional class\nimbalance.\n  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves\nstate-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its\ncentralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81)\nmatches non-federated performance. Additionally, our communication-efficient\nstrategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.\n  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher\nF1-scores and superior scalability for multi-institutional deployment. This\nwork represents the first practical integration of non-parametric models into\nfederated healthcare systems, providing a privacy-preserving solution validated\nunder real-world clinical constraints.", "AI": {"tldr": "FedCVD++\ub294 \uc2ec\ud608\uad00 \uc9c8\ud658 \uc608\uce21\uc744 \uc704\ud55c \ud5a5\uc0c1\ub41c \uc5f0\ud569 \ud559\uc2b5 \ud504\ub808\uc784\uc6cc\ud06c\ub85c, \ub2e4\uc591\ud55c \ubaa8\ub378\uc744 \ud1b5\ud569\ud558\uace0 \ud1b5\uc2e0 \ud6a8\uc728\uc131\uc744 \uadf9\ub300\ud654\ud558\uc5ec \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4.", "motivation": "\uc2ec\ud608\uad00 \uc9c8\ud658\uc774 \uc5f0\uac04 1,700\ub9cc \uba85\uc758 \uc0ac\ub9dd\uc744 \ucd08\ub798\ud558\uace0 \uc788\uc5b4, \ud504\ub77c\uc774\ubc84\uc2dc\ub97c \ubcf4\ud638\ud558\ub294 \uc608\uce21 \uc2dc\uc2a4\ud15c\uc758 \ud544\uc694\uc131\uc774 \uc2dc\uae09\ud569\ub2c8\ub2e4.", "method": "FedCVD++\ub294 \ud30c\ub77c\uba54\ud2b8\ub9ad \ubc0f \ube44\ud30c\ub77c\uba54\ud2b8\ub9ad \ubaa8\ub378\uc744 \ubaa8\ub450 \ud1b5\ud569\ud558\uace0, \ud1b5\uc2e0 \uc624\ubc84\ud5e4\ub4dc\ub97c \uc904\uc774\uae30 \uc704\ud574 \ud2b8\ub9ac \uc11c\ube0c\uc14b \uc0d8\ud50c\ub9c1, \uacbd\ub7c9 \uc5f0\ud569 \uc559\uc0c1\ube14\uc744 \uc704\ud55c XGBoost \uae30\ubc18 \uae30\ub2a5 \ucd94\ucd9c, \ud074\ub798\uc2a4 \ubd88\uade0\ud615 \ud574\uc18c\ub97c \uc704\ud55c \uc5f0\ud569 SMOTE \ub3d9\uae30\ud654\ub97c \uc81c\uc548\ud569\ub2c8\ub2e4.", "result": "FedCVD++\ub294 Framingham \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud3c9\uac00\ub418\uc5c8\uc73c\uba70, \uc5f0\ud569 XGBoost\ub294 F1 = 0.80\uc73c\ub85c \uc911\uc559 \uc9d1\uc911\ud615 \ubaa8\ub378\uc5d0 \ube44\ud574 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uace0, \uc5f0\ud569 Random Forest\ub294 F1 = 0.81\ub85c \ube44\uc5f0\ud569 \uc131\ub2a5\uacfc \uc77c\uce58\ud569\ub2c8\ub2e4.", "conclusion": "FedCVD++\ub294 \uae30\uc874 FL \ud504\ub808\uc784\uc6cc\ud06c\ubcf4\ub2e4 15% \ub192\uc740 F1 \uc810\uc218\ub97c \uc81c\uacf5\ud558\uba70, \ub2e4\uae30\uad00 \ubc30\ud3ec\uc5d0\uc11c \uc6b0\uc218\ud55c \ud655\uc7a5\uc131\uc744 \uc9c0\ub2c8\uace0 \uc788\uc2b5\ub2c8\ub2e4."}}
{"id": "2507.23000", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23000", "abs": "https://arxiv.org/abs/2507.23000", "authors": ["Shengao Yi", "Xiaojiang Li", "Wei Tu", "Tianhong Zhao"], "title": "Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation", "comment": null, "summary": "As extreme heat events intensify due to climate change and urbanization,\ncities face increasing challenges in mitigating outdoor heat stress. While\ntraditional physical models such as SOLWEIG and ENVI-met provide detailed\nassessments of human-perceived heat exposure, their computational demands limit\nscalability for city-wide planning. In this study, we propose GSM-UTCI, a\nmultimodal deep learning framework designed to predict daytime average\nUniversal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The\nmodel fuses surface morphology (nDSM), high-resolution land cover data, and\nhourly meteorological conditions using a feature-wise linear modulation (FiLM)\narchitecture that dynamically conditions spatial features on atmospheric\ncontext. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical\naccuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\\deg}C,\nwhile reducing inference time from hours to under five minutes for an entire\ncity. To demonstrate its planning relevance, we apply GSM-UTCI to simulate\nsystematic landscape transformation scenarios in Philadelphia, replacing bare\nearth, grass, and impervious surfaces with tree canopy. Results show spatially\nheterogeneous but consistently strong cooling effects, with impervious-to-tree\nconversion producing the highest aggregated benefit (-4.18{\\deg}C average\nchange in UTCI across 270.7 km2). Tract-level bivariate analysis further\nreveals strong alignment between thermal reduction potential and land cover\nproportions. These findings underscore the utility of GSM-UTCI as a scalable,\nfine-grained decision support tool for urban climate adaptation, enabling\nscenario-based evaluation of greening strategies across diverse urban\nenvironments.", "AI": {"tldr": "GSM-UTCI\ub294 \ub3c4\uc2dc\uc758 \uc5f4 \uc2a4\ud2b8\ub808\uc2a4 \uc644\ud654\ub97c \uc704\ud55c \uace0\ud574\uc0c1\ub3c4\uc758 \uc5f4 \uc9c0\uc218 \uc608\uce21 \ubaa8\ub378\ub85c, \uc2dc\uac04 \uc18c\ubaa8\ub97c \ud06c\uac8c \uc904\uc774\uba70 \ubbf8\uc138\uc9c0\ub3c4\uc758 \uc628\ub3c4 \ubcc0\ud654\ub97c \uc2dc\ubbac\ub808\uc774\uc158 \ud560 \uc218 \uc788\ub2e4.", "motivation": "\uae30\ud6c4 \ubcc0\ud654\uc640 \ub3c4\uc2dc\ud654\ub85c \uc778\ud55c \uadf9\uc2ec\ud55c \uc5f4 \uc0ac\uac74 \uc99d\uac00\uc5d0 \ub300\ud574 \ub3c4\uc2dc\ub4e4\uc774 \uc678\ubd80 \uc5f4 \uc2a4\ud2b8\ub808\uc2a4\ub97c \uc644\ud654\ud558\ub294 \ub370 \uc9c1\uba74\ud558\ub294 \ub3c4\uc804 \uacfc\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574.", "method": "GSM-UTCI\ub294 \uc2ec\uce35 \ud559\uc2b5 \ud504\ub808\uc784\uc6cc\ud06c\ub85c, \ud45c\uba74 \ud615\ud0dc, \uace0\ud574\uc0c1\ub3c4 \ud1a0\uc9c0 \ud53c\ubcf5 \ub370\uc774\ud130 \ubc0f \uc2dc\uac04\ubcc4 \uae30\uc0c1 \uc870\uac74\uc744 \uacb0\ud569\ud558\uc5ec 1\ubbf8\ud130\uc758 \ucd08\uad6d\uc9c0\uc801 \ud574\uc0c1\ub3c4\ub85c UTCI\ub97c \uc608\uce21\ud55c\ub2e4.", "result": "GSM-UTCI\ub294 R2 0.9151, \ud3c9\uade0 \uc808\ub300 \uc624\ucc28 0.41{\u00178}C\ub97c \ub2ec\uc131\ud558\uba70, \uc804\uccb4 \ub3c4\uc2dc\uc758 \ucd94\ub860 \uc2dc\uac04\uc744 5\ubd84 \uc774\ub0b4\ub85c \ub2e8\ucd95\uc2dc\ucf30\ub2e4.", "conclusion": "GSM-UTCI\ub294 \ub3c4\uc2dc \uae30\ud6c4 \uc801\uc751\uc744 \uc704\ud55c \uacb0\uc815 \uc9c0\uc6d0 \ub3c4\uad6c\ub85c, \ub2e4\uc591\ud55c \ub3c4\uc2dc \ud658\uacbd\uc5d0\uc11c \uc870\uacbd \uc804\ub7b5\uc758 \uc2dc\ub098\ub9ac\uc624 \uae30\ubc18 \ud3c9\uac00\ub97c \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4."}}
{"id": "2507.23009", "categories": ["cs.LG", "cs.AI", "91E45", "I.2"], "pdf": "https://arxiv.org/pdf/2507.23009", "abs": "https://arxiv.org/abs/2507.23009", "authors": ["Tom S\u00fchr", "Florian E. Dorner", "Olawale Salaudeen", "Augustin Kelava", "Samira Samadi"], "title": "Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable results on a range of\nstandardized tests originally designed to assess human cognitive and\npsychological traits, such as intelligence and personality. While these results\nare often interpreted as strong evidence of human-like characteristics in LLMs,\nthis paper argues that such interpretations constitute an ontological error.\nHuman psychological and educational tests are theory-driven measurement\ninstruments, calibrated to a specific human population. Applying these tests to\nnon-human subjects without empirical validation, risks mischaracterizing what\nis being measured. Furthermore, a growing trend frames AI performance on\nbenchmarks as measurements of traits such as ``intelligence'', despite known\nissues with validity, data contamination, cultural bias and sensitivity to\nsuperficial prompt changes. We argue that interpreting benchmark performance as\nmeasurements of human-like traits, lacks sufficient theoretical and empirical\njustification. This leads to our position: Stop Evaluating AI with Human Tests,\nDevelop Principled, AI-specific Tests instead. We call for the development of\nprincipled, AI-specific evaluation frameworks tailored to AI systems. Such\nframeworks might build on existing frameworks for constructing and validating\npsychometrics tests, or could be created entirely from scratch to fit the\nunique context of AI.", "AI": {"tldr": "\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \uc131\uacfc\ub97c \uc778\uac04 \ud14c\uc2a4\ud2b8\uc640 \ube44\uad50\ud558\ub294 \uac83\uc740 \uc874\uc7ac\ub860\uc801 \uc624\ub958\uc774\uba70, AI\uc5d0 \ub9de\ub294 \ud3c9\uac00 \uccb4\uacc4\uc758 \uac1c\ubc1c\uc774 \ud544\uc694\ud558\ub2e4.", "motivation": "LLM\uc758 \uc131\uacfc\uac00 \uc778\uac04 \ud2b9\uc131\uc744 \ubc18\uc601\ud55c\ub2e4\ub294 \ud574\uc11d\uc5d0 \ub300\ud574 \ubb38\uc81c\ub97c \uc81c\uae30\ud558\uace0, \uc801\uc808\ud55c \ud3c9\uac00 \ubc29\ubc95\uc758 \ud544\uc694\uc131\uc744 \uac15\uc870\ud558\uae30 \uc704\ud574\uc11c\ub2e4.", "method": "\uae30\uc874\uc758 \uc2ec\ub9ac \ud3c9\uac00 \ub3c4\uad6c\ub97c \uac80\ud1a0\ud558\uace0, \uc774\ub7ec\ud55c \ub3c4\uad6c\ub4e4\uc774 LLM\uc5d0 \uc801\uc6a9\ub418\ub294 \uac83\uc758 \ud55c\uacc4\ub97c \ubd84\uc11d\ud55c\ub2e4.", "result": "\uae30\uc874 \uc778\uac04 \ud14c\uc2a4\ud2b8\ub97c AI \uc131\ub2a5 \ud3c9\uac00\uc5d0 \uc801\uc6a9\ud558\ub294 \uac83\uc774 \uc774\ub860\uc801 \ubc0f \uacbd\ud5d8\uc801\uc73c\ub85c \uc815\ub2f9\ud654\ub418\uc9c0 \uc54a\uc74c\uc744 \uc8fc\uc7a5\ud55c\ub2e4.", "conclusion": "AI \uc2dc\uc2a4\ud15c\uc5d0 \ud2b9\ud654\ub41c \ud3c9\uac00 \uccb4\uacc4 \uac1c\ubc1c\uc758 \ud544\uc694\uc131\uc744 \uc81c\uc548\ud55c\ub2e4."}}
{"id": "2507.23010", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23010", "abs": "https://arxiv.org/abs/2507.23010", "authors": ["Siwoo Park"], "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods", "comment": null, "summary": "This paper investigates the inverse capabilities and broader utility of\nmultimodal latent spaces within task-specific AI (Artificial Intelligence)\nmodels. While these models excel at their designed forward tasks (e.g.,\ntext-to-image generation, audio-to-text transcription), their potential for\ninverse mappings remains largely unexplored. We propose an optimization-based\nframework to infer input characteristics from desired outputs, applying it\nbidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio\n(Whisper-Large-V3, Chatterbox-TTS) modalities.\n  Our central hypothesis posits that while optimization can guide models\ntowards inverse tasks, their multimodal latent spaces will not consistently\nsupport semantically meaningful and perceptually coherent inverse mappings.\nExperimental results consistently validate this hypothesis. We demonstrate that\nwhile optimization can force models to produce outputs that align textually\nwith targets (e.g., a text-to-image model generating an image that an image\ncaptioning model describes correctly, or an ASR model transcribing optimized\naudio accurately), the perceptual quality of these inversions is chaotic and\nincoherent. Furthermore, when attempting to infer the original semantic input\nfrom generative models, the reconstructed latent space embeddings frequently\nlack semantic interpretability, aligning with nonsensical vocabulary tokens.\n  These findings highlight a critical limitation. multimodal latent spaces,\nprimarily optimized for specific forward tasks, do not inherently possess the\nstructure required for robust and interpretable inverse mappings. Our work\nunderscores the need for further research into developing truly semantically\nrich and invertible multimodal latent spaces.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \ud2b9\uc815 \uc791\uc5c5\uc5d0 \ud2b9\ud654\ub41c AI \ubaa8\ub378\uc758 \ub2e4\uc911\ubaa8\ub4dc \uc7a0\uc7ac \uacf5\uac04\uc5d0\uc11c\uc758 \uc5ed \uae30\ub2a5\uacfc \ud65c\uc6a9 \uac00\ub2a5\uc131\uc744 \uc870\uc0ac\ud569\ub2c8\ub2e4.", "motivation": "\ub2e4\uc911\ubaa8\ub4dc AI \ubaa8\ub378\uc774 \uc804\ubc29\ud5a5 \uc791\uc5c5\uc5d0\uc11c\ub294 \uc6b0\uc218\ud558\uc9c0\ub9cc \uc5ed \ub9e4\ud551\uc758 \uc7a0\uc7ac\ub825\uc740 \ubbf8\uac1c\ucc99 \uc0c1\ud0dc\uc785\ub2c8\ub2e4.", "method": "\ucd5c\uc801\ud654 \uae30\ubc18 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud558\uc5ec \uc6d0\ud558\ub294 \ucd9c\ub825\uc5d0\uc11c \uc785\ub825 \ud2b9\uc131\uc744 \uc720\ucd94\ud569\ub2c8\ub2e4.", "result": "\ucd5c\uc801\ud654\uac00 \ubaa8\ub378\uc744 \uc5ed \uc791\uc5c5\uc73c\ub85c \uc548\ub0b4\ud560 \uc218\ub294 \uc788\uc9c0\ub9cc, \ub2e4\uc911\ubaa8\ub4dc \uc7a0\uc7ac \uacf5\uac04\uc774 \ud56d\uc0c1 \uc758\ubbf8 \uc788\uace0 \uc77c\uad00\ub41c \uc5ed \ub9e4\ud551\uc744 \uc9c0\uc6d0\ud558\uc9c0\ub294 \uc54a\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.", "conclusion": "\uc804\ubc29\ud5a5 \uc791\uc5c5\uc5d0 \ucd5c\uc801\ud654\ub41c \ub2e4\uc911\ubaa8\ub4dc \uc7a0\uc7ac \uacf5\uac04\uc740 \uac15\ub825\ud558\uace0 \ud574\uc11d \uac00\ub2a5\ud55c \uc5ed \ub9e4\ud551\uc744 \uc704\ud55c \uad6c\uc870\ub97c \ubcf8\uc9c8\uc801\uc73c\ub85c \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc74c\uc744 \uac15\uc870\ud569\ub2c8\ub2e4."}}
{"id": "2507.23035", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.23035", "abs": "https://arxiv.org/abs/2507.23035", "authors": ["Xueying Wu", "Baijun Zhou", "Zhihui Gao", "Yuzhe Fu", "Qilin Zheng", "Yintao He", "Hai Li"], "title": "KLLM: Fast LLM Inference with K-Means Quantization", "comment": null, "summary": "Large language model (LLM) inference poses significant challenges due to its\nintensive memory and computation demands. Weight and activation quantization\n(WAQ) offers a promising solution by reducing both memory footprint and\narithmetic complexity. However, two key challenges remain in the existing WAQ\ndesigns. (1) Traditional WAQ designs rely on uniform integer-based quantization\nfor hardware efficiency, but this often results in significant accuracy\ndegradation at low precision. K-Means-based quantization, a non-uniform\nquantization technique, achieves higher accuracy by matching the Gaussian-like\ndistributions of weights and activations in LLMs. However, its non-uniform\nnature prevents direct execution on low-precision compute units, requiring\ndequantization and floating-point matrix multiplications (MatMuls) during\ninference. (2) Activation outliers further hinder effective low-precision WAQ.\nOffline thresholding methods for outlier detection can lead to significant\nmodel performance degradation, while existing online detection techniques\nintroduce substantial runtime overhead.\n  To address the aforementioned challenges and fully unleash the potential of\nWAQ with K-Means quantization for LLM inference, in this paper, we propose\nKLLM, a hardware-software co-design framework. KLLM features an index-based\ncomputation scheme for efficient execution of MatMuls and nonlinear operations\non K-Means-quantized data, which avoids most of the dequantization and\nfull-precision computations. Moreover, KLLM incorporates a novel outlier\ndetection engine, Orizuru, that efficiently identifies the top-$k$ largest and\nsmallest elements in the activation data stream during online inference.\n  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,\n7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the\nA100 GPU and Atom, respectively.", "AI": {"tldr": "KLLM\uc740 LLM \ucd94\ub860\uc744 \uc704\ud55c \ud6a8\uc728\uc801\uc778 \ud558\ub4dc\uc6e8\uc5b4-\uc18c\ud504\ud2b8\uc6e8\uc5b4 \uacf5\ub3d9 \uc124\uacc4 \ud504\ub808\uc784\uc6cc\ud06c\ub85c, K-Means \uc591\uc790\ud654\ub97c \ud1b5\ud574 \uba54\ubaa8\ub9ac \ubc0f \uacc4\uc0b0 \ubcf5\uc7a1\uc131\uc744 \uc904\uc774\uace0, \ube44\uc815\uc0c1\uc801\uc778 \ud65c\uc131\ud654\ub97c \ud1b5\ud55c \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4.", "motivation": "\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uc758 \ucd94\ub860\uc5d0\uc11c \uba54\ubaa8\ub9ac \ubc0f \uacc4\uc0b0 \uc694\uad6c\uac00 \ub192\uc544 WAQ\ub97c \ud1b5\ud55c \uc194\ub8e8\uc158 \ubaa8\uc0c9.", "method": "KLLM\uc740 K-Means \uc591\uc790\ud654\ub41c \ub370\uc774\ud130\uc5d0 \ub300\ud574 \ud6a8\uc728\uc801\uc73c\ub85c \ud589\ub82c \uacf1\uc148\uacfc \ube44\uc120\ud615 \uc5f0\uc0b0\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \uc778\ub371\uc2a4 \uae30\ubc18 \uacc4\uc0b0 \ubc29\uc2dd\uc744 \uc81c\uacf5\ud558\uace0, \uc628\ub77c\uc778 \ucd94\ub860\uc5d0\uc11c \ud65c\uc131\ud654 \ub370\uc774\ud130 \uc2a4\ud2b8\ub9bc\uc5d0\uc11c \uc0c1\uc704 $k$ \uc694\uc18c\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uc2dd\ubcc4\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc774\uc0c1\uce58 \ud0d0\uc9c0 \uc5d4\uc9c4 Orizuru\ub97c \ud1b5\ud569\ud55c\ub2e4.", "result": "KLLM\uc740 A100 GPU\uc640 Atom\uc5d0 \ube44\ud574 \uac01\uac01 \ud3c9\uade0\uc801\uc73c\ub85c 9.67\ubc30 \ubc0f 7.03\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uacfc 229.50\ubc30 \ubc0f 150.21\ubc30\uc758 \uc5d0\ub108\uc9c0 \ud6a8\uc728\uc131 \uac1c\uc120\uc744 \ub2ec\uc131\ud588\ub2e4.", "conclusion": "KLLM\uc740 K-Means \uc591\uc790\ud654\ub97c \ud65c\uc6a9\ud558\uc5ec LLM \ucd94\ub860\uc758 \uc7a0\uc7ac\ub825\uc744 \uadf9\ub300\ud654\ud558\uace0, \uae30\uc874\uc758 \ubb38\uc81c\ub4e4\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud574\uacb0\ud55c\ub2e4."}}
{"id": "2507.23037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23037", "abs": "https://arxiv.org/abs/2507.23037", "authors": ["Aur\u00e9lie Leribaux", "Rafael Oyamada", "Johannes De Smedt", "Zahra Dasht Bozorgi", "Artem Polyvyanyy", "Jochen De Weerdt"], "title": "Linking Actor Behavior to Process Performance Over Time", "comment": "Accepted for presentation at the 5th Workshop on Change, Drift, and\n  Dynamics of Organizational Processes (ProDy), BPM 2025", "summary": "Understanding how actor behavior influences process outcomes is a critical\naspect of process mining. Traditional approaches often use aggregate and static\nprocess data, overlooking the temporal and causal dynamics that arise from\nindividual actor behavior. This limits the ability to accurately capture the\ncomplexity of real-world processes, where individual actor behavior and\ninteractions between actors significantly shape performance. In this work, we\naddress this gap by integrating actor behavior analysis with Granger causality\nto identify correlating links in time series data. We apply this approach to\nrealworld event logs, constructing time series for actor interactions, i.e.\ncontinuation, interruption, and handovers, and process outcomes. Using Group\nLasso for lag selection, we identify a small but consistently influential set\nof lags that capture the majority of causal influence, revealing that actor\nbehavior has direct and measurable impacts on process performance, particularly\nthroughput time. These findings demonstrate the potential of actor-centric,\ntime series-based methods for uncovering the temporal dependencies that drive\nprocess outcomes, offering a more nuanced understanding of how individual\nbehaviors impact overall process efficiency.", "AI": {"tldr": "\ud589\uc704\uc790 \ud589\ub3d9\uc774 \ud504\ub85c\uc138\uc2a4 \uacb0\uacfc\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294 \uac83\uc740 \ud504\ub85c\uc138\uc2a4 \ub9c8\uc774\ub2dd\uc758 \ud575\uc2ec\uc774\ub2e4. \uc804\ud1b5\uc801\uc778 \uc811\uadfc \ubc29\uc2dd\uc740 \uac1c\ubcc4 \ud589\uc704\uc790\uc758 \ud589\ub3d9\uc744 \uac04\uacfc\ud558\uc5ec \uc2e4\uc81c \ud504\ub85c\uc138\uc2a4\uc758 \ubcf5\uc7a1\uc131\uc744 \uc815\ud655\ud788 \ud3ec\ucc29\ud558\uc9c0 \ubabb\ud55c\ub2e4. \ubcf8 \uc5f0\uad6c\ub294 \ud589\uc704\uc790 \ud589\ub3d9 \ubd84\uc11d\uacfc \uadf8\ub79c\uc800 \uc778\uacfc\uad00\uacc4\ub97c \ud1b5\ud569\ud558\uc5ec \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc5d0\uc11c \uc0c1\uad00\uad00\uacc4\ub97c \uc2dd\ubcc4\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "motivation": "\uac1c\ubcc4 \ud589\uc704\uc790 \ud589\ub3d9\uacfc \uc774\ub4e4 \uac04\uc758 \uc0c1\ud638\uc791\uc6a9\uc774 \ud504\ub85c\uc138\uc2a4 \uc131\uacfc\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uba85\ud655\ud788 \uc774\ud574\ud558\uace0, \uae30\uc874 \uc811\uadfc\ubc95\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\uace0\uc790 \ud55c\ub2e4.", "method": "\ud589\uc704\uc790 \ud589\ub3d9 \ubd84\uc11d\uacfc \uadf8\ub79c\uc800 \uc778\uacfc\uad00\uacc4\ub97c \ud1b5\ud569\ud558\uc5ec \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc2dd\ubcc4\ud558\uace0, \uadf8\ub8f9 \ub77c\uc3d8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc9c0\uc5f0 \ubcc0\uc218\ub97c \uc120\ud0dd\ud55c\ub2e4.", "result": "\ud589\uc704\uc790\uc758 \ud589\ub3d9\uc774 \ud504\ub85c\uc138\uc2a4 \uc131\uacfc\uc5d0 \uc9c1\uc811\uc801\uc774\uace0 \uce21\uc815 \uac00\ub2a5\ud55c \uc601\ud5a5\uc744 \ubbf8\uce5c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \ud2b9\ud788, \ucc98\ub9ac \uc2dc\uac04\uc5d0 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud55c\ub2e4.", "conclusion": "\ud589\uc704\uc790 \uc911\uc2ec\uc758 \uc2dc\uacc4\uc5f4 \uae30\ubc18 \ubc29\ubc95\uc744 \ud1b5\ud574 \ud504\ub85c\uc138\uc2a4 \uacb0\uacfc\ub97c \uc8fc\ub3c4\ud558\ub294 \uc2dc\uac04\uc801 \uc758\uc874\uc131\uc744 \ubc1d\ud600\ub0bc \uc218 \uc788\uc73c\uba70, \uc774\ub294 \uac1c\ubcc4 \ud589\ub3d9\uc774 \uc804\uccb4 \ud504\ub85c\uc138\uc2a4 \ud6a8\uc728\uc131\uc5d0 \uc5b4\ub5bb\uac8c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ub354 \uc815\uad50\ud558\uac8c \uc774\ud574\ud560 \uc218 \uc788\ub294 \uae30\ud68c\ub97c \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23043", "abs": "https://arxiv.org/abs/2507.23043", "authors": ["Junyi Fan", "Li Sun", "Shuheng Chen", "Yong Si", "Minoo Ahmadi", "Greg Placencia", "Elham Pishgar", "Kamiar Alaei", "Maryam Pishgar"], "title": "Prediction of Significant Creatinine Elevation in First ICU Stays with Vancomycin Use: A retrospective study through Catboost", "comment": null, "summary": "Background: Vancomycin, a key antibiotic for severe Gram-positive infections\nin ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in\ncritically ill patients is challenging. This study aimed to develop a machine\nlearning model to predict vancomycin-related creatinine elevation using routine\nICU data.\n  Methods: We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV\ndatabase who received vancomycin. Kidney injury was defined by KDIGO criteria\n(creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were\nselected via SelectKBest (top 30) and Random Forest ranking (final 15). Six\nalgorithms were tested with 5-fold cross-validation. Interpretability was\nevaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior\nsampling.\n  Results: Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation.\nCatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800,\nspecificity 0.681, negative predictive value 0.900). Key predictors were\nphosphate, total bilirubin, magnesium, Charlson index, and APSIII. SHAP\nconfirmed phosphate as a major risk factor. ALE showed dose-response patterns.\nBayesian analysis estimated mean risk 60.5% (95% credible interval: 16.8-89.4%)\nin high-risk cases.\n  Conclusions: This machine learning model predicts vancomycin-associated\ncreatinine elevation from routine ICU data with strong accuracy and\ninterpretability, enabling early risk detection and supporting timely\ninterventions in critical care.", "AI": {"tldr": "\uc774 \uc5f0\uad6c\ub294 \uc778\uacf5\uc9c0\ub2a5 \uae30\ubc95\uc744 \ud65c\uc6a9\ud558\uc5ec ICU\uc5d0\uc11c \ubc14\ub098\ucf54\ub9c8\uc774\uc2e0 \uc0ac\uc6a9 \ud658\uc790\uc758 \uc2e0\uc7a5 \uc190\uc0c1 \uc608\uce21 \ubaa8\ub378\uc744 \uac1c\ubc1c\ud588\ub2e4.", "motivation": "\ubc14\ub098\ucf54\ub9c8\uc774\uc2e0\uc740 \uc911\uc99d \uadf8\ub78c \uc591\uc131 \uac10\uc5fc \uce58\ub8cc\uc5d0 \ud544\uc218\uc801\uc778 \ud56d\uc0dd\uc81c\uc9c0\ub9cc, \uc2e0\uc7a5 \ub3c5\uc131 \uc704\ud5d8\uc774 \ub192\uc544 ICU\uc5d0\uc11c \uc2e0\uc7a5 \uc190\uc0c1 \uc870\uae30 \uc608\uce21\uc774 \ud544\uc694\ud558\ub2e4.", "method": "MIMIC-IV \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0\uc11c 10,288\uba85\uc758 \ud658\uc790 \ub370\uc774\ud130\ub97c \ubd84\uc11d\ud558\uace0 6\uac1c\uc758 \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\uc744 \uac80\uc0ac\ud558\uc5ec 5-\uacb9 \ud06c\ub85c\uc2a4 \uac80\uc99d\uc744 \uc2e4\uc2dc\ud588\ub2e4.", "result": "\ucd5c\uace0\uc758 \uc131\ub2a5\uc744 \ubcf4\uc778 CatBoost \ubaa8\ub378\uc740 AUROC 0.818\uc744 \uae30\ub85d\ud588\uc73c\uba70, \uc8fc\uc694 \uc608\uce21 \uc778\uc790\ub294 \uc778\uc0b0\uc5fc, \ucd1d \ube4c\ub9ac\ub8e8\ube48, \ub9c8\uadf8\ub124\uc298, Charlson \uc9c0\uc218, APSIII\uc600\ub2e4.", "conclusion": "\uc774 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc740 \ubc14\ub098\ucf54\ub9c8\uc774\uc2e0 \uad00\ub828 \ud06c\ub808\uc544\ud2f0\ub2cc \uc0c1\uc2b9\uc744 \ub192\uc740 \uc815\ud655\ub3c4\ub85c \uc608\uce21\ud558\uc5ec \uc870\uae30 \uc704\ud5d8 \uac10\uc9c0 \ubc0f \uc2dc\uae30\uc801\uc808\ud55c \uac1c\uc785\uc744 \uc9c0\uc6d0\ud55c\ub2e4."}}
{"id": "2507.23073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23073", "abs": "https://arxiv.org/abs/2507.23073", "authors": ["Annalisa Barbara", "Joseph Lazzaro", "Ciara Pike-Burke"], "title": "Locally Differentially Private Thresholding Bandits", "comment": "18th European Workshop on Reinforcement Learning (EWRL 2025)", "summary": "This work investigates the impact of ensuring local differential privacy in\nthe thresholding bandit problem. We consider both the fixed budget and fixed\nconfidence settings. We propose methods that utilize private responses,\nobtained through a Bernoulli-based differentially private mechanism, to\nidentify arms with expected rewards exceeding a predefined threshold. We show\nthat this procedure provides strong privacy guarantees and derive theoretical\nperformance bounds on the proposed algorithms. Additionally, we present general\nlower bounds that characterize the additional loss incurred by any\ndifferentially private mechanism, and show that the presented algorithms match\nthese lower bounds up to poly-logarithmic factors. Our results provide valuable\ninsights into privacy-preserving decision-making frameworks in bandit problems.", "AI": {"tldr": "\uc774 \uc5f0\uad6c\ub294 \ubb38\ud131 \ubc34\ub527 \ubb38\uc81c\uc5d0\uc11c \uc9c0\uc5ed\uc801 \ucc28\ubcc4\uc801 \ud504\ub77c\uc774\ubc84\uc2dc\ub97c \ubcf4\uc7a5\ud558\ub294 \ubc29\uc2dd\uc758 \uc601\ud5a5\uc744 \uc870\uc0ac\ud55c\ub2e4.", "motivation": "\ubc34\ub527 \ubb38\uc81c\uc5d0\uc11c\uc758 \ud504\ub77c\uc774\ubc84\uc2dc \ubcf4\uc7a5\uc744 \ud1b5\ud574 \uc758\uc0ac \uacb0\uc815 \uacfc\uc815\uc5d0\uc11c\uc758 \ub370\uc774\ud130 \ubcf4\ud638\uac00 \uc911\uc694\ud568\uc744 \uac15\uc870\ud55c\ub2e4.", "method": "\ubc84\ub204\uc774 \uae30\ubc18\uc758 \ucc28\ubcc4\uc801 \ud504\ub77c\uc774\ubc84\uc2dc \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc751\ub2f5\uc744 \uc218\uc9d1\ud558\uace0, \uae30\ub300 \ubcf4\uc0c1\uc774 \uc815\uc758\ub41c \ubb38\ud131\uc744 \ucd08\uacfc\ud558\ub294 \ud314\uc744 \uc2dd\ubcc4\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4.", "result": "\uc81c\uc548\ub41c \uc54c\uace0\ub9ac\uc998\uc774 \uac15\ub825\ud55c \ud504\ub77c\uc774\ubc84\uc2dc \ubcf4\uc7a5\uc744 \uc81c\uacf5\ud558\uace0, \uc774\ub860\uc801 \uc131\ub2a5 \uacbd\uacc4\ub97c \ub3c4\ucd9c\ud558\uc600\ub2e4.", "conclusion": "\uc81c\uc548\ub41c \uc54c\uace0\ub9ac\uc998\uc740 \ub0ae\uc740 \uacbd\uacc4\uc640 poly-\ub85c\uadf8 \ucd5c\uc18c \uc694\uc778\uae4c\uc9c0 \uc77c\uce58\ud558\uba70, \ubc34\ub527 \ubb38\uc81c\uc5d0\uc11c\uc758 \ud504\ub77c\uc774\ubc84\uc2dc \ubcf4\ud638 \uc758\uc0ac \uacb0\uc815 \ud504\ub808\uc784\uc6cc\ud06c\uc5d0 \ub300\ud55c \ud1b5\ucc30\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23077", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.23077", "abs": "https://arxiv.org/abs/2507.23077", "authors": ["Agnese Marcato", "Aleksandra Pachalieva", "Ryley G. Hill", "Kai Gao", "Xiaoyu Wang", "Esteban Rougier", "Zhou Lei", "Vinamra Agrawal", "Janel Chua", "Qinjun Kang", "Jeffrey D. Hyman", "Abigail Hunter", "Nathan DeBardeleben", "Earl Lawrence", "Hari Viswanathan", "Daniel O'Malley", "Javier E. Santos"], "title": "A Foundation Model for Material Fracture Prediction", "comment": null, "summary": "Accurately predicting when and how materials fail is critical to designing\nsafe, reliable structures, mechanical systems, and engineered components that\noperate under stress. Yet, fracture behavior remains difficult to model across\nthe diversity of materials, geometries, and loading conditions in real-world\napplications. While machine learning (ML) methods show promise, most models are\ntrained on narrow datasets, lack robustness, and struggle to generalize.\nMeanwhile, physics-based simulators offer high-fidelity predictions but are\nfragmented across specialized methods and require substantial high-performance\ncomputing resources to explore the input space. To address these limitations,\nwe present a data-driven foundation model for fracture prediction, a\ntransformer-based architecture that operates across simulators, a wide range of\nmaterials (including plastic-bonded explosives, steel, aluminum, shale, and\ntungsten), and diverse loading conditions. The model supports both structured\nand unstructured meshes, combining them with large language model embeddings of\ntextual input decks specifying material properties, boundary conditions, and\nsolver settings. This multimodal input design enables flexible adaptation\nacross simulation scenarios without changes to the model architecture. The\ntrained model can be fine-tuned with minimal data on diverse downstream tasks,\nincluding time-to-failure estimation, modeling fracture evolution, and adapting\nto combined finite-discrete element method simulations. It also generalizes to\nunseen materials such as titanium and concrete, requiring as few as a single\nsample, dramatically reducing data needs compared to standard ML. Our results\nshow that fracture prediction can be unified under a single model architecture,\noffering a scalable, extensible alternative to simulator-specific workflows.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \ub2e4\uc591\ud55c \uc7ac\ub8cc \ubc0f \ud558\uc911 \uc870\uac74\uc5d0\uc11c \uade0\uc5f4 \uc608\uce21\uc744 \uc704\ud55c \ub370\uc774\ud130 \uae30\ubc18 \ubaa8\ub378\uc744 \uc81c\uc548\ud558\uba70, \uc774\ub294 \uae30\uc874\uc758 \uba38\uc2e0\ub7ec\ub2dd \ubc0f \ubb3c\ub9ac \uae30\ubc18 \uc2dc\ubbac\ub808\uc774\ud130\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud55c\ub2e4.", "motivation": "\uc7ac\ub8cc\uc758 \ud30c\uc190 \uc2dc\uc810\uacfc \ubc29\uc2dd\uc744 \uc815\ud655\ud788 \uc608\uce21\ud558\ub294 \uac83\uc740 \uc548\uc804\ud558\uace0 \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \uad6c\uc870\ubb3c\uacfc \uc2dc\uc2a4\ud15c \uc124\uacc4\uc5d0 \ud544\uc218\uc801\uc774\ub2e4.", "method": "\ubcc0\ud615\uae30\ubc18 \uc544\ud0a4\ud14d\ucc98\uc758 \ub370\uc774\ud130 \uae30\ubc18 \ubaa8\ub378\uc744 \uc81c\uc548\ud558\uc5ec \ub2e4\uc591\ud55c \uc7ac\ub8cc\uc640 \ud558\uc911 \uc870\uac74\uc5d0 \ub300\ud574 \uc791\ub3d9\ud558\ub3c4\ub85d \uc124\uacc4\ud558\uc600\ub2e4.", "result": "\uc774 \ubaa8\ub378\uc740 \uae30\uc874\uc758 \uba38\uc2e0\ub7ec\ub2dd \ubc29\uc2dd\ubcf4\ub2e4 \ub370\uc774\ud130 \uc694\uad6c\ub7c9\uc744 \ub300\ud3ed \uc904\uc774\uace0, \uc0c8\ub85c\uc6b4 \uc7ac\ub8cc\uc5d0 \ub300\ud574\uc11c\ub3c4 \ub192\uc740 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "conclusion": "\uade0\uc5f4 \uc608\uce21 \ubb38\uc81c\ub97c \ub2e8\uc77c \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\ub85c \ud1b5\ud569\ud558\uc5ec, \uc2dc\ubbac\ub808\uc774\ud130 \ud2b9\uc815 \uc791\uc5c5\ud750\ub984\uc758 \ud655\uc7a5 \uac00\ub2a5\ud55c \ub300\uc548\uc744 \uc81c\uacf5\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud55c\ub2e4."}}
{"id": "2507.23093", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.23093", "abs": "https://arxiv.org/abs/2507.23093", "authors": ["Ghazal Sobhani", "Md. Monzurul Amin Ifath", "Tushar Sharma", "Israat Haque"], "title": "On the Sustainability of AI Inferences in the Edge", "comment": "14 pages, 8 figures, 6 tables, in preparation for journal submission", "summary": "The proliferation of the Internet of Things (IoT) and its cutting-edge\nAI-enabled applications (e.g., autonomous vehicles and smart industries)\ncombine two paradigms: data-driven systems and their deployment on the edge.\nUsually, edge devices perform inferences to support latency-critical\napplications. In addition to the performance of these resource-constrained edge\ndevices, their energy usage is a critical factor in adopting and deploying edge\napplications. Examples of such devices include Raspberry Pi (RPi), Intel Neural\nCompute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU).\nDespite their adoption in edge deployment for AI inferences, there is no study\non their performance and energy usage for informed decision-making on the\ndevice and model selection to meet the demands of applications. This study\nfills the gap by rigorously characterizing the performance of traditional,\nneural networks, and large language models on the above-edge devices.\nSpecifically, we analyze trade-offs among model F1 score, inference time,\ninference power, and memory usage. Hardware and framework optimization, along\nwith external parameter tuning of AI models, can balance between model\nperformance and resource usage to realize practical edge AI deployments.", "AI": {"tldr": "\uc774 \uc5f0\uad6c\ub294 IoT \uc5e3\uc9c0 \uc7a5\uce58\uc5d0\uc11c \uc804\ud1b5\uc801\uc778 \uc2e0\uacbd\ub9dd \ubc0f \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc131\ub2a5\uacfc \uc5d0\ub108\uc9c0 \uc0ac\uc6a9\uc744 \ubd84\uc11d\ud558\uc5ec \uacb0\uc815\uc801 \uc120\ud0dd\uc744 \uc704\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud55c\ub2e4.", "motivation": "IoT \ubc0f AI \ud65c\uc6a9\uc758 \ud655\uc0b0 \uc18d\uc5d0\uc11c \uc131\ub2a5\uacfc \uc5d0\ub108\uc9c0 \uc18c\ube44\ub294 \uacb0\uc815\uc801 \uc694\uc778\uc774\ub2e4.", "method": "\uc804\ud1b5\uc801 \uc2e0\uacbd\ub9dd\uacfc \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378\uc758 F1 \uc810\uc218, \ucd94\ub860 \uc2dc\uac04, \ucd94\ub860 \uc804\ub825, \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ubd84\uc11d\ud55c\ub2e4.", "result": "\uc131\ub2a5\uacfc \uc790\uc6d0 \uc0ac\uc6a9 \uac04\uc758 \uade0\ud615\uc744 \ub9de\ucd94\ub294 \ud558\ub4dc\uc6e8\uc5b4 \ubc0f \ud504\ub808\uc784\uc6cc\ud06c \ucd5c\uc801\ud654 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "conclusion": "\uc774 \uc5f0\uad6c\ub294 \ud6a8\uc728\uc801\uc778 \uc5e3\uc9c0 AI \ubc30\ud3ec\ub97c \uc704\ud55c \ubaa8\ub378 \uc120\ud0dd\uacfc \ud558\ub4dc\uc6e8\uc5b4 \ud65c\uc6a9 \ubc29\uc548\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23111", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23111", "abs": "https://arxiv.org/abs/2507.23111", "authors": ["Richard Williams", "Eric Nalisnick", "Andrew Holbrook"], "title": "Scalable Generative Modeling of Weighted Graphs", "comment": "25 pages, 5 figures, included appendix. code at\n  https://github.com/rlwilliams34/BiGG-E", "summary": "Weighted graphs are ubiquitous throughout biology, chemistry, and the social\nsciences, motivating the development of generative models for abstract weighted\ngraph data using deep neural networks. However, most current deep generative\nmodels are either designed for unweighted graphs and are not easily extended to\nweighted topologies or incorporate edge weights without consideration of a\njoint distribution with topology. Furthermore, learning a distribution over\nweighted graphs must account for complex nonlocal dependencies between both the\nedges of the graph and corresponding weights of each edge. We develop an\nautoregressive model BiGG-E, a nontrivial extension of the BiGG model, that\nlearns a joint distribution over weighted graphs while still exploiting\nsparsity to generate a weighted graph with $n$ nodes and $m$ edges in $O((n +\nm)\\log n)$ time. Simulation studies and experiments on a variety of benchmark\ndatasets demonstrate that BiGG-E best captures distributions over weighted\ngraphs while remaining scalable and computationally efficient.", "AI": {"tldr": "BiGG-E\ub77c\ub294 \uc624\ud1a0 \ud68c\uadc0 \ubaa8\ub378\uc774 \uac00\uc911 \uadf8\ub798\ud504\uc758 \ubd84\ud3ec\ub97c \ud559\uc2b5\ud558\uace0, \ud6a8\uc728\uc801\uc73c\ub85c \uc0dd\uc131\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "motivation": "\uc0dd\ubb3c\ud559, \ud654\ud559, \uc0ac\ud68c \uacfc\ud559 \ub4f1\uc5d0\uc11c \uac00\uc911 \uadf8\ub798\ud504\uc758 \uc911\uc694\uc131\uc774 \ucee4\uc9d0\uc5d0 \ub530\ub77c, \uc2ec\uce35 \uc2e0\uacbd\ub9dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac00\uc911 \uadf8\ub798\ud504 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc0dd\uc131\ubaa8\ub378 \uac1c\ubc1c \ud544\uc694\uc131\uc774 \ub300\ub450\ub418\uc5c8\ub2e4.", "method": "BiGG \ubaa8\ub378\uc758 \ube44\ub2e8\uc21c\ud55c \ud655\uc7a5\ud310\uc778 BiGG-E\ub97c \uac1c\ubc1c\ud558\uc5ec, \uac00\uc911 \uadf8\ub798\ud504\uc5d0 \ub300\ud55c \uacf5\ub3d9 \ubd84\ud3ec\ub97c \ud559\uc2b5\ud558\uba74\uc11c \ud76c\uc18c\uc131\uc744 \ud65c\uc6a9\ud558\uc5ec \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \uc720\uc9c0\ud55c\ub2e4.", "result": "\uc2dc\ubbac\ub808\uc774\uc158 \uc5f0\uad6c\uc640 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \ub370\uc774\ud130\uc14b\uc5d0\uc11c BiGG-E\uac00 \uac00\uc911 \uadf8\ub798\ud504\uc758 \ubd84\ud3ec\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud3ec\ucc29\ud558\uba70, \ud655\uc7a5\uc131\uacfc \uacc4\uc0b0 \ud6a8\uc728\uc131\uc744 \uc81c\uacf5\ud568\uc744 \uc785\uc99d\ud588\ub2e4.", "conclusion": "BiGG-E \ubaa8\ub378\uc740 \uac00\uc911 \uadf8\ub798\ud504 \uc0dd\uc131\uc744 \uc704\ud55c \ud6a8\uacfc\uc801\uc774\uace0 \ud6a8\uc728\uc801\uc778 \uc811\uadfc \ubc29\uc2dd\uc73c\ub85c, \uc55e\uc73c\ub85c\uc758 \uc5f0\uad6c\uc5d0\ub3c4 \uc720\uc6a9\ud558\uac8c \ud65c\uc6a9\ub420 \uc218 \uc788\ub2e4."}}
{"id": "2507.23115", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23115", "abs": "https://arxiv.org/abs/2507.23115", "authors": ["David J Goetze", "Dahlia J Felten", "Jeannie R Albrecht", "Rohit Bhattacharya"], "title": "FLOSS: Federated Learning with Opt-Out and Straggler Support", "comment": "5 pages", "summary": "Previous work on data privacy in federated learning systems focuses on\nprivacy-preserving operations for data from users who have agreed to share\ntheir data for training. However, modern data privacy agreements also empower\nusers to use the system while opting out of sharing their data as desired. When\ncombined with stragglers that arise from heterogeneous device capabilities, the\nresult is missing data from a variety of sources that introduces bias and\ndegrades model performance. In this paper, we present FLOSS, a system that\nmitigates the impacts of such missing data on federated learning in the\npresence of stragglers and user opt-out, and empirically demonstrate its\nperformance in simulations.", "AI": {"tldr": "FLOSS\ub294 \uc0ac\uc6a9\uc790 \uc120\ud0dd \ud574\uc9c0\ub97c \uace0\ub824\ud558\uc5ec \uacb0\uce21 \ub370\uc774\ud130\ub97c \uc644\ud654\ud558\ub294 \uc5f0\ud569 \ud559\uc2b5 \uc2dc\uc2a4\ud15c\uc774\ub2e4.", "motivation": "\uc0ac\uc6a9\uc790\uac00 \ub370\uc774\ud130\ub97c \uacf5\uc720\ud558\uae30\ub97c \uc6d0\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0\uc5d0\ub3c4 \uc5f0\ud569 \ud559\uc2b5 \uc2dc\uc2a4\ud15c\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uad8c\ub9ac\ub97c \uac00\uc9c0\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0, \uacb0\uce21 \ub370\uc774\ud130\ub85c \uc778\ud55c \ud3b8\ud5a5\uacfc \ubaa8\ub378 \uc131\ub2a5 \uc800\ud558\ub97c \ud574\uacb0\ud560 \ud544\uc694\uac00 \uc788\ub2e4.", "method": "FLOSS \uc2dc\uc2a4\ud15c\uc744 \uc81c\uc548\ud558\uc5ec, \uc9c0\uc5f0 \uc7a5\uce58(stragglers)\uc640 \uc0ac\uc6a9\uc790 \uc120\ud0dd \ud574\uc9c0 \uc0c1\ud669\uc5d0\uc11c \uacb0\uce21 \ub370\uc774\ud130\uc758 \uc601\ud5a5\uc744 \uc644\ud654\ud558\ub294 \ubc29\ubc95\uc744 \ub2e4\ub8ec\ub2e4.", "result": "\uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud1b5\ud574 FLOSS\uc758 \uc131\ub2a5\uc744 \uac80\uc99d\ud558\uc600\ub2e4.", "conclusion": "FLOSS\ub294 \uacb0\uce21 \ub370\uc774\ud130 \ubb38\uc81c\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud574\uacb0\ud558\uc5ec \uc5f0\ud569 \ud559\uc2b5\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4."}}
{"id": "2507.23128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23128", "abs": "https://arxiv.org/abs/2507.23128", "authors": ["Ana\u00efs Baranger", "Lucas Maison"], "title": "Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts", "comment": "Submitted to ICASSP 2026", "summary": "Although prior work in computer vision has shown strong correlations between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, such\nrelationships remain underexplored in audio-based models. In this study, we\ninvestigate how training conditions and input features affect the robustness\nand generalization abilities of spoken keyword classifiers under OOD\nconditions. We benchmark several neural architectures across a variety of\nevaluation sets. To quantify the impact of noise on generalization, we make use\nof two metrics: Fairness (F), which measures overall accuracy gains compared to\na baseline model, and Robustness (R), which assesses the convergence between ID\nand OOD performance. Our results suggest that noise-aware training improves\nrobustness in some configurations. These findings shed new light on the\nbenefits and limitations of noise-based augmentation for generalization in\nspeech models.", "AI": {"tldr": "\uc774\ubc88 \uc5f0\uad6c\uc5d0\uc11c\ub294 \uc74c\uc131 \ud0a4\uc6cc\ub4dc \ubd84\ub958\uae30\uc758 OOD \uc870\uac74\uc5d0\uc11c\uc758 \uac15\uac74\uc131\uacfc \uc77c\ubc18\ud654 \ub2a5\ub825\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ud6c8\ub828 \uc870\uac74\uacfc \uc785\ub825 \ud2b9\uc9d5\uc744 \uc870\uc0ac\ud558\uc600\ub2e4.", "motivation": "\uc774\uc804\uc758 \ucef4\ud4e8\ud130 \ube44\uc804 \uc5f0\uad6c\ub294 ID\uc640 OOD \uc815\ud655\ub3c4 \uac04\uc758 \uac15\ud55c \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\uc5c8\uc73c\ub098, \uc624\ub514\uc624 \uae30\ubc18 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \uad00\uacc4\uac00 \ucda9\ubd84\ud788 \ud0d0\uad6c\ub418\uc9c0 \uc54a\uc558\ub2e4.", "method": "\ub2e4\uc591\ud55c \ud3c9\uac00 \uc138\ud2b8\ub97c \uae30\uc900\uc73c\ub85c \uc5ec\ub7ec \uc2e0\uacbd\ub9dd \uc544\ud0a4\ud14d\ucc98\ub97c \ubca4\uce58\ub9c8\ud0b9\ud558\uace0, \ub178\uc774\uc988\uac00 \uc77c\ubc18\ud654\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc815\ub7c9\ud654\ud558\uae30 \uc704\ud574 \uacf5\uc815\uc131(F)\uacfc \uac15\uac74\uc131(R)\uc774\ub77c\ub294 \ub450 \uac00\uc9c0 \uc9c0\ud45c\ub97c \uc0ac\uc6a9\ud558\uc600\ub2e4.", "result": "\ub178\uc774\uc988\uc5d0 \ub300\ud55c \uc778\uc2dd \ud6c8\ub828\uc774 \uc77c\ubd80 \uad6c\uc131\uc5d0\uc11c \uac15\uac74\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0b4\uc744 \ubc1c\uacac\ud558\uc600\ub2e4.", "conclusion": "\ub178\uc774\uc988 \uae30\ubc18 \uc99d\uac00\uac00 \uc74c\uc131 \ubaa8\ub378\uc758 \uc77c\ubc18\ud654\uc5d0 \ubbf8\uce58\ub294 \uc7a5\uc810\uacfc \ud55c\uacc4\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23136", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23136", "abs": "https://arxiv.org/abs/2507.23136", "authors": ["Erin George", "Deanna Needell", "Berk Ustun"], "title": "Observational Multiplicity", "comment": null, "summary": "Many prediction tasks can admit multiple models that can perform almost\nequally well. This phenomenon can can undermine interpretability and safety\nwhen competing models assign conflicting predictions to individuals. In this\nwork, we study how arbitrariness can arise in probabilistic classification\ntasks as a result of an effect that we call \\emph{observational multiplicity}.\nWe discuss how this effect arises in a broad class of practical applications\nwhere we learn a classifier to predict probabilities $p_i \\in [0,1]$ but are\ngiven a dataset of observations $y_i \\in \\{0,1\\}$. We propose to evaluate the\narbitrariness of individual probability predictions through the lens of\n\\emph{regret}. We introduce a measure of regret for probabilistic\nclassification tasks, which measures how the predictions of a model could\nchange as a result of different training labels change. We present a\ngeneral-purpose method to estimate the regret in a probabilistic classification\ntask. We use our measure to show that regret is higher for certain groups in\nthe dataset and discuss potential applications of regret. We demonstrate how\nestimating regret promote safety in real-world applications by abstention and\ndata collection.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \uad00\ucc30 \ub2e4\uc911\uc131 \ud6a8\uacfc\uc5d0 \uc758\ud574 \ubc1c\uc0dd\ud558\ub294 \uc608\uce21 \ubaa8\ub378 \uac04\uc758 \uc790\uc758\uc131\uc744 \uc5f0\uad6c\ud558\uba70, \ud655\ub960\uc801 \ubd84\ub958 \uc791\uc5c5\uc5d0\uc11c \uac1c\ubcc4 \ud655\ub960 \uc608\uce21\uc758 \uc790\uc758\uc131\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \ud6c4\ud68c \ucc99\ub3c4\ub97c \ub3c4\uc785\ud55c\ub2e4.", "motivation": "\uc5ec\ub7ec \ubaa8\ub378\uc774 \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ubcf4\uc77c \ub54c \ubc1c\uc0dd\ud558\ub294 \uc608\uce21\uc758 \uc790\uc758\uc131\uc73c\ub85c \uc778\ud574 \ud574\uc11d \uac00\ub2a5\uc131 \ubc0f \uc548\uc804\uc131\uc774 \uc800\ud574\ub420 \uc218 \uc788\uc74c\uc744 \ub2e4\ub8ec\ub2e4.", "method": "\ud655\ub960\uc801 \ubd84\ub958 \uc791\uc5c5\uc5d0\uc11c \ubaa8\ub378\uc758 \uc608\uce21\uc774 \ub2e4\uc591\ud55c \ud6c8\ub828 \ub77c\ubca8\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c \ubcc0\ud654\ud560 \uc218 \uc788\ub294\uc9c0\ub97c \uce21\uc815\ud558\ub294 \ud6c4\ud68c \ucc99\ub3c4\ub97c \ub3c4\uc785\ud558\uace0, \uc774\ub97c \ucd94\uc815\ud558\ub294 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4.", "result": "\ud6c4\ud68c \ucc99\ub3c4\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub370\uc774\ud130\uc14b \ub0b4 \ud2b9\uc815 \uadf8\ub8f9\uc5d0\uc11c \ud6c4\ud68c\uac00 \ub354 \ub192\uc74c\uc744 \ud655\uc778\ud558\uc600\ub2e4.", "conclusion": "\ud6c4\ud68c \ucd94\uc815\uc774 \uc2e4\uc81c \uc751\uc6a9\uc5d0\uc11c \uc548\uc804\uc131\uc744 \uc99d\uc9c4\uc2dc\ud0ac \uc218 \uc788\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\uba70, \ub370\uc774\ud130 \uc218\uc9d1 \ubc0f \uc790\uc81c \uacb0\uc815\uc744 \ud1b5\ud574 \uac00\ub2a5\uc131\uc744 \uc81c\uc2dc\ud55c\ub2e4."}}
{"id": "2507.23141", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.23141", "abs": "https://arxiv.org/abs/2507.23141", "authors": ["Xiangshu Gong", "Zhiqiang Xie", "Xiaowei Jin", "Chen Wang", "Yanling Qu", "Wangmeng Zuo", "Hui Li"], "title": "AI paradigm for solving differential equations: first-principles data generation and scale-dilation operator AI solver", "comment": null, "summary": "Many problems are governed by differential equations (DEs). Artificial\nintelligence (AI) is a new path for solving DEs. However, data is very scarce\nand existing AI solvers struggle with approximation of high frequency\ncomponents (AHFC). We propose an AI paradigm for solving diverse DEs, including\nDE-ruled first-principles data generation methodology and scale-dilation\noperator (SDO) AI solver. Using either prior knowledge or random fields, we\ngenerate solutions and then substitute them into the DEs to derive the sources\nand initial/boundary conditions through balancing DEs, thus producing\narbitrarily vast amount of, first-principles-consistent training datasets at\nextremely low computational cost. We introduce a reversible SDO that leverages\nthe Fourier transform of the multiscale solutions to fix AHFC, and design a\nspatiotemporally coupled, attention-based Transformer AI solver of DEs with\nSDO. An upper bound on the Hessian condition number of the loss function is\nproven to be proportional to the squared 2-norm of the solution gradient,\nrevealing that SDO yields a smoother loss landscape, consequently fixing AHFC\nwith efficient training. Extensive tests on diverse DEs demonstrate that our AI\nparadigm achieves consistently superior accuracy over state-of-the-art methods.\nThis work makes AI solver of DEs to be truly usable in broad nature and\nengineering fields.", "AI": {"tldr": "AI \uae30\ubc18\uc758 \uc0c8\ub85c\uc6b4 \uc811\uadfc\ubc95\uc744 \ud1b5\ud574 \ucc28\ubcc4 \ubc29\uc815\uc2dd \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud558\uba70, \ub370\uc774\ud130 \uc0dd\uc131\uacfc \uace0\uc8fc\ud30c \uc131\ubd84 \uc218\uc815\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uc218\ud589\ud558\uc5ec \uae30\uc874 \ubc29\ubc95\ubcf4\ub2e4 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud568.", "motivation": "\ucc28\ubcc4 \ubc29\uc815\uc2dd(DEs)\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 AI\ub97c \ud65c\uc6a9\ud558\ub824\ub294 \ud544\uc694\uc131\uacfc \ub370\uc774\ud130 \ubd80\uc871 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uace0\uc790 \ud568.", "method": "\uccab \uc6d0\uce59 \uae30\ubc18 \ub370\uc774\ud130 \uc0dd\uc131 \ubc29\ubc95\ub860\uacfc \uc2a4\ucf00\uc77c-\ud33d\ucc3d \uc5f0\uc0b0\uc790(SDO)\ub97c \uacb0\ud569\ud558\uc5ec \ud574\uacb0\ud55c \ub2e4\uc591\ud55c DEs\ub97c \uc704\ud55c AI \uc811\uadfc\ubc95\uc744 \uc81c\uc548\ud568.", "result": "\ub2e4\uc591\ud55c \ucc28\ubcc4 \ubc29\uc815\uc2dd\uc5d0 \ub300\ud55c \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8\uc744 \ud1b5\ud574 \uc81c\uc548\ub41c AI \uc811\uadfc\ubc95\uc774 \ucd5c\uc2e0 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \uc77c\uad00\ub418\uac8c \uc6b0\uc218\ud55c \uc815\ud655\ub3c4\ub97c \uae30\ub85d\ud568.", "conclusion": "\uc774 \uc5f0\uad6c\ub294 DEs\uc758 AI \ud574\uacb0 \ubc29\ubc95\uc774 \uc790\uc5f0 \ubc0f \uacf5\ud559 \ubd84\uc57c\uc5d0\uc11c \ub110\ub9ac \uc0ac\uc6a9\ub420 \uc218 \uc788\ub3c4\ub85d \ud558\uc5ec \uc2e4\uc9c8\uc801\uc778 \uae30\uc5ec\ub97c \ud568."}}
{"id": "2507.23154", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23154", "abs": "https://arxiv.org/abs/2507.23154", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations", "comment": "Accepted in the 2025 International Conference on Machine Intelligence\n  for GeoAnalytics and Remote Sensing (MIGARS)", "summary": "Urban heatwaves, droughts, and land degradation are pressing and growing\nchallenges in the context of climate change. A valuable approach to studying\nthem requires accurate spatio-temporal information on land surface conditions.\nOne of the most important variables for assessing and understanding these\nphenomena is Land Surface Temperature (LST), which is derived from satellites\nand provides essential information about the thermal state of the Earth's\nsurface. However, satellite platforms inherently face a trade-off between\nspatial and temporal resolutions. To bridge this gap, we propose FuseTen, a\nnovel generative framework that produces daily LST observations at a fine 10 m\nspatial resolution by fusing spatio-temporal observations derived from\nSentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative\narchitecture trained using an averaging-based supervision strategy grounded in\nphysical principles. It incorporates attention and normalization modules within\nthe fusion process and uses a PatchGAN discriminator to enforce realism.\nExperiments across multiple dates show that FuseTen outperforms linear\nbaselines, with an average 32.06% improvement in quantitative metrics and\n31.42% in visual fidelity. To the best of our knowledge, this is the first\nnon-linear method to generate daily LST estimates at such fine spatial\nresolution.", "AI": {"tldr": "FuseTen\uc740 \ub2e4\uc591\ud55c \uc704\uc131 \ub370\uc774\ud130\ub97c \uc735\ud569\ud558\uc5ec \ud558\ub8e8 \ub2e8\uc704\uc758 10m \ud574\uc0c1\ub3c4 \uc9c0\ud45c\uc628\ub3c4\ub97c \uc0dd\uc131\ud558\ub294 \ud601\uc2e0\uc801 \uc811\uadfc \ubc29\uc2dd\uc774\ub2e4.", "motivation": "\uae30\ud6c4 \ubcc0\ud654 \ub9e5\ub77d\uc5d0\uc11c \uc5f4\ud30c, \uac00\ubb44, \ud1a0\uc9c0 degradation\uc744 \uc815\ud655\ud788 \uc774\ud574\ud558\uace0 \ud3c9\uac00\ud558\uae30 \uc704\ud574 \ud544\uc694\ud55c \uc9c0\ud45c\uc628\ub3c4 \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\uae30 \uc704\ud568\uc774\ub2e4.", "method": "Sentinel-2, Landsat 8, Terra MODIS\ub85c\ubd80\ud130 \uc5bb\uc740 \uad00\uce21 \ub370\uc774\ud130\ub97c \uc735\ud569\ud558\uae30 \uc704\ud55c \uc0dd\uc131\uc801 \ud504\ub808\uc784\uc6cc\ud06c FuseTen\uc744 \uc81c\uc548\ud558\uba70, \ud3c9\uade0 \uae30\ubc18 \uac10\ub3c5 \uc804\ub7b5\uc744 \ud1b5\ud574 \ud2b8\ub808\uc774\ub2dd\ub41c\ub2e4.", "result": "FuseTen\uc740 \uc120\ud615 \uae30\uc900\uc120\uacfc \ube44\uad50\ud558\uc5ec \uc815\ub7c9\uc801 \uc9c0\ud45c\uc5d0\uc11c \ud3c9\uade0 32.06% \ud5a5\uc0c1, \uc2dc\uac01\uc801 \ucda9\uc2e4\ub3c4\uc5d0\uc11c 31.42% \ud5a5\uc0c1\uc744 \ubcf4\uc600\ub2e4.", "conclusion": "\uc5ec\uae30\uc11c\ub294 \ucc98\uc74c\uc73c\ub85c \ube44\uc120\ud615 \ubc29\ubc95\uc744 \ud1b5\ud574 \ud558\ub8e8 \ub2e8\uc704\uc758 \uc9c0\ud45c\uc628\ub3c4\ub97c \uc774\ucc98\ub7fc \uc138\ubc00\ud55c \ud574\uc0c1\ub3c4\ub85c \uc0dd\uc131\ud55c \uac83\uc774\ub2e4."}}
{"id": "2507.23170", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23170", "abs": "https://arxiv.org/abs/2507.23170", "authors": ["Jinan Zhou", "Rajat Ghosh", "Vaishnavi Bhargava", "Debojyoti Dutta", "Aryan Singhal"], "title": "BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning", "comment": null, "summary": "When designing LLM services, practitioners care about three key properties:\ninference-time budget, factual authenticity, and reasoning capacity. However,\nour analysis shows that no model can simultaneously optimize for all three. We\nformally prove this trade-off and propose a principled framework named The BAR\nTheorem for LLM-application design.", "AI": {"tldr": "LLM \uc11c\ube44\uc2a4 \uc124\uacc4\uc5d0\uc11c \uc138 \uac00\uc9c0 \ud575\uc2ec \uc18d\uc131 \uac04\uc758 \ucd5c\uc801\ud654 \ubd88\uac00\ub2a5\uc131\uc744 \uc99d\uba85\ud558\uace0 BAR \uc815\ub9ac\ub97c \uc81c\uc548\ud568.", "motivation": "LLM \uc11c\ube44\uc2a4\uc5d0\uc11c\uc758 \uc608\uce21 \uc2dc\uac04 \uc608\uc0b0, \uc0ac\uc2e4\uc801 \uc9c4\uc704, \ucd94\ub860 \ub2a5\ub825\uc758 \uc911\uc694\uc131 \uc778\uc2dd", "method": "BAR \uc815\ub9ac\ub77c\ub294 \uccb4\uacc4\uc801 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud558\uc5ec \uc138 \uac00\uc9c0 \uc18d\uc131 \uac04\uc758 \ud2b8\ub808\uc774\ub4dc\uc624\ud504\ub97c \ud615\uc2dd\uc801\uc73c\ub85c \uc99d\uba85\ud568.", "result": "\ubaa8\ub4e0 \ubaa8\ub378\uc774 \uc138 \uac00\uc9c0 \uc18d\uc131\uc744 \ub3d9\uc2dc\uc5d0 \ucd5c\uc801\ud654\ud560 \uc218 \uc5c6\uc74c\uc744 \ud655\uc778\ud568.", "conclusion": "LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc124\uacc4\uc5d0\uc11c \uc774\ub860\uc801 \uae30\ubc18 \uc81c\uacf5\uc758 \ud544\uc694\uc131 \uac15\uc870."}}
{"id": "2507.23186", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.23186", "abs": "https://arxiv.org/abs/2507.23186", "authors": ["Peter Sharpe"], "title": "NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions", "comment": null, "summary": "Sparsity detection in black-box functions enables significant computational\nspeedups in gradient-based optimization through Jacobian compression, but\nexisting finite-difference methods suffer from false negatives due to\ncoincidental zero gradients. These false negatives can silently corrupt\ngradient calculations, leading to difficult-to-diagnose errors. We introduce\nNaN-propagation, which exploits the universal contamination property of IEEE\n754 Not-a-Number floating-point values to trace input-output dependencies\nthrough floating-point numerical computations. By systematically contaminating\ninputs with NaN and observing which outputs become NaN, the method reconstructs\nconservative sparsity patterns that eliminate false negatives. We demonstrate\nthe approach on an aerospace wing weight model, achieving a 1.52x speedup while\ndetecting dozens of dependencies missed by conventional methods -- a\nsignificant improvement since gradient computation is the bottleneck in many\noptimization workflows. The technique leverages IEEE 754 compliance to work\nacross programming languages and math libraries without modifying existing\nblack-box codes. Advanced strategies including NaN payload encoding enable\nfaster-than-linear time complexity, improving upon existing black-box sparsity\ndetection methods. Practical algorithms are also proposed to mitigate\nchallenges from branching code execution common in engineering applications.", "AI": {"tldr": "NaN \uc804\ud30c \uae30\ubc95\uc744 \ud65c\uc6a9\ud55c \ube14\ub799\ubc15\uc2a4 \ud568\uc218\uc758 \ud76c\uc18c\uc131 \ud0d0\uc9c0\uac00 \uadf8\ub798\ub514\uc5b8\ud2b8 \uacc4\uc0b0\uc758 \ud6a8\uc728\uc131\uc744 \uac1c\uc120\ud558\uace0 \uc624\ub958\ub97c \uc904\uc778\ub2e4.", "motivation": "\ube14\ub799\ubc15\uc2a4 \ud568\uc218\uc758 \ud76c\uc18c\uc131 \ud0d0\uc9c0\ub294 \uadf8\ub798\ub514\uc5b8\ud2b8 \uae30\ubc18 \ucd5c\uc801\ud654\uc5d0\uc11c\uc758 \uacc4\uc0b0 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubaa9\ud45c\ub85c \ud55c\ub2e4.", "method": "NaN \uc785\ub825\uc744 \uccb4\uacc4\uc801\uc73c\ub85c \uc624\uc5fc\uc2dc\ucf1c \uc5b4\ub5a4 \ucd9c\ub825\uc774 NaN\uc774 \ub418\ub294\uc9c0\ub97c \uad00\ucc30\ud558\uc5ec \ubcf4\uc218\uc801\uc778 \ud76c\uc18c\uc131 \ud328\ud134\uc744 \uc7ac\uad6c\uc131\ud558\ub294 \ubc29\ubc95\uc744 \uc18c\uac1c\ud55c\ub2e4.", "result": "\ud56d\uacf5\uc6b0\uc8fc \ub0a0\uac1c \uc911\ub7c9 \ubaa8\ub378\uc5d0\uc11c 1.52\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uacfc \uae30\uc874 \ubc29\ubc95\uc73c\ub85c \uac10\uc9c0\ud558\uc9c0 \ubabb\ud55c \uc5ec\ub7ec \uc758\uc874\uc131\uc744 \ud3ec\ucc29\ud558\uc600\ub2e4.", "conclusion": "IEEE 754 \ud638\ud658\uc131\uc744 \ud1b5\ud574 \uae30\uc874 \ube14\ub799\ubc15\uc2a4 \ucf54\ub4dc \uc218\uc815 \uc5c6\uc774 \ub2e4\uc591\ud55c \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\uc640 \uc218\ud559 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \ub3d9\uc791\ud558\uba70, \uc0c8\ub85c\uc6b4 \uc54c\uace0\ub9ac\uc998\uc774 \uacf5\ud559 \uc751\uc6a9\uc5d0\uc11c\uc758 \ubd84\uae30 \ucf54\ub4dc \uc2e4\ud589 \ubb38\uc81c\ub97c \uc644\ud654\ud55c\ub2e4."}}
{"id": "2507.23217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23217", "abs": "https://arxiv.org/abs/2507.23217", "authors": ["Hyeon Seong Jeong", "Sangwoo Jo", "Byeong Hyun Yoon", "Yoonseok Heo", "Haedong Jeong", "Taehoon Kim"], "title": "Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation", "comment": null, "summary": "Understanding complex multimodal documents remains challenging due to their\nstructural inconsistencies and limited training data availability. We introduce\n\\textit{DocsRay}, a training-free document understanding system that integrates\npseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented\nGeneration (RAG). Our approach leverages multimodal Large Language Models'\n(LLMs) native capabilities to seamlessly process documents containing diverse\nelements such as text, images, charts, and tables without requiring specialized\nmodels or additional training. DocsRay's framework synergistically combines\nthree key techniques: (1) a semantic structuring module using prompt-based LLM\ninteractions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal\nanalysis that converts diverse document elements into unified, text-centric\nrepresentations using the inherent capabilities of multimodal LLMs, and (3) an\nefficient two-stage hierarchical retrieval system that reduces retrieval\ncomplexity from $O(N)$ to $O(S + k_1 \\cdot N_s)$. Evaluated on documents\naveraging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency\nfrom 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the\nMMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%,\nsubstantially surpassing previous state-of-the-art results.", "AI": {"tldr": "DocsRay\ub294 \ud6c8\ub828 \uc5c6\uc774 \uba40\ud2f0\ubaa8\ub2ec \ubb38\uc11c \uc774\ud574\ub97c \uc704\ud55c \uc2dc\uc2a4\ud15c\uc73c\ub85c, \uc758\uc0ac \ubaa9\ucc28 \uc0dd\uc131\uacfc \uacc4\uce35\ud615 RAG\ub97c \uacb0\ud569\ud558\uc5ec \ud6a8\uc728\uc131\uacfc \uc815\ud655\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4.", "motivation": "\ubcf5\uc7a1\ud55c \uba40\ud2f0\ubaa8\ub2ec \ubb38\uc11c\uc758 \uad6c\uc870\uc801 \ubd88\uc77c\uce58\uc640 \ud6c8\ub828 \ub370\uc774\ud130 \ubd80\uc871 \ubb38\uc81c \ud574\uacb0\uc744 \uc704\ud574.", "method": "DocsRay\ub294 \uc758\uc0ac TOC \uc0dd\uc131, \uc81c\ub85c\uc0f7 \uba40\ud2f0\ubaa8\ub2ec \ubd84\uc11d, \ud6a8\uc728\uc801\uc778 \ub450 \ub2e8\uacc4 \uacc4\uce35 \uac80\uc0c9 \uc2dc\uc2a4\ud15c\uc744 \ud1b5\ud569\ud55c \uc2dc\uc2a4\ud15c\uc774\ub2e4.", "result": "DocsRay\ub294 \ucffc\ub9ac \uc9c0\uc5f0 \uc2dc\uac04\uc744 3.89\ucd08\uc5d0\uc11c 2.12\ucd08\ub85c \uc904\uc600\uc73c\uba70, MMLongBench-Doc \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c 64.7%\uc758 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\ub2e4.", "conclusion": "DocsRay\ub294 \uae30\uc874 \ucd5c\ucca8\ub2e8 \uacb0\uacfc\ub97c \ud06c\uac8c \ucd08\uc6d4\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4."}}
{"id": "2507.23221", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23221", "abs": "https://arxiv.org/abs/2507.23221", "authors": ["Charles O'Neill", "Slava Chalnev", "Chi Chi Zhao", "Max Kirkby", "Mudith Jayasekara"], "title": "A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations", "comment": null, "summary": "Contextual hallucinations -- statements unsupported by given context --\nremain a significant challenge in AI. We demonstrate a practical\ninterpretability insight: a generator-agnostic observer model detects\nhallucinations via a single forward pass and a linear probe on its residual\nstream. This probe isolates a single, transferable linear direction separating\nhallucinated from faithful text, outperforming baselines by 5-27 points and\nshowing robust mid-layer performance across Gemma-2 models (2B to 27B).\nGradient-times-activation localises this signal to sparse, late-layer MLP\nactivity. Critically, manipulating this direction causally steers generator\nhallucination rates, proving its actionability. Our results offer novel\nevidence of internal, low-dimensional hallucination tracking linked to specific\nMLP sub-circuits, exploitable for detection and mitigation. We release the\n2000-example ContraTales benchmark for realistic assessment of such solutions.", "AI": {"tldr": "AI\uc758 \ub9e5\ub77d\uc801 \ud658\uac01 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574, \uc0dd\uc131\uae30\uc640 \ubb34\uad00\ud55c \uad00\ucc30\uc790 \ubaa8\ub378\uc774 \ub2e8\uc77c \uc804\ubc29 \ud328\uc2a4\ub97c \ud1b5\ud574 \ud658\uac01\uc744 \ud0d0\uc9c0\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "motivation": "AI\uc5d0\uc11c\uc758 \ud658\uac01 \ubb38\uc81c\ub294 \uc5ec\uc804\ud788 \ud070 \ub3c4\uc804 \uacfc\uc81c\ub85c \ub0a8\uc544 \uc788\uc73c\uba70, \uc774\ub97c \ubcf4\ub2e4 \ud6a8\uacfc\uc801\uc73c\ub85c \ud0d0\uc9c0\ud558\uace0 \uc644\ud654\ud558\ub294 \ubc29\ubc95\uc744 \ucc3e\ub294 \uac83\uc774 \uc911\uc694\ud558\ub2e4.", "method": "\ub2e8\uc77c \uc804\ubc29 \ud328\uc2a4\uc640 \uc794\uc5ec \uc2a4\ud2b8\ub9bc\uc5d0\uc11c\uc758 \uc120\ud615 \ud504\ub85c\ube0c\ub97c \ud1b5\ud574 \ud658\uac01\uacfc \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \ud14d\uc2a4\ud2b8\ub97c \ubd84\ub9ac\ud558\ub294 \ub2e8\uc77c, \uc804\ub2ec \uac00\ub2a5\ud55c \uc120\ud615 \ubc29\ud5a5\uc744 \uace0\ub9bd\uc2dc\ud0a8\ub2e4.", "result": "\uc5ec\ub7ec Gemma-2 \ubaa8\ub378\uc5d0\uc11c 5-27 \ud3ec\uc778\ud2b8 \uc774\uc0c1\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc774\uba70, \uc911\uac04 \uce35 \uc131\ub2a5\uc5d0 \ub300\ud574 \uac15\ub825\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0b8\ub2e4.", "conclusion": "\ud2b9\uc815 MLP \ud558\uc704 \ud68c\ub85c\uc5d0 \uc5f0\uacb0\ub41c \ub0b4\ubd80 \uc800\ucc28\uc6d0 \ud658\uac01 \ucd94\uc801\uc758 \uc0c8\ub85c\uc6b4 \uc99d\uac70\ub97c \uc81c\uacf5\ud558\uba70, \ud0d0\uc9c0 \ubc0f \uc644\ud654\uc5d0 \uc0ac\uc6a9\ub420 \uc218 \uc788\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4."}}
{"id": "2507.23257", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23257", "abs": "https://arxiv.org/abs/2507.23257", "authors": ["Jiawei Liu", "Chenwang Wu", "Defu Lian", "Enhong Chen"], "title": "Efficient Machine Unlearning via Influence Approximation", "comment": "12 pages, 4 figures", "summary": "Due to growing privacy concerns, machine unlearning, which aims at enabling\nmachine learning models to ``forget\" specific training data, has received\nincreasing attention. Among existing methods, influence-based unlearning has\nemerged as a prominent approach due to its ability to estimate the impact of\nindividual training samples on model parameters without retraining. However,\nthis approach suffers from prohibitive computational overhead arising from the\nnecessity to compute the Hessian matrix and its inverse across all training\nsamples and parameters, rendering it impractical for large-scale models and\nscenarios involving frequent data deletion requests. This highlights the\ndifficulty of forgetting. Inspired by cognitive science, which suggests that\nmemorizing is easier than forgetting, this paper establishes a theoretical link\nbetween memorizing (incremental learning) and forgetting (unlearning). This\nconnection allows machine unlearning to be addressed from the perspective of\nincremental learning. Unlike the time-consuming Hessian computations in\nunlearning (forgetting), incremental learning (memorizing) typically relies on\nmore efficient gradient optimization, which supports the aforementioned\ncognitive theory. Based on this connection, we introduce the Influence\nApproximation Unlearning (IAU) algorithm for efficient machine unlearning from\nthe incremental perspective. Extensive empirical evaluations demonstrate that\nIAU achieves a superior balance among removal guarantee, unlearning efficiency,\nand comparable model utility, while outperforming state-of-the-art methods\nacross diverse datasets and model architectures. Our code is available at\nhttps://github.com/Lolo1222/IAU.", "AI": {"tldr": "\ubcf8 \ub17c\ubb38\uc740 \uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc774 \ud2b9\uc815 \ud6c8\ub828 \ub370\uc774\ud130\ub97c '\uc78a\ub294' \uae30\uacc4 \ube44\ud559\uc2b5\uc744 \ub2e4\ub8e8\uba70, \ud6a8\uc728\uc801\uc778 Influence Approximation Unlearning (IAU) \uc54c\uace0\ub9ac\uc998\uc744 \uc81c\uc548\ud55c\ub2e4.", "motivation": "\ud504\ub77c\uc774\ubc84\uc2dc \ubb38\uc81c \uc99d\uac00\ub85c \uc778\ud574 \uae30\uacc4 \ube44\ud559\uc2b5\uc5d0 \ub300\ud55c \uad00\uc2ec\uc774 \ucee4\uc9c0\uace0 \uc788\uc73c\uba70, \ud2b9\ud788 \uac1c\ubcc4 \ud6c8\ub828 \uc0d8\ud50c\uc758 \uc601\ud5a5\uc744 \ucd94\uc815\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc774 \uc911\uc694\ud558\ub2e4.", "method": "\uae30\uc874\uc758 \uc601\ud5a5 \uae30\ubc18 \ube44\ud559\uc2b5 \ubc29\ubc95\uc758 \ub2e8\uc810\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud574 \uc778\uc9c0 \uacfc\ud559\uc5d0 \uc601\uac10\uc744 \ubc1b\uc544 \uc810\uc9c4\uc801 \ud559\uc2b5\uc758 \uad00\uc810\uc5d0\uc11c \uae30\uacc4 \ube44\ud559\uc2b5\uc744 \ub2e4\ub8ec\ub2e4. IAU \uc54c\uace0\ub9ac\uc998\uc744 \ud1b5\ud574 \ud6a8\uc728\uc801\uc778 \ube44\ud559\uc2b5\uc744 \uad6c\ud604\ud55c\ub2e4.", "result": "IAU\ub294 \uc81c\uac70 \ubcf4\uc7a5, \ube44\ud559\uc2b5 \ud6a8\uc728\uc131 \ubc0f \ubaa8\ub378 \uc720\uc6a9\uc131 \uac04 \uade0\ud615\uc744 \uc798 \uc774\ub8e8\uba70, \ub2e4\uc591\ud55c \ub370\uc774\ud130\uc14b\uacfc \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \ucd5c\uc2e0 \ubc29\ubc95\ub4e4\uc744 \ucd08\uc6d4\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "conclusion": "IAU\ub294 \uae30\uc874 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \ud6a8\uc728\uc801\uc778 \uae30\uacc4 \ube44\ud559\uc2b5\uc744 \uac00\ub2a5\ud558\uac8c \ud558\uc5ec \ub300\uaddc\ubaa8 \ubaa8\ub378\uacfc \uc7a6\uc740 \ub370\uc774\ud130 \uc0ad\uc81c \uc694\uccad \uc0c1\ud669\uc5d0\uc11c\ub3c4 \uc2e4\uc6a9\uc801\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23261", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23261", "abs": "https://arxiv.org/abs/2507.23261", "authors": ["Hui Yi Leong", "Yuqing Wu"], "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System", "comment": null, "summary": "Current multi-agent systems (MAS) frameworks often rely on manually designed\nand static collaboration graph structures, limiting adaptability and\nperformance. To address these limitations, we propose DynaSwarm, a dynamic\nframework that enhances LLM-based MAS through two key innovations: (1) an\nactor-critic reinforcement learning (A2C) mechanism to optimize graph\nstructures with improved stability over prior RL methods, and (2) a dynamic\ngraph selector that adaptively chooses the optimal graph structure for each\ninput sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the\nneed for rigid, one-fits-all graph architectures, instead leveraging\nsample-specific idiosyncrasies to dynamically route queries through specialized\nagent networks. (c) We propose to fine-tune the demonstration retriever to\nfully exploit the power of in-context learning (ICL). Extensive experiments on\nquestion answering, mathematical reasoning, and coding tasks demonstrate that\nDynaSwarm consistently outperforms state-of-the-art single-agent and MAS\nbaselines across multiple LLM backbones. Our findings highlight the importance\nof sample-aware structural flexibility in LLM MAS designs.", "AI": {"tldr": "DynaSwarm\uc740 LLM \uae30\ubc18 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c\uc758 \uc801\uc751\uc131\uacfc \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud55c \ub3d9\uc801 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud558\uba70, \uac1c\uc120\ub41c \uadf8\ub798\ud504 \uad6c\uc870 \ucd5c\uc801\ud654 \ubc0f \ub3d9\uc801 \uadf8\ub798\ud504 \uc120\ud0dd\uae30\ub97c \ud1b5\ud574 \ucd5c\uc801\ud654\ub41c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "motivation": "\ud604\uc7ac\uc758 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uace0\uc815\ub41c \ud611\ub825 \uadf8\ub798\ud504 \uad6c\uc870\uc5d0 \uc758\uc874\ud558\uace0 \uc788\uc5b4 \uc801\uc751\uc131\uacfc \uc131\ub2a5\uc774 \uc81c\ud55c\ub41c\ub2e4.", "method": "A2C \uac15\ud654 \ud559\uc2b5 \uba54\ucee4\ub2c8\uc998\uacfc \ub3d9\uc801 \uadf8\ub798\ud504 \uc120\ud0dd\uae30\ub97c \ud1b5\ud574 \uc785\ub825 \uc0d8\ud50c\uc5d0 \ucd5c\uc801\ud654\ub41c \uadf8\ub798\ud504 \uad6c\uc870\ub97c \uc120\ud0dd\ud558\uace0, \ub3c5\ud2b9\ud55c \uc0d8\ud50c \ud2b9\uc131\uc744 \ud65c\uc6a9\ud558\uc5ec \ucffc\ub9ac\ub97c \ub3d9\uc801\uc73c\ub85c \ub77c\uc6b0\ud305\ud55c\ub2e4.", "result": "DynaSwarm\uc740 \uc9c8\ubb38 \ub2f5\ubcc0, \uc218\ud559\uc801 \ucd94\ub860 \ubc0f \ucf54\ub529 \uc791\uc5c5\uc5d0\uc11c \uc5ec\ub7ec LLM \ubc31\ubcf8\uc5d0 \uac78\uccd0 \uae30\uc874\uc758 \ub2e8\uc77c \uc5d0\uc774\uc804\ud2b8 \ubc0f MAS \uae30\uc900\uc120\uc744 \ucd08\uacfc\ud558\ub294 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0b8\ub2e4.", "conclusion": "DynaSwarm\uc758 \uc5f0\uad6c \uacb0\uacfc\ub294 LLM \uae30\ubc18 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c \uc124\uacc4\uc5d0\uc11c \uc0d8\ud50c \uc778\uc2dd \uad6c\uc870 \uc720\uc5f0\uc131\uc758 \uc911\uc694\uc131\uc744 \uac15\uc870\ud55c\ub2e4."}}
{"id": "2507.23291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23291", "abs": "https://arxiv.org/abs/2507.23291", "authors": ["Yuetian Chen", "Zhiqi Wang", "Nathalie Baracaldo", "Swanand Ravindra Kadhe", "Lei Yu"], "title": "Evaluating the Dynamics of Membership Privacy in Deep Learning", "comment": null, "summary": "Membership inference attacks (MIAs) pose a critical threat to the privacy of\ntraining data in deep learning. Despite significant progress in attack\nmethodologies, our understanding of when and how models encode membership\ninformation during training remains limited. This paper presents a dynamic\nanalytical framework for dissecting and quantifying privacy leakage dynamics at\nthe individual sample level. By tracking per-sample vulnerabilities on an\nFPR-TPR plane throughout training, our framework systematically measures how\nfactors such as dataset complexity, model architecture, and optimizer choice\ninfluence the rate and severity at which samples become vulnerable. Crucially,\nwe discover a robust correlation between a sample's intrinsic learning\ndifficulty, and find that the privacy risk of samples highly vulnerable in the\nfinal trained model is largely determined early during training. Our results\nthus provide a deeper understanding of how privacy risks dynamically emerge\nduring training, laying the groundwork for proactive, privacy-aware model\ntraining strategies.", "AI": {"tldr": "\ubcf8 \ub17c\ubb38\uc740 \uc2ec\uce35 \ud559\uc2b5\uc5d0\uc11c \ud68c\uc6d0 \ucd94\ub860 \uacf5\uaca9(MIA)\uc5d0 \ub300\ud55c \ub3d9\uc801 \ubd84\uc11d \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc2dc\ud558\uba70, \uc0d8\ud50c \uc218\uc900\uc5d0\uc11c\uc758 \uac1c\uc778 \uc815\ubcf4 \uc720\ucd9c \uc5ed\ud559\uc744 \uc815\ub7c9\ud654\ud55c\ub2e4.", "motivation": "\ud68c\uc6d0 \ucd94\ub860 \uacf5\uaca9\uc774 \uc2ec\uce35 \ud559\uc2b5\uc758 \ud6c8\ub828 \ub370\uc774\ud130 \uac1c\uc778 \uc815\ubcf4\uc5d0 \uc2ec\uac01\ud55c \uc704\ud611\uc744 \ubcf8\ub2e4.", "method": "\uac01 \uc0d8\ud50c\uc758 \ucde8\uc57d\uc131\uc744 \ud6c8\ub828 \ub3d9\uc548 FPR-TPR \ud3c9\uba74\uc5d0\uc11c \ucd94\uc801\ud558\uace0, \ub370\uc774\ud130\uc14b \ubcf5\uc7a1\uc131, \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800 \uc120\ud0dd \ub4f1\uc774 \uc0d8\ud50c\uc758 \ucde8\uc57d\uc131\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uce21\uc815\ud55c\ub2e4.", "result": "\uc0d8\ud50c\uc758 \ub0b4\uc7ac\uc801 \ud559\uc2b5 \ub09c\uc774\ub3c4\uc640 \ucd5c\uc885 \ud6c8\ub828 \ubaa8\ub378\uc5d0\uc11c \ub192\uc740 \ucde8\uc57d\uc131\uc744 \uac00\uc9c4 \uc0d8\ud50c\uc758 \uac1c\uc778 \uc815\ubcf4 \uc704\ud5d8 \uac04\uc758 \uac15\ud55c \uc0c1\uad00\uad00\uacc4\ub97c \ubc1c\uacac\ud558\uc600\ub2e4.", "conclusion": "\uac1c\uc778 \uc815\ubcf4 \uc704\ud5d8\uc774 \ud6c8\ub828 \uc911 \uc5b4\ub5bb\uac8c \ub3d9\uc801\uc73c\ub85c \ubc1c\uc0dd\ud558\ub294\uc9c0\uc5d0 \ub300\ud55c \uae4a\uc740 \uc774\ud574\ub97c \uc81c\uacf5\ud558\uba70, \uac1c\uc778 \uc815\ubcf4 \ubcf4\ud638\ub97c \uace0\ub824\ud55c \ubaa8\ub378 \ud6c8\ub828 \uc804\ub7b5\uc758 \uae30\ubc18\uc744 \ub2e4\uc9c4\ub2e4."}}
{"id": "2507.23292", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23292", "abs": "https://arxiv.org/abs/2507.23292", "authors": ["RJ Skerry-Ryan", "Julian Salazar", "Soroosh Mariooryad", "David Kao", "Daisy Stanton", "Eric Battenberg", "Matt Shannon", "Ron J. Weiss", "Robin Scheibler", "Jonas Rothfuss", "Tom Bagby"], "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy", "comment": null, "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.", "AI": {"tldr": "\uc2e0\uacbd\ub9dd \uacc4\uce35 API \ubc0f \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc18c\uac1c\ud558\uba70, \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc744 \uc190\uc27d\uac8c \uc0dd\uc131\ud558\uc5ec \ub808\uc774\uc5b4\ubcc4 \ubc0f \ub2e8\uacc4\ubcc4 \uc2e4\ud589\uc774 \uac00\ub2a5\ud558\ub3c4\ub85d \uc124\uacc4\ub428.", "motivation": "\ubcf5\uc7a1\ud55c \uc2dc\ud000\uc2a4 \ubaa8\ub378\ub9c1\uc744 \uac04\uc18c\ud654\ud558\uace0, \uc77c\ubc18\uc801\uc778 \uc624\ub958\ub97c \uc644\ud654\ud558\uba70, \uc0dd\uc0b0 \uaddc\ubaa8 \ubaa8\ub378 \uad6c\uc131\uc744 \uc27d\uac8c \ud558\ub824\ub294 \ubaa9\uc801.", "method": "\uc0c1\ud0dc\ub97c \uba85\uc2dc\uc801\uc73c\ub85c \ud45c\ud604\ud558\uace0, \uc0c1\ud0dc\ub97c \ubc1c\uc804\uc2dc\ud0a4\ub294 \ub2e8\uacc4 \uba54\uc11c\ub4dc\ub97c \uc815\uc758\ud558\ub294 \uc2dc\ud000\uc2a4 \ub808\uc774\uc5b4 API\ub97c \uc0ac\uc6a9.", "result": "\uc2dc\ud5d8 \uacb0\uacfc, \uc0c1\ud0dc\uac00 \uc5c6\ub294 \ub808\uc774\uc5b4 \ub2e8\uc704 \ud638\ucd9c\uacfc \ub3d9\uc77c\ud55c \uacb0\uacfc\ub97c \uc81c\uacf5\ud558\uba70, \uc774\ub97c \ud1b5\ud574 \uc2a4\ud2b8\ub9ac\ubc0d \ubc0f \ubcd1\ub82c \uc2dc\ud000\uc2a4 \ucc98\ub9ac\uc5d0\uc11c \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 \uc77c\ubc18\uc801\uc778 \ubc84\uadf8\ub97c \uc644\ud654.", "conclusion": "\uc81c\uc548\ub41c \ubc29\ubc95\ub860\uc744 \ud1b5\ud574 \ubcf5\uc7a1\ud55c \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc744 \uc27d\uac8c \ub9cc\ub4e4\uace0 \ub192\uc740 \uc815\ud655\uc131\uc744 \uc720\uc9c0\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23303", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23303", "abs": "https://arxiv.org/abs/2507.23303", "authors": ["Luca Corbucci", "Javier Alejandro Borges Legrottaglie", "Francesco Spinnato", "Anna Monreale", "Riccardo Guidotti"], "title": "An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items", "comment": null, "summary": "Accurately identifying items forgotten during a supermarket visit and\nproviding clear, interpretable explanations for recommending them remains an\nunderexplored problem within the Next Basket Prediction (NBP) domain. Existing\nNBP approaches typically only focus on forecasting future purchases, without\nexplicitly addressing the detection of unintentionally omitted items. This gap\nis partly due to the scarcity of real-world datasets that allow for the\nreliable estimation of forgotten items. Furthermore, most current NBP methods\nrely on black-box models, which lack transparency and limit the ability to\njustify recommendations to end users. In this paper, we formally introduce the\nforgotten item prediction task and propose two novel interpretable-by-design\nalgorithms. These methods are tailored to identify forgotten items while\noffering intuitive, human-understandable explanations. Experiments on a\nreal-world retail dataset show our algorithms outperform state-of-the-art NBP\nbaselines by 10-15% across multiple evaluation metrics.", "AI": {"tldr": "\uc774 \uc5f0\uad6c\ub294 \uc288\ud37c\ub9c8\ucf13 \ubc29\ubb38 \uc911 \uc78a\ud600\uc9c4 \ud488\ubaa9\uc744 \uc815\ud655\ud788 \uc2dd\ubcc4\ud558\uace0, \uadf8 \uc774\uc720\ub97c \uba85\ud655\ud558\uac8c \uc124\uba85\ud558\ub294 \ubb38\uc81c\ub97c \ub2e4\ub8ec\ub2e4.", "motivation": "\uae30\uc874\uc758 \ub2e4\uc74c \ubc14\uad6c\ub2c8 \uc608\uce21(NBP) \ubc29\ubc95\uc774 \uc758\ub3c4\uce58 \uc54a\uac8c \ube60\uc9c4 \ud488\ubaa9 \uac80\ucd9c\uc744 \ub2e4\ub8e8\uc9c0 \uc54a\uc544 \ubc1c\uc0dd\ud558\ub294 \uaca9\ucc28\uc640 \ub370\uc774\ud130 \ubd80\uc871 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uace0\uc790 \ud55c\ub2e4.", "method": "\uc78a\ud600\uc9c4 \ud488\ubaa9 \uc608\uce21 \uc791\uc5c5\uc744 \uacf5\uc2dd\uc801\uc73c\ub85c \ub3c4\uc785\ud558\uace0, \uc778\uac04\uc774 \uc774\ud574\ud560 \uc218 \uc788\ub294 \uc124\uba85\uc744 \uc81c\uacf5\ud558\ub294 \ub450 \uac00\uc9c0 \uc0c8\ub85c\uc6b4 \uc54c\uace0\ub9ac\uc998\uc744 \uc81c\uc548\ud55c\ub2e4.", "result": "\uc2e4\uc81c \uc18c\ub9e4 \ub370\uc774\ud130\uc14b\uc744 \ud1b5\ud574 \uc54c\uace0\ub9ac\uc998\uc774 \ucd5c\uc2e0 NBP \uae30\uc900\uc120\uc744 10-15% \ucd08\uacfc\ud558\ub294 \uc131\uacfc\ub97c \ubcf4\uc600\ub2e4.", "conclusion": "\uc81c\uc548\ud55c \uc54c\uace0\ub9ac\uc998\uc740 \uc78a\ud600\uc9c4 \ud488\ubaa9\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uc2dd\ubcc4\ud558\uace0 \uadf8\uc5d0 \ub300\ud55c \uc774\ud574 \uac00\ub2a5\ud55c \uc124\uba85\uc744 \uc81c\uacf5\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23317", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23317", "abs": "https://arxiv.org/abs/2507.23317", "authors": ["Tao He", "Rongchuan Mu", "Lizi Liao", "Yixin Cao", "Ming Liu", "Bing Qin"], "title": "Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner", "comment": "33 pages, 3 figures, 19 tables", "summary": "Large reasoning models (LRMs) have recently shown promise in solving complex\nmath problems when optimized with Reinforcement Learning (RL). But conventional\napproaches rely on outcome-only rewards that provide sparse feedback, resulting\nin inefficient optimization process. In this work, we investigate the function\nof process reward models (PRMs) to accelerate the RL training for LRMs. We\npropose a novel intrinsic signal-driven generative process evaluation mechanism\noperating at the thought level to address major bottlenecks in RL-based\ntraining. Specifically, instead of requiring PRMs to know how to solve\nproblems, our method uses intrinsic signals in solutions to judge stepwise\ncorrectness and aggregate contiguous correct/incorrect steps into coherent\n'thought' units. This structured, thought-level rewards enable more reliable\ncredit assignment by reducing ambiguity in step segmentation and alleviating\nreward hacking. We further introduce a capability-adaptive reward mechanism\nthat dynamically balances exploration and exploitation based on the LRM's\ncurrent proficiency, guiding learning without stifling creative\ntrial-and-error. These innovations are integrated into a new off-policy RL\nalgorithm, TP-GRPO, which extends grouped proximal optimization with\nprocess-based rewards and improves training efficiency. Experiments on 1.5B and\n7B parameter LRMs demonstrate that our method achieves higher problem-solving\naccuracy with significantly fewer training samples than outcome-only reward\nbaselines. The results validate that well-structured process rewards can\nsubstantially accelerate LRM optimization in math reasoning tasks. Code is\navailable at https://github.com/cs-holder/tp_grpo.", "AI": {"tldr": "\ubcf8 \uc5f0\uad6c\ub294 \ud504\ub85c\uc138\uc2a4 \ubcf4\uc0c1 \ubaa8\ub378(PRMs)\uc744 \ud65c\uc6a9\ud558\uc5ec \uac15\ud654 \ud559\uc2b5(RL) \ud6c8\ub828\uc744 \uac00\uc18d\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4.", "motivation": "\uc804\ud1b5\uc801\uc778 \uacb0\uacfc \uae30\ubc18 \ubcf4\uc0c1\uc740 \ud76c\ubc15\ud55c \ud53c\ub4dc\ubc31\uc744 \uc81c\uacf5\ud558\uc5ec \ucd5c\uc801\ud654 \uacfc\uc815\uc774 \ube44\ud6a8\uc728\uc801\uc774\ub2e4.", "method": "\ub0b4\ubd80 \uc2e0\ud638\uc5d0 \uae30\ubc18\ud55c \uc0dd\uc131\uc801 \ud504\ub85c\uc138\uc2a4 \ud3c9\uac00 \uba54\ucee4\ub2c8\uc998\uacfc \uc5ed\ub3d9\uc801\uc778 \ubcf4\uc0c1 \uba54\ucee4\ub2c8\uc998\uc744 \ub3c4\uc785\ud558\uc5ec \ub2e8\uacc4\ubcc4 \uc62c\ubc14\ub984\uc744 \ud3c9\uac00\ud558\uace0 RL \ud6c8\ub828\uc744 \uc9c0\uc6d0\ud55c\ub2e4.", "result": "1.5B \ubc0f 7B \ud30c\ub77c\ubbf8\ud130 LRM\uc5d0\uc11c \uc2e4\ud5d8\ud55c \uacb0\uacfc, \uc81c\uc548\ub41c \ubc29\ubc95\uc774 \ubb38\uc81c \ud574\uacb0 \uc815\ud655\ub3c4\ub97c \ub192\uc774\uba74\uc11c\ub3c4 \ub354 \uc801\uc740 \ud6c8\ub828 \uc0d8\ud50c\ub85c \uc131\ub2a5\uc744 \ubc1c\ud718\ud55c\ub2e4.", "conclusion": "\uc798 \uad6c\uc870\ud654\ub41c \ud504\ub85c\uc138\uc2a4 \ubcf4\uc0c1\uc774 \uc218\ud559 \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c LRM \ucd5c\uc801\ud654\ub97c \ud06c\uac8c \uac00\uc18d\ud560 \uc218 \uc788\uc74c\uc744 \ud655\uc778\ud588\ub2e4."}}
{"id": "2507.23335", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23335", "abs": "https://arxiv.org/abs/2507.23335", "authors": ["Qilin Zhou", "Haipeng Wang", "Zhengyuan Wei", "W. K. Chan"], "title": "Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions", "comment": "accepted by QRS 2025", "summary": "Patch robustness certification is an emerging verification approach for\ndefending against adversarial patch attacks with provable guarantees for deep\nlearning systems. Certified recovery techniques guarantee the prediction of the\nsole true label of a certified sample. However, existing techniques, if\napplicable to top-k predictions, commonly conduct pairwise comparisons on those\nvotes between labels, failing to certify the sole true label within the top k\nprediction labels precisely due to the inflation on the number of votes\ncontrolled by the attacker (i.e., attack budget); yet enumerating all\ncombinations of vote allocation suffers from the combinatorial explosion\nproblem. We propose CostCert, a novel, scalable, and precise voting-based\ncertified recovery defender. CostCert verifies the true label of a sample\nwithin the top k predictions without pairwise comparisons and combinatorial\nexplosion through a novel design: whether the attack budget on the sample is\ninfeasible to cover the smallest total additional votes on top of the votes\nuncontrollable by the attacker to exclude the true labels from the top k\nprediction labels. Experiments show that CostCert significantly outperforms the\ncurrent state-of-the-art defender PatchGuard, such as retaining up to 57.3% in\ncertified accuracy when the patch size is 96, whereas PatchGuard has already\ndropped to zero.", "AI": {"tldr": "CostCert\ub294 \uacf5\uaca9 \uc608\uc0b0\uc744 \uace0\ub824\ud558\uc5ec \uc0c1\uc704 k\uac1c\uc758 \uc608\uce21 \ub0b4\uc5d0\uc11c \uc9c4\uc9dc \ub808\uc774\ube14\uc744 \uac80\uc99d\ud558\ub294 \uc0c8\ub85c\uc6b4 \ubc29\ubc95\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4.", "motivation": "\ub525\ub7ec\ub2dd \uc2dc\uc2a4\ud15c\uc5d0 \ub300\ud55c \uc801\ub300\uc801 \ud328\uce58 \uacf5\uaca9\uc5d0 \ub300\ud55c \ubc29\uc5b4\ub97c \uc704\ud55c \uac80\uc99d \uc811\uadfc\ubc95\uc758 \ud544\uc694\uc131.", "method": "\ube44\uad50\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0, \uacf5\uaca9\uc790\uac00 \ud1b5\uc81c\ud560 \uc218 \uc5c6\ub294 \ud22c\ud45c\uc640 \ucd5c\uc18c \ucd94\uac00 \ud22c\ud45c\uc758 \ucd1d\ud569\uc744 \ube44\uad50\ud558\uc5ec \uc9c4\uc9dc \ub808\uc774\ube14\uc744 \uac80\uc99d\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc124\uacc4.", "result": "\uc2e4\ud5d8 \uacb0\uacfc, CostCert\ub294 PatchGuard\uc5d0 \ube44\ud574 \ucd5c\ub300 57.3%\uc758 \uc778\uc99d \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uc5ec \uc131\ub2a5\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub428.", "conclusion": "CostCert\ub294 \uae30\uc874\uc758 \uae30\ubc95\ub4e4\uc744 \ub2a5\uac00\ud558\ub294 \ud601\uc2e0\uc801\uc778 \ubc29\uc5b4 \uba54\ucee4\ub2c8\uc998\uc73c\ub85c, \uacf5\uaca9 \uc608\uc0b0\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ucc98\ub9ac\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23344", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23344", "abs": "https://arxiv.org/abs/2507.23344", "authors": ["Tatsuya Mitomi", "Fumiyasu Makinoshima", "Fumiya Makihara", "Eigo Segawa"], "title": "Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation", "comment": null, "summary": "Bike-sharing systems are emerging in various cities as a new ecofriendly\ntransportation system. In these systems, spatiotemporally varying user demands\nlead to imbalanced inventory at bicycle stations, resulting in additional\nrelocation costs. Therefore, it is essential to manage user demand through\noptimal dynamic pricing for the system. However, optimal pricing design for\nsuch a system is challenging because the system involves users with diverse\nbackgrounds and their probabilistic choices. To address this problem, we\ndevelop a differentiable agent-based simulation to rapidly design dynamic\npricing in bike-sharing systems, achieving balanced bicycle inventory despite\nspatiotemporally heterogeneous trips and probabilistic user decisions. We first\nvalidate our approach against conventional methods through numerical\nexperiments involving 25 bicycle stations and five time slots, yielding 100\nparameters. Compared to the conventional methods, our approach obtains a more\naccurate solution with a 73% to 78% reduction in loss while achieving more than\na 100-fold increase in convergence speed. We further validate our approach on a\nlarge-scale urban bike-sharing system scenario involving 289 bicycle stations,\nresulting in a total of 1156 parameters. Through simulations using the obtained\npricing policies, we confirm that these policies can naturally induce balanced\ninventory without any manual relocation. Additionally, we find that the cost of\ndiscounts to induce the balanced inventory can be minimized by setting\nappropriate initial conditions.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc790\uc804\uac70 \uacf5\uc720 \uc2dc\uc2a4\ud15c\uc758 \ub3d9\uc801 \uac00\uaca9 \ucc45\uc815\uc744 \ucd5c\uc801\ud654\ud558\uc5ec \uc790\uc804\uac70 \uc7ac\uace0\uc758 \ubd88\uade0\ud615 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uace0, \ub3d9\uc2dc\uc5d0 \uc0ac\uc6a9\uc790 \uc218\uc694\ub97c \uad00\ub9ac\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4.", "motivation": "\ub3c4\uc2dc\uc758 \uc790\uc804\uac70 \uacf5\uc720 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc218\uc694\uc758 \ubcc0\ud654\ub85c \ubc1c\uc0dd\ud558\ub294 \uc790\uc804\uac70 \uc7ac\uace0 \ubd88\uade0\ud615 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574.", "method": "\ucc28\ubcc4\ud654\ub41c \uc5d0\uc774\uc804\ud2b8 \uae30\ubc18 \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uac1c\ubc1c\ud558\uc5ec \uc790\uc804\uac70 \uacf5\uc720 \uc2dc\uc2a4\ud15c\uc758 \ub3d9\uc801 \uac00\uaca9 \ucc45\uc815\uc744 \uc124\uacc4\ud568.", "result": "\uc804\ud1b5\uc801\uc778 \ubc29\ubc95 \ub300\ube44 73%\uc5d0\uc11c 78%\uc758 \uc190\uc2e4 \uac10\uc18c\uc640 \ub3d9\uc2dc\uc5d0 100\ubc30 \uc774\uc0c1\uc758 \uc218\ub834 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud558\uc600\ub2e4.", "conclusion": "\uc774 \uac00\uaca9 \uc815\ucc45\uc774 \uc790\uc804\uac70 \uc7ac\uace0\uc758 \uade0\ud615\uc744 \uc790\uc5f0\uc2a4\ub7fd\uac8c \uc720\ub3c4\ud560 \uc218 \uc788\uc73c\uba70, \uc801\uc808\ud55c \ucd08\uae30 \uc870\uac74 \uc124\uc815\uc744 \ud1b5\ud574 \ud560\uc778 \ube44\uc6a9\uc744 \ucd5c\uc18c\ud654\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23535", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.23535", "abs": "https://arxiv.org/abs/2507.23535", "authors": ["Dhanesh Ramachandram", "Himanshu Joshi", "Judy Zhu", "Dhari Gandhi", "Lucas Hartman", "Ananya Raval"], "title": "Transparent AI: The Case for Interpretability and Explainability", "comment": null, "summary": "As artificial intelligence systems increasingly inform high-stakes decisions\nacross sectors, transparency has become foundational to responsible and\ntrustworthy AI implementation. Leveraging our role as a leading institute in\nadvancing AI research and enabling industry adoption, we present key insights\nand lessons learned from practical interpretability applications across diverse\ndomains. This paper offers actionable strategies and implementation guidance\ntailored to organizations at varying stages of AI maturity, emphasizing the\nintegration of interpretability as a core design principle rather than a\nretrospective add-on.", "AI": {"tldr": "\uc778\uacf5\uc9c0\ub2a5 \uc2dc\uc2a4\ud15c\uc758 \ud22c\uba85\uc131\uc774 \ucc45\uc784 \uc788\uace0 \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \uad6c\ud604\uc744 \uc704\ud574 \uc911\uc694\ud574\uc9c0\uace0 \uc788\ub2e4.", "motivation": "\uace0\uc704\ud5d8 \uacb0\uc815\uc5d0 \ub300\ud55c \uc778\uacf5\uc9c0\ub2a5 \uc2dc\uc2a4\ud15c\uc758 \uc601\ud5a5\ub825\uc774 \ucee4\uc9d0\uc5d0 \ub530\ub77c \ud22c\uba85\uc131\uc774 \ud544\uc218\uc801\uc774\ub2e4.", "method": "AI \ud574\uc11d \uac00\ub2a5\uc131\uc758 \uc2e4\uc81c \uc751\uc6a9\uc5d0\uc11c \uc5bb\uc740 \ud1b5\ucc30\ub825\uacfc \uad50\ud6c8\uc744 \uc81c\uacf5\ud558\uba70, \ub2e4\uc591\ud55c \ub3c4\uba54\uc778\uc5d0\uc11c \uc801\uc6a9\ub41c \uc0ac\ub840\ub97c \ub2e4\ub8ec\ub2e4.", "result": "AI \uc131\uc219\ub3c4\uc758 \ub2e4\uc591\ud55c \ub2e8\uacc4\uc5d0 \uc788\ub294 \uc870\uc9c1\uc744 \uc704\ud55c \uc2e4\ud589 \uac00\ub2a5\ud55c \uc804\ub7b5\uacfc \uad6c\ud604 \uac00\uc774\ub4dc\ub97c \uc81c\uc2dc\ud55c\ub2e4.", "conclusion": "\ud574\uc11d \uac00\ub2a5\uc131\uc744 \ud575\uc2ec \uc124\uacc4 \uc6d0\uce59\uc73c\ub85c \ud1b5\ud569\ud558\ub294 \uac83\uc774 \ud544\uc694\ud558\ub2e4."}}
{"id": "2507.23389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23389", "abs": "https://arxiv.org/abs/2507.23389", "authors": ["David Komnick", "Kathrin Lammers", "Barbara Hammer", "Valerie Vaquet", "Fabian Hinder"], "title": "Causal Explanation of Concept Drift -- A Truly Actionable Approach", "comment": "This manuscript is accepted to be presented at the TempXAI workshop\n  at the European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (ECMLPKDD 2025)", "summary": "In a world that constantly changes, it is crucial to understand how those\nchanges impact different systems, such as industrial manufacturing or critical\ninfrastructure. Explaining critical changes, referred to as concept drift in\nthe field of machine learning, is the first step towards enabling targeted\ninterventions to avoid or correct model failures, as well as malfunctions and\nerrors in the physical world. Therefore, in this work, we extend model-based\ndrift explanations towards causal explanations, which increases the\nactionability of the provided explanations. We evaluate our explanation\nstrategy on a number of use cases, demonstrating the practical usefulness of\nour framework, which isolates the causally relevant features impacted by\nconcept drift and, thus, allows for targeted intervention.", "AI": {"tldr": "\ubaa8\ub378 \uae30\ubc18 \ub4dc\ub9ac\ud504\ud2b8 \uc124\uba85\uc744 \uc778\uacfc\uc801 \uc124\uba85\uc73c\ub85c \ud655\uc7a5\ud558\uc5ec \uac1c\ub150 \ub4dc\ub9ac\ud504\ud2b8\uac00 \uc2dc\uc2a4\ud15c\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uc774\ud574\ud558\uace0, \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378 \uc2e4\ud328\ub97c \ubc29\uc9c0\ud558\uac70\ub098 \uc218\uc815\ud558\uae30 \uc704\ud55c \uac1c\uc785\uc744 \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4.", "motivation": "\uc0b0\uc5c5 \uc81c\uc870 \ubc0f \uc911\uc694\ud55c \uc778\ud504\ub77c\uc640 \uac19\uc740 \uc2dc\uc2a4\ud15c\uc5d0 \ub300\ud55c \ubcc0\ud654\uc758 \uc601\ud5a5\uc744 \uc774\ud574\ud558\ub294 \uac83\uc774 \uc911\uc694\ud558\ub2e4.", "method": "\ubaa8\ub378 \uae30\ubc18 \ub4dc\ub9ac\ud504\ud2b8 \uc124\uba85\uc744 \uc778\uacfc\uc801 \uc124\uba85\uc73c\ub85c \ud655\uc7a5\ud558\ub294 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\uc600\ub2e4.", "result": "\uc5ec\ub7ec \uc0ac\uc6a9 \uc0ac\ub840\uc5d0 \ub300\ud55c \ud3c9\uac00\ub97c \ud1b5\ud574 \uc81c\uacf5\ub41c \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uc2e4\uc81c \uc720\uc6a9\uc131\uc744 \uc785\uc99d\ud558\uc600\ub2e4.", "conclusion": "\uc778\uacfc\uc801\uc73c\ub85c \uad00\ub828\ub41c \ud2b9\uc9d5\uc744 \ubd84\ub9ac\ud558\uc5ec \ubcf4\ub2e4 \ud45c\uc801\ud654\ub41c \uac1c\uc785\uc744 \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4."}}
{"id": "2507.23536", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23536", "abs": "https://arxiv.org/abs/2507.23536", "authors": ["Georg Slamanig", "Francesco Corti", "Olga Saukh"], "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \uc790\uc6d0 \uc81c\uc57d\uc774 \uc788\ub294 \uc5e3\uc9c0 \ub514\ubc14\uc774\uc2a4\uc5d0\uc11c\uc758 \ud558\ubd80 \uc791\uc5c5\uc5d0 \uc801\ud569\ud558\ub3c4\ub85d \uc2ec\uce35 \ud559\uc2b5 \ubaa8\ub378\uc758 \uc5c5\ub370\uc774\ud2b8 \ube44\uc6a9\uc744 \uc904\uc774\ub294 \ub9e4\uac1c\ubcc0\uc218 \ud6a8\uc728\uc801 \ubbf8\uc138 \uc870\uc815(PEFT) \ubc29\ubc95\uc744 \ubd84\uc11d\ud558\uace0 \ubca4\uce58\ub9c8\ud0b9\ud569\ub2c8\ub2e4.", "motivation": "\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc5f0\uad6c\ub294 \uad11\ubc94\uc704\ud558\uc9c0\ub9cc, \uc790\uc6d0 \uc81c\uc57d\uc774 \uc788\ub294 \uc5e3\uc9c0 \ub514\ubc14\uc774\uc2a4\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uc791\uc740 \ubaa8\ub378\uc5d0 \ub300\ud55c PEFT \ubc29\ubc95\uc758 \uc801\uc6a9\uc740 \uc544\uc9c1 \ucda9\ubd84\ud788 \ud0d0\uad6c\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.", "method": "LoRA, DoRA \ubc0f GaLore\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud45c\uc900 \ubc0f \uae4a\uc774\ubcc4 \ucee8\ubcfc\ub8e8\uc158 \uc544\ud0a4\ud14d\ucc98\ub97c \ud3c9\uac00\ud558\uba70, PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \ud65c\uc6a9\ud558\uc5ec PEFT \ubc29\ubc95\uacfc \uc804\ud1b5\uc801\uc778 \ubbf8\uc138 \uc870\uc815 \ubc29\ubc95\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.", "result": "PEFT \ubc29\ubc95\uc774 \uae4a\uc774\ubcc4 \ubd84\ub9ac \ucee8\ubcfc\ub8e8\uc158 \uc544\ud0a4\ud14d\ucc98\uc5d0 \uc801\uc6a9\ub420 \ub54c \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uc774 LLM\uc5d0 \ube44\ud574 \uc808\ubc18\uc5d0 \ubd88\uacfc\ud558\ub098, \uc5e3\uc9c0 \ubc30\uce58\ub97c \uc704\ud574 \ucd5c\uc801\ud654\ub41c \ucee8\ubcfc\ub8e8\uc158 \uc544\ud0a4\ud14d\ucc98\uc5d0 \ub300\ud55c \uc5b4\ub311\ud130 \uae30\ubc18 PEFT \ubc29\ubc95\uc740 \ubaa8\ub378 \uc5c5\ub370\uc774\ud2b8 \uc911 FLOPs\ub97c \ucd5c\ub300 95% \uc904\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "conclusion": "\ud558\ub4dc\uc6e8\uc5b4 \uc81c\ud55c, \uc131\ub2a5 \uc694\uad6c\uc0ac\ud56d \ubc0f \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8 \ud544\uc694\uc5d0 \ub530\ub77c PEFT \ubc29\ubc95\uc744 \uc120\ud0dd\ud558\ub294 \ub370 \uc720\uc6a9\ud55c \ud1b5\ucc30\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4."}}
{"id": "2507.23391", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23391", "abs": "https://arxiv.org/abs/2507.23391", "authors": ["Tung M. Luu", "Donghoon Lee", "Younghwan Lee", "Chang D. Yoo"], "title": "Policy Learning from Large Vision-Language Model Feedback without Reward Modeling", "comment": "Accepted to IROS 2025", "summary": "Offline reinforcement learning (RL) provides a powerful framework for\ntraining robotic agents using pre-collected, suboptimal datasets, eliminating\nthe need for costly, time-consuming, and potentially hazardous online\ninteractions. This is particularly useful in safety-critical real-world\napplications, where online data collection is expensive and impractical.\nHowever, existing offline RL algorithms typically require reward labeled data,\nwhich introduces an additional bottleneck: reward function design is itself\ncostly, labor-intensive, and requires significant domain expertise. In this\npaper, we introduce PLARE, a novel approach that leverages large\nvision-language models (VLMs) to provide guidance signals for agent training.\nInstead of relying on manually designed reward functions, PLARE queries a VLM\nfor preference labels on pairs of visual trajectory segments based on a\nlanguage task description. The policy is then trained directly from these\npreference labels using a supervised contrastive preference learning objective,\nbypassing the need to learn explicit reward models. Through extensive\nexperiments on robotic manipulation tasks from the MetaWorld, PLARE achieves\nperformance on par with or surpassing existing state-of-the-art VLM-based\nreward generation methods. Furthermore, we demonstrate the effectiveness of\nPLARE in real-world manipulation tasks with a physical robot, further\nvalidating its practical applicability.", "AI": {"tldr": "PLARE\ub294 \uac15\ub825\ud55c \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\uc5ec \uc0ac\uc804 \uc218\uc9d1\ub41c \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud574 \ub85c\ubd07 \uc81c\uc5b4\ub97c \uc704\ud55c \uc0c8\ub85c\uc6b4 \uac15\ud654 \ud559\uc2b5 \uc811\uadfc\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "motivation": "\uc628\ub77c\uc778 \ub370\uc774\ud130 \uc218\uc9d1\uc774 \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4e4\uace0 \uc2e4\uc6a9\uc801\uc774\uc9c0 \uc54a\uc740 \uc548\uc804 \ube44\ud310\uc801\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \uae30\uc874\uc758 \uac15\ud654 \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\uae30 \uc704\ud574.", "method": "PLARE\ub294 \ube44\uc804-\uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uac8c \uc5b8\uc5b4 \uacfc\uc5c5 \uc124\uba85\uc5d0 \uae30\ubc18\ud558\uc5ec \uc2dc\uac01\uc801 \uacbd\ub85c \uc30d\uc5d0 \ub300\ud55c \uc120\ud638 \ub808\uc774\ube14\uc744 \ucffc\ub9ac\ud558\uc5ec \ubcf4\uc0c1 \ud568\uc218\ub97c \uc218\ub3d9\uc73c\ub85c \uc124\uacc4\ud560 \ud544\uc694 \uc5c6\uc774 \uc815\ucc45\uc744 \ud6c8\ub828\uc2dc\ud0a8\ub2e4.", "result": "PLARE\ub294 MetaWorld\uc758 \ub85c\ubd07 \uc870\uc791 \uc791\uc5c5\uc5d0\uc11c \uae30\uc874 \ucd5c\uace0\uc758 VLM \uae30\ubc18 \ubcf4\uc0c1 \uc0dd\uc131 \ubc29\ubc95\ub4e4\uacfc \ub3d9\ub4f1\ud558\uac70\ub098 \uadf8 \uc774\uc0c1 \uc131\uacfc\ub97c \ub2ec\uc131\ud558\uc600\ub2e4.", "conclusion": "PLARE\ub294 \uc2e4\uc81c \ub85c\ubd07\uc744 \ud65c\uc6a9\ud55c \uc870\uc791 \uc791\uc5c5\uc5d0\uc11c\ub3c4 \ud6a8\uacfc\uc131\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc73c\uba70, \uc774\ub294 \uc2e4\uc81c \uc801\uc6a9 \uac00\ub2a5\uc131\uc744 \ub4b7\ubc1b\uce68\ud55c\ub2e4."}}
{"id": "2507.23607", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23607", "abs": "https://arxiv.org/abs/2507.23607", "authors": ["Tien Huu Do", "Antoine Masquelier", "Nae Eoun Lee", "Jonathan Crowther"], "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates", "comment": null, "summary": "Clinical trials are a systematic endeavor to assess the safety and efficacy\nof new drugs or treatments. Conducting such trials typically demands\nsignificant financial investment and meticulous planning, highlighting the need\nfor accurate predictions of trial outcomes. Accurately predicting patient\nenrollment, a key factor in trial success, is one of the primary challenges\nduring the planning phase. In this work, we propose a novel deep learning-based\nmethod to address this critical challenge. Our method, implemented as a neural\nnetwork model, leverages pre-trained language models (PLMs) to capture the\ncomplexities and nuances of clinical documents, transforming them into\nexpressive representations. These representations are then combined with\nencoded tabular features via an attention mechanism. To account for\nuncertainties in enrollment prediction, we enhance the model with a\nprobabilistic layer based on the Gamma distribution, which enables range\nestimation. We apply the proposed model to predict clinical trial duration,\nassuming site-level enrollment follows a Poisson-Gamma process. We carry out\nextensive experiments on real-world clinical trial data, and show that the\nproposed method can effectively predict the number of patients enrolled at a\nnumber of sites for a given clinical trial, outperforming established baseline\nmodels.", "AI": {"tldr": "\uc0c8\ub85c\uc6b4 \uc2ec\uce35 \ud559\uc2b5 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc784\uc0c1 \uc2dc\ud5d8\uc5d0\uc11c \ud658\uc790 \ub4f1\ub85d\uc744 \uc815\ud655\ud558\uac8c \uc608\uce21\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc8fc\ub294 \ub17c\ubb38.", "motivation": "\uc2e0\uc57d \ubc0f \uce58\ub8cc\ubc95\uc758 \uc548\uc804\uc131\uacfc \ud6a8\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \uc784\uc0c1 \uc2dc\ud5d8\uc740 \uc0c1\ub2f9\ud55c \uc7ac\uc815\uc801 \ud22c\uc790\uc640 \ucca0\uc800\ud55c \uacc4\ud68d\uc774 \ud544\uc694\ud558\uba70, \uc131\uacf5\uc801\uc778 \uc2dc\ud5d8\uc744 \uc704\ud574 \ud658\uc790 \ub4f1\ub85d \uc608\uce21\uc758 \uc815\ud655\uc131\uc774 \ud544\uc218\uc801\uc774\ub2e4.", "method": "\uc0ac\uc804 \ud6c8\ub828\ub41c \uc5b8\uc5b4 \ubaa8\ub378(PLM)\uc744 \ud65c\uc6a9\ud558\uc5ec \uc784\uc0c1 \ubb38\uc11c\uc758 \ubcf5\uc7a1\uc131\uc744 \ud3ec\ucc29\ud558\uace0, \uc774\ub97c \ud45c\ud604\uc801\uc778 \ubca1\ud130\ub85c \ubcc0\ud658\ud55c \ud6c4, \uc8fc\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud574 \uc778\ucf54\ub529\ub41c \ud45c \ud615\uc2dd \ud2b9\uc131\uacfc \uacb0\ud569\ud558\ub294 \uc2e0\uacbd\ub9dd \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c\ub2e4. Gamma \ubd84\ud3ec \uae30\ubc18\uc758 \ud655\ub960\uc801 \ub808\uc774\uc5b4\ub97c \ucd94\uac00\ud558\uc5ec \ubd88\ud655\uc2e4\uc131\uc744 \uace0\ub824\ud55c\ub2e4.", "result": "\uc81c\uc548\ub41c \ubc29\ubc95\uc740 \uc2e4\uc81c \uc784\uc0c1 \uc2dc\ud5d8 \ub370\uc774\ud130\ub97c \ubc14\ud0d5\uc73c\ub85c \uc5ec\ub7ec \ud604\uc7a5\uc5d0 \ub300\ud55c \ud658\uc790 \ub4f1\ub85d \uc218\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uc608\uce21\ud560 \uc218 \uc788\uc73c\uba70, \uae30\uc874 \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4.", "conclusion": "\uc784\uc0c1 \uc2dc\ud5d8 \ub370\uc774\ud130\uc758 \uc608\uce21\uc5d0 \uc788\uc5b4 \uc81c\uc548\ub41c \uc2ec\uce35 \ud559\uc2b5 \ubc29\ubc95\uc774 \uae30\uc874 \ubaa8\ub378\ubcf4\ub2e4 \ub098\uc740 \uc131\uacfc\ub97c \ubcf4\uc774\uba70, \uc774\ub294 \uc784\uc0c1 \uc2dc\ud5d8\uc758 \uacc4\ud68d \ubc0f \uc131\uacf5\ub960 \ud5a5\uc0c1\uc5d0 \uae30\uc5ec\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23412", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23412", "abs": "https://arxiv.org/abs/2507.23412", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "A Machine Learning Approach for Honey Adulteration Detection using Mineral Element Profiles", "comment": null, "summary": "This paper aims to develop a Machine Learning (ML)-based system for detecting\nhoney adulteration utilizing honey mineral element profiles. The proposed\nsystem comprises two phases: preprocessing and classification. The\npreprocessing phase involves the treatment of missing-value attributes and\nnormalization. In the classifica-tion phase, we use three supervised ML models:\nlogistic regression, decision tree, and random forest, to dis-criminate between\nauthentic and adulterated honey. To evaluate the performance of the ML models,\nwe use a public dataset comprising measurements of mineral element content of\nauthentic honey, sugar syrups, and adul-terated honey. Experimental findings\nshow that mineral element content in honey provides robust discriminative\ninformation for detecting honey adulteration. Results also demonstrate that the\nrandom forest-based classifier outperforms other classifiers on this dataset,\nachieving the highest cross-validation accuracy of 98.37%.", "AI": {"tldr": "\ubcf8 \uc5f0\uad6c\ub294 \ubbf8\ub124\ub784 \uc131\ubd84 \ud504\ub85c\ud30c\uc77c\uc744 \ud65c\uc6a9\ud558\uc5ec \uafc0\uc758 \ud63c\ud569 \uc5ec\ubd80\ub97c \ud0d0\uc9c0\ud558\ub294 \uba38\uc2e0 \ub7ec\ub2dd \uc2dc\uc2a4\ud15c\uc744 \uac1c\ubc1c\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud55c\ub2e4.", "motivation": "\uafc0\uc758 \uc21c\ub3c4 \ud655\uc778 \ubc0f adulteration \ud0d0\uc9c0\uc758 \ud544\uc694\uc131.", "method": "\ub370\uc774\ud130 \uc804\ucc98\ub9ac \ubc0f \uc138 \uac00\uc9c0 \uc9c0\ub3c4\ud559\uc2b5 \ubaa8\ub378(\ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0, \uc758\uc0ac\uacb0\uc815 \ub098\ubb34, \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud63c\ud569 \uafc0\uacfc \uc815\ud488 \uafc0\uc744 \uad6c\ubd84\ud55c\ub2e4.", "result": "\ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8 \ubaa8\ub378\uc774 \uac00\uc7a5 \ub192\uc740 98.37%\uc758 \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4\ub85c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4.", "conclusion": "\ubbf8\ub124\ub784 \uc131\ubd84 \uc815\ubcf4\uac00 \uafc0\uc758 adulteration \ud0d0\uc9c0\uc5d0 \uc720\uc6a9\ud558\uba70, \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8 \ubaa8\ub378\uc774 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubc1c\ud718\ud558\uc600\ub2e4."}}
{"id": "2507.23418", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23418", "abs": "https://arxiv.org/abs/2507.23418", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning", "comment": null, "summary": "In this paper, we propose a system for detecting adulteration in coconut\nmilk, utilizing infrared spectroscopy. The machine learning-based proposed\nsystem comprises three phases: preprocessing, feature extraction, and\nclassification. The first phase involves removing irrelevant data from coconut\nmilk spectral signals. In the second phase, we employ the Linear Discriminant\nAnalysis (LDA) algorithm for extracting the most discriminating features. In\nthe third phase, we use the K-Nearest Neighbor (KNN) model to classify coconut\nmilk samples into authentic or adulterated. We evaluate the performance of the\nproposed system using a public dataset comprising Fourier Transform Infrared\n(FTIR) spectral information of pure and contaminated coconut milk samples.\nFindings show that the proposed method successfully detects adulteration with a\ncross-validation accuracy of 93.33%.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc801\uc678\uc120 \ubd84\uad11\ubc95\uc744 \uc774\uc6a9\ud574 \ucf54\ucf54\ub11b \ubc00\ud06c\uc758 adulteration\uc744 \uac10\uc9c0\ud558\ub294 \uc2dc\uc2a4\ud15c\uc744 \uc81c\uc548\ud55c\ub2e4.", "motivation": "\ucf54\ucf54\ub11b \ubc00\ud06c\uc758 \ud488\uc9c8\uc744 \ubcf4\uc7a5\ud558\uace0 \uc18c\ube44\uc790\ub97c \ubcf4\ud638\ud558\uae30 \uc704\ud574, adulteration\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uac10\uc9c0\ud560 \ud544\uc694\uc131\uc774 \uc788\ub2e4.", "method": "\uc2dc\uc2a4\ud15c\uc740 \ub370\uc774\ud130 \uc804\ucc98\ub9ac, \ud2b9\uc9d5 \ucd94\ucd9c, \ubd84\ub958\uc758 \uc138 \uac00\uc9c0 \ub2e8\uacc4\ub85c \uad6c\uc131\ub41c\ub2e4. LDA \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud574 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\uace0, KNN \ubaa8\ub378\ub85c \uc0d8\ud50c\uc744 \ubd84\ub958\ud55c\ub2e4.", "result": "\uc81c\uc548\ub41c \uc2dc\uc2a4\ud15c\uc740 \uacf5\uacf5 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud574 \ud3c9\uac00\ud55c \uacb0\uacfc, 93.33%\uc758 \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4\ub97c \uae30\ub85d\ud558\uba70 adulteration\uc744 \uc131\uacf5\uc801\uc73c\ub85c \uac10\uc9c0\ud55c\ub2e4.", "conclusion": "\uc774 \uc5f0\uad6c\ub294 \uc801\uc678\uc120 \ubd84\uad11\ubc95\uacfc \uae30\uacc4 \ud559\uc2b5 \uc811\uadfc\ubc95\uc744 \ud1b5\ud574 \ucf54\ucf54\ub11b \ubc00\ud06c adulteration \uac10\uc9c0\uc758 \ub192\uc740 \uc815\ud655\uc131\uc744 \uc785\uc99d\ud558\uc600\ub2e4."}}
{"id": "2507.23615", "categories": ["cs.LG", "cs.AI", "68T01", "I.5.1; G.3; H.2.8; I.2.1"], "pdf": "https://arxiv.org/pdf/2507.23615", "abs": "https://arxiv.org/abs/2507.23615", "authors": ["Luis Roque", "Carlos Soares", "Vitor Cerqueira", "Luis Torgo"], "title": "L-GTA: Latent Generative Modeling for Time Series Augmentation", "comment": null, "summary": "Data augmentation is gaining importance across various aspects of time series\nanalysis, from forecasting to classification and anomaly detection tasks. We\nintroduce the Latent Generative Transformer Augmentation (L-GTA) model, a\ngenerative approach using a transformer-based variational recurrent\nautoencoder. This model uses controlled transformations within the latent space\nof the model to generate new time series that preserve the intrinsic properties\nof the original dataset. L-GTA enables the application of diverse\ntransformations, ranging from simple jittering to magnitude warping, and\ncombining these basic transformations to generate more complex synthetic time\nseries datasets. Our evaluation of several real-world datasets demonstrates the\nability of L-GTA to produce more reliable, consistent, and controllable\naugmented data. This translates into significant improvements in predictive\naccuracy and similarity measures compared to direct transformation methods.", "AI": {"tldr": "L-GTA \ubaa8\ub378\uc740 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc758 \ubcf8\uc9c8\uc801\uc778 \ud2b9\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ubcc0\ud658\ub41c \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud558\ub294 \uc0dd\uc131\uc801 \uc811\uadfc \ubc29\uc2dd\uc774\ub2e4.", "motivation": "\uc2dc\uacc4\uc5f4 \ubd84\uc11d, \uc608\uce21, \ubd84\ub958 \ubc0f \uc774\uc0c1 \ud0d0\uc9c0 \uc791\uc5c5\uc5d0\uc11c \ub370\uc774\ud130 \uc99d\uac15\uc758 \uc911\uc694\uc131\uc774 \uc99d\uac00\ud558\uace0 \uc788\ub2e4.", "method": "\ubcc0\ud615\ub41c \uc7a0\uc7ac \uacf5\uac04 \ub0b4\uc5d0\uc11c controlled transformations\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\ub294 transformer \uae30\ubc18\uc758 \ubcc0\ub7c9 \uc21c\ud658 \uc624\ud1a0\uc778\ucf54\ub354\ub97c \ucc44\ud0dd\ud558\uc600\ub2e4.", "result": "\uc5ec\ub7ec \uc2e4\uc81c \ub370\uc774\ud130 \uc138\ud2b8\ub97c \ud3c9\uac00\ud55c \uacb0\uacfc, L-GTA\ub294 \ub354 \uc2e0\ub8b0\ud560 \uc218 \uc788\uace0 \uc77c\uad00\uc131 \uc788\uc73c\uba70 \uc81c\uc5b4 \uac00\ub2a5\ud55c \uc99d\uac15 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\ub2e4.", "conclusion": "\uc774 \uacb0\uacfc\ub294 \uc9c1\uc811 \ubcc0\ud658 \ubc29\ubc95\uc5d0 \ube44\ud574 \uc608\uce21 \uc815\ud655\ub3c4\uc640 \uc720\uc0ac\uc131 \uce21\uba74\uc5d0\uc11c \uc0c1\ub2f9\ud55c \ud5a5\uc0c1\uc744 \uc774\ub8e9\ud558\uc600\ub2e4."}}
{"id": "2507.23428", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23428", "abs": "https://arxiv.org/abs/2507.23428", "authors": ["Nodens F. Koren", "Samuel Lanthaler"], "title": "Merging Memory and Space: A Spatiotemporal State Space Neural Operator", "comment": null, "summary": "We propose the Spatiotemporal State Space Neural Operator (ST-SSM), a compact\narchitecture for learning solution operators of time-dependent partial\ndifferential equations (PDEs). ST-SSM introduces a novel factorization of the\nspatial and temporal dimensions, using structured state-space models to\nindependently model temporal evolution and spatial interactions. This design\nenables parameter efficiency and flexible modeling of long-range spatiotemporal\ndynamics. A theoretical connection is established between SSMs and neural\noperators, and a unified universality theorem is proved for the resulting class\nof architectures. Empirically, we demonstrate that our factorized formulation\noutperforms alternative schemes such as zigzag scanning and parallel\nindependent processing on several PDE benchmarks, including 1D Burgers'\nequation, 1D Kuramoto-Sivashinsky equation, and 2D Navier-Stokes equations\nunder varying physical conditions. Our model performs competitively with\nexisting baselines while using significantly fewer parameters. In addition, our\nresults reinforce previous findings on the benefits of temporal memory by\nshowing improved performance under partial observability. Our results highlight\nthe advantages of dimensionally factorized operator learning for efficient and\ngeneralizable PDE modeling, and put this approach on a firm theoretical\nfooting.", "AI": {"tldr": "ST-SSM\uc740 \uc2dc\uacf5\uac04 \uc0c1\ud0dc\uacf5\uac04 \uc2e0\uacbd\uc5f0\uc0b0\uc790\ub97c \uc81c\uc548\ud558\uc5ec PDE\uc758 \ud574 \uc5f0\uc0b0\uc790\ub97c \ud559\uc2b5\ud558\ub294 \ud6a8\uc728\uc801\uc778 \uc544\ud0a4\ud14d\ucc98\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "motivation": "\uc2dc\uac04 \uc758\uc874 \ud3b8\ubbf8\ubd84 \ubc29\uc815\uc2dd(PDEs)\uc758 \ud574 \uc5f0\uc0b0\uc790\ub97c \ud559\uc2b5\ud558\uae30 \uc704\ud55c \ud6a8\uc728\uc801\uc778 \ubaa8\ub378\uc774 \ud544\uc694\ud558\ub2e4.", "method": "\uacf5\uac04 \ubc0f \uc2dc\uac04 \ucc28\uc6d0\uc744 \ub3c5\ub9bd\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud558\ub294 \uad6c\uc870\ud654\ub41c \uc0c1\ud0dc \uacf5\uac04 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2dc\uacf5\uac04\uc744 \uc694\uc57d\ud558\ub294 ST-SSM \uc544\ud0a4\ud14d\ucc98\ub97c \uc81c\uc548\ud55c\ub2e4.", "result": "\uc6b0\ub9ac\uc758 \ubaa8\ub378\uc740 \uae30\uc874 \ubc29\ubc95\uc5d0 \ube44\ud574 \uc801\uc740 \ub9e4\uac1c\ubcc0\uc218\ub85c \uc5ec\ub7ec PDE \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4.", "conclusion": "\ucc28\uc6d0 \uc694\uc778\ud654\ub41c \uc5f0\uc0b0\uc790 \ud559\uc2b5\uc758 \uc774\uc810\uacfc PDE \ubaa8\ub378\ub9c1\uc5d0 \ub300\ud55c \ud6a8\uc728\uc801\uc774\uace0 \uc77c\ubc18\ud654 \uac00\ub2a5\ud55c \uc811\uadfc \ubc29\uc2dd\uc744 \uac15\uc870\ud55c\ub2e4."}}
{"id": "2507.23638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23638", "abs": "https://arxiv.org/abs/2507.23638", "authors": ["Mohammad Karami", "Fatemeh Ghassemi", "Hamed Kebriaei", "Hamid Azadegan"], "title": "OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed medical institutions while preserving patient privacy, but remains\nvulnerable to Byzantine attacks and statistical heterogeneity. We present\nOptiGradTrust, a comprehensive defense framework that evaluates gradient\nupdates through a novel six-dimensional fingerprint including VAE\nreconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency\nratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module\nfor adaptive trust scoring. To address convergence challenges under data\nheterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch\nNormalization with proximal regularization for optimal accuracy-convergence\ntrade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI\ndatasets under various Byzantine attack scenarios demonstrates significant\nimprovements over state-of-the-art defenses, achieving up to +1.6 percentage\npoints over FLGuard under non-IID conditions while maintaining robust\nperformance against diverse attack patterns through our adaptive learning\napproach.", "AI": {"tldr": "OptiGradTrust\ub294 \ube44\uc794\ud2f4 \uacf5\uaca9\uacfc \ud1b5\uacc4\uc801 \uc774\uc9c8\uc131 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \uc5f0\ud569 \ud559\uc2b5 \ubc29\uc5b4 \ud504\ub808\uc784\uc6cc\ud06c\uc774\ub2e4.", "motivation": "\ube44\uc794\ud2f4 \uacf5\uaca9\uacfc \ub370\uc774\ud130 \uc774\uc9c8\uc131\uc5d0 \ub300\ud55c \ucde8\uc57d\uc810\uc744 \uc904\uc774\uae30 \uc704\ud55c \ud544\uc694\uc131\uc5d0\uc11c \ucd9c\ubc1c\ud558\uc600\ub2e4.", "method": "6\ucc28\uc6d0 \uc9c0\ubb38\uc744 \ud3ec\ud568\ud55c gradient \uc5c5\ub370\uc774\ud2b8 \ud3c9\uac00\ub97c \ud1b5\ud574 \uc801\uc751\uc801 \uc2e0\ub8b0 \uc810\uc218\ub97c \uc0dd\uc131\ud558\ub294 \ud558\uc774\ube0c\ub9ac\ub4dc RL-\uc8fc\uc758 \ubaa8\ub4c8\uc744 \uac1c\ubc1c\ud558\uace0, \uc5f0\ud569 \ubc30\uce58 \uc815\uaddc\ud654\uc640 \uc811\uadfc \ub808\uade4\ud654 \uae30\ubc95\uc744 \uacb0\ud569\ud55c FedBN-Prox\ub97c \uc0ac\uc6a9\ud558\uc600\ub2e4.", "result": "MNIST, CIFAR-10, \uc54c\uce20\ud558\uc774\uba38 MRI \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub2e4\uc591\ud55c \ube44\uc794\ud2f4 \uacf5\uaca9 \uc2dc\ub098\ub9ac\uc624 \uc544\ub798\uc5d0\uc11c FLGuard\ubcf4\ub2e4 \ucd5c\ub300 1.6% \ud5a5\uc0c1\ub41c \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4.", "conclusion": "OptiGradTrust\ub294 \ube44\uc794\ud2f4 \uacf5\uaca9\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ubc29\uc5b4\ud558\uba74\uc11c \ub370\uc774\ud130 \ubd88\uade0\ud615 \uc0c1\ud669\uc5d0\uc11c\ub3c4 \uacac\uace0\ud55c \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\ub294 \ud601\uc2e0\uc801\uc778 \uc811\uadfc\ubc95\uc774\ub2e4."}}
{"id": "2507.23437", "categories": ["cs.LG", "I.2.6; C.1.3; C.3"], "pdf": "https://arxiv.org/pdf/2507.23437", "abs": "https://arxiv.org/abs/2507.23437", "authors": ["Yinhui Ma", "Tomomasa Yamasaki", "Zhehui Wang", "Tao Luo", "Bo Wang"], "title": "Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design", "comment": "Accepted to ICCAD 2025 (camera-ready); 9 pages, 5 figures", "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach\nto automatically co-optimizing neural network performance and hardware energy\nefficiency, making it particularly useful for the development of Deep Neural\nNetwork accelerators on the edge. However, the extensive search space and high\ncomputational cost pose significant challenges to its practical adoption. To\naddress these limitations, we propose Coflex, a novel HW-NAS framework that\nintegrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian\noptimization. By leveraging sparse inducing points, Coflex reduces the GP\nkernel complexity from cubic to near-linear with respect to the number of\ntraining samples, without compromising optimization performance. This enables\nscalable approximation of large-scale search space, substantially decreasing\ncomputational overhead while preserving high predictive accuracy. We evaluate\nthe efficacy of Coflex across various benchmarks, focusing on\naccelerator-specific architecture. Our experi- mental results show that Coflex\noutperforms state-of-the-art methods in terms of network accuracy and\nEnergy-Delay-Product, while achieving a computational speed-up ranging from\n1.9x to 9.5x.", "AI": {"tldr": "Coflex\ub294 HW-NAS\ub97c \uc704\ud55c \uc0c8\ub85c\uc6b4 \ud504\ub808\uc784\uc6cc\ud06c\ub85c, Sparse Gaussian Process(SGP)\uc640 \ub2e4\ubaa9\uc801 \ubca0\uc774\uc9c0\uc548 \ucd5c\uc801\ud654\ub97c \uacb0\ud569\ud558\uc5ec \ud6a8\uc728\uc131\uc744 \uadf9\ub300\ud654\ud55c\ub2e4.", "motivation": "HW-NAS\ub294 \uc2e0\uacbd\ub9dd \uc131\ub2a5\uacfc \ud558\ub4dc\uc6e8\uc5b4 \uc5d0\ub108\uc9c0 \ud6a8\uc728\uc131\uc744 \uc790\ub3d9\uc73c\ub85c \ucd5c\uc801\ud654\ud558\ub294\ub370 \uc720\uc6a9\ud558\uc9c0\ub9cc, \uac80\uc0c9 \uacf5\uac04\uacfc \uacc4\uc0b0 \ube44\uc6a9\uc774 \ub192\uc544 \uc2e4\uc6a9\uc801\uc778 \ucc44\ud0dd\uc5d0 \uc5b4\ub824\uc6c0\uc774 \uc788\ub2e4.", "method": "Coflex\ub294 SGP\ub97c \ud65c\uc6a9\ud558\uc5ec \uc218\ud559\uc801 \ubcf5\uc7a1\uc131\uc744 \uc904\uc774\uba70, \ucd5c\uc801\ud654 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ub300\uaddc\ubaa8 \uac80\uc0c9 \uacf5\uac04\uc744 \uadfc\uc0ac\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4.", "result": "Coflex\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uc600\uc73c\uba70, \uae30\uc874 \ucd5c\ucca8\ub2e8 \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ub124\ud2b8\uc6cc\ud06c \uc815\ud655\ub3c4\uc640 \uc5d0\ub108\uc9c0-\uc9c0\uc5f0-\uc81c\ud488 \uba74\uc5d0\uc11c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4.", "conclusion": "Coflex\ub294 1.9\ubc30\uc5d0\uc11c 9.5\ubc30\uc758 \uacc4\uc0b0 \uc18d\ub3c4\ub97c \ub2ec\uc131\ud558\uba74\uc11c\ub3c4 \ub192\uc740 \uc608\uce21 \uc815\ud655\ub3c4\ub97c \uc81c\uacf5\ud558\uc5ec HW-NAS\uc758 \uc2e4\uc6a9\uc131\uc744 \ub300\ud3ed \uac1c\uc120\ud55c\ub2e4."}}
{"id": "2507.23771", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23771", "abs": "https://arxiv.org/abs/2507.23771", "authors": ["Justin Kay", "Grant Van Horn", "Subhransu Maji", "Daniel Sheldon", "Sara Beery"], "title": "Consensus-Driven Active Model Selection", "comment": "ICCV 2025 Highlight. 16 pages, 8 figures", "summary": "The widespread availability of off-the-shelf machine learning models poses a\nchallenge: which model, of the many available candidates, should be chosen for\na given data analysis task? This question of model selection is traditionally\nanswered by collecting and annotating a validation dataset -- a costly and\ntime-intensive process. We propose a method for active model selection, using\npredictions from candidate models to prioritize the labeling of test data\npoints that efficiently differentiate the best candidate. Our method, CODA,\nperforms consensus-driven active model selection by modeling relationships\nbetween classifiers, categories, and data points within a probabilistic\nframework. The framework uses the consensus and disagreement between models in\nthe candidate pool to guide the label acquisition process, and Bayesian\ninference to update beliefs about which model is best as more information is\ncollected. We validate our approach by curating a collection of 26 benchmark\ntasks capturing a range of model selection scenarios. CODA outperforms existing\nmethods for active model selection significantly, reducing the annotation\neffort required to discover the best model by upwards of 70% compared to the\nprevious state-of-the-art. Code and data are available at\nhttps://github.com/justinkay/coda.", "AI": {"tldr": "CODA\ub77c\ub294 \uc0c8\ub85c\uc6b4 \ubc29\ubc95\uc744 \uc81c\uc548\ud558\uc5ec \ud6a8\uc728\uc801\uc778 \ubaa8\ub378 \uc120\ud0dd\uc744 \ub3d5\uace0, \uae30\uc874 \ubc29\ubc95\ubcf4\ub2e4 \uc8fc\uc11d \uc791\uc5c5\uc744 70% \uc774\uc0c1 \uc904\uc784.", "motivation": "\uae30\uc874\uc758 \ubaa8\ub378 \uc120\ud0dd \ud504\ub85c\uc138\uc2a4\uac00 \uc2dc\uac04\uacfc \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4e4\uae30 \ub54c\ubb38\uc5d0, \ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc774 \ud544\uc694\ud558\ub2e4.", "method": "CODA\ub294 \uc608\uce21\uc744 \ud1b5\ud574 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ud3ec\uc778\ud2b8\uc758 \ub77c\ubca8\uc744 \uc6b0\uc120 \ubd80\uc5ec\ud558\uc5ec \ucd5c\uc801\uc758 \ud6c4\ubcf4 \ubaa8\ub378\uc744 \uc120\ud0dd\ud558\ub294 \ubc29\ubc95\uc774\ub2e4.", "result": "CODA\ub294 26\uac1c\uc758 \ubca4\uce58\ub9c8\ud06c \uc791\uc5c5\uc5d0\uc11c \uae30\uc874 \ubc29\ubc95\ubcf4\ub2e4 \ud604\uc800\ud558\uac8c \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uace0, \ud544\uc694\ud55c \uc8fc\uc11d \uc791\uc5c5\uc744 70% \uc774\uc0c1 \uc904\uc600\ub2e4.", "conclusion": "CODA\ub294 \ubaa8\ub378 \uc120\ud0dd\uc5d0 \uc788\uc5b4 \ub354 \ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc744 \uc81c\uacf5\ud558\uba70, \ucf54\ub4dc\uc640 \ub370\uc774\ud130\ub294 GitHub\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23449", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23449", "abs": "https://arxiv.org/abs/2507.23449", "authors": ["Shervin Rahimzadeh Arashloo"], "title": "Manifold-regularised Signature Kernel Large-Margin $\\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection", "comment": null, "summary": "We generalise the recently introduced large-margin $\\ell_p$-SVDD approach to\nexploit the geometry of data distribution via manifold regularising and a\nsignature kernel representation for time series anomaly detection.\nSpecifically, we formulate a manifold-regularised variant of the $\\ell_p$-SVDD\nmethod to encourage label smoothness on the underlying manifold to capture\nstructural information for improved detection performance. Drawing on an\nexisting Representer theorem, we then provide an effective optimisation\ntechnique for the proposed method and show that it can benefit from the\nsignature kernel to capture time series complexities for anomaly detection.\n  We theoretically study the proposed approach using Rademacher complexities to\nanalyse its generalisation performance and also provide an experimental\nassessment of the proposed method across various data sets to compare its\nperformance against other methods.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc2dc\uacc4\uc5f4 \uc774\uc0c1 \ud0d0\uc9c0\ub97c \uc704\ud574 \ub9e4\ub2c8\ud3f4\ub4dc \uc815\uaddc\ud654 \ubc0f \uc2dc\uadf8\ub2c8\ucc98 \ucee4\ub110 \ud45c\ud604\uc744 \ud65c\uc6a9\ud558\uc5ec $\text{\u2113}_p$-SVDD \ubc29\ubc95\uc744 \uc77c\ubc18\ud654\ud569\ub2c8\ub2e4.", "motivation": "\uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc758 \uad6c\uc870\uc801 \uc815\ubcf4\ub97c \uc815\uad50\ud558\uac8c \ud3ec\ucc29\ud558\uc5ec \uc774\uc0c1 \ud0d0\uc9c0 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uae30 \uc704\ud55c \ubc29\ubc95\ub860 \uac1c\ubc1c.", "method": "\ub9e4\ub2c8\ud3f4\ub4dc \uc815\uaddc\ud654\ub97c \ub3c4\uc785\ud558\uace0, \uc2dc\uadf8\ub2c8\ucc98 \ucee4\ub110\uc744 \ud65c\uc6a9\ud558\uc5ec $\text{\u2113}_p$-SVDD \ubc29\ubc95\uc744 \ucd5c\uc801\ud654.", "result": "\uc81c\uc548\ub41c \ubc29\ubc95\uc774 \uc5ec\ub7ec \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c\uc758 \uc131\ub2a5 \ube44\uad50 \uc2e4\ud5d8\uc744 \ud1b5\ud574 \uae30\uc874 \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ud655\uc778.", "conclusion": "\uc81c\uc548\ub41c \ubc29\ubc95\uc740 \uc2dc\uacc4\uc5f4\uc758 \ubcf5\uc7a1\uc131\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ucea1\ucc98\ud558\uace0 \uc77c\ubc18\ud654 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub428\uc744 \uc774\ub860\uc801, \uc2e4\ud5d8\uc801\uc73c\ub85c \uc785\uc99d\ud558\uc600\ub2e4."}}
{"id": "2507.23491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23491", "abs": "https://arxiv.org/abs/2507.23491", "authors": ["Olga Vershinina", "Jacopo Sabbatinelli", "Anna Rita Bonfigli", "Dalila Colombaretti", "Angelica Giuliani", "Mikhail Krivonosov", "Arseniy Trukhanov", "Claudio Franceschi", "Mikhail Ivanchenko", "Fabiola Olivieri"], "title": "Explainable artificial intelligence model predicting the risk of all-cause mortality in patients with type 2 diabetes mellitus", "comment": null, "summary": "Objective. Type 2 diabetes mellitus (T2DM) is a highly prevalent\nnon-communicable chronic disease that substantially reduces life expectancy.\nAccurate estimation of all-cause mortality risk in T2DM patients is crucial for\npersonalizing and optimizing treatment strategies. Research Design and Methods.\nThis study analyzed a cohort of 554 patients (aged 40-87 years) with diagnosed\nT2DM over a maximum follow-up period of 16.8 years, during which 202 patients\n(36%) died. Key survival-associated features were identified, and multiple\nmachine learning (ML) models were trained and validated to predict all-cause\nmortality risk. To improve model interpretability, Shapley additive\nexplanations (SHAP) was applied to the best-performing model. Results. The\nextra survival trees (EST) model, incorporating ten key features, demonstrated\nthe best predictive performance. The model achieved a C-statistic of 0.776,\nwith the area under the receiver operating characteristic curve (AUC) values of\n0.86, 0.80, 0.841, and 0.826 for 5-, 10-, 15-, and 16.8-year all-cause\nmortality predictions, respectively. The SHAP approach was employed to\ninterpret the model's individual decision-making processes. Conclusions. The\ndeveloped model exhibited strong predictive performance for mortality risk\nassessment. Its clinically interpretable outputs enable potential bedside\napplication, improving the identification of high-risk patients and supporting\ntimely treatment optimization.", "AI": {"tldr": "\uc774 \uc5f0\uad6c\ub294 \uc81c2\ud615 \ub2f9\ub1e8\ubcd1 \ud658\uc790\uc758 \uc804\uc778\uad6c \uc0ac\ub9dd \uc704\ud5d8\uc744 \uc608\uce21\ud558\uae30 \uc704\ud574 \uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc744 \uac1c\ubc1c\ud558\uc600\ub2e4.", "motivation": "\uc81c2\ud615 \ub2f9\ub1e8\ubcd1(T2DM)\uc740 \uc0dd\uba85 \uae30\ub300\uce58\ub97c \ud06c\uac8c \ub0ae\ucd94\ub294 \ub9cc\uc131 \uc9c8\ud658\uc73c\ub85c, \ud658\uc790\uc758 \uc0ac\ub9dd \uc704\ud5d8\uc744 \uc815\ud655\ud558\uac8c \ucd94\uc815\ud558\ub294 \uac83\uc774 \uce58\ub8cc \uc804\ub7b5\uc744 \uac1c\uc778\ud654\ud558\uace0 \ucd5c\uc801\ud654\ud558\ub294 \ub370 \uc911\uc694\ud558\ub2e4.", "method": "554\uba85\uc758 T2DM \ud658\uc790\ub97c \ub300\uc0c1\uc73c\ub85c 16.8\ub144 \uac04 Follow-up\ud558\uba70, \uc0dd\uc874\uacfc \uad00\ub828\ub41c \uc8fc\uc694 \ud2b9\uc131\uc744 \uc2dd\ubcc4\ud558\uace0 \uc5ec\ub7ec \uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc744 \ud6c8\ub828 \ubc0f \uac80\uc99d\ud588\ub2e4. \ucd5c\uace0\uc758 \ubaa8\ub378\uc5d0 \ub300\ud574 SHAP\ub97c \uc801\uc6a9\ud558\uc5ec \ud574\uc11d \uac00\ub2a5\uc131\uc744 \ub192\uc600\ub2e4.", "result": "\uc5d1\uc2a4\ud2b8\ub77c \uc11c\ubc14\uc774\ubc8c \ud2b8\ub9ac(EST) \ubaa8\ub378\uc774 \uac00\uc7a5 \ub192\uc740 \uc608\uce21 \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, C-statistic\uc740 0.776, AUC \uac12\uc740 5-, 10-, 15-, 16.8\ub144 \uac01\uac01 0.86, 0.80, 0.841, 0.826\uc774\uc5c8\ub2e4.", "conclusion": "\uac1c\ubc1c\ub41c \ubaa8\ub378\uc740 \uc0ac\ub9dd \uc704\ud5d8 \ud3c9\uac00\uc5d0 \uac15\ub825\ud55c \uc608\uce21 \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, \uc784\uc0c1\uc801\uc73c\ub85c \ud574\uc11d \uac00\ub2a5\ud55c \uacb0\uacfc\ub97c \ud1b5\ud574 \uace0\uc704\ud5d8 \ud658\uc790\uc758 \uc2dd\ubcc4 \ubc0f \uc801\uc2dc \uce58\ub8cc \ucd5c\uc801\ud654\uc5d0 \uae30\uc5ec\ud560 \uc218 \uc788\ub2e4."}}
{"id": "2507.23495", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.23495", "abs": "https://arxiv.org/abs/2507.23495", "authors": ["Maurits Kaptein"], "title": "Incorporating structural uncertainty in causal decision making", "comment": "This work is under review at the Journal of Causal Inference", "summary": "Practitioners making decisions based on causal effects typically ignore\nstructural uncertainty. We analyze when this uncertainty is consequential\nenough to warrant methodological solutions (Bayesian model averaging over\ncompeting causal structures). Focusing on bivariate relationships ($X\n\\rightarrow Y$ vs. $X \\leftarrow Y$), we establish that model averaging is\nbeneficial when: (1) structural uncertainty is moderate to high, (2) causal\neffects differ substantially between structures, and (3) loss functions are\nsufficiently sensitive to the size of the causal effect. We prove optimality\nresults of our suggested methodological solution under regularity conditions\nand demonstrate through simulations that modern causal discovery methods can\nprovide, within limits, the necessary quantification. Our framework complements\nexisting robust causal inference approaches by addressing a distinct source of\nuncertainty typically overlooked in practice.", "AI": {"tldr": "\uad6c\uc870\uc801 \ubd88\ud655\uc2e4\uc131\uc774 \uc911\uc694\ud55c \uacbd\uc6b0\uc5d0 \ub300\ud55c \ubd84\uc11d\uacfc \ubc29\ubc95\ub860\uc801 \ud574\uacb0 \ubc29\ubc95 \uc81c\uc548.", "motivation": "\uacb0\uc815\uc5d0 \uc788\uc5b4 \uc778\uacfc \ud6a8\uacfc\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 \uc804\ubb38\uac00\ub4e4\uc774 \uad6c\uc870\uc801 \ubd88\ud655\uc2e4\uc131\uc744 \uac04\uacfc\ud558\ub294 \uacbd\ud5a5\uc744 \uc9c0\uc801.", "method": "\uacbd\uc7c1\ud558\ub294 \uc778\uacfc \uad6c\uc870\uc5d0 \ub300\ud574 \ubca0\uc774\uc9c0\uc548 \ubaa8\ub378 \ud3c9\uade0\ud654\ub97c \ud65c\uc6a9\ud55c \ubc29\ubc95\ub860.", "result": "\uad6c\uc870\uc801 \ubd88\ud655\uc2e4\uc131\uc774 \uc911\uac04\uc5d0\uc11c \ub192\uace0, \uc778\uacfc \ud6a8\uacfc\uac00 \uad6c\uc870\uc5d0 \ub530\ub77c \uc0c1\ub2f9\ud788 \ub2e4\ub974\uba70, \uc190\uc2e4 \ud568\uc218\uac00 \uc778\uacfc \ud6a8\uacfc\uc758 \ud06c\uae30\uc5d0 \ubbfc\uac10\ud560 \ub54c \ubaa8\ub378 \ud3c9\uade0\ud654\uc758 \uc720\uc775\uc131\uc744 \uc785\uc99d.", "conclusion": "\uc774 \uc811\uadfc\ubc95\uc740 \uae30\uc874\uc758 \uacac\uace0\ud55c \uc778\uacfc \ucd94\ub860 \ubc29\ubc95\uc744 \ubcf4\uc644\ud558\uba70, \uc77c\ubc18\uc801\uc73c\ub85c \ubb34\uc2dc\ub418\ub294 \ubd88\ud655\uc2e4\uc131\uc758 \uc6d0\uc778\uc744 \ub2e4\ub8ec\ub2e4."}}
{"id": "2507.23501", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.23501", "abs": "https://arxiv.org/abs/2507.23501", "authors": ["Nicklas Werge", "Yi-Shan Wu", "Bahareh Tasdighi", "Melih Kandemir"], "title": "Directional Ensemble Aggregation for Actor-Critics", "comment": null, "summary": "Off-policy reinforcement learning in continuous control tasks depends\ncritically on accurate $Q$-value estimates. Conservative aggregation over\nensembles, such as taking the minimum, is commonly used to mitigate\noverestimation bias. However, these static rules are coarse, discard valuable\ninformation from the ensemble, and cannot adapt to task-specific needs or\ndifferent learning regimes. We propose Directional Ensemble Aggregation (DEA),\nan aggregation method that adaptively combines $Q$-value estimates in\nactor-critic frameworks. DEA introduces two fully learnable directional\nparameters: one that modulates critic-side conservatism and another that guides\nactor-side policy exploration. Both parameters are learned using ensemble\ndisagreement-weighted Bellman errors, which weight each sample solely by the\ndirection of its Bellman error. This directional learning mechanism allows DEA\nto adjust conservatism and exploration in a data-driven way, adapting\naggregation to both uncertainty levels and the phase of training. We evaluate\nDEA across continuous control benchmarks and learning regimes - from\ninteractive to sample-efficient - and demonstrate its effectiveness over static\nensemble strategies.", "AI": {"tldr": "DEA\ub294 \uc5f0\uc18d \uc81c\uc5b4 \uc791\uc5c5\uc5d0\uc11cQ-\uac12 \ucd94\uc815\uc744 \uc801\uc751\uc801\uc73c\ub85c \uacb0\ud569\ud558\ub294 \ubc29\ubc95\uc73c\ub85c, \uc815\uc801 \uc9d1\ud569 \uc804\ub7b5\ubcf4\ub2e4 \ud6a8\uacfc\uc801\uc774\ub2e4.", "motivation": "\uc5f0\uc18d \uc81c\uc5b4 \uc791\uc5c5\uc5d0\uc11c \uc624\ud504-\uc815\ucc45 \uac15\ud654 \ud559\uc2b5\uc758 \uc131\uacfc\ub294 Q-\uac12 \ucd94\uc815\uc758 \uc815\ud655\uc131\uc5d0 \ub2ec\ub824 \uc788\ub2e4. \uae30\uc874\uc758 \uc815\uc801 \uc9d1\ud569Rules\ub294 \uacfc\ub300 \ucd94\uc815 \ud3b8\ud5a5\uc744 \uc644\ud654\ud558\uc9c0\ub9cc, \uc815\ubcf4 \uc190\uc2e4\uacfc \uc801\uc751\uc131\uc774 \ubd80\uc871\ud558\ub2e4.", "method": "Directional Ensemble Aggregation (DEA)\ub294 \ub450 \uac1c\uc758 \uc644\uc804 \ud559\uc2b5 \uac00\ub2a5\ud55c \ubc29\ud5a5 \ub9e4\uac1c \ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec Q-\uac12 \ucd94\uc815\uc744 \uacb0\ud569\ud55c\ub2e4. \uc774 \ub9e4\uac1c \ubcc0\uc218\ub294 \ube44\ub840\uc801 \ubca8\ub9cc \uc624\ucc28\ub97c \ud1b5\ud574 \ud559\uc2b5\ub41c\ub2e4.", "result": "DEA\ub294 \uc5f0\uc18d \uc81c\uc5b4 \uae30\uc900 \ud3c9\uac00 \ubc0f \uc5ec\ub7ec \ud559\uc2b5 \uccb4\uc81c\ub97c \ud1b5\ud574 \uc815\uc801 \uc9d1\ud569 \uc804\ub7b5\ubcf4\ub2e4 \ud6a8\uacfc\uc801\uc778 \uc131\uacfc\ub97c \ubcf4\uc778\ub2e4.", "conclusion": "DEA\ub294 \ud6c8\ub828 \ub2e8\uacc4\uc640 \ubd88\ud655\uc2e4\uc131 \uc218\uc900\uc5d0 \ub530\ub77c \uc801\uc751\uc801\uc73c\ub85c \ubcf4\uc218\uc131\uacfc \ud0d0\uc0c9\uc744 \uc870\uc815\ud560 \uc218 \uc788\uc5b4 \ud6a8\uc728\uc801\uc774\ub2e4."}}
{"id": "2507.23504", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23504", "abs": "https://arxiv.org/abs/2507.23504", "authors": ["Maurits Kaptein"], "title": "A Verifier Hierarchy", "comment": "This paper is primarily relevant to cs.CC, but submitted under cs.ML\n  due to lack of endorsement. The paper is under review at \"Information and\n  Communication\"", "summary": "We investigate the trade-off between certificate length and verifier runtime.\nWe prove a Verifier Trade-off Theorem showing that reducing the inherent\nverification time of a language from \\(f(n)\\) to \\(g(n)\\), where \\(f(n) \\ge\ng(n)\\), requires certificates of length at least \\(\\Omega(\\log(f(n) / g(n)))\\).\nThis theorem induces a natural hierarchy based on certificate complexity. We\ndemonstrate its applicability to analyzing conjectured separations between\ncomplexity classes (e.g., \\(\\np\\) and \\(\\exptime\\)) and to studying natural\nproblems such as string periodicity and rotation detection. Additionally, we\nprovide perspectives on the \\(\\p\\) vs. \\(\\np\\) problem by relating it to the\nexistence of sub-linear certificates.", "AI": {"tldr": "\uc778\uc99d\uc11c \uae38\uc774\uc640 \uac80\uc99d\uc790 \uc2e4\ud589 \uc2dc\uac04 \uac04\uc758 \ubb34\uc5ed \uc624\ud504\ub97c \uc5f0\uad6c\ud558\uba70, \uad00\ub828 \uc815\ub9ac\ub97c \uc81c\uc2dc\ud55c\ub2e4.", "motivation": "\uc778\uc99d\uc11c \ubcf5\uc7a1\uc131\uacfc \ubcf5\uc7a1\ub3c4 \ud074\ub798\uc2a4 \uac04 \uacbd\uacc4\ub97c \uc774\ud574\ud558\uae30 \uc704\ud574.", "method": "\uac80\uc99d \uc2dc\uac04\uacfc \uc778\uc99d\uc11c \uae38\uc774 \uac04\uc758 \uad00\uacc4\ub97c \uc815\ub9ac\ud558\uace0 \uc774\ub97c \uc5ec\ub7ec \ubb38\uc81c\uc5d0 \uc801\uc6a9\ud558\uc600\ub2e4.", "result": "\uc778\uc99d\uc11c \uae38\uc774\ub97c \uc904\uc774\uba74 \uac80\uc99d \uc2dc\uac04\uc744 \uc904\uc77c \uc218 \uc5c6\ub2e4\ub294 \uac83\uc744 \uc99d\uba85\ud558\uba70, \uc0c8\ub85c\uc6b4 \uc774\ub860\uc801 \uacc4\uce35\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "conclusion": "\uc774\ub860\uc740 \ubcf5\uc7a1\ub3c4 \ubb38\uc81c\ub97c \ubd84\uc11d\ud558\ub294 \ub370 \ud6a8\uacfc\uc801\uc774\uba70, \ud2b9\ud788 \\(\\p\\)\uc640 \\(\\np\\) \ubb38\uc81c\uc5d0 \ub300\ud55c \uc0c8\ub85c\uc6b4 \uad00\uc810\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23512", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.23512", "abs": "https://arxiv.org/abs/2507.23512", "authors": ["Saleh Vatan Khah", "Savelii Chezhegov", "Shahrokh Farahmand", "Samuel Horv\u00e1th", "Eduard Gorbunov"], "title": "Differentially Private Clipped-SGD: High-Probability Convergence with Arbitrary Clipping Level", "comment": "60 pages", "summary": "Gradient clipping is a fundamental tool in Deep Learning, improving the\nhigh-probability convergence of stochastic first-order methods like SGD,\nAdaGrad, and Adam under heavy-tailed noise, which is common in training large\nlanguage models. It is also a crucial component of Differential Privacy (DP)\nmechanisms. However, existing high-probability convergence analyses typically\nrequire the clipping threshold to increase with the number of optimization\nsteps, which is incompatible with standard DP mechanisms like the Gaussian\nmechanism. In this work, we close this gap by providing the first\nhigh-probability convergence analysis for DP-Clipped-SGD with a fixed clipping\nlevel, applicable to both convex and non-convex smooth optimization under\nheavy-tailed noise, characterized by a bounded central $\\alpha$-th moment\nassumption, $\\alpha \\in (1,2]$. Our results show that, with a fixed clipping\nlevel, the method converges to a neighborhood of the optimal solution with a\nfaster rate than the existing ones. The neighborhood can be balanced against\nthe noise introduced by DP, providing a refined trade-off between convergence\nspeed and privacy guarantees.", "AI": {"tldr": "\uc774 \ub17c\ubb38\uc740 \uace0\uc815 \ud074\ub9ac\ud551 \uc218\uc900\uc5d0\uc11c DP-Clipped-SGD\uc758 \uace0\ud655\ub960 \uc218\ub834 \ubd84\uc11d\uc744 \uc81c\uacf5\ud558\uba70, \uc774\ub294 \uacbd\ub7c9 \ubc0f \ube44\uacbd\ub7c9 \ucd5c\uc801\ud654\uc5d0\uc11c \ub354 \ube60\ub978 \uc218\ub834 \uc18d\ub3c4\ub97c \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "motivation": "\ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378 \ud6c8\ub828 \uc2dc \ubc1c\uc0dd\ud558\ub294 \uac15\ud55c \uc601\ud5a5\ub825\uc744 \uac00\uc9c4 \ub178\uc774\uc988 \ud558\uc5d0\uc11c\ub3c4 SGD, AdaGrad, Adam\uacfc \uac19\uc740 \ud655\ub960\uc801 1\ucc28 \ubc29\ubc95\uc758 \uc218\ub834\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc774 \ud544\uc694\ud558\ub2e4.", "method": "DP-Clipped-SGD\ub97c \uc0ac\uc6a9\ud558\uc5ec \uace0\uc815\ub41c \ud074\ub9ac\ud551 \uc218\uc900\uc5d0\uc11c \uacbd\ub7c9 \ubc0f \ube44\uacbd\ub7c9 \ubd80\ub4dc\ub7ec\uc6b4 \ucd5c\uc801\ud654 \ubb38\uc81c\uc5d0 \ub300\ud574 \uc218\ub834 \ubd84\uc11d\uc744 \uc218\ud589\ud558\uc600\ub2e4.", "result": "\uace0\uc815\ub41c \ud074\ub9ac\ud551 \uc218\uc900\uc744 \uc720\uc9c0\ud558\uba70 \uae30\uc874 \ubc29\ubc95\ubcf4\ub2e4 \ub354 \ube60\ub978 \uc218\ub834\ub960\uc744 \ud655\ubcf4\ud568\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.", "conclusion": "\uc218\ub834 \uc18d\ub3c4\uc640 \uac1c\uc778 \uc815\ubcf4 \ubcf4\ud638 \ubcf4\uc7a5 \uac04\uc758 \ubbf8\uc138\ud55c \uade0\ud615\uc744 \uc81c\uacf5\ud558\uba70, DP \uba54\ucee4\ub2c8\uc998\uc758 \ub178\uc774\uc988\uc5d0 \ub300\ucc98\ud560 \uc218 \uc788\ub294 \uc774\uc810\uc744 \ub204\ub9b4 \uc218 \uc788\ub2e4."}}
{"id": "2507.23534", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23534", "abs": "https://arxiv.org/abs/2507.23534", "authors": ["Chih-Fan Hsu", "Ming-Ching Chang", "Wei-Chao Chen"], "title": "Continual Learning with Synthetic Boundary Experience Blending", "comment": null, "summary": "Continual learning (CL) aims to address catastrophic forgetting in models\ntrained sequentially on multiple tasks. While experience replay has shown\npromise, its effectiveness is often limited by the sparse distribution of\nstored key samples, leading to overly simplified decision boundaries. We\nhypothesize that introducing synthetic data near the decision boundary\n(Synthetic Boundary Data, or SBD) during training serves as an implicit\nregularizer, improving boundary stability and mitigating forgetting. To\nvalidate this hypothesis, we propose a novel training framework, {\\bf\nExperience Blending}, which integrates knowledge from both stored key samples\nand synthetic, boundary-adjacent data. Experience blending consists of two core\ncomponents: (1) a multivariate Differential Privacy (DP) noise mechanism that\ninjects batch-wise noise into low-dimensional feature representations,\ngenerating SBD; and (2) an end-to-end training strategy that jointly leverages\nboth stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100,\nand Tiny ImageNet demonstrate that our method outperforms nine CL baselines,\nachieving accuracy improvements of 10%, 6%, and 13%, respectively.", "AI": {"tldr": "\ubcf8 \uc5f0\uad6c\ub294 \uc5f0\uc18d \ud559\uc2b5\uc5d0\uc11c\uc758 \ub9dd\uac01 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \ud569\uc131 \uacbd\uacc4 \ub370\uc774\ud130\ub97c \ub3c4\uc785\ud558\uc5ec \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uacbd\ud5d8 \ud63c\ud569 \ud6c8\ub828 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud55c\ub2e4.", "motivation": "\uc5f0\uc18d \ud559\uc2b5 \ubaa8\ub378\uc774 \uc5ec\ub7ec \uc791\uc5c5\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \ud559\uc2b5\ud560 \ub54c \ubc1c\uc0dd\ud558\ub294 \ub9dd\uac01 \ubb38\uc81c \ud574\uacb0.", "method": "\uc800\uc7a5\ub41c \uc8fc\uc694 \uc0d8\ud50c\uacfc \ud569\uc131 \uacbd\uacc4 \ub370\uc774\ud130\ub97c \ud1b5\ud569\ud55c \uacbd\ud5d8 \ud63c\ud569 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828.", "result": "CIFAR-10, CIFAR-100, Tiny ImageNet \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uae30\uc874 \uc544\ud649 \uac1c CL \uae30\ubc95\ubcf4\ub2e4 \uac01\uac01 10%, 6%, 13%\uc758 \uc131\ub2a5 \ud5a5\uc0c1 \ub2ec\uc131.", "conclusion": "\ud569\uc131 \uacbd\uacc4 \ub370\uc774\ud130\uc758 \ub3c4\uc785\uc774 \uacbd\uacc4 \uc548\uc815\uc131\uc744 \uac1c\uc120\ud558\uace0 \ub9dd\uac01\uc744 \uc644\ud654\ud568\uc744 \uc785\uc99d\ud558\uc600\ub2e4."}}
{"id": "2507.23539", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.23539", "abs": "https://arxiv.org/abs/2507.23539", "authors": ["Piotr Indyk", "Michael Kapralov", "Kshiteej Sheth", "Tal Wagner"], "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions", "comment": "Published in ICLR 2025", "summary": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.", "AI": {"tldr": "\ube44\ub300\uce6d \uac00\uc6b0\uc2dc\uc548 \ucee4\ub110 \ud589\ub82c\uc758 \ud589\ub82c-\ubca1\ud130 \uacf1\uc744 \ube60\ub974\uac8c \uacc4\uc0b0\ud558\ub294 \uc54c\uace0\ub9ac\uc998\uc744 \uc5f0\uad6c\ud558\uace0, \uc774 \uc54c\uace0\ub9ac\uc998\uc774 \ud2b9\uc815 \uac00\uc815 \ud558\uc5d0 \ucd5c\uc801\ud654\ub428\uc744 \uc2e4\ud5d8\uc801\uc73c\ub85c \uac80\uc99d\ud558\uc600\ub2e4.", "motivation": "\uc8fc\uc758 \ub9e4\ud2b8\ub9ad\uc2a4\uc758 \ube60\ub978 \ucc98\ub9ac\ub97c \uc704\ud55c \ubb38\uc81c \ud574\uacb0 \ub3d9\uae30.", "method": "\ube44\ub300\uce6d \uac00\uc6b0\uc2dc\uc548 \ucee4\ub110 \ud589\ub82c\uc5d0 \ub300\ud574 \uc11c\ube0c\ucffc\ub4dc\ub77c\ud2f1 \uc2dc\uac04 \ub0b4\uc5d0\uc11c \ud589\ub82c-\ubca1\ud130 \uacf1\uc744 \uacc4\uc0b0\ud558\ub294 \uc54c\uace0\ub9ac\uc998\uc744 \uac1c\ubc1c\ud558\uc600\ub2e4.", "result": "\uac00\uc6b0\uc2dc\uc548 \ucee4\ub110 \ud589\ub82c\uc5d0 \ub300\ud574 \ub204\uc801 \ud569\uc774 \uc120\ud615\uc801\uc73c\ub85c \uac10\uc18c\ud558\ub294 \uc870\uac74\uc744 \ub9cc\uc871\ud558\ub294 \ud589\ub82c\uc758 \uccab \uc11c\ube0c\ucffc\ub4dc\ub77c\ud2f1 \uc2dc\uac04 \uc54c\uace0\ub9ac\uc998\uc744 \uc81c\uacf5\ud558\uc600\ub2e4.", "conclusion": "\uc81c\uc548\ub41c \uc54c\uace0\ub9ac\uc998\uc774 \ub2e4\uc591\ud55c \ud658\uacbd\uc5d0\uc11c \ube60\ub978 \uc8fc\uc758 \uacc4\uc0b0 \uc2dc \uc720\uc6a9\ud568\uc744 \ud655\uc778\ud558\uc600\ub2e4."}}
{"id": "2507.23562", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.23562", "abs": "https://arxiv.org/abs/2507.23562", "authors": ["Sirine Arfa", "Bernhard Vogginger", "Christian Mayr"], "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform", "comment": "8 pages, 5 figures, 3 tables", "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.", "AI": {"tldr": "\uc2a4\ud30c\uc774\ud0b9 \uc2e0\uacbd\ub9dd(SNN)\uc740 \ub85c\ubd07 \uc791\uc5c5\uc744 \uc704\ud574 \uc800\uc804\ub825 \uc18c\ube44\uc640 \ub0ae\uc740 \uc9c0\uc5f0\uc2dc\uac04\uc758 \ucd94\ub860\uc744 \uc81c\uacf5\ud558\ub294 \uac00\ub2a5\uc131\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "motivation": "\ub85c\ubd07 \uc791\uc5c5\uc5d0 \ub300\ud55c \ud6a8\uc728\uc801\uc778 \uc2e0\uacbd\ub9dd \uad6c\ud604 \ubc0f \uc800\uc804\ub825 \uc131\ub2a5 \ud544\uc694.", "method": "Q-\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud55c \uc591\uc790\ud654\ub41c SNN \ubaa8\ub378\uc744 \uc2a4\ud53c\ub108\ucee42 \uce69\uc5d0 \ubc30\ud3ec\ud558\uae30 \uc704\ud574 \ud6c8\ub828 \ubc0f \uc138\ubc00 \uc870\uc815.", "result": "\uc2a4\ud53c\ub108\ucee42\uac00 \uc5d0\ub108\uc9c0 \uc18c\ube44\ub97c \ucd5c\ub300 32\ubc30 \uc904\uc774\uba70, \uc9c0\uc5f0 \uc2dc\uac04\uc740 GPU\uc640 \uc720\uc0ac\ud55c \uc218\uc900\uc744 \uc720\uc9c0.", "conclusion": "\uc2a4\ud53c\ub108\ucee42\ub294 \uc2e4\uc2dc\uac04 \uc81c\uc5b4\uc5d0 \uc801\ud569\ud558\uba70, \ud6a8\uacfc\uc801\uc778 \uc2ec\uce35 Q-\ub7ec\ub2dd\uc744 \uc704\ud55c \ub9e4\ub825\uc801\uc778 \ubc29\ud5a5\uc131\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23568", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.23568", "abs": "https://arxiv.org/abs/2507.23568", "authors": ["Fernando Mart\u00ednez-Garc\u00eda", "\u00c1lvaro Rubio-Garc\u00eda", "Samuel Fern\u00e1ndez-Lorenzo", "Juan Jos\u00e9 Garc\u00eda-Ripoll", "Diego Porras"], "title": "Optimised Feature Subset Selection via Simulated Annealing", "comment": "12 pages, 2 figures", "summary": "We introduce SA-FDR, a novel algorithm for $\\ell_0$-norm feature selection\nthat considers this task as a combinatorial optimisation problem and solves it\nby using simulated annealing to perform a global search over the space of\nfeature subsets. The optimisation is guided by the Fisher discriminant ratio,\nwhich we use as a computationally efficient proxy for model quality in\nclassification tasks. Our experiments, conducted on datasets with up to\nhundreds of thousands of samples and hundreds of features, demonstrate that\nSA-FDR consistently selects more compact feature subsets while achieving a high\npredictive accuracy. This ability to recover informative yet minimal sets of\nfeatures stems from its capacity to capture inter-feature dependencies often\nmissed by greedy optimisation approaches. As a result, SA-FDR provides a\nflexible and effective solution for designing interpretable models in\nhigh-dimensional settings, particularly when model sparsity, interpretability,\nand performance are crucial.", "AI": {"tldr": "SA-FDR\ub294 $\text{l}_0$-\ub178\ub984 \ud2b9\uc131 \uc120\ud0dd\uc744 \uc704\ud574 \uc0c8\ub85c\uc6b4 \uc54c\uace0\ub9ac\uc998\uc744 \uc81c\uc548\ud558\uba70, \uc2dc\ubbac\ub808\uc774\ud2f0\ub4dc \uc5b4\ub2d0\ub9c1 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uae00\ub85c\ubc8c \ud0d0\uc0c9\uc744 \uc218\ud589\ud55c\ub2e4.", "motivation": "\ud2b9\uc131 \uc120\ud0dd\uc744 \uc870\ud569 \ucd5c\uc801\ud654 \ubb38\uc81c\ub85c \ub2e4\ub8e8\uace0, \uace0\ucc28\uc6d0 \uc124\uc815\uc5d0\uc11c \ubaa8\ub378\uc758 \ud574\uc11d \uac00\ub2a5\uc131\uacfc \uc131\ub2a5\uc744 \ub192\uc774\uae30 \uc704\ud568\uc774\ub2e4.", "method": "\uc2dc\ubbac\ub808\uc774\ud2f0\ub4dc \uc5b4\ub2d0\ub9c1\uc744 \ud1b5\ud574 \ud2b9\uc131 \ud558\uc704 \uc9d1\ud569 \uacf5\uac04\uc5d0\uc11c \uae00\ub85c\ubc8c \uac80\uc0c9\uc744 \uc218\ud589\ud558\uba70, \ud53c\uc154 \ud310\ubcc4 \ube44\uc728\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378 \ud488\uc9c8\uc744 \uacc4\uc0b0\ud55c\ub2e4.", "result": "\ub300\uaddc\ubaa8 \ub370\uc774\ud130\uc14b\uc5d0\uc11c SA-FDR\uc774 \ub354 \ubc00\uc9d1\ub41c \ud2b9\uc131 \ud558\uc704 \uc9d1\ud569\uc744 \uc120\ud0dd\ud558\uba74\uc11c\ub3c4 \ub192\uc740 \uc608\uce21 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud558\ub294 \uac83\uc744 \uc785\uc99d\ud558\uc600\ub2e4.", "conclusion": "SA-FDR\uc740 \uace0\ucc28\uc6d0 \uc124\uc815\uc5d0\uc11c \ubaa8\ub378\uc758 \ud76c\uc18c\uc131, \ud574\uc11d \uac00\ub2a5\uc131 \ubc0f \uc131\ub2a5\uc744 \uc911\uc694\uc2dc\ud558\ub294 \uacbd\uc6b0\uc5d0 \uc720\uc6a9\ud558\uace0 \ud6a8\uacfc\uc801\uc778 \uc194\ub8e8\uc158\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23581", "abs": "https://arxiv.org/abs/2507.23581", "authors": ["Chuanyue Yu", "Kuo Zhao", "Yuhan Li", "Heng Chang", "Mingjian Feng", "Xiangzhe Jiang", "Yufei Sun", "Jia Li", "Yuzhi Zhang", "Jianxin Li", "Ziwei Zhang"], "title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning", "comment": null, "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements.", "AI": {"tldr": "GraphRAG-R1\uc740 LLM\uc758 \ub2e4\ub2e8\uacc4 \ucd94\ub860 \ub2a5\ub825\uc744 \ud5a5\uc0c1\ud558\uae30 \uc704\ud574 \ud504\ub85c\uc138\uc2a4 \uc81c\uc57d \uae30\ubc18\uc758 \uacb0\uacfc \uae30\ubc18 \uac15\ud654 \ud559\uc2b5\uc744 \ud65c\uc6a9\ud55c \uc0c8\ub85c\uc6b4 \uadf8\ub798\ud504 \uac80\uc0c9 \uc99d\uac15 \uc0dd\uc131 \ud504\ub808\uc784\uc6cc\ud06c\uc774\ub2e4.", "motivation": "\ubcf5\uc7a1\ud55c \ubb38\uc81c\ub97c \ub2e4\ub8e8\ub294 \uae30\uc874\uc758 GraphRAG \ubc29\ubc95\ub4e4\uc774 \uc0ac\uc804 \uc815\uc758\ub41c \ud734\ub9ac\uc2a4\ud2f1\uc5d0 \uc758\uc874\ud558\uc5ec \uba40\ud2f0 \ud649 \ucd94\ub860 \ubb38\uc81c\uc5d0\uc11c \ud55c\uacc4\ub97c \ubcf4\uc774\uace0 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4.", "method": "\ud504\ub85c\uc138\uc2a4 \uc81c\uc57d\uc758 \uacb0\uacfc \uae30\ubc18 \uac15\ud654 \ud559\uc2b5\uc744 \ud65c\uc6a9\ud558\uc5ec LLM\uc774 \ubcf5\uc7a1\ud55c \ubb38\uc81c\ub97c \ubd84\ud574\ud558\uace0, \ud544\uc694 \uc815\ubcf4\ub97c \uc790\uc728\uc801\uc73c\ub85c \uac80\uc0c9\ud558\uba70 \ud6a8\uacfc\uc801\uc73c\ub85c \ucd94\ub860\ud560 \uc218 \uc788\uac8c \ud55c\ub2e4.", "result": "GraphRAG-R1\uc740 \ubcf5\uc7a1\ud55c \ucd94\ub860 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ub370 \uc788\uc5b4 \ucd5c\ucca8\ub2e8 GraphRAG \ubc29\ubc95\ub4e4\uc5d0 \ube44\ud574 LLM\uc758 \ub2a5\ub825\uc744 \uc99d\uac00\uc2dc\ucf30\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "conclusion": "GraphRAG-R1\uc740 \uc720\uc5f0\ud558\uac8c \ub2e4\uc591\ud55c \uae30\uc874 \uac80\uc0c9 \ubc29\ubc95\uacfc \ud1b5\ud569 \uac00\ub2a5\ud558\uba70, \uc77c\uad00\ub418\uac8c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23600", "categories": ["cs.LG", "cs.CE", "G.1.6; G.3; G.4; I.6.5"], "pdf": "https://arxiv.org/pdf/2507.23600", "abs": "https://arxiv.org/abs/2507.23600", "authors": ["Yu-Tang Chang", "Shih-Fang Chen"], "title": "EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution", "comment": null, "summary": "Signal unmixing analysis decomposes data into basic patterns and is widely\napplied in chemical and biological research. Multivariate curve resolution\n(MCR), a branch of signal unmixing, separates mixed chemical signals into base\npatterns (components) and their concentrations, playing a key role in\nunderstanding composition. Classical MCR is typically framed as matrix\nfactorization (MF) and requires a user-specified component count, usually\nunknown in real data. As dataset size or component count increases, the\nscalability and reliability of MF-based MCR face significant challenges. This\nstudy reformulates MCR as a generative process (gMCR), and introduces an\nenergy-based deep learning solver, EB-gMCR, that automatically discovers the\nsmallest component set able to reconstruct the data faithfully. EB-gMCR starts\nfrom a large candidate pool (e.g., 1024 spectra) and employs a differentiable\ngating network to retain only active components while estimating their\nconcentrations. On noisy synthetic datasets containing up to 256 latent\nsources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count\nwithin 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near\nexact component estimation. Additional chemical priors, such as non-negativity\nor nonlinear mixing, enter as simple plug-in functions, enabling adaptation to\nother instruments or domains without altering the core learning process. By\nuniting high-capacity generative modeling and hard component selection, EB-gMCR\noffers a practical route to large-scale signal unmixing analysis, including\nchemical library-driven scenarios. The source code is available at\nhttps://github.com/b05611038/ebgmcr_solver.", "AI": {"tldr": "EB-gMCR\ub294 \uc2e0\ud638 \ud63c\ud569 \ud574\uc11d\uc744 \uc704\ud55c \uc5d0\ub108\uc9c0 \uae30\ubc18 \uae4a\uc740 \ud559\uc2b5 \uc194\ubc84\ub85c, \ub370\uc774\ud130 \uc7ac\uad6c\uc131\uc744 \uc704\ud55c \ucd5c\uc18c\ud55c\uc758 \uad6c\uc131 \uc694\uc18c \uc9d1\ud569\uc744 \uc790\ub3d9\uc73c\ub85c \ubc1c\uacac\ud55c\ub2e4.", "motivation": "\ud63c\ud569\ub41c \ud654\ud559 \uc2e0\ud638\ub97c \ubd84\ub9ac\ud558\ub294 MCR\uc758 \uc804\ud1b5\uc801\uc778 \ubc29\ubc95\uc740 \uc0ac\uc6a9\uc790\uac00 \uc9c0\uc815\ud55c \uad6c\uc131 \uc694\uc18c \uc218\uac00 \ud544\uc694\ud558\uba70, \uc774\ub294 \uc2e4\uc81c \ub370\uc774\ud130\uc5d0\uc11c\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \uc54c\uae30 \uc5b4\ub835\ub2e4.", "method": "MCR\uc744 \uc0dd\uc131 \uacfc\uc815\uc73c\ub85c \uc7ac\uad6c\uc131\ud558\uace0, \ucc28\ubcc4 \uac00\ub2a5 \uac8c\uc774\ud305 \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud65c\uc131 \uad6c\uc131 \uc694\uc18c\ub9cc\uc744 \uc720\uc9c0\ud558\ub294 EB-gMCR\uc744 \uac1c\ubc1c\ud588\ub2e4.", "result": "EB-gMCR\uc740 \ucd5c\ub300 256 \uac1c\uc758 \uc7a0\uc7ac \uc18c\uc2a4\uac00 \ud3ec\ud568\ub41c \ub178\uc774\uc988\uac00 \uc788\ub294 \ud569\uc131 \ub370\uc774\ud130\uc14b\uc5d0\uc11c R^2 >= 0.98\uc744 \uc720\uc9c0\ud558\uace0, \uc2e4\uc81c \uad6c\uc131 \uc694\uc18c \uc218\ub97c 5% \uc774\ub0b4\ub85c \ud68c\ubcf5\ud588\ub2e4.", "conclusion": "EB-gMCR\uc740 \uace0\uc6a9\ub7c9 \uc0dd\uc131 \ubaa8\ub378\ub9c1\uacfc \ud558\ub4dc \uad6c\uc131 \uc694\uc18c \uc120\ud0dd\uc744 \ud1b5\ud569\ud558\uc5ec \ub300\uaddc\ubaa8 \uc2e0\ud638 \ud63c\ud569 \ud574\uc11d\uc744 \uc704\ud55c \uc2e4\uc6a9\uc801\uc778 \uacbd\ub85c\ub97c \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23604", "abs": "https://arxiv.org/abs/2507.23604", "authors": ["Tommaso Marzi", "Cesare Alippi", "Andrea Cini"], "title": "Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning", "comment": null, "summary": "Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for\nlearning scalable multi-agent policies, but suffer from partial observability\nand induced non-stationarity. These challenges can be addressed by introducing\nmechanisms that facilitate coordination and high-level planning. Specifically,\ncoordination and temporal abstraction can be achieved through communication\n(e.g., message passing) and Hierarchical Reinforcement Learning (HRL)\napproaches to decision-making. However, optimization issues limit the\napplicability of hierarchical policies to multi-agent systems. As such, the\ncombination of these approaches has not been fully explored. To fill this void,\nwe propose a novel and effective methodology for learning multi-agent\nhierarchies of message-passing policies. We adopt the feudal HRL framework and\nrely on a hierarchical graph structure for planning and coordination among\nagents. Agents at lower levels in the hierarchy receive goals from the upper\nlevels and exchange messages with neighboring agents at the same level. To\nlearn hierarchical multi-agent policies, we design a novel reward-assignment\nmethod based on training the lower-level policies to maximize the advantage\nfunction associated with the upper levels. Results on relevant benchmarks show\nthat our method performs favorably compared to the state of the art.", "AI": {"tldr": "\ubcf8 \uc5f0\uad6c\ub294 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uacc4\uce35\uc801 \uba54\uc2dc\uc9c0 \uc804\uc1a1 \uc815\ucc45\uc744 \ud559\uc2b5\ud558\uae30 \uc704\ud55c \uc0c8\ub85c\uc6b4 \ubc29\ubc95\ub860\uc744 \uc81c\uc548\ud55c\ub2e4.", "motivation": "\ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \uac15\ud654 \ud559\uc2b5(MARL) \ubc29\ubc95\uc758 \uc81c\ud55c\uc810\uc744 \uadf9\ubcf5\ud558\uace0, \ud611\ub3d9 \ubc0f \uace0\uae09 \uacc4\ud68d \uc218\ub9bd\uc758 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud574 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud574.", "method": "\ubd09\uac74\uc801 HRL \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ucc44\ud0dd\ud558\uace0, \uc5d0\uc774\uc804\ud2b8 \uac04\uc758 \uacc4\ud68d \ubc0f \ud611\ub3d9\uc744 \uc704\ud55c \uacc4\uce35\uc801 \uadf8\ub798\ud504 \uad6c\uc870\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba54\uc2dc\uc9c0 \uc804\uc1a1 \uc815\ucc45\uc744 \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uacc4\ud588\ub2e4.", "result": "\uc81c\uc548\ud55c \ubc29\ubc95\uc774 \uae30\uc874 \ucd5c\ucca8\ub2e8 \uae30\ubc95 \ub300\ube44 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc784\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\ub2e4.", "conclusion": "\uacc4\uce35\uc801 \uba54\uc2dc\uc9c0 \uc804\uc1a1 \uc815\ucc45\uc744 \ud1b5\ud574 MARL\uc758 \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\ub294 \uac00\ub2a5\uc131\uc744 \uc81c\uc2dc\ud558\uc600\ub2e4."}}
{"id": "2507.23632", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23632", "abs": "https://arxiv.org/abs/2507.23632", "authors": ["Gabriel Mongaras", "Eric C. Larson"], "title": "On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective", "comment": null, "summary": "Since its introduction, softmax attention has become the backbone of modern\ntransformer architectures due to its expressiveness and scalability across a\nwide range of tasks. However, the main drawback of softmax attention is the\nquadratic memory requirement and computational complexity with respect to the\nsequence length. By replacing the softmax nonlinearity, linear attention and\nsimilar methods have been introduced to avoid the quadratic bottleneck of\nsoftmax attention. Despite these linear forms of attention being derived from\nthe original softmax formulation, they typically lag in terms of downstream\naccuracy. While strong intuition of the softmax nonlinearity on the query and\nkey inner product suggests that it has desirable properties compared to other\nnonlinearities, the question of why this discrepancy exists still remains\nunanswered. This work demonstrates that linear attention is an approximation of\nsoftmax attention by deriving the recurrent form of softmax attention. Using\nthis form, each part of softmax attention can be described in the language of\nrecurrent neural networks (RNNs). Describing softmax attention as an RNN allows\nfor the ablation of the components of softmax attention to understand the\nimportance of each part and how they interact. In this way, our work helps\nexplain why softmax attention is more expressive than its counterparts.", "AI": {"tldr": "\uc18c\ud504\ud2b8\ub9e5\uc2a4 \uc5b4\ud150\uc158\uc740 \ud604\ub300 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uc544\ud0a4\ud14d\ucc98\uc758 \ud575\uc2ec\uc774\uc9c0\ub9cc, \uba54\ubaa8\ub9ac\uc640 \uacc4\uc0b0 \ubcf5\uc7a1\ub3c4\uac00 \uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub530\ub77c \uc81c\uacf1\uc801\uc73c\ub85c \uc99d\uac00\ud558\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \uc120\ud615 \uc5b4\ud150\uc158\uc774 \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud588\uc9c0\ub9cc \uc815\ud655\uc131 \uba74\uc5d0\uc11c \ubd80\uc871\ud568\uc774 \uc788\ub2e4. \ubcf8 \uc5f0\uad6c\ub294 \uc18c\ud504\ud2b8\ub9e5\uc2a4 \uc5b4\ud150\uc158\uc758 \uc7ac\uadc0\uc801 \ud615\ud0dc\ub97c \uc720\ub3c4\ud558\uc5ec \uc774 \ub458\uc758 \uad00\uacc4\ub97c \ud574\uba85\ud558\uace0, \uc18c\ud504\ud2b8\ub9e5\uc2a4 \uc5b4\ud150\uc158\uc758 \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc911\uc694\uc131\uc744 \ubd84\uc11d\ud55c\ub2e4.", "motivation": "\uc18c\ud504\ud2b8\ub9e5\uc2a4 \uc5b4\ud150\uc158\uc758 \uacc4\uc0b0 \ubcf5\uc7a1\uc131\uacfc \uba54\ubaa8\ub9ac \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uace0\uc790 \ud558\uba70, \uc120\ud615 \uc5b4\ud150\uc158\uc758 \uc131\ub2a5 \uc800\ud558 \uc774\uc720\ub97c \uc774\ud574\ud558\uace0\uc790 \ud55c\ub2e4.", "method": "\uc18c\ud504\ud2b8\ub9e5\uc2a4 \uc5b4\ud150\uc158\uc758 \uc7ac\uadc0\uc801 \ud615\ud0dc\ub97c \uc720\ub3c4\ud558\uace0, \uc774\ub97c \ud1b5\ud574 \uad6c\uc131 \uc694\uc18c\uc758 \uc0c1\ud638\uc791\uc6a9\uacfc \uc911\uc694\uc131\uc744 \ubd84\uc11d\ud55c\ub2e4.", "result": "\uc18c\ud504\ud2b8\ub9e5\uc2a4 \uc5b4\ud150\uc158\uc740 \uc120\ud615 \uc5b4\ud150\uc158\uc5d0 \ube44\ud574 \ub354 \ud45c\ud604\ub825\uc774 \ub6f0\ub098\ub2e4\ub294 \uac83\uc744 \ubc1d\ud788\uace0, \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uc5ed\ud560\uc744 \uc124\uba85\ud55c\ub2e4.", "conclusion": "\uc18c\ud504\ud2b8\ub9e5\uc2a4 \uc5b4\ud150\uc158\uc758 \ubaa8\ub378\uc774 RNN \ud615\ud0dc\ub85c \uc124\uba85\ub420 \uc218 \uc788\uc73c\uba70, \uc774\ub85c \uc778\ud574 \ubcf4\ub2e4 \ub098\uc740 \uc774\ud574\uac00 \uac00\ub2a5\ud558\ub2e4\ub294 \uc810\uc744 \uac15\uc870\ud55c\ub2e4."}}
{"id": "2507.23665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23665", "abs": "https://arxiv.org/abs/2507.23665", "authors": ["Amal Saadallah"], "title": "SHAP-Guided Regularization in Machine Learning Models", "comment": null, "summary": "Feature attribution methods such as SHapley Additive exPlanations (SHAP) have\nbecome instrumental in understanding machine learning models, but their role in\nguiding model optimization remains underexplored. In this paper, we propose a\nSHAP-guided regularization framework that incorporates feature importance\nconstraints into model training to enhance both predictive performance and\ninterpretability. Our approach applies entropy-based penalties to encourage\nsparse, concentrated feature attributions while promoting stability across\nsamples. The framework is applicable to both regression and classification\ntasks. Our first exploration started with investigating a tree-based model\nregularization using TreeSHAP. Through extensive experiments on benchmark\nregression and classification datasets, we demonstrate that our method improves\ngeneralization performance while ensuring robust and interpretable feature\nattributions. The proposed technique offers a novel, explainability-driven\nregularization approach, making machine learning models both more accurate and\nmore reliable.", "AI": {"tldr": "SHAP \uae30\ubc18\uc758 \uc815\uaddc\ud654 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \uc81c\uc548\ud558\uc5ec \uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc758 \uc608\uce21 \uc131\ub2a5\uacfc \ud574\uc11d \uac00\ub2a5\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4.", "motivation": "\uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc758 \ud574\uc11d \uac00\ub2a5\uc131 \ud5a5\uc0c1 \ubc0f \ucd5c\uc801\ud654 \ubc29\ud5a5\uc758 \ud0d0\uc0c9 \ud544\uc694\uc131.", "method": "SHAP\ub97c \uc774\uc6a9\ud558\uc5ec \uac01 \ud2b9\uc131\uc758 \uc911\uc694\uc131\uc744 \uc81c\uc57d\uc73c\ub85c \ud558\ub294 \uc815\uaddc\ud654 \ud504\ub808\uc784\uc6cc\ud06c \ub3c4\uc785, \uc5d4\ud2b8\ub85c\ud53c \uae30\ubc18 \ud328\ub110\ud2f0 \uc801\uc6a9.", "result": "\ubca4\uce58\ub9c8\ud06c \ud68c\uadc0 \ubc0f \ubd84\ub958 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc77c\ubc18\ud654 \uc131\ub2a5 \uac1c\uc120\uc744 \uc785\uc99d.", "conclusion": "\uc81c\uc548\ub41c \uae30\uc220\uc740 \uae30\uacc4 \ud559\uc2b5 \ubaa8\ub378\uc758 \uc815\ud655\uc131\uacfc \uc2e0\ub8b0\uc131\uc744 \ub192\uc774\ub294 \uc0c8\ub85c\uc6b4 \uc815\uaddc\ud654 \uc811\uadfc\ubc95\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
{"id": "2507.23674", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23674", "abs": "https://arxiv.org/abs/2507.23674", "authors": ["Muhammad Taha Cheema", "Abeer Aamir", "Khawaja Gul Muhammad", "Naveed Anwar Bhatti", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses", "comment": "13 pages, 9 figures", "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.", "AI": {"tldr": "TweakLLM\uc740 \uc0ac\uc6a9\uc790 \ucffc\ub9ac\uc5d0 \ub9de\uac8c \uc751\ub2f5 \uce90\uc2f1\uc744 \ub3d9\uc801\uc73c\ub85c \uc870\uc815\ud558\uc5ec LLM\uc758 \uc751\ub2f5 \ud488\uc9c8\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uce90\uc2dc \ud6a8\uc728\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uc0c8\ub85c\uc6b4 \ub77c\uc6b0\ud305 \uc544\ud0a4\ud14d\ucc98\uc774\ub2e4.", "motivation": "LLM\uc758 \ube44\uc6a9\uacfc \uc9c0\uc5f0 \uc2dc\uac04\uc744 \uc904\uc774\uae30 \uc704\ud574 \ud6a8\uc728\uc801\uc778 \uc751\ub2f5 \uce90\uc2f1\uc774 \ud544\uc694\ud558\ub2e4.", "method": "\uacbd\ub7c9 LLM\uc744 \uc0ac\uc6a9\ud558\uc5ec \uce90\uc2dc\ub41c \uc751\ub2f5\uc744 \ub3d9\uc801\uc73c\ub85c \uc870\uc815\ud558\ub294 \ub77c\uc6b0\ud305 \uc544\ud0a4\ud14d\ucc98\uc778 TweakLLM\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "result": "TweakLLM\uc740 \uc0ac\uc6a9\uc790 \uc5f0\uad6c \ubc0f \ub2e4\uc218\uc758 \ud3c9\uac00\ub97c \ud1b5\ud574 \uc751\ub2f5 \ud488\uc9c8\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uce90\uc2dc \ud6a8\uacfc\uc131\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "conclusion": "TweakLLM\uc740 \uc0ac\uc6a9\uc790 \uacbd\ud5d8\uc744 \uc800\ud574\ud558\uc9c0 \uc54a\uc73c\uba74\uc11c \uace0\uc6a9\ub7c9 LLM \ubc30\ud3ec\ub97c \uc704\ud55c \ud655\uc7a5 \uac00\ub2a5\ud558\uace0 \uc790\uc6d0 \ud6a8\uc728\uc801\uc778 \uce90\uc2f1 \uc194\ub8e8\uc158\uc73c\ub85c \uc790\ub9ac\uc7a1\ub294\ub2e4."}}
{"id": "2507.23675", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23675", "abs": "https://arxiv.org/abs/2507.23675", "authors": ["Tianyi Chen", "Haitong Ma", "Na Li", "Kai Wang", "Bo Dai"], "title": "One-Step Flow Policy Mirror Descent", "comment": null, "summary": "Diffusion policies have achieved great success in online reinforcement\nlearning (RL) due to their strong expressive capacity. However, the inference\nof diffusion policy models relies on a slow iterative sampling process, which\nlimits their responsiveness. To overcome this limitation, we propose Flow\nPolicy Mirror Descent (FPMD), an online RL algorithm that enables 1-step\nsampling during policy inference. Our approach exploits a theoretical\nconnection between the distribution variance and the discretization error of\nsingle-step sampling in straight interpolation flow matching models, and\nrequires no extra distillation or consistency training. We present two\nalgorithm variants based on flow policy and MeanFlow policy parametrizations,\nrespectively. Extensive empirical evaluations on MuJoCo benchmarks demonstrate\nthat our algorithms show strong performance comparable to diffusion policy\nbaselines while requiring hundreds of times fewer function evaluations during\ninference.", "AI": {"tldr": "FPMD\ub294 \uc628\ub77c\uc778 \uac15\ud654\ud559\uc2b5\uc5d0\uc11c \ud655\uc0b0 \uc815\ucc45 \ubaa8\ub378\uc758 \ub290\ub9b0 \uc0d8\ud50c\ub9c1 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\uae30 \uc704\ud55c \uc54c\uace0\ub9ac\uc998\uc73c\ub85c, 1\ub2e8\uacc4 \uc0d8\ud50c\ub9c1\uc744 \uac00\ub2a5\ud558\uac8c \ud558\uc5ec \ud6a8\uc728\uc131\uc744 \ub192\uc778\ub2e4.", "motivation": "\ud655\uc0b0 \uc815\ucc45 \ubaa8\ub378\uc774 \uc628\ub77c\uc778 \uac15\ud654\ud559\uc2b5\uc5d0\uc11c \ud070 \uc131\uacf5\uc744 \uac70\ub450\uc5c8\uc9c0\ub9cc, \ub290\ub9b0 \ubc18\ubcf5 \uc0d8\ud50c\ub9c1 \uacfc\uc815\uc774 \ubc18\uc751\uc131\uc744 \uc81c\ud55c\ud55c\ub2e4\ub294 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uace0\uc790 \ud55c\ub2e4.", "method": "Flow Policy Mirror Descent (FPMD) \uc54c\uace0\ub9ac\uc998\uc744 \uc81c\uc548\ud558\uba70, \ud750\ub984 \uc815\ucc45\uacfc MeanFlow \uc815\ucc45 \ub9e4\uac1c\ubcc0\uc218\ud654\ub97c \uae30\ubc18\uc73c\ub85c \ud55c \ub450 \uac00\uc9c0 \uc54c\uace0\ub9ac\uc998 \ubcc0\ud615\uc744 \uc18c\uac1c\ud55c\ub2e4.", "result": "FPMD\ub294 MuJoCo \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8 \ud3c9\uac00\ub97c \ud1b5\ud574 \ud655\uc0b0 \uc815\ucc45 \uae30\uc900\uacfc \ube44\uad50\ud558\uc5ec \uac15\ub825\ud55c \uc131\ub2a5\uc744 \ubc1c\ud718\ud558\uba70, \ud544\uc694\ub85c \ud558\ub294 \ud568\uc218 \ud3c9\uac00 \uc218\uac00 \uc218\ubc31 \ubc30 \uc801\ub2e4.", "conclusion": "FPMD\ub294 \uae30\uc874\uc758 \ud655\uc0b0 \uc815\ucc45\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \ud6a8\uc728\uc801\uc778 \uc0d8\ud50c\ub9c1 \uacfc\uc815\uc744 \uc81c\uacf5\ud558\uba70 \uc628\ub77c\uc778 RL \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4."}}
{"id": "2507.23676", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23676", "abs": "https://arxiv.org/abs/2507.23676", "authors": ["Rabeya Tus Sadia", "Qiang Cheng"], "title": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data", "comment": null, "summary": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation.", "AI": {"tldr": "DepMicroDiff\ub294 \ub9c8\uc774\ud06c\ub85c\ubc14\uc774\uc634 \ub370\uc774\ud130\uc758 \uc815\ud655\ud55c \ubcf4\uac04\uc744 \uc704\ud55c \uc0c8\ub85c\uc6b4 \ud504\ub808\uc784\uc6cc\ud06c\ub85c, \uc0c1\ud638 \uc758\uc874\uc131 \ubc0f \uc790\uc728 \ud68c\uadc0 \uad00\uacc4\ub97c \ud3ec\ucc29\ud558\uc5ec \uae30\uc874 \ubc29\ubc95\ubcf4\ub2e4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4.", "motivation": "\ub9c8\uc774\ud06c\ub85c\ubc14\uc774\uc634 \ub370\uc774\ud130 \ubd84\uc11d\uc740 \ud638\uc2a4\ud2b8\uc758 \uac74\uac15\uacfc \uc9c8\ubcd1 \uc774\ud574\uc5d0 \ud544\uc218\uc801\uc774\ub098, \ub370\uc774\ud130\uc758 \ud76c\uc18c\uc131\uacfc \uc18c\uc74c\uc73c\ub85c \uc778\ud574 \ubcf4\uac04 \uc815\ud655\uc131\uc774 \uc800\ud558\ub418\uace0 \ubc14\uc774\uc624\ub9c8\ucee4 \ubc1c\uacac \ub4f1\uc758 \ud6c4\uc18d \uc791\uc5c5\uc774 \ubc29\ud574\ubc1b\ub294\ub2e4.", "method": "Diffusion \uae30\ubc18 \uc0dd\uc131 \ubaa8\ub378\ub9c1\uacfc Dependency-Aware Transformer(DAT)\ub97c \uacb0\ud569\ud558\uc5ec \uc30d \uc0ac\uc774 \uc758\uc874\uc131\uacfc \uc790\uc728 \ud68c\uadc0 \uad00\uacc4\ub97c \uba85\uc2dc\uc801\uc73c\ub85c \ucea1\ucc98\ud558\uba70, VAE \uae30\ubc18 \uc0ac\uc804 \ud6c8\ub828\uc744 \ud1b5\ud574 \ud658\uc790 \uba54\ud0c0\ub370\uc774\ud130\ub97c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\ub85c \uc778\ucf54\ub529\ud558\uc5ec \ubcf4\uac15\ud55c\ub2e4.", "result": "TCGA \ub9c8\uc774\ud06c\ub85c\ubc14\uc774\uc634 \ub370\uc774\ud130\uc14b \uc2e4\ud5d8\uc5d0\uc11c DepMicroDiff\ub294 \uae30\uc874 \ucd5c\ucca8\ub2e8 \ubaa8\ub378\ubcf4\ub2e4 \uc6d4\ub4f1\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, \ud53c\uc5b4\uc2a8 \uc0c1\uad00\uacc4\uc218 0.712, \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4 0.812, RMSE \ubc0f MAE \uac10\uc18c\ub97c \uae30\ub85d\ud558\uc600\ub2e4.", "conclusion": "DepMicroDiff\ub294 \ub9c8\uc774\ud06c\ub85c\ubc14\uc774\uc634 \ubcf4\uac04\uc744 \uc704\ud55c \uac15\uac74\uc131\uacfc \uc77c\ubc18\ud654 \uac00\ub2a5\uc131\uc744 \uc785\uc99d\ud558\uc600\ub2e4."}}
{"id": "2507.23712", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23712", "abs": "https://arxiv.org/abs/2507.23712", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Anomalous Samples for Few-Shot Anomaly Detection", "comment": null, "summary": "Several anomaly detection and classification methods rely on large amounts of\nnon-anomalous or \"normal\" samples under the assump- tion that anomalous data is\ntypically harder to acquire. This hypothesis becomes questionable in Few-Shot\nsettings, where as little as one anno- tated sample can make a significant\ndifference. In this paper, we tackle the question of utilizing anomalous\nsamples in training a model for bi- nary anomaly classification. We propose a\nmethodology that incorporates anomalous samples in a multi-score anomaly\ndetection score leveraging recent Zero-Shot and memory-based techniques. We\ncompare the utility of anomalous samples to that of regular samples and study\nthe benefits and limitations of each. In addition, we propose an\naugmentation-based validation technique to optimize the aggregation of the\ndifferent anomaly scores and demonstrate its effectiveness on popular\nindustrial anomaly detection datasets.", "AI": {"tldr": "\ubcf8 \ub17c\ubb38\uc740 \uad50\uc721 \uacfc\uc815\uc5d0\uc11c \ube44\uc815\uc0c1 \uc0d8\ud50c\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\ub860\uc744 \uc81c\uc548\ud558\uba70, \uc774\ub97c \ud1b5\ud574 Binary Anomaly Classification\uc744 \uac1c\uc120\ud558\ub294 \ubc29\ubc95\uc744 \ud0d0\uad6c\ud55c\ub2e4.", "motivation": "\uc804\ud1b5\uc801\uc778 \uc774\uc0c1 \ud0d0\uc9c0 \ubc29\ubc95\uc740 \ube44\uc815\uc0c1 \ub370\uc774\ud130\uac00 \uc218\uc9d1\ud558\uae30 \uc5b4\ub824\uc6b4 \uacbd\uc6b0 \uc815\uc0c1 \uc0d8\ud50c\uc744 \ub300\ub7c9\uc73c\ub85c \ud544\uc694\ub85c \ud55c\ub2e4\ub294 \uac00\uc815\uc5d0 \uae30\ubc18\ud55c\ub2e4.", "method": "\ube44\uc815\uc0c1 \uc0d8\ud50c\uc744 \ub2e4\uc911 \uc810\uc218 \uc774\uc0c1 \ud0d0\uc9c0 \uc810\uc218\uc5d0 \ud1b5\ud569\ud558\ub294 \ubc29\ubc95\ub860\uc744 \uc81c\uc548\ud558\uba70, \ucd5c\uadfc\uc758 Zero-Shot \ubc0f \uba54\ubaa8\ub9ac \uae30\ubc18 \uae30\uc220\uc744 \ud65c\uc6a9\ud55c\ub2e4.", "result": "\ube44\uc815\uc0c1 \uc0d8\ud50c\uc758 \uc720\uc6a9\uc131\uc744 \uc815\uaddc \uc0d8\ud50c\uacfc \ube44\uad50\ud558\uace0 \uac01 \uc0d8\ud50c\uc758 \uc7a5\ub2e8\uc810\uc744 \ubd84\uc11d\ud55c\ub2e4.", "conclusion": "\uc81c\uc548\ub41c \ubc29\ubc95\ub860\uc740 \uc778\uae30 \uc788\ub294 \uc0b0\uc5c5 \uc774\uc0c1 \ud0d0\uc9c0 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ud6a8\uacfc\uc131\uc744 \uc785\uc99d\ud588\uc73c\uba70, \ub2e4\uc591\ud55c \uc774\uc0c1 \uc810\uc218\ub97c \ucd5c\uc801\ud654\ud558\ub294 \uc99d\uac15 \uae30\ubc18 \uac80\uc99d \uae30\ubc95\uc744 \ud3ec\ud568\ud55c\ub2e4."}}
{"id": "2507.23756", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.23756", "abs": "https://arxiv.org/abs/2507.23756", "authors": ["Diana Mortagua"], "title": "Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System", "comment": "Master's thesis", "summary": "This study centers on overcoming the challenge of selecting the best\nannotators for each query in Active Learning (AL), with the objective of\nminimizing misclassifications. AL recognizes the challenges related to cost and\ntime when acquiring labeled data, and decreases the number of labeled data\nneeded. Nevertheless, there is still the necessity to reduce annotation errors,\naiming to be as efficient as possible, to achieve the expected accuracy faster.\nMost strategies for query-annotator pairs do not consider internal factors that\naffect productivity, such as mood, attention, motivation, and fatigue levels.\nThis work addresses this gap in the existing literature, by not only\nconsidering how the internal factors influence annotators (mood and fatigue\nlevels) but also presenting a new query-annotator pair strategy, using a\nKnowledge-Based Recommendation System (RS). The RS ranks the available\nannotators, allowing to choose one or more to label the queried instance using\ntheir past accuracy values, and their mood and fatigue levels, as well as\ninformation about the instance queried. This work bases itself on existing\nliterature on mood and fatigue influence on human performance, simulating\nannotators in a realistic manner, and predicting their performance with the RS.\nThe results show that considering past accuracy values, as well as mood and\nfatigue levels reduces the number of annotation errors made by the annotators,\nand the uncertainty of the model through its training, when compared to not\nusing internal factors. Accuracy and F1-score values were also better in the\nproposed approach, despite not being as substantial as the aforementioned. The\nmethodologies and findings presented in this study begin to explore the open\nchallenge of human cognitive factors affecting AL.", "AI": {"tldr": "\uc774 \uc5f0\uad6c\ub294 \ub2a5\ub3d9 \ud559\uc2b5\uc5d0\uc11c \ucd5c\uc801\uc758 \uc8fc\uc11d\uc790\ub97c \uc120\ud0dd\ud558\ub294 \ubb38\uc81c\ub97c \ub2e4\ub8e8\uba70, \uc8fc\uc11d \uc624\ub958\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ub370 \ubaa9\ud45c\ub97c \ub450\uace0 \uc788\ub2e4.", "motivation": "\ube44\uc6a9\uacfc \uc2dc\uac04 \ubb38\uc81c\ub85c \uc778\ud574 \ub77c\ubca8\uc774 \ub2ec\ub9b0 \ub370\uc774\ud130\ub97c \uc5bb\ub294 \ub370 \uc5b4\ub824\uc6c0\uc774 \uc788\uc73c\uba70, \uc774\uc5d0 \ub530\ub77c \ub77c\ubca8\uc774 \ud544\uc694\ud55c \ub370\uc774\ud130 \uc591\uc744 \uc904\uc774\ub294 \uac83\uc774 \ud544\uc694\ud558\ub2e4.", "method": "\uae30\uc874 \ubb38\ud5cc\uc744 \ud1a0\ub300\ub85c, \uae30\ubd84\uacfc \ud53c\ub85c \ub4f1\uc758 \ub0b4\ubd80 \uc694\uc778\uc774 \uc8fc\uc11d\uc790\uc5d0\uac8c \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \uace0\ub824\ud558\uc5ec \uc9c0\uc2dd \uae30\ubc18 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc744 \ud65c\uc6a9\ud55c \uc0c8\ub85c\uc6b4 \ucffc\ub9ac-\uc8fc\uc11d\uc790 \ud398\uc5b4 \uc804\ub7b5\uc744 \uc81c\uc2dc\ud55c\ub2e4.", "result": "\uc8fc\uc11d\uc790\uc758 \uacfc\uac70 \uc815\ud655\ub3c4, \uae30\ubd84 \ubc0f \ud53c\ub85c \uc218\uc900\uc744 \uace0\ub824\ud568\uc73c\ub85c\uc368 \uc8fc\uc11d \uc624\ub958 \uc218\ub97c \uc801\uac8c \ud558\uace0 \ubaa8\ub378\uc758 \ubd88\ud655\uc2e4\uc131\uc744 \uc904\uc600\ub2e4.", "conclusion": "\uc778\uac04\uc758 \uc778\uc9c0 \uc694\uc778\uc774 \ub2a5\ub3d9 \ud559\uc2b5\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ud0d0\uad6c\ud558\ub294 \uc2dc\uc791\uc810\uc744 \uc81c\uacf5\ud55c\ub2e4."}}
