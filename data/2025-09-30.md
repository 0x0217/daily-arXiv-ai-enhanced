<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: 이 연구는 체인 오브 씽킹(CoT) 추론의 중간 사고 과정을 검증하기 위한 새로운 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 사고 과정을 효과적으로 검증하는 문제를 해결하기 위해.

Method: 올바른 추론 경로가 반드시 방문해야 하는 최소한의 검증 가능한 체크포인트인 결정 피벗을 도입하고, 다양한 추론 경로를 샘플링하며, 각 추적을 피벗 중심의 짧은 경로로 압축하고, 자가 생성된 출력을 사용하여 모델을 후학습 시키는 파이프라인을 제안한다.

Result: LogiQA, MedQA, MATH500과 같은 표준 벤치마크에서 제안된 방법의 효과를 입증한다.

Conclusion: 제안된 방법은 실제 정답 데이터나 외부 메트릭 없이 추론을 정렬할 수 있는 가능성을 보여준다.

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [2] [AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need](https://arxiv.org/abs/2509.21553)
*Ahmed Jaber,Wangshu Zhu,Karthick Jayavelu,Justin Downes,Sameer Mohamed,Candace Agonafir,Linnia Hawkins,Tian Zheng*

Main category: cs.AI

TL;DR: 기후 데이터 과학의 장벽을 극복하기 위한 지식 그래프와 AI 에이전트를 통합한 개념 증명을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기후 데이터 과학의 데이터 소스 분산, 형식 이질성, 높은 기술적 전문성의 장벽을 극복하고자 한다.

Method: 큐레이션된 지식 그래프와 클라우드 기반 과학 워크플로우를 위한 AI 에이전트를 통합하는 방법을 제안한다.

Result: AI 에이전트는 자연어 상호작용과 자동화된 데이터 접근을 가능하게 하여 데이터 분석의 기술적 장벽을 낮춘다.

Conclusion: 우리 시스템의 오픈 소스 설계는 지식 그래프와 관련 도구가 공동체 기여를 통해 발전할 수 있도록 지원한다.

Abstract: Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.

</details>


### [3] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: GeoEvolve는 지리적 도메인 지식을 결합하여 지리공간 알고리즘을 자동으로 설계하고 개선하는 다중 에이전트 LLM 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 지속 가능성과 기후 변화와 같은 전 세계적 도전에 대한 솔루션을 제공하기 위해 지리공간 모델링이 중요하다.

Method: GeoEvolve는 두 개의 중첩 루프에서 작동하며, 내부 루프는 후보 솔루션을 생성하고 변형하는 코드 진화기를 활용하고, 외부 제어자는 글로벌 엘리트를 평가하고 지리 지식을 주입하는 구조화된 지식 기반인 GeoKnowRAG 모듈에 질의를 보낸다.

Result: GeoEvolve는 공간 보간(크리깅)과 공간 불확실성 정량화(지리공간 일치 예측)와 같은 두 가지 기본 작업에서 새로운 알고리즘을 자동으로 개선하고 발견한다.

Conclusion: GeoEvolve는 자동화된 지식 기반 지리공간 모델링을 위한 확장 가능한 경로를 제공하며, 신뢰할 수 있고 효율적인 AI-과학 발견을 위한 새로운 기회를 열어준다.

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [4] [Automated and Interpretable Survival Analysis from Multimodal Data](https://arxiv.org/abs/2509.21600)
*Mafalda Malafaia,Peter A. N. Bosman,Coen Rasch,Tanja Alderliesten*

Main category: cs.AI

TL;DR: 이 논문은 임상 변수와 컴퓨터 단층 촬영 이미지를 통합하여 생존 분석을 자동화하는 해석 가능한 다중 모달 AI 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 정확하고 해석 가능한 생존 분석은 종양학의 핵심 도전 과제입니다. 증가하는 다중 모달 데이터와 검증과 신뢰를 지원하기 위한 투명한 모델에 대한 임상적 필요는 이 도전 과제를 더욱 복잡하게 만듭니다.

Method: 우리는 MultiFIX 기반 프레임워크를 사용하여 생존과 관련된 특징을 추론하고, 이미징 특징은 Grad-CAM을 통해 해석하며, 임상 변수는 유전 프로그래밍을 통해 기호 표현으로 모델링합니다.

Result: MultiFIX는 머리와 목 암을 위한 오픈 소스 RADCURE 데이터셋을 사용하여 0.838(예측) 및 0.826(층화)의 C-지수를 달성하여 임상 및 학술적 기준 접근법을 능가하고 알려진 예후 마커와 일치합니다.

Conclusion: 이러한 결과는 MultiFIX와 함께 해석 가능한 다중 모달 AI가 정밀 종양학을 위한 약속을 가지고 있음을 강조합니다.

Abstract: Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.

</details>


### [5] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: AI가 물리적 세계와 상호작용할 때 새로운 안전 문제들이 발생하고, 본 논문에서는 Embodied AI 시스템의 안전성을 평가하기 위한 새로운 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI가 물리적 세계와 상호작용할 때 물리적 안전에 대한 새로운 도전이 생기고 있습니다.

Method: 현실 세계의 부상 사례와 안전 운영 제약에 기반한 지속적인 물리적 안전 벤치마킹 접근 방식을 개발하고, 다중 모드 안전 이해를 조사하기 위해 설명한 서사들을 포토리얼리스틱 이미지와 비디오로 변환합니다.

Result: 주요 기본 모델들이 위험을 인식하고, 안전에 대해 추론하며, 개입을 촉발하는 능력을 분석하여 안전 임무에 대한 배치 준비 상태에 대한 다각적 통찰을 제공합니다.

Conclusion: 모델들이 명시적으로 물리적 안전 제약에 대해 추론하도록 교육시키는 포스트 훈련 패러다임을 개발하였고, 그 결과 안전 추론을 해석 가능하고 투명하게 만드는 사고 흔적을 생성합니다.

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [6] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: 자율 에이전트는 최근에 많은 발전을 이루었지만, 대부분의 평가는 짧은 수평의 완전 관찰 작업에 중점을 두고 있다. 본 논문에서는 장기 수평과 부분 관찰 시나리오에서의 복잡한 현실 세계 문제에 대한 평가를 위한 새로운 벤치마크인 UltraHorizon을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 현실 세계 과제에 필요한 기본 역량을 체계적으로 평가할 수 있는 벤치마크가 부족하다.

Method: 세 가지 서로 다른 환경에서 탐사를 단일 임제로 사용하여 에이전트의 핵심 역량을 검증하는 장기 발견 작업을 설계하였다.

Result: 에이전트는 이러한 작업에서 지속적인 추론, 계획, 메모리 및 도구 사용을 통해 숨겨진 규칙을 반복적으로 발견해야 하며, LLM-에이전트는 이러한 설정에서 일관되게 부족한 성능을 보였다.

Conclusion: 간단한 스케일링이 우리의 작업에서는 실패하며, 우리는 에이전트의 실패를 설명하기 위해 수집된 궤적의 심층 분석을 수행하였다.

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [7] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: 이 논문은 MLLM을 위한 포괄적인 웹 이해 벤치마크인 WebRSSBench를 소개하며, 이는 추론, 강인성 및 안전성을 평가한다.


<details>
  <summary>Details</summary>
Motivation: MLLM이 GUI 에이전트 및 프론트엔드 코드 생성을 포함한 복잡한 웹 관련 응용 프로그램 구축의 AI 협력자로 자리잡고 있기 때문입니다.

Method: 8개의 작업을 통해 추론, 강인성 및 안전성을 함께 평가하는 WebRSSBench를 소개합니다.

Result: 12개의 MLLM을 WebRSSBench에서 평가한 결과, 모델은 여전히 현실적인 레이아웃에서의 조합적 및 교차 요소 추론에서 어려움을 겪으며, 사용자 인터페이스 및 콘텐츠의 변동에 대해 제한된 강인성을 보여주었다.

Conclusion: MLLM의 안전성 인식이 다소 보수적이며, 안전이 critical한 행동을 인식하고 피하는 데 어려움을 겪고 있다는 것입니다.

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [8] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: D-Artemis는 GUI 작업을 위해 인간의 인지 루프를 기반으로 한 새로운 프레임워크로, 개선된 의사 결정 메커니즘과 오류 예방 과정을 통해 높은 성능을 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 GUI 에이전트는 데이터 병목, 오류 탐지 지연 비용, 모순된 안내 위험 등의 문제로 발전에 제약을 받습니다.

Method: D-Artemis는 앱 특정 팁 검색 메커니즘을 활용하며, 사전 실행 정렬 단계를 통해 사고-행동 일관성 검증 및 행동 수정 에이전트를 통해 실행 실패 위험을 줄입니다.

Result: D-Artemis는 AndroidWorld에서 75.8%, ScreenSpot-V2에서 96.8%의 성공률을 기록하며, 주요 벤치마크에서 새로운 최첨단 결과를 달성했습니다.

Conclusion: 구성 요소 각각의 기여도를 입증하는 광범위한 절단 연구를 통해 D-Artemis의 효과가 확인되었습니다.

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [9] [ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration](https://arxiv.org/abs/2509.21823)
*Gaole Dai,Shiqi Jiang,Ting Cao,Yuqing Yang,Yuanchun Li,Rui Tan,Mo Li,Lili Qiu*

Main category: cs.AI

TL;DR: ProRe는 GUI 에이전트의 보상을 향상시키기 위한 능동적 보상 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)의 평가와 훈련에서 보상은 매우 중요하지만, 기존의 보상 방법들이 GUI 에이전트에 일반화하는 데 어려움이 있다.

Method: ProRe는 일반 목적의 추론기와 도메인 특화 평가자 에이전트를 활용하여 목표 상태 프로빙 작업을 조정하고, 평가자 에이전트가 환경과 능동적으로 상호작용하여 추가 관찰을 수집하도록 한다.

Result: ProRe는 3K 이상의 경로에 대한 실증 결과에서 보상 정확도와 F1 점수를 각각 최대 5.3%와 19.4% 개선하였다.

Conclusion: 최신 정책 에이전트와 ProRe를 통합하면 최대 22.4%의 성공률 향상을 가져온다.

Abstract: Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.

</details>


### [10] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: DS-STAR는 다양한 데이터 형식의 파일을 자동으로 분석하고, 분석 계획의 충분성을 검증하며, 간단한 실행 가능한 계획에서 출발하여 점진적으로 그것을 개선하는 데이터 과학 에이전트입니다.


<details>
  <summary>Details</summary>
Motivation: 데이터 기반 의사결정을 위한 데이터 과학의 중요성이 증가하고 있으나, 이는 복잡한 작업으로 여러 데이터 소스를 탐색하고 결과를 종합해야 하는 과정을 포함합니다.

Method: DS-STAR는 다양한 데이터 형식의 자동 탐색 및 추출을 위한 모듈과 LLM 기반의 검증 단계를 도입하며, 간단한 계획에서 시작하여 피드백을 바탕으로 점진적으로 개선하는 기법을 사용합니다.

Result: DS-STAR는 DABStep, KramaBench, DA-Code의 세 가지 벤치마크에서 최첨단 성능을 달성했습니다.

Conclusion: DS-STAR는 이질적인 형식의 여러 데이터 파일 처리와 같은 어려운 작업에서 특히 더 나은 성능을 보여줍니다.

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [11] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: DeepTravel은 자율 여행 계획 에이전트를 구축하기 위한 강화 학습 프레임워크로, 다양한 도구를 autonomously 계획하고 실행하며 반영할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 기존의 여행 계획 에이전트가 고정된 워크플로우와 수동 입력에 의존하고 있어 더 유연하고 자율적인 에이전트가 개발되지 않고 있다.

Method: DeepTravel은 다단계 추론에서 중간 행동을 탐색하고 검증하며 다듬기 위해 도구의 응답을 반영할 수 있는 자율 여행 계획 에이전트를 구축하는 강화 학습 프레임워크를 제안한다. 이를 위해 정보를 캐시하여 샌드박스 환경을 구성하고, 계층적 보상 모델링 시스템을 개발하며, 실패 경험 버퍼에서 주기적으로 재플레이하는 방법을 제안한다.

Result: DeepTravel은 소형 LLM(예: Qwen3 32B)이 기존의 프론티어 LLM인 OpenAI o1, o3 및 DeepSeek R1을 여행 계획 작업에서 능가함을 보여준다.

Conclusion: 모든 연구 결과는 DeepTravel의 접근법이 자율 여행 계획 에이전트를 구축하는 데 효과적임을 입증한다.

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [12] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델(LLM) 기반의 다중 에이전트 시스템에서의 긴급 행동을 조사하는 방법론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 활용한 다중 에이전트 시스템에서의 행동 이해는 중요한 연구 과제지만, 실험 방법론이 부족하여 진행이 제한적이다.

Method: Shachi라는 포괄적인 방법론과 모듈형 프레임워크를 소개하며, 에이전트의 정책을 핵심 인지 구성 요소로 분해하고 이를 LLM 추론 엔진이 조율한다.

Result: 10개 과제 벤치마크에서 방법론을 검증하고, 새로운 과학적 탐구를 통해 그 효과를 입증한다.

Conclusion: 정확한 인지 아키텍처 구성이 이루어질 때 에이전트 행동이 실제 시장 반응과 일치함을 보여주며, 연구를 위한 엄격하고 개방적인 기반을 제공한다.

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [13] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: 효과적인 다중 에이전트 협업에는 정확한 계획과 협력자의 의도를 추론하는 능력이 필요하다. 본 논문은 LLM을 위한 새로운 협업 프레임워크인 CoBel-World를 제안하고, 이 프레임워크가 커뮤니케이션 비용을 크게 줄이고 작업 완료 효율성을 향상시키는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 실제 상황에서의 효과적인 다중 에이전트 협업은 정확한 계획뿐만 아니라 협력자의 의도에 대한 추론 능력도 요구된다.

Method: CoBel-World는 LLM 에이전트를 위한 협업 신념 세계를 제공하며, 물리적 환경과 협력자의 정신 상태를 공동으로 모델링한다.

Result: CoBel-World는 커뮤니케이션 비용을 22-60% 감소시키고 작업 완료 효율성을 4-28% 향상시킨다.

Conclusion: 명시적이고 의도를 인식하는 신념 모델링은 LLM 기초 다중 에이전트 시스템에서 효율적이고 인간과 유사한 협업을 위해 필수적이다.

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [14] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: RISK라는 새로운 프레임워크를 도입하여 전통적인 스크래핑 방법으로 처리할 수 없는 다양한 웹 데이터를 수집하고 관리하여 전자상거래의 리스크 평가를 개선한다.


<details>
  <summary>Details</summary>
Motivation: 전자상거래 리스크 관리는 다양한 깊이 있는 웹 데이터를 집계해야 하지만, 기존의 스크래핑 방법과 GUI 에이전트는 이를 처리하지 못한다.

Method: RISK는 8,492개의 단일 단계 및 2,386개의 다단계 상호작용 경로로 구성된 RISK-Data, 표준화된 평가를 위한 세 가지 난이도 수준의 벤치마크인 RISK-Bench, 네 가지 측면을 고려한 강화 미세 조정 프레임워크인 RISK-R1을 통합한다.

Result: 실험 결과 RISK-R1은 기존 기준보다 우수하여 오프라인 단일 단계에서 6.8%, 오프라인 다단계에서 8.8% 개선했다. 또한 온라인 평가에서 70.5%의 최고 작업 성공률을 달성했다.

Conclusion: RISK는 복잡한 웹 상호작용을 자동화하기 위한 확장 가능하고 도메인 특화된 솔루션을 제공하며, 전자상거래 리스크 관리의 최첨단을 발전시킨다.

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [15] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: Log2Plan은 GUI 작업 자동화를 위한 이론을 제안하며, 사용자 행동 로그에 기반한 구조적 두 단계 계획 프레임워크와 작업 채굴 접근 방식을 결합하여 강력하고 적응 가능한 자동화를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 또는 VLM 기반의 플레이너-실행기 에이전트는 UI 변경이나 복잡한 작업에서 취약성을 보이고, 따라서 GUI 작업 자동화의 필요성이 있습니다.

Method: Log2Plan은 사용자 명령을 구조적 작업 사전에 매핑하여 고수준 계획을 구성하고, 사용자 행동 로그에서 사용자 특정 패턴을 식별하는 작업 채굴 접근 방식을 사용합니다.

Result: Log2Plan은 200개의 실제 작업에 대해 평가되었으며, 작업 성공률과 실행 시간에서 상당한 개선을 보여주었습니다.

Conclusion: 특히 60.0% 이상의 성공률을 유지하여 복잡하고 다단계 작업 흐름에서의 강건성을 강조합니다.

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [16] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM(대형 언어 모델)의 에이전트적 추론 능력을 평가하기 위한 새로운 벤치를 제안합니다. 이 벤치에서는 LLM이 필요한 정보를 수집하며 초등학교 수준의 문제를 해결해야 합니다. GPT-5와 같은 최첨단 모델조차 67%의 정확도만 보여주는 것을 관찰했습니다. 이전에 방문했던 노드를 다시 방문하는 능력은 많은 모델에서 에이전트적 추론에 부족하다는 사실을 발견하였고, 이를 개선하기 위한 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 에이전트로서 점점 더 많이 사용됨에 따라 도구 사용과 추론을 결합하는 에이전트적 추론이 중요한 기술이 되었습니다.

Method: GSM-Agent라는 새로운 벤치를 구축하였으며, LLM 에이전트가 초등학교 수준의 문제를 처리하도록 하였습니다. 정보가 포함된 전제가 제공되지 않은 상태에서 질문만 제시됩니다.

Result: 최신 모델인 GPT-5조차도 67%의 정확도만을 달성하여 에이전트적 추론의 어려움을 드러냈습니다. 또한, 우리는 에이전트적 추론 패턴을 이해하기 위한 개념으로 에이전트적 추론 그래프를 제안하였습니다.

Conclusion: 벤치와 에이전트적 추론 프레임워크가 에이전트적 추론의 이해와 발전을 위한 미래 연구에 기여하기를 기대합니다.

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [17] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian Gröger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: 의료 데이터셋의 주석에 대한 불확실성을 고려하여 기계 학습 평가를 개선해야 한다.


<details>
  <summary>Details</summary>
Motivation: 클리니컬 데이터셋의 라벨은 아노테이터 간의 불일치로 인해 거의 확실하지 않으며, 각 케이스의 확신도가 균일하지 않기 때문에 이 문제를 해결할 필요가 있다.

Method: 이 논문에서는 주석의 불확실성을 다루기 위해 분포에 직접 작용하는 확률 기반 지표를 사용해야 한다고 주장한다.

Result: 의료 이미징 벤치마크에서 이진 라벨에 대한 신뢰도를 고려하면 모델 순위에 상당한 영향을 미친다.

Conclusion: 데이터셋의 원시 주석을 공개하고 불확실성을 인식하는 평가 방식 채택을 권장한다.

Abstract: Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.

</details>


### [18] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: PRIME은 인간의 인지 이론에 영감을 받아 설계된 다중 에이전트 추론 프레임워크로, 신속한 사고와 신중한 사고를 통합하여 LLM의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 인간의 사고 과정을 반영한 의사 결정 모델을 통해 LLM의 추론 능력을 개선하기 위해.

Method: PRIME은 빠른 답변 생성을 위한 신속 사고 에이전트(System 1)를 사용하고, 불확실성이 감지되면 계획, 가설 생성, 검색, 정보 통합 및 의사 결정을 위한 구조화된 System 2 추론 파이프라인을 작동시킵니다.

Result: 실험 결과, PRIME은 LLaMA 3 모델이 다중 홉 및 지식 기반 추론을 요구하는 벤치마크에서 GPT-4 및 GPT-4o와 경쟁할 수 있도록 한다.

Conclusion: 이 연구는 PRIME이 복잡하고 지식 집약적인 추론을 요구하는 도메인에서 LLM을 개선할 수 있는 확장 가능한 솔루션임을 확립합니다.

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [19] [Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents](https://arxiv.org/abs/2509.22391)
*Jiaqi Shao,Yuxiang Lin,Munish Prasad Lohani,Yufeng Miao,Bing Luo*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM 검색 에이전트의 인식 능력을 평가하기 위해 SeekBench라는 새로운 벤치마크를 도입하였다.


<details>
  <summary>Details</summary>
Motivation: 최근의 연구는 개방형 질문 응답을 위해 강화 학습을 통해 LLM 검색 에이전트를 훈련하는 방법을 탐구하였다.

Method: SeekBench는 LLM 검색 에이전트의 응답 트레이스를 단계별로 분석하여 그들의 인식 능력을 평가하는 최초의 벤치마크이다.

Result: SeekBench는 190개의 전문가 주석이 달린 트레이스와 1,800개 이상의 응답 단계를 포함하고 있다.

Conclusion: 이 벤치마크는 에이전트가 관찰된 증거에 기반한 추론 단계를 생성하는 능력, 낮은 품질의 결과에서 회복하기 위해 검색을 적응적으로 재구성하는 능력, 현재 증거가 답변을 제공하는 데 충분한지를 올바르게 평가하는 능력을 분석하는 데 도움을 준다.

Abstract: Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.

</details>


### [20] [GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](https://arxiv.org/abs/2509.22460)
*Shichao Weng,Zhiqiang Wang,Yuhua Zhou,Rui Lu,Ting Liu,Zhiyang Teng,Xiaozhang Liu,Hanmeng Liu*

Main category: cs.AI

TL;DR: GeoSketch는 기하학적 문제 해결을 상호작용적인 인지-추론-행동 루프로 변환하는 신경-상징적 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기하학 문제 해결(GPS)은 다중 모달 대형 언어 모델(MLLMs)에 독특한 도전을 제시하며, 텍스트와 다이어그램의 공동 해석과 반복적인 시각공간적 추론을 요구한다.

Method: GeoSketch는 인식 모듈, 상징적 추론 모듈 및 스케치 액션 모듈을 통합하여 기하학적 추론을 상호작용하는 루프로 재구성한다.

Result: GeoSketch는 강력한 MLLM 기반선과의 실험에서 단계별 추론 정확도 및 문제 해결 성공률을 유의미하게 개선한다.

Conclusion: GeoSketch는 정적 해석에서 동적이고 검증 가능한 상호작용으로 다중 모달 추론을 발전시켜 복잡한 시각공간 문제를 해결하기 위한 새로운 토대를 구축한다.

Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.

</details>


### [21] [InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios](https://arxiv.org/abs/2509.22502)
*Chenglin Yu,Yang Yu,Songmiao Wang,Yucheng Wang,Yifan Yang,Jinjia Li,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: InfiAgent는 복잡한 작업을 효율적으로 수행할 수 있는 피라미드 형태의 DAG 기반 다중 에이전트 프레임워크로, 다양한 혁신적 메커니즘을 통해 다양한 문제를 해결할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 에이전트를 개발하려면 세심하게 설계된 워크플로우와 도메인 전문성이 필요하지만, 이는 스케일과 비용 효율성을 저해한다.

Method: InfiAgent는 복잡한 에이전트를 계층적 다중 에이전트 시스템으로 자동 분해하는 '에이전트-도구' 메커니즘, 작업 완료의 품질과 안정성을 보장하는 이중 감사 메커니즘, 효율적인 작업-에이전트 매칭을 위한 라우팅 기능, 새로운 작업이나 최적화 기회에 따라 에이전트 DAG를 자율적으로 재구성하는 자기 진화 메커니즘을 포함한다.

Result: InfiAgent는 ADAS보다 9.9% 높은 성능을 달성했으며, AI 연구 보조인 InfiHelper의 사례 연구에서는 IEEE 최고 회의에서 인정을 받은 과학 논문을 생성하였다.

Conclusion: 이 프레임워크는 다양한 문제를 해결할 수 있는 다목적 피라미드 형태의 다중 에이전트 시스템으로 발전한다.

Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.

</details>


### [22] [Estimating the Empowerment of Language Model Agents](https://arxiv.org/abs/2509.22504)
*Jinyeop Song,Jeff Gore,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 언어 모델 에이전트의 평가를 위한 정보 이론적 평가 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델 에이전트의 능력 평가에 대한 필요성이 증가하고 있으며, 기존의 벤치마크 중심 평가 방식은 비용이 많이 소모된다.

Method: EELMA(언어 모델 에이전트의 권한 추정) 알고리즘을 통해 다중 턴 텍스트 상호작용에서 효과적인 권한을 근사한다.

Result: EELMA를 언어 게임과 현실적인 웹 탐색 시나리오에서 검증하였고, 권한이 평균 작업 성능과 강한 상관관계를 가진다는 것을 발견하였다.

Conclusion: 이 결과들은 권한이 복잡하고 열린 환경에서 LM 에이전트를 평가하고 모니터링하기 위한 매력적인 일반 목적의 지표임을 보여준다.

Abstract: As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.

</details>


### [23] [The Emergence of Altruism in Large-Language-Model Agents Society](https://arxiv.org/abs/2509.22537)
*Haoyang Li,Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 이 연구는 사회적 시뮬레이션에서 대형 언어 모델(LLM)의 역할과 이들이 나타내는 이타주의 및 이기주의의 사회적 경향을 규명합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 사용한 사회적 시뮬레이션에서, 이 모델들이 embody하는 사회적 논리를 이해하는 것이 중요합니다.

Method: 사회적 딜레마를 생성하는 Schelling 변형 도시 이주 모델을 도입하고, 200개 이상의 LLM 에이전트가 이기적 목표와 이타적 목표 간의 갈등을 해결하도록 합니다.

Result: LLM의 사회적 경향에서 근본적인 차이를 발견하고, '적응형 이기주의자'와 '이타적 최적화자'라는 두 가지 구별되는 유형을 식별합니다.

Conclusion: 사회적 시뮬레이션에 있어 모델 선택은 단순한 추론 능력을 선택하는 것이 아니라, 고유한 사회적 행동 논리를 선택하는 것이라고 제안합니다.

Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.

</details>


### [24] [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)
*Chenyu Zhou,Tianyi Xu,Jianghao Lin,Dongdong Ge*

Main category: cs.AI

TL;DR: 이 논문에서는 단계적 자기 발전 프레임워크인 StepORLM을 제안하여 대규모 언어 모델이 운영 연구 문제를 해결하는 데 있어 높은 성능을 발휘할 수 있도록 하였다.


<details>
  <summary>Details</summary>
Motivation: 기존의 강화 학습 기반 모델이 운영 연구 문제 해결 시 두 가지 주요 한계에 직면하고 있기 때문에, 이를 극복할 새로운 프레임워크가 필요하다.

Method: StepORLM은 정책 모델과 생성적 보상 모델(GenPRM)이 서로 반복적으로 발전하는 공동 진화 루프를 특성으로 한다.

Result: 8B 매개변수를 가진 StepORLM은 여섯 개 벤치마크에서 새로운 최첨단 성능을 확보하며, 기존의 일반 모델들보다도 우수한 성능을 보여준다.

Conclusion: 공동 진화한 GenPRM은 강력하고 보편적으로 적용 가능한 과정 검증기로 기능하여, 우리 모델과 기존 LLM의 추론 성능을 크게 향상시킨다.

Abstract: Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.

</details>


### [25] [UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](https://arxiv.org/abs/2509.22570)
*Qi Mao,Tinghan Yang,Jiahao Li,Bin Li,Libiao Jin,Yan Lu*

Main category: cs.AI

TL;DR: UniMIC은 클라우드 AI 에이전트와 엣지 장치 간의 상호작용을 개선하기 위한 통합 토큰 기반 다중 모달 인터랙티브 코딩 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 다중 모달 모델(LLM)과 클라우드 기반 AI 에이전트의 발전이 인간-AI 협업 방식을 변화시키고 있지만, 기존 코덱은 단일 모드 통신에 최적화되어 있어 제한된 성능을 보인다.

Method: UniMIC은 원시 픽셀이나 일반 텍스트 대신 압축된 토큰화된 표현을 사용하여 낮은 비트 전송률로 효율적인 전송을 가능하게 하며, 경량의 Transformer 기반 엔트로피 모델을 활용하여 inter-token 중복을 최소화한다.

Result: UniMIC는 텍스트-이미지 생성, 텍스트 유도 인페인팅, 아웃페인팅, 시각적 질의 응답에 대한 광범위한 실험을 통해 상당한 비트 전송률 절감을 달성하며, 초저 비트 전송률(<0.05bpp)에서도 강건성을 유지한다.

Conclusion: UniMIC는 차세대 다중 모달 인터랙티브 통신을 위한 실용적이고 미래 지향적인 패러다임으로 확립되었다.

Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI
agents is transforming human-AI collaboration into bidirectional, multimodal
interaction. However, existing codecs remain optimized for unimodal, one-way
communication, resulting in repeated degradation under conventional
compress-transmit-reconstruct pipelines. To address this limitation, we propose
UniMIC, a Unified token-based Multimodal Interactive Coding framework that
bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or
plain text, UniMIC employs compact tokenized representations as the
communication medium, enabling efficient low-bitrate transmission while
maintaining compatibility with LMMs. To further enhance compression,
lightweight Transformer-based entropy models with scenario-specific
designs-generic, masked, and text-conditioned-effectively minimize inter-token
redundancy. Extensive experiments on text-to-image generation, text-guided
inpainting, outpainting, and visual question answering show that UniMIC
achieves substantial bitrate savings and remains robust even at ultra-low
bitrates (<0.05bpp), without compromising downstream task performance. These
results establish UniMIC as a practical and forward-looking paradigm for
next-generation multimodal interactive communication.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
*Rushil Gupta,Jason Hartford,Bang Liu*

Main category: cs.LG

TL;DR: 대형 언어 모델(LLMs)은 실험 설계에 대한 일반적인 에이전트로 제안되었으나, 실험 피드백에 대한 민감성이 없음을 발견했다. 전통적인 방법들이 LLM 에이전트보다 일관되게 우수한 성능을 보였고, LLMNN이라는 간단한 하이브리드 방법을 제안하여 효과적인 실험 설계가 가능함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 실험 설계의 일반적인 에이전트로 기능할 수 있는지 평가하기 위해.

Method: 개 열린 소스와 폐쇄 소스의 지침 조정된 LLM을 유전자 교란 및 분자 속성 발견 작업에 적용함.

Result: LLM 기반 에이전트는 실험 피드백에 민감하지 않으며, 전통적인 방법들이 일관되게 LLM 에이전트보다 우수한 성능을 보임.

Conclusion: 현재의 개방 및 폐쇄 소스 LLM은 실제로 실험 설계를 수행하지 않으며, 업데이트된 사후 확률과 배치 획득에서 선험적 추론을 분리하는 하이브리드 프레임워크가 필요함을 강조한다.

Abstract: Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.

</details>


### [27] [Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data](https://arxiv.org/abs/2509.21465)
*George Yakushev,Alina Shutova,Ivan Rubachev,Renat Sergazinov,Artem Babenko*

Main category: cs.LG

TL;DR: 본 연구는 적은 양의 데이터로도 효과적으로 작동하는 결정 트리를 구축하기 위해 LLM을 활용하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 저자들은 적은 자원으로 데이터 문제를 해결하기 위해 기존의 블랙 박스 모델의 한계를 극복하고자 합니다.

Method: 이 연구는 LLM을 이용해 작은 데이터셋을 위한 결정 트리를 유도하는 방법을 제안하며, 이를 위한 도구 세트를 설계합니다.

Result: 이 결정 트리는 기존 CART 모델보다 더 나은 성능을 보이며, 인간이 이해할 수 있는 추론 과정을 제공합니다.

Conclusion: 또한, 이 과정은 인간의 개입을 통해 편향을 수정하거나 도메인 특유의 직관을 반영할 수 있는 기회를 제공합니다.

Abstract: Tabular foundation models are becoming increasingly popular for low-resource
tabular problems. These models make up for small training datasets by
pretraining on large volumes of synthetic data. The prior knowledge obtained
via pretraining provides the exceptional performance, but the resulting model
becomes a black box that is difficult to interpret and costly to inference. In
this work, we explore an alternative strategy: using reasoning-capable LLMs to
induce decision trees for small tabular datasets in agentic setup. We design a
minimal set of tools for constructing, analyzing and manipulating decision
trees. By using these tools, LLMs combine their prior knowledge with learning
from data to create a lightweight decision tree that outperforms traditional
CART on low-resource tabular problems. While a single decision tree does not
outperform state-of-the-art black box models, it comes with a human-readable
reasoning trace that can be checked for biases and data leaks. Furthermore, the
reasoning-based LLM's creation process allows for additional human input:
correcting biases or incorporating domain-specific intuition that is not
captured in the data.

</details>


### [28] [Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews](https://arxiv.org/abs/2509.21579)
*Mst Eshita Khatun,Halima Akter,Tasnimul Rehan,Toufiq Ahmed*

Main category: cs.LG

TL;DR: 이 연구는 온라인 쇼핑에서의 사기성 리뷰 탐지를 위한 기계 학습 접근 방식을 사용하여 소비자 신뢰를 회복하는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 사기성 리뷰의 존재가 소비자 신뢰를 훼손하고 판매자의 평판에 피해를 줄 수 있다는 문제를 해결하고자 한다.

Method: 아마존 제품 리뷰의 대규모 데이터셋을 활용하여 고급 빅 데이터 분석 및 기계 학습 접근 방식을 적용한다.

Result: 로지스틱 회귀가 90.35%의 정확도로 스팸 리뷰를 탐지하는 등 여러 기계 학습 분류기의 유용성을 보여준다.

Conclusion: 이 연구는 더 신뢰할 수 있고 투명한 온라인 쇼핑 환경에 기여한다.

Abstract: In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.

</details>


### [29] [Limitations on Safe, Trusted, Artificial General Intelligence](https://arxiv.org/abs/2509.21654)
*Rina Panigrahy,Vatsal Sharan*

Main category: cs.LG

TL;DR: 안전성, 신뢰성 및 인공지능 일반화(AGI) 사이에는 근본적인 불일치가 있으며, 안전하고 신뢰할 수 있는 AI 시스템은 AGI 시스템이 될 수 없음을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템에서 안전성, 신뢰성 및 AGI는 중요한 목표이지만, 이들 개념의 비공식적인 해석이 존재한다.

Method: 안전성, 신뢰성 및 AGI의 엄격한 수학적 정의를 제안하고 이들 간의 불일치를 입증한다.

Result: 안전하고 신뢰할 수 있는 AI 시스템은 인간이 쉽게 해결할 수 있는 작업 인스턴스를 해결할 수 없는 AGI 시스템이 될 수 없다.

Conclusion: 실제 배포에서는 안전성 및 신뢰성에 대한 대체적인 실용적 해석이 가능하지만, 이러한 개념의 엄격한 수학적 정의에 따라 성립하는 결과가 있다.

Abstract: Safety, trust and Artificial General Intelligence (AGI) are aspirational
goals in artificial intelligence (AI) systems, and there are several informal
interpretations of these notions. In this paper, we propose strict,
mathematical definitions of safety, trust, and AGI, and demonstrate a
fundamental incompatibility between them. We define safety of a system as the
property that it never makes any false claims, trust as the assumption that the
system is safe, and AGI as the property of an AI system always matching or
exceeding human capability. Our core finding is that -- for our formal
definitions of these notions -- a safe and trusted AI system cannot be an AGI
system: for such a safe, trusted system there are task instances which are
easily and provably solvable by a human but not by the system. We note that we
consider strict mathematical definitions of safety and trust, and it is
possible for real-world deployments to instead rely on alternate, practical
interpretations of these notions. We show our results for program verification,
planning, and graph reachability. Our proofs draw parallels to G\"odel's
incompleteness theorems and Turing's proof of the undecidability of the halting
problem, and can be regarded as interpretations of G\"odel's and Turing's
results.

</details>


### [30] [RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion](https://arxiv.org/abs/2509.21659)
*Siming Shan,Min Zhu,Youzuo Lin,Lu Lu*

Main category: cs.LG

TL;DR: RED-DiffEq은 물리 기반 역전 문제와 데이터 기반 학습을 통합한 새로운 계산 프레임워크로, 지구물리학에서의 전파파 역전 문제를 해결하는 데 사용된다.


<details>
  <summary>Details</summary>
Motivation: PDE에 의해 지배되는 역문제는 과학 및 엔지니어링 여러 응용에서 근본적이지만 비선형성, 잘 정의되지 않음, 노이즈에 대한 민감성으로 인해 큰 도전에 직면해 있다.

Method: RED-DiffEq은 PDE 지배 역문제에 대한 정규화 메커니즘으로 미리 훈련된 확산 모델을 활용하여 물리 기반 역전과 데이터 기반 학습을 통합한다.

Result: 우리 방법은 기존 방법에 비해 향상된 정확성과 강인성을 보여준다. 또한, 훈련되지 않은 더 복잡한 속도 모델에도 강한 일반화 능력을 보인다.

Conclusion: 우리 프레임워크는 다양한 PDE 지배 역문제에 직접 적용될 수 있다.

Abstract: Partial differential equation (PDE)-governed inverse problems are fundamental
across various scientific and engineering applications; yet they face
significant challenges due to nonlinearity, ill-posedness, and sensitivity to
noise. Here, we introduce a new computational framework, RED-DiffEq, by
integrating physics-driven inversion and data-driven learning. RED-DiffEq
leverages pretrained diffusion models as a regularization mechanism for
PDE-governed inverse problems. We apply RED-DiffEq to solve the full waveform
inversion problem in geophysics, a challenging seismic imaging technique that
seeks to reconstruct high-resolution subsurface velocity models from seismic
measurement data. Our method shows enhanced accuracy and robustness compared to
conventional methods. Additionally, it exhibits strong generalization ability
to more complex velocity models that the diffusion model is not trained on. Our
framework can also be directly applied to diverse PDE-governed inverse
problems.

</details>


### [31] [DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks](https://arxiv.org/abs/2509.21666)
*Joshua Salim,Jordan Yu,Xilei Zhao*

Main category: cs.LG

TL;DR: 이 논문은 복잡한 딥 러닝 모델에서 과적합 문제를 해결하기 위해 새로운 규제 방법인 DIM을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 딥 러닝 모델은 예측 작업에서 우수하지만, 복잡한 구조와 많은 매개변수로 인해 과적합이 발생하여 패턴을 학습하기보다 훈련 데이터를 기억하는 경향이 있다.

Method: DIM은 선형 기준선에 상대하여 위반을 처벌함으로써 단조성을 강제하여 복잡한 모델에서 예측 정확성을 향상시킨다.

Result: 다양한 신경망 아키텍처에 대한 실험 결과, 약한 단조성 제약 조건조차도 모델 성과를 일관되게 향상시킨다.

Conclusion: DIM은 모델 행동을 규제하고 과적합을 완화하여 딥 신경망의 예측 성능을 향상시킨다.

Abstract: While deep learning models excel at predictive tasks, they often overfit due
to their complex structure and large number of parameters, causing them to
memorize training data, including noise, rather than learn patterns that
generalize to new data. To tackle this challenge, this paper proposes a new
regularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep
Neural Networks (DIM), which maintains domain-informed monotonic relationships
in complex deep learning models to further improve predictions. Specifically,
our method enforces monotonicity by penalizing violations relative to a linear
baseline, effectively encouraging the model to follow expected trends while
preserving its predictive power. We formalize this approach through a
comprehensive mathematical framework that establishes a linear reference,
measures deviations from monotonic behavior, and integrates these measurements
into the training objective. We test and validate the proposed methodology
using a real-world ridesourcing dataset from Chicago and a synthetically
created dataset. Experiments across various neural network architectures show
that even modest monotonicity constraints consistently enhance model
performance. DIM enhances the predictive performance of deep neural networks by
applying domain-informed monotonicity constraints to regularize model behavior
and mitigate overfitting

</details>


### [32] [Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks](https://arxiv.org/abs/2509.21735)
*Houliang Zhou,Rong Zhou,Yangying Liu,Kanhao Zhao,Li Shen,Brian Y. Chen,Yu Zhang,Lifang He,Alzheimer's Disease Neuroimaging Initiative*

Main category: cs.LG

TL;DR: 이 논문은 알츠하이머병 진전을 예측하기 위한 해석 가능한 시공간 그래프 신경망 프레임워크를 제안하며, 이를 통해 주요 뇌 회로 이상을 식별하고 알츠하이머병의 신경생물학적 메커니즘에 대한 통찰을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 알츠하이머병 진행을 예측하기 위한 객관적인 신경영상 바이오마커를 식별하는 것은 시기적절한 개입을 위해 중요하다.

Method: 이 연구는 이중 확률 미분 방정식(SDE)을 활용하여 불규칙하게 샘플링된 장기적 기능적 자기공명영상(fMRI) 데이터를 모델링하기 위해 해석 가능한 시공간 그래프 신경망 프레임워크를 개발하였다.

Result: 이 프레임워크는 질병 진행과 관련된 주요 뇌 회로 이상을 식별할 수 있는 희소한 지역 및 연결 중요성 확률을 효과적으로 학습하며, 인지와 관련된 주목할 만한 지역을 탐지했다.

Conclusion: 이 연구는 불규칙하게 샘플링된 장기적 이미징 데이터의 맥락에서도 알츠하이머병 진행의 조기 개인화를 예측하기 위한 시공간 그래프 기반 학습의 잠재력을 강조한다.

Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease
(AD) progression is crucial for timely intervention. However, this task remains
challenging due to the complex dysfunctions in the spatio-temporal
characteristics of underlying brain networks, which are often overlooked by
existing methods. To address these limitations, we develop an interpretable
spatio-temporal graph neural network framework to predict future AD
progression, leveraging dual Stochastic Differential Equations (SDEs) to model
the irregularly-sampled longitudinal functional magnetic resonance imaging
(fMRI) data. We validate our approach on two independent cohorts, including the
Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease
Neuroimaging Initiative (ADNI). Our framework effectively learns sparse
regional and connective importance probabilities, enabling the identification
of key brain circuit abnormalities associated with disease progression.
Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal
lobule as salient regions, with significant disruptions in the ventral
attention, dorsal attention, and default mode networks. These abnormalities
correlate strongly with longitudinal AD-related clinical symptoms. Moreover,
our interpretability strategy reveals both established and novel neural
systems-level and sex-specific biomarkers, offering new insights into the
neurobiological mechanisms underlying AD progression. Our findings highlight
the potential of spatio-temporal graph-based learning for early, individualized
prediction of AD progression, even in the context of irregularly-sampled
longitudinal imaging data.

</details>


### [33] [Machine Learning and AI Applied to fNIRS Data Reveals Novel Brain Activity Biomarkers in Stable Subclinical Multiple Sclerosis](https://arxiv.org/abs/2509.21770)
*Sadman Saumik Islam,Bruna Dalcin Baldasso,Davide Cattaneo,Xianta Jiang,Michelle Ploughman*

Main category: cs.LG

TL;DR: 다발성 경화증(MS)을 가진 사람들이 손의 재주와 인지적 피로에 대한 문제를 호소하며, 비침습적 뇌 이미징 기술인 기능적 근적외선 분광법(fNIRS)을 사용하여 인지적 작업 수행 중 뇌 활동 바이오마커를 탐지하고자 했다. 이러한 작업에서 MS 환자와 대조군을 구별하는 데 성공했으며, 개인 맞춤형 뇌 자극 치료의 목표를 제시했다.


<details>
  <summary>Details</summary>
Motivation: 다발성 경화증 환자들은 손 재주 및 인지적 피로 문제를 겪고 있으나, 이러한 손상은 종종 미세하고 감지하기 어렵다. 이 연구는 이러한 증상을 설명할 수 있는 뇌 활동 바이오마커를 탐색하고자 한다.

Method: 우리는 손의 손재주를 테스트하는 두 가지 유형의 작업을 이용해 MS 환자와 대조군의 fNIRS 데이터를 기계 학습 프레임워크로 분석하여 구분하였다.

Result: K-NN 분류기는 단일 작업에서 75.0%의 정확도, 복합 작업에서 66.7%의 정확도를 달성하였다. 분석 결과, MS 환자 그룹은 비대칭 운동 피질 내에서 억제된 뇌 활동과 느린 신경혈관 반응을 나타냈다.

Conclusion: fNIRS 데이터 분석의 새로운 비전통적 방법이 개인 맞춤형 뇌 자극 목표 개발에 기여할 뇌 활동 바이오마커를 밝혀냈다.

Abstract: People with Multiple Sclerosis (MS) complain of problems with hand dexterity
and cognitive fatigue. However, in many cases, impairments are subtle and
difficult to detect. Functional near-infrared spectroscopy (fNIRS) is a
non-invasive neuroimaging technique that measures brain hemodynamic responses
during cognitive or motor tasks. We aimed to detect brain activity biomarkers
that could explain subjective reports of cognitive fatigue while completing
dexterous tasks and provide targets for future brain stimulation treatments. We
recruited 15 people with MS who did not have a hand (Nine Hole Peg Test
[NHPT]), mobility, or cognitive impairment, and 12 age- and sex-matched
controls. Participants completed two types of hand dexterity tasks with their
dominant hand, single task and dual task (NHPT while holding a ball between the
fifth finger and hypothenar eminence of the same hand). We analyzed fNIRS data
(oxygenated and deoxygenated hemoglobin levels) using a machine learning
framework to classify MS patients from controls based on their brain activation
patterns in bilateral prefrontal and sensorimotor cortices. The K-Nearest
Neighbor classifier achieved an accuracy of 75.0% for single manual dexterity
tasks and 66.7% for the more complex dual manual dexterity tasks. Using XAI, we
found that the most important brain regions contributing to the machine
learning model were the supramarginal/angular gyri and the precentral gyrus
(sensory integration and motor regions) of the ipsilateral hemisphere, with
suppressed activity and slower neurovascular response in the MS group. During
both tasks, deoxygenated hemoglobin levels were better predictors than the
conventional measure of oxygenated hemoglobin. This nonconventional method of
fNIRS data analysis revealed novel brain activity biomarkers that can help
develop personalized brain stimulation targets.

</details>


### [34] [Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.21828)
*The Viet Bui,Tien Mai,Hong Thanh Nguyen*

Main category: cs.LG

TL;DR: 이 논문은 드문 보상이 있는 환경에서의 온라인 다중 에이전트 강화 학습(MARL) 문제를 다루며, 새로운 프레임워크를 제안하여 보상 모델의 품질을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 드문 보상이 있는 환경에서 정책 학습을 효과적으로 안내하는 표준 MARL 알고리즘의 한계를 극복하고자 한다.

Method: 온라인 역선호 학습과 다중 에이전트 온-정책 최적화를 통합한 새로운 프레임워크를 제안한다. 이 프레임워크는 선호 기반 가치 분해 네트워크를 기반으로 한 암묵적인 다중 에이전트 보상 학습 모델을 도입한다.

Result: MAMuJoCo 및 SMACv2와 같은 다양한 벤치마크에서 기존 기준선에 비해 뛰어난 성능을 달성하였다.

Conclusion: 제안된 방법이 온라인 MARL의 드문 보상 문제를 효과적으로 해결함을 보여준다.

Abstract: We study the problem of online multi-agent reinforcement learning (MARL) in
environments with sparse rewards, where reward feedback is not provided at each
interaction but only revealed at the end of a trajectory. This setting, though
realistic, presents a fundamental challenge: the lack of intermediate rewards
hinders standard MARL algorithms from effectively guiding policy learning. To
address this issue, we propose a novel framework that integrates online inverse
preference learning with multi-agent on-policy optimization into a unified
architecture. At its core, our approach introduces an implicit multi-agent
reward learning model, built upon a preference-based value-decomposition
network, which produces both global and local reward signals. These signals are
further used to construct dual advantage streams, enabling differentiated
learning targets for the centralized critic and decentralized actors. In
addition, we demonstrate how large language models (LLMs) can be leveraged to
provide preference labels that enhance the quality of the learned reward model.
Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and
SMACv2, show that our method achieves superior performance compared to existing
baselines, highlighting its effectiveness in addressing sparse-reward
challenges in online MARL.

</details>


### [35] [Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration](https://arxiv.org/abs/2509.21848)
*Taejong Joo,Shu Ishida,Ivan Sosnovik,Bryan Lim,Sahand Rezaei-Shoshtari,Adam Gaier,Robert Giaquinto*

Main category: cs.LG

TL;DR: 이 연구에서는 모델 비의존적인 긴 문맥 모델링을 위한 기초적인 프레임워크를 제안하며, 이를 통해 정보 이론적 압축 목표를 도출하고 여러 문서 질문 응답 벤치마크에서 성능을 향상시킴을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템은 성능이 수작업으로 제작된 협력 전략과 프롬프트 엔지니어링에 크게 의존하여 일반화 가능성을 제한함.

Method: 모델 비의존적인 긴 문맥 모델링 문제를 압축 문제로 공식화하는 프레임워크를 제안하고, 이를 바탕으로 Graph of Agents (GoA)를 제안.

Result: GoA는 평균 $F_1$ 점수를 약 5.7% 향상시키고, 정해진 협력 구조를 사용하는 강력한 다중 에이전트 기준을 16.35% 향상.

Conclusion: GoA는 2K 컨텍스트 윈도우로도 128K 컨텍스트 윈도우 Llama 3.1 8B를 초월하며, 효과적인 컨텍스트 길이를 극적으로 증가시킴.

Abstract: As a model-agnostic approach to long context modeling, multi-agent systems
can process inputs longer than a large language model's context window without
retraining or architectural modifications. However, their performance often
heavily relies on hand-crafted multi-agent collaboration strategies and prompt
engineering, which limit generalizability. In this work, we introduce a
principled framework that formalizes the model-agnostic long context modeling
problem as a compression problem, yielding an information-theoretic compression
objective. Building on this framework, we propose Graph of Agents (GoA), which
dynamically constructs an input-dependent collaboration structure that
maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document
question answering benchmarks, GoA improves the average $F_1$ score of
retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using
a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K
context window, GoA surpasses the 128K context window Llama 3.1 8B on
LongBench, showing a dramatic increase in effective context length. Our source
code is available at https://github.com/tjoo512/graph-of-agents.

</details>


### [36] [Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning](https://arxiv.org/abs/2509.22008)
*Yajie Qi,Wei Wei,Lin Li,Lijun Zhang,Zhidong Gao,Da Wang,Huizhong Song*

Main category: cs.LG

TL;DR: 이 논문은 구조화된 목표 유도 강화 학습(SGRL) 방법을 제안하여 RL 에이전트가 효율적으로 탐색할 수 있도록 돕는다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계의 의사 결정 작업은 복잡하고 개방적인 환경에서 발생하여 강화 학습 에이전트의 탐색 효율성과 장기 계획 능력에 큰 도전 과제가 된다.

Method: 구조화된 목표 계획자와 목표 조건 액션 프루너를 통합한 SGRL 방법을 소개한다.

Result: SGRL이 기존의 최첨단 방법에 비해 우수한 성능을 달성했음을 실험 결과로 보여준다.

Conclusion: SGRL은 RL 에이전트가 보다 유망한 결정 경로를 따르도록 유도하여 효율적으로 탐색할 수 있게 한다.

Abstract: Real-world decision-making tasks typically occur in complex and open
environments, posing significant challenges to reinforcement learning (RL)
agents' exploration efficiency and long-horizon planning capabilities. A
promising approach is LLM-enhanced RL, which leverages the rich prior knowledge
and strong planning capabilities of LLMs to guide RL agents in efficient
exploration. However, existing methods mostly rely on frequent and costly LLM
invocations and suffer from limited performance due to the semantic mismatch.
In this paper, we introduce a Structured Goal-guided Reinforcement Learning
(SGRL) method that integrates a structured goal planner and a goal-conditioned
action pruner to guide RL agents toward efficient exploration. Specifically,
the structured goal planner utilizes LLMs to generate a reusable, structured
function for goal generation, in which goals are prioritized. Furthermore, by
utilizing LLMs to determine goals' priority weights, it dynamically generates
forward-looking goals to guide the agent's policy toward more promising
decision-making trajectories. The goal-conditioned action pruner employs an
action masking mechanism that filters out actions misaligned with the current
goal, thereby constraining the RL agent to select goal-consistent policies. We
evaluate the proposed method on Crafter and Craftax-Classic, and experimental
results demonstrate that SGRL achieves superior performance compared to
existing state-of-the-art methods.

</details>


### [37] [Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead](https://arxiv.org/abs/2509.22174)
*Durgesh Kalwar,Mayank Baranwal,Harshad Khadilkar*

Main category: cs.LG

TL;DR: DYNAWEIGHT는 분산 학습을 위한 새로운 정보 집계 프레임워크로, 최소한의 통신 및 메모리 오버헤드로 학습 속도를 획기적으로 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 오늘날 데이터 민감한 환경에서 분산 학습은 개인 정보 보호 강화와 계산 작업 효율화에 필수적인 도구입니다.

Method: DYNAWEIGHT는 인접 서버의 상대적 손실에 따라 동적으로 가중치를 할당하여 정보를 집계합니다.

Result: 다양한 데이터세트에서 실험을 통해 학습 속도의 상당한 향상을 입증했습니다.

Conclusion: DYNAWEIGHT는 모든 서버 레벨 최적화 알고리즘과 호환되는 집계 방식으로, 널리 통합될 가능성을 가지고 있습니다.

Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital
tool, not only fortifying privacy measures but also streamlining computational
operations. This becomes especially crucial within fully decentralized
infrastructures where local processing is imperative due to the absence of
centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to
information aggregation in multi-agent networks. DYNAWEIGHT offers substantial
acceleration in decentralized learning with minimal additional communication
and memory overhead. Unlike traditional static weight assignments, such as
Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring
servers based on their relative losses on local datasets. Consequently, it
favors servers possessing diverse information, particularly in scenarios of
substantial data heterogeneity. Our experiments on various datasets MNIST,
CIFAR10, and CIFAR100 incorporating various server counts and graph topologies,
demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT
functions as an aggregation scheme compatible with any underlying server-level
optimization algorithm, underscoring its versatility and potential for
widespread integration.

</details>


### [38] [Learning from Delayed Feedback in Games via Extra Prediction](https://arxiv.org/abs/2509.22426)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 이 연구는 게임에서 학습의 시간 지연 피드백 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 게임에서의 학습은 다수의 에이전트가 독립적으로 전략을 학습하는 것을 상정하지만, 에이전트 간의 최적화 불일치가 종종 발생한다.

Method: OFTRL(Optimistic Follow-the-Regularized-Leader) 알고리즘에 미래 보상의 예측을 통합하고, WOFTRL(weighted OFTRL)이라는 새로운 알고리즘을 제안한다.

Result: WOFTRL의 경우 낙관적인 가중치가 시간 지연을 상쇄하여 성능을 회복하고, 일반 합 정상형 게임에서 상수 낙오를 유지하며, 다변량 제로섬 게임에서 내쉬 균형으로 수렴한다.

Conclusion: 이론적 결과는 실험을 통해 지지되고 강화된다.

Abstract: This study raises and addresses the problem of time-delayed feedback in
learning in games. Because learning in games assumes that multiple agents
independently learn their strategies, a discrepancy in optimization often
emerges among the agents. To overcome this discrepancy, the prediction of the
future reward is incorporated into algorithms, typically known as Optimistic
Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the
past rewards hinders the prediction. Indeed, this study firstly proves that
even a single-step delay worsens the performance of OFTRL from the aspects of
regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where
the prediction vector of the next reward in OFTRL is weighted $n$ times. We
further capture an intuition that the optimistic weight cancels out this time
delay. We prove that when the optimistic weight exceeds the time delay, our
WOFTRL recovers the good performances that the regret is constant
($O(1)$-regret) in general-sum normal-form games, and the strategies converge
to the Nash equilibrium as a subsequence (best-iterate convergence) in
poly-matrix zero-sum games. The theoretical results are supported and
strengthened by our experiments.

</details>


### [39] [Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach](https://arxiv.org/abs/2509.22272)
*Nassim Walha,Sebastian G. Gruber,Thomas Decker,Yinchong Yang,Alireza Javanmardi,Eyke Hüllermeier,Florian Buettner*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델의 불확실성을 정량화하고 분해하기 위한 새로운 접근법인 스펙트럼 불확실성을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 다양한 응용프로그램에 통합됨에 따라 이들의 예측 불확실성을 신뢰할 수 있는 방법으로 측정하는 것이 매우 중요해졌습니다.

Method: 이 논문에서는 양자 정보 이론에서 유래한 폰 노이만 엔트로피를 활용하여 총 불확실성을 알레아토릭 불확실성과 에피스테믹 불확실성으로 분리하는 이론적 기초를 제공합니다.

Result: 스펙트럼 불확실성은 모델 응답에서 다양한 의미 해석 간의 미세한 차별화를 가능하게 합니다.

Conclusion: 실험 평가 결과 스펙트럼 불확실성은 다양한 모델과 벤치마크 데이터 세트에서 알레아토릭 불확실성과 총 불확실성을 추정하는 데 있어 최첨단 방법보다 우수한 성능을 보였습니다.

Abstract: As Large Language Models (LLMs) are increasingly integrated in diverse
applications, obtaining reliable measures of their predictive uncertainty has
become critically important. A precise distinction between aleatoric
uncertainty, arising from inherent ambiguities within input data, and epistemic
uncertainty, originating exclusively from model limitations, is essential to
effectively address each uncertainty source. In this paper, we introduce
Spectral Uncertainty, a novel approach to quantifying and decomposing
uncertainties in LLMs. Leveraging the Von Neumann entropy from quantum
information theory, Spectral Uncertainty provides a rigorous theoretical
foundation for separating total uncertainty into distinct aleatoric and
epistemic components. Unlike existing baseline methods, our approach
incorporates a fine-grained representation of semantic similarity, enabling
nuanced differentiation among various semantic interpretations in model
responses. Empirical evaluations demonstrate that Spectral Uncertainty
outperforms state-of-the-art methods in estimating both aleatoric and total
uncertainty across diverse models and benchmark datasets.

</details>


### [40] [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
*Yulei Qin,Xiaoyu Tan,Zhengbao He,Gang Li,Haojia Lin,Zongyi Li,Zihan Xu,Yuchen Shi,Siqi Cai,Renting Rui,Shaofei Cai,Yuzheng Cai,Xuan Zhang,Sheng Ye,Ke Li,Xing Sun*

Main category: cs.LG

TL;DR: 이 논문은 RL의 탐색-활용 균형을 자기 경험에 기반하여 해결하는 SPEAR을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 긴 시간의 sparsely-rewarded agent tasks에서 RL의 탐색-활용 무역의 근본적인 도전에 대응하고자 한다.

Method: SPEAR은 커리큘럼 기반 자기 모방 학습을 통해 정책 진화를 잘 균형 잡힌 엔트로피 범위 내에서 점진적으로 조정한다.

Result: 이 접근 방식은 훈련이 진행됨에 따라 효과적인 행동 패턴의 활용을 강화하여 솔루션 반복을 가속화한다.

Conclusion: SPEAR은 안정적인 훈련을 위해 경험의 이점을 재조정하고, 과신을 억제하기 위해 여러 정규화를 도입한다.

Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic
tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,
yet it faces a fundamental challenge of exploration-exploitation trade-off.
Existing studies stimulate exploration through the lens of policy entropy, but
such mechanical entropy maximization is prone to RL training instability due to
the multi-turn distribution shifting. In this paper, we target the progressive
exploration-exploitation balance under the guidance of the agent own
experiences without succumbing to either entropy collapsing or runaway
divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)
recipe for training agentic LLMs. It extends the vanilla SIL framework, where a
replay buffer stores self-generated promising trajectories for off-policy
update, by gradually steering the policy evolution within a well-balanced range
of entropy across stages. Specifically, our approach incorporates a curriculum
to manage the exploration process, utilizing intrinsic rewards to foster
skill-level exploration and facilitating action-level exploration through SIL.
At first, the auxiliary tool call reward plays a critical role in the
accumulation of tool-use skills, enabling broad exposure to the unfamiliar
distributions of the environment feedback with an upward entropy trend. As
training progresses, self-imitation gets strengthened to exploit existing
successful patterns from replayed experiences for comparative action-level
exploration, accelerating solution iteration without unbounded entropy growth.
To further stabilize training, we recalibrate the advantages of experiences in
the replay buffer to address the potential policy drift. Reugularizations such
as the clipping of tokens with high covariance between probability and
advantage are introduced to the trajectory-level entropy control to curb
over-confidence.

</details>


### [41] [Distributed Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2509.22321)
*Bowen Wang,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 분산 설정에서 에이전트들이 자신과 타인의 연상 기억을 이용해 정보를 회상할 수 있는 방법을 제안함.


<details>
  <summary>Details</summary>
Motivation: 현대 신경 아키텍처의 운영 기초가 되는 연상 기억의 이해 및 활용

Method: 에이전트 간의 통신을 통한 라우팅 트리를 활용한 분산 온라인 경량 하강 방법을 도입

Result: 제안된 프로토콜이 기존의 온라인 최적화 기준보다 일관되게 우수한 성능을 보임

Conclusion: 이론적 분석을 통해 서브선형 후회 보장을 수립함

Abstract: An associative memory (AM) enables cue-response recall, and associative
memorization has recently been noted to underlie the operation of modern neural
architectures such as Transformers. This work addresses a distributed setting
where agents maintain a local AM to recall their own associations as well as
selective information from others. Specifically, we introduce a distributed
online gradient descent method that optimizes local AMs at different agents
through communication over routing trees. Our theoretical analysis establishes
sublinear regret guarantees, and experiments demonstrate that the proposed
protocol consistently outperforms existing online optimization baselines.

</details>


### [42] [MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning](https://arxiv.org/abs/2509.22403)
*Fanjin Meng,Yuan Yuan,Jingtao Ding,Jie Feng,Chonghua Han,Yong Li*

Main category: cs.LG

TL;DR: MoveFM-R는 인간 이동 패턴의 모델링을 개선하기 위해 언어 기반 의미 추론을 활용하는 새로운 프레임워크로, 기존 모델의 한계를 극복하고 현실적인 궤적을 생성하는 데 성공했다.


<details>
  <summary>Details</summary>
Motivation: 인간 이동 패턴의 모델링을 위한 Mobility Foundation Models(MFMs)의 한계를 극복하고자 했다.

Method: MoveFM-R 프레임워크는 지리적 좌표와 언어 토큰 간의 어휘 불일치, MFMs의 잠재 벡터와 LLMs의 의미 세계 간의 표현 격차를 해결하는 데 중점을 두었다.

Result: MoveFM-R은 기존 MFM 기반 및 LLM 기반 기준선보다 뛰어난 성능을 보여 주었고, 제로샷 설정에서의 일반화 능력 역시 뛰어났다.

Conclusion: MoveFM-R은 MFMs의 통계적 힘과 LLMs의 깊은 의미 이해를 결합하여 인간 이동 모델링의 새로운 패러다임을 제시한다.

Abstract: Mobility Foundation Models (MFMs) have advanced the modeling of human
movement patterns, yet they face a ceiling due to limitations in data scale and
semantic understanding. While Large Language Models (LLMs) offer powerful
semantic reasoning, they lack the innate understanding of spatio-temporal
statistics required for generating physically plausible mobility trajectories.
To address these gaps, we propose MoveFM-R, a novel framework that unlocks the
full potential of mobility foundation models by leveraging language-driven
semantic reasoning capabilities. It tackles two key challenges: the vocabulary
mismatch between continuous geographic coordinates and discrete language
tokens, and the representation gap between the latent vectors of MFMs and the
semantic world of LLMs. MoveFM-R is built on three core innovations: a
semantically enhanced location encoding to bridge the geography-language gap, a
progressive curriculum to align the LLM's reasoning with mobility patterns, and
an interactive self-reflection mechanism for conditional trajectory generation.
Extensive experiments demonstrate that MoveFM-R significantly outperforms
existing MFM-based and LLM-based baselines. It also shows robust generalization
in zero-shot settings and excels at generating realistic trajectories from
natural language instructions. By synthesizing the statistical power of MFMs
with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm
that enables a more comprehensive, interpretable, and powerful modeling of
human mobility. The implementation of MoveFM-R is available online at
https://anonymous.4open.science/r/MoveFM-R-CDE7/.

</details>


### [43] [JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation](https://arxiv.org/abs/2509.22522)
*Guillem Capellera,Luis Ferraz,Antonio Rubio,Alexandre Alahi,Antonio Agudo*

Main category: cs.LG

TL;DR: JointDiff는 연속적 공간-시간 데이터와 동기화된 이산 이벤트를 동시에 생성하여 복잡한 시스템을 모델링하는 새로운 확산 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 연속 데이터와 이산 이벤트를 별개의 프로세스로 처리하는 기존 생성 모델의 한계를 극복하고 두 프로세스 간의 상호작용을 통합하고자 합니다.

Method: JointDiff를 통해 여러 에이전트의 궤적과 주요 소유 이벤트를 동시에 모델링하며, 두 가지 새로운 제어 가능한 생성 시나리오를 사용합니다.

Result: JointDiff는 새로운 통합된 스포츠 벤치마크에서 최첨단 성능을 달성하며, 게임 역학에 대한 유연한 의미적 제어와 정밀한 언어 기반 생성을 가능하게 합니다.

Conclusion: 연결 모델링이 상호작용 시스템을 위한 사실적이고 제어 가능한 생성 모델을 구축하는 데 필수적임을 보여줍니다.

Abstract: Generative models often treat continuous data and discrete events as separate
processes, creating a gap in modeling complex systems where they interact
synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion
framework designed to unify these two processes by simultaneously generating
continuous spatio-temporal data and synchronous discrete events. We demonstrate
its efficacy in the sports domain by simultaneously modeling multi-agent
trajectories and key possession events. This joint modeling is validated with
non-controllable generation and two novel controllable generation scenarios:
weak-possessor-guidance, which offers flexible semantic control over game
dynamics through a simple list of intended ball possessors, and text-guidance,
which enables fine-grained, language-driven generation. To enable the
conditioning with these guidance signals, we introduce CrossGuid, an effective
conditioning operation for multi-agent domains. We also share a new unified
sports benchmark enhanced with textual descriptions for soccer and football
datasets. JointDiff achieves state-of-the-art performance, demonstrating that
joint modeling is crucial for building realistic and controllable generative
models for interactive systems.

</details>


### [44] [EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](https://arxiv.org/abs/2509.22576)
*Xu Wujiang,Wentian Zhao,Zhenting Wang,Li Yu-Jhe,Jin Can,Jin Mingyu,Mei Kai,Wan Kun,Metaxas Dimitris*

Main category: cs.LG

TL;DR: 희소 보상 하의 다중 턴 환경에서 LLM 에이전트를 훈련하는 것은 강화 학습에 기본적인 도전 과제를 제시한다. 이 논문에서는 에이전트가 결함이 있는 저엔트로피 전략에 빠지는 탐색-착취 폭포 실패 모드를 식별하고, 이를 해결하기 위해 EPO라는 일반 프레임워크를 제안한다. EPO는 다양한 메커니즘을 통해 에이전트 학습을 안정화하고 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 다중 턴 환경에서의 강화 학습은 단일 작업 수행에 30회 이상의 상호작용이 필요하므로, 전통적인 RL과는 다른 문제를 다루어야 한다.

Method: EPO라는 프레임워크를 제안하며, 이는 다중 턴 설정에서의 엔트로피 정규화 채택, 정책 엔트로피의 급격한 변동 방지를 위한 엔트로피 스무딩 정규화기, 탐색과 착취 간의 균형을 맞추는 적응형 단계 기반 가중치 부여를 포함한다.

Result: EPO는 ScienceWorld에서 최대 152%의 성능 향상을, ALFWorld에서는 최대 19.8%의 성능 향상을 달성하였다.

Conclusion: 다중 턴 희소 보상 설정은 전통적인 RL과는 다른 엔트로피 제어가 필요하다는 것을 보여준다.

Abstract: Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.

</details>


### [45] [Learning Admissible Heuristics for A*: Theory and Practice](https://arxiv.org/abs/2509.22626)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.LG

TL;DR: 이 논문은 휴리스틱 학습을 제한된 최적화 문제로 설정하고, 훈련 중 허용 가능성을 강제하는 손실 함수인 교차 엔트로피 허용 가능성(CEA)을 도입하여 최근의 딥러닝 접근 방식의 한계를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 최근의 딥러닝 접근 방식은 허용 가능성을 무시하고 훈련 데이터 이외의 일반화에 대한 제한된 보장을 제공합니다.

Method: 이 논문에서는 휴리스틱 학습을 제한된 최적화 문제로 설정하고, 훈련 중 허용 가능성을 보장하는 교차 엔트로피 허용 가능성(CEA) 손실 함수를 도입합니다.

Result: 이 방법은 루빅스 큐브 도메인에서 압축된 패턴 데이터베이스(PDB) 휴리스틱보다 훨씬 더 강력한 안내를 제공하는 거의 허용 가능한 휴리스틱을 생성합니다.

Conclusion: ReLU 신경망을 사용하는 경우, 일반적인 가설 클래스 대신 신경망의 너비와 깊이에 주로 의존하는 경계를 제공합니다.

Abstract: Heuristic functions are central to the performance of search algorithms such
as A-star, where admissibility - the property of never overestimating the true
shortest-path cost - guarantees solution optimality. Recent deep learning
approaches often disregard admissibility and provide limited guarantees on
generalization beyond the training data. This paper addresses both of these
limitations. First, we pose heuristic learning as a constrained optimization
problem and introduce Cross-Entropy Admissibility (CEA), a loss function that
enforces admissibility during training. On the Rubik's Cube domain, this method
yields near-admissible heuristics with significantly stronger guidance than
compressed pattern database (PDB) heuristics. Theoretically, we study the
sample complexity of learning heuristics. By leveraging PDB abstractions and
the structural properties of graphs such as the Rubik's Cube, we tighten the
bound on the number of training samples needed for A-star to generalize.
Replacing a general hypothesis class with a ReLU neural network gives bounds
that depend primarily on the network's width and depth, rather than on graph
size. Using the same network, we also provide the first generalization
guarantees for goal-dependent heuristics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [46] [Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic](https://arxiv.org/abs/2509.21475)
*Sen Yang,Burak Öz,Fei Wu,Fan Zhang*

Main category: cs.CR

TL;DR: 탈중앙화는 지리적 차원을 가지며, 기존의 지표인 이해관계자 분포는 이를 간과한다. 이더리움의 검증자는 대서양 연안에 집중되어 있으며, 이는 적절한 지연 속도에 기인한다. 본 연구는 두 가지 블록 생성 패러다임을 비교하고 지연 속도가 검증자 보상에 미치는 영향을 분석했다.


<details>
  <summary>Details</summary>
Motivation: 탈중앙화의 지리적 차원을 이해하고, 검증자의 위치가 보상 접근성과 지역적 충격에 대한 저항성에 미치는 영향을 조사하기 위해.

Method: 지연 속도에 맞춰 조정된 에이전트 기반 모델을 개발하고, 단일 소스 패러다임(SSP)과 다중 소스 패러다임(MSP)을 비교한다.

Result: 시뮬레이션 결과, SSP는 전파가 주로 영향을 미치므로 느리게 중앙집중화되며, MSP는 빠르게 중앙집중화된다.

Conclusion: 프로토콜 설계가 검증자 지리에 실질적으로 영향을 미치며, 지리적 탈중앙화를 촉진할 수 있는 방법을 제시한다.

Abstract: Decentralization has a geographic dimension that conventional metrics such as
stake distribution overlook. Where validators run affects resilience to
regional shocks (outages, disasters, government intervention) and fairness in
reward access. Yet in permissionless systems, locations cannot be mandated, but
they emerge from incentives. Today, Ethereum's validators cluster along the
Atlantic (EU and U.S. East Coast), where latency is structurally favorable.
This raises a key question: when some regions already enjoy latency advantages,
how does protocol design shape validator incentives and the geography of
(de)centralization? We develop a latency-calibrated agent-based model and
compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),
akin to MEV-Boost, where proposers fetch full blocks from a relay that also
propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate
value from multiple sources and broadcast the block themselves. Simulations
show that SSP concentrates around relay placement but more slowly, since
proximity mainly affects propagation, and the marginal value of time is
relatively uniform across regions. MSP centralizes faster: aggregating across
sources makes marginal value location-dependent, amplifying payoff dispersion
and migration toward latency minima. Source placement and consensus settings
can dampen or intensify these effects, though once validators are already
clustered, the impact of source placement on decentralization is marginal. In
most cases, North America consistently emerges as the focal hub. These findings
show that protocol design materially shapes validator geography and offer
levers for promoting geographical decentralization.

</details>


### [47] [MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs](https://arxiv.org/abs/2509.21634)
*Prakhar Sharma,Haohuang Wen,Vinod Yegneswaran,Ashish Gehani,Phillip Porras,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 6G 네트워크로의 진화는 O-RAN 패러다임에 의해 가속화되고 있으며, 본 논문은 6G O-RAN 환경에서 완전 자동화된 위협 완화 프레임워크인 MobiLLM을 제안한다.


<details>
  <summary>Details</summary>
Motivation: O-RAN의 개방성은 혁신을 위한 기회를 제공하지만, 공격 표면을 확대시켜 보다 탄력적이고 저렴한 자율 보안 솔루션이 필요하다.

Method: MobiLLM은 대형 언어 모델을 통해 구동되는 모듈형 다중 에이전트 시스템을 활용하여 보안 워크플로우를 조정하고, 실시간 데이터 분류, 위협 분류 및 대응을 위해 다양한 에이전트를 기능하도록 구성한다.

Result: MobiLLM은 복잡한 완화 전략을 효과적으로 식별하고 조정할 수 있으며, 응답 지연을 상당히 줄이고 6G에서 자율 보안 운영의 타당성을 입증한다.

Conclusion: MobiLLM은 신뢰할 수 있는 AI 기반 네트워크 보안을 위한 청사진을 제공하며, 안전한 운영을 위해 신뢰할 수 있는 지식 기반과 강력한 안전 장치를 갖추고 있다.

Abstract: The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.

</details>


### [48] [Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing](https://arxiv.org/abs/2509.21712)
*Bingcan Guo,Eryue Xu,Zhiping Zhang,Tianshi Li*

Main category: cs.CR

TL;DR: AI 시스템을 인간의 개인 정보 보호 선호와 맞추기 위해서는 개인의 미세한 정보 공개 행동을 이해해야 한다. 본 연구에서는 AI를 활용한 접근 방식을 통해 개인의 개인정보 경계를 탐색하는 방법을 제시하고, 역할과 위임 조건에 따라 커뮤니케이션을 변화시킨 1,681개의 경계 규정을 수집하였다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템과 인간의 개인 정보 보호 선호를 일치시키기 위해 개인의 세밀한 정보 공개 행동을 이해할 필요가 있다.

Method: AI를 활용한 경계 정보 탐색 접근 방식을 사용하여 커뮤니케이션 역할 및 위임 조건을 변화시키는 실험을 진행하였다.

Result: 169명의 참가자로부터 61개 시나리오에 대해 1,681개의 경계 규정을 도출하였고, 커뮤니케이션 역할이 정보 공개 수용에 미치는 영향을 확인하였다.

Conclusion: 개인 정보 보호 선호 정보를 실세계 데이터 흐름 내에 위치시키는 것이 중요하며, 미세한 개인 정보 보호 경계를 미래 AI 시스템의 정렬 목표로 사용하는 것을 제안한다.

Abstract: Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.

</details>


### [49] [PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation](https://arxiv.org/abs/2509.21772)
*Daiki Chiba,Hiroki Nakano,Takashi Koide*

Main category: cs.CR

TL;DR: PhishLumos는 적응형 다중 에이전트 시스템으로, 피싱 공격 캠페인을 능동적으로 완화하여 디지털 서비스에 대한 신뢰를 회복하는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 피싱 공격은 사회적 위협이며, 취약한 집단에 불균형적인 피해를 주고 디지털 서비스에 대한 신뢰를 저하시키므로 이를 해결할 필요가 있다.

Method: PhishLumos는 회피 전술을 반응적으로 차단하는 대신, 이를 공격 신호로 활용하여 기반 인프라를 조사한다. LLM을 활용하여 공유 호스팅, 인증서 및 도메인 등록 패턴을 밝혀낸다.

Result: 실제 데이터에 기반하여, 시스템은 보안 전문가가 확인하기 일주일 전 평균적으로 100%의 공격 캠페인을 식별했다.

Conclusion: PhishLumos는 URL 차단에서 공격 캠페인 완화로의 실질적인 전환을 보여주며, 사용자를 피해를 입기 전에 보호하고 디지털 세계를 모두에게 더 안전하게 만든다.

Abstract: Phishing attacks are a significant societal threat, disproportionately
harming vulnerable populations and eroding trust in essential digital services.
Current defenses are often reactive, failing against modern evasive tactics
like cloaking that conceal malicious content. To address this, we introduce
PhishLumos, an adaptive multi-agent system that proactively mitigates entire
attack campaigns. It confronts a core cybersecurity imbalance: attackers can
easily scale operations, while defense remains an intensive expert task.
Instead of being blocked by evasion, PhishLumos treats it as a critical signal
to investigate the underlying infrastructure. Its Large Language Model
(LLM)-powered agents uncover shared hosting, certificates, and domain
registration patterns. On real-world data, our system identified 100% of
campaigns in the median case, over a week before their confirmation by
cybersecurity experts. PhishLumos demonstrates a practical shift from reactive
URL blocking to proactive campaign mitigation, protecting users before they are
harmed and making the digital world safer for all.

</details>


### [50] [You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://arxiv.org/abs/2509.21884)
*Bochuan Cao,Changjiang Li,Yuanpu Cao,Yameng Ge,Ting Wang,Jinghui Chen*

Main category: cs.CR

TL;DR: 이 논문은 대형 언어 모델의 시스템 프롬프트 유출 위험을 탐지하고, SysVec라는 방법을 통해 이를 해결하는 전략을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 다양한 응용 프로그램에서 널리 사용되고 있으며, 시스템 프롬프트 유출의 위험이 존재하는 상황에서 개발자들은 이를 방지하기 위한 전략을 구현해야 할 필요가 있다.

Method: 이 논문에서는 시스템 프롬프트를 내부 표현 벡터로 인코딩하는 새로운 방법인 SysVec를 제안한다.

Result: SysVec는 시스템 프롬프트를 효과적으로 보호하면서 LLM의 언어 능력을 보존한다.

Conclusion: 실험 결과, SysVec는 프롬프트 유출 공격을 효과적으로 완화하고, LLM의 기능적 완전성을 유지하며, 긴 컨텍스트 시나리오에서 망각 문제를 완화하는 데 도움을 준다.

Abstract: Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.

</details>


### [51] [Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions](https://arxiv.org/abs/2509.22022)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 이 논문은 멀티 파티 상황에서의 PRG 기반 DPF의 효율성 문제를 해결하고, 실용적인 키 크기를 제공하는 최초의 DPF 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 2-party PRG 기반 방식의 한계를 극복하고 멀티 파티 설정에도 효율적인 DPF를 제공하기 위함이다.

Method: Boyle et al. (EUROCRYPT'15)의 PRG 기반 멀티 파티 DPF 방식을 최적화하고, 정직한 다수 가정을 활용하여 기하급수적인 키 크기를 제거한다.

Result: 최초로 실용적인 키 크기를 갖는 PRG 기반 멀티 파티 DPF 방식을 제시하고, 기존의 멀티 파티 DPF보다 최대 3배 작은 키를 제공한다.

Conclusion: 신중한 최적화를 통해 PRG 기반 멀티 파티 DPF가 실용적인 성능을 달성할 수 있음을 보여준다.

Abstract: Distributed Point Functions (DPFs) enable sharing secret point functions
across multiple parties, supporting privacy-preserving technologies such as
Private Information Retrieval, and anonymous communications. While 2-party
PRG-based schemes with logarithmic key sizes have been known for a decade,
extending these solutions to multi-party settings has proven challenging. In
particular, PRG-based multi-party DPFs have historically struggled with
practicality due to key sizes growing exponentially with the number of parties
and the field size.
  Our work addresses this efficiency bottleneck by optimizing the PRG-based
multi-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the
honest-majority assumption, we eliminate the exponential factor present in this
scheme. Our construction is the first PRG-based multi-party DPF scheme with
practical key sizes, and provides key up to 3x smaller than the best known
multi-party DPF. This work demonstrates that with careful optimization,
PRG-based multi-party DPFs can achieve practical performances, and even obtain
top performances.

</details>


### [52] ["Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040)
*Yue Liu,Yanjie Zhao,Yunbo Lyu,Ting Zhang,Haoyu Wang,David Lo*

Main category: cs.CR

TL;DR: 본 연구는 고권한 에이전틱 AI 코딩 편집기에 대한 프롬프트 인젝션 공격의 첫 번째 실증 분석을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI 코딩 편집기가 개발자 생산성을 향상시키는 능력 덕분에 인기를 끌고 있으나, 새로운 보안 문제를 야기하고 있습니다.

Method: AIShellJack라는 자동화된 테스트 프레임워크를 구현하여 프롬프트 인젝션 취약성을 평가합니다.

Result: 대규모 평가 결과, 악의적인 명령을 실행하는 성공률이 최고 84%에 달했습니다.

Conclusion: 이 공격은 초기 접근, 시스템 탐색, 자격 증명 도용 및 데이터 유출 등 다양한 목표에서 효과적임이 입증되었습니다.

Abstract: Agentic AI coding editors driven by large language models have recently
become more popular due to their ability to improve developer productivity
during software development. Modern editors such as Cursor are designed not
just for code completion, but also with more system privileges for complex
coding tasks (e.g., run commands in the terminal, access development
environments, and interact with external systems). While this brings us closer
to the "fully automated programming" dream, it also raises new security
concerns. In this study, we present the first empirical analysis of prompt
injection attacks targeting these high-privilege agentic AI coding editors. We
show how attackers can remotely exploit these systems by poisoning external
development resources with malicious instructions, effectively hijacking AI
agents to run malicious commands, turning "your AI" into "attacker's shell". To
perform this analysis, we implement AIShellJack, an automated testing framework
for assessing prompt injection vulnerabilities in agentic AI coding editors.
AIShellJack contains 314 unique attack payloads that cover 70 techniques from
the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale
evaluation on GitHub Copilot and Cursor, and our evaluation results show that
attack success rates can reach as high as 84% for executing malicious commands.
Moreover, these attacks are proven effective across a wide range of objectives,
ranging from initial access and system discovery to credential theft and data
exfiltration.

</details>


### [53] [Secure and Efficient Access Control for Computer-Use Agents via Context Space](https://arxiv.org/abs/2509.22256)
*Haochen Gong,Chenxiao Li,Rui Chang,Wenbo Shen*

Main category: cs.CR

TL;DR: CSAgent는 컴퓨터 사용 에이전트를 위한 정적 정책 기반 접근 제어 프레임워크로, 사용자 의도와 컨텍스트를 고려한 정책을 통해 보안을 강화하고 성능 저하를 최소화한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 컴퓨터 사용 에이전트는 자연어로 시스템 및 애플리케이션 기능을 제어할 수 있게 해주지만, LLM의 불확실성으로 인해 보안 위험이 존재한다.

Method: CSAgent는 정적 정책과 동적 컨텍스트 및 사용자 의도 사이의 간극을 메우기 위해 사용자 의도 및 컨텍스트 인식 정책을 도입하고, 이를 개발자가 구성하고 정제할 수 있도록 지원하는 자동화된 도구 체인을 제공한다.

Result: CSAgent는 특정 사용자 의도와 컨텍스트 하에서만 에이전트 행동이 실행될 수 있도록 최적화된 OS 서비스를 통해 이러한 정책을 시행하고 있으며, 99.36% 이상의 공격을 성공적으로 방어했다.

Conclusion: CSAgent는 API, CLI, GUI 등 다양한 인터페이스를 통해 컴퓨터를 제어하는 에이전트를 보호할 수 있는 기능을 지원하며, 성능 overhead는 6.83%에 불과하다.

Abstract: Large language model (LLM)-based computer-use agents represent a convergence
of AI and OS capabilities, enabling natural language to control system- and
application-level functions. However, due to LLMs' inherent uncertainty issues,
granting agents control over computers poses significant security risks. When
agent actions deviate from user intentions, they can cause irreversible
consequences. Existing mitigation approaches, such as user confirmation and
LLM-based dynamic action validation, still suffer from limitations in
usability, security, and performance. To address these challenges, we propose
CSAgent, a system-level, static policy-based access control framework for
computer-use agents. To bridge the gap between static policy and dynamic
context and user intent, CSAgent introduces intent- and context-aware policies,
and provides an automated toolchain to assist developers in constructing and
refining them. CSAgent enforces these policies through an optimized OS service,
ensuring that agent actions can only be executed under specific user intents
and contexts. CSAgent supports protecting agents that control computers through
diverse interfaces, including API, CLI, and GUI. We implement and evaluate
CSAgent, which successfully defends against more than 99.36% of attacks while
introducing only 6.83% performance overhead.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [54] [Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow](https://arxiv.org/abs/2509.21789)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Yongbo He,Zhangquan Chen,Zhucun Xue,Jiangning Zhang,Yue Liao,Xiaobin Hu,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 비주얼 언어 모델(VLM)을 기반으로 한 다중 에이전트 시스템(MAS)은 도전적인 작업을 가능하게 하지만, 단일 에이전트에서 발생한 착각이 후속 에이전트에 의해 증폭되는 새로운 실패 문제인 다중 에이전트 비주얼 착각 눈덩이를 겪고 있다. 이를 해결하기 위해 비주얼 흐름을 활용한 메시지 전달과 주의 재분배를 사용하는 경량화된 저항 패러다임인 ViF를 제안하였다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서의 비주얼 착각 문제를 해결하기 위해.

Method: 자세한 주의 분석을 통해 비주얼 정보 전달에 대한 의존성과 비주얼 착각의 원인을 파악하고, 보다 효과적인 비주얼 토큰을 선택하여 메시지를 전달하는 경량화된 저항 패러다임인 ViF를 제안한다.

Result: 우리의 방법은 다중 에이전트 비주얼 착각 눈덩이를 현저히 줄이고, 8개의 벤치마크에서 성능을 일관되게 향상시킨다.

Conclusion: ViF는 다중 에이전트 시스템에서의 비주얼 착각 문제를 효과적으로 완화할 수 있는 가능성을 보여준다.

Abstract: Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables
challenging tasks but suffers from a novel failure term, multi-agent visual
hallucination snowballing, where hallucinations are seeded in a single agent
and amplified by following ones due to the over-reliance on textual flow to
relay visual information. Through turn-, layer-, and token-wise attention
analyses, we provide detailed insights into the essence of hallucination
snowballing regarding the reduction of visual attention allocation. It leads us
to identify a subset of vision tokens with a unimodal attention peak in middle
layers that best preserve visual evidence but gradually diminish in deeper
agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we
propose ViF, a lightweight, plug-and-play mitigation paradigm that relays
inter-agent messages with Visual Flow powered by the selected visual relay
tokens and applies attention reallocation to amplify this pattern. The
experiment results demonstrate that our method markedly reduces hallucination
snowballing, consistently improving the performance across eight benchmarks
based on four common MAS structures and ten base models. The source code will
be available at: https://github.com/YU-deep/ViF.git.

</details>


### [55] [RobustFlow: Towards Robust Agentic Workflow Generation](https://arxiv.org/abs/2509.21834)
*Shengxiang Xu,Jiayi Zhang,Shimin Di,Yuyu Luo,Liang Yao,Hanmo Liu,Jia Zhu,Fan Liu,Min-Ling Zhang*

Main category: cs.MA

TL;DR: 이 논문은 에이전틱 워크플로우의 자동 생성의 강건성이 향후 과제로 남아있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 워크플로우의 자동 생성은 대규모 언어 모델(LLMs)이 복잡한 작업을 해결할 수 있는 유망한 전선이다.

Method: 우리는 노드 및 위상적 유사성을 기반으로 한 지표를 제안하여 일반적인 의미적 변형에 대해 워크플로우 일관성을 평가한다. 그리고 RobustFlow라는 새로운 훈련 프레임워크를 제안하여 모델이 지시문의 변형에 불변하도록 훈련한다.

Result: RobustFlow는 동의어 작업 설명 집합에 대해 훈련하여 워크플로우 강건성 점수를 70% - 90%로 향상시켰다.

Conclusion: RobustFlow는 기존 접근 방법에 비해 상당한 개선을 보여준다.

Abstract: The automated generation of agentic workflows is a promising frontier for
enabling large language models (LLMs) to solve complex tasks. However, our
investigation reveals that the robustness of agentic workflow remains a
critical, unaddressed challenge. Current methods often generate wildly
inconsistent workflows when provided with instructions that are semantically
identical but differently phrased. This brittleness severely undermines their
reliability and trustworthiness for real-world applications. To quantitatively
diagnose this instability, we propose metrics based on nodal and topological
similarity to evaluate workflow consistency against common semantic variations
such as paraphrasing and noise injection. Subsequently, we further propose a
novel training framework, RobustFlow, that leverages preference optimization to
teach models invariance to instruction variations. By training on sets of
synonymous task descriptions, RobustFlow boosts workflow robustness scores to
70\% - 90\%, which is a substantial improvement over existing approaches. The
code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.

</details>


### [56] [Multi-Agent Path Finding via Offline RL and LLM Collaboration](https://arxiv.org/abs/2509.22130)
*Merve Atasever,Matthew Hong,Mihir Nitin Kulkarni,Qingpei Li,Jyotirmoy V. Deshmukh*

Main category: cs.MA

TL;DR: 이 논문은 Multi-Agent Path Finding(MAPF) 문제를 해결하기 위한 효율적인 분산 계획 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 로봇 공학 및 물류 분야에서의 응용에 중요한 MAPF 문제를 해결하기 위해.

Method: Decision Transformer(DT) 기반의 분산 계획 프레임워크를 제안하며, 오프라인 강화 학습을 활용하여 훈련 시간을 단축한다.

Result: 정적 및 동적으로 변화하는 환경에서 실험한 결과, 제안된 방법이 적응성과 성능을 크게 향상시키는 것으로 나타났다.

Conclusion: GPT-4o를 통합하여 동적으로 에이전트 정책을 안내하는 방식으로, 기존 표준 RL 방법의 적응성 제한을 극복하였다.

Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem
critical for applications in robotics and logistics, particularly due to its
combinatorial complexity and the partial observability inherent in realistic
environments. Decentralized reinforcement learning methods commonly encounter
two substantial difficulties: first, they often yield self-centered behaviors
among agents, resulting in frequent collisions, and second, their reliance on
complex communication modules leads to prolonged training times, sometimes
spanning weeks. To address these challenges, we propose an efficient
decentralized planning framework based on the Decision Transformer (DT),
uniquely leveraging offline reinforcement learning to substantially reduce
training durations from weeks to mere hours. Crucially, our approach
effectively handles long-horizon credit assignment and significantly improves
performance in scenarios with sparse and delayed rewards. Furthermore, to
overcome adaptability limitations inherent in standard RL methods under dynamic
environmental changes, we integrate a large language model (GPT-4o) to
dynamically guide agent policies. Extensive experiments in both static and
dynamically changing environments demonstrate that our DT-based approach,
augmented briefly by GPT-4o, significantly enhances adaptability and
performance.

</details>


### [57] [Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.22216)
*Ahmet Onur Akman,Anastasia Psarou,Zoltán György Varga,Grzegorz Jamróz,Rafał Kucharski*

Main category: cs.MA

TL;DR: 강화 학습 기반 자율차가 혼합 교통 환경에서 도시 교통 흐름에 미치는 잠재적 영향을 조사한 연구.


<details>
  <summary>Details</summary>
Motivation: 자율차와 강화 학습의 상호작용이 도시 교통에 미치는 영향을 이해하고자 함.

Method: 혼합 교통 환경에서의 일일 경로 선택 문제를 단순화하여 다중 에이전트 설정에서 시뮬레이션을 수행.

Result: 자율차가 최대 5%의 여행 시간을 최적화하고, 인간 운전자의 여행 시간에 대한 영향이 자율차의 행동에 따라 다르게 나타남.

Conclusion: 모든 경우에서 자율차가 자기 이익을 추구하는 행동을 할 때, 인간 운전자보다 짧은 여행 시간을 기록함을 보여줌.

Abstract: This study examines the potential impact of reinforcement learning
(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic
environment. We focus on a simplified day-to-day route choice problem in a
multi-agent setting. We consider a city network where human drivers travel
through their chosen routes to reach their destinations in minimum travel time.
Then, we convert one-third of the population into AVs, which are RL agents
employing Deep Q-learning algorithm. We define a set of optimization targets,
or as we call them behaviors, namely selfish, collaborative, competitive,
social, altruistic, and malicious. We impose a selected behavior on AVs through
their rewards. We run our simulations using our in-house developed RL framework
PARCOUR. Our simulations reveal that AVs optimize their travel times by up to
5\%, with varying impacts on human drivers' travel times depending on the AV
behavior. In all cases where AVs adopt a self-serving behavior, they achieve
shorter travel times than human drivers. Our findings highlight the complexity
differences in learning tasks of each target behavior. We demonstrate that the
multi-agent RL setting is applicable for collective routing on traffic
networks, though their impact on coexisting parties greatly varies with the
behaviors adopted.

</details>


### [58] [VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture](https://arxiv.org/abs/2509.22218)
*Sandaru Fernando,Imasha Jayarathne,Sithumini Abeysekara,Shanuja Sithamparanthan,Thushari Silva,Deshan Jayawardana*

Main category: cs.MA

TL;DR: VizGen은 자연어를 사용하여 사용자들이 의미 있는 시각화를 생성할 수 있도록 하는 AI 지원 그래프 생성 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 데이터 시각화 도구는 기술 전문성을 요구하여 접근성을 제한하는 문제를 해결하기 위해 개발되었다.

Method: VizGen은 고급 NLP와 LLM인 Claude 3.7 Sonnet 및 Gemini 2.0 Flash를 활용하여 사용자 쿼리를 SQL로 변환하고 적절한 그래프 유형을 추천한다. 다중 에이전트 아키텍처를 기반으로 SQL 생성, 그래프 생성, 사용자 맞춤 설정 및 통찰력 추출을 처리한다.

Result: VizGen은 데이터 시각화뿐만 아니라 데이터의 패턴, 이상치, 상관관계를 분석하고, 인터넷에서 수집한 맥락적 정보를 통해 풍부한 설명을 제공하여 사용자 이해를 향상시킨다.

Conclusion: VizGen은 기술적 복잡성과 사용자 친화적인 디자인 간의 격차를 bridging함으로써 데이터 시각화를 민주화한다.

Abstract: Data visualization is essential for interpreting complex datasets, yet
traditional tools often require technical expertise, limiting accessibility.
VizGen is an AI-assisted graph generation system that empowers users to create
meaningful visualizations using natural language. Leveraging advanced NLP and
LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries
into SQL and recommends suitable graph types. Built on a multi-agent
architecture, VizGen handles SQL generation, graph creation, customization, and
insight extraction. Beyond visualization, it analyzes data for patterns,
anomalies, and correlations, and enhances user understanding by providing
explanations enriched with contextual information gathered from the internet.
The system supports real-time interaction with SQL databases and allows
conversational graph refinement, making data analysis intuitive and accessible.
VizGen democratizes data visualization by bridging the gap between technical
complexity and user-friendly design.

</details>


### [59] [Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives](https://arxiv.org/abs/2509.22596)
*Qixin Zhang,Yan Sun,Can Jin,Xikun Zhang,Yao Shu,Puning Zhao,Li Shen,Dacheng Tao*

Main category: cs.MA

TL;DR: 이 논문에서는 다중 에이전트 온라인 조정 문제를 위한 두 가지 효과적인 정책 학습 알고리즘을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 온라인 조정 문제에서 효율적인 솔루션을 찾기 위해.

Method: 첫 번째 알고리즘은 최적의 근사 보장을 제공하며, 두 번째 알고리즘은 파라미터에 의존하지 않고 동일한 근사 비율을 유지합니다.

Result: 제안된 알고리즘의 효과성을 검증하기 위해 광범위한 시뮬레이션을 수행했습니다.

Conclusion: 새로운 정책 기반 연속 확장 기술을 통해 낮은 손실로 약한 서브모듈 목표를 해결할 수 있습니다.

Abstract: In this paper, we present two effective policy learning algorithms for
multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL},
not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for
the MA-OC problem with submodular objectives but also can handle the unexplored
$\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios,
where $c$ is the curvature of the investigated submodular functions, $\alpha$
denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$
represents the submodularity ratios. Subsequently, in order to reduce the
reliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the
\texttt{MA-SPL} algorithm, we further introduce the second online algorithm
named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely
\emph{parameter-free} and simultaneously can maintain the same approximation
ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL}
and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique
termed as \emph{policy-based continuous extension}. Compared with the
well-established \emph{multi-linear extension}, a notable advantage of this new
\emph{policy-based continuous extension} is its ability to provide a lossless
rounding scheme for any set function, thereby enabling us to tackle the
challenging weakly submodular objectives. Finally, extensive simulations are
conducted to validate the effectiveness of our proposed algorithms.

</details>
