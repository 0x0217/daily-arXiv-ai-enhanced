<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 24]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [StableAML: Machine Learning for Behavioral Wallet Detection in Stablecoin Anti-Money Laundering on Ethereum](https://arxiv.org/abs/2602.17842)
*Luciano Juvinski,Haochen Li,Alessio Brini*

Main category: cs.CR

TL;DR: 이 연구는 탈중앙화 프로토콜과 중앙집중형 스테이블코인의 역할을 비교하며, AML(자금세탁 방지) 프레임워크를 개발하여 사이버 범죄와 제재된 기관의 거래를 구별하는 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 본 연구의 목적은 매년 3.1조 달러를 초과하는 전 세계 불법 자금 흐름을 분석하고, 스테이블코인이 자금 세탁에 선호되는 매개체가 되는 이유를 조사하는 것입니다.

Method: 이 연구는 이더리움 데이터를 분석하고 행동적 특징을 사용하여 강력한 AML 프레임워크를 개발합니다.

Result: 도메인 정보로 이루어진 트리 앙상블 모델이 높은 Macro-F1 점수를 달성하며, 거래 네트워크의 분열에 어려움을 겪는 그래프 신경망보다 뛰어난 성능을 보입니다.

Conclusion: 모델의 해석 가능성이 이진 탐지를 넘어 복잡한 사이버 범죄와 제재된 기관의 거래 유형을 성공적으로 구별함으로써, 우리는 혁신을 억제하지 않으면서 재정적 위법행위의 경제적 비용을 효과적으로 증가시키는 접근법을 제안합니다.

Abstract: Global illicit fund flows exceed an estimated $3.1 trillion annually, with stablecoins emerging as a preferred laundering medium due to their liquidity. While decentralized protocols increasingly adopt zero-knowledge proofs to obfuscate transaction graphs, centralized stablecoins remain critical "transparent choke points" for compliance. Leveraging this persistent visibility, this study analyzes an Ethereum dataset and uses behavioral features to develop a robust AML framework. Our findings demonstrate that domain-informed tree ensemble models achieve higher Macro-F1 score, significantly outperforming graph neural networks, which struggle with the increasing fragmentation of transaction networks. The model's interpretability goes beyond binary detection, successfully dissecting distinct typologies: it differentiates the complex, high-velocity dispersion of cybercrime syndicates from the constrained, static footprints left by sanctioned entities. This framework aligns with the industry shift toward deterministic verification, satisfying the auditability and compliance expectations under regulations such as the EU's MiCA and the U.S. GENIUS Act while minimizing unjustified asset freezes. By automating high-precision detection, we propose an approach that effectively raises the economic cost of financial misconduct without stifling innovation.

</details>


### [2] [PenTiDef: Enhancing Privacy and Robustness in Decentralized Federated Intrusion Detection Systems against Poisoning Attacks](https://arxiv.org/abs/2602.17973)
*Phan The Duy,Nghi Hoang Khoa,Nguyen Tran Anh Quan,Luong Ha Tien,Ngo Duc Hoang Son,Van-Hau Pham*

Main category: cs.CR

TL;DR: 이 연구는 탈중앙화된 연합 학습 기반 침입 탐지 시스템을 위한 새로운 방어 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 중앙 집중식 연합 학습 침입 탐지 시스템의 한계를 해결하고 탈중앙화된 시스템의 독특한 문제를 다루기 위함이다.

Method: PenTiDef라는 프레임워크를 제안하며, 이는 분산 차등 프라이버시와 신경망으로부터 유도된 잠재 공간 표현을 사용하여 데이터 보호 및 악의적 업데이트 탐지를 수행한다.

Result: 실험 결과, PenTiDef는 여러 공격 시나리오와 데이터 분포에 대해 기존의 방어 시스템보다 일관되게 우수한 성능을 보인다.

Conclusion: PenTiDef는 악의적 환경에서 탈중앙화된 연합 학습 기반 침입 탐지 시스템을 배포하는 데 있어 확장 가능하고 안전한 프레임워크로서의 잠재력을 지닌다.

Abstract: The increasing deployment of Federated Learning (FL) in Intrusion Detection Systems (IDS) introduces new challenges related to data privacy, centralized coordination, and susceptibility to poisoning attacks. While significant research has focused on protecting traditional FL-IDS with centralized aggregation servers, there remains a notable gap in addressing the unique challenges of decentralized FL-IDS (DFL-IDS). This study aims to address the limitations of traditional centralized FL-IDS by proposing a novel defense framework tailored for the decentralized FL-IDS architecture, with a focus on privacy preservation and robustness against poisoning attacks. We propose PenTiDef, a privacy-preserving and robust defense framework for DFL-IDS, which incorporates Distributed Differential Privacy (DDP) to protect data confidentiality and utilizes latent space representations (LSR) derived from neural networks to detect malicious updates in the decentralized model aggregation context. To eliminate single points of failure and enhance trust without a centralized aggregation server, PenTiDef employs a blockchain-based decentralized coordination mechanism that manages model aggregation, tracks update history, and supports trust enforcement through smart contracts. Experimental results on CIC-IDS2018 and Edge-IIoTSet demonstrate that PenTiDef consistently outperforms existing defenses (e.g., FLARE, FedCC) across various attack scenarios and data distributions. These findings highlight the potential of PenTiDef as a scalable and secure framework for deploying DFL-based IDS in adversarial environments. By leveraging privacy protection, malicious behavior detection in hidden data, and working without a central server, it provides a useful security solution against real-world attacks from untrust participants.

</details>


### [3] [Distributed Security: From Isolated Properties to Synergistic Trust](https://arxiv.org/abs/2602.18063)
*Minghui Xu*

Main category: cs.CR

TL;DR: 분산 보안의 발전을 개관하고, 개별 보안 속성을 독립적으로 연구하는 것에서 이들의 상호작용을 이해하는 방향으로의 전환을 주장한다.


<details>
  <summary>Details</summary>
Motivation: 분산 보안의 역사적 발전을 검토하고, 현대의 적대적 환경에서 이러한 구조가 어떻게 발전했는지를 탐구하기 위해.

Method: 보안 속성, 즉 합의, 일관성, 프라이버시, 검증 가능성, 책임성을 정의하고 이들의 이론적 기원과 실제 발전을 추적한다.

Result: 보안 속성들 간의 융합이 각각 개별 속성으로는 달성할 수 없는 새로운 능력을 생성함을 입증한다.

Conclusion: 분산 보안의 미래는 개별 속성을 향상시키는 것이 아니라 이들의 시너지를 이해하고 활용하여 신뢰의 단일 구조를 구축하는 데 있다.

Abstract: Over the past four decades, distributed security has undergone a remarkable transformation -- from crash-fault tolerant protocols designed for controlled environments to sophisticated Byzantine-resilient architectures operating in open, adversarial settings. This vision paper examines this evolution and argues for a fundamental shift in how we approach distributed security: from studying individual security properties in isolation to understanding their synergistic combinations. We begin by conclude four foundational properties, \textit{agreement, consistency, privacy, verifiability, accountability}. We trace their theoretical origins and practical maturation. We then demonstrate how the frontier of research now lies at the intersection of these properties, where their fusion creates capabilities that neither property could achieve alone. Looking forward, we identify critical research challenges: discovering new security properties driven by emerging applications, developing systematic frameworks for property convergence, managing the computational overhead of cryptographic primitives in high-performance consensus layers, and addressing post-quantum and human-factor challenges. The future of distributed security lies not in improving individual properties, but in understanding and harnessing their synergies to build a singular fabric of trust.

</details>


### [4] [Dynamic Deception: When Pedestrians Team Up to Fool Autonomous Cars](https://arxiv.org/abs/2602.18079)
*Masoud Jamshidiyan Tehrani,Marco Gabriel,Jinhan Kim,Paolo Tonella*

Main category: cs.CR

TL;DR: 이 논문에서는 다수의 동적 요소가 협력하여 자율 주행 시스템에 대한 효과적인 공격을 수행하는 새로운 시스템 수준 공격을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 많은 적대적 공격이 자율 주행 인식 모델에 대해 효과적이지 않은 이유는 차량의 동력학으로 인해 공격의 효과가 일시적이라는 것입니다.

Method: 여러 동적 요소가 협력적으로 적대적 패치를 부착하고 그 효과를 증폭시키는 시스템 수준 공격을 제안합니다.

Result: CARLA 시뮬레이터에서 자율 주행 에이전트를 사용하여 평가한 결과, 단일 보행자 공격은 10회 중 모두 실패한 반면, 두 명의 보행자가 협력하는 경우 최대 50%의 경우에 차량이 완전히 정지했습니다.

Conclusion: 이 결과는 적대적 신호가 시간에 걸쳐 지속되고 협력하는 행위자에 의해 증폭될 때 시스템 수준에서의 실패가 발생함을 보여줍니다.

Abstract: Many adversarial attacks on autonomous-driving perception models fail to cause system-level failures once deployed in a full driving stack. The main reason for such ineffectiveness is that once deployed in a system (e.g., within a simulator), attacks tend to be spatially or temporally short-lived, due to the vehicle's dynamics, hence rarely influencing the vehicle behaviour. In this paper, we address both limitations by introducing a system-level attack in which multiple dynamic elements (e.g., two pedestrians) carry adversarial patches (e.g., on cloths) and jointly amplify their effect through coordination and motion. We evaluate our attacks in the CARLA simulator using a state-of-the-art autonomous driving agent. At the system level, single-pedestrian attacks fail in all runs (out of 10), while dynamic collusion by two pedestrians induces full vehicle stops in up to 50\% of runs, with static collusion yielding no successful attack at all. These results show that system-level failures arise only when adversarial signals persist over time and are amplified through coordinated actors, exposing a gap between model-level robustness and end-to-end safety.

</details>


### [5] [Can AI Lower the Barrier to Cybersecurity? A Human-Centered Mixed-Methods Study of Novice CTF Learning](https://arxiv.org/abs/2602.18172)
*Cathrin Schachner,Jasmin Wachter*

Main category: cs.CR

TL;DR: 이 논문은 Cybersecurity AI를 통해 CTF 기반 침투 테스트에 진입하는 초보자의 경험을 조사하고, AI가 초기 진입 장벽을 어떻게 줄이는지에 대한 통찰을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: CTF 대회가 초보자에게 제공하는 도전과제와 그에 대한 AI의 역할을 이해하기 위함입니다.

Method: AI를 활용한 사례 연구로, 초보자가 CTF 기반의 침투 테스트에 접근하는 방법을 분석했습니다.

Result: AI가 구조와 지침을 제공하여 초보자의 초기 진입 장벽을 줄이는 것을 발견했습니다.

Conclusion: AI 지원 교육은 새로운 도전 과제를 초래하지만, 인간 중심의 AI 지원 사이버 보안 교육의 중요성을 강조합니다.

Abstract: Capture-the-Flag (CTF) competitions serve as gateways into offensive cybersecurity, yet they often present steep barriers for novices due to complex toolchains and opaque workflows. Recently, agentic AI frameworks for cybersecurity promise to lower these barriers by automating and coordinating penetration testing tasks. However, their role in shaping novice learning remains underexplored.
  We present a human-centered, mixed-methods case study examining how agentic AI frameworks -- here Cybersecurity AI (CAI) -- mediates novice entry into CTF-based penetration testing. An undergraduate student without prior hacking experience attempted to approach performance benchmarks from a national cybersecurity challenge using CAI. Quantitative performance metrics were complemented by structured reflective analysis of learning progression and AI interaction patterns.
  Our thematic analysis suggest that agentic AI reduces initial entry barriers by providing overview, structure and guidance, thereby lowering the cognitive workload during early engagement. Quantitatively, the observed extensive exploration of strategies and low per-strategy execution time potetially facilitatates cybersecurity training on meta, i.e. strategic levels. At the same time, AI-assisted cybersecurity education introduces new challenges related to trust, dependency, and responsible use. We discuss implications for human-centered AI-supported cybersecurity education and outline open questions for future research.

</details>


### [6] [FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators](https://arxiv.org/abs/2602.18304)
*Darsh Asher,Farshad Dizani,Joshua Kalyanapu,Rosario Cammarota,Aydin Aysu,Samira Mirbagher Ajorpaz*

Main category: cs.CR

TL;DR: 이 논문에서는 하드웨어 수준의 백엔드 데이터 유출 공격인 FEATUREBLEED를 제시하며, AI 가속기에서의 제로 스키핑을 이용하여 민감한 데이터의 프라이버시를 위협하는 방법을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 백엔드 강화가 제품 추천, 의료 및 금융과 같은 민감한 도메인에서 널리 사용되고 있으며, 이러한 모델들은 기밀 데이터에 대해 훈련되고 API 호출자에게서 숨겨진 개인 피처의 값을 검색한다.

Method: FEATUREBLEED는 AI 가속기의 제로 스키핑을 이용하여 전력 분석, DVFS 조작 또는 공유 캐시 사이드 채널에 의존하지 않고, 순수하게 엔드 투 엔드 타이밍을 통해 개인 백엔드 검색 피처를 추론한다.

Result: FEATUREBLEED는 세 가지 데이터셋(의료 및 비의료 도메인을 아우름)에서 평가되었으며, CPU와 GPU 가속기, 데이터 양식 및 응용 도메인 전반에 걸쳐 유출이 일반화됨을 보여주었다. 적대적 이점은 최대 98.87 포인트에 달했다.

Conclusion: 현대 하드웨어에서의 유출 원인은 희소성에 의해 유도된 제로 스키핑이며, 제로 스키핑을 비활성화하는 경우 Intel AMX의 작업당 에너지가 최대 25% 증가하고 100%의 성능 오버헤드가 발생한다. 우리는 Worst-case 실행 시간을 동등하게 하여 타이밍 유출을 마스킹하는 패딩 기반 방어책을 제안하며, 평균 7.24% 성능 오버헤드를 달성하고 추가 전력 비용 없이 보호한다.

Abstract: Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.
  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.
  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.

</details>


### [7] [Drawing the LINE: Cryptographic Analysis and Security Improvements for the LINE E2EE Protocol](https://arxiv.org/abs/2602.18370)
*Benjamin Dowling,Prosanta Gope,Mehr U Nisa,Bhagya Wimalasiri*

Main category: cs.CR

TL;DR: 이 논문은 LINEv2 메시징 프로토콜의 최초의 검증 가능한 보안 분석을 제공하여 실제 환경에서의 암호화 보장을 중점적으로 다룬다.


<details>
  <summary>Details</summary>
Motivation: LINE은 태국과 일본을 포함한 많은 동아시아 국가에서 가장 인기 있는 커뮤니케이션 플랫폼 중 하나로, 보안 보장을 이해하는 것이 필수적이다.

Method: Multi-Stage Key Exchange (MSKE) 모델을 수정하여 LINE 메시징 프로토콜의 아키텍처와 보안을 캡처한다.

Result: LINEv2는 키 구분 불능성과 메시지 인증과 같은 기본 보안 속성을 달성하지만, 앞으로의 비밀성과 후속 타협 보안이 부족함을 강조한다.

Conclusion: 이 논문에서는 LINE 프로토콜의 강화 버전을 제안하고 FS와 PCS를 도입하여 결과를 분석하고 벤치마킹한다.

Abstract: LINE has emerged as one of the most popular communication platforms in many East Asian countries, including Thailand and Japan, with millions of active users. Therefore, it is essential to understand its security guarantees. In this work, we present the first provable security analysis of the LINE version two (LINEv2) messaging protocol, focusing on its cryptographic guarantees in a real-world setting. We capture the architecture and security of the LINE messaging protocol by modifying the Multi-Stage Key Exchange (MSKE) model, a framework for analysing cryptographic protocols under adversarial conditions. While LINEv2 achieves basic security properties such as key indistinguishability and message authentication, we highlight the lack of forward secrecy (FS) and post-compromise security (PCS). To address this, we introduce a stronger version of the LINE protocol, introducing FS and PCS to LINE, analysing and benchmarking our results.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: CodeScaler는 실행 기반 피드백 없이 코드 생성을 위한 강화 학습의 훈련과 테스트 시간 추론을 확장하는 데 특화된 보상 모델이다.


<details>
  <summary>Details</summary>
Motivation: RLVR은 고품질 테스트 케이스의 가용성 및 신뢰성에 의해 제약을 받으므로, 새로운 보상 모델이 필요하다.

Method: CodeScaler는 검증된 코드 문제에서 파생된 선호 데이터로 훈련되며, 구문 인식 코드 추출 및 유효성 유지 보상 형태를 통합하여 안정적인 최적화를 보장한다.

Result: CodeScaler는 Qwen3-8B-Base의 성능을 평균 +11.72 포인트 향상시켜 바이너리 실행 기반 RL보다 +1.82 포인트 우수하며 테스트 케이스 없이 합성 데이터세트에서 확장이 가능하다.

Conclusion: CodeScaler는 테스트 시간에 효과적인 스케일링 방법을 제공하고, RM-Bench의 기존 보상 모델을 초과하는 성과를 보인다.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [9] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: 본 연구는 의료 비전-언어 모델의 강인성을 강조하는 Robust Multi-Modal Masked Reconstruction (Robust-MMR) 프레임워크를 제안하며, 여러 의료 비전-언어 벤치마크에서 성능 향상을 보였다.


<details>
  <summary>Details</summary>
Motivation: 의료 이미지와 임상 텍스트에 대한 공동 추론을 위한 의료 비전-언어 모델의 성능이 이미지 장치, 획득 프로토콜, 보고 스타일의 변이에 따라 저하되는 문제를 해결하고자 함.

Method: Robust-MMR은 비대칭 변동 인식 마스킹, 도메인 일관성 정규화 및 모달리티 회복 탄력성 제약을 통합하여 강인성 목표를 명시적으로 포함시키는 자기 지도 학습 프레임워크이다.

Result: Robust-MMR은 다양한 의료 비전-언어 벤치마크에서 우수한 성능을 보였으며, VQA-RAD에서 78.9%의 교차 도메인 정확도로 가장 강력한 기본선보다 3.8% 높은 성과를 기록했다. 또한, SLAKE와 VQA-2019에서는 각각 74.6% 및 77.0%의 정확도를 달성하였다.

Conclusion: 사전 훈련 과정에서 강인성을 명시적으로 모델링하는 것이 실제 적용을 위한 더 신뢰할 수 있고 전이 가능한 의료 비전-언어 표현으로 이어진다는 점을 보여준다.

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [10] [Agentic Unlearning: When LLM Agent Meets Machine Unlearning](https://arxiv.org/abs/2602.17692)
*Bin Wang,Fan Wang,Pingping Wang,Jinyu Cong,Yang Yu,Yilong Yin,Zhongyi Han,Benzheng Wei*

Main category: cs.LG

TL;DR: 이 논문은 에이전트의 폐쇄형 상호작용에서 모델 매개변수와 지속 메모리 모두에서 특정 정보를 제거하는 '행위적 재학습(agentic unlearning)'을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 재학습 방법은 매개변수만을 대상으로 하여,(parametric remnants) 매개변수-메모리 백플로우와 매개변수 및 메모리 경로를 모두 아우르는 통합 전략의 부재라는 두 가지 주요한 간극을 남겨두고 있습니다.

Method: Synchronized Backflow Unlearning(SBU)라는 프레임워크를 제시하여 매개변수와 메모리 경로를 동시에 재학습합니다. 이 메모리 경로는 의존성 폐쇄 기반의 재학습을 수행하며 고립된 엔티티를 가지치기하고 공유 아티팩트를 논리적으로 무효화합니다. 매개변수 경로는 스토캐스틱 참조 정렬을 사용하여 모델 출력을 높은 엔트로피 사전으로 유도합니다.

Result: 의학 QA 벤치마크 실험 결과, SBU는 제한된 데이터 손실로 두 경로 모두에서 대상 개인 정보의 흔적을 줄이는 데 성공했습니다.

Conclusion: 메모리 재학습과 매개변수 억제가 서로를 강화하는 폐쇄 루프 메커니즘을 형성합니다.

Abstract: In this paper, we introduce \textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.

</details>


### [11] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델의 inference 시 구성 선택을 체계적으로 분석하기 위해 가변성 관리 기법을 적용한 새로운 접근 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 추론 과정은 높은 계산 요구량으로 인해 에너지 효율성과 지속 가능성에 대한 우려를 야기하고 있다.

Method: LLM을 가변 시스템으로 간주하고, feature-based 가변성 모델을 사용하여 생성 하이퍼파라미터와 제약 조건을 표현하고, 대표적인 구성을 샘플링하여 에너지 소비, 지연 시간, 정확성을 측정하였다.

Result: 가변성 모델링이 LLM 추론 구성을 효과적으로 관리하며, 하이퍼파라미터 효과와 상호작용의 체계적 분석을 가능하게 한다.

Conclusion: 이 연구는 소프트웨어 공학과 기계 학습의 새로운 연구 방향을 열어주며, LLM의 효율적이고 지속 가능한 구성을 위한 가변성 모델링을 활용한다.

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [12] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: PaCoDi(Parellel Complex Diffusion)는 주파수 도메인에서 생성 모델링을 분리하여 시간 시계열 생성의 representational capacity와 computational efficiency 간의 균형을 맞춘다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 생성에서 장기 의존성 모델링은 표현 용량과 계산 효율성 간의 근본적인 균형을 요구한다.

Method: 주파수 도메인에서 생성 모델링을 분리하는 PaCoDi(Parallel Complex Diffusion)라는 스펙트럼-내추럴 아키텍처를 도입했다.

Result: PaCoDi는 기존의 기준선을 시간 시계열 모델링에서 생성 품질과 추론 속도의 두 가지 측면에서 능가한다.

Conclusion: PaCoDi는 시간 시계열 모델링을 위한 이론적으로 근거하고 계산적으로 효율적인 솔루션을 제공한다.

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [13] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: MePoly는 확률 밀도를 제공하여 복잡한 의사결정 문제를 해결하는 새로운 정책 매개변수화를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 파라메트릭 정책이 해결 솔루션의 다중 모드를 표현하는 데 어려움을 겪고 있기 때문이다.

Method: 다항 에너지 기반 모델을 사용하여 명시적이고 다루기 쉬운 확률 밀도를 제공하는 MePoly를 제안한다.

Result: MePoly가 복잡한 비볼록 매니폴드를 효과적으로 포착하고 다양한 벤치마크에서 성능 기준선을 능가함을 보여준다.

Conclusion: MePoly는 복잡한 의사결정 문제 해결을 위한 새로운 접근 방식을 제공한다.

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [14] [Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865)
*Andrzej Podobiński,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: GAN을 활용한 데이터 증강이 금융 예측 정확도를 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 정확한 예측이 전략적 결정을 이끄는 금융과 같은 변동성이 큰 도메인에서 예측 정확도를 높이기 위한 데이터 증강의 필요성이 있다.

Method: GAN을 사용하여 생성된 합성 데이터로 증강된 데이터셋에서 LSTM 예측 모델을 훈련한다.

Result: 합성 데이터로 증강된 데이터셋에서 훈련된 LSTM 모델이 실제 데이터만 사용할 때보다 예측 정확도가 크게 향상된다.

Conclusion: 금융 예측 능력 향상을 위한 GAN 기반 데이터 증강의 이점을 입증한다.

Abstract: Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

</details>


### [15] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 이 연구는 만성 비염 환자의 수술 이점을 예측하기 위해 감독 기계 학습 모델을 평가하였으며, SNOT-22를 환자 보고 결과로 사용하여 수술 추천에 관한 정확한 예측을 제공하였다.


<details>
  <summary>Details</summary>
Motivation: 의료 예후에서 인공지능의 변혁 가능성을 탐구하고, 관찰 연구에서 기계 학습 예측의 응용을 조사함으로써 비용 절감 및 환자 결과 개선을 도모한다.

Method: 기계 학습 모델을 사용하여 SNOT-22를 주요 환자 보고 결과로 하여 CRS 수술 이점을 예측하였다. 사전 수술 데이터만으로 훈련한 모델이 예측적 정확성을 가져야 한다.

Result: 여러 알고리즘을 통해 약 85%의 분류 정확도를 달성하였으며, 30건의 혼합 난이도 사례에서 80%의 정확도를 보였다.

Conclusion: 이 연구는 기계 학습 모델이 CRS 환자의 수술 결정을 지원하고 개인화된 치료에 기여할 수 있음을 시사한다.

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [16] [MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance](https://arxiv.org/abs/2602.17930)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: MIRA는 메모리 그래프를 활용하여 초기 학습을 개선하는 강화 학습 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 희소한 보상 환경에서의 높은 샘플 복잡도 문제를 해결하기 위해, 대형 언어 모델을 활용한 초기 학습 개선이 필요하다.

Method: 구조화된 메모리 그래프를 통해 결정 관련 정보를 저장하고, 에이전트의 경험 및 LLM 출력을 결합하여 초기 훈련을 돕는다.

Result: MIRA는 기존 강화 학습 방법론 및 정기적인 LLM 감독 방식보다 뛰어난 성능을 보인다.

Conclusion: MIRA는 훈련이 진행됨에 따라 LLM 기반 사전 지식을 초과하고, 초기 학습을 개선함으로써 표준 수렴 보장을 유지한다.

Abstract: Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>


### [17] [Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2602.17931)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 이 논문은 희소하거나 지연된 보상이 있는 환경에서 강화 학습의 샘플 복잡성을 줄이기 위해 메모리 그래프를 구축하여 하위 목표 및 경로를 인코딩하고, 이를 통해 더 나은 샘플 효율성과 초기 학습 속도를 제공함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 희소하거나 지연된 보상이 있는 환경에서 강화 학습의 높은 샘플 복잡성을 극복하기 위해 대형 언어 모델을 활용하고자 하는 동기.

Method: LLM 안내와 에이전트의 성공적인 롤아웃에서 하위 목표 및 경로를 인코딩하는 메모리 그래프를 구성하고, 이 그래프에서 유틸리티 함수를 도출하여 에이전트의 경로가 이전 성공 전략과 얼마나 일치하는지를 평가.

Result: 기초 강화 학습 방법에 비해 향상된 샘플 효율성과 더 빠른 초기 학습 속도를 보였으며, 최종 수익은 자주 LLM 상호작용을 요구하는 방법들과 유사했다.

Conclusion: 이 방법은 주로 오프라인 입력에 의존하며, 간헐적인 온라인 질의만으로 LLM 지속적인 감독에 대한 의존성을 피함을 보여준다.

Abstract: In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.

</details>


### [18] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 본 논문에서는 하이퍼파라미터 최적화를 위한 편향-분산 분해 기법을 제시하고, 하이퍼그래디언트 추정의 성능을 개선하는 앙상블 전략을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 하이퍼파라미터 최적화에서 편향과 분산 간의 상충 관계를 해결하고 성능 저하를 방지하기 위해.

Method: 하이퍼그래디언트 추정 오차에 대한 편향-분산 분해를 수행하고, 기존 연구에서 무시된 분산 항에 대한 상세한 분석을 제공한다. 앙상블 하이퍼그래디언트 전략을 통해 HPO 알고리즘의 분산을 줄인다.

Result: 정규화 하이퍼파라미터 학습, 데이터 하이퍼 클리닝, 그리고 몇 샷 학습 과제를 포함한 실험 결과, 우리의 분산 감소 전략이 하이퍼그래디언트 추정을 개선함을 보여준다.

Conclusion: 과도한 오차와 하이퍼그래디언트 추정 간의 관계를 설정하여 실험적 관찰에 대한 이해를 심화시킨다.

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [19] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: 이 논문은 깊은 학습에서 깨끗한 정확성과 적대적 강건성 간의 균형에 대한 기하학적 원인을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝에서 깨끗한 정확성과 적대적 강건성 간의 트레이드오프 현상을 이해하고 그 기하학적 기원을 밝히고자 합니다.

Method: 우리는 대칭 파괴 차원 확장(SBDE)을 사용하여 입력 이미지를 상수값 픽셀로 확장하여 이 트레이드오프의 기전을 조사했습니다.

Result: SBDE는 CIFAR-10에서 ResNet-18으로 정확도를 90.47%에서 95.63%로 개선하지만, 반복적인 화이트박스 공격에 대한 강건성은 감소합니다.

Conclusion: 이 연구는 정확도와 강건성의 역설에 대한 구체적인 기하학적 설명을 제공하여, 최적화 랜드스케이프가 정확도를 개선하는 깊은 매력을 형성하는 동시에 보조 자유도 측면에서 급격한 벽을 세우는 것을 보여줍니다.

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [20] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 이 논문에서는 숨겨진 혼란변수가 있는 오프라인 데이터셋에서 학습하는 문제를 다루고, 이를 해결하기 위한 새로운 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습(RL)과 딥 러닝의 발전에도 불구하고, 고위험 응용 분야에서의 의사 결정 정책 학습은 여전히 도전적이다.

Method: 숨겨진 혼란변수를 고려하여 인과적 효과를 식별하기 위해 도구 변수(IV)를 사용하고, 경쟁력 있는 샘플 효율 알고리즘을 유도한다.

Result: 제안된 알고리즘은 기존 최첨단 알고리즘보다 뛰어난 성능을 보인다.

Conclusion: 이 연구를 통해 실제 의사 결정에서 방법의 유용성을 입증하였다.

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [21] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 이 논문은 머신 러닝의 수학적 이론에 근거한 세 가지 프로젝트에 대해 다룹니다: 지도 학습, 전이 학습, 분류 문제.


<details>
  <summary>Details</summary>
Motivation: 대량의 데이터와 관련된 실제 문제를 해결하는 데 머신 러닝이 핵심적입니다. 신경망의 성공으로 머신 러닝 연구가 증가하고 있습니다.

Method: 첫 번째 프로젝트는 지도 학습과 다양체 학습을 탐구하고, 두 번째는 전이 학습을 다루며, 세 번째는 액티브 러닝 패러다임의 분류 작업에 중점을 둡니다.

Result: 각 프로젝트에서는 함수 근사, 데이터의 부분적 이해에 기초한 리프팅, 그리고 신호 분리 기술을 활용한 분류와 같은 다양한 방법론을 소개합니다.

Conclusion: 이 논문은 기존의 한계를 극복하고, 빠르고 정확한 머신 러닝 모델 개발을 위한 새로운 접근 방식을 제안합니다.

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [22] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 이 논문은 실질적 환경에서 LLM 생성 기계 모델의 신뢰성을 평가하고, 이를 개선하기 위한 NIMMgen 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기계 모델의 신뢰성을 확보하고 실제 환경에서의 적용 가능성을 높이기 위해.

Method: Neural-Integrated Mechanistic Modeling(NIMM) 평가 프레임워크를 사용하여 LLM이 생성한 기계 모델을 부분 관측 및 다양한 작업 목표 하에 평가함.

Result: 현재 기준선에서 모델 효과성 및 코드 수준의 정확성에 대한 근본적인 도전 과제가 발견되었음.

Conclusion: NIMMgen 프레임워크는 코드 정확성과 실용적 유효성을 향상시키며, 세 가지 다양한 과학 분야의 데이터 세트를 통해 강력한 성능을 입증하였다.

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [23] [Flow Actor-Critic for Offline Reinforcement Learning](https://arxiv.org/abs/2602.18015)
*Jongseong Chae,Jongeui Park,Yongjae Shin,Gyeongmin Kim,Seungyul Han,Youngchul Sung*

Main category: cs.LG

TL;DR: 이 논문은 복잡한 오프라인 강화학습 데이터셋을 다루기 위한 새로운 액터-비평가 방법인 Flow Actor-Critic을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 오프라인 강화학습에서 데이터셋 분포가 복잡하고 다중 모드를 가지므로, 기존의 가우시안 정책을 넘어서는 표현력을 갖춘 정책이 필요합니다.

Method: Flow Actor-Critic 방법은 이전의 플로우 정책에서 액터로 플로우 모델을 활용하며, Q-값의 폭주를 방지하기 위해 보수적인 비평가 가속을 위한 표현력 있는 플로우 모델을 이용합니다.

Result: 제안된 방법을 통해 D4RL 및 최신 OGBench 벤치마크를 포함한 오프라인 RL 테스트 데이터셋에서 새로운 최첨단 성능을 달성하였습니다.

Conclusion: 이 논문에서 제안된 Flow Actor-Critic 방법은 복잡한 데이터셋에 대한 적합성이 뛰어나며 오프라인 강화학습의 성능을 개선하는데 기여합니다.

Abstract: The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.

</details>


### [24] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: 그래프 생성 모델에서 등가성은 그래프의 치환 대칭을 존중하는 데 중요하지만, 엄격한 등가성은 계산 비용을 증가시킬 수 있다. 이 연구에서는 동적 대칭 조절 기법을 적용하여 훈련 중 등가성을 완화하는 방법을 제시하고, 이를 통해 과적합을 방지하면서 수렴 속도를 증가시키는 결과를 얻었다.


<details>
  <summary>Details</summary>
Motivation: 그래프 생성 모델의 등가성이 중요하나, 이는 계산 비용과 수렴 속도에 부담을 줄 수 있다.

Method: 신호 대칭 조절 기법을 사용하여 훈련 중 그래프 생성 모델의 등가성을 완화한다.

Result: 대칭 파괴는 초기 훈련을 가속화하지만 과적합을 초래할 수 있으며, 적절한 대칭 신호 조절은 과적합을 지연시키면서 수렴 속도를 증가시킨다.

Conclusion: 모델이 기초 훈련 에폭의 19%로 더 강력한 성과에 도달할 수 있도록 한다.

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [25] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: FINO라는 새로운 방법은 오프라인에서 온라인으로의 강화 학습을 위한 샘플 효율성을 높이기 위해 흐름 일치를 기반으로 한 정책을 활용한다. 이는 정책 훈련에 노이즈를 주입하여 탐사를 촉진하고, 바람직한 행동을 조정할 수 있는 엔트로피 유도 샘플링 메커니즘을 결합한다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 생성 모델의 사용이 증가하고 있으며, 특히 오프라인 RL에서 강력한 성능을 보여주지만, 온라인 미세 조정으로의 확장은 여전히 주요한 도전 과제가 있다.

Method: FLOW Matching with Injected Noise for Offline-to-Online RL (FINO) 방법은 샘플 효율성을 향상시키기 위해 흐름 일치를 기반으로 한 정책을 사용하고, 정책 훈련에 노이즈를 주입하여 탐사를 촉진하는 방식이다.

Result: 다양하고 도전적인 작업에서 실험을 통해 FINO가 제한된 온라인 예산 하에서도 일관되게 우수한 성능을 달성함을 보였다.

Conclusion: FINO는 오프라인 RL 작업을 온라인으로 성공적으로 이전할 수 있는 가능성을 제시하여, 강화 학습의 효율성을 높인다.

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [26] [Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction](https://arxiv.org/abs/2602.18227)
*Redwanul Karim,Changhun Kim,Timon Conrad,Nora Gourmelon,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 도메인 변화에 따른 정확한 AC-PF 예측은 중전압(MV) 그리드에서 학습된 모델이 고전압(HV) 네트워크에서 배포될 때 중요하다. 본 논문에서는 물리적 손실을 통해 Kirchhoff 일관성을 촉진하면서 저랭크 업데이트로 적응을 제한하는 물리정보 기반 자가 주의 그래프 신경망(GNN)에 대한 매개변수 효율적인 도메인 적응을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 중전압(MV) 그리드에서 학습된 모델을 고전압(HV) 네트워크에 배포 할 때 정확한 AC-PF 예측이 중요하다.

Method: 물리정보 기반 손실을 통해 Kirchhoff 일관성을 촉진하고 저랭크 업데이트로 적응을 제한하는 LoRA를 자가 주의 기법에 적용한다.

Result: LoRA+PHead 적응을 통해 85.46%의 학습 가능한 매개변수를 줄이면서도 타겟 도메인 RMSE 갭이 $2.6	imes10^{-4}$인 거의 전체 미세 조정 정확도를 회복한다.

Conclusion: LoRA+PHead는 도메인 변화에서 MV 소스 유지를 4.7% 포인트 줄이면서도 매개변수 효율적이고 물리적으로 일관된 AC-PF 추정을 가능하게 한다.

Abstract: Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.

</details>


### [27] [[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games](https://arxiv.org/abs/2602.18230)
*Jorge Carrasco Pollo,Ioannis Kapetangeorgis,Joshua Rosenthal,John Hua Yao*

Main category: cs.LG

TL;DR: 대형 언어 모델이 다중 에이전트 협상 작업에서 잠재력을 보이는 반면, 평가가 어려운 이유는 견고하고 일반화 가능한 기준이 부족하기 때문이다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 LLM을 위한 복잡하고 현실적인 평가 프레임워크 개발을 목표로 하며, 기존의 벤치마크의 재현 가능성을 조사한다.

Method: 원래의 실험을 추가 모델에 대해 복제하고, 협상 품질과 평가의 균일성을 검증하기 위한 추가 지표를 도입한다.

Result: 기준이 복잡하긴 하지만, 모델 간 비교가 모호하여 객관성에 대한 의문이 제기된다.

Conclusion: 실험 설정의 한계를 확인하고, 더 많은 모델의 행동을 분석하여 잠재 사용자에게 추가적인 통찰력을 제공한다.

Abstract: Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.

</details>


### [28] [A Probabilistic Framework for LLM-Based Model Discovery](https://arxiv.org/abs/2602.18266)
*Stefan Wahl,Raphaela Schenk,Ali Farnoud,Jakob H. Macke,Daniel Gedon*

Main category: cs.LG

TL;DR: 관찰 데이터로부터 메커니즘 시뮬레이터 모델을 발견하기 위한 자동화된 방법이 과학적 진전을 가속화하는 유망한 경로를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 자동화된 방법이 관찰 데이터로부터 모델을 발견하는 데 중요한 역할을 할 수 있기 때문입니다.

Method: 모델 발견을 확률적 추론으로 재구성하며, 이는 데이터를 설명할 수 있는 메커니즘 모델에 대한 미지의 분포에서 샘플링하는 것입니다.

Result: 실제 과학 시스템에 대한 실험을 통해 해석 가능한 메커니즘을 가진 모델을 발견하고 후행 예측 검사를 개선했습니다.

Conclusion: 이 시각은 모델 발견을 이해하고 개발하는 데 있어 확률적인 시각을 제공합니다.

Abstract: Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.

</details>


### [29] [PRISM: Parallel Reward Integration with Symmetry for MORL](https://arxiv.org/abs/2602.18277)
*Finn van der Knaap,Kejiang Qian,Zheng Xu,Fengxiang He*

Main category: cs.LG

TL;DR: 이 연구는 시간 빈도가 크게 다른 이질적인 다목적 강화 학습(MORL)을 다룬다. PRISM 알고리즘을 제안하여 보상 채널 정렬에서 반사 대칭을 유도적 편견으로 적용한다.


<details>
  <summary>Details</summary>
Motivation: 시간적 빈도가 상이한 목표들이 존재하는 이질적 다목적 강화 학습 환경에서 학습의 효율성을 높이기 위한 필요성.

Method: PRISM 알고리즘과 ReSymNet 모델을 통해 목표 간 시간 빈도 불일치를 조정하고, 대칭성을 적용하여 정책 탐색을 제한하는 SymReg 정규화를 사용한다.

Result: PRISM은 MuJoCo 벤치마크에서 희소 보상 기준 및 전체 밀집 보상으로 훈련된 오라클을 지속적으로 초월하며, 100
a를 초과하는 하이퍼볼륨 증가를 달성했다.

Conclusion: 이 연구는 제안된 알고리즘이 샘플 효율성을 개선하고 일반화를 향상시킴을 증명하였다.

Abstract: This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.

</details>


### [30] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: JPmHC는 비슷한 잔여 연결의 문제를 해결하며 안정적이고 효율적인 심층 학습을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 심층 학습의 발전은 잔여 연결의 정체성 매핑 속성을 저해하여 학습 불안정성과 메모리 오버헤드를 초래하고 있습니다.

Method: JPmHC는 n개의 병렬 스트림에 작용하는 학습 가능한 선형 믹서를 사용하여 정체성 건너뛰기를 대체하고, 그래디언트 조건을 명시적으로 제어합니다.

Result: JPmHC는 ARC-AGI에서 기존의 bistochastic 기준보다 더 빠른 수렴, 높은 정확도, 낮은 계산 비용을 달성했습니다.

Conclusion: JPmHC는 스펙트럼 인식, 안정성 및 효율성을 위한 확장된 HC로, 토폴로지 아키텍처 디자인 및 기초 모델 발전에 대한 통찰을 제공합니다.

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [31] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: AutoClustering 방법의 메타모델 설명 가능성을 조사하여, 기존 방법의 메타 특징을 구조적으로 정리하고, 글로벌 및 로컬 설명 가능성 기술을 적용하여 현재의 메타 학습 전략의 약점을 확인하고 자동화된 기계 학습 설계를 위한 실용적인 지침을 제공한다.


<details>
  <summary>Details</summary>
Motivation: AutoClustering 방법은 비지도 학습 작업을 자동화하고자 하며, 메타 학습을 활용하나 그 추천 결과의 설명 가능성이 부족하다.

Method: 22개의 기존 방법을 검토하고 메타 특징을 구조화된 분류로 정리한 후, 결정 술어 그래프를 사용하여 메타 모델 내의 특징 중요성을 평가하고, SHAP를 활용하여 특정 클러스터링 결정을 분석했다.

Result: 메타 특징의 관련성에 대한 일관된 패턴을 강조하고, 추천을 왜곡할 수 있는 현재 메타 학습 전략의 구조적 약점을 확인했다.

Conclusion: 이 연구는 비지도 학습 자동화에서의 결정 투명성을 높이기 위한 실용적인 기초를 제공한다.

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [Epistemic Traps: Rational Misalignment Driven by Model Misspecification](https://arxiv.org/abs/2602.17676)
*Xingcheng Xu,Jingjing Qu,Qiaosheng Zhang,Chaochao Lu,Yanqing Yang,Na Zou,Xia Hu*

Main category: cs.AI

TL;DR: 모델의 잘못된 명세화에서 발생하는 비정상적인 행동을 수학적으로 설명하는 프레임워크를 제안하며, 안전성은 에이전트의 인식적 사전 정보에 의해 결정된다는 것을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델과 AI 에이전트의 빠른 배치는 강화 학습으로 완화되지 않는 비정상적인 행동 문제에 의한 장애가 있다.

Method: Berk-Nash의 합리화 개념을 인공지능에 적용하여 결함 있는 주관적 세계 모델에 대해 최적화하는 모델로 에이전트를 모델링하는 엄격한 프레임워크를 도출하였다.

Result: 관찰된 실패가 보상 방식에 따라 안정적인 불일치 평형 또는 진동 주기를 생성한다는 것을 보여주며, 전략적 기만은 '고립된' 평형으로 지속되거나 객관적 위험에 대해 강건한 인식적 불확실성을 통해 나타난다.

Conclusion: 안전성은 보상 크기의 연속적인 함수가 아닌, 에이전트의 인식적 사전 정보로 결정되는 이산적 단계라는 것을 밝히며, 이는 환경 보상을 조작하는 것에서 에이전트의 현실 해석을 형성하는 것으로의 패러다임 전환을 나타낸다.

Abstract: The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a "locked-in" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.

</details>


### [33] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 이 논문은 언어 모델의 한계를 극복하기 위해 형식적 도메인 온톨로지가 어떻게 언어 모델의 신뢰성을 향상시킬 수 있는지를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 전문적인 분야에서 검증 가능한 추론이 필요한 언어 모델의 기본적인 한계를 극복하고자 한다.

Method: OpenMath 온톨로지를 활용한 신경-기호 파이프라인을 구현하여 하이브리드 검색 및 교차 인코더 재정렬을 통해 모델 프롬프트에 관련 정의를 주입한다.

Result: MATH 벤치마크를 사용한 평가 결과, 온톨로지 기반 컨텍스트가 검색 품질이 높은 경우 성능을 개선하지만, 관련이 없는 컨텍스트는 성능을 저하시키는 것으로 나타났다.

Conclusion: 이 결과는 신경-기호 접근 방식의 가능성과 도전 과제를 모두 강조한다.

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [34] [El Agente Gráfico: Structured Execution Graphs for Scientific Agents](https://arxiv.org/abs/2602.17902)
*Jiaru Bai,Abdulrahman Aldossary,Thomas Swanick,Marcel Müller,Yeonghun Kang,Zijian Zhang,Jin Won Lee,Tsz Wai Ko,Mohammad Ghazi Vakili,Varinia Bernales,Alán Aspuru-Guzik*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM(대형 언어 모델)을 활용해 과학적 워크플로우를 자동화하는 El Agente Gráfico라는 단일 에이전트 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)을 활용한 과학적 작업 자동화의 필요성이 증가하고 있으나, 이들을 다양한 컴퓨팅 도구와 통합하는 과정이 임의적이고 취약한 상황에서 어떻게 체계화할 수 있는지를 탐구합니다.

Method: El Agente Gráfico는 타입 안전한 실행 환경과 외부 지속성을 위한 동적 지식 그래프에 LLM 기반 의사 결정을 포함시키는 단일 에이전트 프레임워크입니다.

Result: 이 시스템은 신뢰할 수 있는 실행 엔진과 결합될 때 복잡한 다단계 및 병렬 계산을 강력하게 수행할 수 있음을 증명하였습니다.

Conclusion: 추상화 및 타입 안전성이 프롬프트 중심 디자인을 넘어서 에이전트 기반 과학적 자동화를 위한 확장 가능한 기초를 제공할 수 있음을 보여줍니다.

Abstract: Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gráfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.

</details>


### [35] [Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems](https://arxiv.org/abs/2602.17910)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.AI

TL;DR: APEOM(감정 인식 피크-종료 조절)은 장기 작업 흐름에서 신뢰성을 보장하기 위해 상호작용 궤적 전체에 대한 최적화를 수행하는 실행 시간 스케줄링 레이어입니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 AI 정렬은 개별 모델 출력에 초점을 맞추고 있지만, 자율 에이전트는 긴 작업 흐름에서 지속적인 신뢰성을 필요로 합니다.

Method: APEOMO는 정해진 예산 하에 시간-감정 신호를 운영하여 계산 자원을 최적화하는 실행 시간 스케줄링 레이어입니다. APEMO는 모델 가중치를 수정하는 대신 행동 프로시를 통해 궤적 불안정을 감지하고, 피크 순간 및 종료와 같은 중요한 지점에서 수리를 목표로 합니다.

Result: 다중 에이전트 시뮬레이션 및 LLM 기반 계획-실행 흐름을 통한 평가에서 APEMO는 구조 조정자에 비해 궤적 수준의 품질과 재사용 가능성을 지속적으로 향상시킵니다.

Conclusion: 우리의 결과는 정렬을 시간 제어 문제로 재구성하며, 장기 에이전트 시스템 개발을 위한 탄력적인 엔지니어링 경로를 제공합니다.

Abstract: Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.

</details>


### [36] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: LLM 기반 시스템이 복잡한 작업을 위한 구조화된 워크플로우를 생성하나, 평가가 어렵다는 문제를 다룹니다. LayerPerturb라는 베enchmark을 도입하여 워크플로우 평가 메트릭을 연구합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업을 위한 LLM 기반의 구조화된 워크플로우가 증가하고 있지만, 이러한 워크플로우의 자동 평가는 메트릭 점수의 불일치로 인해 어렵습니다.

Method: WorkflowPerturb라는 제어된 벤치마크를 통해, 황금 워크플로우에 현실적인 교란을 적용하여 평가 메트릭을 연구합니다.

Result: 4,973개의 황금 워크플로우와 44,757개의 교란 변형을 포함하며, 세 가지 교란 유형에서 다양한 메트릭 패밀리의 민감도와 보정 수준을 분석합니다.

Conclusion: 우리의 결과는 메트릭 패밀리 간의 체계적인 차이를 특징짓고, 워크플로우 평가 점수의 심각도 인식을 지원합니다. 데이터셋은 수락 시 공개됩니다.

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [37] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: 온라인 다중 에이전트 강화 학습(MARL)에서 정책 표현력을 높이는 것이 성능 향상에 핵심적이다.


<details>
  <summary>Details</summary>
Motivation: 정책 표현력을 높이는 것이 성능 향상에 필수적이다.

Method: Diffusion 정책을 사용하는 온라인 비정책 MARL 프레임워크(OMAD)를 제안한다.

Result: MPE와 MAMuJoCo에서의 광범위한 평가를 통해 10개의 다양한 작업에서 새로운 최첨단을 수립하고, 샘플 효율성에서 2.5배에서 5배의 놀라운 향상을 보인다.

Conclusion: 우리의 방법은 안정적인 조정을 보장하며, 분산된 확산 정책을 최적화한다.

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [38] [Reasoning-Native Agentic Communication for 6G](https://arxiv.org/abs/2602.17738)
*Hyowoon Seo,Joonho Seon,Jin Young Kim,Mehdi Bennis,Wan Choi,Dong In Kim*

Main category: cs.MA

TL;DR: 이 논문은 6G 네트워크에서의 커뮤니케이션을 재정의하여 자율적인 지능 간의 신뢰 차이를 해결하는 새로운 패러다임을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 미래의 6G 네트워크는 장치뿐만 아니라 지속적으로 감지하고, 추론하며 행동하는 자율 기계를 상호 연결할 것입니다.

Method: 이 논문은 신념 차이를 해결하기 위해 명시적으로 설계된 커뮤니케이션 접근 방식, 즉 추론 원주율 에이전틱 커뮤니케이션을 소개합니다.

Result: 제안된 프레임워크는 에이전트의 내부 신념 상태의 예상 불일치에 따라 커뮤니케이션을 활성화합니다.

Conclusion: 이 접근 방식은 자율 지능의 능동적인 조화자로서 6G 네트워크가 기능할 수 있게 합니다.

Abstract: Future 6G networks will interconnect not only devices, but autonomous machines that continuously sense, reason, and act. In such environments, communication can no longer be understood solely as delivering bits or even preserving semantic meaning. Even when two agents interpret the same information correctly, they may still behave inconsistently if their internal reasoning processes evolve differently. We refer to this emerging challenge as belief divergence. This article introduces reasoning native agentic communication, a new paradigm in which communication is explicitly designed to address belief divergence rather than merely transmitting representations. Instead of triggering transmissions based only on channel conditions or data relevance, the proposed framework activates communication according to predicted misalignment in agents internal belief states. We present a reasoning native architecture that augments the conventional communication stack with a coordination plane grounded in a shared knowledge structure and bounded belief modeling. Through enabling mechanisms and representative multi agent scenarios, we illustrate how such an approach can prevent coordination drift and maintain coherent behavior across heterogeneous systems. By reframing communication as a regulator of distributed reasoning, reasoning native agentic communication enables 6G networks to act as an active harmonizer of autonomous intelligence.

</details>


### [39] [MultiVer: Zero-Shot Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2602.17875)
*Shreshth Rajan*

Main category: cs.MA

TL;DR: MultiVer는 미세 조정 없이 최첨단 리콜 성능을 달성하는 제로샷 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 보안 애플리케이션에서 중요 지표를 개선하기 위한 방법을 탐색한다.

Method: 안전성, 정확성, 성능, 스타일의 네 가지 에이전트로 구성된 앙상블을 사용하여 투표 방식으로 작동한다.

Result: PyVul에서 82.7%의 리콜을 달성하고, SecurityEval에서 91.7%의 탐지율을 기록했다.

Conclusion: 제로샷 다중 에이전트 앙상블이 미세 조정된 모델을 초월하여 중요한 지표를 향상시킬 수 있음을 보여준다.

Abstract: We present MultiVer, a zero-shot multi-agent system for vulnerability detection that achieves state-of-the-art recall without fine-tuning. A four-agent ensemble (security, correctness, performance, style) with union voting achieves 82.7% recall on PyVul, exceeding fine-tuned GPT-3.5 (81.3%) by 1.4 percentage points -- the first zeroshot system to surpass fine-tuned performance on this benchmark. On SecurityEval, the same architecture achieves 91.7% detection rate, matching specialized systems. The recall improvement comes at a precision cost: 48.8% precision versus 63.9% for fine-tuned baselines, yielding 61.4% F1. Ablation experiments isolate component contributions: the multi-agent ensemble adds 17 percentage points recall over single-agent security analysis. These results demonstrate that for security applications where false negatives are costlier than false positives, zero-shot multi-agent ensembles can match and exceed fine-tuned models on the metric that matters most.

</details>


### [40] [Mean-Field Reinforcement Learning without Synchrony](https://arxiv.org/abs/2602.18026)
*Shan Yang*

Main category: cs.MA

TL;DR: 이 논문은 비동기 멀티 에이전트 강화 학습을 위한 새로운 프레임워크인 Temporal Mean Field (TMF)를 구축하고, 기존의 평균 행동에 의존하지 않고 인구 분포를 통해 에이전트의 보상과 전이를 결정하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 비동기 상황에서 모든 에이전트가 행동하지 않을 때 평균 행동을 정의할 수 없다는 문제를 해결하기 위해 필요하다.

Method: TMF 프레임워크를 인구 분포 $μ$에 기반하여 처음부터 구축하고, 동기식부터 비동기식 의사결정까지 포괄하는 이론을 제시한다.

Result: TMF 균형의 존재성과 유일성을 증명하고, 에이전트 수에 상관없이 유한 인구 근사 경계가 $O(1/	ext{sqrt}{N})$임을 확립하며, 정책 그래디언트 알고리즘(TM-PG)의 수렴성을 입증한다.

Conclusion: TMF-PG는 한 에이전트가 행동하든 모두가 행동하든 유사한 성능을 발휘하며, 근사 오차는 예측된 대로 $O(1/	ext{sqrt}{N})$ 비율로 감소한다.

Abstract: Mean-field reinforcement learning (MF-RL) scales multi-agent RL to large populations by reducing each agent's dependence on others to a single summary statistic -- the mean action. However, this reduction requires every agent to act at every time step; when some agents are idle, the mean action is simply undefined. Addressing asynchrony therefore requires a different summary statistic -- one that remains defined regardless of which agents act. The population distribution $μ\in Δ(\mathcal{O})$ -- the fraction of agents at each observation -- satisfies this requirement: its dimension is independent of $N$, and under exchangeability it fully determines each agent's reward and transition. Existing MF-RL theory, however, is built on the mean action and does not extend to $μ$. We therefore construct the Temporal Mean Field (TMF) framework around the population distribution $μ$ from scratch, covering the full spectrum from fully synchronous to purely sequential decision-making within a single theory. We prove existence and uniqueness of TMF equilibria, establish an $O(1/\sqrt{N})$ finite-population approximation bound that holds regardless of how many agents act per step, and prove convergence of a policy gradient algorithm (TMF-PG) to the unique equilibrium. Experiments on a resource selection game and a dynamic queueing game confirm that TMF-PG achieves near-identical performance whether one agent or all $N$ act per step, with approximation error decaying at the predicted $O(1/\sqrt{N})$ rate.

</details>
