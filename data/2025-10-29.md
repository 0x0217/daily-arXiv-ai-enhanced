<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.LG](#cs.LG) [Total: 32]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda,Takehiro Takayanagi*

Main category: cs.AI

TL;DR: PREFINE은 개인 사용자의 선호도를 반영한 창의적인 텍스트 생성을 위한 새로운 프레임워크로, 사용자 피드백이나 매개변수 업데이트 없이 개인화된 이야기를 생성한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 발전에도 불구하고个人화된 스토리를 생성하는 데 여전히 큰 도전이 있다.

Method: PREFINE은 사용자의 상호작용 기록을 기반으로 의사 사용자 에이전트를 구성하고 사용자를 위한 특정 평가 기준을 생성하여 이를 바탕으로 결과를 비판하고 개선한다.

Result: 자동 평가에서 PREFINE은 기준 모델보다 높은 승률과 통계적으로 유의미한 점수를 기록했다.

Conclusion: 이 방법은 대화 시스템, 교육, 추천과 같은 더 넓은 응용 분야에서도 효율적인 개인화가 가능하다.

Abstract: While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [2] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: 이는 경량 구조가 규약 형성을 어떻게 유도할 수 있는지를 연구하며, 이름 짓기 게임에서 자연어에 비해 빠른 수렴과 높은 합의를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 실제 AI 시스템이 복잡한 문제를 해결하기 위해 협력하고 있지만, 일관성이 없는 규약 개발로 인해 통합이 깨질 수 있다.

Method: 이 논문은 Schema-Induced Games for Naming (SIGN)라는 이름 짓기 게임을 소개하여 경량 구조가 규약 형성을 유도하는 방식을 조사한다.

Result: 규약 유도 통신이 자연어보다 빠른 수렴을 보였으며, 최대 5.8배 더 높은 합의율을 나타냈다.

Conclusion: 최소한의 구조가 효율적인 다중 에이전트 조정을 위한 간단한 제어 장치 역할을 할 수 있으며, 이름 짓기 게임을 넘어선 더 넓은 응용 가능성을 시사한다.

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [3] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: 이 논문은 모바일 플랫폼에서의 GUI 에이전트 시스템 구축을 위한 LightAgent라는 해결책을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 앱 생태계와 직관적인 터치 상호작용 덕분에 모바일 플랫폼에서 GUI 에이전트 시스템의 구축이 점점 더 유망한 방향이 되고 있다.

Method: LightAgent는 설계된 장치-클라우드 협업을 통해 on-device 모델의 비용 효율성과 클라우드 모델의 높은 능력을 결합하는 모바일 에이전틱 기초 모델 솔루션이다.

Result: LightAgent는 온라인 AndroidLab 벤치마크 및 다양한 앱에서 실험을 통해 더 큰 모델에 필적하거나 이에 가까운 성능을 보인다.

Conclusion: 여기에서 LightAgent는 클라우드 비용을 유의미하게 줄이면서도 우수한 성능을 제공한다.

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [4] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 메타 강화 학습(Meta-RL)에서 예측 코딩 모듈을 통합하면 베이즈 최적 표현 학습에 도움이 된다.


<details>
  <summary>Details</summary>
Motivation: 부분적으로 관찰 가능한 환경에서 역사에 대한 компакт한 표현 학습이 계획 및 일반화에 필수적이다.

Method: 신경 과학의 예측 코딩 및 심층 강화 학습의 보조 예측 목표에서 영감을 받아, 자기 감독식 예측 코딩 모듈을 메타-RL에 통합하는 방법을 조사했다.

Result: 예측 모듈을 포함한 메타-RL이 전통적인 메타-RL에 비해 베이즈 최적 신념 상태를 더 잘 근사하는 해석 가능한 표현을 일관되게 생성함을 보였다.

Conclusion: 더 나은 표현 학습이 향상된 일반화로 이어진다는 것을 입증하였다.

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [5] [Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms](https://arxiv.org/abs/2510.22052)
*Abhijit Chatterjee,Niraj K. Jha,Jonathan D. Cohen,Thomas L. Griffiths,Hongjing Lu,Diana Marculescu,Ashiqur Rasul,Keshab K. Parhi*

Main category: cs.AI

TL;DR: 인공지능(AI) 분야는 사회, 산업, 비즈니스, 거버넌스의 다양한 측면에 큰 영향을 미치고 있으며, AI 시장 규모는 2023년 1890억 달러에서 2033년까지 4.8조 달러로 성장할 것으로 예상된다. 현재 AI는 대형 언어 모델에 의해 주도되지만, 이러한 모델은 훈련에 막대한 데이터와 에너지를 소모하며, 종종 환각 현상이 발생하여 중요한 응용 분야에 사용되지 못한다. 인류의 뇌는 극히 적은 전력 소비로 기능하여, 경량 도메인 특화 멀티모달 모델이 필요하다.


<details>
  <summary>Details</summary>
Motivation: AI의 발전이 사회와 산업에 미치는 영향과 AI 시장의 급격한 성장을 분석하고, 기존 대형 언어 모델의 한계를 극복할 필요성을 제기한다.

Method: 경량 도메인 특화 멀티모달 모델을 개발하여, 실시간 데이터와 과거 지식을 기반으로 동적인 환경에서 사고, 계획 및 결정을 내리는 AI 에이전트를 제안한다.

Result: 제안된 모델은 향후 의사 결정 능력을 향상시키며, 기존의 대형 모델보다 에너지 효율성이 1000배 이상 향상될 것으로 기대된다.

Conclusion: 이 연구는 미래 AI 시스템의 비전을 제시하며, 다음 세대 AI 에이전트를 위한 새로운 하드웨어 요구사항을 제안한다.

Abstract: The field of artificial intelligence (AI) has taken a tight hold on broad
aspects of society, industry, business, and governance in ways that dictate the
prosperity and might of the world's economies. The AI market size is projected
to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI
is dominated by large language models that exhibit linguistic and visual
intelligence. However, training these models requires a massive amount of data
scraped from the web as well as large amounts of energy (50--60 GWh to train
GPT-4). Despite these costs, these models often hallucinate, a characteristic
that prevents them from being deployed in critical application domains. In
contrast, the human brain consumes only 20~W of power. What is needed is the
next level of AI evolution in which lightweight domain-specific multimodal
models with higher levels of intelligence can reason, plan, and make decisions
in dynamic environments with real-time data and prior knowledge, while learning
continuously and evolving in ways that enhance future decision-making
capability. This will define the next wave of AI, progressing from today's
large models, trained with vast amounts of data, to nimble energy-efficient
domain-specific agents that can reason and think in a world full of
uncertainty. To support such agents, hardware will need to be reimagined to
allow energy efficiencies greater than 1000x over the state of the art. Such a
vision of future AI systems is developed in this work.

</details>


### [6] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 이 논문은 뇌-컴퓨터 인터페이스(BCI)에서 뇌-에이전트 협력(BAC)으로의 패러다임 전환을 주장하며, 능동적이고 협력적인 파트너로서의 에이전트 역할에 초점을 맞추어 윤리적 데이터 처리 및 신뢰성 향상에 대한 중요성을 강조합니다.


<details>
  <summary>Details</summary>
Motivation: BCI는 심각한 신경학적 장애가 있는 개인에게 유망한 의사소통 경로를 제공하지만, 낮은 정보 전송률과 사용자 맞춤 보정의 필요성 등의 한계로 널리 채택되지 못하고 있습니다.

Method: 최근 연구에서는 대형 언어 모델(LLM) 통합을 탐색하여 단순한 명령 해독에서 복잡한 인지 상태 이해로 초점을 확대했습니다.

Result: 이러한 발전에도 불구하고, 에이전트 AI의 배포는 기술적 장애물과 윤리적 우려에 직면해 있습니다.

Conclusion: 우리는 에이전트를 수동적인 신호 프로세서가 아닌 지능적 지원을 위한 능동적이고 협력적인 파트너로 재정의하고, 이러한 시스템이 안전하고 신뢰할 수 있으며 효과적이도록 하기 위해 윤리적 데이터 처리, 모델 신뢰성 및 robust한 인간-에이전트 협력 프레임워크에 집중할 것을 요구합니다.

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [7] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti,Alessandro Druetto,Cataldo Basile,Tharindu Ranasinghe,Marcos Zampieri*

Main category: cs.AI

TL;DR: 사이버 보안과 법률의 교차점이 증가하면서 전통적인 법률 연구 도구들이 복잡한 정보 공간에서 어려움을 겪고 있다. 이 연구는 사이버 법률 분야를 탐색할 수 있는 지능형 시스템을 위한 첫 번째 단계를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 법률 전문가와 사이버 보안 전문가 간의 협업을 저해하는 지식 격차를 해소하고자 한다.

Method: 멀티링궐 과제를 통한 초기 결과를 시연한다.

Result: 첫 번째 단계인 지능형 시스템 개발을 위한 유망한 초기 결과를 보여준다.

Conclusion: 이 연구는 사이버 법률 분야의 복잡성을 탐색할 수 있는 인공지능 시스템의 필요성을 강조한다.

Abstract: The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [8] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM)이 다자간 임무에서 협업하는 능력을 평가하는 방법에 대해 논의하며, 이상적인 파트너 인식 협력자를 학습하여 작업 관련 제안에 대한 공통 기반을 증가시키는 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 인간과 협업하는 환경에서 효과적으로 협업할 수 있는 능력을 평가하는 것이 중요해지고 있다.

Method: Modified-Action MDP를 사용하여 기존 AI 에이전트의 비효율적인 행동을 분석하고, Interruptible Collaborative Roleplayer (ICR)라는 파트너 인식 학습 알고리즘을 제안한다.

Result: ICR 알고리즘이 여러 협업 작업 환경에서 CG 수렴을 촉진하고, 더 다양하고 성공적인 솔루션을 탐색하는 데 더 효과적이라는 것을 보여준다.

Conclusion: ICR은 그룹의 공통 기반 정렬을 개선하며 협업 과제를 수행하는 데 있어서 더 나은 성능을 발휘한다.

Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [9] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan,Rafid Mostafiz,Md. Abir Hossain,Bikash Kumar Paul*

Main category: cs.AI

TL;DR: CLIN-LLM은 다중 모드 환자 인코딩, 불확실성 조정 질병 분류 및 검색 보강 치료 생성을 통합하여 안전성을 최대화한 하이브리드 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 정확한 증상-질병 분류와 임상적으로 기반을 둔 치료 추천은 여전히 도전 과제로 남아 있으며, 특히 진단 위험이 높은 이질적인 환자 환경에서 더욱 그러하다.

Method: CLIN-LLM은 Symptom2Disease 데이터셋의 1,200개의 임상 사례에 BioBERT를 미세 조정하고, Focus Loss와 Monte Carlo Dropout을 통합하여 무료 텍스트 증상 및 구조적 주요 징후에서 신뢰성 있는 예측을 돕는다. 치료 생성을 위해 Biomedical Sentence-BERT를 사용하여 MedDialog 데이터셋에서 상위 k개의 관련 대화를 검색하고, 이를 FLAN-T5 모델로 개인 맞춤형 치료를 생성한다.

Result: CLIN-LLM은 98% 정확도와 F1 점수를 달성하여 ClinicalBERT보다 7.1% 향상되었으며(p < 0.001), 상위 5개 검색 정밀도는 78%로, 임상의 평가에서 5점 만점에 4.2를 받았다.

Conclusion: CLIN-LLM은 자원 제한된 의료 환경에서 배포 가능한 인간-진행 결정 지원 프레임워크를 제공하며, 향후 의료 이미징 및 실험실 데이터 통합, 다국어 확장, 임상 시험 검증을 포함할 예정이다.

Abstract: Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [10] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng,Sinuo Zhang,Rongpeng Li,Xiangming Meng,Chan Wang,Ming Lei,Zhifeng Zhao*

Main category: cs.AI

TL;DR: 효율적이고 적응적인 자원 할당이 무선 통신 시스템의 서비스 품질(QoS) 향상에 중요하다. 본 논문에서는 분산 학습과 결정을 위한 MA-CDMP라는 다중 에이전트 조건부 확산 모델 계획자를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 무선 통신 시스템에서 효율적이고 적응적인 자원 할당이 전체 QoS를 향상시키는 데 중요한 역할을 한다.

Method: MA-CDMP는 환경 역학을 포착하고 미래 궤적을 계획하기 위해 확산 모델을 사용하며, 역동적 모델이 행동 생성을 안내한다.

Result: MA-CDMP는 평균 보상 및 QoS 메트릭에서 기존 MARL 기준을 지속적으로 초과하여 실제 무선 네트워크 최적화에 대한 확장성과 실용성을 보여준다.

Conclusion: 기사에서는 MF 기반 확산 생성으로 도입된 분포 근사 오류의 상한을 이론적으로 설정하여 수렴 안정성과 다중 에이전트 확률적 동역학의 신뢰할 수 있는 모델링을 보장한다.

Abstract: In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [11] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh,Aaron Shen,Brian Law,Ahmed Ismail,Jonas Rohweder,Sean O'Brien,Kevin Zhu*

Main category: cs.AI

TL;DR: SwiftSolve는 경쟁 프로그래밍을 위한 복잡성 인식 다중 에이전트 시스템으로, 알고리즘 계획, 경험적 프로파일링 및 복잡성 유도 수정을 결합한다. 이 시스템은 프로그래밍 환경에서 다양한 역할의 전문 에이전트들이 상호작용하는 방식으로 설계되었으며, 실험 결과는 성공률이 73.08%에 이른다고 보고한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 생성한 프로그램은 유닛 테스트를 만족하더라도 경쟁 시간이나 메모리 예산을 위반하는 문제를 해결하기 위해 복잡성 인식 시스템이 필요하다.

Method: SwiftSolve는 알고리즘 스케치 제안, 고위험 계획 필터링, ISO C++17 코드 생성, 프로파일링을 통한 실행 기록, 그리고 복잡성 분석을 통해 타겟 패치를 전송하는 방식으로 운영된다.

Result: SwiftSolve는 26개의 문제에서 첫 번째 시도에서 pass@1이 61.54%, 3회 이하의 시도로 Solved@<=3이 80.77%를 기록하였으며, 전체 성공률은 73.08%로 나타났다.

Conclusion: 프로파일링과 복잡성 유도 재계획을 통해 비효율성을 줄이면서 정확성을 유지할 수 있음을 입증하였다.

Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [12] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi,Zahra Najafabadi Samani,Thomas Fahringer*

Main category: cs.AI

TL;DR: AutoStreamPipe는 대규모 언어 모델을 활용하여 실시간 데이터 스트리밍 파이프라인의 설계, 생성 및 배포를 자동화하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 데이터 파이프라인은 실시간 데이터의 효율적인 수집, 처리 및 전달을 가능하게 하여 빠른 데이터 분석을 지원한다.

Method: AutoStreamPipe는 고차원 사용자 의도와 플랫폼 특정 구현 간의 의미적 차이를 해소하고, HGoT(Thoughts의 하이퍼그래프)를 통합하여 분산 스트림 처리 시스템에서 구조화된 다중 에이전트 추론을 지원한다.

Result: 다양한 파이프라인에 대한 실험 평가를 통해 AutoStreamPipe는 LLM 코드 생성 방법에 비해 개발 시간을 6.3배, 오류율을 5.19배 감소시킴을 보여주었다.

Conclusion: AutoStreamPipe는 높은 정확도를 가진 강력한 실행 전략과 고급 쿼리 분석을 결합하여 성능을 극대화한다.

Abstract: Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [13] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: AI 에이전트는 다양한 작업 분야에서 인간의 작업을 수행하는 방식에 대한 비교 연구를 통해, 인력의 효율적인 협업 잠재력을 확인하였다. 그러나 에이전트는 종종 인간의 작업 방식과는 다른 접근을 취하며, 작업 품질이 낮은 경우가 많음에도 불구하고 빠르고 저렴한 결과를 제공할 수 있다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 인간의 작업과 관련된 과제를 최적화하는 경향이 있으며, 이는 인력에 중대한 영향을 미친다.

Method: 인간과 에이전트 작업자의 직접 비교를 통해 여러 필수 작업 관련 기술에서의 성능을 분석하고, 해석 가능한 구조화된 작업 흐름을 유도하기 위한 도구 키트를 도입하였다.

Result: AI 에이전트는 빠르고 저렴한 결과를 제공하지만, 작업 품질이 떨어지고 인간과는 다른 프로그래밍 중심적 접근을 사용한다.

Conclusion: 작업이 프로그래밍 가능한 에이전트에 위임되어 효율적인 협업 가능성을 높일 수 있지만, 품질 저하 문제를 해결해야 한다.

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [14] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu,Yunshen Zhou*

Main category: cs.AI

TL;DR: 본 논문에서는 Microsoft Copilot 서비스를 위한 Agentic Meta-orchestrator(AMO)를 제안하며, 다양한 작업과 확장 가능한 에이전트를 처리할 수 있는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: Microsoft Copilot은 다양한 에이전트를 통해 고객 지원부터 기업 프로그래밍 코드의 취약성 탐지까지 중요한 작업을 수행할 수 있도록 하는 보편적 진입점 역할을 합니다.

Method: 본 연구에서는 자연어 및 행동 응답을 제공할 수 있는 다중 작업 처리 및 확장 가능한 에이전트를 위한 Agentic Meta-orchestrator(AMO)를 제안합니다.

Result: M365 E-Commerce Copilot과 코드 준수 코파일럿의 두 가지 생산 사례를 통해 AMO의 효과를 시연합니다.

Conclusion: M365 E-Commerce Copilot은 외부 고객에게 Microsoft 제품을 광고하며, 코드 준수 코파일럿은 내부 DevOps 코드를 스캔하여 PR에서 알려진 및 새로운 준수 문제를 감지합니다.

Abstract: Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [15] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi,Emna Ben Romdhane*

Main category: cs.AI

TL;DR: 이 기사는 인공지능(AI)이 인류에 미치는 존재론적 위험을 분석하며, 현재 AI에서 초지능으로의 경로를 추적한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능의 발전이 인류에 미치는 위험을 평가하고자 한다.

Method: Irving J. Good과 Nick Bostrom의 이론적 작업과 최근의 출판물을 참고하여 AGI와 초지능을 탐구한다.

Result: 기계의 기하급수적으로 증가하는 인지 능력과 가상의 IQ를 고려하여 인류를 초월하는 지능의 윤리적, 존재론적 함의를 논의한다.

Conclusion: 인류의 멸종은 악의로 인한 것이 아니라 제어할 수 없는 무관심한 인지 우월성에서 기인할 수 있다.

Abstract: This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [16] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang,David Rawlinson*

Main category: cs.AI

TL;DR: HRM은 작은 크기에도 불구하고 인상적인 추론 능력을 가지고 있으며, 동적이거나 불확실한 문제에 적용되는 HRM-Agent를 제안한다.


<details>
  <summary>Details</summary>
Motivation: HRM은 소규모임에도 불구하고 뛰어난 추론 능력을 지니고 있지만, 동적이거나 불확실한 문제에 적용할 수 없습니다.

Method: HRM-Agent는 강화 학습만을 사용하여 훈련된 HRM의 변형입니다.

Result: HRM은 동적이고 불확실한 미로 환경에서 목표를 향해 탐색하는 방법을 학습할 수 있음을 보여준다.

Conclusion: 반복적 추론 과정의 역학을 탐구하고 이전 환경의 시간 단계에서 계산을 성공적으로 재사용하고 있다는 증거를 찾았다.

Abstract: The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [17] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein,Jessica Chen,Gwyneth Anne Delos Santos,Bayley Sapara,Michael Bowling*

Main category: cs.AI

TL;DR: 강화 학습 에이전트는 복잡한 작업에서 초인적인 성과를 달성할 수 있지만, 보통 개선 과정에서 계산 효율성이 높아지지 않는다. 이 연구에서는 에이전트가 계산 비용을 인식하고 이를 제어할 수 있도록 하여 계산을 줄이는 방법을 실험한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트가 학습 과정에서 자신의 계산을 고려할 수 있다면 계산 비용을 줄일 수 있을지 탐구하고자 한다.

Method: Arcade Learning Environment에서 에이전트가 계산 비용을 인식하고 이를 제어하는 기능을 실험한다.

Result: 동일한 훈련 계산 예산으로, 계산을 고려하는 에이전트는 75%의 게임에서 더 나은 성과를 보였고, 평균적으로 세 배 적은 계산을 사용하였다.

Conclusion: 개별 게임을 분석하여 에이전트가 효율성을 얻는 지점을 보여준다.

Abstract: While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [18] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat,Omkar Ghugarkar,Julian McAuley*

Main category: cs.AI

TL;DR: 이 연구는 다수의 도구 호출 벤치마크에 대한 최신 대형 언어 모델(LLM)의 성능을 평가하고, 다단계 추론을 명시적으로 검증하고 적대적 작업을 조합하여 스트레스 테스트를 수행하는 새로운 벤치마크 MAVEN을 소개합니다. 결과적으로, 기존 모델들이 MAVEN에서 50% 미만의 정확도를 보이며 도구 사용 설정 간의 일반화 격차를 드러냈습니다. 이를 해결하기 위해 CoreThink 에이전틱 추론기를 제시하며, 이는 경량의 기호적 추론 계층을 추가하여 구조적 분해 및 적응형 도구 조정 기능을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 도구 호출 환경에서의 일반화는 신뢰할 수 있는 에이전틱 추론 시스템 개발에 있어 주요 과제로 남아 있습니다.

Method: 우리는 다수의 도구 호출 벤치마크에 대한 최신 대형 언어 모델을 대규모로 평가하고, 다단계 추론을 명시적으로 검증하고 적대적 작업 구성으로 스트레스 테스트를 수행하기 위해 MAVEN이라는 새로운 벤치마크를 도입합니다.

Result: 대부분의 현재 모델들이 MAVEN에서 50% 미만의 정확도를 기록하여 도구 사용 설정 간의 유의미한 일반화 격차를 드러냈습니다.

Conclusion: CoreThink 에이전틱 추론기를 통해 모든 벤치마크에서 일반화되어 기존 기준에 비해 약 530%의 향상된 성능을 달성하며, 계산 비용은 약 10분의 1에 불과합니다.

Abstract: Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [19] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang,Jie Chen,Yongxu Liu,Xingpeng Zhang,Xiaoming Li,Wei Chen*

Main category: cs.AI

TL;DR: 이 연구는 다중 차원 신뢰성 평가 프레임워크를 도입하여 LLM이 생성한 간 MRI 보고서의 신뢰성을 향상시키고 기관별 프롬프트 최적화에 대한 지침을 제공하는 데 목적이 있다.


<details>
  <summary>Details</summary>
Motivation: LLM이 이미징 findings에서 진단 결론을 생성하는 데 유망한 성능을 보여주고 있지만, 임상 문맥에 맞는 프롬프트 설계 최적화에 대한 체계적인 지침이 부족하다.

Method: 다중 차원 신뢰성 평가(MDCA) 프레임워크를 도입하고, 여러 고급 LLM의 성능을 평가하고 비교하기 위해 SilliconFlow 플랫폼을 이용한다.

Result: Kimi-K2-Instruct-0905, Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, ByteDance-Seed-OSS-36B-Instruct와 같은 여러 고급 LLM의 성능을 평가하고 비교한다.

Conclusion: MDCA 프레임워크는 LLM이 생성한 간 MRI 보고서의 신뢰성을 향상시키고, 기관별 프롬프트 최적화에 대한 지침을 제공한다.

Abstract: Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [20] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang,Jiawei Zhang,Yumou Liu,Hanqun Cao,Chunbin Gu,Mengdi Liu,Zhangyang Gao,Zitong Jerry Wang,Xuanhe Zhou,Pheng-Ann Heng,Lijun Wu,Conghui He,Cheng Tan*

Main category: cs.AI

TL;DR: Sci-LLMs는 생물학적 발견을 가속화할 수 있는 유망한 도구지만, 원시 생체 분자 서열 처리에서의 토큰화 문제로 인해 제한된 추론 능력을 가진다. 고급 구조화된 문맥을 제공함으로써 이러한 제한을 극복할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 생물학적 발견을 가속화할 수 있는 새로운 방향으로 Sci-LLMs가 주목받고 있지만, 기존의 서열 중심 패러다임이 그들의 추론 능력을 제한하고 있다.

Method: 서열 전용, 문맥 전용, 두 가지 결합 방식의 세 가지 입력 모드를 통해 선도적인 Sci-LLMs를 생물학적 추론 작업에서 체계적으로 비교하였다.

Result: 문맥 전용 접근 방식이 모든 다른 접근 방식을 지속적이고 상당히 초월하는 성과를 냈다. 원시 서열의 포함은 성능을 저하시켰다.

Conclusion: Sci-LLMs는 서열 해독기가 아니라 인간이 읽을 수 있는 지식에 대한 강력한 추론 엔진으로서 재구성되어야 한다.

Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [21] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini,Joakim Bergdahl,Jean-Philippe Barrette-LaPierre,Florian Fuchs,Brady Chen,Michael Jones,Linus Gisslén*

Main category: cs.AI

TL;DR: 이 연구는 게임 산업을 위한 샘플 효율적인 딥 강화 학습(DRL) 방법을 제안하며, 이는 인간 같은 에이전트를 효과적으로 훈련하고 미세 조정하는 데 초점을 맞추고 있다.


<details>
  <summary>Details</summary>
Motivation: 많은 비디오 게임이 딥 강화 학습의 테스트베드로 사용되었지만, 게임 산업에서는 실제 AI 행동을 제작하는 데 드물게 활용되었다.

Method: 우리의 방법은 사전 수집된 데이터를 활용하고 네트워크 가소성을 높여 가치 기반 DRL의 샘플 효율성을 개선한다.

Result: 우리 방법은 EA SPORTS FC 25에서 골키퍼 에이전트를 학습시키며, 게임 내 장착 AI보다 10% 높은 공 방어율을 기록했다.

Conclusion: 우리의 접근 방식은 손으로 제작된 에이전트에 비해 더 인간 같은 게임플레이를 창출하며, 향후 시리즈의 버전에서 손으로 제작된 대체품으로 사용될 예정이다.

Abstract: While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [22] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello,Daniele Lizzio Bosco,Jacopo Cossio,Dusan Sutulovic,Giuseppe Serra,Carla Piazza,Paolo Burelli*

Main category: cs.AI

TL;DR: CNOT 최소화를 위해 새로운 강화학습 접근법을 제시하며, 고정 크기 m까지의 단일 에이전트를 사용하여 다양한 크기의 매트릭스에 대한 평가를 수행하였다.


<details>
  <summary>Details</summary>
Motivation: CNOT 게이트는 양자 컴퓨팅의 기본 요소로, 양자 알고리즘에 필요한 얽힘을 촉진합니다. CNOT의 사용을 최소화하는 것은 여전히 해결되지 않은 문제입니다.

Method: 강화학습을 활용하여 m 크기까지 단일 에이전트로 CNOT 게이트를 최소화하는 문제를 다룹니다. m과 다른 크기의 매트릭스는 임베딩 또는 가우시안 스트라이핑으로 전처리됩니다.

Result: m = 8인 에이전트를 훈련시키고 n이 3에서 15까지인 매트릭스에서 평가한 결과, 본 방법이 최신 알고리즘보다 우수한 성능을 보였습니다.

Conclusion: n의 값이 증가함에 따라 우리의 방법이 최신 알고리즘을 초월하는 결과를 얻었습니다.

Abstract: CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [23] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 적응형 에이전트 설계는 급변하는 환경에서 시간에 민감한 작업에 대한 인간-AI 협력을 개선할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 급변하는 환경에서의 시간 민감 작업에서 인간과 AI의 협력을 향상시키기 위한 필요성.

Method: 합리적 커뮤니케이션 원칙을 바탕으로 한 이론적 틀을 제시하고, 메세지의 최적 타이밍과 내용을 계획하는 방법론을 개발하였다.

Result: 사용자 및 시나리오의 특성에 따라 메시지의 구체성과 타이밍을 조정하여 효과적인 커뮤니케이션을 실현했다.

Conclusion: 인간-에이전트 팀에서의 실용적 커뮤니케이션을 위한 이론적 기반을 다졌으며, 인지 과학의 통찰을 활용하여 보조 에이전트 디자인에 기여할 수 있음을 강조한다.

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [24] [Causal Deep Q Network](https://arxiv.org/abs/2510.23424)
*Elouanes Khelifi,Amir Saki,Usef Faghihi*

Main category: cs.AI

TL;DR: 본 논문은 DQN에 인과 원리를 통합하여 문제 해결 능력을 향상시키는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: DQN은 여러 강화 학습 작업에서 성공을 거두었으나, 연상 학습에 의존하여 잘못된 상관관계를 습득하는 문제가 있습니다.

Method: PEACE 공식을 활용하여 인과 효과를 추정하고, 훈련 중에 인과 추론을 통합하는 프레임워크를 제안합니다.

Result: 제안된 방법이 기존 DQN보다 성능 저하 없이 문제 해결 능력을 향상시킨다는 것을 보여줍니다.

Conclusion: 인과 추론을 통해 심층 강화 학습 에이전트의 능력을 발전시키기 위한 유망한 경로를 제시합니다.

Abstract: Deep Q Networks (DQN) have shown remarkable success in various reinforcement
learning tasks. However, their reliance on associative learning often leads to
the acquisition of spurious correlations, hindering their problem-solving
capabilities. In this paper, we introduce a novel approach to integrate causal
principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational
Causal Effect) formula for estimating causal effects. By incorporating causal
reasoning during training, our proposed framework enhances the DQN's
understanding of the underlying causal structure of the environment, thereby
mitigating the influence of confounding factors and spurious correlations. We
demonstrate that integrating DQNs with causal capabilities significantly
enhances their problem-solving capabilities without compromising performance.
Experimental results on standard benchmark environments showcase that our
approach outperforms conventional DQNs, highlighting the effectiveness of
causal reasoning in reinforcement learning. Overall, our work presents a
promising avenue for advancing the capabilities of deep reinforcement learning
agents through principled causal inference.

</details>


### [25] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.AI

TL;DR: 이 논문은 불확실성 정량화 내에서 인간과 AI의 협업 원칙을 설명하고, 제안된 예측 세트를 보완하는 효율적인 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: AI 예측 시스템이 점점 더 의사결정 과정에 통합되면서 높은 이해관계의 선택이 인간에 의해 단독으로 이루어지던 것을 변화시키고 있습니다. 그러나 현재 AI가 부족한 분야 지식, 긴 시간적 맥락 및 물리적 세계에 기반한 추론 등을 고려할 때, 불확실성 하에서 강력한 결정을 내리는 것은 여전히 도전 과제가 됩니다.

Method: 본 연구에서는 인간 AI 협업 불확실성 정량화(Human AI Collaborative Uncertainty Quantification) 프레임워크를 도입하여 AI 모델이 두 가지 목표를 달성하면서 인간 전문가가 제안한 예측 세트를 어떻게 개선할 수 있는지를 형식화합니다: 반사실적 해를 피하고, AI가 올바른 인간 판단을 저해하지 않도록 보장하며, 인간이 놓친 올바른 결과를 보완하는 것입니다. 우리는 최적의 협업 예측 세트가 단일 점수 함수에 대해 직관적인 두 가지 임계값 구조를 따른다는 것을 보여줍니다.

Result: 이미지 분류, 회귀 및 텍스트 기반 의사결정에서의 실험 결과, 협업 예측 세트가 단독 에이전트보다 일관되게 더 나은 성과를 내며, 다양한 조건에서 더 높은 커버리지와 작은 세트 크기를 달성하였습니다.

Conclusion: 결과적으로, 본 연구는 인간과 AI의 협업이 의사결정 과정에서 어떻게 그들의 강점을 결합하고 신뢰할 수 있는 결정을 내리는 데 기여할 수 있는지를 보여줍니다.

Abstract: AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [26] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani,Ziyou Li,Anton Podkopaev,Maliheh Izadi*

Main category: cs.AI

TL;DR: 이 논문은 현대 에이전트 AI 시스템의 아키텍처 클래스와 촘스키 계층의 추상 기계 간의 공식적인 동등성을 확립한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 메모리 아키텍처가 그 계산 능력을 결정하는 주요 특징이라는 점을 제시하고, 이에 따라 특정 오토마타 클래스와 직접적으로 연결된다는 것을 탐구한다.

Method: 단순 반사 에이전트는 유한 오토마타와 동등하고, 계층적 작업 분해 에이전트는 푸시다운 오토마타와 동등하며, 읽고 쓸 수 있는 메모리를 사용하는 에이전트는 튜링 기계와 동등하다고 입증한다.

Result: 이 아우토마타-에이전트 프레임워크는 에이전트 아키텍처를 조정하여 계산 효율성과 비용을 최적화하는 방법론을 제공하고, 에이전트의 안전성과 예측 가능성을 보장하기 위해 오토마타 이론의 성숙한 기술을 적용할 수 있는 직접적인 경로를 창출한다.

Conclusion: 이 프레임워크를 확장하여 확률적 오토마타를 도입하고 정적 분석 도구 및 에이전트 프레임워크를 위한 문법 개발을 위한 의제를 제시한다.

Abstract: This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [27] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh,Rodrigue Rizk,Longwei Wang*

Main category: cs.AI

TL;DR: 이 논문은 지속 가능한 AI 솔루션으로의 전환을 주장하며, 새로운 Human AI(HAI) 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI의 빠른 발전은 전례 없는 계산 요구를 초래하였고, 이는 환경적 및 윤리적 문제를 야기하고 있다.

Method: Incremental learning, carbon-aware optimization, 및 human-in-the-loop 협업을 강조하는 HAI 프레임워크를 제안하였다.

Result: HAI는 성능과 생태적 책임을 조화시키고 지속적으로 학습할 수 있도록 설계되었다.

Conclusion: 이 접근법은 책임감 있고 인간 중심의 인공지능을 향한 경로를 제공한다.

Abstract: The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [28] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 이 논문은 텍스트 기반 소스 코드를 넘어서 프로그램이 생성하는 시각적 출력을 포함하는 신경 코드 인텔리전스의 확장을 다루고 있습니다.


<details>
  <summary>Details</summary>
Motivation: 고급 애플리케이션을 위한 시각적 차원의 중요성은 프로그램 구동 형식의 정확한 편집과 유연한 콘텐츠 생성에서 필요합니다.

Method: 상호 작용적인 웹 UI, 표준 차트 및 코드 주도 애니메이션을 아우르는 고품질 다중 모드 코드 데이터의 대규모 코퍼스를 생성하는 합성 도구 키트를 제시합니다.

Result: JanusCode-800K라는 현재까지 가장 큰 다중 모드 코드 코퍼스를 구축하고 이를 통해 JanusCoder 및 JanusCoderV 모델을 훈련합니다.

Conclusion: 우리는 기존 접근 방식을 탈피하여 텍스트 지침이나 시각적 입력 또는 두 가지의 조합으로부터 코드를 생성하기 위한 시각-프로그램 인터페이스를 확립했습니다.

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [29] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: ReCode는 계획과 행동을 통합하여 동적 의사결정 적응성을 향상시키는 새로운 패러다임을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 대형 언어 모델 기반 에이전트는 의사결정의 다양한 세분화에서 원활하게 작동하는 능력이 부족합니다.

Method: ReCode는 계획을 추상적 자리 표시자 함수로 처리하고 이를 재귀적으로 세분화하여 기본 행동에 도달하는 방식을 사용합니다.

Result: ReCode는 추론 성능에서 고급 기준을 크게 초월하고, 훈련 시 데이터 효율성을 보여줍니다.

Conclusion: 재귀적 코드 생성을 통해 계획과 행동을 통합하는 것은 범용적인 세분화 제어를 달성하는 강력하고 효과적인 접근법입니다.

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [30] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen,Yiding Wang,Siqi Zhu,Haofei Yu,Tao Feng,Muhan Zhang,Mostofa Patwary,Jiaxuan You*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM의 사고 능력을 지원하는 새로운 방법인 Multi-Agent Evolve (MAE)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning이 LLM의 사고 능력을 향상시키는 데 큰 잠재력을 보이고 있으나, 이는 인간이 큐레이터한 데이터셋과 검증 가능한 보상에 의존하여 확장성과 일반성이 제한되는 문제를 해결하고자 함.

Method: MAE는 Proposer, Solver, Judge의 세 가지 상호작용하는 에이전트로 구성되어 있으며, LLM이 다양한 과제를 해결하도록 자기 진화를 가능하게 합니다. 이 구조는 강화 학습을 적용하여 에이전트들의 행동을 최적화합니다.

Result: Qwen2.5-3B-Instruct에 대한 실험을 통해 MAE가 여러 벤치마크에서 평균 4.54%의 개선을 달성함을 보여줍니다.

Conclusion: MAE는 최소한의 인간 큐레이터의 감독에 의존하여 LLM의 일반적인 사고 능력을 향상시키는 확장 가능하고 데이터 효율적인 방법으로 강조됩니다.

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [31] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu,Xuan Qi,Hongru Wang,Xinzhe Juan,Yimin Wang,Zelin Zhao,Jiayi Geng,Jiacheng Guo,Peihang Li,Jingzhe Shi,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: ALITA-G는 일반 에이전트를 도메인 전문가로 변화시키는 자기 진화 프레임워크로, 성능을 향상시키고 계산 비용을 줄인다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)이 메모리, 도구 및 피드백이 있는 에이전트로 구성될 때 성능이 개선되지만, 현재의 자기 진화 에이전트는 주로 프롬프트 재작성이나 실패 재시도에 국한되어 있어 더 발전이 필요하다.

Method: ALITA-G는 모델 컨텍스트 프로토콜(MCP) 도구를 체계적으로 생성, 추상화 및 큐레이션하여 일반 에이전트를 도메인 전문 에이전트로 변화시킨다. 일반 에이전트가 큐레이션된 도메인 작업을 실행하고 성공적인 경로에서 후보 MCP를 합성하며, 이를 매개변수화된 원형으로 추상화하고 MCP 상자로 통합한다.

Result: ALITA-G는 GAIA, PathVQA 및 Humanity's Last Exam과 같은 여러 벤치마크에서 강력한 성과를 달성하였으며, GAIA 검증에서 83.03% pass@1 및 89.09% pass@3을 기록하며 새로운 최첨단 결과를 세웠다. 또한 강력한 기준 에이전트에 비해 평균 토큰 수를 약 15% 줄인다.

Conclusion: ALITA-G는 일반적인 능력으로부터 재사용 가능한 도메인별 능력으로 나아가는 원칙적인 경로를 제공하고, 복잡한 추론 작업의 정확성과 효율성을 개선한다.

Abstract: Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [Collaborative Task Assignment, Sequencing and Multi-agent Path-finding](https://arxiv.org/abs/2510.21738)
*Yifan Bai,Shruti Kotpalliwar,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.MA

TL;DR: 이 논문은 협력적 작업 할당, 순서 지정, 다중 에이전트 경로 탐색 문제를 해결하는 CBS-TS라는 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 협력적 작업 할당에서 충돌 없이 임무를 수행하고 흐름 시간을 최소화하는 것이 중요하다.

Method: CBS-TS는 새로운 작업 순서를 찾고 경로의 충돌을 해결하는 최적의 알고리즘으로, 혼합 정수 선형 프로그래밍(MILP)을 사용하여 작업 순서를 최적화한다.

Result: CBS-TS는 CBSS보다 더 높은 성공률과 일관된 최적 솔루션을 달성하며, 다양한 테스트 시나리오에서 우수한 성능을 보인다.

Conclusion: CBS-TS는 TSPF 문제를 보다 효과적으로 해결할 수 있는 가능성을 보여준다.

Abstract: In this article, we address the problem of collaborative task assignment,
sequencing, and multi-agent pathfinding (TSPF), where a team of agents must
visit a set of task locations without collisions while minimizing flowtime.
TSPF incorporates agent-task compatibility constraints and ensures that all
tasks are completed. We propose a Conflict-Based Search with Task Sequencing
(CBS-TS), an optimal and complete algorithm that alternates between finding new
task sequences and resolving conflicts in the paths of current sequences.
CBS-TS uses a mixed-integer linear program (MILP) to optimize task sequencing
and employs Conflict-Based Search (CBS) with Multi-Label A* (MLA*) for
collision-free path planning within a search forest. By invoking MILP for the
next-best sequence only when needed, CBS-TS efficiently limits the search
space, enhancing computational efficiency while maintaining optimality. We
compare the performance of our CBS-TS against Conflict-based Steiner Search
(CBSS), a baseline method that, with minor modifications, can address the TSPF
problem. Experimental results demonstrate that CBS-TS outperforms CBSS in most
testing scenarios, achieving higher success rates and consistently optimal
solutions, whereas CBSS achieves near-optimal solutions in some cases. The
supplementary video is available at https://youtu.be/QT8BYgvefmU.

</details>


### [33] [CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating](https://arxiv.org/abs/2510.22222)
*Yumeng Shi,Zhongliang Yang,Yisi Wang,Linna Zhou*

Main category: cs.MA

TL;DR: CreditXAI는 기업 신용 평가의 예측 정확도를 7% 이상 향상시키는 다중 에이전트 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 심층 학습 방법은 예측 정확도를 향상시키지만, 블랙박스 문제와 해석 가능성의 한계가 있다.

Method: CreditXAI라는 다중 에이전트 시스템 프레임워크를 제안하여 전문가 신용 분석가의 협업적 의사결정 과정을 시뮬레이션 한다.

Result: 실험 결과, 다중 에이전트 협력이 단일 에이전트 기준선 대비 예측 정확도를 7% 이상 향상시켰음을 보여준다.

Conclusion: 이 연구는 지능적이고 해석 가능한 신용 평가 모델을 구축하기 위한 새로운 기술 경로를 제공한다.

Abstract: In the domain of corporate credit rating, traditional deep learning methods
have improved predictive accuracy but still suffer from the inherent
'black-box' problem and limited interpretability. While incorporating
non-financial information enriches the data and provides partial
interpretability, the models still lack hierarchical reasoning mechanisms,
limiting their comprehensive analytical capabilities. To address these
challenges, we propose CreditXAI, a Multi-Agent System (MAS) framework that
simulates the collaborative decision-making process of professional credit
analysts. The framework focuses on business, financial, and governance risk
dimensions to generate consistent and interpretable credit assessments.
Experimental results demonstrate that multi-agent collaboration improves
predictive accuracy by more than 7% over the best single-agent baseline,
confirming its significant synergistic advantage in corporate credit risk
evaluation. This study provides a new technical pathway to build intelligent
and interpretable credit rating models.

</details>


### [34] [CGoT: A Novel Inference Mechanism for Embodied Multi-Agent Systems Using Composable Graphs of Thoughts](https://arxiv.org/abs/2510.22235)
*Yixiao Nie,Yang Zhang,Yingjie Jin,Zhepeng Wang,Xiu Li,Xiang Li*

Main category: cs.MA

TL;DR: 이 연구는 자율주행차와 서비스 로봇의 통합 시스템을 제안하고, 대형 언어 모델을 활용하여 운영 효율성을 향상시키고자 한다.


<details>
  <summary>Details</summary>
Motivation: 자율주행차와 서비스 로봇의 통합이 산업과 일상생활에서 점점 더 중요해지고 있다.

Method: 자율차가 서비스 로봇을 사무공간 내로 운반하고, 로봇은 여러 작업을 수행하는 시스템을 제안하며, CGOT라는 추론 메커니즘을 도입한다.

Result: 실험 결과는 제안된 방법의 성과를 검증했다.

Conclusion: 이 연구는 자율차와 서비스 로봇 간의 협력 메커니즘의 잠재력을 극대화하는 방법을 제시한다.

Abstract: The integration of self-driving cars and service robots is becoming
increasingly prevalent across a wide array of fields, playing a crucial and
expanding role in both industrial applications and everyday life. In parallel,
the rapid advancements in Large Language Models (LLMs) have garnered
substantial attention and interest within the research community. This paper
introduces a novel vehicle-robot system that leverages the strengths of both
autonomous vehicles and service robots. In our proposed system, two autonomous
ego-vehicles transports service robots to locations within an office park,
where they perform a series of tasks. The study explores the feasibility and
potential benefits of incorporating LLMs into this system, with the aim of
enhancing operational efficiency and maximizing the potential of the
cooperative mechanisms between the vehicles and the robots. This paper proposes
a novel inference mechanism which is called CGOT toward this type of system
where an agent can carry another agent. Experimental results are presented to
validate the performance of the proposed method.

</details>


### [35] [IFS: Information Flow Structure for Multi-agent Ad Hoc System](https://arxiv.org/abs/2510.22320)
*Yanqing Fu,Chenrun Wang,Chao Huang,Zhuping Wang*

Main category: cs.MA

TL;DR: 본 논문은 다중 에이전트 임시 시스템에서 정보 흐름 구조(IFS)를 제안하며, 통신 및 정보 융합 관점에서의 한계와 해결책을 논의한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 임시 시스템은 동적이고 협력적인 시스템이며, 불확실성과 부분 가시성 하에서 작동하여 정보 흐름의 한계를 해결할 필요가 있다.

Method: 정보 흐름 구조(IFS)를 통해 통신 및 정보 융합 관점에서 이 문제를 해결한다.

Result: StarCraft II에서의 실험 결과, IFS는 정보 흐름과 처리 능력을 크게 개선하였으며, 복잡한 팀워크 시나리오에서 기존 방법보다 뛰어난 성능을 보였다.

Conclusion: IFS는 다중 에이전트 협력의 효율성을 증대시키고, 강력한 일반화 능력을 통해 다양한 환경에서 유용하게 적용될 수 있다.

Abstract: Multi-agent ad hoc systems are dynamic collaborative systems in which
multiple autonomous agents must cooperate with both known and unknown teammates
in open environments, without relying on pre-coordinated strategies. These
systems operate under conditions of uncertainty and partial observability,
where team composition, agent behaviors, and environmental factors may change
during execution. Through an analysis of information flow in such systems, we
identify two key limitations in existing research: insufficient information
flow and limited information processing capacity. To address these issues, we
propose an information flow structure for multi-agent ad hoc systems (IFS),
which tackles these challenges from the perspectives of communication and
information fusion. Experimental results in StarCraft II demonstrate that IFS
significantly improves both information flow and processing capacity, while
exhibiting strong generalization capabilities and outperforming baseline
methods in complex ad hoc teamwork scenarios.

</details>


### [36] [Group size effects and collective misalignment in LLM multi-agent systems](https://arxiv.org/abs/2510.22422)
*Ariel Flint,Luca Maria Aiello,Romualdo Pastor-Satorras,Andrea Baronchelli*

Main category: cs.MA

TL;DR: 다중 에이전트 시스템에서 그룹의 크기가 동역학에 미치는 영향을 탐구한 연구.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 동역학 이해를 심화하기 위해.

Method: 다양한 그룹 크기에서의 상호작용을 기반으로 다중 에이전트 불일치 현상 분석.

Result: 그룹 크기와 상호작용이 개인의 편향을 증폭하거나 새로운 편향을 도입하며 비선형적으로 동역학에 영향을 미침을 발견.

Conclusion: 그룹 크기는 다중 에이전트 동역학의 핵심 요인이며, LLM 기반 시스템을 대규모로 배포할 때 인구 수준의 효과를 고려해야 함.

Abstract: Multi-agent systems of large language models (LLMs) are rapidly expanding
across domains, introducing dynamics not captured by single-agent evaluations.
Yet, existing work has mostly contrasted the behavior of a single agent with
that of a collective of fixed size, leaving open a central question: how does
group size shape dynamics? Here, we move beyond this dichotomy and
systematically explore outcomes across the full range of group sizes. We focus
on multi-agent misalignment, building on recent evidence that interacting LLMs
playing a simple coordination game can generate collective biases absent in
individual models. First, we show that collective bias is a deeper phenomenon
than previously assessed: interaction can amplify individual biases, introduce
new ones, or override model-level preferences. Second, we demonstrate that
group size affects the dynamics in a non-linear way, revealing model-dependent
dynamical regimes. Finally, we develop a mean-field analytical approach and
show that, above a critical population size, simulations converge to
deterministic predictions that expose the basins of attraction of competing
equilibria. These findings establish group size as a key driver of multi-agent
dynamics and highlight the need to consider population-level effects when
deploying LLM-based systems at scale.

</details>


### [37] [Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration](https://arxiv.org/abs/2510.22431)
*Zheng Wei,Mingchen Li,Zeqian Zhang,Ruibin Yuan,Pan Hui,Huamin Qu,James Evans,Maneesh Agrawala,Anyi Rao*

Main category: cs.MA

TL;DR: 이 연구는 멀티 에이전트 협업을 개선하기 위한 세 가지 혁신을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 시스템의 발전이 창의적인 작업 성능 향상에 중요한 잠재력을 보여주고 있다.

Method: OmniAgent라는 계층적 그래프 기반 멀티 에이전트 프레임워크와 컨텍스트 공학에서 영감을 받은 하이퍼그래프 노드, 그리고 제한된 재시도를 가진 유향순환 그래프로 전환하는 방법을 제안한다.

Result: 이 방법들은 에이전트 간 협력을 강화하고, 개별 기억 요구를 줄이며, 출력을 반복적으로 개선할 수 있게 한다.

Conclusion: 이러한 기여는 창의적 작업에서 더 강력한 멀티 에이전트 시스템 개발의 기초가 된다.

Abstract: Recent advancements in multi-agent systems have demonstrated significant
potential for enhancing creative task performance, such as long video
generation. This study introduces three innovations to improve multi-agent
collaboration. First, we propose OmniAgent, a hierarchical, graph-based
multi-agent framework for long video generation that leverages a
film-production-inspired architecture to enable modular specialization and
scalable inter-agent collaboration. Second, inspired by context engineering, we
propose hypergraph nodes that enable temporary group discussions among agents
lacking sufficient context, reducing individual memory requirements while
ensuring adequate contextual information. Third, we transition from directed
acyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing
agents to reflect and refine outputs iteratively, thereby improving earlier
stages through feedback from subsequent nodes. These contributions lay the
groundwork for developing more robust multi-agent systems in creative tasks.

</details>


### [38] [Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization](https://arxiv.org/abs/2510.22477)
*Yijia Fan,Jusheng Zhang,Jing Yang,Keze Wang*

Main category: cs.MA

TL;DR: Agent-GSPO라는 프레임워크를 통해 비용 효율적인 다중 에이전트 시스템을 위해 통신 인식 보상을 통해 에이전트를 훈련시키며 새로운 최첨단 성능을 달성함.


<details>
  <summary>Details</summary>
Motivation: 비용이 높은 '모두를 위한' 다중 에이전트 시스템의 통신 비용을 줄이기 위함.

Method: 시퀀스 수준의 강화 학습을 사용하여 토큰 경제를 직접 최적화하는 Agent-GSPO 프레임워크를 소개하고, GSPO 알고리즘을 통해 에이전트를 훈련.

Result: 일곱 가지 reasoning benchmark에서 기존 방법보다 적은 토큰 소비로 새로운 최첨단 성능을 달성함.

Conclusion: 전략적 침묵과 같은 유도 전략을 촉진하여 확장 가능하고 경제적으로 실행 가능한 다중 에이전트 시스템 개발을 위한 실용적인 청사진을 제공함.

Abstract: To combat the prohibitive communication costs of ``free-for-all" multi-agent
systems (MAS), we introduce \textbf{Agent-GSPO}, a framework that directly
optimizes for token economy using sequence-level reinforcement learning.
Agent-GSPO leverages the stable and memory-efficient Group Sequence Policy
Optimization (GSPO) algorithm to train agents on a communication-aware reward
that explicitly penalizes verbosity. Across seven reasoning benchmarks,
Agent-GSPO not only achieves new state-of-the-art performance but does so with
a fraction of the token consumption of existing methods. By fostering emergent
strategies like ``strategic silence," our approach provides a practical
blueprint for developing scalable and economically viable multi-agent systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability](https://arxiv.org/abs/2510.21770)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: 저널의 역할을 확인하고, 오류가 언제 발생하는지를 예측하는 첫 번째 모듈 수준의 이론을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 저정밀로 훈련된 변환기가 전방 오류 증폭을 겪을 수 있다는 점에서, 이러한 오류의 성장 예측이 필요합니다.

Method: 레이어별 한계를 도출하고 해석 가능한 진단 지표를 제공합니다.

Result: Tiny-ViT/CIFAR-10에서 계산된 한계를 기반으로, 여러 가지 예측자의 성능을 평가합니다.

Conclusion: 우리의 이론은 자가 주의가 취약할 때를 설명하고, 불안정을 예측하며, 최소한의 간섭으로 완화할 수 있는 방법을 제시합니다.

Abstract: Transformers trained in low precision can suffer forward-error amplification.
We give a first-order, module-wise theory that predicts when and where errors
grow. For self-attention we derive a per-layer bound that factorizes into three
interpretable diagnostics: a score-scale ratio $\kappa_{\rm score}$, a rowwise
softmax sensitivity $\kappa_{\rm softmax}$, and value conditioning $\kappa(V)$.
We prove a residual relaxation inequality showing that residual blocks
attenuate depth-wise accumulation, and we introduce a precision- and
width-aware LayerNorm indicator $\rho_{\rm LN}$ with a matching first-order
bound in the $\epsilon$-dominated regime. These pieces yield a unified
forward-stability bound whose right-hand side is directly estimable during
training.
  On Tiny-ViT/CIFAR-10 we evaluate the bound and components. (1) The combined
predictor $\kappa_{\rm softmax},(1+\kappa_{\rm
score}),\kappa(V),|W_O|2+\kappa{\rm eff}+C_{\rm LN}$ tracks
FP32$\leftrightarrow$LP mismatches across seeds, widths, and precisions;
scaling by $\epsilon_{\rm mach}$ collapses mixed-precision points. (2) The
time-series maximum of $\kappa_{\rm softmax}$ acts as an early-warning signal,
leading error spikes by 16-24 steps (corr. 0.65-0.82; permutation
$p!\approx!10^{-3}$; Precision@K 0.89-1.00). (3) Guided by $\rho_{\rm LN}$, a
small LayerNorm-$\epsilon$ tweak targeting $\rho_\star$ gives consistent
stabilization (mean tail-loss $\downarrow\ \approx0.010$ at $\rho_\star!=!0.6$,
cap$=10^{-2}$) with negligible overhead.
  Overall, our theory supplies actionable, unitless diagnostics that (i)
explain when self-attention is fragile, (ii) forecast instability, and (iii)
motivate a minimally invasive mitigation.

</details>


### [40] [Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making](https://arxiv.org/abs/2510.21788)
*Larkin Liu,Jalal Etesami*

Main category: cs.LG

TL;DR: 전문가 안내 밴딧 학습을 탐구하는 본 연구에서, 온라인 전문가 혼합 방식(OMoE)으로서 전문가의 출력을 최적의 집합 정확도를 달성하기 위해 집계하는 방법을 결정한다.


<details>
  <summary>Details</summary>
Motivation: 전문가의 출력을 집계하여 최적의 결과를 도출하는 문제를 해결하기 위해.

Method: 첫 번째 알고리즘은 집계 투표와 UCB 기반의 연속 제거를 결합하여 비최적 탐색 행동을 효율적으로 제거하며, 두 번째 알고리즘은 각 전문가의 예측 능력에 비례하여 투표 권한을 갖는 온라인 가중 다수결 메커니즘을 사용한다.

Result: 모든 상황에서 밴딧 설정의 후회 속성에 대한 이론적 보장을 도출하고, 경험적 결과를 제공한다. 또한, 실질적인 응용 연구로서, 전문가 대형 언어 모델의 온라인 미세 조정에 적용된다.

Conclusion: 이 결과는 여러 전문가를 결합하여 집계 모델의 성능을 향상시키기 위한 새로운 방법론과 무후회의 보장을 제시한다.

Abstract: We explore the use of expert-guided bandit learning, which we refer to as
online mixture-of-experts (OMoE). In this setting, given a context, a candidate
committee of experts must determine how to aggregate their outputs to achieve
optimal results in terms of aggregate accuracy. We propose two algorithms to
address this problem. The first algorithm combines aggregate voting with
UCB-driven successive elimination, efficiently pruning suboptimal exploration
actions. The second algorithm employs an online weighted-majority-voting
mechanism, leveraging the respective voting power of each expert proportional
to their predictive power. We derive theoretical guarantees for the regret
properties in the bandit setting under ideal circumstances, and empirical
results are provided accordingly. As a modern study on applications, these
methods are applied to the online fine-tuning of a set of expert large language
models (LLMs), where after each response, the generative LLM dynamically
reweighs its set of experts and/or selects the optimal committee of experts to
generate the most accurate response. Our results introduce new methodologies
and no-regret guarantees for combining multiple experts to improve on the
performance of the an aggregate model overall.

</details>


### [41] [AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing](https://arxiv.org/abs/2510.21935)
*Samuel Bright-Thonney,Christina Reissel,Gaia Grosso,Nathaniel Woodward,Katya Govorkova,Andrzej Novak,Sang Eon Park,Eric Moreno,Philip Harris*

Main category: cs.LG

TL;DR: 이 논문은 고차원 과학 데이터에서의 이상 탐지를 위한 통합 파이프라인의 첫 단계를 제시하며, AutoSciDACT라는 자동화된 과학적 발견 시스템을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 과학 데이터셋에서의 새로운 발견 탐지는 실험 데이터의 잡음과 고차원 특성으로 인해 어려움을 겪고 있으며, 통계적으로 신뢰할 수 있는 이상치에 대한 주장을 할 필요성이 있다.

Method: AutoSciDACT는 대조적 프리트레이닝을 통해 표현력이 풍부한 저차원 데이터 표현을 생성하며, 고품질의 시뮬레이션 데이터를 활용하여 데이터 증가 전략을 적용한다.

Result: AutoSciDACT는 NPLM 프레임워크를 사용하여 두 샘플 테스트를 수행하고, 관측된 데이터의 편차를 통계적으로 정량화한다.

Conclusion: 우리는 다양한 분야의 데이터셋에서 실험을 수행하여 작고 비정상적인 데이터 주입에 대한 높은 민감성을 보여주었다.

Abstract: Novelty detection in large scientific datasets faces two key challenges: the
noisy and high-dimensional nature of experimental data, and the necessity of
making statistically robust statements about any observed outliers. While there
is a wealth of literature on anomaly detection via dimensionality reduction,
most methods do not produce outputs compatible with quantifiable claims of
scientific discovery. In this work we directly address these challenges,
presenting the first step towards a unified pipeline for novelty detection
adapted for the rigorous statistical demands of science. We introduce
AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive
Testing), a general-purpose pipeline for detecting novelty in scientific data.
AutoSciDACT begins by creating expressive low-dimensional data representations
using a contrastive pre-training, leveraging the abundance of high-quality
simulated data in many scientific domains alongside expertise that can guide
principled data augmentation strategies. These compact embeddings then enable
an extremely sensitive machine learning-based two-sample test using the New
Physics Learning Machine (NPLM) framework, which identifies and statistically
quantifies deviations in observed data relative to a reference distribution
(null hypothesis). We perform experiments across a range of astronomical,
physical, biological, image, and synthetic datasets, demonstrating strong
sensitivity to small injections of anomalous data across all domains.

</details>


### [42] [Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model](https://arxiv.org/abs/2510.22057)
*James Thiering,Tarun Sethupat Radha Krishna,Dylan Zelkin,Ashis Kumer Biswas*

Main category: cs.LG

TL;DR: 온라인 학습에서 학생 참여를 모니터링하고 향상시키기 위한 자동화된 시스템 개발을 다룬 연구.


<details>
  <summary>Details</summary>
Motivation: 온라인과 가상 학습의 증가로 인해 학생의 참여를 모니터링하는 것이 효과적인 교육의 중요한 측면이 되었다.

Method: 감정적인 특성(예: 성별)을 예측에서 사용하지 않도록 훈련 방법을 개발하여 속성-직교 정규화 기법을 적용하였다.

Result: 예측 간의 불균형을 줄이는 데 있어 모델의 피어슨 상관 계수를 0.897에서 0.999로 개선하였다.

Conclusion: 윤리적 기준을 강화하고 모델 예측의 해석 가능성을 향상시키는 데 기여하였다.

Abstract: With the rise of online and virtual learning, monitoring and enhancing
student engagement have become an important aspect of effective education.
Traditional methods of assessing a student's involvement might not be
applicable directly to virtual environments. In this study, we focused on this
problem and addressed the need to develop an automated system to detect student
engagement levels during online learning. We proposed a novel training method
which can discourage a model from leveraging sensitive features like gender for
its predictions. The proposed method offers benefits not only in the
enforcement of ethical standards, but also to enhance interpretability of the
model predictions. We applied an attribute-orthogonal regularization technique
to a split-model classifier, which uses multiple transfer learning strategies
to achieve effective results in reducing disparity in the distribution of
prediction for sensitivity groups from a Pearson correlation coefficient of
0.897 for the unmitigated model, to 0.999 for the mitigated model. The source
code for this project is available on
https://github.com/ashiskb/elearning-engagement-study .

</details>


### [43] [Deep Gaussian Processes for Functional Maps](https://arxiv.org/abs/2510.22068)
*Matthew Lowery,Zhitong Xu,Da Long,Keyan Chen,Daniel S. Johnson,Yang Bai,Varun Shankar,Shandian Zhe*

Main category: cs.LG

TL;DR: 이 논문에서는 기능적 데이터 분석에서 중요한 역할을 하는 기능 공간 간의 매핑 학습을 위한 Deep Gaussian Processes for Functional Maps(DGPFM)를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기능적 데이터 분석 및 여러 응용 분야에서 매핑 학습의 필요성을 강조하며 기존 접근 방식의 한계를 지적합니다.

Method: GP 기반의 선형 및 비선형 변환으로 구성된 DGPFM을 설계하고, 커널의 적분 변환을 활용하여 유연한 설계를 가능하게 합니다.

Result: 실제 데이터와 PDE 벤치마크 데이터셋에서 DGPFM의 예측 성능과 불확실성 보정의 이점을 보여줍니다.

Conclusion: DGPFM은 복잡한 비선형성을 효과적으로 캡처하고 다양한 설계를 통합할 수 있는 유연한 접근 방식을 제공합니다.

Abstract: Learning mappings between functional spaces, also known as
function-on-function regression, plays a crucial role in functional data
analysis and has broad applications, e.g. spatiotemporal forecasting, curve
prediction, and climate modeling. Existing approaches, such as functional
linear models and neural operators, either fall short of capturing complex
nonlinearities or lack reliable uncertainty quantification under noisy, sparse,
and irregularly sampled data. To address these issues, we propose Deep Gaussian
Processes for Functional Maps (DGPFM). Our method designs a sequence of
GP-based linear and nonlinear transformations, leveraging integral transforms
of kernels, GP interpolation, and nonlinear activations sampled from GPs. A key
insight simplifies implementation: under fixed locations, discrete
approximations of kernel integral transforms collapse into direct functional
integral transforms, enabling flexible incorporation of various integral
transform designs. To achieve scalable probabilistic inference, we use inducing
points and whitening transformations to develop a variational learning
algorithm. Empirical results on real-world and PDE benchmark datasets
demonstrate that the advantage of DGPFM in both predictive performance and
uncertainty calibration.

</details>


### [44] [Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics](https://arxiv.org/abs/2510.22158)
*Lorenzo Magnino,Kai Shao,Zida Wu,Jiacheng Shen,Mathieu Laurière*

Main category: cs.LG

TL;DR: 비정상 연속 평균장 게임을 위한 새로운 심층 강화 학습 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법이 유한 공간이나 정적 모델에 한정되어 있어 실제 문제에 적용하기 어렵기 때문이다.

Method: 가상 플레이(FP) 방법론을 기반으로 하여 DRL을 행동 반응 계산에 활용하고, 평균 정책 표현을 위해 감독 학습을 적용한다. 또한, 조건부 정규화 흐름을 사용하여 시간 의존적인 인구 분포의 표현을 학습한다.

Result: 세 가지 복잡성이 증가하는 예제를 통해 우리의 방법의 효과성을 평가하였다.

Conclusion: 이 연구는 확장성과 밀도 근사라는 주요 한계를 해결함으로써 DRL 기법을 복잡한 MFG 문제에 적용하는 데 중요한 발전을 의미하며, 실제 다중 에이전트 시스템에 더 가까워졌다.

Abstract: Mean field games (MFGs) have emerged as a powerful framework for modeling
interactions in large-scale multi-agent systems. Despite recent advancements in
reinforcement learning (RL) for MFGs, existing methods are typically limited to
finite spaces or stationary models, hindering their applicability to real-world
problems. This paper introduces a novel deep reinforcement learning (DRL)
algorithm specifically designed for non-stationary continuous MFGs. The
proposed approach builds upon a Fictitious Play (FP) methodology, leveraging
DRL for best-response computation and supervised learning for average policy
representation. Furthermore, it learns a representation of the time-dependent
population distribution using a Conditional Normalizing Flow. To validate the
effectiveness of our method, we evaluate it on three different examples of
increasing complexity. By addressing critical limitations in scalability and
density approximation, this work represents a significant advancement in
applying DRL techniques to complex MFG problems, bringing the field closer to
real-world multi-agent systems.

</details>


### [45] [MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification](https://arxiv.org/abs/2510.22070)
*Luca Caldera,Giacomo Bottacini,Lara Cavinato*

Main category: cs.LG

TL;DR: MAGIC-Flow는 의료 이미징 분야에서 효율적으로 생성 및 분류 작업을 수행하는 조건부 다중 스케일 정규화 흐름 아키텍처이다.


<details>
  <summary>Details</summary>
Motivation: 생성 모델링은representation learning을 위한 강력한 패러다임으로 떠올랐지만, 의료 이미징과 같은 도전적인 분야에 대한 직접적인 적용성은 제한적이다.

Method: MAGIC-Flow는 생성 및 분류를 수행하는 단일 모듈식 프레임워크로서, 가역적이고 미분 가능한 일대일 변환의 계층으로 구축되며, Jacobian 행렬의 행렬식이 하위 변환 across에서 분해된다.

Result: MAGIC-Flow는 스캐너 잡음 아래에서의 생성 및 분류, 모달리티 특화 합성과 식별을 다루며, 현실적이고 다양한 샘플을 생성하고 분류를 개선한다.

Conclusion: MAGIC-Flow는 데이터가 제한된 영역에서 생성 및 분류를 위한 효과적인 전략으로, 개인 정보 보호를 위한 증강, 강력한 일반화, 신뢰할 수 있는 의료 AI에서 직접적인 이점을 제공한다.

Abstract: Generative modeling has emerged as a powerful paradigm for representation
learning, but its direct applicability to challenging fields like medical
imaging remains limited: mere generation, without task alignment, fails to
provide a robust foundation for clinical use. We propose MAGIC-Flow, a
conditional multiscale normalizing flow architecture that performs generation
and classification within a single modular framework. The model is built as a
hierarchy of invertible and differentiable bijections, where the Jacobian
determinant factorizes across sub-transformations. We show how this ensures
exact likelihood computation and stable optimization, while invertibility
enables explicit visualization of sample likelihoods, providing an
interpretable lens into the model's reasoning. By conditioning on class labels,
MAGIC-Flow supports controllable sample synthesis and principled
class-probability estimation, effectively aiding both generative and
discriminative objectives. We evaluate MAGIC-Flow against top baselines using
metrics for similarity, fidelity, and diversity. Across multiple datasets, it
addresses generation and classification under scanner noise, and
modality-specific synthesis and identification. Results show MAGIC-Flow creates
realistic, diverse samples and improves classification. MAGIC-Flow is an
effective strategy for generation and classification in data-limited domains,
with direct benefits for privacy-preserving augmentation, robust
generalization, and trustworthy medical AI.

</details>


### [46] [UCB-type Algorithm for Budget-Constrained Expert Learning](https://arxiv.org/abs/2510.22654)
*Ilgam Latypov,Alexandra Suvorikova,Alexey Kroshnin,Alexander Gasnikov,Yuriy Dorn*

Main category: cs.LG

TL;DR: 이 논문에서는 동적 환경에서 여러 적응형 학습 알고리즘을 선택하는 문제를 다루며, 계산적으로 효율적인 메타 알고리즘 M-LCB를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 많은 응용 프로그램에서 시스템은 동적으로 여러 적응형 학습 알고리즘 중에서 선택해야 합니다.

Method: M-LCB라는 계산적으로 효율적인 UCB 스타일의 메타 알고리즘을 제안합니다.

Result: M-LCB는 적응형 전문가가 동시에 훈련될 때 후회 보장을 제공합니다.

Conclusion: M-LCB는 상태를 유지하며 자가 학습하는 전문가를 제한된 자원 하에서 조정하는 보다 현실적인 시나리오로 고전적인 밴딧 패러다임을 확장합니다.

Abstract: In many modern applications, a system must dynamically choose between several
adaptive learning algorithms that are trained online. Examples include model
selection in streaming environments, switching between trading strategies in
finance, and orchestrating multiple contextual bandit or reinforcement learning
agents. At each round, a learner must select one predictor among $K$ adaptive
experts to make a prediction, while being able to update at most $M \le K$ of
them under a fixed training budget.
  We address this problem in the \emph{stochastic setting} and introduce
\algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that
provides \emph{anytime regret guarantees}. Its confidence intervals are built
directly from realized losses, require no additional optimization, and
seamlessly reflect the convergence properties of the underlying experts. If
each expert achieves internal regret $\tilde O(T^\alpha)$, then \algname{M-LCB}
ensures overall regret bounded by $\tilde O\!\Bigl(\sqrt{\tfrac{KT}{M}} \;+\;
(K/M)^{1-\alpha}\,T^\alpha\Bigr)$.
  To our knowledge, this is the first result establishing regret guarantees
when multiple adaptive experts are trained simultaneously under per-round
budget constraints. We illustrate the framework with two representative cases:
(i) parametric models trained online with stochastic losses, and (ii) experts
that are themselves multi-armed bandit algorithms. These examples highlight how
\algname{M-LCB} extends the classical bandit paradigm to the more realistic
scenario of coordinating stateful, self-learning experts under limited
resources.

</details>


### [47] [Agentic Reinforcement Learning for Real-World Code Repair](https://arxiv.org/abs/2510.22075)
*Siyu Zhu,Anastasiya Karpovich,Albert Chen,Jessica Koscheka,Shailesh Jannu,Di Wen,Yuqing Zhu,Rohit Jain,Alborz Geramifard*

Main category: cs.LG

TL;DR: 복잡한 빌드와 변화하는 의존성으로 인해 불안정한 평가가 이루어지는 실제 저장소에서 신뢰할 수 있는 코드 수정 에이전트를 훈련하는 문제를 다룬다. 검증 가능한 파이프라인을 개발하여, ~1,000개의 실제 이슈에서 의존성을 고정하고 자동 업그레이드를 비활성화하여 성공을 정의하고 재현 가능성을 개선했다. 이를 기반으로 대규모 강화 학습을 위한 간소화된 확장 가능한 파이프라인을 도입했다.


<details>
  <summary>Details</summary>
Motivation: 실제 저장소에서 신뢰할 수 있는 코드 수정 에이전트를 훈련하는 과정에서 발생하는 불안정성을 해결하기 위해.

Method: 검증 가능한 파이프라인을 개발하고, Qwen3-32B를 전체 파이프라인에서 감독하여 정교화했으며, 간소화된 환경에서 SFT 모델 위에 강화 학습을 적용했다.

Result: GPT-4.1 궤적에서 정제된 SFT 모델은 56배 더 작으면서 비슷한 성능을 보였고, 강화 학습은 일치된 훈련-테스트 조건에서 7-20%의 절대적인 향상을 이끌어냈다. '생각 모드'는 우리의 실험에서 동등하거나 더 나쁜 성과를 보였다.

Conclusion: SFT와 RL 모델 모두 환경에 따라 일반화에 실패하여 신뢰할 수 있는 현실 세계 코드 수정 에이전트를 구축하기 위해서는 훈련-테스트 환경의 일치를 중요하게 생각해야 한다.

Abstract: We tackle the challenge of training reliable code-fixing agents in real
repositories, where complex builds and shifting dependencies make evaluation
unstable. We developed a verifiable pipeline with success defined as post-fix
build validation and improved reproducibility across ~1K real issues by pinning
dependencies and disabling automatic upgrades. Building on this, we introduced
a scalable simplified pipeline for large-scale reinforcement learning (RL).
Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and
applied RL on top of the SFT model in the simplified environment. The SFT model
distilled from GPT-4.1 trajectories performs on par while being 56x smaller,
and RL added 7-20% absolute gains under matched train-test conditions.
"Thinking mode" was on par or worse in our experiments. Both SFT and RL models
failed to generalize across environments, highlighting the importance of
matching train-test environments for building reliable real-world code-fixing
agents.

</details>


### [48] [ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation](https://arxiv.org/abs/2510.22732)
*Jiali Cheng,Anjishnu Kumar,Roshan Lal,Rishi Rajasekaran,Hani Ramezani,Omar Zia Khan,Oleg Rokhlenko,Sunny Chiu-Webster,Gang Hua,Hadi Amiri*

Main category: cs.LG

TL;DR: ATLAS라는 새로운 메모리 증강 에이전트를 소개하며, 이 에이전트는 환경 모델에 기반한 계획 수립을 통해 새로운 환경에 효과적으로 적응할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 현재 최신 웹 에이전트들이 새로운 환경에 효과적으로 적응하지 못하고, 신경망 미세 조정 없이는 비효율적인 실행 계획을 생성한다는 점을 해결하고자 한다.

Method: ATLAS는 인지 공간에서 행동의 결과를 시뮬레이션하여 환경 모델에 기반한 계획을 세운다. 에이전트는 경량 호기심 주도 탐사를 통해 '인지 맵'을 구축하고, 계획자는 후보 행동을 제안하고, 시뮬레이터는 결과를 예측하며, 비평가는 최상의 롤아웃을 선택하고 원래 계획을 업데이트한다.

Result: WebArena-Lite 벤치마크에서 63%의 성공률을 달성했으며, 이는 기존의 최신 기술이 53.9%의 성공률을 기록한 것에 비해 월등히 높은 수치이다.

Conclusion: 모듈식 아키텍처는 웹사이트 특정 LLM fine-tuning이 필요하지 않으며, 세계 모델, 계층적 계획자 및 선행 계획 재설계자의 중요성이 확인되었다.

Abstract: We observe that current state-of-the-art web-agents are unable to effectively
adapt to new environments without neural network fine-tuning, without which
they produce inefficient execution plans due to a lack of awareness of the
structure and dynamics of the new environment. To address this limitation, we
introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action
Simulation), a memory-augmented agent that is able to make plans grounded in a
model of the environment by simulating the consequences of those actions in
cognitive space. Our agent starts by building a "cognitive map" by performing a
lightweight curiosity driven exploration of the environment. The planner
proposes candidate actions; the simulator predicts their consequences in
cognitive space; a critic analyzes the options to select the best roll-out and
update the original plan; and a browser executor performs the chosen action. On
the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%
success rate for the previously published state-of-the-art. Unlike previous
systems, our modular architecture requires no website-specific LLM fine-tuning.
Ablations show sizable drops without the world-model, hierarchical planner, and
look-ahead-based replanner confirming their complementary roles within the
design of our system

</details>


### [49] [Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training](https://arxiv.org/abs/2510.22094)
*Thomas Bailie,S. Karthik Mukkavilli,Varvara Vetrova,Yun Sing Koh*

Main category: cs.LG

TL;DR: 이 논문에서는 기후 사건 예측의 정확성을 높이기 위해 물리학을 통합한 다중 스케일 예측 환경에서 작동하는 계층적 그래프 신경망(HGNN)인 HiFlowCast와 그 앙상블 변형인 HiAntFlow를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 정확한 기상 예측이 글로벌 동력에 의해 영향을 받는 복잡한 다변량 동역학으로 인해 어렵다는 점에서, 물리학을 통합한 예측을 통해 이를 개선하고자 한다.

Method: 기후 사건 예측을 위해 Latent-Memory-Retention 메커니즘과 Latent-to-Physics 가지를 활용한 HiFlowCast 및 HiAntFlow를 제안한다.

Result: 이 모델은 13일 예측에서 5% 이상의 오차를 줄이고, 1퍼센트 및 99퍼센트 극단에서 5-8%의 개선을 보여줌으로써 희귀 사건의 신뢰성을 높인다.

Conclusion: 사전 훈련된 모델 가중치를 활용하여 훈련 비용과 탄소 발자국을 줄이고, 기계 학습의 지속 가능성 문제를 해결하기 위한 효율성을 제공한다.

Abstract: Climate events arise from intricate, multivariate dynamics governed by
global-scale drivers, profoundly impacting food, energy, and infrastructure.
Yet, accurate weather prediction remains elusive due to physical processes
unfolding across diverse spatio-temporal scales, which fixed-resolution methods
cannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale
representation, but nonlinear downward mappings often erase global trends,
weakening the integration of physics into forecasts. We introduce HiFlowCast
and its ensemble variant HiAntFlow, HGNNs that embed physics within a
multiscale prediction framework. Two innovations underpin their design: a
Latent-Memory-Retention mechanism that preserves global trends during downward
traversal, and a Latent-to-Physics branch that integrates PDE solution fields
across diverse scales. Our Flow models cut errors by over 5% at 13-day lead
times and by 5-8% under 1st and 99th quantile extremes, improving reliability
for rare events. Leveraging pretrained model weights, they converge within a
single epoch, reducing training cost and their carbon footprint. Such
efficiency is vital as the growing scale of machine learning challenges
sustainability and limits research accessibility. Code and model weights are in
the supplementary materials.

</details>


### [50] [Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs](https://arxiv.org/abs/2510.22139)
*Jinzhe Liu,Junshu Sun,Shufan Shen,Chenxue Yang,Shuhui Wang*

Main category: cs.LG

TL;DR: 생애 전반에 걸친 지식 편집을 위한 새로운 프레임워크인 Neuron-Specific Masked Knowledge Editing(NMKE)을 제안하며, 이 방법은 신경 수준의 기여도 분석과 동적 희소 마스킹을 결합하여 정확한 지식 편집을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델에서 오래된 지식을 계산 비용이 많이 드는 전체 재학습 없이 지속적이고 정확하게 업데이트할 수 있는 방법이 필요하다.

Method: NMKE는 신경 기능 기여도를 활용하여 지식 일반 신경 및 지식 특정 신경이라는 두 가지 주요 유형의 지식 신경을 식별하고, 엔트로피 가이드 동적 희소 마스크를 도입하여 대상 지식과 관련된 신경을 식별한다.

Result: 수천 개의 연속 편집 실험 결과, NMKE는 기존 방법보다 높은 편집 성공률을 유지하고 모델의 일반화 능력을 보존하는 데 탁월함을 입증하였다.

Conclusion: NMKE는 신경 수준에서 더 적은 파라미터 수정으로 정확한 지식 편집을 가능하게 한다.

Abstract: Lifelong knowledge editing enables continuous, precise updates to outdated
knowledge in large language models (LLMs) without computationally expensive
full retraining. However, existing methods often accumulate errors throughout
the editing process, causing a gradual decline in both editing accuracy and
generalization. To tackle this problem, we propose Neuron-Specific Masked
Knowledge Editing (NMKE), a novel fine-grained editing framework that combines
neuron-level attribution with dynamic sparse masking. Leveraging neuron
functional attribution, we identify two key types of knowledge neurons, with
knowledge-general neurons activating consistently across prompts and
knowledge-specific neurons activating to specific prompts. NMKE further
introduces an entropy-guided dynamic sparse mask, locating relevant neurons to
the target knowledge. This strategy enables precise neuron-level knowledge
editing with fewer parameter modifications. Experimental results from thousands
of sequential edits demonstrate that NMKE outperforms existing methods in
maintaining high editing success rates and preserving model general
capabilities in lifelong editing.

</details>


### [51] [When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs](https://arxiv.org/abs/2510.22228)
*Keyu Wang,Tian Lyu,Guinan Su,Jonas Geiping,Lu Yin,Marco Canini,Shiwei Liu*

Main category: cs.LG

TL;DR: 레이어 프루닝이 대규모 언어 모델의 효율성을 향상시키는 데 널리 사용되지만, 긴 체인 추론 능력에 미치는 영향은 충분히 조사되지 않았다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 효율성을 높이기 위해 레이어 프루닝의 영향을 분석하고, 긴 체인 추론에서의 성능 저하를 이해하는 것.

Method: Test-time scaling을 통해 레이어 프루닝이 긴 체인 추론에 미치는 영향을 연구하며, 다양한 실험을 수행했다.

Result: 하나 또는 두 개의 레이어를 프루닝하는 것이 테스트 시 스케일링에 심각한 손상을 초래하고, 지식 집약적이며 얕은 추론 작업에서는 성능이 안정적인데 반해 긴 추론 벤치마크에서는 성능이 급락함을 입증했다.

Conclusion: 레이어 프루닝이 추론 집중형 대규모 언어 모델에 미치는 기본적인 위험성을 강조하고, 이러한 전략을 재고할 필요가 있음을 제안한다.

Abstract: Layer pruning has emerged as a widely adopted technique for improving the
efficiency of large language models (LLMs). Although existing methods
demonstrate strong performance retention on general knowledge tasks, their
effect on long-chain reasoning, a more brittle yet crucial capability, remains
largely unexplored. In this work, we study the impact of layer pruning on
long-chain reasoning through the lens of test-time scaling, a key mechanism in
modern LLMs that enables strong reasoning capacity by allocating more
computation at inference time. With extensive experiments, we demonstrate that
pruning even one or two layers can severely impair test-time scaling, with
performance collapsing drastically on long reasoning benchmarks even when
performance on knowledge-intensive and shallow reasoning tasks remains stable.
Furthermore, we find that standard supervised fine-tuning remedies fail to
recover test-time scaling once it has deteriorated. Through in-depth analyses,
we identify the mechanisms underlying this fragility of test-time scaling and
highlight the fundamental risks of applying layer pruning to
reasoning-intensive LLMs. These findings call for a rethinking of layer pruning
strategies and provide insights for developing methods that preserve the
robustness of reasoning. We open-source the codebase in
\href{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}.

</details>


### [52] [Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know](https://arxiv.org/abs/2510.22261)
*Shireen Kudukkil Manchingal*

Main category: cs.LG

TL;DR: 이 논문은 기계 학습이 불확실성을 관리하지 못하는 본질적인 문제로 인해 안전-critical 도메인에서의 활용이 제한되고 있다는 문제를 다룹니다. 이를 해결하기 위해 에피스테믹 인공지능 모델을 발전시키고, 무작위 집합 신경망(RS-NN)을 개발하여 불확실성을 정량화하고, 이를 통해 더 신뢰할 수 있는 예측을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습의 안전-critical 도메인에서 활용이 불가능한 이유는 불확실성을 관리할 수 있는 능력이 부족하기 때문이다.

Method: 무작위 집합 이론을 활용하여 신념 함수를 예측하는 RS-NN을 개발하고, 이를 통해 에피스테믹 불확실성을 정량화한다.

Result: 에피스테믹 인식을 통합하는 것이 과신 예측의 위험을 완화할 뿐만 아니라 신뢰할 수 있는 시스템의 특징이 되는 '모를 때 아는' 능력을 형성한다는 것을 실험을 통해 검증하였다.

Conclusion: 이 연구는 지식의 한계를 인식하고 관리하는 것이 진정한 지능에 포함된다는 핵심 철학을 강조한다.

Abstract: Machine learning has achieved remarkable successes, yet its deployment in
safety-critical domains remains hindered by an inherent inability to manage
uncertainty, resulting in overconfident and unreliable predictions when models
encounter out-of-distribution data, adversarial perturbations, or naturally
fluctuating environments. This thesis, titled Epistemic Deep Learning: Enabling
Machine Learning Models to 'Know When They Do Not Know', addresses these
critical challenges by advancing the paradigm of Epistemic Artificial
Intelligence, which explicitly models and quantifies epistemic uncertainty: the
uncertainty arising from limited, biased, or incomplete training data, as
opposed to the irreducible randomness of aleatoric uncertainty, thereby
empowering models to acknowledge their limitations and refrain from
overconfident decisions when uncertainty is high.
  Central to this work is the development of the Random-Set Neural Network
(RS-NN), a novel methodology that leverages random set theory to predict belief
functions over sets of classes, capturing the extent of epistemic uncertainty
through the width of associated credal sets, applications of RS-NN, including
its adaptation to Large Language Models (LLMs) and its deployment in weather
classification for autonomous racing. In addition, the thesis proposes a
unified evaluation framework for uncertainty-aware classifiers. Extensive
experiments validate that integrating epistemic awareness into deep learning
not only mitigates the risks associated with overconfident predictions but also
lays the foundation for a paradigm shift in artificial intelligence, where the
ability to 'know when it does not know' becomes a hallmark of robust and
dependable systems. The title encapsulates the core philosophy of this work,
emphasizing that true intelligence involves recognizing and managing the limits
of one's own knowledge.

</details>


### [53] [AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals](https://arxiv.org/abs/2510.22301)
*Yujie Xiao,Gongzhen Tang,Wenhui Liu,Jun Li,Guangkun Nie,Zhuoran Kan,Deyun Zhang,Qinghao Zhao,Shenda Hong*

Main category: cs.LG

TL;DR: 이 연구는 심전도를 활용한 비침습적 실험실 수치 추정의 가능성을 검토하고, ECGFounder 모델을 미세 조정하여 예측 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 신속한 실험실 수치 접근이 임상 의사결정에 중요하지만, 현재 방법은 침습적이고 지연이 발생한다.

Method: 대규모 사전 훈련된 ECGFoundation 모델을 전이 학습으로 미세 조정하여 실험실 지표를 예측하기 위한 연구를 진행했다.

Result: 모델은 33개의 실험실 지표에 대해 0.65 이상의 AUC를 기록했으며, 59개의 지표는 0.55에서 0.65, 16개의 지표는 0.55 미만의 성능을 보였다.

Conclusion: 이 연구는 실시간 비침습적 실험실 수치 추정의 효율적인 인공지능 기반 솔루션을 제공한다.

Abstract: Timely access to laboratory values is critical for clinical decision-making,
yet current approaches rely on invasive venous sampling and are intrinsically
delayed. Electrocardiography (ECG), as a non-invasive and widely available
signal, offers a promising modality for rapid laboratory estimation. Recent
progress in deep learning has enabled the extraction of latent hematological
signatures from ECGs. However, existing models are constrained by low
signal-to-noise ratios, substantial inter-individual variability, limited data
diversity, and suboptimal generalization, especially when adapted to low-lead
wearable devices. In this work, we conduct an exploratory study leveraging
transfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG
foundation model, on the Multimodal Clinical Monitoring in the Emergency
Department (MC-MED) dataset from Stanford. We generated a corpus of more than
20 million standardized ten-second ECG segments to enhance sensitivity to
subtle biochemical correlates. On internal validation, the model demonstrated
strong predictive performance (area under the curve above 0.65) for
thirty-three laboratory indicators, moderate performance (between 0.55 and
0.65) for fifty-nine indicators, and limited performance (below 0.55) for
sixteen indicators. This study provides an efficient artificial-intelligence
driven solution and establishes the feasibility scope for real-time,
non-invasive estimation of laboratory values.

</details>


### [54] [Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization](https://arxiv.org/abs/2510.22383)
*David Freire-Obregón,José Salas-Cáceres,Modesto Castrillón-Santana*

Main category: cs.LG

TL;DR: 본 논문에서는 드롭아웃 대신 Conway의 생명 게임을 활용한 새로운 정규화 접근 방식을 제안하며, 이를 통해 신경망의 일반화 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 정규화 기법은 신경망의 과적합을 방지하고 일반화 성능을 향상시키는 데 중요한 역할을 합니다.

Method: 신경망 유닛을 Conway의 생명 게임의 셀로 표현하고 훈련 중 동적 유닛 비활성화를 구현하여 정규화를 진행합니다.

Result: CIFAR-10 데이터셋에서 제안한 접근 방식의 효과를 입증했으며, 전통적인 드롭아웃 기법과 비슷한 성능을 달성하였습니다.

Conclusion: 제안된 방식은 더 깊은 아키텍처에서도 적용 가능하며, 다양한 드롭아웃 기법의 성능을 향상시킵니다.

Abstract: Regularization techniques play a crucial role in preventing overfitting and
improving the generalization performance of neural networks. Dropout, a widely
used regularization technique, randomly deactivates units during training to
introduce redundancy and prevent co-adaptation among neurons. Despite its
effectiveness, dropout has limitations, such as its static nature and lack of
interpretability. In this paper, we propose a novel approach to regularization
by substituting dropout with Conway's Game of Life (GoL), a cellular automata
with simple rules that govern the evolution of a grid of cells. We introduce
dynamic unit deactivation during training by representing neural network units
as cells in a GoL grid and applying the game's rules to deactivate units. This
approach allows for the emergence of spatial patterns that adapt to the
training data, potentially enhancing the network's ability to generalize. We
demonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing
that dynamic unit deactivation using GoL achieves comparable performance to
traditional dropout techniques while offering insights into the network's
behavior through the visualization of evolving patterns. Furthermore, our
discussion highlights the applicability of our proposal in deeper
architectures, demonstrating how it enhances the performance of different
dropout techniques.

</details>


### [55] [Scalable Oversight via Partitioned Human Supervision](https://arxiv.org/abs/2510.22500)
*Ren Yin,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: AI 시스템의 평가를 위한 인간의 약한 신호를 활용하는 프레임워크 제안


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 전문가의 성능을 초월함에 따라 고품질 인간 감독이 점점 더 어려워지고 있다.

Method: 보완적인 레이블을 기반으로 한 비편향 추정기를 도출하고、희소한 일반 레이블과 풍부한 보완 레이블을 결합하기 위한 두 개의 추정기를 도입한다.

Result: 보완적인 레이블을 활용하여 대형 언어 모델의 출력을 평가할 수 있음을 보여준다.

Conclusion: 약한 신호를 활용하여 AI 시스템을 훈련시키고, 인간의 분할 감독으로 더 나은 성능을 발휘하는 에이전틱 AI 시스템을 자동으로 설계할 수 있음을 보여준다.

Abstract: As artificial intelligence (AI) systems approach and surpass expert human
performance across a broad range of tasks, obtaining high-quality human
supervision for evaluation and training becomes increasingly challenging. Our
focus is on tasks that require deep knowledge and skills of multiple domains.
Unfortunately, even the best human experts are knowledgeable only in a single
narrow area, and will not be able to evaluate the correctness of advanced AI
systems on such superhuman tasks. However, based on their narrow expertise,
humans may provide a weak signal, i.e., a complementary label indicating an
option that is incorrect. For example, a cardiologist could state that "this is
not related to cardiology,'' even if they cannot identify the true disease.
Based on this weak signal, we propose a scalable oversight framework that
enables us to evaluate frontier AI systems without the need to prepare the
ground truth. We derive an unbiased estimator of top-1 accuracy from
complementary labels and quantify how many complementary labels are needed to
match the variance of ordinary labels. We further introduce two estimators to
combine scarce ordinary labels with abundant complementary labels. We provide
finite-sample deviation guarantees for both complementary-only and the mixed
estimators. Empirically, we show that we can evaluate the output of large
language models without the ground truth, if we have complementary labels. We
further show that we can train an AI system with such weak signals: we show how
we can design an agentic AI system automatically that can perform better with
this partitioned human supervision. Our code is available at
https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.

</details>


### [56] [Random Search Neural Networks for Efficient and Expressive Graph Learning](https://arxiv.org/abs/2510.22520)
*Michael Ito,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: 랜덤 탐색 신경망(RSNNs)은 랜덤 워크 신경망(RWNNs)의 한계를 극복하여 그래프에서 보다 효과적인 학습을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: RWNNs는 랜덤 워크를 처리하기 위해 시퀀스 모델의 발전을 활용하지만, 현실적인 샘플링 제한 아래에서 글로벌 구조를 포착하지 못하고 그 표현력이 제한된다.

Method: RSNNs는 랜덤 검색을 기반으로 하며, 모든 노드가 완전히 커버되도록 보장한다. 이론적으로 희소 그래프에서 전체 엣지 커버를 달성하기 위해 필요한 검색 수는 O(log |V|)에 불과하다.

Result: RSNNs는 분자 및 단백질 벤치마크에서 RWNNs에 비해 일관되게 우수한 성능을 보이며, 최대 16배 적은 샘플 시퀀스로도 그에 상응하거나 더 나은 성과를 달성했다.

Conclusion: 우리 연구는 랜덤 워크 기반 접근 방식에서 이론적 및 실용적 발전을 연결하여 희소 그래프에서의 학습을 위한 효율적이고 표현력이 풍부한 프레임워크를 제공한다.

Abstract: Random walk neural networks (RWNNs) have emerged as a promising approach for
graph representation learning, leveraging recent advances in sequence models to
process random walks. However, under realistic sampling constraints, RWNNs
often fail to capture global structure even in small graphs due to incomplete
node and edge coverage, limiting their expressivity. To address this, we
propose \textit{random search neural networks} (RSNNs), which operate on random
searches, each of which guarantees full node coverage. Theoretically, we
demonstrate that in sparse graphs, only $O(\log |V|)$ searches are needed to
achieve full edge coverage, substantially reducing sampling complexity compared
to the $O(|V|)$ walks required by RWNNs (assuming walk lengths scale with graph
size). Furthermore, when paired with universal sequence models, RSNNs are
universal approximators. We lastly show RSNNs are probabilistically invariant
to graph isomorphisms, ensuring their expectation is an isomorphism-invariant
graph function. Empirically, RSNNs consistently outperform RWNNs on molecular
and protein benchmarks, achieving comparable or superior performance with up to
16$\times$ fewer sampled sequences. Our work bridges theoretical and practical
advances in random walk based approaches, offering an efficient and expressive
framework for learning on sparse graphs.

</details>


### [57] [Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections](https://arxiv.org/abs/2510.22655)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 자기 감독 학습(SSL)은 라벨 데이터 없이 표현을 학습하는 강력한 패러다임으로 떠올랐다. 본 연구에서는 특수한 도메인 지식 없이도 다양한 표현을 생성하기 위해 정규 직교 기저 및 과완전 프레임을 사용하는 비지도 표현 학습 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 SSL 접근법은 데이터 증강을 통해 표현 학습을 위한 다양한 뷰를 생성하지만, 이는 도메인 특정 지식이 필요하며 일반화를 제한할 수 있다.

Method: 정규 직교 기저와 과완전 프레임을 사용하여 증강 대신 뷰를 생성하는 비지도 표현 학습 방법을 제안한다.

Result: 정규 직교 및 과완전 공간에서 학습된 임베딩은 다른 공간의 샘플 표현에서 형성된 기하학적 편향에 의해 구분된 매니폴드에 위치함을 보여준다.

Conclusion: 우리의 방법은 9개의 데이터셋에서 15-20%의 성능 향상을 달성하며, 이는 기존 자기 감독 접근법에 비해 데이터 다양성의 인위적 증대 없이 얻은 결과이다.

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations without labeled data. Most SSL approaches rely on
strong, well-established, handcrafted data augmentations to generate diverse
views for representation learning. However, designing such augmentations
requires domain-specific knowledge and implicitly imposes representational
invariances on the model, which can limit generalization. In this work, we
propose an unsupervised representation learning method that replaces
augmentations by generating views using orthonormal bases and overcomplete
frames. We show that embeddings learned from orthonormal and overcomplete
spaces reside on distinct manifolds, shaped by the geometric biases introduced
by representing samples in different spaces. By jointly leveraging the
complementary geometry of these distinct manifolds, our approach achieves
superior performance without artificially increasing data diversity through
strong augmentations. We demonstrate the effectiveness of our method on nine
datasets across five temporal sequence tasks, where signal-specific
characteristics make data augmentations particularly challenging. Without
relying on augmentation-induced diversity, our method achieves performance
gains of up to 15--20\% over existing self-supervised approaches. Source code:
https://github.com/eth-siplab/Learning-with-FrameProjections

</details>


### [58] [Distributed Multi-Agent Bandits Over Erdős-Rényi Random Networks](https://arxiv.org/abs/2510.22811)
*Jingyuan Liu,Hao Qiu,Lin Yang,Mengfan Xu*

Main category: cs.LG

TL;DR: 이 논문은 이질적인 보상을 가진 분산 멀티 에이전트 다중 손잡이 문제를 연구하고, 무작위 통신 그래프 상에서의 에이전트 간 통신에 대한 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 환경에서 다수의 에이전트가 통신하며 최적의 보상을 찾는 문제는 중요하며, 특히 무작위 통신 그래프에서의 효율적인 접근 방법이 필요합니다.

Method: 에이전트별로 다르게 설정된 시간 불변 분포를 따르는 보상 시스템과 임의의 gossip 알고리즘을 통합한 완전 분산 알고리즘을 제안합니다.

Result: 이론적으로 보상을 최소화하는 상한선이 로그 T의 차수에 해당하며, 이것은 통신 효율성과 후회 사이의 기본적인 상충 관계를 강조합니다.

Conclusion: 제안된 알고리즘이 기존 벤치마크에 비해 우수성을 보이며, 후회의 이론적 스케일링이 문제의 복잡성과 일치함을 검증합니다.

Abstract: We study the distributed multi-agent multi-armed bandit problem with
heterogeneous rewards over random communication graphs. Uniquely, at each time
step $t$ agents communicate over a time-varying random graph $G_t$ generated by
applying the Erd\H{o}s-R\'enyi model to a fixed connected base graph $G$ (for
classical Erd\H{o}s-R\'enyi graphs, $G$ is a complete graph), where each
potential edge in $G$ is randomly and independently present with the link
probability $p$. Notably, the resulting random graph is not necessarily
connected at each time step. Each agent's arm rewards follow time-invariant
distributions, and the reward distribution for the same arm may differ across
agents. The goal is to minimize the cumulative expected regret relative to the
global mean reward of each arm, defined as the average of that arm's mean
rewards across all agents. To this end, we propose a fully distributed
algorithm that integrates the arm elimination strategy with the random gossip
algorithm. We theoretically show that the regret upper bound is of order $\log
T$ and is highly interpretable, where $T$ is the time horizon. It includes the
optimal centralized regret $O\left(\sum_{k: \Delta_k>0} \frac{\log
T}{\Delta_k}\right)$ and an additional term $O\left(\frac{N^2 \log T}{p
\lambda_{N-1}(Lap(G))} + \frac{KN^2 \log T}{p}\right)$ where $N$ and $K$ denote
the total number of agents and arms, respectively. This term reflects the
impact of $G$'s algebraic connectivity $\lambda_{N-1}(Lap(G))$ and the link
probability $p$, and thus highlights a fundamental trade-off between
communication efficiency and regret. As a by-product, we show a nearly optimal
regret lower bound. Finally, our numerical experiments not only show the
superiority of our algorithm over existing benchmarks, but also validate the
theoretical regret scaling with problem complexity.

</details>


### [59] [Clustering by Denoising: Latent plug-and-play diffusion for single-cell data](https://arxiv.org/abs/2510.22835)
*Dominik Meier,Shixing Yu,Sagnik Nandy,Promit Ghosal,Kyra Gan*

Main category: cs.LG

TL;DR: 단일 세포 RNA 시퀀싱(scRNA-seq)에서 측정 노이즈와 생물학적 변동성으로 인해 클러스터링 정확도가 도전 과제로 남아있습니다. 본 논문에서는 관찰 공간과 디노이징 공간을 구분하는 라텐트 플러그 앤 플레이 차별화 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 단일 세포 RNA 시퀀싱(scRNA-seq) 기술을 통해 세포의 이질성을 연구할 수 있지만, 측정 노이즈와 생물학적 변동성으로 인해 클러스터링 정확도와 세포 레이블에 기반한 분석이 어려워서 이를 해결하기 위한 새로운 접근 방식이 필요하다.

Method: 우리는 관찰 공간과 디노이징 공간을 분리하는 라텐트 플러그 앤 플레이 차별화 프레임워크를 소개하며, 이를 새로운 기브스 샘플링 절차를 통해 운영화합니다. 학습된 확산 우선 규칙은 낮은 차원의 라텐트 공간에서 디노이징을 수행하고, 이 과정을 조율하기 위해 오리지널 고차원 관찰 공간으로 노이즈를 재도입합니다.

Result: 우리 방법은 다양한 노이즈 수준과 데이터셋 이동에 걸쳐 합성 데이터에서 클러스터링 정확도를 개선하며, 실제 단일 세포 데이터에서는 생성된 세포 클러스터가 기존의 세포 유형 마커 및 개발 궤적과 더 잘 정렬되는 생물학적 일관성을 보여줍니다.

Conclusion: 우리의 접근 방식은 조절 가능한 우선 규칙과 관찰된 데이터 간의 균형을 통해 적응형 노이즈 처리, 다운스트림 분석을 위한 원칙적인 불확실성 추정에 의한 불확실성 정량화, 그리고 깨끗한 참조 데이터를 활용하여 노이즈가 많은 데이터셋을 디노이징하고, 평균화를 통해 훈련 세트를 넘어선 품질 향상을 실현합니다.

Abstract: Single-cell RNA sequencing (scRNA-seq) enables the study of cellular
heterogeneity. Yet, clustering accuracy, and with it downstream analyses based
on cell labels, remain challenging due to measurement noise and biological
variability. In standard latent spaces (e.g., obtained through PCA), data from
different cell types can be projected close together, making accurate
clustering difficult. We introduce a latent plug-and-play diffusion framework
that separates the observation and denoising space. This separation is
operationalized through a novel Gibbs sampling procedure: the learned diffusion
prior is applied in a low-dimensional latent space to perform denoising, while
to steer this process, noise is reintroduced into the original high-dimensional
observation space. This unique "input-space steering" ensures the denoising
trajectory remains faithful to the original data structure. Our approach offers
three key advantages: (1) adaptive noise handling via a tunable balance between
prior and observed data; (2) uncertainty quantification through principled
uncertainty estimates for downstream analysis; and (3) generalizable denoising
by leveraging clean reference data to denoise noisier datasets, and via
averaging, improve quality beyond the training set. We evaluate robustness on
both synthetic and real single-cell genomics data. Our method improves
clustering accuracy on synthetic data across varied noise levels and dataset
shifts. On real-world single-cell data, our method demonstrates improved
biological coherence in the resulting cell clusters, with cluster boundaries
that better align with known cell type markers and developmental trajectories.

</details>


### [60] [Transforming volcanic monitoring: A dataset and benchmark for onboard volcano activity detection](https://arxiv.org/abs/2510.22889)
*Darshana Priyasad,Tharindu Fernando,Maryam Haghighat,Harshala Gammulle,Clinton Fookes*

Main category: cs.LG

TL;DR: 화산 폭발과 같은 자연 재해는 일상 생활에 큰 도전을 주고 있으며, 상당한 경제적 손실을 초래한다. 다음 세대의 소형 위성이 이러한 사건의 실시간 모니터링과 온보드 처리를 가능하게 하지만, 화산 활동을 포착한 포괄적인 주석 데이터셋의 부족이 주요 장애물로 남아 있다. 이 논문은 세계 다양한 화산을 포괄하는 화산 활동 및 폭발 감지용의 새로운 데이터셋을 소개하고, 이는 감지 모델 개발 및 평가를 위한 기초 자원을 제공한다. 또한, 최신 모델을 사용한 포괄적인 벤치마크를 제시하여 향후 연구를 위한 기준선을 설정하고, 이러한 모델을 차세대 위성에 온보드 배치할 가능성을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 화산 폭발과 관련된 풍부한 주석 데이터셋이 부족하여 감지 시스템 개발이 저해되고 있다. 따라서 이 문제를 해결하기 위한 데이터셋의 필요성이 있다.

Method: 세계 다양한 화산을 포괄하는 화산 활동 및 폭발 감지용 데이터셋을 개발하고, 이를 바탕으로 감지 모델을 개발 및 평가한다. 또한 Intel Movidius Myriad X VPU를 사용하여 이들 모델을 온보드 배치 가능성을 탐구한다.

Result: 새로운 데이터셋은 화산 이상 및 비이상 식별을 위한 이진 주석을 제공하며, 최신 모델을 이용한 벤치마크 테스트를 통해 향후 연구를 위한 기준선을 설정한다. 또한, 직접 온보드에서 화산 활동 감지가 가능함을 입증하였다.

Conclusion: 이 연구는 화산 재해 관리에서 혁신적인 해결책을 제시하고, 향후 온보드 모니터링 기술의 탐구 및 개선을 장려한다.

Abstract: Natural disasters, such as volcanic eruptions, pose significant challenges to
daily life and incur considerable global economic losses. The emergence of
next-generation small-satellites, capable of constellation-based operations,
offers unparalleled opportunities for near-real-time monitoring and onboard
processing of such events. However, a major bottleneck remains the lack of
extensive annotated datasets capturing volcanic activity, which hinders the
development of robust detection systems. This paper introduces a novel dataset
explicitly designed for volcanic activity and eruption detection, encompassing
diverse volcanoes worldwide. The dataset provides binary annotations to
identify volcanic anomalies or non-anomalies, covering phenomena such as
temperature anomalies, eruptions, and volcanic ash emissions. These annotations
offer a foundational resource for developing and evaluating detection models,
addressing a critical gap in volcanic monitoring research. Additionally, we
present comprehensive benchmarks using state-of-the-art models to establish
baselines for future studies. Furthermore, we explore the potential for
deploying these models onboard next-generation satellites. Using the Intel
Movidius Myriad X VPU as a testbed, we demonstrate the feasibility of volcanic
activity detection directly onboard. This capability significantly reduces
latency and enhances response times, paving the way for advanced early warning
systems. This paves the way for innovative solutions in volcanic disaster
management, encouraging further exploration and refinement of onboard
monitoring technologies.

</details>


### [61] [RL-AUX: Reinforcement Learning for Auxiliary Task Generation](https://arxiv.org/abs/2510.22940)
*Judah Goldfeder,Matthew So,Hod Lipson*

Main category: cs.LG

TL;DR: RL 기반의 방법을 통해 보조 작업을 동적으로 생성하는 접근 방식을 제시하고, 최적의 보조 작업 가중치를 학습하여 성능을 향상시키는 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 보조 학습(AL)은 주 작업의 성능을 향상시키기 위해 보조 작업에서 훈련하는 다중 작업 학습(MTL)의 특별한 사례이다. 하지만 라벨이 있는 보조 작업의 필요성 때문에 인간의 노력과 도메인 전문 지식이 요구된다.

Method: RL 에이전트가 훈련 세트의 각 데이터 포인트에 대해 보조 라벨을 선택하고, 선택이 주 작업의 성능을 향상시킬 때 보상을 받는 방식으로 동적으로 보조 작업을 생성하는 접근 방식을 사용한다.

Result: 20-Superclass CIFAR100 문제에서, 우리의 RL 접근 방식이 인간 라벨링 된 보조 작업보다 우수한 성능을 보여주며, 저명한 Bi-Level 최적화 기법에 필적하는 성과를 보인다.

Conclusion: 이 연구의 목표는 (1) RL이 동적으로 보조 작업을 생성할 수 있는 실행 가능한 접근 방식임을 입증하고, (2) 샘플별 보조 작업 가중치를 보조 작업 라벨과 함께 학습할 수 있으며, 강력한 성과를 달성할 수 있음을 보여주는 것이다.

Abstract: Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in
which a network trains on auxiliary tasks to improve performance on its main
task. This technique is used to improve generalization and, ultimately,
performance on the network's main task. AL has been demonstrated to improve
performance across multiple domains, including navigation, image
classification, and natural language processing. One weakness of AL is the need
for labeled auxiliary tasks, which can require human effort and domain
expertise to generate. Meta Learning techniques have been used to solve this
issue by learning an additional auxiliary task generation network that can
create helpful tasks for the primary network. The most prominent techniques
rely on Bi-Level Optimization, which incurs computational cost and increased
code complexity. To avoid the need for Bi-Level Optimization, we present an
RL-based approach to dynamically create auxiliary tasks. In this framework, an
RL agent is tasked with selecting auxiliary labels for every data point in a
training set. The agent is rewarded when their selection improves the
performance on the primary task. We also experiment with learning optimal
strategies for weighing the auxiliary loss per data point. On the 20-Superclass
CIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and
performs as well as a prominent Bi-Level Optimization technique. Our weight
learning approaches significantly outperform all of these benchmarks. For
example, a Weight-Aware RL-based approach helps the VGG16 architecture achieve
80.9% test accuracy while the human-labeled auxiliary task setup achieved
75.53%. The goal of this work is to (1) prove that RL is a viable approach to
dynamically generate auxiliary tasks and (2) demonstrate that per-sample
auxiliary task weights can be learned alongside the auxiliary task labels and
can achieve strong results.

</details>


### [62] [SARNet: A Spike-Aware consecutive validation Framework for Accurate Remaining Useful Life Prediction](https://arxiv.org/abs/2510.22955)
*Junhao Fan,Wenrui Liang,Wei-Qiang Zhang*

Main category: cs.LG

TL;DR: SARNet는 RUL 예측 정확도를 향상시키기 위한 현대적 시간 컨볼루션 네트워크를 기반으로 하여 스파이크 감지 기능을 추가하는 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 잔여 유효 수명(RUL)의 정확한 예측은 시스템 신뢰성을 향상시키고 유지보수 리스크를 줄이는 데 필수적입니다.

Method: SARNet는 현대적 시간 컨볼루션 네트워크를 기반으로 하여 스파이크 감지 기능을 추가하고, 적응형 연속 임계값을 사용하여 진정한 스파이크를 검증하며 노이즈를 억제합니다.

Result: SARNet는 벤치마크 포팅된 데이터셋에서 최근 기준선에 비해 오류를 일관되게 낮추었습니다 (RMSE 0.0365, MAE 0.0204).

Conclusion: SARNet는 경량, 견고하며 배포가 용이합니다.

Abstract: Accurate prediction of remaining useful life (RUL) is essential to enhance
system reliability and reduce maintenance risk. Yet many strong contemporary
models are fragile around fault onset and opaque to engineers: short,
high-energy spikes are smoothed away or misread, fixed thresholds blunt
sensitivity, and physics-based explanations are scarce. To remedy this, we
introduce SARNet (Spike-Aware Consecutive Validation Framework), which builds
on a Modern Temporal Convolutional Network (ModernTCN) and adds spike-aware
detection to provide physics-informed interpretability. ModernTCN forecasts
degradation-sensitive indicators; an adaptive consecutive threshold validates
true spikes while suppressing noise. Failure-prone segments then receive
targeted feature engineering (spectral slopes, statistical derivatives, energy
ratios), and the final RUL is produced by a stacked RF--LGBM regressor. Across
benchmark-ported datasets under an event-triggered protocol, SARNet
consistently lowers error compared to recent baselines (RMSE 0.0365, MAE
0.0204) while remaining lightweight, robust, and easy to deploy.

</details>


### [63] [The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination](https://arxiv.org/abs/2510.22977)
*Chenlong Yin,Zeyang Sha,Shiwen Cui,Changhua Meng*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델(LLM)의 추론 능력을 강화하는 것이 도구 환각(tool hallucination)을 증가시키는지를 분석한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 추론 능력 향상은 '생각한 후 행동하는' 에이전트를 구축하기 위한 중요한 전략이다.

Method: SimpleToolHalluBench라는 진단 벤치마크를 도입하여 두 가지 실패 모드에서 도구 환각을 측정한다.

Result: 추론을 강화하는 것이 도구 환각을 비례적으로 증가시킨다는 인과 관계를 입증하고, 과적합을 초월하여 훈련된 비도구 작업에서도 도구 환각이 증가함을 보여준다.

Conclusion: 현재의 추론 강화 방법이 본질적으로 도구 환각을 증폭시키고 있음을 밝혀내어 능력과 신뢰성을 공동으로 최적화하는 새로운 훈련 목표의 필요성을 강조한다.

Abstract: Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key
strategy for building Agents that "think then act." However, recent
observations, like OpenAI's o3, suggest a paradox: stronger reasoning often
coincides with increased hallucination, yet no prior work has systematically
examined whether reasoning enhancement itself causes tool hallucination. To
address this gap, we pose the central question: Does strengthening reasoning
increase tool hallucination? To answer this, we introduce SimpleToolHalluBench,
a diagnostic benchmark measuring tool hallucination in two failure modes: (i)
no tool available, and (ii) only distractor tools available. Through controlled
experiments, we establish three key findings. First, we demonstrate a causal
relationship: progressively enhancing reasoning through RL increases tool
hallucination proportionally with task performance gains. Second, this effect
transcends overfitting - training on non-tool tasks (e.g., mathematics) still
amplifies subsequent tool hallucination. Third, the effect is method-agnostic,
appearing when reasoning is instilled via supervised fine-tuning and when it is
merely elicited at inference by switching from direct answers to step-by-step
thinking. We also evaluate mitigation strategies including Prompt Engineering
and Direct Preference Optimization (DPO), revealing a fundamental
reliability-capability trade-off: reducing hallucination consistently degrades
utility. Mechanistically, Reasoning RL disproportionately collapses
tool-reliability-related representations, and hallucinations surface as
amplified divergences concentrated in late-layer residual streams. These
findings reveal that current reasoning enhancement methods inherently amplify
tool hallucination, highlighting the need for new training objectives that
jointly optimize for capability and reliability.

</details>


### [64] [QoSGMAA: A Robust Multi-Order Graph Attention and Adversarial Framework for Sparse QoS Prediction](https://arxiv.org/abs/2510.22982)
*Guanchen Du,Jianlong Xu,Mingtong Li,Ruiqi Wang,Qianqing Guo,Caiyi Chen,Qingcao Dai,Yuxiang Zeng*

Main category: cs.LG

TL;DR: 이 논문에서는 네트워크 서비스 환경에서의 품질 예측 정확성을 향상시키기 위한 새로운 아키텍처 QoSMGAA를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 인터넷 기술의 급속한 발전에 따라 네트워크 서비스는 사용자에게 다양한 응용 프로그램을 제공하는 데 중요해졌으나, 서비스 수의 기하급수적 증가로 인해 최적 서비스를 선택하는 데 많은 어려움이 존재합니다.

Method: QoSMGAA는 다중 주문 주의 메커니즘을 통합하여 광범위한 컨텍스트 데이터를 집계하고 결측 QoS 값을 효과적으로 예측합니다. 또한, 변환된 상호작용 행렬 기반으로 자가 회귀 감독 학습을 수행하기 위해 적대적 신경망을 포함합니다.

Result: 대규모 실제 데이터 세트에서 수행된 포괄적인 실험 검증 결과, 제안한 모델이 기존 기준 방법보다 우수한 성능을 보입니다.

Conclusion: 이 모델은 서비스 선택 및 추천 시나리오에서 실용적인 배치 가능성에 대한 강력한 잠재력을 강조합니다.

Abstract: With the rapid advancement of internet technologies, network services have
become critical for delivering diverse and reliable applications to users.
However, the exponential growth in the number of available services has
resulted in many similar offerings, posing significant challenges in selecting
optimal services. Predicting Quality of Service (QoS) accurately thus becomes a
fundamental prerequisite for ensuring reliability and user satisfaction.
However, existing QoS prediction methods often fail to capture rich contextual
information and exhibit poor performance under extreme data sparsity and
structural noise. To bridge this gap, we propose a novel architecture, QoSMGAA,
specifically designed to enhance prediction accuracy in complex and noisy
network service environments. QoSMGAA integrates a multi-order attention
mechanism to aggregate extensive contextual data and predict missing QoS values
effectively. Additionally, our method incorporates adversarial neural networks
to perform autoregressive supervised learning based on transformed interaction
matrices. To capture complex, higher-order interactions among users and
services, we employ a discrete sampling technique leveraging the Gumbel-Softmax
method to generate informative negative samples. Comprehensive experimental
validation conducted on large-scale real-world datasets demonstrates that our
proposed model significantly outperforms existing baseline methods,
highlighting its strong potential for practical deployment in service selection
and recommendation scenarios.

</details>


### [65] [AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing](https://arxiv.org/abs/2510.23053)
*Zhiyu Wang,Suman Raj,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 본 논문에서는 다수의 무인 항공기(UAV) 협력 모바일 엣지 컴퓨팅(MEC) 시스템의 궤적 계획, 작업 오프로드 및 자원 할당 문제를 해결하기 위해 AirFed라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: UAV 협력 MEC 시스템은 동적이고 불확실한 환경에서 서비스 품질(QoS)를 보장하면서 궤적 계획, 작업 오프로드 및 자원 할당을 조율하는 데 심각한 도전에 직면해 있습니다.

Method: AirFed는 세 가지 주요 혁신을 통해 이러한 문제를 해결하는 연합 그래프 강화 학습 프레임워크입니다. 첫째, UAV와 IoT 장치 간의 공간-시간 의존성을 모델링하는 이중 계층 동적 그래프 주의 네트워크(GAT)를 설계하였습니다. 둘째, 연속 궤적 조정과 이산 작업 오프로드 결정을 공동으로 최적화하는 이중 액터 단일 비평가 아키텍처를 개발하였습니다. 셋째, 이질적인 UAV 간의 효율적이고 강력한 지식 공유를 가능하게 하는 평판 기반 분산 연합 학습 메커니즘을 제안하였습니다.

Result: AirFed는 최신 기준에 비해 42.9%의 가중 비용 절감, 99% 이상의 기한 만족도, 94.2%의 IoT 장치 커버리지 비율, 54.5%의 통신 오버헤드 감소를 달성했습니다.

Conclusion: AirFed의 확장성 분석 결과는 다양한 UAV 수, IoT 장치 밀도 및 시스템 규모에서 강력한 성능을 확인하였으며, 대규모 UAV-MEC 배치에 대한 실제 적용 가능성을 검증했습니다.

Abstract: Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing
(MEC) systems face critical challenges in coordinating trajectory planning,
task offloading, and resource allocation while ensuring Quality of Service
(QoS) under dynamic and uncertain environments. Existing approaches suffer from
limited scalability, slow convergence, and inefficient knowledge sharing among
UAVs, particularly when handling large-scale IoT device deployments with
stringent deadline constraints. This paper proposes AirFed, a novel federated
graph-enhanced multi-agent reinforcement learning framework that addresses
these challenges through three key innovations. First, we design dual-layer
dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal
dependencies among UAVs and IoT devices, capturing both service relationships
and collaborative interactions within the network topology. Second, we develop
a dual-Actor single-Critic architecture that jointly optimizes continuous
trajectory control and discrete task offloading decisions. Third, we propose a
reputation-based decentralized federated learning mechanism with
gradient-sensitive adaptive quantization, enabling efficient and robust
knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate
that AirFed achieves 42.9% reduction in weighted cost compared to
state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2%
IoT device coverage rate, and reduces communication overhead by 54.5%.
Scalability analysis confirms robust performance across varying UAV numbers,
IoT device densities, and system scales, validating AirFed's practical
applicability for large-scale UAV-MEC deployments.

</details>


### [66] [Rethinking GSPO: The Perplexity-Entropy Equivalence](https://arxiv.org/abs/2510.23142)
*Chi Liu*

Main category: cs.LG

TL;DR: GSPO의 길이 정규화된 중요도 비율을 정보 이론적 양과 연결지어 새로운 관점을 제시하고, 이는 GSPO의 이해에 도움이 되는 통계적 해석을 제공한다.


<details>
  <summary>Details</summary>
Motivation: GSPO의 중요도 비율을 더 깊이 이해하기 위해

Method: GSPO의 시퀀스 수준 가중치를 역당황도 비율과 지수적 교차 엔트로피 변화로 표현하였다.

Result: GSPO의 중요도 가중치를 당황도 비율로 가중할 수 있음을 보여주었다.

Conclusion: 수학적 동등성과 분산 예측은 수학적 추론 작업에 대한 통제된 실험을 통해 검증되었다.

Abstract: We provide a new perspective on GSPO's length-normalized importance ratios by
establishing their connection to information-theoretic quantities. We show that
GSPO's sequence-level weight $s(\theta) =
(\pi_\theta/\pi_{\theta_{\text{old}}})^{1/|y|}$ can be equivalently expressed
as the inverse perplexity ratio
$\text{PPL}_{\theta_{\text{old}}}/\text{PPL}_\theta$ and as the exponential
cross-entropy change $\exp(\Delta H)$. While the perplexity-entropy
relationship follows from standard definitions, this observation provides a
useful lens for understanding GSPO: the algorithm weights policy gradient
updates by perplexity ratios, offering an information-theoretic interpretation
of the importance weights. This perspective helps explain GSPO's empirical
properties, including log-domain variance reduction through geometric averaging
and stability in training mixture-of-experts models. We validate the
mathematical equivalences and variance predictions through controlled
experiments on mathematical reasoning tasks.

</details>


### [67] [Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI](https://arxiv.org/abs/2510.23148)
*Aryan Mathur,Asaduddin Ahmed*

Main category: cs.LG

TL;DR: PDiT 모델은 시각과 언어 이해를 통합하여 더 효과적인 심층 강화 학습 에이전트를 개발합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 아키텍처는 지각과 의사결정을 분리하여 비효율성을 초래합니다. 이는 의사결정의 실패가 지각 모듈의 학습에 도움이 되지 않기 때문입니다.

Method: PDiT 아키텍처는 단일 변환기 내에서 지각 및 의사결정 계층을 번갈아 가며 구현하여 의사결정으로부터 피드백을 동적으로 반영합니다.

Result: PDiT 인코더는 BabyAI GoToLocal 환경에서 평가하였고, 전통적인 PPO 기준선에 비해 더 안정적인 보상과 강한 정렬을 달성하였습니다.

Conclusion: 인터리브 변환기 인코더는 통합된 자율 에이전트를 개발하기 위한 유망한 방향성을 제시합니다.

Abstract: Deep reinforcement learning agents often struggle when tasks require
understanding both vision and language. Conventional architectures typically
isolate perception (for example, CNN-based visual encoders) from
decision-making (policy networks). This separation can be inefficient, since
the policy's failures do not directly help the perception module learn what is
important. To address this, we implement the Perception-Decision Interleaving
Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that
alternates between perception and decision layers within a single transformer.
This interleaving allows feedback from decision-making to refine perceptual
features dynamically. In addition, we integrate a contrastive loss inspired by
CLIP to align textual mission embeddings with visual scene features. We
evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that
the approach achieves more stable rewards and stronger alignment compared to a
standard PPO baseline. The results suggest that interleaved transformer
encoders are a promising direction for developing more integrated autonomous
agents.

</details>


### [68] [The Benchmarking Epistemology: Construct Validity for Evaluating Machine Learning Models](https://arxiv.org/abs/2510.23191)
*Timo Freiesleben,Sebastian Zezulka*

Main category: cs.LG

TL;DR: 예측 벤치마킹은 머신러닝 연구에서 중요한 실천 방법으로, 예측 성능과 경쟁 순위를 기반으로 모델을 평가한다. 하지만 벤치마크 점수는 평가 데이터셋과 특정 학습 문제와 관련하여 모델 성능을 측정하는 데 한정적이다. 이 논문에서는 심리 측정 이론에서 영감을 받아 구축 타당성의 조건을 개발하고, 세 가지 사례 연구를 통해 이러한 가정이 실제로 어떻게 적용되는지 분석한다. 이러한 연구는 벤치마크 점수가 다양한 과학적 주장을 지지할 수 있는 조건을 명확히 하여, 예측 벤치마킹을 머신러닝에서 개념적 및 이론적 추론의 중요한 장으로 조망한다.


<details>
  <summary>Details</summary>
Motivation: 예측 벤치마킹이 머신 러닝 연구에서의 중요한 실천 방법으로 자리잡고 있으며, 이를 통해 머신 러닝 모델의 성능을 평가하고 있다.

Method: 심리 측정 이론에서 영감을 받아 구축 타당성의 조건을 개발하고, 세 가지 사례 연구를 통해 이 가정들이 실제로 어떻게 적용되는지를 검토한다.

Result: 세 가지 사례 연구(이미지넷, 웨더벤치, 취약한 가족 도전)를 통해 벤치마크 점수가 과학적 주장에 지지를 줄 수 있는 조건들을 명확히 한다.

Conclusion: 예측 벤치마킹을 머신러닝에서 개념적 및 이론적 추론의 중요한 장으로 이해할 수 있도록 한다.

Abstract: Predictive benchmarking, the evaluation of machine learning models based on
predictive performance and competitive ranking, is a central epistemic practice
in machine learning research and an increasingly prominent method for
scientific inquiry. Yet, benchmark scores alone provide at best measurements of
model performance relative to an evaluation dataset and a concrete learning
problem. Drawing substantial scientific inferences from the results, say about
theoretical tasks like image classification, requires additional assumptions
about the theoretical structure of the learning problems, evaluation functions,
and data distributions. We make these assumptions explicit by developing
conditions of construct validity inspired by psychological measurement theory.
We examine these assumptions in practice through three case studies, each
exemplifying a typical intended inference: measuring engineering progress in
computer vision with ImageNet; evaluating policy-relevant weather predictions
with WeatherBench; and examining limitations of the predictability of life
events with the Fragile Families Challenge. Our framework clarifies the
conditions under which benchmark scores can support diverse scientific claims,
bringing predictive benchmarking into perspective as an epistemological
practice and a key site of conceptual and theoretical reasoning in machine
learning.

</details>


### [69] [Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter](https://arxiv.org/abs/2510.23215)
*Hong Wang,Jie Wang,Jian Luo,huanshuo dong,Yeqiu Chen,Runmin Jiang,Zhen huang*

Main category: cs.LG

TL;DR: 이 논문에서는 고유값 데이터 생성을 가속화하기 위한 새로운 방법인 SCSF(Sorting Chebyshev Subspace Filter)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 고유값 방법들은 대량의 라벨링된 데이터, 즉 연산자 및 그들의 고유값을 요구합니다.

Method: SCSF는 유사한 고유값 분포를 가진 연산자들을 그룹화하기 위해 절단된 빠른 푸리에 변환 정렬을 적용하고, 이전에 해결된 문제의 고유쌍을 활용하여 이후 문제 해결을 지원하는 체비셰프 서브스페이스 필터를 구축합니다.

Result: 실험 결과에 따르면 SCSF는 다양한 수치적 해결책에 비해 최대 3.5배의 속도 향상을 달성했습니다.

Conclusion: SCSF는 고유값 데이터 생성을 가속화하는 첫 번째 방법으로, 기존 방법론의 한계를 극복합니다.

Abstract: Eigenvalue problems are among the most important topics in many scientific
disciplines. With the recent surge and development of machine learning, neural
eigenvalue methods have attracted significant attention as a forward pass of
inference requires only a tiny fraction of the computation time compared to
traditional solvers. However, a key limitation is the requirement for large
amounts of labeled data in training, including operators and their eigenvalues.
To tackle this limitation, we propose a novel method, named Sorting Chebyshev
Subspace Filter (SCSF), which significantly accelerates eigenvalue data
generation by leveraging similarities between operators -- a factor overlooked
by existing methods. Specifically, SCSF employs truncated fast Fourier
transform sorting to group operators with similar eigenvalue distributions and
constructs a Chebyshev subspace filter that leverages eigenpairs from
previously solved problems to assist in solving subsequent ones, reducing
redundant computations. To the best of our knowledge, SCSF is the first method
to accelerate eigenvalue data generation. Experimental results show that SCSF
achieves up to a $3.5\times$ speedup compared to various numerical solvers.

</details>


### [70] [Sequential Multi-Agent Dynamic Algorithm Configuration](https://arxiv.org/abs/2510.23535)
*Chen Lu,Ke Xue,Lei Yuan,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 이 논문에서는 복잡한 알고리즘의 하이퍼파라미터의 상호 의존성을 고려하여 성능을 향상시키는 Seq-MADAC 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 자동 기계 학습 방식들이 하이퍼파라미터 간의 상호 의존성을 무시하여 최적의 결과를 내지 못하는 문제를 해결하고자 합니다.

Method: Seq-MADAC 프레임워크는 하이퍼파라미터 간의 상호 의존성을 고려하며, 순차적 이점 분해 네트워크를 통해 행동 순서 정보를 활용합니다.

Result: 다양한 실험에서 Seq-MADAC이 최신 MARL 방법들에 비해 우수한 성능을 보이며, 문제 클래스 간 강력한 일반화를 보여줍니다.

Conclusion: Seq-MADAC은 종속성을 인식한 자동 알고리즘 구성의 새로운 패러다임을 확립합니다.

Abstract: Dynamic algorithm configuration (DAC) is a recent trend in automated machine
learning, which can dynamically adjust the algorithm's configuration during the
execution process and relieve users from tedious trial-and-error tuning tasks.
Recently, multi-agent reinforcement learning (MARL) approaches have improved
the configuration of multiple heterogeneous hyperparameters, making various
parameter configurations for complex algorithms possible. However, many complex
algorithms have inherent inter-dependencies among multiple parameters (e.g.,
determining the operator type first and then the operator's parameter), which
are, however, not considered in previous approaches, thus leading to
sub-optimal results. In this paper, we propose the sequential multi-agent DAC
(Seq-MADAC) framework to address this issue by considering the inherent
inter-dependencies of multiple parameters. Specifically, we propose a
sequential advantage decomposition network, which can leverage action-order
information through sequential advantage decomposition. Experiments from
synthetic functions to the configuration of multi-objective optimization
algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art
MARL methods and show strong generalization across problem classes. Seq-MADAC
establishes a new paradigm for the widespread dependency-aware automated
algorithm configuration. Our code is available at
https://github.com/lamda-bbo/seq-madac.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [PortGPT: Towards Automated Backporting Using Large Language Models](https://arxiv.org/abs/2510.22396)
*Zhaoyang Li,Zheng Yu,Jingyi Song,Meng Xu,Yuxuan Luo,Dongliang Mu*

Main category: cs.CR

TL;DR: PORTGPT는 실제 시나리오에서 패치 백포팅을 위한 LLM-agent로, 89.15%의 성공률을 기록하며 기존 도구들을 능가한다.


<details>
  <summary>Details</summary>
Motivation: 인기 있는 오픈소스 프로젝트(예: 리눅스 커널)의 유지 보수를 위해선 패치 백포팅이 필수적이다.

Method: PORTGPT는 코드 접근, Git 역사 요약, 피드백에 따라 패치를 자율적으로 수정할 수 있는 도구로 LLM을 강화한다.

Result: PORTGPT는 기존 데이터셋에서 89.15%, 복잡한 146개 케이스의 데이터셋에서 62.33%의 성공률을 기록하였다.

Conclusion: PORTGPT를 통해 백포팅된 9개의 패치가 리눅스 커널 커뮤니티에 기여되었고, 모든 패치는 병합되었다.

Abstract: Patch backporting, the process of migrating mainline security patches to
older branches, is an essential task in maintaining popular open-source
projects (e.g., Linux kernel). However, manual backporting can be
labor-intensive, while existing automated methods, which heavily rely on
predefined syntax or semantic rules, often lack agility for complex patches.
  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation
of patch backporting in real-world scenarios. PORTGPT enhances an LLM with
tools to access code on-demand, summarize Git history, and revise patches
autonomously based on feedback (e.g., from compilers), hence, simulating
human-like reasoning and verification. PORTGPT achieved an 89.15% success rate
on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex
cases, both outperforms state-of-the-art of backporting tools. We contributed 9
backported patches from PORTGPT to the Linux kernel community and all patches
are now merged.

</details>


### [72] [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
*Julia Bazinska,Max Mathys,Francesco Casucci,Mateo Rojas-Carulla,Xander Davies,Alexandra Souly,Niklas Pfister*

Main category: cs.CR

TL;DR: 대형 언어 모델(LLM)을 기반으로 한 AI 에이전트의 보안에 대한 체계적인 이해가 부족하다. 이 논문은 LLM의 취약점을 드러내는 에이전트 실행 흐름의 특정 상태를 분리하는 '위협 스냅샷' 프레임워크를 도입하고, 이를 통해 보안 위험을 체계적으로 식별하고 분류한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 보안에 대한 체계적 이해 부족과 기존 프레임워크의 한계를 해결하기 위해.

Method: '위협 스냅샷' 프레임워크를 사용하여 LLM의 취약점이 드러나는 특정 상태를 분리하고, 194331개의 고유한 군중 소싱 적대 공격을 기반으로 하는 보안 벤치마크 $	ext{b}^3$를 구성.

Result: 31개의 인기 LLM을 평가하여 향상된 추론 능력이 보안을 개선하지만 모델 크기와 보안 간 상관관계는 없음을 발견했다.

Conclusion: 벤치마크와 데이터셋, 평가 코드를 공개하여 LLM 제공자와 실무자들이 널리 채택할 수 있도록 하고, 에이전트 개발자에게는 지침을 제공하며 모델 개발자에게는 백본 보안 개선을 우선시하도록 유도한다.

Abstract: AI agents powered by large language models (LLMs) are being deployed at
scale, yet we lack a systematic understanding of how the choice of backbone LLM
affects agent security. The non-deterministic sequential nature of AI agents
complicates security modeling, while the integration of traditional software
with AI components entangles novel LLM vulnerabilities with conventional
security risks. Existing frameworks only partially address these challenges as
they either capture specific vulnerabilities only or require modeling of
complete agents. To address these limitations, we introduce threat snapshots: a
framework that isolates specific states in an agent's execution flow where LLM
vulnerabilities manifest, enabling the systematic identification and
categorization of security risks that propagate from the LLM to the agent
level. We apply this framework to construct the $\operatorname{b}^3$ benchmark,
a security benchmark based on 194331 unique crowdsourced adversarial attacks.
We then evaluate 31 popular LLMs with it, revealing, among other insights, that
enhanced reasoning capabilities improve security, while model size does not
correlate with security. We release our benchmark, dataset, and evaluation code
to facilitate widespread adoption by LLM providers and practitioners, offering
guidance for agent developers and incentivizing model developers to prioritize
backbone security improvements.

</details>


### [73] [SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking](https://arxiv.org/abs/2510.22726)
*Van Le,Tan Le*

Main category: cs.CR

TL;DR: SpoofTrackBench는 레이더 스푸핑 공격 하의 실시간 위치 추적 시스템의 적대적 강건성을 평가하기 위한 재현 가능한 모듈형 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 실시간 위치 추적 시스템에서 스푸핑 공격에 대한 강건성을 평가할 필요성.

Method: Hampton University Skyler Radar Sensor 데이터셋을 활용하여 드리프트, 유령 및 미러 타입 스푸핑 공격을 시뮬레이션하고, JPDA 및 GNN 아키텍처를 사용하여 추적기 성능을 평가한다.

Result: 클러스터 오버레이, 주입 인식 타임라인 및 시나리오 적응형 시각화를 통해 스푸핑 유형과 구성에 따른 해석 가능성을 제공하고, 평가 수치와 로그를 자동 내보내어 비교 가능성을 높인다.

Conclusion: SpoofTrackBench는 스푸프 인식 추적 파이프라인의 개방적이고 윤리적인 벤치마킹을 위한 새로운 표준을 설정하며, 엄밀한 아키텍처 간 분석 및 커뮤니티 검증을 가능하게 한다.

Abstract: SpoofTrackBench is a reproducible, modular benchmark for evaluating
adversarial robustness in real-time localization and tracking (RTLS) systems
under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor
dataset, we simulate drift, ghost, and mirror-type spoofing attacks and
evaluate tracker performance using both Joint Probabilistic Data Association
(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates
clean and spoofed detection streams, visualizes spoof-induced trajectory
divergence, and quantifies assignment errors via direct drift-from-truth
metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive
visualizations enable interpretability across spoof types and configurations.
Evaluation figures and logs are auto-exported for reproducible comparison.
SpoofTrackBench sets a new standard for open, ethical benchmarking of
spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis
and community validation.

</details>


### [74] [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963)
*Zesen Liu,Zhixiang Zhang,Yuchong Xie,Dongdong She*

Main category: cs.CR

TL;DR: LLM 기반 에이전트의 프롬프트 압축이 새로운 보안 위험을 초래하며, CompressionAttack 프레임워크를 통해 이를 악용할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: LLM을 사용하는 에이전트에서 프롬프트 압축은 추론 비용을 줄이지만, 이는 새로운 보안 위험을 발생시킬 수 있다.

Method: CompressionAttack 프레임워크는 하드 압축을 위한 HardCom과 소프트 압축을 위한 SoftCom의 두 가지 전략을 포함한다.

Result: 여러 LLM에 대한 실험에서 최대 80%의 공격 성공률과 98%의 선호도 변화가 확인되었다.

Conclusion: 현재의 방어는 효과적이지 않으며, 더 강력한 보호 장치가 필요하다.

Abstract: LLM-powered agents often use prompt compression to reduce inference costs,
but this introduces a new security risk. Compression modules, which are
optimized for efficiency rather than safety, can be manipulated by adversarial
inputs, causing semantic drift and altering LLM behavior. This work identifies
prompt compression as a novel attack surface and presents CompressionAttack,
the first framework to exploit it. CompressionAttack includes two strategies:
HardCom, which uses discrete adversarial edits for hard compression, and
SoftCom, which performs latent-space perturbations for soft compression.
Experiments on multiple LLMs show up to 80% attack success and 98% preference
flips, while remaining highly stealthy and transferable. Case studies in VSCode
Cline and Ollama confirm real-world impact, and current defenses prove
ineffective, highlighting the need for stronger protections.

</details>


### [75] [Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy](https://arxiv.org/abs/2510.23274)
*Weixuan Chen,Qianqian Yang,Shuo Shao,Shunpu Tang,Zhiguo Shi,Shui Yu*

Main category: cs.CR

TL;DR: 본 논문은 이미지 전송을 위한 새로운 안전한 의미 통신 프레임워크를 제안하며, 이는 차별적 개인 정보 보호를 활용하여 근사적인 개인 정보 보호 보장을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 의미 통신 접근 방식들이 갖고 있는 보안 한계와 비현실적인 가정들을 해결하기 위한 필요성이 있다.

Method: 생성적 적대 신경망(GAN) 반전 방법을 사용하여 소스 이미지에서 분리된 의미 표현을 추출하고, 이후 사적인 의미 표현에 근사적인 DP 노이즈를 선택적으로 적용하는 방법을 제안한다.

Result: 제안된 방법은 이전 DP 기반 방식 및 직접 전송에 비해 도청자의 재구성 품질을 상당히 저하시키면서도 작업 성능에만 경미한 저하를 초래한다.

Conclusion: 제안된 방법은 보안 수준을 특정 요구 사항에 따라 조정할 수 있는 명시적으로 제어 가능한 보안 레벨을 제공한다.

Abstract: While semantic communication (SemCom) improves transmission efficiency by
focusing on task-relevant information, it also raises critical privacy
concerns. Many existing secure SemCom approaches rely on restrictive or
impractical assumptions, such as favorable channel conditions for the
legitimate user or prior knowledge of the eavesdropper's model. To address
these limitations, this paper proposes a novel secure SemCom framework for
image transmission over wiretap channels, leveraging differential privacy (DP)
to provide approximate privacy guarantees. Specifically, our approach first
extracts disentangled semantic representations from source images using
generative adversarial network (GAN) inversion method, and then selectively
perturbs private semantic representations with approximate DP noise. Distinct
from conventional DP-based protection methods, we introduce DP noise with
learnable pattern, instead of traditional white Gaussian or Laplace noise,
achieved through adversarial training of neural networks (NNs). This design
mitigates the inherent non-invertibility of DP while effectively protecting
private information. Moreover, it enables explicitly controllable security
levels by adjusting the privacy budget according to specific security
requirements, which is not achieved in most existing secure SemCom approaches.
Experimental results demonstrate that, compared with the previous DP-based
method and direct transmission, the proposed method significantly degrades the
reconstruction quality for the eavesdropper, while introducing only slight
degradation in task performance. Under comparable security levels, our approach
achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86
for the legitimate user compared with the previous DP-based method.

</details>
