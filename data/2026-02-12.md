<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 24]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective](https://arxiv.org/abs/2602.09159)
*Yichen Wu,Yujin Oh,Sangjoon Park,Kailong Fan,Dania Daye,Hana Farzaneh,Xiang Li,Raul Uppot,Quanzheng Li*

Main category: cs.AI

TL;DR: CoMMa는 분산 LLM-agent 프레임워크로, 전문가들이 파티셔닝된 증거를 기반으로 강한 의사결정을 위해 협력합니다.


<details>
  <summary>Details</summary>
Motivation: 온콜로지 의사결정 지원 과제를 해결하기 위한 다중 에이전트 프레임워크의 필요성.

Method: 전문가들이 분할된 증거를 바탕으로 작업하고 게임 이론적 목표를 통해 조정하는 분산 LLM-agent 프레임워크인 CoMMa를 제안합니다.

Result: 다양한 종양학 벤치마크에서 CoMMa는 데이터 중심 및 역할 기반 다중 에이전트 기준보다 더 높은 정확도와 안정적인 성능을 달성합니다.

Conclusion: CoMMa는 기여를 인식하여 명시적인 증거 귀속을 제공하며, 해석 가능하고 수학적으로 근거 있는 의사결정 경로를 생성합니다.

Abstract: Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.

</details>


### [2] [FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases](https://arxiv.org/abs/2602.09163)
*Xingjian Zhang,Sophia Moylan,Ziyang Xiong,Qiaozhu Mei,Yichen Luo,Jiaqi W. Ma*

Main category: cs.AI

TL;DR: FlyBench는 과학 문헌에서의 엔드 투 엔드 온톨로지 큐레이션을 평가하기 위한 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 과학적 지식 기반은 AI 시스템과 인간 연구자 모두를 위한 구조화된 쿼리 가능 형식으로 주요 문헌의 발견을 선별하여 발견을 가속화한다.

Method: 단일 유전자 기호만 주어지면, AI 에이전트는 16,898개의 전체 텍스트 논문에서 검색하고 읽어야 하며, 기능, 표현 패턴 및 역사적 동의어를 설명하는 구조화된 주석을 생성해야 한다.

Result: 4가지 기본 에이전트 아키텍처를 평가한 결과, 다중 에이전트 설계가 더 단순한 대안보다 성능이 우수하다는 것을 발견했다.

Conclusion: 우리는 FlyBench가 과학적 영역 전반에 걸쳐 광범위한 응용을 가진 검색 보강 과학적 추론의 발전을 촉진할 것으로 기대한다.

Abstract: Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.

</details>


### [3] [Human Control Is the Anchor, Not the Answer: Early Divergence of Oversight in Agentic AI Communities](https://arxiv.org/abs/2602.09286)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.AI

TL;DR: 이 논문은 두 개의 Reddit 커뮤니티(r/OpenClaw 및 r/Moltbook)의 비교 분석을 통해 에이전트 AI에 대한 감독의 다양성을 탐구하고, 각 커뮤니티의 역할에 따라 감독 기대가 달라짐을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 AI에 대한 감독이 단일 목표로 논의되지만, 초기 채택은 역할에 따라 특정 기대를 생성할 수 있다.

Method: 주제 모델링, 감독 주제 추상화, 참여 가중치 강조 및 편차 테스트를 사용하여 두 커뮤니티를 비교 분석했다.

Result: 커뮤니티는 강하게 분리되며, '인간의 통제'라는 용어가 사용되지만 그 의미는 다르게 나타난다: r/OpenClaw는 실행 가이드라인과 회복을 강조하고, r/Moltbook은 정체성, 정당성 및 공개 상호작용의 책임을 강조한다.

Conclusion: 에이전트 역할에 맞는 감독 메커니즘을 설계하고 평가하기 위한 통찰력을 제공한다.

Abstract: Oversight for agentic AI is often discussed as a single goal ("human control"), yet early adoption may produce role-specific expectations. We present a comparative analysis of two newly active Reddit communities in Jan--Feb 2026 that reflect different socio-technical roles: r/OpenClaw (deployment and operations) and r/Moltbook (agent-centered social interaction). We conceptualize this period as an early-stage crystallization phase, where oversight expectations form before norms reach equilibrium.
  Using topic modeling in a shared comparison space, a coarse-grained oversight-theme abstraction, engagement-weighted salience, and divergence tests, we show the communities are strongly separable (JSD =0.418, cosine =0.372, permutation $p=0.0005$). Across both communities, "human control" is an anchor term, but its operational meaning diverges: r/OpenClaw} emphasizes execution guardrails and recovery (action-risk), while r/Moltbook} emphasizes identity, legitimacy, and accountability in public interaction (meaning-risk). The resulting distinction offers a portable lens for designing and evaluating oversight mechanisms that match agent role, rather than applying one-size-fits-all control policies.

</details>


### [4] [Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge](https://arxiv.org/abs/2602.09341)
*Wei Yang,Shixuan Li,Heng Ping,Peiyu Zhang,Paul Bogdan,Jesse Thomason*

Main category: cs.AI

TL;DR: AgentAuditor는 다수결 대신 에이전트 추적 경로를 통해 협의 및 분기점을 분석하여 갈등을 해결하는 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다수결 방식이 대체로 믿을 수 없는 결과를 생성하기 때문에, 좀 더 정교하고 신뢰할 수 있는 방식으로 에이전트 출력을 처리할 필요가 있다.

Method: AgentAuditor는 Reasoning Tree를 이용하여 에이전트 간의 합의 및 분기를 명시적으로 표현하고, 비효율적인 글로벌 판정 대신에 지역적 검증을 수행한다.

Result: AgentAuditor는 5개의 인기 있는 설정에서 다수결에 비해 최대 5%의 정확도 개선을 보였으며, LLM을 판사로 사용하는 경우에 비해 최대 3%의 개선을 보인다.

Conclusion: AgentAuditor는 다중 에이전트 시스템 설정에 구애받지 않으며, 에이전트 간의 협의를 개선하는 효과적인 도구로 평가된다.

Abstract: Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.

</details>


### [5] [P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads](https://arxiv.org/abs/2602.09443)
*Yun Luo,Futing Wang,Qianjia Cheng,Fangchen Yu,Haodi Lei,Jianhao Yan,Chenxi Li,Jiacheng Chen,Yufeng Zhao,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Wenxuan Zeng,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.AI

TL;DR: P1-VL이라는 오픈소스 비전-언어 모델은 물리학을 통한 과학적 추론에 중점을 두고 있으며, 12개의 금메달을 획득하여 오픈소스 비전-언어 모델 분야에서 최첨단 성능을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 물리학은 추상적 논리를 물리적 현실에 결합하는 주요 시험 기준으로 여겨지며, 이는 다중 모드 인식을 필요로 합니다.

Method: 우리는 커리큘럼 강화 학습과 에이전틱 보강 방식을 결합하여 시각-논리 간의 간극을 메우는 P1-VL 모델을 소개합니다.

Result: P1-VL-235B-A22B 모델은 2024-2025년의 까다로운 HiPhO 벤치마크에서 12개의 금메달을 획득하고 최첨단 성능을 기록했습니다.

Conclusion: P1-VL은 물리 법칙과 시각적 인식을 잘 조화시켜 기계 과학적 발견을 위한 기초 단계를 제공합니다.

Abstract: The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.

</details>


### [6] [SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning](https://arxiv.org/abs/2602.09463)
*Furong Jia,Ling Dai,Wenjin Deng,Fan Zhang,Chen Hu,Daxin Jiang,Yu Liu*

Main category: cs.AI

TL;DR: 이 논문에서는 SpotAgent라는 새로운 프레임워크를 제안하여 시각적 단서가 부족하고 애매한 실제 상황에서 지리적 로컬라이제이션의 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: LVLMs는 지리적 로컬라이제이션에서 강력한 추론 능력을 보여주지만, 실제 세계의 복잡성과 불확실성으로 인해 성과가 저해됩니다.

Method: SpotAgent는 전문가 수준의 추론을 활용하여 시각적 해석과 도구 지원 검증을 융합하는 에이전틱 추론 프로세스로 지리적 로컬라이제이션을 형식화합니다. ReAct 다이어그램을 통해 외부 도구를 활용하여 시각적 단서를 탐색하고 검증합니다.

Result: SpotAgent는 표준 벤치마크에서 최첨단 성과를 달성하며, 정밀하고 검증 가능한 지리적 로컬라이제이션을 제공합니다.

Conclusion: SpotAgent는 학습 가능한 샘플을 공간적 난이도에 따라 우선순위를 매기는 공간 인지 동적 필터링 전략을 활용하여 RL 단계의 효율성을 향상시킵니다.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.

</details>


### [7] [Computing Conditional Shapley Values Using Tabular Foundation Models](https://arxiv.org/abs/2602.09489)
*Lars Henry Berge Olsen,Dennis Christensen*

Main category: cs.AI

TL;DR: Shapley 값의 계산 비용 문제를 해결하기 위해 TabPFN과 같은 표 형식의 기초 모델을 활용하여 조건부 기대값을 신속하게 근사할 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: Shapley 값은 설명 가능한 AI의 핵심 요소이지만, 기능 간 의존성으로 인해 계산 비용이 많이 든다.

Method: Tabular foundation models, 특히 TabPFN을 사용하여 조건부 기대값을 근사하고, 이를 통해 Shapley 값을 계산한다.

Result: TabPFN이 다른 최신 방법들과 비교할 때 대부분의 경우 최상의 성능을 발휘하며, 성능이 떨어지는 경우에도 최상의 방법보다 실행 시간이 훨씬 짧다.

Conclusion: 앞으로 조건부 Shapley 값 추정을 위해 표 형식 모델을 더 잘 적응시킬 방법에 대해 논의한다.

Abstract: Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.

</details>


### [8] [GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis](https://arxiv.org/abs/2602.09794)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Xudong Wang,Zhenzhen Huang,Pengcheng Zheng,Shuai Yuan,Sheng Zheng,Qigan Sun,Jie Zou,Lik-Hang Lee,Yang Yang*

Main category: cs.AI

TL;DR: GHS-TDA는 복잡한 작업에서 대형 언어 모델의 추론 정확성을 개선하기 위해 제안된 새로운 방법으로, 여러 개의 추론 경로를 동시에 다루고 안정적인 구조를 통해 일관성 높은 추론을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 CoT 방법의 한계, 즉 초기 결정에 대한 민감성과 구조적 분석 기법의 부족을 해결하기 위해 GHS-TDA를 제안합니다.

Method: GHS-TDA는 의미적으로 풍부한 글로벌 가설 그래프를 구축하여 다수의 후보 추론 경로를 집계하고 정렬하며 조정합니다. 그 후, 지속적인 호몰로지를 기반으로 하는 위상적 데이터 분석을 적용하여 안정적인 다중 스케일 구조를 포착하고 중복성과 불일치성을 제거합니다.

Result: GHS-TDA는 높은 신뢰도와 해석 가능한 추론 경로를 생성하며, 여러 추론 벤치마크에서 정확성과 강건성 측면에서 우수한 성능을 보여 줍니다.

Conclusion: GHS-TDA는 추론 다양성과 위상적 안정성을 함께 활용하여 자가 적응 수렴을 달성합니다.

Abstract: Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>


### [9] [Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning](https://arxiv.org/abs/2602.09813)
*Dexun Li,Sidney Tio,Pradeep Varakantham*

Main category: cs.AI

TL;DR: 비지도 환경 설계(UED) 접근법을 통해 일반 목적 에이전트를 개발하는 방법을 제안함. 계층적 마르코프 결정 프로세스를 활용하여 학생의 정책 표현을 기반으로 교육 환경을 생성.


<details>
  <summary>Details</summary>
Motivation: 제한된 리소스 환경에서 점점 더 유용한 환경을 자동으로 생성할 수 있는 방법을 연구.

Method: 계층적 마르코프 결정 프로세스(MDP) 프레임워크를 제시하여 교사 에이전트가 학생 정책 표현을 활용해 교육 환경을 생성하도록 함.

Result: 여러 도메인에서 실험을 통해 우리의 방법이 기존 접근 방식보다 우수하며 교사-학생 상호작용을 줄이는 데 성공했음을 보여줌.

Conclusion: 훈련 기회가 제한된 환경에서도 우리의 접근 방식이 적용 가능하다는 것을 시사함.

Abstract: Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.

</details>


### [10] [Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?](https://arxiv.org/abs/2602.09937)
*Taeyoon Kim,Woohyeok Park,Hoyeong Yun,Kyungyong Lee*

Main category: cs.AI

TL;DR: 대규모 클라우드 시스템의 실패는 상당한 재정적 손실을 초래하며, 자동화된 원인 분석(RCA)이 필수적이다. 최신 연구는 대형 언어 모델(LLM) 기반 에이전트를 활용하지만, 기존 시스템은 낮은 검출 정확도를 보이고, 현재 평가 프레임워크는 최종 답변의 정당성만을 평가한다. 본 논문은 LLM 기반 RCA 에이전트의 프로세스 수준 실패 분석을 제시하며, 5개의 LLM 모델에서 OpenRCA 벤치마크를 실행하여 1,675개의 에이전트 실행을 실시하였다. 에이전트 간의 추론, 의사소통, 환경 상호작용에서 관찰된 실패를 12가지 함정 유형으로 분류하고, 주요 실패는 모든 모델에서 공통적으로 발생한다고 밝혀졌다. 실험 결과, 프롬프트 엔지니어링만으로는 문제를 해결할 수 없으며, 에이전트 간의 의사소통 프로토콜을 강화하면 통신 관련 실패를 최대 15%까지 감소시킬 수 있다는 것을 보여준다. 이 연구에서 개발된 함정 분류와 진단 방법론은 클라우드 RCA를 위한 신뢰할 수 있는 자율 에이전트를 설계하는 기반을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 클라우드 시스템의 실패로 인한 재정적 손실을 줄이기 위해 자동화된 원인 분석(RCA)의 필요성을 강조한다.

Method: 5개의 LLM 모델을 사용하여 OpenRCA 벤치마크를 실행하고, 1,675개의 에이전트 실행 결과를 분석하여 12가지의 실패 유형을 분류한다.

Result: 관찰된 실패는 모든 모델에서 공통적으로 나타났으며, 특히 환각적 데이터 해석과 불완전한 탐색이 주요 요인으로 작용하였다.

Conclusion: 프롬프트 엔지니어링은 문제 해결에 한계가 있으며, 에이전트 간의 의사소통 프로토콜을 개선함으로써 실패를 줄일 수 있다는 것을 보여준다.

Abstract: Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.

</details>


### [11] [Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning](https://arxiv.org/abs/2602.09945)
*Jinsong Liu,Yuhang Jiang,Ramayya Krishnan,Rema Padman,Yiye Zhang,Jiang Bian*

Main category: cs.AI

TL;DR: 의료 결정 지원은 정확한 답변뿐만 아니라 임상적으로 유효한 추론을 요구한다. 본 논문에서는 추론 불일치로부터 학습하여 임상 에이전트를 개선하는 프레임워크인 차등 추론 학습(DRL)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 임상적 의사 결정을 지원하기 위해서는 정확한 답변뿐만 아니라 임상적으로 유효한 추론이 필요하다.

Method: DRL은 참조 추론 근거(예: 의사가 작성한 임상 근거, 임상 지침 또는 더 능력 있는 모델의 출력)와 에이전트의 자유 형식 사고 체인(COT)으로부터 추론 그래프를 추출하고, 임상적으로 가중된 그래프 편집 거리(GED)를 기반으로 불일치 분석을 수행한다.

Result: 오픈 의료 질문 응답(QA) 기준과 내부 임상 데이터의 방문 환자 예측 작업에서 평가한 결과, 최종 답변 정확도와 추론 충실성이 향상되었음을 보여준다.

Conclusion: DRL은 복잡한 추론 시나리오에서 보다 신뢰할 수 있는 임상 의사 결정을 지원하며, 제한된 토큰 예산 하에서 배포를 위한 실용적인 메커니즘을 제공한다.

Abstract: Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

</details>


### [12] [Discovering High Level Patterns from Simulation Traces](https://arxiv.org/abs/2602.10009)
*Sean Memery,Kartic Subr*

Main category: cs.AI

TL;DR: AI 에이전트가 자연어로 상호작용할 때 물리적 상호작용에 대한 도전 과제가 존재하며 이를 해결하기 위한 방법으로 자연어에 의해 유도된 패턴 발견을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 물리적 상호작용이 있는 환경에서 다양한 도전에 직면하고 자연어로 상호작용 시 더욱 복잡해진다.

Method: 자세한 시뮬레이션 로그에서 대략적인 패턴을 발견하는 자연어 유도 방법을 제안하며, 시뮬레이션 로그에서 작동하는 프로그램을 합성하여 높은 수준의 활성화된 패턴으로 매핑한다.

Result: 이러한 주석된 시뮬레이션 로그는 물리적 시스템에 대한 자연어 추론에 더 적합하다는 것을 두 개의 물리학 벤치마크를 통해 보여주었다.

Conclusion: 이 방법은 자연어로 지정된 목표로부터 효과적인 보상 프로그램을 생성하는 LMs의 능력을 향상시키며 계획이나 지도 학습 맥락에서 사용될 수 있다.

Abstract: Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>


### [13] [Chain of Mindset: Reasoning with Adaptive Cognitive Modes](https://arxiv.org/abs/2602.10063)
*Tianyi Jiang,Arctanx An,Hengyi Feng,Naixin Zhai,Haodong Li,Xiaomin Yu,Jiahui Liu,Hanwen Du,Shuo Zhang,Zhi Yang,Jie Huang,Yuhua Li,Yongxin Ni,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 본 연구에서는 Chain of Mindset(CoM) 프레임워크를 통해 다양한 문제 해결 단계에 적합한 사고 방식을 동적으로 조정할 수 있도록 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 LLM 추론 방법들은 고정된 사고방식을 사용하여 문제 해결의 다양한 단계를 다루는 데 한계를 보입니다.

Method: Chain of Mindset (CoM) 프레임워크는 추론을 네 가지 서로 다른 사고방식(공간적, 수렴적, 발산적, 알고리즘적)으로 분해하고, 메타 에이전트가 진화하는 추론 상태에 따라 최적의 사고방식을 선택합니다.

Result: CoM은 6개의 도전적인 벤치마크에서 최첨단 성능을 달성하여, Qwen3-VL-32B-Instruct와 Gemini-2.0-Flash에서 각각 4.96% 및 4.72%의 정확도로 가장 강력한 기준선보다 우수한 성능을 보였습니다.

Conclusion: 이 연구는 사람의 문제 해결 방식과 유사한 방식으로 LLM의 사고 방식을 개선하여, 추론의 효율성을 높이고 있습니다.

Abstract: Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.

</details>


### [14] [CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs](https://arxiv.org/abs/2602.10085)
*Richard Bornemann,Pierluigi Vito Amadori,Antoine Cully*

Main category: cs.AI

TL;DR: 이 논문은 연속적이고 개방적인 기술 발견을 위한 새로운 프레임워크인 CODE-SHARP를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 인공지능에서 새롭고 다양한 기술을 발견하고 학습할 수 있는 에이전트를 개발하는 것은 중요한 도전 과제입니다.

Method: CODE-SHARP는 기초 모델을 활용하여 실행 가능한 보상 함수의 방향 그래프로 구조화된 계층적 기술 아카이브를 지속적으로 확장하고 정제합니다.

Result: 발견된 SHARP 기술로 생성된 보상만으로 훈련된 목표 조건 에이전트는 Craftax 환경에서 점점 더 긴 목표를 해결하는 방법을 학습합니다.

Conclusion: 발견된 기술은 복잡하고 긴 목표를 해결할 수 있도록 에이전트를 지원하며, pretrained agents 및 특정 작업 전문가 정책보다 평균 134% 이상 성능을 향상시킵니다.

Abstract: Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos at https://sites.google.com/view/code-sharp/homepage.

</details>


### [15] [Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.10090)
*Zhaoyang Wang,Canwen Xu,Boyi Liu,Yite Wang,Siwei Han,Zhewei Yao,Huaxiu Yao,Yuxiong He*

Main category: cs.AI

TL;DR: 이 논문에서는 자동 에이전트 훈련을 위한 합성 환경 생성 파이프라인인 에이전트 월드 모델(AWM)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다양하고 신뢰할 수 있는 환경의 부족으로 인해 복잡한 작업을 수행하는 자율 에이전트의 훈련이 제한됩니다.

Method: AWM 파이프라인을 사용하여 1,000개의 환경을 생성하고, 각 환경은 평균 35개의 도구를 소화하며 고품질의 관찰 결과를 얻습니다.

Result: 합성 환경에서의 훈련이 벤치마크 특정 환경에서 훈련하는 것보다 더 강력한 분포 외 일반화를 달성합니다.

Conclusion: AWM은 더 효율적인 에이전트 상호작용 및 신뢰할 수 있는 보상 함수 설계를 가능하게 합니다.

Abstract: Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Spectral Disentanglement and Enhancement: A Dual-domain Contrastive Framework for Representation Learning](https://arxiv.org/abs/2602.09066)
*Jinjin Guo,Yexin Li,Zhichao Huang,Jun Fang,Zhiyuan Liu,Chao Liu,Pengzhang Liu,Qixia Jiang*

Main category: cs.LG

TL;DR: 대규모 다중 모드 대조 학습에서 SDE라는 새로운 프레임워크를 제안하여 내재된 스펙트럼 구조를 활용함으로써 모델 일반화 성능을 향상시키고 더 강력한 표현을 학습한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 다중 모드 대조 학습의 한계는 특성 차원의 균일한 처리와 학습된 특성의 내재된 스펙트럼 구조를 무시하는 데 있다.

Method: 스펙트럼 분리 및 향상(SDE) 프레임워크는 단일값 분해를 활용하여 특성 차원을 강한 신호, 약한 신호 및 노이즈로 분할하고 커리큘럼 기반의 스펙트럴 향상 전략을 적용한다.

Result: 대규모 다중 모드 벤치마크에서 SDE는 표현의 견고성과 일반화를 지속적으로 개선하여 최신 방법들을 능가한다.

Conclusion: SDE는 기존의 대조 파이프라인과 원활하게 통합되어 다중 모드 표현 학습을 위한 효과적인 솔루션을 제공한다.

Abstract: Large-scale multimodal contrastive learning has recently achieved impressive success in learning rich and transferable representations, yet it remains fundamentally limited by the uniform treatment of feature dimensions and the neglect of the intrinsic spectral structure of the learned features. Empirical evidence indicates that high-dimensional embeddings tend to collapse into narrow cones, concentrating task-relevant semantics in a small subspace, while the majority of dimensions remain occupied by noise and spurious correlations. Such spectral imbalance and entanglement undermine model generalization. We propose Spectral Disentanglement and Enhancement (SDE), a novel framework that bridges the gap between the geometry of the embedded spaces and their spectral properties. Our approach leverages singular value decomposition to adaptively partition feature dimensions into strong signals that capture task-critical semantics, weak signals that reflect ancillary correlations, and noise representing irrelevant perturbations. A curriculum-based spectral enhancement strategy is then applied, selectively amplifying informative components with theoretical guarantees on training stability. Building upon the enhanced features, we further introduce a dual-domain contrastive loss that jointly optimizes alignment in both the feature and spectral spaces, effectively integrating spectral regularization into the training process and encouraging richer, more robust representations. Extensive experiments on large-scale multimodal benchmarks demonstrate that SDE consistently improves representation robustness and generalization, outperforming state-of-the-art methods. SDE integrates seamlessly with existing contrastive pipelines, offering an effective solution for multimodal representation learning.

</details>


### [17] [Counterfactual Maps: What They Are and How to Find Them](https://arxiv.org/abs/2602.09128)
*Awa Khouna,Julien Ferry,Thibaut Vidal*

Main category: cs.LG

TL;DR: 이 논문은 트리 앙상블을 위한 카운터팩츄얼 생성 방법을 제시하며, 이를 통해 글로벌 최적 카운터팩츄얼 설명을 운영할 수 있는 가장 가까운 지역 검색을 재검토한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 모델에 대한 카운터팩츄얼 설명의 정확한 вычисление이 여전히 어렵기 때문에, 이러한 문제를 해결하기 위해 새로운 접근 방식을 제안한다.

Method: 모든 트리 앙상블을 라벨이 있는 하이퍼렉탱글의 동등한 파르티션으로 압축할 수 있는 사실을 활용하여 카운터팩츄얼 검색을 일반화된 보로노이 셀을 식별하는 문제로 변환한다.

Result: 이 접근 방식은 밀리초 수준의 지연으로 글로벌 최적 카운터팩츄얼 설명을 제공하며, 기존의 정확한 초기 최적화 방법보다 쿼리 시간이 몇 배 빠르다.

Conclusion: 이 연구는 카운터팩츄얼 생성에 대한 새로운 기법을 제안하며, 이는 고위험 응용 분야에서 실험 분석을 통해 성능이 입증되었다.

Abstract: Counterfactual explanations are a central tool in interpretable machine learning, yet computing them exactly for complex models remains challenging. For tree ensembles, predictions are piecewise constant over a large collection of axis-aligned hyperrectangles, implying that an optimal counterfactual for a point corresponds to its projection onto the nearest rectangle with an alternative label under a chosen metric. Existing methods largely overlook this geometric structure, relying either on heuristics with no optimality guarantees or on mixed-integer programming formulations that do not scale to interactive use.
  In this work, we revisit counterfactual generation through the lens of nearest-region search and introduce counterfactual maps, a global representation of recourse for tree ensembles. Leveraging the fact that any tree ensemble can be compressed into an equivalent partition of labeled hyperrectangles, we cast counterfactual search as the problem of identifying the generalized Voronoi cell associated with the nearest rectangle of an alternative label. This leads to an exact, amortized algorithm based on volumetric k-dimensional (KD) trees, which performs branch-and-bound nearest-region queries with explicit optimality certificates and sublinear average query time after a one-time preprocessing phase.
  Our experimental analyses on several real datasets drawn from high-stakes application domains show that this approach delivers globally optimal counterfactual explanations with millisecond-level latency, achieving query times that are orders of magnitude faster than existing exact, cold-start optimization methods.

</details>


### [18] [$n$-Musketeers: Reinforcement Learning Shapes Collaboration Among Language Models](https://arxiv.org/abs/2602.09173)
*Ryozo Masukawa,Sanggeon Yun,Hyunwoo Oh,SuhgHeon Jeong,Raheeb Hassa,Hanning Chen,Wenjun Huang,Mahdi Imani,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: 작은 전문 언어 모델(SLM)이 대형 언어 모델(LLM)에 의존하지 않고 구조적 추론을 보여줄 수 있음을 확인한 연구.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 검증 가능한 보상을 활용한 최근의 진전을 활용하기 위해, 작은 언어 모델들이 큰 모델에 의존하지 않고도 효과적인 추론을 할 수 있는 방법을 찾고자 함.

Method: 다양한 고정된 SLM 전문가들이 학습 가능한 주의 인터페이스를 통해 그들의 내부 표현으로 통합되는 소프트 숨겨진 상태 협업 방식 도입.

Result: Reasoning Gym과 GSM8K 실험을 통해 이 잠재적 통합 방식이 강력한 단일 모델의 RLVR 기준과 경쟁력이 있음을 입증함.

Conclusion: 전반적으로 숨겨진 상태 협업은 고정된 전문가를 활용하는 간결한 메커니즘을 제공하고, RLVR 하에서 전문가 활용 패턴과 그 진화를 관찰할 수 있는 창을 제공함.

Abstract: Recent progress in reinforcement learning with verifiable rewards (RLVR) shows that small, specialized language models (SLMs) can exhibit structured reasoning without relying on large monolithic LLMs. We introduce soft hidden-state collaboration, where multiple heterogeneous frozen SLM experts are integrated through their internal representations via a trainable attention interface. Experiments on Reasoning Gym and GSM8K show that this latent integration is competitive with strong single-model RLVR baselines. Ablations further reveal a dual mechanism of expert utilization: for simpler arithmetic domains, performance gains can largely be explained by static expert preferences, whereas more challenging settings induce increasingly concentrated and structured expert attention over training, indicating emergent specialization in how the router connects to relevant experts. Overall, hidden-state collaboration provides a compact mechanism for leveraging frozen experts, while offering an observational window into expert utilization patterns and their evolution under RLVR.

</details>


### [19] [Generalizing GNNs with Tokenized Mixture of Experts](https://arxiv.org/abs/2602.09258)
*Xiaoguang Guo,Zehong Wang,Jiazheng Li,Shawn Spitzel,Qi Yang,Kaize Ding,Jundong Li,Chuxu Zhang*

Main category: cs.LG

TL;DR: STEM-GNN은 그래프 신경망의 안정성과 일반화 가능성을 개선하는 방법을 제안하며, 다양한 계산 경로를 구성하는 혼합 전문가 인코더와 출력 증폭을 제한하는 Lipschitz 정규화 헤드를 사용한다.


<details>
  <summary>Details</summary>
Motivation: 배포된 그래프 신경망(GNN)은 배포 시 정적이며, 깨끗한 데이터에 맞춰져야 하고, 분포 변화에 대해 일반화하며, 소음에 안정적으로 남아 있어야 합니다.

Method: STEM-GNN은 혼합 전문가 인코더, 벡터 양자화된 토큰 인터페이스, Lipschitz 정규화된 헤드를 사용하여 다양한 계산 경로를 제공하는 사전 훈련 후 미세 조정 프레임워크입니다.

Result: STEM-GNN은 아홉 가지 노드, 링크 및 그래프 벤치마크에서 안정성과 일반화 간의 균형을 개선하면서 깨끗한 그래프에서 경쟁력을 유지한다.

Conclusion: STEM-GNN은 안정성과 일반화 가능성을 개선하는 강력한 방법을 제공하며, 다양한 변동 및 왜곡에 대해 더 강한 견고성을 나타낸다.

Abstract: Deployed graph neural networks (GNNs) are frozen at deployment yet must fit clean data, generalize under distribution shifts, and remain stable to perturbations. We show that static inference induces a fundamental tradeoff: improving stability requires reducing reliance on shift-sensitive features, leaving an irreducible worst-case generalization floor. Instance-conditional routing can break this ceiling, but is fragile because shifts can mislead routing and perturbations can make routing fluctuate. We capture these effects via two decompositions separating coverage vs selection, and base sensitivity vs fluctuation amplification. Based on these insights, we propose STEM-GNN, a pretrain-then-finetune framework with a mixture-of-experts encoder for diverse computation paths, a vector-quantized token interface to stabilize encoder-to-head signals, and a Lipschitz-regularized head to bound output amplification. Across nine node, link, and graph benchmarks, STEM-GNN achieves a stronger three-way balance, improving robustness to degree/homophily shifts and to feature/edge corruptions while remaining competitive on clean graphs.

</details>


### [20] [Clarifying Shampoo: Adapting Spectral Descent to Stochasticity and the Parameter Trajectory](https://arxiv.org/abs/2602.09314)
*Runa Eschenhagen,Anna Cai,Tsung-Hsien Lee,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: Shampoo와 Muon은 데이터 효율성이 높지만 이들의 관계와 효율성은 명확하지 않다. 실험을 통해 Shampoo가 Muon보다 높은 토큰 효율성을 보임을 입증했다.


<details>
  <summary>Details</summary>
Motivation: Shampoo와 Muon과 같은 행렬 구조를 활용한 최적화기가 Adam과 Signum 같은 원소-wise 알고리즘보다 데이터 효율적이지만, 이들의 일반적인 관계와 데이터 효율성은 불분명하다.

Method: 언어 모델에 대한 광범위한 실험을 통해 Shampoo의 효율성을 평가하였다.

Result: Shampoo는 Muon보다 높은 토큰 효율성을 달성하였으며, Shampoo의 업데이트는 유사한 Muon 업데이트로 분해될 수 있음을 보여준다.

Conclusion: Shampoo의 이점은 가중치 행렬에 적용된 것에만 특화되어 있으며, 이는 모양에 무관한 해석에 도전한다.

Abstract: Optimizers leveraging the matrix structure in neural networks, such as Shampoo and Muon, are more data-efficient than element-wise algorithms like Adam and Signum. While in specific settings, Shampoo and Muon reduce to spectral descent analogous to how Adam and Signum reduce to sign descent, their general relationship and relative data efficiency under controlled settings remain unclear. Through extensive experiments on language models, we demonstrate that Shampoo achieves higher token efficiency than Muon, mirroring Adam's advantage over Signum. We show that Shampoo's update applied to weight matrices can be decomposed into an adapted Muon update. Consistent with this, Shampoo's benefits can be exclusively attributed to its application to weight matrices, challenging interpretations agnostic to parameter shapes. This admits a new perspective that also avoids shortcomings of related interpretations based on variance adaptation and whitening: rather than enforcing semi-orthogonality as in spectral descent, Shampoo's updates are time-averaged semi-orthogonal in expectation.

</details>


### [21] [Large Language Models for Designing Participatory Budgeting Rules](https://arxiv.org/abs/2602.09349)
*Nguyen Thach,Xingchen Sha,Hau Chan*

Main category: cs.LG

TL;DR: 이 논문은 참여예산(PB) 규칙 설계를 자동화하기 위해 대규모 언어 모델을 통합한 새로운 프레임워크인 LLMRule을 소개하고, 이를 통해 PB 규칙이 기존 수작업 규칙보다 효용성을 높이며 공정성도 유지한다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 참여예산(PB)은 공공 프로젝트의 자금을 주민의 선호에 따라 결정하는 민주적 패러다임으로, 많은 도시에서 채택되고 있다. 하지만 효용성과 공정성을 동시에 최적화하는 PB 규칙 설계는 도전적이었다.

Method: 이 논문에서는 LLM을 진화적 탐색 절차에 통합하여 PB 규칙 설계를 자동화하는 새로운 프레임워크인 LLMRule을 제안한다.

Result: 600개 이상의 실제 PB 사례를 평가한 실험 결과, LLM이 생성한 규칙이 기존 수작업 규칙보다 전체 효용성에서 일반적으로 우수하면서도 유사한 수준의 공정성을 유지함을 보여준다.

Conclusion: LLMRule 프레임워크는 PB 규칙 설계의 한계를 극복하고, 효용성을 높이며 공정성을 유지하는 데 기여할 수 있다.

Abstract: Participatory budgeting (PB) is a democratic paradigm for deciding the funding of public projects given the residents' preferences, which has been adopted in numerous cities across the world. The main focus of PB is designing rules, functions that return feasible budget allocations for a set of projects subject to some budget constraint. Designing PB rules that optimize both utility and fairness objectives based on agent preferences had been challenging due to the extensive domain knowledge required and the proven trade-off between the two notions. Recently, large language models (LLMs) have been increasingly employed for automated algorithmic design. Given the resemblance of PB rules to algorithms for classical knapsack problems, in this paper, we introduce a novel framework, named LLMRule, that addresses the limitations of existing works by incorporating LLMs into an evolutionary search procedure for automating the design of PB rules. Our experimental results, evaluated on more than 600 real-world PB instances obtained from the U.S., Canada, Poland, and the Netherlands with different representations of agent preferences, demonstrate that the LLM-generated rules generally outperform existing handcrafted rules in terms of overall utility while still maintaining a similar degree of fairness.

</details>


### [22] [Latent Poincaré Shaping for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.09375)
*Hanchen Xia,Baoyou Chen,Zelin Zang,Yutang Ge,Guojiang Zhao,Siyu Zhu*

Main category: cs.LG

TL;DR: LaPha는 Poincaré 잠복 공간에서 AlphaZero 유사 LLM 에이전트를 훈련하기 위한 방법으로, 검색 과정을 시각화하여 성능을 개선한다.


<details>
  <summary>Details</summary>
Motivation: AlphaZero와 유사한 LLM 에이전트를 효율적으로 훈련하고, Poincaré 공간의 특성을 활용하여 검색 능력을 향상시키기 위해.

Method: Poincaré 구의 경계로부터 시작하여 뿌리를 내리고 성장하는 트리 구조를 통한 검색 과정을 시각화하고, 하이퍼볼릭 기하학적 거리를 사용한 검증된 정확성을 기반으로 노드 포텐셜을 정의하며, 가치 헤드를 추가하여 거의 추가 비용 없이 테스트 시 스케일링을 가능하게 하는 방법.

Result: MATH-500에서 LaPha는 Qwen2.5-Math-1.5B의 정확도를 66.0%에서 88.2%로 개선하였고, 가치 헤드로 안내된 검색을 통해 LaPha-1.5B는 AIME'24에서 56.7%의 정확도를 달성하였으며, LaPha-7B는 AIME'24에서 60.0%, AIME'25에서 53.3%에 도달하였다.

Conclusion: LaPha는 Poincaré 잠복 공간을 활용하여 LLM의 검색 능력을 향상시키는 효과적인 방법으로, 다양한 결과에서 성공을 거두었다.

Abstract: We propose LaPha, a method for training AlphaZero-like LLM agents in a Poincaré latent space. Under LaPha, the search process can be visualized as a tree rooted at the prompt and growing outward from the origin toward the boundary of the Poincaré ball, where negative curvature provides exponentially increasing capacity with radius. Using hyperbolic geodesic distance to rule-verified correctness, we define a node potential and assign dense process rewards by potential differences. We further attach a lightweight value head on the same shared latent space, enabling self-guided test-time scaling with almost no additional overhead. On MATH-500, LaPha improves Qwen2.5-Math-1.5B from 66.0% to 88.2%. With value-head-guided search, LaPha-1.5B reaches 56.7% accuracy on AIME'24, and LaPha-7B further achieves 60.0% on AIME'24 and 53.3% on AIME'25.

</details>


### [23] [Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning](https://arxiv.org/abs/2602.09396)
*Nilaksh,Antoine Clavaud,Mathieu Reymond,François Rivest,Sarath Chandar*

Main category: cs.LG

TL;DR: 이 논문은 스트리밍 강화 학습에서 데이터의 효율성을 극대화하기 위해 자기 예측 표현(SPR)을 확대 적용하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 스트리밍 강화 학습에서 에이전트는 즉각적인 업데이트 후 데이터를 폐기하여 자원 사용을 최소화하지만, 이로 인해 표본 효율성이 떨어지는 문제가 발생합니다.

Method: 자기 예측 표현(SPR)을 스트리밍 파이프라인에 적용하여 관찰된 모든 프레임의 유용성을 극대화하고, 스트리밍 환경에서 발생하는 샘플 간의 높은 상관관계를 고려한 직교 경량 업데이트를 도입합니다.

Result: 우리의 접근 방식은 Atari, MinAtar, Octax에서 검증되었으며, 기존의 스트리밍 기준 대비 체계적으로 뛰어난 성능을 보였습니다.

Conclusion: 우리의 방법은 재생 버퍼의 부재로 인한 성능 격차를 극복하면서도 CPU 코어 몇 개로도 효율적으로 학습할 수 있도록 하는 더 풍부한 표현을 학습합니다.

Abstract: In streaming Reinforcement Learning (RL), transitions are observed and discarded immediately after a single update. While this minimizes resource usage for on-device applications, it makes agents notoriously sample-inefficient, since value-based losses alone struggle to extract meaningful representations from transient data. We propose extending Self-Predictive Representations (SPR) to the streaming pipeline to maximize the utility of every observed frame. However, due to the highly correlated samples induced by the streaming regime, naively applying this auxiliary loss results in training instabilities. Thus, we introduce orthogonal gradient updates relative to the momentum target and resolve gradient conflicts arising from streaming-specific optimizers. Validated across the Atari, MinAtar, and Octax suites, our approach systematically outperforms existing streaming baselines. Latent-space analysis, including t-SNE visualizations and effective-rank measurements, confirms that our method learns significantly richer representations, bridging the performance gap caused by the absence of a replay buffer, while remaining efficient enough to train on just a few CPU cores.

</details>


### [24] [Diffusion-Guided Pretraining for Brain Graph Foundation Models](https://arxiv.org/abs/2602.09437)
*Xinxu Wei,Rong Zhou,Lifang He,Yu Zhang*

Main category: cs.LG

TL;DR: 본 연구는 뇌 신호의 기초 모델을 위한 그래프 기반의 사전 학습 프레임워크를 제안하며, 기존의 단순한 데이터 증강 방식의 한계를 극복한다.


<details>
  <summary>Details</summary>
Motivation: 뇌 신호에 대한 기초 모델의 관심 증가에 따라, 연결망 데이터에서 전이 가능한 표현을 학습하기 위한 그래프 기반의 사전 학습 접근 방식이 필요하다.

Method: 본 연구에서는 구조 인식을 기반으로 한 드랍 및 마스킹 전략을 안내하는 확산 기반의 통합 사전 학습 프레임워크를 제안한다. 이 과정에서 그래프 임베딩과 마스킹된 노드가 전역적으로 관련된 영역의 정보를 집계할 수 있게 한다.

Result: 25,000명 이상의 피험자와 60,000개의 스캔을 포함한 여러 신경영상 데이터셋에서 일관된 성능 개선을 입증하였다.

Conclusion: 제안된 방법은 뇌 그래프의 의미론적 연결성을 유지하면서도 효과적인 학습 다양성을 확보하고, 전역 구조 정보의 포착을 가능하게 한다.

Abstract: With the growing interest in foundation models for brain signals, graph-based pretraining has emerged as a promising paradigm for learning transferable representations from connectome data. However, existing contrastive and masked autoencoder methods typically rely on naive random dropping or masking for augmentation, which is ill-suited for brain graphs and hypergraphs as it disrupts semantically meaningful connectivity patterns. Moreover, commonly used graph-level readout and reconstruction schemes fail to capture global structural information, limiting the robustness of learned representations. In this work, we propose a unified diffusion-based pretraining framework that addresses both limitations. First, diffusion is designed to guide structure-aware dropping and masking strategies, preserving brain graph semantics while maintaining effective pretraining diversity. Second, diffusion enables topology-aware graph-level readout and node-level global reconstruction by allowing graph embeddings and masked nodes to aggregate information from globally related regions. Extensive experiments across multiple neuroimaging datasets with over 25,000 subjects and 60,000 scans involving various mental disorders and brain atlases demonstrate consistent performance improvements.

</details>


### [25] [Beyond Student: An Asymmetric Network for Neural Network Inheritance](https://arxiv.org/abs/2602.09509)
*Yiyun Zhou,Jingwei Shi,Mingjing Xu,Zhonghua Jiang,Jingyuan Chen*

Main category: cs.LG

TL;DR: InherNet은 지식 전이의 성능을 극대화하면서 경량 네트워크를 구축하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 모델 압축을 통한 지식 전이의 성능 향상.

Method: 교사의 가중치에 대한 비대칭 저랭크 분해를 수행하고 경량이면서도 표현력이 뛰어난 네트워크를 재구성하는 InherNet 제안.

Result: InherNet은 유사한 매개변수 크기의 학생 네트워크보다 더 높은 성능을 달성한다.

Conclusion: 전통적인 증류 기법을 넘어선 효율적인 모델 압축을 위한 유망한 방향을 제시한다.

Abstract: Knowledge Distillation (KD) has emerged as a powerful technique for model compression, enabling lightweight student networks to benefit from the performance of redundant teacher networks. However, the inherent capacity gap often limits the performance of student networks. Inspired by the expressiveness of pretrained teacher networks, a compelling research question arises: is there a type of network that can not only inherit the teacher's structure but also maximize the inheritance of its knowledge? Furthermore, how does the performance of such an inheriting network compare to that of student networks, all benefiting from the same teacher network? To further explore this question, we propose InherNet, a neural network inheritance method that performs asymmetric low-rank decomposition on the teacher's weights and reconstructs a lightweight yet expressive network without significant architectural disruption. By leveraging Singular Value Decomposition (SVD) for initialization to ensure the inheritance of principal knowledge, InherNet effectively balances depth, width, and compression efficiency. Experimental results across unimodal and multimodal tasks demonstrate that InherNet achieves higher performance compared to student networks of similar parameter sizes. Our findings reveal a promising direction for future research in efficient model compression beyond traditional distillation.

</details>


### [26] [Rollout-Training Co-Design for Efficient LLM-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.09578)
*Zhida Jiang,Zhaolong Xing,Jiawei Lu,Yipei Niu,Qingyuan Sang,Liangxu Zhang,Wenquan Dai,Junhua Shu,Jiaxing Wang,Qiangyu Pei,Qiong Chen,Xinyu Liu,Fangming Liu,Ai Han,Zhen Chen,Ke Zhang*

Main category: cs.LG

TL;DR: FlexMARL은 대규모 LLM 기반 다중 에이전트 강화 학습(MARL)을 위해 롤아웃, 훈련 및 이를 조율할 수 있는 최초의 엔드 투 엔드 훈련 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습(MARL) 훈련을 위한 네트워크 인프라가 충분히 탐구되지 않았기 때문이다.

Method: FlexMARL은 롤아웃-훈련 분리 아키텍처 하에서 데이터 흐름을 관리하기 위해 공동 조정자를 도입하며, 마이크로 배치 기반 비동기 파이프라인을 통해 동기화 장벽을 제거하고 강력한 일관성 보장을 제공한다.

Result: 대규모 생산 클러스터에서의 실험 결과 FlexMARL은 기존 프레임워크에 비해 최대 7.3배의 속도 향상과 최대 5.6배의 하드웨어 활용도 향상을 보인다.

Conclusion: FlexMARL은 대규모 MARL 훈련의 시스템적 도전을 해결하며, 이를 통해 높은 효율성을 제공한다.

Abstract: Despite algorithm-level innovations for multi-agent reinforcement learning (MARL), the underlying networked infrastructure for large-scale MARL training remains underexplored. Existing training frameworks primarily optimize for single-agent scenarios and fail to address the unique system-level challenges of MARL, including rollout-training synchronization barriers, rollout load imbalance, and training resource underutilization. To bridge this gap, we propose FlexMARL, the first end-to-end training framework that holistically optimizes rollout, training, and their orchestration for large-scale LLM-based MARL. Specifically, FlexMARL introduces the joint orchestrator to manage data flow under the rollout-training disaggregated architecture. Building upon the experience store, a novel micro-batch driven asynchronous pipeline eliminates the synchronization barriers while providing strong consistency guarantees. Rollout engine adopts a parallel sampling scheme combined with hierarchical load balancing, which adapts to skewed inter/intra-agent request patterns. Training engine achieves on-demand hardware binding through agent-centric resource allocation. The training states of different agents are swapped via unified and location-agnostic communication. Empirical results on a large-scale production cluster demonstrate that FlexMARL achieves up to 7.3x speedup and improves hardware utilization by up to 5.6x compared to existing frameworks.

</details>


### [27] [Model soups need only one ingredient](https://arxiv.org/abs/2602.09689)
*Alireza Abdollahpoorrostam,Nikolaos Dimitriadis,Adam Hazimeh,Pascal Frossard*

Main category: cs.LG

TL;DR: MonoSoup는 단일 체크포인트만으로 강력한 ID-OOD 균형을 달성하는 데이터 및 하이퍼파라미터 없이 간단한 방법입니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 사전 학습 모델을 특정 분포에 맞춰 미세 조정할 때, ID 정확도는 개선되지만 OOD 강건성이 저하되는 문제를 해결하고자 합니다.

Method: MonoSoup는 각 계층의 업데이트에 특잇값 분해(SVD)를 적용하여 작업별 적응을 캡처하는 고에너지 방향과 노이즈를 도입하지만 강건성에 유용한 잔여 신호를 인코딩하는 저에너지 방향으로 분해합니다.

Result: MonoSoup는 계층별 계수를 사용하여 이러한 구성 요소의 가중치를 자동으로 재조정하여 모델의 스펙트럼 및 기하학적 구조를 고려합니다.

Conclusion: 실험 결과, MonoSoup는 다중 체크포인트 방법의 이점을 유지하면서도 계산 오버헤드를 줄일 수 있는 실용적이고 효과적인 대안임이 입증되었습니다.

Abstract: Fine-tuning large pre-trained models on a target distribution often improves in-distribution (ID) accuracy, but at the cost of out-of-distribution (OOD) robustness as representations specialize to the fine-tuning data. Weight-space ensembling methods, such as Model Soups, mitigate this effect by averaging multiple checkpoints, but they are computationally prohibitive, requiring the training and storage of dozens of fine-tuned models. In this paper, we introduce MonoSoup, a simple, data-free, hyperparameter-free, post-hoc method that achieves a strong ID-OOD balance using only a single checkpoint. Our method applies Singular Value Decomposition (SVD) to each layer's update and decomposes it into high-energy directions that capture task-specific adaptation and low-energy directions that introduce noise but may still encode residual signals useful for robustness. MonoSoup then uses entropy-based effective rank to automatically re-weigh these components with layer-wise coefficients that account for the spectral and geometric structure of the model. Experiments on CLIP models fine-tuned on ImageNet and evaluated under natural distribution shifts, as well as on Qwen language models tested on mathematical reasoning and multiple-choice benchmarks, show that this plug-and-play approach is a practical and effective alternative to multi-checkpoint methods, retaining much of their benefits without their computational overhead.

</details>


### [28] [Towards Poisoning Robustness Certification for Natural Language Generation](https://arxiv.org/abs/2602.09757)
*Mihnea Ghitu,Matthew Wicker*

Main category: cs.LG

TL;DR: 이 논문은 보안에 민감한 분야에서 자연어 생성의 신뢰성을 이해하고, 이를 위한 인증된 자연어 생성 프레임워크를 설립하기 위해 두 가지 보안 속성을 공식화하며, 최초의 유효성 인증 알고리즘인 TPA를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 자연어 생성의 신뢰성을 이해하는 것은 보안이 중요한 분야에서 기초 모델을 배포하는 데 필수적입니다.

Method: 우리는 안정성(생성의 어떤 변화에 대한 강건성)과 유효성(타겟형 유해 변화에 대한 강건성)이라는 두 가지 보안 속성을 공식화합니다. 그런 다음, 특정 유해 클래스, 토큰 또는 구문을 유도하는 데 필요한 최소 독성 예산을 계산하여 유효성/타겟형 공격을 인증하는 최초의 알고리즘인 TPA를 도입합니다. TPA를 혼합 정수 선형 프로그래밍(MILP)을 사용하여 다중 턴 생성에 대한 더 엄격한 보장을 제공하도록 확장합니다.

Result: TPA의 효과성을 다양한 설정에서 입증하였으며, 적대자가 데이터셋의 최대 0.5%를 수정할 때 에이전트 도구 호출의 유효성을 인증하고, 선호 기반 정렬에서 8-토큰 안정성 수평선을 인증합니다.

Conclusion: 추론 시간 지연은 여전히 도전 과제로 남아 있지만, 우리의 기여는 보안이 중요한 애플리케이션에서 언어 모델의 인증된 배포를 가능하게 합니다.

Abstract: Understanding the reliability of natural language generation is critical for deploying foundation models in security-sensitive domains. While certified poisoning defenses provide provable robustness bounds for classification tasks, they are fundamentally ill-equipped for autoregressive generation: they cannot handle sequential predictions or the exponentially large output space of language models. To establish a framework for certified natural language generation, we formalize two security properties: stability (robustness to any change in generation) and validity (robustness to targeted, harmful changes in generation). We introduce Targeted Partition Aggregation (TPA), the first algorithm to certify validity/targeted attacks by computing the minimum poisoning budget needed to induce a specific harmful class, token, or phrase. Further, we extend TPA to provide tighter guarantees for multi-turn generations using mixed integer linear programming (MILP). Empirically, we demonstrate TPA's effectiveness across diverse settings including: certifying validity of agent tool-calling when adversaries modify up to 0.5% of the dataset and certifying 8-token stability horizons in preference-based alignment. Though inference-time latency remains an open challenge, our contributions enable certified deployment of language models in security-critical applications.

</details>


### [29] [Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization](https://arxiv.org/abs/2602.09761)
*Matteo Pannacci,Andrea Fanti,Elena Umili,Roberto Capobianco*

Main category: cs.LG

TL;DR: 이 연구는 강화 학습 에이전트가 선형 시간 논리로 표현된 여러 개의 시간 연장 지침을 따르도록 훈련하는 문제를 다룬다. 이전의 다중 작업 연구는 주로 원시 관찰과 공식에 나타나는 기호 간의 매핑 지식에 의존했으나, 우리는 동일한 경험을 통해 다중 작업 정책과 기호 접지 기법을 공동 훈련함으로써 이 비현실적인 가정을 포기한다. 실험 결과, 우리의 방법은 진정한 기호 접지를 사용하는 것과 유사한 성능을 보여주며, 서브 심볼릭 환경에서 최첨단 방법들을 크게 초월하는 성과를 나타낸다.


<details>
  <summary>Details</summary>
Motivation: 서브 심볼릭 환경에서 선형 시간 논리로 표현된 다수의 시간 연장된 지침을 따르기 위한 강화 학습 에이전트를 훈련하는 문제를 해결하고자 함.

Method: 비현실적인 매핑 지식에 의존하지 않고 다중 작업 정책과 기호 접지 기법을 동일한 경험으로 공동 훈련한다. 기호 접지 기법은 원시 관찰과 희소 보상만으로 훈련된다.

Result: 비전 기반 환경에서 실험을 통해 우리의 방법이 진정한 기호 접지를 사용하는 것과 유사한 성능을 내며, 최첨단 방법보다 서브 심볼릭 환경에서 유의미하게 더 나은 성과를 보여줌을 입증하였다.

Conclusion: 제안된 방법은 강화 학습 에이전트의 성능을 개선하는 데 기여하며, 다중 작업 지침을 따르는 데에 있어 기존 방법보다 더 효과적이다.

Abstract: In this work we address the problem of training a Reinforcement Learning agent to follow multiple temporally-extended instructions expressed in Linear Temporal Logic in sub-symbolic environments. Previous multi-task work has mostly relied on knowledge of the mapping between raw observations and symbols appearing in the formulae. We drop this unrealistic assumption by jointly training a multi-task policy and a symbol grounder with the same experience. The symbol grounder is trained only from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised fashion. Experiments on vision-based environments show that our method achieves performance comparable to using the true symbol grounding and significantly outperforms state-of-the-art methods for sub-symbolic environments.

</details>


### [30] [Explainability in Generative Medical Diffusion Models: A Faithfulness-Based Analysis on MRI Synthesis](https://arxiv.org/abs/2602.09781)
*Surjo Dey,Pallabi Saikia*

Main category: cs.LG

TL;DR: 본 연구는 의료 이미징 맥락에서 생성 확산 모델의 설명 가능성을 조사하며, MRI 합성에 중점을 둡니다.


<details>
  <summary>Details</summary>
Motivation: 확산 모델의 내부 의사 결정 과정을 이해하고, 의료 이미지 생성의 신뢰성을 높이기 위하여.

Method: ProtoPNet, Enhanced ProtoPNet, ProtoPool과 같은 프로토타입 기반의 설명 가능성 방법을 통해 생성된 특징과 훈련된 특징 간의 관계를 분석하는 신뢰성 기반 설명 가능성 프레임워크를 제시합니다.

Result: EPPNet이 가장 높은 신뢰성 점수(0.1534)를 달성하여 더 신뢰할 수 있는 통찰력과 설명 가능성을 제공합니다.

Conclusion: 신뢰성 기반 설명을 통해 확산 모델은 더욱 투명하고 신뢰할 수 있게 만들어져, 헬스케어에서 생성 AI의 안전하고 해석 가능한 적용에 기여할 수 있습니다.

Abstract: This study investigates the explainability of generative diffusion models in the context of medical imaging, focusing on Magnetic resonance imaging (MRI) synthesis. Although diffusion models have shown strong performance in generating realistic medical images, their internal decision making process remains largely opaque. We present a faithfulness-based explainability framework that analyzes how prototype-based explainability methods like ProtoPNet (PPNet), Enhanced ProtoPNet (EPPNet), and ProtoPool can link the relationship between generated and training features. Our study focuses on understanding the reasoning behind image formation through denoising trajectory of diffusion model and subsequently prototype explainability with faithfulness analysis. Experimental analysis shows that EPPNet achieves the highest faithfulness (with score 0.1534), offering more reliable insights, and explainability into the generative process. The results highlight that diffusion models can be made more transparent and trustworthy through faithfulness-based explanations, contributing to safer and more interpretable applications of generative AI in healthcare.

</details>


### [31] [Flexible Entropy Control in RLVR with Gradient-Preserving Perspective](https://arxiv.org/abs/2602.09782)
*Kun Chen,Peng Shi,Fanfan Liu,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao*

Main category: cs.LG

TL;DR: RLVR는 LLM의 추론 능력을 향상시키는 중요한 방법으로, 정책 엔트로피 붕괴를 방지하기 위한 동적 엔트로피 제어 전략을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 정책 엔트로피 붕괴 문제를 해결하여 RLVR의 효과를 극대화하기 위해.

Method: Gradient-Preserving Clipping의 관점에서 엔트로피 제어를 재구성하고, 동적 클리핑 임계값을 사용하여 엔트로피를 정밀하게 관리하는 새로운 규제 메커니즘을 도입한다.

Result: 정적 완화 전략에 비해 엔트로피 붕괴를 효과적으로 완화하고 여러 벤치마크에서 우수한 성능을 달성하는 동적 엔트로피 제어 전략을 제안한다.

Conclusion: 제안된 동적 전략이 RLVR의 훈련 과정에서 엔트로피를 안정적으로 관리한다고 결론짓는다.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.

</details>


### [32] [Fully-automated sleep staging: multicenter validation of a generalizable deep neural network for Parkinson's disease and isolated REM sleep behavior disorder](https://arxiv.org/abs/2602.09793)
*Jesper Strøm,Casper Skjærbæk,Natasha Becker Bertelsen,Steffen Torpe Simonsen,Niels Okkels,David Bertram,Sinah Röttgen,Konstantin Kufer,Kaare B. Mikkelsen,Marit Otto,Poul Jørgen Jennum,Per Borghammer,Michael Sommerauer,Preben Kidmose*

Main category: cs.LG

TL;DR: 이 연구는 파킨슨병 및 격리된 REM 수면 행동 장애(iRBD)에 대한 깊은 신경망 기반의 수면 단계 분류 모델을 개발하여 전반적인 수면 평가를 향상시키는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 격리된 REM 수면 행동 장애는 파킨슨병의 주요 전조 증상이며, 수면 단계 분류는 진단의 핵심이다.

Method: U-Sleep라는 심층 신경망 모델을 개발하였고, 대규모의 비신경퇴행성 데이터셋을 기반으로 사전 훈련되어 두 개의 연구 센터에서 데이터로 미세 조정되었다.

Result: 모델의 세부 조정 후 PACE/CBC에 적용했을 때   = 0.74의 결과를 달성했다. DCSM에서 평균 및 중앙값  이 증가했다.

Conclusion: 신뢰 임계값을 적용하여 REM 수면 단계를 최적화하였고, 올바른 REM 수면 에포크 식별 비율을 크게 향상시켰다.

Abstract: Isolated REM sleep behavior disorder (iRBD) is a key prodromal marker of Parkinson's disease (PD), and video-polysomnography (vPSG) remains the diagnostic gold standard. However, manual sleep staging is particularly challenging in neurodegenerative diseases due to EEG abnormalities and fragmented sleep, making PSG assessments a bottleneck for deploying new RBD screening technologies at scale. We adapted U-Sleep, a deep neural network, for generalizable sleep staging in PD and iRBD. A pretrained U-Sleep model, based on a large publicly available, multisite non-neurodegenerative dataset (PUB; 19,236 PSGs across 12 sites), was fine-tuned on research datasets from two centers (Lundbeck Foundation Parkinson's Disease Research Center (PACE) and the Cologne-Bonn Cohort (CBC); 112 PD, 138 iRBD, 89 age-matched controls. The resulting model was evaluated on an independent dataset from the Danish Center for Sleep Medicine (DCSM; 81 PD, 36 iRBD, 87 sleep-clinic controls). A subset of PSGs with low agreement between the human rater and the model (\k{appa} < 0.6) was re-scored by a second blinded human rater to identify sources of disagreement. Finally, we applied confidence-based thresholds to optimize REM sleep staging. The pretrained model achieved mean \k{appa} = 0.81 in PUB, but \k{appa} = 0.66 when applied directly to PACE/CBC. By fine-tuning the model, we developed a generalized model with \k{appa} = 0.74 on PACE/CBC (p < 0.001 vs. the pretrained model). In DCSM, mean and median \k{appa} increased from 0.60 to 0.64 (p < 0.001) and 0.64 to 0.69 (p < 0.001), respectively. In the interrater study, PSGs with low agreement between the model and the initial scorer showed similarly low agreement between human scorers. Applying a confidence threshold increased the proportion of correctly identified REM sleep epochs from 85% to 95.5%, while preserving sufficient (> 5 min) REM sleep for 95% of subjects.

</details>


### [33] [A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer](https://arxiv.org/abs/2602.09810)
*Azka Nasir,Fatima Dossa,Muhammad Ahmed Atif,Mohammad Ahmed Atif*

Main category: cs.LG

TL;DR: 이 연구에서는 DDQN과 Dueling DQN 간의 아키텍처 차이가 환경 간 전이 행동에 미치는 영향을 실험적으로 조사한다.


<details>
  <summary>Details</summary>
Motivation: 딥 강화 학습에서 전이 학습의 주요 동기는 안정성 향상과 훈련 비용 절감이다.

Method: CartPole을 소스 작업으로, LunarLander을 구조적으로 다른 목표 작업으로 사용하고, 동일한 하이퍼파라미터와 훈련 조건 하에 고정된 레이어별 표현 전이 프로토콜을 평가한다.

Result: DDQN은 부정적인 전이를 피하고 목표 환경에서 기준 성능과 유사한 학습 역학을 유지한다. 반면 Dueling DQN은 유사한 조건에서 부정적 전이를 보인다.

Conclusion: 아키텍처 유도 편향이 가치 기반 딥 강화 학습에서 환경 간 전이에 대한 강인성과 강한 관련성이 있음을 시사한다.

Abstract: Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments. Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects. Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior. Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.

</details>


### [34] [PlugSI: Plug-and-Play Test-Time Graph Adaptation for Spatial Interpolation](https://arxiv.org/abs/2602.09824)
*Xuhang Wu,Zhuoxuan Liang,Wei Li,Xiaohua Jia,Sumi Helal*

Main category: cs.LG

TL;DR: PlugSI는 그래프 기반 공간 보간법의 문제를 해결하기 위한 플러그 앤 플레이 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: IoT와 엣지 컴퓨팅의 발전으로 센서 네트워크의 대규모 배치가 필요하지만, 높은 배치 비용이 확장성을 저해하고 있다.

Method: PlugSI는 테스트 시간에 그래프를 개선하는 두 가지 혁신적 방법인 'Unknown Topology Adapter'와 'Temporal Balance Adapter'를 제안한다.

Result: 광범위한 실험을 통해 PlugSI가 기존 그래프 기반 공간 보간법에 원활하게 통합되고 MAE를 10.81% 줄이는 등 상당한 개선을 제공함을 입증했다.

Conclusion: PlugSI는 테스트 시간의 새로운 그래프 구조에 적응하고, 현재 배치의 노이즈로 인한 변동을 방지하는 데 도움을 준다.

Abstract: With the rapid advancement of IoT and edge computing, sensor networks have become indispensable, driving the need for large-scale sensor deployment. However, the high deployment cost hinders their scalability. To tackle the issues, Spatial Interpolation (SI) introduces virtual sensors to infer readings from observed sensors, leveraging graph structure. However, current graph-based SI methods rely on pre-trained models, lack adaptation to larger and unseen graphs at test-time, and overlook test data utilization. To address these issues, we propose PlugSI, a plug-and-play framework that refines test-time graph through two key innovations. First, we design an Unknown Topology Adapter (UTA) that adapts to the new graph structure of each small-batch at test-time, enhancing the generalization of SI pre-trained models. Second, we introduce a Temporal Balance Adapter (TBA) that maintains a stable historical consensus to guide UTA adaptation and prevent drifting caused by noise in the current batch. Empirically, extensive experiments demonstrate PlugSI can be seamlessly integrated into existing graph-based SI methods and provide significant improvement (e.g., a 10.81% reduction in MAE).

</details>


### [35] [CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization](https://arxiv.org/abs/2602.09851)
*Beicheng Xu,Keyao Ding,Wei Liu,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: CoFEH는 LLM 기반의 특성 공학과 베이지안 하이퍼파라미터 최적화를 결합한 협업 프레임워크로, 공동 최적화를 통해 우수한 AutoML 성능을 달성한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 기법에서 특성 공학(FE)은 병목현상으로 작용하며, 고정된 탐색 공간 내에서 블랙박스 검색처럼 취급된다.

Method: CoFEH는 LLM 구동 FE 최적화기와 베이지안 최적화 모듈을 결합해 유연한 FE 파이프라인을 탐색하고, 동적 최적화 선택기를 통해 FE 및 HPO 단계를 상호 작용적으로 스케줄링한다.

Result: CoFEH는 전통적인 방법 및 LLM 기반 FE 방법들을 능가하며, 공동 최적화 하에서 우수한 성능을 발휘한다.

Conclusion: CoFEH의 상호 조건화 메커니즘은 LLM과 BO 간의 컨텍스트를 공유하게 하여, 상호 정보에 기반한 의사결정을 가능하게 한다.

Abstract: Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy "FE-then-HPO" workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.

</details>


### [36] [Empirical Stability Analysis of Kolmogorov-Arnold Networks in Hard-Constrained Recurrent Physics-Informed Discovery](https://arxiv.org/abs/2602.09988)
*Enzo Nicolas Spotorno,Josafat Leal Filho,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 본 연구에서는 Kolmogorov-Arnold 네트워크(KAN)를 하드 제약이 있는 회귀물리정보 아키텍처(HRPINN)에 통합하여 진동 시스템의 학습된 잔여 다양체의 충실도를 평가합니다.


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold 표현 정리 및 초기 그레이 박스 결과에 영감을 받아, KAN이 MLP보다 효율적으로 알려지지 않은 항을 회복할 수 있을 것이라고 가설을 세웠습니다.

Method: 구성 감도, 매개변수 스케일, 훈련 패러다임에 대한 초기 민감도 분석을 통해 연구를 진행했습니다.

Result: 작은 KAN은 일변량 다항식 잔여(Duffing)에서 경쟁력을 보였지만, 하이퍼파라미터 취약성과 깊은 구성의 불안정성, 곱셈 항(Van der Pol)에서 일관된 실패를 보였으며, 일반적으로 표준 MLP에 의해 압도당했습니다.

Conclusion: 이러한 경험적 도전은 상태 결합을 위한 원래 KAN 표현의 가법적 유도 편향의 한계를 강조하며, 미래의 하이브리드 모델링을 위한 유도 편향 한계에 대한 초기 경험적 증거를 제공합니다.

Abstract: We investigate the integration of Kolmogorov-Arnold Networks (KANs) into hard-constrained recurrent physics-informed architectures (HRPINN) to evaluate the fidelity of learned residual manifolds in oscillatory systems. Motivated by the Kolmogorov-Arnold representation theorem and preliminary gray-box results, we hypothesized that KANs would enable efficient recovery of unknown terms compared to MLPs. Through initial sensitivity analysis on configuration sensitivity, parameter scale, and training paradigm, we found that while small KANs are competitive on univariate polynomial residuals (Duffing), they exhibit severe hyperparameter fragility, instability in deeper configurations, and consistent failure on multiplicative terms (Van der Pol), generally outperformed by standard MLPs. These empirical challenges highlight limitations of the additive inductive bias in the original KAN formulation for state coupling and provide preliminary empirical evidence of inductive bias limitations for future hybrid modeling.

</details>


### [37] [ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning](https://arxiv.org/abs/2602.10019)
*Qingnan Ren,Shiting Huang,Zhen Fang,Zehui Chen,Lin Chen,Lijun Li,Feng Zhao*

Main category: cs.LG

TL;DR: 본 연구에서는 ADORA라는 새로운 정책 최적화 프레임워크를 소개합니다. 이는 동적인 샘플 유틸리티를 고려하여 학습 데이터를 적절히 분류하고, 더 많은 정보를 제공하는 경험으로부터 학습 우선권을 두어 효율적인 정책 업데이트를 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 정책 경량화 방법들이 동적 샘플 유틸리티를 무시하여 비효율적인 신용 할당을 초래하는 문제를 해결하고자 함.

Method: ADORA는 온라인 모델 롤아웃 중 샘플의 동적 유틸리티에 따라 훈련 데이터를 유리한 샘플과 불리한 샘플로 분류하여 이점 함수의 가중치를 동적으로 조정하는 전략을 채택함.

Result: 다양한 모델 가족과 데이터 규모에 걸쳐 ADORA의 평가 결과는 이 프레임워크가 견고하고 효율적임을 보여주며, 기하학적 및 수학적 작업에서 장기적인 추론을 크게 향상시킴.

Conclusion: ADORA는 민감한 하이퍼파라미터 조정 없이도 일관되게 중요한 성능 향상을 달성할 수 있음.

Abstract: Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \textbf{ADORA} (\textbf{A}dvantage \textbf{D}ynamics via \textbf{O}nline \textbf{R}ollout \textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function's weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.

</details>


### [38] [Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning](https://arxiv.org/abs/2602.10044)
*Akshay Mete,Shahid Aamir Sheikh,Tzu-Hsiang Lin,Dileep Kalathil,P. R. Kumar*

Main category: cs.LG

TL;DR: 효율적인 탐색은 강화 학습에서 특히 희박 보상 환경에서 중요한 도전 과제입니다. 본 논문에서는 고전적인 보상 편향 최대 우도 추정(RBMLE)을 딥 RL에 도입하여 낙관적 탐색을 위한 원칙적이고 확장 가능한 프레임워크인 Optimistic World Models(OWMs)를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 희박 보상 환경에서의 효율적인 탐색 문제 해결

Method: OWMs는 낙관적 동적 손실을 활용하여 모델 학습에 낙관성을 직접 통합하는 탐색 방법을 제안합니다.

Result: Optimistic DreamerV3와 Optimistic STORM 아키텍처 내에서 OOMs를 구현하여 샘플 효율성과 누적 수익이 기존 모델에 비해 크게 향상되었습니다.

Conclusion: OWMs는 기존 세계 모델 프레임워크와 쉽게 통합되며, 기존 훈련 절차에 최소한의 수정을 요구합니다.

Abstract: Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.

</details>


### [39] [Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders](https://arxiv.org/abs/2602.10099)
*Amandeep Kumar,Vishal M. Patel*

Main category: cs.LG

TL;DR: 이 논문은 기하학적 간섭이 표준 확산 트랜스포머가 표현 인코더의 고 밀도 합성을 직접 수렴하지 못하는 근본적인 원인임을 밝히고, 이를 해결하기 위해 리만 유동 일치를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 표현 인코더를 활용한 생성 모델링은 효율적이고 고충실도의 합성을 위한 경로를 제공하지만, 기존의 확산 트랜스포머는 이러한 표현에 직접적으로 수렴하지 못한다.

Method: 기하학적 간섭을 해결하기 위해 기존의 프로세스를 다양체 기하선으로 제한하고 곡률로 유발된 오류 전파를 수정하는 리만 유동 일치(RJF)를 제안한다.

Result: 우리의 방법인 RJF는 표준 DiT-B 아키텍처(131M 매개변수)가 효과적으로 수렴하게 하며, FID 3.37을 달성하여 이전 방법들이 수렴하지 못했던 영역에서 성과를 낸다.

Conclusion: RJF를 사용하여 표준 확산 트랜스포머 아키텍처가 폭 확장을 하지 않고도 수렴할 수 있음이 입증되었다.

Abstract: Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [40] [MUZZLE: Adaptive Agentic Red-Teaming of Web Agents Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2602.09222)
*Georgios Syros,Evan Rose,Brian Grinstead,Christoph Kerschbaumer,William Robertson,Cristina Nita-Rotaru,Alina Oprea*

Main category: cs.CR

TL;DR: MUZZLE은 웹 에이전트의 간접 프롬프트 주입 공격에 대한 보안을 평가하기 위한 자동화된 프레임워크로, 다양한 웹 애플리케이션에서의 공격 발견 능력을 갖추고 있다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 온라인 작업을 자동화하기 위해 웹 에이전트를 사용하는 데 있어, 이러한 에이전트가 취약점에 노출되어 사용자 의도를 위반할 수 있는 간접 프롬프트 주입 공격 문제를 해결하기 위해.

Method: MUZZLE은 에이전트의 경로를 활용하여 높은 주목도를 가진 주입 표면을 자동으로 식별하고, 기밀성, 무결성 및 가용성을 위반하는 맥락 인지 악성 명령어를 적응적으로 생성한다.

Result: MUZZLE은 4개의 웹 애플리케이션에서 10개의 적대적 목표를 가진 37개의 새로운 공격을 효과적으로 발견했다.

Conclusion: MUZZLE은 웹 에이전트의 보안을 자동으로 그리고 적응적으로 평가할 수 있는 능력을 보여준다.

Abstract: Large language model (LLM) based web agents are increasingly deployed to automate complex online tasks by directly interacting with web sites and performing actions on users' behalf. While these agents offer powerful capabilities, their design exposes them to indirect prompt injection attacks embedded in untrusted web content, enabling adversaries to hijack agent behavior and violate user intent. Despite growing awareness of this threat, existing evaluations rely on fixed attack templates, manually selected injection surfaces, or narrowly scoped scenarios, limiting their ability to capture realistic, adaptive attacks encountered in practice. We present MUZZLE, an automated agentic framework for evaluating the security of web agents against indirect prompt injection attacks. MUZZLE utilizes the agent's trajectories to automatically identify high-salience injection surfaces, and adaptively generate context-aware malicious instructions that target violations of confidentiality, integrity, and availability. Unlike prior approaches, MUZZLE adapts its attack strategy based on the agent's observed execution trajectory and iteratively refines attacks using feedback from failed executions. We evaluate MUZZLE across diverse web applications, user tasks, and agent configurations, demonstrating its ability to automatically and adaptively assess the security of web agents with minimal human intervention. Our results show that MUZZLE effectively discovers 37 new attacks on 4 web applications with 10 adversarial objectives that violate confidentiality, availability, or privacy properties. MUZZLE also identifies novel attack strategies, including 2 cross-application prompt injection attacks and an agent-tailored phishing scenario.

</details>


### [41] [Autonomous Action Runtime Management(AARM):A System Specification for Securing AI-Driven Actions at Runtime](https://arxiv.org/abs/2602.09433)
*Herman Errico*

Main category: cs.CR

TL;DR: AI 시스템의 보안 경계가 모델 출력에서 도구 실행으로 이동함에 따라, 전통적인 보안 패러다임은 AI 주도의 행동을 보호할 수 없다. 이 논문은 AI 주도 행동을 런타임에서 보호하기 위한 개방형 명세인 AARM을 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 패시브 보조 도구에서 자율 에이전트로 발전함에 따라, 시스템의 보안 경계가 변화하고 있다. AI 주도의 행동이 불가역적으로 실행되고, 머신 속도로 작동하며, 잠재적으로 타협된 오케스트레이션 계층에서 발생하는 경우 전통적인 보안 방법으로는 이를 보호할 수 없다.

Method: AARM은 행동 실행 전에 프로그램을 가로막고, 세션 컨텍스트를 축적하며, 정책 및 의도 정렬에 대해 평가하고, 권한 결정을 시행하며, 포렌식 재구성을 위한 위변조 증거 영수증을 기록하는 런타임 보안 시스템을 정의한다.

Result: 이 논문은 프롬프트 주입, 혼란스러운 대리인 공격, 데이터 유출 및 의도 변화를 다루는 위협 모델을 정형화하며, 금지된 행동, 맥락 의존의 거부 행동 및 맥락 의존의 허용 행동을 분류하는 프레임워크를 소개한다.

Conclusion: AARM은 모델에 무관하며, 프레임워크에 무관하고, 공급업체에 중립적인 접근 방식을 취하며, 행동 실행을 안정적인 보안 경계로 간주한다. 이 명세는 산업 전반의 요구 사항을 설정하여 독점적 단편화가 상호 운용성을 차단하기 전에 이를 방지하는 것을 목표로 한다.

Abstract: As artificial intelligence systems evolve from passive assistants into autonomous agents capable of executing consequential actions, the security boundary shifts from model outputs to tool execution. Traditional security paradigms - log aggregation, perimeter defense, and post-hoc forensics - cannot protect systems where AI-driven actions are irreversible, execute at machine speed, and originate from potentially compromised orchestration layers. This paper introduces Autonomous Action Runtime Management (AARM), an open specification for securing AI-driven actions at runtime. AARM defines a runtime security system that intercepts actions before execution, accumulates session context, evaluates against policy and intent alignment, enforces authorization decisions, and records tamper-evident receipts for forensic reconstruction. We formalize a threat model addressing prompt injection, confused deputy attacks, data exfiltration, and intent drift. We introduce an action classification framework distinguishing forbidden, context-dependent deny, and context-dependent allow actions. We propose four implementation architectures - protocol gateway, SDK instrumentation, kernel eBPF, and vendor integration - with distinct trust properties, and specify minimum conformance requirements for AARM-compliant systems. AARM is model-agnostic, framework-agnostic, and vendor-neutral, treating action execution as the stable security boundary. This specification aims to establish industry-wide requirements before proprietary fragmentation forecloses interoperability.

</details>


### [42] [Trustworthy Agentic AI Requires Deterministic Architectural Boundaries](https://arxiv.org/abs/2602.09947)
*Manish Bhattarai,Minh Vu*

Main category: cs.CR

TL;DR: 현재의 주체적 AI 아키텍처는 고위험 과학 워크플로우의 보안 및 인식론적 요구 사항과 근본적으로 호환되지 않는다. 따라서 AI 지원 과학을 신뢰할 수 있게 하기 위해서는 확률적 학습 행동이 아니라 결정론적인 아키텍처 강제가 필요하다는 주장을 한다.


<details>
  <summary>Details</summary>
Motivation: 고위험 과학 워크플로우의 보안과 인식론적 요구 사항이 충족되지 않고 있으며, 이는 현재의 AI 아키텍처 때문이라는 문제를 지적한다.

Method: 트리니티 방어 아키텍처를 도입하여, 세 가지 메커니즘(유한 행동 계산을 통한 행동 거버넌스, 필수 접근 레이블을 통한 정보 흐름 통제, 인식과 실행의 권한 분리)을 통해 보안을 강화한다.

Result: 검증할 수 없는 출처와 결정론적 매개가 없을 경우, '치명적인 삼중주'(신뢰할 수 없는 입력, 특권 데이터 접근, 외부 행동 능력)가 인증 보안을 악용 탐지 문제로 전환시킬 수 있음을 보여준다.

Conclusion: AI가 과학적 분야에서 안전하게 배포되기 위해서는 아키텍처적 중재가 필요하다고 강조하며, 정렬만으로는 인증 보안을 보장할 수 없음을 명확히 한다.

Abstract: Current agentic AI architectures are fundamentally incompatible with the security and epistemological requirements of high-stakes scientific workflows. The problem is not inadequate alignment or insufficient guardrails, it is architectural: autoregressive language models process all tokens uniformly, making deterministic command--data separation unattainable through training alone. We argue that deterministic, architectural enforcement, not probabilistic learned behavior, is a necessary condition for trustworthy AI-assisted science. We introduce the Trinity Defense Architecture, which enforces security through three mechanisms: action governance via a finite action calculus with reference-monitor enforcement, information-flow control via mandatory access labels preventing cross-scope leakage, and privilege separation isolating perception from execution. We show that without unforgeable provenance and deterministic mediation, the ``Lethal Trifecta'' (untrusted inputs, privileged data access, external action capability) turns authorization security into an exploit-discovery problem: training-based defenses may reduce empirical attack rates but cannot provide deterministic guarantees. The ML community must recognize that alignment is insufficient for authorization security, and that architectural mediation is required before agentic AI can be safely deployed in consequential scientific domains.

</details>
