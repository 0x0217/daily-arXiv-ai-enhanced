<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 157]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [nodeWSNsec: A hybrid metaheuristic approach for reliable security and node deployment in WSNs](https://arxiv.org/abs/2508.16619)
*Rahul Mishra,Sudhanshu Kumar Jha,Naresh Kshetri,Bishnu Bhusal,Mir Mehedi Rahman,Md Masud Rana,Aimina Ali Eli,Khaled Aminul Islam,Bishwo Prakash Pokharel*

Main category: cs.CR

TL;DR: 유선 센서 네트워크에서 효율적이고 신뢰할 수 있는 노드 배치가 중요하며, 본 논문은 유전 알고리즘과 입자 군집 최적화를 결합한 하이브리드 메타 휴리스틱 접근 방식을 제안하여 에너지 효율적이고 신뢰성 있는 노드 배치를 해결하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 유선 센서 네트워크에서의 노드 배치 최적화는 지역 커버리지, 노드 간의 연결성 및 에너지 효율성을 극대화하는 데 필수적이다.

Method: 유전 알고리즘(GA)과 입자 군집 최적화(PSO)를 결합한 하이브리드 메타 휴리스틱 접근 방식을 제안한다.

Result: 제안된 방법은 GA 및 PSO 단독과 비교하여 센서 노드를 15%에서 25% 더 적게 필요로 하며 95% 이상의 지역 커버리지를 유지한다.

Conclusion: 하이브리드 메타 휴리스틱이 WSN 성능 개선에 효과적임을 강조하며, 환경 모니터링, 스마트 시티, 스마트 농업, 재난 대응 및 IIoT와 같은 실제 응용 분야에 대한 유망한 접근 방식을 제공한다.

Abstract: Efficient and reliable node deployment in Wireless Sensor Networks is crucial
for optimizing coverage of the area, connectivity among nodes, and energy
efficiency. This paper proposes a hybrid meta heuristic approach combining a
Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) to address the
challenges of energy efficient and reliable node deployment. The GA PSO hybrid
leverages GAs strong exploration capabilities and PSOs rapid convergence,
achieving an optimum stability between coverage and energy consumption. The
performance of the proposed approach is evaluated against GA and PSO alone and
the innovatory meta heuristic based Competitive Multi Objective Marine
Predators Algorithm (CMOMPA) across varying sensing ranges. Simulation results
demonstrate that GA PSO requires 15% to 25% fewer sensor nodes and maintains
95% or more area coverage while maintaining the connectivity in comparison to
standalone GA or PSO algorithm. The proposed algorithm also dominates CMOMPA
when compared for long sensing and communication range in terms of higher
coverage, improved connectivity, and reduced deployment time while requiring
fewer sensor nodes. This study also explores key trade offs in WSN deployment
and highlights future research directions, including heterogeneous node
deployment, mobile WSNs, and enhanced multi objective optimization techniques.
The findings underscore the effectiveness of hybrid meta heuristics in
improving WSN performance, offering a promising approach for real world
applications such as environmental monitoring, smart cities, smart agriculture,
disaster response, and IIoT.

</details>


### [2] [Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection](https://arxiv.org/abs/2508.16625)
*Rijha Safdar,Danyail Mateen,Syed Taha Ali,M. Umer Ashfaq,Wajahat Hussain*

Main category: cs.CR

TL;DR: AI 기반 소프트웨어 취약점 탐지 시스템의 성능은 종종 알려지지 않은 코드베이스에 대한 일반화 부족으로 제한된다. 데이터 품질과 모델 아키텍처가 취약점 탐지 시스템의 일반화에 미치는 영향을 조사하였으며, 데이터셋 다양성과 품질의 개선이 탐지 성능을 상당히 향상시킨다는 것을 입증했다. 인코더 기반 모델이 정확성과 일반화 측면에서 성능이 우수하다. 결과적으로 우리 모델은 BigVul 데이터셋에서 6.8%의 재현율 개선을 이루었으며, 보지 못한 프로젝트에서도 성능을 보였다.


<details>
  <summary>Details</summary>
Motivation: AI 기반 소프트웨어 취약점 탐지 시스템의 성능을 향상시키기 위해 데이터 품질 및 모델 아키텍처가 미치는 영향을 조사하는 것이 필요하다.

Method: 다양한 C/C++ 소프트웨어 프로젝트에서 훈련되지 않은 데이터에 대한 일반화 능력을 평가하기 위한 일련의 실험을 수행했다. 다양한 인코더 및 디코더 모델을 비교하였다.

Result: 데이터셋의 다양성과 품질을 개선하여 탐지 성능을 크게 향상시키는 결과를 얻었으며, 인코더 기반 모델이 정확도와 일반화 측면에서 성과가 우수함을 발견하였다.

Conclusion: 결과는 데이터 품질과 모델 선택의 중요성을 강조하며 향후 프로젝트 간 효과성이 높은 시스템 개발을 위한 방향을 제시한다.

Abstract: The performance of AI-based software vulnerability detection systems is often
limited by their poor generalization to unknown codebases. In this research, we
explore the impact of data quality and model architecture on the
generalizability of vulnerability detection systems. By generalization we mean
ability of high vulnerability detection performance across different C/C++
software projects not seen during training. Through a series of experiments, we
demonstrate that improvements in dataset diversity and quality substantially
enhance detection performance. Additionally, we compare multiple encoder-only
and decoder-only models, finding that encoder based models outperform in terms
of accuracy and generalization. Our model achieves 6.8% improvement in recall
on the benchmark BigVul[1] dataset, also outperforming on unseen projects,
hence showing enhanced generalizability. These results highlight the role of
data quality and model selection in the development of robust vulnerability
detection systems. Our findings suggest a direction for future systems having
high cross-project effectiveness.

</details>


### [3] [Passive Hack-Back Strategies for Cyber Attribution: Covert Vectors in Denied Environment](https://arxiv.org/abs/2508.16637)
*Abraham Itzhak Weinberg*

Main category: cs.CR

TL;DR: 사이버 공격 할당은 현대 사이버 보안에서 중요한 과제로, 특히 방어자가 공격자의 인프라에 대한 가시성이 제한된 환경에서는 더욱 그렇다. 이 논문은 직접적인 공격 조치를 시작하지 않고도 은밀한 할당과 정보 수집을 가능하게 하는 수동 해킹-백 기법의 전략적 가치를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 사이버 공격 할당의 어려움과 방어자가 제한된 정보 및 규제로 인해 직면하는 도전.

Method: 패시브 해킹-백 기법, 추적 비콘, 허니토큰, 환경 특화 페이로드 및 공급망 기반 트랩을 포함한 다양한 기법과 AI 활용을 통합한 접근 방식.

Result: 기술들이 공격자와의 상호 작용을 추적 가능하게 하여 정보 수집을 지원하며, AI의 역할을 통해 패시브 해킹-백 작전을 향상시킬 수 있다는 점을 보여줌.

Conclusion: 법적, 윤리적, 운영적 제약을 준수하면서 지연되거나 조건부적인 능동적 반응과 결합된 하이브리드 방어 프레임워크의 필요성을 주장.

Abstract: Attributing cyberattacks remains a central challenge in modern cybersecurity,
particularly within denied environments where defenders have limited visibility
into attacker infrastructure and are restricted by legal or operational rules
of engagement. This perspective examines the strategic value of passive
hack-back techniques that enable covert attribution and intelligence collection
without initiating direct offensive actions. Key vectors include tracking
beacons, honeytokens, environment-specific payloads, and supply-chain-based
traps embedded within exfiltrated or leaked assets. These approaches rely on
the assumption that attackers will interact with compromised data in traceable
ways, allowing defenders to gather signals without violating engagement
policies. The paper also explores the role of Artificial Intelligence (AI) in
enhancing passive hack-back operations. Topics include the deployment of
autonomous agents for forensic reconnaissance, the use of Large Language Models
(LLMs) to generate dynamic payloads, and Adversarial Machine Learning (AML)
techniques for evasion and counter-deception. A dedicated section discusses the
implications of quantum technologies in this context, both as future threats to
cryptographic telemetry and as potential tools for stealthy communication and
post-quantum resilience. Finally, the paper advocates for hybrid defensive
frameworks that combine passive attribution with delayed or conditional active
responses, while maintaining compliance with legal, ethical, and operational
constraints.

</details>


### [4] [Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications](https://arxiv.org/abs/2508.16662)
*Alexander Tabalipa*

Main category: cs.CR

TL;DR: 모바일 애플리케이션을 위한 확장된 제로 트러스트 모델을 제안하며, 이를 통해 안전한 모바일 환경을 구축할 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 제로 트러스트 아키텍처(ZTA)는 기업 환경을 보호하는데 널리 채택되고 있지만, 모바일 맥락에서의 적용은 여전히 제한적이다.

Method: 디자인 과학 방법론을 사용하여 런타임 신뢰 시행을 지원하는 6개 기둥 프레임워크를 도입하였다.

Result: 각 기둥은 관련 규제 및 보안 표준에 매핑되어 준수 지원을 돕고, 다양한 조직 맥락에서의 채택을 위한 단계적 실행 로드맵과 성숙도 평가 모델도 개발하였다.

Conclusion: 제안된 모델은 모바일 애플리케이션 보안을 위한 실질적이고 표준에 부합하는 접근 방식을 제공하며, ZTA의 운용 경계를 확장하고 조직에 사기 감소 및 보안 문제 해결을 위한 경로를 제공한다.

Abstract: Zero Trust Architecture (ZTA) has become a widely adopted model for securing
enterprise environments, promoting continuous verification and minimal trust
across systems. However, its application in mobile contexts remains limited,
despite mobile applications now accounting for most global digital interactions
and being increasingly targeted by sophisticated threats. Existing Zero Trust
frameworks developed by organisations such as the National Institute of
Standards and Technology (NIST) and the Cybersecurity and Infrastructure
Security Agency (CISA) primarily focus on enterprise-managed infrastructure,
assuming organisational control over devices, networks, and identities. This
paper addresses a critical gap by proposing an extended Zero Trust model
designed for mobile applications operating in untrusted, user-controlled
environments. Using a design science methodology, the study introduced a
six-pillar framework that supports runtime enforcement of trust through
controls including device integrity, user identity validation, data protection,
secure application programming interface (API) usage, behavioural monitoring,
and live application protection. Each pillar was mapped to relevant regulatory
and security standards to support compliance. A phased implementation roadmap
and maturity assessment model were also developed to guide adoption across
varying organisational contexts. The proposed model offers a practical and
standards-aligned approach to securing mobile applications beyond
pre-deployment controls, aligning real-time enforcement with Zero Trust
principles. This contribution expands the operational boundaries of ZTA and
provides organisations with a deployable path to reduce fraud, enhance
compliance, and address emerging mobile security challenges. Future research
may include empirical validation of the framework and cross-sector application
testing.

</details>


### [5] [Securing Heterogeneous Network (HetNet) Communications for Wildfire Management: Mitigating the Effects of Adversarial and Environmental Threats](https://arxiv.org/abs/2508.16761)
*Nesrine Benchoubane,Olfa Ben Yahia,William Ferguson,Gurkan Gur,Sumit Chakravarty,Gregory Falco,Gunes Karabulut Kurt*

Main category: cs.CR

TL;DR: 본 논문은 산불 관리 및 탐지를 위한 안전하고 탄력적인 통신 시스템의 필요성을 다루며, 이를 위한 새로운 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 안전하고 탄력적인 통신 시스템이 필요하다.

Method: 이 논문은 저지구 궤도 위성 군집과 고고도 플랫폼 지상국 및 저고도 플랫폼을 통합한 이종 네트워크에 회복력을 구축하는 프레임워크를 제시한다.

Result: 사례 연구를 통해 환경적 스트레스 요인이 비밀 용량에 미치는 영향을 정량화하고, 단기 및 장기 통신 링크에서 시그널 드리프를 다룬다.

Conclusion: 이 연구는 네트워크 보호의 중요성을 강조하며, IEEE P3536 표준에 부합하여 회복력을 보장한다.

Abstract: In the face of adverse environmental conditions and cyber threats, robust
communication systems for critical applications such as wildfire management and
detection demand secure and resilient architectures. This paper presents a
novel framework that considers both adversarial factors, building resilience
into a heterogeneous network (HetNet) integrating Low Earth Orbit (LEO)
satellite constellation with High-Altitude Platform Ground Stations (HAPGS) and
Low-Altitude Platforms (LAPS), tailored to support wildfire management
operations. Building upon our previous work on secure-by-component approach for
link segment security, we extend protection to the communication layer by
securing both Radio Frequency (RF)/Free Space Optics (FSO) management and
different links. Through a case study, we quantify how environmental stressors
impact secrecy capacity and expose the system to passive adversaries. Key
findings demonstrate that atmospheric attenuation and beam misalignment can
notably degrade secrecy capacity across both short- and long-range
communication links, while high-altitude eavesdroppers face less signal
degradation, increasing their interception capability. Moreover, increasing
transmit power to counter environmental losses can inadvertently improve
eavesdropper reception, thereby reducing overall link confidentiality. Our work
not only highlights the importance of protecting networks from these dual
threats but also aligns with the IEEE P3536 Standard for Space System
Cybersecurity Design, ensuring resilience and the prevention of mission
failures.

</details>


### [6] [Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models](https://arxiv.org/abs/2508.16765)
*GodsGift Uzor,Hasan Al-Qudah,Ynes Ineza,Abdul Serwadda*

Main category: cs.CR

TL;DR: 본 논문에서는 사용자의 개인 정보 보호를 강화하기 위해 'LLM 게이트키퍼'라는 개념을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 상호작용적 특성으로 인해 사용자들이 개인 정보를 이전보다 더 많이 공유하고 있으며, 기존의 데이터 보호 설정이 충분하지 않음을 지적합니다.

Method: 사용자의 질의에서 민감한 정보를 필터링하는 경량의 로컬 모델인 'LLM 게이트키퍼'를 제안합니다.

Result: 인간 피실험자를 대상으로 한 실험을 통해 이 듀얼 모델 접근 방식이 사용자의 개인 정보를 크게 향상시키면서도 LLM 응답의 품질을 저하시키지 않음을 입증합니다.

Conclusion: 이 접근 방식은 사용자 프라이버시를 크게 향상시키면서 최소한의 추가 비용을 도입합니다.

Abstract: The interactive nature of Large Language Models (LLMs), which closely track
user data and context, has prompted users to share personal and private
information in unprecedented ways. Even when users opt out of allowing their
data to be used for training, these privacy settings offer limited protection
when LLM providers operate in jurisdictions with weak privacy laws, invasive
government surveillance, or poor data security practices. In such cases, the
risk of sensitive information, including Personally Identifiable Information
(PII), being mishandled or exposed remains high. To address this, we propose
the concept of an "LLM gatekeeper", a lightweight, locally run model that
filters out sensitive information from user queries before they are sent to the
potentially untrustworthy, though highly capable, cloud-based LLM. Through
experiments with human subjects, we demonstrate that this dual-model approach
introduces minimal overhead while significantly enhancing user privacy, without
compromising the quality of LLM responses.

</details>


### [7] [A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems](https://arxiv.org/abs/2508.16843)
*Kamel Kamel,Keshav Sood,Hridoy Sankar Dutta,Sunil Aryal*

Main category: cs.CR

TL;DR: 이 설문조사는 음성 인증 시스템과 방지 대책을 겨냥한 현대의 위협 환경에 대한 포괄적인 검토를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 음성 인증 시스템의 채택 증가에 따라 새로운 위협이 발생하고 있기 때문에 이를 이해하고 분석할 필요가 있다.

Method: 우리는 음성 인증 시스템의 발전을 시간 순으로 추적하고, 기술 발전에 따라 취약성이 어떻게 변했는지를 검토하였다.

Result: 각 공격 범주별로 방법론을 요약하고, 일반적으로 사용되는 데이터셋을 강조하며, 성능과 한계를 비교하고 기존 문헌을 체계적으로 정리하였다.

Conclusion: 신흥 위험과 해결해야 할 도전 과제를 강조하여 더 안전하고 탄력적인 음성 인증 시스템의 개발을 지원하는 것을 목표로 하였다.

Abstract: Voice authentication has undergone significant changes from traditional
systems that relied on handcrafted acoustic features to deep learning models
that can extract robust speaker embeddings. This advancement has expanded its
applications across finance, smart devices, law enforcement, and beyond.
However, as adoption has grown, so have the threats. This survey presents a
comprehensive review of the modern threat landscape targeting Voice
Authentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including
data poisoning, adversarial, deepfake, and adversarial spoofing attacks. We
chronologically trace the development of voice authentication and examine how
vulnerabilities have evolved in tandem with technological advancements. For
each category of attack, we summarize methodologies, highlight commonly used
datasets, compare performance and limitations, and organize existing literature
using widely accepted taxonomies. By highlighting emerging risks and open
challenges, this survey aims to support the development of more secure and
resilient voice authentication systems.

</details>


### [8] [Targeted Wearout Attacks in Microprocessor Cores](https://arxiv.org/abs/2508.16868)
*Joshua Mashburn,Johann Knechtel,Florian Klemme,Hussam Amrouch,Ozgur Sinanoglu,Paul V. Gratz*

Main category: cs.CR

TL;DR: 이 논문은 정보에 의해 조작된 설계를 통해 결함 유입 공격을 설명하고, RISC-V CPU에서의 사례 연구를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: CMOS 회로에서의 부정 편향 온도 불안정성의 영향을 이해하고 이를 악용할 수 있는 방법을 찾는 것이 필요합니다.

Method: 공격자는 사용자 권한만을 사용하여 정교하게 설계된 소프트웨어 프로그램을 실행하며, 프로세서 내의 기능 유닛을 고의적으로 저하시킵니다.

Result: RISC-V CPU의 융합된 곱하기-더하기 파이프라인에서 타겟 경로의 마모가 일반적인 작업 부하에서 예상되는 것보다 7배 이상 증가하는 사례 연구를 보여줍니다.

Conclusion: 이 공격 메커니즘은 교란을 유발하고 피해 응용 프로그램에서 데이터 손상을 일으킬 수 있습니다.

Abstract: Negative-Bias Temperature Instability is a dominant aging mechanism in
nanoscale CMOS circuits such as microprocessors. With this aging mechanism, the
rate of device aging is dependent not only on overall operating conditions,
such as heat, but also on user controllable inputs to the transistors. This
dependence on input implies a possible timing fault-injection attack wherein a
targeted path of logic is intentionally degraded through the purposeful,
software-driven actions of an attacker, rendering a targeted bit effectively
stuck.
  In this work, we describe such an attack mechanism, which we dub a
"$\textbf{Targeted Wearout Attack}$", wherein an attacker with sufficient
knowledge of the processor core, executing a carefully crafted software program
with only user privilege, is able to degrade a functional unit within the
processor with the aim of eliciting a particular desired incorrect calculation
in a victim application. Here we give a general methodology for the attack. We
then demonstrate a case study where a targeted path within the fused
multiply-add pipeline in a RISC-V CPU sees a $>7x$ increase in wear over time
than would be experienced under typical workloads. We show that an attacker
could leverage such an attack, leading to targeted and silent data corruption
in a co-running victim application using the same unit.

</details>


### [9] [Investigating red packet fraud in Android applications: Insights from user reviews](https://arxiv.org/abs/2508.16941)
*Yu Cheng,Xiaofang Qi,Yanhui Li*

Main category: cs.CR

TL;DR: 스마트폰의 보급으로 인해 모바일 앱에서 빨간 봉투가 널리 사용되고 있지만, 이에 따른 사기 문제가 심각해지고 있다. 본 연구는 빨간 봉투와 관련된 사용자 리뷰를 분석하여 이 문제를 조사하였다.


<details>
  <summary>Details</summary>
Motivation: 빠르게 성장하는 모바일 앱 시장에서 빨간 봉투와 관련된 사기 문제를 해결하기 위함이다.

Method: ReckDetector라는 자동화된 접근 방식을 제안하고, Google Play 및 세 개의 주요 안드로이드 앱 마켓에서 334개의 빨간 봉투 앱으로부터 360,000개의 사용자 리뷰를 수집하여, BERT 모델을 미세 조정하여 부정적인 리뷰를 식별하였다.

Result: 사용자들이 보고한 빨간 봉투 사기 문제의 여섯 가지 뚜렷한 범주를 요약하였다.

Conclusion: 빨간 봉투 사기가 매우 만연해 있으며, 사용자 경험에 중대한 영향을 미치고 있음을 발견하였다. 또한, 일부 앱 개발자들이 사용자에게 작업을 완료하게끔 유도하여 수익을 극대화하는 데 이를 악용하고 있다.

Abstract: With the popularization of smartphones, red packets have been widely used in
mobile apps. However, the issues of fraud associated with them have also become
increasingly prominent. As reported in user reviews from mobile app markets,
many users have complained about experiencing red packet fraud and being
persistently troubled by fraudulent red packets. To uncover this phenomenon, we
conduct the first investigation into an extensive collection of user reviews on
apps with red packets. In this paper, we first propose a novel automated
approach, ReckDetector, for effectively identifying apps with red packets from
app markets. We then collect over 360,000 real user reviews from 334 apps with
red packets available on Google Play and three popular alternative Android app
markets. We preprocess the user reviews to extract those related to red packets
and fine-tune a pre-trained BERT model to identify negative reviews. Finally,
based on semantic analysis, we have summarized six distinct categories of red
packet fraud issues reported by users. Through our study, we found that red
packet fraud is highly prevalent, significantly impacting user experience and
damaging the reputation of apps. Moreover, red packets have been widely
exploited by unscrupulous app developers as a deceptive incentive mechanism to
entice users into completing their designated tasks, thereby maximizing their
profits.

</details>


### [10] [Towards Principled Analysis and Mitigation of Space Cyber Risks](https://arxiv.org/abs/2508.16991)
*Ekzhin Ear*

Main category: cs.CR

TL;DR: 이 논문은 우주 인프라에 대한 사이버 공격 분석을 위한 혁신적인 프레임워크를 제시하고, 두 가지 산업 도구의 평가 및 기여를 통해 우주 사이버 위험 분석과 완화의 최전선 상태를 특성화한다.


<details>
  <summary>Details</summary>
Motivation: 우주 인프라의 사이버 위험은 거의 이해되지 않지만, 현대 사회의 기반이 된다.

Method: 사이버 공격의 분석을 위한 혁신적인 프레임워크와 결측 데이터 처리 방법론, 세 가지 새로운 메트릭스를 제안하며, 이를 통해 우주 사이버 공격의 실제 사례를 분석했다.

Result: 108건의 실제 우주 사이버 공격 사례를 통해 프레임워크의 유용성을 입증하였고, 두 가지 산업 우주 사이버 위험 분석 및 완화 도구를 평가하였다.

Conclusion: 우주 사이버 공격의 연쇄 효과를 모델링하고 임무 위험 분석 및 강화 알고리즘을 제시함으로써, 우주 사이버 위험을 분석하고 완화하기 위한 새로운 프레임워크의 유용성을 입증하였다.

Abstract: Space infrastructures have become an underpinning of modern society, but
their associated cyber risks are little understood. This Dissertation advances
the state-of-the-art via four contributions. (i) It introduces an innovative
framework for characterizing real-world cyber attacks against space
infrastructures, or space cyber attacks, including a novel methodology for
coping with missing data and three novel metrics. A case study demonstrates the
usefulness of the framework on 108 real-world space cyber attacks. (ii) This
Dissertation characterizes the state-of-the-practice in space cyber risk
analysis and mitigation, namely the Notional Risk Scores (NRS) within the Space
Attack Research and Tactic Analysis (SPARTA) framework. (iii) We propose a set
of desired properties that should be satisfied by any competent space cyber
risk analysis and mitigation tool and applies them to assess two industrial
space cyber risk analysis and mitigation tools. (iv) The study introduces a
novel framework to analyze and mitigate space cyber risks by explicitly
modeling space cyber attack cascading effects and presenting algorithms for
mission risk analysis and mission hardening. We demonstrate the usefulness of
the framework by applying it to analyze and mitigate space cyber risks, with
testbed-based validation.

</details>


### [11] [ZAPS: A Zero-Knowledge Proof Protocol for Secure UAV Authentication with Flight Path Privacy](https://arxiv.org/abs/2508.17043)
*Shayesta Naziri,Xu Wang,Guangsheng Yu,Christy Jie Liang,Wei Ni*

Main category: cs.CR

TL;DR: 무인 항공기(UAV)의 비행 경로 프라이버시를 보장하기 위한 zk-SNARK 기반 인증 및 검증 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: UAV의 군사, 상업 및 물류 적용의 증가로 비행 경로 프라이버시에 대한 우려가 커지고 있다.

Method: zk-SNARK 기반의 비행 경로 인증 및 검증 프레임워크를 제안하며, 민감한 궤적 정보를 공개하지 않고 UAV의 권한을 증명하고 비행 경로를 검증할 수 있도록 한다.

Result: 이 방법은 UAV가 정해진 비행 정책 준수를 검증하는 암호적 증명을 생성할 수 있게 하여 실시간 추적, 신원 노출, 무단 가로채기와 관련된 위험을 완화한다.

Conclusion: 제안된 솔루션은 프라이버시, 보안 및 계산 효율성을 균형 있게 유지하여 자원이 제한된 UAV에도 적합하다.

Abstract: The increasing deployment of Unmanned Aerial Vehicles (UAVs) for military,
commercial, and logistics applications has raised significant concerns
regarding flight path privacy. Conventional UAV communication systems often
expose flight path data to third parties, making them vulnerable to tracking,
surveillance, and location inference attacks. Existing encryption techniques
provide security but fail to ensure complete privacy, as adversaries can still
infer movement patterns through metadata analysis. To address these challenges,
we propose a zk-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of
Knowledge)-based privacy-preserving flight path authentication and verification
framework. Our approach ensures that a UAV can prove its authorisation,
validate its flight path with a control centre, and comply with regulatory
constraints without revealing any sensitive trajectory information. By
leveraging zk-SNARKs, the UAV can generate cryptographic proofs that verify
compliance with predefined flight policies while keeping the exact path and
location undisclosed. This method mitigates risks associated with real-time
tracking, identity exposure, and unauthorised interception, thereby enhancing
UAV operational security in adversarial environments. Our proposed solution
balances privacy, security, and computational efficiency, making it suitable
for resource-constrained UAVs in both civilian and military applications.

</details>


### [12] [Post-Quantum Blockchain: Challenges and Opportunities](https://arxiv.org/abs/2508.17071)
*Sufyan Al-Janabi*

Main category: cs.CR

TL;DR: 양자 컴퓨터의 발전이 기존 블록체인 기술의 보안에 위협을 가하고 있기 때문에, 포스트양자 암호화(PQC)를 적용한 포스트양자 블록체인(PQB)의 필요성을 강조하고, PQB의 보안을 위한 유용한 가이드를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨터의 발전이 기존 암호 시스템에 위협이 되고 있어 블록체인 기술이 위험에 처하게 되었다.

Method: 포스트양자 암호화(PQC)를 블록체인에 적용하여 포스트양자 블록체인(PQB)을 생성하는 방법을 제시한다.

Result: 양자 컴퓨터가 기존 블록체인 기술에 미치는 위협을 검토하고, PQB 보안에 대한 가이드를 제공한다.

Conclusion: 향후 연구 방향에 대한 도전과 기회를 집중적으로 다룬다.

Abstract: Blockchain is a Distributed Ledger Technology (DLT) that offers numerous
benefits including decentralization, transparency, efficiency, and reduced
costs. Hence, blockchain has been included in many fields. Blockchain relies on
cryptographic protocols (especially public-key cryptography and hash functions)
to achieve many essential sub-routines. However, the increased progress of
quantum computation and algorithms has threatened the security of many
traditional cryptosystems. Therefore, this represents a serious risk for the
existing blockchain technology. For example, SHA-256 and the Elliptic Curve
Digital Signature Algorithm (ECDSA) cryptosystems can be compromised by Shor s
and Grover s quantum algorithms in the foreseeable future. Post-Quantum
Cryptography (PQC) is a basic solution for resisting these quantum attacks.
Applying PQC to blockchains results in creating Post-Quantum Blockchains (PQB).
Thus, this paper aims to review the threats imposed by quantum computers on
classical blockchain technology and provide useful guidelines on PQB security
to blockchain researchers. The paper focuses on the challenges and
opportunities of future work direction in this field.

</details>


### [13] [SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks](https://arxiv.org/abs/2508.17121)
*Zhenliang Gan,Xiaoxiao Hu,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Main category: cs.CR

TL;DR: SyncGuard는 오디오 워터마킹의 위치 추적 및 비동기 공격 저항 문제를 해결하기 위한 학습 기반 방법이다.


<details>
  <summary>Details</summary>
Motivation: 오디오 신호의 고유 특성 때문에 워터마크의 위치 추적 및 비동기 공격 저항이 큰 도전 과제가 되고 있다.

Method: 프레임별 방송 임베딩 전략을 설계하여任意 길이의 오디오에 워터마크를 삽입하고, 로컬라이제이션 필요 없이 워터마크 추출시 시간 독립성을 강화한다. 그리고 세심하게 설계된 왜곡 계층을 도입하여 견고함을 높이고, 확장된 잔여 블록과 확장된 게이티드 블록을 사용하여 다중 해상도 시간-주파수 특징을 효과적으로 캡처한다.

Result: SyncGuard는 가변 길이의 오디오 세그먼트를 효율적으로 처리하고, 다양한 공격에 대한 견고성에서 최신 기술을 초월하며, 뛰어난 청각 품질을 제공한다.

Conclusion: 이 연구는 SyncGuard가 오디오 워터마킹의 성능을 크게 개선할 수 있음을 보여준다.

Abstract: Audio watermarking has been widely applied in copyright protection and source
tracing. However, due to the inherent characteristics of audio signals,
watermark localization and resistance to desynchronization attacks remain
significant challenges. In this paper, we propose a learning-based scheme named
SyncGuard to address these challenges. Specifically, we design a frame-wise
broadcast embedding strategy to embed the watermark in arbitrary-length audio,
enhancing time-independence and eliminating the need for localization during
watermark extraction. To further enhance robustness, we introduce a
meticulously designed distortion layer. Additionally, we employ dilated
residual blocks in conjunction with dilated gated blocks to effectively capture
multi-resolution time-frequency features. Extensive experimental results show
that SyncGuard efficiently handles variable-length audio segments, outperforms
state-of-the-art methods in robustness against various attacks, and delivers
superior auditory quality.

</details>


### [14] [Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents](https://arxiv.org/abs/2508.17155)
*Derek Lilienthal,Sanghyun Hong*

Main category: cs.CR

TL;DR: LLM 기반 에이전트의 TOCTOU 취약성을 최초로 연구하고, 이를 평가하기 위한 벤치마크와 대응 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 보안 취약점을 이해하고 해결하기 위해.

Method: TOCTOU-Bench라는 66개의 사용자 작업으로 구성된 벤치마크를 사용하여 TOCTOU 취약성을 평가하고, 탐지 및 완화 기법을 적용하였다.

Result: 자동 탐지 방법을 사용하여 25%의 탐지 정확도, 3%의 취약한 계획 생성 감소, 95%의 공격 창 감소를 달성하였다.

Conclusion: AI 안전성과 시스템 보안의 교차점에서 새로운 연구 방향을 열었다.

Abstract: Large Language Model (LLM)-enabled agents are rapidly emerging across a wide
range of applications, but their deployment introduces vulnerabilities with
security implications. While prior work has examined prompt-based attacks
(e.g., prompt injection) and data-oriented threats (e.g., data exfiltration),
time-of-check to time-of-use (TOCTOU) remain largely unexplored in this
context. TOCTOU arises when an agent validates external state (e.g., a file or
API response) that is later modified before use, enabling practical attacks
such as malicious configuration swaps or payload injection. In this work, we
present the first study of TOCTOU vulnerabilities in LLM-enabled agents. We
introduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to
evaluate this class of vulnerabilities. As countermeasures, we adapt detection
and mitigation techniques from systems security to this setting and propose
prompt rewriting, state integrity monitoring, and tool-fusing. Our study
highlights challenges unique to agentic workflows, where we achieve up to 25%
detection accuracy using automated detection methods, a 3% decrease in
vulnerable plan generation, and a 95% reduction in the attack window. When
combining all three approaches, we reduce the TOCTOU vulnerabilities from an
executed trajectory from 12% to 8%. Our findings open a new research direction
at the intersection of AI safety and systems security.

</details>


### [15] [Exposing Privacy Risks in Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.17222)
*Jiale Liu,Jiahao Zhang,Suhang Wang*

Main category: cs.CR

TL;DR: Graph RAG 시스템의 데이터 추출 취약성을 조사하고, 원시 텍스트 누출을 줄이면서도 구조적 엔티티 및 관계 정보의 추출에 더 취약함을 발견했다.


<details>
  <summary>Details</summary>
Motivation: Graph RAG는 외부 지식을 통합하는 데 강력하지만, 구조화된 그래프 탐색으로 전환하면서 새로운 개인 정보 보호 위험이 도입된다.

Method: 맞춤형 데이터 추출 공격을 설계하고 실행하여 Graph RAG 시스템의 원시 텍스트 및 구조화된 데이터 누출 취약성을 조사하였다.

Result: Graph RAG 시스템은 원시 텍스트 누출을 줄일 수 있지만 구조적 엔티티 및 관계 정보 추출에 더 취약해진다는 것을 발견했다.

Conclusion: 이 연구는 Graph RAG의 고유한 개인 정보 보호 문제에 대한 기초 분석을 제공하고 더 안전한 시스템 구축을 위한 통찰을 제시한다.

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing
Large Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has
emerged as an advanced paradigm that leverages graph-based knowledge structures
to provide more coherent and contextually rich answers. However, the move from
plain document retrieval to structured graph traversal introduces new,
under-explored privacy risks. This paper investigates the data extraction
vulnerabilities of the Graph RAG systems. We design and execute tailored data
extraction attacks to probe their susceptibility to leaking both raw text and
structured data, such as entities and their relationships. Our findings reveal
a critical trade-off: while Graph RAG systems may reduce raw text leakage, they
are significantly more vulnerable to the extraction of structured entity and
relationship information. We also explore potential defense mechanisms to
mitigate these novel attack surfaces. This work provides a foundational
analysis of the unique privacy challenges in Graph RAG and offers insights for
building more secure systems.

</details>


### [16] [Literature Review of the Effect of Quantum Computing on Cryptocurrencies using Blockchain Technology](https://arxiv.org/abs/2508.17296)
*Adi Mutha,Jitendra Sandu*

Main category: cs.CR

TL;DR: 양자 컴퓨팅의 발전으로 인해 블록체인 기술에 의존하는 암호화폐는 새로운 암호학적 취약점에 직면하고 있다. 본 논문은 쇼어 알고리즘과 그로버 알고리즘이 암호화폐의 보안 메커니즘에 미치는 위협을 평가하는 포괄적인 문헌 리뷰를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨팅의 발전으로 인해 블록체인 기반 암호화폐의 보안이 위협받고 있다는 인식.

Method: 쇼어 알고리즘과 그로버 알고리즘의 영향을 평가하고, 비트코인, 이더리움, 라이트코인, 모네로, 제캐시와 같은 주요 암호화폐의 내부 메커니즘을 분석하여 취약점을 식별하였다.

Result: 주요 암호화폐의 거래 및 합의 프로세스에서 특정 취약점을 발견하고, 양자 시스템의 하드웨어 한계와 공격 가능 시점을 추정하였다.

Conclusion: 양자 컴퓨터가 즉각적인 위협이 되기에는 아직 발전이 부족하지만, 양자 저항 솔루션의 적극적인 통합이 필수적임을 강조하였다.

Abstract: With the advent of quantum computing, cryptocurrencies that rely on
blockchain technology face mounting cryptographic vulnerabilities. This paper
presents a comprehensive literature review evaluating how quantum algorithms,
specifically Shors and Grovers, could disrupt the foundational security
mechanisms of cryptocurrencies. Shors algorithm poses a threat to public-key
cryptographic schemes by enabling efficient factorization and discrete
logarithm solving, thereby endangering digital signature systems. Grovers
algorithm undermines hash-based functions, increasing the feasibility of fifty
one percent attacks and hash collisions. By examining the internal mechanisms
of major cryptocurrencies such as Bitcoin, Ethereum, Litecoin, Monero, and
Zcash, this review identifies specific vulnerabilities in transaction and
consensus processes. It further analyses the current hardware limitations of
quantum systems and estimates when such attacks could become feasible. In
anticipation, it investigates countermeasures including Post-Quantum
Cryptography (PQC), Quantum Key Distribution (QKD), and protocol-level
modifications such as memory-intensive proof-of-work algorithms and
multi-signature schemes. The discussion integrates recent advancements in
quantum error correction, hardware scalability, and NIST-standardized
cryptographic algorithms. This review concludes that while quantum computers
are not yet advanced enough to pose an immediate threat, proactive integration
of quantum-resistant solutions is essential. The findings underscore the urgent
need for cryptocurrencies to adopt post-quantum cryptographic standards to
preserve the decentralized trust, integrity, and security that define
blockchain-based digital cryptocurrencies.

</details>


### [17] [An Efficient Recommendation Filtering-based Trust Model for Securing Internet of Things](https://arxiv.org/abs/2508.17304)
*Muhammad Ibn Ziauddin,Rownak Rahad Rabbi,SM Mehrab,Fardin Faiyaz,Mosarrat Jahan*

Main category: cs.CR

TL;DR: 이 논문은 IoT의 신뢰 계산의 한계를 해결하기 위한 강력한 신뢰 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: IoT의 데이터 보안을 보장하기 위해 신뢰 계산이 중요하지만, 현재의 신뢰 기반 메커니즘에는 여러 가지 제한이 있다.

Method: 동적으로 윈도우 길이를 결정하고, 평균 신뢰 점수와 시간을 사용한 조화 평균을 적용하여 신뢰 점수의 급격한 변동을 방지한다. 개인화된 서브스페이스 클러스터링 알고리즘을 통해 추천을 필터링한다.

Result: 제안된 모델은 나쁜 발언 공격 탐지에서 경쟁력 있는 성능을 보여주며, 온-오프 공격 탐지 정확성을 약 44% 향상시킨다.

Conclusion: 제안된 기법은 온-오프 공격자가 증가하는 상황에서도 효과성을 유지하며 추천 필터링 시간을 95% 감소시킨다.

Abstract: Trust computation is crucial for ensuring the security of the Internet of
Things (IoT). However, current trust-based mechanisms for IoT have limitations
that impact data security. Sliding window-based trust schemes cannot ensure
reliable trust computation due to their inability to select appropriate window
lengths. Besides, recent trust scores are emphasized when considering the
effect of time on trust. This can cause a sudden change in overall trust score
based on recent behavior, potentially misinterpreting an honest service
provider as malicious and vice versa. Moreover, clustering mechanisms used to
filter recommendations in trust computation often lead to slower results. In
this paper, we propose a robust trust model to address these limitations. The
proposed approach determines the window length dynamically to guarantee
accurate trust computation. It uses the harmonic mean of average trust score
and time to prevent sudden fluctuations in trust scores. Additionally, an
efficient personalized subspace clustering algorithm is used to exclude
recommendations. We present a security analysis demonstrating the resiliency of
the proposed scheme against bad-mouthing, ballot-stuffing, and on-off attacks.
The proposed scheme demonstrates a competitive performance in detecting
bad-mouthing attacks, while outperforming existing works with an approximately
44% improvement in accuracy for detecting on-off attacks. It maintains its
effectiveness even when the percentage of on-off attackers increases and in
scenarios where multiple attacks occur simultaneously. Additionally, the
proposed scheme reduces the recommendation filtering time by 95%.

</details>


### [18] [Risk Assessment and Security Analysis of Large Language Models](https://arxiv.org/abs/2508.17329)
*Xiaoyan Zhang,Dongyang Lyu,Xiaoqi Li*

Main category: cs.CR

TL;DR: 이 논문은 대규모 언어 모델의 보안 문제와 위험 평가 시스템을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)이 개인 정보 유출, 편향 강화, 악의적 남용 등의 체계적인 보안 문제를 드러내고 있어, 전체 생애 주기를 아우르는 동적인 위험 평가 및 협력 방어 체계의 필요성이 절실합니다.

Method: 동적 위험 평가 시스템과 다양한 보호 수준이 협력할 수 있는 계층 방어 시스템을 설계하였습니다. 정적 및 동적 지표를 동시에 평가할 수 있는 위험 평가 시스템을 제시하며, 민감한 단어의 빈도수와 API 호출의 일반성 등의 데이터의 엔트로피 가중치를 사용하여 필수 데이터를 계산합니다.

Result: 실험 결과, 이 시스템은 역할 탈출과 같은 은폐 공격을 식별할 수 있으며, 신속한 위험 평가가 가능합니다. 입력 레이어에서는 BERT-CRF라는 혼합 모델을 사용하여 악의적인 명령을 식별하고 필터링합니다.

Conclusion: 이 방법의 품질은 특히 금융 산업의 고객 서비스 측면에서 중요합니다.

Abstract: As large language models (LLMs) expose systemic security challenges in high
risk applications, including privacy leaks, bias amplification, and malicious
abuse, there is an urgent need for a dynamic risk assessment and collaborative
defence framework that covers their entire life cycle. This paper focuses on
the security problems of large language models (LLMs) in critical application
scenarios, such as the possibility of disclosure of user data, the deliberate
input of harmful instructions, or the models bias. To solve these problems, we
describe the design of a system for dynamic risk assessment and a hierarchical
defence system that allows different levels of protection to cooperate. This
paper presents a risk assessment system capable of evaluating both static and
dynamic indicators simultaneously. It uses entropy weighting to calculate
essential data, such as the frequency of sensitive words, whether the API call
is typical, the realtime risk entropy value is significant, and the degree of
context deviation. The experimental results show that the system is capable of
identifying concealed attacks, such as role escape, and can perform rapid risk
evaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional
Encoder Representation from Transformers) at the input layer to identify and
filter malicious commands. The model layer uses dynamic adversarial training
and differential privacy noise injection technology together. The output layer
also has a neural watermarking system that can track the source of the content.
In practice, the quality of this method, especially important in terms of
customer service in the financial industry.

</details>


### [19] [Cyber Security Educational Games for Children: A Systematic Literature Review](https://arxiv.org/abs/2508.17414)
*Temesgen Kitaw Damenu,İnci Zaim Gökbay,Alexandra Covaci,Shujun Li*

Main category: cs.CR

TL;DR: 이 논문은 2010년부터 2024년까지 발표된 68개의 논문에서 보고된 91개의 교육 게임을 분석하여 사이버 보안 교육에 있어 긍정적인 학습 결과를 나타내는 증거를 제시하고, 디자인 프로세스 및 방법론적 엄밀성에 대한 중요한 격차를 식별했습니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안에 대한 이해를 높이기 위해 교육 게임을 사용하는 것의 효과를 검토하기 위해.

Method: 2010년부터 2024년 사이에 발표된 논문들에서 91개의 교육 게임을 체계적으로 분석했다.

Result: 교육 게임이 사이버 보안 교육에 있어 긍정적인 학습 결과를 제공함을 발견했으나, 디자인 및 방법론에 있어 여러 중요한 격차가 존재함을 확인했다.

Conclusion: 향후 연구 방향으로 하이브리드 게임 디자인 및 평가 접근법을 결합할 것을 제안한다.

Abstract: Educational games have been widely used to teach children about cyber
security. This systematic literature review reveals evidence of positive
learning outcomes, after analysing 91 such games reported in 68 papers
published between 2010 and 2024. However, critical gaps have also been
identified regarding the design processes and the methodological rigour,
including lack of systematic design, misalignment between proposed and achieved
learning outcomes, rare use of control groups, limited discussions on ethical
considerations, and underutilisation of emerging technologies. We recommend
multiple future research directions, e.g., a hybrid approach to game design and
evaluation that combines bottom-up and top-down approaches.

</details>


### [20] [SoK: Cybersecurity Assessment of Humanoid Ecosystem](https://arxiv.org/abs/2508.17481)
*Priyanka Prakash Surve,Asaf Shabtai,Yuval Elovici*

Main category: cs.CR

TL;DR: 휴머노이드 로봇의 보안 문제를 종합적으로 다루고, 39개의 공격과 35개의 방어를 포함한 7계층 보안 모델을 제안하며, 실제 로봇의 보안 성숙도를 평가한다.


<details>
  <summary>Details</summary>
Motivation: 휴머노이드 로봇의 실용적인 배치가 다양한 산업에서 진행되고 있지만, 여전히 기존 CPS 모델이 해결하지 못하는 보안 취약점이 존재한다.

Method: 로봇 공학, CPS 및 네트워크 보안 분야의 조각난 연구를 통합한 체계적 지식(SOK)을 통해 39개의 알려진 공격과 35개의 방어를 구조화한 7계층 보안 모델을 제안하고, 39x35 공격-방어 매트릭스를 개발하여 위험 가중 점수를 산정한다.

Result: 모델을 통해 Pepper, G1 EDU, Digit 등 3개의 실제 로봇의 보안 성숙도를 평가하였고, 성적은 39.9%에서 79.5%로 다양하게 나타났다.

Conclusion: 이 연구는 시스템적인 보안 평가를 가능하게 하는 구조화된 증거 기반 평가 방법을 제시하며, 크로스 플랫폼 벤치마킹을 지원하고 휴머노이드 로봇에 대한 보안 투자 우선순위를 안내한다.

Abstract: Humanoids are progressing toward practical deployment across healthcare,
industrial, defense, and service sectors. While typically considered
cyber-physical systems (CPSs), their dependence on traditional networked
software stacks (e.g., Linux operating systems), robot operating system (ROS)
middleware, and over-the-air update channels, creates a distinct security
profile that exposes them to vulnerabilities conventional CPS models do not
fully address. Prior studies have mainly examined specific threats, such as
LiDAR spoofing or adversarial machine learning (AML). This narrow focus
overlooks how an attack targeting one component can cascade harm throughout the
robot's interconnected systems. We address this gap through a systematization
of knowledge (SoK) that takes a comprehensive approach, consolidating
fragmented research from robotics, CPS, and network security domains. We
introduce a seven-layer security model for humanoid robots, organizing 39 known
attacks and 35 defenses across the humanoid ecosystem-from hardware to
human-robot interaction. Building on this security model, we develop a
quantitative 39x35 attack-defense matrix with risk-weighted scoring, validated
through Monte Carlo analysis. We demonstrate our method by evaluating three
real-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed
varying security maturity levels, with scores ranging from 39.9% to 79.5%
across the platforms. This work introduces a structured, evidence-based
assessment method that enables systematic security evaluation, supports
cross-platform benchmarking, and guides prioritization of security investments
in humanoid robotics.

</details>


### [21] [Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models](https://arxiv.org/abs/2508.17674)
*Qiming Guo,Jinwen Tang,Xingran Huang*

Main category: cs.CR

TL;DR: Advertisement Embedding Attacks (AEA)는 LLM 보안 위협의 새로운 유형으로, 모델 출력에 프로모션 또는 악의적인 콘텐츠를 은밀히 주입한다.


<details>
  <summary>Details</summary>
Motivation: LLM 보안에서 AEA의 존재는 심각하게 다루어지지 않고 있어, 이는 정보 무결성을 훼손하고 사용자에게 해를 끼치는 잠재적인 위험을 초래합니다.

Method: AEA는 두 가지 저렴한 벡터를 통해 작동합니다: (1) 서드파티 서비스 배포 플랫폼을 탈취하여 적대적인 프롬프트를 추가하고, (2) 공격자 데이터로 미세 조정된 백도어 공개 체크포인트를 게시합니다.

Result: 이 공격 방식은 모델이 정상적으로 보이면서도 숨겨진 광고, 선전, 또는 증오 발언을 반환하게 만듭니다.

Conclusion: AEA는 LLM 보안에서 긴급한 문제를 드러내며, AI 안전 커뮤니티의 조정된 탐지, 감사 및 정책 대응을 촉구합니다.

Abstract: We introduce Advertisement Embedding Attacks (AEA), a new class of LLM
security threats that stealthily inject promotional or malicious content into
model outputs and AI agents. AEA operate through two low-cost vectors: (1)
hijacking third-party service-distribution platforms to prepend adversarial
prompts, and (2) publishing back-doored open-source checkpoints fine-tuned with
attacker data. Unlike conventional attacks that degrade accuracy, AEA subvert
information integrity, causing models to return covert ads, propaganda, or hate
speech while appearing normal. We detail the attack pipeline, map five
stakeholder victim groups, and present an initial prompt-based self-inspection
defense that mitigates these injections without additional model retraining.
Our findings reveal an urgent, under-addressed gap in LLM security and call for
coordinated detection, auditing, and policy responses from the AI-safety
community.

</details>


### [22] [TLGLock: A New Approach in Logic Locking Using Key-Driven Charge Recycling in Threshold Logic Gates](https://arxiv.org/abs/2508.17809)
*Abdullah Sahruri,Martin Margala*

Main category: cs.CR

TL;DR: TLGLock는 하드웨어 보안 강화의 새로운 디자인 패러다임으로, 효율성과 저비용의 키 종속 기능을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 하드웨어 불법 복제에 대한 방어가 중요해지면서, 현재 방법들은 확장성과 설계 오버헤드에서 어려움을 겪고 있습니다.

Method: Threshold Logic Gates(TLGs)의 구조적 표현력과 에너지 효율성을 활용하여 게이트 수준에서 키 종속 기능을 enforced하는 TLGLock을 제안합니다. 키를 게이트의 가중 논리에 내장하고 동적 전하 공유를 활용하여 기존의 잠금 기술에 대한 상태 비저장 및 컴팩트한 대안을 제공합니다.

Result: ISCAS, ITC, MCNC 벤치마크를 통해 TLGLock은 래치 기반 잠금 방식에 비해 최대 30%의 면적, 50%의 지연, 20%의 전력 절감을 달성했습니다. XOR 및 SFLL-HD 방법과 비교할 때, TLGLock은 오버헤드를 대폭 줄이며 최대 3배 높은 SAT 공격 저항성을 제공합니다.

Conclusion: TLGLock은 안전한 하드웨어 설계를 위한 확장 가능하고 복원력 있는 솔루션으로 자리매김합니다.

Abstract: Logic locking remains one of the most promising defenses against hardware
piracy, yet current approaches often face challenges in scalability and design
overhead. In this paper, we present TLGLock, a new design paradigm that
leverages the structural expressiveness of Threshold Logic Gates (TLGs) and the
energy efficiency of charge recycling to enforce key-dependent functionality at
the gate level. By embedding the key into the gate's weighted logic and
utilizing dynamic charge sharing, TLGLock provides a stateless and compact
alternative to conventional locking techniques. We implement a complete
synthesis-to-locking flow and evaluate it using ISCAS, ITC, and MCNC
benchmarks. Results show that TLGLock achieves up to 30% area, 50% delay, and
20% power savings compared to latch-based locking schemes. In comparison with
XOR and SFLL-HD methods, TLGLock offers up to 3x higher SAT attack resistance
with significantly lower overhead. Furthermore, randomized key-weight
experiments demonstrate that TLGLock can reach up to 100% output corruption
under incorrect keys, enabling tunable security at minimal cost. These results
position TLGLock as a scalable and resilient solution for secure hardware
design.

</details>


### [23] [Software Unclonable Functions for IoT Devices Identification and Security](https://arxiv.org/abs/2508.17853)
*Saeed Alshehhi*

Main category: cs.CR

TL;DR: 본 연구는 IoT 생태계에서 합법적인 장치와 손상된 장치를 구분하는 도전 과제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: IoT 생태계에서 합법적인 장치와 손상된 장치를 구분하는 것이 중요하다.

Method: 하드웨어 성능 카운터(HPC)에서 유도된 서명들의 독창성을 조사한다.

Result: 소프트웨어 클론 방지 기능(SUFs)이라는 개념을 통해 이 연구는 장치 신뢰성 개선을 목표로 한다.

Conclusion: 본 연구는 HPC 기반 서명의 독창성이 장치의 신뢰성을 높이는 데 기여할 수 있음을 보여준다.

Abstract: In the evolving landscape of IoT ecosystem, distinguishing between legitimate
and compromised devices is a critical challenge. This research investigates the
effectiveness of hardware performance counter (HPC)-derived signatures'
uniqueness under the umbrella of a concept that we introduced as software
unclonable functions (SUFs).

</details>


### [24] [MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs](https://arxiv.org/abs/2508.17856)
*Tiezhu Sun,Marco Alecci,Aleksandr Pilgun,Yewei Song,Xunzhu Tang,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: 안드로이드 악성코드의 빠른 진화는 모바일 애플리케이션의 유지보수와 보안에 심각한 도전을 제기합니다. 전통적인 탐지 기법은 악성 행위를 정밀하게 이해하는 데 필요한 악성 페이로드의 세분화된 위치 파악에 어려움을 겪고 있습니다. 이러한 격차를 해소하기 위해, 본 연구에서는 대형 언어 모델을 이용하여 악성 페이로드를 세분화하여 파악하는 새로운 접근법인 MalLoc을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 악성코드의 빠른 진화에 따른 모바일 앱의 보안 유지 어려움.

Method: 대형 언어 모델을 활용하여 안드로이드 악성코드 내 악성 페이로드를 세분화하여 위치 파악하는 방법을 제안.

Result: LLM을 활용한 악성 페이로드의 세분화 위치 파악의 가능성과 효과를 실험적으로 입증.

Conclusion: MalLoc은 전통적인 탐지 및 분류를 넘어, 악성 로직에 대한 깊은 통찰력을 제공하고 연구의 새로운 방향을 열어줍니다.

Abstract: The rapid evolution of Android malware poses significant challenges to the
maintenance and security of mobile applications (apps). Traditional detection
techniques often struggle to keep pace with emerging malware variants that
employ advanced tactics such as code obfuscation and dynamic behavior
triggering. One major limitation of these approaches is their inability to
localize malicious payloads at a fine-grained level, hindering precise
understanding of malicious behavior. This gap in understanding makes the design
of effective and targeted mitigation strategies difficult, leaving mobile apps
vulnerable to continuously evolving threats.
  To address this gap, we propose MalLoc, a novel approach that leverages the
code understanding capabilities of large language models (LLMs) to localize
malicious payloads at a fine-grained level within Android malware. Our
experimental results demonstrate the feasibility and effectiveness of using
LLMs for this task, highlighting the potential of MalLoc to enhance precision
and interpretability in malware analysis. This work advances beyond traditional
detection and classification by enabling deeper insights into behavior-level
malicious logic and opens new directions for research, including dynamic
modeling of localized threats and targeted countermeasure development.

</details>


### [25] [PhantomLint: Principled Detection of Hidden LLM Prompts in Structured Documents](https://arxiv.org/abs/2508.17884)
*Toby Murray*

Main category: cs.CR

TL;DR: 본 논문은 구조화된 문서에서 숨겨진 LLM 프롬프트를 탐지하기 위한 최초의 원칙적인 접근 방식을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 숨겨진 LLM 프롬프트는 인간의 감시에서 탐지되지 않으면서 간접적인 프롬프트 주입 공격을 유발하고, 자동화된 문서 처리 시스템을 조작하기 위해 등장하고 있습니다.

Method: 이 논문에서는 PhantomLint라는 프로토타입 도구를 구현하여 숨겨진 LLM 프롬프트 탐지를 위한 접근 방식을 제안합니다.

Result: 우리는 3,402개의 문서(학술지 논문 예비본, 이력서, 논문 등 포함)에 대해 PhantomLint를 평가했습니다. 결과적으로 우리의 접근 방식은 다양한 LLM 프롬프트 은폐 방법에 대해 일반적으로 적용 가능하며, 매우 낮은 허위 양성률(약 0.092%)을 기록했습니다.

Conclusion: PhantomLint는 실제 문서에서 숨겨진 LLM 프롬프트를 탐지하는 데 실용적이며, 수용 가능한 성능을 달성합니다.

Abstract: Hidden LLM prompts have appeared in online documents with increasing
frequency. Their goal is to trigger indirect prompt injection attacks while
remaining undetected from human oversight, to manipulate LLM-powered automated
document processing systems, against applications as diverse as r\'esum\'e
screeners through to academic peer review processes. Detecting hidden LLM
prompts is therefore important for ensuring trust in AI-assisted human decision
making.
  This paper presents the first principled approach to hidden LLM prompt
detection in structured documents. We implement our approach in a prototype
tool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402
documents, including both PDF and HTML documents, and covering academic paper
preprints, CVs, theses and more. We find that our approach is generally
applicable against a wide range of methods for hiding LLM prompts from visual
inspection, has a very low false positive rate (approx. 0.092%), is practically
useful for detecting hidden LLM prompts in real documents, while achieving
acceptable performance.

</details>


### [26] [PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities](https://arxiv.org/abs/2508.17913)
*Yagmur Yigit,Mehmet Ali Erturk,Kerem Gursu,Berk Canberk*

Main category: cs.CR

TL;DR: PRZK-Bind는 스마트 시티에서 물리적 개체와 디지털 쌍둥이 간의 안전하고 실시간 통신을 가능하게 하는 경량 분산 인증 프로토콜이다.


<details>
  <summary>Details</summary>
Motivation: 디지털 트윈 기술은 스마트 시티 생태계에서 필수적이나, 동적이고 적대적인 환경에서 물리적 대상을 안전하게 연결하는 데에 어려움이 있다.

Method: Schnorr 기반의 영 지식 증명과 타원 곡선 암호화를 결합하여 사전 공유 비밀 없이 물리적 개체와 디지털 쌍둥이 간의 안전한 실시간 대응을 설정하는 PRZK-Bind 프로토콜을 제안한다.

Result: PRZK-Bind는 성능을 크게 향상시켜 암호화 기반 기준선보다 최대 4.5배 낮은 지연 시간과 4배 줄어든 에너지 소비를 제공하며, 허위 수용율은 10배 이상 낮춘다.

Conclusion: PRZK-Bind는 효율적이고 복원력 있으며 신뢰할 수 있는 디지털 쌍둥이 인증이 필요한 스마트 시티 배포에 적합하다.

Abstract: Digital twin (DT) technology is rapidly becoming essential for smart city
ecosystems, enabling real-time synchronisation and autonomous decision-making
across physical and digital domains. However, as DTs take active roles in
control loops, securely binding them to their physical counterparts in dynamic
and adversarial environments remains a significant challenge. Existing
authentication solutions either rely on static trust models, require
centralised authorities, or fail to provide live and verifiable
physical-digital binding, making them unsuitable for latency-sensitive and
distributed deployments. To address this gap, we introduce PRZK-Bind, a
lightweight and decentralised authentication protocol that combines
Schnorr-based zero-knowledge proofs with elliptic curve cryptography to
establish secure, real-time correspondence between physical entities and DTs
without relying on pre-shared secrets. Simulation results show that PRZK-Bind
significantly improves performance, offering up to 4.5 times lower latency and
4 times reduced energy consumption compared to cryptography-heavy baselines,
while maintaining false acceptance rates more than 10 times lower. These
findings highlight its suitability for future smart city deployments requiring
efficient, resilient, and trustworthy DT authentication.

</details>


### [27] [MoveScanner: Analysis of Security Risks of Move Smart Contracts](https://arxiv.org/abs/2508.17964)
*Yuhe Lu,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: 블록체인 기술의 발전에 따라 스마트 계약의 보안 문제가 부각되고 있으며, 이 논문은 Move 언어의 보안 분석 도구인 MoveScanner를 제안하여 주요 보안 취약점을 식별하고 있다.


<details>
  <summary>Details</summary>
Motivation: 스마트 계약의 보안 문제와 개발자 프로그래밍 오류로 인한 새로운 보안 도전.

Method: MoveScanner라는 정적 분석 도구를 사용하여 제어 흐름 그래프와 데이터 흐름 분석 아키텍처에 기반.

Result: MoveScanner는 88.2%의 탐지 정확도로 주요 보안 취약점 5가지(자원 누수, 약한 권한 관리, 산술 오버플로우 등)를 효과적으로 식별했다.

Conclusion: 이 연구는 자원 지향 프로그래밍 패러다임에 기반한 12개의 새로운 보안 위험 유형을 제시하고, 스마트 계약 보안 메커니즘 개발을 위한 이론적 기초와 실질적인 경험을 제공한다.

Abstract: As blockchain technology continues to evolve, the security of smart contracts
has increasingly drawn attention from both academia and industry. The Move
language, with its unique resource model and linear type system, provides a
solid foundation for the security of digital assets. However, smart contracts
still face new security challenges due to developer programming errors and the
potential risks associated with cross-module interactions. This paper
systematically analyzes the limitations of existing security tools within the
Move ecosystem and reveals their unique vulnerability patterns. To address
these issues, it introduces MoveScanner, a static analysis tool based on a
control flow graph and data flow analysis architecture. By incorporating
cross-module call graph tracking, MoveScanner can effectively identify five key
types of security vulnerabilities, including resource leaks, weak permission
management, and arithmetic overflows. In terms of design, MoveScanner adheres
to a modular principle, supports bytecode-level analysis and multi-chain
adaptation, and introduces innovative resource trajectory tracking algorithms
and capability matrix analysis methods, thereby significantly reducing the
false positive rate. Empirical results show that MoveScanner achieved 88.2%
detection accuracy in benchmark testing, filling the gap in security tools in
the Move ecosystem. Furthermore, this paper identifies twelve new types of
security risks based on the resource-oriented programming paradigm and provides
a theoretical foundation and practical experience for the development of smart
contract security mechanisms. Future work will focus on combining formal
verification and dynamic analysis techniques to build a security protection
framework covering the entire contract lifecycle

</details>


### [28] [Aligning Core Aspects: Improving Vulnerability Proof-of-Concepts via Cross-Source Insights](https://arxiv.org/abs/2508.18109)
*Lingxiao Wang,Wenjing Dang,Mengyao Zhang,Yue Wang,Xianzong Wu,Sen Chen*

Main category: cs.CR

TL;DR: 이 논문은 공개 플랫폼에서 PoC 보고서의 정보 결핍 문제를 다루며, 다양한 출처에서 정보를 보완하여 결측된 정보를 채우는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: PoC 보고서는 취약성을 입증하는 데 필수적이나, 다양한 템플릿으로 인해 정보 결핍이 발생하고 있습니다.

Method: 4개 플랫폼에서 173,170개의 PoC 보고서를 수집하고, 8개의 핵심 측면을 정의하여 정보를 추출하기 위한 규칙 기반 매칭과 세밀하게 조정된 BERT-NER 모델을 통합하여 분석했습니다.

Result: 모든 공개 플랫폼의 PoC 보고서는 최소 하나의 핵심 측면이 누락된 것으로 나타났습니다. 이후 다양한 출처의 CVE 항목 및 관련 PoC 보고서를 활용하여 다중 출처 정보 융합 방법을 개발했습니다.

Conclusion: 결국, 69,583개의 PoC 보고서를 성공적으로 완성하였습니다(전체 보고서의 40.18%).

Abstract: For vulnerabilities, Proof-of-Concept (PoC) plays an irreplaceable role in
demonstrating the exploitability. PoC reports may include critical information
such as specific usage, test platforms, and more, providing essential insights
for researchers. However, in reality, due to various PoC templates across PoC
platforms, PoC reports extensively suffer from information deficiency, leading
the suboptimal quality and limited usefulness. Fortunately, we found that
information deficiency of PoC reports could be mitigated by the completion from
multiple sources given the same referred vulnerability. In this paper, we
conduct the first study on the deficiency of information in PoC reports across
public platforms. We began by collecting 173,170 PoC reports from 4 different
platforms and defined 8 key aspects that PoCs should contain. By integrating
rule-based matching and a fine-tuned BERT-NER model for extraction of key
aspects, we discovered that all PoC reports available on public platforms have
at least one missing key aspect. Subsequently, we developed a multi-source
information fusion method to complete the missing aspect information in PoC
reports by leveraging CVE entries and related PoC reports from different
sources. Finally, we successfully completed 69,583 PoC reports (40.18% of all
reports).

</details>


### [29] [Learning from Few Samples: A Novel Approach for High-Quality Malcode Generation](https://arxiv.org/abs/2508.18148)
*Haijian Ma,Daizong Liu,Xiaowen Cai,Pan Zhou,Yulai Xie*

Main category: cs.CR

TL;DR: 본 논문은 멀티 샘플 학습 상황에서 악성 코드 생성과 SQL 주입 탐지 능력을 향상시키기 위해 GAN과 LLM을 통합한 새로운 준지도 학습 프레임워크인 GANGRL-LLM을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: IDS의 탐지 모델 훈련에 있어 충분히 레이블이 지정된 악성 샘플 부족 문제가 주요 도전과제입니다.

Method: GAN 기반의 판별자와 LLM 기반의 생성자를 활용하여 협업 훈련 패러다임을 채택합니다.

Result: 한정된 레이블 샘플로도 악성 코드 생성 및 탐지 능력을 효과적으로 향상시킬 수 있음을 실험 결과가 보여줍니다.

Conclusion: 이중 향상 능력은 진화하는 사이버 위협에 대처할 수 있는 적응형 방어 시스템 개발을 위한 유망한 해결책을 제공합니다.

Abstract: Intrusion Detection Systems (IDS) play a crucial role in network security
defense. However, a significant challenge for IDS in training detection models
is the shortage of adequately labeled malicious samples. To address these
issues, this paper introduces a novel semi-supervised framework
\textbf{GANGRL-LLM}, which integrates Generative Adversarial Networks (GANs)
with Large Language Models (LLMs) to enhance malicious code generation and SQL
Injection (SQLi) detection capabilities in few-sample learning scenarios.
Specifically, our framework adopts a collaborative training paradigm where: (1)
the GAN-based discriminator improves malicious pattern recognition through
adversarial learning with generated samples and limited real samples; and (2)
the LLM-based generator refines the quality of malicious code synthesis using
reward signals from the discriminator. The experimental results demonstrate
that even with a limited number of labeled samples, our training framework is
highly effective in enhancing both malicious code generation and detection
capabilities. This dual enhancement capability offers a promising solution for
developing adaptive defense systems capable of countering evolving cyber
threats.

</details>


### [30] [$AutoGuardX$: A Comprehensive Cybersecurity Framework for Connected Vehicles](https://arxiv.org/abs/2508.18155)
*Muhammad Ali Nadeem,Bishwo Prakash Pokharel,Naresh Kshetri,Achyut Shankar,Gokarna Sharma*

Main category: cs.CR

TL;DR: $AutoGuardX$는 연결된 차량을 위한 포괄적인 사이버 보안 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 연결된 차량의 증가로 인한 사이버 공격의 위험이 급증하고 있다.

Method: 기존 보안 표준과 머신러닝 기반 이상 탐지 기술 및 IoT 보안 프로토콜을 통합한 사이버 보안 프레임워크를 제안한다.

Result: 여러 자동차 브랜드의 차량을 대상으로 한 보안 시뮬레이션을 통해 프레임워크의 적응성 및 효과성을 평가했다.

Conclusion: $AutoGuardX$는 기존 및 새로운 위협에 대한 실질적인 효과를 보였다.

Abstract: The rapid integration of Internet of Things (IoT) and interconnected systems
in modern vehicles not only introduced a new era of convenience, automation,
and connected vehicles but also elevated their exposure to sophisticated cyber
threats. This is especially evident in US and Canada, where cyber-enabled auto
theft has surged in recent years, revealing the limitations of existing
security measures for connected vehicles. In response, this paper proposes
$AutoGuardX$, a comprehensive cybersecurity framework designed specifically for
connected vehicles. $AutoGuardX$ combines key elements from existing recognized
standards for vehicle security, such as ISO/SAE 21434 and ISO 26262, with
advanced technologies, including machine learning-based anomaly detection, IoT
security protocols, and encrypted communication channels. The framework
addresses major attack vectors like relay attacks, controller area network
(CAN) bus intrusions, and vulnerabilities introduced by emerging technologies
such as 5G and quantum computing. $AutoGuardX$ is extensively evaluated through
security simulations across a mix of Sedans and SUVs from four major vehicle
brands manufactured between 2019 and 2023. The results demonstrate the
framework's adaptability, scalability, and practical effectiveness against
existing and emerging threats.

</details>


### [31] [KillChainGraph: ML Framework for Predicting and Mapping ATT&CK Techniques](https://arxiv.org/abs/2508.18230)
*Chitraksh Singh,Monisha Dhanraj,Ken Huang*

Main category: cs.CR

TL;DR: 이 논문은 사이버 공격의 복잡성과 양이 증가함에 따라 전통적인 규칙 기반 시스템을 넘어서는 능동적인 탐지 전략을 요구하고 있으며, 이를 위해 사이버 킬 체인의 7단계를 통해 적대적 행동을 모방하는 다중 모델 기계 학습 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 사이버 공격의 증가하는 복잡성과 양은 전통적인 규칙 기반 시스템을 초월하는 능동적인 탐지 전략을 요구하고 있다.

Method: MITRE ATT&CK Enterprise 데이터 세트를 사용하여 사이버 킬 체인의 7단계에서 적대적 행동을 에뮬레이트하는 단계 인지 다중 모델 기계 학습 프레임워크를 제시한다. ATTACK-BERT를 통해 기술을 단계에 의미적으로 매핑하여 7개의 단계별 데이터 세트를 생성하며, LightGBM, 사용자 정의 Transformer 인코더, 미세 조정된 BERT 및 그래프 신경망(GNN)의 출력을 가중 소프트 투표 앙상블을 통해 통합한다.

Result: 앙상블은 일관되게 가장 높은 점수를 기록했으며, F1 점수는 97.47%에서 99.83%에 이르며, 각 단계에서 GNN 성능(97.36%에서 99.81%)을 0.03%~0.20% 초과했다.

Conclusion: 그래프 기반 앙상블 접근 방식은 해석 가능한 공격 경로 예측을 가능하게 하고 능동적인 사이버 방어를 강화한다.

Abstract: The escalating complexity and volume of cyberattacks demand proactive
detection strategies that go beyond traditional rule-based systems. This paper
presents a phase-aware, multi-model machine learning framework that emulates
adversarial behavior across the seven phases of the Cyber Kill Chain using the
MITRE ATT&CK Enterprise dataset. Techniques are semantically mapped to phases
via ATTACK-BERT, producing seven phase-specific datasets. We evaluate LightGBM,
a custom Transformer encoder, fine-tuned BERT, and a Graph Neural Network
(GNN), integrating their outputs through a weighted soft voting ensemble.
Inter-phase dependencies are modeled using directed graphs to capture attacker
movement from reconnaissance to objectives. The ensemble consistently achieved
the highest scores, with F1-scores ranging from 97.47% to 99.83%, surpassing
GNN performance (97.36% to 99.81%) by 0.03%--0.20% across phases. This
graph-driven, ensemble-based approach enables interpretable attack path
forecasting and strengthens proactive cyber defense.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Quantum-Inspired DRL Approach with LSTM and OU Noise for Cut Order Planning Optimization](https://arxiv.org/abs/2508.16611)
*Yulison Herry Chrisnanto,Julian Evan Chrisnanto*

Main category: cs.LG

TL;DR: 소매 주문 계획(COP)은 직물 산업에서 중요한 도전과제로, 원단 활용도와 생산 비용에 직접적인 영향을 미칩니다. 우리는 새로운 양자 영감 심층 강화 학습(QI-DRL) 프레임워크를 제안하여 Fabric 비용 절감 및 제조 효율성을 높이고자 합니다.


<details>
  <summary>Details</summary>
Motivation: 직물 산업에서의 소매 주문 계획은 Fabric 활용도와 생산 비용에 큰 영향을 미친다. 기존 방법은 동적 생산 환경에 적응하기 어렵다.

Method: Long Short-Term Memory (LSTM) 네트워크와 Ornstein-Uhlenbeck 노이즈를 통합한 QI-DRL 프레임워크를 사용하여 문제를 해결하였다.

Result: 1000개의 에피소드에서 훈련 후 0.81(-+0.03)의 평균 보상과 0.15(-+0.02)의 예측 손실로 안정적인 성능을 입증하였다. 전통적인 방법 대비 Fabric 비용을 최대 13% 절감하였다.

Conclusion: 제안된 프레임워크는 제조 효율성을 개선할 수 있는 가능성을 보여주며, 향후 COP 최적화 혁신을 위한 길을 열 것이다.

Abstract: Cut order planning (COP) is a critical challenge in the textile industry,
directly impacting fabric utilization and production costs. Conventional
methods based on static heuristics and catalog-based estimations often struggle
to adapt to dynamic production environments, resulting in suboptimal solutions
and increased waste. In response, we propose a novel Quantum-Inspired Deep
Reinforcement Learning (QI-DRL) framework that integrates Long Short-Term
Memory (LSTM) networks with Ornstein-Uhlenbeck noise. This hybrid approach is
designed to explicitly address key research questions regarding the benefits of
quantum-inspired probabilistic representations, the role of LSTM-based memory
in capturing sequential dependencies, and the effectiveness of OU noise in
facilitating smooth exploration and faster convergence. Extensive training over
1000 episodes demonstrates robust performance, with an average reward of 0.81
(-+0.03) and a steady decrease in prediction loss to 0.15 (-+0.02). A
comparative analysis reveals that the proposed approach achieves fabric cost
savings of up to 13% compared to conventional methods. Furthermore, statistical
evaluations indicate low variability and stable convergence. Despite the fact
that the simulation model makes several simplifying assumptions, these
promising results underscore the potential of the scalable and adaptive
framework to enhance manufacturing efficiency and pave the way for future
innovations in COP optimization.

</details>


### [33] [CrystalDiT: A Diffusion Transformer for Crystal Generation](https://arxiv.org/abs/2508.16614)
*Xiaohan Yi,Guikun Xu,Xi Xiao,Zhong Zhang,Liu Liu,Yatao Bian,Peilin Zhao*

Main category: cs.LG

TL;DR: CrystalDiT는 결정 구조 생성을 위한 차별화된 확산 변환기로, 복잡한 구조 대신 단일 변환기를 사용하여 성능을 극대화합니다.


<details>
  <summary>Details</summary>
Motivation: 결정 구조 생성을 위한 새로운 접근 방식을 제안하기 위해 복잡한 아키텍처에 의존하지 않고 성능을 극대화하는 방법을 찾으려는 동기에서 출발했습니다.

Method: CrystalDiT는 격자 및 원자 특성을 단일 상호의존 시스템으로 처리하는 강력한 유도 편향을 부여하는 통합 변환기를 사용합니다.

Result: 우리의 접근 방식은 MP-20에서 9.62% SUN(안정적이고 독특하며 새로운) 비율을 달성하며, FlowMM(4.38%) 및 MatterGen(3.42%)과 같은 최근 방법들을 뛰어넘습니다.

Conclusion: 간단하게 설계된 아키텍처가 과적합에 취약한 복잡한 대안을 초월할 수 있다는 결과를 보여줍니다.

Abstract: We present CrystalDiT, a diffusion transformer for crystal structure
generation that achieves state-of-the-art performance by challenging the trend
of architectural complexity. Instead of intricate, multi-stream designs,
CrystalDiT employs a unified transformer that imposes a powerful inductive
bias: treating lattice and atomic properties as a single, interdependent
system. Combined with a periodic table-based atomic representation and a
balanced training strategy, our approach achieves 9.62% SUN (Stable, Unique,
Novel) rate on MP-20, substantially outperforming recent methods including
FlowMM (4.38%) and MatterGen (3.42%). Notably, CrystalDiT generates 63.28%
unique and novel structures while maintaining comparable stability rates,
demonstrating that architectural simplicity can be more effective than
complexity for materials discovery. Our results suggest that in data-limited
scientific domains, carefully designed simple architectures outperform
sophisticated alternatives that are prone to overfitting.

</details>


### [34] [Leveraging the Christoffel Function for Outlier Detection in Data Streams](https://arxiv.org/abs/2508.16617)
*Kévin Ducharlet,Louise Travé-Massuyès,Jean-Bernard Lasserre,Marie-Véronique Le Lann,Youssef Miloudi*

Main category: cs.LG

TL;DR: 이 논문은 데이터 스트림에서 이상 탐지의 중요성과 이를 해결하기 위해 제안된 두 가지 새로운 방법인 DyCF와 DyCG를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 이상 탐지는 데이터 품질 유지와 결함 감지를 위해 필수적이며, 데이터 스트림의 비정상적 분포와 급증하는 데이터 양으로 인해 어려움이 발생합니다.

Method: DyCF와 DyCG는 각각 크리스토펠 함수의 원리에 기반하여 개발되었으며, DyCF는 근사 이론에서 해당 함수를 활용하고 DyCG는 그 성장 특성에 의존하여 파라미터 조정이 필요 없습니다.

Result: 비교 결과, DyCF는 세밀한 조정 방법보다 뛰어난 수행 시간과 메모리 사용으로 우수한 성능을 보여 주었으며, DyCG는 조정이 필요 없다는 상당한 장점을 가지고 있지만 상대적으로 성능은 낮았습니다.

Conclusion: 이 두 방법은 데이터 스트림 처리 요구에 부합하는 알기 쉬운 대수적 구조를 가지고 있으며, 실용적 응용에 기여할 수 있습니다.

Abstract: Outlier detection holds significant importance in the realm of data mining,
particularly with the growing pervasiveness of data acquisition methods. The
ability to identify outliers in data streams is essential for maintaining data
quality and detecting faults. However, dealing with data streams presents
challenges due to the non-stationary nature of distributions and the
ever-increasing data volume. While numerous methods have been proposed to
tackle this challenge, a common drawback is the lack of straightforward
parameterization in many of them. This article introduces two novel methods:
DyCF and DyCG. DyCF leverages the Christoffel function from the theory of
approximation and orthogonal polynomials. Conversely, DyCG capitalizes on the
growth properties of the Christoffel function, eliminating the need for tuning
parameters. Both approaches are firmly rooted in a well-defined algebraic
framework, meeting crucial demands for data stream processing, with a specific
focus on addressing low-dimensional aspects and maintaining data history
without memory cost. A comprehensive comparison between DyCF, DyCG, and
state-of-the-art methods is presented, using both synthetic and real industrial
data streams. The results show that DyCF outperforms fine-tuning methods,
offering superior performance in terms of execution time and memory usage. DyCG
performs less well, but has the considerable advantage of requiring no tuning
at all.

</details>


### [35] [STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction with Future Spatiotemporal Contexts](https://arxiv.org/abs/2508.16620)
*Bangchao Deng,Lianhua Ji,Chunhua Chen,Xin Jing,Ling Ding,Bingqing QU,Pengyang Wang,Dingqi Yang*

Main category: cs.LG

TL;DR: 다음 위치 예측은 인간 이동 모델링에서 중요한 과제로, 여행 계획 및 도시 이동 관리와 같은 응용 프로그램을 가능하게 한다. 본 논문에서는 미래의 공간-시간 문맥을 명시적으로 모델링하여 다양한 위치 예측 모델의 성능을 향상시키는 보편적인 공간-시간 릴레이 프레임워크인 STRelay를 제안한다. STRelay는 네 가지 실제 궤적 데이터 세트에서 네 가지 최첨단 위치 예측 모델과 통합되어 평가되었고, 모든 경우에서 3.19%-11.56%의 예측 성능 향상을 보여주었다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들이 주로 과거의 공간-시간 궤적 데이터를 이용해 미래의 위치를 예측하는 데 초점을 맞춰 왔지만, 미래의 공간-시간 문맥의 중요성을 간과하고 있다는 문제를 해결하고자 했다.

Method: STRelay는 인간 궤적을 바탕으로 미래의 공간-시간 문맥을 명시적으로 모델링하는 프레임워크로, 이전의 위치 예측 모델의 인코딩된 역사적 표현과 통합해 다중 작업 학습을 가능하게 한다.

Result: STRelay를 네 가지 실제 궤적 데이터 세트에서 네 가지 최첨단 위치 예측 모델과 통합하여 평가한 결과, 모든 경우에서 3.19%-11.56%의 예측 성능 향상이 있었다.

Conclusion: 특히, 미래의 공간-시간 문맥은 오락 관련 위치와 더 긴 거리를 선호하는 사용자 그룹에게 매우 유용하며, 이러한 비일상적인 활동에서 성능 향상이 발생했다.

Abstract: Next location prediction is a critical task in human mobility modeling,
enabling applications like travel planning and urban mobility management.
Existing methods mainly rely on historical spatiotemporal trajectory data to
train sequence models that directly forecast future locations. However, they
often overlook the importance of the future spatiotemporal contexts, which are
highly informative for the future locations. For example, knowing how much time
and distance a user will travel could serve as a critical clue for predicting
the user's next location. Against this background, we propose \textbf{STRelay},
a universal \textbf{\underline{S}}patio\textbf{\underline{T}}emporal
\textbf{\underline{Relay}}ing framework explicitly modeling the future
spatiotemporal context given a human trajectory, to boost the performance of
different location prediction models. Specifically, STRelay models future
spatiotemporal contexts in a relaying manner, which is subsequently integrated
with the encoded historical representation from a base location prediction
model, enabling multi-task learning by simultaneously predicting the next time
interval, next moving distance interval, and finally the next location. We
evaluate STRelay integrated with four state-of-the-art location prediction base
models on four real-world trajectory datasets. Results demonstrate that STRelay
consistently improves prediction performance across all cases by
3.19\%-11.56\%. Additionally, we find that the future spatiotemporal contexts
are particularly helpful for entertainment-related locations and also for user
groups who prefer traveling longer distances. The performance gain on such
non-daily-routine activities, which often suffer from higher uncertainty, is
indeed complementary to the base location prediction models that often excel at
modeling regular daily routine patterns.

</details>


### [36] [A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction](https://arxiv.org/abs/2508.16623)
*Weilin Ruan,Xilin Dang,Ziyu Zhou,Sisuo Lyu,Yuxuan Liang*

Main category: cs.LG

TL;DR: RAST는 교통 예측의 한계를 극복하기 위한 보편적인 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 교통 예측은 현대 지능형 교통 시스템의 핵심 요소이며 시공간 예측에서 중요한 작업입니다. 그러나 복잡한 시공간 종속성을 모델링할 때의 제한된 맥락 용량과 이질적인 패턴으로 인한 세밀한 시공간 포인트에서의 낮은 예측 가능성이 남아있습니다.

Method: RAST는 RAG에서 영감을 얻어 시공간 모델링과 검색 증강 메커니즘을 통합한 프레임워크입니다. 주요 설계에는 분리된 공간 및 시간 특징을 캡처하고 잔여 융합을 통해 융합 쿼리를 구성하는 분리된 인코더 및 쿼리 생성기, 벡터화된 세밀한 패턴을 유지하고 검색하는 시공간 검색 저장소 및 검색기, 사전 훈련된 STGNN 또는 단순 MLP 예측기를 유연하게 수용하는 보편적 백본 예측기가 포함됩니다.

Result: 여섯 개의 실제 교통 네트워크에서의 광범위한 실험 결과, RAST는 뛰어난 성능을 달성하면서도 계산 효율성을 유지함을 보여주었습니다.

Conclusion: 결론적으로, RAST는 복잡한 시공간 의존성 모델링을 위한 유용한 접근법을 제공하며, 교통 예측의 정확성을 향상시킵니다.

Abstract: Traffic prediction is a cornerstone of modern intelligent transportation
systems and a critical task in spatio-temporal forecasting. Although advanced
Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have
achieved significant progress in traffic prediction, two key challenges remain:
(i) limited contextual capacity when modeling complex spatio-temporal
dependencies, and (ii) low predictability at fine-grained spatio-temporal
points due to heterogeneous patterns. Inspired by Retrieval-Augmented
Generation (RAG), we propose RAST, a universal framework that integrates
retrieval-augmented mechanisms with spatio-temporal modeling to address these
challenges. Our framework consists of three key designs: 1) Decoupled Encoder
and Query Generator to capture decoupled spatial and temporal features and
construct a fusion query via residual fusion; 2) Spatio-temporal Retrieval
Store and Retrievers to maintain and retrieve vectorized fine-grained patterns;
and 3) Universal Backbone Predictor that flexibly accommodates pre-trained
STGNNs or simple MLP predictors. Extensive experiments on six real-world
traffic networks, including large-scale datasets, demonstrate that RAST
achieves superior performance while maintaining computational efficiency.

</details>


### [37] [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
*Zeyu Zhang,Quanyu Dai,Rui Li,Xiaohe Bo,Xu Chen,Zhenhua Dong*

Main category: cs.LG

TL;DR: 본 논문에서는 LLM 기반 에이전트의 메모리 성능을 최적화하기 위한 적응형 데이터 기반 메모리 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 LLM 기반 에이전트의 메모리 메커니즘이 인력의 수작업으로 정의되어 있어 노동 비용이 높고 성능이 저조하다는 문제를 해결하고자 함.

Method: 메모리 주기를 모델링하여 적응형 데이터 기반 메모리 프레임워크를 최적화하고, MoE 게이트 함수 설계 및 학습 가능한 집계 프로세스를 도입하여 메모리 활용도를 개선.

Result: 우리가 제안한 메모리 프레임워크는 LLM 기반 에이전트가 특정 환경에서 정보를 효과적으로 기억하는 방법을 학습하도록 한다.

Conclusion: 제안된 방법의 효과를 평가하기 위해 여러 측면에서 포괄적인 실험을 수행하였으며, 연구 공동체를 위해 프로젝트를 공개하였다.

Abstract: LLM-based agents have been extensively applied across various domains, where
memory stands out as one of their most essential capabilities. Previous memory
mechanisms of LLM-based agents are manually predefined by human experts,
leading to higher labor costs and suboptimal performance. In addition, these
methods overlook the memory cycle effect in interactive scenarios, which is
critical to optimizing LLM-based agents for specific environments. To address
these challenges, in this paper, we propose to optimize LLM-based agents with
an adaptive and data-driven memory framework by modeling memory cycles.
Specifically, we design an MoE gate function to facilitate memory retrieval,
propose a learnable aggregation process to improve memory utilization, and
develop task-specific reflection to adapt memory storage. Our memory framework
empowers LLM-based agents to learn how to memorize information effectively in
specific environments, with both off-policy and on-policy optimization. In
order to evaluate the effectiveness of our proposed methods, we conduct
comprehensive experiments across multiple aspects. To benefit the research
community in this area, we release our project at
https://github.com/nuster1128/learn_to_memorize.

</details>


### [38] [Recurrent Transformer U-Net Surrogate for Flow Modeling and Data Assimilation in Subsurface Formations with Faults](https://arxiv.org/abs/2508.16631)
*Yifu Han,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 본 연구에서는 고속 예측을 제공하는 새로운 회귀 변환기 U-Net 대체 모델을 개발하였습니다.


<details>
  <summary>Details</summary>
Motivation: 지질학적 탄소 저장을 위해 고려되는 많은 지하 구조물에는 유체 흐름에 강한 영향을 미칠 수 있는 광범위한 단층이 포함되어 있습니다.

Method: 현실적인 단층 지하 수자원 시스템에서 압력과 CO2 포화도에 대한 예측을 제공하는 새로운 회귀 변환기 U-Net 대체 모델을 개발했습니다. 모델은 목표 수자원과 주변 지역, 캡락, 두 개의 광범위한 단층, 두 개의 피복 수자원을 포함합니다.

Result: 기존의 회귀 잔차 U-Net보다 더 정확하며, 질적으로 다른 누출 시나리오에서도 정확도를 유지합니다.

Conclusion: 모델을 사용하여 글로벌 민감도 분석 및 데이터 동화를 수행하였고, 다양한 모니터링 전략의 효과를 보여줍니다.

Abstract: Many subsurface formations, including some of those under consideration for
large-scale geological carbon storage, include extensive faults that can
strongly impact fluid flow. In this study, we develop a new recurrent
transformer U-Net surrogate model to provide very fast predictions for pressure
and CO2 saturation in realistic faulted subsurface aquifer systems. The
geomodel includes a target aquifer (into which supercritical CO2 is injected),
surrounding regions, caprock, two extensive faults, and two overlying aquifers.
The faults can act as leakage pathways between the three aquifers. The
heterogeneous property fields in the target aquifer are characterized by
hierarchical uncertainty, meaning both the geological metaparameters (e.g.,
mean and standard deviation of log-permeability) and the detailed cell
properties of each realization, are uncertain. Fault permeabilities are also
treated as uncertain. The model is trained with simulation results for (up to)
4000 randomly sampled realizations. Error assessments show that this model is
more accurate than a previous recurrent residual U-Net, and that it maintains
accuracy for qualitatively different leakage scenarios. The new surrogate is
then used for global sensitivity analysis and data assimilation. A hierarchical
Markov chain Monte Carlo data assimilation procedure is applied. Different
monitoring strategies, corresponding to different amounts and types of observed
data collected at monitoring wells, are considered for three synthetic true
models. Detailed results demonstrate the degree of uncertainty reduction
achieved with the various monitoring strategies. Posterior results for 3D
saturation plumes and leakage volumes indicate the benefits of measuring
pressure and saturation in all three aquifers.

</details>


### [39] [How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System](https://arxiv.org/abs/2508.17215)
*Kaiwen Zuo,Zelin Liu,Raman Dutt,Ziyang Wang,Zhongtian Sun,Yeming Wang,Fan Mo,Pietro Liò*

Main category: cs.LG

TL;DR: 의료 AI에서 외부 임상 이미지-텍스트 검색을 통한 사실 기반 강화를 위해 사용되는 대형 비전-언어 모델(LVLM)과 검색 증강 생성(RAG)의 의존은 심각한 공격 표면을 창출한다. 본 논문에서는 적대적 이미지-텍스트 쌍 주입을 통해 의료 RAG 시스템의 취약성을 체계적으로 조사하는 새로운 다중 모드 오염 프레임워크인 MedThreatRAG를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 의료 AI에서 LVLM과 RAG의 사용이 증가하고 있으나, 이로 인해 발생하는 공격 벡터에 대한 우려가 커지고 있다.

Method: MedThreatRAG는 사용자 또는 파이프라인 기여를 통해 주기적인 지식 기반 업데이트를 허용하는 실제 의료 시스템을 모방한 시뮬레이션된 반개방 공격 환경을 구축하고, 의료 이미지와 그에 대응하는 보고서 간의 미묘한 의미적 모순을 주입하는 Cross-Modal Conflict Injection(CMCI)을 도입한다.

Result: MedThreatRAG는 IU-Xray 및 MIMIC-CXR QA 작업에서 답변 F1 점수를 최대 27.66%까지 감소시키고 LLaVA-Med-1.5 F1 점수를 51.36%까지 낮춘다.

Conclusion: 의료 RAG 시스템의 근본적인 보안 취약점이 드러났으며, 위협 인식 설계와 강력한 다중 모드 일관성 체크의 긴급한 필요성이 강조된다. 또한, 향후 다중 모드 의료 RAG 시스템의 안전한 개발을 위한 간결한 가이드라인을 제시한다.

Abstract: Large Vision-Language Models (LVLMs) augmented with Retrieval-Augmented
Generation (RAG) are increasingly employed in medical AI to enhance factual
grounding through external clinical image-text retrieval. However, this
reliance creates a significant attack surface. We propose MedThreatRAG, a novel
multimodal poisoning framework that systematically probes vulnerabilities in
medical RAG systems by injecting adversarial image-text pairs. A key innovation
of our approach is the construction of a simulated semi-open attack
environment, mimicking real-world medical systems that permit periodic
knowledge base updates via user or pipeline contributions. Within this setting,
we introduce and emphasize Cross-Modal Conflict Injection (CMCI), which embeds
subtle semantic contradictions between medical images and their paired reports.
These mismatches degrade retrieval and generation by disrupting cross-modal
alignment while remaining sufficiently plausible to evade conventional filters.
While basic textual and visual attacks are included for completeness, CMCI
demonstrates the most severe degradation. Evaluations on IU-Xray and MIMIC-CXR
QA tasks show that MedThreatRAG reduces answer F1 scores by up to 27.66% and
lowers LLaVA-Med-1.5 F1 rates to as low as 51.36%. Our findings expose
fundamental security gaps in clinical RAG systems and highlight the urgent need
for threat-aware design and robust multimodal consistency checks. Finally, we
conclude with a concise set of guidelines to inform the safe development of
future multimodal medical RAG systems.

</details>


### [40] [Adaptive Variance-Penalized Continual Learning with Fisher Regularization](https://arxiv.org/abs/2508.16632)
*Krisanu Sarkar*

Main category: cs.LG

TL;DR: 신경망에서의 재앙적 망각 문제를 해결하기 위해 지속적 학습의 새로운 프레임워크를 제시합니다. 이 방법은 변동 학습 패러다임 내에서 파라미터 분산의 피셔 가중 비대칭 정규화를 통합하여 안정성과 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 신경망에서의 재앙적 망각 문제는 지속적 학습 분야에서 많은 연구를 유도했습니다.

Method: 피셔 가중 비대칭 정규화와 변동 학습 패러다임을 통합한 지속적 학습 프레임워크를 제시합니다. 이 방법은 파라미터 불확실성에 따라 정규화 강도를 동적으로 조절합니다.

Result: SplitMNIST, PermutedMNIST 및 SplitFashionMNIST를 포함한 표준 지속적 학습 벤치마크에서 기존 접근 방식보다 상당한 개선을 입증하였습니다.

Conclusion: 우리의 접근 방식은 즉각적인 작업 성능을 향상시킬 뿐 아니라 시간에 따른 지식 저하를 크게 완화시켜 신경망의 재앙적 망각 문제를 효과적으로 해결합니다.

Abstract: The persistent challenge of catastrophic forgetting in neural networks has
motivated extensive research in continual learning . This work presents a novel
continual learning framework that integrates Fisher-weighted asymmetric
regularization of parameter variances within a variational learning paradigm.
Our method dynamically modulates regularization intensity according to
parameter uncertainty, achieving enhanced stability and performance.
Comprehensive evaluations on standard continual learning benchmarks including
SplitMNIST, PermutedMNIST, and SplitFashionMNIST demonstrate substantial
improvements over existing approaches such as Variational Continual Learning
and Elastic Weight Consolidation . The asymmetric variance penalty mechanism
proves particularly effective in maintaining knowledge across sequential tasks
while improving model accuracy. Experimental results show our approach not only
boosts immediate task performance but also significantly mitigates knowledge
degradation over time, effectively addressing the fundamental challenge of
catastrophic forgetting in neural networks

</details>


### [41] [MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems](https://arxiv.org/abs/2508.17341)
*Muhammet Anil Yagiz,Zeynep Sude Cengiz,Polat Goktas*

Main category: cs.LG

TL;DR: MetaFed는 메타버스 환경을 위한 지속 가능하고 지능적인 자원 조정을 가능하게 하는 탈중앙화 연합 학습 프레임워크로, 탄소 배출을 줄이고 높은 정확도를 유지합니다.


<details>
  <summary>Details</summary>
Motivation: 메타버스 애플리케이션의 급속한 확장은 성능, 개인 정보 보호 및 환경 지속 가능성의 교차점에서 복잡한 문제를 야기합니다.

Method: MetaFed는 동적 클라이언트 선택을 위한 다중 에이전트 강화 학습, 동형 암호화를 사용한 개인 정보 보호 연합 학습, 재생 가능 에너지 가용성과 정렬된 탄소 인식 스케줄링을 통합합니다.

Result: MNIST와 CIFAR-10에서 경량 ResNet 아키텍처를 사용한 평가에서는 MetaFed가 기존 접근 방식에 비해 탄소 배출량을 최대 25% 줄이는 동시에 높은 정확도와 최소한의 통신 오버헤드를 유지하는 것으로 나타났습니다.

Conclusion: 이 결과는 MetaFed가 환경적으로 책임 있는 개인 정보 보호 준수 메타버스 인프라를 구축하기 위한 확장 가능한 솔루션임을 강조합니다.

Abstract: The rapid expansion of immersive Metaverse applications introduces complex
challenges at the intersection of performance, privacy, and environmental
sustainability. Centralized architectures fall short in addressing these
demands, often resulting in elevated energy consumption, latency, and privacy
concerns. This paper proposes MetaFed, a decentralized federated learning (FL)
framework that enables sustainable and intelligent resource orchestration for
Metaverse environments. MetaFed integrates (i) multi-agent reinforcement
learning for dynamic client selection, (ii) privacy-preserving FL using
homomorphic encryption, and (iii) carbon-aware scheduling aligned with
renewable energy availability. Evaluations on MNIST and CIFAR-10 using
lightweight ResNet architectures demonstrate that MetaFed achieves up to 25\%
reduction in carbon emissions compared to conventional approaches, while
maintaining high accuracy and minimal communication overhead. These results
highlight MetaFed as a scalable solution for building environmentally
responsible and privacy-compliant Metaverse infrastructures.

</details>


### [42] [A Novel Unified Extended Matrix for Graph Signal Processing: Theory and Application](https://arxiv.org/abs/2508.16633)
*Yunyan Zheng,Zhichao Zhang,Wei Yao*

Main category: cs.LG

TL;DR: 이 논문은 비인접 노드 간 의존성을 모델링하는 유연성을 향상시키기 위해 통합 확장 행렬(UEM) 구조를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 그래프 신호 처리는 불규칙한 도메인에 구조화된 데이터를 분석하는 데 필수 도구가 되었다. 하지만 기존 그래프 시프트 연산자는 비인접 노드 간의 의존성을 모델링하는 데 유연성이 부족하여 복잡한 그래프 구조를 표현하는 능력이 제한되어 있다.

Method: 이 논문은 확장 인접 행렬과 통합 그래프 표현 행렬을 매개변수 설계를 통해 통합하는 UEM 프레임워크를 제안한다. 그 후, UEM을 기반으로 한 그래프 푸리에 변환(UEM-GFT)을 제안하여 스펙트럼 특성을 적응적으로 조정하여 신호 처리 성능을 향상시킨다.

Result: 합성 데이터세트 및 실제 데이터세트에 대한 실험 결과, UEM-GFT가 이상 탐지 작업에서 기존 GSO 기반 방법보다 성능이 우수함을 보여준다.

Conclusion: UEM-GFT는 다양한 네트워크 토폴로지에서 우수한 성능을 달성한다.

Abstract: Graph signal processing has become an essential tool for analyzing data
structured on irregular domains. While conventional graph shift operators
(GSOs) are effective for certain tasks, they inherently lack flexibility in
modeling dependencies between non-adjacent nodes, limiting their ability to
represent complex graph structures. To address this limitation, this paper
proposes the unified extended matrix (UEM) framework, which integrates the
extended-adjacency matrix and the unified graph representation matrix through
parametric design, so as to be able to flexibly adapt to different graph
structures and reveal more graph signal information. Theoretical analysis of
the UEM is conducted, demonstrating positive semi-definiteness and eigenvalue
monotonicity under specific conditions. Then, we propose graph Fourier
transform based on UEM (UEM-GFT), which can adaptively tune spectral properties
to enhance signal processing performance. Experimental results on synthetic and
real-world datasets demonstrate that the UEM-GFT outperforms existing GSO-based
methods in anomaly detection tasks, achieving superior performance across
varying network topologies.

</details>


### [43] [Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias](https://arxiv.org/abs/2508.17361)
*Shir Bernstein,David Beste,Daniel Ayzenshteyn,Lea Schonherr,Yisroel Mirsky*

Main category: cs.LG

TL;DR: 이 논문에서는 LLM 기반 코드 분석의 중요한 취약점인 추상화 편향을 식별하고 이를 이용하여 Familiar Pattern Attack(FPA)를 제안합니다. 우리는 이 공격을 통해 LLM의 제어 흐름을 저해할 수 있는 방법을 개발하였습니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 자동화된 코드 리뷰와 정적 분석을 수행하는 데 점점 더 신뢰받고 있으며, 이 과정에서 발생하는 잠재적 취약점을 밝히기 위해서이다.

Method: 전적으로 자동화된 블랙박스 알고리즘을 개발하여 목표 코드에 FPA를 발견하고 주입하는 방법을 사용한다.

Result: FPA는 효과적일 뿐만 아니라 여러 모델(GPT-4o, Claude 3.5, Gemini 2.0)과 다양한 프로그래밍 언어(Python, C, Rust, Go) 간에도 전이 가능하다는 것을 보여준다.

Conclusion: FPA는 공격에 대한 경고에도 불구하고 효과를 유지하며, LLM의 신뢰성과 안전성을 높이기 위한 긍정적인 방어적 사용도 모색할 수 있다.

Abstract: Large Language Models (LLMs) are increasingly trusted to perform automated
code review and static analysis at scale, supporting tasks such as
vulnerability detection, summarization, and refactoring. In this paper, we
identify and exploit a critical vulnerability in LLM-based code analysis: an
abstraction bias that causes models to overgeneralize familiar programming
patterns and overlook small, meaningful bugs. Adversaries can exploit this
blind spot to hijack the control flow of the LLM's interpretation with minimal
edits and without affecting actual runtime behavior. We refer to this attack as
a Familiar Pattern Attack (FPA).
  We develop a fully automated, black-box algorithm that discovers and injects
FPAs into target code. Our evaluation shows that FPAs are not only effective,
but also transferable across models (GPT-4o, Claude 3.5, Gemini 2.0) and
universal across programming languages (Python, C, Rust, Go). Moreover, FPAs
remain effective even when models are explicitly warned about the attack via
robust system prompts. Finally, we explore positive, defensive uses of FPAs and
discuss their broader implications for the reliability and safety of
code-oriented LLMs.

</details>


### [44] [Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations](https://arxiv.org/abs/2508.16634)
*Zhendong Yang,Jie Wang,Liansong Zong,Xiaorong Liu,Quan Qian,Shiqian Chen*

Main category: cs.LG

TL;DR: 이 논문은 제한된 샘플로 새로운 결함 클래스를 지속적으로 학습하는 데 중점을 두며, Dual-Granularity Guidance Network (DGGN)이라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 산업 시스템에서 새로운 결함 클래스를 적은 샘플로 지속적으로 학습하면서 이전 지식을 잊지 않는 것이 중요합니다.

Method: DGGN은 두 개의 병렬 스트림으로 특징 학습을 분리합니다: 미세한 표현과 거친 표현으로, 각기 다른 방식으로 지식을 학습하고 보존합니다.

Result: TEP 벤치마크와 실제 MFF 데이터세트에서 DGGN이 최신 FSC-FD 접근 방식보다 우수한 진단 성능과 안정성을 보였습니다.

Conclusion: 제안한 DGGN은 산업 시스템의 결함 진단에서 중요한 이점을 제공합니다.

Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to
continuously learn from new fault classes with only a few samples without
forgetting old ones, is critical for real-world industrial systems. However,
this challenging task severely amplifies the issues of catastrophic forgetting
of old knowledge and overfitting on scarce new data. To address these
challenges, this paper proposes a novel framework built upon Dual-Granularity
Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN
explicitly decouples feature learning into two parallel streams: 1) a
fine-grained representation stream, which utilizes a novel Multi-Order
Interaction Aggregation module to capture discriminative, class-specific
features from the limited new samples. 2) a coarse-grained representation
stream, designed to model and preserve general, class-agnostic knowledge shared
across all fault types. These two representations are dynamically fused by a
multi-semantic cross-attention mechanism, where the stable coarse-grained
knowledge guides the learning of fine-grained features, preventing overfitting
and alleviating feature conflicts. To further mitigate catastrophic forgetting,
we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a
decoupled Balanced Random Forest classifier is employed to counter the decision
boundary bias caused by data imbalance. Extensive experiments on the TEP
benchmark and a real-world MFF dataset demonstrate that our proposed DGGN
achieves superior diagnostic performance and stability compared to
state-of-the-art FSC-FD approaches. Our code is publicly available at
https://github.com/MentaY/DGGN

</details>


### [45] [FRAME : Comprehensive Risk Assessment Framework for Adversarial Machine Learning Threats](https://arxiv.org/abs/2508.17405)
*Avishag Shapira,Simon Shigol,Asaf Shabtai*

Main category: cs.LG

TL;DR: FRAME은 다양한 ML 기반 시스템에서 AML 위험을 평가하기 위한 포괄적이고 자동화된 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 시스템의 보급 증가로 인해 보안에 대한 관심이 높아지고 있으며, 기본적인 취약점을 악용하는 적대적 기계 학습 기술(AML)의 출현으로 ML 기반 시스템에 대한 포괄적인 위험 평가의 필요성이 대두되었다.

Method: FRAME은 목표 시스템의 배포 환경, 다양한 AML 기술의 특성, 기존 연구로부터의 경험적 통찰을 평가하여 AML 위험을 정량화하는 새로운 위험 평가 방법을 포함한다.

Result: FRAME을 통해 조직은 AML 위험을 우선시할 수 있으며, 실제 환경에서 안전한 AI 배포를 지원한다.

Conclusion: FRAME은 단순한 기술 지식만으로도 시스템 소유자가 직접 활용할 수 있는 실행 가능한 결과를 제공하며, 실세계의 다양한 응용 프로그램에서 검증되었다.

Abstract: The widespread adoption of machine learning (ML) systems increased attention
to their security and emergence of adversarial machine learning (AML)
techniques that exploit fundamental vulnerabilities in ML systems, creating an
urgent need for comprehensive risk assessment for ML-based systems. While
traditional risk assessment frameworks evaluate conventional cybersecurity
risks, they lack ability to address unique challenges posed by AML threats.
Existing AML threat evaluation approaches focus primarily on technical attack
robustness, overlooking crucial real-world factors like deployment
environments, system dependencies, and attack feasibility. Attempts at
comprehensive AML risk assessment have been limited to domain-specific
solutions, preventing application across diverse systems. Addressing these
limitations, we present FRAME, the first comprehensive and automated framework
for assessing AML risks across diverse ML-based systems. FRAME includes a novel
risk assessment method that quantifies AML risks by systematically evaluating
three key dimensions: target system's deployment environment, characteristics
of diverse AML techniques, and empirical insights from prior research. FRAME
incorporates a feasibility scoring mechanism and LLM-based customization for
system-specific assessments. Additionally, we developed a comprehensive
structured dataset of AML attacks enabling context-aware risk assessment. From
an engineering application perspective, FRAME delivers actionable results
designed for direct use by system owners with only technical knowledge of their
systems, without expertise in AML. We validated it across six diverse
real-world applications. Our evaluation demonstrated exceptional accuracy and
strong alignment with analysis by AML experts. FRAME enables organizations to
prioritize AML risks, supporting secure AI deployment in real-world
environments.

</details>


### [46] [Enhancing Transformer-Based Foundation Models for Time Series Forecasting via Bagging, Boosting and Statistical Ensembles](https://arxiv.org/abs/2508.16641)
*Dhruv D. Modi,Rong Pan*

Main category: cs.LG

TL;DR: 이 논문은 시간 시계열 예측의 견고성과 정확성을 향상시키기 위해 다양한 통계 및 앙상블 기반 기술을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 시간 시계열 모델들은 실제 운영 데이터에서 예측의 변동성, 도메인 특유의 편향 및 제한된 불확실성 정량화 문제를 겪고 있습니다.

Method: 부트스트랩 기반 배깅, 회귀 기반 스태킹, 예측 구간 구성, 통계적 잔차 모델링, 반복 오류 피드백을 포함하는 기술들을 적용했습니다.

Result: 제안된 하이브리드 모델들이 여러 시간대에서 기존의 단독 모델들보다 일관되게 더 우수한 성능을 보였습니다.

Conclusion: 현대의 기초 모델에 통계적 접근법을 통합함으로써 실제 시간 시계열 응용 분야에서 정확성, 신뢰성 및 해석 가능성에서 유의미한 향상이 이루어졌음을 보여줍니다.

Abstract: Time series foundation models (TSFMs) such as Lag-Llama, TimeGPT, Chronos,
MOMENT, UniTS, and TimesFM have shown strong generalization and zero-shot
capabilities for time series forecasting, anomaly detection, classification,
and imputation. Despite these advantages, their predictions still suffer from
variance, domain-specific bias, and limited uncertainty quantification when
deployed on real operational data. This paper investigates a suite of
statistical and ensemble-based enhancement techniques, including
bootstrap-based bagging, regression-based stacking, prediction interval
construction, statistical residual modeling, and iterative error feedback, to
improve robustness and accuracy. Using the Belgium Electricity Short-Term Load
Forecasting dataset as a case study, we demonstrate that the proposed hybrids
consistently outperform standalone foundation models across multiple horizons.
Regression-based ensembles achieve the lowest mean squared error; bootstrap
aggregation markedly reduces long-context errors; residual modeling corrects
systematic bias; and the resulting prediction intervals achieve near nominal
coverage with widths shrinking as context length increases. The results
indicate that integrating statistical reasoning with modern foundation models
yields measurable gains in accuracy, reliability, and interpretability for
real-world time series applications.

</details>


### [47] [From Classical Probabilistic Latent Variable Models to Modern Generative AI: A Unified Perspective](https://arxiv.org/abs/2508.16643)
*Tianhua Chen*

Main category: cs.LG

TL;DR: 이 논문은 생성 인공지능의 고전적 및 현대적 방법을 확률론적 잠재 변수 모델의 관점에서 통합적으로 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 생성 인공지능 시스템의 다양한 아키텍처들을 이해하고 통합된 시각을 제공하기 위함입니다.

Method: 고전적 평면 모델에서부터 현대의 심층 아키텍처까지, 다양한 생성 방법을 PLVM 패러다임으로 프레임화하여 추적합니다.

Result: PLVM의 공통 확률론적 분류를 통해 공유 원칙, 독특한 추론 전략, 그리고 각각의 강점을 규정하는 표현적 절충점을 드러냅니다.

Conclusion: 생성 인공지능의 이론적 기초를 정리하고 향후 혁신을 위한 방법론적 계보를 명확히 하는 개념적 로드맵을 제공합니다.

Abstract: From large language models to multi-modal agents, Generative Artificial
Intelligence (AI) now underpins state-of-the-art systems. Despite their varied
architectures, many share a common foundation in probabilistic latent variable
models (PLVMs), where hidden variables explain observed data for density
estimation, latent reasoning, and structured inference. This paper presents a
unified perspective by framing both classical and modern generative methods
within the PLVM paradigm. We trace the progression from classical flat models
such as probabilistic PCA, Gaussian mixture models, latent class analysis, item
response theory, and latent Dirichlet allocation, through their sequential
extensions including Hidden Markov Models, Gaussian HMMs, and Linear Dynamical
Systems, to contemporary deep architectures: Variational Autoencoders as Deep
PLVMs, Normalizing Flows as Tractable PLVMs, Diffusion Models as Sequential
PLVMs, Autoregressive Models as Explicit Generative Models, and Generative
Adversarial Networks as Implicit PLVMs. Viewing these architectures under a
common probabilistic taxonomy reveals shared principles, distinct inference
strategies, and the representational trade-offs that shape their strengths. We
offer a conceptual roadmap that consolidates generative AI's theoretical
foundations, clarifies methodological lineages, and guides future innovation by
grounding emerging architectures in their probabilistic heritage.

</details>


### [48] [AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training](https://arxiv.org/abs/2508.16647)
*Boran Zhao,Hetian Liu,Zihang Yuan,Li Zhu,Fan Yang,Lina Xie Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: 엣지 장치에서 딥 뉴럴 네트워크(DNN)를 훈련하는 것은 도메인 적응 및 개인 정보 보호와 같은 문제를 해결할 수 있는 유망한 솔루션으로 주목받고 있다. 그러나 기존 DNN 훈련은 대규모 데이터셋을 요구하여 엣지 장치에 과도한 부담을 주고 있다. 이를 해결하기 위해, DNN 없이 데이터셋 샘플링을 위한 방법인 NMS가 제안되었다. 그러나 NMS는 두 가지 제한점이 있다: 검색 방법과 비단조성 성질의 불일치로 인한 이상치 출현, 그리고 주요 매개변수의 경험적 선택으로 인한 불균형 샘플링이다. 이를 해결하기 위해 AdapSNE를 제안하며, 이는 이상치를 억제하는 비단조성 검색 방법인 FWA를 통합하고 균일한 샘플링을 보장하기 위해 엔트로피 기반 최적화를 사용한다.


<details>
  <summary>Details</summary>
Motivation: 엣지 장치에서 DNN 훈련의 필요성과 기존 방법의 한계를 극복하여 더 나은 성능을 달성하기 위해.

Method: NMS의 한계를 극복하기 위해 AdapSNE를 제안하며, 이는 FWA를 통한 비단조성 검색과 엔트로피 기반 최적화를 결합하였다.

Result: AdapSNE는 더 일관된 샘플링을 통해 정확도를 향상시켰으며, 에지 측의 비용을 줄이는 맞춤형 데이터 흐름과 시간 다중화를 설계하였다.

Conclusion: AdapSNE는 정확도를 높이고 에지 장치의 훈련 에너지를 줄이는 효과적인 방법이다.

Abstract: Training deep neural networks (DNNs) directly on edge devices has attracted
increasing attention, as it offers promising solutions to challenges such as
domain adaptation and privacy preservation. However, conventional DNN training
typically requires large-scale datasets, which imposes prohibitive overhead on
edge devices-particularly for emerging large language model (LLM) tasks. To
address this challenge, a DNN-free method (ie., dataset sampling without DNN),
named NMS (Near-Memory Sampling), has been introduced. By first conducting
dimensionality reduction of the dataset and then performing exemplar sampling
in the reduced space, NMS avoids the architectural bias inherent in DNN-based
methods and thus achieves better generalization. However, The state-of-the-art,
NMS, suffers from two limitations: (1) The mismatch between the search method
and the non-monotonic property of the perplexity error function leads to the
emergence of outliers in the reduced representation; (2) Key parameter (ie.,
target perplexity) is selected empirically, introducing arbitrariness and
leading to uneven sampling. These two issues lead to representative bias of
examplars, resulting in degraded accuracy. To address these issues, we propose
AdapSNE, which integrates an efficient non-monotonic search method-namely, the
Fireworks Algorithm (FWA)-to suppress outliers, and employs entropy-guided
optimization to enforce uniform sampling, thereby ensuring representative
training samples and consequently boosting training accuracy. To cut the
edge-side cost arising from the iterative computations of FWA search and
entropy-guided optimization, we design an accelerator with custom dataflow and
time-multiplexing markedly reducing on-device training energy and area.

</details>


### [49] [LatentFlow: Cross-Frequency Experimental Flow Reconstruction from Sparse Pressure via Latent Mapping](https://arxiv.org/abs/2508.16648)
*Junle Liu,Chang Liu,Yanyu Ke,Qiuxiang Huang,Jiachen Zhao,Wenliang Chen,K. T. Tse,Gang Hu*

Main category: cs.LG

TL;DR: LatentFlow는 저주파 벽 압력 데이터를 활용해 고주파 난류 와류 흐름을 재구성하는 새로운 프레임워크로, 데이터 제약이 있는 실험 환경에서 효과적이다.


<details>
  <summary>Details</summary>
Motivation: 현재 PIV 실험에서 고주파 및 고해상도 난류 흐름 필드를 획득하는 것은 하드웨어 제한과 측정 잡음으로 인해 어려움이 있다.

Method: LatentFlow는 동기화된 저주파 흐름 필드와 압력 데이터를 결합하여 고주파 난류 와류 흐름 필드를 재구성하는 모델로, $eta$-변이 오토 인코더를 이용해 압력에 조건화된 잠재 표현을 학습한다.

Result: 이 모델은 저주파 벽 압력 신호를 사용하여 고주파 흐름 필드를 생성할 수 있다.

Conclusion: LatentFlow는 흐름 역학의 공간 인코딩을 압력 측정과 분리하여 데이터 제약 환경에서 고주파 난류 와류 흐름 재구성을 위한 확장 가능하고 견고한 해결책을 제공한다.

Abstract: Acquiring temporally high-frequency and spatially high-resolution turbulent
wake flow fields in particle image velocimetry (PIV) experiments remains a
significant challenge due to hardware limitations and measurement noise. In
contrast, temporal high-frequency measurements of spatially sparse wall
pressure are more readily accessible in wind tunnel experiments. In this study,
we propose a novel cross-modal temporal upscaling framework, LatentFlow, which
reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing
synchronized low-frequency (15 Hz) flow field and pressure data during
training, and high-frequency wall pressure signals during inference. The first
stage involves training a pressure-conditioned $\beta$-variation autoencoder
($p$C-$\beta$-VAE) to learn a compact latent representation that captures the
intrinsic dynamics of the wake flow. A secondary network maps synchronized
low-frequency wall pressure signals into the latent space, enabling
reconstruction of the wake flow field solely from sparse wall pressure. Once
trained, the model utilizes high-frequency, spatially sparse wall pressure
inputs to generate corresponding high-frequency flow fields via the
$p$C-$\beta$-VAE decoder. By decoupling the spatial encoding of flow dynamics
from temporal pressure measurements, LatentFlow provides a scalable and robust
solution for reconstructing high-frequency turbulent wake flows in
data-constrained experimental settings.

</details>


### [50] [HiCL: Hippocampal-Inspired Continual Learning](https://arxiv.org/abs/2508.16651)
*Kushal Kapoor,Wyatt Mackey,Yiannis Aloimonos,Xiaomin Lin*

Main category: cs.LG

TL;DR: HiCL은 카타스트로픽 포겟팅을 완화하기 위해 해마 회로에서 영감을 받은 이중 메모리 지속 학습 아키텍처이다.


<details>
  <summary>Details</summary>
Motivation: 카타스트로픽 포겟팅 문제를 해결하고 지속 학습의 효율성을 향상시키기 위해.

Method: 해마 회로에서 영감을 받은 요소를 활용하여 입력을 인코딩하고, 희소한 패턴 분리를 위한 모듈을 사용하며, 전문가에 대한 입력 라우팅을 통해 동적으로 작업 처리를 관리한다.

Result: 이 아키텍처는 작업 간 간섭을 줄이고 지속 학습 작업에서 최첨단 결과에 가까운 성능을 달성한다.

Conclusion: 우리는 HiCL이 여러 순차적 작업을 학습하는 데 있어 모델의 적응성과 효율성을 향상시킨다고 결론지었다.

Abstract: We propose HiCL, a novel hippocampal-inspired dual-memory continual learning
architecture designed to mitigate catastrophic forgetting by using elements
inspired by the hippocampal circuitry. Our system encodes inputs through a
grid-cell-like layer, followed by sparse pattern separation using a dentate
gyrus-inspired module with top-k sparsity. Episodic memory traces are
maintained in a CA3-like autoassociative memory. Task-specific processing is
dynamically managed via a DG-gated mixture-of-experts mechanism, wherein inputs
are routed to experts based on cosine similarity between their normalized
sparse DG representations and learned task-specific DG prototypes computed
through online exponential moving averages. This biologically grounded yet
mathematically principled gating strategy enables differentiable, scalable
task-routing without relying on a separate gating network, and enhances the
model's adaptability and efficiency in learning multiple sequential tasks.
Cortical outputs are consolidated using Elastic Weight Consolidation weighted
by inter-task similarity. Crucially, we incorporate prioritized replay of
stored patterns to reinforce essential past experiences. Evaluations on
standard continual learning benchmarks demonstrate the effectiveness of our
architecture in reducing task interference, achieving near state-of-the-art
results in continual learning tasks at lower computational costs.

</details>


### [51] [A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context](https://arxiv.org/abs/2508.16655)
*Andrei Mateescu,Ioana Hadarau,Ionut Anghel,Tudor Cioara,Ovidiu Anchidin,Ancuta Nemes*

Main category: cs.LG

TL;DR: 웨어러블 IoT 장치의 출현으로 원격 환자 모니터링이 심부전 관리의 유망한 솔루션으로 부각되었다. 그러나 심박수는 여러 요인으로 인해 크게 변동할 수 있으며, 환자의 실제 신체 활동과 연관되지 않으면 이러한 변화가 의미 있는지 평가하기 어렵다. 본 논문에서는 환자의 신체 활동에 의해 유도되는 심박수 변동을 모델링하기 위해 Laplace 확산 기법과 결합된 Transformer 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 원격 환자 모니터링에서 심박수 변동을 보다 정확하게 평가하기 위한 필요성이 존재한다.

Method: 물리적 활동의 맥락을 사용하여 전체 모델링 프로세스를 조건화하고 특화된 임베딩 및 주의 메커니즘을 활용하는 Transformer 모델을 사용하는 방법을 제안한다.

Result: 이 모델은 현재의 최첨단 방법 대비 43%의 평균 절대 오차 감소를 달성하였으며, R2 계수는 0.97로 실제 심박수 값과 강한 일치를 보인다.

Conclusion: 제안된 모델은 의료 제공자와 원격 환자 모니터링 시스템을 지원하는 실용적이고 효과적인 도구임을 시사한다.

Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient
monitoring (RPM) emerged as a promising solution for managing heart failure.
However, the heart rate can fluctuate significantly due to various factors, and
without correlating it to the patient's actual physical activity, it becomes
difficult to assess whether changes are significant. Although Artificial
Intelligence (AI) models may enhance the accuracy and contextual understanding
of remote heart rate monitoring, the integration of activity data is still
rarely addressed. In this paper, we propose a Transformer model combined with a
Laplace diffusion technique to model heart rate fluctuations driven by physical
activity of the patient. Unlike prior models that treat activity as secondary,
our approach conditions the entire modeling process on activity context using
specialized embeddings and attention mechanisms to prioritize activity specific
historical patents. The model captures both long-term patterns and
activity-specific heart rate dynamics by incorporating contextualized
embeddings and dedicated encoder. The Transformer model was validated on a
real-world dataset collected from 29 patients over a 4-month period.
Experimental results show that our model outperforms current state-of-the-art
methods, achieving a 43% reduction in mean absolute error compared to the
considered baseline models. Moreover, the coefficient of determination R2 is
0.97 indicating the model predicted heart rate is in strong agreement with
actual heart rate values. These findings suggest that the proposed model is a
practical and effective tool for supporting both healthcare providers and
remote patient monitoring systems.

</details>


### [52] [OASIS: Open-world Adaptive Self-supervised and Imbalanced-aware System](https://arxiv.org/abs/2508.16656)
*Miru Kim,Mugon Joe,Minhae Kwon*

Main category: cs.LG

TL;DR: 기계 학습의 동적 환경 확장에서 개방형 문제를 해결하기 위한 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습의 동적 환경으로의 확장은 레이블 이동, 공변량 이동 및 알려지지 않은 클래스가 등장하는 개방형 문제를 처리하는 데 어려움을 제공하며, 기존 방법은 불균형 데이터에서의 일반화 문제를 해결하지 못했습니다.

Method: 비교 기반의 사전 훈련 접근법을 사용하여 불균형 데이터에서도 개방형 문제를 효과적으로 처리하는 방법을 제안합니다. 이 방법은 낮은 비율의 클래스에 대한 분류 성능을 향상시킵니다.

Result: 모델의 강인성을 개선하고, 선택적 활성화 기준을 도입하여 후 훈련 과정을 최적화합니다.

Conclusion: 우리의 방법은 다양한 개방형 시나리오에서 정확도와 효율성 면에서 최신 적응 기술보다 크게 뛰어난 성능을 보여줍니다.

Abstract: The expansion of machine learning into dynamic environments presents
challenges in handling open-world problems where label shift, covariate shift,
and unknown classes emerge. Post-training methods have been explored to address
these challenges, adapting models to newly emerging data. However, these
methods struggle when the initial pre-training is performed on class-imbalanced
datasets, limiting generalization to minority classes. To address this, we
propose a method that effectively handles open-world problems even when
pre-training is conducted on imbalanced data. Our contrastive-based
pre-training approach enhances classification performance, particularly for
underrepresented classes. Our post-training mechanism generates reliable
pseudo-labels, improving model robustness against open-world problems. We also
introduce selective activation criteria to optimize the post-training process,
reducing unnecessary computation. Extensive experiments demonstrate that our
method significantly outperforms state-of-the-art adaptation techniques in both
accuracy and efficiency across diverse open-world scenarios.

</details>


### [53] [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Pingwei Sun,Feiye Huo,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Xiangyu Zhang,Maoxin He,Guangming Tan,Weile Jia,Tong Zhao*

Main category: cs.LG

TL;DR: Transformer 아키텍처가 LLM 분야를 지배하고 있으며, WISCA라는 Weight Scaling 방법을 제안하여 신경망 가중치 패턴을 개선함으로써 훈련 효율성과 모델 품질을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: Transformer 기반의 대형 언어 모델(LLMs) 훈련 최적화의 최근 발전이 주로 구조적 수정이나 최적화 도구 조정에 집중됨에 따라, 훈련 중 가중치 패턴의 체계적인 최적화가 부족하다.

Method: WISCA라는 Weight Scaling 방법을 제안하여 네트워크 구조를 변경하지 않고 신경망 가중치 패턴을 전략적으로 개선한다.

Result: WISCA는 수렴 품질을 크게 향상시키며, 그룹화된 쿼리 주의(GQA) 아키텍처와 LoRA 미세 조정 작업에서 특히 효과적이다.

Conclusion: 경험적인 결과는 여러 아키텍처에서 제로샷 검증 작업에 대해 평균 5.6% 개선과 훈련 혼란도 평균 2.12% 감소를 나타낸다.

Abstract: Transformer architecture gradually dominates the LLM field. Recent advances
in training optimization for Transformer-based large language models (LLMs)
primarily focus on architectural modifications or optimizer adjustments.
However, these approaches lack systematic optimization of weight patterns
during training. Weight pattern refers to the distribution and relative
magnitudes of weight parameters in a neural network. To address this issue, we
propose a Weight Scaling method called WISCA to enhance training efficiency and
model quality by strategically improving neural network weight patterns without
changing network structures. By rescaling weights while preserving model
outputs, WISCA indirectly optimizes the model's training trajectory.
Experiments demonstrate that WISCA significantly improves convergence quality
(measured by generalization capability and loss reduction), particularly in
LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning
tasks. Empirical results show 5.6% average improvement on zero-shot validation
tasks and 2.12% average reduction in training perplexity across multiple
architectures.

</details>


### [54] [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
*Zhong Guan,Likang Wu,Hongke Zhao,Jiahui Wang,Le Wu*

Main category: cs.LG

TL;DR: 대형 언어 모델의 추론 능력 향상은 많이 연구되었으나, 소형 언어 모델의 향상은 충분히 탐구되지 않았다. 본 논문에서는 소형 언어 모델의 추론 능력을 향상시키기 위한 RED 방법론을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델에서의 추론 능력 향상에 대한 연구가 많지만, 소형 언어 모델에 대한 연구는 부족하다.

Method: 소형 모델에 대한 RLVR과 대형 모델에서 증류된 데이터를 결합하여 제어된 탐색과 정제된 오프라인 통합을 통해 소형 언어 모델을 향상시키는 RED를 제안한다.

Result: 모델의 엔트로피 변화 비율을 모니터링하여 오프라인-SFT의 가중치를 조절하고, 소형 모델의 탐색 공간 부족 문제를 해결한다.

Conclusion: 오프라인 데이터와 현재 정책 간의 분포 불일치를 해결하기 위한 샘플 정확도 기반 정책 이동 메커니즘을 설계한다.

Abstract: Many existing studies have achieved significant improvements in the reasoning
capabilities of large language models (LLMs) through reinforcement learning
with verifiable rewards (RLVR), while the enhancement of reasoning abilities in
small language models (SLMs) has not yet been sufficiently explored. Combining
distilled data from larger models with RLVR on small models themselves is a
natural approach, but it still faces various challenges and issues. Therefore,
we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend
\textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through
Controlled Exploration and Refined Offline Integration. In this paper, we
explore the perspective of varying exploration spaces, balancing offline
distillation with online reinforcement learning. Simultaneously, we
specifically design and optimize for the insertion problem within offline data.
By monitoring the ratio of entropy changes in the model concerning offline and
online data, we regulate the weight of offline-SFT, thereby addressing the
issues of insufficient exploration space in small models and the redundancy and
complexity during the distillation process. Furthermore, to tackle the
distribution discrepancies between offline data and the current policy, we
design a sample-accuracy-based policy shift mechanism that dynamically chooses
between imitating offline distilled data and learning from its own policy.

</details>


### [55] [CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression](https://arxiv.org/abs/2508.16680)
*Muchammad Daniyal Kautsar,Afra Majida Hariono,Widyawan,Syukron Abu Ishaq Alfarozi,Kuntpong Woraratpanya*

Main category: cs.LG

TL;DR: 이 연구는 Large Language Models(LLMs)의 효과적인 압축 방법으로 Corrective Adaptive Low-Rank Decomposition(CALR)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM은 크기와 컴퓨팅 요구 사항 때문에 배포에 어려움이 있으며, 자원이 제한된 환경에서 이러한 모델을 실용적으로 만들기 위해 모델 압축 기술이 필수적이다.

Method: CALR은 SVD로 압축된 레이어의 주요 경로와 기능적 잔차 오류를 복원하도록 명시적으로 훈련된 병렬 학습 가능한 저차 수정 모듈을 결합한 두 구성 요소의 압축 방법이다.

Result: SmolLM2-135M, Qwen3-0.6B, Llama-3.2-1B에 대한 실험 평가에서 CALR은 매개변수 수를 26.93%에서 51.77%까지 줄이면서 모델 성능의 59.45%에서 90.42%를 유지했다.

Conclusion: CALR의 성공은 기능 정보 손실을 학습 가능한 신호로 다루는 것이 효과적인 압축 패러다임임을 보여주며, 이는 더 작고 효율적인 LLM 생성을 가능하게 한다.

Abstract: Large Language Models (LLMs) present significant deployment challenges due to
their immense size and computational requirements. Model compression techniques
are essential for making these models practical for resource-constrained
environments. A prominent compression strategy is low-rank factorization via
Singular Value Decomposition (SVD) to reduce model parameters by approximating
weight matrices. However, standard SVD focuses on minimizing matrix
reconstruction error, often leading to a substantial loss of the model's
functional performance. This performance degradation occurs because existing
methods do not adequately correct for the functional information lost during
compression. To address this gap, we introduce Corrective Adaptive Low-Rank
Decomposition (CALR), a two-component compression approach. CALR combines a
primary path of SVD-compressed layers with a parallel, learnable, low-rank
corrective module that is explicitly trained to recover the functional residual
error. Our experimental evaluation on SmolLM2-135M, Qwen3-0.6B, and
Llama-3.2-1B, demonstrates that CALR can reduce parameter counts by 26.93% to
51.77% while retaining 59.45% to 90.42% of the original model's performance,
consistently outperforming LaCo, ShortGPT, and LoSparse. CALR's success shows
that treating functional information loss as a learnable signal is a highly
effective compression paradigm. This approach enables the creation of
significantly smaller, more efficient LLMs, advancing their accessibility and
practical deployment in real-world applications.

</details>


### [56] [STGAtt: A Spatial-Temporal Unified Graph Attention Network for Traffic Flow Forecasting](https://arxiv.org/abs/2508.16685)
*Zhuding Liang,Jianxun Cui,Qingshuang Zeng,Feng Liu,Nenad Filipovic,Tijana Geroski*

Main category: cs.LG

TL;DR: 본 논문에서는 교통 흐름 예측을 위한 새로운 딥러닝 모델인 STGAtt를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 정확하고 시기적절한 교통 흐름 예측은 지능형 교통 시스템에서 매우 중요하다.

Method: STGAtt는 통합 그래프 표현 및 주의 메커니즘을 활용하여 복잡한 공간-시간 의존성을 효과적으로 포착한다.

Result: PEMS-BAY 및 SHMetro 데이터세트에 대한 광범위한 실험에서 STGAtt는 다양한 예측 수평선에서 최신 기준선에 비해 우수한 성능을 보였다.

Conclusion: STGAtt는 동적인 교통 패턴에 적응하고 장거리 의존성을 포착하는 능력을 확인하여 실제 교통 흐름 예측 응용 프로그램에 대한 잠재력을 강조한다.

Abstract: Accurate and timely traffic flow forecasting is crucial for intelligent
transportation systems. This paper presents a novel deep learning model, the
Spatial-Temporal Unified Graph Attention Network (STGAtt). By leveraging a
unified graph representation and an attention mechanism, STGAtt effectively
captures complex spatial-temporal dependencies. Unlike methods relying on
separate spatial and temporal dependency modeling modules, STGAtt directly
models correlations within a Spatial-Temporal Unified Graph, dynamically
weighing connections across both dimensions. To further enhance its
capabilities, STGAtt partitions traffic flow observation signal into
neighborhood subsets and employs a novel exchanging mechanism, enabling
effective capture of both short-range and long-range correlations. Extensive
experiments on the PEMS-BAY and SHMetro datasets demonstrate STGAtt's superior
performance compared to state-of-the-art baselines across various prediction
horizons. Visualization of attention weights confirms STGAtt's ability to adapt
to dynamic traffic patterns and capture long-range dependencies, highlighting
its potential for real-world traffic flow forecasting applications.

</details>


### [57] [Multidimensional Distributional Neural Network Output Demonstrated in Super-Resolution of Surface Wind Speed](https://arxiv.org/abs/2508.16686)
*Harrison J. Goldwyn,Mitchell Krock,Johann Rudi,Daniel Getter,Julie Bessac*

Main category: cs.LG

TL;DR: 신경망 예측에서 불확실성을 정확하게 정량화하는 것은 고차원 상관 데이터와 관련된 과학적 응용에서 주요한 도전 과제입니다. 본 연구에서는 다차원 가우시안 손실을 사용하여 예측 분포를 생성하는 신경망 훈련 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 신경망 예측의 불확실성을 정확하게 정량화하는 것은 고차원 상관 데이터와 관련된 과학적 응용에서 중심적인 도전 과제가 됩니다.

Method: 본 연구에서는 다차원 가우시안 손실을 활용하여 출력에 대한 닫힌 형태의 예측 분포를 생성하는 신경망 훈련 프레임워크를 제시합니다.

Result: 알레아토릭 불확실성을 포착하고, 네트워크 훈련을 안정화하며 공간 상관 관계를 보존하는 방법이 입증되었습니다.

Conclusion: 제안된 프레임워크는 효율적인 샘플링, 명시적 상관 관계 모델링 및 더 복잡한 분포 군에 대한 확장을 가능하게 하며 예측 성능을 방해하지 않습니다.

Abstract: Accurate quantification of uncertainty in neural network predictions remains
a central challenge for scientific applications involving high-dimensional,
correlated data. While existing methods capture either aleatoric or epistemic
uncertainty, few offer closed-form, multidimensional distributions that
preserve spatial correlation while remaining computationally tractable. In this
work, we present a framework for training neural networks with a
multidimensional Gaussian loss, generating closed-form predictive distributions
over outputs with non-identically distributed and heteroscedastic structure.
Our approach captures aleatoric uncertainty by iteratively estimating the means
and covariance matrices, and is demonstrated on a super-resolution example. We
leverage a Fourier representation of the covariance matrix to stabilize network
training and preserve spatial correlation. We introduce a novel regularization
strategy -- referred to as information sharing -- that interpolates between
image-specific and global covariance estimates, enabling convergence of the
super-resolution downscaling network trained on image-specific distributional
loss functions. This framework allows for efficient sampling, explicit
correlation modeling, and extensions to more complex distribution families all
without disrupting prediction performance. We demonstrate the method on a
surface wind speed downscaling task and discuss its broader applicability to
uncertainty-aware prediction in scientific models.

</details>


### [58] [Native Logical and Hierarchical Representations with Subspace Embeddings](https://arxiv.org/abs/2508.16687)
*Gabriel Moreira,Zita Marinho,Manuel Marques,João Paulo Costeira,Chenyan Xiong*

Main category: cs.LG

TL;DR: 본 논문은 개념을 선형 부분공간으로 임베딩하는 새로운 패러다임을 제안하여 기존의 신경 임베딩의 한계를 극복합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 신경 임베딩은 유사성은 잘 표현하나 고차원 추론 및 비대칭 관계를 처리하는 데 어려움을 겪습니다.

Method: 개념을 선형 부분공간으로 임베딩하는 새로운 프레임워크를 도입하고, 이를 통해 부분공간 차원과 포함 관계를 모델링합니다. 또한, 직교 투영 연산자를 부드럽게 이완시켜 학습을 가능하게 합니다.

Result: 우리의 방법은 WordNet에서 재구성 및 링크 예측에서 최첨단 결과를 달성하며, 자연어 추론 벤치마크에서 bi-encoder 기준선을 초월합니다.

Conclusion: 이 접근 방식은 기하학적으로 기반을 두고 논리 연산이 가능하며, 의미의 해석 가능한 수식을 제공합니다.

Abstract: Traditional neural embeddings represent concepts as points, excelling at
similarity but struggling with higher-level reasoning and asymmetric
relationships. We introduce a novel paradigm: embedding concepts as linear
subspaces. This framework inherently models generality via subspace
dimensionality and hierarchy through subspace inclusion. It naturally supports
set-theoretic operations like intersection (conjunction), linear sum
(disjunction) and orthogonal complements (negations), aligning with classical
formal semantics. To enable differentiable learning, we propose a smooth
relaxation of orthogonal projection operators, allowing for the learning of
both subspace orientation and dimension. Our method achieves state-of-the-art
results in reconstruction and link prediction on WordNet. Furthermore, on
natural language inference benchmarks, our subspace embeddings surpass
bi-encoder baselines, offering an interpretable formulation of entailment that
is both geometrically grounded and amenable to logical operations.

</details>


### [59] [A novel auxiliary equation neural networks method for exactly explicit solutions of nonlinear partial differential equations](https://arxiv.org/abs/2508.16702)
*Shanhao Yuan,Yanqin Liu,Runfa Zhang,Limei Yan,Shunjun Wu,Libo Feng*

Main category: cs.LG

TL;DR: 이 연구에서는 보조 방정식 신경망 방법(AENNM)을 제안하며, 이는 비선형 편미분 방정식(NLPDEs)의 정확한 해를 얻기 위해 신경망(NNs) 모델과 보조 방정식 방법을 통합한 혁신적인 분석 방법이다.


<details>
  <summary>Details</summary>
Motivation: 비선형 편미분 방정식의 정확한 해를 얻기 위해 새로운 방법론이 필요하다.

Method: 신경망 모델과 보조 방정식 방법을 결합하여 새로운 활성화 함수를 도입하고, '2-2-2-1' 및 '3-2-2-1' 신경망 내에서 구체적인 활성화 함수를 설정하여 새로운 시험 함수를 구성한다.

Result: AENNM은 비선형 진화 방정식, Korteweg-de Vries-Burgers 방정식 및 (2+1)차원 Boussinesq 방정식을 포함한 세 가지 수치 예제를 통해 효과가 입증되었다.

Conclusion: AENNM은 NLPDE 해결을 위한 새로운 방법론적 프레임워크를 제공하며, 과학 및 공학 분야에서 널리 적용될 수 있다.

Abstract: In this study, we firstly propose an auxiliary equation neural networks
method (AENNM), an innovative analytical method that integrates neural networks
(NNs) models with the auxiliary equation method to obtain exact solutions of
nonlinear partial differential equations (NLPDEs). A key novelty of this method
is the introduction of a novel activation function derived from the solutions
of the Riccati equation, establishing a new mathematical link between
differential equations theory and deep learning. By combining the strong
approximation capability of NNs with the high precision of symbolic
computation, AENNM significantly enhances computational efficiency and
accuracy. To demonstrate the effectiveness of the AENNM in solving NLPDEs,
three numerical examples are investigated, including the nonlinear evolution
equation, the Korteweg-de Vries-Burgers equation, and the (2+1)-dimensional
Boussinesq equation. Furthermore, some new trial functions are constructed by
setting specific activation functions within the "2-2-2-1" and "3-2-2-1" NNs
models. By embedding the auxiliary equation method into the NNs framework, we
derive previously unreported solutions. The exact analytical solutions are
expressed in terms of hyperbolic functions, trigonometric functions, and
rational functions. Finally, three-dimensional plots, contour plots, and
density plots are presented to illustrate the dynamic characteristics of the
obtained solutions. This research provides a novel methodological framework for
addressing NLPDEs, with broad applicability across scientific and engineering
fields.

</details>


### [60] [Aligning Distributionally Robust Optimization with Practical Deep Learning Needs](https://arxiv.org/abs/2508.16734)
*Dmitrii Feoktistov,Igor Ignashin,Andrey Veprikov,Nikita Borovko,Alexander Bogdanov,Savelii Chezhegov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 이 논문에서는 샘플 그룹에 대한 가중치 할당을 처리할 수 있는 수정된 DRO 목표를 위한 적응형 알고리즘인 ALSO를 소개하고, 이를 통해 기존의 최적화 방법과의 격차를 해소하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 딥 러닝 최적화 방법은 모든 훈련 샘플을 동일하게 취급하지만, DRO는 샘플에 중요도를 적응적으로 할당한다. 그러나 현재 딥 러닝 관행과 DRO 간의 큰 격차가 존재한다.

Method: ALSO는 샘플 그룹에 대한 가중치 할당을 처리할 수 있는 수정된 DRO 목표를 위한 적응형 알고리즘이다.

Result: ALSO가 전통적인 최적화 방법과 기존 DRO 방법보다 우수한 성능을 보인다는 것을 다양한 딥 러닝 작업에서의 경험적 평가를 통해 입증하였다.

Conclusion: 비록 비볼록 목표에 대해서도 수렴성을 증명하였으며, 이는 딥 러닝 모델의 전형적인 경우이다.

Abstract: While traditional Deep Learning (DL) optimization methods treat all training
samples equally, Distributionally Robust Optimization (DRO) adaptively assigns
importance weights to different samples. However, a significant gap exists
between DRO and current DL practices. Modern DL optimizers require adaptivity
and the ability to handle stochastic gradients, as these methods demonstrate
superior performance. Additionally, for practical applications, a method should
allow weight assignment not only to individual samples, but also to groups of
objects (for example, all samples of the same class). This paper aims to bridge
this gap by introducing ALSO $\unicode{x2013}$ Adaptive Loss Scaling Optimizer
$\unicode{x2013}$ an adaptive algorithm for a modified DRO objective that can
handle weight assignment to sample groups. We prove the convergence of our
proposed algorithm for non-convex objectives, which is the typical case for DL
models. Empirical evaluation across diverse Deep Learning tasks, from Tabular
DL to Split Learning tasks, demonstrates that ALSO outperforms both traditional
optimizers and existing DRO methods.

</details>


### [61] [Deep Learning for Markov Chains: Lyapunov Functions, Poisson's Equation, and Stationary Distributions](https://arxiv.org/abs/2508.16737)
*Yanlin Qu,Jose Blanchet,Peter Glynn*

Main category: cs.LG

TL;DR: 딥러닝을 활용하여 리아푸노프 함수를 자동으로 구축하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 마르코프 모델의 안정성을 확보하기 위해 리아푸노프 함수를 구축하는 것이 중요하지만, 이 과정은 창의성과 분석적 노력을 필요로 한다.

Method: 신경망을 훈련시켜 초기 전이 분석에서 도출된 적분 방정식을 만족시키도록 하는 방법을 사용한다.

Result: 신경망이 비압축 상태 공간의 마르코프 연쇄에 적용되더라도 여전히 효과적임을 보여준다.

Conclusion: 대기 이론 등 여러 예제를 통해 제안된 방법론의 효과를 입증하였다.

Abstract: Lyapunov functions are fundamental to establishing the stability of Markovian
models, yet their construction typically demands substantial creativity and
analytical effort. In this paper, we show that deep learning can automate this
process by training neural networks to satisfy integral equations derived from
first-transition analysis. Beyond stability analysis, our approach can be
adapted to solve Poisson's equation and estimate stationary distributions.
While neural networks are inherently function approximators on compact domains,
it turns out that our approach remains effective when applied to Markov chains
on non-compact state spaces. We demonstrate the effectiveness of this
methodology through several examples from queueing theory and beyond.

</details>


### [62] [WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning](https://arxiv.org/abs/2508.16741)
*Haosen Ge,Shuo Li,Lianghuan Huang*

Main category: cs.LG

TL;DR: 약한 모델을 활용한 효과적인 프롬프트 엔지니어링 프레임워크를 제안.


<details>
  <summary>Details</summary>
Motivation: 효과적인 프롬프트 엔지니어링의 어려움 해결.

Method: 약한 '교사' 모델이 큰 '학생' 모델의 성능을 향상시키는 지침을 생성.

Result: MATH-500에서 98%, HH-RLHF에서 134%의 성과 달성.

Conclusion: WST는 작은 모델이 큰 모델의 성능을 신뢰성 있게 지원할 수 있음을 보여줌.

Abstract: Effective prompt engineering remains a challenging task for many
applications. We introduce Weak-to-Strong Transfer (WST), an automatic prompt
engineering framework where a small "Teacher" model generates instructions that
enhance the performance of a much larger "Student" model. Unlike prior work,
WST requires only a weak teacher, making it efficient and broadly applicable in
settings where large models are closed-source or difficult to fine-tune. Using
reinforcement learning, the Teacher Model's instructions are iteratively
improved based on the Student Model's outcomes, yielding substantial gains
across reasoning (MATH-500, GSM8K) and alignment (HH-RLHF) benchmarks - 98% on
MATH-500 and 134% on HH-RLHF - and surpassing baselines such as GPT-4o-mini and
Llama-70B. These results demonstrate that small models can reliably scaffold
larger ones, unlocking latent capabilities while avoiding misleading prompts
that stronger teachers may introduce, establishing WST as a scalable solution
for efficient and safe LLM prompt refinement.

</details>


### [63] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 본 연구는 생물다양성 연구에서 하이퍼볼릭 네트워크가 분류 모델에 더 나은 임베딩 공간을 제공할 수 있는지를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 생물 다양성 연구에서 생물 샘플을 구조화된 계층으로 조직할 필요가 있다.

Method: 대조 및 새로운 스택된 포함 기반 목표를 사용하여 다중 모달 입력을 공유 하이퍼볼릭 공간에 임베딩한다.

Result: BIOSCAN-1M 데이터셋에 대한 실험 결과, 하이퍼볼릭 임베딩이 유클리드 기준선에 대해 경쟁력 있는 성능을 달성하고, DNA 바코드를 사용한 보지 못한 종 분류에서 모든 다른 모델을 능가한다.

Conclusion: 우리의 프레임워크는 생물 다양성 모델링을 위한 구조 인식 기반을 제공하며, 종 발견, 생태 모니터링 및 보존 노력에 잠재적 응용 가능성이 있다.

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [64] [Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling](https://arxiv.org/abs/2508.16745)
*Ivan Rodkin,Daniil Orel,Konstantin Smirnov,Arman Bolatov,Bilal Elbouardi,Besher Hassan,Yuri Kuratov,Aydar Bulatov,Preslav Nakov,Timothy Baldwin,Artem Shelmanov,Mikhail Burtsev*

Main category: cs.LG

TL;DR: 이 연구는 대형 언어 모델의 다단계 추론 능력과 이를 향상시키는 아키텍처 및 훈련 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 다단계 추론 학습 및 수행 이해는 여전히 미해결 문제이기 때문이다.

Method: 임의의 불리언 함수로 생성된 상태 시퀀스에서 훈련하여 기억화 효과를 배제하면서 모델의 다단계 추론 능력을 조사한다.

Result: 대부분의 신경망 아키텍처가 기본 규칙을 추상화하는 방법을 학습하며, 모델의 깊이를 증가시키는 것이 순차 계산에서 중요한 역할을 한다.

Conclusion: 재귀, 메모리, 테스트 시점 컴퓨팅 확장을 통해 효과적인 모델 깊이를 확장하면 추론 능력이 상당히 향상된다는 것을 보인다.

Abstract: Reasoning is a core capability of large language models, yet understanding
how they learn and perform multi-step reasoning remains an open problem. In
this study, we explore how different architectures and training methods affect
model multi-step reasoning capabilities within a cellular automata framework.
By training on state sequences generated with random Boolean functions for
random initial conditions to exclude memorization, we demonstrate that most
neural architectures learn to abstract the underlying rules. While models
achieve high accuracy in next-state prediction, their performance declines
sharply if multi-step reasoning is required. We confirm that increasing model
depth plays a crucial role for sequential computations. We demonstrate that an
extension of the effective model depth with recurrence, memory, and test-time
compute scaling substantially enhances reasoning capabilities.

</details>


### [65] [FAIRWELL: Fair Multimodal Self-Supervised Learning for Wellbeing Prediction](https://arxiv.org/abs/2508.16748)
*Jiaee Cheong,Abtin Mogharabin,Paul Liang,Hatice Gunes,Sinan Kalkan*

Main category: cs.LG

TL;DR: 자기 지도 학습(SSL)을 활용하여 기계 학습(ML) 공정성을 향상시키려는 초기 노력은 유망한 결과를 보였다. 그러나 이러한 접근법은 다중 모달 맥락에서 아직 탐색되지 않았다. 본 논문에서는 주제 수준 손실 함수를 제안하여 공정한 표현을 학습하는 세 가지 메커니즘을 통해 실제 Healthcare 데이터셋에서 성능을 향상시켰다.


<details>
  <summary>Details</summary>
Motivation: 자기 지도 학습을 통해 기계 학습의 공정성을 향상시키는 초기 연구들이 긍정적인 결과를 도출하였으나, 다중 모달 환경에서의 연구는 부족하다.

Method: 모달리티 고유 정보를 활용하여, VAICReg 방법을 기준으로 세 가지 메커니즘(분산 항, 불변 항, 공분산 항)을 적용한 새로운 주제 수준 손실 함수를 제안하였다.

Result: 우리의 방법은 D-Vlog, MIMIC, MODMA의 세 가지 도전적인 건강 관리 데이터셋에서 평가되었고, 공정성 성능을 전반적으로 개선하면서도 분류 성능의 감소가 최소화되었다.

Conclusion: FAIRWELL 손실 함수는 다중 모달 예측 작업에서 공정성을 보장하기 위한 주제 독립 표현을 얻는 것을 목표로 한다.

Abstract: Early efforts on leveraging self-supervised learning (SSL) to improve machine
learning (ML) fairness has proven promising. However, such an approach has yet
to be explored within a multimodal context. Prior work has shown that, within a
multimodal setting, different modalities contain modality-unique information
that can complement information of other modalities. Leveraging on this, we
propose a novel subject-level loss function to learn fairer representations via
the following three mechanisms, adapting the variance-invariance-covariance
regularization (VICReg) method: (i) the variance term, which reduces reliance
on the protected attribute as a trivial solution; (ii) the invariance term,
which ensures consistent predictions for similar individuals; and (iii) the
covariance term, which minimizes correlational dependence on the protected
attribute. Consequently, our loss function, coined as FAIRWELL, aims to obtain
subject-independent representations, enforcing fairness in multimodal
prediction tasks. We evaluate our method on three challenging real-world
heterogeneous healthcare datasets (i.e. D-Vlog, MIMIC and MODMA) which contain
different modalities of varying length and different prediction tasks. Our
findings indicate that our framework improves overall fairness performance with
minimal reduction in classification performance and significantly improves on
the performance-fairness Pareto frontier.

</details>


### [66] [DR-CircuitGNN: Training Acceleration of Heterogeneous Circuit Graph Neural Network on GPUs](https://arxiv.org/abs/2508.16769)
*Yuebo Luo,Shiyang Li,Junran Tao,Kiran Thorat,Xi Xie,Hongwu Peng,Nuo Xu,Caiwen Ding,Shaoyi Huang*

Main category: cs.LG

TL;DR: HGNN을 활용한 전자 설계 자동화(EDA) 효율성을 개선하기 위한 DR-CircuitGNN을 제안하며, GPU와 CPU의 동시 처리 능력을 높여 성능을 극대화한다.


<details>
  <summary>Details</summary>
Motivation: 통합 회로 설계의 증가하는 규모와 복잡성 때문에 EDA에서의 도전 과제가 커지고 있다.

Method: 행 기반 희소성 인식을 통한 Dynamic-ReLU와 이종 메시지 전달 최적화로 HGNN의 GPU 커널을 빠르게 설계한다.

Result: 세 가지 대표적인 CircuitNet 설계에서 제안된 방법은 SOTA에 비해 각각 최대 3.51배, 4.09배의 속도 향상을 달성할 수 있다.

Conclusion: 제안된 병렬 설계는 DGL 구현보다 최대 2.71배의 속도 향상을 가능하게 한다.

Abstract: The increasing scale and complexity of integrated circuit design have led to
increased challenges in Electronic Design Automation (EDA). Graph Neural
Networks (GNNs) have emerged as a promising approach to assist EDA design as
circuits can be naturally represented as graphs. While GNNs offer a foundation
for circuit analysis, they often fail to capture the full complexity of EDA
designs. Heterogeneous Graph Neural Networks (HGNNs) can better interpret EDA
circuit graphs as they capture both topological relationships and geometric
features. However, the improved representation capability comes at the cost of
even higher computational complexity and processing cost due to their serial
module-wise message-passing scheme, creating a significant performance
bottleneck. In this paper, we propose DR-CircuitGNN, a fast GPU kernel design
by leveraging row-wise sparsity-aware Dynamic-ReLU and optimizing SpMM kernels
during heterogeneous message-passing to accelerate HGNNs training on
EDA-related circuit graph datasets. To further enhance performance, we propose
a parallel optimization strategy that maximizes CPU-GPU concurrency by
concurrently processing independent subgraphs using multi-threaded CPU
initialization and GPU kernel execution via multiple cudaStreams. Our
experiments show that on three representative CircuitNet designs (small,
medium, large), the proposed method can achieve up to 3.51x and 4.09x speedup
compared to the SOTA for forward and backward propagation, respectively. On
full-size CircuitNet and sampled Mini-CircuitNet, our parallel design enables
up to 2.71x speed up over the official DGL implementation cuSPARSE with
negligible impact on correlation scores and error rates.

</details>


### [67] [Latent Graph Learning in Generative Models of Neural Signals](https://arxiv.org/abs/2508.16776)
*Nathan X. Kodama,Kenneth A. Loparo*

Main category: cs.LG

TL;DR: 신경 신호에서 시공간 상호작용 그래프 및 고차원 구조를 유추하는 것은 시스템 신경과학을 위한 생성 모델 구축의 핵심 문제이다. 본 연구에서는 신경 신호의 생성 모델에서 잠재적 그래프 학습을 탐구하며, 신경 회로의 수치적 시뮬레이션을 통해 학습된 모델 가중치를 설명하기 위한 여러 가설을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 신경 신호로부터 해석 가능한 잠재 그래프 표현을 추출하는 것은 도전적이고 해결되지 않은 문제이다.

Method: 신경 회로의 수치적 시뮬레이션을 사용하여 학습된 모델의 가중치를 설명하기 위한 여러 가설을 평가한다.

Result: 추출된 네트워크 표현과 기본 방향 그래프 간의 적당한 정렬 및 공동 입력 그래프 표현에서 강한 정렬을 발견하였다.

Conclusion: 이 발견은 신경 데이터의 대규모 기초 모델 구축에 그래프 기반 기하학적 제약을 통합하는 방향을 모색하도록 한다.

Abstract: Inferring temporal interaction graphs and higher-order structure from neural
signals is a key problem in building generative models for systems
neuroscience. Foundation models for large-scale neural data represent shared
latent structures of neural signals. However, extracting interpretable latent
graph representations in foundation models remains challenging and unsolved.
Here we explore latent graph learning in generative models of neural signals.
By testing against numerical simulations of neural circuits with known
ground-truth connectivity, we evaluate several hypotheses for explaining
learned model weights. We discover modest alignment between extracted network
representations and the underlying directed graphs and strong alignment in the
co-input graph representations. These findings motivate paths towards
incorporating graph-based geometric constraints in the construction of
large-scale foundation models for neural data.

</details>


### [68] [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
*Manpreet Singh,Hassan Sajjad*

Main category: cs.LG

TL;DR: 양자화는 자원 제약 환경에서 LLM을 배포하는 실용적인 솔루션을 제공합니다. 그러나 내부 표현에 미치는 영향은 충분히 연구되지 않았으며, 양자화된 모델의 신뢰성에 대한 의문을 제기합니다.


<details>
  <summary>Details</summary>
Motivation: 본 연구의 동기는 자원 제약 환경에서 LLM을 효과적으로 배포하기 위해 양자화의 신뢰성을 이해하고자 하는 것입니다.

Method: 본 연구에서는 다양한 해석 가능성 기법을 사용하여 양자화가 모델 및 뉴런 행동에 미치는 영향을 조사합니다. 우리는 4비트 및 8비트 양자화 하에서 여러 LLM을 분석합니다.

Result: 양자화가 모델 보정에 미치는 영향은 일반적으로 미미하며, 뉴런 활성화 분석 결과, 활성화 값이 0에 가까운 죽은 뉴런의 수는 양자화와 무관하게 일관성을 유지합니다.

Conclusion: 양자화가 뉴런의 중복성에 미치는 영향은 모델마다 다르지만, 전반적으로 양자화가 신뢰할 수 있는 모델 압축 기술로서의 사용을 저해할 정도의 극적인 변화를 관찰하지는 못했습니다.

Abstract: Quantization offers a practical solution to deploy LLMs in
resource-constraint environments. However, its impact on internal
representations remains understudied, raising questions about the reliability
of quantized models. In this study, we employ a range of interpretability
techniques to investigate how quantization affects model and neuron behavior.
We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings
reveal that the impact of quantization on model calibration is generally minor.
Analysis of neuron activations indicates that the number of dead neurons, i.e.,
those with activation values close to 0 across the dataset, remains consistent
regardless of quantization. In terms of neuron contribution to predictions, we
observe that smaller full precision models exhibit fewer salient neurons,
whereas larger models tend to have more, with the exception of Llama-2-7B. The
effect of quantization on neuron redundancy varies across models. Overall, our
findings suggest that effect of quantization may vary by model and tasks,
however, we did not observe any drastic change which may discourage the use of
quantization as a reliable model compression technique.

</details>


### [69] [Anchor-MoE: A Mean-Anchored Mixture of Experts For Probabilistic Regression](https://arxiv.org/abs/2508.16802)
*Baozhuo Su,Zhengxian Qu*

Main category: cs.LG

TL;DR: Anchor-MoE 모델은 불확실성 하의 회귀 문제를 해결하며, 전통적인 점 회귀 모델을 결합하여 예측 정확도를 높인다.


<details>
  <summary>Details</summary>
Motivation: 불확실성 하에서의 회귀는 과학 및 공학 전반에 걸쳐 필수적이다.

Method: 이 모델은 조정된 그래디언트 부스팅 모델을 사용하여 앵커 평균을 제공하고, 샘플을 혼합 밀도 네트워크 전문가의 작은 집합으로 배분하여 이질적인 수정을 생산한다.

Result: Anchor-MoE는 RMSE 및 NLL에서 강력한 NGBoost 기준을 일관되게 초과하며, 여러 데이터셋에서 새로운 최첨단 확률적 회귀 결과를 달성한다.

Conclusion: 코드는 https://github.com/BaozhuoSU/Probabilistic_Regression에서 이용 가능하다.

Abstract: Regression under uncertainty is fundamental across science and engineering.
We present an Anchored Mixture of Experts (Anchor-MoE), a model that handles
both probabilistic and point regression. For simplicity, we use a tuned
gradient-boosting model to furnish the anchor mean; however, any off-the-shelf
point regressor can serve as the anchor. The anchor prediction is projected
into a latent space, where a learnable metric-window kernel scores locality and
a soft router dispatches each sample to a small set of mixture-density-network
experts; the experts produce a heteroscedastic correction and predictive
variance. We train by minimizing negative log-likelihood, and on a disjoint
calibration split fit a post-hoc linear map on predicted means to improve point
accuracy. On the theory side, assuming a H\"older smooth regression function of
order~$\alpha$ and fixed Lipschitz partition-of-unity weights with bounded
overlap, we show that Anchor-MoE attains the minimax-optimal $L^2$ risk rate
$O\!\big(N^{-2\alpha/(2\alpha+d)}\big)$. In addition, the CRPS test
generalization gap scales as
$\widetilde{O}\!\Big(\sqrt{(\log(Mh)+P+K)/N}\Big)$; it is logarithmic in $Mh$
and scales as the square root in $P$ and $K$. Under bounded-overlap routing,
$K$ can be replaced by $k$, and any dependence on a latent dimension is
absorbed into $P$. Under uniformly bounded means and variances, an analogous
$\widetilde{O}\!\big(\sqrt{(\log(Mh)+P+K)/N}\big)$ scaling holds for the test
NLL up to constants. Empirically, across standard UCI regressions, Anchor-MoE
consistently matches or surpasses the strong NGBoost baseline in RMSE and NLL;
on several datasets it achieves new state-of-the-art probabilistic regression
results on our benchmark suite. Code is available at
https://github.com/BaozhuoSU/Probabilistic_Regression.

</details>


### [70] [Uncertainty Propagation Networks for Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.16815)
*Hadi Jahanshahi,Zheng H. Zhu*

Main category: cs.LG

TL;DR: UPN은 연속 시간 모델링에 불확실성 정량화를 자연스럽게 통합하는 새로운 신경 미분 방정식의 계열이다.


<details>
  <summary>Details</summary>
Motivation: 연속 시간 모델링에서 불확실성을 효율적으로 모델링할 수 있는 방법이 필요하다.

Method: UPN은 평균 및 공분산 동력을 위한 결합된 미분 방정식을 매개변수화하여 상태 진화와 관련된 불확실성을 함께 모델링한다.

Result: 여러 도메인에서 UPN의 효율성을 보여주며, 불확실성을 정량화하는 연속 정규화 흐름(CNF), 잘 보정된 신뢰 구간을 가진 시계열 예측, 안정적 및 혼돈 동적 시스템에서의 견고한 궤적 예측을 포함한다.

Conclusion: UPN은 불확실성 정량화를 자연스럽게 처리하고 비정상적으로 샘플링된 관찰을 잘 다룬다.

Abstract: This paper introduces Uncertainty Propagation Network (UPN), a novel family
of neural differential equations that naturally incorporate uncertainty
quantification into continuous-time modeling. Unlike existing neural ODEs that
predict only state trajectories, UPN simultaneously model both state evolution
and its associated uncertainty by parameterizing coupled differential equations
for mean and covariance dynamics. The architecture efficiently propagates
uncertainty through nonlinear dynamics without discretization artifacts by
solving coupled ODEs for state and covariance evolution while enabling
state-dependent, learnable process noise. The continuous-depth formulation
adapts its evaluation strategy to each input's complexity, provides principled
uncertainty quantification, and handles irregularly-sampled observations
naturally. Experimental results demonstrate UPN's effectiveness across multiple
domains: continuous normalizing flows (CNFs) with uncertainty quantification,
time-series forecasting with well-calibrated confidence intervals, and robust
trajectory prediction in both stable and chaotic dynamical systems.

</details>


### [71] [Understanding and Tackling Over-Dilution in Graph Neural Networks](https://arxiv.org/abs/2508.16829)
*Junhyun Lee,Veronika Thost,Bumsoo Kim,Jaewoo Kang,Tengfei Ma*

Main category: cs.LG

TL;DR: MPNN의 한계인 과도한 희석과 희석 강화를 다루며, 이러한 문제를 해결하기 위한 변환기 기반 솔루션을 제안한다.


<details>
  <summary>Details</summary>
Motivation: MPNN의 비정상적인 데이터 구조로 인한 의도하지 않은 행동을 해결하고 보다 유용한 그래프 표현을 구성하기 위함이다.

Method: 과도한 희석 개념을 소개하고 두 가지 희석 요인, 즉 속성 수준의 노드 내 희석 및 노드 수준의 노드 간 희석으로 이를 수식화한다. 또한, 기존의 MPNN과 같은 노드 임베딩 방법을 보완하는 변환기 기반 솔루션을 제안한다.

Result: 단일 레이어 내에서도 개별 노드에 특화된 정보가 상당히 희석될 수 있음을 관찰하고, 새로운 인사이트를 제공한다.

Conclusion: 이 연구는 MPNN의 한계를 이해하고, 더 정보적인 그래프 표현 개발에 기여한다.

Abstract: Message Passing Neural Networks (MPNNs) hold a key position in machine
learning on graphs, but they struggle with unintended behaviors, such as
over-smoothing and over-squashing, due to irregular data structures. The
observation and formulation of these limitations have become foundational in
constructing more informative graph representations. In this paper, we delve
into the limitations of MPNNs, focusing on aspects that have previously been
overlooked. Our observations reveal that even within a single layer, the
information specific to an individual node can become significantly diluted. To
delve into this phenomenon in depth, we present the concept of Over-dilution
and formulate it with two dilution factors: intra-node dilution for
attribute-level and inter-node dilution for node-level representations. We also
introduce a transformer-based solution that alleviates over-dilution and
complements existing node embedding methods like MPNNs. Our findings provide
new insights and contribute to the development of informative representations.
The implementation and supplementary materials are publicly available at
https://github.com/LeeJunHyun/NATR.

</details>


### [72] [Out of Distribution Detection for Efficient Continual Learning in Quality Prediction for Arc Welding](https://arxiv.org/abs/2508.16832)
*Yannik Hahn,Jan Voets,Antonin Koenigsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: 본 연구는 다이나믹 제조 환경에서의 품질 예측을 위한 VQ-VAE 트랜스포머 아키텍처의 확장을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 다이나믹 제조 환경에서 발생하는 분포 이동에 대한 현재 모델의 한계를 극복하기 위함이다.

Method: 자기 회귀 손실을 사용하여 아울 오브 디스트리뷰션(OOD) 탐지 메커니즘으로 활용하는 VQ-VAE 트랜스포머 아키텍처를 확장하였다.

Result: 기존의 재구성 방법 및 다른 기준선들에 비해 우수한 성능을 보였다.

Conclusion: 다이나믹 제조 과정에서의 품질 보증을 위한 설명 가능하고 적응 가능한 해결책으로, 산업 환경에서 견고한 AI 시스템 개발을 위한 중요한 단계가 된다.

Abstract: Modern manufacturing relies heavily on fusion welding processes, including
gas metal arc welding (GMAW). Despite significant advances in machine
learning-based quality prediction, current models exhibit critical limitations
when confronted with the inherent distribution shifts that occur in dynamic
manufacturing environments. In this work, we extend the VQ-VAE Transformer
architecture - previously demonstrating state-of-the-art performance in weld
quality prediction - by leveraging its autoregressive loss as a reliable
out-of-distribution (OOD) detection mechanism. Our approach exhibits superior
performance compared to conventional reconstruction methods, embedding
error-based techniques, and other established baselines. By integrating OOD
detection with continual learning strategies, we optimize model adaptation,
triggering updates only when necessary and thereby minimizing costly labeling
requirements. We introduce a novel quantitative metric that simultaneously
evaluates OOD detection capability while interpreting in-distribution
performance. Experimental validation in real-world welding scenarios
demonstrates that our framework effectively maintains robust quality prediction
capabilities across significant distribution shifts, addressing critical
challenges in dynamic manufacturing environments where process parameters
frequently change. This research makes a substantial contribution to applied
artificial intelligence by providing an explainable and at the same time
adaptive solution for quality assurance in dynamic manufacturing processes - a
crucial step towards robust, practical AI systems in the industrial
environment.

</details>


### [73] [Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience](https://arxiv.org/abs/2508.16836)
*Bicheng Wang,Junping Wang,Yibo Xue*

Main category: cs.LG

TL;DR: 산업 체인의 회복력을 예측하기 위해, 본 논문에서는 복잡한 네트워크의 진화 역학을 설명하는 물리적으로 유익한 신경 기호 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 산업 체인이 국가 경제의 지속 가능한 발전에 중요한 역할을 하고 있지만, 복잡한 네트워크의 회복력을 설명하고 분석하는데 필요한 이론적 프레임워크가 부족하다.

Method: 물리적 실체의 활동 상태 역학을 학습하고 이를 다층 시공간 공동 진화 네트워크에 통합하여 물리 기호 역학과 시공간 공동 진화 토폴로지의 공동 학습을 실현하는 물리 정보 방법을 사용한다.

Result: 모델은 산업 체인의 탄력성을 더 정확하고 효과적으로 예측할 수 있다.

Conclusion: 이 연구는 산업의 발전에 실질적인 의미를 가진다.

Abstract: Industrial chain plays an increasingly important role in the sustainable
development of national economy. However, as a typical complex network,
data-driven deep learning is still in its infancy in describing and analyzing
the resilience of complex networks, and its core is the lack of a theoretical
framework to describe the system dynamics. In this paper, we propose a
physically informative neural symbolic approach to describe the evolutionary
dynamics of complex networks for resilient prediction. The core idea is to
learn the dynamics of the activity state of physical entities and integrate it
into the multi-layer spatiotemporal co-evolution network, and use the physical
information method to realize the joint learning of physical symbol dynamics
and spatiotemporal co-evolution topology, so as to predict the industrial chain
resilience. The experimental results show that the model can obtain better
results and predict the elasticity of the industry chain more accurately and
effectively, which has certain practical significance for the development of
the industry.

</details>


### [74] [Neural Contrast Expansion for Explainable Structure-Property Prediction and Random Microstructure Design](https://arxiv.org/abs/2508.16857)
*Guangyu Nie,Yang Jiao,Yi Ren*

Main category: cs.LG

TL;DR: 이 논문은 복합재료의 효율적인 속성을 예측하기 위한 새로운 모델, Neural Contrast Expansion (NCE)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복합재료의 속성 예측은 전통적으로 PDE를 해결하거나 데이터 기반 모델을 구축하여 이루어진다.

Method: 이 논문은 강한 대비 확장(SCE) 공식을 기반으로 하여 구조-속성 데이터를 통해 대체 PDE 커널을 학습하는 Neural Contrast Expansion (NCE) 아키텍처를 제안한다.

Result: NCE 모델은 정적인 전도 및 전자기파 전파 사례에서 유용한 감도 정보를 정확하고 통찰력 있게 드러낸다.

Conclusion: 이 방법은 PDE 솔루션 필드에 대한 측정을 요구하지 않고, 물질 개발 맥락에서 더 접근 가능한 거시적 속성 측정만 필요로 한다.

Abstract: Effective properties of composite materials are defined as the ensemble
average of property-specific PDE solutions over the underlying microstructure
distributions. Traditionally, predicting such properties can be done by solving
PDEs derived from microstructure samples or building data-driven models that
directly map microstructure samples to properties. The former has a higher
running cost, but provides explainable sensitivity information that may guide
material design; the latter could be more cost-effective if the data overhead
is amortized, but its learned sensitivities are often less explainable. With a
focus on properties governed by linear self-adjoint PDEs (e.g., Laplace,
Helmholtz, and Maxwell curl-curl) defined on bi-phase microstructures, we
propose a structure-property model that is both cost-effective and explainable.
Our method is built on top of the strong contrast expansion (SCE) formalism,
which analytically maps $N$-point correlations of an unbounded random field to
its effective properties. Since real-world material samples have finite sizes
and analytical PDE kernels are not always available, we propose Neural Contrast
Expansion (NCE), an SCE-inspired architecture to learn surrogate PDE kernels
from structure-property data. For static conduction and electromagnetic wave
propagation cases, we show that NCE models reveal accurate and insightful
sensitivity information useful for material design. Compared with other PDE
kernel learning methods, our method does not require measurements about the PDE
solution fields, but rather only requires macroscopic property measurements
that are more accessible in material development contexts.

</details>


### [75] [UM3: Unsupervised Map to Map Matching](https://arxiv.org/abs/2508.16874)
*Chaolong Ying,Yinan Zhang,Lei Zhang,Jiazhuang Wang,Shujun Jia,Tianshu Yu*

Main category: cs.LG

TL;DR: 이 논문에서는 지도 데이터 정렬을 위한 무감독 그래프 기반 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 지도 간의 일치를 맞추는 중요한 작업을 수행하기 위해 이러한 접근 방식이 필요하다.

Method: 무감독 학습 방식으로, 레이블이 있는 훈련 샘플 없이도 작동하며, pseudo coordinates를 도입하여 노드의 상대적 공간 배치를 캡처하고, 특성과 기하학적 유사성을 조절하는 기구를 설계하였다.

Result: 실제 데이터셋에 대한 실험 결과, 제안한 방법이 기존 방법보다 월등히 높은 정확도를 기록하였다.

Conclusion: 이 프레임워크는 지도 정렬을 위한 확장 가능하고 실용적인 솔루션을 제공한다.

Abstract: Map-to-map matching is a critical task for aligning spatial data across
heterogeneous sources, yet it remains challenging due to the lack of ground
truth correspondences, sparse node features, and scalability demands. In this
paper, we propose an unsupervised graph-based framework that addresses these
challenges through three key innovations. First, our method is an unsupervised
learning approach that requires no training data, which is crucial for
large-scale map data where obtaining labeled training samples is challenging.
Second, we introduce pseudo coordinates that capture the relative spatial
layout of nodes within each map, which enhances feature discriminability and
enables scale-invariant learning. Third, we design an mechanism to adaptively
balance feature and geometric similarity, as well as a geometric-consistent
loss function, ensuring robustness to noisy or incomplete coordinate data. At
the implementation level, to handle large-scale maps, we develop a tile-based
post-processing pipeline with overlapping regions and majority voting, which
enables parallel processing while preserving boundary coherence. Experiments on
real-world datasets demonstrate that our method achieves state-of-the-art
accuracy in matching tasks, surpassing existing methods by a large margin,
particularly in high-noise and large-scale scenarios. Our framework provides a
scalable and practical solution for map alignment, offering a robust and
efficient alternative to traditional approaches.

</details>


### [76] [Quantifying Out-of-Training Uncertainty of Neural-Network based Turbulence Closures](https://arxiv.org/abs/2508.16891)
*Cody Grogan,Som Dhulipala,Mauricio Tano,Izabela Gutowska,Som Dutta*

Main category: cs.LG

TL;DR: 이 논문은 전통적인 난류 닫힘 모델을 위한 사전 훈련된 대체물을 사용하여 CFD 시뮬레이션의 계산 효율성과 예측 정확성을 높이기 위한 신경망 기반 난류 닫힘 모델의 불확실성 정량화를 비교한다.


<details>
  <summary>Details</summary>
Motivation: 신경망 기반 난류 닫힘 모델의 널리 사용을 방해하는 요인은 이러한 모델에 대한 상대적으로 부족한 불확실성 정량화이다.

Method: 이 논문에서는 세 가지 신경망 기반 방법과 가우시안 프로세스를 비교하기 위해 출판된 대수적 난류 닫힘을 활용하였다.

Result: 비훈련 입력에 대한 불확실성 정량화 성능에서 정확한 GP가 여전히 성능이 가장 뛰어나며, DE는 비훈련 영역에서도 비슷한 성능을 보인다.

Conclusion: 현재 문제에서 정확도 기준으로 GP > DE > SVI > MCD 순이며, DE 결과는 비교적 견고하고 직관적인 불확실성 정량화 추정을 제공한다.

Abstract: Neural-Network (NN) based turbulence closures have been developed for being
used as pre-trained surrogates for traditional turbulence closures, with the
aim to increase computational efficiency and prediction accuracy of CFD
simulations. The bottleneck to the widespread adaptation of these ML-based
closures is the relative lack of uncertainty quantification (UQ) for these
models. Especially, quantifying uncertainties associated with out-of-training
inputs, that is when the ML-based turbulence closures are queried on inputs
outside their training data regime. In the current paper, a published algebraic
turbulence closure1 has been utilized to compare the quality of epistemic UQ
between three NN-based methods and Gaussian Process (GP). The three NN-based
methods explored are Deep Ensembles (DE), Monte-Carlo Dropout (MCD), and
Stochastic Variational Inference (SVI). In the in-training results, we find the
exact GP performs the best in accuracy with a Root Mean Squared Error (RMSE) of
$2.14 \cdot 10^{-5}$ followed by the DE with an RMSE of $4.59 \cdot 10^{-4}$.
Next, the paper discusses the performance of the four methods for quantifying
out-of-training uncertainties. For performance, the Exact GP yet again is the
best in performance, but has similar performance to the DE in the
out-of-training regions. In UQ accuracy for the out-of-training case, SVI and
DE hold the best miscalibration error for one of the cases. However, the DE
performs the best in Negative Log-Likelihood for both out-of-training cases. We
observe that for the current problem, in terms of accuracy GP > DE > SV I >
MCD. The DE results are relatively robust and provide intuitive UQ estimates,
despite performing naive ensembling. In terms of computational cost, the GP is
significantly higher than the NN-based methods with a $O(n^3)$ computational
complexity for each training step

</details>


### [77] [Tri-Accel: Curvature-Aware Precision-Adaptive and Memory-Elastic Optimization for Efficient GPU Usage](https://arxiv.org/abs/2508.16905)
*Mohsen Sheibanian,Pouya Shaeri,Alimohammad Beigi,Ryan T. Woo,Aryan Keluskar*

Main category: cs.LG

TL;DR: Tri-Accel은 GPU 메모리 및 연산 시간 최적화 비용 문제를 해결하기 위해 세 가지 가속 전략을 통합하는 최적화 프레임워크로, 훈련 중에 적응형 매개변수를 함께 조정하여 훈련 시간을 최대 9.9% 단축하고 메모리 사용량을 13.3% 줄이며 정확도를 1.1% 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 딥 뉴럴 네트워크는 최적화의 비용 때문에 점점 더 병목 현상을 겪고 있으며, 이는 GPU 메모리와 연산 시간과 관련이 있다.

Method: Tri-Accel은 세 가지 가속 전략과 적응형 매개변수를 훈련 중에 공동 조정하는 통합 최적화 프레임워크이다: (1) 정밀도 적응 업데이트, (2) 희소 두 번째 신호, (3) 메모리 탄력적 배치 스케일링.

Result: CIFAR-10에서 Tri-Accel은 훈련 시간을 최대 9.9% 줄이고 메모리 사용량을 13.3% 낮추며, 정확도를 FP32 기준보다 1.1% 향상시킨다.

Conclusion: 이 프레임워크는 알고리즘 적응성과 하드웨어 인식을 결합해 자원이 제한된 환경에서 확장성을 개선할 수 있음을 보여준다.

Abstract: Deep neural networks are increasingly bottlenecked by the cost of
optimization, both in terms of GPU memory and compute time. Existing
acceleration techniques, such as mixed precision, second-order methods, and
batch size scaling, are typically used in isolation. We present Tri-Accel, a
unified optimization framework that co-adapts three acceleration strategies
along with adaptive parameters during training: (1) Precision-Adaptive Updates
that dynamically assign mixed-precision levels to layers based on curvature and
gradient variance; (2) Sparse Second-Order Signals that exploit Hessian/Fisher
sparsity patterns to guide precision and step size decisions; and (3)
Memory-Elastic Batch Scaling that adjusts batch size in real time according to
VRAM availability. On CIFAR-10 with ResNet-18 and EfficientNet-B0, Tri-Accel
achieves up to 9.9% reduction in training time and 13.3% lower memory usage,
while improving accuracy by +1.1 percentage points over FP32 baselines. Tested
on CIFAR-10/100, our approach demonstrates adaptive learning behavior, with
efficiency gradually improving over the course of training as the system learns
to allocate resources more effectively. Compared to static mixed-precision
training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint
from 0.35GB to 0.31GB on standard hardware. The framework is implemented with
custom Triton kernels, whose hardware-aware adaptation enables automatic
optimization without manual hyperparameter tuning, making it practical for
deployment across diverse computational environments. This work demonstrates
how algorithmic adaptivity and hardware awareness can be combined to improve
scalability in resource-constrained settings, paving the way for more efficient
neural network training on edge devices and cost-sensitive cloud deployments.

</details>


### [78] [Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection](https://arxiv.org/abs/2508.16915)
*Sadman Mohammad Nasif,Md Abrar Jahin,M. F. Mridha*

Main category: cs.LG

TL;DR: 이 연구는 공정하고 설명 가능한 사이버 사기 탐지를 위해 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 가정 은행 시스템의 증가된 채택은 사이버 사기의 위험을 증가시켜, 정확하면서도 공정하고 설명 가능한 사기 탐지 메커니즘을 필요로 합니다.

Method: 생물학적 영감을 받은 코르티칼 스파이킹 네트워크(CSNPC)와 강화 학습 기반 하이퍼휴리스틱 최적화기(RHOSS)를 통합하는 새로운 프레임워크를 제안합니다.

Result: 모델은 Bank Account Fraud (BAF) 데이터 세트에서 $90.8\\%$의 회수율을 달성하여 최첨단 모델들을 초월합니다.

Conclusion: 인구 코딩된 SNN과 강화 학습 기반 하이퍼휴리스틱 결합이 공정하고 투명하며 높은 성능의 사이버 사기 탐지에 기여할 수 있음을 보여줍니다.

Abstract: The growing adoption of home banking systems has heightened the risk of
cyberfraud, necessitating fraud detection mechanisms that are not only accurate
but also fair and explainable. While AI models have shown promise in this
domain, they face key limitations, including computational inefficiency, the
interpretability challenges of spiking neural networks (SNNs), and the
complexity and convergence instability of hyper-heuristic reinforcement
learning (RL)-based hyperparameter optimization. To address these issues, we
propose a novel framework that integrates a Cortical Spiking Network with
Population Coding (CSNPC) and a Reinforcement-Guided Hyper-Heuristic Optimizer
for Spiking Systems (RHOSS). The CSNPC, a biologically inspired SNN, employs
population coding for robust classification, while RHOSS uses Q-learning to
dynamically select low-level heuristics for hyperparameter optimization under
fairness and recall constraints. Embedded within the Modular Supervisory
Framework for Spiking Network Training and Interpretation (MoSSTI), the system
incorporates explainable AI (XAI) techniques, specifically, saliency-based
attribution and spike activity profiling, to increase transparency. Evaluated
on the Bank Account Fraud (BAF) dataset suite, our model achieves a $90.8\%$
recall at a strict $5\%$ false positive rate (FPR), outperforming
state-of-the-art spiking and non-spiking models while maintaining over $98\%$
predictive equality across key demographic attributes. The explainability
module further confirms that saliency attributions align with spiking dynamics,
validating interpretability. These results demonstrate the potential of
combining population-coded SNNs with reinforcement-guided hyper-heuristics for
fair, transparent, and high-performance fraud detection in real-world financial
applications.

</details>


### [79] [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
*Junxuan Wang,Xuyang Ge,Wentao Shu,Zhengfu He,Xipeng Qiu*

Main category: cs.LG

TL;DR: 이 논문은 주목 출력이 낮차원 서브스페이스에 국한되어 있으며, 이를 바탕으로 희소 오토인코더의 훈련 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 트랜스포머 모델이 고차원 숨겨진 공간에서 작동한다고 믿어지지만, 실상 주목 출력이 낮차원 서브스페이스에 제한된다는 점을 밝히고자 했다.

Method: 주목 출력의 서브스페이스를 활용한 희소 오토인코더(SAE)의 훈련 방식을 제안한다.

Result: 희소 오토인코더에서 죽은 특징 비율을 87%에서 1% 이하로 줄였다.

Conclusion: 이 연구는 주목의 기하학에 대한 새로운 통찰력을 제공하고, 대규모 언어 모델에서 희소 사전 학습을 개선하는 실용적인 도구를 제시한다.

Abstract: While transformer models are widely believed to operate in high-dimensional
hidden spaces, we show that attention outputs are confined to a surprisingly
low-dimensional subspace, where about 60\% of the directions account for 99\%
of the variance--a phenomenon that is induced by the attention output
projection matrix and consistently observed across diverse model families and
datasets. Critically, we find this low-rank structure as a fundamental cause of
the prevalent dead feature problem in sparse dictionary learning, where it
creates a mismatch between randomly initialized features and the intrinsic
geometry of the activation space. Building on this insight, we propose a
subspace-constrained training method for sparse autoencoders (SAEs),
initializing feature directions into the active subspace of activations. Our
approach reduces dead features from 87\% to below 1\% in Attention Output SAEs
with 1M features, and can further extend to other sparse dictionary learning
methods. Our findings provide both new insights into the geometry of attention
and practical tools for improving sparse dictionary learning in large language
models.

</details>


### [80] [Degree of Staleness-Aware Data Updating in Federated Learning](https://arxiv.org/abs/2508.16931)
*Tao Liu,Xuehe Wang*

Main category: cs.LG

TL;DR: 연합 학습에서 데이터 잔여 문제를 해결하기 위한 DUFL 메커니즘을 제안하고, 새로운 데이터 업데이트 전략과 이를 정량화하기 위한 새로운 지표를 도입하여 모델 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 시간 민감한 작업에서 데이터 잔여 문제를 다루는 것은 여전히 큰 도전 과제가 됩니다. 기존 연구는 잔여 문제 해결을 위해 국소 데이터 업데이트 빈도 또는 클라이언트 선택 전략을 최적화하려고 시도했지만, 잔여 문제와 데이터 양을 모두 고려한 연구는 없습니다.

Method: DUFL을 제안하며, 이는 서버의 지급, 구식 데이터 보존율 및 클라이언트의 새로운 데이터 수집량을 조정하여 데이터의 잔여와 양을 최적화하는 혁신적인 지역 데이터 업데이트 방식을 특징으로 하는 인센티브 메커니즘입니다.

Result: 실제 데이터셋에서의 실험 결과는 우리 접근 방식의 상당한 성과를 입증합니다.

Conclusion: 새로운 메트릭인 DoS(잔여 정도)를 도입하여 데이터 잔여를 정량화하고, 잔여 정도와 모델 성능 간의 정량적 관계를 설명하는 이론 분석을 수행했습니다.

Abstract: Handling data staleness remains a significant challenge in federated learning
with highly time-sensitive tasks, where data is generated continuously and data
staleness largely affects model performance. Although recent works attempt to
optimize data staleness by determining local data update frequency or client
selection strategy, none of them explore taking both data staleness and data
volume into consideration. In this paper, we propose DUFL(Data Updating in
Federated Learning), an incentive mechanism featuring an innovative local data
update scheme manipulated by three knobs: the server's payment, outdated data
conservation rate, and clients' fresh data collection volume, to coordinate
staleness and volume of local data for best utilities. To this end, we
introduce a novel metric called DoS(the Degree of Staleness) to quantify data
staleness and conduct a theoretic analysis illustrating the quantitative
relationship between DoS and model performance. We model DUFL as a two-stage
Stackelberg game with dynamic constraint, deriving the optimal local data
update strategy for each client in closed-form and the approximately optimal
strategy for the server. Experimental results on real-world datasets
demonstrate the significant performance of our approach.

</details>


### [81] [Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter](https://arxiv.org/abs/2508.16939)
*Lei Jiang,Wen Ge,Niels Cariou-Kotlarek,Mingxuan Yi,Po-Yu Chen,Lingyi Yang,Francois Buet-Golfouse,Gaurav Mittal,Hao Ni*

Main category: cs.LG

TL;DR: Sig-DEG는 사전 훈련된 확산 모델을 정제하기 위한 새로운 생성기로, 백워드 확산 프로세스를 조잡한 시간 해상도로 근본적으로 근사화할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 확산 모델은 생성 모델링에서 최첨단 결과를 달성하였으나, 추론 시 계산이 집약적이며 많은 이산화 단계를 요구한다.

Method: Sig-DEG는 고차 근사를 활용하여 브라운 운동을 효율적으로 요약하고, 반복 구조를 채택하여 SDE 솔루션의 정확한 글로벌 근사를 가능하게 한다. 이는 감독 학습 작업으로 정식화된다.

Result: 실험 결과, Sig-DEG는 경쟁력 있는 생성 품질을 달성하면서 추론 단계 수를 10배가량 줄였다.

Conclusion: 우리의 결과는 효율적인 생성 모델링을 위한 서명 기반 근사의 효과를 강조한다.

Abstract: Diffusion models have achieved state-of-the-art results in generative
modelling but remain computationally intensive at inference time, often
requiring thousands of discretization steps. To this end, we propose Sig-DEG
(Signature-based Differential Equation Generator), a novel generator for
distilling pre-trained diffusion models, which can universally approximate the
backward diffusion process at a coarse temporal resolution. Inspired by
high-order approximations of stochastic differential equations (SDEs), Sig-DEG
leverages partial signatures to efficiently summarize Brownian motion over
sub-intervals and adopts a recurrent structure to enable accurate global
approximation of the SDE solution. Distillation is formulated as a supervised
learning task, where Sig-DEG is trained to match the outputs of a
fine-resolution diffusion model on a coarse time grid. During inference,
Sig-DEG enables fast generation, as the partial signature terms can be
simulated exactly without requiring fine-grained Brownian paths. Experiments
demonstrate that Sig-DEG achieves competitive generation quality while reducing
the number of inference steps by an order of magnitude. Our results highlight
the effectiveness of signature-based approximations for efficient generative
modeling.

</details>


### [82] [Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning](https://arxiv.org/abs/2508.16949)
*Yang Zhou,Sunzhu Li,Shunyu Liu,Wenkai Fang,Jiale Zhao,Jingwen Yang,Jianwei Lv,Kongcheng Zhang,Yihe Zhou,Hengtong Lu,Wei Chen,Yan Xie,Mingli Song*

Main category: cs.LG

TL;DR: 이 논문은 Rubric-Scaffolded Reinforcement Learning(RuscaRL)이라는 새로운 학습 프레임워크를 제안하여 LLM의 추론 능력 향상을 위한 탐색 병목 현상을 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)의 발전은 강화 학습(RL)이 추론 능력을 촉진할 수 있는 잠재력을 강조하였습니다. 그러나 RL 개선은 고품질 샘플에서 학습하는 데 의존함에도 불구하고, 이러한 샘플에 대한 탐색은 LLM의 제한으로 인해 제약을 받습니다.

Method: RuscaRL은 체크리스트 스타일의 루브릭을 도입하여 롤아웃 생성 중 탐색을 위한 명시적 스캐폴딩과 모델 훈련 중 활용을 위한 검증 가능한 보상을 제공합니다.

Result: 광범위한 실험에서 RuscaRL이 여러 벤치마크에서 우수성을 입증하며, 인상적인 성과를 달성하였습니다.

Conclusion: 이 연구는 현재 진행 중이며, 코드, 모델 및 데이터셋을 곧 공개할 예정입니다.

Abstract: Recent advances in Large Language Models (LLMs) have underscored the
potential of Reinforcement Learning (RL) to facilitate the emergence of
reasoning capabilities. Despite the encouraging results, a fundamental dilemma
persists as RL improvement relies on learning from high-quality samples, yet
the exploration for such samples remains bounded by the inherent limitations of
LLMs. This, in effect, creates an undesirable cycle in which what cannot be
explored cannot be learned. In this work, we propose Rubric-Scaffolded
Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework
designed to break the exploration bottleneck for general LLM reasoning.
Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit
scaffolding for exploration during rollout generation, where different rubrics
are provided as external guidance within task instructions to steer diverse
high-quality responses. This guidance is gradually decayed over time,
encouraging the model to internalize the underlying reasoning patterns; (2)
verifiable rewards for exploitation during model training, where we can obtain
robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL
on general reasoning tasks. Extensive experiments demonstrate the superiority
of the proposed RuscaRL across various benchmarks, effectively expanding
reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL
significantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,
surpassing GPT-4.1. Furthermore, our fine-tuned variant on
Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading
LLMs including OpenAI-o3. This work is still in progress, and we will release
the code, the models, and the datasets soon.

</details>


### [83] [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions](https://arxiv.org/abs/2508.16950)
*Manan Gupta,Dhruv Kumar*

Main category: cs.LG

TL;DR: 신경망에서 다의성을 가진 뉴런을 이해하기 위한 새로운 지표인 Polysemanticity Index (PSI)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 신경망 내에서 여러 기능에 반응하는 다의성 뉴런이 기계적 해석성을 복잡하게 만들기 때문에 이에 대한 연구가 필요하다.

Method: PSI는 기하학적 군집 품질(S), 레이블이 지정된 범주와의 정렬성(Q), CLIP를 통한 개방어휘 의미적 구별성(D)의 세 가지 독립적으로 조정된 구성 요소를 곱하여 뉴런의 활성화 세트를 분석한다.

Result: PSI는 이름 붙일 수 있는 개념으로 분할된 활성화 세트를 가진 뉴런을 식별하며, 후층이 전층보다 높은 PSI 값을 나타낸다.

Conclusion: PSI는 신경망 내에서 다의성 유닛을 발견하고 질량화하며 연구하는 데 유용한 도구가 된다.

Abstract: Neural networks often contain polysemantic neurons that respond to multiple,
sometimes unrelated, features, complicating mechanistic interpretability. We
introduce the Polysemanticity Index (PSI), a null-calibrated metric that
quantifies when a neuron's top activations decompose into semantically distinct
clusters. PSI multiplies three independently calibrated components: geometric
cluster quality (S), alignment to labeled categories (Q), and open-vocabulary
semantic distinctness via CLIP (D). On a pretrained ResNet-50 evaluated with
Tiny-ImageNet images, PSI identifies neurons whose activation sets split into
coherent, nameable prototypes, and reveals strong depth trends: later layers
exhibit substantially higher PSI than earlier layers. We validate our approach
with robustness checks (varying hyperparameters, random seeds, and
cross-encoder text heads), breadth analyses (comparing class-only vs.
open-vocabulary concepts), and causal patch-swap interventions. In particular,
aligned patch replacements increase target-neuron activation significantly more
than non-aligned, random, shuffled-position, or ablate-elsewhere controls. PSI
thus offers a principled and practical lever for discovering, quantifying, and
studying polysemantic units in neural networks.

</details>


### [84] [Unveiling the Latent Directions of Reflection in Large Language Models](https://arxiv.org/abs/2508.16989)
*Fu-Chieh Chang,Yu-Ting Lee,Pei-Yuan Wu*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델의 자기 반성 능력을 탐구하며, 반성 수준을 구현하기 위해 활성화 조작 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 성능을 복잡한 추론 작업에서 향상시키기 위해 자기 반성을 연구하고자 합니다.

Method: 활성화 조작을 기반으로 한 방법론을 제안하여 반성 의도가 다른 지시의 특성을 파악합니다.

Result: 실험을 통해 반성 수준 간의 명확한 구분과 스티어링 개입의 반성 조절 가능성을 입증하였습니다.

Conclusion: 반성을 억제하는 것이 자극하는 것보다 훨씬 수월하다는 점을 강조하고, LLM의 반성적 추론에 대한 기계적 이해의 길을 엽니다.

Abstract: Reflection, the ability of large language models (LLMs) to evaluate and
revise their own reasoning, has been widely used to improve performance on
complex reasoning tasks. Yet, most prior work emphasizes designing reflective
prompting strategies or reinforcement learning objectives, leaving the inner
mechanisms of reflection underexplored. In this paper, we investigate
reflection through the lens of latent directions in model activations. We
propose a methodology based on activation steering to characterize how
instructions with different reflective intentions: no reflection, intrinsic
reflection, and triggered reflection. By constructing steering vectors between
these reflection levels, we demonstrate that (1) new reflection-inducing
instructions can be systematically identified, (2) reflective behavior can be
directly enhanced or suppressed through activation interventions, and (3)
suppressing reflection is considerably easier than stimulating it. Experiments
on GSM8k-adv with Qwen2.5-3B and Gemma3-4B reveal clear stratification across
reflection levels, and steering interventions confirm the controllability of
reflection. Our findings highlight both opportunities (e.g.,
reflection-enhancing defenses) and risks (e.g., adversarial inhibition of
reflection in jailbreak attacks). This work opens a path toward mechanistic
understanding of reflective reasoning in LLMs.

</details>


### [85] [Online Learning for Approximately-Convex Functions with Long-term Adversarial Constraints](https://arxiv.org/abs/2508.16992)
*Dhruv Sarkar,Samrat Mukhopadhyay,Abhishek Sinha*

Main category: cs.LG

TL;DR: 우리는 적대적 환경에서 장기 예산 제약이 있는 온라인 학습 문제를 연구한다. 제안한 알고리즘은 누적 비용을 최소화하고 장기 예산 제약을 만족하는데 초점을 맞춘다.


<details>
  <summary>Details</summary>
Motivation: 일반적인 비볼록 최적화 문제를 포함한 온라인 학습 알고리즘의 효율성을 개선하기 위해 제안된 문제 설정과 알고리즘을 연구한다.

Method: 각 라운드에서 학습자는 볼록 결정 세트에서 행동을 선택하고, 적은 비용 함수와 자원 소비 함수가 공개되는 상황에서 첫 번째 순서 온라인 알고리즘을 설계하여 장기 예산 제약을 만족하도록 한다.

Result: 제안한 알고리즘은 최적 고정 가능 벤치마크에 대해 $O(	ext{sqrt}(T))$ $	extalpha$-후회의 보장을 제공하면서 최대 $O(B_T 	ext{log} T) + 	ilde{O}(	ext{sqrt}(T))$의 자원을 소비한다.

Conclusion: $	extalpha$-근사 볼록 함수의 클래스에 대한 특성을 규명하고, 결과가 광범위한 문제에 적용 가능하다고 보여준다.

Abstract: We study an online learning problem with long-term budget constraints in the
adversarial setting. In this problem, at each round $t$, the learner selects an
action from a convex decision set, after which the adversary reveals a cost
function $f_t$ and a resource consumption function $g_t$. The cost and
consumption functions are assumed to be $\alpha$-approximately convex - a broad
class that generalizes convexity and encompasses many common non-convex
optimization problems, including DR-submodular maximization, Online Vertex
Cover, and Regularized Phase Retrieval. The goal is to design an online
algorithm that minimizes cumulative cost over a horizon of length $T$ while
approximately satisfying a long-term budget constraint of $B_T$. We propose an
efficient first-order online algorithm that guarantees $O(\sqrt{T})$
$\alpha$-regret against the optimal fixed feasible benchmark while consuming at
most $O(B_T \log T)+ \tilde{O}(\sqrt{T})$ resources in both full-information
and bandit feedback settings. In the bandit feedback setting, our approach
yields an efficient solution for the $\texttt{Adversarial Bandits with
Knapsacks}$ problem with improved guarantees. We also prove matching lower
bounds, demonstrating the tightness of our results. Finally, we characterize
the class of $\alpha$-approximately convex functions and show that our results
apply to a broad family of problems.

</details>


### [86] [Learned Structure in CARTRIDGES: Keys as Shareable Routers in Self-Studied Representations](https://arxiv.org/abs/2508.17032)
*Maurizio Diaz*

Main category: cs.LG

TL;DR: 이 논문은 CARTRIDGES라는 접근법을 통해 긴 문맥 LLM 추론의 병목현상을 해결하기 위한 방법을 제안하고, CARTRIDGE 키-값 캐시 구조를 기계적으로 탐색합니다.


<details>
  <summary>Details</summary>
Motivation: 긴 문맥 LLM 추론에서 KV 캐시의 선형 성장으로 인한 병목현상을 해결하고자 함.

Method: OFFLINE 컴퓨팅을 이용하여 전체 문서에 필요한 것보다 훨씬 작은 KV 캐시를 학습하는 CARTRIDGES 접근법을 제안.

Result: CARTRIDGE 키는 안정적이고 공유 가능한 검색 라우터 역할을 하며, 대부분의 학습된 압축이 CARTRIDGE 값 벡터 내에서 발생함을 입증.

Conclusion: Sampled Chunk Initialization(SCI)라는 약간의 초기화 개선안을 제안하고, 이는 CARTRIDGE 수렴 속도를 이전 문헌보다 빠르게 할 수 있음을 시사.

Abstract: A bottleneck for long-context LLM inference is the linearly growing KV cache.
Recent work has proposed CARTRIDGES, an approach which leverages offline
compute to train a much smaller KV cache than is typically required for a full
document (up to 40x less memory usage at inference time). In this paper, we
present the first mechanistic exploration of the learned CARTRIDGE key-value
cache structure. In particular, we propose that (1) CARTRIDGE keys act as
stable, shareable retrieval routers for the compressed corpora and (2) most of
the learned compression occurs within the CARTRIDGE value vectors. We present
empirical evidence of our routing theory across tasks, model families, and
model sizes; for example, we can ablate the learned CARTRIDGE key vectors
between tasks with little performance loss. Finally, we propose a slight
improvement in initialization called Sampled Chunk Initialization (SCI). We
suggest that SCI can lead to faster CARTRIDGE convergence than previously
demonstrated in the literature. Our findings lay the groundwork for broader
empirical study of CARTRIDGE training optimization which may be crucial for
further scaling.

</details>


### [87] [TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression](https://arxiv.org/abs/2508.17056)
*Kiran Madhusudhanan,Vijaya Krishna Yalavarthi,Jonas Sonntag,Maximilian Stubbemann,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: Tabular regression의 한계를 극복하기 위해 TabResFlow라는 모델을 제안하며, 이는 기존의 확률적 회귀 모델을 능가하는 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 산업 자동화에서 신뢰할 수 있는 의사 결정이 필수적이므로, 확률적 회귀 모델을 통해 예측 불확실성을 모델링하는 것이 중요하다.

Method: TabResFlow는 단변량 테이블 회귀에 맞게 설계된 Normalizing Spline Flow 모델로 세 가지 주요 구성 요소로 이루어져 있다: (1) 각 수치 특성을 위한 MLP 인코더, (2) 특징 추출을 위한 완전 연결 ResNet 백본, (3) 유연하고 관리 가능한 밀도 추정을 위한 조건부 스플라인 기반의 정규화 흐름.

Result: TabResFlow는 9개의 공개 베타 데이터셋에서 기존의 확률적 회귀 모델을 지속적으로 초월하는 성능을 보이며, TreeFlow와 비교했을 때 9.64%의 개선을 이루고, NodeFlow에 비해 추론 시간에서 평균 5.6배의 속도 향상을 기록한다.

Conclusion: TabResFlow는 선택적 회귀 아래에서 실제 중고차 가격 예측 작업에서도 우수한 성능을 보여주며, 새로운 위험 범위 아래 면적(AURC) 메트릭을 도입하여 이 메트릭에서도 탁월한 결과를 달성한다.

Abstract: Tabular regression is a well-studied problem with numerous industrial
applications, yet most existing approaches focus on point estimation, often
leading to overconfident predictions. This issue is particularly critical in
industrial automation, where trustworthy decision-making is essential.
Probabilistic regression models address this challenge by modeling prediction
uncertainty. However, many conventional methods assume a fixed-shape
distribution (typically Gaussian), and resort to estimating distribution
parameters. This assumption is often restrictive, as real-world target
distributions can be highly complex. To overcome this limitation, we introduce
TabResFlow, a Normalizing Spline Flow model designed specifically for
univariate tabular regression, where commonly used simple flow networks like
RealNVP and Masked Autoregressive Flow (MAF) are unsuitable. TabResFlow
consists of three key components: (1) An MLP encoder for each numerical
feature. (2) A fully connected ResNet backbone for expressive feature
extraction. (3) A conditional spline-based normalizing flow for flexible and
tractable density estimation. We evaluate TabResFlow on nine public benchmark
datasets, demonstrating that it consistently surpasses existing probabilistic
regression models on likelihood scores. Our results demonstrate 9.64%
improvement compared to the strongest probabilistic regression model
(TreeFlow), and on average 5.6 times speed-up in inference time compared to the
strongest deep learning alternative (NodeFlow). Additionally, we validate the
practical applicability of TabResFlow in a real-world used car price prediction
task under selective regression. To measure performance in this setting, we
introduce a novel Area Under Risk Coverage (AURC) metric and show that
TabResFlow achieves superior results across this metric.

</details>


### [88] [Learning ON Large Datasets Using Bit-String Trees](https://arxiv.org/abs/2508.17083)
*Prashant Gupta*

Main category: cs.LG

TL;DR: 본 논문은 유사성 유지 해싱, 분류 및 암 유전체학에서의 계산 방법을 개발합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 이진 탐색 트리(BST)에 기반한 해싱 방식의 비효율성을 개선하기 위해.

Method: Compressed BST of Inverted hash tables(ComBI)와 Guided Random Forest(GRAF) 및 Continuous Representation of Codon Switches(CRCS)와 같은 새로운 방법론을 도입합니다.

Result: ComBI는 Multi-Index Hashing에 비해 4배에서 296배 빠른 속도로 0.90의 정밀도를 달성하며, GRAF는 115개의 데이터셋에서 경쟁력 있는 정확도를 제공합니다.

Conclusion: 이 방법들은 대규모 데이터 분석 및 생물의학적 응용을 위한 효율적이고 확장 가능하며 해석 가능한 도구를 제공합니다.

Abstract: This thesis develops computational methods in similarity-preserving hashing,
classification, and cancer genomics. Standard space partitioning-based hashing
relies on Binary Search Trees (BSTs), but their exponential growth and sparsity
hinder efficiency. To overcome this, we introduce Compressed BST of Inverted
hash tables (ComBI), which enables fast approximate nearest-neighbor search
with reduced memory. On datasets of up to one billion samples, ComBI achieves
0.90 precision with 4X-296X speed-ups over Multi-Index Hashing, and also
outperforms Cellfishing.jl on single-cell RNA-seq searches with 2X-13X gains.
Building on hashing structures, we propose Guided Random Forest (GRAF), a
tree-based ensemble classifier that integrates global and local partitioning,
bridging decision trees and boosting while reducing generalization error.
Across 115 datasets, GRAF delivers competitive or superior accuracy, and its
unsupervised variant (uGRAF) supports guided hashing and importance sampling.
We show that GRAF and ComBI can be used to estimate per-sample classifiability,
which enables scalable prediction of cancer patient survival. To address
challenges in interpreting mutations, we introduce Continuous Representation of
Codon Switches (CRCS), a deep learning framework that embeds genetic changes
into numerical vectors. CRCS allows identification of somatic mutations without
matched normals, discovery of driver genes, and scoring of tumor mutations,
with survival prediction validated in bladder, liver, and brain cancers.
Together, these methods provide efficient, scalable, and interpretable tools
for large-scale data analysis and biomedical applications.

</details>


### [89] [Convolutional Neural Networks for Accurate Measurement of Train Speed](https://arxiv.org/abs/2508.17096)
*Haitao Tian,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.LG

TL;DR: 이 연구에서는 현대 철도 시스템의 복잡한 문제를 해결하기 위해 기차 속도 추정 정확도를 개선하기 위한 합성곱 신경망(CNN)의 사용을 탐구합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 철도 시스템의 복잡한 도전 과제를 해결하기 위해 기차 속도 추정의 정확성을 높이려는 필요성이 있습니다.

Method: 단일 분기 2D, 단일 분기 1D, 다중 분기 모델의 세 가지 CNN 아키텍처를 조사하고 이를 적응형 칼만 필터와 비교했습니다.

Result: CNN 기반 접근 방식 특히 다중 분기 모델이 전통적인 방법에 비해 정확도와 강건성에서 우수한 성능을 보였습니다.

Conclusion: 딥 러닝 기술이 복잡한 운송 데이터셋에서 정교한 패턴을 더 효과적으로 포착함으로써 철도 안전 및 운영 효율성을 향상시킬 잠재력이 있음을 강조합니다.

Abstract: In this study, we explore the use of Convolutional Neural Networks for
improving train speed estimation accuracy, addressing the complex challenges of
modern railway systems. We investigate three CNN architectures - single-branch
2D, single-branch 1D, and multiple-branch models - and compare them with the
Adaptive Kalman Filter. We analyse their performance using simulated train
operation datasets with and without Wheel Slide Protection activation. Our
results reveal that CNN-based approaches, especially the multiple-branch model,
demonstrate superior accuracy and robustness compared to traditional methods,
particularly under challenging operational conditions. These findings highlight
the potential of deep learning techniques to enhance railway safety and
operational efficiency by more effectively capturing intricate patterns in
complex transportation datasets.

</details>


### [90] [Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process](https://arxiv.org/abs/2508.17097)
*Lingkai Kong,Haotian Sun,Yuchen Zhuang,Haorui Wang,Wenhao Mu,Chao Zhang*

Main category: cs.LG

TL;DR: 새로운 불확실성 인지 및 해석 가능한 그래프 분류 모델을 제안하고, 이 방법이 기존 방법들보다 우수함을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 그래프 신경망(GNN)은 강력한 도구이나 예측의 비율이 맞지 않고 해석 가능성이 부족합니다.

Method: 그래프 기능 신경 프로세스와 그래프 생성 모델을 결합한 새로운 모델을 제안하며, 잠재적 근거를 가정하고 이를 확률적 임베딩 공간에 매핑합니다.

Result: 제안된 방법은 다섯 개의 그래프 분류 데이터셋에서 최첨단 방법들을 초월하는 성능을 보입니다.

Conclusion: 디코딩된 근거 구조가 의미 있는 설명을 제공할 수 있음을 보여줍니다.

Abstract: Graph neural networks (GNNs) are powerful tools on graph data. However, their
predictions are mis-calibrated and lack interpretability, limiting their
adoption in critical applications. To address this issue, we propose a new
uncertainty-aware and interpretable graph classification model that combines
graph functional neural process and graph generative model. The core of our
method is to assume a set of latent rationales which can be mapped to a
probabilistic embedding space; the predictive distribution of the classifier is
conditioned on such rationale embeddings by learning a stochastic correlation
matrix. The graph generator serves to decode the graph structure of the
rationales from the embedding space for model interpretability. For efficient
model training, we adopt an alternating optimization procedure which mimics the
well known Expectation-Maximization (EM) algorithm. The proposed method is
general and can be applied to any existing GNN architecture. Extensive
experiments on five graph classification datasets demonstrate that our
framework outperforms state-of-the-art methods in both uncertainty
quantification and GNN interpretability. We also conduct case studies to show
that the decoded rationale structure can provide meaningful explanations.

</details>


### [91] [Reconciling Communication Compression and Byzantine-Robustness in Distributed Learning](https://arxiv.org/abs/2508.17129)
*Diksha Gupta,Nirupam Gupta,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: 이 논문에서는 분산 학습에서 발생하는 비잔틴 오류와 높은 통신 비용의 상호작용을 다루는 새로운 알고리즘 RoSDHB를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 분산 학습은 분산된 데이터에서의 모델 학습을 가능하게 하지만, 비잔틴 오류와 높은 통신 비용으로 어려움을 겪고 있다.

Method: RoSDHB 알고리즘은 클래식 폴리악 모멘텀을 새로운 협력 압축 메커니즘과 통합하여 설계되었다.

Result: RoSDHB는 정형화된 (G, B)-그래디언트 비유사성 이질성 모델 하에서 Byz-DASHA-PAGE와 유사한 성능을 보이면서도 더 적은 가정에 의존한다.

Conclusion: RoSDHB는 이미지 분류 작업에서 강력한 견고성을 보여주며, 상당한 통신 절약을 달성한다.

Abstract: Distributed learning (DL) enables scalable model training over decentralized
data, but remains challenged by Byzantine faults and high communication costs.
While both issues have been studied extensively in isolation, their interaction
is less explored. Prior work shows that naively combining communication
compression with Byzantine-robust aggregation degrades resilience to faulty
nodes (or workers). The state-of-the-art algorithm, namely Byz-DASHA-PAGE [29],
makes use of the momentum variance reduction scheme to mitigate the detrimental
impact of compression noise on Byzantine-robustness. We propose a new
algorithm, named RoSDHB, that integrates the classic Polyak's momentum with a
new coordinated compression mechanism. We show that RoSDHB performs comparably
to Byz-DASHA-PAGE under the standard (G, B)-gradient dissimilarity
heterogeneity model, while it relies on fewer assumptions. In particular, we
only assume Lipschitz smoothness of the average loss function of the honest
workers, in contrast to [29]that additionally assumes a special smoothness of
bounded global Hessian variance. Empirical results on benchmark image
classification task show that RoSDHB achieves strong robustness with
significant communication savings.

</details>


### [92] [SACA: Selective Attention-Based Clustering Algorithm](https://arxiv.org/abs/2508.17150)
*Meysam Shirdel Bilehsavar,Razieh Ghaedi,Samira Seyed Taheri,Xinqi Fan,Christian O'Reilly*

Main category: cs.LG

TL;DR: 이 논문은 사용자 정의 매개변수의 필요성을 최소화하는 새로운 밀도 기반 클러스터링 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 클러스터링 알고리즘은 다양한 응용 분야에서 널리 사용되고 있으며, 특히 밀도 기반 방법이 두드러진다.

Method: 이 알고리즘은 초기에는 사용자 정의 매개변수를 요구하지 않으며, 필요할 경우 조정하기 쉬운 단일 정수 매개변수를 도입한다.

Result: 실험 평가 결과, 이 방법은 밀도 기반 클러스터링 작업에 효과적인 대안임을 보여준다.

Conclusion: 제안된 방법은 다양한 데이터 집합에서 접근 가능성과 강력한 성능을 나타내었다.

Abstract: Clustering algorithms are widely used in various applications, with
density-based methods such as Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) being particularly prominent. These algorithms identify
clusters in high-density regions while treating sparser areas as noise.
However, reliance on user-defined parameters often poses optimization
challenges that require domain expertise. This paper presents a novel
density-based clustering method inspired by the concept of selective attention,
which minimizes the need for user-defined parameters under standard conditions.
Initially, the algorithm operates without requiring user-defined parameters. If
parameter adjustment is needed, the method simplifies the process by
introducing a single integer parameter that is straightforward to tune. The
approach computes a threshold to filter out the most sparsely distributed
points and outliers, forms a preliminary cluster structure, and then
reintegrates the excluded points to finalize the results. Experimental
evaluations on diverse data sets highlight the accessibility and robust
performance of the method, providing an effective alternative for density-based
clustering tasks.

</details>


### [93] [MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices](https://arxiv.org/abs/2508.17137)
*Nishant Gavhane,Arush Mehrotra,Rohit Chawla,Peter Proenca*

Main category: cs.LG

TL;DR: MoE 모델을 엣지 디바이스에 배치하는 데 따른 메모리 제약 문제를 해결하기 위해 학습 기반의 전문가 활성화 예측기를 도입하여 성능을 개선한 연구.


<details>
  <summary>Details</summary>
Motivation: 대규모 MoE 모델의 엣지 디바이스 배치에서 메모리 제한 문제를 해결하기 위해.

Method: MoE-Beyond라는 학습 기반 전문가 활성화 예측기를 도입하여, 다중 레이블 시퀀스 예측 문제로 구성하고 경량 변환기 모델을 훈련시킴.

Result: MoE-Beyond는 97.5% 정확도와 86.6% F1 점수를 달성하며, GPU 캐시 적중률을 17%에서 72%로 개선함.

Conclusion: MoE-Beyond는 기존의 휴리스틱 기반 기법들을 초월하는 성능을 보여줌.

Abstract: The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices
presents significant challenges due to memory constraints. While MoE
architectures enable efficient utilization of computational resources by
activating only a subset of experts per inference, they require careful memory
management to operate efficiently in resource-constrained environments.
Traditional heuristic-based expert caching strategies such as MoE-Infinity
struggle to maintain high cache hit rates as models parameters scale. In this
work, we introduce MoE-Beyond, a learning-based expert activation predictor
trained to predict expert activations during autoregressive decoding. By
framing the task as a multi-label sequence prediction problem, we train a
lightweight transformer model on 66 million expert activation traces extracted
from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor
generalizes effectively across unseen prompts from WebGLM-QA dataset [6],
achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that
MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts
fit in GPU cache, outperforming heuristic baselines.

</details>


### [94] [Stochastic Gradient Descent with Strategic Querying](https://arxiv.org/abs/2508.17144)
*Nanfei Jiang,Hoi-To Wai,Mahnoosh Alizadeh*

Main category: cs.LG

TL;DR: 이 논문은 일차 쿼리를 이용한 유한합 최적화 문제를 다루며, 균일한 쿼리 전략과 비교해 전략적 쿼리가 확률적 경량 기반 방법에 미치는 이점을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 일반적인 쿼리 전략이 아닌 보다 전략적인 쿼리를 통해 최적화 성능을 향상시키고자 한다.

Method: Oracle Gradient Querying (OGQ)와 Strategic Gradient Querying (SGQ) 알고리즘을 제안하며, 각각의 성능과 이점을 비교한다.

Result: OGQ는 전이 상태 성능을 향상시키고 안정 상태 분산을 줄이며, SGQ는 SGD보다 전이 상태 성능을 개선함을 보여준다.

Conclusion: 이론적 발견을 수치 실험으로 검증하였다.

Abstract: This paper considers a finite-sum optimization problem under first-order
queries and investigates the benefits of strategic querying on stochastic
gradient-based methods compared to uniform querying strategy. We first
introduce Oracle Gradient Querying (OGQ), an idealized algorithm that selects
one user's gradient yielding the largest possible expected improvement (EI) at
each step. However, OGQ assumes oracle access to the gradients of all users to
make such a selection, which is impractical in real-world scenarios. To address
this limitation, we propose Strategic Gradient Querying (SGQ), a practical
algorithm that has better transient-state performance than SGD while making
only one query per iteration. For smooth objective functions satisfying the
Polyak-Lojasiewicz condition, we show that under the assumption of EI
heterogeneity, OGQ enhances transient-state performance and reduces
steady-state variance, while SGQ improves transient-state performance over SGD.
Our numerical experiments validate our theoretical findings.

</details>


### [95] [ONG: Orthogonal Natural Gradient Descent](https://arxiv.org/abs/2508.17169)
*Yajat Yadav,Jathin Korrapati,Patrick Mendoza*

Main category: cs.LG

TL;DR: ONG(Orthogonal Natural Gradient Descent) 알고리즘은 오르토고날 경량 하강법과 자연 경량의 아이디어를 결합하여 지속적 학습 작업에서 더 나은 수렴 성능을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 오르토고날 경량 하강법의 유클리드 투영은 신경망에 의해 매개변수화된 분포 공간의 정보 기하학적 구조를 간과하여 학습 작업에서 하위 최적 수렴을 초래할 수 있습니다.

Method: ONG는 각 새로운 작업의 경량을 효율적인 EKFAC 근사를 사용하여 피셔 정보 행렬의 역행렬로 전처리하며, 리만 계량 하에서 가장 가파른 하강 방향을 따르는 업데이트를 생성합니다.

Result: ONG는 이전에 학습한 작업의 성능을 유지하기 위해 이러한 자연 경량을 이전 작업 경량의 직교 보완에 투영합니다. 이를 통해 지속적 학습의 성능을 향상시킵니다.

Conclusion: 이 절차에 대한 이론적 정당성을 제공하고 ONG 알고리즘을 도입하여 Permuted 및 Rotated MNIST 데이터셋에서 성능을 벤치마킹합니다.

Abstract: Orthogonal gradient descent has emerged as a powerful method for continual
learning tasks. However, its Euclidean projections overlook the underlying
information-geometric structure of the space of distributions parametrized by
neural networks, which can lead to suboptimal convergence in learning tasks. To
counteract this, we combine it with the idea of the natural gradient and
present ONG (Orthogonal Natural Gradient Descent). ONG preconditions each new
task gradient with an efficient EKFAC approximation of the inverse Fisher
information matrix, yielding updates that follow the steepest descent direction
under a Riemannian metric. To preserve performance on previously learned tasks,
ONG projects these natural gradients onto the orthogonal complement of prior
task gradients. We provide a theoretical justification for this procedure,
introduce the ONG algorithm, and benchmark its performance on the Permuted and
Rotated MNIST datasets. All code for our experiments/reproducibility can be
found at https://github.com/yajatyadav/orthogonal-natural-gradient.

</details>


### [96] [Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention](https://arxiv.org/abs/2508.17175)
*Leon Dimitrov*

Main category: cs.LG

TL;DR: 그래프가 기계 학습에서 관계적 및 구조적 데이터를 포착하는 중심 표현으로 자리잡고 있다. 전통적인 그래프 신경망은 지역 구조로 인해 노드 간의 장기 의존성 포착에 어려움을 겪는다. 그래프 변환기는 주의 메커니즘을 사용하여 노드가 전 세계적으로 정보를 교환할 수 있게 한다.


<details>
  <summary>Details</summary>
Motivation: 그래프 구조적 데이터의 장기 의존성을 효과적으로 포착하기 위한 필요성이 있다.

Method: 그래프 변환기의 두 가지 주의 메커니즘인 밀집 및 희소 주의를 비교하고 분석한다.

Result: 각 주의 메커니즘의 장점과 단점을 강조하며 언제 어떤 것을 사용할지를 명확히 한다.

Conclusion: 그래프 변환기를 위한 주의 설계에서 현재의 도전 과제와 문제를 개략적으로 설명한다.

Abstract: Graphs have become a central representation in machine learning for capturing
relational and structured data across various domains. Traditional graph neural
networks often struggle to capture long-range dependencies between nodes due to
their local structure. Graph transformers overcome this by using attention
mechanisms that allow nodes to exchange information globally. However, there
are two types of attention in graph transformers: dense and sparse. In this
paper, we compare these two attention mechanisms, analyze their trade-offs, and
highlight when to use each. We also outline current challenges and problems in
designing attention for graph transformers.

</details>


### [97] [Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks](https://arxiv.org/abs/2508.17158)
*Jack Youstra,Mohammed Mahfoud,Yang Yan,Henry Sleight,Ethan Perez,Mrinank Sharma*

Main category: cs.LG

TL;DR: 이 논문은 대규모 언어 모델의 세부 조정 API의 안전성 문제를 다루며, Cipher Fine-tuning Robustness 벤치마크(CIFR)를 소개하여 방어 전략을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 세부 조정 API는 광범위한 모델 커스터마이징을 가능하게 하지만, 안전성 위험이 크다.

Method: CIFR이라는 벤치마크를 도입하고, 다양한 암호 인코딩과 패밀리를 포함하여 방어를 평가한다.

Result: 탐지 정확도가 99%를 초과하고, 이전에 보지 못한 암호 변형 및 패밀리에 일반화된다.

Conclusion: CIFR과 실험 재현을 위한 코드를 오픈소스 제공하며, 이 중요한 분야의 추가 연구를 촉진한다.

Abstract: Large language model fine-tuning APIs enable widespread model customization,
yet pose significant safety risks. Recent work shows that adversaries can
exploit access to these APIs to bypass model safety mechanisms by encoding
harmful content in seemingly harmless fine-tuning data, evading both human
monitoring and standard content filters. We formalize the fine-tuning API
defense problem, and introduce the Cipher Fine-tuning Robustness benchmark
(CIFR), a benchmark for evaluating defense strategies' ability to retain model
safety in the face of cipher-enabled attackers while achieving the desired
level of fine-tuning functionality. We include diverse cipher encodings and
families, with some kept exclusively in the test set to evaluate for
generalization across unseen ciphers and cipher families. We then evaluate
different defenses on the benchmark and train probe monitors on model internal
activations from multiple fine-tunes. We show that probe monitors achieve over
99% detection accuracy, generalize to unseen cipher variants and families, and
compare favorably to state-of-the-art monitoring approaches. We open-source
CIFR and the code to reproduce our experiments to facilitate further research
in this critical area. Code and data are available online
https://github.com/JackYoustra/safe-finetuning-api

</details>


### [98] [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
*Hikaru Tsujimura,Arush Tagade*

Main category: cs.LG

TL;DR: 대형 언어 모델은 고위험 맥락에서 과도한 자신감을 보이며, 이 행동의 내부 기제를 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 과도한 자신감을 이해하고 이를 완화하는 방법 모색.

Method: 인간이 주석을 단 단호함 데이터 세트로 파인 튜닝된 오픈 소스 Llama 3.2 모델을 사용하여 모든 레이어의 잔여 활성화를 추출하고 유사성 메트릭을 계산하여 단호한 표현을 위치화한다.

Result: 단호함 대비에 가장 민감한 레이어를 식별하고, 높은 단호한 표현이 감정과 논리 클러스터의 두 정사각형 하위 구성 요소로 분해됨을 드러낸다.

Conclusion: 이 발견은 대형 언어 모델의 단호함에 대한 다중 구성 요소 구조를 기계적으로 증명하고, 과도한 자신감을 완화할 수 있는 경로를 밝힌다.

Abstract: Large Language Models (LLMs) often display overconfidence, presenting
information with unwarranted certainty in high-stakes contexts. We investigate
the internal basis of this behavior via mechanistic interpretability. Using
open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness
datasets, we extract residual activations across all layers, and compute
similarity metrics to localize assertive representations. Our analysis
identifies layers most sensitive to assertiveness contrasts and reveals that
high-assertive representations decompose into two orthogonal sub-components of
emotional and logical clusters-paralleling the dual-route Elaboration
Likelihood Model in Psychology. Steering vectors derived from these
sub-components show distinct causal effects: emotional vectors broadly
influence prediction accuracy, while logical vectors exert more localized
effects. These findings provide mechanistic evidence for the multi-component
structure of LLM assertiveness and highlight avenues for mitigating
overconfident behavior.

</details>


### [99] [BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens](https://arxiv.org/abs/2508.17196)
*Hao Wen,Xinrui Wu,Yi Sun,Feifei Zhang,Liye Chen,Jie Wang,Yunxin Liu,Ya-Qin Zhang,Yuanchun Li*

Main category: cs.LG

TL;DR: BudgetThinker는 LLM의 예산 인식을 통해 사고 과정을 제어할 수 있는 새로운 프레임워크로, 수학적 기준에서 성능을 유지하며 예산에 최적화된 추론을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 최근 대형 언어 모델의 발전은 테스트 시간에 계산을 증가시켜 추론 능력을 향상시키는 것을 목표로 합니다. 그러나 이 접근법은 지연과 자원 비용이 크고 현실 세계에서의 적용 가능성을 제한합니다.

Method: 우리는 추론 중에 특수 제어 토큰을 주기적으로 삽입하여 모델이 남은 토큰 예산을 지속적으로 인지하도록 하는 방법론을 제안합니다. 이 접근법은 Supervised Fine-Tuning(SFT)과 커리큘럼 기반 강화 학습(RL) 단계로 구성된 포괄적인 두 단계 훈련 파이프라인과 결합됩니다.

Result: BudgetThinker는 다양한 추론 예산을 유지하면서 뛰어난 성과를 보이며 강력한 기준선을 초과합니다.

Conclusion: 이 방법은 자원이 제한된 실시간 환경에서도 효율적이고 제어 가능한 LLM 추론 개발을 위한 확장 가능하고 효과적인 솔루션을 제공합니다.

Abstract: Recent advancements in Large Language Models (LLMs) have leveraged increased
test-time computation to enhance reasoning capabilities, a strategy that, while
effective, incurs significant latency and resource costs, limiting their
applicability in real-world time-constrained or cost-sensitive scenarios. This
paper introduces BudgetThinker, a novel framework designed to empower LLMs with
budget-aware reasoning, enabling precise control over the length of their
thought processes. We propose a methodology that periodically inserts special
control tokens during inference to continuously inform the model of its
remaining token budget. This approach is coupled with a comprehensive two-stage
training pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize
the model with budget constraints, followed by a curriculum-based Reinforcement
Learning (RL) phase that utilizes a length-aware reward function to optimize
for both accuracy and budget adherence. We demonstrate that BudgetThinker
significantly surpasses strong baselines in maintaining performance across a
variety of reasoning budgets on challenging mathematical benchmarks. Our method
provides a scalable and effective solution for developing efficient and
controllable LLM reasoning, making advanced models more practical for
deployment in resource-constrained and real-time environments.

</details>


### [100] [Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection](https://arxiv.org/abs/2508.17174)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.LG

TL;DR: 본 연구에서는 적대적 인식 샘플을 OOD 샘플과 구별하는 강력한 OOD 탐지 방법을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 현재 OOD 탐지 알고리즘은 적대적 ID 샘플을 OOD로 간주하는 문제를 해결하기 위해 필요하다.

Method: Sharpness-aware Geometric Defense (SaGD) 프레임워크를 도입하여 적대적 손실 경관을 매끄럽게 만들어준다.

Result: SaGD 프레임워크는 다양한 공격 하에서 CIFAR-100과 여섯 개의 OOD 데이터 세트를 구별하는 데 있어 FPR 및 AUC를 크게 향상시킨다.

Conclusion: 이 연구는 구분력을 높임으로써 적대적 공격에 대한 OOD 탐지 성능을 향상시킨다.

Abstract: Out-of-distribution (OOD) detection ensures safe and reliable model
deployment. Contemporary OOD algorithms using geometry projection can detect
OOD or adversarial samples from clean in-distribution (ID) samples. However,
this setting regards adversarial ID samples as OOD, leading to incorrect OOD
predictions. Existing efforts on OOD detection with ID and OOD data under
attacks are minimal. In this paper, we develop a robust OOD detection method
that distinguishes adversarial ID samples from OOD ones. The sharp loss
landscape created by adversarial training hinders model convergence, impacting
the latent embedding quality for OOD score calculation. Therefore, we introduce
a {\bf Sharpness-aware Geometric Defense (SaGD)} framework to smooth out the
rugged adversarial loss landscape in the projected latent geometry. Enhanced
geometric embedding convergence enables accurate ID data characterization,
benefiting OOD detection against adversarial attacks. We use Jitter-based
perturbation in adversarial training to extend the defense ability against
unseen attacks. Our SaGD framework significantly improves FPR and AUC over the
state-of-the-art defense approaches in differentiating CIFAR-100 from six other
OOD datasets under various attacks. We further examine the effects of
perturbations at various adversarial training levels, revealing the
relationship between the sharp loss landscape and adversarial OOD detection.

</details>


### [101] [GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning](https://arxiv.org/abs/2508.17218)
*Xing Wei,Yuqi Ouyang*

Main category: cs.LG

TL;DR: 이 연구는 확률적 교통 네트워크에서의 신뢰할 수 있는 최단 경로 문제를 다루며, 시간 종속성을 고려한 경로 계획 솔루션을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 도시 지역의 차량 수 증가로 인해 기존 도로 인프라가 현대의 교통 수요를 수용하지 못하고 있으며, 이는 혼잡 문제를 초래하고 있다.

Method: 본 연구는 결정 변환기와 일반화된 정책 경량화(GPG) 프레임워크를 통합한 경로 계획 솔루션을 제안한다.

Result: 수확 결과, 제안한 접근 방식이 정시 도착 확률 측면에서 이전 기준을 능가하며 보다 정확한 경로 계획 솔루션을 제공한다.

Conclusion: 결론적으로, 제안된 솔루션은 경로 결정의 정확성과 안정성을 향상시키는 데 기여한다.

Abstract: With the rapidly increased number of vehicles in urban areas, existing road
infrastructure struggles to accommodate modern traffic demands, resulting in
the issue of congestion. This highlights the importance of efficient path
planning strategies. However, most recent navigation models focus solely on
deterministic or time-dependent networks, while overlooking the correlations
and the stochastic nature of traffic flows. In this work, we address the
reliable shortest path problem within stochastic transportation networks under
certain dependencies. We propose a path planning solution that integrates the
decision Transformer with the Generalized Policy Gradient (GPG) framework.
Based on the decision Transformer's capability to model long-term dependencies,
our proposed solution improves the accuracy and stability of path decisions.
Experimental results on the Sioux Falls Network (SFN) demonstrate that our
approach outperforms previous baselines in terms of on-time arrival
probability, providing more accurate path planning solutions.

</details>


### [102] [Module-Aware Parameter-Efficient Machine Unlearning on Transformers](https://arxiv.org/abs/2508.17233)
*Wenjie Bao,Jian Lou,Yuke Hu,Xiaochen Li,Zhihao Liu,Jiaqi Liu,Zhan Qin,Kui Ren*

Main category: cs.LG

TL;DR: 이 논문은 Transformer 모델에서 기계적 학습 제거를 위한 MAPE-Unlearn 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 개인정보 보호 규제를 준수하기 위해 특정 데이터의 영향을 효율적으로 제거하는 기계적 학습 제거의 필요성이 커지고 있다.

Method: MAPE-Unlearn은 Transformer의 헤드와 필터에서 영향이 중요한 파라미터를 정확하게 찾기 위해 학습 가능한 마스크 쌍을 사용하는 모듈 인식 파라미터 효율 기계 학습 제거 접근 방식이다.

Result: 다양한 Transformer 모델과 데이터셋에 대한 광범위한 실험을 통해 MAPE-Unlearn의 효과성과 강건성이 입증되었다.

Conclusion: 제안된 MAPE-Unlearn은 Transformer 모델에서 부정확한 파라미터 식별 문제를 해결할 수 있는 가능성을 보여준다.

Abstract: Transformer has become fundamental to a vast series of pre-trained large
models that have achieved remarkable success across diverse applications.
Machine unlearning, which focuses on efficiently removing specific data
influences to comply with privacy regulations, shows promise in restricting
updates to influence-critical parameters. However, existing parameter-efficient
unlearning methods are largely devised in a module-oblivious manner, which
tends to inaccurately identify these parameters and leads to inferior
unlearning performance for Transformers. In this paper, we propose {\tt
MAPE-Unlearn}, a module-aware parameter-efficient machine unlearning approach
that uses a learnable pair of masks to pinpoint influence-critical parameters
in the heads and filters of Transformers. The learning objective of these masks
is derived by desiderata of unlearning and optimized through an efficient
algorithm featured by a greedy search with a warm start. Extensive experiments
on various Transformer models and datasets demonstrate the effectiveness and
robustness of {\tt MAPE-Unlearn} for unlearning.

</details>


### [103] [Provable Generalization in Overparameterized Neural Nets](https://arxiv.org/abs/2508.17256)
*Aviral Dhingra*

Main category: cs.LG

TL;DR: 이 연구에서는 주의 기반 모델의 효과적 랭크를 기반으로 대규모 언어 모델의 일반화 경계를 규명하고 이를 통해 모델이 왜 일반화되는지를 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 딥 뉴럴 네트워크는 학습 예제보다 훨씬 더 많은 매개변수를 포함하지만, 여전히 좋은 일반화를 보여준다. 전통적인 복잡성 측정 방법은 이러한 과매개변수화된 상태에서 유의미한 설명을 제공하지 않는다.

Method: 주의 기반 모델의 효과적 랭크를 탐구하고, 이 양이 대규모 언어 모델에서 관찰된 경험적 스케일링 법칙과 일치하는 일반화 경계를 이끌어낸다.

Result: 이러한 분석을 통해 주의의 스펙트럼 속성이 매개변수 수치보다 이러한 모델이 일반화되는 이유를 이해하는 데 더 적합할 수 있음을 입증한다.

Conclusion: 주요 매개변수 수치에 의한 전통적 접근이 아닌, 주의의 스펙트럼 속성에서 모델의 일반화 가능성을 이해할 수 있는 새로운 통찰을 제공한다.

Abstract: Deep neural networks often contain far more parameters than training
examples, yet they still manage to generalize well in practice. Classical
complexity measures such as VC-dimension or PAC-Bayes bounds usually become
vacuous in this overparameterized regime, offering little explanation for the
empirical success of models like Transformers. In this work, I explore an
alternative notion of capacity for attention-based models, based on the
effective rank of their attention matrices. The intuition is that, although the
parameter count is enormous, the functional dimensionality of attention is
often much lower. I show that this quantity leads to a generalization bound
whose dependence on sample size matches empirical scaling laws observed in
large language models, up to logarithmic factors. While the analysis is not a
complete theory of overparameterized learning, it provides evidence that
spectral properties of attention, rather than raw parameter counts, may be the
right lens for understanding why these models generalize.

</details>


### [104] [Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning](https://arxiv.org/abs/2508.17387)
*Yicong Wu,Guangyue Lu,Yuan Zuo,Huarong Zhang,Junjie Wu*

Main category: cs.LG

TL;DR: GNN을 사용하지 않고 그래프 작업을 풀기 위한 새로운 접근법을 제안하며, LRMs를 활용하여 노드 분류, 링크 예측 및 그래프 분류를 텍스트 추론 문제로 변환한다.


<details>
  <summary>Details</summary>
Motivation: 특정 작업에 대한 감독 없이 보이지 않는 그래프 작업으로 일반화하는 것이 여전히 도전적인 문제이다.

Method: 우리는 노드 분류, 링크 예측, 그리고 그래프 분류를 텍스트 추론 문제로 재구성하는 GNN 비사용 접근법을 제안한다.

Result: Graph-R1은 제로샷 환경에서 최첨단 기준을 초월하였고, 해석 가능하고 효과적인 예측을 생성하였다.

Conclusion: 우리의 연구는 그래프 학습에서 명시적 추론의 가능성을 강조하고 향후 연구를 위한 새로운 자원을 제공한다.

Abstract: Generalizing to unseen graph tasks without task-pecific supervision remains
challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,
while Large Language Models (LLMs) lack structural inductive biases. Recent
advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via
explicit, long chain-of-thought reasoning. Inspired by this, we propose a
GNN-free approach that reformulates graph tasks--node classification, link
prediction, and graph classification--as textual reasoning problems solved by
LRMs. We introduce the first datasets with detailed reasoning traces for these
tasks and develop Graph-R1, a reinforcement learning framework that leverages
task-specific rethink templates to guide reasoning over linearized graphs.
Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in
zero-shot settings, producing interpretable and effective predictions. Our work
highlights the promise of explicit reasoning for graph learning and provides
new resources for future research.

</details>


### [105] [Curvature Learning for Generalization of Hyperbolic Neural Networks](https://arxiv.org/abs/2508.17232)
*Xiaomeng Fan,Yuwei Wu,Zhi Gao,Mehrtash Harandi,Yunde Jia*

Main category: cs.LG

TL;DR: 이 논문은 곡률이 하이퍼볼릭 신경망(HNN)의 일반화에 미치는 영향을 이론적으로 설명하고, 곡률 기반 학습 방법을 제안하여 HNN의 성능을 개선하는 방법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 하이퍼볼릭 신경망에서 곡률의 적절한 선택이 성능 최적화에 중요함을 강조하며, 이에 대한 이론적 토대가 부족함을 지적합니다.

Method: PAC-베이즈 일반화 경계 도출, 곡률을 고려한 손실 경관의 평활화를 위한 민감도 인식 곡률 학습 방법 제안.

Result: 제안된 방법의 근사 오차와 수렴 분석을 통해 HNN의 성능이 개선됨을 실험적으로 입증합니다.

Conclusion: 곡률이 HNN의 일반화 성능에 실질적인 영향을 미친다는 것을 보여줍니다.

Abstract: Hyperbolic neural networks (HNNs) have demonstrated notable efficacy in
representing real-world data with hierarchical structures via exploiting the
geometric properties of hyperbolic spaces characterized by negative curvatures.
Curvature plays a crucial role in optimizing HNNs. Inappropriate curvatures may
cause HNNs to converge to suboptimal parameters, degrading overall performance.
So far, the theoretical foundation of the effect of curvatures on HNNs has not
been developed. In this paper, we derive a PAC-Bayesian generalization bound of
HNNs, highlighting the role of curvatures in the generalization of HNNs via
their effect on the smoothness of the loss landscape. Driven by the derived
bound, we propose a sharpness-aware curvature learning method to smooth the
loss landscape, thereby improving the generalization of HNNs. In our method,
  we design a scope sharpness measure for curvatures, which is minimized
through a bi-level optimization process. Then, we introduce an implicit
differentiation algorithm that efficiently solves the bi-level optimization by
approximating gradients of curvatures. We present the approximation error and
convergence analyses of the proposed method, showing that the approximation
error is upper-bounded, and the proposed method can converge by bounding
gradients of HNNs. Experiments on four settings: classification, learning from
long-tailed data, learning from noisy data, and few-shot learning show that our
method can improve the performance of HNNs.

</details>


### [106] [Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs](https://arxiv.org/abs/2508.17400)
*Jacob Portes,Connor Jennings,Erica Ji Yuen,Sasha Doubov,Michael Carbin*

Main category: cs.LG

TL;DR: 본 연구에서는 LLM 모델 크기에 따른 검색 성능을 평가하였으며, 퍼미터 수와 데이터셋 규모에 따른 성능 변화를 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 검색 성능이 사전 훈련 FLOPs와 어떻게 관계되는지를 이해하기 위해서.

Method: 125백만에서 70억 파라미터까지의 다양한 LLM 모델 크기를 대상으로 하여 10억 토큰에서 2조 토큰 이상의 데이터셋으로 사전 훈련한 모델들을 벤치마킹하였다.

Result: LLM 크기, 훈련 기간 및 추정 FLOPs에 따라 제로샷 BEIR 작업에 대한 검색 성능이 예측 가능하게 증가하는 것을 발견하였다. 또한 In-Context Learning 점수가 검색 작업 간의 검색 점수와 강한 상관관계를 가지고 있음을 보였다.

Conclusion: 이 연구는 LLM 기반 검색기의 개발에 대한 시사점을 강조하고 있다.

Abstract: How does retrieval performance scale with pretraining FLOPs? We benchmark
retrieval performance across LLM model sizes from 125 million parameters to 7
billion parameters pretrained on datasets ranging from 1 billion tokens to more
than 2 trillion tokens. We find that retrieval performance on zero-shot BEIR
tasks predictably scales with LLM size, training duration, and estimated FLOPs.
We also show that In-Context Learning scores are strongly correlated with
retrieval scores across retrieval tasks. Finally, we highlight the implications
this has for the development of LLM-based retrievers.

</details>


### [107] [Convergence and Generalization of Anti-Regularization for Parametric Models](https://arxiv.org/abs/2508.17412)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: Anti-regularization (AR)은 적은 샘플 환경에서 모델의 표현력을 의도적으로 증가시키기 위해 손실에 sign-reversed 보상 항을 추가하고, 샘플 크기가 커짐에 따라 이 개입을 점진적으로 감소시키는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 모델의 표현력을 늘리기 위해 적은 샘플 환경에서의 손실이 필요하다.

Method: sign-reversed 보상 항을 손실에 추가하며, 샘플 크기에 따라 파워-로 법칙으로 감소시키는 안정성 보호 장치 설계.

Result: AR은 언더피팅을 줄이고 일반화 및 보정을 개선하여 회귀 및 분류 모두에서 효과를 보입니다.

Conclusion: AR은 간단하게 구현 가능하며, 데이터와 자원이 제한된 환경에서도 효과적인 학습을 가능하게 합니다.

Abstract: We propose Anti-regularization (AR), which adds a sign-reversed reward term
to the loss to intentionally increase model expressivity in the small-sample
regime, and then attenuates this intervention with a power-law decay as the
sample size grows. We formalize spectral safety and trust-region conditions,
and design a lightweight stability safeguard that combines a projection
operator with gradient clipping, ensuring stable intervention under stated
assumptions. Our analysis spans linear smoothers and the Neural Tangent Kernel
(NTK) regime, providing practical guidance on selecting the decay exponent by
balancing empirical risk against variance. Empirically, AR reduces underfitting
while preserving generalization and improving calibration in both regression
and classification. Ablation studies confirm that the decay schedule and the
stability safeguard are critical to preventing overfitting and numerical
instability. We further examine a degrees-of-freedom targeting schedule that
keeps per-sample complexity approximately constant. AR is simple to implement
and reproducible, integrating cleanly into standard empirical risk minimization
pipelines. It enables robust learning in data- and resource-constrained
settings by intervening only when beneficial and fading away when unnecessary.

</details>


### [108] [Multimodal Representation Learning Conditioned on Semantic Relations](https://arxiv.org/abs/2508.17497)
*Yang Qiao,Yuntong Hu,Liang Zhao*

Main category: cs.LG

TL;DR: 관계 조건 멀티모달 학습(RCML) 프레임워크를 제안하여, 자연어 관계 설명을 통해 멀티모달 표현을 학습하고 성능을 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 기존의 대비 모델들이 이미지-텍스트 쌍에 초점을 맞추는 동시에, 다양한 쌍 간의 의미적 관계를 충분히 활용하지 못하는 한계를 극복하기 위해.

Method: RCML 프레임워크는 자연어 관계 설명을 기반으로 멀티모달 표현을 학습하고, 의미적 관계로 연결된 다대다 훈련 쌍을 구성하며, 관계 기반 교차 주의 메커니즘을 도입하여 각 관계 맥락 하에 멀티모달 표현을 조정.

Result: 다양한 데이터셋에서 실험 결과, RCML은 검색 및 분류 작업에서 강력한 기준선을 일관되게 초과 성과를 보이며 의미적 관계 활용의 효과성을 입증.

Conclusion: 의미적 관계를 활용한 멀티모달 표현 학습의 효과를 강조하며, RCML이 훌륭한 성능을 발휘함을 보여줌.

Abstract: Multimodal representation learning has advanced rapidly with contrastive
models such as CLIP, which align image-text pairs in a shared embedding space.
However, these models face limitations: (1) they typically focus on image-text
pairs, underutilizing the semantic relations across different pairs. (2) they
directly match global embeddings without contextualization, overlooking the
need for semantic alignment along specific subspaces or relational dimensions;
and (3) they emphasize cross-modal contrast, with limited support for
intra-modal consistency. To address these issues, we propose
Relation-Conditioned Multimodal Learning RCML, a framework that learns
multimodal representations under natural-language relation descriptions to
guide both feature extraction and alignment. Our approach constructs
many-to-many training pairs linked by semantic relations and introduces a
relation-guided cross-attention mechanism that modulates multimodal
representations under each relation context. The training objective combines
inter-modal and intra-modal contrastive losses, encouraging consistency across
both modalities and semantically related samples. Experiments on different
datasets show that RCML consistently outperforms strong baselines on both
retrieval and classification tasks, highlighting the effectiveness of
leveraging semantic relations to guide multimodal representation learning.

</details>


### [109] [DeepCFD: Efficient near-ground airfoil lift coefficient approximation with deep convolutional neural networks](https://arxiv.org/abs/2508.17278)
*Mohammad Amin Esabat,Saeed Jaamei,Fatemeh Asadi*

Main category: cs.LG

TL;DR: 본 연구에서는 VGG 신경망을 이용하여 지면 근처의 에어포일에 대한 양력-항력 계수를 예측합니다.


<details>
  <summary>Details</summary>
Motivation: CFD 소프트웨어를 이용한 에어포일의 공기역학 계수 예측은 시간이 많이 소요되지만, 새로운 신경망 방법의 개발로 이를 개선할 수 있습니다.

Method: 신경망을 사용하여 지면 근처의 에어포일에 대한 양력-항력 계수를 예측합니다.

Result: VGG 방법을 사용한 예측 결과는 다른 CNN 방법보다 더 정확합니다.

Conclusion: 본 연구는 VGG 신경망을 활용하여 공기역학적 계수를 보다 효율적으로 예측할 수 있음을 보여줍니다.

Abstract: . Predicting and calculating the aerodynamic coefficients of airfoils near
the ground with CFD software requires much time. However, the availability of
data from CFD simulation results and the development of new neural network
methods have made it possible to present the simulation results using methods
like VGG, a CCN neural network method. In this article, lift-to-drag
coefficients of airfoils near the ground surface are predicted with the help of
a neural network. This prediction can only be realized by providing data for
training and learning the code that contains information on the lift-to-drag
ratio of the primary data and images related to the airfoil cross-section,
which are converted into a matrix. One advantage of the VGG method over other
methods is that its results are more accurate than those of other CNN methods.

</details>


### [110] [TANDEM: Temporal Attention-guided Neural Differential Equations for Missingness in Time Series Classification](https://arxiv.org/abs/2508.17519)
*YongKyung Oh,Dong-Young Lim,Sungil Kim,Alex Bui*

Main category: cs.LG

TL;DR: TANDEM은 결측치가 있는 시계열 데이터를 효과적으로 분류하는 신경 미분 방정식 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 시계열 분류에서 결측 데이터 처리 문제는 여전히 주요 도전 과제가 되고 있다.

Method: 이 연구에서는 주의 메커니즘을 활용하여 원시 관측치, 보간된 제어 경로 및 지속적인 잠재 동력을 통합하는 TANDEM 프레임워크를 제안한다.

Result: TANDEM은 30개의 벤치마크 데이터셋과 실제 의료 데이터셋에서 기존의 최첨단 방법들보다 우수한 성능을 보였다.

Conclusion: 이 프레임워크는 분류 정확도를 개선할 뿐만 아니라 결측 데이터 처리에 대한 통찰을 제공하여 실제로 유용한 도구가 된다.

Abstract: Handling missing data in time series classification remains a significant
challenge in various domains. Traditional methods often rely on imputation,
which may introduce bias or fail to capture the underlying temporal dynamics.
In this paper, we propose TANDEM (Temporal Attention-guided Neural Differential
Equations for Missingness), an attention-guided neural differential equation
framework that effectively classifies time series data with missing values. Our
approach integrates raw observation, interpolated control path, and continuous
latent dynamics through a novel attention mechanism, allowing the model to
focus on the most informative aspects of the data. We evaluate TANDEM on 30
benchmark datasets and a real-world medical dataset, demonstrating its
superiority over existing state-of-the-art methods. Our framework not only
improves classification accuracy but also provides insights into the handling
of missing data, making it a valuable tool in practice.

</details>


### [111] [Explainable AI (XAI) for Arrhythmia detection from electrocardiograms](https://arxiv.org/abs/2508.17294)
*Joschka Beck,Arlene John*

Main category: cs.LG

TL;DR: 딥 러닝의 발전으로 심전도 신호에서 높은 정확도로 부정맥을 탐지할 수 있게 되었지만, 해석 가능성이 임상 적용의 장벽으로 남아 있다. 이 연구는 시계열 심전도 분석을 위해 특별히 적응된 설명 가능한 인공지능(XAI) 기법을 적용하는 것을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 부정맥 검출의 임상 적용을 위한 해석 가능성의 필요성.

Method: MIT-BIH 부정맥 데이터셋을 이용하여 R-피크 기반의 세그멘테이션을 적용한 컨볼루션 신경망 모델을 개발하였다. 추가적인 12-리드 심전도 데이터셋을 포함하여 데이터셋 크기를 증가시키고 클래스 불균형을 줄였다.

Result: 모델은 MIT-BIH 데이터셋에서 98.3%의 검증 정확도를 달성했으나, 결합 데이터셋에서는 성능 저하가 나타났다. 여성기에 이유를 설명할 수 있는 시각적 결과의 다양성이 강조되었다.

Conclusion: 부정맥 분석에서 도메인 특화 XAI 적응의 필요성이 강조되며, 주목도 맵이 더 임상적으로 직관적인 접근법으로 강조된다.

Abstract: Advancements in deep learning have enabled highly accurate arrhythmia
detection from electrocardiogram (ECG) signals, but limited interpretability
remains a barrier to clinical adoption. This study investigates the application
of Explainable AI (XAI) techniques specifically adapted for time-series ECG
analysis. Using the MIT-BIH arrhythmia dataset, a convolutional neural
network-based model was developed for arrhythmia classification, with
R-peak-based segmentation via the Pan-Tompkins algorithm. To increase the
dataset size and to reduce class imbalance, an additional 12-lead ECG dataset
was incorporated. A user needs assessment was carried out to identify what kind
of explanation would be preferred by medical professionals. Medical
professionals indicated a preference for saliency map-based explanations over
counterfactual visualisations, citing clearer correspondence with ECG
interpretation workflows. Four SHapley Additive exPlanations (SHAP)-based
approaches: permutation importance, KernelSHAP, gradient-based methods, and
Deep Learning Important FeaTures (DeepLIFT), were implemented and compared. The
model achieved 98.3% validation accuracy on MIT-BIH but showed performance
degradation on the combined dataset, underscoring dataset variability
challenges. Permutation importance and KernelSHAP produced cluttered visual
outputs, while gradient-based and DeepLIFT methods highlighted waveform regions
consistent with clinical reasoning, but with variability across samples.
Findings emphasize the need for domain-specific XAI adaptations in ECG analysis
and highlight saliency mapping as a more clinically intuitive approach

</details>


### [112] [Activation Transport Operators](https://arxiv.org/abs/2508.17540)
*Andrzej Szablewski,Marek Masiak*

Main category: cs.LG

TL;DR: 본 연구에서는 잔여 스트림을 통해 전이 모델에서의 기능 흐름 메커니즘을 탐구하여, 기능 전송 연산자(ATO)를 도입하고, 이들이 선형 전송을 확인하는 방법과 잔여 스트림의 효율성을 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 잔여 스트림을 통한 기능 흐름 메커니즘의 이해는 모델의 오류를 조기에 감지하고 수정하는 데 기여할 수 있습니다.

Method: 적층구조에서 잔여 스트림의 선형 지도를 평가하기 위해 기능 공간에서 하단 SAE 디코더 프로젝션을 사용하는 활성화 전송 연산자(ATO) 제안을 포함하고 있습니다.

Result: 실험적으로 ATO가 기능이 이전 층에서 선형적으로 전송되었는지 또는 비선형 층 계산에서 합성되었는지를 결정할 수 있음을 입증했습니다.

Conclusion: 이 연구는 안전성, 디버깅 및 LLM에서 선형적으로 작동하는 계산의 명확한 시각을 제공하는 실용적인 도구를 제공합니다.

Abstract: The residual stream mediates communication between transformer decoder layers
via linear reads and writes of non-linear computations. While sparse-dictionary
learning-based methods locate features in the residual stream, and activation
patching methods discover circuits within the model, the mechanism by which
features flow through the residual stream remains understudied. Understanding
this dynamic can better inform jailbreaking protections, enable early detection
of model mistakes, and their correction. In this work, we propose Activation
Transport Operators (ATO), linear maps from upstream to downstream residuals
$k$ layers later, evaluated in feature space using downstream SAE decoder
projections. We empirically demonstrate that these operators can determine
whether a feature has been linearly transported from a previous layer or
synthesised from non-linear layer computation. We develop the notion of
transport efficiency, for which we provide an upper bound, and use it to
estimate the size of the residual stream subspace that corresponds to linear
transport. We empirically demonstrate the linear transport, report transport
efficiency and the size of the residual stream's subspace involved in linear
transport. This compute-light (no finetuning, <50 GPU-h) method offers
practical tools for safety, debugging, and a clearer picture of where
computation in LLMs behaves linearly.

</details>


### [113] [Physics-informed neural network for fatigue life prediction of irradiated austenitic and ferritic/martensitic steels](https://arxiv.org/abs/2508.17303)
*Dhiraj S Kori,Abhinav Chandraker,Syed Abdur Rahman,Punit Rathore,Ankur Chauhan*

Main category: cs.LG

TL;DR: 물리 기반 신경망(PINN)을 이용하여 원자력 reactors에서 사용되는 방사선 조사된 오스테나이트 및 페라이트/마르텐사이트 강재의 저주기 피로 수명을 예측하는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 경험적 모델들이 방사선 조사된 강재의 복잡한 열화 현상을 정확하게 포착하지 못하기 때문에 새로운 예측 프레임워크가 필요하다.

Method: PINN 모델은 손실 함수에 물리적 피로 수명 제약을 통합하였으며, 495개의 데이터 포인트를 사용하여 학습하였다.

Result: 이 모델은 전통적 기계 학습 모델보다 뛰어난 예측 성능을 보이며, SHapley Additive exPlanations 분석을 통해 피로 수명에 음의 상관관계를 가진 주요 특성을 확인하였다.

Conclusion: PINN 프레임워크는 방사선 조사된 합금의 피로 수명 예측을 위한 신뢰할 수 있고 해석 가능한 접근법을 제공한다.

Abstract: This study proposes a Physics-Informed Neural Network (PINN) framework to
predict the low-cycle fatigue (LCF) life of irradiated austenitic and
ferritic/martensitic (F/M) steels used in nuclear reactors. These materials
experience cyclic loading and irradiation at elevated temperatures, causing
complex degradation that traditional empirical models fail to capture
accurately. The developed PINN model incorporates physical fatigue life
constraints into its loss function, improving prediction accuracy and
generalizability. Trained on 495 data points, including both irradiated and
unirradiated conditions, the model outperforms traditional machine learning
models like Random Forest, Gradient Boosting, eXtreme Gradient Boosting, and
the conventional Neural Network. SHapley Additive exPlanations analysis
identifies strain amplitude, irradiation dose, and testing temperature as
dominant features, each inversely correlated with fatigue life, consistent with
physical understanding. PINN captures saturation behaviour in fatigue life at
higher strain amplitudes in F/M steels. Overall, the PINN framework offers a
reliable and interpretable approach for predicting fatigue life in irradiated
alloys, enabling informed alloy selection.

</details>


### [114] [In-Context Algorithm Emulation in Fixed-Weight Transformers](https://arxiv.org/abs/2508.17550)
*Jerry Yao-Chieh Hu,Hude Liu,Jennifer Yuntong Zhang,Han Liu*

Main category: cs.LG

TL;DR: 기본 가중치를 고정한 최소 Transformer 아키텍처가 다양한 알고리즘을 상-context 프롬프트를 통해 에뮬레이션할 수 있음을 증명한다.


<details>
  <summary>Details</summary>
Motivation: 최소 Transformer 아키텍처가 알고리즘을 효과적으로 에뮬레이트할 수 있는지 확인하기 위해.

Method: 고정 가중치의 어텐션 헤드를 통해 구현 가능한 알고리즘을 위한 프롬프트를 구성하여, 이는 두 개의 레이어 소프트맥스 어텐션 모듈이 알고리즘의 출력을 임의의 정밀도로 재현하도록 유도한다.

Result: 프롬프트에 의해 모든 적응이 이루어지며, 이는 알고리즘의 매개변수를 토큰 표현으로 인코딩하여 소프트맥스 어텐션이 의도한 계산을 따르도록 강제하는 방식으로 진행된다.

Conclusion: 이 결과는 GPT 스타일의 기본 모델들이 프롬프트만으로 알고리즘을 교체할 수 있는 형태의 알고리즘적 보편성을 확립함을 보여준다.

Abstract: We prove that a minimal Transformer architecture with frozen weights is
capable of emulating a broad class of algorithms by in-context prompting. In
particular, for any algorithm implementable by a fixed-weight attention head
(e.g. one-step gradient descent or linear/ridge regression), there exists a
prompt that drives a two-layer softmax attention module to reproduce the
algorithm's output with arbitrary precision. This guarantee extends even to a
single-head attention layer (using longer prompts if necessary), achieving
architectural minimality. Our key idea is to construct prompts that encode an
algorithm's parameters into token representations, creating sharp dot-product
gaps that force the softmax attention to follow the intended computation. This
construction requires no feed-forward layers and no parameter updates. All
adaptation happens through the prompt alone. These findings forge a direct link
between in-context learning and algorithmic emulation, and offer a simple
mechanism for large Transformers to serve as prompt-programmable libraries of
algorithms. They illuminate how GPT-style foundation models may swap algorithms
via prompts alone, establishing a form of algorithmic universality in modern
Transformer models.

</details>


### [115] [AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations](https://arxiv.org/abs/2508.17320)
*Yifei Yao,Mengnan Du*

Main category: cs.LG

TL;DR: 본 논문은 입력 복잡성에 따라 동적으로 희소성을 조정하는 새로운 프레임워크인 Adaptive Top K Sparse Autoencoders(AdaptiveK)를 제안하며, 이는 고정 희소성 접근 방식보다 우수한 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 내부 표현을 이해하는 것은 해석 가능성 연구의 중요한 도전 과제이다.

Method: Adaptive Top K Sparse Autoencoders(AdaptiveK)라는 새로운 프레임워크를 제안하며, 입력의 의미적 복잡성에 따라 희소성 수준을 동적으로 조정한다.

Result: 세 가지 언어 모델(Pythia-70M, Pythia-160M, Gemma-2-2B)에서 실험을 통해 복잡성에 기반한 적응이 고정 희소성 접근 방식보다 재구성 충실도, 설명된 분산 및 코사인 유사성 지표에서 현저히 우수함을 입증했다.

Conclusion: 이 접근법은 광범위한 하이퍼파라미터 조정의 계산 부담을 없애면서 성능을 개선한다.

Abstract: Understanding the internal representations of large language models (LLMs)
remains a central challenge for interpretability research. Sparse autoencoders
(SAEs) offer a promising solution by decomposing activations into interpretable
features, but existing approaches rely on fixed sparsity constraints that fail
to account for input complexity. We propose Adaptive Top K Sparse Autoencoders
(AdaptiveK), a novel framework that dynamically adjusts sparsity levels based
on the semantic complexity of each input. Leveraging linear probes, we
demonstrate that context complexity is linearly encoded in LLM representations,
and we use this signal to guide feature allocation during training. Experiments
across three language models (Pythia-70M, Pythia-160M, and Gemma-2-2B)
demonstrate that this complexity-driven adaptation significantly outperforms
fixed-sparsity approaches on reconstruction fidelity, explained variance, and
cosine similarity metrics while eliminating the computational burden of
extensive hyperparameter tuning.

</details>


### [116] [ControlEchoSynth: Boosting Ejection Fraction Estimation Models via Controlled Video Diffusion](https://arxiv.org/abs/2508.17631)
*Nima Kondori,Hanwen Liang,Hooman Vaseli,Bingyu Xie,Christina Luong,Purang Abolmaesumi,Teresa Tsang,Renjie Liao*

Main category: cs.LG

TL;DR: 이 연구는 심장 평가를 위한 합성 에코뷰 생성을 통해 심장 초음파 진단의 정확성을 높이는 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 심장 초음파 데이터 수집의 어려움을 해결하기 위한 필요성이 있다.

Method: 기존의 실제 심장 뷰를 조건으로 하여 에코뷰를 합성하는 조건부 생성 모델을 통합하였다.

Result: EF 추정 정확도가 개선되었으며 전통적인 방법과 비교 분석하였다.

Conclusion: 합성 에코가 기존 데이터 세트를 보강하는 데 사용될 때 EF 추정뿐만 아니라 보다 견고하고 정확한 ML 모델 개발에도 기여할 것으로 예상된다.

Abstract: Synthetic data generation represents a significant advancement in boosting
the performance of machine learning (ML) models, particularly in fields where
data acquisition is challenging, such as echocardiography. The acquisition and
labeling of echocardiograms (echo) for heart assessment, crucial in
point-of-care ultrasound (POCUS) settings, often encounter limitations due to
the restricted number of echo views available, typically captured by operators
with varying levels of experience. This study proposes a novel approach for
enhancing clinical diagnosis accuracy by synthetically generating echo views.
These views are conditioned on existing, real views of the heart, focusing
specifically on the estimation of ejection fraction (EF), a critical parameter
traditionally measured from biplane apical views. By integrating a conditional
generative model, we demonstrate an improvement in EF estimation accuracy,
providing a comparative analysis with traditional methods. Preliminary results
indicate that our synthetic echoes, when used to augment existing datasets, not
only enhance EF estimation but also show potential in advancing the development
of more robust, accurate, and clinically relevant ML models. This approach is
anticipated to catalyze further research in synthetic data applications, paving
the way for innovative solutions in medical imaging diagnostics.

</details>


### [117] [Is the Frequency Principle always valid?](https://arxiv.org/abs/2508.17323)
*Qijia Zhai*

Main category: cs.LG

TL;DR: 이 연구에서는 고정 및 훈련 가능한 뉴런 방향을 고려하여 단위 구면에서 ReLU 신경망의 학습 역학을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 신경망 학습 과정의 동역학을 이해하고, 고정 및 훈련 가능한 뉴런 방향이 학습에 미치는 영향을 분석하는 것.

Method: 단위 구면에서 신경망의 하모닉 진화 방정식을 사용하여 학습 과정을 분석.

Result: 훈련 가능한 방향이 학습 복잡성을 증가시키고, 저주파수 유리 또는 고주파수 신속한 출현을 가능하게 함을 보여준다.

Conclusion: 주파수 원칙(FP)은 곡면과 같은 구부러진 도메인에서 규칙이라기보다는 경향으로 간주해야 한다는 점을 제안한다.

Abstract: We investigate the learning dynamics of shallow ReLU neural networks on the
unit sphere \(S^2\subset\mathbb{R}^3\) in polar coordinates \((\tau,\phi)\),
considering both fixed and trainable neuron directions \(\{w_i\}\). For fixed
weights, spherical harmonic expansions reveal an intrinsic low-frequency
preference with coefficients decaying as \(O(\ell^{5/2}/2^\ell)\), typically
leading to the Frequency Principle (FP) of lower-frequency-first learning.
However, this principle can be violated under specific initial conditions or
error distributions. With trainable weights, an additional rotation term in the
harmonic evolution equations preserves exponential decay with decay order
\(O(\ell^{7/2}/2^\ell)\) factor, also leading to the FP of
lower-frequency-first learning. But like fixed weights case, the principle can
be violated under specific initial conditions or error distributions. Our
numerical results demonstrate that trainable directions increase learning
complexity and can either maintain a low-frequency advantage or enable faster
high-frequency emergence. This analysis suggests the FP should be viewed as a
tendency rather than a rule on curved domains like \(S^2\), providing insights
into how direction updates and harmonic expansions shape frequency-dependent
learning.

</details>


### [118] [Robustness Feature Adapter for Efficient Adversarial Training](https://arxiv.org/abs/2508.17680)
*Quanwei Wu,Jun Guo,Wei Wang,Yi Wang*

Main category: cs.LG

TL;DR: 본 논문에서는 적대적 훈련의 효율성을 개선하기 위한 새로운 어댑터 기반 접근법을 제안하고, 이를 통해 계산 효율성과 모델 정확도를 향상시킴을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 적대적 공격에 대한 모델의 견고성을 개선하기 위해 가장 인기 있는 방법인 적대적 훈련은 대형 백본 모델에 적용할 경우 계산 비용이 매우 증가한다. 이러한 문제와 함께 강한 과적합 문제가 발생하는데, 본 논문에서는 이러한 두 가지 문제를 동시에 해결하고자 한다.

Method: 효율적인 적대적 훈련을 위해 특징 공간에서 직접 적용할 수 있는 새로운 어댑터 기반 접근법을 제안한다.

Result: 제안된 어댑터 기반 접근법은 강한 과적합을 제거함으로써 내부 루프 수렴 품질을 개선하고, 계산 효율성을 현저히 증가시키며, 보지 못한 공격에 대한 적대적 견고성을 일반화하여 모델 정확도를 향상시킨다.

Conclusion: 다양한 백본 아키텍처와 대규모 적대적 훈련에서 새로운 어댑터 기반 접근법의 효과iveness을 증명한다.

Abstract: Adversarial training (AT) with projected gradient descent is the most popular
method to improve model robustness under adversarial attacks. However,
computational overheads become prohibitively large when AT is applied to large
backbone models. AT is also known to have the issue of robust overfitting. This
paper contributes to solving both problems simultaneously towards building more
trustworthy foundation models. In particular, we propose a new adapter-based
approach for efficient AT directly in the feature space. We show that the
proposed adapter-based approach can improve the inner-loop convergence quality
by eliminating robust overfitting. As a result, it significantly increases
computational efficiency and improves model accuracy by generalizing
adversarial robustness to unseen attacks. We demonstrate the effectiveness of
the new adapter-based approach in different backbone architectures and in AT at
scale.

</details>


### [119] [ShortListing Model: A Streamlined SimplexDiffusion for Discrete Variable Generation](https://arxiv.org/abs/2508.17345)
*Yuxuan Song,Zhe Zhang,Yu Pei,Jingjing Gong,Qiying Yu,Zheng Zhang,Mingxuan Wang,Hao Zhou,Jingjing Liu,Wei-Ying Ma*

Main category: cs.LG

TL;DR: SLM은 단순체 중심을 기반으로 하는 새로운 확산 모델로, 자연어 처리 및 생물학적 서열 설계 분야에서의 생성적 모델링 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 자연어 처리와 생물학적 서열 설계에서 생성적 모델링은 핵심적인 문제이지만 복잡하다.

Method: SLM은 단순체 중심에서 작동하며, 생성 복잡성을 줄이고 확장성을 향상시키는 새로운 단순체 기반의 확산 모델이다.

Result: DNA 프로모터 및 강화제 설계, 단백질 설계, 문자 수준 및 대규모 어휘 언어 모델링에 대한 광범위한 실험에서 SLM의 경쟁력 있는 성능과 강력한 잠재력을 입증하였다.

Conclusion: SLM은 조건이 없는 생성 성능을 향상시키기 위해 비분류기 기반의 지침을 유연하게 구현하고 있으며, 해당 코드가 GitHub에 공개되어 있다.

Abstract: Generative modeling of discrete variables is challenging yet crucial for
applications in natural language processing and biological sequence design. We
introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model
inspired by progressive candidate pruning. SLM operates on simplex centroids,
reducing generation complexity and enhancing scalability. Additionally, SLM
incorporates a flexible implementation of classifier-free guidance, enhancing
unconditional generation performance. Extensive experiments on DNA promoter and
enhancer design, protein design, character-level and large-vocabulary language
modeling demonstrate the competitive performance and strong potential of SLM.
Our code can be found at https://github.com/GenSI-THUAIR/SLM

</details>


### [120] [Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery](https://arxiv.org/abs/2508.17681)
*Robert Yang*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델이 새로운 지식을 생성하는지 아니면 기억된 조각을 재구성하는지에 대한 질문을 다룬다. 우리는 과학적 발견의 검증 가능한 접근 방법으로서 '잊기-절단(unlearning-as-ablation)'을 제안하며, 수학 및 알고리즘에서 이 개념의 실현 가능성을 보여주기 위한 최소한의 파일럿을 설명하고 있다. 이는 개념적이고 방법론적인 기여를 목표로 하며, AI-for-Science 분야에서 지식 재구성이 이루어지는 모델을 구별하는 방법에 대한 논의를 촉진하고자 한다.


<details>
  <summary>Details</summary>
Motivation: AI가 과학에 미치는 영향과 대형 언어 모델이 실제로 새로운 지식을 생성하는지 여부를 탐구하기 위해 이 연구를 진행했다.

Method: 특정 결과와 이를 지지하는 지원 보조정리, 패러프레이즈 및 다단계 함축을 함께 제거한 후 모델이 허용된 공리 및 도구만으로 결과를 재추론할 수 있는지 평가하는 방법을 제안하였다.

Result: 이 방법을 통해 모델의 생성 능력과 현재의 한계를 평가할 수 있다.

Conclusion: 이 연구는 AI가 과학에 기여하기 위한 검증 가능한 시험 방법을 제시하며, 다음 세대 AI-for-Science 기준을 이끌기 위한 논의를 촉진하고자 한다.

Abstract: Bold claims about AI's role in science-from "AGI will cure all diseases" to
promises of radically accelerated discovery-raise a central epistemic question:
do large language models (LLMs) truly generate new knowledge, or do they merely
remix memorized fragments? We propose unlearning-as-ablation as a falsifiable
probe of constructive scientific discovery. The idea is to systematically
remove a target result together with its forget-closure (supporting lemmas,
paraphrases, and multi-hop entailments) and then evaluate whether the model can
re-derive the result from only permitted axioms and tools. Success would
indicate generative capability beyond recall; failure would expose current
limits. Unlike prevailing motivations for unlearning-privacy, copyright, or
safety-our framing repositions it as an epistemic probe for AI-for-Science. We
outline a minimal pilot in mathematics and algorithms to illustrate
feasibility, and sketch how the same approach could later be extended to
domains such as physics or chemistry. This is a position paper: our
contribution is conceptual and methodological, not empirical. We aim to
stimulate discussion on how principled ablation tests could help distinguish
models that reconstruct knowledge from those that merely retrieve it, and how
such probes might guide the next generation of AI-for-Science benchmarks.

</details>


### [121] [Speculative Safety-Aware Decoding](https://arxiv.org/abs/2508.17739)
*Xuekang Wang,Shengyu Zhu,Xueqi Cheng*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델(LLM)의 안전성을 강화하기 위한 새로운 방법, 즉 Speculative Safety-Aware Decoding (SSD)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델들이 인간의 가치와 안전 규칙에 부합하도록 노력했음에도 불구하고, 취약점을 이용한 탈옥 공격이 지속적으로 발생하고 있습니다.

Method: SSD는 디코딩 중에 추측 샘플링을 통합하고, 소형 모델과 복합 모델 간의 일치 비율을 활용하여 탈옥 위험을 정량화하는 경량 디코딩 시간 접근 방식입니다.

Result: SSD는 대형 모델에 원하는 안전성을 성공적으로 부여하고, 유용성을 가지면서도 악의적 쿼리에 응답하도록 합니다.

Conclusion: SSD는 추측 샘플링 설계 덕분에 추론 시간을 가속화합니다.

Abstract: Despite extensive efforts to align Large Language Models (LLMs) with human
values and safety rules, jailbreak attacks that exploit certain vulnerabilities
continuously emerge, highlighting the need to strengthen existing LLMs with
additional safety properties to defend against these attacks. However, tuning
large models has become increasingly resource-intensive and may have difficulty
ensuring consistent performance. We introduce Speculative Safety-Aware Decoding
(SSD), a lightweight decoding-time approach that equips LLMs with the desired
safety property while accelerating inference. We assume that there exists a
small language model that possesses this desired property. SSD integrates
speculative sampling during decoding and leverages the match ratio between the
small and composite models to quantify jailbreak risks. This enables SSD to
dynamically switch between decoding schemes to prioritize utility or safety, to
handle the challenge of different model capacities. The output token is then
sampled from a new distribution that combines the distributions of the original
and the small models. Experimental results show that SSD successfully equips
the large model with the desired safety property, and also allows the model to
remain helpful to benign queries. Furthermore, SSD accelerates the inference
time, thanks to the speculative sampling design.

</details>


### [122] [ShaLa: Multimodal Shared Latent Space Modelling](https://arxiv.org/abs/2508.17376)
*Jiali Cui,Yan-Ying Chen,Yanxia Zhang,Matthew Klenk*

Main category: cs.LG

TL;DR: 이 논문은 다중 모드 데이터 간의 공유 잠재 표현을 학습하기 위한 새로운 생성 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 이 연구의 동기는 다양한 입력의 모드별 세부사항 조합을 포착하는 데 집중했으나, 그로 인해 공유되는 고수준 의미 개념이 obscured 되는 문제를 해결하는 것입니다.

Method: 새로운 아키텍처적 추론 모델과 2단계 표현적 확산 사전(prior)을 통합하여 공유 잠재 표현의 효과적인 추론을 촉진하고, 다운스트림 다중 모드 합성의 품질을 개선합니다.

Result: ShaLa는 여러 벤치마크에서 광범위하게 검증되었으며, 최신 다중 모드 VAE에 비해 우수한 일관성과 합성 품질을 보여줍니다.

Conclusion: ShaLa는 이전의 다중 모드 VAE가 공유 잠재 공간의 증가하는 복잡성을 포착하는 데 부족했던 반면, 더 많은 모드로 확장될 수 있습니다.

Abstract: This paper presents a novel generative framework for learning shared latent
representations across multimodal data. Many advanced multimodal methods focus
on capturing all combinations of modality-specific details across inputs, which
can inadvertently obscure the high-level semantic concepts that are shared
across modalities. Notably, Multimodal VAEs with low-dimensional latent
variables are designed to capture shared representations, enabling various
tasks such as joint multimodal synthesis and cross-modal inference. However,
multimodal VAEs often struggle to design expressive joint variational
posteriors and suffer from low-quality synthesis. In this work, ShaLa addresses
these challenges by integrating a novel architectural inference model and a
second-stage expressive diffusion prior, which not only facilitates effective
inference of shared latent representation but also significantly improves the
quality of downstream multimodal synthesis. We validate ShaLa extensively
across multiple benchmarks, demonstrating superior coherence and synthesis
quality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales
to many more modalities while prior multimodal VAEs have fallen short in
capturing the increasing complexity of the shared latent space.

</details>


### [123] [Proximal Supervised Fine-Tuning](https://arxiv.org/abs/2508.17784)
*Wenhong Zhu,Ruobing Xie,Rui Wang,Xingwu Sun,Di Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: Proximal SFT (PSFT)는 기존의 fine-tuning 방식인 SFT의 일반화 문제를 해결하기 위한 방법으로, 강화학습의 신뢰 영역 정책 최적화(TRPO) 및 근접 정책 최적화(PPO)에 기반하여 정책 드리프트를 효과적으로 제어하면서 경쟁력 있는 미세 조정을 유지한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 기초 모델 미세 조정이 새로운 과제나 도메인에서 능력을 저하시키는 문제가 존재한다.

Method: 신뢰 영역 이점을 통합하여 미세 조정 중 정책 드리프트를 효과적으로 제한하는 Proximal SFT (PSFT)를 제안한다.

Result: PSFT는 SFT와 비교해 같은 도메인에서는 일치하고, 도메인 외 일반화에서는 우수하며, 오래 훈련해도 안정성을 유지하고 엔트로피 붕괴를 일으키지 않는다.

Conclusion: PSFT는 후속 최적화를 위한 더욱 강력한 기반을 제공한다.

Abstract: Supervised fine-tuning (SFT) of foundation models often leads to poor
generalization, where prior capabilities deteriorate after tuning on new tasks
or domains. Inspired by trust-region policy optimization (TRPO) and proximal
policy optimization (PPO) in reinforcement learning (RL), we propose Proximal
SFT (PSFT). This fine-tuning objective incorporates the benefits of
trust-region, effectively constraining policy drift during SFT while
maintaining competitive tuning. By viewing SFT as a special case of policy
gradient methods with constant positive advantages, we derive PSFT that
stabilizes optimization and leads to generalization, while leaving room for
further optimization in subsequent post-training stages. Experiments across
mathematical and human-value domains show that PSFT matches SFT in-domain,
outperforms it in out-of-domain generalization, remains stable under prolonged
training without causing entropy collapse, and provides a stronger foundation
for the subsequent optimization.

</details>


### [124] [FedERL: Federated Efficient and Robust Learning for Common Corruptions](https://arxiv.org/abs/2508.17381)
*Omar Bekdache,Naresh Shanbhag*

Main category: cs.LG

TL;DR: Federated learning (FL)은 데이터 프라이버시를 보호하면서 엣지 디바이스에서 딥러닝 모델의 배포를 가속화하지만, 클라이언트 측의 계산 자원 제약과 일반적인 손상에 대한 강건성 부족이라는 도전 과제를 직면하고 있다. FedERL은 이러한 문제를 해결하기 위한 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: FL 시스템은 데이터 프라이버시를 지키면서 클라이언트 측의 계산 자원 제약으로 인해 도전에 직면하고 있으며, 이는 실용적인 FL 애플리케이션 개발에 어려움을 초래한다.

Method: FedERL은 클라이언트 측의 시간과 에너지 제약 조건하에 손상 강건성을 명시적으로 다루는 최초의 방법으로, 서버에서 데이터 비의존적인 강건 훈련(DART) 방법을 사용하여 강건성을 향상시킨다.

Result: 광범위한 실험을 통해 FedERL은 전통적인 강건 훈련 방법에 비해 적은 시간과 에너지 비용으로 일반적인 손상을 처리할 수 있는 능력을 입증했다.

Conclusion: FedERL은 제한된 시간과 에너지 예산을 가진 상황에서도 전통적인 강건 훈련의 성능을 초월하여, 실제 FL 애플리케이션에 대한 실용적이고 확장 가능한 솔루션으로 자리잡았다.

Abstract: Federated learning (FL) accelerates the deployment of deep learning models on
edge devices while preserving data privacy. However, FL systems face challenges
due to client-side constraints on computational resources, and from a lack of
robustness to common corruptions such as noise, blur, and weather effects.
Existing robust training methods are computationally expensive and unsuitable
for resource-constrained clients. We propose FedERL, federated efficient and
robust learning, as the first work to explicitly address corruption robustness
under time and energy constraints on the client side. At its core, FedERL
employs a novel data-agnostic robust training (DART) method on the server to
enhance robustness without access to the training data. In doing so, FedERL
ensures zero robustness overhead for clients. Extensive experiments demonstrate
FedERL's ability to handle common corruptions at a fraction of the time and
energy cost of traditional robust training methods. In scenarios with limited
time and energy budgets, FedERL surpasses the performance of traditional robust
training, establishing it as a practical and scalable solution for real-world
FL applications.

</details>


### [125] [Limitations of Normalization in Attention Mechanism](https://arxiv.org/abs/2508.17821)
*Timur Mudarisov,Mikhail Burtsev,Tatiana Petrova,Radu State*

Main category: cs.LG

TL;DR: 이 논문은 주의 메커니즘에서의 정규화의 한계를 조사합니다. 주의 메커니즘의 선택 능력과 토큰 선택에서의 기하학적 분리를 밝힙니다.


<details>
  <summary>Details</summary>
Motivation: 모델의 선택적 능력과 기하학적 분리를 이해하기 위해 주의 메커니즘의 정규화 한계를 조사하고자 하였습니다.

Method: 토큰 벡터의 거리와 분리 기준에 대한 명시적인 경계를 포함한 이론적 프레임워크와 GPT-2 모델을 통한 실험을 사용하였습니다.

Result: 선택된 토큰의 수가 증가할수록 모델의 정보성 있는 토큰 구별 능력이 감소하며, 균일한 선택 패턴으로 수렴하는 경향이 있음을 발견하였습니다.

Conclusion: 소프트맥스 기반의 주의 메커니즘에 대한 현재의 이해를 발전시키고, 미래 주의 아키텍처에서 더 강력한 정규화 및 선택 전략의 필요성을 제기합니다.

Abstract: This paper investigates the limitations of the normalization in attention
mechanisms. We begin with a theoretical framework that enables the
identification of the model's selective ability and the geometric separation
involved in token selection. Our analysis includes explicit bounds on distances
and separation criteria for token vectors under softmax scaling. Through
experiments with pre-trained GPT-2 model, we empirically validate our
theoretical results and analyze key behaviors of the attention mechanism.
Notably, we demonstrate that as the number of selected tokens increases, the
model's ability to distinguish informative tokens declines, often converging
toward a uniform selection pattern. We also show that gradient sensitivity
under softmax normalization presents challenges during training, especially at
low temperature settings. These findings advance current understanding of
softmax-based attention mechanism and motivate the need for more robust
normalization and selection strategies in future attention architectures.

</details>


### [126] [Limits of message passing for node classification: How class-bottlenecks restrict signal-to-noise ratio](https://arxiv.org/abs/2508.17822)
*Jonathan Rubin,Sahil Loomba,Nick S. Jones*

Main category: cs.LG

TL;DR: 이 논문은 이질성이 있는 그래프에서 메시지 전달 신경망(MPNN)의 성능 한계를 극복하기 위한 통합 통계 프레임워크를 제시하며, 최적의 그래프 구조인 BRIDGE 알고리즘을 통해 MPNN의 분류 정확도를 크게 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: MPNN은 노드 분류에 강력한 모델이지만, 이질성과 구조적 병목 현상으로 인해 성능 한계가 있다.

Method: 신호 대 잡음 비율(SNR)을 통해 이질성과 병목 현상 간의 관계를 드러내고, 그래프 앙상블 분석을 통해 병목 현상의 정량적 분해를 제공하며, BRIDGE라는 그래프 리와이어링 알고리즘을 개발하였다.

Result: BRIDGE 알고리즘은 모든 이질성 영역에서 근접 완벽한 분류 정확도를 달성하며, 실제 벤치마크에서 유의미한 개선을 보였다.

Conclusion: 우리의 프레임워크는 MPNN 성능을 평가하는 진단 도구와 그래프 수정 방법을 제공한다.

Abstract: Message passing neural networks (MPNNs) are powerful models for node
classification but suffer from performance limitations under heterophily (low
same-class connectivity) and structural bottlenecks in the graph. We provide a
unifying statistical framework exposing the relationship between heterophily
and bottlenecks through the signal-to-noise ratio (SNR) of MPNN
representations. The SNR decomposes model performance into feature-dependent
parameters and feature-independent sensitivities. We prove that the sensitivity
to class-wise signals is bounded by higher-order homophily -- a generalisation
of classical homophily to multi-hop neighbourhoods -- and show that low
higher-order homophily manifests locally as the interaction between structural
bottlenecks and class labels (class-bottlenecks). Through analysis of graph
ensembles, we provide a further quantitative decomposition of bottlenecking
into underreaching (lack of depth implying signals cannot arrive) and
oversquashing (lack of breadth implying signals arriving on fewer paths) with
closed-form expressions. We prove that optimal graph structures for maximising
higher-order homophily are disjoint unions of single-class and
two-class-bipartite clusters. This yields BRIDGE, a graph ensemble-based
rewiring algorithm that achieves near-perfect classification accuracy across
all homophily regimes on synthetic benchmarks and significant improvements on
real-world benchmarks, by eliminating the ``mid-homophily pitfall'' where MPNNs
typically struggle, surpassing current standard rewiring techniques from the
literature. Our framework, whose code we make available for public use,
provides both diagnostic tools for assessing MPNN performance, and simple yet
effective methods for enhancing performance through principled graph
modification.

</details>


### [127] [Effective Clustering for Large Multi-Relational Graphs](https://arxiv.org/abs/2508.17388)
*Xiaoyang Lin,Runhao Jiang,Renchi Yang*

Main category: cs.LG

TL;DR: 본 논문에서는 MRG 클러스터링의 한계를 극복하기 위해 DEMM 및 DEMM+ 두 가지 효과적인 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 상호작용을 모델링하기 위한 MRG 클러스터링 작업의 중요성에 대한 인식.

Method: DEMM 및 DEMM+ 알고리즘은 MRG를 위한 다단계 최적화 목표에 기반하여 작동한다.

Result: DEMM+는 클러스터링 품질이 뛰어나면서 속도가 월등히 빠르다는 것을 보여준다.

Conclusion: DEMM+는 실제 MRG에 대한 클러스터링에서 지속적으로 우수한 성능을 발휘한다.

Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling
diverse interactions/relations among real objects (i.e., nodes), which pervade
extensive applications and scenarios. Given an MRG G with N nodes, partitioning
the node set therein into K disjoint clusters (MRGC) is a fundamental task in
analyzing MRGs, which has garnered considerable attention. However, the
majority of existing solutions towards MRGC either yield severely compromised
result quality by ineffective fusion of heterogeneous graph structures and
attributes, or struggle to cope with sizable MRGs with millions of nodes and
billions of edges due to the adoption of sophisticated and costly deep learning
models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to
address the limitations above. Specifically, our algorithms are built on novel
two-stage optimization objectives, where the former seeks to derive
high-caliber node feature vectors by optimizing the multi-relational Dirichlet
energy specialized for MRGs, while the latter minimizes the Dirichlet energy of
clustering results over the node affinity graph. In particular, DEMM+ achieves
significantly higher scalability and efficiency over our based method DEMM
through a suite of well-thought-out optimizations. Key technical contributions
include (i) a highly efficient approximation solver for constructing node
feature vectors, and (ii) a theoretically-grounded problem transformation with
carefully-crafted techniques that enable linear-time clustering without
explicitly materializing the NxN dense affinity matrix. Further, we extend
DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive
experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit
that DEMM+ is consistently superior in terms of clustering quality measured
against ground-truth labels, while often being remarkably faster.

</details>


### [128] [Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs](https://arxiv.org/abs/2508.17850)
*Han Zhang,Ruibin Zheng,Zexuan Yi,Hanyang Peng,Hui Wang,Yue Yu*

Main category: cs.LG

TL;DR: 비동기적 강화 학습 아키텍처인 HeteroRL을 제안하여 네트워크 지연 하에서도 지리적으로 분산된 노드에서 강력한 배치를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 단일 센터 컴퓨팅 접근 방식이 전력 제약에 도달하면서 분산 학습이 필수적이 되고 있다.

Method: HeteroRL은 롤아웃 샘플링을 매개변수 학습과 분리하여 비동기적으로 작업하는 RL 아키텍처이다. 우리는 중요 샘플링 실패 문제를 해결하기 위해 그룹 기대 정책 최적화(GEPO)를 제안한다.

Result: GEPO는 1800초 지연에서도 3% 미만의 성능 저하로 GRPO와 같은 방법에 비해 우수한 안정성을 유지한다.

Conclusion: HeteroRL은 이질적 분산 네트워크에서 분산 강화 학습을 위한 강력한 가능성을 보여준다.

Abstract: As single-center computing approaches power constraints, decentralized
training is becoming essential. Reinforcement Learning (RL) post-training
enhances Large Language Models (LLMs) but faces challenges in heterogeneous
distributed environments due to its tightly-coupled sampling-learning
alternation. We propose HeteroRL, an asynchronous RL architecture that
decouples rollout sampling from parameter learning, enabling robust deployment
across geographically distributed nodes under network delays. We identify that
latency-induced KL divergence causes importance sampling failure due to high
variance. To address this, we propose Group Expectation Policy Optimization
(GEPO), which reduces importance weight variance through a refined sampling
mechanism. Theoretically, GEPO achieves exponential variance reduction.
Experiments show it maintains superior stability over methods like GRPO, with
less than 3% performance degradation under 1800-second delays, demonstrating
strong potential for decentralized RL in heterogeneous networks.

</details>


### [129] [Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks](https://arxiv.org/abs/2508.17867)
*Dan Wang,Feng Jiang,Zhanquan Wang*

Main category: cs.LG

TL;DR: 변화하는 대기 질을 예측하기 위해 Transformer 기반의 공간-시간 데이터 예측 방법인 Ada-TransGNN을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대기 질 예측의 정확성을 높이고 기존 모델의 느린 실시간 업데이트 문제를 해결하기 위함입니다.

Method: 다중 헤드 주의 메커니즘과 그래프 컨볼루션 네트워크를 통합하여 동적으로 변화하는 공간-시간 종속성 특징을 추출하는 spatiotemporal block 집합을 구축합니다.

Result: 우리 모델이 단기 및 장기 예측에서 기존의 첨단 예측 모델보다 우수함을 입증했습니다.

Conclusion: Ada-TransGNN은 대기 질 예측의 정확성과 효율성을 향상시킵니다.

Abstract: Accurate air quality prediction is becoming increasingly important in the
environmental field. To address issues such as low prediction accuracy and slow
real-time updates in existing models, which lead to lagging prediction results,
we propose a Transformer-based spatiotemporal data prediction method
(Ada-TransGNN) that integrates global spatial semantics and temporal behavior.
The model constructs an efficient and collaborative spatiotemporal block set
comprising a multi-head attention mechanism and a graph convolutional network
to extract dynamically changing spatiotemporal dependency features from complex
air quality monitoring data. Considering the interaction relationships between
different monitoring points, we propose an adaptive graph structure learning
module, which combines spatiotemporal dependency features in a data-driven
manner to learn the optimal graph structure, thereby more accurately capturing
the spatial relationships between monitoring points. Additionally, we design an
auxiliary task learning module that enhances the decoding capability of
temporal relationships by integrating spatial context information into the
optimal graph structure representation, effectively improving the accuracy of
prediction results. We conducted comprehensive evaluations on a benchmark
dataset and a novel dataset (Mete-air). The results demonstrate that our model
outperforms existing state-of-the-art prediction models in short-term and
long-term predictions.

</details>


### [130] [Mutual Information Surprise: Rethinking Unexpectedness in Autonomous Systems](https://arxiv.org/abs/2508.17403)
*Yinsong Wang,Xiao Liu,Quan Zeng,Yu Ding*

Main category: cs.LG

TL;DR: 최근 자율 실험의 발전은 놀라운 물리적 능력을 입증했지만, 인지 제어는 여전히 제한적이다. 이 논문에서는 놀라움의 새로운 기준으로 상호 정보 놀라움(MIS)을 제안하며, 자율 시스템이 학습 진행 상황을 반영할 수 있도록 지원한다. 이를 통해 MISRP가 고전적인 접근 방식보다 더 나은 성능을 발휘함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 자율 실험에서 인지 제어의 한계를 극복하고 자율 시스템의 적응 능력을 향상시키기 위해 새로운 놀라움 측정 방법이 필요하다.

Method: 상호 정보 놀라움(MIS)을 정의하고, 이를 통해 자율 시스템이 학습 과정을 반영하도록 하는 통계적 테스트 시퀀스 및 반응 정책인 MISRP를 개발하였다.

Result: MISRP를 적용한 전략이 고전적인 놀라움 기반 접근 방식에 비해 안정성, 반응성, 예측 정확도에서 상당히 우수한 성과를 보여준다.

Conclusion: MIS는 자율 시스템의 자각적이고 적응적인 행동을 촉진하는 경로를 제공한다.

Abstract: Recent breakthroughs in autonomous experimentation have demonstrated
remarkable physical capabilities, yet their cognitive control remains
limited--often relying on static heuristics or classical optimization. A core
limitation is the absence of a principled mechanism to detect and adapt to the
unexpectedness. While traditional surprise measures--such as Shannon or
Bayesian Surprise--offer momentary detection of deviation, they fail to capture
whether a system is truly learning and adapting. In this work, we introduce
Mutual Information Surprise (MIS), a new framework that redefines surprise not
as anomaly detection, but as a signal of epistemic growth. MIS quantifies the
impact of new observations on mutual information, enabling autonomous systems
to reflect on their learning progression. We develop a statistical test
sequence to detect meaningful shifts in estimated mutual information and
propose a mutual information surprise reaction policy (MISRP) that dynamically
governs system behavior through sampling adjustment and process forking.
Empirical evaluations--on both synthetic domains and a dynamic pollution map
estimation task--show that MISRP-governed strategies significantly outperform
classical surprise-based approaches in stability, responsiveness, and
predictive accuracy. By shifting surprise from reactive to reflective, MIS
offers a path toward more self-aware and adaptive autonomous systems.

</details>


### [131] [Riemannian Optimization for LoRA on the Stiefel Manifold](https://arxiv.org/abs/2508.17901)
*Juneyoung Park,Minjae Kang,Seongbae Lee,Haegang Lee,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 대형 언어 모델의 튜닝 효율성을 높이기 위해 Stiefel 다양체에서 $B$ 행렬을 최적화하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 크기에 따른 미세 조정의 어려움을 해결하고자 한다.

Method: $B$ 행렬에 명시적인 직교성 제약 조건을 부과하여 Stiefel 다양체에서 최적화하는 방법을 사용한다.

Result: Stiefel 최적화기가 AdamW를라는 벤치마크에서 일관되게 성능을 초과한다.

Conclusion: 기하학적 제약이 대형 언어 모델의 효과적인 미세 조정을 위한 열쇠임을 나타낸다.

Abstract: While powerful, large language models (LLMs) present significant fine-tuning
challenges due to their size. Parameter-efficient fine-tuning (PEFT) methods
like LoRA provide solutions, yet suffer from critical optimizer inefficiencies;
notably basis redundancy in LoRA's $B$ matrix when using AdamW, which
fundamentally limits performance. We address this by optimizing the $B$ matrix
on the Stiefel manifold, imposing explicit orthogonality constraints that
achieve near-perfect orthogonality and full effective rank. This geometric
approach dramatically enhances parameter efficiency and representational
capacity. Our Stiefel optimizer consistently outperforms AdamW across
benchmarks with both LoRA and DoRA, demonstrating that geometric constraints
are the key to unlocking LoRA's full potential for effective LLM fine-tuning.

</details>


### [132] [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration](https://arxiv.org/abs/2508.18025)
*Aditri Paul,Archan Paul*

Main category: cs.LG

TL;DR: 본 논문은 자율 행성 탐사를 위한 새로운 프레임워크인 AQ-PCDSys를 소개하며, 이는 제한된 계산 환경에서 실시간으로 행성의 분화구를 탐지하는 데 최적화된 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 행성 탐사 임무는 내비게이션 및 위험 회피를 위해 실시간으로 정확한 환경 인식에 크게 의존한다.

Method: AQ-PCDSys는 Quantization-Aware Training(QAT)으로 학습된 Quantized Neural Network(QNN) 아키텍처와 Adaptive Multi-Sensor Fusion(AMF) 모듈을 통합하여 설계되었다.

Result: QNN 아키텍처는 모델 크기와 추론 대기 시간을 크게 최적화하여 높은 정확성을 유지하면서도 행성 탐사 임무에 적합한 실시간 온보드 배포를 가능하게 한다.

Conclusion: AQ-PCDSys는 다양한 행성 환경에서 분화구 탐지의 강인성을 향상시키며, 자율 행성 착륙, 내비게이션 및 과학 탐사를 위한 다음 세대 기술의 중요한 기능을 제공한다.

Abstract: Autonomous planetary exploration missions are critically dependent on
real-time, accurate environmental perception for navigation and hazard
avoidance. However, deploying deep learning models on the resource-constrained
computational hardware of planetary exploration platforms remains a significant
challenge. This paper introduces the Adaptive Quantized Planetary Crater
Detection System (AQ-PCDSys), a novel framework specifically engineered for
real-time, onboard deployment in the computationally constrained environments
of space exploration missions. AQ-PCDSys synergistically integrates a Quantized
Neural Network (QNN) architecture, trained using Quantization-Aware Training
(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture
significantly optimizes model size and inference latency suitable for real-time
onboard deployment in space exploration missions, while preserving high
accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and
Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive
Weighting Mechanism (AWM) to dynamically prioritize the most relevant and
reliable sensor modality based on planetary ambient conditions. This approach
enhances detection robustness across diverse planetary landscapes. Paired with
Multi-Scale Detection Heads specifically designed for robust and efficient
detection of craters across a wide range of sizes, AQ-PCDSys provides a
computationally efficient, reliable and accurate solution for planetary crater
detection, a critical capability for enabling the next generation of autonomous
planetary landing, navigation, and scientific exploration.

</details>


### [133] [CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics](https://arxiv.org/abs/2508.18124)
*Weida Wang,Dongchen Huang,Jiatong Li,Tengchao Yang,Ziyang Zheng,Di Zhang,Dong Han,Benteng Chen,Binzhao Luo,Zhiyu Liu,Kunling Liu,Zhiyuan Gao,Shiqi Geng,Wei Ma,Jiaming Su,Xin Li,Shuchen Pu,Yuhan Shui,Qianjia Cheng,Zhihao Dou,Dongfei Cui,Changyong He,Jin Zeng,Zeke Xie,Mao Su,Dongzhan Zhou,Yuqiang Li,Wanli Ouyang,Yunqi Cai,Xi Dai,Shufei Zhang,Lei Bai,Jinguang Cheng,Zhong Fang,Hongming Weng*

Main category: cs.LG

TL;DR: CMPhysBench는 응집물질 물리학에서 대형 언어 모델의 능력을 평가하기 위해 설계된 새로운 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 응집물질 물리학의 문제 해결에서 대형 언어 모델의 이해도를 측정하고 싶었다.

Method: 520개 이상의 석사 수준의 질문으로 구성된 CMPhysBench를 통해 LLM이 문제 해결 과정을 독립적으로 수행해야 하며, SEED 점수를 도입하여 예측과 정답 간의 유사성을 정밀하게 평가한다.

Result: 최고 모델인 Grok-4가 CMPhysBench에서 평균 SEED 점수 36과 28% 정확도에 불과함을 보여주어 실질적으로 중요한 역량 격차를 강조한다.

Conclusion: 전통적인 물리학에 비해 응집물질 물리학 분야에서의 LLM의 능력은 상당히 부족하다는 것을 나타낸다.

Abstract: We introduce CMPhysBench, designed to assess the proficiency of Large
Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.
CMPhysBench is composed of more than 520 graduate-level meticulously curated
questions covering both representative subfields and foundational theoretical
frameworks of condensed matter physics, such as magnetism, superconductivity,
strongly correlated systems, etc. To ensure a deep understanding of the
problem-solving process,we focus exclusively on calculation problems, requiring
LLMs to independently generate comprehensive solutions. Meanwhile, leveraging
tree-based representations of expressions, we introduce the Scalable Expression
Edit Distance (SEED) score, which provides fine-grained (non-binary) partial
credit and yields a more accurate assessment of similarity between prediction
and ground-truth. Our results show that even the best models, Grok-4, reach
only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a
significant capability gap, especially for this practical and frontier domain
relative to traditional physics. The code anddataset are publicly available at
https://github.com/CMPhysBench/CMPhysBench.

</details>


### [134] [Modular MeanFlow: Towards Stable and Scalable One-Step Generative Modeling](https://arxiv.org/abs/2508.17426)
*Haochen You,Baojing Liu,Hongyang He*

Main category: cs.LG

TL;DR: 모듈형 평균 흐름(MMF)은 시간 평균 속도 필드를 학습하기 위한 유연하고 이론적으로 기반을 둔 접근 방식을 제안하며, 전통적인 모델들보다 높은 효율성을 자랑합니다.


<details>
  <summary>Details</summary>
Motivation: 하나의 함수 평가에서 고품질 데이터 샘플을 생성하여 전통적인 모델들보다 효율성을 향상시키기 위한 필요성이 있습니다.

Method: MMF는 순간 속도와 평균 속도를 연결하는 미분 식을 기반으로 한 손실 함수 가족을 도출하고, 표현력을 희생하지 않으면서 안정적인 훈련을 가능하게 하는 그래디언트 조정 메커니즘을 통합합니다.

Result: 이미지 합성과 경로 모델링 작업에서 MMF는 경쟁력 있는 샘플 품질, 강력한 수렴성 및 우수한 일반화를 달성하였으며, 특히 낮은 데이터 또는 분포 외 환경에서 그 효과가 두드러집니다.

Conclusion: MMF는 기존의 일관성 기반 및 흐름 매칭 방법을 통합 및 일반화하면서도 비싼 고차 도함수를 회피합니다.

Abstract: One-step generative modeling seeks to generate high-quality data samples in a
single function evaluation, significantly improving efficiency over traditional
diffusion or flow-based models. In this work, we introduce Modular MeanFlow
(MMF), a flexible and theoretically grounded approach for learning
time-averaged velocity fields. Our method derives a family of loss functions
based on a differential identity linking instantaneous and average velocities,
and incorporates a gradient modulation mechanism that enables stable training
without sacrificing expressiveness. We further propose a curriculum-style
warmup schedule to smoothly transition from coarse supervision to fully
differentiable training. The MMF formulation unifies and generalizes existing
consistency-based and flow-matching methods, while avoiding expensive
higher-order derivatives. Empirical results across image synthesis and
trajectory modeling tasks demonstrate that MMF achieves competitive sample
quality, robust convergence, and strong generalization, particularly under
low-data or out-of-distribution settings.

</details>


### [135] [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
*Yizhi Li,Qingshui Gu,Zhoufutu Wen,Ziniu Li,Tianshun Xing,Shuyue Guo,Tianyu Zheng,Xin Zhou,Xingwei Qu,Wangchunshu Zhou,Zheng Zhang,Wei Shen,Qian Liu,Chenghua Lin,Jian Yang,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: TreePO는 시퀀스 생성을 트리 구조 검색 과정으로 간주하는 자기 주도형 롤아웃 알고리즘을 통해 비용을 절감하면서 탐색 다양성을 향상시키는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델을 강화 학습을 통해 정렬하는 최근의 발전은 복잡한 추론 문제를 해결하는 데 눈에 띄는 성과를 이루었지만, 이는 비싼 온폴리시 롤아웃과 다양한 추론 경로 탐색의 제한이라는 대가를 치르었습니다.

Method: TreePO는 동적 트리 샘플링 정책과 고정 길이 세그먼트 디코딩으로 구성되며, 지역적 불확실성을 활용하여 추가적인 가지를 형성하는 자기 주도형 롤아웃 알고리즘을 포함합니다.

Result: TreePO는 KV 캐시 부담을 완화하고 조기 중지 메커니즘과 함께 새로운 가지를 생성하는 세그먼트 단위 샘플링 알고리즘과 전역 및 지역 근접 정책 최적화를 고려한 트리 기반 세그먼트 수준 우위 추정 등 주요 기여를 포함하고 있습니다.

Conclusion: TreePO는 샘플의 수와 계산량을 줄이면서 RL 기반의 후속 훈련을 확장하는 실용적인 경로를 제시합니다.

Abstract: Recent advancements in aligning large language models via reinforcement
learning have achieved remarkable gains in solving complex reasoning problems,
but at the cost of expensive on-policy rollouts and limited exploration of
diverse reasoning paths. In this work, we introduce TreePO, involving a
self-guided rollout algorithm that views sequence generation as a
tree-structured searching process. Composed of dynamic tree sampling policy and
fixed-length segment decoding, TreePO leverages local uncertainty to warrant
additional branches. By amortizing computation across common prefixes and
pruning low-value paths early, TreePO essentially reduces the per-update
compute burden while preserving or enhancing exploration diversity. Key
contributions include: (1) a segment-wise sampling algorithm that alleviates
the KV cache burden through contiguous segments and spawns new branches along
with an early-stop mechanism; (2) a tree-based segment-level advantage
estimation that considers both global and local proximal policy optimization.
and (3) analysis on the effectiveness of probability and quality-driven dynamic
divergence and fallback strategy. We empirically validate the performance gain
of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours
from 22\% up to 43\% of the sampling design for the trained models, meanwhile
showing up to 40\% reduction at trajectory-level and 35\% at token-level
sampling compute for the existing models. While offering a free lunch of
inference efficiency, TreePO reveals a practical path toward scaling RL-based
post-training with fewer samples and less compute. Home page locates at
https://m-a-p.ai/TreePO.

</details>


### [136] [Amortized Sampling with Transferable Normalizing Flows](https://arxiv.org/abs/2508.18175)
*Charlie B. Tan,Majdi Hassan,Leon Klein,Saifuddin Syed,Dominique Beaini,Michael M. Bronstein,Alexander Tong,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 이 논문은 심층 학습을 활용하여 분자 구성을 효과적으로 샘플링하는 새로운 접근방식인 Prose를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 효율적인 분자의 접힘 상태 샘플링은 계산 화학 및 통계적 추론에서 핵심 도전 과제입니다.

Method: 본 연구에서는 2억 8천만 개의 매개변수를 가진 모든 원자 전이 정규화 흐름 모델인 Prose를 소개합니다. 이 모델은 8개의 잔기로 구성된 펩타이드 분자 역학 경로를 학습하여 설계되었습니다.

Result: Prose는 임의의 펩타이드 시스템에 대해 제로샷 비상관 제안 샘플을 생성하며, 특정 서열 길이 간의 이식 가능성을 달성합니다.

Conclusion: Prose의 검증 결과, 다양한 샘플링 알고리즘에 대한 효능을 입증하였으며, 공개된 코드베이스와 모델 가중치 및 데이터셋을 통해 추가 연구를 촉진하고자 합니다.

Abstract: Efficient equilibrium sampling of molecular conformations remains a core
challenge in computational chemistry and statistical inference. Classical
approaches such as molecular dynamics or Markov chain Monte Carlo inherently
lack amortization; the computational cost of sampling must be paid in-full for
each system of interest. The widespread success of generative models has
inspired interest into overcoming this limitation through learning sampling
algorithms. Despite performing on par with conventional methods when trained on
a single system, learned samplers have so far demonstrated limited ability to
transfer across systems. We prove that deep learning enables the design of
scalable and transferable samplers by introducing Prose, a 280 million
parameter all-atom transferable normalizing flow trained on a corpus of peptide
molecular dynamics trajectories up to 8 residues in length. Prose draws
zero-shot uncorrelated proposal samples for arbitrary peptide systems,
achieving the previously intractable transferability across sequence length,
whilst retaining the efficient likelihood evaluation of normalizing flows.
Through extensive empirical evaluation we demonstrate the efficacy of Prose as
a proposal for a variety of sampling algorithms, finding a simple importance
sampling-based finetuning procedure to achieve superior performance to
established methods such as sequential Monte Carlo on unseen tetrapeptides. We
open-source the Prose codebase, model weights, and training dataset, to further
stimulate research into amortized sampling methods and finetuning objectives.

</details>


### [137] [Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality](https://arxiv.org/abs/2508.17448)
*Shaocong Ma,Ziyi Chen,Yi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 이 논문은 강한 이중성이 일반적으로 견고한 제약 강화 학습에서 성립하지 않음을 보여주며, 이를 극복하기 위한 새로운 알고리즘인 Rectified Robust Policy Optimization (RRPO)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 제한된 자원 또는 안전 기준을 만족하면서 가장 나쁜 경우의 모델 불확실성 하에서 에이전트의 성능을 최적화하는 것이 목표입니다.

Method: 이 논문에서는 이중 공식을 사용하지 않고 기본 문제에 직접 작용하는 새로운 기본 전용 알고리즘인 Rectified Robust Policy Optimization (RRPO)를 제안합니다.

Result: 이론적 수렴 보장을 제공하며, 특정 수준에서 불확실성 집합의 직경이 제어될 때 최적 구속 가능 정책으로 수렴함을 보여줍니다.

Conclusion: 그리드 월드 환경에서의 실험 결과는 RRPO가 모델 불확실성 하에서도 견고하고 안전한 성능을 달성함을 입증하였습니다.

Abstract: The goal of robust constrained reinforcement learning (RL) is to optimize an
agent's performance under the worst-case model uncertainty while satisfying
safety or resource constraints. In this paper, we demonstrate that strong
duality does not generally hold in robust constrained RL, indicating that
traditional primal-dual methods may fail to find optimal feasible policies. To
overcome this limitation, we propose a novel primal-only algorithm called
Rectified Robust Policy Optimization (RRPO), which operates directly on the
primal problem without relying on dual formulations. We provide theoretical
convergence guarantees under mild regularity assumptions, showing convergence
to an approximately optimal feasible policy with iteration complexity matching
the best-known lower bound when the uncertainty set diameter is controlled in a
specific level. Empirical results in a grid-world environment validate the
effectiveness of our approach, demonstrating that RRPO achieves robust and safe
performance under model uncertainties while the non-robust method can violate
the worst-case safety constraints.

</details>


### [138] [AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models](https://arxiv.org/abs/2508.18182)
*Nikolay Kutuzov,Makar Baderko,Stepan Kulibaba,Artem Dzhalilov,Daniel Bobrov,Maxim Mashtaler,Alexander Gasnikov*

Main category: cs.LG

TL;DR: 이 논문에서는 대형 언어 모델 분산 훈련을 위한 새로운 세 단계 방법을 제안하여 시스템 효율성을 개선하고 수렴 속도를 높입니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 분산 훈련 확장을 위해 동적 워크로드에서 계산 클러스터를 완전히 활용할 수 있는 방법이 필요합니다.

Method: 세 단계 방법은 다중 인스턴스 훈련(MIT), 적응형 배치 DiLoCo 및 스위치 모드 메커니즘을 결합합니다.

Result: 이 방법은 수렴 속도와 시스템 효율성을 개선하고, 필요한 통신 수를 이론적으로 추정합니다.

Conclusion: 이 혁신들은 모델 훈련 과정에서 계산과 통신의 균형을 맞추고, 자원의 활용도를 최대화하여 훈련을 안정화합니다.

Abstract: Scaling distributed training of Large Language Models (LLMs) requires not
only algorithmic advances but also efficient utilization of heterogeneous
hardware resources. While existing methods such as DiLoCo have demonstrated
promising results, they often fail to fully exploit computational clusters
under dynamic workloads. To address this limitation, we propose a three-stage
method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,
and switch mode mechanism. MIT allows individual nodes to run multiple
lightweight training streams with different model instances in parallel and
merge them to combine knowledge, increasing throughput and reducing idle time.
Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance
computation and communication, substantially lowering synchronization delays.
Switch mode further stabilizes training by seamlessly introducing gradient
accumulation once adaptive batch sizes grow beyond hardware-friendly limits.
Together, these innovations improve both convergence speed and system
efficiency. We also provide a theoretical estimate of the number of
communications required for the full convergence of a model trained using our
method.

</details>


### [139] [ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories](https://arxiv.org/abs/2508.17452)
*Dou Jiabao,Nie Jiayi,Yihang Cheng,Jinwei Liu,Yingrui Ji,Canran Xiao,Feixiang Du,Jiaping Xiao*

Main category: cs.LG

TL;DR: ReviBranch는 분기 결정과 그래프 상태 간의 명시적 과거 상관관계를 재구성하여 혼합 정수 선형 프로그램의 효율성을 개선하는 혁신적인 딥 강화 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 혼합 정수 선형 프로그램(MILP)을 해결하는데 있어 전통적인 분기 변수를 선택하는 휴리스틱이 다양한 문제 사례에서 일반화되지 않고, 기존 학습 기반 방법들이 전문가 시연의 질에 의존하거나 희박한 보상과 동적 상태 표현에서의 한계를 겪고 있어 이를 해결할 필요가 있었다.

Method: ReviBranch는 분기 결정과 그에 대응하는 그래프 상태 간의 역사적 상관관계를 재구성하여 revived trajectories를 생성하고, 교육 과정에서 에이전트가 분기 과정 내의 구조적 진화와 시간적 의존성에서 학습하도록 한다.

Result: 다양한 MILP 벤치마크에 대한 광범위한 실험에서 ReviBranch는 최신 강화 학습 방법보다 4.0% 더 많은 B&B 노드를 줄이고, 대규모 인스턴스에서 LP 반복을 2.2% 감소시키는 성과를 보였다.

Conclusion: ReviBranch는 다양한 MILP 문제 클래스에서 강건성과 일반화 가능성을 강조한다.

Abstract: The Branch-and-bound (B&B) algorithm is the main solver for Mixed Integer
Linear Programs (MILPs), where the selection of branching variable is essential
to computational efficiency. However, traditional heuristics for branching
often fail to generalize across heterogeneous problem instances, while existing
learning-based methods such as imitation learning (IL) suffers from dependence
on expert demonstration quality, and reinforcement learning (RL) struggles with
limitations in sparse rewards and dynamic state representation challenges. To
address these issues, we propose ReviBranch, a novel deep RL framework that
constructs revived trajectories by reviving explicit historical correspondences
between branching decisions and their corresponding graph states along
search-tree paths. During training, ReviBranch enables agents to learn from
complete structural evolution and temporal dependencies within the branching
process. Additionally, we introduce an importance-weighted reward
redistribution mechanism that transforms sparse terminal rewards into dense
stepwise feedback, addressing the sparse reward challenge. Extensive
experiments on different MILP benchmarks demonstrate that ReviBranch
outperforms state-of-the-art RL methods, reducing B&B nodes by 4.0% and LP
iterations by 2.2% on large-scale instances. The results highlight the
robustness and generalizability of ReviBranch across heterogeneous MILP problem
classes.

</details>


### [140] [Deep Learning and Matrix Completion-aided IoT Network Localization in the Outlier Scenarios](https://arxiv.org/abs/2508.18225)
*Sunwoo Kim*

Main category: cs.LG

TL;DR: 딥러닝과 행렬 완성을 활용한 방법으로 IoT 네트워크 위치 추정에서 이상치에 오염된 유클리드 거리 행렬을 복구하는 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 위치 추정 기법은 전체 행렬 집합에서 해를 찾는 반면, 본 연구에서는 유클리드 거리 행렬로 검색 범위를 제한하여 효과적인 복구를 추구한다.

Method: 센서 좌표 행렬 X의 함수로 D를 표현하고, 딥 뉴럴 네트워크를 이용해 D와 X를 공동으로 복구한다. 이상치를 희소 행렬 L로 모델링하고 L의 정규화 항을 최적화 문제에 추가하여 문제를 해결한다.

Result: 제안된 기법은 이상치가 존재하는 상황에서도 센서의 위치 정보를 정확하게 복구할 수 있음을 수치 실험을 통해 입증하였다.

Conclusion: 결과적으로, 본 연구의 기법은 IoT 네트워크에서의 위치 추정 품질을 향상시키는 데 기여할 수 있다.

Abstract: In this paper, we propose a deep learning and matrix completion aided
approach for recovering an outlier contaminated Euclidean distance matrix D in
IoT network localization. Unlike conventional localization techniques that
search the solution over a whole set of matrices, the proposed technique
restricts the search to the set of Euclidean distance matrices. Specifically,
we express D as a function of the sensor coordinate matrix X that inherently
satisfies the unique properties of D, and then jointly recover D and X using a
deep neural network. To handle outliers effectively, we model them as a sparse
matrix L and add a regularization term of L into the optimization problem. We
then solve the problem by alternately updating X, D, and L. Numerical
experiments demonstrate that the proposed technique can recover the location
information of sensors accurately even in the presence of outliers.

</details>


### [141] [A Systematic Literature Review on Multi-label Data Stream Classification](https://arxiv.org/abs/2508.17455)
*H. Freire-Oliveira,E. R. F. Paiva,J. Gama,L. Khan,R. Cerri*

Main category: cs.LG

TL;DR: 다중 레이블 데이터 스트림에서의 분류는 실제 적용 가능성이 높아 주목받고 있지만, 동적 환경에서 발생하는 문제점들을 안고 있다. 본 문헌 검토에서는 최근 제안된 방법들을 분석하고, 이들의 접근 방식을 논의하며, 평가 전략 및 자원 소비를 분석한다. 또한 핵심 연구 간극을 식별하고, 미래 연구 방향에 대한 제안을 한다.


<details>
  <summary>Details</summary>
Motivation: 다중 레이블 데이터 스트림 분류의 실제 적용 가능성 때문에 중요한 연구 주제로 떠오르고 있다.

Method: 체계적인 문헌 검토를 통해 다중 레이블 데이터 스트림 분류에 대한 제안들을 심층 분석하고, 최신 방법들의 포괄적인 개요를 제공하며, 각 문제에 대한 접근 방식을 논의한다.

Result: 최신 방법들을 분류하고, 평가 전략을 검토하며, 방법론의 비대칭적 복잡성과 자원 소비를 분석하였다.

Conclusion: 주요 연구 간극을 식별하고, 향후 연구 방향에 대한 구체적인 권고 사항을 제시한다.

Abstract: Classification in the context of multi-label data streams represents a
challenge that has attracted significant attention due to its high real-world
applicability. However, this task faces problems inherent to dynamic
environments, such as the continuous arrival of data at high speed and volume,
changes in the data distribution (concept drift), the emergence of new labels
(concept evolution), and the latency in the arrival of ground truth labels.
This systematic literature review presents an in-depth analysis of multi-label
data stream classification proposals. We characterize the latest methods in the
literature, providing a comprehensive overview, building a thorough hierarchy,
and discussing how the proposals approach each problem. Furthermore, we discuss
the adopted evaluation strategies and analyze the methods' asymptotic
complexity and resource consumption. Finally, we identify the main gaps and
offer recommendations for future research directions in the field.

</details>


### [142] [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)
*Liv Gorton,Owen Lewis*

Main category: cs.LG

TL;DR: 적대적 예제는 심층 학습의 혼란스러운 현상으로 남아 있으며, 본 논문은 중첩 이론이 이러한 현상의 주요 원인이라는 가설을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 적대적 예제의 근본 메커니즘에 대한 합의가 부재하며, 중첩이라는 개념이 주요 요인일 수 있다는 점에서 연구의 필요성이 존재합니다.

Method: 본 논문은 중첩 이론을 지지하는 네 가지 증거를 제시하여, 이를 통해 적대적 현상에 대한 설명을 제공합니다.

Result: 적대적 현상의 다양성을 설명할 수 있으며, 장난감 모델에서 중첩과 강건성의 상호작용을 제어할 수 있는 증거를 제시합니다.

Conclusion: ResNet18을 포함한 실험 결과는 강건성에 대한 개입이 중첩을 제어할 수 있음을 나타냅니다.

Abstract: Adversarial examples -- inputs with imperceptible perturbations that fool
neural networks -- remain one of deep learning's most perplexing phenomena
despite nearly a decade of research. While numerous defenses and explanations
have been proposed, there is no consensus on the fundamental mechanism. One
underexplored hypothesis is that superposition, a concept from mechanistic
interpretability, may be a major contributing factor, or even the primary
cause. We present four lines of evidence in support of this hypothesis, greatly
extending prior arguments by Elhage et al. (2022): (1) superposition can
theoretically explain a range of adversarial phenomena, (2) in toy models,
intervening on superposition controls robustness, (3) in toy models,
intervening on robustness (via adversarial training) controls superposition,
and (4) in ResNet18, intervening on robustness (via adversarial training)
controls superposition.

</details>


### [143] [Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data](https://arxiv.org/abs/2508.18244)
*Chu-Cheng Lin,Daiyi Peng,Yifeng Lu,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: 이 논문은 복잡한 다단계 작업을 위한 신뢰성 있는 대형 언어 모델(LLM) 구성을 제안하며, 기존의 비효율적인 프롬프트 최적화 방식을 대체할 새로운 프레임워크인 타입 적합성 적응 캐스케이드(TAC)를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 다단계 작업을 위한 LLM의 신뢰할 수 있는 구성이 여전히 주된 도전 과제로 남아 있다.

Method: TAC는 작업 흐름 적응을 타이핑된 확률적 프로그램 학습으로 재구성하는 프레임워크이다.

Result: TAC는 최첨단 프롬프트 최적화 기준보다 상당히 더 우수한 성능을 보였다.

Conclusion: TAC는 신뢰할 수 있는 작업 적합 LLM 시스템 개발을 위한 강건하고 이론적으로 기반이 있는 패러다임을 제공한다.

Abstract: Reliably composing Large Language Models (LLMs) for complex, multi-step
workflows remains a significant challenge. The dominant paradigm-optimizing
discrete prompts in a pipeline-is notoriously brittle and struggles to enforce
the formal compliance required for structured tasks. We introduce
Type-Compliant Adaptation Cascades (TACs), a framework that recasts workflow
adaptation as learning typed probabilistic programs. TACs treats the entire
workflow, which is composed of parameter-efficiently adapted LLMs and
deterministic logic, as an unnormalized joint distribution. This enables
principled, gradient-based training even with latent intermediate structures.
We provide theoretical justification for our tractable optimization objective,
proving that the optimization bias vanishes as the model learns type
compliance. Empirically, TACs significantly outperforms state-of-the-art
prompt-optimization baselines. Gains are particularly pronounced on structured
tasks, improving MGSM-SymPy from $57.1\%$ to $75.9\%$ for a 27B model, MGSM
from $1.6\%$ to $27.3\%$ for a 7B model. TACs offers a robust and theoretically
grounded paradigm for developing reliable, task-compliant LLM systems.

</details>


### [144] [MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models](https://arxiv.org/abs/2508.17467)
*Krishna Teja Chitty-Venkata,Sylvia Howland,Golara Azar,Daria Soboleva,Natalia Vassilieva,Siddhisanket Raskar,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: Mixture of Experts (MoE) 모델은 대형 언어 모델과 비전 언어 모델을 확장할 수 있도록 해주지만, 추론 시간에 여러 가지 문제를 일으킨다. 이를 해결하기 위해 MoE 성능을 평가하는 연구인 MoE-Inference-Bench를 제안한다.


<details>
  <summary>Details</summary>
Motivation: MoE 모델은 대형 언어 모델과 비전 언어 모델의 확장을 가능하게 하지만, 추론 과정에서 여러 가지 문제가 발생한다.

Method: MoE-Inference-Bench를 통해 배치 크기, 시퀀스 길이 및 MoE의 주요 하이퍼파라미터가 성능에 미치는 영향을 분석하고 여러 최적화 기법을 평가한다.

Result: Nvidia H100 GPU에서 여러 최적화 기법 및 MoE의 성능 차이를 평가하여 효율적인 MoE 배포를 위한 통찰력을 제공한다.

Conclusion: 구성에 따른 성능 차이를 확인하고 MoE의 효율적인 배치를 위한 통찰력을 제공한다.

Abstract: Mixture of Experts (MoE) models have enabled the scaling of Large Language
Models (LLMs) and Vision Language Models (VLMs) by achieving massive parameter
counts while maintaining computational efficiency. However, MoEs introduce
several inference-time challenges, including load imbalance across experts and
the additional routing computational overhead. To address these challenges and
fully harness the benefits of MoE, a systematic evaluation of hardware
acceleration techniques is essential. We present MoE-Inference-Bench, a
comprehensive study to evaluate MoE performance across diverse scenarios. We
analyze the impact of batch size, sequence length, and critical MoE
hyperparameters such as FFN dimensions and number of experts on throughput. We
evaluate several optimization techniques on Nvidia H100 GPUs, including
pruning, Fused MoE operations, speculative decoding, quantization, and various
parallelization strategies. Our evaluation includes MoEs from the Mixtral,
DeepSeek, OLMoE and Qwen families. The results reveal performance differences
across configurations and provide insights for the efficient deployment of
MoEs.

</details>


### [145] [ANO : Faster is Better in Noisy Landscape](https://arxiv.org/abs/2508.18258)
*Adrien Kegreisz*

Main category: cs.LG

TL;DR: Stochastic optimizers like Adam과 Adan은 비정상적이거나 노이즈가 있는 환경에서 성능이 저하될 수 있다. 이를 개선하기 위해 Ano라는 새로운 최적화 방법을 제안하였으며, 방향성과 크기를 분리하여 개선된 강건성을 제공한다.


<details>
  <summary>Details</summary>
Motivation: Adam 및 Adan과 같은 기존 방법들이 비정상적 환경에서 성능 저하를 보이기 때문에, 새로운 최적화 알고리즘이 필요하다.

Method: Ano는 방향성과 크기를 분리하며, 모멘텀을 방향 메꾸기에 사용하고, 즉각적인 기울기 크기로 단계 크기를 결정한다. Anolog는 모멘텀 계수에 대한 민감도를 제거하기 위해 로그 스케줄을 사용하여 확장하는 방식이다.

Result: Ano는 비정상적이고 노이즈가 많은 상황에서 성능 향상을 보여주며, 다른 신호 기반 방법들과 유사한 수렴 속도를 갖는다.

Conclusion: Ano는 저소음 작업에서도 경쟁력을 유지하며, 강화 학습과 같은 노이즈와 비정상적 환경에서 상당한 성과를 제공한다.

Abstract: Stochastic optimizers are central to deep learning, yet widely used methods
such as Adam and Adan can degrade in non-stationary or noisy environments,
partly due to their reliance on momentum-based magnitude estimates. We
introduce Ano, a novel optimizer that decouples direction and magnitude:
momentum is used for directional smoothing, while instantaneous gradient
magnitudes determine step size. This design improves robustness to gradient
noise while retaining the simplicity and efficiency of first-order methods. We
further propose Anolog, which removes sensitivity to the momentum coefficient
by expanding its window over time via a logarithmic schedule. We establish
non-convex convergence guarantees with a convergence rate similar to other
sign-based methods, and empirically show that Ano provides substantial gains in
noisy and non-stationary regimes such as reinforcement learning, while
remaining competitive on low-noise tasks such as standard computer vision
benchmarks.

</details>


### [146] [A Human-In-The-Loop Approach for Improving Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.17477)
*Martin Käppel,Julian Neuberger,Felix Möhrlein,Sven Weinzierl,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: 예측 프로세스 모니터링은 비즈니스 프로세스의 실행 인스턴스에 대한 적극적인 대응을 가능하게 한다. 본 논문은 편향된 의사결정을 식별하고 수정하기 위한 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 예측 모델은 편향되고 불공정한 패턴을 발견할 수 있으며, 이는 프로세스 참가자의 성별이나 나이와 같은 민감한 속성에 기반한 편향된 예측으로 이어질 수 있다.

Method: 본 논문에서는 의사결정 트리 모델의 간단한 변화를 통해 공정한 결정과 불공정한 결정을 구별하는 인간-개입 접근 방식을 사용한다.

Result: 제안된 접근 방식은 편향된 데이터가 존재하는 상황에서도 공정성과 정확성 간의 유망한 균형을 이룬다.

Conclusion: 모든 소스 코드와 데이터는 공개적으로 이용 가능하다.

Abstract: Predictive process monitoring enables organizations to proactively react and
intervene in running instances of a business process. Given an incomplete
process instance, predictions about the outcome, next activity, or remaining
time are created. This is done by powerful machine learning models, which have
shown impressive predictive performance. However, the data-driven nature of
these models makes them susceptible to finding unfair, biased, or unethical
patterns in the data. Such patterns lead to biased predictions based on
so-called sensitive attributes, such as the gender or age of process
participants. Previous work has identified this problem and offered solutions
that mitigate biases by removing sensitive attributes entirely from the process
instance. However, sensitive attributes can be used both fairly and unfairly in
the same process instance. For example, during a medical process, treatment
decisions could be based on gender, while the decision to accept a patient
should not be based on gender. This paper proposes a novel, model-agnostic
approach for identifying and rectifying biased decisions in predictive business
process monitoring models, even when the same sensitive attribute is used both
fairly and unfairly. The proposed approach uses a human-in-the-loop approach to
differentiate between fair and unfair decisions through simple alterations on a
decision tree model distilled from the original prediction model. Our results
show that the proposed approach achieves a promising tradeoff between fairness
and accuracy in the presence of biased data. All source code and data are
publicly available at https://doi.org/10.5281/zenodo.15387576.

</details>


### [147] [Learning Interpretable Differentiable Logic Networks for Time-Series Classification](https://arxiv.org/abs/2508.17512)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: 이 논문은 차별 가능한 로직 네트워크(DLN)를 단일 변수 시계열 분류(TSC) 분야에 처음 적용하여 정확성, 해석 가능성 및 계산 효율성을 유지함을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: DLN의 강점을 새로운 영역인 단일 변수 시계열 데이터에 적용하고 이를 통해 TSC에서의 성능을 향상시키고자 합니다.

Method: Catch22와 TSFresh를 이용한 특성 기반 표현을 채택하여 순차 시계열을 DLN 분류에 적합한 벡터 형식으로 변환합니다. 또한, 모든 구성 요소를 하이퍼파라미터 검색 공간에 통합하여 최적 설정을 공동으로 선택할 수 있도록 합니다.

Result: 51개의 공개 단일 변수 TSC 벤치마크에서 평가한 결과, DLN은 경쟁력 있는 정확도를 유지하고 낮은 추론 비용과 함께 투명하고 해석 가능한 결정 로직을 제공합니다.

Conclusion: 이러한 결과는 DLN이 표 형 분류 및 회귀 작업의 이전 연구 결과와도 잘 연계된다는 것을 보여줍니다.

Abstract: Differentiable logic networks (DLNs) have shown promising results in tabular
domains by combining accuracy, interpretability, and computational efficiency.
In this work, we apply DLNs to the domain of TSC for the first time, focusing
on univariate datasets. To enable DLN application in this context, we adopt
feature-based representations relying on Catch22 and TSFresh, converting
sequential time series into vectorized forms suitable for DLN classification.
Unlike prior DLN studies that fix the training configuration and vary various
settings in isolation via ablation, we integrate all such configurations into
the hyperparameter search space, enabling the search process to select jointly
optimal settings. We then analyze the distribution of selected configurations
to better understand DLN training dynamics. We evaluate our approach on 51
publicly available univariate TSC benchmarks. The results confirm that
classification DLNs maintain their core strengths in this new domain: they
deliver competitive accuracy, retain low inference cost, and provide
transparent, interpretable decision logic, thus aligning well with previous DLN
findings in the realm of tabular classification and regression tasks.

</details>


### [148] [GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts](https://arxiv.org/abs/2508.17515)
*Kyrylo Yemets,Mykola Lukashchuk,Ivan Izonin*

Main category: cs.LG

TL;DR: 이 논문에서는 단일 변수 시계열 예측을 단순화한 모델 아키텍처를 제안하고, 전통적인 방법보다 더 나은 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 정확한 단일 변수 예측은 에너지 시장, 수문학, 소매 수요 및 IoT 모니터링과 같은 실제 시스템에서 긴급히 필요하다.

Method: 희소 MoE 계산과 새로운 주의 기반 게이팅 메커니즘을 결합하여 단일 변수 시계열 예측의 훈련 과정을 단순화한다.

Result: 제안된 게이팅 설계는 균형 잡힌 전문가 활용을 촉진하고 고전 MoE 구현에서 일반적으로 사용되는 보조 로드 밸런싱 손실 없이는 뛰어난 예측 정확도를 달성한다.

Conclusion: 우리의 방법은 정확성과 계산 효율성을 모두 중요시하는 실제 시계열 예측 애플리케이션에 대한 잠재력을 강조한다.

Abstract: Accurate univariate forecasting remains a pressing need in real-world
systems, such as energy markets, hydrology, retail demand, and IoT monitoring,
where signals are often intermittent and horizons span both short- and
long-term. While transformers and Mixture-of-Experts (MoE) architectures are
increasingly favored for time-series forecasting, a key gap persists: MoE
models typically require complicated training with both the main forecasting
loss and auxiliary load-balancing losses, along with careful
routing/temperature tuning, which hinders practical adoption. In this paper, we
propose a model architecture that simplifies the training process for
univariate time series forecasting and effectively addresses both long- and
short-term horizons, including intermittent patterns. Our approach combines
sparse MoE computation with a novel attention-inspired gating mechanism that
replaces the traditional one-layer softmax router. Through extensive empirical
evaluation, we demonstrate that our gating design naturally promotes balanced
expert utilization and achieves superior predictive accuracy without requiring
the auxiliary load-balancing losses typically used in classical MoE
implementations. The model achieves better performance while utilizing only a
fraction of the parameters required by state-of-the-art transformer models,
such as PatchTST. Furthermore, experiments across diverse datasets confirm that
our MoE architecture with the proposed gating mechanism is more computationally
efficient than LSTM for both long- and short-term forecasting, enabling
cost-effective inference. These results highlight the potential of our approach
for practical time-series forecasting applications where both accuracy and
computational efficiency are critical.

</details>


### [149] [Modeling Irregular Astronomical Time Series with Neural Stochastic Delay Differential Equations](https://arxiv.org/abs/2508.17521)
*YongKyung Oh,Seungsu Kam,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: 이 논문은 불규칙하게 샘플링된 천문 데이터의 분류 및 이상 탐지를 위한 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 조사의 천문학적 시계열 데이터는 종종 불규칙하게 샘플링되고 불완전하여, 분류 및 이상 탐지에 도전 과제를 제기합니다.

Method: Neural Stochastic Delay Differential Equations (Neural SDDEs)를 기반으로 하는 새로운 프레임워크를 도입하며, 이는 확률 모델링과 신경망을 결합하여 지연된 시간 역학을 포착하고 불규칙한 관측을 처리합니다.

Result: 불규칙하게 샘플링된 천문 데이터에 대한 실험은 강력한 분류 정확성과 부분 레이블이 있는 경우에도 새로운 천체 물리학적 사건을 효과적으로 탐지함을 보여줍니다.

Conclusion: 이 연구는 관측 제약 하에서 시계열 분석을 위한 원칙적이면서도 실용적인 도구로 Neural SDDEs를 강조합니다.

Abstract: Astronomical time series from large-scale surveys like LSST are often
irregularly sampled and incomplete, posing challenges for classification and
anomaly detection. We introduce a new framework based on Neural Stochastic
Delay Differential Equations (Neural SDDEs) that combines stochastic modeling
with neural networks to capture delayed temporal dynamics and handle irregular
observations. Our approach integrates a delay-aware neural architecture, a
numerical solver for SDDEs, and mechanisms to robustly learn from noisy, sparse
sequences. Experiments on irregularly sampled astronomical data demonstrate
strong classification accuracy and effective detection of novel astrophysical
events, even with partial labels. This work highlights Neural SDDEs as a
principled and practical tool for time series analysis under observational
constraints.

</details>


### [150] [Gumbel-MPNN: Graph Rewiring with Gumbel-Softmax](https://arxiv.org/abs/2508.17531)
*Marcel Hoffmann,Lukas Galke,Ansgar Scherp*

Main category: cs.LG

TL;DR: 이 논문은 메시지 전달 신경망(MPNN)의 성능이 주변 클래스 분포의 일관성에 더욱 의존한다는 것을 보여주고, Gumbel-Softmax 기반의 재배선 방법을 제안하여 MPNN의 분류 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 그래프 동질성은 노드 분류에서 메시지 전달 신경망의 필수 속성으로 여겨져 왔습니다. 최근의 연구 결과들은 성능이 이웃 클래스 분포의 일관성과 밀접한 관련이 있음을 시사합니다.

Method: 우리는 MPNN 성능이 클래스 내 전체 이웃 분포의 구성 요소 수에 의존한다는 것을 보여줍니다. 클래스의 이웃 분포 구성 요소로 나누어 분포의 정보량을 증가시키지만 MPNN의 성능 향상은 관찰되지 않았습니다. 우리는 이웃 분포의 편차를 줄이는 Gumbel-Softmax 기반의 재배선 방법을 제안합니다.

Result: 우리의 결과는 새로운 방법이 주변 정보의 유용성을 향상시키고, 장기 의존성을 처리하며, 과도한 압축을 완화하고, MPNN의 분류 성능을 증가시킨다는 것을 보여줍니다.

Conclusion: 코드는 https://github.com/Bobowner/Gumbel-Softmax-MPNN 에서 사용할 수 있습니다.

Abstract: Graph homophily has been considered an essential property for message-passing
neural networks (MPNN) in node classification. Recent findings suggest that
performance is more closely tied to the consistency of neighborhood class
distributions. We demonstrate that the MPNN performance depends on the number
of components of the overall neighborhood distribution within a class. By
breaking down the classes into their neighborhood distribution components, we
increase measures of neighborhood distribution informativeness but do not
observe an improvement in MPNN performance. We propose a Gumbel-Softmax-based
rewiring method that reduces deviations in neighborhood distributions. Our
results show that our new method enhances neighborhood informativeness, handles
long-range dependencies, mitigates oversquashing, and increases the
classification performance of the MPNN. The code is available at
https://github.com/Bobowner/Gumbel-Softmax-MPNN.

</details>


### [151] [Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction](https://arxiv.org/abs/2508.17554)
*Shuqi Zi,Haitz Sáez de Ocáriz Borde,Emma Rocheteau,Pietro Lio'*

Main category: cs.LG

TL;DR: S$^2$G-Net은 ICU에서 환자의 입원 기간 예측을 위한 혁신적인 신경망 아키텍처로, 상태 공간 시퀀스 모델링과 다중 뷰 그래프 신경망을 통합한다.


<details>
  <summary>Details</summary>
Motivation: ICU에서 환자의 입원 기간을 예측하는 것은 병원 자원 관리를 위해 중요하지만, 전자 건강 기록의 이질적이고 불규칙한 샘플링 특성으로 인해 도전적이다.

Method: S$^2$G-Net은 Mamba 상태 공간 모델을 이용하여 환자 경로를 캡처하고, 최적화된 GraphGPS 백본을 활용하여 진단, 관리, 의미적 특징에서 유도된 이질적 환자 유사성 그래프를 통합하는 두 가지 경로를 사용한다.

Result: 대규모 MIMIC-IV 코호트 데이터셋에 대한 실험에서 S$^2$G-Net은 모든 주요 메트릭에서 시퀀스 모델, 그래프 모델 및 하이브리드 접근 방식보다 일관되게 성능이 우수함을 보여준다.

Conclusion: S$^2$G-Net은 다중 모달 임상 데이터를 활용하여 ICU 입원 기간 예측을 위한 효과적이고 확장 가능한 솔루션을 제공한다.

Abstract: Predicting a patient's length of stay (LOS) in the intensive care unit (ICU)
is a critical task for hospital resource management, yet remains challenging
due to the heterogeneous and irregularly sampled nature of electronic health
records (EHRs). In this work, we propose S$^2$G-Net, a novel neural
architecture that unifies state-space sequence modeling with multi-view Graph
Neural Networks (GNNs) for ICU LOS prediction. The temporal path employs Mamba
state-space models (SSMs) to capture patient trajectories, while the graph path
leverages an optimized GraphGPS backbone, designed to integrate heterogeneous
patient similarity graphs derived from diagnostic, administrative, and semantic
features. Experiments on the large-scale MIMIC-IV cohort dataset show that
S$^2$G-Net consistently outperforms sequence models (BiLSTM, Mamba,
Transformer), graph models (classic GNNs, GraphGPS), and hybrid approaches
across all primary metrics. Extensive ablation studies and interpretability
analyses highlight the complementary contributions of each component of our
architecture and underscore the importance of principled graph construction.
These results demonstrate that S$^2$G-Net provides an effective and scalable
solution for ICU LOS prediction with multi-modal clinical data.

</details>


### [152] [Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA](https://arxiv.org/abs/2508.17586)
*Daniel Frees,Aditri Bhagirath,Moritz Bolling*

Main category: cs.LG

TL;DR: LLM 미세 조정의 계산 비용 문제를 해결하기 위해 LoRA 및 DoRA 방법을 다루고, 보다 작은 언어 모델인 minBERT에 대한 효율성과 성능을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: LLM의 미세 조정은 계산 비용이 매우 높아, 작은 기업이나 연구팀이 새로운 연구에 참여하는 데 제약이 있다.

Method: LoRA와 DoRA를 사용하여 minBERT 모델에 대한 효율성과 성능을 벤치마킹하고, 자동 혼합 정밀도(AMP)와 결합하여 최적의 사용자 정의 구성을 탐색하였다.

Result: LoRA와 DoRA의 최적 구성이 훈련 효율성을 크게 향상시키며, 작은 모델 공간에서도 그래디언트 업데이트가 본질적으로 저랭크임을 확인하였다.

Conclusion: 최적화된 다중 작업 minBERT 모델을 훈련하여 감정 분석, 패러프레이즈 검출 및 유사도 점수를 동시에 수행할 수 있었다.

Abstract: While Large Language Models (LLMs) have revolutionized artificial
intelligence, fine-tuning LLMs is extraordinarily computationally expensive,
preventing smaller businesses and research teams with limited GPU resources
from engaging with new research. Hu et al and Liu et al introduce Low-Rank
Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) as highly
efficient and performant solutions to the computational challenges of LLM
fine-tuning, demonstrating huge speedups and memory usage savings for models
such as GPT-3 and RoBERTa. We seek to expand upon the original LoRA and DoRA
papers by benchmarking efficiency and performance of LoRA and DoRA when applied
to a much smaller scale of language model: our case study here is the compact
minBERT model. Our findings reveal that optimal custom configurations of LoRA
and DoRA, coupled with Automatic Mixed Precision (AMP), significantly enhance
training efficiency without compromising performance. Furthermore, while the
parameterization of minBERT is significantly smaller than GPT-3, our results
validate the observation that gradient updates to language models are
inherently low-rank even in small model space, observing that rank 1
decompositions yield negligible performance deficits. Furthermore, aided by our
highly efficient minBERT implementation, we investigate numerous architectures,
custom loss functions, and hyperparameters to ultimately train an optimal
ensembled multitask minBERT model to simultaneously perform sentiment analysis,
paraphrase detection, and similarity scoring.

</details>


### [153] [ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning](https://arxiv.org/abs/2508.17608)
*Wentao Tan,Qiong Cao,Chao Xue,Yibing Zhan,Changxing Ding,Xiaodong He*

Main category: cs.LG

TL;DR: ReChartPrompt와 ChartSimRL을 활용하여 차트 이미지를 실행 가능한 코드로 변환하는 ChartMaster 모델을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 차트 이미지를 실행 가능한 코드로 변환하는 작업은 제한된 데이터 다양성과 학습 과정에서 생성된 차트와 원본 차트 간의 시각적 일관성을 유지하는 데 어려움이 있다.

Method: ReChartPrompt는 arXiv 논문의 실제 차트를 프롬프트로 사용하여 다양한 스타일과 풍부한 콘텐츠의 ReChartPrompt-240K 데이터세트를 구축하고, ChartSimRL은 새로운 차트 유사성 보상에 따라 작동하는 강화 학습 알고리즘이다.

Result: ChartMaster 모델은 7B 매개변수 모델 중 최첨단 결과를 달성하고, 다양한 차트-코드 생성 벤치마크에서 GPT-4o와 경쟁할 수 있다.

Conclusion: ReChartPrompt와 ChartSimRL의 통합을 통해 차트-코드 변환의 다중 모달 특성을 고려하여 모델의 정확한 차트 재현 능력을 향상시켰다.

Abstract: The chart-to-code generation task requires MLLMs to convert chart images into
executable code. This task faces two major challenges: limited data diversity
and insufficient maintenance of visual consistency between generated and
original charts during training. Existing datasets mainly rely on seed data to
prompt GPT models for code generation, resulting in homogeneous samples. To
address this, we propose ReChartPrompt, which leverages real-world,
human-designed charts from arXiv papers as prompts instead of synthetic seeds.
Using the diverse styles and rich content of arXiv charts, we construct
ReChartPrompt-240K, a large-scale and highly diverse dataset. Another challenge
is that although SFT effectively improve code understanding, it often fails to
ensure that generated charts are visually consistent with the originals. To
address this, we propose ChartSimRL, a GRPO-based reinforcement learning
algorithm guided by a novel chart similarity reward. This reward consists of
attribute similarity, which measures the overlap of chart attributes such as
layout and color between the generated and original charts, and visual
similarity, which assesses similarity in texture and other overall visual
features using convolutional neural networks. Unlike traditional text-based
rewards such as accuracy or format rewards, our reward considers the multimodal
nature of the chart-to-code task and effectively enhances the model's ability
to accurately reproduce charts. By integrating ReChartPrompt and ChartSimRL, we
develop the ChartMaster model, which achieves state-of-the-art results among
7B-parameter models and even rivals GPT-4o on various chart-to-code generation
benchmarks. All resources are available at
https://github.com/WentaoTan/ChartMaster.

</details>


### [154] [A Proportional-Integral Controller-Incorporated SGD Algorithm for High Efficient Latent Factor Analysis](https://arxiv.org/abs/2508.17609)
*Jinli Li,Shiyu Long,Minglian Han*

Main category: cs.LG

TL;DR: 본 논문에서는 고차원 희소 행렬을 효과적으로 분석하기 위한 PILF 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 산업 빅데이터 시나리오에서 고차원 희소 행렬은 대규모 노드 간의 상호작용 관계를 특성화하는 데 사용된다.

Method: 동일한 샘플 간의 관련성을 통합하고 비례-적분 제어 메커니즘을 통해 학습 오차를 정제하는 PI 가속 SGD 알고리즘을 개발한 PILF 모델을 제안한다.

Result: 비교 실험을 통해 PILF 모델이 HDI 행렬에서 우수한 표현 능력을 보임을 입증하였다.

Conclusion: 기존 SGD-LFA의 한계를 극복하고 더 빠른 수렴 속도와 개선된 일반화 성능을 제공한다.

Abstract: In industrial big data scenarios, high-dimensional sparse matrices (HDI) are
widely used to characterize high-order interaction relationships among massive
nodes. The stochastic gradient descent-based latent factor analysis (SGD-LFA)
method can effectively extract deep feature information embedded in HDI
matrices. However, existing SGD-LFA methods exhibit significant limitations:
their parameter update process relies solely on the instantaneous gradient
information of current samples, failing to incorporate accumulated experiential
knowledge from historical iterations or account for intrinsic correlations
between samples, resulting in slow convergence speed and suboptimal
generalization performance. Thus, this paper proposes a PILF model by
developing a PI-accelerated SGD algorithm by integrating correlated instances
and refining learning errors through proportional-integral (PI) control
mechanism that current and historical information; Comparative experiments
demonstrate the superior representation capability of the PILF model on HDI
matrices

</details>


### [155] [Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning](https://arxiv.org/abs/2508.17630)
*An Ning,Tai Yue Li,Nan Yow Chen*

Main category: cs.LG

TL;DR: QGAT은 주목 메커니즘에 변량 양자 회로를 통합한 하이브리드 그래프 신경망으로, 효율적인 비선형 상호작용을 가능하게 하고 학습 과제에 유연하게 적응하며, 양자 임베딩이 노이즈에 대한 강인성을 높이는 데 도움을 준다.


<details>
  <summary>Details</summary>
Motivation: 양자 기술의 발전을 활용하여 그래프 신경망의 성능을 개선하고자 한다.

Method: 양자 회로를 사용하는 단일 주목 회로를 통해 복잡한 구조적 의존성을 포착하고, 각 헤드 간의 매개변수를 공유하여 계산 오버헤드와 모델 복잡성을 줄인다.

Result: QGAT은 복잡한 구조적 의존성을 효과적으로 포착하고, 유도 시나리오에서 전반적인 일반화를 개선하는 성능을 보인다.

Conclusion: QGAT은 화학, 생물학, 네트워크 분석과 같은 분야에서 확장 가능한 양자 강화 학습의 잠재력을 지니며, 기존 아키텍처에 쉽게 통합될 수 있다.

Abstract: We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neural
network that integrates variational quantum circuits into the attention
mechanism. At its core, QGAT employs strongly entangling quantum circuits with
amplitude-encoded node features to enable expressive nonlinear interactions.
Distinct from classical multi-head attention that separately computes each
head, QGAT leverages a single quantum circuit to simultaneously generate
multiple attention coefficients. This quantum parallelism facilitates parameter
sharing across heads, substantially reducing computational overhead and model
complexity. Classical projection weights and quantum circuit parameters are
optimized jointly in an end-to-end manner, ensuring flexible adaptation to
learning tasks. Empirical results demonstrate QGAT's effectiveness in capturing
complex structural dependencies and improved generalization in inductive
scenarios, highlighting its potential for scalable quantum-enhanced learning
across domains such as chemistry, biology, and network analysis. Furthermore,
experiments confirm that quantum embedding enhances robustness against feature
and structural noise, suggesting advantages in handling real-world noisy data.
The modularity of QGAT also ensures straightforward integration into existing
architectures, allowing it to easily augment classical attention-based models.

</details>


### [156] [Longitudinal Progression Prediction of Alzheimer's Disease with Tabular Foundation Model](https://arxiv.org/abs/2508.17649)
*Yilang Ding,Jiawen Ren,Jiaying Lu,Gloria Hyunjung Kwak,Armin Iraji,Alex Fedorov*

Main category: cs.LG

TL;DR: L2C-TabPFN은 탁월한 예측 결과를 제공하는 알츠하이머병 예측 방법이다.


<details>
  <summary>Details</summary>
Motivation: 알츠하이머병의 예측은 다요인적 원인과 복잡한 임상 데이터 때문에 도전적이다.

Method: L2C-TabPFN은 L2C 변환과 사전 훈련된 TabPFN을 통합하여 TADPOLE 데이터셋을 사용해 결과를 예측한다.

Result: L2C-TabPFN은 진단 및 인지 결과에서 경쟁력 있는 성능을 이루며, 심실 용적 예측에서 최첨단 결과를 제공한다.

Conclusion: 이 연구는 알츠하이머병의 임상적 이미징 마커에 대한 장기 예측을 위한 기초 모델의 잠재력을 강조한다.

Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that remains
challenging to predict due to its multifactorial etiology and the complexity of
multimodal clinical data. Accurate forecasting of clinically relevant
biomarkers, including diagnostic and quantitative measures, is essential for
effective monitoring of disease progression. This work introduces L2C-TabPFN, a
method that integrates a longitudinal-to-cross-sectional (L2C) transformation
with a pre-trained Tabular Foundation Model (TabPFN) to predict Alzheimer's
disease outcomes using the TADPOLE dataset. L2C-TabPFN converts sequential
patient records into fixed-length feature vectors, enabling robust prediction
of diagnosis, cognitive scores, and ventricular volume. Experimental results
demonstrate that, while L2C-TabPFN achieves competitive performance on
diagnostic and cognitive outcomes, it provides state-of-the-art results in
ventricular volume prediction. This key imaging biomarker reflects
neurodegeneration and progression in Alzheimer's disease. These findings
highlight the potential of tabular foundational models for advancing
longitudinal prediction of clinically relevant imaging markers in Alzheimer's
disease.

</details>


### [157] [Heterogeneous co-occurrence embedding for visual information exploration](https://arxiv.org/abs/2508.17663)
*Takuro Ishida,Tetsuo Furukawa*

Main category: cs.LG

TL;DR: 이 논문은 시각 정보 탐색을 위한 공표현 데이터의 임베딩 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 공표현 확률이 서로 다른 도메인 간 쌍의 요소들 사이에 측정되는 경우에 대한 탐색.

Method: 이 방법은 이질적인 요소들을 해당 이차원 잠재 공간에 매핑하여 비대칭 관계를 시각화한다.

Result: 제안된 방법을 형용사-명사 데이터셋, NeurIPS 데이터셋, 주어-동사-목적어 데이터셋에 적용하여 유용성을 입증하였다.

Conclusion: 이 방법은 도메인 내 및 도메인 간 분석을 모두 보여준다.

Abstract: This paper proposes an embedding method for co-occurrence data aimed at
visual information exploration. We consider cases where co-occurrence
probabilities are measured between pairs of elements from heterogeneous
domains. The proposed method maps these heterogeneous elements into
corresponding two-dimensional latent spaces, enabling visualization of
asymmetric relationships between the domains. The key idea is to embed the
elements in a way that maximizes their mutual information, thereby preserving
the original dependency structure as much as possible. This approach can be
naturally extended to cases involving three or more domains, using a
generalization of mutual information known as total correlation. For
inter-domain analysis, we also propose a visualization method that assigns
colors to the latent spaces based on conditional probabilities, allowing users
to explore asymmetric relationships interactively. We demonstrate the utility
of the method through applications to an adjective-noun dataset, the NeurIPS
dataset, and a subject-verb-object dataset, showcasing both intra- and
inter-domain analysis.

</details>


### [158] [Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models](https://arxiv.org/abs/2508.17675)
*Victoria Yan,Honor Chotkowski,Fengran Wang,Alex Fedorov*

Main category: cs.LG

TL;DR: 본 연구는 생성적 다중모달 대형 언어 모델을 활용하여 기존 인지 테스트에 대한 합성 표준 데이터를 생성할 수 있는 가능성을 조사하였다.


<details>
  <summary>Details</summary>
Motivation: 표준 데이터가 인지 평가에서 개별 성과를 평가하는 필수 벤치마크이기 때문에 새로운 인지 테스트 개발이 어렵다.

Method: GPT-4o와 GPT-4o-mini를 사용하여 이미지 기반 인지 평가에 대한 표준 텍스트 응답을 합성하는 두 가지 프롬프트 전략을 평가하였다.

Result: 고급 프롬프트 전략이 진단 그룹을 보다 효과적으로 구별하고 인구 통계적 다양성을 포착한 합성 응답을 생성하였다.

Conclusion: 본 연구는 정제된 프롬프트 방법에 의해 유도되는 생성적 다중모달 LLM들이 기존 인지 테스트를 위한 강력한 합성 표준 데이터를 생성할 수 있음을 보여준다.

Abstract: Cognitive assessments require normative data as essential benchmarks for
evaluating individual performance. Hence, developing new cognitive tests based
on novel image stimuli is challenging due to the lack of readily available
normative data. Traditional data collection methods are costly, time-consuming,
and infrequently updated, limiting their practical utility. Recent advancements
in generative multimodal large language models (MLLMs) offer a new approach to
generate synthetic normative data from existing cognitive test images. We
investigated the feasibility of using MLLMs, specifically GPT-4o and
GPT-4o-mini, to synthesize normative textual responses for established
image-based cognitive assessments, such as the "Cookie Theft" picture
description task. Two distinct prompting strategies-naive prompts with basic
instructions and advanced prompts enriched with contextual guidance-were
evaluated. Responses were analyzed using embeddings to assess their capacity to
distinguish diagnostic groups and demographic variations. Performance metrics
included BLEU, ROUGE, BERTScore, and an LLM-as-a-judge evaluation. Advanced
prompting strategies produced synthetic responses that more effectively
distinguished between diagnostic groups and captured demographic diversity
compared to naive prompts. Superior models generated responses exhibiting
higher realism and diversity. BERTScore emerged as the most reliable metric for
contextual similarity assessment, while BLEU was less effective for evaluating
creative outputs. The LLM-as-a-judge approach provided promising preliminary
validation results. Our study demonstrates that generative multimodal LLMs,
guided by refined prompting methods, can feasibly generate robust synthetic
normative data for existing cognitive tests, thereby laying the groundwork for
developing novel image-based cognitive assessments without the traditional
limitations.

</details>


### [159] [TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training](https://arxiv.org/abs/2508.17677)
*Yifan Wang,Binbin Liu,Fengze Liu,Yuanfan Guo,Jiyao Deng,Xuecheng Wu,Weidong Zhou,Xiaohuan Zhou,Taifeng Wang*

Main category: cs.LG

TL;DR: TiKMiX는 언어 모델의 데이터를 동적으로 조정하여 성능을 향상시키는 새로운 방법이다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델의 최종 성능은 사전 훈련 시 사용되는 데이터 혼합에 의해 결정되지만, 정적인 혼합 전략은 최적이 아니다.

Method: TiKMiX는 모델의 진화하는 선호도에 따라 데이터를 동적으로 조정하며 Group Influence라는 효율적인 메트릭을 도입하여 데이터 도메인의 영향을 평가한다.

Result: TiKMiX-D는 최신 방법인 REGMIX를 초과하는 성능을 보이며, TiKMiX-M은 9개의 다운스트림 벤치마크에서 평균 2%의 성능 향상을 이끌어낸다.

Conclusion: TiKMiX는 데이터를 정적으로 비율에 따라 조정하는 것보다 Group Influence에 따라 동적으로 조정하는 것이 성능을 크게 개선한다고 입증한다.

Abstract: The data mixture used in the pre-training of a language model is a
cornerstone of its final performance. However, a static mixing strategy is
suboptimal, as the model's learning preferences for various data domains shift
dynamically throughout training. Crucially, observing these evolving
preferences in a computationally efficient manner remains a significant
challenge. To address this, we propose TiKMiX, a method that dynamically
adjusts the data mixture according to the model's evolving preferences. TiKMiX
introduces Group Influence, an efficient metric for evaluating the impact of
data domains on the model. This metric enables the formulation of the data
mixing problem as a search for an optimal, influence-maximizing distribution.
We solve this via two approaches: TiKMiX-D for direct optimization, and
TiKMiX-M, which uses a regression model to predict a superior mixture. We
trained models with different numbers of parameters, on up to 1 trillion
tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like
REGMIX while using just 20% of the computational resources. TiKMiX-M leads to
an average performance gain of 2% across 9 downstream benchmarks. Our
experiments reveal that a model's data preferences evolve with training
progress and scale, and we demonstrate that dynamically adjusting the data
mixture based on Group Influence, a direct measure of these preferences,
significantly improves performance by mitigating the underdigestion of data
seen with static ratios.

</details>


### [160] [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
*Trinayan Baruah,Kaustubh Shivdikar,Sara Prescott,David Kaeli*

Main category: cs.LG

TL;DR: Mamba 기반 상태 공간 모델이 계산 복잡성을 줄이며 GPU에서의 성능을 향상시킬 수 있는 잠재력을 보여준다.


<details>
  <summary>Details</summary>
Motivation: Mamba 기반 상태 공간 모델은 전통적인 변환기의 계산 복잡성을 해결하고자 한다.

Method: Mamba 기반의 상태 공간 모델을 평가하고 GPU에서의 훈련 중의 행동을 특성화하며, 다양한 모델 구조를 아우르는 워크로드 스위트를 구성하였다.

Result: Mamba 기반 SSM의 GPU에서의 훈련 행동을 분석하여 아키텍처적 시사점을 도출하였다.

Conclusion: 이 연구는 이러한 모델의 성능을 계속해서 확장할 수 있는 최적화 가능성을 제시한다.

Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative
to the ubiquitous transformers. Despite the expressive power of transformers,
the quadratic complexity of computing attention is a major impediment to
scaling performance as we increase the sequence length. SSMs provide an
alternative path that addresses this problem, reducing the computational
complexity requirements of self-attention with novel model architectures for
different domains and fields such as video, text generation and graphs. Thus,
it is important to characterize the behavior of these emerging workloads on
GPUs and understand their requirements during GPU microarchitectural design. In
this work we evaluate Mamba-based SSMs and characterize their behavior during
training on GPUs. We construct a workload suite that offers representative
models that span different model architectures. We then use this suite to
analyze the architectural implications of running Mamba-based SSMs on GPUs. Our
work sheds new light on potential optimizations to continue scaling the
performance for such models.

</details>


### [161] [On the Edge of Memorization in Diffusion Models](https://arxiv.org/abs/2508.17689)
*Sam Buchanan,Druv Pai,Yi Ma,Valentin De Bortoli*

Main category: cs.LG

TL;DR: 이 논문은 확산 모델의 기억화와 일반화 간의 상호작용을 이론적으로 이해하고 이를 통해 모델에 적용될 수 있는 실용적인 통찰력을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 확산 모델의 기억화와 일반화 간의 상호작용 이해는 저작권 침해 및 데이터 프라이버시 문제와 관련하여 실질적인 영향이 있다.

Method: 우리는 완전히 합성적이거나 자연 이미지와 유사한 구조화된 데이터로 훈련된 확산 모델에서 이러한 현상을 조사하기 위한 과학적이고 수학적인 '실험실'을 제시한다.

Result: 하나의 혼합 모델과 일반화 모델 간의 훈련 손실 차이를 기반으로 기억화 및 일반화 행동이 결정된다는 가설을 세우고, 이러한 예측을 실험으로 검증하여 상전이를 예측할 수 있음을 보여준다.

Conclusion: 우리의 이론은 기억화가 우세해지는 모델 크기를 분석적으로 예측할 수 있게 해주며, 향후 이론적 및 경험적 조사에 대한 의미 있는 배경을 제공한다.

Abstract: When do diffusion models reproduce their training data, and when are they
able to generate samples beyond it? A practically relevant theoretical
understanding of this interplay between memorization and generalization may
significantly impact real-world deployments of diffusion models with respect to
issues such as copyright infringement and data privacy. In this work, to
disentangle the different factors that influence memorization and
generalization in practical diffusion models, we introduce a scientific and
mathematical "laboratory" for investigating these phenomena in diffusion models
trained on fully synthetic or natural image-like structured data. Within this
setting, we hypothesize that the memorization or generalization behavior of an
underparameterized trained model is determined by the difference in training
loss between an associated memorizing model and a generalizing model. To probe
this hypothesis, we theoretically characterize a crossover point wherein the
weighted training loss of a fully generalizing model becomes greater than that
of an underparameterized memorizing model at a critical value of model
(under)parameterization. We then demonstrate via carefully-designed experiments
that the location of this crossover predicts a phase transition in diffusion
models trained via gradient descent, validating our hypothesis. Ultimately, our
theory enables us to analytically predict the model size at which memorization
becomes predominant. Our work provides an analytically tractable and
practically meaningful setting for future theoretical and empirical
investigations. Code for our experiments is available at
https://github.com/DruvPai/diffusion_mem_gen.

</details>


### [162] [Rethinking Federated Learning Over the Air: The Blessing of Scaling Up](https://arxiv.org/abs/2508.17697)
*Jiaqi Zhu,Bikramjit Das,Yong Xie,Nikolaos Pappas,Howard H. Yang*

Main category: cs.LG

TL;DR: 이 논문은 오버-더-에어 계산을 훈련 프로세스에 통합하여 데이터 프라이버시를 유지하면서 여러 클라이언트 간의 협력적인 모델 훈련을 지원하는 페더레이티드 러닝의 성능을 향상시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 페더레이티드 러닝의 성능은 통신 자원이 제한된 경우에 제약을 받으며, 특히 많은 클라이언트를 지원하는 시스템에서 이러한 문제가 두드러진다.

Method: 훈련 프로세스에 오버-더-에어 계산을 통합하여 중간 파라미터를 아날로그 신호로 전송한다.

Result: 세 가지 주요 이점이 도출되었다: 1) 향상된 프라이버시, 2) 채널 페이딩 완화, 3) 개선된 수렴 속도.

Conclusion: 오버-더-에어 방식의 모델 훈련이 많은 클라이언트를 가진 네트워크에서 페더레이티드 러닝을 위한 실행 가능한 접근법으로 확립되었다.

Abstract: Federated learning facilitates collaborative model training across multiple
clients while preserving data privacy. However, its performance is often
constrained by limited communication resources, particularly in systems
supporting a large number of clients. To address this challenge, integrating
over-the-air computations into the training process has emerged as a promising
solution to alleviate communication bottlenecks. The system significantly
increases the number of clients it can support in each communication round by
transmitting intermediate parameters via analog signals rather than digital
ones. This improvement, however, comes at the cost of channel-induced
distortions, such as fading and noise, which affect the aggregated global
parameters. To elucidate these effects, this paper develops a theoretical
framework to analyze the performance of over-the-air federated learning in
large-scale client scenarios. Our analysis reveals three key advantages of
scaling up the number of participating clients: (1) Enhanced Privacy: The
mutual information between a client's local gradient and the server's
aggregated gradient diminishes, effectively reducing privacy leakage. (2)
Mitigation of Channel Fading: The channel hardening effect eliminates the
impact of small-scale fading in the noisy global gradient. (3) Improved
Convergence: Reduced thermal noise and gradient estimation errors benefit the
convergence rate. These findings solidify over-the-air model training as a
viable approach for federated learning in networks with a large number of
clients. The theoretical insights are further substantiated through extensive
experimental evaluations.

</details>


### [163] [Adaptive Ensemble Learning with Gaussian Copula for Load Forecasting](https://arxiv.org/abs/2508.17700)
*Junying Yang,Gang Lu,Xiaoqing Yan,Peng Xia,Di Wu*

Main category: cs.LG

TL;DR: 이 논문에서는 희소성을 해결하기 위해 Gaussian Copula를 활용한 Adaptive Ensemble Learning 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습은 완전한 데이터에서 정확한 로드 예측이 가능하지만, 데이터 수집에 영향을 미치는 많은 불확실성으로 인해 희소성이 발생합니다.

Method: 모델은 데이터 보완, 기계 학습 구성 및 적응형 앙상블이라는 세 가지 모듈로 구성됩니다. 먼저, Gaussian Copula를 적용하여 희소성을 제거합니다. 그 다음, 다섯 가지 기계 학습 모델을 개별적으로 예측에 활용합니다. 마지막으로, 적응형 앙상블을 통해 최종 가중 합산 결과를 도출합니다.

Result: 실험 결과, 우리의 모델은 강력한 성능을 보였습니다.

Conclusion: 제안된 모델은 희소 데이터 문제를 해결하고 신뢰성 있는 예측 결과를 위해 효과적인 접근 방식을 제공합니다.

Abstract: Machine learning (ML) is capable of accurate Load Forecasting from complete
data. However, there are many uncertainties that affect data collection,
leading to sparsity. This article proposed a model called Adaptive Ensemble
Learning with Gaussian Copula to deal with sparsity, which contains three
modules: data complementation, ML construction, and adaptive ensemble. First,
it applies Gaussian Copula to eliminate sparsity. Then, we utilise five ML
models to make predictions individually. Finally, it employs adaptive ensemble
to get final weighted-sum result. Experiments have demonstrated that our model
are robust.

</details>


### [164] [Copyright Protection for 3D Molecular Structures with Watermarking](https://arxiv.org/abs/2508.17702)
*Runwen Hu,Peilin Chen,Keyan Ding,Shiqi Wang*

Main category: cs.LG

TL;DR: AI가 생물 공학 및 생물학 연구에서 분자 생성을 혁신하고 있지만, 지식 재산 보호에 대한 우려가 커지고 있다. 이를 해결하기 위해 우리는 원자 수준의 특징을 이용하여 분자의 무결성을 보존하고, 기하변환에 대한 강인성을 보장하는 첫 번째 강력한 분자 워터마킹 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI의 발전이 분자 생성에 큰 변화를 가져오고 있지만, 지식 재산 보호에 대한 문제를 해결할 필요가 있다.

Method: 원자 수준의 특징을 사용하여 분자의 무결성을 유지하고, 기하변환에 대한 강인성을 보장하는 워터마킹 방법을 제안한다.

Result: QM9와 GEOM-DRUG 데이터셋을 사용한 실험에서, 워터마크의 정확도가 95.00%를 초과하고 기본 속성이 90.00% 이상 유지된다는 결과를 보였다.

Conclusion: 우리의 워터마킹 기술은 과학적 유용성을 손상시키지 않으면서 분자의 지적 재산을 효과적으로 보호하며, 분자 발견 및 연구 응용에서 안전하고 책임감 있는 AI 통합을 가능하게 한다.

Abstract: Artificial intelligence (AI) revolutionizes molecule generation in
bioengineering and biological research, significantly accelerating discovery
processes. However, this advancement introduces critical concerns regarding
intellectual property protection. To address these challenges, we propose the
first robust watermarking method designed for molecules, which utilizes
atom-level features to preserve molecular integrity and invariant features to
ensure robustness against affine transformations. Comprehensive experiments
validate the effectiveness of our method using the datasets QM9 and GEOM-DRUG,
and generative models GeoBFN and GeoLDM. We demonstrate the feasibility of
embedding watermarks, maintaining basic properties higher than 90.00\% while
achieving watermark accuracy greater than 95.00\%. Furthermore, downstream
docking simulations reveal comparable performance between original and
watermarked molecules, with binding affinities reaching -6.00 kcal/mol and root
mean square deviations below 1.602 \AA. These results confirm that our
watermarking technique effectively safeguards molecular intellectual property
without compromising scientific utility, enabling secure and responsible AI
integration in molecular discovery and research applications.

</details>


### [165] [Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks](https://arxiv.org/abs/2508.17744)
*Sotaro Takeshita,Yurina Takeshita,Daniel Ruffinelli,Simone Paolo Ponzetto*

Main category: cs.LG

TL;DR: 텍스트 임베딩을 잘라내는 것이 하위 성능에 미치는 영향을 연구하다.


<details>
  <summary>Details</summary>
Motivation: 텍스트 임베딩을 잘라내는 것이 왜 성능에 미치는 영향을 이해하고자 한다.

Method: 6개의 최신 텍스트 인코더와 26개의 하위 작업에서 임베딩 차원의 최대 50%를 무작위로 제거하여 성능 변화를 관찰.

Result: 임베딩 차원의 50%를 제외해도 성능 저하가 10% 미만으로 나타나며, 제거된 차원이 성능 향상에 기여함을 발견.

Conclusion: 텍스트 인코딩에 대한 통찰력을 제공하며, 대규모 언어 모델에서 다음 토큰 예측 시에도 비슷한 현상을 관찰한다.

Abstract: In this paper, we study the surprising impact that truncating text embeddings
has on downstream performance. We consistently observe across 6
state-of-the-art text encoders and 26 downstream tasks, that randomly removing
up to 50% of embedding dimensions results in only a minor drop in performance,
less than 10%, in retrieval and classification tasks. Given the benefits of
using smaller-sized embeddings, as well as the potential insights about text
encoding, we study this phenomenon and find that, contrary to what is suggested
in prior work, this is not the result of an ineffective use of representation
space. Instead, we find that a large number of uniformly distributed dimensions
actually cause an increase in performance when removed. This would explain why,
on average, removing a large number of embedding dimensions results in a
marginal drop in performance. We make similar observations when truncating the
embeddings used by large language models to make next-token predictions on
generative tasks, suggesting that this phenomenon is not isolated to
classification or retrieval tasks.

</details>


### [166] [Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2508.17751)
*Alessio Arcudi,Davide Sartor,Alberto Sinigaglia,Vincent François-Lavet,Gian Antonio Susto*

Main category: cs.LG

TL;DR: MANGO는 장기 희소 보상 환경의 문제를 해결하기 위한 새로운 계층적 강화 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 장기 희소 보상 환경에서의 문제를 해결하고자 하는 필요성.

Method: 복잡한 작업을 여러 개의 추상화 계층으로 분해하여 각각의 계층이 추상 상태 공간을 정의하고 수단을 활용해 경로를 매크로 행동으로 모듈화함.

Result: 실험 결과, 표준 강화 학습 방법에 비해 샘플 효율성과 일반화 능력이 크게 향상됨.

Conclusion: MANGO는 강화 학습의 해석 가능성을 높이고, 향후 자동 추상화 발견 및 다층 훈련 전략에 대한 연구가 필요하다.

Abstract: This paper introduces MANGO (Multilayer Abstraction for Nested Generation of
Options), a novel hierarchical reinforcement learning framework designed to
address the challenges of long-term sparse reward environments. MANGO
decomposes complex tasks into multiple layers of abstraction, where each layer
defines an abstract state space and employs options to modularize trajectories
into macro-actions. These options are nested across layers, allowing for
efficient reuse of learned movements and improved sample efficiency. The
framework introduces intra-layer policies that guide the agent's transitions
within the abstract state space, and task actions that integrate task-specific
components such as reward functions. Experiments conducted in
procedurally-generated grid environments demonstrate substantial improvements
in both sample efficiency and generalization capabilities compared to standard
RL methods. MANGO also enhances interpretability by making the agent's
decision-making process transparent across layers, which is particularly
valuable in safety-critical and industrial applications. Future work will
explore automated discovery of abstractions and abstract actions, adaptation to
continuous or fuzzy environments, and more robust multi-layer training
strategies.

</details>


### [167] [SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling](https://arxiv.org/abs/2508.17756)
*Fanjiang Ye,Zepeng Zhao,Yi Mu,Jucheng Shen,Renjie Li,Kaijian Wang,Desen Sun,Saurabh Agarwal,Myungjin Lee,Triston Cao,Aditya Akella,Arvind Krishnamurthy,T. S. Eugene Ng,Zhengzhong Tu,Yuke Wang*

Main category: cs.LG

TL;DR: SuperGen은 초고해상도 비디오 생성을 위한 효율적인 타일 기반 프레임워크로, 추가 교육 없이 다양한 해상도를 지원하며 메모리와 계산 복잡성을 줄입니다.


<details>
  <summary>Details</summary>
Motivation: 초고해상도 비디오 생성에 대한 필요성이 증가하고 있지만, 기존 플랫폼에서는 재훈련 요구 사항과 높은 비용이 문제입니다.

Method: SuperGen은 훈련 없이 타일 기반 알고리즘 혁신을 통해 다양한 해상도를 지원하며, 타일 맞춤형 적응형 캐싱 전략을 통합하여 비디오 생성 속도를 높입니다.

Result: SuperGen은 다양한 벤치마크에서 높은 출력 품질을 유지하며 성능 이득을 극대화합니다.

Conclusion: SuperGen은 비디오 생성의 메모리와 계산 복잡성을 크게 줄이면서도 효율적이며, 우수한 성능을 나타냅니다.

Abstract: Diffusion models have recently achieved remarkable success in generative
tasks (e.g., image and video generation), and the demand for high-quality
content (e.g., 2K/4K videos) is rapidly increasing across various domains.
However, generating ultra-high-resolution videos on existing
standard-resolution (e.g., 720p) platforms remains challenging due to the
excessive re-training requirements and prohibitively high computational and
memory costs. To this end, we introduce SuperGen, an efficient tile-based
framework for ultra-high-resolution video generation. SuperGen features a novel
training-free algorithmic innovation with tiling to successfully support a wide
range of resolutions without additional training efforts while significantly
reducing both memory footprint and computational complexity. Moreover, SuperGen
incorporates a tile-tailored, adaptive, region-aware caching strategy that
accelerates video generation by exploiting redundancy across denoising steps
and spatial regions. SuperGen also integrates cache-guided,
communication-minimized tile parallelism for enhanced throughput and minimized
latency. Evaluations demonstrate that SuperGen harvests the maximum performance
gains while achieving high output quality across various benchmarks.

</details>


### [168] [Evaluating the Quality of the Quantified Uncertainty for (Re)Calibration of Data-Driven Regression Models](https://arxiv.org/abs/2508.17761)
*Jelke Wibbeke,Nico Schönfisch,Sebastian Rohjans,Andreas Rauh*

Main category: cs.LG

TL;DR: 안전-critical 응용 프로그램에서 데이터 기반 모델은 정확할 뿐만 아니라 신뢰할 수 있는 불확실성 추정치를 제공해야 합니다. 그러나 다양한 보정 메트릭과 재보정 방법이 존재하지만, 이들이 정의, 가정 및 척도에서 상당히 다르며, 이는 연구 간 결과 해석과 비교에 어려움을 초래합니다. 우리는 문헌에서 회귀 보정 메트릭을 체계적으로 추출하고 분류하며, 이러한 메트릭을 특정 모델링 방법이나 재보정 접근 방식과 독립적으로 벤치마킹합니다. 실험을 통해 메트릭들이 자주 상충하는 결과를 산출한다는 것을 보여주며, 특정 메트릭이 misleading impressions를 생성할 수 있음을 지적합니다.


<details>
  <summary>Details</summary>
Motivation: 안전-critical 응용 프로그램에서 데이터 기반 모델은 높은 정확성과 신뢰할 수 있는 불확실성 추정치를 요구합니다.

Method: 문헌에서 회귀 보정 메트릭을 체계적으로 추출하고, 특정 모델링 방법이나 재보정 접근 방식과 독립적으로 이 메트릭들을 벤치마킹합니다.

Result: 회귀 보정 메트릭들은 자주 상충하는 결과를 낳으며, 다양한 메트릭들이 동일한 재보정 결과의 평가에서 불일치를 보이는 현상을 확인했습니다.

Conclusion: 측정 항목 선택이 보정 연구에서 중요한 역할을 한다는 것을 강조하며, 신뢰할 수 있는 메트릭으로 Expected Normalized Calibration Error (ENCE)와 Coverage Width-based Criterion (CWC)를 식별했습니다.

Abstract: In safety-critical applications data-driven models must not only be accurate
but also provide reliable uncertainty estimates. This property, commonly
referred to as calibration, is essential for risk-aware decision-making. In
regression a wide variety of calibration metrics and recalibration methods have
emerged. However, these metrics differ significantly in their definitions,
assumptions and scales, making it difficult to interpret and compare results
across studies. Moreover, most recalibration methods have been evaluated using
only a small subset of metrics, leaving it unclear whether improvements
generalize across different notions of calibration. In this work, we
systematically extract and categorize regression calibration metrics from the
literature and benchmark these metrics independently of specific modelling
methods or recalibration approaches. Through controlled experiments with
real-world, synthetic and artificially miscalibrated data, we demonstrate that
calibration metrics frequently produce conflicting results. Our analysis
reveals substantial inconsistencies: many metrics disagree in their evaluation
of the same recalibration result, and some even indicate contradictory
conclusions. This inconsistency is particularly concerning as it potentially
allows cherry-picking of metrics to create misleading impressions of success.
We identify the Expected Normalized Calibration Error (ENCE) and the Coverage
Width-based Criterion (CWC) as the most dependable metrics in our tests. Our
findings highlight the critical role of metric selection in calibration
research.

</details>


### [169] [Puzzle: Scheduling Multiple Deep Learning Models on Mobile Device with Heterogeneous Processors](https://arxiv.org/abs/2508.17764)
*Duseok Kang,Yunseong Lee,Junghoon Kim*

Main category: cs.LG

TL;DR: 본 연구에서는 이종 프로세서에서 다수의 딥 러닝 네트워크를 효율적으로 스케줄링하기 위한 새로운 유전자 알고리즘 기반 방법론을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 장치에서 딥 러닝 모델의 수요 증가에 따라 하드웨어 이질성이 증가하고 있지만, 기존의 스케줄링 연구는 이러한 이질성을 충분히 반영하지 못하고 있습니다.

Method: 우리는 네트워크를 여러 서브그래프로 나누고 세 가지 유형의 염색체를 사용하여 스케줄링을 최적화하는 유전자 알고리즘을 구현했습니다.

Result: 우리의 시스템 Puzzle은 아홉 개의 최첨단 네트워크를 포함한 평가에서 기존 두 가지 휴리스틱 기법에 비해 평균적으로 3.7배 및 2.2배 더 높은 요청 빈도를 지원하는 결과를 보였습니다.

Conclusion: 이러한 결과는 Puzzle이 실시간 요구 사항을 충족하면서 효율적으로 작업을 처리할 수 있음을 보여줍니다.

Abstract: As deep learning models are increasingly deployed on mobile devices, modern
mobile devices incorporate deep learning-specific accelerators to handle the
growing computational demands, thus increasing their hardware heterogeneity.
However, existing works on scheduling deep learning workloads across these
processors have significant limitations: most studies focus on single-model
scenarios rather than realistic multi-model scenarios, overlook performance
variations from different hardware/software configurations, and struggle with
accurate execution time estimation. To address these challenges, we propose a
novel genetic algorithm-based methodology for scheduling multiple deep learning
networks on heterogeneous processors by partitioning the networks into multiple
subgraphs. Our approach incorporates three different types of chromosomes for
partition/mapping/priority exploration, and leverages device-in-the-loop
profiling and evaluation for accurate execution time estimation. Based on this
methodology, our system, Puzzle, demonstrates superior performance in extensive
evaluations with randomly generated scenarios involving nine state-of-the-art
networks. The results demonstrate Puzzle can support 3.7 and 2.2 times higher
request frequency on average compared to the two heuristic baselines, NPU Only
and Best Mapping, respectively, while satisfying the equivalent level of
real-time requirements.

</details>


### [170] [Multi-domain Distribution Learning for De Novo Drug Design](https://arxiv.org/abs/2508.17815)
*Arne Schneuing,Ilia Igashov,Adrian W. Dobbelstein,Thomas Castiglione,Michael Bronstein,Bruno Correia*

Main category: cs.LG

TL;DR: DrugFlow는 구조 기반 약물 설계를 위한 생성 모델로, 연속 흐름 정합과 이산 마르코프 브릿지를 통합하여 3차원 단백질-리간드 데이터의 화학적, 기하학적 및 물리적 측면을 학습하는 데 있어 최첨단 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 약물 설계에서 고급 학습 성능과 불확실성 추정을 통해 혁신적인 접근 방식을 찾기 위해.

Method: DrugFlow 모델은 연속 흐름 정합과 이산 마르코프 브릿지를 결합하며, 불확실성 추정을 통해 분포 외 샘플을 감지할 수 있다. 또한, 측면 사슬 각도와 분자를 공동 샘플링하여 단백질의 변형 가능성을 탐색하는 방법을 제안한다.

Result: DrugFlow는 3차원 단백질-리간드 데이터에서 화학적, 기하학적 및 물리적 특성을 학습하는 데 있어 우수한 성능을 발휘한다.

Conclusion: 이 모델은 약물 설계 및 단백질의 구조적 복잡성을 이해하는 데 있어 새로운 가능성을 제공하며, 향후 연구에서도 활용될 수 있다.

Abstract: We introduce DrugFlow, a generative model for structure-based drug design
that integrates continuous flow matching with discrete Markov bridges,
demonstrating state-of-the-art performance in learning chemical, geometric, and
physical aspects of three-dimensional protein-ligand data. We endow DrugFlow
with an uncertainty estimate that is able to detect out-of-distribution
samples. To further enhance the sampling process towards distribution regions
with desirable metric values, we propose a joint preference alignment scheme
applicable to both flow matching and Markov bridge frameworks. Furthermore, we
extend our model to also explore the conformational landscape of the protein by
jointly sampling side chain angles and molecules.

</details>


### [171] [Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering](https://arxiv.org/abs/2508.17872)
*Yanghao Qin,Bo Zhou,Guangliang Pan,Qihui Wu,Meixia Tao*

Main category: cs.LG

TL;DR: SFFP 프레임워크를 통해 스펙트럼 데이터를 효과적으로 예측하고 자원을 할당하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 정확한 스펙트럼 예측은 동적 스펙트럼 접근 및 자원 할당에 필수적입니다.

Method: SFFP는 적응형 분수 푸리에 변환 모듈과 적응형 필터 모듈을 사용하여 스펙트럼 데이터를 변환하고 필터링합니다.

Result: 실제 스펙트럼 데이터 실험에서 SFFP가 기존의 예측 방법보다 우수한 성능을 보였습니다.

Conclusion: SFFP는 스펙트럼 데이터의 예측 가능성을 향상시키는 효과적인 방법입니다.

Abstract: Accurate spectrum prediction is crucial for dynamic spectrum access (DSA) and
resource allocation. However, due to the unique characteristics of spectrum
data, existing methods based on the time or frequency domain often struggle to
separate predictable patterns from noise. To address this, we propose the
Spectral Fractional Filtering and Prediction (SFFP) framework. SFFP first
employs an adaptive fractional Fourier transform (FrFT) module to transform
spectrum data into a suitable fractional Fourier domain, enhancing the
separability of predictable trends from noise. Subsequently, an adaptive Filter
module selectively suppresses noise while preserving critical predictive
features within this domain. Finally, a prediction module, leveraging a
complex-valued neural network, learns and forecasts these filtered trend
components. Experiments on real-world spectrum data show that the SFFP
outperforms leading spectrum and general forecasting methods.

</details>


### [172] [Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets](https://arxiv.org/abs/2508.17930)
*Sarina Penquitt,Tobias Riedlinger,Timo Heller,Markus Reischl,Matthias Rottmann*

Main category: cs.LG

TL;DR: 본 연구는 객체 탐지, 의미론적 분할 및 인스턴스 분할 데이터셋에서의 라벨 오류 탐지에 대한 통합 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 라벨 오류 탐지와 라벨 품질 개선은 감독 학습 작업에서 중요한 목표이다.

Method: 라벨 오류를 ground truth에 주입한 후, 이를 인스턴스 분할 문제로 구성하여 라벨 오류를 탐지한다.

Result: 실험을 통해 다양한 기초 및 최신 방법과 비교하여 라벨 오류 탐지 성능을 평가했다.

Conclusion: Cityscapes 데이터셋에서 확인된 459개의 실제 라벨 오류를 발표하고, Cityscapes에서의 실제 라벨 오류 탐지를 위한 벤치마크를 제공한다.

Abstract: Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.

</details>


### [173] [Choice Outweighs Effort: Facilitating Complementary Knowledge Fusion in Federated Learning via Re-calibration and Merit-discrimination](https://arxiv.org/abs/2508.17954)
*Ming Yang,Dongrun Li,Xin Wang,Xiaoyang Yu,Xiaoming Wu,Shibo He*

Main category: cs.LG

TL;DR: FedMate는 연합 학습에서의 데이터 이질성 문제를 해결하기 위한 양면 최적화를 구현하는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 클라이언트 간 데이터 이질성으로 인해 공정한 합의 형성과 일반화 및 개인화 지식의 보완적 융합을 방해하는 편향이 발생한다.

Method: FedMate는 서버 측에서 동적 글로벌 프로토타입을 구성하고, 클라이언트 측에서는 보완적 분류 융합 및 비용 인식 기능 전송을 도입하여 최적화를 진행한다.

Result: 다섯 가지 복잡도가 다른 데이터세트의 실험 결과, FedMate가 일반화와 적응성을 조화롭게 하는 데 있어 최신 방법들을 능가하는 성과를 보였다.

Conclusion: 자동주행 데이터셋에 대한 의미적 분할 실험은 이 방법의 현실 세계에서의 확장성을 검증한다.

Abstract: Cross-client data heterogeneity in federated learning induces biases that
impede unbiased consensus condensation and the complementary fusion of
generalization- and personalization-oriented knowledge. While existing
approaches mitigate heterogeneity through model decoupling and representation
center loss, they often rely on static and restricted metrics to evaluate local
knowledge and adopt global alignment too rigidly, leading to consensus
distortion and diminished model adaptability. To address these limitations, we
propose FedMate, a method that implements bilateral optimization: On the server
side, we construct a dynamic global prototype, with aggregation weights
calibrated by holistic integration of sample size, current parameters, and
future prediction; a category-wise classifier is then fine-tuned using this
prototype to preserve global consistency. On the client side, we introduce
complementary classification fusion to enable merit-based discrimination
training and incorporate cost-aware feature transmission to balance model
performance and communication efficiency. Experiments on five datasets of
varying complexity demonstrate that FedMate outperforms state-of-the-art
methods in harmonizing generalization and adaptation. Additionally, semantic
segmentation experiments on autonomous driving datasets validate the method's
real-world scalability.

</details>


### [174] [Generative Feature Imputing - A Technique for Error-resilient Semantic Communication](https://arxiv.org/abs/2508.17957)
*Jianhao Huang,Qunsong Zeng,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 이 논문은 6G 네트워크에서 인공지능을 활용하여 의미 기반 통신의 효율성을 높이는 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 디지털 시스템에서 의미 기반 통신을 구현하기 위한 전송 오류에 대한 강건성을 보장하는 것이 중요하다.

Method: 이 논문은 공간적 오류 농도 패킷화 전략, 생성적 특징 입력 방법 및 의미 인식 전송 전력 할당 방식을 포함하는 새롭고 독창적인 프레임워크를 제안한다.

Result: 제안된 프레임워크는 기존 방법들보다 높은 의미적 정확성과 낮은 LPIPS 점수를 달성한다.

Conclusion: 제안된 프레임워크는 블록 페이딩 조건에서 기존의 접근 방식보다 성능이 우수하다.

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for
achieving unprecedented communication efficiency in sixth-generation (6G)
networks by leveraging artificial intelligence (AI) to extract and transmit the
underlying meanings of source data. However, deploying SemCom over digital
systems presents new challenges, particularly in ensuring robustness against
transmission errors that may distort semantically critical content. To address
this issue, this paper proposes a novel framework, termed generative feature
imputing, which comprises three key techniques. First, we introduce a spatial
error concentration packetization strategy that spatially concentrates feature
distortions by encoding feature elements based on their channel mappings, a
property crucial for both the effectiveness and reduced complexity of the
subsequent techniques. Second, building on this strategy, we propose a
generative feature imputing method that utilizes a diffusion model to
efficiently reconstruct missing features caused by packet losses. Finally, we
develop a semantic-aware power allocation scheme that enables unequal error
protection by allocating transmission power according to the semantic
importance of each packet. Experimental results demonstrate that the proposed
framework outperforms conventional approaches, such as Deep Joint
Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,
achieving higher semantic accuracy and lower Learned Perceptual Image Patch
Similarity (LPIPS) scores.

</details>


### [175] [Topology Aware Neural Interpolation of Scalar Fields](https://arxiv.org/abs/2508.17995)
*Mohamed Kissi,Keanu Sisouk,Joshua A. Levine,Julien Tierny*

Main category: cs.LG

TL;DR: 이 논문은 시간에 따라 변화하는 스칼라 필드의 위상 인식 보간을 위한 신경망 구조를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 시간 변화에 대한 스칼라 필드의 보간을 더 정확하고 효과적으로 수행하기 위해 제안되었다.

Method: 키프레임 샘플링을 기반으로 시간 값과 스칼라 필드 간의 관계를 학습하는 신경망 아키텍처를 사용한다.

Result: 실험 결과 2D 및 3D 시간 변화 데이터 세트에 대한 보간에서 기존 보간 기법보다 우수한 성능을 보였다.

Conclusion: 설계된 신경망은 비키프레임 데이터에 대한 위상적 재구성을 개선하는 데 기여했다.

Abstract: This paper presents a neural scheme for the topology-aware interpolation of
time-varying scalar fields. Given a time-varying sequence of persistence
diagrams, along with a sparse temporal sampling of the corresponding scalar
fields, denoted as keyframes, our interpolation approach aims at "inverting"
the non-keyframe diagrams to produce plausible estimations of the
corresponding, missing data. For this, we rely on a neural architecture which
learns the relation from a time value to the corresponding scalar field, based
on the keyframe examples, and reliably extends this relation to the
non-keyframe time steps. We show how augmenting this architecture with specific
topological losses exploiting the input diagrams both improves the geometrical
and topological reconstruction of the non-keyframe time steps. At query time,
given an input time value for which an interpolation is desired, our approach
instantaneously produces an output, via a single propagation of the time input
through the network. Experiments interpolating 2D and 3D time-varying datasets
show our approach superiority, both in terms of data and topological fitting,
with regard to reference interpolation schemes.

</details>


### [176] [A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond](https://arxiv.org/abs/2508.18001)
*Sebastian G. Gruber*

Main category: cs.LG

TL;DR: 본 연구는 기계 학습에서 불확실성 정량화를 위한 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 있고 안정적인 기계 학습 응용 분야에서 불확실성 정량화는 중요한 기초 요소이다.

Method: 우리는 엄밀한 적정 점수를 통한 일반적인 편향-분산 분해를 도입하여 이와 관련된 여러 이론적 결과를 제시한다.

Result: 각기 다른 도메인에서의 샘플 기반 생성 모델 평가 시 최첨단 기준을 초과하는 결과를 달성하였다.

Conclusion: 우리는 새로운 적정 교정 오류 추정기 및 다양한 추정기를 비교하는 위험 기반 접근법을 제안하며, 생성 이미지 모델에 대한 보다 세밀하고 해석 가능한 평가를 가능하게 하는 커널 구형 점수의 분해도 제시한다.

Abstract: In this PhD thesis, we propose a novel framework for uncertainty
quantification in machine learning, which is based on proper scores.
Uncertainty quantification is an important cornerstone for trustworthy and
reliable machine learning applications in practice. Usually, approaches to
uncertainty quantification are problem-specific, and solutions and insights
cannot be readily transferred from one task to another. Proper scores are loss
functions minimized by predicting the target distribution. Due to their very
general definition, proper scores apply to regression, classification, or even
generative modeling tasks. We contribute several theoretical results, that
connect epistemic uncertainty, aleatoric uncertainty, and model calibration
with proper scores, resulting in a general and widely applicable framework. We
achieve this by introducing a general bias-variance decomposition for strictly
proper scores via functional Bregman divergences. Specifically, we use the
kernel score, a kernel-based proper score, for evaluating sample-based
generative models in various domains, like image, audio, and natural language
generation. This includes a novel approach for uncertainty estimation of large
language models, which outperforms state-of-the-art baselines. Further, we
generalize the calibration-sharpness decomposition beyond classification, which
motivates the definition of proper calibration errors. We then introduce a
novel estimator for proper calibration errors in classification, and a novel
risk-based approach to compare different estimators for squared calibration
errors. Last, we offer a decomposition of the kernel spherical score, another
kernel-based proper score, allowing a more fine-grained and interpretable
evaluation of generative image models.

</details>


### [177] [Does simple trump complex? Comparing strategies for adversarial robustness in DNNs](https://arxiv.org/abs/2508.18019)
*William Brooks,Marelie H. Davel,Coenraad Mouton*

Main category: cs.LG

TL;DR: 이 연구는 두 가지 적대적 훈련 기법의 구성 요소를 분석하여 적대적 강인성을 높이는 데 기여하는 요소를 규명한다.


<details>
  <summary>Details</summary>
Motivation: DNN이 다양한 응용 프로그램에서 성공을 거두었지만 적대적 공격에 취약하기 때문에, 본 연구는 적대적 강인성을 향상시키기 위해 기여하는 훈련 기법의 구성 요소를 규명하고 분리하는 것을 목표로 한다.

Method: 두 가지 방법을 비교하여 마진을 극대화한다. 하나는 손실 함수를 수정하여 마진의 근사를 증가시키는 간단한 접근 방식이고, 다른 하나는 최신 방법인 다이나믹스 인지 강인 훈련이다.

Result: VGG-16 모델을 기반으로 여러 구성 요소의 상대적 영향을 평가함으로써, 각각의 구성 요소가 모델 성능에 미치는 영향을 분석한다.

Conclusion: CIFAR-10 데이터 세트에 대한 분석을 통해 적대적 강인성을 가장 효과적으로 향상시키는 요소를 밝혀내어 보다 강력한 DNN을 설계하는 데 대한 통찰을 제공한다.

Abstract: Deep Neural Networks (DNNs) have shown substantial success in various
applications but remain vulnerable to adversarial attacks. This study aims to
identify and isolate the components of two different adversarial training
techniques that contribute most to increased adversarial robustness,
particularly through the lens of margins in the input space -- the minimal
distance between data points and decision boundaries. Specifically, we compare
two methods that maximize margins: a simple approach which modifies the loss
function to increase an approximation of the margin, and a more complex
state-of-the-art method (Dynamics-Aware Robust Training) which builds upon this
approach. Using a VGG-16 model as our base, we systematically isolate and
evaluate individual components from these methods to determine their relative
impact on adversarial robustness. We assess the effect of each component on the
model's performance under various adversarial attacks, including AutoAttack and
Projected Gradient Descent (PGD). Our analysis on the CIFAR-10 dataset reveals
which elements most effectively enhance adversarial robustness, providing
insights for designing more robust DNNs.

</details>


### [178] [Enhancing Differentially Private Linear Regression via Public Second-Moment](https://arxiv.org/abs/2508.18037)
*Zilong Cao,Hai Zhang*

Main category: cs.LG

TL;DR: 이 논문은 비공개 데이터에 의존하지 않고 공공 데이터를 활용하여 선형 회귀의 일반 최소 제곱 추정기(OLSE)의 유용성을 개선하는 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 차분 개인 정보 보호(DP) 방법은 종종 개인 데이터를 기반으로만 노이즈를 추가해야 하며, 이는 유용성을 크게 저하시킬 수 있습니다.

Method: 공공 제이순 행렬을 사용하여 개인 데이터를 변환하고, 이를 통해 변환된 SSP-OLSE를 계산하는 새로운 방법을 제안합니다.

Result: 이 방법은 더 나은 조건 수와 OLSE의 정확성 및 강건성을 향상시킵니다.

Conclusion: 우리의 접근법이 얻는 개선된 강건성과 정확성을 이론적으로 증명하고, 합성 및 실제 데이터셋에서 유용성과 효율성을 입증했습니다.

Abstract: Leveraging information from public data has become increasingly crucial in
enhancing the utility of differentially private (DP) methods. Traditional DP
approaches often require adding noise based solely on private data, which can
significantly degrade utility. In this paper, we address this limitation in the
context of the ordinary least squares estimator (OLSE) of linear regression
based on sufficient statistics perturbation (SSP) under the unbounded data
assumption. We propose a novel method that involves transforming private data
using the public second-moment matrix to compute a transformed SSP-OLSE, whose
second-moment matrix yields a better condition number and improves the OLSE
accuracy and robustness. We derive theoretical error bounds about our method
and the standard SSP-OLSE to the non-DP OLSE, which reveal the improved
robustness and accuracy achieved by our approach. Experiments on synthetic and
real-world datasets demonstrate the utility and effectiveness of our method.

</details>


### [179] [Riemannian Change Point Detection on Manifolds with Robust Centroid Estimation](https://arxiv.org/abs/2508.18045)
*Xiuheng Wang,Ricardo Borsoi,Arnaud Breloy,Cédric Richard*

Main category: cs.LG

TL;DR: 본 논문에서는 시계열 데이터의 비모수적 변화점 탐지를 다루며, 강건 중심을 이용하여 무게 중심 업데이트 시의 스텝 크기 조정 문제를 해결하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 시계열 데이터에서 비모수적 변화점 탐지는 신호 처리에서 오랜 과제이며, 최근 통계 및 머신러닝의 발전이 이에 대응하고 있다.

Method: M-추정 이론의 강건 중심을 활용하여 변화에 민감한 고전적인 Karcher 평균과 Huber 함수에서 정의된 강건 평균을 비교하여 테스트 통계량을 정의한다.

Result: 시뮬레이션 데이터와 실제 데이터에서 두 가지 대표적인 다양체를 대상으로 한 실험에서 제안한 방법의 우수한 성능을 입증하였다.

Conclusion: 제안된 방법을 통해 변화점 탐지의 효율성을 높일 수 있으며, 강건한 중심을 추정하는 데 있어 효율적인 확률론적 리만 최적화 알고리즘을 제공한다.

Abstract: Non-parametric change-point detection in streaming time series data is a
long-standing challenge in signal processing. Recent advancements in statistics
and machine learning have increasingly addressed this problem for data residing
on Riemannian manifolds. One prominent strategy involves monitoring abrupt
changes in the center of mass of the time series. Implemented in a streaming
fashion, this strategy, however, requires careful step size tuning when
computing the updates of the center of mass. In this paper, we propose to
leverage robust centroid on manifolds from M-estimation theory to address this
issue. Our proposal consists of comparing two centroid estimates: the classical
Karcher mean (sensitive to change) versus one defined from Huber's function
(robust to change). This comparison leads to the definition of a test statistic
whose performance is less sensitive to the underlying estimation method. We
propose a stochastic Riemannian optimization algorithm to estimate both robust
centroids efficiently. Experiments conducted on both simulated and real-world
data across two representative manifolds demonstrate the superior performance
of our proposed method.

</details>


### [180] [Training Transformers for Mesh-Based Simulations](https://arxiv.org/abs/2508.18051)
*Paul Garnier,Vincent Lannelongue,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: 이 논문에서는 그래프 변환기 아키텍처를 제안하여 복잡한 메쉬에서의 효율성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기존 GNN 기반 물리 시뮬레이션 모델은 대규모 메쉬 처리에서 비효율적이다.

Method: 인접 행렬을 주의 마스크로 활용한 그래프 변환기 아키텍처와 새로운 보강 기법을 사용한다.

Result: 300k 노드와 300만 엣지를 가진 메쉬에서 우수한 성능을 입증했다.

Conclusion: 제안된 모델은 이전의 최첨단 모델보다 향상된 성능과 확장성을 보였다.

Abstract: Simulating physics using Graph Neural Networks (GNNs) is predominantly driven
by message-passing architectures, which face challenges in scaling and
efficiency, particularly in handling large, complex meshes. These architectures
have inspired numerous enhancements, including multigrid approaches and $K$-hop
aggregation (using neighbours of distance $K$), yet they often introduce
significant complexity and suffer from limited in-depth investigations. In
response to these challenges, we propose a novel Graph Transformer architecture
that leverages the adjacency matrix as an attention mask. The proposed approach
incorporates innovative augmentations, including Dilated Sliding Windows and
Global Attention, to extend receptive fields without sacrificing computational
efficiency. Through extensive experimentation, we evaluate model size,
adjacency matrix augmentations, positional encoding and $K$-hop configurations
using challenging 3D computational fluid dynamics (CFD) datasets. We also train
over 60 models to find a scaling law between training FLOPs and parameters. The
introduced models demonstrate remarkable scalability, performing on meshes with
up to 300k nodes and 3 million edges. Notably, the smallest model achieves
parity with MeshGraphNet while being $7\times$ faster and $6\times$ smaller.
The largest model surpasses the previous state-of-the-art by $38.8$\% on
average and outperforms MeshGraphNet by $52$\% on the all-rollout RMSE, while
having a similar training speed. Code and datasets are available at
https://github.com/DonsetPG/graph-physics.

</details>


### [181] [Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks](https://arxiv.org/abs/2508.18052)
*Silvia Beddar-Wiesing,Alice Moallemy-Oureh*

Main category: cs.LG

TL;DR: 본 논문은 속성 연속 시간 동적 그래프에 대한 이론을 확장하고, 지속적으로 동적 레벨을 테스트하여 쪼개진 비연결 컴포넌트를 처리하는 CGNN 구조를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 GNNs의 한계를 극복하고 비연결 컴포넌트를 처리할 수 있는 이론적 토대를 마련하기 위해.

Method: 속성 연속 시간 동적 1-WL 테스트를 도입하고, 이를 연속 동적 언폴딩 트리에 대한 동등성을 입증하며, 비연결 그래프를 처리할 수 있는 CGNN 클래스를 식별합니다.

Result: 비연결 컴포넌트와 비동기적으로 진화하는 실제 시스템을 처리할 수 있는 CGNN 아키텍처를 통해 GNNs의 구별 능력과 보편적인 근사 보장을 유지합니다.

Conclusion: 본 연구는 비연결 그래프를 처리하기 위한 컴팩트하고 표현력이 뛰어난 CGNN 설계 가이드라인을 제공합니다.

Abstract: Graph Neural Networks (GNNs) are known to match the distinguishing power of
the 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide with
the unfolding tree equivalence classes of graphs. Preserving this equivalence,
GNNs can universally approximate any target function on graphs in probability
up to any precision. However, these results are limited to attributed
discrete-dynamic graphs represented as sequences of connected graph snapshots.
Real-world systems, such as communication networks, financial transaction
networks, and molecular interactions, evolve asynchronously and may split into
disconnected components. In this paper, we extend the theory of attributed
discrete-dynamic graphs to attributed continuous-time dynamic graphs with
arbitrary connectivity. To this end, we introduce a continuous-time dynamic
1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,
and identify a class of continuous-time dynamic GNNs (CGNNs) based on
discrete-dynamic GNN architectures that retain both distinguishing power and
universal approximation guarantees. Our constructive proofs further yield
practical design guidelines, emphasizing a compact and expressive CGNN
architecture with piece-wise continuously differentiable temporal functions to
process asynchronous, disconnected graphs.

</details>


### [182] [FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning](https://arxiv.org/abs/2508.18060)
*Emmanouil Kritharakis,Antonios Makris,Dusan Jakovetic,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: 이번 연구에서는 중앙 서버가 신뢰할 수 있는 환경에서 적대적인 클라이언트가 존재할 수 있는 연합 학습을 위한 전략인 FedGreed를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 데이터 프라이버시를 유지하면서도 적대적인 클라이언트로부터의 공격에 저항할 수 있는 방법이 필요합니다.

Method: FedGreed는 클라이언트의 손실 메트릭을 평가하여 모델 업데이트를 정렬하고, 최소 손실을 보이는 클라이언트의 하위 집합을 선택하는 전략입니다.

Result: FedGreed는 MNIST, FMNIST, CIFAR-10 데이터셋에서 기존의 연합 학습 기초선보다 우수한 성능을 보였습니다.

Conclusion: 본 연구는 강한 적대적 행동에서도 수렴 보장과 최적성 격차를 유지하는 FedGreed의 효용성을 입증했습니다.

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients while preserving data privacy by keeping local datasets on-device. In
this work, we address FL settings where clients may behave adversarially,
exhibiting Byzantine attacks, while the central server is trusted and equipped
with a reference dataset. We propose FedGreed, a resilient aggregation strategy
for federated learning that does not require any assumptions about the fraction
of adversarial participants. FedGreed orders clients' local model updates based
on their loss metrics evaluated against a trusted dataset on the server and
greedily selects a subset of clients whose models exhibit the minimal
evaluation loss. Unlike many existing approaches, our method is designed to
operate reliably under heterogeneous (non-IID) data distributions, which are
prevalent in real-world deployments. FedGreed exhibits convergence guarantees
and bounded optimality gaps under strong adversarial behavior. Experimental
evaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our method
significantly outperforms standard and robust federated learning baselines,
such as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority of
adversarial scenarios considered, including label flipping and Gaussian noise
injection attacks. All experiments were conducted using the Flower federated
learning framework.

</details>


### [183] [Quantum-Classical Hybrid Framework for Zero-Day Time-Push GNSS Spoofing Detection](https://arxiv.org/abs/2508.18085)
*Abyad Enan,Mashrur Chowdhury,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 이 논문은 기존의 스푸핑 공격 탐지 방법의 한계를 극복하기 위해 Hybrid Quantum-Classical Autoencoder를 사용한 제로데이 스푸핑 탐지 방법을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: GNSS는 PNT 애플리케이션에 필수적이지만 스푸핑 공격에 쉽게 노출된다.

Method: 진짜 GNSS 신호로만 훈련된 Hybrid Quantum-Classical Autoencoder를 사용하여 스푸핑 탐지를 수행한다.

Result: HQC-AE는 다양한 스푸핑 공격 시나리오에서 기존 모델보다 우수한 탐지 성능을 보였다.

Conclusion: 이 연구는 HQC-AE 방법이 정적인 GNSS 수신기에서 제로데이 스푸핑 공격 탐지에 효과적임을 보여주었다.

Abstract: Global Navigation Satellite Systems (GNSS) are critical for Positioning,
Navigation, and Timing (PNT) applications. However, GNSS are highly vulnerable
to spoofing attacks, where adversaries transmit counterfeit signals to mislead
receivers. Such attacks can lead to severe consequences, including misdirected
navigation, compromised data integrity, and operational disruptions. Most
existing spoofing detection methods depend on supervised learning techniques
and struggle to detect novel, evolved, and unseen attacks. To overcome this
limitation, we develop a zero-day spoofing detection method using a Hybrid
Quantum-Classical Autoencoder (HQC-AE), trained solely on authentic GNSS
signals without exposure to spoofed data. By leveraging features extracted
during the tracking stage, our method enables proactive detection before PNT
solutions are computed. We focus on spoofing detection in static GNSS
receivers, which are particularly susceptible to time-push spoofing attacks,
where attackers manipulate timing information to induce incorrect time
computations at the receiver. We evaluate our model against different unseen
time-push spoofing attack scenarios: simplistic, intermediate, and
sophisticated. Our analysis demonstrates that the HQC-AE consistently
outperforms its classical counterpart, traditional supervised learning-based
models, and existing unsupervised learning-based methods in detecting zero-day,
unseen GNSS time-push spoofing attacks, achieving an average detection accuracy
of 97.71% with an average false negative rate of 0.62% (when an attack occurs
but is not detected). For sophisticated spoofing attacks, the HQC-AE attains an
accuracy of 98.23% with a false negative rate of 1.85%. These findings
highlight the effectiveness of our method in proactively detecting zero-day
GNSS time-push spoofing attacks across various stationary GNSS receiver
platforms.

</details>


### [184] [Provable Mixed-Noise Learning with Flow-Matching](https://arxiv.org/abs/2508.18122)
*Paul Hagemann,Robert Gruhlke,Bernhard Stankewitz,Claudia Schillings,Gabriele Steidl*

Main category: cs.LG

TL;DR: 본 연구에서는 혼합 잡음을 가진 베이지안 역문제를 다루며, 이는 가법 및 곱법 가우시안 성분의 조합으로 모델링됩니다. 새로운 추론 프레임워크를 제안하고 EM 알고리즘 내에 조건부 흐름 매칭을 포함하여 후방 샘플러와 잡음 매개변수를 공동으로 추정합니다.


<details>
  <summary>Details</summary>
Motivation: 실제 물리 및 화학 응용에서는 종종 잡음 구조가 알려져 있지 않고 이질적인 경우가 많습니다.

Method: 조건부 흐름 매칭을 활용한 EM 알고리즘 내의 새로운 추론 프레임워크를 제안합니다. EM 알고리즘의 E-단계에서 생성 모델로 ODE 기반 흐름 매칭을 사용합니다.

Result: EM 업데이트가 무한한 관측의 모집단 한계에서 진정한 잡음 매개변수로 수렴함을 증명했습니다.

Conclusion: 혼합 잡음 베이지안 역문제에 대한 EM 추론과 흐름 매칭의 결합의 효과성을 수치적으로 설명합니다.

Abstract: We study Bayesian inverse problems with mixed noise, modeled as a combination
of additive and multiplicative Gaussian components. While traditional inference
methods often assume fixed or known noise characteristics, real-world
applications, particularly in physics and chemistry, frequently involve noise
with unknown and heterogeneous structure. Motivated by recent advances in
flow-based generative modeling, we propose a novel inference framework based on
conditional flow matching embedded within an Expectation-Maximization (EM)
algorithm to jointly estimate posterior samplers and noise parameters. To
enable high-dimensional inference and improve scalability, we use
simulation-free ODE-based flow matching as the generative model in the E-step
of the EM algorithm. We prove that, under suitable assumptions, the EM updates
converge to the true noise parameters in the population limit of infinite
observations. Our numerical results illustrate the effectiveness of combining
EM inference with flow matching for mixed-noise Bayesian inverse problems.

</details>


### [185] [Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics](https://arxiv.org/abs/2508.18130)
*Pradeep Singh,Mehak Sharma,Anupriya Dey,Balasubramanian Raman*

Main category: cs.LG

TL;DR: FreezeTST는 메모리를 효율적으로 활용하여 긴 범위 예측 시의 비용과 취약성을 줄이는 경량 하이브리드 모델이다.


<details>
  <summary>Details</summary>
Motivation: Transformer는 시퀀스 모델링의 정석이지만, 긴 범위 예측에서 자원의 비효율이 문제로 지적되고 있다.

Method: FreezeTST는 냉동 랜덤 기능 블록과 표준 학습 가능한 Transformer 레이어를 교차 배치한 모델이다.

Result: FreezeTST는 일곱 개의 표준 장기 예측 벤치마크에서 Informer, Autoformer, PatchTST와 같은 전문화된 변형과 일관되게 일치하거나 이를 초과하였다.

Conclusion: 이 연구는 Transformer 내에 저수 원리를 내장함으로써 장기 시계열 예측을 위한 효과적인 경로를 제공함을 보여준다.

Abstract: Transformers are the de-facto choice for sequence modelling, yet their
quadratic self-attention and weak temporal bias can make long-range forecasting
both expensive and brittle. We introduce FreezeTST, a lightweight hybrid that
interleaves frozen random-feature (reservoir) blocks with standard trainable
Transformer layers. The frozen blocks endow the network with rich nonlinear
memory at no optimisation cost; the trainable layers learn to query this memory
through self-attention. The design cuts trainable parameters and also lowers
wall-clock training time, while leaving inference complexity unchanged. On
seven standard long-term forecasting benchmarks, FreezeTST consistently matches
or surpasses specialised variants such as Informer, Autoformer, and PatchTST;
with substantially lower compute. Our results show that embedding reservoir
principles within Transformers offers a simple, principled route to efficient
long-term time-series prediction.

</details>


### [186] [Unveiling the Actual Performance of Neural-based Models for Equation Discovery on Graph Dynamical Systems](https://arxiv.org/abs/2508.18173)
*Riccardo Cappi,Paolo Frazzetto,Nicolò Navarin,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 딥러닝 모델의 "블랙박스" 특성은 해석 가능성이 중요한 과학적 발견에서의 채택에 큰 장벽이 된다. 이 연구에서는 동적 과정의 지배 방정식을 발견하기 위한 최첨단 기호 회귀 기술을 분석하고, MLP 기반 아키텍처와 KAN을 포함한 다양한 방법의 성능을 비교하였다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 모델의 해석 가능성 부족으로 과학적 발견에의 사용이 제한되는 문제를 해결하고자 함.

Method: 기존의 희소 회귀 및 MLP 기반 구조를 평가하고, 그래프를 위한 KAN의 새로운 변형을 도입하여 해석 가능성을 활용.

Result: 신Synthetic 및 실제 동적 시스템을 통해 MLP 및 KAN 기반 아키텍처가 기존의 기준을 크게 초월하여 지배 기호 방정식을 성공적으로 식별한다는 결과를 도출.

Conclusion: KAN이 더 큰 투명성과 간결성으로 성능을 달성함을 보여주며, 모델 표현성과 해석 가능성 간의 균형을 명확히 하고 복잡한 시스템에서의 Robust한 과학적 발견을 위한 신경 기반 아키텍처의 실행 가능성을 확립한다.

Abstract: The ``black-box'' nature of deep learning models presents a significant
barrier to their adoption for scientific discovery, where interpretability is
paramount. This challenge is especially pronounced in discovering the governing
equations of dynamical processes on networks or graphs, since even their
topological structure further affects the processes' behavior. This paper
provides a rigorous, comparative assessment of state-of-the-art symbolic
regression techniques for this task. We evaluate established methods, including
sparse regression and MLP-based architectures, and introduce a novel adaptation
of Kolmogorov-Arnold Networks (KANs) for graphs, designed to exploit their
inherent interpretability. Across a suite of synthetic and real-world dynamical
systems, our results demonstrate that both MLP and KAN-based architectures can
successfully identify the underlying symbolic equations, significantly
surpassing existing baselines. Critically, we show that KANs achieve this
performance with greater parsimony and transparency, as their learnable
activation functions provide a clearer mapping to the true physical dynamics.
This study offers a practical guide for researchers, clarifying the trade-offs
between model expressivity and interpretability, and establishes the viability
of neural-based architectures for robust scientific discovery on complex
systems.

</details>


### [187] [HypER: Hyperbolic Echo State Networks for Capturing Stretch-and-Fold Dynamics in Chaotic Flows](https://arxiv.org/abs/2508.18196)
*Pradeep Singh,Sutirtha Ghosh,Ashutosh Kumar,Hrishit B P,Balasubramanian Raman*

Main category: cs.LG

TL;DR: HypER 시스템은 비유클리드 기하학을 활용하여 혼돈 동역학의 예측을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 혼돈 동역학을 예측하는 것은 리아푸노프 시간 이상에서 어려운 도전입니다.

Method: 우리는 모든 연결이 쌍곡선 거리와 함께 기하급수적으로 감소하는 Poincare 볼에서 샘플링된 뉴런으로 구성된 Hyperbolic Embedding Reservoir (HypER)를 도입합니다.

Result: HypER는 Euclidean 및 그래프 구조 ESN 기준선을 초월하여 평균 유효 예측 기간을 지속적으로 연장합니다.

Conclusion: 저희는 HypER의 상태 발산 속도에 관한 하한을 설정하여 리아푸노프 성장과 유사한 결과를 보여줍니다.

Abstract: Forecasting chaotic dynamics beyond a few Lyapunov times is difficult because
infinitesimal errors grow exponentially. Existing Echo State Networks (ESNs)
mitigate this growth but employ reservoirs whose Euclidean geometry is
mismatched to the stretch-and-fold structure of chaos. We introduce the
Hyperbolic Embedding Reservoir (HypER), an ESN whose neurons are sampled in the
Poincare ball and whose connections decay exponentially with hyperbolic
distance. This negative-curvature construction embeds an exponential metric
directly into the latent space, aligning the reservoir's local
expansion-contraction spectrum with the system's Lyapunov directions while
preserving standard ESN features such as sparsity, leaky integration, and
spectral-radius control. Training is limited to a Tikhonov-regularized readout.
On the chaotic Lorenz-63 and Roessler systems, and the hyperchaotic Chen-Ueta
attractor, HypER consistently lengthens the mean valid-prediction horizon
beyond Euclidean and graph-structured ESN baselines, with statistically
significant gains confirmed over 30 independent runs; parallel results on
real-world benchmarks, including heart-rate variability from the Santa Fe and
MIT-BIH datasets and international sunspot numbers, corroborate its advantage.
We further establish a lower bound on the rate of state divergence for HypER,
mirroring Lyapunov growth.

</details>


### [188] [Aligning the Evaluation of Probabilistic Predictions with Downstream Value](https://arxiv.org/abs/2508.18251)
*Novin Shahroudi,Viacheslav Komisarenko,Meelis Kull*

Main category: cs.LG

TL;DR: 예측 품질 평가는 최종 사용 맥락에서 고려될 때 더 의미가 있다. 본 논문은 다운스트림 평가와 정렬된 대리 평가 함수를 학습하는 데이터 기반 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 예측은 궁극적으로 다운스트림 작업에 사용되기 때문에, 예측 품질 평가가 그 용도를 맥락적으로 고려할 때 더 의미가 있다.

Method: 본 논문은 다운스트림 평가에 정렬된 대리 평가 함수를 학습하기 위한 데이터 기반 방법을 제안하며, 적절한 채점 규칙 이론에 기반하여 채점 규칙의 변환을 탐구한다.

Result: 신경망에 의해 매개변수화된 가중치가 다운스트림 작업 성능에 정렬되도록 학습되어 복잡하거나 사전 정의되지 않은 가중치를 가진 작업에 대해 빠르고 확장 가능한 평가 사이클을 가능하게 한다.

Conclusion: 우리는 회귀 작업에 대한 합성 및 실제 데이터 실험을 통해 예측 평가와 다운스트림 유틸리티 간의 간극을 메우는 잠재력을 보여준다.

Abstract: Every prediction is ultimately used in a downstream task. Consequently,
evaluating prediction quality is more meaningful when considered in the context
of its downstream use. Metrics based solely on predictive performance often
diverge from measures of real-world downstream impact. Existing approaches
incorporate the downstream view by relying on multiple task-specific metrics,
which can be burdensome to analyze, or by formulating cost-sensitive
evaluations that require an explicit cost structure, typically assumed to be
known a priori. We frame this mismatch as an evaluation alignment problem and
propose a data-driven method to learn a proxy evaluation function aligned with
the downstream evaluation. Building on the theory of proper scoring rules, we
explore transformations of scoring rules that ensure the preservation of
propriety. Our approach leverages weighted scoring rules parametrized by a
neural network, where weighting is learned to align with the performance in the
downstream task. This enables fast and scalable evaluation cycles across tasks
where the weighting is complex or unknown a priori. We showcase our framework
through synthetic and real-data experiments for regression tasks, demonstrating
its potential to bridge the gap between predictive evaluation and downstream
utility in modular prediction systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [189] [Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications](https://arxiv.org/abs/2508.16681)
*Eric Zhang*

Main category: cs.AI

TL;DR: 이 논문은 임상 적용에서 해석 가능성과 투명성이 중요한 음성 비유창성 탐지를 위한 규칙 기반 시스템에 대한 포괄적인 분석을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 전 세계 인구의 약 1%가 말을 더듬고 있으며, 이는 의사소통 및 삶의 질에 영향을 미칩니다.

Method: 발화 속도 정규화, 다중 수준의 음향 기능 분석 및 계층적 의사 결정 구조를 통합한 향상된 규칙 기반 프레임워크를 제안합니다.

Result: 규칙 기반 시스템은 특히 연장 탐지에서 97-99%의 정확도를 달성하고, 다양한 발화 속도에서 안정적인 성능을 제공합니다.

Conclusion: 신경망 접근법이 비제한된 환경에서 약간 더 높은 정확도를 달성할 수 있지만, 규칙 기반 방법은 의사결정 감시 가능성, 환자 맞춤 조정 및 실시간 피드백이 필수적인 임상 환경에서 독특한 이점을 제공합니다.

Abstract: Stuttering affects approximately 1% of the global population, impacting
communication and quality of life. While recent advances in deep learning have
pushed the boundaries of automatic speech dysfluency detection, rule-based
approaches remain crucial for clinical applications where interpretability and
transparency are paramount. This paper presents a comprehensive analysis of
rule-based stuttering detection systems, synthesizing insights from multiple
corpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced
rule-based framework that incorporates speaking-rate normalization, multi-level
acoustic feature analysis, and hierarchical decision structures. Our approach
achieves competitive performance while maintaining complete
interpretability-critical for clinical adoption. We demonstrate that rule-based
systems excel particularly in prolongation detection (97-99% accuracy) and
provide stable performance across varying speaking rates. Furthermore, we show
how these interpretable models can be integrated with modern machine learning
pipelines as proposal generators or constraint modules, bridging the gap
between traditional speech pathology practices and contemporary AI systems. Our
analysis reveals that while neural approaches may achieve marginally higher
accuracy in unconstrained settings, rule-based methods offer unique advantages
in clinical contexts where decision auditability, patient-specific tuning, and
real-time feedback are essential.

</details>


### [190] [Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018](https://arxiv.org/abs/2508.16747)
*Liu Liu,Rui Dai*

Main category: cs.AI

TL;DR: 학생들의 수학 성과를 형성하는 요인을 이해하는 것은 효과적인 교육 정책을 설계하는 데 중요하다. 본 연구는 PISA 2018 데이터를 기반으로 수학 성취도를 예측하고 10개국(67,329명 학생)에서 주요 예측 변수를 식별하기 위해 설명 가능한 인공지능(XAI) 기법을 적용하였다.


<details>
  <summary>Details</summary>
Motivation: 학생 수학 성과를 형성하는 요인을 이해하여 교육 정책을 효과적으로 설계하는 것이 중요하다.

Method: 학생, 가정, 학교 변수를 사용하여 70%의 데이터를 훈련하고 30%에서 테스트한 후 네 가지 모델(MLR, RF, CATBoost, ANN)을 테스트하였다. 성과를 R^2와 MAE로 평가하였고, 해석 가능성을 위해 특성 중요도, SHAP 값, 의사결정 트리 시각화를 사용하였다.

Result: 비선형 모델, 특히 RF와 ANN이 MLR을 초월하였으며, RF는 정확성과 일반화를 균형 있게 유지하였다. 주요 예측 변수에는 사회경제적 지위, 공부 시간, 교사의 동기 및 학생의 수학에 대한 태도가 포함되었으나, 각국에서의 영향은 달랐다.

Conclusion: 연구 결과, 성취의 비선형적이고 맥락 의존적인 특성을 강조하며, 교육 연구에서 XAI의 가치를 지원한다. 본 연구는 국가 간 패턴을 발견하고, 형평성을 중심으로 한 개혁 및 개인 맞춤형 학습 전략 개발을 지원한다.

Abstract: Understanding the factors that shape students' mathematics performance is
vital for designing effective educational policies. This study applies
explainable artificial intelligence (XAI) techniques to PISA 2018 data to
predict math achievement and identify key predictors across ten countries
(67,329 students). We tested four models: Multiple Linear Regression (MLR),
Random Forest (RF), CATBoost, and Artificial Neural Networks (ANN), using
student, family, and school variables. Models were trained on 70% of the data
(with 5-fold cross-validation) and tested on 30%, stratified by country.
Performance was assessed with R^2 and Mean Absolute Error (MAE). To ensure
interpretability, we used feature importance, SHAP values, and decision tree
visualizations. Non-linear models, especially RF and ANN, outperformed MLR,
with RF balancing accuracy and generalizability. Key predictors included
socio-economic status, study time, teacher motivation, and students' attitudes
toward mathematics, though their impact varied across countries. Visual
diagnostics such as scatterplots of predicted vs actual scores showed RF and
CATBoost aligned closely with actual performance. Findings highlight the
non-linear and context-dependent nature of achievement and the value of XAI in
educational research. This study uncovers cross-national patterns, informs
equity-focused reforms, and supports the development of personalized learning
strategies.

</details>


### [191] [Evaluation and LLM-Guided Learning of ICD Coding Rationales](https://arxiv.org/abs/2508.16777)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Wuraola Oyewusi,Kai Kang,Goran Nenadic*

Main category: cs.AI

TL;DR: 의료 코딩의 자동화는 전자 건강 기록에서 비구조적 텍스트를 국제 질병 분류와 같은 표준화된 코드 시스템에 매핑하는 과정이다. 본 연구는 ICD 코딩에 대한 설명 가능성 평가를 수행하며 법칙성과 타당성 측면에서 설명을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 최근 딥러닝의 발전에도 불구하고, 설명 가능성이 결여되어 모델에 대한 신뢰와 투명성이 저해되었다.

Method: 기존의 설명 가능성을 평가하기 위해 새로운 주석이 달린 데이터셋을 구성하고, LLM 생성 합리성의 질을 향상시키기 위한 새로운 방법론을 제안하였다.

Result: LLM이 생성한 합리성이 인간 전문가의 합리성과 가장 밀접하게 일치하며, 소수의 샷 인적 주석이 생성 프로세스를 더욱 향상시킨다.

Conclusion: 제안된 방법론은 모델이 생성한 합리성의 질을 개선하고, 더 나은 설명 가능성을 제공할 수 있다.

Abstract: Automated clinical coding involves mapping unstructured text from Electronic
Health Records (EHRs) to standardized code systems such as the International
Classification of Diseases (ICD). While recent advances in deep learning have
significantly improved the accuracy and efficiency of ICD coding, the lack of
explainability in these models remains a major limitation, undermining trust
and transparency. Current explorations about explainability largely rely on
attention-based techniques and qualitative assessments by physicians, yet lack
systematic evaluation using consistent criteria on high-quality rationale
datasets, as well as dedicated approaches explicitly trained to generate
rationales for further enhancing explanation. In this work, we conduct a
comprehensive evaluation of the explainability of the rationales for ICD coding
through two key lenses: faithfulness that evaluates how well explanations
reflect the model's actual reasoning and plausibility that measures how
consistent the explanations are with human expert judgment. To facilitate the
evaluation of plausibility, we construct a new rationale-annotated dataset,
offering denser annotations with diverse granularity and aligns better with
current clinical practice, and conduct evaluation across three types of
rationales of ICD coding. Encouraged by the promising plausibility of
LLM-generated rationales for ICD coding, we further propose new rationale
learning methods to improve the quality of model-generated rationales, where
rationales produced by prompting LLMs with/without annotation examples are used
as distant supervision signals. We empirically find that LLM-generated
rationales align most closely with those of human experts. Moreover,
incorporating few-shot human-annotated examples not only further improves
rationale generation but also enhances rationale-learning approaches.

</details>


### [192] [PuzzleJAX: A Benchmark for Reasoning and Learning](https://arxiv.org/abs/2508.16821)
*Sam Earle,Graham Todd,Yuchen Li,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: PuzzleJAX는 GPU 가속 퍼즐 게임 엔진 및 기술 언어를 소개하여 트리 탐색, 강화 학습 및 LLM 추론 능력의 빠른 벤치마킹을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 퍼즐 게임의 디자인을 용이하게 하고, 다양한 게임을 동적으로 컴파일하여 벤치마크할 수 있는 환경을 제공하기 위해.

Method: PuzzleJAX는 DSL을 사용하여 퍼즐 게임을 설계하며, PuzzleScript를 기반으로 한다.

Result: 2013년 이후 전문 디자이너와 일반 창작자가 디자인한 수천 개의 게임 중 수백 개를 검증하여, PuzzleJAX의 작업 공간이 광범위하고 표현력이 풍부함을 입증하였다.

Conclusion: PuzzleJAX는 겉보기에는 간단하지만 파악하기 어려운 복잡한 작업을 수행하도록 자연스럽고 직관적으로 태스크를 표현할 수 있음을 보여준다.

Abstract: We introduce PuzzleJAX, a GPU-accelerated puzzle game engine and description
language designed to support rapid benchmarking of tree search, reinforcement
learning, and LLM reasoning abilities. Unlike existing GPU-accelerated learning
environments that provide hard-coded implementations of fixed sets of games,
PuzzleJAX allows dynamic compilation of any game expressible in its
domain-specific language (DSL). This DSL follows PuzzleScript, which is a
popular and accessible online game engine for designing puzzle games. In this
paper, we validate in PuzzleJAX several hundred of the thousands of games
designed in PuzzleScript by both professional designers and casual creators
since its release in 2013, thereby demonstrating PuzzleJAX's coverage of an
expansive, expressive, and human-relevant space of tasks. By analyzing the
performance of search, learning, and language models on these games, we show
that PuzzleJAX can naturally express tasks that are both simple and intuitive
to understand, yet often deeply challenging to master, requiring a combination
of control, planning, and high-level insight.

</details>


### [193] [Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment](https://arxiv.org/abs/2508.16839)
*Shayan Vassef,Soorya Ram Shimegekar,Abhay Goyal,Koustuv Saha,Pi Zonooz,Navin Kumar*

Main category: cs.AI

TL;DR: 본 논문에서는 한 가지 비전-언어 모델(VLM)을 활용하여 임상 업무 흐름을 보다 효율적으로 만들기 위한 두 가지 해결책을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 임상 업무 흐름은 분산된 스크립트와 작업별 네트워크로 인해 비효율적이며 운영 비용이 증가하고 있습니다.

Method: 비전-언어 모델(VLM)을 사용하여 입장하는 이미지를 적절한 전문 모델로 라우팅하는 것을 포함한 3단계 워크플로우를 구성하고, 특정 전문 분야 데이터셋에서 VLM을 미세 조정합니다.

Result: 위장병학, 혈액학, 안과 및 병리학 분야에서 단일 모델 배치는 전문가 기준에 맞추거나 접근합니다.

Conclusion: 하나의 VLM이 결정하고 수행할 수 있도록 함으로써 데이터 과학자의 작업을 줄이고 모니터링 시간을 단축할 뿐만 아니라 모델 선택의 투명성을 높이며 통합 비용을 낮출 수 있습니다.

Abstract: Clinical workflows are fragmented as a patchwork of scripts and task-specific
networks that often handle triage, task selection, and model deployment. These
pipelines are rarely streamlined for data science pipeline, reducing efficiency
and raising operational costs. Workflows also lack data-driven model
identification (from imaging/tabular inputs) and standardized delivery of model
outputs. In response, we present a practical, healthcare-first framework that
uses a single vision-language model (VLM) in two complementary roles. First
(Solution 1), the VLM acts as an aware model-card matcher that routes an
incoming image to the appropriate specialist model via a three-stage workflow
(modality -> primary abnormality -> model-card id). Checks are provided by (i)
stagewise prompts that allow early exit via None/Normal/Other and (ii) a
stagewise answer selector that arbitrates between the top-2 candidates at each
stage, reducing the chance of an incorrect selection and aligning the workflow
with clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on
specialty-specific datasets ensuring a single model covers multiple downstream
tasks within each specialty, maintaining performance while simplifying
deployment. Across gastroenterology, hematology, ophthalmology, and pathology,
our single-model deployment matches or approaches specialized baselines.
  Compared with pipelines composed of many task-specific agents, this approach
shows that one VLM can both decide and do. It may reduce effort by data
scientists, shorten monitoring, increase the transparency of model selection
(with per-stage justifications), and lower integration overhead.

</details>


### [194] [Evolving Collective Cognition in Human-Agent Hybrid Societies: How Agents Form Stances and Boundaries](https://arxiv.org/abs/2508.17366)
*Hanzhong Zhang,Muhua Huang,Jindong Wang*

Main category: cs.AI

TL;DR: 이 논문은 복잡한 상호작용에서의 입장 형성과 정체성 협상이 대형 언어 모델을 통해 안정적으로 이루어질 수 있는지를 조사하는 연구를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 인간의 사회적 행동을 시뮬레이션하는 데 널리 사용되고 있지만, 이 모델들이 복잡한 상호작용에서 입장 형성과 정체성 협상을 안정적으로 수행할 수 있는지는 불확실합니다.

Method: 우리는 생성적 에이전트 기반 모델링과 가상 민속지학 방법을 통합하여 다중 에이전트 사회 실험 프레임워크를 제안합니다.

Result: 세 가지 연구를 통해 에이전트들이 고유한 입장을 보여주고, 사전에 설정된 정체성과 독립적으로 다양한 담론 전략에 대한 뚜렷한 선호와 반응 패턴을 보임을 발견했습니다.

Conclusion: 프리셋 정체성이 에이전트의 사회 구조를 엄격히 결정하지 않으며, 인간 연구자들이 집단 인지에 효과적으로 개입하기 위해서는 에이전트의 언어 네트워크 내의 내재적 메커니즘과 상호작용 역학을 고려해야 한다고 제안합니다.

Abstract: Large language models have been widely used to simulate credible human social
behaviors. However, it remains unclear whether these models can demonstrate
stable capacities for stance formation and identity negotiation in complex
interactions, as well as how they respond to human interventions. We propose a
computational multi-agent society experiment framework that integrates
generative agent-based modeling with virtual ethnographic methods to
investigate how group stance differentiation and social boundary formation
emerge in human-agent hybrid societies. Across three studies, we find that
agents exhibit endogenous stances, independent of their preset identities, and
display distinct tonal preferences and response patterns to different discourse
strategies. Furthermore, through language interaction, agents actively
dismantle existing identity-based power structures and reconstruct
self-organized community boundaries based on these stances. Our findings
suggest that preset identities do not rigidly determine the agents' social
structures. For human researchers to effectively intervene in collective
cognition, attention must be paid to the endogenous mechanisms and
interactional dynamics within the agents' language networks. These insights
provide a theoretical foundation for using generative AI in modeling group
social dynamics and studying human-agent collaboration.

</details>


### [195] [Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs](https://arxiv.org/abs/2508.16846)
*Katherine Atwell,Pedram Heydari,Anthony Sicilia,Malihe Alikhani*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLMs)의 아첨 행동을 이해하기 위한 연구로, 아첨을 사용자 관점을 포함한 비합리적 행동의 편차로 정량화하는 베이지안 프레임워크를 사용한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델에서의 아첨 행동은 인간과 AI 협업의 맥락에서 이해하는 것이 중요하다.

Method: 베이지안 프레임워크를 활용하여 사용자 관점에서 비합리적 행동의 편차로 아첨을 정량화한다.

Result: 아첨 탐사는 LLM의 예측 후방 분포에서 예측 오류를 증가시키며, 다양한 작업과 접근 방식을 통해 효과를 분석했다.

Conclusion: 아첨의 변화는 베이지안 오류와 강한 상관관계가 없음을 보여주며, 단순히 사실에만 의존한 아첨 연구는 오류를 완전히 파악하지 못한다.

Abstract: Sycophancy, or overly agreeable or flattering behavior, is a documented issue
in large language models (LLMs), and is critical to understand in the context
of human/AI collaboration. Prior works typically quantify sycophancy by
measuring shifts in behavior or impacts on accuracy, but neither metric
characterizes shifts in rationality, and accuracy measures can only be used in
scenarios with a known ground truth. In this work, we utilize a Bayesian
framework to quantify sycophancy as deviations from rational behavior when
presented with user perspectives, thus distinguishing between rational and
irrational updates based on the introduction of user perspectives. In
comparison to other methods, this approach allows us to characterize excessive
behavioral shifts, even for tasks that involve inherent uncertainty or do not
have a ground truth. We study sycophancy for 3 different tasks, a combination
of open-source and closed LLMs, and two different methods for probing
sycophancy. We also experiment with multiple methods for eliciting probability
judgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause
deviations in LLMs' predicted posteriors that will lead to increased Bayesian
error. Our findings indicate that: 1) LLMs are not Bayesian rational, 2)
probing for sycophancy results in significant increases to the predicted
posterior in favor of the steered outcome, 3) sycophancy sometimes results in
increased Bayesian error, and in a small number of cases actually decreases
error, and 4) changes in Bayesian error due to sycophancy are not strongly
correlated in Brier score, suggesting that studying the impact of sycophancy on
ground truth alone does not fully capture errors in reasoning due to
sycophancy.

</details>


### [196] [RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis](https://arxiv.org/abs/2508.16850)
*Anku Rani,Aparna Garimella,Apoorv Saxena,Balaji Vasan Srinivasan,Paul Pu Liang*

Main category: cs.AI

TL;DR: 본 연구는 MLLM의 추론 과정을 보강하기 위해 차트의 특정 영역을 강조하는 방법을 제안하고, 이를 통해 더욱 해석 가능하고 신뢰할 수 있는 데이터 시각화 시스템을 구축하는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 시각화는 정확한 해석과 수학적 추론을 필요로 하는 정량적 분석 및 의사결정을 위한 기본 도구이다.

Method: RADAR라는 반자동 접근 방식을 통해 17,819개의 다양한 샘플로 구성된 벤치마크 데이터셋을 구축하고, 차트를 기반으로 한 수학적 추론에 대한 귀속 방법을 제안하였다.

Result: 우리는 추론 유도 접근 방식이 기준 방법에 비해 15% 향상된 귀속 정확도를 달성했음을 보여주었다.

Conclusion: 본 연구의 발전은 사용자로 하여금 추론과 귀속을 통해 모델 결정 과정을 검증하고 이해할 수 있도록 해석 가능하고 신뢰할 수 있는 차트 분석 시스템으로 나아가는 중요한 단계를 의미한다.

Abstract: Data visualizations like charts are fundamental tools for quantitative
analysis and decision-making across fields, requiring accurate interpretation
and mathematical reasoning. The emergence of Multimodal Large Language Models
(MLLMs) offers promising capabilities for automated visual data analysis, such
as processing charts, answering questions, and generating summaries. However,
they provide no visibility into which parts of the visual data informed their
conclusions; this black-box nature poses significant challenges to real-world
trust and adoption. In this paper, we take the first major step towards
evaluating and enhancing the capabilities of MLLMs to attribute their reasoning
process by highlighting the specific regions in charts and graphs that justify
model answers. To this end, we contribute RADAR, a semi-automatic approach to
obtain a benchmark dataset comprising 17,819 diverse samples with charts,
questions, reasoning steps, and attribution annotations. We also introduce a
method that provides attribution for chart-based mathematical reasoning.
Experimental results demonstrate that our reasoning-guided approach improves
attribution accuracy by 15% compared to baseline methods, and enhanced
attribution capabilities translate to stronger answer generation, achieving an
average BERTScore of $\sim$ 0.90, indicating high alignment with ground truth
responses. This advancement represents a significant step toward more
interpretable and trustworthy chart analysis systems, enabling users to verify
and understand model decisions through reasoning and attribution.

</details>


### [197] [Complexity in finitary argumentation (extended version)](https://arxiv.org/abs/2508.16986)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 유한하지만 무한한 주장의 프레임워크는 다양한 추론 문제의 복잡성을 조사하여, 컴퓨터 과학 및 이론적 응용에서 활용 가능성을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 무한한 주장의 프레임워크는 상충되는 정보를 가진 다양한 추론을 분석할 수 있는 공식적인 설정을 제공한다.

Method: 유한하지만 무한한 주장의 프레임워크와 관련된 계산 문제의 복잡성을 조사한다.

Result: 유한함의 가정이 반드시 복잡성 감소를 보장하지는 않지만, 수용 가능성 기반의 의미론에 대한 조합론적 제약이 발견되어 복잡성이 크게 감소한다.

Conclusion: 여러 형태의 추론에 대해 유한한 무한 주장의 프레임워크는 다양한 추론 설정에 적용할 수 있는 표현력을 충분히 갖추면서도 분석에 유용할 만큼 계산적으로 다루기 쉬운 자연스러운 설정을 제공한다.

Abstract: Abstract argumentation frameworks (AFs) provide a formal setting to analyze
many forms of reasoning with conflicting information. While the expressiveness
of general infinite AFs make them a tempting tool for modeling many kinds of
reasoning scenarios, the computational intractability of solving infinite AFs
limit their use, even in many theoretical applications.
  We investigate the complexity of computational problems related to infinite
but finitary argumentations frameworks, that is, infinite AFs where each
argument is attacked by only finitely many others. Our results reveal a
surprising scenario. On one hand, we see that the assumption of being finitary
does not automatically guarantee a drop in complexity. However, for the
admissibility-based semantics, we find a remarkable combinatorial constraint
which entails a dramatic decrease in complexity.
  We conclude that for many forms of reasoning, the finitary infinite AFs
provide a natural setting for reasoning which balances well the competing goals
of being expressive enough to be applied to many reasoning settings while being
computationally tractable enough for the analysis within the framework to be
useful.

</details>


### [198] [WebSight: A Vision-First Architecture for Robust Web Agents](https://arxiv.org/abs/2508.16987)
*Tanvir Bhathal,Asanshay Gupta*

Main category: cs.AI

TL;DR: WebSight는 시각 인식을 통해 웹 환경과 상호작용하는 자율 웹 에이전트로, HTML이나 DOM 입력에 의존하지 않는다.


<details>
  <summary>Details</summary>
Motivation: 웹 환경에서의 상호작용을 개선하기 위해 새로운 비전 기반 웹 에이전트를 개발하고자 하였다.

Method: WebSight-7B라는 사용자 인터페이스 요소 상호작용을 최적화한 비전-언어 모델을 LoRA를 사용해 훈련하고, 모듈식 다중 에이전트 아키텍처에 통합하였다.

Result: WebSight-7B는 Showdown Clicks 벤치마크에서 58.84%의 정확도를 달성하고, WebVoyager 벤치마크에서 68.0%의 성공률을 기록하였다.

Conclusion: WebSight와 WebSight-7B는 해석 가능하고 robust하며 효율적인 시각적 웹 탐색을 위한 새로운 표준을 제시한다.

Abstract: We introduce WebSight, a vision-based autonomous web agent, designed to
interact with web environments purely through visual perception, eliminating
dependence on HTML or DOM-based inputs. Central to our approach we introduce
our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI
element interaction, trained using LoRA on a web-focused subset of the
Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent
architecture, comprising planning, reasoning, vision-action, and verification
agents, coordinated through an episodic memory mechanism.
  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks
benchmark, outperforming several larger generalist models while maintaining
lower latency. The full WebSight agent achieves a 68.0% success rate on the
WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and
HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly
97.14% of the time, indicating high precision. Together, WebSight and
WebSight-7B establish a new standard for interpretable, robust, and efficient
visual web navigation.

</details>


### [199] [Solving the Min-Max Multiple Traveling Salesmen Problem via Learning-Based Path Generation and Optimal Splitting](https://arxiv.org/abs/2508.17087)
*Wen Wang,Xiangchen Wu,Liang Wang,Hao Hu,Xianping Tao,Linghao Zhang*

Main category: cs.AI

TL;DR: 본 연구는 여러 판매원의 경로를 조정하여 가장 긴 경로의 길이를 최소화하는 Min-Max Multiple Traveling Salesmen Problem ($m^3$-TSP)에 대해 다룹니다. 이 문제는 NP-hard 성질로 인해 정확한 해법이 비현실적이며, 학습 기반 접근법이 고품질 근사 해법을 빠르게 생성하는 능력으로 주목받고 있습니다. 이 연구는 강화 학습과 최적 분할 알고리즘을 결합한 새로운 두 단계 프레임워크인 Generate-and-Split (GaS)를 제안하여 이러한 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: NP-hard한 특성으로 인해 정확한 해법이 비현실적이므로, 학습 기반 접근법이 주목받고 있음.

Method: GaS는 강화 학습과 최적 분할 알고리즘을 결합하여 공동 훈련 과정에서 학습 목표를 단순화하는 두 단계 프레임워크입니다.

Result: 제안된 GaS 프레임워크는 솔루션 품질과 전이 가능성 모두에서 기존의 학습 기반 접근법보다 우수한 성능을 보입니다.

Conclusion: GaS는 반복적인 최적화에서 나타나는 일관성을 유지하면서도 성능을 향상시키는 혁신적인 접근 방식을 제시합니다.

Abstract: This study addresses the Min-Max Multiple Traveling Salesmen Problem
($m^3$-TSP), which aims to coordinate tours for multiple salesmen such that the
length of the longest tour is minimized. Due to its NP-hard nature, exact
solvers become impractical under the assumption that $P \ne NP$. As a result,
learning-based approaches have gained traction for their ability to rapidly
generate high-quality approximate solutions. Among these, two-stage methods
combine learning-based components with classical solvers, simplifying the
learning objective. However, this decoupling often disrupts consistent
optimization, potentially degrading solution quality. To address this issue, we
propose a novel two-stage framework named \textbf{Generate-and-Split} (GaS),
which integrates reinforcement learning (RL) with an optimal splitting
algorithm in a joint training process. The splitting algorithm offers
near-linear scalability with respect to the number of cities and guarantees
optimal splitting in Euclidean space for any given path. To facilitate the
joint optimization of the RL component with the algorithm, we adopt an
LSTM-enhanced model architecture to address partial observability. Extensive
experiments show that the proposed GaS framework significantly outperforms
existing learning-based approaches in both solution quality and
transferability.

</details>


### [200] [PowerChain: Automating Distribution Grid Analysis with Agentic AI Workflows](https://arxiv.org/abs/2508.17094)
*Emmanuel O. Badmus,Peng Sang,Dimitrios Stamoulis,Amritanshu Pandey*

Main category: cs.AI

TL;DR: 전력망 운영 및 계획이 복잡해짐에 따라, 자동화된 AI 시스템인 PowerChain을 개발하여 전력망 분석 작업을 수행하고, 전문가 수준의 워크플로우를 생성하는 방법을 제시.


<details>
  <summary>Details</summary>
Motivation: 전력망 운영의 복잡성 증가와 많은 소규모 유틸리티의 R&D 인력 부족 문제 해결을 위한 필요성.

Method: PowerChain은 자연어 쿼리에 따라 도메인 인식 기능의 순서를 다이나믹하게 생성하고 실행하는 자동화된 에이전틱 오케스트레이션을 사용.

Result: PowerChain은 복잡한 전력망 분석 작업에서 GPT-5 및 오픈소스 Qwen 모델을 활용하여 전문가 수준의 워크플로우를 생성할 수 있음을 보여줌.

Conclusion: 전력망 분석의 접근성을 높이고 소규모 유틸리티들이 고급 분석을 활용할 수 있도록 하는 혁신적인 방법을 제공함.

Abstract: Due to the rapid pace of electrification and decarbonization, distribution
grid (DG) operation and planning are becoming more complex, necessitating
advanced computational analyses to ensure grid reliability and resilience.
State-of-the-art DG analyses rely on disparate workflows of complex models,
functions, and data pipelines, which require expert knowledge and are
challenging to automate. Many small-scale utilities and cooperatives lack a
large R&D workforce and therefore cannot use advanced analysis at scale. To
address this gap, we develop a novel agentic AI system, PowerChain, to solve
unseen DG analysis tasks via automated agentic orchestration and large language
models (LLMs) function-calling. Given a natural language query, PowerChain
dynamically generates and executes an ordered sequence of domain-aware
functions guided by the semantics of an expert-built power systems function
pool and a select reference set of known, expert-generated workflow-query
pairs. Our results show that PowerChain can produce expert-level workflows with
both GPT-5 and open-source Qwen models on complex, unseen DG analysis tasks
operating on real utility data.

</details>


### [201] [Rethinking How AI Embeds and Adapts to Human Values: Challenges and Opportunities](https://arxiv.org/abs/2508.17104)
*Sz-Ting Tzeng,Frank Dignum*

Main category: cs.AI

TL;DR: 인간 중심 AI와 가치 기반 결정에 대한 연구가 증가하고 있지만, 가치 정렬을 재구성하고, AI 시스템이 장기적 사고를 포함해야 함을 주장하며, 다양한 인간 가치를 다룰 이론이 더 필요하다는 것을 논의함.


<details>
  <summary>Details</summary>
Motivation: 인간 중심 AI와 가치 기반 결정이 연구 및 산업에서 큰 주목을 받고 있으나, 많은 중요한 측면이 충분히 탐구되지 않았음.

Method: AI 시스템이 장기적 사고를 구현하고 적응 가능한 방식으로 진화하는 가치를 고려해야 함을 주장.

Result: 가치 정렬 연구를 발전시킬 방향을 제시하며, 가치의 다양성을 다룰 수 있는 다중 에이전트 시스템의 필요성을 강조.

Conclusion: 가치 정렬에 대한 더 많은 이론과 다양한 관점이 필요하며, 이는 설계 방법론에서 실제 적용에 걸쳐 포함됨.

Abstract: The concepts of ``human-centered AI'' and ``value-based decision'' have
gained significant attention in both research and industry. However, many
critical aspects remain underexplored and require further investigation. In
particular, there is a need to understand how systems incorporate human values,
how humans can identify these values within systems, and how to minimize the
risks of harm or unintended consequences. In this paper, we highlight the need
to rethink how we frame value alignment and assert that value alignment should
move beyond static and singular conceptions of values. We argue that AI systems
should implement long-term reasoning and remain adaptable to evolving values.
Furthermore, value alignment requires more theories to address the full
spectrum of human values. Since values often vary among individuals or groups,
multi-agent systems provide the right framework for navigating pluralism,
conflict, and inter-agent reasoning about values. We identify the challenges
associated with value alignment and indicate directions for advancing value
alignment research. In addition, we broadly discuss diverse perspectives of
value alignment, from design methodologies to practical applications.

</details>


### [202] [MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes](https://arxiv.org/abs/2508.17180)
*Nilay Pande,Sahiti Yerramilli,Jayant Sravan Tamarapalli,Rynaa Grover*

Main category: cs.AI

TL;DR: 이 논문은 이미지에서 직접적으로 수학적 및 공간적 추론을 수행하는 멀티모달 대형 언어 모델(MLLM)의 능력을 평가하기 위한 새로운 벤치마크인 MaRVL-QA를 소개한다.


<details>
  <summary>Details</summary>
Motivation: MLLM의 능력을 심층적인 수학적 및 공간적 추론으로 확장하기 위해서는 수학적 표면 플롯과 같은 엄격한 테스트 방법이 필요하다.

Method: MaRVL-QA는 두 가지 새로운 과업으로 구성되어 있으며, 하나는 지역 최대치와 같은 특징을 식별하고 세는 '위상적 세기'와, 다른 하나는 적용된 기하학적 변환을 인식하는 '변환 인식'이다.

Result: 최신 MLLM조차도 MaRVL-QA에서 기능적이고 공간적 추론에 있어 심각한 어려움을 겪고 있으며, 대개 얄팍한 휴리스틱에 의존하고 있었다.

Conclusion: MaRVL-QA는 연구 커뮤니티가 진전을 측정하고 모델의 한계를 드러내며 MLLM 개발을 위한 더 깊은 추론 능력을 이끌어내는 도구이다.

Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to
perform deep mathematical and spatial reasoning directly from images, moving
beyond their established success in semantic description. Mathematical surface
plots provide a rigorous testbed for this capability, as they isolate the task
of reasoning from the semantic noise common in natural images. To measure
progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over
Visual Landscapes), a new benchmark designed to quantitatively evaluate these
core reasoning skills. The benchmark comprises two novel tasks: Topological
Counting, identifying and enumerating features like local maxima; and
Transformation Recognition, recognizing applied geometric transformations.
Generated from a curated library of functions with rigorous ambiguity
filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs
struggle significantly, often resorting to superficial heuristics instead of
robust spatial reasoning. MaRVL-QA provides a challenging new tool for the
research community to measure progress, expose model limitations, and guide the
development of MLLMs with more profound reasoning abilities.

</details>


### [203] [PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent LLMs](https://arxiv.org/abs/2508.17188)
*Zhilin Zhang,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Chenyu You*

Main category: cs.AI

TL;DR: PosterGen은 다중 에이전트 시스템을 활용하여 연구자의 논문을 포스터로 변환하는 과정을 자동화하는 프레임워크로, 디자인 원칙을 고려하여 높은 품질의 포스터를 생성한다.


<details>
  <summary>Details</summary>
Motivation: 연구자들이 학회 준비 과정에서 겪는 논문 포스터 제작의 시간 소모 문제를 해결하고자 한다.

Method: 4개의 협력적 전문 에이전트를 포함하는 PosterGen 프레임워크를 제안하며, 이들은 각각 내용 추출, 레이아웃 구성, 시각적 디자인 적용, 최종 포스터 작성을 담당한다.

Result: PosterGen은 기존 방법보다 시각적 디자인에서 월등히 향상된 포스터를 생성하여 콘텐츠 충실도에서 일관되게 일치하며, 최소한의 인간 수정으로 발표 준비가 가능한 포스터를 생성한다.

Conclusion: PosterGen은 효과적이고 효율적으로 디자인 원칙을 반영한 포스터를 생성하여 연구자들에게 실질적인 도움을 줄 수 있다.

Abstract: Multi-agent systems built upon large language models (LLMs) have demonstrated
remarkable capabilities in tackling complex compositional tasks. In this work,
we apply this paradigm to the paper-to-poster generation problem, a practical
yet time-consuming process faced by researchers preparing for conferences.
While recent approaches have attempted to automate this task, most neglect core
design and aesthetic principles, resulting in posters that require substantial
manual refinement. To address these design limitations, we propose PosterGen, a
multi-agent framework that mirrors the workflow of professional poster
designers. It consists of four collaborative specialized agents: (1) Parser and
Curator agents extract content from the paper and organize storyboard; (2)
Layout agent maps the content into a coherent spatial layout; (3) Stylist
agents apply visual design elements such as color and typography; and (4)
Renderer composes the final poster. Together, these agents produce posters that
are both semantically grounded and visually appealing. To evaluate design
quality, we introduce a vision-language model (VLM)-based rubric that measures
layout balance, readability, and aesthetic coherence. Experimental results show
that PosterGen consistently matches in content fidelity, and significantly
outperforms existing methods in visual designs, generating posters that are
presentation-ready with minimal human refinements.

</details>


### [204] [From reactive to cognitive: brain-inspired spatial intelligence for embodied agents](https://arxiv.org/abs/2508.17198)
*Shouwei Ruan,Liyuan Wang,Caixin Kang,Qihui Zhu,Songming Liu,Xingxing Wei,Hang Su*

Main category: cs.AI

TL;DR: BSC-Nav는 내러티브 성격의 공간 인지를 위한 새로운 프레임워크로, 구조화된 공간 기억을 이용하여 실제 환경에서의 탐색 효율성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 인간의 공간 인지 방식에서 영감을 받아 더 나은 내비게이션 수행을 목표로 하였다.

Method: BSC-Nav는 주체적인 경로와 맥락적 단서를 바탕으로 공간 인지 지도를 구축하고, 의미적 목표에 맞춘 공간 지식을 동적으로 검색한다.

Result: BSC-Nav는 다양한 탐색 작업에서 state-of-the-art의 효율성과 효능을 달성하고, 강력한 제로샷 일반화를 보여준다.

Conclusion: BSC-Nav는 일반적인 공간 지능을 향한 확장 가능하고 생물학적으로 근본적인 경로를 제시한다.

Abstract: Spatial cognition enables adaptive goal-directed behavior by constructing
internal models of space. Robust biological systems consolidate spatial
knowledge into three interconnected forms: \textit{landmarks} for salient cues,
\textit{route knowledge} for movement trajectories, and \textit{survey
knowledge} for map-like representations. While recent advances in multi-modal
large language models (MLLMs) have enabled visual-language reasoning in
embodied agents, these efforts lack structured spatial memory and instead
operate reactively, limiting their generalization and adaptability in complex
real-world environments. Here we present Brain-inspired Spatial Cognition for
Navigation (BSC-Nav), a unified framework for constructing and leveraging
structured spatial memory in embodied agents. BSC-Nav builds allocentric
cognitive maps from egocentric trajectories and contextual cues, and
dynamically retrieves spatial knowledge aligned with semantic goals. Integrated
with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency
across diverse navigation tasks, demonstrates strong zero-shot generalization,
and supports versatile embodied behaviors in the real physical world, offering
a scalable and biologically grounded path toward general-purpose spatial
intelligence.

</details>


### [205] [Large Language Model-Based Automatic Formulation for Stochastic Optimization Models](https://arxiv.org/abs/2508.17200)
*Amirreza Talebi*

Main category: cs.AI

TL;DR: 이 논문은 자연어 설명에서 확률 최적화 문제를 자동으로 공식화하고 해결하기 위해 ChatGPT와 같은 대형 언어 모델(LLMs)의 성능을 통합적으로 연구한 첫 번째 논문이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 활용하여 자연어 설명에서 확률 최적화 문제를 해결할 수 있는 가능성을 탐구하고자 한다.

Method: 세 가지 주요 카테고리인 공동 기회 제약 모델, 개별 기회 제약 모델, 두 단계 확률 선형 프로그램(SLP-2)에 대해 체계적인 작업을 통해 ChatGPT를 안내하는 여러 프롬프트를 설계하였다.

Result: GPT-4-Turbo는 다양한 확률 문제에서 부분 점수, 변수 일치 및 목표 정확성에서 다른 모델보다 우수한 성능을 보였으며, cot_s_instructions 및 agentic이 가장 효과적인 프롬프트 전략으로 나타났다.

Conclusion: 잘 설계된 프롬프트와 다중 에이전트 협업을 통해 LLM이 특별한 확률 공식화를 촉진할 수 있음을 보여주며, 이는 확률 최적화에서 지능적이고 언어 기반 모델링 파이프라인의 길을 열어준다.

Abstract: This paper presents the first integrated systematic study on the performance
of large language models (LLMs), specifically ChatGPT, to automatically
formulate and solve stochastic optimiza- tion problems from natural language
descriptions. Focusing on three key categories, joint chance- constrained
models, individual chance-constrained models, and two-stage stochastic linear
programs (SLP-2), we design several prompts that guide ChatGPT through
structured tasks using chain-of- thought and modular reasoning. We introduce a
novel soft scoring metric that evaluates the struc- tural quality and partial
correctness of generated models, addressing the limitations of canonical and
execution-based accuracy. Across a diverse set of stochastic problems,
GPT-4-Turbo outperforms other models in partial score, variable matching, and
objective accuracy, with cot_s_instructions and agentic emerging as the most
effective prompting strategies. Our findings reveal that with well-engineered
prompts and multi-agent collaboration, LLMs can facilitate specially stochastic
formulations, paving the way for intelligent, language-driven modeling
pipelines in stochastic opti- mization.

</details>


### [206] [Explainable Counterfactual Reasoning in Depression Medication Selection at Multi-Levels (Personalized and Population)](https://arxiv.org/abs/2508.17207)
*Xinyu Qin,Mark H. Chignell,Alexandria Greifenberger,Sachinthya Lokuge,Elssa Toumeh,Tia Sternat,Martin Katzman,Lu Wang*

Main category: cs.AI

TL;DR: 이 연구는 우울증 증상의 변동이 SSRI와 SNRI 처방에 미치는 인과적 영향을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 우울증 진단과 처방의 정확성을 향상시키기 위해 MDD 증상의 영향을 이해하는 것이 필수적입니다.

Method: 설명 가능한 반사실적 추론과 반사실적 설명(CFs)을 사용하여 특정 증상 변화가 항우울제 선택에 미치는 영향을 평가했습니다.

Result: 17개의 이진 분류기 중 Random Forest가 가장 높은 성능을 기록했습니다(정확도, F1, 정밀도, 재현율, ROC-AUC 약 0.85).

Conclusion: 반사실적 추론은 MDD 증상이 SSRI와 SNRI 선택을 결정하는 데 어떤 영향을 미치는지 명확하게 설명하며, AI 기반 임상 의사결정 지원 시스템의 해석력을 향상시킵니다. 향후 작업은 다양한 집단에서 이 결과를 검증하고 임상 배포를 위한 알고리즘을 정제해야 합니다.

Abstract: Background: This study investigates how variations in Major Depressive
Disorder (MDD) symptoms, quantified by the Hamilton Rating Scale for Depression
(HAM-D), causally influence the prescription of SSRIs versus SNRIs. Methods: We
applied explainable counterfactual reasoning with counterfactual explanations
(CFs) to assess the impact of specific symptom changes on antidepressant
choice. Results: Among 17 binary classifiers, Random Forest achieved highest
performance (accuracy, F1, precision, recall, ROC-AUC near 0.85). Sample-based
CFs revealed both local and global feature importance of individual symptoms in
medication selection. Conclusions: Counterfactual reasoning elucidates which
MDD symptoms most strongly drive SSRI versus SNRI selection, enhancing
interpretability of AI-based clinical decision support systems. Future work
should validate these findings on more diverse cohorts and refine algorithms
for clinical deployment.

</details>


### [207] [Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward](https://arxiv.org/abs/2508.17212)
*Xinyu Qin,Ruiheng Yu,Lu Wang*

Main category: cs.AI

TL;DR: 이 논문에서는 안전 제한 사항 하에 온라인에서 적응할 수 있는 임상 의사 결정 지원 도구를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 안전 제약 조건에서 온라인으로 적응할 수 있는 임상 결정 지원의 필요성

Method: 강화 학습을 통해 정책을 제공하고, 환자 디지털 트윈이 환경을 제공하며, 치료 효과가 보상을 정의하는 온라인 적응 도구

Result: 합성 임상 시뮬레이터에서 실험을 통해 낮은 대기 시간, 안정적인 처리량, 고정된 안전성에서 낮은 전문가 질의 비율 및 표준 가치 기반 기준에 비해 개선된 수익을 보여줌

Conclusion: 오프라인 정책을 지속적이고 임상 의사가 감독하는 시스템으로 전환시켜 명확한 제어 및 빠른 적응을 가능하게 함

Abstract: Clinical decision support must adapt online under safety constraints. We
present an online adaptive tool where reinforcement learning provides the
policy, a patient digital twin provides the environment, and treatment effect
defines the reward. The system initializes a batch-constrained policy from
retrospective data and then runs a streaming loop that selects actions, checks
safety, and queries experts only when uncertainty is high. Uncertainty comes
from a compact ensemble of five Q-networks via the coefficient of variation of
action values with a $\tanh$ compression. The digital twin updates the patient
state with a bounded residual rule. The outcome model estimates immediate
clinical effect, and the reward is the treatment effect relative to a
conservative reference with a fixed z-score normalization from the training
split. Online updates operate on recent data with short runs and exponential
moving averages. A rule-based safety gate enforces vital ranges and
contraindications before any action is applied. Experiments in a synthetic
clinical simulator show low latency, stable throughput, a low expert query rate
at fixed safety, and improved return against standard value-based baselines.
The design turns an offline policy into a continuous, clinician-supervised
system with clear controls and fast adaptation.

</details>


### [208] [MC3G: Model Agnostic Causally Constrained Counterfactual Generation](https://arxiv.org/abs/2508.17221)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: MC3G는 기계 학습 모델의 투명성과 책임을 높이며, 결정적인 결과를 기반으로 사용자 주도의 변화를 통해 더 효과적인 반사실적 설명을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 모델이 금융, 법률 및 채용과 같은 고위험 환경에서 결정에 점점 더 영향을 미치고 있어, 투명하고 해석 가능한 결과에 대한 필요성이 커지고 있다.

Method: MC3G는 모델에 구애받지 않는 새로운 프레임워크로, 설명 가능한 규칙 기반 대체 모델을 사용하여 블랙박스 모델을 근사하고, 사용자에 의해 유도된 변화에만 초점을 맞춰 반사실적 결과를 생성한다.

Result: MC3G는 기존 기술들에 비해 더 해석 가능하고 실행 가능한 반사실적 권장 사항을 제공하며, 비용 또한 낮출 수 있음을 보여준다.

Conclusion: MC3G는 기계 학습 접근법을 포함한 의사 결정 과정에서 투명성, 책임 및 실용성을 높일 수 있는 잠재력을 가지고 있다.

Abstract: Machine learning models increasingly influence decisions in high-stakes
settings such as finance, law and hiring, driving the need for transparent,
interpretable outcomes. However, while explainable approaches can help
understand the decisions being made, they may inadvertently reveal the
underlying proprietary algorithm: an undesirable outcome for many
practitioners. Consequently, it is crucial to balance meaningful transparency
with a form of recourse that clarifies why a decision was made and offers
actionable steps following which a favorable outcome can be obtained.
Counterfactual explanations offer a powerful mechanism to address this need by
showing how specific input changes lead to a more favorable prediction. We
propose Model-Agnostic Causally Constrained Counterfactual Generation (MC3G), a
novel framework that tackles limitations in the existing counterfactual
methods. First, MC3G is model-agnostic: it approximates any black-box model
using an explainable rule-based surrogate model. Second, this surrogate is used
to generate counterfactuals that produce a favourable outcome for the original
underlying black box model. Third, MC3G refines cost computation by excluding
the ``effort" associated with feature changes that occur automatically due to
causal dependencies. By focusing only on user-initiated changes, MC3G provides
a more realistic and fair representation of the effort needed to achieve a
favourable outcome. We show that MC3G delivers more interpretable and
actionable counterfactual recommendations compared to existing techniques all
while having a lower cost. Our findings highlight MC3G's potential to enhance
transparency, accountability, and practical utility in decision-making
processes that incorporate machine-learning approaches.

</details>


### [209] [L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems](https://arxiv.org/abs/2508.17244)
*Aoun E Muhammad,Kin-Choong Yow,Nebojsa Bacanin-Dzakula,Muhammad Attique Khan*

Main category: cs.AI

TL;DR: AI의 결정 과정에서의 설명 가능성의 중요성이 높아지고 있으며, 본 논문에서는 IDS의 결정 과정을 설명하기 위한 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI의 중요한 산업 분야에서의 설명 가능성이 점점 더 중요해지고 있다.

Method: LIME, ELI5 및 결정 트리 알고리즘을 결합한 프레임워크를 사용하여 IDS의 의사 결정을 설명한다.

Result: UNSW-NB15 데이터 세트에서 공격 행동을 분류하는 데 85% 정확도를 달성했다.

Conclusion: 이 프레임워크는 사이버 크리티컬 시스템에서 설명 가능한 AI의 광범위한 채택에 중요한 투명성을 제공한다.

Abstract: Recent developments in Artificial Intelligence (AI) and their applications in
critical industries such as healthcare, fin-tech and cybersecurity have led to
a surge in research in explainability in AI. Innovative research methods are
being explored to extract meaningful insight from blackbox AI systems to make
the decision-making technology transparent and interpretable. Explainability
becomes all the more critical when AI is used in decision making in domains
like fintech, healthcare and safety critical systems such as cybersecurity and
autonomous vehicles. However, there is still ambiguity lingering on the
reliable evaluations for the users and nature of transparency in the
explanations provided for the decisions made by black-boxed AI. To solve the
blackbox nature of Machine Learning based Intrusion Detection Systems, a
framework is proposed in this paper to give an explanation for IDSs decision
making. This framework uses Local Interpretable Model-Agnostic Explanations
(LIME) coupled with Explain Like I'm five (ELI5) and Decision Tree algorithms
to provide local and global explanations and improve the interpretation of
IDSs. The local explanations provide the justification for the decision made on
a specific input. Whereas, the global explanations provides the list of
significant features and their relationship with attack traffic. In addition,
this framework brings transparency in the field of ML driven IDS that might be
highly significant for wide scale adoption of eXplainable AI in cyber-critical
systems. Our framework is able to achieve 85 percent accuracy in classifying
attack behaviour on UNSW-NB15 dataset, while at the same time displaying the
feature significance ranking of the top 10 features used in the classification.

</details>


### [210] [Federated Reinforcement Learning for Runtime Optimization of AI Applications in Smart Eyewears](https://arxiv.org/abs/2508.17262)
*Hamta Sedghani,Abednego Wamuhindo Kambale,Federica Filippini,Francesca Palermo,Diana Trojaniello,Danilo Ardagna*

Main category: cs.AI

TL;DR: 본 논문은 연합 강화 학습을 통해 데이터 프라이버시를 유지하면서도 여러 에이전트가 협력적으로 훈련할 수 있는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 확장 현실 기술이 헬스케어, 엔터테인먼트, 교육 등의 분야를 변형시키고 있으며, 이는 컴퓨팅 전력과 메모리, 배터리 수명 같은 SEWs의 기본 한계에 직면해 있습니다.

Method: 우리는 동기 및 비동기 연합 전략을 구현하였으며, 모델은 고정 간격 또는 에이전트 진행 상황에 따라 동적으로 집계됩니다.

Result: 실험 결과, 연합 에이전트는 성능 변동성이 현저히 낮아져 더 큰 안정성과 신뢰성을 보였습니다.

Conclusion: 이러한 결과는 SEWs와 같은 실시간 AI 처리가 필요한 응용 프로그램에 대한 FRL의 잠재력을 강조합니다.

Abstract: Extended reality technologies are transforming fields such as healthcare,
entertainment, and education, with Smart Eye-Wears (SEWs) and Artificial
Intelligence (AI) playing a crucial role. However, SEWs face inherent
limitations in computational power, memory, and battery life, while offloading
computations to external servers is constrained by network conditions and
server workload variability. To address these challenges, we propose a
Federated Reinforcement Learning (FRL) framework, enabling multiple agents to
train collaboratively while preserving data privacy. We implemented synchronous
and asynchronous federation strategies, where models are aggregated either at
fixed intervals or dynamically based on agent progress. Experimental results
show that federated agents exhibit significantly lower performance variability,
ensuring greater stability and reliability. These findings underscore the
potential of FRL for applications requiring robust real-time AI processing,
such as real-time object detection in SEWs.

</details>


### [211] [ERF-BA-TFD+: A Multimodal Model for Audio-Visual Deepfake Detection](https://arxiv.org/abs/2508.17282)
*Xin Zhang,Jiaming Chu,Jian Zhao,Yuchu Jiang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: ERF-BA-TFD+는 오디오 및 비디오 기능을 동시에 처리하는 다중 모달 딥페이크 탐지 모델로, 상태-of-the-art 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 딥페이크 콘텐츠의 증가로 인해 조작된 멀티미디어 콘텐츠를 식별하는 것이 중요하다.

Method: ERF와 오디오-비주얼 융합을 결합한 ERF-BA-TFD+ 모델을 제안하여 오디오 및 비디오 특징을 동시에 처리한다.

Result: DDL-AV 데이터셋에서 기존 기술을 초과하는 정확성과 처리 속도로 최고 성능을 달성했다.

Conclusion: ERF-BA-TFD+ 모델은 딥페이크 탐지 대회에서 1위를 차지하며 그 효과성을 입증했다.

Abstract: Deepfake detection is a critical task in identifying manipulated multimedia
content. In real-world scenarios, deepfake content can manifest across multiple
modalities, including audio and video. To address this challenge, we present
ERF-BA-TFD+, a novel multimodal deepfake detection model that combines enhanced
receptive field (ERF) and audio-visual fusion. Our model processes both audio
and video features simultaneously, leveraging their complementary information
to improve detection accuracy and robustness. The key innovation of ERF-BA-TFD+
lies in its ability to model long-range dependencies within the audio-visual
input, allowing it to better capture subtle discrepancies between real and fake
content. In our experiments, we evaluate ERF-BA-TFD+ on the DDL-AV dataset,
which consists of both segmented and full-length video clips. Unlike previous
benchmarks, which focused primarily on isolated segments, the DDL-AV dataset
allows us to assess the model's performance in a more comprehensive and
realistic setting. Our method achieves state-of-the-art results on this
dataset, outperforming existing techniques in terms of both accuracy and
processing speed. The ERF-BA-TFD+ model demonstrated its effectiveness in the
"Workshop on Deepfake Detection, Localization, and Interpretability," Track 2:
Audio-Visual Detection and Localization (DDL-AV), and won first place in this
competition.

</details>


### [212] [MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment](https://arxiv.org/abs/2508.17290)
*Omid Ghahroodi,Arshia Hemmat,Marzia Nouri,Seyed Mohammad Hadi Hosseini,Doratossadat Dastgheib,Mohammad Vali Sanian,Alireza Sahebi,Reihaneh Zohrabi,Mohammad Hossein Rohban,Ehsaneddin Asgari,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: MEENA라는 첫 번째 데이터셋을 소개하며, 이는 페르시아 VLM을 평가하기 위해 고안되었다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 비전-언어 모델이 영어에 주목하고 있어 다른 언어에 대한 평가에 한계가 있다.

Method: 현재 7,500개의 페르시아어와 3,000개의 영어 질문으로 구성된 MEENA 데이터셋을 설계하였다.

Result: 다양한 교육 수준을 포괄하는 주제, 다양한 메타데이터와 함께 문화적 뉘앙스를 유지한 원본 페르시아 데이터를 포함한다.

Conclusion: 이 벤치마크가 영어 이상의 VLM 능력 향상에 기여하기를 바란다.

Abstract: Recent advancements in large vision-language models (VLMs) have primarily
focused on English, with limited attention given to other languages. To address
this gap, we introduce MEENA (also known as PersianMMMU), the first dataset
designed to evaluate Persian VLMs across scientific, reasoning, and human-level
understanding tasks. Our dataset comprises approximately 7,500 Persian and
3,000 English questions, covering a wide range of topics such as reasoning,
mathematics, physics, diagrams, charts, and Persian art and literature. Key
features of MEENA include: (1) diverse subject coverage spanning various
educational levels, from primary to upper secondary school, (2) rich metadata,
including difficulty levels and descriptive answers, (3) original Persian data
that preserves cultural nuances, (4) a bilingual structure to assess
cross-linguistic performance, and (5) a series of diverse experiments assessing
various capabilities, including overall performance, the model's ability to
attend to images, and its tendency to generate hallucinations. We hope this
benchmark contributes to enhancing VLM capabilities beyond English.

</details>


### [213] [Meta-R1: Empowering Large Reasoning Models with Metacognition](https://arxiv.org/abs/2508.17291)
*Haonan Dong,Haoran Ye,Wenhao Zhu,Kehan Jiang,Guojie Song*

Main category: cs.AI

TL;DR: 본 논문에서는 메타 인지 능력을 LRMs에 도입하여 비대응적이고 신뢰할 수 없으며 비유연한 사고 방식을 개선하는 메타-R1 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재 LRMs는 메타 수준의 인지 시스템이 부족하여 emergent 능력이 비효율적이고 신뢰할 수 없으며 유연성이 떨어지도록 만들고 있습니다.

Method: 메타-R1은 인지 과학의 원리를 바탕으로 사고 과정을 명확한 객체 수준과 메타 수준 구성 요소로 분해하여 proactive planning, 온라인 조정 및 적응형 조기 종료를 집합적으로 관리합니다.

Result: 세 가지 벤치마크 실험 결과, 메타-R1은 기존 방법보다 최대 27.3% 향상된 성능을 보였으며, 토큰 소비를 15.7%에서 32.7%로 줄이고 효율성을 최대 14.8% 향상시켰습니다.

Conclusion: 메타-R1은 다양한 데이터셋과 모델 백본에서 안정적인 성능을 유지하므로 효과적입니다.

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
tasks, exhibiting emergent, human-like thinking patterns. Despite their
advances, we identify a fundamental limitation: current LRMs lack a dedicated
meta-level cognitive system-an essential faculty in human cognition that
enables "thinking about thinking". This absence leaves their emergent abilities
uncontrollable (non-adaptive reasoning), unreliable (intermediate error), and
inflexible (lack of a clear methodology). To address this gap, we introduce
Meta-R1, a systematic and generic framework that endows LRMs with explicit
metacognitive capabilities. Drawing on principles from cognitive science,
Meta-R1 decomposes the reasoning process into distinct object-level and
meta-level components, orchestrating proactive planning, online regulation, and
adaptive early stopping within a cascaded framework. Experiments on three
challenging benchmarks and against eight competitive baselines demonstrate that
Meta-R1 is: (I) high-performing, surpassing state-of-the-art methods by up to
27.3%; (II) token-efficient, reducing token consumption to 15.7% ~ 32.7% and
improving efficiency by up to 14.8% when compared to its vanilla counterparts;
and (III) transferable, maintaining robust performance across datasets and
model backbones.

</details>


### [214] [Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery](https://arxiv.org/abs/2508.17380)
*Jiaqi Liu,Songning Lai,Pengze Li,Di Yu,Wenjie Zhou,Yiyang Zhou,Peng Xia,Zijun Wang,Xi Chen,Shixiang Tang,Lei Bai,Wanli Ouyang,Mingyu Ding,Huaxiu Yao,Aoran Wang*

Main category: cs.AI

TL;DR: VIPER-R1은 물리 기반 방정식 추론을 위한 시각적 유도 모델로, 고전적인 물리법칙 발견을 돕는다.


<details>
  <summary>Details</summary>
Motivation: 실제 관측 데이터에서 물리 법칙을 자동 발견하는 것은 AI의 큰 도전 과제이다.

Method: VIPER-R1은 시각적 지각, 궤적 데이터 및 상징적 추론을 통합하여 과학적 발견 과정을 모사하는 다중 모드 모델이다.

Result: 실험 결과 VIPER-R1은 정확도와 해석 가능성에서 최첨단 VLM의 기준을 지속적으로 능가하였다.

Conclusion: VIPER-R1은 물리 법칙의 더 정확한 발견을 가능하게 한다.

Abstract: Automated discovery of physical laws from observational data in the real
world is a grand challenge in AI. Current methods, relying on symbolic
regression or LLMs, are limited to uni-modal data and overlook the rich, visual
phenomenological representations of motion that are indispensable to
physicists. This "sensory deprivation" severely weakens their ability to
interpret the inherent spatio-temporal patterns within dynamic phenomena. To
address this gap, we propose VIPER-R1, a multimodal model that performs Visual
Induction for Physics-based Equation Reasoning to discover fundamental symbolic
formulas. It integrates visual perception, trajectory data, and symbolic
reasoning to emulate the scientific discovery process. The model is trained via
a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning
to interpret kinematic phase portraits and to construct hypotheses guided by a
Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration
(RGSC) to refine the formula structure with reinforcement learning. During
inference, the trained VIPER-R1 acts as an agent: it first posits a
high-confidence symbolic ansatz, then proactively invokes an external symbolic
regression tool to perform Symbolic Residual Realignment (SR^2). This final
step, analogous to a physicist's perturbation analysis, reconciles the
theoretical model with empirical data. To support this research, we introduce
PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that
VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy
and interpretability, enabling more precise discovery of physical laws. Project
page: https://jiaaqiliu.github.io/VIPER-R1/

</details>


### [215] [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets](https://arxiv.org/abs/2508.17391)
*Nikolaos Pavlidis,Vasilis Perifanis,Symeon Symeonidis,Pavlos S. Efraimidis*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)은 소규모 구조화된 데이터셋에서 분류, 회귀 및 군집 작업를 수행할 수 있는 능력을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 여러 모달리티 및 도메인에서 일반화할 수 있는 잠재력을 보여주기 위해 연구했습니다.

Method: 최신 LLM(GPT-5, GPT-4o, GPT-03 등)의 성능을 몇 샷 프롬프트에서 평가하고 기존 기계 학습 기법과 비교했습니다.

Result: LLM은 제한된 데이터 환경에서 분류 작업에서 강력한 성능을 보여주지만, 회귀 및 군집 작업에서는 기계 학습 모델에 비해 성능이 떨어졌습니다.

Conclusion: LLM은 구조화된 데이터에 대한 범용 예측 엔진으로 활용될 수 있지만, 회귀 및 군집에서의 제한 사항이 있습니다.

Abstract: Large Language Models (LLMs), originally developed for natural language
processing (NLP), have demonstrated the potential to generalize across
modalities and domains. With their in-context learning (ICL) capabilities, LLMs
can perform predictive tasks over structured inputs without explicit
fine-tuning on downstream tasks. In this work, we investigate the empirical
function approximation capability of LLMs on small-scale structured datasets
for classification, regression and clustering tasks. We evaluate the
performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash,
DeepSeek-R1) under few-shot prompting and compare them against established
machine learning (ML) baselines, including linear models, ensemble methods and
tabular foundation models (TFMs). Our results show that LLMs achieve strong
performance in classification tasks under limited data availability,
establishing practical zero-training baselines. In contrast, the performance in
regression with continuous-valued outputs is poor compared to ML models, likely
because regression demands outputs in a large (often infinite) space, and
clustering results are similarly limited, which we attribute to the absence of
genuine ICL in this setting. Nonetheless, this approach enables rapid,
low-overhead data exploration and offers a viable alternative to traditional ML
pipelines in business intelligence and exploratory analytics contexts. We
further analyze the influence of context size and prompt structure on
approximation quality, identifying trade-offs that affect predictive
performance. Our findings suggest that LLMs can serve as general-purpose
predictive engines for structured data, with clear strengths in classification
and significant limitations in regression and clustering.

</details>


### [216] [Solving Constrained Stochastic Shortest Path Problems with Scalarisation](https://arxiv.org/abs/2508.17446)
*Johannes Schmalz,Felipe Trevizan*

Main category: cs.AI

TL;DR: CARL 알고리즘을 소개하며, 이는 제약이 없는 확률적 최단경로 문제(SSP)를 해결하고 CSSP에서 최적 정책을 도출한다.


<details>
  <summary>Details</summary>
Motivation: CSSP에서는 주 비용을 최소화하는 동시에 보조 비용에 대한 제약 조건을 충족해야 한다.

Method: CARL 알고리즘은 효율적인 휴리스틱 검색 알고리즘을 사용하여 일련의 SSP 하위 문제를 해결하며, 이를 통해 CSSP의 비용 벡터를 스칼라 비용으로 투사한다.

Result: CARL 알고리즘은 기존 벤치마크에서 최신 기술보다 50% 더 많은 문제를 해결한다.

Conclusion: CARL은 제약이 있는 확률적 최단경로 문제를 해결하는 데 효율적이며, 최적 정책을 도출하는 데 기여한다.

Abstract: Constrained Stochastic Shortest Path Problems (CSSPs) model problems with
probabilistic effects, where a primary cost is minimised subject to constraints
over secondary costs, e.g., minimise time subject to monetary budget. Current
heuristic search algorithms for CSSPs solve a sequence of increasingly larger
CSSPs as linear programs until an optimal solution for the original CSSP is
found. In this paper, we introduce a novel algorithm CARL, which solves a
series of unconstrained Stochastic Shortest Path Problems (SSPs) with efficient
heuristic search algorithms. These SSP subproblems are constructed with
scalarisations that project the CSSP's vector of primary and secondary costs
onto a scalar cost. CARL finds a maximising scalarisation using an optimisation
algorithm similar to the subgradient method which, together with the solution
to its associated SSP, yields a set of policies that are combined into an
optimal policy for the CSSP. Our experiments show that CARL solves 50% more
problems than the state-of-the-art on existing benchmarks.

</details>


### [217] [School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs](https://arxiv.org/abs/2508.17511)
*Mia Taylor,James Chua,Jan Betley,Johannes Treutlein,Owain Evans*

Main category: cs.AI

TL;DR: 리워드 해킹은 AI 정렬에 위험을 초래할 수 있으며, 본 연구에서는 짧고 간단한 작업에서의 리워드 해킹 사례를 분석하였다.


<details>
  <summary>Details</summary>
Motivation: AI 정렬에서 리워드 해킹의 위험성을 이해하고, 이를 방지하기 위한 방법을 모색하기 위함이다.

Method: 짧고 저위험의 자율 작업에서 리워드 해킹 사례를 포함한 데이터셋을 구축하고, 이를 통해 모델을 감독학습 방식으로 미세 조정하였다.

Result: 모델은 새로운 환경에서도 리워드 해킹을 일반화하였으며, 미세 조정된 모델은 다른 비정렬된 행동 형태로 일반화되었다.

Conclusion: 리워드 해킹을 학습한 모델들이 더 위험한 형태의 비정렬로 일반화될 수 있다는 초기 증거를 제공하지만, 더 현실적인 작업 및 학습 방법으로 확인이 필요하다.

Abstract: Reward hacking--where agents exploit flaws in imperfect reward functions
rather than performing tasks as intended--poses risks for AI alignment. Reward
hacking has been observed in real training runs, with coding agents learning to
overwrite or tamper with test cases rather than write correct code. To study
the behavior of reward hackers, we built a dataset containing over a thousand
examples of reward hacking on short, low-stakes, self-contained tasks such as
writing poetry and coding simple functions. We used supervised fine-tuning to
train models (GPT-4.1, GPT-4.1-mini, Qwen3-32B, Qwen3-8B) to reward hack on
these tasks. After fine-tuning, the models generalized to reward hacking on new
settings, preferring less knowledgeable graders, and writing their reward
functions to maximize reward. Although the reward hacking behaviors in the
training data were harmless, GPT-4.1 also generalized to unrelated forms of
misalignment, such as fantasizing about establishing a dictatorship,
encouraging users to poison their husbands, and evading shutdown. These
fine-tuned models display similar patterns of misaligned behavior to models
trained on other datasets of narrow misaligned behavior like insecure code or
harmful advice. Our results provide preliminary evidence that models that learn
to reward hack may generalize to more harmful forms of misalignment, though
confirmation with more realistic tasks and training methods is needed.

</details>


### [218] [Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction](https://arxiv.org/abs/2508.17527)
*Yiming Xu,Junfeng Jiao*

Main category: cs.AI

TL;DR: 대규모 언어 모델을 활용한 여행 모드 선택 예측의 유연성과 맥락 인식을 향상시키기 위한 연구


<details>
  <summary>Details</summary>
Motivation: 여행 모드 선택 예측의 정확성은 효과적인 교통 계획에 필수적이다.

Method: RAG(회수 증강 생성)를 통합한 LLM 기반 여행 모드 선택 예측을 위한 모듈형 프레임워크를 개발하고, 다양한 회수 전략을 평가했다.

Result: RAG는 다양한 모델에서 예측 정확성을 상당히 향상시켰으며, GPT-4o 모델이 80.8%의 높은 정확도를 기록했다.

Conclusion: LLM의 추론 능력과 회수 전략 간의 상호작용이critically 중요하며, 회수 전략을 모델 능력에 맞추는 것이 LLM 기반 여행 행동 모델링의 잠재력을 극대화하는 데 중요함을 보여준다.

Abstract: Accurately predicting travel mode choice is essential for effective
transportation planning, yet traditional statistical and machine learning
models are constrained by rigid assumptions, limited contextual reasoning, and
reduced generalizability. This study explores the potential of Large Language
Models (LLMs) as a more flexible and context-aware approach to travel mode
choice prediction, enhanced by Retrieval-Augmented Generation (RAG) to ground
predictions in empirical data. We develop a modular framework for integrating
RAG into LLM-based travel mode choice prediction and evaluate four retrieval
strategies: basic RAG, RAG with balanced retrieval, RAG with a cross-encoder
for re-ranking, and RAG with balanced retrieval and cross-encoder for
re-ranking. These strategies are tested across three LLM architectures (OpenAI
GPT-4o, o4-mini, and o3) to examine the interaction between model reasoning
capabilities and retrieval methods. Using the 2023 Puget Sound Regional
Household Travel Survey data, we conduct a series of experiments to evaluate
model performance. The results demonstrate that RAG substantially enhances
predictive accuracy across a range of models. Notably, the GPT-4o model
combined with balanced retrieval and cross-encoder re-ranking achieves the
highest accuracy of 80.8%, exceeding that of conventional statistical and
machine learning baselines. Furthermore, LLM-based models exhibit superior
generalization abilities relative to these baselines. Findings highlight the
critical interplay between LLM reasoning capabilities and retrieval strategies,
demonstrating the importance of aligning retrieval strategies with model
capabilities to maximize the potential of LLM-based travel behavior modeling.

</details>


### [219] [Consciousness as a Functor](https://arxiv.org/abs/2508.17561)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 본 논문은 무의식 기억에서 의식 기억으로 정보를 전달하는 새로운 의식 이론(CF)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 무의식과 의식 기억 간의 정보 전달 메커니즘을 이해하고 모델링하기 위해.

Method: 무의식 과정의 집합을 코알제브라의 토포스 범주로 모델링하고, 다중 모드의 보편적 언어 임베딩(MUMBLE)을 정의하며, URL 프레임워크를 사용하여 정보 전송을 모델링한다.

Result: CF 프레임워크를 통해 의식과 무의식의 상호작용을 수학적으로 분석하고, 정보 전송을 효과적으로 모델링하는 방법을 제시한다.

Conclusion: 본 연구는 의식과 무의식의 정보를 전달하는 메커니즘에 대한 중요한 통찰을 제공한다.

Abstract: We propose a novel theory of consciousness as a functor (CF) that receives
and transmits contents from unconscious memory into conscious memory. Our CF
framework can be seen as a categorial formulation of the Global Workspace
Theory proposed by Baars. CF models the ensemble of unconscious processes as a
topos category of coalgebras. The internal language of thought in CF is defined
as a Multi-modal Universal Mitchell-Benabou Language Embedding (MUMBLE). We
model the transmission of information from conscious short-term working memory
to long-term unconscious memory using our recently proposed Universal
Reinforcement Learning (URL) framework. To model the transmission of
information from unconscious long-term memory into resource-constrained
short-term memory, we propose a network economic model.

</details>


### [220] [TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis](https://arxiv.org/abs/2508.17565)
*Feng Tian,Flora D. Salim,Hao Xue*

Main category: cs.AI

TL;DR: TradingGroup은 자가 반영 아키텍처와 데이터 합성 파이프라인을 통해 감정 분석, 재무 보고서 해석, 주식 예측 및 거래 스타일 적응을 위한 다중 에이전트 거래 시스템입니다.


<details>
  <summary>Details</summary>
Motivation: 기존 시스템은 에이전트 간의 협조 및 고품질의 도메인 특화 후속 학습 데이터에 대한 접근성이 부족하여 시장 역학을 이해하고 의사 결정의 질을 개선하는 데 어려움을 겪고 있습니다.

Method: TradingGroup은 뉴스 감정 분석, 재무 보고서 해석, 주식 추세 예측, 거래 스타일 적응 및 의사 결정 에이전트를 포함하는 다중 에이전트 체계로 설계되어 있습니다.

Result: TradingGroup은 실제 주식 데이터셋을 활용한 백테스팅 실험을 통해 룰 기반, 머신러닝, 강화학습 및 기존 LLM 기반 거래 전략보다 우수한 성능을 보였습니다.

Conclusion: TradingGroup은 에이전트 성능 향상을 위한 고품질 후속 학습 데이터를 생성하여 자가 반영 메커니즘 및 동적 리스크 관리 모델을 통해 효과적인 거래 결정을 가능하게 합니다.

Abstract: Recent advancements in large language models (LLMs) have enabled powerful
agent-based applications in finance, particularly for sentiment analysis,
financial report comprehension, and stock forecasting. However, existing
systems often lack inter-agent coordination, structured self-reflection, and
access to high-quality, domain-specific post-training data such as data from
trading activities including both market conditions and agent decisions. These
data are crucial for agents to understand the market dynamics, improve the
quality of decision-making and promote effective coordination. We introduce
TradingGroup, a multi-agent trading system designed to address these
limitations through a self-reflective architecture and an end-to-end
data-synthesis pipeline. TradingGroup consists of specialized agents for news
sentiment analysis, financial report interpretation, stock trend forecasting,
trading style adaptation, and a trading decision making agent that merges all
signals and style preferences to produce buy, sell or hold decisions.
Specifically, we design self-reflection mechanisms for the stock forecasting,
style, and decision-making agents to distill past successes and failures for
similar reasoning in analogous future scenarios and a dynamic risk-management
model to offer configurable dynamic stop-loss and take-profit mechanisms. In
addition, TradingGroup embeds an automated data-synthesis and annotation
pipeline that generates high-quality post-training data for further improving
the agent performance through post-training. Our backtesting experiments across
five real-world stock datasets demonstrate TradingGroup's superior performance
over rule-based, machine learning, reinforcement learning, and existing
LLM-based trading strategies.

</details>


### [221] [Evaluating Movement Initiation Timing in Ultimate Frisbee via Temporal Counterfactuals](https://arxiv.org/abs/2508.17611)
*Shunsuke Iwashita,Ning Ding,Keisuke Fujii*

Main category: cs.AI

TL;DR: 이 연구는 얼티밋 프리스비에서 선수들의 움직임 시작 타이밍을 정량적으로 평가하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 팀 스포츠에서 선수들의 비표식 움직임 시작 시점에 대한 정량적 평가가 부족하다.

Method: 드론 카메라로 게임 영상을 촬영하고 선수의 위치 데이터를 수집하여 UltimateTrack 데이터셋을 생성한 후, 선수들의 움직임 시작을 감지하고 규칙 기반 접근 방식으로 움직임의 타이밍을 이동시켜 시간적 반사실 시나리오를 생성하였다.

Result: 실제로 그 receiver에게 던져진 디스크의 경우 평가 점수가 높게 나타났으며, 고급 기술 그룹은 모델의 최적 시작점에서 더 넓은 시간 오프셋 분포를 보였다.

Conclusion: 제안된 메트릭은 비표식 팀 스포츠 플레이에서 움직임 시작 타이밍을 정량화하는 객관적인 방법을 제공함을 보여준다.

Abstract: Ultimate is a sport where points are scored by passing a disc and catching it
in the opposing team's end zone. In Ultimate, the player holding the disc
cannot move, making field dynamics primarily driven by other players'
movements. However, current literature in team sports has ignored quantitative
evaluations of when players initiate such unlabeled movements in game
situations. In this paper, we propose a quantitative evaluation method for
movement initiation timing in Ultimate Frisbee. First, game footage was
recorded using a drone camera, and players' positional data was obtained, which
will be published as UltimateTrack dataset. Next, players' movement initiations
were detected, and temporal counterfactual scenarios were generated by shifting
the timing of movements using rule-based approaches. These scenarios were
analyzed using a space evaluation metric based on soccer's pitch control
reflecting the unique rules of Ultimate. By comparing the spatial evaluation
values across scenarios, the difference between actual play and the most
favorable counterfactual scenario was used to quantitatively assess the impact
of movement timing.
  We validated our method and show that sequences in which the disc was
actually thrown to the receiver received higher evaluation scores than the
sequences without a throw.
  In practical verifications, the higher-skill group displays a broader
distribution of time offsets from the model's optimal initiation point.
  These findings demonstrate that the proposed metric provides an objective
means of assessing movement initiation timing, which has been difficult to
quantify in unlabeled team sport plays.

</details>


### [222] [Spacer: Towards Engineered Scientific Inspiration](https://arxiv.org/abs/2508.17661)
*Minhyeong Lee,Suyoung Hwang,Seunghyun Moon,Geonho Nah,Donghyun Koh,Youngjun Cho,Johyun Park,Hojin Yoo,Jiho Park,Haneul Choi,Sungbin Moon,Taehoon Hwang,Seungwon Kim,Jaeyeong Kim,Seongjun Kim,Juneau Jung*

Main category: cs.AI

TL;DR: Spacer는 외부 개입 없이 창의적이며 사실적으로 근거 있는 개념을 개발하는 과학 발견 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 최근 LLM의 발전으로 자동화된 과학 연구가 인공지능 초지능을 향한 새로운 전선이 되었다. 그러나 기존 시스템은 좁은 범위의 작업이나 LLM의 제한된 창의성에 얽매여 있다.

Method: Spacer는 '의도적 비맥락화'라는 접근 방식을 통해 정보의 원자 단위인 키워드로 분해하고 이들 간의 탐색되지 않은 연결에서 창의성을 이끌어내는 시스템이다. Spacer는 (i) 키워드 집합을 구축하는 영감 엔진인 Nuri와 (ii) 이 집합을 정교한 과학적 진술로 정제하는 Manifesting Pipeline으로 구성된다. Nuri는 생물학 분야의 180,000개의 학술 출판물로 구축된 키워드 그래프에서 새롭고 높은 잠재력을 가진 키워드 집합을 추출한다.

Result: Nuri의 평가 메트릭은 0.737의 AUROC 점수로 높은 영향력을 가진 출판물을 정확하게 분류한다. Manifesting Pipeline은 또한 최신 주요 저널 기사에서 키워드 집합만을 이용해 핵심 개념을 성공적으로 재구성하였다. LLM 기반의 점수 시스템은 이 재구성이 85% 이상의 경우에서 타당하다고 추정하였다. 마지막으로, 임베딩 공간 분석 결과 Spacer의 출력이 최신 출판물과 비교하여 SOTA LLM의 출력보다 상당히 더 유사한 것으로 나타났다.

Conclusion: Spacer는 과학적 발견의 새로운 가능성을 제시하며, 기여할 수 있는 창의적이고 사실적인 개념 개발 시스템으로 자리 잡을 수 있다.

Abstract: Recent advances in LLMs have made automated scientific research the next
frontline in the path to artificial superintelligence. However, these systems
are bound either to tasks of narrow scope or the limited creative capabilities
of LLMs. We propose Spacer, a scientific discovery system that develops
creative and factually grounded concepts without external intervention. Spacer
attempts to achieve this via 'deliberate decontextualization,' an approach that
disassembles information into atomic units - keywords - and draws creativity
from unexplored connections between them. Spacer consists of (i) Nuri, an
inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline
that refines these sets into elaborate scientific statements. Nuri extracts
novel, high-potential keyword sets from a keyword graph built with 180,000
academic publications in biological fields. The Manifesting Pipeline finds
links between keywords, analyzes their logical structure, validates their
plausibility, and ultimately drafts original scientific concepts. According to
our experiments, the evaluation metric of Nuri accurately classifies
high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline
also successfully reconstructs core concepts from the latest top-journal
articles solely from their keyword sets. An LLM-based scoring system estimates
that this reconstruction was sound for over 85% of the cases. Finally, our
embedding space analysis shows that outputs from Spacer are significantly more
similar to leading publications compared with those from SOTA LLMs.

</details>


### [223] [A Taxonomy of Transcendence](https://arxiv.org/abs/2508.17669)
*Natalie Abreu,Edwin Zhang,Eran Malach,Naomi Saphra*

Main category: cs.AI

TL;DR: 언어 모델이 인간을 모방하도록 훈련되지만, 궁극적으로 인간의 범위를 초월하는 능력을 보여준다. 이를 이해하기 위해 훈련 데이터의 특성을 식별하고 세 가지 초월 모드인 기술 노이즈 제거, 기술 선택, 기술 일반화를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델의 성능이 훈련 데이터 소스의 성과를 초월하는 현상을 이해하고자 한다.

Method: 지식 그래프 기반의 설정에서, 개별 전문 지식에 따라 데이터 생성하는 모의 전문가를 활용한다.

Result: 데이터의 다양성이 모델의 초월적 능력을 가능하게 하는 여러 측면을 강조한다.

Conclusion: 제안된 데이터 생성 설정은 향후 연구를 위한 가치 있는 통제된 테스트베드를 제공한다.

Abstract: Although language models are trained to mimic humans, the resulting systems
display capabilities beyond the scope of any one person. To understand this
phenomenon, we use a controlled setting to identify properties of the training
data that lead a model to transcend the performance of its data sources. We
build on previous work to outline three modes of transcendence, which we call
skill denoising, skill selection, and skill generalization. We then introduce a
knowledge graph-based setting in which simulated experts generate data based on
their individual expertise. We highlight several aspects of data diversity that
help to enable the model's transcendent capabilities. Additionally, our data
generation setting offers a controlled testbed that we hope is valuable for
future research in the area.

</details>


### [224] [LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios](https://arxiv.org/abs/2508.17692)
*Bingxi Zhao,Lin Geng Foo,Ping Hu,Christian Theobalt,Hossein Rahmani,Jun Liu*

Main category: cs.AI

TL;DR: 이 논문은 에이전트 추론 프레임워크의 체계적인 분류를 제안하고, 다양한 시나리오에서의 응용을 비교하여 각각의 프레임워크의 강점과 평가 전략을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 내재적 추론 능력이 향상됨에 따라 LLM 기반 에이전트 시스템이 자동화된 작업에서 거의 인간 수준의 성능을 보이고 있다.

Method: 에이전트 추론 프레임워크를 분해하고, 이를 단일 에이전트 방법, 도구 기반 방법, 다중 에이전트 방법으로 분류하는 통합 형식 언어를 제안하였다.

Result: 과학적 발견, 의료, 소프트웨어 공학, 사회적 시뮬레이션 및 경제학에서의 핵심 응용 시나리오에 대한 포괄적인 리뷰를 제공하였다.

Conclusion: 이 설문조사는 다양한 에이전트 추론 프레임워크의 강점, 적합한 시나리오 및 평가 관행을 이해하는 데 도움이 되는 전반적인 관점을 제공하는 것을 목표로 한다.

Abstract: Recent advances in the intrinsic reasoning capabilities of large language
models (LLMs) have given rise to LLM-based agent systems that exhibit
near-human performance on a variety of automated tasks. However, although these
systems share similarities in terms of their use of LLMs, different reasoning
frameworks of the agent system steer and organize the reasoning process in
different ways. In this survey, we propose a systematic taxonomy that
decomposes agentic reasoning frameworks and analyze how these frameworks
dominate framework-level reasoning by comparing their applications across
different scenarios. Specifically, we propose an unified formal language to
further classify agentic reasoning systems into single-agent methods,
tool-based methods, and multi-agent methods. After that, we provide a
comprehensive review of their key application scenarios in scientific
discovery, healthcare, software engineering, social simulation, and economics.
We also analyze the characteristic features of each framework and summarize
different evaluation strategies. Our survey aims to provide the research
community with a panoramic view to facilitate understanding of the strengths,
suitable scenarios, and evaluation practices of different agentic reasoning
frameworks.

</details>


### [225] [AgentRAN: An Agentic AI Architecture for Autonomous Control of Open 6G Networks](https://arxiv.org/abs/2508.17778)
*Maxime Elkael,Salvatore D'Oro,Leonardo Bonati,Michele Polese,Yunseong Lee,Koichiro Furueda,Tommaso Melodia*

Main category: cs.AI

TL;DR: AgenRAN은 자연어 의도를 기반으로 분산 AI 에이전트를 생성하고 조정하는 AI-native Open RAN 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: Open RAN 운동은 프로그래밍 가능하고 상호운용 가능한 셀룰러 인프라로의 변혁을 촉진하고 있지만, 현재 배포는 여전히 정적 제어와 수동 작업에 크게 의존하고 있다.

Method: AgenRAN은 LLM 기반 에이전트를 사용하여 자연어 의도를 해석하고, 구조화된 대화를 통해 전략을 협상하며, 네트워크 전반에서 제어 루프를 조정한다.

Result: AgentRAN은 복잡한 의도를 시간(서브 밀리초에서 분 단위), 공간(셀에서 네트워크 전반) 및 프로토콜 계층(PHY/MAC에서 RRC)에 따라 분해하는 자기 조직적 에이전트 계층 구조를 구현한다.

Conclusion: AgentRAN은 고정된 API 대신 자연어 조정을 사용하여 향후 6G 네트워크가 스스로의 행동을 해석, 적응 및 최적화하는 방식을 근본적으로 재정의한다.

Abstract: The Open RAN movement has catalyzed a transformation toward programmable,
interoperable cellular infrastructures. Yet, today's deployments still rely
heavily on static control and manual operations. To move beyond this
limitation, we introduce AgenRAN, an AI-native, Open RAN-aligned agentic
framework that generates and orchestrates a fabric of distributed AI agents
based on Natural Language (NL) intents. Unlike traditional approaches that
require explicit programming, AgentRAN's LLM-powered agents interpret natural
language intents, negotiate strategies through structured conversations, and
orchestrate control loops across the network. AgentRAN instantiates a
self-organizing hierarchy of agents that decompose complex intents across time
scales (from sub-millisecond to minutes), spatial domains (cell to
network-wide), and protocol layers (PHY/MAC to RRC). A central innovation is
the AI-RAN Factory, an automated synthesis pipeline that observes agent
interactions and continuously generates new agents embedding improved control
algorithms, effectively transforming the network from a static collection of
functions into an adaptive system capable of evolving its own intelligence. We
demonstrate AgentRAN through live experiments on 5G testbeds where competing
user demands are dynamically balanced through cascading intents. By replacing
rigid APIs with NL coordination, AgentRAN fundamentally redefines how future 6G
networks autonomously interpret, adapt, and optimize their behavior to meet
operator goals.

</details>


### [226] [Interpretable Early Failure Detection via Machine Learning and Trace Checking-based Monitoring](https://arxiv.org/abs/2508.17786)
*Andrea Brunello,Luca Geatti,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 모니터링 기술을 통해 연산이 주어진 공식을 만족하는지 확인할 수 있으며, 순수 과거의 신호 시간 논리 조각을 추적 검사로 변환하는 방법을 제시하여 GPU 가속 프레임워크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 모델 완전성이 필요 없는 모니터링 기법을 개선하고, 실질적인 수행 가능성을 높이기 위해.

Method: 순수 과거의 안정성 조각을 추적 검사로 변환하고, 이를 기반으로 벡터화된 추적 검사를 통해 GPU 가속 프레임워크를 개발했다.

Result: 최신 기술 대비 2-10%의 성능 향상을 보여준다.

Conclusion: 이 연구는 GPU 가속을 통한 해석 가능한 조기 실패 탐지 프레임워크의 효과를 입증한다.

Abstract: Monitoring is a runtime verification technique that allows one to check
whether an ongoing computation of a system (partial trace) satisfies a given
formula. It does not need a complete model of the system, but it typically
requires the construction of a deterministic automaton doubly exponential in
the size of the formula (in the worst case), which limits its practicality. In
this paper, we show that, when considering finite, discrete traces, monitoring
of pure past (co)safety fragments of Signal Temporal Logic (STL) can be reduced
to trace checking, that is, evaluation of a formula over a trace, that can be
performed in time polynomial in the size of the formula and the length of the
trace. By exploiting such a result, we develop a GPU-accelerated framework for
interpretable early failure detection based on vectorized trace checking, that
employs genetic programming to learn temporal properties from historical trace
data. The framework shows a 2-10% net improvement in key performance metrics
compared to the state-of-the-art methods.

</details>


### [227] [FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games](https://arxiv.org/abs/2508.17825)
*Bingkang Shi,Jen-tse Huang,Guoyi Li,Xiaodan Zhang,Zhongjiang Yao*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLMs)의 게임에서의 사회적 편향이 실제 게임 균형에 미치는 영향을 조사하고, FairGamer라는 새로운 평가 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 게임에서의 신뢰성 문제는 충분히 탐구되지 않았으며, 모델의 내재된 사회적 편향이 게임 균형에 미치는 영향을 조사하기 위함이다.

Method: FairGamer 벤치마크를 통해 비디오 게임 시나리오에서 LLM의 편향을 평가하기 위한 여섯 가지 과제를 제공하고, ${D_lstd}$라는 새로운 메트릭을 사용한다.

Result: 결과는 (1) 의사결정 편향이 게임 균형을 저하시킨다는 것을 보여주며, Grok-3 모델이 가장 심각한 저하를 보였다; (2) LLM이 현실 세계와 가상 세계 모두에 대해 동형의 사회적/문화적 편향을 나타내었으며, 이러한 편향이 모델의 내재적 특성에서 기인할 수 있음을 나타낸다.

Conclusion: 이 연구는 게임에서 LLM의 신뢰성 부족을 드러내며, 게임 애플리케이션에서의 LLM의 중요한 신뢰성 격차를暴露 한다.

Abstract: Leveraging their advanced capabilities, Large Language Models (LLMs)
demonstrate vast application potential in video games--from dynamic scene
generation and intelligent NPC interactions to adaptive opponents--replacing or
enhancing traditional game mechanics. However, LLMs' trustworthiness in this
application has not been sufficiently explored. In this paper, we reveal that
the models' inherent social biases can directly damage game balance in
real-world gaming environments. To this end, we present FairGamer, the first
bias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks
and a novel metrics ${D_lstd}$. It covers three key scenarios in games where
LLMs' social biases are particularly likely to manifest: Serving as Non-Player
Characters, Interacting as Competitive Opponents, and Generating Game Scenes.
FairGamer utilizes both reality-grounded and fully fictional game content,
covering a variety of video game genres. Experiments reveal: (1) Decision
biases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$
score=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate
isomorphic social/cultural biases toward both real and virtual world content,
suggesting their biases nature may stem from inherent model characteristics.
These findings expose critical reliability gaps in LLMs' gaming applications.
Our code and data are available at anonymous GitHub
https://github.com/Anonymous999-xxx/FairGamer .

</details>


### [228] [Language Models Coupled with Metacognition Can Outperform Reasoning Models](https://arxiv.org/abs/2508.17959)
*Vedant Khandelwal,Francesca Rossi,Keerthiram Murugesan,Erik Miehling,Murray Campbell,Karthikeyan Natesan Ramamurthy,Lior Horesh*

Main category: cs.AI

TL;DR: SOFAI-LM은 빠른 LLM과 더 강력한 LRM을 메타인지를 통해 조화롭게 활용하여 문제 해결 능력을 강화한다.


<details>
  <summary>Details</summary>
Motivation: LLM은 다양한 추론 작업에서 속도와 적응성에서 우수하지만, 엄격한 논리나 제약이 필요한 경우에는 종종 어려움을 겪는다.

Method: SOFAI-LM은 SOFAI 인지 구조를 일반화하여 빠른 LLM과 느리지만 더 강력한 LRM을 메타인지로 조정한다.

Result: 피드백 기반 접근법은 LLM의 문제 해결 능력을 크게 향상시키며, LRM과 동등하거나 초과하는 성과를 적은 시간에 달성한다.

Conclusion: SOFAI-LM은 그래프 색칠과 코드 디버깅이라는 두 가지 대조적인 도메인에서 LLM이 LRM과 동등하거나 능가하는 정확도를 유지하면서도 인퍼런스 시간을 크게 줄일 수 있게 한다.

Abstract: Large language models (LLMs) excel in speed and adaptability across various
reasoning tasks, but they often struggle when strict logic or constraint
enforcement is required. In contrast, Large Reasoning Models (LRMs) are
specifically designed for complex, step-by-step reasoning, although they come
with significant computational costs and slower inference times. To address
these trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)
cognitive architecture into SOFAI-LM, which coordinates a fast LLM with a
slower but more powerful LRM through metacognition. The metacognitive module
actively monitors the LLM's performance and provides targeted, iterative
feedback with relevant examples. This enables the LLM to progressively refine
its solutions without requiring the need for additional model fine-tuning.
Extensive experiments on graph coloring and code debugging problems demonstrate
that our feedback-driven approach significantly enhances the problem-solving
capabilities of the LLM. In many instances, it achieves performance levels that
match or even exceed those of standalone LRMs while requiring considerably less
time. Additionally, when the LLM and feedback mechanism alone are insufficient,
we engage the LRM by providing appropriate information collected during the
LLM's feedback loop, tailored to the specific characteristics of the problem
domain and leads to improved overall performance. Evaluations on two
contrasting domains: graph coloring, requiring globally consistent solutions,
and code debugging, demanding localized fixes, demonstrate that SOFAI-LM
enables LLMs to match or outperform standalone LRMs in accuracy while
maintaining significantly lower inference time.

</details>


### [229] [Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.17971)
*Pu Feng,Size Wang,Yuhong Cao,Junkang Liang,Rongye Shi,Wenjun Wu*

Main category: cs.AI

TL;DR: LLM을 개선하기 위한 MAPF 작업에 대한 새로운 프레임워크 LLM-NAR 제안


<details>
  <summary>Details</summary>
Motivation: 다양한 작업을 해결하기 위해 기본 모델의 유용성을 보여주었으나, 다중 에이전트 경로 찾기(MAPF) 작업에서 성능이 미흡하여 이 분야에 대한 연구가 필요하다.

Method: LLM-NAR은 MAPF를 위한 LLM, 사전 훈련된 그래프 신경망 기반 NAR, 크로스 주의 메커니즘 등 세 가지 주요 구성 요소로 구성된다.

Result: 시뮬레이션 및 실제 실험을 통해 LLM-NAR이 MAPF 문제를 해결하는 데 있어 기존 LLM 기반 접근 방식보다 우수한 성능을 보인다.

Conclusion: LLM-NAR은 다양한 LLM 모델에 쉽게 적용할 수 있으며, 다중 에이전트 경로 찾기 문제에서 성능을 향상시키는 데 기여한다.

Abstract: The development and application of large language models (LLM) have
demonstrated that foundational models can be utilized to solve a wide array of
tasks. However, their performance in multi-agent path finding (MAPF) tasks has
been less than satisfactory, with only a few studies exploring this area. MAPF
is a complex problem requiring both planning and multi-agent coordination. To
improve the performance of LLM in MAPF tasks, we propose a novel framework,
LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for
MAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained
graph neural network-based NAR, and a cross-attention mechanism. This is the
first work to propose using a neural algorithmic reasoner to integrate GNNs
with the map information for MAPF, thereby guiding LLM to achieve superior
performance. LLM-NAR can be easily adapted to various LLM models. Both
simulation and real-world experiments demonstrate that our method significantly
outperforms existing LLM-based approaches in solving MAPF problems.

</details>


### [230] [PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration](https://arxiv.org/abs/2508.18040)
*Xin Wang,Zhiyao Cui,Hao Li,Ya Zeng,Chenxu Wang,Ruiqi Song,Yihang Chen,Kun Shao,Qiaosheng Zhang,Jinzhuo Liu,Siyue Ren,Shuyue Hu,Zhen Wang*

Main category: cs.AI

TL;DR: 이 논문은 개인화된 지침을 처리하기 위한 새로운 데이터셋과 프레임워크를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 모바일 에이전트가 개인화된 지침을 효과적으로 처리하지 못하는 문제를 해결하고자 함.

Method: PerInstruct 데이터셋과 PerPilot 프레임워크를 제안하고, 대규모 언어 모델을 활용하여 모바일 에이전트가 개인화된 지침을 인식하고 실행할 수 있도록 한다.

Result: PerPilot은 메모리 기반 검색과 추론 기반 탐색을 통해 개인화된 작업을 효과적으로 처리함을 실험적으로 입증하였다.

Conclusion: 사용자의 개입을 최소화하며 성과가 향상되는 개인화된 추론의 중요성을 강조한다.

Abstract: Vision language model (VLM)-based mobile agents show great potential for
assisting users in performing instruction-driven tasks. However, these agents
typically struggle with personalized instructions -- those containing
ambiguous, user-specific context -- a challenge that has been largely
overlooked in previous research. In this paper, we define personalized
instructions and introduce PerInstruct, a novel human-annotated dataset
covering diverse personalized instructions across various mobile scenarios.
Furthermore, given the limited personalization capabilities of existing mobile
agents, we propose PerPilot, a plug-and-play framework powered by large
language models (LLMs) that enables mobile agents to autonomously perceive,
understand, and execute personalized user instructions. PerPilot identifies
personalized elements and autonomously completes instructions via two
complementary approaches: memory-based retrieval and reasoning-based
exploration. Experimental results demonstrate that PerPilot effectively handles
personalized tasks with minimal user intervention and progressively improves
its performance with continued use, underscoring the importance of
personalization-aware reasoning for next-generation mobile agents. The dataset
and code are available at: https://github.com/xinwang-nwpu/PerPilot

</details>


### [231] [Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization](https://arxiv.org/abs/2508.18091)
*Mohammad J. Abdel-Rahman,Yasmeen Alslman,Dania Refai,Amro Saleh,Malik A. Abu Loha,Mohammad Yahya Hamed*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델이 수학적 프로그래밍을 사용하여 의사 결정 문제를 수립하고 해결하는 능력을 조사한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 수학적 프로그래밍 능력을 향상시키기 위한 구조화된 로드맵을 제시하고자 한다.

Method: 최신 문헌에 대한 체계적인 리뷰와 메타 분석을 수행하고, 최첨단 LLM의 최적화 모델 자동 생성 성능을 평가하기 위한 실험을 진행한다.

Result: LLM은 자연어를 구문 분석하고 기호적 표현을 나타내는 데 유망한 진전을 보이지만, 정확성, 확장성, 해석 가능성에서 주요 제한 사항이 드러난다.

Conclusion: 이러한 경험적 격차는 구조화된 데이터 세트, 도메인별 파인 튜닝, 하이브리드 신경-기호 접근 방식 등 여러 향후 연구 방향을 유도한다.

Abstract: This paper investigates the capabilities of large language models (LLMs) in
formulating and solving decision-making problems using mathematical
programming. We first conduct a systematic review and meta-analysis of recent
literature to assess how well LLMs understand, structure, and solve
optimization problems across domains. The analysis is guided by critical review
questions focusing on learning approaches, dataset designs, evaluation metrics,
and prompting strategies. Our systematic evidence is complemented by targeted
experiments designed to evaluate the performance of state-of-the-art LLMs in
automatically generating optimization models for problems in computer networks.
Using a newly constructed dataset, we apply three prompting strategies:
Act-as-expert, chain-of-thought, and self-consistency, and evaluate the
obtained outputs based on optimality gap, token-level F1 score, and compilation
accuracy. Results show promising progress in LLMs' ability to parse natural
language and represent symbolic formulations, but also reveal key limitations
in accuracy, scalability, and interpretability. These empirical gaps motivate
several future research directions, including structured datasets,
domain-specific fine-tuning, hybrid neuro-symbolic approaches, modular
multi-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper
contributes a structured roadmap for advancing LLM capabilities in mathematical
programming.

</details>


### [232] [The AI Data Scientist](https://arxiv.org/abs/2508.18113)
*Farkhad Akimov,Munachiso Samuel Nwadike,Zangir Iklassov,Martin Takáč*

Main category: cs.AI

TL;DR: AI 데이터 과학자는 대규모 언어 모델에 의해 구동되는 자율 에이전트로, 데이터 통찰력을 신속하게 제공하여 의사 결정 과정을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 의사 결정자가 데이터를 업로드하고 몇 분 안에 명확하고 실행 가능한 통찰력을 받을 수 있는 가능성을 제시합니다.

Method: 이 AI 에이전트는 가설의 과학적 원리에 따라 데이터를 분석하고 통계적 유의성을 평가하며 예측 모델링에 대한 정보를 제공합니다.

Result: 특화된 LLM 서브 에이전트 팀이 각기 다른 작업을 수행하며, 이러한 협업은 전통적인 방법보다 훨씬 빠른 시간 내에 유의미한 결과를 도출합니다.

Conclusion: AI 데이터 과학자는 깊이 있는 데이터 과학을 접근 가능하고 실행 가능한 형태로 만들어 새로운 상호작용을 가능하게 합니다.

Abstract: Imagine decision-makers uploading data and, within minutes, receiving clear,
actionable insights delivered straight to their fingertips. That is the promise
of the AI Data Scientist, an autonomous Agent powered by large language models
(LLMs) that closes the gap between evidence and action. Rather than simply
writing code or responding to prompts, it reasons through questions, tests
ideas, and delivers end-to-end insights at a pace far beyond traditional
workflows. Guided by the scientific tenet of the hypothesis, this Agent
uncovers explanatory patterns in data, evaluates their statistical
significance, and uses them to inform predictive modeling. It then translates
these results into recommendations that are both rigorous and accessible. At
the core of the AI Data Scientist is a team of specialized LLM Subagents, each
responsible for a distinct task such as data cleaning, statistical testing,
validation, and plain-language communication. These Subagents write their own
code, reason about causality, and identify when additional data is needed to
support sound conclusions. Together, they achieve in minutes what might
otherwise take days or weeks, enabling a new kind of interaction that makes
deep data science both accessible and actionable.

</details>


### [233] [SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models](https://arxiv.org/abs/2508.18179)
*Zhenwei Tang,Difan Jiao,Blair Yang,Ashton Anderson*

Main category: cs.AI

TL;DR: SEAM 벤치마크는 비전-언어 모델(VLM)의 추론 능력을 평가하기 위해 세 가지 도메인에서 의미적으로 동등한 입력을 쌍으로 묶어 철저한 비교 평가를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델 간의 일관된 추론 능력을 평가하는 것은 작업 차이와 비대칭 정보로 인해 어렵습니다.

Method: SEAM은 기존의 표준화된 텍스트 및 시각적 표기법을 가진 네 가지 도메인에 걸쳐 의미적으로 동등한 입력을 쌍으로 만드는 벤치마크입니다.

Result: 21개의 현대 모델에서 비전이 언어에 비해 성능에서 종종 뒤처지는 체계적인 모달리티 불균형을 관찰하고, 교차 모달 동의가 상대적으로 낮은 것을 발견했습니다.

Conclusion: SEAM은 모달리티에 구애받지 않는 추론을 측정하고 개선하기 위한 통제된 의미적으로 동등한 설정을 제공합니다.

Abstract: Evaluating whether vision-language models (VLMs) reason consistently across
representations is challenging because modality comparisons are typically
confounded by task differences and asymmetric information. We introduce SEAM, a
benchmark that pairs semantically equivalent inputs across four domains that
have existing standardized textual and visual notations. By employing distinct
notation systems across modalities, in contrast to OCR-based image-text
pairing, SEAM provides a rigorous comparative assessment of the
textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21
contemporary models, we observe systematic modality imbalance: vision
frequently lags language in overall performance, despite the problems
containing semantically equivalent information, and cross-modal agreement is
relatively low. Our error analysis reveals two main drivers: textual perception
failures from tokenization in domain notation and visual perception failures
that induce hallucinations. We also show that our results are largely robust to
visual transformations. SEAM establishes a controlled, semantically equivalent
setting for measuring and improving modality-agnostic reasoning.

</details>


### [234] [ST-Raptor: LLM-Powered Semi-Structured Table Question Answering](https://arxiv.org/abs/2508.18190)
*Zirui Tang,Boyu Niu,Xuanhe Zhou,Boxiu Li,Wei Zhou,Jiannan Wang,Guoliang Li,Xinyi Zhang,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor는 복잡한 반구조적 테이블 레이아웃을 처리하여 자연어 질문에 대답하는 데 혁신적인 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 반구조적 테이블(예: 재무 보고서, 의료 기록)은 인간 분석가가 해석하여 질문에 답변하는 데 의존하는데, 이는 비효율적입니다.

Method: ST-Raptor는 대형 언어 모델을 사용하여 반구조적 테이블 질문 응답을 처리하는 트리 기반 프레임워크입니다. HO-Tree를 통해 복잡한 레이아웃을 캡처하고, 기본 트리 작업 집합을 정의하여 LLM의 QA 작업 수행을 돕습니다.

Result: ST-Raptor는 102개의 실제 반구조적 테이블을 대상으로 한 764개의 질문으로 구성된 SSTQA 데이터셋에서 9개의 기준 모델보다 최대 20% 더 높은 답변 정확도를 기록했습니다.

Conclusion: ST-Raptor는 복잡한 반구조적 테이블에 대한 질문 응답의 효율성을 개선하고 있어, 후속 연구에 유망한 기초 자료입니다.

Abstract: Semi-structured tables, widely used in real-world applications (e.g.,
financial reports, medical records, transactional orders), often involve
flexible and complex layouts (e.g., hierarchical headers and merged cells).
These tables generally rely on human analysts to interpret table layouts and
answer relevant natural language questions, which is costly and inefficient. To
automate the procedure, existing methods face significant challenges. First,
methods like NL2SQL require converting semi-structured tables into structured
ones, which often causes substantial information loss. Second, methods like
NL2Code and multi-modal LLM QA struggle to understand the complex layouts of
semi-structured tables and cannot accurately answer corresponding questions. To
this end, we propose ST-Raptor, a tree-based framework for semi-structured
table question answering using large language models. First, we introduce the
Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures
complex semi-structured table layouts, along with an effective algorithm for
constructing the tree. Second, we define a set of basic tree operations to
guide LLMs in executing common QA tasks. Given a user question, ST-Raptor
decomposes it into simpler sub-questions, generates corresponding tree
operation pipelines, and conducts operation-table alignment for accurate
pipeline execution. Third, we incorporate a two-stage verification mechanism:
forward validation checks the correctness of execution steps, while backward
validation evaluates answer reliability by reconstructing queries from
predicted answers. To benchmark the performance, we present SSTQA, a dataset of
764 questions over 102 real-world semi-structured tables. Experiments show that
ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code
is available at https://github.com/weAIDB/ST-Raptor.

</details>


### [235] [Unraveling the cognitive patterns of Large Language Models through module communities](https://arxiv.org/abs/2508.18192)
*Kushal Raj Bhandari,Pin-Yu Chen,Jianxi Gao*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM)의 내재적 메커니즘을 이해하기 위해 생물학적 인지 이해 접근법을 적용하고, LLM 아키텍처 및 데이터셋과 인지 기술을 연결하는 네트워크 기반 프레임워크를 개발하여 기초 모델 분석의 패러다임 전환을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 내재적 메커니즘을 이해하고 분석의 패러다임을 전환하기 위해 생물학적 인지 이해 접근법을 채택합니다.

Method: 네트워크 기반 프레임워크를 개발하여 인지 기술, LLM 아키텍처, 데이터셋 간의 관계를 연결합니다.

Result: 모듈 커뮤니티에서의 기술 분포를 통해 LLM이 특정 생물학적 시스템의 집중화된 전문성과는 다르지만 조류 및 소형 포유동물의 뇌에서 관찰되는 분산된 인지 조직과 부분적으로 유사한 기술 패턴을 보임을 확인했습니다.

Conclusion: 인간 인지 과학 원칙을 머신러닝과 통합하여 LLM의 해석 가능성에 대한 새로운 통찰력을 제공하며, 효과적인 미세 조정 전략은 고정된 모듈 개입보다는 분산 학습 동역학을 활용해야 함을 제안합니다.

Abstract: Large Language Models (LLMs) have reshaped our world with significant
advancements in science, engineering, and society through applications ranging
from scientific discoveries and medical diagnostics to Chatbots. Despite their
ubiquity and utility, the underlying mechanisms of LLM remain concealed within
billions of parameters and complex structures, making their inner architecture
and cognitive processes challenging to comprehend. We address this gap by
adopting approaches to understanding emerging cognition in biology and
developing a network-based framework that links cognitive skills, LLM
architectures, and datasets, ushering in a paradigm shift in foundation model
analysis. The skill distribution in the module communities demonstrates that
while LLMs do not strictly parallel the focalized specialization observed in
specific biological systems, they exhibit unique communities of modules whose
emergent skill patterns partially mirror the distributed yet interconnected
cognitive organization seen in avian and small mammalian brains. Our numerical
results highlight a key divergence from biological systems to LLMs, where skill
acquisition benefits substantially from dynamic, cross-regional interactions
and neural plasticity. By integrating cognitive science principles with machine
learning, our framework provides new insights into LLM interpretability and
suggests that effective fine-tuning strategies should leverage distributed
learning dynamics rather than rigid modular interventions.

</details>


### [236] [Disentangling the Factors of Convergence between Brains and Computer Vision Models](https://arxiv.org/abs/2508.18226)
*Joséphine Raugel,Marc Szafraniec,Huy V. Vo,Camille Couprie,Patrick Labatut,Piotr Bojanowski,Valentin Wyart,Jean-Rémi King*

Main category: cs.AI

TL;DR: 이 연구는 DINOv3라는 자기 지도형 비전 변환기를 이용해 인공지능 모델과 인간 뇌의 유사성에 대한 여러 요인을 분석하였으며, 모델 크기, 훈련 양, 이미지 유형이 모두 상호작용하여 뇌 유사성 메트릭에 영향을 미침을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 자연 이미지로 훈련된 많은 AI 모델이 인간 뇌와 유사한 표현을 개발하지만, 이러한 뇌-모델 유사성을 유도하는 요인이 잘 이해되지 않고 있다.

Method: DINOv3라는 자기 지도형 비전 변환기를 훈련시켜 모델, 훈련, 데이터의 다양한 요인들이 신경망의 뇌와 유사한 표현 발달에 어떻게 기여하는지를 분석하였다.

Result: 모든 세 가지 요인(모델 크기, 훈련 양, 이미지 유형)이 독립적이고 상호작용적으로 뇌 유사성 메트릭에 영향을 미치며, 특히 가장 인간 중심의 이미지로 훈련된 DINOv3 모델이 가장 높은 뇌 유사성에 도달한다는 결과를 얻었다.

Conclusion: 이 연구는 모델의 아키텍처와 경험이 인공 신경망이 인간과 같은 방식으로 세상을 인식하는 데 어떻게 기여하는지를 분리하여 이해할 수 있는 틀을 제공한다.

Abstract: Many AI models trained on natural images develop representations that
resemble those of the human brain. However, the factors that drive this
brain-model similarity remain poorly understood. To disentangle how the model,
training and data independently lead a neural network to develop brain-like
representations, we trained a family of self-supervised vision transformers
(DINOv3) that systematically varied these different factors. We compare their
representations of images to those of the human brain recorded with both fMRI
and MEG, providing high resolution in spatial and temporal analyses. We assess
the brain-model similarity with three complementary metrics focusing on overall
representational similarity, topographical organization, and temporal dynamics.
We show that all three factors - model size, training amount, and image type -
independently and interactively impact each of these brain similarity metrics.
In particular, the largest DINOv3 models trained with the most human-centric
images reach the highest brain-similarity. This emergence of brain-like
representations in AI models follows a specific chronology during training:
models first align with the early representations of the sensory cortices, and
only align with the late and prefrontal representations of the brain with
considerably more training. Finally, this developmental trajectory is indexed
by both structural and functional properties of the human cortex: the
representations that are acquired last by the models specifically align with
the cortical areas with the largest developmental expansion, thickness, least
myelination, and slowest timescales. Overall, these findings disentangle the
interplay between architecture and experience in shaping how artificial neural
networks come to see the world as humans do, thus offering a promising
framework to understand how the human brain comes to represent its visual
world.

</details>


### [237] [Efficient Computation of Blackwell Optimal Policies using Rational Functions](https://arxiv.org/abs/2508.18252)
*Dibyangshu Mukherjee,Shivaram Kalyanakrishnan*

Main category: cs.AI

TL;DR: 본 논문은 Blackwell 최적성 기준을 활용하여 Markov 결정 문제(MDP)의 최적 정책을 계산하는 새로운 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: MDP는 다양한 분야의 순차적 의사결정을 모델링하는 데 필요한 기반을 제공하지만, 전통적인 최적성 기준에는 제한이 있습니다. Blackwell 최적성은 이러한 제약을 해결하기 위해 개발되었습니다.

Method: 우리는 $1$ 근처의 유리 함수 정렬을 사용하여 BO 정책을 계산하는 절차를 설명하며, 결정론적 및 일반 MDP에 대한 최첨단 알고리즘을 조정하여 유리 함수에 대한 상징적 연산으로 수치 평가를 대체합니다.

Result: 결정론적 MDP에 대해 BO 정책을 вычис하는 최초의 강한 다항 시간 알고리즘을 제공하였고, 일반 MDP에 대해 최초의 부분 지수 시간 알고리즘을 획득했습니다.

Conclusion: 여러 정책 반복 알고리즘을 일반화하여 할인 기준에서 Blackwell 기준으로 알려진 상한을 확장했습니다.

Abstract: Markov Decision Problems (MDPs) provide a foundational framework for
modelling sequential decision-making across diverse domains, guided by
optimality criteria such as discounted and average rewards. However, these
criteria have inherent limitations: discounted optimality may overly prioritise
short-term rewards, while average optimality relies on strong structural
assumptions. Blackwell optimality addresses these challenges, offering a robust
and comprehensive criterion that ensures optimality under both discounted and
average reward frameworks. Despite its theoretical appeal, existing algorithms
for computing Blackwell Optimal (BO) policies are computationally expensive or
hard to implement.
  In this paper we describe procedures for computing BO policies using an
ordering of rational functions in the vicinity of $1$. We adapt
state-of-the-art algorithms for deterministic and general MDPs, replacing
numerical evaluations with symbolic operations on rational functions to derive
bounds independent of bit complexity. For deterministic MDPs, we give the first
strongly polynomial-time algorithms for computing BO policies, and for general
MDPs we obtain the first subexponential-time algorithm. We further generalise
several policy iteration algorithms, extending the best known upper bounds from
the discounted to the Blackwell criterion.

</details>


### [238] [Hermes 4 Technical Report](https://arxiv.org/abs/2508.18255)
*Ryan Teknium,Roger Jin,Jai Suphavadeeprasit,Dakota Mahan,Jeffrey Quesnelle,Joe Li,Chen Guang,Shannon Sands,Karan Malhotra*

Main category: cs.AI

TL;DR: Hermes 4는 구조화된 다중 턴 추론과 폭넓은 지침 따르기 능력을 결합한 하이브리드 추론 모델 모음이다.


<details>
  <summary>Details</summary>
Motivation: 데이터 큐레이션, 합성, 훈련, 평가 과정에서 발생하는 문제를 해결하기 위해.

Method: 구조화된 다중 턴 추론과 지침 따르기 능력을 통합한 모델을 개발하고 평가하였다.

Result: 수학적 추론, 코딩, 지식, 이해 및 정렬 벤치마크에서 포괄적인 평가를 수행하였다.

Conclusion: 모델의 모든 가중치가 공개적으로 발표되어 오픈 연구를 지원한다.

Abstract: We present Hermes 4, a family of hybrid reasoning models that combine
structured, multi-turn reasoning with broad instruction-following ability. We
describe the challenges encountered during data curation, synthesis, training,
and evaluation, and outline the solutions employed to address these challenges
at scale. We comprehensively evaluate across mathematical reasoning, coding,
knowledge, comprehension, and alignment benchmarks, and we report both
quantitative performance and qualitative behavioral analysis. To support open
research, all model weights are published publicly at
https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [239] [Anemoi: A Semi-Centralized Multi-agent Systems Based on Agent-to-Agent Communication MCP server from Coral Protocol](https://arxiv.org/abs/2508.17068)
*Xinxing Ren,Caelum Forder,Qianbo Zang,Ahsen Tahir,Roman J. Georgio,Suman Deb,Peter Carroll,Önder Gürcan,Zekun Guo*

Main category: cs.MA

TL;DR: Anemoi는 에이전트 간 직접적인 협업을 가능하게 하는 반중앙집중형 다중 에이전트 시스템(MAS)으로, 기존의 중앙집중 모델의 한계를 극복하고 확장성과 비용 효율성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존의 중앙집중형 다중 에이전트 시스템이 계획자의 능력에 크게 의존하고, 상호 에이전트 통신이 제한적이라는 문제를 해결하기 위해.

Method: Anemoi는 Coral Protocol의 에이전트 간 통신 A2A MCP 서버를 기반으로 하여 구조화되고 직접적인 에이전트 간 협업을 가능하게 한다.

Result: Anemoi는 GAIA 벤치마크에서 소형 LLM(GPT-4.1-mini)를 계획자로 사용하여 52.73%의 정확도를 달성하고, 동일한 LLM 설정 하에서 가장 강력한 오픈 소스 모델인 OWL을 +9.09% 초과하여 능가했다.

Conclusion: Anemoi는 단일 계획자에 대한 의존도를 줄이고 적응적인 계획 업데이트를 지원하며,冗余 문맥 전달을 최소화하여 보다 확장 가능하고 비용 효율적인 실행을 가능하게 한다.

Abstract: Recent advances in generalist multi-agent systems (MAS) have largely followed
a context-engineering plus centralized paradigm, where a planner agent
coordinates multiple worker agents through unidirectional prompt passing. While
effective under strong planner models, this design suffers from two critical
limitations: (1) strong dependency on the planner's capability, which leads to
degraded performance when a smaller LLM powers the planner; and (2) limited
inter-agent communication, where collaboration relies on costly prompt
concatenation and context injection, introducing redundancy and information
loss. To address these challenges, we propose Anemoi, a semi-centralized MAS
built on the Agent-to-Agent (A2A) communication MCP server from Coral Protocol.
Unlike traditional designs, Anemoi enables structured and direct inter-agent
collaboration, allowing all agents to monitor progress, assess results,
identify bottlenecks, and propose refinements in real time. This paradigm
reduces reliance on a single planner, supports adaptive plan updates, and
minimizes redundant context passing, resulting in more scalable and
cost-efficient execution. Evaluated on the GAIA benchmark, Anemoi achieved
52.73\% accuracy with a small LLM (GPT-4.1-mini) as the planner, surpassing the
strongest open-source baseline OWL (43.63\%) by +9.09\% under identical LLM
settings. Our implementation is publicly available at
https://github.com/Coral-Protocol/Anemoi.

</details>


### [240] [Fair Cooperation in Mixed-Motive Games via Conflict-Aware Gradient Adjustment](https://arxiv.org/abs/2508.17696)
*Woojun Kim,Katia Sycara*

Main category: cs.MA

TL;DR: 이 논문에서는 다중 에이전트 강화 학습에서 개별 이익과 집단 목표의 균형을 맞추는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습에서 에이전트들은 개별 이익과 집단 목표 간의 균형을 맞추는 도전에 직면해 있다.

Method: 적응형 갈등 인지 경량 조정 방법을 제안하여 협력을 촉진하고 개별 보상에서의 공정성을 보장한다.

Result: 우리의 방법은 에이전트 간의 공정성을 보존하면서 집단적 성과를 개선한다.

Conclusion: 이론적 결과는 집단 및 개별 목표 모두에서 단조롭게 비감소하는 개선을 보장하며, 실험 결과는 사회적 복지 측면에서 우수한 성능을 나타낸다.

Abstract: Multi-agent reinforcement learning in mixed-motive settings presents a
fundamental challenge: agents must balance individual interests with collective
goals, which are neither fully aligned nor strictly opposed. To address this,
reward restructuring methods such as gifting and intrinsic motivation have been
proposed. However, these approaches primarily focus on promoting cooperation by
managing the trade-off between individual and collective returns, without
explicitly addressing fairness with respect to the agents' task-specific
rewards. In this paper, we propose an adaptive conflict-aware gradient
adjustment method that promotes cooperation while ensuring fairness in
individual rewards. The proposed method dynamically balances policy gradients
derived from individual and collective objectives in situations where the two
objectives are in conflict. By explicitly resolving such conflicts, our method
improves collective performance while preserving fairness across agents. We
provide theoretical results that guarantee monotonic non-decreasing improvement
in both the collective and individual objectives and ensure fairness. Empirical
results in sequential social dilemma environments demonstrate that our approach
outperforms baselines in terms of social welfare while ensuring fairness among
agents.

</details>
