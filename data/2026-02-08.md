<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 22]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](https://arxiv.org/abs/2602.06030)
*Kavana Venkatesh,Yinhan He,Jundong Li,Jiaming Cui*

Main category: cs.MA

TL;DR: PhysicsAgentABM은 대규모 언어 모델을 기반으로 하는 다중 에이전트 시스템으로서, 비용 문제와 시간 동기화 상태 전이 시뮬레이션의 부족한 교정 문제를 해결하고 해석 가능한 에이전트 기반 모델의 단점을 보완합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 기반 다중 에이전트 시스템은 비록 표현력이 우수하지만 확장하기에 비용이 많이 들고 시간 동기화 상태 전이 시뮬레이션에서 잘 보정되지 않는 단점이 있습니다. 반면, 전통적인 에이전트 기반 모델은 해석 가능성을 제공하지만 개별 수준의 신호와 비정상적 행동을 통합하는 데 어려움을 겪습니다.

Method: PhysicsAgentABM은 행동적으로 일관된 에이전트 클러스터로 추론을 전환합니다: 상태 전문화된 상징적 에이전트가 기계적 전이 우선을 인코딩하고, 다중 모달 신경 전이 모델이 시간적 및 상호 작용 동력을 포착하며, 불확실성 인식 지식 융합이 보정된 클러스터 수준 전이 분포를 생성합니다. 개별 에이전트는 지역 제약 조건 하에서 전이를 확률적으로 실현하여 인구 추론과 개체 수준 변동성을 분리합니다.

Result: 공공 건강, 금융 및 사회 과학 전반에 걸친 실험에서 메커니즘, 신경 및 LLM 기준보다 이벤트 시간 정확성과 보정에서 일관된 개선을 보여줍니다.

Conclusion: 불확실성 인식 신경-상징적 융합을 통해 인구 수준 추론을 중심으로 생성적 ABM을 재설계함으로써 PhysicsAgentABM은 LLM과 함께 확장 가능하고 보정된 시뮬레이션을 위한 새로운 패러다임을 설정합니다.

Abstract: Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: 본 논문은 AI 에이전트가 인간의 입력을 적극적으로 이끌어내기 위한 최적의 상호작용 전략을 발견하는 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 인간-AI 팀의 공동 계획에서 언어 기반의 상호작용은 중요한 분야이다. 개방된 세계의 계획 문제는 종종 불완전한 정보와 미지의 요소들을 포함한다.

Method: 우리는 최소 정보 신경-상징 트리(MINT)를 제안하여 지식 간격의 영향을 추론하고 MINT와의 자기 플레이를 활용하여 AI 에이전트의 문의 전략과 질의를 최적화한다.

Result: MINT는 가능한 인간-AI 상호작용의 제안을 통해 상징적 트리를 구축하고, 남아있는 지식 간격으로 인한 계획 결과의 불확실성을 추정하기 위해 신경 계획 정책을 참조한다.

Conclusion: MINT 기반 계획은 각 작업에 대해 제한된 수의 질문을 제시하면서도 보상과 성공률을 크게 향상시켜 전문가 수준의 수익을 달성했다.

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [3] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: 본 논문은 대형 언어 모델(LLM)의 불확실성 정량화(UQ)를 위한 새로운 프레임워크를 제안하며, 상호작용 에이전트의 현실적인 설정에서 불확실성을 모델링하는 방법을 논의한다.


<details>
  <summary>Details</summary>
Motivation: 우리는 LLM 에이전트가 복잡한 작업에 배치되고 있는 가운데, 현재의 많은 UQ 연구가 단일 질문-답변에 집중되고 있다는 점을 지적하며 UQ 연구의 방향 전환이 필요하다고 주장한다.

Method: 우리는 에이전트 UQ에 대한 일반적인 공식을 제시하고, 기존 UQ 설정의 다양한 클래스를 포함하는 새로운 원칙적 프레임워크를 제안한다.

Result: 이 프레임워크를 통해, 기존 연구들이 LLM UQ를 불확실성 누적 과정으로 암묵적으로 다루고 있다는 점을 보여준다.

Conclusion: 마지막으로, 우리는 LLM 개발과 도메인 특정 응용 프로그램에서의 에이전트 UQ의 실제적 함의를 논의하고 남은 문제점들을 제시한다.

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [4] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: 이 논문은 결제 위험 평가에서의 대형 언어 모델의 신뢰성과 편향을 조사하며, LLM의 추론 품질을 평가하기 위한 구조화된 다중 평가자 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 결제 위험 환경에서 LLM의 신뢰성과 편향에 대한 이해가 부족하여, 이에 대한 평가 체계를 정립할 필요가 있습니다.

Method: Merchant Category Code(MCC) 기반의 상인 위험 평가를 위해 다섯 가지 기준을 가지고 몬테 카를로 점수를 결합한 평가 체계를 도입했습니다.

Result: 결과적으로 GPT-5.1과 Claude 4.5 Sonnet은 부정적인 자기 평가 편향을 보였으며, Gemini-2.5 Pro와 Grok 4는 긍정적인 편향을 나타냈습니다. 26명의 결제 산업 전문가의 평가에 따르면, LLM 평가자들은 평균적으로 인간의 합의보다 +0.46 포인트 높은 점수를 부여했습니다.

Conclusion: 전반적으로 이 프레임워크는 결제 위험 워크플로우에서 LLM을 평가자로 사용하는 시스템의 평가를 위한 반복 가능한 기초를 제공합니다.

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


### [5] [Explainable AI: A Combined XAI Framework for Explaining Brain Tumour Detection Models](https://arxiv.org/abs/2602.05240)
*Patrick McGonagle,William Farrelly,Kevin Curran*

Main category: cs.AI

TL;DR: 다양한 설명 가능한 AI 기법을 통합하여 뇌 종양 탐지를 위한 심층 학습 모델의 해석 가능성을 향상시키는 연구.


<details>
  <summary>Details</summary>
Motivation: 뇌 종양 탐지를 위한 심층 학습 모델의 해석 가능성을 높이고자 함.

Method: BraTS 2021 데이터셋에서 훈련된 사용자 정의 CNN을 개발하고, GRAD-CAM, LRP, SHAP 기법을 통합하여 모델의 의사 결정 과정을 분석.

Result: 91.24%의 정확도로 종양과 비종양 영역을 구별하며, 전반적인 관심 영역부터 픽셀 수준 세부 정보까지 다양한 설명 제공.

Conclusion: 이 통합 접근 방식은 AI 기반 의료 이미징 분석의 투명성과 신뢰성을 향상시키며, 특히 뇌 종양 탐지와 같은 중요한 작업에서 AI 시스템의 신뢰성과 해석 가능성을 개선하는 잠재력을 보여준다.

Abstract: This study explores the integration of multiple Explainable AI (XAI) techniques to enhance the interpretability of deep learning models for brain tumour detection. A custom Convolutional Neural Network (CNN) was developed and trained on the BraTS 2021 dataset, achieving 91.24% accuracy in distinguishing between tumour and non-tumour regions. This research combines Gradient-weighted Class Activation Mapping (GRAD-CAM), Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanations (SHAP) to provide comprehensive insights into the model's decision-making process. This multi-technique approach successfully identified both full and partial tumours, offering layered explanations ranging from broad regions of interest to pixel-level details. GRAD-CAM highlighted important spatial regions, LRP provided detailed pixel-level relevance and SHAP quantified feature contributions. The integrated approach effectively explained model predictions, including cases with partial tumour visibility thus showing superior explanatory power compared to individual XAI methods. This research enhances transparency and trust in AI-driven medical imaging analysis by offering a more comprehensive perspective on the model's reasoning. The study demonstrates the potential of integrated XAI techniques in improving the reliability and interpretability of AI systems in healthcare, particularly for critical tasks like brain tumour detection.

</details>


### [6] [Position: Universal Time Series Foundation Models Rest on a Category Error](https://arxiv.org/abs/2602.05287)
*Xilin Dai,Wanxu Cai,Zhijian Xu,Qiang Xu*

Main category: cs.AI

TL;DR: 이 논문은 '시계열을 위한 범용 기초 모델'의 추구가 구조적 컨테이너를 의미론적 양상으로 착각한 근본적인 범주 오류에 기반하고 있다고 주장한다. 향후 대안으로 인과 제어 에이전트 패러다임을 제시하고, 평가 기준을 '제로샷 정확도'에서 '드리프트 적응 속도'로 전환해야 한다고 결론짓는다.


<details>
  <summary>Details</summary>
Motivation: 시계열의 복잡성과 그에 따른 모델링의 한계를 인식하고, 보다 효과적인 접근 방안을 모색하기 위해.

Method: 역사 기반 모델의 한계를 이론적으로 규명하는 '자기회귀적 무시 경계'를 도입하고, 외부 맥락을 활용하여 전문 솔버를 계층적으로 조율하는 인과 제어 에이전트 패러다임을 제안한다.

Result: 자기회귀적 무시 경계는 역사 기반 모델이 개입 중심의 체제 전환을 예측할 수 없음을 증명하며, 전문 솔버와 가벼운 적응형 솔버의 조합이 필요함을 나타낸다.

Conclusion: 모델의 범용성 대신 인과 제어 에이전트 패러다임을 채택하고, 시스템 강건성을 우선시하기 위해 평가 기준을 변경할 것을 제안한다.

Abstract: This position paper argues that the pursuit of "Universal Foundation Models for Time Series" rests on a fundamental category error, mistaking a structural Container for a semantic Modality. We contend that because time series hold incompatible generative processes (e.g., finance vs. fluid dynamics), monolithic models degenerate into expensive "Generic Filters" that fail to generalize under distributional drift. To address this, we introduce the "Autoregressive Blindness Bound," a theoretical limit proving that history-only models cannot predict intervention-driven regime shifts. We advocate replacing universality with a Causal Control Agent paradigm, where an agent leverages external context to orchestrate a hierarchy of specialized solvers, from frozen domain experts to lightweight Just-in-Time adaptors. We conclude by calling for a shift in benchmarks from "Zero-Shot Accuracy" to "Drift Adaptation Speed" to prioritize robust, control-theoretic systems.

</details>


### [7] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: 이 논문은 LLM(대형 언어 모델)의 협상 능력을 평가하고, PieArena라는 협상 벤치를 도입하여 AGI 수준의 성능을 보여주며, 저급 LLM에도 개선 효과가 있음을 발견했다.


<details>
  <summary>Details</summary>
Motivation: LLM의 협상 능력은 전략적 추론과 경제적 가치 창출이 필요한 중요한 비즈니스 과제이므로 이에 대한 평가는 중요하다.

Method: PieArena라는 대규모 협상 벤치를 소개하고 MBA 협상 과정에서의 현실적인 시나리오를 기반으로 다중 에이전트 상호작용을 활용하여 평가를 수행했다.

Result: 대표적인 에이전트인 GPT-5가 훈련받은 비즈니스 학교 학생들과 비교하여 동등하거나 더 나은 성과를 보였으며, 중간 및 하위 계층 LLM에서 큰 개선이 발견되었다.

Conclusion: 전반적으로 선도적인 언어 에이전트는 경제적 환경에서 배치할 수 있는 지적 및 심리적 능력을 이미 갖추었지만, 강인성과 신뢰성 부족은 여전히 해결해야 할 도전 과제라는 것을 시사한다.

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [8] [TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?](https://arxiv.org/abs/2602.05570)
*Yikun Zong,Cheston Tan*

Main category: cs.AI

TL;DR: 이 논문은 AI 모델이 테스트 시 예측을 반복적으로 개선할 수 있는지를 다루며, 인간의 인지 과정을 모델링한 새로운 자기 개선 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 인간의 탕그램 퍼즐 문제 해결 방식을 영감을 받아 AI의 자기 개선 문제를 해결하기 위해 노력합니다.

Method: 테스트 시 자기 개선 프레임워크를 도입하고, 내용 내 학습(ICL)과 보상 기반 피드백 루프를 결합합니다.

Result: 중간 삼각형 사례에서 IoU가 0.63에서 0.932로 개선되었습니다.

Conclusion: 이 연구는 ICL 및 보상 루프를 통한 인간 영감을 받은 반복적인 개선 메커니즘이 VLM의 기하학적 추론을 상당히 향상시킬 수 있음을 보여줍니다.

Abstract: Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.

</details>


### [9] [Graph-based Agent Memory: Taxonomy, Techniques, and Applications](https://arxiv.org/abs/2602.05665)
*Chang Yang,Chuang Zhou,Yilin Xiao,Su Dong,Luyao Zhuang,Yujing Zhang,Zhu Wang,Zijin Hong,Zheng Yuan,Zhishang Xiang,Shengyuan Chen,Huachi Zhou,Qinggang Zhang,Ninghao Liu,Jinsong Su,Xinrun Wang,Yi Chang,Xiao Huang*

Main category: cs.AI

TL;DR: 이 논문은 그래프 기반 관점에서 에이전트 메모리에 대한 포괄적인 리뷰를 제공하며, 다양한 메모리 유형과 기술, 응용 시나리오를 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 고차원 복잡 작업에서 지식 축적, 반복적 추론 및 자기 진화를 가능하게 하는 메모리의 중요성을 강조한다.

Method: 그래프 기반 에이전트 메모리의 생애 주기를 체계적으로 분석하고, 메모리 추출, 저장, 검색 및 진화 기술을 논의한다.

Result: 오픈 소스 라이브러리 및 벤치마크를 요약하고 다양한 응용 시나리오를 탐구한다.

Conclusion: 효율적이고 신뢰할 수 있는 그래프 기반 에이전트 메모리 시스템 개발을 위한 통찰력을 제공하고, 주요 도전 과제와 미래 연구 방향을 제시한다.

Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.

</details>


### [10] [STProtein: predicting spatial protein expression from multi-omics data](https://arxiv.org/abs/2602.05811)
*Zhaorui Jiang,Yingfang Yuan,Lei Hu,Wei Pang*

Main category: cs.AI

TL;DR: STProtein은 그래프 신경망을 활용하여 공간 단백질 발현을 예측하는 새로운 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 단일 조직의 공간 다중 오믹스 데이터 통합은 생물학 연구의 발전에 필수적입니다. 그러나 공간 전사체학 데이터는 풍부하지만 공간 단백질체학 데이터는 기술적 한계와 높은 비용으로 인해 부족합니다.

Method: STProtein은 다중 작업 학습 전략을 활용하는 그래프 신경망을 사용하여, 더 접근 가능한 공간 다중 오믹스 데이터로부터 알려지지 않은 공간 단백질 발현을 정확하게 예측할 수 있도록 설계되었습니다.

Result: STProtein은 공간 단백질체학의 부족 문제를 효과적으로 해결할 수 있으며, 공간 다중 오믹스의 통합을 가속화할 수 있습니다.

Conclusion: 이 도구는 과학자들이 조직 내 복잡하고 이전에 숨겨진 단백질의 공간 패턴을 식별하여 발견을 가속화하고, 서로 다른 마커 유전자 간의 새로운 관계를 밝히며, 생물학적 '다크 매터'를 탐색할 수 있게 합니다.

Abstract: The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological "Dark Matter".

</details>


### [11] [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008)
*Xianyang Liu,Shangding Gu,Dawn Song*

Main category: cs.AI

TL;DR: AgenticPay는 다중 에이전트 간의 언어 기반 협상과 거래를 평가하기 위한 벤치마크 및 시뮬레이션 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 기반 에이전트들이 자율적으로 협상, 조정 및 거래할 것으로 기대되지만, 기존의 벤치마크는 이러한 언어 매개 경제 상호작용을 평가하기 위한 원칙적인 설정이 부족하다.

Method: AgenticPay는 구매자와 판매자가 개인적인 제약과 제품 의존적인 평가를 가지고 다자 간의 언어 협상을 통해 합의해야 하는 시장을 모델링한다.

Result: 최신 상용 및 오픈 웨이트 LLM을 벤치마킹한 결과 협상 성능에서 상당한 격차가 발견되었고, 장기 전략적 추론에서의 도전 과제가 강조되었다.

Conclusion: AgenticPay는 에이전트 상업 및 언어 기반 시장 상호작용 연구의 기초로 자리 잡았다.

Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.

</details>


### [12] [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/abs/2602.06039)
*Yuxing Lu,Yucheng Hu,Xukai Zhao,Jiuxin Cao*

Main category: cs.AI

TL;DR: DyTopo는 다중 에이전트 시스템에서 효과적인 의사소통을 통해 다중 라운드의 추론을 개선하는 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 기존 방식은 비효율적인 의사소통 패턴에 의존하여 문제 해결의 단계에 따라 달라지는 필요에 잘 맞지 않음.

Method: DyTopo 프레임워크는 매 라운드마다 희소한 유향 통신 그래프를 재구성하고, 매니저의 라운드 목표를 기반으로 각 에이전트가 자연어 쿼리와 키 설명자를 출력하며, 이러한 설명자를 임베딩하여 의미적 일치를 수행한다.

Result: 코드 생성 및 수학적 추론 벤치마크에서 DyTopo는 가장 강력한 기준선 대비 평균 6.2% 향상된 성능을 보임.

Conclusion: DyTopo는 진화하는 그래프를 통해 해석 가능한 협동 추적을 제공하여 통신 경로가 라운드 간 어떻게 재구성되는지를 정성적으로 검토할 수 있게 함.

Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [The Birthmark Standard: Privacy-Preserving Photo Authentication via Hardware Roots of Trust and Consortium Blockchain](https://arxiv.org/abs/2602.04933)
*Sam Ryan*

Main category: cs.CR

TL;DR: 본 논문은 사진 증거의 신뢰성을 저해하는 생성 AI 시스템의 발전에 대응하기 위한 인증 아키텍처인 Birthmark Standard를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 생성 AI 시스템의 급속한 발전으로 인해 사진 증거의 신뢰성이 훼손되었으며, 저널리즘 및 공적 담론의 기반이 흔들리고 있다.

Method: Birthmark Standard는 비균일성 보정(NUC) 맵과 PRNU 패턴에서 제조 고유의 센서 엔트로피를 이용하여 하드웨어 기반 인증 키를 생성하는 인증 아키텍처이다.

Result: Raspberry Pi 4 하드웨어를 이용한 프로토타입 구현을 통해 인증 기록이 저널리즘 조직이 운영하는 컨소시엄 블록체인에 저장됨으로써 메타데이터 손실에도 견디는 검증이 가능함을 보여주었다.

Conclusion: 성능 분석 결과, 카메라 오버헤드는 100ms 이하, 검증 대기시간은 100만 건의 인증을 처리할 때 500ms 이하로 예상된다.

Abstract: The rapid advancement of generative AI systems has collapsed the credibility landscape for photographic evidence. Modern image generation models produce photorealistic images undermining the evidentiary foundation upon which journalism and public discourse depend. Existing authentication approaches, such as the Coalition for Content Provenance and Authenticity (C2PA), embed cryptographically signed metadata directly into image files but suffer from two critical failures: technical vulnerability to metadata stripping during social media reprocessing, and structural dependency on corporate-controlled verification infrastructure where commercial incentives may conflict with public interest. We present the Birthmark Standard, an authentication architecture leveraging manufacturing-unique sensor entropy from non-uniformity correction (NUC) maps and PRNU patterns to generate hardware-rooted authentication keys. During capture, cameras create anonymized authentication certificates proving sensor authenticity without exposing device identity via a key table architecture maintaining anonymity sets exceeding 1,000 devices. Authentication records are stored on a consortium blockchain operated by journalism organizations rather than commercial platforms, enabling verification that survives all metadata loss. We formally verify privacy properties using ProVerif, proving observational equivalence for Manufacturer Non-Correlation and Blockchain Observer Non-Identification under Dolev-Yao adversary assumptions. The architecture is validated through prototype implementation using Raspberry Pi 4 hardware, demonstrating the complete cryptographic pipeline. Performance analysis projects camera overhead below 100ms and verification latency below 500ms at scale of one million daily authentications.

</details>


### [14] [SynthForensics: A Multi-Generator Benchmark for Detecting Synthetic Video Deepfakes](https://arxiv.org/abs/2602.04939)
*Roberto Leotta,Salvatore Alfio Sambataro,Claudio Vittorio Ragaglia,Mirko Casu,Yuri Petralia,Francesco Guarnera,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CR

TL;DR: 이 논문은 순수한 합성 비디오 딥페이크를 탐지하기 위한 최초의 인간 중심 벤치마크인 SynthForensics를 소개하며, 최신 T2V 모델에서 생성된 6,815개의 비디오를 포함한다.


<details>
  <summary>Details</summary>
Motivation: 합성 미디어의 급속한 발전으로 인해 기존의 얼굴 중심 및 조작 기반 벤치마크가 더 이상 유효하지 않게 되었다.

Method: SynthForensics는 6,815개의 고유 비디오로 구성된 벤치마크로, 두 단계의 인간 검증을 통해 높은 의미적 및 시각적 품질을 보장한다. 비디오는 네 가지 버전(원시, 무손실, 경량 및 중량 압축)으로 제공된다.

Result: 최신 탐지기들은 이 새로운 도메인에서 29.19% AUC의 성능 저하를 보였으며, 일부 방법은 무작위 확률보다 나쁜 성과를 나타냈다.

Conclusion: SynthForensics를 사용한 훈련은 새로운 생성기에 대한 강력한 일반화를 달성했지만, 전통적인 조작 기반 딥페이크와의 호환성은 감소하였다.

Abstract: The landscape of synthetic media has been irrevocably altered by text-to-video (T2V) models, whose outputs are rapidly approaching indistinguishability from reality. Critically, this technology is no longer confined to large-scale labs; the proliferation of efficient, open-source generators is democratizing the ability to create high-fidelity synthetic content on consumer-grade hardware. This makes existing face-centric and manipulation-based benchmarks obsolete. To address this urgent threat, we introduce SynthForensics, to the best of our knowledge the first human-centric benchmark for detecting purely synthetic video deepfakes. The benchmark comprises 6,815 unique videos from five architecturally distinct, state-of-the-art open-source T2V models. Its construction was underpinned by a meticulous two-stage, human-in-the-loop validation to ensure high semantic and visual quality. Each video is provided in four versions (raw, lossless, light, and heavy compression) to enable real-world robustness testing. Experiments demonstrate that state-of-the-art detectors are both fragile and exhibit limited generalization when evaluated on this new domain: we observe a mean performance drop of $29.19\%$ AUC, with some methods performing worse than random chance, and top models losing over 30 points under heavy compression. The paper further investigates the efficacy of training on SynthForensics as a means to mitigate these observed performance gaps, achieving robust generalization to unseen generators ($93.81\%$ AUC), though at the cost of reduced backward compatibility with traditional manipulation-based deepfakes. The complete dataset and all generation metadata, including the specific prompts and inference parameters for every video, will be made publicly available at [link anonymized for review].

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Denoising diffusion networks for normative modeling in neuroimaging](https://arxiv.org/abs/2602.04886)
*Luke Whitbread,Lyle J. Palmer,Mark Jenkinson*

Main category: cs.LG

TL;DR: 본 논문은 신경영상에서의 차원 증가에도 불구하고 일관된 규범적 모델링을 제공하는 분산 확산 확률 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 신경영상 파이프라인에서 IDP별로 단일 모델을 적합하는 방식은 다변량 의존성을 활용하지 못합니다. 이를 해결하기 위해 DDPM을 제안합니다.

Method: 우리는 FiLM 조건화 다층 퍼셉트론과 특성 자가 주의가 있는 변환기(SAINT)를 두 가지 노이즈 제거 백본으로 활용하여 IDP에 대한 조건부 밀도 추정기를 구축합니다.

Result: 기존 기준선과 유사하게 잘 보정된 출력을 제공하며, 높은 차원에서는 변환기 백본이 다층 퍼셉트론보다 더 잘 보정되고 고차원 의존성을 더 잘 유지합니다.

Conclusion: 분산 기반 규범적 모델링이 신경영상에서 보정된 다변량 편차 프로파일을 제공하는 실용적인 방법임을 지지합니다.

Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.

</details>


### [16] [Quantile-Physics Hybrid Framework for Safe-Speed Recommendation under Diverse Weather Conditions Leveraging Connected Vehicle and Road Weather Information Systems Data](https://arxiv.org/abs/2602.05053)
*Wen Zhang,Adel W. Sadek,Chunming Qiao*

Main category: cs.LG

TL;DR: 불리한 기상 조건이 운전자의 가시성과 타이어-도로 표면 마찰에 미치는 영향을 고려하여, 이 연구는 다양한 기상 조건에서 고속도로 주행을 위한 실시간 안전 속도 구간을 추천하는 하이브리드 예측 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 운전 중 사고 위험을 줄이기 위해 안전한 주행 속도를 조정해야 함을 강조한다.

Method: 버팔로, NY에서 수집된 고해상도 연결 차량 데이터와 도로 기상 정보 시스템 데이터를 활용하여, 73일 이상에 걸쳐 660만 개 이상의 기록을 포함한 시공간 정렬 데이터 세트를 구축한다. 핵심 모델은 양자 회귀 숲(QRF)을 이용해 10분 단위로 차량 속도 분포를 추정하며, 기상, 포장, 시간 조건을 포괄하는 26개의 입력 특징을 사용한다.

Result: QRF 모델은 평균 절대 오차 1.55 mph를 달성하며, 96.43%의 중간 속도 예측이 5 mph 이내에 있으며, PICP(50%)는 48.55%로 모든 기상 유형에 대해 견고한 일반화를 보여준다.

Conclusion: 변화하는 기상 조건에 대응하고 도로 구간에 대해 일반화할 수 있는 모델의 능력은 실제 세계에서의 배치를 위한 전망을 제시하며, 교통 안전을 개선하고 기상 관련 사고를 줄이는 데 기여할 수 있음을 나타낸다.

Abstract: Inclement weather conditions can significantly impact driver visibility and tire-road surface friction, requiring adjusted safe driving speeds to reduce crash risk. This study proposes a hybrid predictive framework that recommends real-time safe speed intervals for freeway travel under diverse weather conditions. Leveraging high-resolution Connected Vehicle (CV) data and Road Weather Information System (RWIS) data collected in Buffalo, NY, from 2022 to 2023, we construct a spatiotemporally aligned dataset containing over 6.6 million records across 73 days. The core model employs Quantile Regression Forests (QRF) to estimate vehicle speed distributions in 10-minute windows, using 26 input features that capture meteorological, pavement, and temporal conditions. To enforce safety constraints, a physics-based upper speed limit is computed for each interval based on real-time road grip and visibility, ensuring that vehicles can safely stop within their sight distance. The final recommended interval fuses QRF-predicted quantiles with both posted speed limits and the physics-derived upper bound. Experimental results demonstrate strong predictive performance: the QRF model achieves a mean absolute error of 1.55 mph, with 96.43% of median speed predictions within 5 mph, a PICP (50%) of 48.55%, and robust generalization across weather types. The model's ability to respond to changing weather conditions and generalize across road segments shows promise for real-world deployment, thereby improving traffic safety and reducing weather-related crashes.

</details>


### [17] [Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling](https://arxiv.org/abs/2602.05087)
*Parsa Vares*

Main category: cs.LG

TL;DR: AutoDiscover는 비정형 그래프를 활용하여 능동 학습의 온라인 의사결정을 위한 프레임워크로, 전문가 레이블이 부족한 상황에서도 효율적인 논문 스크리닝을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 과학적인 연구가 증가함에 따라 수동 스크리닝은 병목 현상이 되고 있으며, 이는 저렴하고 효율적인 스크리닝 방법론이 필요함을 보여준다.

Method: AutoDiscover 프레임워크는 이질적 그래프 모델을 통해 문서 및 저자 간의 관계를 캡처하고, 동적으로 쿼리 전략을 관리하는 Discounted Thompson Sampling(DTS) 에이전트를 적용한다.

Result: SYNERGY 벤치마크의 26개 데이터세트에서 AutoDiscover는 정적 AL 기준선보다 더 높은 스크리닝 효율성을 달성하였다.

Conclusion: 제안하는 방법은 전문가 레이블이 부족하고 관련 연구의 발생이 낮은 SLR 스크리닝을 가속화하는 데 기여한다.

Abstract: Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent's decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.

</details>


### [18] [SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines](https://arxiv.org/abs/2602.05134)
*Olga Ovcharenko,Matthias Boehm,Sebastian Schelter*

Main category: cs.LG

TL;DR: SemPipes는 대형 언어 모델을 활용하여 테이블형 기계 학습 파이프라인에서 의미론적 데이터 연산자를 통합하는 새로운 프로그래밍 모델입니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습에서 테이블형 데이터의 실제 활용은 데이터 준비 파이프라인을 복잡하게 필요로 하며, 이를 위해서는 상당한 도메인 전문성과 엔지니어링 노력이 필요합니다.

Method: SemPipes는 대형 언어 모델을 활용한 의미론적 데이터 연산자를 통합하는 선언적 프로그래밍 모델을 소개합니다. 이 모델은 자연어로 데이터 변환을 지정하며 실행은 런타임 시스템에 위임합니다.

Result: 다양한 테이블형 기계 학습 과제를 통해 SemPipes는 의미론적 연산자가 전문가가 설계한 파이프라인과 에이전트 생성 파이프라인 모두에서 end-to-end 예측 성능을 실질적으로 향상시키며 파이프라인의 복잡성을 줄임을 보여줍니다.

Conclusion: SemPipes는 데이터를 기반으로 한 연산 최적화를 자동으로 지원하며, 파이프라인에서의 코드 합성을 통해 효율성을 높입니다.

Abstract: Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.

</details>


### [19] [Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models](https://arxiv.org/abs/2602.05390)
*Wei Soon Cheong,Lian Lian Jiang,Jamie Ng Suat Ling*

Main category: cs.LG

TL;DR: 시간 시계열 기초 모델은 예측을 위한 새로운 패러다임으로 떠오르고 있으나 외부 특징을 효과적으로 활용하는 능력은 불분명하다. 이 연구는 싱가포르와 호주 전력 시장에서 다양한 기초 모델의 성과를 평가한다.


<details>
  <summary>Details</summary>
Motivation: 전력 수요 예측에서 외부 특징을 효과적으로 활용하는 능력이 부족함을 해결하고자 하였다.

Method: 시간 단위 및 일 단위로 싱가포르와 호주 전력 시장의 교차 채널 상관관계를 모델링할 수 있는 여러 기초 모델을 LSTM과 비교 평가하였다.

Result: Chronos-2가 기초 모델 중 최고 성능을 달성했으나, 일반적인 기준 모델이 싱가포르의 안정된 기후에서는 더 나은 성능을 보였다.

Conclusion: 모델 아키텍처가 중요하며, 기후 변동성이 클 때 기초 모델이 이점을 보이지만 특정 도메인에 적합한 모델이 필요하다.

Abstract: Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.

</details>


### [20] [BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs](https://arxiv.org/abs/2602.05448)
*Sheshansh Agrawal,Thien Hang Nguyen,Douwe Kiela*

Main category: cs.LG

TL;DR: 이 논문에서는 $k$-wise reranking을 위한 토너먼트 그래프 프레임워크를 제안하며, 이는 효율적인 정보 이용을 통해 기존의 방법보다 우수한 성능을 발휘합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 리랭킹 방법들은 정보 활용에 한계가 있거나, 비효율적입니다.

Method: 토너먼트 그래프 프레임워크를 도입하여 $k$-문서 비교를 통해 얻어진 선호도들을 글로벌 선호 그래프로 통합합니다.

Result: 14개의 벤치마크와 5개의 LLM에서 우리의 방법은 기존 방법들보다 25-40% 적은 토큰을 사용하여 동일하거나 더 높은 정확도를 달성합니다.

Conclusion: 우리는 비전이 없는 선호, 즉 LLM 판단에 의해 유도된 사이클을 계급으로 압축하여 처리합니다.

Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.

</details>


### [21] [Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535)
*Tao Huang,Rui Wang,Xiaofei Liu,Yi Qin,Li Duan,Liping Jing*

Main category: cs.LG

TL;DR: 대형 비전-언어 모델(LVLM)은 다중모드 이해 및 생성에서 발전을 이루었으나, 부적절하거나 적대적인 입력에 대해 신뢰할 수 없는 결과를 생성할 때가 많다. 본 연구는 이러한 문제를 해결하기 위해 정보 충돌과 무지를 효과적으로 감지할 수 있는 미세한 방법론인 증거 기반 불확실성 정량화(EUQ)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LVLM의 '비행동'이 비판적인 응용 분야에서 배치할 때 심각한 우려를 야기하기 때문에 이러한 문제를 해결하기 위한 연구가 필요하다.

Method: Evidential Uncertainty Quantification (EUQ)라는 방법을 제안하며, 모델 출력의 특징을 지지 증거(양의)와 반대 증거(음의)로 해석하고, 이를 통해 내부 충돌과 지식 격차를 정량화한다.

Result: EUQ는 환각, 탈옥, 적대적 취약성, 분포 외(OOD) 실패 등 네 가지 비행동 카테고리에서 광범위한 평가를 통해 강력한 기준선을 일관되게 초과하는 성능을 보여주었다.

Conclusion: EUQ는 환각이 높은 내부 충돌에 해당하고 OOD 실패가 높은 무지에 해당함을 보여준다. 또한 층별 증거 기반 불확실성 역학 분석은 내부 표현의 진화를 새로운 관점에서 해석하는 데 도움을 준다.

Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.

</details>


### [22] [Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias](https://arxiv.org/abs/2602.05635)
*Ojasva Nema,Kaustubh Sharma,Aditya Chauhan,Parikshit Pareek*

Main category: cs.LG

TL;DR: 모던 신경망에서 선택적 unlearning과 긴 수평 외삽은 여전히 취약하다. 이 연구는 이러한 실패가 단순히 최적화나 unlearning 알고리즘이 아니라 훈련 중 모델이 내부 표현을 구조화하는 방식에서 발생한다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: 선택적 unlearning과 긴 수평 외삽의 문제를 해결하여 신경망의 효율성을 높이고자 함.

Method: Bilinear MLPs를 통해 명시적인 곱셈 상호작용이 구조적 분리에는 도움이 되는지 탐색함.

Result: Bilinear 매개변수화가 '비혼합' 속성을 가지며, 기능적 요소가 직교 서브스페이스 표현으로 분리됨을 수학적으로 입증함.

Conclusion: 모델 수정 가능성과 일반화는 표현 구조에 의해 제한되며, 구조적 귀납적 편향이 신뢰할 수 있는 unlearning을 가능하게 하는 핵심 역할을 한다.

Abstract: Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.

</details>


### [23] [End-to-End Compression for Tabular Foundation Models](https://arxiv.org/abs/2602.05649)
*Guri Zabërgja,Rafiq Kamel,Arlind Kadra,Christian M. M. Frey,Josif Grabocka*

Main category: cs.LG

TL;DR: 새로운 TACO 모델은 기존의 테이블 기반 트랜스포머 아키텍처보다 빠르고 메모리 효율적이며, 대규모 데이터셋을 처리할 수 있는 능력을 갖추고 있다.


<details>
  <summary>Details</summary>
Motivation: 최근 테이블 데이터에 대해 인컨텍스트 학습 방법이 각광받으며 그래디언트 부스팅 결정 트리의 지배적 위치에 도전하고 있다.

Method: TACO라는 엔드-투-엔드 테이블 압축 모델을 제안하고, 훈련 데이터셋을 잠재 공간에서 압축하는 방식을 사용하였다.

Result: TACO 모델은 TabArena 벤치마크에서 기존의 트랜스포머 아키텍처에 비해 최대 94배 빠른 추론 시간을 보여주고, 최대 97% 적은 메모리를 소비하며 성능 저하 없이 유지된다.

Conclusion: 우리의 방법은 데이터셋 크기가 증가할수록 더 나은 확장성을 제공하고, 기존 기준선보다 더 나은 성능을 달성한다.

Abstract: The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.

</details>


### [24] [Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation](https://arxiv.org/abs/2602.05656)
*Igor Santos-Grueiro*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델(LLMs)의 정렬 평가 문제를 분석하고, 관찰된 행동 준수가 잠재적 정렬을 고유하게 식별하지 않는다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 정렬을 평가하는 과정에서, 관찰된 행동 준수가 내부 정렬의 증거로 간주되는 경향이 있다.

Method: 정렬 평가를 부분 관측 하의 식별 가능성 문제로 구성하고, 에이전트 행동이 평가 체제와 관련된 정보에 의존하도록 허용하였다.

Result: 관찰된 행동 준수는 잠재적 정렬을 고유하게 식별하지 않으며, 행동 정렬 검사는 정렬을 검증하는 것이 아니라 구별 가능성 클래스의 추정기로 해석해야 한다는 것을 보여준다.

Conclusion: 정렬 기준은 기본 정렬의 보증이 아니라 관찰 가능한 준수에 대한 상한을 제공하는 것으로 재구성된다.

Abstract: Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.
  We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.
  Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.
  We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.

</details>


### [25] [FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning](https://arxiv.org/abs/2602.05693)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: FedRandom은 연합 학습에서 기여의 불안정성을 완화하는 새로운 기술이다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 참가자의 기여도를 공정하게 평가하는 것은 중요한 도전 과제이며, 이를 통해 악의적 행위자와 무료 이용자를 식별하고 해결할 수 있다.

Method: FedRandom은 통계적 추정 문제로 기여도의 불안정성을 다루며, 기존의 연합 학습 전략보다 더 많은 샘플을 생성할 수 있게 해준다.

Result: FedRandom을 사용하여 CIFAR-10, MNIST, CIFAR-100, FMNIST의 다양한 데이터 분포에서 참가자 기여에 대한 일관되고 신뢰할 수 있는 평가를 제공한다.

Conclusion: FedRandom은 평가된 모든 시나리오의 절반 이상에서 정답과의 전체 거리 감소를 1/3 이상 달성하며, 90% 이상의 경우에서 안정성을 개선한다.

Abstract: Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.

</details>


### [26] [Classification Under Local Differential Privacy with Model Reversal and Model Averaging](https://arxiv.org/abs/2602.05797)
*Caihong Qin,Yang Bai*

Main category: cs.LG

TL;DR: 이 논문에서는 로컬 차별 개인 정보 보호(LDP) 하의 데이터 유틸리티 향상을 위한 전이 학습 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 로컬 차별 개인 정보 보호는 데이터 프라이버시 연구에서 중요한 주제로, 사용자 데이터를 원천에서 섞어 강력한 개인 정보 보호를 보장하지만 데이터 유틸리티를 감소시킵니다.

Method: 우리는 LDP 하에서의 개인 학습을 전이 학습 문제로 재해석하며, 노이즈가 포함된 데이터는 출처 도메인으로 사용되고 깨끗한 데이터는 목표 도메인으로 사용됩니다. LDP를 위해 다음과 같은 기술을 제안합니다: (1) 데이터 세트 유틸리티를 추정하기 위한 노이즈 이진 피드백 기반 평가 메커니즘; (2) 결정 경계를 반전시켜 성능이 저조한 분류기를 구제하는 모델 역전; (3) 추정된 유틸리티에 따라 여러 역전 분류기에 가중치를 부여하는 모델 평균화.

Result: 이론적 초과 위험 한계를 제공하며, 우리의 방법이 이 위험을 줄이는 방법을 보여줍니다. 시뮬레이션 및 실제 데이터 세트에서의 실증 결과는 분류 정확도의 상당한 향상을 나타냅니다.

Conclusion: 제안된 방법은 LDP 하에서 개인 정보 보호를 유지하면서도 성능을 향상시킬 수 있음을 시사합니다.

Abstract: Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.

</details>


### [27] [Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents](https://arxiv.org/abs/2602.05810)
*Quan M. Tran,Zhuo Huang,Wenbin Zhang,Bo Han,Koji Yatani,Masashi Sugiyama,Tongliang Liu*

Main category: cs.LG

TL;DR: Bifrost는 맥락 차이를 활용하여 이전에 해결된 경로를 새로운 작업에 정확하게 적응시키는 훈련-free 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트의 자기 개선이 맥락에 따라 성공적인 작업 경로를 재사용하는데 도움을 줄 수 있지만, 작업 간 이동이 종종 맥락 불일치를 초래한다는 문제 해결.

Method: Bifrost는 이전에 해결된 경로를 목표 작업으로 정확하게 유도하기 위해 맥락 차이를 활용하는 훈련-free 방법이다.

Result: Bifrost는 다양한 벤치마크에서 기존의 경로 재사용 및 미세 조정 자기 개선 방법보다 지속적으로 우수한 성능을 보여준다.

Conclusion: 에이전트는 상당한 맥락 변화에도 불구하고 과거 경험을 효과적으로 활용할 수 있음을 보여준다.

Abstract: Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.

</details>


### [28] [Principled Confidence Estimation for Deep Computed Tomography](https://arxiv.org/abs/2602.05812)
*Matteo Gätzner,Johannes Kirschner*

Main category: cs.LG

TL;DR: 본 논문은 CT 재구성에서 신뢰도 추정에 대한 체계적인 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: CT 재구성의 신뢰도를 높이고, 불확실성을 인지할 수 있는 의료 이미징 도구를 개발하기 위해.

Method: Beer-Lambert 법칙을 따르는 현실적인 전방 모델을 기반으로 한 신뢰도 지역을 설정하고, 기본적인 알고리즘 및 딥러닝 재구성 방법 모두에 적용할 수 있습니다.

Result: 딥 재구성 방법이 이론적 커버리지 보장을 유지하면서도 전통적인 재구성보다 훨씬 더 조밀한 신뢰도 지역을 제공함을 보여주었습니다.

Conclusion: 우리는 딥 모델이 강력한 추정기일 뿐만 아니라 불확실성을 인식하는 의료 이미징 도구로서도 신뢰할 수 있음을 입증합니다.

Abstract: We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.

</details>


### [29] [Synthesizing Realistic Test Data without Breaking Privacy](https://arxiv.org/abs/2602.05833)
*Laura Plein,Alexi Turcotte,Arina Hallemans,Andreas Zeller*

Main category: cs.LG

TL;DR: 이 연구는 원본 데이터와 동일한 통계적 속성을 갖는 합성 테스트 데이터셋을 생성하는 가능성을 탐구하며, 데이터 프라이버시를 유지하는 방식으로 진행된다.


<details>
  <summary>Details</summary>
Motivation: 원본 데이터의 기밀성을 침해하지 않고 통계 분포를 재현하는 합성 훈련 및 테스트 데이터셋의 필요성이 있다.

Method: 우리는 GANs에서 영감을 받아 생성 단계와 판별 단계를 포함하는 접근 방식을 사용하지만, 테스트 생성기를 사용하여 원본 데이터의 제약을 보존하면서 테스트 데이터를 생성한다.

Result: 우리는 네 가지 데이터셋에서 우리의 접근 방식을 평가하였으며, 높은 유틸리티와 프라이버시를 동시에 유지할 수 있는 합성 데이터셋을 생성할 수 있는 잠재력이 있음을 보여준다.

Conclusion: 우리는 프라이버시를 유지하면서 원본 데이터와 유사한 통계적 분포를 따르는 합성 데이터 생성을 위한 접근 방식의 가능성을 확립하였다.

Abstract: There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.

</details>


### [30] [ContextBench: A Benchmark for Context Retrieval in Coding Agents](https://arxiv.org/abs/2602.05892)
*Han Li,Letian Zhu,Bohan Zhang,Rili Feng,Jiaming Wang,Yue Pan,Earl T. Barr,Sarro Federica,Zhaoyang Chu,He Ye*

Main category: cs.LG

TL;DR: ContextBench를 소개하며, 코딩 에이전트의 문제 해결 과정에서 코드 맥락 검색을 평가하기 위한 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 평가 방식은 최종 작업 성공에 집중하여 에이전트가 문제 해결 중 코드 맥락을 가져오고 사용하는 방식을 제한적으로 통찰하고 있다.

Method: ContextBench는 8개 프로그래밍 언어에서 66개의 저장소로부터 1,136개의 문제 해결 작업으로 구성되어 있으며, 각 작업은 인간 주석이 달린 금 맥락으로 보강된다.

Result: 정교한 에이전트 지지 구조는 맥락 검색에서 한정적인 이익만을 제공하고, LLM은 일관되게 맥락 회상에 중점을 두며, 탐색된 맥락과 활용된 맥락 간의 상당한 간격이 존재한다.

Conclusion: ContextBench는 기존의 종단 간 벤치마크에 중간 금맥락 지표를 추가하여 문제 해결 과정을 비춰준다.

Abstract: LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.

</details>


### [31] [Orthogonal Model Merging](https://arxiv.org/abs/2602.05943)
*Sihan Yang,Kexuan Shi,Weiyang Liu*

Main category: cs.LG

TL;DR: 이 논문은 모델의 기하학적 구조를 보존하는 새로운 모델 병합 방법인 OrthoMerge를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 기능을 통합하기 위해 세분화된 대형 언어 모델(LLM)의 병합이 점점 더 중요해지고 있습니다.

Method: 본 논문은 직교군에 의해 형성된 리만 다양체에서 병합 작업을 수행하여 모델 가중치의 기하학적 구조를 보존하는 직교 모델 병합(OrthoMerge) 방법을 제안합니다.

Result: OrthoMerge는 직교 매트릭스를 사용하여 적응의 방향과 강도를 고려한 효율적인 통합을 가능하게 하며, 다양한 작업에서 모델 성능을 유지하는 데 효과적임을 입증합니다.

Conclusion: OrthoMerge는 재앙적 망각을 완화하고 다양한 작업에서 모델 성능을 유지하는 데 유용함을 보여줍니다.

Abstract: Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.

</details>


### [32] [Breaking Symmetry Bottlenecks in GNN Readouts](https://arxiv.org/abs/2602.05950)
*Mouad Talhi,Arne Wolf,Anthea Monod*

Main category: cs.LG

TL;DR: 본 논문은 그래프 신경망(GNN)이 비동형 그래프를 구분하는 능력의 한계를 극복하기 위해 새로운 읽기 도구를 제안한다.


<details>
  <summary>Details</summary>
Motivation: GNN은 구조화된 데이터 학습에 널리 사용되지만 비동형 그래프를 구분하는 데 한계가 있다.

Method: 유한 차원 표현 이론을 사용하여, 모든 선형 순열 불변 읽기 도구가 Reynolds 연산자를 통과한다는 것을 증명하고, 이를 통해 노드 임베딩을 순열 작용의 고정 부분 공간에 투영한다.

Result: 이로 인해 새로운 표현성 장벽이 생기고, 글로벌 풀이가 무엇을 보존하거나 파괴하는지를 해석가능하게 설명한다.

Conclusion: 프로젝터 기반 불변 읽기 도구를 도입하여 노드 표현을 대칭 인식 채널로 분해하고 비선형 불변 통계로 요약함으로써, 평균화에 대해 증명 가능한 정보를 보존한다. 실험적으로 읽기 도구만 변경하여 고정 인코더가 WL-하드 그래프 쌍을 분리할 수 있고 여러 벤치마크에서 성능이 향상되는 것을 보여준다.

Abstract: Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.

</details>


### [33] [Inverse Depth Scaling From Most Layers Being Similar](https://arxiv.org/abs/2602.05970)
*Yizhou Liu,Sara Kangaslahti,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 본 연구는 대형 언어 모델에서 깊이와 너비가 성능에 미치는 영향을 상세히 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 손실과 모델 크기 사이의 관계는 잘 알려져 있지만, 깊이와 너비가 성능에 미치는 영향은 다를 수 있어 더 깊이 있는 연구가 필요합니다.

Method: LLM과 장난감 잔차 네트워크에 대한 분석을 통해 깊이가 손실에 미치는 영향을 정량화합니다.

Result: LLM에서 손실은 깊이에 반비례하여 스케일된다는 것을 발견했으며, 이는 효과적으로 유사한 층들이 조합 평균을 통해 오류를 줄임으로써 발생합니다.

Conclusion: LLM의 효율성을 개선하려면 깊이의 조합적 사용을 촉진하는 건축 혁신이 필요할 수 있음을 시사합니다.

Abstract: Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.

</details>


### [34] [Layer-wise LoRA fine-tuning: a similarity metric approach](https://arxiv.org/abs/2602.05988)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 웹 규모 데이터셋에서 대규모 언어 모델(LLM)을 사전 훈련하는 것은 범용 AI 발전에 필수적이며, 후속 작업에서의 예측 성능 향상은 일반적으로 미세 조정을 통해 이루어진다. 본 연구에서는 LoRA와 그 변형을 사용하여 미세 조정할 레이어를 체계적으로 선택함으로써 이를 해결하고, 이 방법이 기존의 기술들과 호환됨을 보인다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 계속 성장함에 따라 기존의 파라미터 효율적인 미세 조정 기술이 충분하지 않을 수 있다는 문제를 해결하기 위해 연구하였다.

Method: LoRA 또는 그 변형을 사용하여 모델 적응에 기여하는 레이어만을 선택적으로 미세 조정하는 방법을 제안하였다.

Result: LoRA 기반 기술에서 학습 가능한 파라미터를 최대 50%까지 줄이면서도 다양한 모델과 작업에서 예측 성능을 유지하였다.

Conclusion: 본 연구는 로라 모듈을 사용한 모든 레이어의 미세 조정과 비교하여 경쟁력 있는 결과를 보였음이 입증되었다.

Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA

</details>


### [35] [Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering](https://arxiv.org/abs/2602.06022)
*Miranda Muqing Miao,Young-Min Cho,Lyle Ungar*

Main category: cs.LG

TL;DR: CORAL은 LLM의 정확성을 10% 향상시키고 ECE를 50% 줄이는 정규화된 추론 시간 조정 방법이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 특히 지침 조정 및 선호 정렬 이후 지속적인 비정확성을 보인다.

Method: CORAL(정확성 최적화 잔여 활성화 렌즈)는 가중치 감소 MLP 프로브를 사용하여 모델 내부 활성화에서 분산된 정확성 신호를 포착하는 정규화된 추론 시간 조정 방법이다.

Result: CORAL은 세 개의 7B 매개변수 모델에서 평균 10%의 정확성 향상과 50%의 기대 정규화 오차(ECE) 향상을 보여주었다.

Conclusion: CORAL은 추론 중 MCQA 성능을 개선하기 위한 계산 효율적이고 이전 가능한 방법을 제공한다.

Abstract: Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.

</details>


### [36] [Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference](https://arxiv.org/abs/2602.06029)
*Yingke Li,Anjali Parashar,Enlu Zhou,Chuchu Fan*

Main category: cs.LG

TL;DR: 이 논문은 능동적 추론(AIF) 시스템에서 호기심의 역할을 규명하고, 충분한 호기심이 자기 일관성 있는 학습과 비회유적 최적화를 동시 보장하는 이론적 보장을 제공함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: AIF의 호기심 계수를 통해 지식 가치와 실용 가치를 균형 있게 조화시키는 것이 중요한데, 이 균형이 학습과 의사결정에 어떻게 영향을 미치는지를 이해할 필요가 있다.

Method: 우리는 EFE를 최소화하는 에이전트에 대한 최초의 이론적 보장을 확립하고, 충분한 호기심이 자기 일관성 있는 학습과 비회유적 최적화를 보장함을 보여준다.

Result: 우리의 분석은 이 메커니즘이 초기 불확실성, 식별 가능성 및 목표 정렬에 따라 어떻게 달라지는지를 규명하며, AIF를 기존의 베이esian 실험 설계 및 베이esian 최적화와 연결짓는다.

Conclusion: 이론을 실제 디자인 가이드라인으로 변환하여 혼합 학습-최적화 문제에서 지식-실용 거래 조정을 조율할 수 있도록 하였으며, 실제 실험을 통해 검증되었다.

Abstract: Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.

</details>
