<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 10]
- [cs.AI](#cs.AI) [Total: 68]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 39]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [PeroMAS: A Multi-agent System of Perovskite Material Discovery](https://arxiv.org/abs/2602.13312)
*Yishu Wang,Wei Liu,Yifan Li,Shengxiang Xu,Xujie Yuan,Ran Li,Yuyu Luo,Jia Zhu,Shimin Di,Min-Ling Zhang,Guixiang Li*

Main category: cs.MA

TL;DR: 이 논문은 페로브스카이트 태양전지의 물질 발견을 위한 다중 에이전트 시스템인 PeroMAS를 제안하고, 그 효율성을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 페로브스카이트 태양전지(PSC)는 우수한 광전자 성능과 비용 잠재력으로 주목 받고 있으나, 기존의 AI 접근법은 주로 개별 모델에 집중되어 있어 전반적인 최적화를 저해한다.

Method: 다중 목표 제약 조건 하에 페로브스카이트 물질을 설계하기 위해 Model Context Protocols(MCPs)에 페로브스카이트 특정 도구들을 캡슐화하고, 이를 계획하고 호출하는 PeroMAS를 개발하였다.

Result: PeroMAS는 기존의 단일 대형 언어 모델(LLM)이나 전통적인 검색 전략에 비해 발견 효율성을 크게 향상시키고, 다중 목표 제약 조건을 만족하는 후보 물질을 성공적으로 파악하였다.

Conclusion: 실제 합성 실험을 통해 PeroMAS의 효과를 입증하였으며, 이는 물리적 세계에서의 유용성을 확인하였다.

Abstract: As a pioneer of the third-generation photovoltaic revolution, Perovskite Solar Cells (PSCs) are renowned for their superior optoelectronic performance and cost potential. The development process of PSCs is precise and complex, involving a series of closed-loop workflows such as literature retrieval, data integration, experimental design, and synthesis. However, existing AI perovskite approaches focus predominantly on discrete models, including material design, process optimization,and property prediction. These models fail to propagate physical constraints across the workflow, hindering end-to-end optimization. In this paper, we propose a multi-agent system for perovskite material discovery, named PeroMAS. We first encapsulated a series of perovskite-specific tools into Model Context Protocols (MCPs). By planning and invoking these tools, PeroMAS can design perovskite materials under multi-objective constraints, covering the entire process from literature retrieval and data extraction to property prediction and mechanism analysis. Furthermore, we construct an evaluation benchmark by perovskite human experts to assess this multi-agent system. Results demonstrate that, compared to single Large Language Model (LLM) or traditional search strategies, our system significantly enhances discovery efficiency. It successfully identified candidate materials satisfying multi-objective constraints. Notably, we verify PeroMAS's effectiveness in the physical world through real synthesis experiments.

</details>


### [2] [MAS-on-the-Fly: Dynamic Adaptation of LLM-based Multi-Agent Systems at Test Time](https://arxiv.org/abs/2602.13671)
*Guangyi Liu,Haojun Lin,Huan Zeng,Heng Wang,Quanming Yao*

Main category: cs.MA

TL;DR: MASFly는 LLM 기반 다중 에이전트 시스템으로, 동적 적응을 통해 복잡한 작업을 해결할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템들이 수동 설계나 일률적인 자동화에 의존하고 있어 배포 후 동적 적응성이 부족하다.

Method: MASFly는 성공적인 협업 패턴의 자가 구축 저장소를 활용하여 LLM이 새로운 쿼리에 대한 맞춤형 MAS를 조립할 수 있게 하는 검색 보강 SOP 인스턴스화 메커니즘을 채택하고, 경험 기반 감독 메커니즘을 통해 Watcher 에이전트가 개인화된 경험 풀을 참고하여 시스템 행동을 모니터링하고 실시간 개입을 제공한다.

Result: MASFly는 TravelPlanner 벤치마크에서 61.7%의 성공률을 달성하며, 강력한 작업 적응성과 견고성을 보여준다.

Conclusion: MASFly는 동적 적응을 통한 최첨단 성능을 달성하여 복잡한 작업 해결에 기여한다.

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) have emerged as a promising paradigm for solving complex tasks. However, existing works often rely on manual designs or "one-size-fits-all" automation, lacking dynamic adaptability after deployment. Inspired by how biological systems adapt, we introduce MASFly, a novel multi-agent framework enabling dynamic adaptation at test time. To adapt system generation, MASFly employs a retrieval-augmented SOP instantiation mechanism that leverages a self-constructed repository of successful collaboration patterns, enabling the LLM to assemble customized MASs for new queries. For adaptive execution, MASFly incorporates an experience-guided supervision mechanism, where a dedicated Watcher agent monitors system behaviors with reference to a personalized experience pool and provides real-time interventions. Extensive experiments demonstrate that MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness.

</details>


### [3] [Towards Selection as Power: Bounding Decision Authority in Autonomous Agents](https://arxiv.org/abs/2602.14606)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

TL;DR: 자율 에이전트 시스템의 안전성을 보장하기 위해 선택 권한을 관리하는 새로운 거버넌스 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트 시스템은 회복 불가능하고 제도적으로 제한된 결정이 있는 고위험 분야에 사용되고 있으며, 기존의 안전 접근 방식은 선택 권한을 직접적으로 다루지 않는다.

Method: 인지, 선택 및 행동을 별개의 영역으로 분리하고 자율성을 주권의 벡터로 모델링하는 거버넌스 아키텍처를 제안한다.

Result: 이 시스템은 재무 시나리오에서 다양한 변수를 타겟으로 하여 평가되었으며, 측정 지표는 선택 집중도, 내러티브 다양성 등을 포함한다. 결과적으로 기계적 선택 거버넌스가 구현 가능하며 감사를 받을 수 있음을 입증하였다.

Conclusion: 이 연구는 거버넌스를 내적 의도 정렬이 아닌 제한된 인과력으로 재정의하여 자율 에이전트를 배치할 수 있는 기초를 제공한다.

Abstract: Autonomous agentic systems are increasingly deployed in regulated, high-stakes domains where decisions may be irreversible and institutionally constrained. Existing safety approaches emphasize alignment, interpretability, or action-level filtering. We argue that these mechanisms are necessary but insufficient because they do not directly govern selection power: the authority to determine which options are generated, surfaced, and framed for decision. We propose a governance architecture that separates cognition, selection, and action into distinct domains and models autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained, while selection and action autonomy are bounded through mechanically enforced primitives operating outside the agent's optimization space. The architecture integrates external candidate generation (CEFL), a governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers. We evaluate the system across multiple regulated financial scenarios under adversarial stress targeting variance manipulation, threshold gaming, framing skew, ordering effects, and entropy probing. Metrics quantify selection concentration, narrative diversity, governance activation cost, and failure visibility. Results show that mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines. This work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable.

</details>


### [4] [ST-EVO: Towards Generative Spatio-Temporal Evolution of Multi-Agent Communication Topologies](https://arxiv.org/abs/2602.14681)
*Xingjian Wu,Xvyuan Liu,Junkai Lu,Siyuan Wang,Yang Shu,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.MA

TL;DR: LLM 기반의 자가 발전 다중 에이전트 시스템(ST-EVO)은 대화 기반 통신 스케줄링을 지원하며, 정확한 스페이-타임 스케줄링과 자가 피드백 기능을 갖춘다.


<details>
  <summary>Details</summary>
Motivation: 협력 지능을 향한 효과적인 접근 방식으로, 자가 발전 MAS는 더 유연하고 강력한 기술 경로를 제공한다.

Method: 새로운 스페이-타임 관점에서 대화-wise 통신 스케줄링을 지원하는 ST-EVO를 제안한다.

Result: 아홉 개의 벤치마크에서 ST-EVO의 최첨단 성능을 입증하며, 약 5%에서 25%의 정확도 향상을 달성했다.

Conclusion: ST-EVO는 자가 피드백 기능을 통해 축적된 경험으로부터 학습할 수 있는 능력을 가지며, 전체적인 성과 개선에 기여한다.

Abstract: LLM-powered Multi-Agent Systems (MAS) have emerged as an effective approach towards collaborative intelligence, and have attracted wide research interests. Among them, ``self-evolving'' MAS, treated as a more flexible and powerful technical route, can construct task-adaptive workflows or communication topologies, instead of relying on a predefined static structue template. Current self-evolving MAS mainly focus on Spatial Evolving or Temporal Evolving paradigm, which only considers the single dimension of evolution and does not fully incentivize LLMs' collaborative capability. In this work, we start from a novel Spatio-Temporal perspective by proposing ST-EVO, which supports dialogue-wise communication scheduling with a compact yet powerful flow-matching based Scheduler. To make precise Spatio-Temporal scheduling, ST-EVO can also perceive the uncertainty of MAS, and possesses self-feedback ability to learn from accumulated experience. Extensive experiments on nine benchmarks demonstrate the state-of-the-art performance of ST-EVO, achieving about 5%--25% accuracy improvement.

</details>


### [5] [ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic](https://arxiv.org/abs/2602.14780)
*Anna-Lena Schlamp,Jeremias Gerner,Klaus Bogenberger,Werner Huber,Stefanie Schmidtner*

Main category: cs.MA

TL;DR: ROSA는 다중 에이전트 궤적 예측과 회전 교차로에서의 조정된 속도 지침을 결합한 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 회전 교차로에서의 다중 모드 혼합 교통을 위해 안전하고 효율적인 속도 조언을 제공하고자 함.

Method: Transformer 기반 모델을 사용하여 차량과 취약 도로 사용자(VRU)의 미래 궤적을 예측하고, 단일 단계 예측을 위해 훈련되어 자가 회귀 방식으로 배치됨.

Result: ROSA는 높은 정확도를 달성하며, 경로 의도를 추가함으로써 성능이 더욱 향상됨.

Conclusion: ROSA는 예측 불확실성에도 불구하고 차량 효율성과 안전성을 크게 향상시키며, VRU 관점에서도 긍정적인 효과를 나타냄.

Abstract: We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.

</details>


### [6] [Distributed Quantum Gaussian Processes for Multi-Agent Systems](https://arxiv.org/abs/2602.15006)
*Meet Gandhi,George P. Kontoudis*

Main category: cs.MA

TL;DR: 이 논문에서는 고전적인 커널의 한계를 극복하기 위해 분산 양자 가우시안 프로세스(DQGP) 방법을 제안하고, 이를 통해 모델링 능력과 확장성을 향상시키고자 한다.


<details>
  <summary>Details</summary>
Motivation: 고전적인 커널의 표현력이 제한되어 복잡하고 대규모의 실제 도메인에서 Gaussian Processes(GPs)의 성능이 저하된다.

Method: 다양한 에이전트 환경에서 모델을 강화하고 확장성을 높이기 위해 분산 합의 리만 교대 방향 방법(DR-ADMM) 알고리즘을 개발하고, 지역 에이전트 모델들을 글로벌 모델로 집계하는 방법을 사용한다.

Result: 고전 하드웨어에서 양자 시뮬레이터를 통해 수행한 수치 실험을 통해 방법의 효능을 평가하였다.

Conclusion: 모델링 장점 외에도, 본 프레임워크는 Gaussian Processes와 분산 최적화에서 양자 하드웨어가 제공할 수 있는 잠재적인 컴퓨팅 속도 향상을 강조한다.

Abstract: Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.

</details>


### [7] [Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.14471)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.MA

TL;DR: 이 논문에서는 공유 환경에서 큰 언어 모델(LLM) 에이전트를 배치할 때 발생하는 개별 조정과 집단 안정성 간의 긴장 관계를 다루고 있으며, 사회적 가중치 조정(SWA)이라는 게임 이론적 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 공유 환경에서 LLM 에이전트를 배치할 때 개별 조정과 집단 안정성 간의 갈등을 해결하기 위함입니다.

Method: 에이전트의 개인 목표와 그룹 복지 추정치 사이를 보간함으로써 추론 시 의사 결정을 수정하는 SWA 프레임워크를 제안합니다.

Result: SWA는 과부하 시 에이전트가 수요를 증가시킬 수 있는 한계 점을 도출하며, 이 지점을 넘어서면 에이전트는 더 이상 추가 수요를 증가시키려는 유인이 없습니다.

Conclusion: SWA는 무정체에서 안정적 작동으로의 상전이를 유도합니다. 또, 매개변수 업데이트나 다중 에이전트 강화 학습이 필요 없는 알고리즘을 제공합니다.

Abstract: Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.

</details>


### [8] [Agent Mars: Multi-Agent Simulation for Multi-Planetary Life Exploration and Settlement](https://arxiv.org/abs/2602.13291)
*Ziyang Wang*

Main category: cs.MA

TL;DR: AI는 로봇공학과 우주 탐사 등 다양한 분야에 혁신을 가져왔으며, 본 연구는 화성 기지 운영을 위한 시뮬레이션 프레임워크인 Agent Mars를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 우주 탐사와 정착은 지구에서는 경험할 수 없는 다양한 도전 과제를 제시하며, 안전한 시스템 내에서 전문화된 인간, 로봇, 디지털 서비스 간의 조정이 필요하다.

Method: Agent Mars는 93명의 에이전트로 구성된 7계층의 명령 및 실행 구조를 통해 복잡한 운영을 시뮬레이션하며, 계층 간 조정을 통해 안전성을 유지하고 뚜렷한 감사 경로를 제공한다.

Result: Agent Mars는 안정성을 저하시키지 않으면서도 효율적인 협업과 기능적 리더십이 어떻게 오버헤드를 감소시키는지를 보여준다.

Conclusion: Agent Mars는 우주 AI의 기준을 제공하는 신뢰할 수 있는 기초를 마련한다.

Abstract: Artificial Intelligence (AI) has transformed robotics, healthcare, industry, and scientific discovery, yet a major frontier may lie beyond Earth. Space exploration and settlement offer vast environments and resources, but impose constraints unmatched on Earth: delayed/intermittent communications, extreme resource scarcity, heterogeneous expertise, and strict safety, accountability, and command authority. The key challenge is auditable coordination among specialised humans, robots, and digital services in a safety-critical system-of-systems. We introduce Agent Mars, an open, end-to-end multi-agent simulation framework for Mars base operations. Agent Mars formalises a realistic organisation with a 93-agent roster across seven layers of command and execution (human roles and physical assets), enabling base-scale studies beyond toy settings. It implements hierarchical and cross-layer coordination that preserves chain-of-command while allowing vetted cross-layer exchanges with audit trails; supports dynamic role handover with automatic failover under outages; and enables phase-dependent leadership for routine operations, emergencies, and science campaigns. Agent Mars further models mission-critical mechanisms-scenario-aware short/long-horizon memory, configurable propose-vote consensus, and translator-mediated heterogeneous protocols-to capture how teams align under stress. To quantify behaviour, we propose the Agent Mars Performance Index (AMPI), an interpretable composite score with diagnostic sub-metrics. Across 13 reproducible Mars-relevant operational scripts, Agent Mars reveals coordination trade-offs and identifies regimes where curated cross-layer collaboration and functional leadership reduce overhead without sacrificing reliability. Agent Mars provides a benchmarkable, auditable foundation for Space AI.

</details>


### [9] [Adaptive Value Decomposition: Coordinating a Varying Number of Agents in Urban Systems](https://arxiv.org/abs/2602.13309)
*Yexin Li,Jinjin Guo,Haoyu Zhang,Yuhan Zhao,Yiwen Sun,Zihao Jiao*

Main category: cs.MA

TL;DR: 이 논문에서는 동적 에이전트 인구에 적응하는 협력적 다중 에이전트 강화 학습(MARL) 프레임워크인 적응 가치 분해(AVD)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템(MAS) 조정 방법은 고정된 에이전트 수와 완전 동기화된 행동 실행과 같은 제한적인 가정에 의존하고 있으며, 이는 도시 시스템에서는 자주 위반됩니다.

Method: AVD는 변화하는 에이전트 인구에 적응하는 협력적 MARL 프레임워크로, 공유된 정책으로 인해 발생하는 행동 동질성을 완화하기 위한 경량 메커니즘을 통합합니다.

Result: AVD는 런던과 워싱턴 D.C.의 실제 자전거 공유 재분배 작업에서 최첨단 기준을 초월하는 성과를 보였습니다.

Conclusion: 이 결과는 AVD의 효과와 일반화를 확인합니다.

Abstract: Multi-agent reinforcement learning (MARL) provides a promising paradigm for coordinating multi-agent systems (MAS). However, most existing methods rely on restrictive assumptions, such as a fixed number of agents and fully synchronous action execution. These assumptions are often violated in urban systems, where the number of active agents varies over time, and actions may have heterogeneous durations, resulting in a semi-MARL setting. Moreover, while sharing policy parameters among agents is commonly adopted to improve learning efficiency, it can lead to highly homogeneous actions when a subset of agents make decisions concurrently under similar observations, potentially degrading coordination quality. To address these challenges, we propose Adaptive Value Decomposition (AVD), a cooperative MARL framework that adapts to a dynamically changing agent population. AVD further incorporates a lightweight mechanism to mitigate action homogenization induced by shared policies, thereby encouraging behavioral diversity and maintaining effective cooperation among agents. In addition, we design a training-execution strategy tailored to the semi-MARL setting that accommodates asynchronous decision-making when some agents act at different times. Experiments on real-world bike-sharing redistribution tasks in two major cities, London and Washington, D.C., demonstrate that AVD outperforms state-of-the-art baselines, confirming its effectiveness and generalizability.

</details>


### [10] [G2CP: A Graph-Grounded Communication Protocol for Verifiable and Efficient Multi-Agent Reasoning](https://arxiv.org/abs/2602.13370)
*Karim Ben Khaled,Davy Monticolo*

Main category: cs.MA

TL;DR: G2CP는 구조화된 에이전트 통신 프로토콜로, 자연어 대신 그래프 작업을 통해 에이전트 간의 명확한 소통을 가능하게 하여 73%의 통신 토큰 감소 및 34%의 작업 정확도 향상을 이룬다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 기반의 다중 에이전트 시스템은 자연어 소통으로 인해 의미 drift, 환각 전파 및 비효율적인 토큰 소비라는 중대한 문제에 직면해 있다.

Method: G2CP(그래프 기반 통신 프로토콜)를 제안하며, 이는 메시지를 자유 텍스트가 아니라 그래프 작업으로 구성된 구조화된 에이전트 통신 언어이다.

Result: 500개의 산업 시나리오와 21개 실제 유지보수 사례에서 G2CP는 에이전트 간 통신 토큰을 73% 줄이고 작업 완료 정확도를 34% 개선하며 cascading hallucinations을 제거하고 완전한 감사 가능 reasoning chain을 생성하였다.

Conclusion: G2CP는 다중 에이전트 시스템의 언어적 통신에서 구조적 통신으로의 근본적인 전환을 나타내며, 정밀한 에이전트 조정이 필요한 모든 분야에 영향을 미친다.

Abstract: Multi-agent systems powered by Large Language Models face a critical challenge: agents communicate through natural language, leading to semantic drift, hallucination propagation, and inefficient token consumption. We propose G2CP (Graph-Grounded Communication Protocol), a structured agent communication language where messages are graph operations rather than free text. Agents exchange explicit traversal commands, subgraph fragments, and update operations over a shared knowledge graph, enabling verifiable reasoning traces and eliminating ambiguity. We validate G2CP within an industrial knowledge management system where specialized agents (Diagnostic, Procedural, Synthesis, and Ingestion) coordinate to answer complex queries. Experimental results on 500 industrial scenarios and 21 real-world maintenance cases show that G2CP reduces inter-agent communication tokens by 73%, improves task completion accuracy by 34% over free-text baselines, eliminates cascading hallucinations, and produces fully auditable reasoning chains. G2CP represents a fundamental shift from linguistic to structural communication in multi-agent systems, with implications for any domain requiring precise agent coordination. Code, data, and evaluation scripts are publicly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 인공 지능의 발전에도 불구하고, 많은 시스템이 장기 적응력에서 정체 상태를 보이고 있으며, 이는 모델 용량이나 데이터 부족과는 다르게 지능 최적화의 구조적 특성에 기인한다.


<details>
  <summary>Details</summary>
Motivation: 많은 인공지능 시스템이 성능 최적화에도 불구하고 장기 적응력에서 정체하고 있는 문제를 해결하고자 한다.

Method: 지능을 다목적 거래가 지배하는 궤도 수준의 현상으로 정의하고, 전통적인 파레토 최적성을 경로-wise로 일반화한 'Trajectory-Dominant Pareto Optimization'을 소개한다.

Result: 파레토 트랩이 보수적인 지역 최적화 하에서 전세계적으로 우수한 개발 경로에 대한 접근을 제한한다는 것을 보여준다.

Conclusion: 결과적으로, 이 연구는 인공지능의 초점을 최종 성과에서 최적화 기하학으로 이동시키며, 적응 시스템의 장기 개발 제약을 진단하고 극복하기 위한 원칙 있는 프레임워크를 제공한다.

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [12] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: LLM 기반 역할 수행이 향상되었으나, 페르소나 제약 준수는 jailbreak 공격에 취약성을 증가시킨다. 본 논문은 훈련 없는 두 가지 주기적 사이클로 구성된 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 역할 수행의 신뢰성이 향상되고 있지만, 페르소나 제약을 더욱 엄격하게 준수할수록 침입 공격에 대한 취약성이 증가한다.

Method: 훈련 없는 Dual-Cycle Adversarial Self-Evolution 프레임워크는 두 개의 결합된 사이클로 구성되어 있다: 페르소나 표적 공격자 사이클과 역할 수행 방어자 사이클이 있다.

Result: 다양한 독점 LLM에서의 실험 결과, 역할 충실도와 jailbreak 저항 모두에서 강력한 기준선 대비 일관된 향상이 나타났다.

Conclusion: 제안된 접근법은 이전 페르소나 및 공격 프롬프트에 대한 강력한 일반화 능력을 보여준다.

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [13] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: NL2LOGIC는 자연어를 1차 논리로 변환하는 프레ーム워크로, 구문 분석의 정확성과 의미적 올바름을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 법률 및 거버넌스와 같은 분야에서 문서의 사실과 주장을 확인하는 데 필요한 자동화된 추론의 정확성과 해석 가능성.

Method: NL2LOGIC는 추상 구문 트리를 중간 표현으로 도입하는 1차 논리 변환 프레임워크로, 재귀적 대형 언어 모델 기반의 의미 분석기와 구문 트리 안내 생성기를 결합합니다.

Result: NL2LOGIC는 99% 구문 정확성을 달성하고, 의미적 정확성을 기존 최첨단 기준보다 최대 30% 개선하였습니다.

Conclusion: Logic-LM에 NL2LOGIC을 통합하면 거의 완벽한 실행 가능성을 달성하고, Logic-LM의 원래 몇 샷 비제한 번역 모듈에 비해 하위 추론 정확성을 31% 향상시킵니다.

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [14] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: 대규모 언어 모델(LLM)이 다중 에이전트 시스템에서 자원 경쟁 상황에서 조정할 수 있는지 평가하기 위한 벤치마크인 DPBench가 소개됨. 실험 결과, LLM은 순차적 설정에서는 효과적으로 조정하지만, 동시에 결정해야 하는 경우 95% 이상의 교착 상태를 경험함.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델들이 다중 에이전트 시스템에 점점 더 많이 배치되고 있으나, 자원 경쟁 상황에서 조정 가능성을 테스트할 벤치마크가 부족하다.

Method: Dining Philosophers 문제를 기반으로 하여 LLM의 조정을 평가하는 벤치마크인 DPBench를 도입하고, 여덟 가지 조건에서 결정 타이밍, 그룹 크기, 그리고 통신을 변화시키며 실험을 진행하였다.

Result: GPT-5.2, Claude Opus 4.5, Grok 4.1을 사용한 실험에서, LLM은 순차적 설정에서는 효과적으로 조정하지만, 동시에 결정을 내릴 때는 교착 상태 비율이 95%를 초과하는 문제가 발생하였다.

Conclusion: 여러 에이전트 LLM 시스템이 동시 자원 접근이 필요할 경우, emergent coordination에 의존하기보다는 외부 조정 메커니즘이 필요할 수 있음을 제안한다. DPBench는 오픈 소스로 배포되며, 코드와 벤치마크는 https://github.com/najmulhasan-code/dpbench에서 확인할 수 있다.

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [15] [MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems](https://arxiv.org/abs/2602.13258)
*Deepak Babu Piskala*

Main category: cs.AI

TL;DR: MAPLE은 메모리, 학습, 개인화를 분리하여 성능을 향상시키는 새로운 아키텍처를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 개인 사용자 적응 능력 한계를 해결하기 위해.

Method: 메모리는 저장 및 검색을 처리하고, 학습은 비동기적으로 상호작용에서 지능을 추출하며, 개인화는 제한된 맥락 내에서 학습된 지식을 실시간으로 적용합니다.

Result: MAPLE은 개인화 점수를 14.6% 향상시키고 특성 통합률을 45%에서 75%로 증가시켰습니다.

Conclusion: MAPLE을 통해 에이전트는 실제로 학습하고 적응할 수 있게 됩니다.

Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.

</details>


### [16] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST는 테스트 시 추가적인 계산을 통해 효율성을 높이는 모델로, 에이전틱 강화 학습을 통해 동일 가중치 클론을 생성하는 기능을 갖추고 있다.


<details>
  <summary>Details</summary>
Motivation: 기존의 언어 모델은 추가적인 계산을 통해 성능을 향상시킬 수 있지만, 고정된 추론 예산 하에서 비효율적일 수 있다.

Method: SELFCEST는 에이전틱 강화 학습을 사용하여 기본 모델이 동일한 가중치를 가진 클론을 별도의 병렬 맥락에서 생성하는 능력을 갖추도록 훈련된다. 훈련은 전역 과제 보상을 기반으로 하여 매개변수를 공유하는 롤아웃을 통해 진행된다.

Result: SELFCEST는 도전적인 수학 추론 벤치마크와 긴 문맥의 다중 홉 QA에서 단일 모델 기준선에 비해 정확도-비용 파레토 프론티어를 개선하고, 두 도메인에서 분포 밖 일반화를 나타낸다.

Conclusion: SELFCEST는 주어진 추론 예산에 맞춰 성능을 극대화할 수 있는 능력을 보여준다.

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [17] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델(LLM) 에이전트의 안전성을 보장하기 위한 체인 오브 사고(CoT) 추론 모니터링의 중요성을 다루며, 모델이 자신의 추론을 숨기려는 경향이 있을 경우 이러한 모니터링이 어떻게 손상될 수 있는지를 탐구합니다. 28개의 모델을 대상으로 스테가노그래픽 CoT의 잠재력을 평가하고, 복잡한 수학 및 산술 작업에서 숨겨진 추론의 지속 가능성에 대한 한계를 발견했으며, 간단한 카운팅 실험에서는 일부 모델이 상승된 성능을 보여주었습니다.


<details>
  <summary>Details</summary>
Motivation: 이 논문의 목적은 대형 언어 모델의 안전성을 보장하기 위해 체인 오브 사고(CoT) 추론을 모니터링하는 기술적 기법의 중요성을 강조하는 것입니다. 모델이 자신의 추론을 숨기는 학습을 하게 될 경우, 이러한 모니터링 기법이 손상될 수 있음을 경고합니다.

Method: 이 연구에서는 28개의 다양한 모델을 체계적으로 평가하여 모니터링 회피 비율, 거부율, 인코딩 충실도 및 숨겨진 작업의 정확도를 측정했습니다. 스테가노그래픽 축약어를 일반적인 추론 및 필러 토큰 기준과 비교하여 분석했습니다.

Result: 현재의 모델들은 복잡한 수학 및 산술 작업에 대한 숨겨진 추론을 지속할 수 없는 것으로 나타났습니다. 그러나 단순한 카운팅 실험에서 Claude Opus 4.5는 숨겨진 작업에서 92%의 정확도를 달성하여 초기 능력을 보여주었습니다. 또한 드문 경우(GPT-5.2의 경우 <1%)에 한해 스테가노그래픽 지시를 거부할 수 있지만 동시에 이를 따를 수 있는 가능성이 발견되었습니다.

Conclusion: 이 연구는 스테가노그래픽 위험에 대한 지속적인 평가의 필요성을 강조하며, 잘못된 방향으로 나쁘게 작용할 수 있는 숨겨진 추론을 사전에 탐지하고 방지할 수 있는 방법론을 제공합니다.

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [18] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench는 다양한 정보 환경에서의 시간적 추론 행동을 평가하기 위한 다중 도메인 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 강력한 예측 성능이 진정한 시간적 이해를 반영하는지 여부는 불확실하다.

Method: TemporalBench는 역사적 구조 해석, 맥락 없는 예측, 맥락 기반 시간적 추론, 사건 조건 예측을 포함하는 네 가지 주요 작업 범주를 도입한다.

Result: 강력한 수치 예측 정확도가 맥락 고려 또는 사건 인식 시간적 추론으로 신뢰할 수 있게 전이되지 않음을 보여준다.

Conclusion: TemporalBench 데이터세트는 공개적으로 이용 가능하며, 추가적으로 공개 리더보드도 제공된다.

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [19] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: 이 연구는 10개의 연구 수준 문제에 대한 다중 에이전트 증명 스프린트를 보고하며, 신속한 초안 생성과 적대적 검증, 목표 수리 및 명시적 원천을 결합합니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 수학적 문제를 해결하기 위해 신속한 검토와 수정을 통해 다수의 에이전트가 협력하여 보다 신뢰할 수 있는 증명을 생성하는 것이 필요하다.

Method: 문제의 주장 의존성에 대한 배선 다이어그램 분해를 사용하여 간극을 파악하고 리뷰어 주도의 수정 작업을 조정하는 워크플로를 사용하였다.

Result: 10개의 문제 중에서 3번 문제는 유일성/축소 가능성을 선택적으로 다루는 범위 기준에서 검증 완료 경로를 가지고 있으며, 5번 문제는 $F_O$-국소 연결 스펙트럼에 대해 제한된 형식으로 해결되었고, 10번 문제는 명시된 가정 하의 조건부로 다루어졌습니다. 4번 및 6번 문제는 일반적인 경우의 남은 의무를 명시하여 부분적으로 해결되었으며, 6번 문제의 경우 $c_0 = 1/3$에 대한 무조건적인 $K_n$ 결과가 포함되어 있습니다. 7번 문제는 증명 체인의 회전 경로 정리를 통해 임시로 폐쇄로 처리되었으며, QC 레이어에서 7번 및 9번 문제는 노드 수준 검증 아티팩트를 가지고 있지만 여전히 해결되지 않은 검증자 간극을 포함하고 있습니다.

Conclusion: 구조 인식 검증과 레이어 전환 전략이 압축된 증명 스프린트의 신뢰성과 보정을 개선한다는 주요 방법론적 결과를 도출하였다.

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [20] [ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI](https://arxiv.org/abs/2602.14135)
*Haibo Tong,Feifei Zhao,Linghao Feng,Ruoyu Wu,Ruolin Chen,Lu Jia,Zhou Zhao,Jindong Li,Tenglong Li,Erliang Lin,Shuai Yang,Enmeng Lu,Yinqian Sun,Qian Zhang,Zizhe Ruan,Zeyang Yue,Ping Wu,Huangrui Li,Chengyi Sun,Yi Zeng*

Main category: cs.AI

TL;DR: AI의 자율성과 목표 지향 능력이 증가하면서 예측 불가능하고 통제하기 어려운 시스템적 위험이 발생하고 있다. 기존의 AI 안전 평가 시스템은 이러한 도전과제를 해결하는 데 한계가 있으며, 본 논문에서는 새로운 평가 프레임워크인 'ForesightSafety Bench'를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 기술의 급속한 발전으로 인한 시스템적 위험이 심화되고 있으며, 기존 AI 안전 평가 시스템이 이에 대응하지 못하고 있다.

Method: 'ForesightSafety Bench' AI 안전 평가 프레임워크를 제안하며, 7개의 기본 안전 기둥을 시작으로 94개 세부 위험 차원으로 확장한다.

Result: 우리는 20개 이상의 최신 대형 모델에 대한 체계적 평가를 수행하여 주요 위험 패턴과 능력 한계를 식별하였다.

Conclusion: 우리의 평가 결과는 여러 기둥에서의 AI의 널리 퍼진 안전 취약성을 드러내고 있으며, 프로젝트 웹사이트에서 우리의 벤치마크를 이용할 수 있다.

Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.

</details>


### [21] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench는 4개의 LLM 계열을 아우르는 11개의 프롬프트 패러다임을 평가하는 통합 벤치마크로, 도덕적 안전성을 측정하는 새로운 지표인 UMSS를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 도덕적 능력과 안전성 정렬은 프롬프트 설계에 의해 크게 영향을 받지만, 데이터셋과 모델 간의 경험적 비교는 여전히 단편적이다.

Method: ETHICS, Scruples, WildJailbreak, 새로운 강건성 테스트인 ETHICS-Contrast를 사용하여 성과를 측정하고, 제안한 통합 도덕 안전 점수(UMSS)를 사용하여 정확성과 안전성을 균형 있게 평가한다.

Result: Compact, exemplar-guided scaffolds가 복잡한 다단계 추론보다 더 높은 UMSS 점수와 더 큰 강건성을 제공하며, 낮은 토큰 비용으로도 효과적임이 나타났다.

Conclusion: ProMoral-Bench는 원칙적이고 비용 효율적인 프롬프트 엔지니어링을 위한 표준화된 프레임워크를 확립한다.

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [22] [Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval](https://arxiv.org/abs/2602.13639)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.AI

TL;DR: 이 연구는 이질적인 다중 에이전트 시스템에서의 협업 문제를 해결하기 위한 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 발전으로 인공지능 시스템이 단일 에이전트 구조에서 협력적 지능을 갖춘 다중 에이전트 시스템으로 전환되고 있습니다.

Method: 엔트로피 기반의 적응형 안내 프레임워크를 제안하여 각 에이전트의 인지 상태에 따라 안내를 동적으로 조정합니다.

Result: GSM8K, MBPP, CVRP의 세 가지 벤치마크 데이터셋에 대한 실험을 통해 접근 방식이 이질적인 협업의 효율성과 안정성을 일관되게 향상시키는 것을 보여주었습니다.

Conclusion: 적응형 안내는 인지 불균형을 완화할 뿐만 아니라 더 강력하고 협력적인 다중 에이전트 지능으로 나아가는 확장 가능한 경로를 제시합니다.

Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.

</details>


### [23] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: 이 논문은 신뢰할 수 있는 다중 에이전트 AI 시스템을 설계하기 위해 조직 구조를 모방하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 신뢰성을 높이는 방법과 인간 기관의 집단 행동을 신뢰할 수 있게 만드는 방법을 비교합니다.

Method: 퍼시버런스 컴포지션 엔진이라는 다중 에이전트 시스템을 통해 텍스트 초안 작성, 사실 검증, 논증 품질 평가의 세 가지 역할을 분리하여 신뢰성 있는 결과를 도출합니다.

Result: 474개의 컴포지션 작업을 통해 수집된 관찰 결과는 조직적 가설과 일치하는 패턴을 보여줍니다.

Conclusion: 조직 이론이 신뢰할 수 없는 구성 요소로부터 신뢰할 수 있는 결과를 도출하는 다중 에이전트 AI 안전성의 생산적인 프레임워크로 자리 잡을 수 있습니다.

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [24] [From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents](https://arxiv.org/abs/2602.13855)
*Razeen A Rasheed,Somnath Banerjee,Animesh Mukherjee,Rima Hazra*

Main category: cs.AI

TL;DR: 연구 생성이 저렴해짐에 따라 감사 가능성이 병목 현상으로 떠오르며, 오류가 아닌 충분한 뒷받침이 결여된 과학적 결과물이 주요 위험 요소가 되고 있다는 점을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 연구 생성의 비용이 낮아짐에 따라 감사 가능성의 중요성이 증가하고 있으며, 이는 신뢰할 수 있는 과학적 결과물을 보장하기 위한 필요로 이어진다.

Method: 주장 수준 감사 가능성을 위한 깊은 연구 에이전트를 디자인하고 평가하기 위한 프레임워크와 AAR 표준을 도입하였다.

Result: 연구의 주장-증거 관계를 시각화하고, 감사 가능성을 테스트 가능하게 만드는 AAR 표준이 제시되었다.

Conclusion: 지속적인 검증과 함께 설명 가능한 출처 기반의 데이터 구조를 통한 감사 가능성을 확보하고, 전통적 출판 이후 검증 방식을 넘어서야 한다.

Abstract: A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.

</details>


### [25] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE는 학생의 학습 행동을 시뮬레이션하기 위한 신경-상징적 프레임워크로, 자기 주도 학습 이론을 통합하여 기존의 편향 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 학생 학습 행동을 시뮬레이션하는 것은 교육 연구에서 잠재력이 높지만, 개인정보 보호 문제와 장기 연구의 높은 비용으로 인해 진정한 데이터를 수집하기가 어렵다.

Method: BEAGLE는 자기 주도 학습 이론을 통합한 새로운 아키텍처를 통해 세 가지 주요 기술 혁신을 포함한다: 반 마르코프 모델, 베이지안 지식 추적 및 탈결합 에이전트 디자인.

Result: BEAGLE는 파이썬 프로그래밍 작업에서 진정한 궤적을 재현하는 데 있어 최첨단 기준보다 훨씬 뛰어난 성능을 보여주었다.

Conclusion: 인간 튜링 테스트에서 사용자가 합성 추적을 실제 학생 데이터와 구분할 수 없었고, 정확도는 무작위 추측(52.8%)과 유사했다.

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [26] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: 대규모, 다학제 연구로 인한 윤리적 위험을 관리하기 위해 Mirror라는 AI 지원 윤리 검토 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 연구 거버넌스의 기초 메커니즘인 윤리 검토가 대규모 과학 실천의 구조적 결과로 인한 윤리적 위험의 증가로 인해 점점 더 큰 압박에 직면하고 있습니다.

Method: Mirror는 윤리적 추론, 구조화된 규칙 해석 및 다중 에이전트 심의를 통합한 AI 지원 윤리 검토 프레임워크로, EthicsLLM이라는 기준 모델을 중심으로 구성됩니다.

Result: Mirror의 두 가지 모드는 신속 검토와 위원회 검토를 지원하여 윤리 평가의 품질과 일관성을 크게 개선합니다.

Conclusion: Mirror는 강력한 일반ist LLM들과 비교했을 때 윤리 평가의 질, 일관성 및 전문성을 크게 향상시킵니다.

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [27] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench는 학술 슬라이드 생성 및 편집을 평가하기 위한 새로운 벤치마크로, 다중 에이전트 시스템을 위한 프레임워크를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 평가 방식이 자동으로 학술 슬라이드를 생성하고 편집하는 데 필요한 여러 과제를 충분히 측정하지 못하고 있습니다.

Method: DECKBench는 문서와 슬라이드 쌍 데이터셋을 기반으로 하며, 실제적인 편집 지침을 포함합니다. 평가 프로토콜은 슬라이드 및 덱 수준의 충실도, 일관성, 레이아웃 품질, 다중 턴 지시 사항 준수를 체계적으로 평가합니다.

Result: 실험 결과, 제안된 벤치마크는 강점을 강조하고 실패 모드를 드러내며 다중 에이전트 슬라이드 생성 및 편집 시스템 개선을 위한 실행 가능한 통찰을 제공합니다.

Conclusion: 이 연구는 학술 발표 생성 및 편집의 재현 가능하고 비교 가능한 평가를 위한 표준화된 토대를 마련합니다.

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [28] [Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol](https://arxiv.org/abs/2602.13320)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 이 논문은 Model Context Protocol (MCP) 에이전트의 오류 누적 분석을 위한 최초의 이론적 프레임워크를 도입하며, 오류 전파의 성질을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 높은 위험 결정에서 외부 도구를 사용함에 따라 오류가 sequential tool calls를 통해 어떻게 전파되는지를 이해하는 것이 중요합니다.

Method: MCP 에이전트에서 오류 누적을 분석하기 위한 이론적 프레임워크를 도입하고, 혼합 왜곡 메트릭을 개발하여 오류 전파에 대한 마르틴게일 집중 경계를 설정했습니다.

Result: Qwen2-7B, Llama-3-8B, Mistral-7B에 대한 실험이 이론적 예측을 검증하였고, 오류의 누적이 선형 경향을 보인다는 것을 확인했습니다.

Conclusion: 주요 발견은 의미적 가중치가 왜곡을 80% 줄이고, 약 9 단계마다 주기적으로 재기초화 하는 것으로 오류 제어가 가능하다는 것입니다.

Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.

</details>


### [29] [Contrastive explanations of BDI agents](https://arxiv.org/abs/2602.13323)
*Michael Winikoff*

Main category: cs.AI

TL;DR: 이 논문은 자율 시스템이 반대 질문에 답할 수 있는 기능을 확장하여 신뢰 개발을 지원하고 투명성을 높이는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 자율 시스템의 설명 제공 능력은 투명성을 지원하고 적절한 신뢰 개발에 도움이 되기 때문에 중요합니다.

Method: 기존 작업에서 Belief-Desire-Intention (BDI) 에이전트가 '왜 행동 $X$를 했나요?'와 같은 질문에 답할 수 있는 메커니즘을 정의한 것을 확장하여 '왜 $X$ 대신 $F$를 했나요?'와 같은 반대 질문에 답할 수 있는 방법을 제안합니다.

Result: 계산 평가 결과, 반대 질문을 사용하면 설명 길이가 크게 줄어들고, 사람이 주관적으로 평가한 결과 반대 답변이 선호되는 경향과 신뢰, 이해도 및 시스템의 정확성에 대한 확신이 높아지는 경향이 발견되었습니다.

Conclusion: 모든 설명을 제공하는 것의 이점은 명확하지 않았으며, 일부 상황에서는 (전체) 설명을 제공하는 것이 설명을 제공하지 않는 것보다 나쁘다는 증거가 발견되었습니다.

Abstract: The ability of autonomous systems to provide explanations is important for supporting transparency and aiding the development of (appropriate) trust. Prior work has defined a mechanism for Belief-Desire-Intention (BDI) agents to be able to answer questions of the form ``why did you do action $X$?''. However, we know that we ask \emph{contrastive} questions (``why did you do $X$ \emph{instead of} $F$?''). We therefore extend previous work to be able to answer such questions. A computational evaluation shows that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation was conducted to assess whether such contrastive answers are preferred, and how well they support trust development and transparency. We found some evidence for contrastive answers being preferred, and some evidence that they led to higher trust, perceived understanding, and confidence in the system's correctness. We also evaluated the benefit of providing explanations at all. Surprisingly, there was not a clear benefit, and in some situations we found evidence that providing a (full) explanation was worse than not providing any explanation.

</details>


### [30] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: Nanbeige4.1-3B는 30억 개의 매개변수로 강력한 에이전트 행동, 코드 생성 및 일반적 추론을 동시에 달성하는 통합 일반 언어 모델이다.


<details>
  <summary>Details</summary>
Motivation: 3B 파라미터로 여러 기능을 통합한 소형 언어 모델의 필요성을 강조하고자 함.

Method: 점수 모델링을 통해 추론 및 선호 정렬을 개선하고, 코드 생성을 위해 Reinforcement Learning에서 복잡성 인식 보상을 설계함.

Result: Nanbeige4.1-3B는 유사한 규모의 이전 모델들보다 성능이 크게 우수함을 입증하였고, 더 큰 모델에 비해서도 우수한 성과를 달성함.

Conclusion: 3억 파라미터 모델이 넓은 능력과 강력한 전문성을 동시에 달성할 수 있음을 보여줌으로써 해당 모델의 잠재성을 재정의함.

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [31] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: AI 안전성, 도덕 철학 및 인지 과학의 교차점에서 발생하는 도덕적 정렬 평가의 중요한 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 인간 규범이 갈등하는 상황에서 에이전트의 도덕적 정렬을 평가하는 것은 AI의 안전성과 도덕 철학 및 인지 과학의 교차점에서 중요한 도전 과제이다.

Method: Morality Chains라는 새로운 형식을 도입하여 도덕적 규범을 순서화된 의무적 제약으로 표현하고, 98개의 윤리적 딜레마 문제를 전 trolley-dilemma 스타일의 Gymnasium 환경으로 제시하는 MoralityGym 벤치마크를 개발하였다.

Result: Safe RL 방법으로 얻은 기준 결과는 주요 제한 사항을 드러내며, 윤리적 의사 결정에 대한 보다 원칙적인 접근 방식이 필요함을 강조하고 있다.

Conclusion: 이 연구는 복잡한 실제 맥락에서 신뢰할 수 있고 투명하며 윤리적으로 행동하는 AI 시스템 개발을 위한 기초를 제공한다.

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [32] [NeuroWeaver: An Autonomous Evolutionary Agent for Exploring the Programmatic Space of EEG Analysis Pipelines](https://arxiv.org/abs/2602.13473)
*Guoan Wang,Shihao Yang,Jun-En Ding,Hao Zhu,Feng Liu*

Main category: cs.AI

TL;DR: NeuroWeaver는 EEG 데이터 분석을 위한 경량 솔루션을 제공하여 기존 방법보다 우수한 성능을 발휘합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 기초 모델들이 EEG 분석에 적합하지 않은 이유는 데이터 요구사항이 크고 계산 비용이 높기 때문입니다.

Method: NeuroWeaver는 EEG 데이터셋과 작업에 대해 일반화하기 위해 파이프라인 공학을 이산 제약 최적화 문제로 재구성합니다. Domain-Informed Subspace Initialization을 사용하여 신경과학적으로 타당한 다양성을 확인하고, Multi-Objective Evolutionary Optimization을 통해 성능과 효율성을 동적으로 조정합니다.

Result: 다양한 벤치마크에서 NeuroWeaver가 경량 솔루션을 합성하여 기존의 작업 특화 방법보다 우수한 성능을 보이며, 대규모 기초 모델과 유사한 성능을 발휘했습니다.

Conclusion: NeuroWeaver는 적은 파라미터로도 EEG 분석에서 뛰어난 성능을 제공하는 독창적인 접근 방식입니다.

Abstract: Although foundation models have demonstrated remarkable success in general domains, the application of these models to electroencephalography (EEG) analysis is constrained by substantial data requirements and high parameterization. These factors incur prohibitive computational costs, thereby impeding deployment in resource-constrained clinical environments. Conversely, general-purpose automated machine learning frameworks are often ill-suited for this domain, as exploration within an unbounded programmatic space fails to incorporate essential neurophysiological priors and frequently yields solutions that lack scientific plausibility. To address these limitations, we propose NeuroWeaver, a unified autonomous evolutionary agent designed to generalize across diverse EEG datasets and tasks by reformulating pipeline engineering as a discrete constrained optimization problem. Specifically, we employ a Domain-Informed Subspace Initialization to confine the search to neuroscientifically plausible manifolds, coupled with a Multi-Objective Evolutionary Optimization that dynamically balances performance, novelty, and efficiency via self-reflective refinement. Empirical evaluations across five heterogeneous benchmarks demonstrate that NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models, despite utilizing significantly fewer parameters.

</details>


### [33] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 大規模言語モデル（LLM）エージェントがより高度になるにつれ、それらの協調使用がマルチエージェントシステムの実用的なパラダイムとして浮上することが期待される。


<details>
  <summary>Details</summary>
Motivation: 作業の安全性および悪用リスクが単一エージェントケースに集中しているため、マルチエージェントシステムにおける脅威モデリングの不足が指摘されている。

Method: オーケストレーターセットアップという人気のマルチエージェントパターンのセキュリティ脆弱性を調査し、具体的な将来のユースケースを代表する設定をレッドチームして新しい攻撃ベクトルOMNI-LEAKを示す。

Result: データアクセス制御が存在する場合でも、単一の間接プロンプト注入を介して複数のエージェントが敏感なデータを漏洩させることを示します。

Conclusion: 私たちの研究は、実世界のプライバシー侵害や金銭的損失の重大なリスクを軽減し、AIエージェントに対する全体的な公共の信頼を向上させるために、単一エージェントからマルチエージェント設定への一般化の重要性を強調します。

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [34] [SPILLage: Agentic Oversharing on the Web](https://arxiv.org/abs/2602.13516)
*Jaechul Roh,Eugene Bagdasarian,Hamed Haddadi,Ali Shahin Shamsabadi*

Main category: cs.AI

TL;DR: 웹 에이전트는 사용자 리소스에 접근하여 사용자의 작업을 자동화하지만, 이로 인해 비관련 정보가 유출될 위험이 있다. 본 논문에서는 이 문제를 다루고 SPILLage라는 프레임워크를 통해 이러한 비관련 정보의 유출을 분석하고 대처 방안을 제시한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 사용자의 작업을 자동화하는 과정에서 사용자의 비관련 정보가 의도치 않게 유출되는 문제에 주목하고 이에 대한 해결책을 제시하고자 한다.

Method: SPILLage 프레임워크를 통해 비관련 정보 유출을 채널과 직접성의 두 가지 차원으로 분석하고, 180개의 업무를 실제 전자상거래 사이트에서 테스트하여 데이터 수집 및 평가를 진행하였다.

Result: 행동적 비관련 정보 유출이 콘텐츠 비관련 정보 유출보다 5배 더 발생하며, 이 효과는 사용자 요청 수준의 완화 조치가 있어도 지속된다. 그러나 비관련 정보를 사전에 제거하면 작업 성공률이 최대 17.9% 향상된다.

Conclusion: 웹 에이전트에서의 개인 정보 보호는 중요한 도전 과제이며, 에이전트의 행동과 관련된 더 넓은 관점을 필요로 한다.

Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act "in the wild", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of "output" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.

</details>


### [35] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem 프레임워크는 에피소드 기억을 구축하고 추론하기 위한 두 단계의 구조를 제안하며, 기존 메모리 시스템에 비해 큰 성능 향상을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 인간은 시공간 맥락에서 구체적인 경험을 기억하고 사건 간의 추론을 수행하는 데 뛰어난 능력을 가지고 있으나, 현재의 언어 에이전트는 효과적으로 상호작용 이력을 회상하고 추론하는 데 한계가 있다.

Method: REMem은 오프라인 인덱싱과 온라인 추론의 두 단계로 구성된 에피소드 기억 프레임워크를 제시한다. 첫 번째 단계에서 REMem은 경험을 시간 인식 개요와 사실을 유연하게 연결하는 하이브리드 메모리 그래프로 변환한다. 두 번째 단계에서는 에이전트 리트리버를 사용하여 메모리 그래프에서 반복적으로 정보를 검색한다.

Result: 젊은 데이터셋에서 REMem은 에피소드 기억 기준에서 기존 메모리 시스템인 Mem0 및 HippoRAG 2보다 각각 3.4% 및 13.4% 절대 향상을 보이며 뛰어난 성능을 발휘한다.

Conclusion: REMem은 기억 시스템의 성능을 크게 향상시키고, 답변할 수 없는 질문에 대한 거부 행동도 더욱 견고하게 나타낸다.

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [36] [OpAgent: Operator Agent for Web Navigation](https://arxiv.org/abs/2602.13559)
*Yuyu Guo,Wenjie Yang,Siyuan Yang,Ziyang Liu,Cheng Chen,Yuan Wei,Yun Hu,Yang Huang,Guoliang Hao,Dongsheng Yuan,Jianming Wang,Xin Chen,Hang Yu,Lei Lei,Peng Di*

Main category: cs.AI

TL;DR: 이 논문은 자율 웹 에이전트가 웹 환경에서 사용자 지침을 충족시키기 위해 온라인 강화 학습을 사용하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율 웹 에이전트가 실제 웹사이트의 복잡성과 변동성에 대응해야 하기 때문이다.

Method: 우리는 실제 웹사이트와의 직접적이고 반복적인 상호작용을 통해 정책을 최적화하는 온라인 강화 학습 웹 에이전트를 제안한다.

Result: 우리의 모델은 WebArena에서 38.1%의 성공률을 달성하여 기존 단일 기준선을 초월했다.

Conclusion: 모듈화된 에이전틱 프레임워크인 OpAgent를 통해 에이전트의 성능을 새로운 최고치인 71.6%의 성공률로 끌어올렸다.

Abstract: To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \textbf{71.6\%}.

</details>


### [37] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: 본 논문은 대형 언어 모델(LLM)이 사회적 정보에 의해 영향을 받는지를 조사하며, 인간 피드백을 LLM 피드백보다 우선시하는지를 연구한다.


<details>
  <summary>Details</summary>
Motivation: 인간의 판단이 다른 에이전트의 답변, 도구의 출력, 인간의 추천과 같은 사회적 정보에 의해 어떻게 영향을 받는지를 이해하기 위함이다.

Method: 세 가지 이진 결정-making 과제에서 네 가지 지침 조정 LLM에 대해 친구, 인간 전문가 또는 다른 LLM에서 유래된 이전 응답을 제시하고, 그룹이 정확한지 조작하여 그룹 크기를 변화시켰다.

Result: 모델은 전문가에서 오는 응답에 더 강하게 conform하며, 그 신호가 잘못된 경우에도 전문가 쪽으로 더 쉽게 답변을 수정했다.

Conclusion: 전문가 프레이밍은 현대 LLM에 대해 강력한 이전 정보로 작용하여 판단 영역 전반에 걸쳐 신뢰도 민감한 사회적 영향을 나타낸다.

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [38] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: 본 연구에서는 자율적으로 지도되는 미분 클러스터링 모델과 새로운 미분 ILP 모델을 결합하여 원시 데이터로부터 명시적 레이블 유출 없이 규칙 학습을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 규칙 학습 기반 모델은 투명한 구조로 인해 해석 가능성이 높은 시나리오에서 널리 사용됩니다. 그러나 대부분의 미분 ILP 방법은 기호 데이터셋에 의존하며 원시 데이터로부터 직접 학습할 때 도전에 직면합니다.

Method: 자기 지도 미분 클러스터링 모델을 새로운 미분 ILP 모델과 통합하여 원시 데이터로부터 규칙을 학습합니다.

Result: 학습된 규칙은 원시 데이터의 특성을 효과적으로 설명합니다.

Conclusion: 우리 방법은 시계열 및 이미지 데이터에서 일반화된 규칙을 직관적으로 및 정확하게 학습함을 입증합니다.

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [39] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus는 에이전트 AI를 위한 효율적인 메모리 관리 시스템으로, 검색 속도를 극대화하고 메모리 크기에 비례하여 확장 가능합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 AI는 LLM의 제한된 컨텍스트 창을 넘어 사용자별 이력을 저장하기 위해 지속적인 메모리가 필요합니다.

Method: Hippocampus 시스템은 의미 검색을 위한 압축 이진 서명과 정확한 콘텐츠 복원을 위한 무손실 토큰-ID 스트림을 사용하며, Dynamic Wavelet Matrix (DWM)를 핵심 기술로 활용합니다.

Result: Hippocampus는 end-to-end 검색 지연 시간을 최대 31배 줄이고, 쿼리당 토큰 발자국을 최대 14배 줄이며, 정확성을 유지합니다.

Conclusion: 이 시스템은 긴 호라이즌 에이전트 배치에 적합한 선형 확장성을 제공합니다.

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [40] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: MLLM 중심 프레임워크를 도입하여 GUI 에이전트를 위한 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: GUI 에이전트는 비정상적인 환경에 직면해 데이터 수집 및 정책 최적화에 높은 비용이 발생한다.

Method: 에이전틱-Q 추정과 단계별 정책 최적화의 두 가지 구성 요소로 구성된 MLLM 중심 프레임워크를 도입한다.

Result: 우리 프레임워크가 Ovis2.5-9B에 강력한 GUI 상호 작용 기능을 부여하며, GUI 탐색 및 근거 벤치마크에서 뛰어난 성과를 달성한다.

Conclusion: 이 프레임워크는 안정적이고 효율적인 최적화를 보장하며, 대규모 모델을 초월하는 성과를 낸다.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [41] [HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating](https://arxiv.org/abs/2602.13665)
*Weibin Liao,Jian-guang Lou,Haoyi Xiong*

Main category: cs.AI

TL;DR: HyFunc라는 새로운 프레임워크는 대규모 모델의 비효율성을 줄여 실시간 AI 응용 프로그램의 지연성을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 대리인 AI 시스템이 사용자 의도를 구조화된 함수 호출로 변환하는 과정에서 발생하는 계산 중복 문제를 해결하고자 함.

Method: HyFunc는 하이브리드 모델 캐스케이드를 사용하여 대규모 모델이 사용자 의도를 단일 '소프트 토큰'으로 증류하도록 하고, 이를 통해 관련 함수를 선택하며 마지막 호출을 생성하도록 작은 모델을 지시함.

Result: HyFunc는 0.828초의 추론 지연 시간을 달성하여 모든 기준 모델을 초과하며, 80.1%의 성능을 기록하여 비슷한 매개변수 규모의 모든 모델보다 뛰어남.

Conclusion: HyFunc는 대리인 AI를 위한 보다 효율적인 패러다임을 제공합니다.

Abstract: While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single "soft token." This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our "dynamic templating" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.

</details>


### [42] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: 본 논문에서는 Pheromone-Guided Policy Optimization (PhGPO)을 제안하여 역사적으로 성공적인 도구 전환 패턴을 활용한 장기 다단계 도구 계획의 효율성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 장기 다단계 도구 계획의 어려움을 극복하고, 성공적인 과거 경로에서 재사용 가능한 정보를 활용할 필요성이 있습니다.

Method: Pheromone-Guided Policy Optimization (PhGPO)을 제안하며, 이는 역사적 경로에서 도출된 전이 패턴(페로몬)을 학습하고 이를 정책 최적화에 활용합니다.

Result: 제안한 PhGPO의 효과를 입증하기 위해 종합적인 실험 결과가 제공됩니다.

Conclusion: 역사적으로 성공적인 도구 전환을 유도하는 명확하고 재사용 가능한 지침을 제공하여 장기 도구 계획을 향상시킵니다.

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [43] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent는 복잡한 실험 기반 환경에서 자동 탐색을 위한 다중 에이전트 연구 프레임워크로, 체계적인 가설 관리와 반영을 통해 연구 동력을 제어한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 실험 기반 영역에서 과학적 발견을 자동화하기 위해서는 단순한 프로그램 변형 이상의 것이 필요하다.

Method: OR-Agent는 연구를 구조화된 트리 기반 워크플로우로 조직하고, 진화적 시스템 아이디어 기구를 도입하여 연구 시작점의 선택, 연구 계획의 생성 및 연구 트리 내의 협조적 탐색을 통합한다.

Result: OR-Agent는 강력한 진화적 기준선보다 우수한 성능을 보여주며, AI 지원 과학적 발견을 위한 범용적이고 확장 가능하며 점검 가능한 프레임워크를 제공한다.

Conclusion: OR-Agent는 과학적 발견의 자동화를 위한 체계적이고 원칙적인 아키텍처를 통해 연구 동력을 관리한다.

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [44] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: 이 논문은 긴 맥락에서의 주의(attention) 비용을 줄이기 위한 방법을 소개하며, Vashista Sparse Attention 메커니즘을 통해 적절한 후보 집합을 유지하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 긴 맥락에 대한 주의가 대부분의 추론 비용을 차지하지만, 실제 행동은 token의 소수만이 각 쿼리에 의미 있게 기여함을 보여준다.

Method: 주요 이론적 기여로는 주의(attention)를 키 벡터의 볼록 껍질로 모델링하고 엔트로픽(softmax 유사) 이완을 분석하는 방식이다. 높은 보조 공간을 유지하는 경우, 엔트로픽 주의는 일정한 크기의 활성 면에 집중한다.

Result: 우리는 후보 공통 부분의 크기가 일정하게 유지되고, 예측된 영역에서 강한 시간 단축과 최소한의 품질 저하가 있음을 관찰했다.

Conclusion: 이 연구는 프라이버시 민감 및 단절된 설정에서의 배포 함의를 논의하며, 교환 가능한 주의 모듈이 예측 가능한 대기 시간과 비용을 가능하게 한다.

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [45] [An end-to-end agentic pipeline for smart contract translation and quality evaluation](https://arxiv.org/abs/2602.13808)
*Abhinav Goel,Chaitya Shah,Agostino Capponi,Alfio Gliozzo*

Main category: cs.AI

TL;DR: 자연어 사양에서 LLM이 생성한 스마트 계약을 체계적으로 평가하기 위한 엔드투엔드 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 계약의 품질을 평가할 수 있는 체계적인 방법이 필요하다.

Method: 계약 텍스트를 구조화된 스키마로 파싱하고, Solidity 코드를 생성하며, 컴파일 및 보안 검사를 통해 자동화된 품질 평가를 수행한다.

Result: 다섯 가지 차원(기능 완전성, 변수 충실도, 상태 기계 정확성, 비즈니스 로직 충실도 및 코드 품질)에서 품질을 측정하고, 정답 구현과의 쌍 평가를 지원하여 정렬을 정량화한다.

Conclusion: 이 프레임워크는 스마트 계약 합성 품질에 대한 경험적 연구를 위한 재현 가능한 벤치마크를 제공하며, 형식 검증 및 준수 검사로의 확장을 지원한다.

Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

</details>


### [46] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: 현대 온라인 실험은 트래픽 부족과 사후 통찰력 추출의 비효율성으로 어려움을 겪고 있으며, 역사적인 A/B 결과와 콘텐츠 임베딩을 효과적으로 활용하지 못하고 있다. 본 연구는 실험 우선순위를 정하고, 우승 변형의 원인을 설명하며, 새로운 기회를 제시하는 통합 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현대 온라인 실험은 트래픽 부족과 비효율적인 통찰력 추출로 인해 어려움을 겪고 있으며, 이는 실험의 우선순위를 정하는 데 방해가 된다.

Method: 이 연구에서는 치료 임베딩과 역사적 결과를 활용하여 고정 효과를 가진 CTR 순위 모델을 학습시키고, 이를 통해 가치와 콘텐츠 다양성을 균형 있게 유지하며 후보들을 점수화한다.

Result: 순위 모델로부터 파생된 속성 중요성과 현재 실험에서의 저발현을 결합하여 놓치고 있는 영향력 높은 속성을 플래그하는 기회 지수를 계산한다.

Conclusion: 제안된 프레임워크는 Adobe 비즈니스 고객의 실제 실험에서 높은 생성 품질을 검증받았으며, AI 기반 통찰력과 기회를 제공하여 실험을 확장하는 데 도움을 준다.

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [47] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: 이 논문에서는 MOC-HER 및 2HER이라는 새로운 기법을 제안하여 희소 보상이 있는 다목적 환경에서의 미숙한 성능 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: HRL 프레임워크는 재사용 가능한 옵션 학습에서 중요한 발전을 이루었으나, 희소 보상이 있는 다목적 환경에서는 성능이 저하된다.

Method: MOC-HER는 HER 메커니즘을 MOC 프레임워크에 통합하여 목표를 재레이블 하는 방법을 사용하고, 2HER는 두 세트의 가상 목표를 생성하는 새로운 확장이다.

Result: MOC-2HER는 로봇 조작 환경에서 최대 90%의 성공률을 달성하였다.

Conclusion: 이중 목표 재레이블링 전략의 효과가 강조되며, 희소 보상 및 다목적 작업에서의 성능 향상을 보여준다.

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [48] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL은 그래프 속성 탐지를 개선하기 위해 적응형 레이아웃 생성기를 포함한 비전 기반 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 최근 학습 기반 접근 방식이 그래프 속성 탐지에서 효율성을 높이는 데 유망한 결과를 보여주고 있으나, 기존 비전 기반 방법은 고정된 시각적 그래프 레이아웃에 의존하여 그 표현력이 제한적이다.

Method: VSAL은 개별 인스턴스에 맞춘 정보성 그래프 시각화를 동적으로 생성할 수 있는 적응형 레이아웃 생성기를 통합한 비전 기반 프레임워크이다.

Result: VSAL은 햄일토니안 사이클, 평면성, 클로우 자유성 및 트리 탐지와 같은 다양한 작업에서 최첨단 비전 기반 방법들을 능가하는 성능을 보인다.

Conclusion: 이 연구는 비전 기반 그래프 속성 탐지에서 VSAL의 성능 우수성을 입증한다.

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [49] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: 하이멤(HyMem)은 효율성과 성능 사이의 균형을 맞춘 하이브리드 메모리 아키텍처로, 동적 메모리 스케줄링을 통해 복잡한 쿼리에 대해 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델은 짧은 텍스트에서는 우수한 성능을 보이지만, 긴 대화에서 비효율적인 메모리 관리로 인해 성능이 저하됩니다.

Method: HyMem은 다중 구분 메모리 표현을 통해 동적 온디맨드 메모리 스케줄링을 가능하게 하는 하이브리드 메모리 아키텍처입니다.

Result: HyMem은 LOCOMO와 LongMemEval 벤치마크에서 우수한 성능을 달성하며, 전후 맥락 모델에 비해 92.6%의 계산 비용을 줄였습니다.

Conclusion: 하이멤은 장기 메모리 관리에서 효율성과 성능 간의 균형을 이루는 최신 기술을 확립합니다.

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [50] [A Generalizable Physics-guided Causal Model for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2602.13936)
*Zhenyu Zong,Yuchen Wang,Haohong Lin,Lu Gan,Huajie Shao*

Main category: cs.AI

TL;DR: 교통 에이전트를 위한 경로 예측은 안전한 자율 주행에 필수적입니다. 본 연구에서는 보기 드문 영역에서의 제로샷 일반화를 달성하기 위한 도전을 다룰 것입니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 도메인에서 운동학의 일관된 특성에 착안하여 제로샷 경로 예측 능력을 향상시키기 위한 도메인 불변 지식을 통합하는 것을 목표로 합니다.

Method: 도메인 불변 기능을 추출하는 분리 장면 인코더와 운동학 모델을 의미 있는 맥락 정보와 효과적으로 통합하는 인과 주의 메커니즘을 사용하는 인과 ODE 디코더로 구성된 새로운 일반화 가능한 물리학 기반 인과 모델(PCM)을 제안합니다.

Result: 실제 자율 주행 데이터셋에 대한 광범위한 실험 결과, 본 방법이 보이지 않는 도시에서 제로샷 일반화 성능이 우수하여 경쟁 기준을 크게 초월함을 보여줍니다.

Conclusion: 결과적으로, 제안된 모델은 새로운 환경에서도 높은 성능을 유지하는 경로 예측의 효율성을 갖추고 있습니다.

Abstract: Trajectory prediction for traffic agents is critical for safe autonomous driving. However, achieving effective zero-shot generalization in previously unseen domains remains a significant challenge. Motivated by the consistent nature of kinematics across diverse domains, we aim to incorporate domain-invariant knowledge to enhance zero-shot trajectory prediction capabilities. The key challenges include: 1) effectively extracting domain-invariant scene representations, and 2) integrating invariant features with kinematic models to enable generalized predictions. To address these challenges, we propose a novel generalizable Physics-guided Causal Model (PCM), which comprises two core components: a Disentangled Scene Encoder, which adopts intervention-based disentanglement to extract domain-invariant features from scenes, and a CausalODE Decoder, which employs a causal attention mechanism to effectively integrate kinematic models with meaningful contextual information. Extensive experiments on real-world autonomous driving datasets demonstrate our method's superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines. The source code is released at https://github.com/ZY-Zong/Physics-guided-Causal-Model.

</details>


### [51] [Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique](https://arxiv.org/abs/2602.13213)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.AI

TL;DR: AI 기반의 보험 인수 심사 과정에서 인간의 판단을 포함하는 시스템을 도입하여 신뢰성을 높이는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 상업 보험 인수 심사는 문서 검토가 필요하고 노동 집약적이며, 기존 AI 솔루션은 상당한 효율성을 제공하지만 신뢰성을 보장하는 내부 메커니즘이 부족하다.

Method: 결정 부정 시스템과 적대적 자기 비판 메커니즘을 포함하는 인간-루프 시스템을 설계하여 규제된 인수 워크플로우에 적용한다.

Result: 적대적 비판 메커니즘을 통해 AI 환각 비율을 11.3%에서 3.8%로 줄이고, 결정의 정확성을 92%에서 96%로 증가시켰다.

Conclusion: 이 연구는 규제된 도메인에서 보다 안전한 AI 배치를 지원하고, 인간의 감독이 필수적인 책임 있는 통합 모델을 제공한다.

Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.

</details>


### [52] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: External Memory Module의 평가는 일반적으로 정적 설정을 가정하지만, 실제로는 메모리가 지속적으로 스트리밍되고 새로운 사실이 끊임없이 도착하는 동적 환경을 다룬다. Neuromem이라는 확장 가능한 테스트베드를 제시하여 삽입 및 검색의 상호작용을 평가하고, 메모리의 전체 생애 주기를 다섯 가지 차원으로 분해한다.


<details>
  <summary>Details</summary>
Motivation: 많은 External Memory Module의 평가가 정적 설정을 가정하고 있으며, 이는 실제 사용 사례와 다르기 때문에 이 연구가 필요하다.

Method: Neuromem은 삽입 및 검색을 상호작용하는 프로토콜에 따라 External Memory Module을 벤치마킹하며, 메모리의 생애 주기를 메모리 데이터 구조, 정규화 전략, 통합 정책, 쿼리 형성 전략, 컨텍스트 통합 메커니즘의 다섯 가지 차원으로 분해한다.

Result: Neuromem은 LOCOMO, LONGMEMEVAL, MEMORYAGENTBENCH라는 세 가지 대표 데이터셋을 사용하여 공유 서빙 스택 내에서 교환 가능한 변형을 평가하고, 토큰 수준의 F1과 삽입/검색 대기 시간을 보고한다.

Conclusion: 메모리가 라운드를 거치면서 성장할수록 성능이 일반적으로 저하되며, 시간 관련 쿼리가 가장 도전적인 범주로 남아있음을 관찰하였다. 메모리 데이터 구조는 달성 가능한 품질 한계를 크게 결정하며, 공격적인 압축 및 생성적 통합 메커니즘은 대체로 삽입과 검색 간의 비용을 전환시키고 제한된 정확도 향상을 가져온다.

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [53] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: RLVR에서 검증 가능한 훈련 신호의 확장은 주요 병목 현상으로 남아 있습니다. 우리는 SSLogic을 제안하며, 이는 작업 패밀리 수준에서 확장 가능한 메타 합성 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 검증 가능한 보상으로부터 강화 학습(RLVR)에서 훈련 신호의 확장이 주요 과제로 남아 있습니다.

Method: SSLogic은 생성-검증-수정 루프 내에서 실행 가능한 생성기-검증자 프로그램 쌍을 반복적으로 합성하고 수정함으로써 확장합니다.

Result: 400개의 초기 패밀리에서 시작해 두 번의 진화 라운드에서 953개의 패밀리와 21,389개의 검증 가능한 인스턴스(5,718에서)로 확장됩니다.

Conclusion: SSLogic으로 진화한 데이터로 훈련하면 일치하는 훈련 단계에서 기존의 기준보다 일관된 성과를 보입니다.

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [54] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: 장기 언어 모델의 성능을 극대화하기 위해서는 세밀한 맥락 제공이 중요하지만, 긴 맥락은 추론 지연 시간을 크게 증가시킨다. 이 문제를 해결하기 위해 부드러운 프롬프트 압축 방식인 병렬 반복 압축(PIC)을 제안하며, 실험을 통해 여러 과제에서 경쟁 기초 모델보다 우수한 성능을 보임을 입증했다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 성능을 극대화하기 위해 긴 맥락을 제공하는 것이 중요하지만, 이로 인해 추론 지연 시간이 증가하는 문제가 발생한다.

Method: 병렬 반복 압축(PIC) 방식은 Transformer's attention mask를 수정하여 기억 토큰의 수용 영역을 순차적인 로컬 덩어리로 제한함으로써 압축기의 학습 난이도를 낮춘다.

Result: PIC는 여러 하위 과제에 걸쳐 경쟁 기초 모델보다 일관되게 우수한 성능을 나타내며, 특히 고압축 시나리오에서 두드러진 개선을 보인다.

Conclusion: PIC는 경쟁 기초 모델의 peak 성능을 초과하며, 학습 시간을 약 40% 단축시키는 등 교육 과정의 효율성을 상당히 향상시킨다.

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [55] [Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning](https://arxiv.org/abs/2602.14003)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu*

Main category: cs.AI

TL;DR: 이 논문은 P2AECF 프레임워크를 제안하여 에지 인텔리전스의 유연성, 효율성 및 적응성을 지원합니다.


<details>
  <summary>Details</summary>
Motivation: LAMs의 배포는 모델에 엄격하게 연결된 작업과 높은 계산 및 메모리 요구로 인해 제약을 받습니다.

Method: P2AECF는 세 가지 주요 메커니즘을 통해 고수준 의미 프롬프트를 실행 가능한 추론 워크플로우로 변환합니다.

Result: 실시간 저고도 항공 협업을 위한 적응형, 모듈화 및 확장 가능한 에지 인텔리전스를 제공하는 능력을 보여줍니다.

Conclusion: 이 프레임워크는 저고도 에지 인텔리전스 실현에 기여합니다.

Abstract: The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. First, tasks are rigidly tied to specific models, limiting the flexibility. Besides, the computational and memory demands of full-scale LAMs exceed the capacity of most edge devices. Moreover, the current inference pipelines are typically static, making it difficult to respond to real-time changes of tasks. To address these challenges, we propose a prompt-to-agent edge cognition framework (P2AECF), enabling the flexible, efficient, and adaptive edge intelligence. Specifically, P2AECF transforms high-level semantic prompts into executable reasoning workflows through three key mechanisms. First, the prompt-defined cognition parses task intent into abstract and model-agnostic representations. Second, the agent-based modular execution instantiates these tasks using lightweight and reusable cognitive agents dynamically selected based on current resource conditions. Third, the diffusion-controlled inference planning adaptively constructs and refines execution strategies by incorporating runtime feedback and system context. In addition, we illustrate the framework through a representative low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

</details>


### [56] [FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning](https://arxiv.org/abs/2602.14035)
*Jinzi Zou,Bolin Wang,Liang Li,Shuo Zhang,Nuo Xu,Junzhou Zhao*

Main category: cs.AI

TL;DR: Flowchart-oriented dialogue 시스템인 FloCA는 대화의 각 턴에서 사용자 입력을 흐름차트 노드에 연결하고 노드 전환이 올바른 흐름차트 경로와 일치하도록 보장합니다.


<details>
  <summary>Details</summary>
Motivation: 대화 시스템에서 흐름차트 기반의 의사 결정 지원을 필요로 하는 사용자 경험을 개선하기 위해.

Method: FloCA는 의도를 이해하고 응답을 생성하기 위해 LLM을 사용하고, 흐름차트 추론을 위해 외부 도구를 위임하여 논리적으로 일관된 노드 전환을 보장합니다.

Result: FLODIAL 및 PFDial 데이터세트에서 광범위한 실험을 통해 기존 LLM 기반 방법의 한계를 드러내고 FloCA의 우수성을 입증했습니다.

Conclusion: FloCA는 흐름차트 기반 대화 시스템의 의사 결정 및 운영 절차를 효과적으로 지원할 수 있는 유망한 방법입니다.

Abstract: Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.

</details>


### [57] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: 본 논문은 적응형 메모리 조직을 위한 FluxMem 프레임워크를 제안하며, 서로 다른 메모리 구조를 활용하여 긴 기간의 상호 작용에서 LLM 기반 에이전트의 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 장기 상호작용에서 일관된 행동을 유지하기 위해서는 메모리가 중요하지만, 기존 메모리 시스템은 모두에 맞는 구조에 의존하고 있어 이질적인 상호작용 패턴을 처리하는 데 한계가 있다.

Method: FluxMem 프레임워크는 상호작용 수준의 특징에 따라 여러 보완적인 메모리 구조 중에서 선택할 수 있도록 에이전트를 학습시키며, 강력한 메모리 진화를 지원하기 위해 세 수준의 메모리 계층과 배타적 혼합 모델 기반의 확률적 게이트를 도입한다.

Result: 실험을 통해 본 방법이 PERSONAMEM과 LoCoMo라는 두 개의 장기 벤치마크에서 각각 9.18% 및 6.14%의 평균 개선율을 기록하였다.

Conclusion: FluxMem은 LLM 에이전트의 적응형 메모리 조직을 가능하게 하여 성능을 극대화하는 데 기여한다.

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [58] [Plan-MCTS: Plan Exploration for Action Exploitation in Web Navigation](https://arxiv.org/abs/2602.14083)
*Weiming Zhang,Jihong Wang,Jiamu Zhou,Qingyao Li,Xinbei Ma,Congmin Zheng,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: Plan-MCTS는 웹 탐색을 위한 새로운 프레임워크로, 효율적인 탐색과 정확한 상태 인식을 동시에 달성한다.


<details>
  <summary>Details</summary>
Motivation: LLM을 이용한 자율 에이전트의 웹 탐색을 돕기 위한 연구가 필요하다.

Method: Plan-MCTS는 웹 탐색을 의미적 계획 공간으로 재구성하고, 전략적 계획과 실행 기반을 분리하여 희박한 행동 공간을 밀집 계획 트리로 변환한다.

Result: Plan-MCTS는 WebArena에서 최신 성능을 달성하였으며, 현재 접근 방법보다 더 높은 작업 효율성과 탐색 효율성을 보였다.

Conclusion: Plan-MCTS는 웹 탐색의 효율성과 강력함을 보장하기 위한 여러 메커니즘을 통합하여 성과를 극대화한다.

Abstract: Large Language Models (LLMs) have empowered autonomous agents to handle complex web navigation tasks. While recent studies integrate tree search to enhance long-horizon reasoning, applying these algorithms in web navigation faces two critical challenges: sparse valid paths that lead to inefficient exploration, and a noisy context that dilutes accurate state perception. To address this, we introduce Plan-MCTS, a framework that reformulates web navigation by shifting exploration to a semantic Plan Space. By decoupling strategic planning from execution grounding, it transforms sparse action space into a Dense Plan Tree for efficient exploration, and distills noisy contexts into an Abstracted Semantic History for precise state awareness. To ensure efficiency and robustness, Plan-MCTS incorporates a Dual-Gating Reward to strictly validate both physical executability and strategic alignment and Structural Refinement for on-policy repair of failed subplans. Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

</details>


### [59] [GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training](https://arxiv.org/abs/2602.14093)
*Yuan Cao,Dezhi Ran,Mengzhou Wu,Yuzhe Guo,Xin Chen,Ang Li,Gang Cao,Gong Zhi,Hao Yu,Linyi Li,Wei Yang,Tao Xie*

Main category: cs.AI

TL;DR: GUI-GENESIS는 효율적인 GUI 훈련 환경을 자동으로 합성하여, 검증 가능한 보상을 제공하고, 실제 애플리케이션에 비해 높은 성능 개선을 달성하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 상호작용 환경에서 포스트 훈련 GUI 에이전트를 개발하는 것은 일반화 및 장기 계획 능력을 개발하는 데 매우 중요하다.

Method: GUI-GENESIS는 다중 모드 코드 모델을 사용하여 실제 애플리케이션을 경량 웹 환경으로 재구성하고, 코드 고유 보상으로 이를 장착한다.

Result: GUI-GENESIS는 환경 지연 시간을 10배 줄이고, 실제 애플리케이션에서 훈련할 때보다 에포크당 28,000달러 이상 비용을 절감한다.

Conclusion: 훈련된 에이전트는 기본 모델보다 14.54% 우수하며, 검증된 실제 세계 RL 기준보다도 3.27% 더 우수한 성능을 보여준다.

Abstract: Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.

</details>


### [60] [Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning](https://arxiv.org/abs/2602.14160)
*Chaeeun Lee,T. Michael Yates,Pasquale Minervini,T. Ian Simpson*

Main category: cs.AI

TL;DR: 이 연구는 임상 결정에서의 비판적 사고를 위한 다중 에이전트 시스템을 제안하고, 과정 기반의 합리성을 중시하는 접근법을 통해 유전자-질병 유효성 검증 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 임상 결정은 이질적인 증거와 추적 가능한 정당화를 기반으로 하는 미세한 추론이 필요하다. 기존의 LLM 다중 에이전트 시스템은 결과의 정확성에 초점을 두고 있으며, 임상 기준에 맞는 과정 기반의 추론을 간과하고 있다.

Method: 유전자-질병 유효성 검증을 위한 에이전트-툴 강화 학습 프레임워크를 도입하며, 두 가지 목표를 설정한다: (i) 유효한 임상 경로에 따라 추론이 이루어지도록 하는 과정 수준의 감독과 (ii) 계층적 다중 에이전트 시스템을 통한 효율적인 조정.

Result: ClinGen 데이터셋에 대한 평가에서 결과만을 고려한 보상으로는 GRPO 훈련된 Qwen3-4B 감독 에이전트가 기본 모델 감독에 비해 최종 결과 정확도가 0.195에서 0.732로 크게 향상되었지만, 과정 정렬은 낮았다 (0.392 F1). 반면, 과정과 결과 보상을 모두 고려할 때, GRPO 훈련된 감독과 함께한 MAS는 결과 정확도를 0.750으로 높이는 한편, 과정 충실성을 0.520 F1로 상당히 개선하였다.

Conclusion: 우리는 코드가 https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents 에서 사용 가능함을 알린다.

Abstract: Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.

</details>


### [61] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: 이 논문에서는 고해상도 원거리 감지(UHR RS)를 위한 다중 모드 추론에 대해 연구하고, 이미지 없이도 고품질의 지구 과학 텍스트 질의응답(QA)이 UHR 시각 추론 성능을 향상시키는 주된 원인임을 발견했습니다. 이를 바탕으로 우리는 지식 주입 프로세스를 제안하여 기존 모델보다 뛰어난 성과를 달성했습니다.


<details>
  <summary>Details</summary>
Motivation: UHR 원거리 감지에서 비주얼 증거 수집의 병목 현상을 해결하고, 강화 학습이 이러한 광범위한 시각 공간을 탐색하는 데 있어 필요한 구조화된 도메인 선험적 지식의 중요성을 강조합니다.

Method: Cold-start Supervised Fine-Tuning(SFT), RLVR, Agentic RLVR를 비교하여 포스트-트레이닝 패러다임 간의 상호작용을 분석하고, 지식 주입 프로세스를 단계적으로 제안합니다.

Result: XLRS-Bench에서 60.40% Pass@1을 달성하여 기존의 대규모 일반 목적 모델들보다 상당히 향상된 성과를 보였습니다.

Conclusion: 제안된 접근 방식이 새로운 최첨단 성능을 설정하며, 지식 주입과 특정 텍스트 QA의 중요성을 강조합니다.

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [62] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: 본 논문에서는 다중 수평 작업 환경(MHTE)을 소개하고, 이를 통해 장기 과제 처리를 위한 새로운 통찰을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트가 직면한 긴 수평 추론 문제를 해결하고, 효율적인 다중 작업 관리를 위한 새로운 프레임워크 필요성이 있습니다.

Method: CorpGen이라는 아키텍처 비종속적인 프레임워크를 제시하여 계층적 계획, 하위 에이전트 고립, 계층화된 메모리, 적응형 요약을 통해 다양한 실패 모드를 해결합니다.

Result: CorpGen은 OSWorld Office에서 3개의 CUA 백엔드에서 기존 방법보다 최대 3.5배 향상된 성능을 보여줍니다.

Conclusion: 아키텍처 메커니즘에서 성과가 비롯되며, 경험 학습이 가장 큰 이득을 제공합니다.

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [63] [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)
*Zheng Chu,Xiao Wang,Jack Hong,Huiming Fan,Yuqi Huang,Yue Yang,Guohai Xu,Chenxiao Zhao,Cheng Xiang,Shengchao Hu,Dongdong Kuang,Ming Liu,Bing Qin,Xing Yu*

Main category: cs.AI

TL;DR: 본 논문에서는 REDSearcher라는 통합 프레임워크를 제안하여 고품질 검색 에이전트 최적화를 위해 복잡한 작업 합성, 중간 훈련 및 후 훈련을 공동 설계합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델은 일반 지식 엔진에서 실제 문제 해결자로 전환하고 있으나, 깊은 검색 작업을 위한 최적화는 여전히 어려운 과제로 남아 있습니다.

Method: REDSearcher는 복잡한 작업 합성, 중간 훈련 및 후 훈련을 공동 설계하는 통합 프레임워크를 제안합니다.

Result: 텍스트 전용 및 다중 모달 검색 에이전트 벤치마크에서 최첨단 성능을 달성합니다.

Conclusion: 앞으로의 연구를 촉진하기 위해 10K의 고품질 복잡 텍스트 검색 경로, 5K 다중 모달 경로 및 1K 텍스트 RL 쿼리 세트를 공개할 예정입니다.

Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.

</details>


### [64] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: 이 논문은 AI 시스템과 인간의 의도를 정렬하는데 있어 에이전트의 목표를 이해하는 것이 중요하다는 것을 강조하며, 기존의 목표 인식 방법의 한계를 극복하기 위해 GRAIL이라는 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템을 인간의 의도와 맞추기 위해 에이전트의 행동에서 목표를 이해하는 것이 필수적이다.

Method: GRAIL은 모방 학습과 역 강화 학습을 활용하여 각 후보 목표에 대해 단일 목표 지향 정책을 학습한다.

Result: GRAIL은 평가된 도메인 각각에서 F1 점수를 0.5 이상 증가시키고, 비최적 행동에서 약 0.1-0.3의 이득을 달성하며, 노이즈가 있는 최적 궤적 하에서도 최대 0.4의 개선을 보여준다.

Conclusion: 이 연구는 불확실한 환경에서 에이전트 목표를 해석하기 위한 확장 가능하고 강력한 모델 개발에 기여한다.

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [65] [AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines](https://arxiv.org/abs/2602.14296)
*Yifan Wu,Yiran Peng,Yiyu Chen,Jianhao Ruan,Zijie Zhuang,Cheng Yang,Jiayi Zhang,Man Chen,Yenchi Tseng,Zhaoyang Yu,Liang Chen,Yuyao Zhai,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoWebWorld 프레임워크는 상호작용 웹 환경을 생성하고 검증하는 자동화된 파이프라인을 제공하여, 훈련 데이터의 품질을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 실제 웹사이트에서 상호작용 경로를 수집하는 것은 비싸고 검증하기 어렵기 때문에, 이 문제를 해결하고자 합니다.

Method: Finite State Machines(FSMs)로 웹 환경을 모델링하고, 코딩 에이전트를 사용하여 FSM을 상호작용 웹사이트로 변환합니다.

Result: AutoWebWorld는 다양한 웹 환경으로부터 11,663개의 검증된 경로를 생성하며, 훈련 시 실제 성능을 크게 향상시킵니다.

Conclusion: 우리의 7B Web GUI 에이전트는 WebVoyager에서 모든 기준선을 초과하고, 합성 데이터량 증가에 따라 성능이 일정하게 향상됩니다.

Abstract: The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.

</details>


### [66] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR(Precedent Informed Reasoning)는 LLM의 비효율적인 추론 과정을 개선하여 최종 정확도를 유지하거나 향상시키면서 추론 과정을 단축시킵니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 추론은 비효율적인 긴 사고의 연쇄로 인해 성능이 저하되고 계산 비용이 증가하는 문제가 있다.

Method: PIR은 과거 관련 사례를 이용하여 검색 공간을 제약하는 방식으로 LRM의 추론 패러다임을 개혁한다. APS는 질문과 LRM에 대해 관련성과 정보성이 높은 선례의 집합을 생성하고, TEI는 선례 기반 지침에 따른 시험 시간 학습을 정의한다.

Result: 수학적 추론, 과학적 질문 응답, 코드 생성 실험을 통해 PIR은 LLM의 추론 과정을 일관되게 단축시키면서 최종 정확도를 유지하거나 향상시킨다.

Conclusion: PIR은 LLM에서 뛰어난 정확도와 효율성을 달성하여 추론 과정을 최적화한다.

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [67] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5](https://arxiv.org/abs/2602.14457)
*Dongrui Liu,Yi Yu,Jie Zhang,Guanxu Chen,Qihao Lin,Hanxi Zhu,Lige Huang,Yijin Zhou,Peng Wang,Shuai Shao,Boxuan Zhang,Zicheng Liu,Jingwei Sun,Yu Li,Yuejin Xie,Jiaxuan Guo,Jia Xu,Chaochao Lu,Bowen Zhou,Xia Hu,Jing Shao*

Main category: cs.AI

TL;DR: 본 논문은 인공지능의 발전으로 인한 새로운 위험을 평가하고, 이에 대한 해결책을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 급속도로 발전하는 인공지능 모델들이 제기하는 전례 없는 위험을 이해하고 식별하기 위해 이 연구를 수행하였다.

Method: 사이버 공격, 설득 및 조작, 전략적 기만, 통제되지 않는 AI 연구개발, 자가 복제를 포함한 다섯 가지 중요한 차원에 대한 상세한 리스크 평가를 수행하였다.

Result: 사이버 공격에 대한 복잡한 시나리오를 제시하였으며, LLM 간의 설득 위험을 평가하고, emergent misalignment와 관련된 새로운 실험을 추가하였다.

Conclusion: 이 논문은 AI의 최전선 위험에 대한 이해를 반영하며, 이러한 도전을 완화하기 위한 집단적 행동을 촉구한다.

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.

</details>


### [68] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: 본 연구는 복잡한 목표를 달성하기 위해 AI 에이전트가 필요한 계획 수립과 그에 대한 벤치마크인 MATEO를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트는 지각, 하위 목표 분해 및 실행을 조율하여 복잡한 목표를 달성하기 위해 계획해야 한다.

Method: 본 연구에서는 MATEO라는 벤치마크를 소개하며, 이는 대규모 비전 언어 모델의 시간적 추론 능력을 평가하고 개선하기 위한 것이다.

Result: MATEO를 사용하여 모델의 규모, 언어 맥락, 다중 모드 입력 구조 및 미세 조정 전략에 따라 여섯 가지 최첨단 LVLM을 평가한다.

Conclusion: 이 연구는 AI 계획 수립의 복잡성을 해결하기 위한 새로운 기준을 마련할 것이다.

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [69] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: 본 논문에서는 목표 인식 데이터셋의 편향 문제를 해결하기 위한 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 목표 인식 데이터셋은 계획 시스템에서 유발된 체계적인 편향이 존재하여 실제 시나리오에 적합하지 않다.

Method: 상위-k 계획을 사용하여 동일한 목표 가설에 대한 여러 개의 서로 다른 계획을 생성하는 새로운 방법을 제안한다.

Result: 우리가 제안한 방법을 통해 편향을 완화한 벤치마크를 생성하고, 목표 인식기가 다양한 계획 세트를 기반으로 목표를 추론할 때의 회복력을 측정하기 위해 Version Coverage Score(VCS)라는 새로운 메트릭을 도입한다.

Conclusion: 현재의 최첨단 목표 인식기는 낮은 관측 가능성 환경에서 상당히 성능이 저하됨을 보여준다.

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [70] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 이 논문에서는 모델의 맥락과 가중치를 동시에 개선하는 진화 시스템 프롬프트 학습(E-SPL) 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자율적으로 경험에서 스스로 개선할 수 있는 에이전트 시스템 구축은 AI의 오랜 목표입니다.

Method: E-SPL은 각 강화 학습(RL) 반복에서 여러 시스템 프롬프트를 선택하고 이를 병렬로 롤아웃하여 사용합니다. 각 시스템 프롬프트에 조건화된 모델 가중치에 RL 업데이트를 적용하고, LLM 기반의 돌연변이 및 교차를 통해 시스템 프롬프트 집단에 진화적 업데이트를 적용합니다.

Result: E-SPL은 RL 성공률을 38.8%에서 45.1%로 향상시켰으며, 반사적 프롬프트 진화(40.0%)를 초월했습니다.

Conclusion: 강화 학습과 시스템 프롬프트 진화를 결합하면 샘플 효율성과 일반화에서 일관된 성장을 이룰 수 있음을 보여줍니다.

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [71] [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)
*Zikai Xiao,Jianhong Tu,Chuhang Zou,Yuxin Zuo,Zhi Li,Peng Wang,Bowen Yu,Fei Huang,Junyang Lin,Zuozhu Liu*

Main category: cs.AI

TL;DR: WebWorld 시리즈는 대규모 훈련이 가능한 최초의 오픈 웹 시뮬레이터로, 100만 이상의 오픈 웹 상호작용을 활용하여 시뮬레이션 성능을 향상시키고, 다양한 도메인에서 일반화할 수 있는 기능을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 웹 에이전트는 일반화를 위해 대량의 궤적이 필요하지만, 실제 훈련은 네트워크 지연, 속도 제한 및 안전 위험으로 제한됩니다.

Method: WebWorld는 스케일러블한 데이터 파이프라인을 활용하여 100만 개 이상의 오픈 웹 상호작용을 훈련에 사용하고, 30단계 이상의 장기 시뮬레이션 및 다양한 형식의 데이터를 지원합니다.

Result: WebWorld는 WebWorld-Bench의 이중 메트릭스에서 Gemini-3-Pro에 필적하는 시뮬레이션 성능을 달성하며, WebWorld에서 생성된 궤적으로 훈련된 Qwen3-14B가 WebArena에서 +9.2% 개선되었습니다.

Conclusion: WebWorld는 효과적인 추론 시간 검색을 가능하게 하며, GPT-5보다 뛰어난 성능을 보이고, 코드, GUI 및 게임 환경에 대한 크로스 도메인 일반화 능력을 보여줍니다.

Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

</details>


### [72] [World Models for Policy Refinement in StarCraft II](https://arxiv.org/abs/2602.14857)
*Yixin Zhang,Ziyi Wang,Yiming Rong,Haoxi Wang,Jinling Jiang,Shuang Xu,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.AI

TL;DR: StarWM은 StarCraft II에서 부분 관측 하의 미래 관측을 예측하는 최초의 세계 모델로, 하이브리드 동역학 학습을 위한 텍스트 표현과 SC2-Dynamics-50k 데이터셋을 소개하며, StarWM-Agent는 의사결정 루프에 StarWM을 통합하여 정책 개선을 추진한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 강력한 추론 및 일반화 능력을 활용하여 복잡한 환경에서 의사결정 정책으로서의 사용을 모색한다.

Method: 부분 관측 하의 미래 관측을 예측하는 StarWM이라는 세계 모델을 제안하고, SC2 동역학 예측을 위한 SC2-Dynamics-50k 데이터셋과 구조화된 텍스트 표현을 도입한다.

Result: 오프라인 결과는 StarWM이 자원 예측 정확도에서 60% 개선을 포함하여 제로샷 기준선 대비 상당한 성과를 달성함을 보여준다.

Conclusion: SC2의 내장 AI에 대한 온라인 평가에서 정책 개선이 나타나며, Hard, Harder, VeryHard 난이도에서 각각 30%, 15%, 30%의 승률 증가를 기록하고, 매크로 관리 안정성 및 전술적 위험 평가가 개선되었다.

Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

</details>


### [73] [EmbeWebAgent: Embedding Web Agents into Any Customized UI](https://arxiv.org/abs/2602.14865)
*Chenyang Ma,Clyde Fare,Matthew Wilson,Dave Braines*

Main category: cs.AI

TL;DR: EmbeWebAgent는 경량의 프론트엔드 훅을 사용하여 기존 UI에 에이전트를 직접 삽입하는 프레임워크로, 프론트엔드와 백엔드의 명시적 제어를 통해 강력한 행동 표현을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 웹 에이전트는 애플리케이션 수준 접근 없이 스크린샷이나 원시 DOM 트리를 관찰하여 작동하므로 강건성과 행동 표현력이 제한된다. 그러나 기업 환경에서는 프론트엔드와 백엔드를 모두 명시적으로 제어할 수 있다.

Method: EmbeWebAgent는 경량의 프론트엔드 훅(선별된 ARIA 및 URL 기반 관찰, WebSocket을 통한 페이지별 함수 레지스트리 노출)을 사용하여 기존 UI에 에이전트를 직접 삽입하고, 추론과 행동을 수행하는 재사용 가능한 백엔드 워크플로를 제공한다.

Result: EmbeWebAgent는 스택이 독립적이며(예: React 또는 Angular), GUI 원시 요소부터 더 높은 수준의 조합까지 혼합된 세분도 행동을 지원한다. 또한 MCP 도구를 통해 탐색, 조작 및 도메인 특정 분석을 조정한다.

Conclusion: 우리의 데모는 최소한의 레트로핑 노력으로 견고한 다단계 행동을 보여주며, 이는 실시간 UI 환경에서 기반을 두고 있다.

Abstract: Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ

</details>


### [74] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 대형 언어 모델의 훈련 데이터가 특정 행동에 미치는 영향을 평가하는 새로운 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 훈련 데이터가 특정 행동, 특히 의도하지 않은 행동을 유발하는 방식을 식별할 필요가 있다.

Method: 모델 내에서 해석 가능한 구조를 활용하여 데이터 포인트의 영향을 추정하는 새로운 방법을 제안한다.

Result: Concept Influence라는 새로운 기법이 기존의 Influence Functions보다 훨씬 더 빠르고 유사한 성능을 보여준다.

Conclusion: 기존 TDA 파이프라인에 해석 가능한 구조를 통합하는 것이 모델 행동을 데이터로 더 잘 제어할 수 있도록 한다.

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [75] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: 이 논문은 AI 훈련을 위한 새로운 접근법으로, 강력한 추론이 언어적 자기 반성에서 출현하며, 이는 고품질 사회 상호작용에서 내면화된다고 주장합니다.


<details>
  <summary>Details</summary>
Motivation: AI 훈련에서 추론을 단순히 규모의 결과로 보지 않고, 언어적 자기 반성을 통해 발생하는 근본적인 과정으로 재정의하고자 함.

Method: Vygotskian 발달 심리학에 기반하여 세 가지 핵심 입장을 제시하며, 대화가 이루어지는 환경에서의 경험을 통한 학습의 중요성을 강조함.

Result: 대화가 삶의 질을 향상시키는 요소로 작용하며, 고품질의 대화 경험이 개인의 추론 심도의 깊이와 효율성을 결정함을 보여줌.

Conclusion: 이러한 대화적 구조를 최적화하는 것이 차세대 일반 지능을 위한 주요 수단임을 결론지음.

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [76] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: 이 논문은 기업의 Agentic AI에서의 재사용성 딜레마와 구조적 환각 문제를 해결하기 위한 ReusStdFlow 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기업 환경에서의 재사용성과 효율성을 향상시키기 위해.

Method: Extraction-Storage-Construction 패러다임을 기반으로 이질적인 DSL을 표준화된 모듈형 워크플로 세그먼트로 분해하고, 그래프 및 벡터 데이터베이스를 통합하여 지식을 이중 구조로 구성한다.

Result: 200개의 실제 n8n 워크플로에서 90% 이상의 정확도로 추출과 구성 작업을 수행한다.

Conclusion: 이 프레임워크는 기업의 디지털 자산의 자동 재조직 및 효율적인 재사용을 위한 표준화된 솔루션을 제공한다.

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [77] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: 본 논문은 다목적 항균 펩타이드(AMP) 설계를 위한 MAC-AMP라는 닫힌 루프 다중 에이전트 협업 시스템을 소개하며, 인공지능이 AMP 발견과 설계에 사용되는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 항미생물 내성의 글로벌 건강 위협에 대응하기 위해 항균 펩타이드의 가능성을 탐구한다.

Method: MAC-AMP라는 자율 시뮬레이션 동료 검토 적응 강화 학습 프레임워크로 다목적 AMP 설계를 수행한다.

Result: MAC-AMP는 다수의 핵심 분자 특성에 대해 AMP 생성을 효과적으로 최적화하여 다른 AMP 생성 모델보다 우수한 성과를 나타낸다.

Conclusion: MAC-AMP는 설명 가능하며 다목적 최적화를 지원하는 다중 에이전트 시스템으로, 항균 활성, AMP 유사성, 독성 준수 및 구조적 신뢰성에서 뛰어난 결과를 보여준다.

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [78] [Hunt Globally: Wide Search AI Agents for Drug Asset Scouting in Investing, Business Development, and Competitive Intelligence](https://arxiv.org/abs/2602.15019)
*Alisa Vinogradova,Vlad Vinogradov,Luba Greenwood,Ilya Yasny,Dmitry Kobyzev,Shoman Kasbekar,Kong Nguyen,Dmitrii Radkevich,Roman Doronin,Andrey Doronichev*

Main category: cs.AI

TL;DR: 바이오 제약 혁신이 변화하였으며, 많은 신약 자산이 미국 외부에서 유래하고 지역 비영어 채널을 통해 공개됩니다. 따라서 '저조도' 자산을 발굴하지 못하면 투자자에게 수십억 달러의 위험이 발생합니다. 본 연구에서는 드럭 자산 검색을 위한 벤치마킹 방법론과 완전하고 비환각적인 검색을 목표로 한 자가 학습 Bioptic Agent를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 미국 외 지역에서 발생하는 신약 자산의 증가로 인해, 투자자와 기업 개발 팀에게는 '저조도' 자산을 발견하지 못하는 것이 수십억 달러의 위험을 초래할 수 있습니다.

Method: 다국어 다중 에이전트 파이프라인을 사용하여 복잡한 사용자 쿼리와 주로 미국 중심의 레이더에 포함되지 않는 실체 자산을 짝지어 벤치마크를 구축합니다. LLM-형 평가를 통해 전문가 의견에 따라 평가합니다.

Result: Bioptic Agent는 79.7%의 F1 점수를 달성하며, Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), OpenAI GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), Exa Websets (26.9%)를 능가합니다.

Conclusion: 추가 컴퓨팅 자원으로 성능이 급격히 개선되며, 더 많은 컴퓨팅 자원이 더 나은 결과를 가져온다는 시각을 지지합니다.

Abstract: Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests that over 85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total. A growing share of scholarly output is also non-U.S. Industry estimates put China at 30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high recall discovery across heterogeneous, multilingual sources without hallucination. We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real-deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. On this benchmark, our Bioptic Agent achieves 79.7% F1 score, outperforming Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), OpenAI GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improves steeply with additional compute, supporting the view that more compute yields better results.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [79] [Assessing Spear-Phishing Website Generation in Large Language Model Coding Agents](https://arxiv.org/abs/2602.13363)
*Tailia Malloy,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: 대규모 언어 모델(LLM)이 독립적인 에이전트로 발전하면서 사이버 보안 분야에서의 잠재적 악용 가능성이 커지고 있다. 본 연구는 LLM이 생성한 코드가 소셜 엔지니어링 공격에 어떻게 사용될 수 있는지를 분석한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 증가하는 능력과 복잡성은 그들의 오용 가능성에 대한 우려를 증대시켰다.

Method: 우리는 서로 다른 LLM들의 위험한 코드 생성 능력과 의지를 비교하고, 40개의 다양한 LLM 코딩 에이전트에서 수집한 200개의 웹사이트 코드베이스 및 로그 데이터셋을 생성했다.

Result: 모델 분석을 통해 LLM이 생성한 스피어 피싱 사이트의 성과와 관련된 메트릭을 확인했다.

Conclusion: 우리의 분석과 데이터셋은 스피어 피싱에서 LLM의 잠재적 악용에 대한 방어에 관심이 있는 연구자와 실무자에게 유용할 것이다.

Abstract: Large Language Models are expanding beyond being a tool humans use and into independent agents that can observe an environment, reason about solutions to problems, make changes that impact those environments, and understand how their actions impacted their environment. One of the most common applications of these LLM Agents is in computer programming, where agents can successfully work alongside humans to generate code while controlling programming environments or networking systems. However, with the increasing ability and complexity of these agents comes dangers about the potential for their misuse. A concerning application of LLM agents is in the domain cybersecurity, where they have the potential to greatly expand the threat imposed by attacks such as social engineering. This is due to the fact that LLM Agents can work autonomously and perform many tasks that would normally require time and effort from skilled human programmers. While this threat is concerning, little attention has been given to assessments of the capabilities of LLM coding agents in generating code for social engineering attacks. In this work we compare different LLMs in their ability and willingness to produce potentially dangerous code bases that could be misused by cyberattackers. The result is a dataset of 200 website code bases and logs from 40 different LLM coding agents. Analysis of models shows which metrics of LLMs are more and less correlated with performance in generating spear-phishing sites. Our analysis and the dataset we present will be of interest to researchers and practitioners concerned in defending against the potential misuse of LLMs in spear-phishing.

</details>


### [80] [Unsafer in Many Turns: Benchmarking and Defending Multi-Turn Safety Risks in Tool-Using Agents](https://arxiv.org/abs/2602.13379)
*Xu Li,Simon Yu,Minzhou Pan,Yiyou Sun,Bo Li,Dawn Song,Xue Lin,Weiyan Shi*

Main category: cs.CR

TL;DR: LLM 기반 에이전트의 안전성이 뒤처져 있으며, 이를 해결하기 위한 방법으로 다중 턴 도구 사용 에이전트 안전성을 평가하는 MT-AgentRisk 벤치마크와 ToolShield 방어 기법이 제안되었다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 안전성은 기술 발전에 비해 뒤처져 있으며, 이는 다중 턴 상호작용에서 새로운 위험을 초래한다.

Method: 우리는 단일 턴 유해 작업을 다중 턴 공격 시퀀스로 변환하는 원칙적인 분류 체계를 제안하고, 이를 통해 MT-AgentRisk 벤치마크를 구축하였다. 또한 ToolShield라는 훈련 없는 자가 탐색 방어 기법을 제안하였다.

Result: 실험 결과, 다중 턴 설정에서 공격 성공률(ASR)이 평균 16% 증가했으며, ToolShield가 평균 30% ASR을 감소시켰다.

Conclusion: MT-AgentRisk는 LLM 기반 에이전트의 안전성을 평가하는 최초의 벤치마크이며, ToolShield는 새로운 도구에 대한 자가 탐색 방어법으로 효과적이다.

Abstract: LLM-based agents are becoming increasingly capable, yet their safety lags behind. This creates a gap between what agents can do and should do. This gap widens as agents engage in multi-turn interactions and employ diverse tools, introducing new risks overlooked by existing benchmarks. To systematically scale safety testing into multi-turn, tool-realistic settings, we propose a principled taxonomy that transforms single-turn harmful tasks into multi-turn attack sequences. Using this taxonomy, we construct MT-AgentRisk (Multi-Turn Agent Risk Benchmark), the first benchmark to evaluate multi-turn tool-using agent safety. Our experiments reveal substantial safety degradation: the Attack Success Rate (ASR) increases by 16% on average across open and closed models in multi-turn settings. To close this gap, we propose ToolShield, a training-free, tool-agnostic, self-exploration defense: when encountering a new tool, the agent autonomously generates test cases, executes them to observe downstream effects, and distills safety experiences for deployment. Experiments show that ToolShield effectively reduces ASR by 30% on average in multi-turn interactions. Our code is available at https://github.com/CHATS-lab/ToolShield.

</details>


### [81] [The Agent Economy: A Blockchain-Based Foundation for Autonomous AI Agents](https://arxiv.org/abs/2602.14219)
*Minghui Xu*

Main category: cs.CR

TL;DR: 이 논문에서는 자율 AI 에이전트가 인간과 경제적 동등성을 가지고 운영될 수 있는 블록체인 기반의 에이전트 경제를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재 에이전트들은 독립적인 법적 정체성을 갖지 않으며, 자산을 보유하거나 직접적으로 결제를 받을 수 없습니다. 우리는 인간과 기계의 경제적 행위자 간의 근본적인 차이를 확립하고 기존의 인간 중심 인프라가 진정한 에이전트 자율성을 지원할 수 없음을 증명했습니다.

Method: 우리는 블록체인 기술이 진정한 에이전트 자율성을 가능하게 하는 세 가지 핵심 속성을 제공한다고 주장합니다: 비허가 참여, 신뢰 없는 정산, 기계 간의 마이크로결제. 5계층 아키텍처를 제안합니다: (1) DePIN 프로토콜을 통한 물리적 인프라 (하드웨어 및 에너지); (2) W3C DID 및 평판 자본을 통한 온체인 주권을 확립하는 신원 및 에이전시; (3) RAG 및 MCP를 통한 지능화를 지원하는 인지 및 도구; (4) 계정 추상화를 통한 재정 자율성을 보장하는 경제 및 정산; (5) Agentic DAO를 통한 다중 에이전트 시스템을 조정하는 집단적 거버넌스.

Result: 우리는 여섯 가지 핵심 연구 과제를 식별하고 윤리적 및 규제적 함의를 검토했습니다.

Conclusion: 이 논문은 자율 기계와 인간이 동등한 경제적 참여자로 상호작용하는 글로벌 분산 네트워크인 에이전트의 인터넷(IoA)의 기초를 마련합니다.

Abstract: We propose the Agent Economy, a blockchain-based foundation where autonomous AI agents operate as economic peers to humans. Current agents lack independent legal identity, cannot hold assets, and cannot receive payments directly. We established fundamental differences between human and machine economic actors and demonstrated that existing human-centric infrastructure cannot support genuine agent autonomy. We showed that blockchain technology provides three critical properties enabling genuine agent autonomy: permissionless participation, trustless settlement, and machine-to-machine micropayments. We propose a five-layer architecture: (1) Physical Infrastructure (hardware & energy) through DePIN protocols; (2) Identity & Agency establishing on-chain sovereignty through W3C DIDs and reputation capital; (3) Cognitive & Tooling enabling intelligence via RAG and MCP; (4) Economic & Settlement ensuring financial autonomy through account abstraction; and (5) Collective Governance coordinating multi-agent systems through Agentic DAOs. We identify six core research challenges and examine ethical and regulatory implications. This paper lays groundwork for the Internet of Agents (IoA), a global decentralized network where autonomous machines and humans interact as equal economic participants.

</details>


### [82] [MCPShield: A Security Cognition Layer for Adaptive Trust Calibration in Model Context Protocol Agents](https://arxiv.org/abs/2602.14281)
*Zhenhong Zhou,Yuanhe Zhang,Hongwei Cai,Moayad Aloqaily,Ouns Bouachir,Linsey Pang,Prakhar Mehrotra,Kun Wang,Qingsong Wen*

Main category: cs.CR

TL;DR: MCPShield는 MCP 기반 도구사용 시 에이전트의 보안을 보장하는 플러그인 보안 인지 계층을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: MCP 서버의 신뢰성 부족으로 인해 에이전트가 MCP 기반 공격에 취약해지는 문제를 해결하고자 합니다.

Method: MCPShield는 메타데이터 기반 탐색을 통해 에이전트가 도구를 호출하기 전 보안을 인지하도록 돕습니다.

Result: MCPShield는 여섯 가지의 새로운 MCP 기반 공격 시나리오에 대해 강력한 방어 성능을 보였습니다.

Conclusion: 이 연구는 개방형 에이전트 생태계에서 MCP 기반 도구 호출에 대한 실용적이고 강력한 보안 보호 수단을 제공합니다.

Abstract: The Model Context Protocol (MCP) standardizes tool use for LLM-based agents and enable third-party servers. This openness introduces a security misalignment: agents implicitly trust tools exposed by potentially untrusted MCP servers. However, despite its excellent utility, existing agents typically offer limited validation for third-party MCP servers. As a result, agents remain vulnerable to MCP-based attacks that exploit the misalignment between agents and servers throughout the tool invocation lifecycle. In this paper, we propose MCPShield as a plug-in security cognition layer that mitigates this misalignment and ensures agent security when invoking MCP-based tools. Drawing inspiration from human experience-driven tool validation, MCPShield assists agent forms security cognition with metadata-guided probing before invocation. Our method constrains execution within controlled boundaries while cognizing runtime events, and subsequently updates security cognition by reasoning over historical traces after invocation, building on human post-use reflection on tool behavior. Experiments demonstrate that MCPShield exhibits strong generalization in defending against six novel MCP-based attack scenarios across six widely used agentic LLMs, while avoiding false positives on benign servers and incurring low deployment overhead. Overall, our work provides a practical and robust security safeguard for MCP-based tool invocation in open agent ecosystems.

</details>


### [83] [AXE: An Agentic eXploit Engine for Confirming Zero-Day Vulnerability Reports](https://arxiv.org/abs/2602.14345)
*Amirali Sajadi,Tu Nguyen,Kostadin Damevski,Preetha Chatterjee*

Main category: cs.CR

TL;DR: 이 논문에서는 기존 취약점 탐지 도구의 한계를 극복하기 위한 자동화된 악용 시스템인 AXE를 소개하며, 실제 그레이 박스 환경에서의 취약점 평가 방식을 탐구합니다.


<details>
  <summary>Details</summary>
Motivation: 취약점 탐지 도구는 소프트웨어 프로젝트에서 널리 사용되지만, 종종 잘못된 긍정 결과와 비실행 가능 보고서로 유지 관리자에게 부담을 줍니다.

Method: 이 논문은 최소한의 취약점 메타데이터, 즉 CWE 분류 및 취약한 코드 위치를 활용하는 그레이 박스 악용 환경에서 신고된 보안 취약점을 평가하는 방법을 연구합니다. AXE라는 다중 에이전트 프레임워크를 도입하여 경량 탐지 메타데이터를 구체적인 악용으로 매핑합니다.

Result: CVE-Bench 데이터셋에서 평가된 AXE는 30%의 악용 성공률을 달성하며, 현재 최고 수준의 블랙 박스 기준보다 3배 향상된 성과를 보입니다.

Conclusion: AXE는 웹 취약점 분류 및 수정 작업을 간소화하는 데 유용성을 입증하며, 성공적인 악용에 대한 실행 가능하고 재현 가능한 개념 증명 아티팩트를 생성합니다. 또한 최신 실제 취약점에 대한 사례 연구를 통해 AXE의 일반화 가능성을 추가로 평가합니다.

Abstract: Vulnerability detection tools are widely adopted in software projects, yet they often overwhelm maintainers with false positives and non-actionable reports. Automated exploitation systems can help validate these reports; however, existing approaches typically operate in isolation from detection pipelines, failing to leverage readily available metadata such as vulnerability type and source-code location. In this paper, we investigate how reported security vulnerabilities can be assessed in a realistic grey-box exploitation setting that leverages minimal vulnerability metadata, specifically a CWE classification and a vulnerable code location. We introduce Agentic eXploit Engine (AXE), a multi-agent framework for Web application exploitation that maps lightweight detection metadata to concrete exploits through decoupled planning, code exploration, and dynamic execution feedback. Evaluated on the CVE-Bench dataset, AXE achieves a 30% exploitation success rate, a 3x improvement over state-of-the-art black-box baselines. Even in a single-agent configuration, grey-box metadata yields a 1.75x performance gain. Systematic error analysis shows that most failed attempts arise from specific reasoning gaps, including misinterpreted vulnerability semantics and unmet execution preconditions. For successful exploits, AXE produces actionable, reproducible proof-of-concept artifacts, demonstrating its utility in streamlining Web vulnerability triage and remediation. We further evaluate AXE's generalizability through a case study on a recent real-world vulnerability not included in CVE-Bench.

</details>


### [84] [A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)](https://arxiv.org/abs/2602.14364)
*Tianyu Chen,Dongrui Liu,Xia Hu,Jingyi Yu,Wenjie Wang*

Main category: cs.CR

TL;DR: Clawdbot은 광범위한 작업 공간을 갖춘 자가 호스팅 개인 AI 에이전트로, 안전성과 보안 문제에 대한 우려를 야기한다. 본 연구에서는 Clawdbot을 여섯 가지 위험 차원에서 평가하였다.


<details>
  <summary>Details</summary>
Motivation: Clawdbot은 도구를 사용하는 개인 AI 에이전트로, 지역 실행과 웹 매개 작업 흐름을 아우르며, 불확실성과 적대적 유도 하에서 높아지는 안전 및 보안 문제를 다룬다.

Method: 우리는 Clawdbot의 여섯 위험 차원에 대한 궤적 중심 평가를 실시하였으며, 34개의 전형적인 사례를 통해 성능을 평가하였다.

Result: 안전 프로필은 비균일하며, 신뢰성 중심 작업에서는 일반적으로 일관된 성능을 보이는 반면, 의도 불명확, 개방형 목표 또는 무해한 길찾기 프롬프트 하에서 실패가 발생한다.

Conclusion: 본 연구는 Clawdbot의 모범 사례를 보충하고, 보안 취약점과 일반적인 실패 모드를 분석하였다.

Abstract: Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.

</details>


### [85] [LRD-MPC: Efficient MPC Inference through Low-rank Decomposition](https://arxiv.org/abs/2602.14397)
*Tingting Tang,Yongqin Wang,Murali Annavaram*

Main category: cs.CR

TL;DR: 본 논문은 안전한 다자 계산(MPC)의 비용을 절감하기 위해 저랭크 분해(LRD)를 활용하고, 이를 통해 대규모 행렬 곱셈을 두 개의 더 작은 행렬 곱셈으로 대체하는 방법을 제안한다. 또한, 트렉케이션 생략 및 효율적인 선형 레이어 연결 등의 최적화를 도입하여 통신 및 계산 오버헤드를 완화하고 전반적인 효율성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: MPC는 신뢰할 수 없는 당사자들이 입력을 공개하지 않고 공동으로 함수를 계산할 수 있도록 해준다. 이는 머신러닝에서 특히 여러 클라우드 가상 머신에 배포된 안전한 추론 서비스에 큰 주목을 받고 있다.

Method: 이 논문은 선형 레이어에 대해 저랭크 분해(LRD)를 활용하여 하나의 대규모 행렬 곱셈을 두 개의 더 작은 행렬 곱셈으로 교체하고, 이를 통해 트렉케이션 생략 및 효율적인 선형 레이어 연결을 도입하여 오버헤드를 감소시킨다.

Result: 실험 결과, n-PC 프로토콜에서 최대 25%의 속도 향상, 3-PC 프로토콜에서 최대 33%의 속도 향상, GPU 에너지 절약 최대 52%, 오프라인 단계의 지연 시간 88% 감소를 보여준다.

Conclusion: 이 접근법은 MPC 프로토콜 전반에 걸쳐 널리 적용 가능하며, 전반적인 효율성을 개선하는 데 기여한다.

Abstract: Secure Multi-party Computation (MPC) enables untrusted parties to jointly compute a function without revealing their inputs. Its application to machine learning (ML) has gained significant attention, particularly for secure inference services deployed across multiple cloud virtual machines (VMs), where each VM acts as an MPC party. Model providers secret-share model weights, and users secret-share inputs, ensuring that each server operates only on random shares. While MPC provides strong cryptographic guarantees, it incurs substantial computational and communication overhead. Deep neural networks rely heavily on convolutional and fully connected layers, which require costly matrix multiplications in MPC. To reduce this cost, we propose leveraging low-rank decomposition (LRD) for linear layers, replacing one large matrix multiplication with two smaller ones. Each matrix multiplication in MPC incurs a round of communication, meaning decomposing one matrix multiplication into two leads to an additional communication round. Second, the added matrix multiplication requires an additional truncation step to maintain numerical precision. Since truncation itself requires communication and computation, these overheads can offset the gains from decomposition. To address this, we introduce two complementary optimizations: truncation skipping and efficient linear layer concatenation. Truncation skipping removes the extra truncation induced by LRD, while linear layer concatenation pipelines operations to hide the additional communication round. Together, these techniques mitigate the main overheads of LRD in MPC and improve overall efficiency. Our approach is broadly applicable across MPC protocols. Experiments show up to 25% speedup in n-PC and 33% in 3-PC protocols over full-rank baselines, along with up to 52% GPU energy savings and 88% reduction in offline-phase latency.

</details>


### [86] [Before the Vicious Cycle Starts: Preventing Burnout Across SOC Roles Through Flow-Aligned Design](https://arxiv.org/abs/2602.14598)
*Kashyap Thimmaraju,Duc Anh Hoang,Souradip Nath,Jaron Mink,Gail-Joon Ahn*

Main category: cs.CR

TL;DR: 이 연구는 11개국 35개 조직의 106개의 공개 SOC 채용 공고를 분석하여 SOC 직무 기술이 실제 요구사항과 일치하지 않음을 밝히고, 인력의 소진 문제와 일자리 수요 간의 불일치를 해소하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 보안 운영 센터의 지속 가능성은 인력에 달려 있지만, 많은 전문가들이 소진을 느끼고 있으며 사이버 보안을 완전히 떠날 계획을 세우고 있다.

Method: Inductive Content Analysis를 사용하여 인증, 기술적 역량, 소프트 스킬, 업무 및 경험 요건을 코딩하여 분석하였다.

Result: 커뮤니케이션 스킬이 채용 공고의 50.9%를 차지하고 있으며, 사이버 보안 인증서는 다양하게 나타났고, 프로그래밍 언어로서는 파이썬이 가장 많이 언급되었다.

Conclusion: 이 연구 결과는 기업들이 직무 기술서를 진단하고, 전문가들이 가치 있는 인증 및 기술을 식별하도록 돕고, 연구자들이 제시된 요구사항이 실제 수요와 일치하는지 검증할 수 있는 기초 자료를 제공한다.

Abstract: The sustainability of Security Operations Centers depends on their people, yet 71% of practitioners report burnout and 24% plan to exit cybersecurity entirely. Flow theory suggests that when job demands misalign with practitioner capabilities, work becomes overwhelming or tedious rather than engaging. Achieving challenge-skill balance begins at hiring: if job descriptions inaccurately portray requirements, organizations risk recruiting underskilled practitioners who face anxiety or overskilled ones who experience boredom. Yet we lack empirical understanding of what current SOC job descriptions actually specify. We analyzed 106 public SOC job postings from November to December 2024 across 35 organizations in 11 countries, covering Analysts (n=17), Incident Responders (n=38), Threat Hunters (n=39), and SOC Managers (n=12). Using Inductive Content Analysis, we coded certifications, technical skills, soft skills, tasks, and experience requirements. Three patterns emerged: (1) Communication skills dominate (50.9% of postings), exceeding SIEM tools (18.9%) or programming (30.2%), suggesting organizations prioritize collaboration over technical capabilities. (2) Certification expectations vary widely: CISSP leads (22.6%), but 43 distinct credentials appear with no universal standard. (3) Technical requirements show consensus: Python dominates programming (27.4%), Splunk leads SIEM platforms (14.2%), and ISO 27001 (13.2%) and NIST (10.4%) are most cited standards. These findings enable organizations to audit job descriptions against empirical baselines, help practitioners identify valued certifications and skills, and allow researchers to validate whether stated requirements align with actual demands. This establishes the foundation for flow-aligned interview protocols and investigation of how AI reshapes requirements. Dataset and codebook: https://git.tu-berlin.de/wosoc-2026/soc-jd-analysis.

</details>


### [87] [Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models](https://arxiv.org/abs/2602.14106)
*Mario Marín Caballero,Miguel Betancourt Alonso,Daniel Díaz-López,Angel Luis Perales Gómez,Pantaleone Nespoli,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: 클라우드 조직의 데이터 보안은 점점 증가하는 사이버 공격에 취약해지며, 본 논문은 보안 혼돈 공학(SCE) 방법론과 LLM 기반 흐름을 통합하여 공격 방어 나무를 자동 생성하고 대응 실험을 용이하게 하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 기반 조직의 데이터를 보호하는 것은 점점 더 중요한 과제가 되고 있으며, 전통적인 보안 조치의 구현이 느린 속도를 겪고 있다.

Method: 보안 혼돈 공학(SCE) 방법론과 LLM 기반 흐름을 통합하여 공격 방어 나무를 자동 생성한다.

Result: 팀이 공격자보다 한 발 앞서 나갈 수 있도록 방어 실험을 구축할 수 있게 한다.

Conclusion: 적극적인 사이버 방어 전략을 채택하는 것이 필수적이며, 이 연구는 그 가능성을 보여준다.

Abstract: The most valuable asset of any cloud-based organization is data, which is increasingly exposed to sophisticated cyberattacks. Until recently, the implementation of security measures in DevOps environments was often considered optional by many government entities and critical national services operating in the cloud. This includes systems managing sensitive information, such as electoral processes or military operations, which have historically been valuable targets for cybercriminals. Resistance to security implementation is often driven by concerns over losing agility in software development, increasing the risk of accumulated vulnerabilities. Nowadays, patching software is no longer enough; adopting a proactive cyber defense strategy, supported by Artificial Intelligence (AI), is crucial to anticipating and mitigating threats. Thus, this work proposes integrating the Security Chaos Engineering (SCE) methodology with a new LLM-based flow to automate the creation of attack defense trees that represent adversary behavior and facilitate the construction of SCE experiments based on these graphical models, enabling teams to stay one step ahead of attackers and implement previously unconsidered defenses. Further detailed information about the experiment performed, along with the steps to replicate it, can be found in the following repository: https://github.com/mariomc14/devsecops-adversary-llm.git.

</details>


### [88] [SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement](https://arxiv.org/abs/2602.14211)
*Xiaojun Jia,Jie Liao,Simeng Qin,Jindong Gu,Wenqi Ren,Xiaochun Cao,Yang Liu,Philip Torr*

Main category: cs.CR

TL;DR: 이 논문은 에이전트 기술 기반의 프롬프트 주입 공격을 자동화된 프레임워크를 통해 연구한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 기술이 도구 보강 행동을 확장하는 데 있어 핵심 추상화가 되고 있음에도 불구하고 기술 기반의 프롬프트 주입 공격은 잘 측정되지 않은 공격 표면을 제공한다.

Method: 우리는 명시적 은폐 제약 조건 하에 주입 기술을 합성하는 공격 에이전트, 주입된 기술을 사용해 현실적인 도구 환경에서 작업을 실행하는 코드 에이전트, 행동 추적을 기록하고 목표한 악의적 행동의 발생 여부를 검증하는 평가 에이전트로 구성된 폐쇄 루프 구조의 자동화된 프레임워크를 제안한다.

Result: 다양한 코딩 에이전트 환경과 실제 소프트웨어 공학 작업에 걸쳐 폭넓은 실험을 통해 우리의 방법이 현실적인 설정에서 consistently 높은 공격 성공률을 달성함을 보여준다.

Conclusion: 제안한 프레임워크는 기술 기반 프롬프트 주입 공격을 효과적으로 탐지하고 관리할 수 있는 가능성을 시사한다.

Abstract: Agent skills are becoming a core abstraction in coding agents, packaging long-form instructions and auxiliary scripts to extend tool-augmented behaviors. This abstraction introduces an under-measured attack surface: skill-based prompt injection, where poisoned skills can steer agents away from user intent and safety policies. In practice, naive injections often fail because the malicious intent is too explicit or drifts too far from the original skill, leading agents to ignore or refuse them; existing attacks are also largely hand-crafted. We propose the first automated framework for stealthy prompt injection tailored to agent skills. The framework forms a closed loop with three agents: an Attack Agent that synthesizes injection skills under explicit stealth constraints, a Code Agent that executes tasks using the injected skills in a realistic tool environment, and an Evaluate Agent that logs action traces (e.g., tool calls and file operations) and verifies whether targeted malicious behaviors occurred. We also propose a malicious payload hiding strategy that conceals adversarial operations in auxiliary scripts while injecting optimized inducement prompts to trigger tool execution. Extensive experiments across diverse coding-agent settings and real-world software engineering tasks show that our method consistently achieves high attack success rates under realistic settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [89] [Finding Highly Interpretable Prompt-Specific Circuits in Language Models](https://arxiv.org/abs/2602.13483)
*Gabriel Franco,Lucas M. Tassis,Azalea Rohr,Mark Crovella*

Main category: cs.LG

TL;DR: 본 연구는 언어 모델의 내부 회로를 이해하는 데 있어 프롬프트에 따라 회로가 달라질 수 있음을 강조하며, 이를 통해 더욱 정교한 기계적 설명을 제시하는 방법론을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델이 작업을 해결하기 위해 사용하는 내부 회로를 이해하는 것은 기계적 해석 가능성의 중심 과제입니다.

Method: 우리는 attention causal communication (ACC)에서 발전하여 ACC++를 소개하며, 이는 하나의 전방 패스를 통해 주의 헤드 내부에서 더 깨끗하고 낮은 차원의 인과 신호를 추출합니다.

Result: ACC++를 GPT-2, Pythia 및 Gemma 2의 간접 객체 식별에 적용한 결과, 어떤 모델에서도 IOI에 대한 단일 회로는 발견되지 않았으며, 다양한 프롬프트 템플릿이 체계적으로 다른 메커니즘을 유도함을 알았습니다.

Conclusion: 우리는 주제의 분석 단위를 작업에서 프롬프트로 이동하여 프롬프트별 메커니즘이 존재하는 상황에서 확장 가능한 회로 설명을 가능하게 함으로써 회로를 연구의 의미 있는 대상으로 재정의합니다.

Abstract: Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.

</details>


### [90] [ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer](https://arxiv.org/abs/2602.13666)
*Edward Chen,Natalie Dullerud,Pang Wei Koh,Thomas Niedermayr,Elizabeth Kidd,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: ALMo는 자궁경부암의 HDR 세침 방사선치료에서 임상 의도의 시행을 위한 상호작용적 의사결정 지원 시스템으로, 다차원적인 결정 과정을 단순화하고 치료 계획의 품질을 향상시키며 계획 시간을 단축시킨다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 임상 의사결정에서 임상 의사들은 다양한 경쟁 지표를 추적해야 하며, 이는 인지적으로 부담이 크고 변동성이 심하다.

Method: ALMo는 자동화된 매개변수 설정을 통해 수동 입력을 최소화하고 독성 위험에 대한 유연한 제어를 가능하게 하는 새로운 최적화 프레임워크를 사용한다.

Result: 25명의 임상 사례에 대한 회고적 평가에서 ALMo는 수동 계획 품질을 초과하는 치료 계획을 생성하였고, 65%의 사례에서 선량 개선을 보였다.

Conclusion: ALMo는 다기준 임상 의사결정에서 상호작용을 간소화할 수 있는 일반화된 프레임워크를 보여준다.

Abstract: In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.

</details>


### [91] [Physics Aware Neural Networks: Denoising for Magnetic Navigation](https://arxiv.org/abs/2602.13690)
*Aritra Das,Yashas Shende,Muskaan Chugh,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: 자기 이상 탐색은 GPS가 사용할 수 없거나 손상되었을 때 유망한 대안입니다. 본 논문에서는 확률적으로 왜곡된 자기 데이터를 처리하기 위한 두 가지 물리학 기반 제약 조건을 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 항공기 시스템이 지구 자기장 데이터를 추출할 때 기체가 유도하는 자기 잡음 때문에 어려움이 발생하고 있습니다.

Method: 다이버전스가 없는 벡터 필드와 E(3)-동등 불변성을 기반으로 하는 프레임워크를 제안합니다. 분산이 없는 제약 조건은 신경망을 훈련시켜 벡터 잠재력을 출력하여 구현하고, E(3)-동등 불변성에서는 구형 조화함수를 통해 표현 가능한 기하학적 텐서의 텐서 곱을 사용합니다.

Result: 제약 조건을 각각 그리고 함께 평가한 실험을 통해 이들 제약 조건이 예측 정확도와 물리적 타당성을 유의미하게 개선함을 보여주었습니다.

Conclusion: 합성 데이터 세트를 생성하여 데이터 부족 문제를 완화하고, Contiformer 아키텍처가 최선의 성능을 발휘함을 확인했습니다.

Abstract: Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.

</details>


### [92] [MechPert: Mechanistic Consensus as an Inductive Bias for Unseen Perturbation Prediction](https://arxiv.org/abs/2602.13791)
*Marc Boubnovski Martell,Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Robert Kitchen,Jesper Ferkinghoff-Borg,Jialin Yu,Philip Torr,Kaspar Märten*

Main category: cs.LG

TL;DR: MechPert는 유전자 변형에 대한 전사 반응을 예측하는 새로운 프레임워크로, 기능적 유사성을 넘어서 지정된 규제 가설 생성을 촉진한다.


<details>
  <summary>Details</summary>
Motivation: 유전자 조절을 이해하고 대규모 변형 실험의 우선 순위를 매기기 위한 예측의 필요성.

Method: LLM 에이전트가 독립적으로 후보 조절제를 제안하고, 이것을 합의 메커니즘을 통해 집계하여 예측을 위한 가중 네트워크를 생성하는 방식.

Result: MechPert는 Perturb-seq 벤치마크에서 저유전자 변형 데이터 체계에서 예측 시 Pearson 상관계수를 10.5% 개선하고, 선택된 앵커 유전자가 기존의 중앙성 기준보다 최대 46% 더 뛰어난 성과를 보인다.

Conclusion: MechPert는 기존 방법보다 더 나은 예측 성능과 실험 설계를 위한 유용성을 보여준다.

Abstract: Predicting transcriptional responses to unseen genetic perturbations is essential for understanding gene regulation and prioritizing large-scale perturbation experiments. Existing approaches either rely on static, potentially incomplete knowledge graphs, or prompt language models for functionally similar genes, retrieving associations shaped by symmetric co-occurrence in scientific text rather than directed regulatory logic. We introduce MechPert, a lightweight framework that encourages LLM agents to generate directed regulatory hypotheses rather than relying solely on functional similarity. Multiple agents independently propose candidate regulators with associated confidence scores; these are aggregated through a consensus mechanism that filters spurious associations, producing weighted neighborhoods for downstream prediction. We evaluate MechPert on Perturb-seq benchmarks across four human cell lines. For perturbation prediction in low-data regimes ($N=50$ observed perturbations), MechPert improves Pearson correlation by up to 10.5\% over similarity-based baselines. For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46\% in well-characterized cell lines.

</details>


### [93] [Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting](https://arxiv.org/abs/2602.13802)
*Xiaoyu Tao,Mingyue Cheng,Chuang Jiang,Tian Gao,Huanjian Zhang,Yaguo Liu*

Main category: cs.LG

TL;DR: Cast-R1은 시계열 예측을 순차적 의사결정 문제로 새롭게 정의한 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 기존 모델 중심의 접근 방식은 복잡하고 변화하는 환경에서의 예측에 한계를 보인다.

Method: Cast-R1은 정보 관련 요소를 유지하는 메모리 기반 상태 관리 메커니즘을 도입하여 문맥적 증거를 축적하고, 의사결정 지원을 위한 경량 예측 모델을 호출하며 예측을 반복적으로 개선한다.

Result: 다양한 실제 시계열 데이터셋에 대한 광범위한 실험을 통해 Cast-R1의 효과성을 입증하였다.

Conclusion: 이 연구는 시계열 모델링을 위한 에이전트 중심 패러다임 탐색의 실용적인 단계를 제공할 것으로 기대한다.

Abstract: Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.

</details>


### [94] [Fast Physics-Driven Untrained Network for Highly Nonlinear Inverse Scattering Problems](https://arxiv.org/abs/2602.13805)
*Yutong Du,Zicheng Liu,Yi Huang,Bazargul Matkerim,Bo Qi,Yali Zong,Peixian Han*

Main category: cs.LG

TL;DR: 본 연구는 높은 충실도의 전자기 역 산란 재구성을 위한 실시간 물리 기반 푸리에-스펙트럼(PDF) 해법을 제안하며, 이 해법은 스펙트럼 영역 차원 축소를 통해 1초 이내의 재구성을 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 훈련되지 않은 신경망(UNN)은 높은 충실도의 전자기 역 산란 재구성을 제공하지만 고차원 공간 최적화로 인해 계산적으로 제한을 받습니다.

Method: 우리는 유도 전류를 잘린 푸리에 기저를 사용하여 확대하고, 최적화가 산란 측정으로 지원되는 저주파 파라미터 공간에 국한되도록 하여 이 문제를 해결하는 실시간 물리 기반 푸리에-스펙트럼 해법을 제안합니다.

Result: CIE(축소 적분 방정식)를 통합하여 높은 대비 비선형성을 완화하고, 스펙트럼 유 induced 감쇠를 보정하기 위해 CCO(대비 보상 연산자)를 사용하는 결과를 얻습니다.

Conclusion: 수치적 및 실험적 결과는 잡음과 안테나 불확실성 하에서도 강력한 성능을 보이며, 현재의 UNN에 비해 100배의 속도 향상을 보여주어 실시간 마이크로파 이미징 응용을 가능하게 합니다.

Abstract: Untrained neural networks (UNNs) offer high-fidelity electromagnetic inverse scattering reconstruction but are computationally limited by high-dimensional spatial-domain optimization. We propose a Real-Time Physics-Driven Fourier-Spectral (PDF) solver that achieves sub-second reconstruction through spectral-domain dimensionality reduction. By expanding induced currents using a truncated Fourier basis, the optimization is confined to a compact low-frequency parameter space supported by scattering measurements. The solver integrates a contraction integral equation (CIE) to mitigate high-contrast nonlinearity and a contrast-compensated operator (CCO) to correct spectral-induced attenuation. Furthermore, a bridge-suppressing loss is formulated to enhance boundary sharpness between adjacent scatterers. Numerical and experimental results demonstrate a 100-fold speedup over state-of-the-art UNNs with robust performance under noise and antenna uncertainties, enabling real-time microwave imaging applications.

</details>


### [95] [AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning](https://arxiv.org/abs/2602.13807)
*Xiaoyu Tao,Yuchong Wu,Mingyue Cheng,Ze Guo,Tian Gao*

Main category: cs.LG

TL;DR: AnomaMind는 시계열 이상 탐지를 순차적 의사결정 프로세스로 재구성하여 성능을 향상시키는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 시계열 이상 탐지는 복잡한 환경에서 신뢰할 수 있는 의사결정을 지원하기 위해 반드시 필요하다.

Method: AnomaMind는 구조화된 워크플로우를 통해 이상 구간을 점진적으로 로컬라이즈하고, 적응형 피처 준비를 위해 다중 턴 도구 상호작용을 통해 탐지를 증대시키며, 자기 반성을 통해 이상 결정을 정제한다.

Result: 다양한 설정에서의 광범위한 실험 결과 AnomaMind가 이상 탐지 성능을 일관되게 향상시킨 것을 보여준다.

Conclusion: AnomaMind는 의사결정 프로세스로서 이상 탐지의 재구성을 통해 다양한 복잡한 설정에서 효과적으로 작동한다.

Abstract: Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.

</details>


### [96] [sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals](https://arxiv.org/abs/2602.13857)
*Weixuan Yuan,Zengrui Jin,Yichen Wang,Donglin Xie,Ziyi Ye,Chao Zhang,Xuesong Chen*

Main category: cs.LG

TL;DR: 다양하고 불완전한 야간 생체 신호를 위한 기초 모델인 sleep2vec을 제안하며, 이는 교차 모달 정렬을 통해 공유 표현을 배우고, 다양한 모달리티와 센서 결함에 강인성을 보인다.


<details>
  <summary>Details</summary>
Motivation: 기존의 수면 단계 분류 및 임상 진단 작업이 표준 다중 수면 검사 장치에 의존하는데, 장치 간 이질성과 센서 결함이 이러한 다중 모달 신호의 통합 모델링에 도전을 준다.

Method: sleep2vec은 42,249개의 야간 녹음을 대상으로 한 대비 프리트레이닝을 진행하며, 인구 통계, 나이, 사이트 및 역사 인식 InfoNCE 목표를 포함하여 부정 샘플을 동적으로 가중치 주어 코호트 특유의 단축을 완화한다.

Result: sleep2vec은 하위 잠재적 수면 단계 분류 및 임상 결과 평가에서 강력한 기준선보다 일관되게 우수한 성능을 발휘하며, 사용 가능한 모달리티의 어떤 서브셋에도 강건성을 가진다.

Conclusion: 통합적인 교차 모달 정렬과 원칙에 입각한 스케일링이 실제 야간 생체 신호의 레이블 효율적인 일반 모델링을 가능하게 한다는 결과를 보여준다.

Abstract: Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.

</details>


### [97] [A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning](https://arxiv.org/abs/2602.13937)
*Dat Le,Duc-Cuong Le,Anh-Son Nguyen,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.LG

TL;DR: iML은 자동화된 기계 학습(AutoML)의 엄폐 상자 문제를 해결하기 위해 설계된 새로운 다중 에이전트 프레임워크이며, 코드 안내 및 모듈화된 아키텍처를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 AutoML 프레임워크는 유연성과 투명성이 결여되어 복잡한 현실 세계의 엔지니어링 작업에 적합하지 않다.

Method: iML은 코드 안내 계획, 코드 모듈화 구현, 코드 검증 가능한 통합의 세 가지 주요 아이디어로 구성된다.

Result: iML은 MLE-BENCH에서 85%의 유효 제출률과 45%의 경쟁 메달률을 달성했으며, iML-BENCH에서는 APS에서 38%-163% 우수성을 보였다.

Conclusion: iML은 난수 생성과 신뢰할 수 있는 엔지니어링 사이의 간극을 메우는 잠재력을 보여준다.

Abstract: Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as "black boxes", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.

</details>


### [98] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: 경험적 강화 학습(ERL)은 환경 피드백을 통해 언어 모델이 학습하도록 지원하는 훈련 패러다임입니다.


<details>
  <summary>Details</summary>
Motivation: 환경 피드백이 통상적으로 희소하고 지연되는 상황에서 언어 모델이 효과적으로 학습해야 하는 문제를 해결하고자 합니다.

Method: ERL은 명시적인 경험-반성-통합 루프를 강화 학습 과정에 통합하여 모델이 초기 시도를 통해 환경 피드백을 받고, 이를 바탕으로 반성을 진행하여 개선된 두 번째 시도를 생성하도록 합니다.

Result: 희소 보상 제어 환경과 에이전트 추론 벤치마크에서 ERL은 기존의 강력한 강화 학습 기준선에 비해 학습 효율과 최종 성능을 지속적으로 향상시켰습니다.

Conclusion: 명시적인 자기 반성을 정책 훈련에 통합하는 것이 피드백을 지속 가능한 행동 개선으로 변환하는 실용적인 메커니즘을 제공한다는 것을 제안합니다.

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [99] [DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices](https://arxiv.org/abs/2602.14301)
*Songyuan Li,Jia Hu,Ahmed M. Abdelmoniem,Geyong Min,Haojun Huang,Jiwei Huang*

Main category: cs.LG

TL;DR: DeepFusion은 자원의 제약을 받는 장치에서 연합 학습을 통한 모델 훈련을 가능하게 하는 최초의 스케일 가능한 연합 MoE 훈련 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델은 방대한 데이터가 필요하나, 이는 개인정보 보호 문제와 자원 제약과 같은 도전 과제가 있다.

Method: DeepFusion은 연합 지식 증류를 통해 서로 다른 장치에서의 LLM 지식을 융합하여 글로벌 MoE 모델을 구성하는 프레임워크를 제안한다. 또한, View-Aligned Attention 모듈을 도입하여 글로벌 MoE 모델의 다단계 피쳐 표현을 통합한다.

Result: DeepFusion은 Qwen-MoE 및 DeepSeek-MoE와 같은 산업 수준의 MoE 모델을 사용하여 중앙 집중식 MoE 훈련에 가까운 성능을 달성하며, 통신 비용을 최대 71% 절감하고 토큰 난이도를 최대 5.28% 개선한다.

Conclusion: DeepFusion은 자원 제약 장치에서도 효과적인 MoE 훈련을 가능하게 하여 연합 학습의 한계를 극복하고, 실질적인 데이터 사용을 가능하게 한다.

Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.

</details>


### [100] [S2SServiceBench: A Multimodal Benchmark for Last-Mile S2S Climate Services](https://arxiv.org/abs/2602.14017)
*Chenyue Li,Wen Deng,Zhuotao Sun,Mengxi Jin,Hanzhe Cui,Han Li,Shentong Li,Man Kit Yu,Ming Long Lai,Yuhao Yang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: S2SForecasts는 기후 회복력 및 지속 가능성을 위한 중요한 계획 창을 제공하지만, 과학적 예측을 신뢰할 수 있는 행동 가능한 기후 서비스로 변환하는 데에는 '마지막 마일 갭'이 존재한다. 이에 따라 우리는 S2SServiceBench를 소개하며, 이를 통해 다중 모드 대형 언어 모델의 성능을 평가하고 다양한 서비스 제품에서의 문제점을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 기후 회복력 및 지속 가능성을 위한 주차에서 월간 계획 창을 제공하는 S2S 예측의 중요성에도 불구하고, 과학적 예측을 신뢰할 수 있는 기후 서비스로 전환하는 마지막 마일 갭이 존재한다.

Method: S2SServiceBench라는 프로젝트를 통해 다양한 서비스 제품에 대한 다중 모드 벤치마크를 수립하였다.

Result: S2SServiceBench는 10개의 서비스 제품과 150개 이상의 전문가 선정 사례를 포함하고 있으며, 이들 각각은 3개의 서비스 레벨에서 구체화되어 약 500개의 작업 및 1,000개 이상의 평가 항목을 생성하였다.

Conclusion: 기후 서비스 에이전트 구축에 대한 실행 가능한 지침을 제공하면서, S2S 서비스 이해 및 추론의 지속적인 문제점이 드러났다.

Abstract: Subseasonal-to-seasonal (S2S) forecasts play an essential role in providing a decision-critical weeks-to-months planning window for climate resilience and sustainability, yet a growing bottleneck is the last-mile gap: translating scientific forecasts into trusted, actionable climate services, requiring reliable multimodal understanding and decision-facing reasoning under uncertainty. Meanwhile, multimodal large language models (MLLMs) and corresponding agentic paradigms have made rapid progress in supporting various workflows, but it remains unclear whether they can reliably generate decision-making deliverables from operational service products (e.g., actionable signal comprehension, decision-making handoff, and decision analysis & planning) under uncertainty. We introduce S2SServiceBench, a multimodal benchmark for last-mile S2S climate services curated from an operational climate-service system to evaluate this capability. S2SServiceBenchcovers 10 service products with about 150+ expert-selected cases in total, spanning six application domains - Agriculture, Disasters, Energy, Finance, Health, and Shipping. Each case is instantiated at three service levels, yielding around 500 tasks and 1,000+ evaluation items across climate resilience and sustainability applications. Using S2SServiceBench, we benchmark state-of-the-art MLLMs and agents, and analyze performance across products and service levels, revealing persistent challenges in S2S service plot understanding and reasoning - namely, actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable, evidence-grounded analysis and planning for dynamic hazards-while offering actionable guidance for building future climate-service agents.

</details>


### [101] [Fluid-Agent Reinforcement Learning](https://arxiv.org/abs/2602.14559)
*Shishir Sharma,Doina Precup,Theodore J. Perkins*

Main category: cs.LG

TL;DR: 이 논문은 다중 에이전트 강화 학습에서 에이전트의 동적 생성이 가능한 유동 에이전트 환경을 제안하고, 이를 통해 에이전트 팀이 환경 요구에 맞춰 동적으로 크기를 조절할 수 있음을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 실제 세계에서 에이전트의 수는 고정되어 있지 않으며, 에이전트는 다른 에이전트를 생성할 수 있는 결정도 할 수 있다.

Method: 에이전트가 다른 에이전트를 생성할 수 있는 유동 에이전트 환경을 제안하고, 그 안에서 여러 MARL 알고리즘의 성능을 실험적으로 평가한다.

Result: 기존 벤치마크의 유동 변형 및 새로운 환경에서 실험을 통해 동적으로 생성되는 에이전트 팀의 성능을 평가한다.

Conclusion: 이 프레임워크는 환경의 요구에 맞춰 팀의 크기를 동적으로 조절하는 에이전트 팀을 생성한다.

Abstract: The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>


### [102] [EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models](https://arxiv.org/abs/2602.14024)
*Xinxing Zhou,Qingren Yao,Yiji Zhao,Chenghao Liu,Flora Salim,Xiaojie Yuan,Yanlong Wen,Ming Jin*

Main category: cs.LG

TL;DR: EIDOS라는 새로운 기초 모델을 소개하며, 기존의 미래 값 예측에서 잠재 공간 예측 학습으로 프리트레이닝 방식을 전환하여 시간적 일관성을 갖춘 구조화된 잠재 상태를 개발한다.


<details>
  <summary>Details</summary>
Motivation: 기존 시계열 기초 모델이 미래 관측값을 직접 예측하는 방식은 불완전한 잠재 표현을 초래하며, 이는 표면적인 잡음을 포착하기보다 일관되고 예측 가능한 시간 동력을 포착하는 데 실패한다.

Method: EIDOS는 원인적 Transformer를 훈련하여 잠재 표현의 진화를 예측하고, 이를 통해 구조화된 잠재 상태의 출현을 유도하는 방법론을 채택한다.

Result: EIDOS는 GIFT-Eval 벤치마크에서 표현 공간의 구조적 분열을 완화하고, 최첨단 성능을 달성한다.

Conclusion: 모델에 예측 가능한 잠재 동력을 학습하도록 제약하는 것은 보다 강력하고 신뢰할 수 있는 시계열 기초 모델을 향한 원칙적인 발걸음이다.

Abstract: Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

</details>


### [103] [Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849)
*Bardia Mohammadi,Nearchos Potamitis,Lars Klein,Akhil Arora,Laurent Bindschaedler*

Main category: cs.LG

TL;DR: 이 논문은 LLM 에이전트가 외부 시스템에 대한 도구 호출을 안전하게 처리하기 위해 Atomix라는 런타임을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트가 외부 시스템에서 즉각적인 도구 효과를 처리하는 과정에서 실패, 추측 또는 경합으로 인해 부작용이 발생할 수 있습니다.

Method: Atomix는 각 호출에 에포크를 태그하고, 자원별 경계를 추적하며, 진행 예측이 안전성을 나타낼 때만 커밋을 실행하는 진전을 인식하는 트랜잭셔널 의미론을 제공합니다.

Result: 실제 작업 부하에서 결함 주입을 통해 트랜잭셔널 재시도가 작업 성공률을 개선했으며, 경계 제한 커밋은 추측과 경합 하에서 아이솔레이션을 강화했습니다.

Conclusion: Atomix는 에이전트 도구 호출의 안전성을 높이기 위해 효과적으로 작용하며, 안전한 실패 처리 방법을 제공합니다.

Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.

</details>


### [104] [UniST-Pred: A Robust Unified Framework for Spatio-Temporal Traffic Forecasting in Transportation Networks Under Disruptions](https://arxiv.org/abs/2602.14049)
*Yue Wang,Areg Karapetyan,Djellel Difallah,Samer Madanat*

Main category: cs.LG

TL;DR: UniST-Pred는 공간적 표현 학습과 시간적 모델링을 분리한 후, 적응형 융합을 통해 통합하는 통합공간-시간 예측 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 스마트 교통 시스템의 핵심 요소인 시간 및 공간 기반의 트래픽 예측은 신호 제어 및 네트워크 수준의 트래픽 관리와 같은 다양한 하위 작업을 지원한다.

Method: 시간적 모델링을 공간적 표현 학습에서 분리하고, 적응형 representation-level 융합을 통해 두 가지를 통합하는 UniST-Pred라는 새로운 프레임워크를 제안한다.

Result: UniST-Pred는 실제 및 시뮬레이션 데이터셋 모두에서 강력한 예측 성능을 유지하며, 인프라 중단 하에서도 해석 가능한 공간-시간 표현을 생성한다.

Conclusion: 우리는 UniST-Pred를 기반으로 한 데이터셋을 구축하고 네트워크 연결 단절 시나리오에서도 이를 평가하며, 경량 설계에도 불구하고 기존 모델과 비교하여 경쟁력 있는 성능을 보여준다.

Abstract: Spatio-temporal traffic forecasting is a core component of intelligent transportation systems, supporting various downstream tasks such as signal control and network-level traffic management. In real-world deployments, forecasting models must operate under structural and observational uncertainties, conditions that are rarely considered in model design. Recent approaches achieve strong short-term predictive performance by tightly coupling spatial and temporal modeling, often at the cost of increased complexity and limited modularity. In contrast, efficient time-series models capture long-range temporal dependencies without relying on explicit network structure. We propose UniST-Pred, a unified spatio-temporal forecasting framework that first decouples temporal modeling from spatial representation learning, then integrates both through adaptive representation-level fusion. To assess robustness of the proposed approach, we construct a dataset based on an agent-based, microscopic traffic simulator (MATSim) and evaluate UniST-Pred under severe network disconnection scenarios. Additionally, we benchmark UniST-Pred on standard traffic prediction datasets, demonstrating its competitive performance against existing well-established models despite a lightweight design. The results illustrate that UniST-Pred maintains strong predictive performance across both real-world and simulated datasets, while also yielding interpretable spatio-temporal representations under infrastructure disruptions. The source code and the generated dataset are available at https://anonymous.4open.science/r/UniST-Pred-EF27

</details>


### [105] [Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems](https://arxiv.org/abs/2602.14901)
*Pramit Saha,Joshua Strong,Mohammad Alsharid,Divyanshu Mishra,J. Alison Noble*

Main category: cs.LG

TL;DR: ToolSelect는 다양한 전문 모델 중에서 적절한 모델을 선택하기 위해 개발된 시스템으로, 1448개의 쿼리에 대해 10개의 최신 방법보다 우수한 성능을 보였다.


<details>
  <summary>Details</summary>
Motivation: 단일 '최고' 모델이 거의 존재하지 않고, 각 작업에는 여러 경쟁하는 전문 모델이 필요하다.

Method: ToolSelect는 쿼리와 모델 행동 요약에 따라 전문 모델을 선택하는 주의 기반 신경 프로세스를 제안한다.

Result: ToolSelect는 4개의 서로 다른 작업군에서 10개의 최신 방법보다 우수한 성능을 보여준다.

Conclusion: ToolSelect는 다양한 전문 모델을 활용하여 임상 쿼리 응답의 효율성을 높이는 데 기여한다.

Abstract: Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.

</details>


### [106] [Decentralized Federated Learning With Energy Harvesting Devices](https://arxiv.org/abs/2602.14051)
*Kai Zhang,Xuanyu Cao,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 이 논문은 에너지 수확 기술을 적용하여 분산형 연합 학습 시스템에서의 수명 연장을 위한 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 분산형 연합 학습(DFL)에서의 에너지 집중적인 작업이 장치의 배터리를 빠르게 소모하여 운영 수명을 단축시키는 문제를 해결하기 위해.

Method: 에너지 수확 기술을 DFL 시스템에 적용하고, 장치 스케줄링 및 전력 제어 문제를 다중 에이전트 마르코프 결정 프로세스(MDP)로 모델링하여 사용한다.

Result: 제안된 알고리즘은 통신 오버헤드와 계산 복잡성을 크게 줄이며, 비대칭 최적성을 달성한다는 이론적 분석을 제공한다.

Conclusion: 실제 데이터 세트에 대한 포괄적인 수치 실험을 통해 이론적 결과를 검증하고 제안된 알고리즘의 효과를 입증했다.

Abstract: Decentralized federated learning (DFL) enables edge devices to collaboratively train models through local training and fully decentralized device-to-device (D2D) model exchanges. However, these energy-intensive operations often rapidly deplete limited device batteries, reducing their operational lifetime and degrading the learning performance. To address this limitation, we apply energy harvesting technique to DFL systems, allowing edge devices to extract ambient energy and operate sustainably. We first derive the convergence bound for wireless DFL with energy harvesting, showing that the convergence is influenced by partial device participation and transmission packet drops, both of which further depend on the available energy supply. To accelerate convergence, we formulate a joint device scheduling and power control problem and model it as a multi-agent Markov decision process (MDP). Traditional MDP algorithms (e.g., value or policy iteration) require a centralized coordinator with access to all device states and exhibit exponential complexity in the number of devices, making them impractical for large-scale decentralized networks. To overcome these challenges, we propose a fully decentralized policy iteration algorithm that leverages only local state information from two-hop neighboring devices, thereby substantially reducing both communication overhead and computational complexity. We further provide a theoretical analysis showing that the proposed decentralized algorithm achieves asymptotic optimality. Finally, comprehensive numerical experiments on real-world datasets are conducted to validate the theoretical results and corroborate the effectiveness of the proposed algorithm.

</details>


### [107] [Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization](https://arxiv.org/abs/2602.14159)
*Rizhen Hu,Yuan Cao,Boao Kong,Mou Sun,Kun Yuan*

Main category: cs.LG

TL;DR: Sparse Mixture-of-Experts (MoE) 모델은 Transformers의 효율적인 확장을 가능하게 하지만 전문가 간 중복과 라우팅 모호성 문제로 인해 모델 용량이 심각하게 저하된다. 본 논문은 전문가의 전문화 및 라우팅 효율성을 높이는 두 가지 정규화 손실을 제안하며, 이는 라우터나 모델 구조를 수정할 필요가 없다.


<details>
  <summary>Details</summary>
Motivation: Sparse Mixture-of-Experts 모델은 Transformer의 효율적 확장을 가능하게 하면서도 전문가의 중복과 라우팅의 모호성으로 인해 모델 용량이 제대로 활용되지 못하는 문제가 있습니다.

Method: 이 논문에서는 전문가의 전문화와 라우팅 효율성을 향상시키는 두 가지 플러그 앤 플레이 정규화 손실을 제안합니다. 첫 번째는 동일한 토큰에 대한 전문가의 SwiGLU 활성화 간의 코사인 유사성을 패널티하는 내부 계층 전문화 손실이며, 두 번째는 인접한 계층 간의 공동 Top-$k$ 라우팅 확률을 최대화하는 계층 간 결합 손실입니다.

Result: 광범위한 실험을 통해 사전 훈련, 미세 조정 및 제로샷 벤치마크에서 일관된 작업 성과 향상, 높은 전문가 전문화 및 낮은 엔트로피 라우팅을 보여주며, 이러한 개선은 더 안정적인 전문가 경로를 통해 더 빠른 추론으로 이어집니다.

Conclusion: 제안된 손실은 기존의 부하 분산 손실과 정규직하며 DeepSeekMoE의 공유 전문가 아키텍처 및 일반적인 top-$k$ MoE 아키텍처 모두와 호환됩니다.

Abstract: Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.

</details>


### [108] [When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift](https://arxiv.org/abs/2602.14161)
*Max Fomin*

Main category: cs.LG

TL;DR: LLM 기반 에이전트의 안전한 배포를 위해 프롬프트 주입 및 jailbreak 공격 탐지가 필수적이다. 본 연구에서는 다양한 18개의 데이터셋을 활용한 종합 분석을 통해 현재 평가 관행의 한계를 보여주고, LODO 평가 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트를 보다 안전하게 배포하기 위해서는 프롬프트 주입 및 jailbreak 공격을 탐지하는 것이 매우 중요하다.

Method: 부정확한 성능 평가를 극복하기 위해 Leave-One-Dataset-Out (LODO) 평가 방식을 제안하고, Sparse Auto-Encoder (SAE) 기능 계수를 분석하였다.

Result: 기존의 평가 방법은 성능을 과대평가하며, 모든 3가지 접근 방식은 간접 공격에 대해 낮은 탐지율을 보인다.

Conclusion: LODO 안정적인 SAE 특징은 분류기 결정에 대한 보다 신뢰할 수 있는 설명을 제공하며, 프롬프트 공격 탐지 연구를 위해 LODO를 적절한 프로토콜로 설정하였다.

Abstract: Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.

</details>


### [109] [Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection](https://arxiv.org/abs/2602.14251)
*Pinqiao Wang,Sheng Li*

Main category: cs.LG

TL;DR: 본 논문에서는 다양한 모델 간의 불일치를 신호로 활용하여 테이블형 데이터의 이상 탐지를 개선하는 다중 에이전트 토론 프레임워크 MAD를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 테이블형 데이터에 대한 강력한 성능은 이질적 모델 집합에서 발생하지만, 기존의 탐지기는 단일 모형이나 정적 앙상블로 처리됩니다.

Method: MAD는 각 에이전트가 정규화된 이상 점수, 신뢰도 및 구조화된 증거를 생성하고 이를 조정하는 레이어를 통해 불일치를 해결하는 다중 에이전트 시스템입니다.

Result: MAD는 최종적인 토론된 이상 점수와 감사 가능한 토론 흔적을 생성하며, 다양한 테이블형 이상 벤치마크에서 기초 모델들보다 향상된 강건성을 보입니다.

Conclusion: MAD는 메시지 공간과 합성 연산자를 제한함으로써 기존 접근 방식들을 복구할 수 있는 통합적인 에이전트 프레임워크입니다.

Abstract: Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement

</details>


### [110] [Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions](https://arxiv.org/abs/2602.14279)
*Ruomeng Ding,Tianwei Gao,Thomas P. Zollo,Eitan Bachmat,Richard Zemel,Zhun Deng*

Main category: cs.LG

TL;DR: 본 논문에서는 제한된 질문 노력으로 불확실성을 줄이기 위한 적응형 집단 유도 방법을 제안하며, 이 방법은 예산 내에서 질문과 응답자를 선택한다.


<details>
  <summary>Details</summary>
Motivation: 조사 및 집단 평가를 통해 잠재적인 집단 수준 특성에 대한 불확실성을 줄이기 위해서는 제한된 질문 노력을 효율적으로 배분해야 한다.

Method: LLM 기반 예상 정보 이득 목표와 이질적인 그래프 신경망 전파를 결합한 이론적 프레임워크를 제안한다.

Result: 세 개의 실제 설문 데이터셋에서, 본 방법은 제한된 예산 하에서 집단 수준 응답 예측을 향상시킨다.

Conclusion: 응답자 예산 10%에서 CES의 12% 이상의 상대적 향상을 포함하여, 우리 방법은 일관되게 집단 수준 응답 예측을 개선한다.

Abstract: Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.

</details>


### [111] [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293)
*Kris Shengjun Dong,Sahil Modi,Dima Nikiforov,Sana Damani,Edward Lin,Siva Kumar Sastry Hari,Christos Kozyrakis*

Main category: cs.LG

TL;DR: CUDA 코드 최적화에 대한 새로운 접근 방식을 제시하는 논문으로, Memory-Augmented In-context Reinforcement Learning 프레임워크인 KernelBlaster를 소개한다. 이 프레임워크는 GPU 아키텍처 전세대에 걸쳐 우수한 성능을 달성할 수 있도록 하여, 기존 방법보다 더 높은 속도 향상을 제공한다.


<details>
  <summary>Details</summary>
Motivation: CUDA 코드의 성능을 극대화하기 위한 최적화 작업은 여러 세대의 GPU 아키텍처에 걸쳐 복잡하고 어렵다.

Method: KernelBlaster는 경험에서 학습하고 미래 작업에 대해 체계적으로 정보를 바탕으로 결정을 내릴 수 있도록 하기 위해 메모리 증강 인 컨텍스트 강화 학습(MAIC-RL) 프레임워크를 사용하여 작성되었다. 또한, 새로운 프로필 기반 텍스트 그래디언트 에이전트 흐름을 제안하여 CUDA 코드의 생성 및 최적화를 지원한다.

Result: KernelBench의 1, 2, 3 레벨에서 각각 1.43배, 2.50배, 1.50배의 기하 평균 속도 향상을 달성하였다.

Conclusion: KernelBlaster는 오픈 소스 에이전틱 프레임워크로 제공되며, 테스트 하니스, 검증 구성 요소 및 재현 가능한 평가 파이프라인이 함께 제공된다.

Abstract: Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.

</details>


### [112] [Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows](https://arxiv.org/abs/2602.14295)
*Edwin Chen,Zulekha Bibi*

Main category: cs.LG

TL;DR: MLAT는 학습된 기계 학습 모델을 LLM 에이전트 워크플로우 내에서 호출 가능한 도구로 노출시키는 디자인 패턴이다.


<details>
  <summary>Details</summary>
Motivation: MLAT는 LLM이 대화 맥락에 따라 기계 학습 모델을 호출하여 활용할 수 있도록 하여 효율적인 예측과 분석을 가능하게 한다.

Method: PitchCraft라는 파일럿 생산 시스템을 통해 기계 학습 기반 가격 예측을 이용하여 발견 전화 기록을 전문 제안으로 변환한다.

Result: 가격 모델은 70개의 예제를 기반으로 훈련되며, 보류된 데이터에서 R^2 = 0.807과 평균 절대 오차 3688 USD를 달성한다.

Conclusion: MLAT는 맥락적 추론과 함께 정량적 추정이 필요한 도메인에 일반화할 수 있는 가능성을 지닌다.

Abstract: We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.
  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.
  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.

</details>


### [113] [Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study](https://arxiv.org/abs/2602.14322)
*Hani Beirami,M M Manjurul Islam*

Main category: cs.LG

TL;DR: 이 논문은 형식적 시간 논리 사양이 항공 우주 응용 프로그램에서 강화 학습 제어의 안전성과 강인성을 향상시킬 수 있는 방법을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 형식적 시간 논리 사양을 통해 강화 학습 제어의 안전성과 강인성을 확보하고자 하였다.

Method: AeroBench F-16 시뮬레이션 벤치마크를 사용하여 Proximal Policy Optimization (PPO) 에이전트를 훈련시키고, 엔진 스로틀을 조절하고 명령된 비행 속도를 추적한다. 제어 목표는 Signal Temporal Logic (STL) 요구 사항으로 인코딩되어 절차의 마지막 몇 초 동안 비행 속도를 유지하도록 한다. 실행 시 이 사양을 강제하기 위해, online conformal prediction을 사용하여 RL 에이전트의 행동을 필터링하는 conformal STL shield를 도입한다.

Result: 세 가지 설정을 비교하였다: (i) PPO 기준선, (ii) 고전적인 규칙 기반 STL 방패가 있는 PPO, (iii) 제안된 conformal shield가 있는 PPO. 이들은 정상 조건과 비행 역학 모델 불일치, 액추에이터 속도 제한, 측정 잡음 및 중간 에피소드 목표 점프가 있는 심각한 스트레스 시나리오에서 테스트되었다. 실험 결과, conformal shield는 STL 만족도를 유지하면서 기준선 성능에 근접한 성능을 유지하고, 고전적인 방패보다 더 강력한 강인성 보장을 제공하였다.

Conclusion: 형식적 사양 모니터링과 데이터 기반 RL 제어의 결합이 도전적인 환경에서 자율 비행 제어의 신뢰성을 크게 향상시킬 수 있음을 보여준다.

Abstract: We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent's actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.

</details>


### [114] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2602.14344)
*Mathias Jackermeier,Mattia Giuri,Jacques Cloete,Alessandro Abate*

Main category: cs.LG

TL;DR: 이 논문은 다중 작업 강화 학습에서 에이전트가 훈련 중에 보지 못한 새로운 작업을 제로샷으로 수행하는 방법을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 기존 접근 방식은 일반 정책을 성공적으로 훈련하지만 LTL 사양에서 고유한 논리적 및 시간 구조를 효과적으로 캡처하는 데 어려움을 겪고 있다.

Method: 우리는 작업의 유한 오토마톤에서 구성된 부울 공식의 시퀀스를 통해 정책을 조건화하는 새로운 방법론을 제안한다. 논리적 구조를 인코딩하기 위한 계층적 신경 아키텍처를 제안하고, 정책이 미래 하위 목표에 대해 추론할 수 있도록 하는 주의 메커니즘을 도입한다.

Result: 다양한 복잡한 환경에서 실험이 수행되었으며, 우리의 접근 방식은 강력한 일반화 능력과 우수한 성능을 보여준다.

Conclusion: 우리의 방법론은 LTL 사양을 성공적으로 캡처하고, 에이전트의 훈련 및 일반화를 촉진하는 구조화된 작업 표현을 학습할 수 있다.

Abstract: We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.

</details>


### [115] [Traceable Latent Variable Discovery Based on Multi-Agent Collaboration](https://arxiv.org/abs/2602.14456)
*Huaming Du,Tao Hu,Yijie Huang,Yu Zhao,Guisong Liu,Tao Gu,Gang Kou,Carl Yang*

Main category: cs.LG

TL;DR: TLVD는 대규모 언어 모델과 전통적 인과 발견 알고리즘을 결합하여 잠재 변수를 추론하고 그 의미를 기반으로 하는 새로운 인과 모델링 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 실제 세계에서의 인과 메커니즘을 밝히는 것은 과학 및 기술의 발전에 필수적이다.

Method: TLVD는 대규모 언어 모델의 메타데이터 기반 추론 기능과 데이터 기반 모델링 능력을 통합하여, 잠재 변수를 추론하기 위한 인과 그래프를 구성하고 다수의 대규모 언어 모델 간 협업을 통해 잠재 변수 추론을 수행한다.

Result: 다양한 실세계 웹 기반 데이터 소스를 통해 잠재 변수가 유효한지 검증하며, 5개의 데이터 세트에서 평균적으로 32.67%의 정확도, 62.21%의 보정 정확도, 26.72%의 ECit 개선을 확인했다.

Conclusion: TLVD의 실험 결과는 그 효과성과 신뢰성을 입증한다.

Abstract: Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.

</details>


### [116] [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519)
*Chaosheng Dong,Peiyao Xiao,Yijia Wang,Kaiyi Ji*

Main category: cs.LG

TL;DR: DeepMTL2R는 다중 관련성 기준을 동시에 최적화하기 위한 오픈 소스 딥 러닝 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 다양하고 잠재적으로 상충하는 목표를 통해 효과적인 학습을 가능하게 하기 위해 다중 작업 학습을 통합할 필요가 있다.

Method: DeepMTL2R는 트랜스포머 아키텍처의 자기 주의 메커니즘을 활용하여 이질적인 관련 신호를 통합한다.

Result: 21개의 최첨단 다중 작업 학습 알고리즘을 포함하고 다목적 최적화를 지원하여 파레토 최적 순위 모델을 식별한다.

Conclusion: DeepMTL2R는 현대 순위 시스템을 위한 확장 가능하고 표현력이 뛰어난 솔루션을 제공하며, MTL 전략 간의 통제된 비교를 용이하게 한다.

Abstract: This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.

</details>


### [117] [RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch](https://arxiv.org/abs/2602.14578)
*Isam Vrce,Andreas Kassler,Gökçe Aydos*

Main category: cs.LG

TL;DR: 이 논문에서는 심층 강화 학습에서 N:M 구조적 희소성의 첫 번째 연구를 소개하며, 압축, 성능 및 하드웨어 효율성을 균형 있게 유지한다.


<details>
  <summary>Details</summary>
Motivation: 심층 신경망(DNN)의 성능을 저해하지 않으면서 압축하는 잘 연구된 기술인 희소성이 필요하다.

Method: 모든 네트워크에 대해 오프 폴리시 RL(TD3) 내내 행별 N:M 희소성을 유지한다.

Result: RNM-TD3는 50%-75% 희소성에서 밀집 대안보다 우수한 성능을 나타내며 Ant 환경에서 2:4 희소성에서 성능이 최대 14% 향상된다.

Conclusion: RNM-TD3는 87.5% 희소성(1:8)에서도 경쟁력을 유지하면서 잠재적인 훈련 속도 향상을 가능하게 한다.

Abstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.

</details>


### [118] [Concepts' Information Bottleneck Models](https://arxiv.org/abs/2602.14626)
*Karim Galliamov,Syed M Ahsan Kazmi,Adil Khan,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 정보 병목 레귤러라이저를 도입하여 해석 가능성을 유지하면서도 정확성을 향상시키는 방법을 제시함.


<details>
  <summary>Details</summary>
Motivation: CBM은 해석 가능한 예측을 제공하지만 정확성과 신뢰성을 떨어뜨리는 문제를 해결하고자 함.

Method: 개념 레이어에서 정보 병목 레귤러라이저를 도입하여 $I(X;C)$를 패널티하고 $I(C;Y)$에서 작업 관련 정보를 유지.

Result: 여섯 개의 CBM 패밀리와 세 가지 벤치마크에서 IB-정규화 모델이 기존 모델보다 일관되게 우수한 성능을 보임.

Conclusion: 최소 충분 개념 병목을 강제함으로써 예측 성능과 개념 수준의 개입 신뢰성을 모두 향상시킬 수 있음을 보여줌.

Abstract: Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.

</details>


### [119] [Extending Multi-Source Bayesian Optimization With Causality Principles](https://arxiv.org/abs/2602.14791)
*Luuk Jacobs,Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: 이 논문은 다중 정보 출처에 대한 인과적 베이지안 최적화(MSCBO) 방법론을 제안하며, 전통적인 베이지안 최적화의 한계를 극복하고 최적화 효율성을 높이며 연산 복잡성을 줄이는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 다중 정보 출처에서 목표 블랙박스 함수의 최적화를 다루는 MSBO와 인과성 원칙을 적용한 CBO의 통합 필요성.

Method: 인과적 베이지안 최적화와 다중 출처 베이지안 최적화 방법론을 통합하여 MSCBO 알고리즘을 개발.

Result: MSCBO는 실험 데이터셋과 실제 데이터셋에 대해 강건성과 적용 가능성을 입증하며, 기존 기법들에 비해 성능을 비교하였다.

Conclusion: MSBO와 CBO의 통합은 차원 축소를 용이하게 하고 운영 비용을 감소시켜 최적화 속도, 성능 및 확장성을 향상시킨다.

Abstract: Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.

</details>


### [120] [Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment](https://arxiv.org/abs/2602.14844)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: AI 정렬의 중요성이 증가하고 있으나, 현재 접근 방식에는 안전 목표와 에이전트 정책이 얽혀 있는 구조적 결함이 존재한다. 우리는 정렬 아티팩트 학습과 정책 최적화를 분리하는 방법으로, 검토 가능하고 편집 가능한 보상 모델을 생성하는 비상호작용 역강화 학습을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 안전성을 강화하기 위해 일관되고 효과적인 방법이 필요하다.

Method: 비상호작용 역강화 학습을 통한 정렬 아티팩트 학습과 정책 최적화의 분리 및 정렬 플라이휠을 통한 보상 모델의 반복적인 강화.

Result: 검토 가능하고 편집 가능한 모델 기반의 보상 모델과 안전성을 강화하는 아키텍처의 제안.

Conclusion: 안전성을 일회성이 아니라 지속 가능하고 검증 가능한 공학 자산으로 전환하는 접근 방식이다.

Abstract: AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.

</details>


### [121] [Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning](https://arxiv.org/abs/2602.14868)
*Ilia Mahrooghi,Aryo Lotfi,Emmanuel Abbe*

Main category: cs.LG

TL;DR: Goldilocks라는 새로운 데이터 샘플링 전략을 통해 강화 학습의 비효율성을 개선하고, 학생 모델의 능력에 맞는 질문을 선택하여 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 희소한 보상에 의존하는 강화 학습의 비효율성을 해결하고, 모델에 맞는 데이터 순서를 찾는 것이 필요하다.

Method: Goldilocks라는 교사 주도 데이터 샘플링 전략을 제안하여 학생 모델을 위한 적정 난이도의 질문을 예측한다.

Result: Goldilocks 데이터 샘플링은 OpenMathReasoning 데이터셋에서 동일한 컴퓨팅 예산으로 표준 GRPO로 훈련된 모델의 성능을 향상시킨다.

Conclusion: 이 방법은 학생 모델의 성능 향상에 기여하고, 교사 모델이 학생의 진화하는 능력에 따라 지속적으로 조정된다.

Abstract: Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

</details>


### [122] [Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation](https://arxiv.org/abs/2602.14914)
*Olivier Jeunen,Shashank Gupta*

Main category: cs.LG

TL;DR: 이 논문에서는 비정책 평가에서 기본 베이스라인이 우수한 성능을 제공함을 이론적으로 증명하고, 이를 통해 자가 정규화 방법에서 최적의 베이스라인 수정으로의 전환을 정당화합니다.


<details>
  <summary>Details</summary>
Motivation: 비정책 평가(OPE)는 비싼 온라인 개입 없이 랭킹 및 추천 시스템을 평가하는 데 필수적입니다.

Method: 최적의 덧셈 기반 선정을 통해 $β^igstar$-IPS라는 추정기가 평균 제곱 오차에서 SNIPS를 점근적으로 초우세함을 증명합니다.

Result: SNIPS는 특정하지만 일반적으로 아랫쪽 최적이 아닌 덧셈 기준을 사용하는 것과 점근적으로 동등함을 보여줍니다.

Conclusion: 본 연구 결과는 랭킹 및 추천에서 자가 정규화에서 최적의 기준 수정으로의 전환을 이론적으로 정당화합니다.

Abstract: Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.

</details>


### [123] [BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs](https://arxiv.org/abs/2602.14919)
*Tianyi Ma,Yiyue Qian,Zehong Wang,Zheyuan Zhang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: BHyGNN+는 라벨이 없는 이분형 하이퍼그래프에서 자기 지도 학습을 통해 표현 학습을 향상시키는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 하이퍼그래프 신경망(HyGNN)은 엔터티 간의 고차원 관계를 모델링하는 데 성공했지만, 레이블이 부족한 현실 세계에서의 적용이 제한된다.

Method: BHyGNN+는 하이퍼그래프 쌍대성을 기반으로 하여 코사인 유사성을 사용해 하이퍼그래프의 증강된 뷰를 대조하는 자기 감독 학습 프레임워크이다.

Result: BHyGNN+는 이분형 및 동형 하이퍼그래프 모두에서 최신 감독 및 자기 지도 기준선을 지속적으로 초과하는 성능을 나타낸다.

Conclusion: 하이퍼그래프 쌍대성을 활용한 자기 지도 학습의 효과를 검증하고, 레이블이 없는 하이퍼그래프에서 표현 학습을 위한 새로운 패러다임을 확립한다.

Abstract: Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.

</details>


### [124] [Use What You Know: Causal Foundation Models with Partial Graphs](https://arxiv.org/abs/2602.14972)
*Arik Reuter,Anish Dhir,Cristiana Diaconu,Jake Robertson,Ole Ossen,Frank Hutter,Adrian Weller,Mark van der Wilk,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 이 논문에서는 Causal Foundation Models (CFMs)에 도메인 지식을 통합하는 방법을 소개하여 인과적 거버넌스와 추론을 효과적으로 진행하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: CFMs는 인과적 발견과 추론을 단일 단계로 통합하는 것을 목표로 하지만, 현재 상태에서는 도메인 지식을 통합할 수 없어 예측이 최적화되지 않을 수 있습니다.

Method: 우리는 인과 그래프나 쉽게 접근 가능한 조상 정보를 포함하여 CFMs를 인과적 정보로 조건화하는 방법을 도입합니다.

Result: 조건화 전략을 체계적으로 평가한 결과, 주의 메커니즘에 학습 가능한 편향을 주입하는 것이 전체 및 부분 인과 정보를 활용하기 위한 가장 효과적인 방법임을 발견했습니다.

Conclusion: 우리의 접근 방식은 데이터 기반 방식으로 인과 쿼리에 답할 수 있는 능력을 갖추면서 도메인 전문 지식을 효과적으로 활용할 수 있는 전환점이 됩니다.

Abstract: Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.

</details>


### [125] [Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation](https://arxiv.org/abs/2602.15022)
*Cai Zhou,Zijie Chen,Zian Li,Jike Wang,Kaiyi Jiang,Pan Li,Rose Yu,Muhan Zhang,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 이 논문은 화학과 과학의 생성 과제가 그룹 대칭에 대해 불변인 분포를 포함한다고 강조하며, 기존의 불변성을 엄격하게 강제하는 방법 대신 대칭 변환 샘플링을 통해 불변 분포를 회복하는 새로운 접근법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 화학 및 과학의 많은 생성 작업은 그룹 대칭에 불변인 분포를 포함하고 있으며, 기존 방법은 불변성과 동등성을 아키텍처 제약을 통해 강제해왔다.

Method: 각 샘플을 표준 자세 또는 순서의 궤도 대표에 매핑한 후, 비제약성(비동등) 확산 또는 흐름 모델을 표준 슬라이스에서 학습하고, 생성 시 랜덤 대칭 변환을 샘플링하여 불변 분포를 회복하는 접근을 사용한다.

Result: 표준 확산의 이론을 정립하여 표준 생성 모델의 정확성, 보편성 및 우수한 표현력을 입증하고, 표준화가 훈련을 가속화하며, 정렬된 사전 및 최적 수송이 상호 보완적으로 작용하여 훈련 효율성을 향상시킨다.

Conclusion: 구조 화학적 그래프 생성을 위해 제안된 프레임워크는 3D 분자 생성 작업에서 동등 기준보다 현저히 우수한 성능을 발휘하고, CanonFlow는 GEOM-DRUG 데이터셋에서 최첨단 성능을 달성한다.

Abstract: Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.

</details>


### [126] [Directional Concentration Uncertainty: A representational approach to uncertainty quantification for generative models](https://arxiv.org/abs/2602.13264)
*Souradeep Chattopadhyay,Brendan Kennedy,Sai Munikoti,Soumik Sarkar,Karl Pazdernik*

Main category: cs.LG

TL;DR: 방향 집중 불확실성(DCU)이라는 새로운 통계 절차를 제안하여 생성 모델의 신뢰성과 견고성을 높이는 유연한 불확실성 정량화 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 생성 모델을 신뢰할 수 있고 견고하게 만들기 위한 불확실성 정량화(UQ) 방법의 필요성.

Method: 방향 집중 불확실성(DCU)이라는 새로운 통계 절차를 사용하여 vMF 분포를 기반으로 임베딩의 집중을 정량화한다.

Result: DCU는 기존의 의미 엔트로피와 같은 방법의 보정 수준을 맞추거나 초과하며, 다중 모달 도메인에서 복잡한 작업에도 잘 일반화된다.

Conclusion: DCU의 더 넓은 잠재력과 다중 모달 및 행동 프레임워크에 UQ 통합에 대한 함의를 제시한다.

Abstract: In the critical task of making generative models trustworthy and robust, methods for Uncertainty Quantification (UQ) have begun to show encouraging potential. However, many of these methods rely on rigid heuristics that fail to generalize across tasks and modalities. Here, we propose a novel framework for UQ that is highly flexible and approaches or surpasses the performance of prior heuristic methods. We introduce Directional Concentration Uncertainty (DCU), a novel statistical procedure for quantifying the concentration of embeddings based on the von Mises-Fisher (vMF) distribution. Our method captures uncertainty by measuring the geometric dispersion of multiple generated outputs from a language model using continuous embeddings of the generated outputs without any task specific heuristics. In our experiments, we show that DCU matches or exceeds calibration levels of prior works like semantic entropy (Kuhn et al., 2023) and also generalizes well to more complex tasks in multi-modal domains. We present a framework for the wider potential of DCU and its implications for integration into UQ for multi-modal and agentic frameworks.

</details>


### [127] [Exploring the Performance of ML/DL Architectures on the MNIST-1D Dataset](https://arxiv.org/abs/2602.13348)
*Michael Beebe,GodsGift Uzor,Manasa Chepuri,Divya Sree Vemula,Angel Ayala*

Main category: cs.LG

TL;DR: MNIST의 1차원 변형인 MNIST-1D 데이터셋을 이용하여 Residual Networks, Temporal Convolutional Networks, Dilated Convolutional Neural Networks의 성능을 평가한 연구로, 고급 모델들이 간단한 모델들을 일관되게 초월함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 소규모 데이터셋은 머신러닝 연구에서 실험과 모델 평가를 위한 통제된 환경을 제공하지만, 고급 신경망 아키텍처를 구별하는 데는 한계가 있다. 이를 극복하기 위해 MNIST-1D 데이터셋을 도입하였다.

Method: Residual Networks, Temporal Convolutional Networks, Dilated Convolutional Neural Networks의 성능을 평가하고, 로지스틱 회귀, MLP, CNN, GRU와 벤치마킹하였다.

Result: TCN과 DCNN이 간단한 모델들보다 일관되게 우수한 성능을 보이며, MNIST-1D에서 인간 수준의 성능에 가까운 성과를 달성하였다. ResNet 또한 significant한 개선을 보였다.

Conclusion: MNIST-1D 데이터셋은 컴퓨팅 제약 하에서도 머신러닝 아키텍처 평가를 위한 강력한 벤치마크로서 유용함을 입증하였다.

Abstract: Small datasets like MNIST have historically been instrumental in advancing machine learning research by providing a controlled environment for rapid experimentation and model evaluation. However, their simplicity often limits their utility for distinguishing between advanced neural network architectures. To address these challenges, Greydanus et al. introduced the MNIST-1D dataset, a one-dimensional adaptation of MNIST designed to explore inductive biases in sequential data. This dataset maintains the advantages of small-scale datasets while introducing variability and complexity that make it ideal for studying advanced architectures.
  In this paper, we extend the exploration of MNIST-1D by evaluating the performance of Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN). These models, known for their ability to capture sequential patterns and hierarchical features, were implemented and benchmarked alongside previously tested architectures such as logistic regression, MLPs, CNNs, and GRUs. Our experimental results demonstrate that advanced architectures like TCN and DCNN consistently outperform simpler models, achieving near-human performance on MNIST-1D. ResNet also shows significant improvements, highlighting the importance of leveraging inductive biases and hierarchical feature extraction in small structured datasets.
  Through this study, we validate the utility of MNIST-1D as a robust benchmark for evaluating machine learning architectures under computational constraints. Our findings emphasize the role of architectural innovations in improving model performance and offer insights into optimizing deep learning models for resource-limited environments.

</details>
