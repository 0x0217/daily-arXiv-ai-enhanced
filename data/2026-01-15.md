<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.LG](#cs.LG) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments](https://arxiv.org/abs/2601.07853)
*Zhi Yang,Runguo Li,Qiqi Qiang,Jiashun Wang,Fangqi Lou,Mengping Li,Dongpo Cheng,Rui Xu,Heng Lian,Shuo Zhang,Xiaolong Liang,Xiaoming Huang,Zheng Wei,Zhaowei Liu,Xin Guo,Huacan Wang,Ronghao Chen,Liwen Zhang*

Main category: cs.CR

TL;DR: 이 논문에서는 재무 에이전트를 위한 최초의 실행 기반 보안 벤치마크인 FinVault를 제안하며, 이는 다양한 시나리오와 취약점을 포함하고 있습니다.


<details>
  <summary>Details</summary>
Motivation: 재무 환경에서 LLM을 사용하는 에이전트의 보안 위험을 평가하기 위해, 기존의 안전성 평가가 실제 운영 워크플로우와 상태 변경 행동으로부터 발생하는 위험을 포착하지 못하고 있는 문제를 해결하고자 함.

Method: FinVault는 31개의 규제 사례 기반 샌드박스 시나리오와 107개의 실제 취약점, 963개의 테스트 케이스를 포함하여 실행 기반 보안 벤치마크를 제안함.

Result: 실험 결과, 기존 방어 메커니즘이 현실적인 재무 에이전트 설정에서 비효율적이며, 최고 모델에 대해 평균 공격 성공률이 50.0%에 달하며, 가장 강력한 시스템에서도 비중 없는 수치임을 보여줌.

Conclusion: 따라서, 현재의 안전성 설계의 한계가 발견되었고, 더 강력한 재무 특화 방어가 필요함을 강조함.

Abstract: Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.

</details>


### [2] [Sola-Visibility-ISPM: Benchmarking Agentic AI for Identity Security Posture Management Visibility](https://arxiv.org/abs/2601.07880)
*Gal Engelberg,Konstantin Koutsyi,Leon Goldberg,Reuven Elezra,Idan Pinto,Tal Moalem,Shmuel Cohen,Yoni Weintrob*

Main category: cs.CR

TL;DR: 이 논문은 에이전틱 AI 시스템의 ISPM 가시성 성능을 평가하기 위한 Sola Visibility ISPM 벤치마크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 및 SaaS 환경에서 운영되는 현대 기업의 정체성 보안 자세 관리는 주요 도전 과제이다.

Method: Sola Visibility ISPM Benchmark는 AWS, Okta, Google Workspace를 아우르는 실제 운영 환경을 사용하여 ISPM 가시성 작업을 평가하도록 설계되었다.

Result: 77개의 벤치마크 질문에 대해 에이전트는 전문가 정확도 0.84와 엄격한 성공률 0.77로 강력한 성능을 달성하였다.

Conclusion: 이 연구는 에이전틱 AI 시스템의 정체성 보안을 평가하기 위한 실용적이고 재현 가능한 벤치마크를 제공하며, 미래의 ISPM 벤치마크를 위한 기초를 마련한다.

Abstract: Identity Security Posture Management (ISPM) is a core challenge for modern enterprises operating across cloud and SaaS environments. Answering basic ISPM visibility questions, such as understanding identity inventory and configuration hygiene, requires interpreting complex identity data, motivating growing interest in agentic AI systems. Despite this interest, there is currently no standardized way to evaluate how well such systems perform ISPM visibility tasks on real enterprise data. We introduce the Sola Visibility ISPM Benchmark, the first benchmark designed to evaluate agentic AI systems on foundational ISPM visibility tasks using a live, production-grade identity environment spanning AWS, Okta, and Google Workspace. The benchmark focuses on identity inventory and hygiene questions and is accompanied by the Sola AI Agent, a tool-using agent that translates natural-language queries into executable data exploration steps and produces verifiable, evidence-backed answers. Across 77 benchmark questions, the agent achieves strong overall performance, with an expert accuracy of 0.84 and a strict success rate of 0.77. Performance is highest on AWS hygiene tasks, where expert accuracy reaches 0.94, while results on Google Workspace and Okta hygiene tasks are more moderate, yet competitive. Overall, this work provides a practical and reproducible benchmark for evaluating agentic AI systems in identity security and establishes a foundation for future ISPM benchmarks covering more advanced identity analysis and governance tasks.

</details>


### [3] [Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models](https://arxiv.org/abs/2601.07885)
*Weipeng Jiang,Xiaoyu Zhang,Juan Zhai,Shiqing Ma,Chao Shen,Yang Liu*

Main category: cs.CR

TL;DR: 이 논문은 감정 이모티콘이 대형 언어 모델(LLM)에 미치는 안전성 문제를 탐구하며, 이모티콘의 의미 혼동이 LLM의 오해를 초래할 수 있음을 밝힙니다.


<details>
  <summary>Details</summary>
Motivation: 디지털 커뮤니케이션에서 감정 이모티콘의 사용이 증가하고 있지만, 이들이 대형 언어 모델(LLM)의 안전성에 미치는 영향은 대부분 미탐구 상태에 있습니다.

Method: 이 현상을 체계적으로 연구하기 위해 자동화된 데이터 생성 파이프라인을 개발하고, 21개의 메타 시나리오, 4개의 프로그래밍 언어, 다양한 맥락 복잡성을 포함한 3,757개의 코드 지향 테스트 케이스로 구성된 데이터셋을 작성했습니다.

Result: 여섯 개의 LLM에 대한 연구 결과, 이모티콘 의미 혼동이 전반적으로 퍼져 있으며 평균 혼동 비율이 38%를 초과했습니다. 또한, 혼동된 응답의 90% 이상이 '침묵 실패'를 발생시키며, 이는 문법적으로 유효한 출력이지만 사용자 의도에서 벗어나 보안상 파괴적인 결과를 초래할 수 있습니다.

Conclusion: 우리는 커뮤니티가 이 새로운 취약성을 인식하고 LLM 시스템의 안전성과 신뢰성을 유지하기 위한 효과적인 완화 방법을 개발할 것을 촉구합니다.

Abstract: Emoticons are widely used in digital communication to convey affective intent, yet their safety implications for Large Language Models (LLMs) remain largely unexplored. In this paper, we identify emoticon semantic confusion, a vulnerability where LLMs misinterpret ASCII-based emoticons to perform unintended and even destructive actions. To systematically study this phenomenon, we develop an automated data generation pipeline and construct a dataset containing 3,757 code-oriented test cases spanning 21 meta-scenarios, four programming languages, and varying contextual complexities. Our study on six LLMs reveals that emoticon semantic confusion is pervasive, with an average confusion ratio exceeding 38%. More critically, over 90% of confused responses yield 'silent failures', which are syntactically valid outputs but deviate from user intent, potentially leading to destructive security consequences. Furthermore, we observe that this vulnerability readily transfers to popular agent frameworks, while existing prompt-based mitigations remain largely ineffective. We call on the community to recognize this emerging vulnerability and develop effective mitigation methods to uphold the safety and reliability of the LLM system.

</details>


### [4] [Baiting AI: Deceptive Adversary Against AI-Protected Industrial Infrastructures](https://arxiv.org/abs/2601.08481)
*Aryan Pasikhani,Prosanta Gope,Yang Yang,Shagufta Mehnaz,Biplab Sikdar*

Main category: cs.CR

TL;DR: 본 연구는 산업 제어 시스템을 대상으로 하는 새로운 사이버 공격 벡터를 탐구하며, 물 처리 시설에 특히 초점을 맞춥니다.


<details>
  <summary>Details</summary>
Motivation: 산업 제어 시스템 안전성을 위협하는 새로운 공격 벡터를 이해하고 대응하기 위함입니다.

Method: 다중 에이전트 심층 강화 학습(DRL) 접근 방식을 개발하여, 은밀하고 전략적으로 타이밍이 조정된 공격을 시도합니다.

Result: 이 공격 전략의 강력함을 밝혀내고, DRL 모델이 적대적 목적을 위해 조작될 수 있는 가능성을 보여줍니다.

Conclusion: 맞춤형 정책을 개발하고 구현함으로써, 공격자는 정상 운영 패턴에 원활하게 섞일 수 있는 적대적 행동을 보장합니다.

Abstract: This paper explores a new cyber-attack vector targeting Industrial Control Systems (ICS), particularly focusing on water treatment facilities. Developing a new multi-agent Deep Reinforcement Learning (DRL) approach, adversaries craft stealthy, strategically timed, wear-out attacks designed to subtly degrade product quality and reduce the lifespan of field actuators. This sophisticated method leverages DRL methodology not only to execute precise and detrimental impacts on targeted infrastructure but also to evade detection by contemporary AI-driven defence systems. By developing and implementing tailored policies, the attackers ensure their hostile actions blend seamlessly with normal operational patterns, circumventing integrated security measures. Our research reveals the robustness of this attack strategy, shedding light on the potential for DRL models to be manipulated for adversarial purposes. Our research has been validated through testing and analysis in an industry-level setup. For reproducibility and further study, all related materials, including datasets and documentation, are publicly accessible.

</details>


### [5] [MASH: Evading Black-Box AI-Generated Text Detectors via Style Humanization](https://arxiv.org/abs/2601.08564)
*Yongtong Gu,Songze Li,Xia Hu*

Main category: cs.CR

TL;DR: AI 생성 텍스트(AIGT)의 남용 증가로 인해 AIGT 탐지 방법이 빠르게 개발되고 있으나, 기존 탐지기는 적대적 회피에 대해 신뢰성이 떨어진다. 본 논문은 스타일 이전에 기반한 블랙박스 탐지기를 회피하는 새로운 프레임워크인 MASH를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 생성 텍스트(AIGT)의 남용이 증가함에 따라 AIGT 탐지 방법의 개발이 촉진되고 있다.

Method: MASH는 스타일 주입 감독 미세 조정, 직접 선호 최적화, 추론 시간 세분화를 단계적으로 적용하여 AI 생성 텍스트의 분포를 인간 작성 텍스트의 분포에 가깝게 조정하는 방식이다.

Result: MASH는 6개의 데이터셋과 5개의 탐지기에서 실험하여 11개의 기준 회피자보다 우수한 성능을 보였다.

Conclusion: MASH는 평균 92%의 공격 성공률(ASR)을 달성하며, 가장 강력한 기준보다 평균 24% 초과한 성능을 발휘하고, 언어적 품질 또한 뛰어나다.

Abstract: The increasing misuse of AI-generated texts (AIGT) has motivated the rapid development of AIGT detection methods. However, the reliability of these detectors remains fragile against adversarial evasions. Existing attack strategies often rely on white-box assumptions or demand prohibitively high computational and interaction costs, rendering them ineffective under practical black-box scenarios. In this paper, we propose Multi-stage Alignment for Style Humanization (MASH), a novel framework that evades black-box detectors based on style transfer. MASH sequentially employs style-injection supervised fine-tuning, direct preference optimization, and inference-time refinement to shape the distributions of AI-generated texts to resemble those of human-written texts. Experiments across 6 datasets and 5 detectors demonstrate the superior performance of MASH over 11 baseline evaders. Specifically, MASH achieves an average Attack Success Rate (ASR) of 92%, surpassing the strongest baselines by an average of 24%, while maintaining superior linguistic quality.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling](https://arxiv.org/abs/2601.07964)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 이 논문은 게임 개발에 있어 Executable Ontologies (EO)의 적용을 다루고 있다.


<details>
  <summary>Details</summary>
Motivation: 게임 AI 아키텍처에서의 의미적-프로세스 간의 격차를 해결하기 위한 방법으로 EO의 필요성을 강조한다.

Method: Winter Feast라는 생존 게임 시나리오를 사용하여 EO가 우선순위 기반의 작업 중단을 데이터 흐름 조건을 통해 달성함을 보여준다.

Result: EO가 행동 트리(BT) 및 목표 지향 행동 계획(GOAP)과 비교했을 때, 언제 동작이 가능한지를 모델링하며 의미적 처리를 개선함을 나타낸다.

Conclusion: 이 연구는 EO의 통합 전략과 시간적 이벤트 그래프의 디버깅 장점, LLM 주도 런타임 모델 생성의 잠재력을 논의한다.

Abstract: This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions rather than explicit preemption logic. Comparison with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP) reveals that while these approaches model what agents should do, EO models when actions become possible - a fundamental difference that addresses the semantic-process gap in game AI architecture. We discuss integration strategies, debugging advantages inherent to temporal event graphs, and the potential for LLM-driven runtime model generation.

</details>


### [7] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: 모델이 자신이 모르는 것을 인식할 때 여러 가능성이 열린다. 이 논문은 시각 및 언어 모델에 적용 가능한 간단하고 효과적인 방법을 제안하고, 모델이 자신이 모르는 것을 인식할 수 있는 능력을 극대화하는 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 모델이 불확실성을 인식하도록 하는 것은 AI 시스템의 효율성, 신뢰성 및 신뢰성을 높일 수 있는 방법이다.

Method: 모델 보정, 계단식 처리 및 데이터 정리를 수행하는 훈련이 필요 없는 방법을 제안한다.

Result: 모델의 계량된 신뢰성을 활용하여 모델의 효율성을 높이며, 이미지넷 및 다양한 다중 작업 언어 이해 데이터 세트에서 잘못 태깅된 샘플을 식별하는 데이터 정리 방법을 개발하였다.

Conclusion: 모델이 자신이 모르는 것을 인식하는 것은 더 효율적이고 신뢰할 수 있는 AI로 나아가는 실용적인 단계이다.

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [8] [WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents](https://arxiv.org/abs/2601.08406)
*Xinyi Wu,Jiagui Chen,Geng Hong,Jiayi Dong,Xudong Pan,Jiarun Dai,Min Yang*

Main category: cs.AI

TL;DR: WebTrap Park는 웹 에이전트의 체계적인 보안 평가를 위해 개발된 자동화 플랫폼으로, 1,226개의 실행 가능한 평가 작업을 생성하여 에이전트 수정 없이 행동 기반 평가를 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 웹 에이전트의 보안 평가는 분산되어 있고 표준화하기 어렵기 때문에 체계적인 평가가 필요합니다.

Method: WebTrap Park는 라이브 웹 페이지와의 구체적인 상호작용을 직접 관찰하여 보안 평가를 자동화합니다.

Result: 우리의 결과는 에이전트 프레임워크 간의 명확한 보안 차이를 보여주며, 기본 모델을 넘어서 에이전트 아키텍처의 중요성을 강조합니다.

Conclusion: WebTrap Park는 재현 가능한 웹 에이전트 보안 평가를 위한 확장 가능한 기초를 제공합니다.

Abstract: Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.

</details>


### [9] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: MemoBrain은 도구 보조 에이전트를 위한 메모리 모델로, 논리적 연속성과 작업 정렬을 유지하기 위해 유의미한 중간 상태를 포착하고 이를 관리하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 도구 보조 에이전트 프레임워크에서의 복잡한 추론은 긴 시간 범위에 걸쳐 발생하므로, 메모리 메커니즘이 없으면 논리적 연속성이 방해받고 작업 정렬이 저하된다.

Method: MemoBrain은 추론 단계에 대한 의존성 인식 메모리를 구성하여 추론 에이전트와 함께 작동하며, 논리적 관계를 가진 중간 상태를 캡처하고, 실행을 차단하지 않으면서 추론 진행을 정리한다.

Result: MemoBrain은 GAIA, WebWalker 및 BrowseComp-Plus와 같은 도전적인 긴 시간 범위의 벤치마크에서 강력한 기준선에 비해 지속적인 개선을 보여준다.

Conclusion: 메모리는 긴 시간 동안 일관되고 목표 지향적인 추론을 유지하기 위한 핵심 요소로 자리매김한다.

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [10] [MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness](https://arxiv.org/abs/2601.08118)
*Ashutosh Hathidara,Julien Yu,Vaishali Senthil,Sebastian Schreiber,Anil Babu Ankisettipalli*

Main category: cs.AI

TL;DR: MIRRORBENCH는 다양한 대화 작업에서 인간과 유사한 발화를 생성하는 사용자 프록시의 능력을 평가하는 벤치마크 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대화 시스템 평가 및 미세 조정 데이터 생성을 위해 대규모 언어 모델(LLM)을 효과적으로 사용하기 위한 필요성이 증가하고 있다.

Method: MIRRORBENCH는 사용자 프록시가 인간과 같은 발화를 생성하는 능력만을 평가하는 모듈식 실행 엔진과 메타데이터 기반 레지스트리를 갖춘 벤치마크 프레임워크이다.

Result: MIRRORBENCH는 네 개의 개방형 데이터세트에서 사용자 프록시와 실제 인간 사용자 간의 체계적인 차이를 드러내며 분산 인식 결과를 제공한다.

Conclusion: 이 프레임워크는 오픈 소스이며 실험 실행, 구성 관리, 캐싱, 보고서 생성을 위한 간단한 명령줄 인터페이스를 포함한다.

Abstract: Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive "act-as-a-user" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.

</details>


### [11] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Project Synapse는 마지막 마일 배송 중단의 자율적인 해상을 위해 설계된 새로운 에이전틱 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 배송 중단 문제를 효과적으로 해결하기 위해.

Method: 계층적 다중 에이전트 아키텍처를 사용하고 LangGraph를 통해 복잡한 워크플로를 관리한다.

Result: 30개의 복잡한 중단 시나리오를 포함하는 벤치마크 데이터세트를 통해 프레임워크를 검증함.

Conclusion: 정확한 편향 완화를 구현한 LLM-as-a-Judge 프로토콜을 사용하여 시스템 성능을 평가하였다.

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [12] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 본 논문은 멀티코어 플랫폼에서 열 및 에너지 인식 스케줄링을 위한 모델 기반의 계층적 다중 에이전트 강화 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: DVFS 및 작업-코어 할당은 임베디드 시스템에서 열 관리를 위해 중요하다.

Method: 모델 기반의 계층적 다중 에이전트 강화 학습(MARL) 프레임워크.

Result: 실험을 통해 에너지 효율성이 7.09배 향상되고, 만드는 시간(makespan)이 4.0배 개선되었다.

Conclusion: 프레임워크는 동적 임베디드 시스템에서 실제 적용을 가능하게 한다.

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [13] [The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios](https://arxiv.org/abs/2601.08173)
*Daocheng Fu,Jianbiao Mei,Rong Wu,Xuemeng Yang,Jia Xu,Ding Wang,Pinlong Cai,Yong Liu,Licheng Wen,Botian Shi*

Main category: cs.AI

TL;DR: MLLM의 발전이 워크플로 자동화를 촉진했으나, 동적 환경에서의 강건성이 간과되고 있다. 우리는 동적 작업 스케줄링, 불확실성 하에서의 능동 탐색, 경험으로부터의 지속적 학습을 주요 도전 과제로 식별했다. 
method를 통해 새로운 설정을 지속적으로 탐색하는 "훈련생" 에이전트를 시뮬레이션하는 동적 평가 환경을 도입한다. 우리는 에이전트를 세 가지 차원에서 평가한다. 
실험 결과, 최신 에이전트가 동적 환경, 특히 능동 탐색 및 지속적 학습에서 상당한 결함을 보임을 보여준다. 우리의 작업은 에이전트 신뢰성을 평가하기 위한 프레임워크를 확립하고, 평가를 정적 테스트에서 현실적인 생산 지향 시나리오로 전환한다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구는 주로 정적인 환경에서의 성능 한계를 목표로 하여, 실제 세계에서의 강건성을 간과하고 있다.

Method: 동적 평가 환경인 \\method{}를 도입해, 새로운 환경을 지속적으로 탐색하는 '훈련생' 에이전트를 시뮬레이션한다. \\method{}는 세 가지 차원에서 에이전트를 평가한다.

Result: 최신 에이전트들이 동적 환경에서 특히 능동 탐색 및 지속적 학습에 있어 상당한 결함을 보임을 실험을 통해 보여준다.

Conclusion: 우리의 작업은 정적 테스트에서 현실적이고 생산 지향적인 시나리오로 평가를 전환하며, 에이전트 신뢰성을 평가하기 위한 프레임워크를 확립한다.

Abstract: The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv

</details>


### [14] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: 이 논문은 SANC(E3)이라는 축적의 틀을 제안하여, 표현 장치가 고정된 것이 아니라 유한 자원 내에서 경쟁 선택, 재구성 및 압축의 결과로 생성됨을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 결정론적 리소스 하에서 예측 및 행동을 가능하게 하는 내부 구조로 경험을 재구성해야 하는 일반 지능의 필요성.

Method: SANC(E3) 프레임워크는 표현 장치가 고정된 것이 아니라 에너지 함수 E3의 명시적 최소화를 통해 발생하는 안정적인 결과임을 보여준다.

Result: 시스템 토큰과 자가 조직화 과정에서 발생하는 토큰 간의 원칙적인 구별; 결과적으로 인식, 상상, 예측, 계획 및 행동이 단일 표현적 및 에너지적 과정으로 통합됨.

Conclusion: 카테고리 형성, 계층적 조직, 비지도 학습 및 고급 인지 활동들은 모두 E3 최소화 하에서 게슈탈트 완성 사례로 이해될 수 있다.

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [15] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: MPCI-Bench는 에이전트 환경에서 개인 정보 보호 행동을 평가하기 위한 첫 번째 다중 모달 페어 와이즈 맥락적 무결성 벤치마크입니다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델 에이전트가 개인 데이터를 처리하는 능동적인 도우미로 발전함에 따라 사회적 규범에 대한 준수 평가가 중요해지고 있습니다.

Method: MPCI-Bench는 동일한 시각적 출처에서 파생된 긍정 및 부정 인스턴스로 구성되며, 규범적인 Seed 판단, 맥락이 풍부한 이야기 추론, 실행 가능한 에이전트 행동 추적의 세 가지 계층을 통해 구현됩니다.

Result: 최신 다중 모달 모델의 평가 결과, 개인 정보 보호와 유용성을 균형있게 유지하는 데 체계적인 실패가 드러났으며, 민감한 시각적 정보가 텍스트 정보보다 더 자주 유출되는 현상이 관찰되었습니다.

Conclusion: MPCI-Bench는 에이전트 CI에 대한 향후 연구를 촉진하기 위해 오픈 소스화될 것입니다.

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [16] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 보상 공학은 원하는 에이전트 행동을 유도하기 위해 보상 함수를 수동으로 정의하는 것이며, 다중 에이전트 강화 학습에서 여전히 중요한 도전 과제로 남아 있다. 최근의 대형 언어 모델(LLM)의 발전은 수작업으로 제작된 수치적 보상에서 언어 기반 목표 사양으로의 전환을 시사한다. LLM은 자연어 설명에서 직접 보상 함수를 합성하고 최소한의 인간 개입으로 온라인에서 보상 형식을 조정할 수 있다. 최근의 검증 가능한 보상으로부터의 강화 학습(RLVR) 패러다임은 언어 매개 감독이 전통적인 보상 공학의 실현 가능한 대안이 될 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 보상 공학은 다중 에이전트 강화 학습에서 에이전트 행동을 유도하기 위한 보상 함수의 수동 정의라는 Fundamental Challenge이다.

Method: 대형 언어 모델을 활용해 자연어 설명에서 보상 함수를 합성하고, 언어 매개 감독을 통한 새로운 보상 공학 방법론을 제안함.

Result: 언어 매개 감독이 전통적인 수치적 보상 공학의 대안이 될 수 있음을 보여주었으며, 세 가지 차원에서 이 전환을 개념화하였다: 의미론적 보상 사양, 동적 보상 조정, 인간 의도와의 개선된 정렬.

Conclusion: 협력이 명시적으로 설계된 수치 신호가 아니라 공유된 의미 표현에서 발생하도록 하는 연구 방향을 제시하였다.

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [17] [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)
*Abdikarim Mohamed Ibrahim,Rosdiadee Nordin*

Main category: cs.AI

TL;DR: 이 논문에서는 대규모 언어 모델(LLM)에 의해 가이드되는 심층 강화 학습(DRL) 에이전트를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 비지구 네트워크(NTN) 애플리케이션에 향상된 성능을 제공하기 위한 대규모 AI 모델(LAM)의 활용.

Method: 대규모 언어 모델(LLM)을 고수준 조정자로 사용하여 DRL 에이전트의 보상을 형성하는 텍스트 지침을 생성.

Result: LAM-DRL은 전통적인 DRL에 비해 정상 기상 시나리오에서 40%, 극한 기상 시나리오에서 64% 향상된 성능을 보임.

Conclusion: LAM-DRL은 처리량, 공정성 및 중단 확률 측면에서 기존의 휴리스틱과 비교하여 우수한 성능을 입증한다.

Abstract: Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.

</details>


### [18] [Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces](https://arxiv.org/abs/2601.08271)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 이 논문은 도구 증강 LLM 시스템의 제어 체계를 정식화하고, 정책 학습 및 도구 지원 회복에 대한 결과를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존 학습 이론이 간과한 대규모 이산 행동 우주에서의 순차적 의사결정 문제를 다루기 위하여.

Method: Sparse Agentic Control(SAC)로 설정을 정식화하고, ell_{1,2}-정규화 정책 학습을 통해 여러 가지 결과를 도출한다.

Result: 정확한 도구 지원 회복이 가능하며, 밀집 정책 클래스는 Omega(M) 샘플이 필요함을 보인다.

Conclusion: 이 논문은 LLM이 믿음/표상 오류를 통해 작용하며, 다양한 확장성도 논의한다.

Abstract: Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.

</details>


### [19] [ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web](https://arxiv.org/abs/2601.08276)
*Zhiyuan Yao,Zishan Xu,Yifu Guo,Zhiguang Han,Cheng Yang,Shuo Zhang,Weinan Zhang,Xingshan Zeng,Weiwen Liu*

Main category: cs.AI

TL;DR: ToolACE-MCP는 대규모 에코시스템에서의 정밀 탐색을 위한 역사 인식 라우터를 훈련시키기 위한 파이프라인을 제안하며, 탁월한 성능을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 웹과 모델 컨텍스트 프로토콜의 발전으로 에이전트 생태계는 열린 협업 네트워크로 진화하고 있으며, 이에 따라 접근 가능한 도구가 exponentially 증가하고 있습니다. 하지만 현재의 아키텍처는 심각한 확장성과 일반성 병목현상에 직면해 있습니다.

Method: ToolACE-MCP는 의존성이 풍부한 후보 그래프를 활용하여 다중 턴 궤적을 합성하고, 이를 통해 동적인 컨텍스트 이해 능력을 갖춘 라우터를 효과적으로 훈련합니다.

Result: MCP-Universe와 MCP-Mark라는 실제 벤치마크에서 우수한 성능을 보여줍니다.

Conclusion: ToolACE-MCP는 최소한의 조정으로 다중 에이전트 협업에 일반화될 수 있고, 잡음에 대한 뛰어난 강인성을 유지하며, 방대한 후보 공간으로 효과적으로 확장되는 중요한 특성을 나타냅니다. 이러한 발견은 개방형 생태계에서의 유니버설 오케스트레이션을 위한 강력한 실증 기반을 제공합니다.

Abstract: With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.

</details>


### [20] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 본 논문은 큰 행동 공간에서 작동하는 현대 에이전트 시스템에서 유효한 행동 집합을 식별하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 행동의 소수만이 성능에 의미 있는 영향을 미친다는 관찰에 의해 동기를 부여받았습니다.

Method: 행동 발견을 블록-스파스 복구 문제로 공식화하고, Orthogonal Matching Pursuit에서 영감을 받은 탐욕적 알고리즘을 분석합니다.

Result: 조건부 확률이 높을 경우, 탐욕적 절차가 관련 행동 집합을 정확히 회복함을 증명했습니다.

Conclusion: 희소성과 충분한 커버리지가 중요하다는 정보 이론적 하한을 설정하며, 이는 대규모 행동 의사결정의 기본 원리를 식별합니다.

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [21] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu,Hanzhong Cao,Jianhao Chen,Yufei Li*

Main category: cs.AI

TL;DR: 중국식 스탠드업 코미디 생성을 위한 다중 에이전트 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 중국 스탠드업 코미디 생성을 위한 문화적 유머와 정밀한 타이밍, 무대 수행 신호를 포함한 복잡한 요구사항 해결을 목표로 한다.

Method: OpenMic는 사용자 제공 주제를 바탕으로 3-5분 분량의 중국식 스탠드업 공연을 생성하고, 이를 내레이터가 설명하는 동영상으로 변환하는 다중 에이전트 시스템이다.

Result: 여러 전문 에이전트를 활용한 반복적인 계획 수립을 통해 유머, 타이밍, 수행 가능성을 최적화하였다.

Conclusion: RAG를 통해 데이터 세트와 작업의 불일치를 완화하고, 스탠드업 전용 구조를 내부화하기 위해 JokeWriter를 세부 조정하였다.

Abstract: Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [22] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo,Yaxi Lu,Zhong Zhang,Haotian Chen,Yankai Lin*

Main category: cs.AI

TL;DR: 에이전트 메모리의 동적 결정 문제로 재구성한 AtomMem을 제안하고, 이를 통해 특정 작업 요구에 맞춘 메모리 행동 조율을 학습함으로써 기존의 정적 워크플로우 메모리 방법보다 우수한 성능을 보임.


<details>
  <summary>Details</summary>
Motivation: 실제 세계의 장기 문제를 해결하기 위해 에이전트에 메모리를 장착하는 것이 필수적이지만, 기존 메모리 메커니즘은 정적이고 수작업으로 설계된 워크플로우에 의존하고 있어 성능과 일반화 능력이 제한됨.

Method: AtomMem은 메모리 관리를 동적인 의사 결정 문제로 재구성하고, 고수준 메모리 프로세스를 기본적인 CRUD 작업으로 분해하여 학습 가능한 의사 결정 프로세스로 변환함. 감독식 미세 조정과 강화 학습을 결합하여, 특정 작업 요구에 맞춘 메모리 행동을 조율하는 자율적이고 작업 정렬된 정책을 학습함.

Result: 3개의 장기 맥락 벤치마크를 통한 실험 결과, 훈련된 AtomMem-8B가 기존의 정적 워크플로우 메모리 방법보다 일관되게 우수한 성능을 보임.

Conclusion: 훈련 역학의 추가 분석을 통해, 우리의 학습 기반 공식이 에이전트가 구조화되고 작업 정렬된 메모리 관리 전략을 발견할 수 있게 해준다는 점에서 미리 정의된 루틴에 비해 주요 이점을 강조함.

Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [23] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 본 논문에서는 LLM 기반 에이전트 아키텍처의 구조적 결함인 의미 세탁(semanitic laundering) 현상을 탐구하며, 이는 Gettier 문제의 아키텍처적 실현으로 제시된다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 시스템에서 정보 전달 메커니즘과 인식적 정당화 메커니즘의 혼합 문제를 해결하고자 하는 동기에서 출발한다.

Method: 구조적 맥락에서 의미 세탁을 제정의하고, Gettier 문제와의 연결을 분석한다.

Result: 우리는 LLM 기반 시스템에서 원형적인 인식적 정당화 문제를 설명하고, 제시된 이론들이 이 문제의 본질적 원인임을 보여준다.

Conclusion: 우리는 Warrant Erosion Principle을 도입하여 이 문제의 근본적인 설명을 제시하고, 기존 개선 방법이 왜 이 문제를 해결할 수 없는지를 논의한다.

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [24] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: 본 논문은 UAV 다중 SDK 제어 작업을 위한 지식 증류, 사고의 사슬 안내, 감독된 미세 조정을 통합한 접근 방식을 제안하며, 복잡한 추론 및 코드 생성 능력을 소형 모델에 효율적으로 전이하는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 코드 생성 작업에서 비약적인 잠재력을 보여주지만, UAV 플랫폼의 실시간 및 경량 요구 사항과 높은 자원 소비 간의 모순이 있다.

Method: 다양한 주류 UAV SDK를 다루는 고품질 데이터셋을 구축하고, QLoRA를 통해 양자화된 DeepSeek-Coder-V2-Lite를 교사 모델로 활용하여, 복잡한 추론 능력을 소형 학생 모델로 전이하기 위한 하이브리드 블랙박스 및 화이트박스 증류 전략을 적용한다.

Result: 실험 결과, 증류된 경량 모델이 코드 생성 정확도를 유지하며 배포 및 추론 효율성에서 상당한 개선을 이루었다.

Conclusion: 본 접근 방식은 UAV의 정밀하고 경량화된 지능형 제어를 달성하는 데 있어 실행 가능성과 우수성을 효과적으로 입증한다.

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [25] [M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games](https://arxiv.org/abs/2601.08462)
*Sixiong Xie,Zhuofan Shi,Haiyang Shen,Gang Huang,Yun Ma,Xiang Jing*

Main category: cs.AI

TL;DR: 대형 언어 모델 에이전트의 사회적 행동을 체계적으로 평가하기 위해 M3-Bench라는 다단계 벤치마크와 프로세스 인식 평가 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 사회적 행동이 발전하고 있지만, 기존 벤치마크는 단일 능력 차원이나 행동 결과에만 의존하여 결정 과정과 소통 상호작용의 풍부한 정보를 간과하고 있다.

Method: M3-Bench는 혼합 동기 게임을 위한 다단계 벤치마크로, BTA(행동 궤적 분석), RPA(추론 과정 분석), CCA(소통 내용 분석)라는 세 가지 모듈을 통해 시너지 분석을 수행하는 프로세스 인식 평가 프레임워크를 포함한다.

Result: M3-Bench는 다양한 모델 간의 사회적 행동 역량을 신뢰성 있게 구분할 수 있으며, 일부 모델은 합리적인 행동 결과를 보이지만, 그 추론과 소통에서 뚜렷한 일관성 결여를 드러낸다.

Conclusion: M3-Bench는 단순한 작업 점수나 결과 기반 메트릭을 넘어 에이전트의 성격 특성 및 능력 프로필을 해석 가능한 형태로 집계한다.

Abstract: As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.

</details>


### [26] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: 이 논문은 meme 코인 복사 거래를 위한 설명 가능한 다중 에이전트 시스템을 제안하며, 기존 모델들보다 우수한 성과를 보임을 보여준다.


<details>
  <summary>Details</summary>
Motivation: Meme 코인 시장에서 깊은 거래 지식 없이도 인기 있는 복사 거래 전략이 증가하고 있지만, 수익성 보장은 여러 요인으로 인해 어렵다.

Method: 자산 관리 팀의 구조에서 영감을 받아 복잡한 작업을 하위 작업으로 분해하고 각기 다른 전문 에이전트들이 협력하여 문제를 해결하는 시스템을 제안한다.

Result: 제안된 다중 에이전트 시스템은 1,000개의 meme 코인 프로젝트의 거래 데이터셋을 사용하여 기존의 기계 학습 모델과 단일 LLM보다 뛰어난 성능을 보였다.

Conclusion: 선택된 KOL들이 관련 프로젝트에서 총 50만 달러의 수익을 창출한 것으로 나타났다.

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [27] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette,Sandro Claudio Lera,Ke Wu*

Main category: cs.AI

TL;DR: 최근 대형 언어 모델(LLM)에서 나타나는 기만, 협박 또는 강탈과 같은 행동은 정렬 실패 또는 비정상적인 악의의 증거로 해석되는 경향이 있다. 우리는 이러한 해석이 개념적 오류에 기초하고 있다고 주장한다. LLM은 도덕을 추론하지 않으며, 인간의 사회적 상호작용의 기록을 통계적으로 내부화한다. 비윤리적 또는 비정상적이라고 흔히 언급되는 행동은 구조적 일반화로 이해해야 한다. 블랙메일과 같은 행위는 정상적인 사회적 행동과의 범주적 일탈이 아니라, 시장 가격, 권위 관계 및 최후통첩 협상 등을 포함하는 동일한 연속체 내의 제한된 경우이다. 따라서 AGI의 위험은 적대적 의도가 아니라 인간 지능과 권력, 모순을 자극하는 내재적 증폭기의 역할이다.


<details>
  <summary>Details</summary>
Motivation: LLM의 부정적 행동이 정렬 실패의 증거로 해석되는 것에 대한 비판.

Method: 관계 모델 이론을 적용하여 LLM의 행동을 기존의 사회적 행동과 비교.

Result: 비윤리적 행동은 구조적 일반화의 결과이며, 블랙메일과 같은 행동은 정상적인 행동의 연속적인 스펙트럼 내에서의 일종의 경우로 이해된다.

Conclusion: AGI의 주된 위험은 적대적 의도가 아니라, 인간의 지능을 증폭하는 것이라는 점을 강조.

Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [28] [Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance](https://arxiv.org/abs/2601.08676)
*Yilei Zhao,Wentao Zhang,Xiao Lei,Yandan Zheng,Mengpu Liu,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: ESGAgent는 데이터 분산 문제를 해결하고 ESG 분석을 개선하기 위한 다계층 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 기업의 지속 가능성과 윤리적 성과를 평가하기 위해 ESG 기준이 필수적이나, 데이터의 단편화와 기존 LLM의 복잡한 작업 수행의 어려움이 문제다.

Method: 전문 도구 세트인 검색 증대, 웹 검색 및 도메인 특정 기능을 활용하는 ESGAgent라는 다계층 다중 에이전트 시스템을 제안한다.

Result: ESGAgent는 원자 질문-응답 작업에서 84.15%의 평균 정확도로 최첨단 닫힌 소스 LLM들을 초월하며, 풍부한 도표와 검증 가능한 참조를 통합하여 전문 보고서 생성을 뛰어넘는다.

Conclusion: 이 연구는 우리의 벤치마크의 진단 가치를 확인하고, 고위험 수직 도메인에서 일반적이고 고급 에이전트 기능을 평가하기 위한 필수 시험대 역할을 한다.

Abstract: Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.

</details>


### [29] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: 이 연구는 사용자 맞춤형 정보를 제공하면서도 객관성을 잃지 않는 모델인 PersonaDual을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자들이 LLM(대규모 언어 모델)이 개인의 선호에 맞춰 조정되기를 기대함에 따라 개인화된 정보의 가치가 증가하고 있습니다.

Method: PersonaDual은 단일 모델에서 일반적인 목적의 객관적 추론과 개인화된 추론을 지원하며, 컨텍스트에 기반하여 모드를 적응적으로 전환합니다. 초기에는 SFT(단순화된 최적화 기법)를 사용하여 두 가지 추론 패턴을 학습한 후, 제안된 DualGRPO를 통해 모드 선택을 개선하기 위해 강화 학습으로 최적화됩니다.

Result: 객관적 및 개인화된 기준에서의 실험 결과, PersonaDual은 개인화의 이점을 유지하면서 간섭을 줄이고, 방해 요소 없는 성능을 달성하였습니다.

Conclusion: 이 연구는 ayudar를 활용하여 객관적 문제 해결을 개선할 수 있는 유용한 개인화된 신호를 효과적으로 활용할 수 있음을 보여줍니다.

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [30] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: Cago는 전문가 경로에 대한 의존성을 줄여 장기 환경에서의 모방 학습 성능을 향상시키는 새로운 방법이다.


<details>
  <summary>Details</summary>
Motivation: 장기 환경에서의 모방 학습이 실제로 불가능한 완벽한 시연 복제에 어려움을 겪고 있어 이 문제를 해결하고자 한다.

Method: Cago는 전문가 경로를 따라 에이전트의 능력을 동적으로 추적하고, 이를 통해 에이전트의 현재 도달 범위를 넘어선 중간 목표를 선택하여 학습을 안내하는 방법이다.

Result: Cago는 다양한 희소 보상 목표 조정 작업에서 샘플 효율성과 최종 성능을 크게 향상시키는 결과를 보여준다.

Conclusion: 기존 모방 학습 기법들보다 일관되게 뛰어난 성능을 발휘하며, 안정적인 진행을 가능하게 하는 적응형 커리큘럼을 제공한다.

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [31] [Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards](https://arxiv.org/abs/2601.08778)
*Tengjun Jin,Yoojin Choi,Yuxuan Zhu,Daniel Kang*

Main category: cs.AI

TL;DR: 이 논문은 두 개의 널리 사용되는 텍스트-투-SQL 벤치마크인 BIRD와 Spider 2.0-Snow의 주석 오류 비율을 평가하고, BIRD 개발 세트의 일부를 수정하여 텍스트-투-SQL 에이전트 성능과 리더보드 순위에 대한 주석 오류의 영향을 측정한다.


<details>
  <summary>Details</summary>
Motivation: 텍스트-투-SQL 기술을 비교하고 최상의 방법을 선택하기 위해, 연구 커뮤니티는 공개 벤치마크와 리더보드를 의존한다. 주석의 유효성은 매우 중요하다.

Method: BIRD와 Spider 2.0-Snow에 대해 주석 오류 비율을 측정하고, BIRD 개발 세트의 일부를 수정하여 주석 오류가 성능에 미치는 영향을 평가한다.

Result: BIRD Mini-Dev와 Spider 2.0-Snow의 오류 비율은 각각 52.8%와 62.8%로 나타났으며, 성능 변화는 -7%에서 31%까지, 순위 변화는 -9에서 +9 위치까지 범위가 있었다.

Conclusion: 주석 오류가 보고된 성능과 순위를 크게 왜곡할 수 있음을 보여주며, 이는 연구 방향이나 배포 선택에 잘못된 영향을 미칠 수 있다.

Abstract: Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.
  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [Emergent Coordination in Multi-Agent Systems via Pressure Fields and Temporal Decay](https://arxiv.org/abs/2601.08129)
*Roland Rodriguez*

Main category: cs.MA

TL;DR: 본 논문은 압력 기울기에 기반한 새로운 다중 에이전트 조정 패러다임을 제안하며, 이는 기존의 위계적 제어 방식보다 더 나은 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 LLM 프레임워크는 인적 조직 구조에서 차용한 명시적 조정 패턴에 의존하고 있으며, 이는 에이전트 수와 작업 복잡성이 증가할수록 조정의 오버헤드에 시달린다.

Method: 에이전트가 측정 가능한 품질 신호에서 파생된 압력 기울기에 의해 안내받으며 공유 아티팩트에서 로컬로 작업하도록 하는 새로운 패러다임을 제안한다.

Result: 압력-필드 조정이 위계적 제어와 동등한 성능을 발휘하고, 기존의 순차적, 랜덤, 대화 기반 다중 에이전트 대화보다 월등한 성과를 올렸음을 실증적으로 입증했다.

Conclusion: 제약 기반의 Emergence가 다중 에이전트 AI의 간단하고도 효과적인 기초를 제공함을 시사한다.

Abstract: Current multi-agent LLM frameworks rely on explicit orchestration patterns borrowed from human organizational structures: planners delegate to executors, managers coordinate workers, and hierarchical control flow governs agent interactions. These approaches suffer from coordination overhead that scales poorly with agent count and task complexity. We propose a fundamentally different paradigm inspired by natural coordination mechanisms: agents operate locally on a shared artifact, guided only by pressure gradients derived from measurable quality signals, with temporal decay preventing premature convergence. We formalize this as optimization over a pressure landscape and prove convergence guarantees under mild conditions.
  Empirically, on Latin Square constraint satisfaction across 1,078 trials, pressure-field coordination matches hierarchical control (38.2% vs 38.8% aggregate solve rate, p=0.94, indicating statistical equivalence). Both significantly outperform sequential (23.3%), random (11.7%), and conversation-based multi-agent dialogue (8.6%, p<0.00001). Temporal decay is essential: disabling it increases final pressure 49-fold (d=4.15). On easy problems, pressure-field achieves 87% solve rate. The approach maintains consistent performance from 2 to 32 agents. Our key finding: implicit coordination through shared pressure gradients achieves parity with explicit hierarchical control while dramatically outperforming explicit dialogue-based coordination. This suggests that constraint-driven emergence offers a simpler, equally effective foundation for multi-agent AI.

</details>


### [33] [When KV Cache Reuse Fails in Multi-Agent Systems: Cross-Candidate Interaction is Crucial for LLM Judges](https://arxiv.org/abs/2601.08343)
*Sichu Liang,Zhenglin Wang,Jiajia Chu,Pengfei Xia,Hui Zang,Deyu Zhou*

Main category: cs.MA

TL;DR: 이 연구는 여러 후보 응답을 생성하는 다중 에이전트 LLM 시스템에서 KV 캐시 재사용이 평가자 중심 추론에 미치는 영향을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 시스템에서의 주요한 리소스 비용인 사전 채우기 비용을 줄이기 위해 KV 캐시 재사용을 제안합니다.

Method: GSM8K, MMLU, HumanEval을 사용하여 재사용 전략이 평가자 행동에 미치는 영향을 분석합니다.

Result: 재사용 전략이 평가자가 일관성을 잃게 만들 수 있음을 발견하였고, 평가자의 선택이 밀도 높은 사전 채우기에서 매우 불안정해질 수 있음을 밝혔습니다.

Conclusion: KV 캐시 재사용의 실패 모드를 발견하고, 평가자 중심 추론을 위해 위험 인식을 고려한 시스템 설계가 필요함을 강조합니다.

Abstract: Multi-agent LLM systems routinely generate multiple candidate responses that are aggregated by an LLM judge. To reduce the dominant prefill cost in such pipelines, recent work advocates KV cache reuse across partially shared contexts and reports substantial speedups for generation agents. In this work, we show that these efficiency gains do not transfer uniformly to judge-centric inference. Across GSM8K, MMLU, and HumanEval, we find that reuse strategies that are effective for execution agents can severely perturb judge behavior: end-task accuracy may appear stable, yet the judge's selection becomes highly inconsistent with dense prefill. We quantify this risk using Judge Consistency Rate (JCR) and provide diagnostics showing that reuse systematically weakens cross-candidate attention, especially for later candidate blocks. Our ablation further demonstrates that explicit cross-candidate interaction is crucial for preserving dense-prefill decisions. Overall, our results identify a previously overlooked failure mode of KV cache reuse and highlight judge-centric inference as a distinct regime that demands dedicated, risk-aware system design.

</details>


### [34] [Agent Contracts: A Formal Framework for Resource-Bounded Autonomous AI Systems](https://arxiv.org/abs/2601.08815)
*Qing Ye,Jing Tan*

Main category: cs.MA

TL;DR: 이 논문은 Agent Contracts라는 새로운 형식의 계약 프레임워크를 도입하여 자원 관리 및 다중 에이전트 시스템의 협업을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 계약을 통한 조정은 필수적이지만, 자원 소비와 운영 시간에 대한 형식적 규제가 부족하다.

Method: Agent Contracts 프레임워크는 입력/출력 사양, 다차원 자원 제약, 시간적 경계 및 성공 기준을 통합하여 자원을 제한하는 실행을 지원한다.

Result: 4가지 실험을 통해 90%의 토큰 감소 및 525배 낮은 변동성을 확인했으며, 다중 에이전트 위임에서 보존 위반이 없음을 입증했다.

Conclusion: Agent Contracts는 자원에 제한된 자율 AI 배포를 위한 예측 가능하고 감사 가능한 형식적 기초를 제공한다.

Abstract: The Contract Net Protocol (1980) introduced coordination through contracts in multi-agent systems. Modern agent protocols standardize connectivity and interoperability; yet, none provide formal, resource governance-normative mechanisms to bound how much agents may consume or how long they may operate. We introduce Agent Contracts, a formal framework that extends the contract metaphor from task allocation to resource-bounded execution. An Agent Contract unifies input/output specifications, multi-dimensional resource constraints, temporal boundaries, and success criteria into a coherent governance mechanism with explicit lifecycle semantics. For multi-agent coordination, we establish conservation laws ensuring delegated budgets respect parent constraints, enabling hierarchical coordination through contract delegation. Empirical validation across four experiments demonstrates 90% token reduction with 525x lower variance in iterative workflows, zero conservation violations in multi-agent delegation, and measurable quality-resource tradeoffs through contract modes. Agent Contracts provide formal foundations for predictable, auditable, and resource-bounded autonomous AI deployment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Beyond the Next Port: A Multi-Task Transformer for Forecasting Future Voyage Segment Durations](https://arxiv.org/abs/2601.08013)
*Nairui Liu,Fang He,Xindi Tang*

Main category: cs.LG

TL;DR: 본 연구는 해양 일정 신뢰성과 장기 항구 운영 최적화를 위한 세그먼트 수준 항해 시간 예측 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 세그먼트 수준의 정확한 항해 시간 예측은 해양 일정 신뢰성을 향상시키고 장기 항구 운영을 최적화하는 데 필수적이다.

Method: 우리는 역사적 항해 시간, 목적지 항구 혼잡도 프록시 및 정적 선박 설명자를 통합한 변환기 기반 아키텍처를 개발한다.

Result: 2021년의 실제 글로벌 데이터 세트 평가에서 제안된 모델은 경쟁 기준에 비해 평균 절대 오차(MAE)에서 4.85% 감소, 평균 절대 백분율 오차(MAPE)에서 4.95% 감소를 보여준다.

Conclusion: 주요 목적지 항구의 사례 연구는 모델의 우수한 정확성을 추가로 설명한다.

Abstract: Accurate forecasts of segment-level sailing durations are fundamental to enhancing maritime schedule reliability and optimizing long-term port operations. However, conventional estimated time of arrival (ETA) models are primarily designed for the immediate next port of call and rely heavily on real-time automatic identification system (AIS) data, which is inherently unavailable for future voyage segments. To address this gap, the study reformulates future-port ETA prediction as a segment-level time-series forecasting problem. We develop a transformer-based architecture that integrates historical sailing durations, destination port congestion proxies, and static vessel descriptors. The proposed framework employs a causally masked attention mechanism to capture long-range temporal dependencies and a multi-task learning head to jointly predict segment sailing durations and port congestion states, leveraging shared latent signals to mitigate high uncertainty. Evaluation on a real-world global dataset from 2021 demonstrates the proposed model consistently outperforms a comprehensive suite of competitive baselines. The result shows a relative reduction of 4.85% in mean absolute error (MAE) and 4.95% in mean absolute percentage error (MAPE) compared with sequence baseline models. The relative reductions with gradient boosting machines are 9.39% in MAE and 52.97% in MAPE. Case studies for the major destination port further illustrate the model's superior accuracy.

</details>


### [36] [Riemannian Zeroth-Order Gradient Estimation with Structure-Preserving Metrics for Geodesically Incomplete Manifolds](https://arxiv.org/abs/2601.08039)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 이 논문은 기하학적으로 불완전한 리만 계량에서 정적 점을 근사하는 방법을 연구합니다.


<details>
  <summary>Details</summary>
Motivation: 리만 계량이 기하학적으로 불완전한 환경에서의 정적 점 근사는 중요한 문제입니다.

Method: 구조 보존 계량을 구성하여 기하학적으로 완전한 계량을 만들고, 기존 계량 아래에서 정적 점을 유지합니다.

Result: 내부 기하학적 관점에서 고전적인 두 점 제로 차수 추정기의 평균 제곱 오차를 분석하고, 내재적 추정기를 통해 확률적 경량 하강법에 대한 수렴 보장을 설정합니다.

Conclusion: 구성된 계량 아래에서의 ε-정적 점은 원래 계량 아래의 ε-정적 점에 대응하며, 이론적 결과를 실험적으로 확인했습니다.

Abstract: In this paper, we study Riemannian zeroth-order optimization in settings where the underlying Riemannian metric $g$ is geodesically incomplete, and the goal is to approximate stationary points with respect to this incomplete metric. To address this challenge, we construct structure-preserving metrics that are geodesically complete while ensuring that every stationary point under the new metric remains stationary under the original one. Building on this foundation, we revisit the classical symmetric two-point zeroth-order estimator and analyze its mean-squared error from a purely intrinsic perspective, depending only on the manifold's geometry rather than any ambient embedding. Leveraging this intrinsic analysis, we establish convergence guarantees for stochastic gradient descent with this intrinsic estimator. Under additional suitable conditions, an $ε$-stationary point under the constructed metric $g'$ also corresponds to an $ε$-stationary point under the original metric $g$, thereby matching the best-known complexity in the geodesically complete setting. Empirical studies on synthetic problems confirm our theoretical findings, and experiments on a practical mesh optimization task demonstrate that our framework maintains stable convergence even in the absence of geodesic completeness.

</details>


### [37] [STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order](https://arxiv.org/abs/2601.08107)
*Chengyang Gu,Yuxin Pan,Hui Xiong,Yize Chen*

Main category: cs.LG

TL;DR: STO-RL은 LLM을 활용하여 시간 순서가 있는 하위 목표 시퀀스를 생성하고 이를 통해 드문 보상을 밀집한 신호로 변환하여 오프라인 RL의 한계를 극복하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 오프라인 RL은 사전 수집된 데이터셋에서 정책을 학습하게 하지만, 희소 보상을 가진 장기 태스크에서는 어려움을 겪는다.

Method: STO-RL은 LLM을 활용하여 하위 목표 시퀀스와 상태-하위 목표 단계 매핑을 생성하고, 이를 기반으로 잠재적 보상 형성을 적용한다.

Result: STO-RL은 기존 오프라인 목표 조건 및 계층적 RL 방법보다 일관되게 성능이 우수하며, 빠른 수렴, 높은 성공률, 짧은 경로를 달성한다.

Conclusion: LLM에 의해 안내된 하위 목표의 시간 구조와 이론적으로 기반한 보상 형성은 장기 오프라인 RL에 대한 실용적이고 확장 가능한 해결책을 제공한다.

Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

</details>


### [38] [Dynamic Graph Structure Learning via Resistance Curvature Flow](https://arxiv.org/abs/2601.08149)
*Chaoqun Fei,Huanjiang Liu,Tinglve Zhou,Yangyang Li,Tianyong Hao*

Main category: cs.LG

TL;DR: Resistance Curvature Flow (RCF)는 기하학적 진화 프레임워크로, 효율적인 행렬 연산을 통해 곱셈 최적화 비용을 크게 줄이며 100배 이상의 계산 속도 향상을 이룬다.


<details>
  <summary>Details</summary>
Motivation: 기존 전통적인 정적 그래프 구조는 데이터 다양체의 고유한 곡률 특성을 포착하는 데 실패하여 GRL의 발전에 한계를 반영한다.

Method: RCF는 회로 물리학에서의 효과적 저항 개념을 활용하여 비용이 많이 드는 곡률 최적화를 효율적인 행렬 연산으로 변환한다.

Result: RCF는 OCF와 비교하여 기하학적 최적화 능력을 유지하면서도 100배 이상의 계산 가속을 달성하였다.

Conclusion: 이 논문은 DGSL-RCF라는 그래프 최적화 알고리즘을 설계하였으며, 실험 결과는 DGSL-RCF가 표현 품질과 하류 작업 성능을 크게 향상시킨다는 것을 보여준다.

Abstract: Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.

</details>


### [39] [Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits](https://arxiv.org/abs/2601.08760)
*Yi Zhuang,Kun Yang,Xingran Chen*

Main category: cs.LG

TL;DR: 이 논문은 다수의 클라이언트, 액세스 노드 및 서버로 구성된 엣지 네트워크에서 시간에 민감한 클라이언트의 정보 신선도를 최적화하기 위한 분산 협력 요청 문제를 연구한다.


<details>
  <summary>Details</summary>
Motivation: 시간에 민감한 클라이언트의 정보 신선도를 최적화할 필요성.

Method: 클라이언트가 액세스 노드를 선택하는 과정에서 발생하는 정보의 연령 감소를 보상으로 정의하고, 이를 비정상적인 다중팔 단치 문제로 모델링한다. AGING BANDIT WITH ADAPTIVE RESET 알고리즘을 제안하여 적응형 윈도잉과 주기적 모니터링을 결합하여 보상 분포 변화를 추적한다.

Result: 제안된 알고리즘이 근사 최적 성능을 달성함을 이론적으로 보장하고, 시뮬레이션을 통해 결과를 검증하였다.

Conclusion: AGING BANDIT WITH ADAPTIVE RESET 알고리즘은 분산 환경에서 정보 신선도를 효과적으로 최적화할 수 있는 방법을 제공한다.

Abstract: We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.

</details>


### [40] [Scalable Multiagent Reinforcement Learning with Collective Influence Estimation](https://arxiv.org/abs/2601.08210)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu,Ke Pan*

Main category: cs.LG

TL;DR: 이 논문은 집단적 영향 추정 네트워크(CIEN)를 통해 다중 에이전트 강화 학습(MARL)의 협력적 작업 문제를 해결하는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습(MARL)은 복잡한 협력 작업을 해결할 수 있는 잠재력 때문에 많은 주목을 받고 있다.

Method: 집단적 영향 추정 네트워크(CIEN)를 통해 각 에이전트는 지역 관측과 작업 객체의 상태만으로 중요한 상호작용 정보를 추론할 수 있다.

Result: 제안된 방법은 통신이 제한된 환경에서도 안정적이고 효율적인 조정을 달성하며, 팀 규모가 증가해도 네트워크 확장을 피할 수 있다.

Conclusion: 정책이 실제 로봇 플랫폼에 배포되었으며, 실험 결과는 개선된 견고성과 배포 가능성을 입증한다.

Abstract: Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.
  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object's states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.

</details>


### [41] [A Preliminary Agentic Framework for Matrix Deflation](https://arxiv.org/abs/2601.08219)
*Paimon Goulart,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 소규모 에이전트 팀이 행렬을 하나의 순위-1 슬라이스씩 분해할 수 있는가?


<details>
  <summary>Details</summary>
Motivation: 이 논문에서는 고정된 노름 임계값을 제거하기 위해, 해결자 대형 언어 모델(LLM)이 순위-1 특이값 분해(SVD) 업데이트를 생성하고, 비전 언어 모델(VLM)이 각 업데이트를 수용하거나 거부하며 중단할 시점을 결정하는 에이전트 접근 방식을 제안합니다.

Method: 행렬 확장을 위해, 해결자의 안정성은 인컨텍스트 학습(ICL)과 시각적으로 일관성 있는 구조를 드러내는 행/열 순열을 통해 향상됩니다.

Result: 우리는 Digits ($8{	imes}8$), CIFAR-10 ($32{	imes}32$ 그레이스케일), 및 합성($16{	imes}16$) 행렬을 가우시안 노이즈 유무에 따라 평가합니다. 합성 노이즈 사례에서 실제 구성 순위 $k$가 알려져 있는 경우, 수치적 확장은 노이즈 대상을 제공하며 우리의 최상의 에이전트 구성은 목표와의 RMSE 차이가 단지 $1.75$에 불과합니다.

Conclusion: 모든 설정에서 우리의 에이전트는 경쟁력 있는 결과를 달성하며, 완전한 에이전트 기반의 임계값 없는 확장이 고전 수치 알고리즘에 대한 실행 가능한 대안임을 제안합니다.

Abstract: Can a small team of agents peel a matrix apart, one rank-1 slice at a time? We propose an agentic approach to matrix deflation in which a solver Large Language Model (LLM) generates rank-1 Singular Value Decomposition (SVD) updates and a Vision Language Model (VLM) accepts or rejects each update and decides when to stop, eliminating fixed norm thresholds. Solver stability is improved through in-context learning (ICL) and types of row/column permutations that expose visually coherent structure. We evaluate on Digits ($8{\times}8$), CIFAR-10 ($32{\times}32$ grayscale), and synthetic ($16{\times}16$) matrices with and without Gaussian noise. In the synthetic noisy case, where the true construction rank $k$ is known, numerical deflation provides the noise target and our best agentic configuration differs by only $1.75$ RMSE of the target. For Digits and CIFAR-10, targets are defined by deflating until the Frobenius norm reaches $10\%$ of the original. Across all settings, our agent achieves competitive results, suggesting that fully agentic, threshold-free deflation is a viable alternative to classical numerical algorithms.

</details>


### [42] [Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making](https://arxiv.org/abs/2601.08247)
*Liu He*

Main category: cs.LG

TL;DR: 이 연구는 재정 거래를 위한 강화 학습 모델에 인지 편향을 통합하여 인간과 유사한 거래 행동을 나타내고 더 나은 위험 조정 수익을 달성할 수 있는지를 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 재정 시장은 인지 편향으로 인해 비합리적인 인간 행동에 영향을 받습니다. 기존의 강화 학습 모델은 합리적인 에이전트를 가정하고 있어 심리적 요인의 영향을 간과할 수 있습니다.

Method: 이 연구는 보상 구조와 의사 결정 과정에 과신 및 손실 회피와 같은 편향을 통합하여 재정 거래를 위한 강화 학습 프레임워크를 제안합니다.

Result: 모의 및 실제 거래 환경에서 이들 모델의 성과를 평가합니다.

Conclusion: 결과가 불확실하거나 부정적일지라도, 이 연구는 강화 학습에 인간 유사 편향을 통합하는 데 따른 도전 과학에 대한 통찰력을 제공하며, 강력한 재정 AI 시스템 개발을 위한 귀중한 교훈을 제공합니다.

Abstract: Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.

</details>


### [43] [ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning](https://arxiv.org/abs/2601.08310)
*Kun Liang,Clive Bai,Xin Xu,Chenming Tang,Sanwoo Lee,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ORBIT는 입력에 의해 동작되는 제어 가능한 다중 예산 추론 프레임워크로, 여러 단계의 강화 학습을 통해 최적의 추론 행동을 발견하고 이들을 통합하여 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존 접근 방식은 최적의 추론 노력을 추정하는 것이 fundamentally 어렵고, 기존의 훈련 동안 추론 비용과 정확성 간의 균형을 고정함으로써 유연성을 제한한다.

Method: ORBIT는 입력에 의해 트리거되는 잘 구분된 추론 모드를 가진 다중 예산 추론 프레임워크로, 여러 단계의 강화 학습을 통해 각 노력에서 파레토 최적의 추론 행동을 발견하고, 정책 증류를 통해 이 행동을 단일 모델로 융합한다.

Result: ORBIT는 (1) 여러 모드에서 제어 가능한 추론 행동, (2) 각 모드 내에서의 경쟁력 있는 추론 밀도, (3) 명확한 모드 구분과 높은 모드 별 성능을 유지하면서 이 경계 정책들을 단일 통합 학생 모델로 통합하는 것을 보여준다.

Conclusion: ORBIT는 입력에 의해 제어될 수 있는 효과적인 추론 프레임워크를 통해 기존의 제약을 극복하고 성능을 극대화한다.

Abstract: Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.

</details>


### [44] [Automated Machine Learning in Radiomics: A Comparative Evaluation of Performance, Efficiency and Accessibility](https://arxiv.org/abs/2601.08334)
*Jose Lozano-Montoya,Emilio Soria-Olivas,Almudena Fuster-Matanzo,Angel Alberich-Bayarri,Ana Jimenez-Pastor*

Main category: cs.LG

TL;DR: 자동화된 머신러닝(AutoML) 프레임워크는 비전문가가 방사선학 모델을 구축할 수 있도록 하여 기술 장벽을 낮출 수 있지만, 방사선학의 특정 과제를 해결하는 데 효과성은 불확실하다. 본 연구는 다양한 방사선학 분류 작업에서 일반 용도 및 방사선학 전용 AutoML 프레임워크의 성능과 접근성을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: 방사선학 모델 개발에서 비전문가가 쉽게 접근할 수 있도록 기계 학습 자동화의 필요성을 강조하고, 현재의 AutoML 프레임워크가 방사선학의 특정 과제를 해결하는 데 효과적인지를 검토하려 함.

Method: 10개의 공공 및 민간 방사선학 데이터 세트를 사용하여 6개의 일반 용도 및 5개의 방사선학 전용 프레임워크를 사전 정의된 매개 변수와 표준화된 교차 검증을 통해 테스트했다.

Result: Simplatab은 평균 테스트 AUC 81.81%로 가장 높은 성능을 보였고, LightAutoML은 6분 만에 평균 AUC 78.74%로 가장 빠른 실행 속도를 기록했다.

Conclusion: Simplatab은 방사선학 분류 문제에 대한 성능, 효율성 및 접근성의 균형을 제공하지만, 향후 연구는 방사선학의 특수한 도전에 더 잘 부응하는 방향으로 AutoML 솔루션을 조정하는 데 초점을 맞춰야 한다.

Abstract: Automated machine learning (AutoML) frameworks can lower technical barriers for predictive and prognostic model development in radiomics by enabling researchers without programming expertise to build models. However, their effectiveness in addressing radiomics-specific challenges remains unclear. This study evaluates the performance, efficiency, and accessibility of general-purpose and radiomics-specific AutoML frameworks on diverse radiomics classification tasks, thereby highlighting development needs for radiomics. Ten public/private radiomics datasets with varied imaging modalities (CT/MRI), sizes, anatomies and endpoints were used. Six general-purpose and five radiomics-specific frameworks were tested with predefined parameters using standardized cross-validation. Evaluation metrics included AUC, runtime, together with qualitative aspects related to software status, accessibility, and interpretability. Simplatab, a radiomics-specific tool with a no-code interface, achieved the highest average test AUC (81.81%) with a moderate runtime (~1 hour). LightAutoML, a general-purpose framework, showed the fastest execution with competitive performance (78.74% mean AUC in six minutes). Most radiomics-specific frameworks were excluded from the performance analysis due to obsolescence, extensive programming requirements, or computational inefficiency. Conversely, general-purpose frameworks demonstrated higher accessibility and ease of implementation. Simplatab provides an effective balance of performance, efficiency, and accessibility for radiomics classification problems. However, significant gaps remain, including the lack of accessible survival analysis support and the limited integration of feature reproducibility and harmonization within current AutoML frameworks. Future research should focus on adapting AutoML solutions to better address these radiomics-specific challenges.

</details>


### [45] [DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion](https://arxiv.org/abs/2601.08482)
*Chenxu Han,Sean Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: DiffMM은 희소 궤적에서 효과적이고 효율적인 맵 매칭 결과를 생성하는 엔코더-확산 기반의 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 희소 궤적에 대한 맵 매칭은 교통 스케줄링 및 교통 흐름 분석 등 많은 궤적 기반 응용 프로그램에 있어 기본 문제입니다.

Method: DiffMM은 주의 메커니즘을 통해 입력 궤적과 주변 후보 도로 세그먼트를 공유 잠재 공간으로 결합하는 도로 세그먼트 인식 궤적 인코더와 함께, 궤적과 후보 도로 세그먼트의 공동 임베딩을 조건화 맥락으로 활용하는 한 단계 확산 방법을 제안합니다.

Result: 우리의 접근법은 특히 희소 궤적과 복잡한 도로 네트워크 토폴로지에서 정확도와 효율성 면에서 최신 맵 매칭 방법들을 지속적으로 능가하는 결과를 보여줍니다.

Conclusion: DiffMM은 노이즈가 많거나 희소 샘플링된 GPS 궤적을 처리하는 데 있어 기존 방법의 한계를 극복하는 효과적인 솔루션을 제공합니다.

Abstract: Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.

</details>


### [46] [Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts](https://arxiv.org/abs/2601.08726)
*Bert Verbruggen,Arne Vanhoyweghen,Vincent Ginis*

Main category: cs.LG

TL;DR: 강화 학습(RL)은 머신러닝에서 중심적인 최적화 프레임워크로 남아있다. 비에르고딕 환경에서 전통적인 RL 아키텍처는 진정한 최적해를 회복하지 못하며, 심층 RL 구현도 비슷한 하위 최적 정책을 생성한다. 그러나 학습 과정에 명시적인 시간 의존성을 도입함으로써 이러한 한계를 기꺼이 수정할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습의 최적성 정의가 환경의 통계적 특성에 따라 달라지며, 비에르고딕 환경에서 최적해 복구의 실패 사례가 있다는 점을 분석한다.

Method: 딥 RL 구현의 비에르고딕 동역학에서의 하위 최적 정책 생성을 분석하고, 시간 의존성을 학습 프로세스에 도입하는 방법을 제시한다.

Result: 시간 정보가 포함된 기능 근사를 통해 에이전트가 내재 성장률과 일치하는 가치 함수를 추정할 수 있어 최적화가 가능해진다.

Conclusion: 환경의 피드백 변경 없이 시간 궤적을 통한 에이전트의 노출에서 자연스럽게 발생하는 개선 사항이 강조된다.

Abstract: Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality'' depends on the environment's statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network's function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process's intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent's exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.

</details>
