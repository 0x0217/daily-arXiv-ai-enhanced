{"id": "2509.19512", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19512", "abs": "https://arxiv.org/abs/2509.19512", "authors": ["Charles Dansereau", "Junior-Samuel Lopez-Yepez", "Karthik Soma", "Antoine Fagette"], "title": "The Heterogeneous Multi-Agent Challenge", "comment": "7 pages. To Appear at ECAI 2025", "summary": "Multi-Agent Reinforcement Learning (MARL) is a growing research area which\ngained significant traction in recent years, extending Deep RL applications to\na much wider range of problems. A particularly challenging class of problems in\nthis domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where\nagents with different sensors, resources, or capabilities must cooperate based\non local information. The large number of real-world situations involving\nheterogeneous agents makes it an attractive research area, yet underexplored,\nas most MARL research focuses on homogeneous agents (e.g., a swarm of identical\nrobots). In MARL and single-agent RL, standardized environments such as ALE and\nSMAC have allowed to establish recognized benchmarks to measure progress.\nHowever, there is a clear lack of such standardized testbed for cooperative\nHeMARL. As a result, new research in this field often uses simple environments,\nwhere most algorithms perform near optimally, or uses weakly heterogeneous MARL\nenvironments."}
{"id": "2509.19599", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19599", "abs": "https://arxiv.org/abs/2509.19599", "authors": ["Danilo Trombino", "Vincenzo Pecorella", "Alessandro de Giulii", "Davide Tresoldi"], "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems", "comment": null, "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."}
{"id": "2509.19456", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19456", "abs": "https://arxiv.org/abs/2509.19456", "authors": ["Krisztian Balog", "ChengXiang Zhai"], "title": "The Indispensable Role of User Simulation in the Pursuit of AGI", "comment": "Accepted for publication in Communications of the ACM", "summary": "Progress toward Artificial General Intelligence (AGI) faces significant\nbottlenecks, particularly in rigorously evaluating complex interactive systems\nand acquiring the vast interaction data needed for training adaptive agents.\nThis paper posits that user simulation -- creating computational agents that\nmimic human interaction with AI systems -- is not merely a useful tool, but is\na critical catalyst required to overcome these bottlenecks and accelerate AGI\ndevelopment. We argue that realistic simulators provide the necessary\nenvironments for scalable evaluation, data generation for interactive learning,\nand fostering the adaptive capabilities central to AGI. Therefore, research\ninto user simulation technology and intelligent task agents are deeply\nsynergistic and must advance hand-in-hand. This article elaborates on the\ncritical role of user simulation for AGI, explores the interdisciplinary nature\nof building realistic simulators, identifies key challenges including those\nposed by large language models, and proposes a future research agenda."}
{"id": "2509.19485", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19485", "abs": "https://arxiv.org/abs/2509.19485", "authors": ["Hafijul Hoque Chowdhury", "Riad Ahmed Anonto", "Sourov Jajodia", "Suryadipta Majumdar", "Md. Shohrab Hossain"], "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs", "comment": "10 pages, accepted at PST 2025", "summary": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models."}
{"id": "2509.19363", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19363", "abs": "https://arxiv.org/abs/2509.19363", "authors": ["Zhuqi Wang", "Qinghe Zhang", "Zhuopei Cheng"], "title": "Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System", "comment": null, "summary": "Credit card fraud is assuming growing proportions as a major threat to the\nfinancial position of American household, leading to unpredictable changes in\nhousehold economic behavior. To solve this problem, in this paper, a new hybrid\nanalysis method is presented by using the Enhanced ANFIS. The model proposes\nseveral advances of the conventional ANFIS framework and employs a\nmulti-resolution wavelet decomposition module and a temporal attention\nmechanism. The model performs discrete wavelet transformations on historical\ntransaction data and macroeconomic indicators to generate localized economic\nshock signals. The transformed features are then fed into a deep fuzzy rule\nlibrary which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian\nmembership functions. The model proposes a temporal attention encoder that\nadaptively assigns weights to multi-scale economic behavior patterns,\nincreasing the effectiveness of relevance assessment in the fuzzy inference\nstage and enhancing the capture of long-term temporal dependencies and\nanomalies caused by fraudulent activities. The proposed method differs from\nclassical ANFIS which has fixed input-output relations since it integrates\nfuzzy rule activation with the wavelet basis selection and the temporal\ncorrelation weights via a modular training procedure. Experimental results show\nthat the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and\nconventional LSTM models."}
{"id": "2509.19517", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.19517", "abs": "https://arxiv.org/abs/2509.19517", "authors": ["Sai Teja Reddy Adapala"], "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning", "comment": null, "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap\nbetween their performance on static benchmarks and their fragility in dynamic,\ninformation-rich environments. While models excel at isolated tasks, the\ncomputational limits that govern their reasoning under cognitive load remain\npoorly understood. In this work, we introduce a formal theory of computational\ncognitive load, positing that extraneous, task-irrelevant information (Context\nSaturation) and interference from task-switching (Attentional Residue) are key\nmechanisms that degrade performance. We designed the Interleaved Cognitive\nEvaluation (ICE), a deconfounded benchmark to systematically manipulate these\nload factors on challenging multi-hop reasoning tasks. A comprehensive study (N\n= 10 replications per item across 200 questions) revealed significant\nperformance variations across five instruction-tuned models. Smaller\nopen-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)\nexhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all\nconditions, including clean controls, on this high-intrinsic-load task. In\ncontrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%\naccuracy in control conditions, with a statistically significant degradation\nunder context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These\nfindings provide preliminary evidence that cognitive load is a key contributor\nto reasoning failures, supporting theories of hallucination-as-guessing under\nuncertainty. We conclude that dynamic, cognitive-aware stress testing, as\nexemplified by the ICE benchmark, is essential for evaluating the true\nresilience and safety of advanced AI systems."}
{"id": "2509.20166", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20166", "abs": "https://arxiv.org/abs/2509.20166", "authors": ["Lauren Deason", "Adam Bali", "Ciprian Bejean", "Diana Bolocan", "James Crnkovich", "Ioana Croitoru", "Krishna Durai", "Chase Midler", "Calin Miron", "David Molnar", "Brad Moon", "Bruno Ostarcevic", "Alberto Peltea", "Matt Rosenberg", "Catalin Sandu", "Arthur Saputkin", "Sagar Shah", "Daniel Stan", "Ernest Szocs", "Shengye Wan", "Spencer Whitman", "Sven Krasser", "Joshua Saxe"], "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning", "comment": null, "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities."}
{"id": "2509.19379", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19379", "abs": "https://arxiv.org/abs/2509.19379", "authors": ["Returaj Burnwal", "Hriday Mehta", "Nirav Pravinbhai Bhatt", "Balaraman Ravindran"], "title": "Learning from Observation: A Survey of Recent Advances", "comment": null, "summary": "Imitation Learning (IL) algorithms offer an efficient way to train an agent\nby mimicking an expert's behavior without requiring a reward function. IL\nalgorithms often necessitate access to state and action information from expert\ndemonstrations. Although expert actions can provide detailed guidance,\nrequiring such action information may prove impractical for real-world\napplications where expert actions are difficult to obtain. To address this\nlimitation, the concept of learning from observation (LfO) or state-only\nimitation learning (SOIL) has recently gained attention, wherein the imitator\nonly has access to expert state visitation information. In this paper, we\npresent a framework for LfO and use it to survey and classify existing LfO\nmethods in terms of their trajectory construction, assumptions and algorithm's\ndesign choices. This survey also draws connections between several related\nfields like offline RL, model-based RL and hierarchical RL. Finally, we use our\nframework to identify open problems and suggest future research directions."}
{"id": "2509.19566", "categories": ["cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.19566", "abs": "https://arxiv.org/abs/2509.19566", "authors": ["George Hong", "Daniel Trejo Banos"], "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics", "comment": null, "summary": "We investigate the application of Small Language Models (<10 billion\nparameters) for genomics question answering via agentic framework to address\nhallucination issues and computational cost challenges. The Nano Bio-Agent\n(NBA) framework we implemented incorporates task decomposition, tool\norchestration, and API access into well-established systems such as NCBI and\nAlphaGenome. Results show that SLMs combined with such agentic framework can\nachieve comparable and in many cases superior performance versus existing\napproaches utilising larger models, with our best model-agent combination\nachieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B\nparameter models consistently achieve 85-97% accuracy while requiring much\nlower computational resources than conventional approaches. This demonstrates\npromising potential for efficiency gains, cost savings, and democratization of\nML-powered genomics tools while retaining highly robust and accurate\nperformance."}
{"id": "2509.20190", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20190", "abs": "https://arxiv.org/abs/2509.20190", "authors": ["Tanmay Khule", "Stefan Marksteiner", "Jose Alguindigue", "Hannes Fuchs", "Sebastian Fischmeister", "Apurva Narayan"], "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation", "comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)", "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."}
{"id": "2509.19417", "categories": ["cs.LG", "math.ST", "stat.TH", "I.6.4; I.6.5; I.6.6"], "pdf": "https://arxiv.org/pdf/2509.19417", "abs": "https://arxiv.org/abs/2509.19417", "authors": ["Andreas Lebedev", "Abhinav Das", "Sven Pappert", "Stephan SchlÃ¼ter"], "title": "Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting", "comment": null, "summary": "Precise probabilistic forecasts are fundamental for energy risk management,\nand there is a wide range of both statistical and machine learning models for\nthis purpose. Inherent to these probabilistic models is some form of\nuncertainty quantification. However, most models do not capture the full extent\nof uncertainty, which arises not only from the data itself but also from model\nand distributional choices. In this study, we examine uncertainty\nquantification in state-of-the-art statistical and deep learning probabilistic\nforecasting models for electricity price forecasting in the German market. In\nparticular, we consider deep distributional neural networks (DDNNs) and augment\nthem with an ensemble approach, Monte Carlo (MC) dropout, and conformal\nprediction to account for model uncertainty. Additionally, we consider the\nLASSO-estimated autoregressive (LEAR) approach combined with quantile\nregression averaging (QRA), generalized autoregressive conditional\nheteroskedasticity (GARCH), and conformal prediction. Across a range of\nperformance metrics, we find that the LEAR-based models perform well in terms\nof probabilistic forecasting, irrespective of the uncertainty quantification\nmethod. Furthermore, we find that DDNNs benefit from incorporating both data\nand model uncertainty, improving both point and probabilistic forecasting.\nUncertainty itself appears to be best captured by the models using conformal\nprediction. Overall, our extensive study shows that all models under\nconsideration perform competitively. However, their relative performance\ndepends on the choice of metrics for point and probabilistic forecasting."}
{"id": "2509.19736", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19736", "abs": "https://arxiv.org/abs/2509.19736", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Jielin Qiu", "Zhiwei Liu", "Haolin Chen", "Shirley Kokane", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning", "comment": "28 Pages, 15 Figures, 6 Tables; Built upon latest UserBench release:\n  arXiv:2507.22034", "summary": "Reinforcement learning (RL) has shown promise in training agentic models that\nmove beyond static benchmarks to engage in dynamic, multi-turn interactions.\nYet, the ultimate value of such agents lies in their ability to assist users, a\nsetting where diversity and dynamics of user interaction pose challenges. In\nthis work, we propose UserRL, a unified framework for training and evaluating\nuser-centric abilities through standardized gym environments paired with\nsimulated users. We systematically vary turn-level reward assignment and\ntrajectory-level score calculation to analyze how different formulations affect\nlearning under the GRPO algorithm. Our experiments across Qwen3 models reveal\nthree key findings: (i) SFT cold start is critical for unlocking initial\ninteraction ability and enabling sustained RL improvements; (ii) deliberate\ntrajectory scoring yields more efficient and effective multi-turn interactions;\nand (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,\nopen-source simulators (e.g., Qwen3-32B) remain a cost-effective and\ntransferable option. Together, these results highlight that careful design of\nreward shaping and user simulation choice is as crucial as model scale, and\nestablish UserRL as a practical pathway for developing robust user-centric\nagentic models. All codes and data are public for future research."}
{"id": "2509.19921", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.19921", "abs": "https://arxiv.org/abs/2509.19921", "authors": ["Balazs Pejo", "Marcell Frank", "Krisztian Varga", "Peter Veliczky"], "title": "On the Fragility of Contribution Score Computation in Federated Learning", "comment": null, "summary": "This paper investigates the fragility of contribution evaluation in federated\nlearning, a critical mechanism for ensuring fairness and incentivizing\nparticipation. We argue that contribution scores are susceptible to significant\ndistortions from two fundamental perspectives: architectural sensitivity and\nintentional manipulation. First, we explore how different model aggregation\nmethods impact these scores. While most research assumes a basic averaging\napproach, we demonstrate that advanced techniques, including those designed to\nhandle unreliable or diverse clients, can unintentionally yet significantly\nalter the final scores. Second, we explore vulnerabilities posed by poisoning\nattacks, where malicious participants strategically manipulate their model\nupdates to inflate their own contribution scores or reduce the importance of\nother participants. Through extensive experiments across diverse datasets and\nmodel architectures, implemented within the Flower framework, we rigorously\nshow that both the choice of aggregation method and the presence of attackers\nare potent vectors for distorting contribution scores, highlighting a critical\nneed for more robust evaluation schemes."}
{"id": "2509.19771", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19771", "abs": "https://arxiv.org/abs/2509.19771", "authors": ["Hyunwoo Kim", "Hyo Kyung Lee"], "title": "Frictional Q-Learning", "comment": null, "summary": "We draw an analogy between static friction in classical mechanics and\nextrapolation error in off-policy RL, and use it to formulate a constraint that\nprevents the policy from drifting toward unsupported actions. In this study, we\npresent Frictional Q-learning, a deep reinforcement learning algorithm for\ncontinuous control, which extends batch-constrained reinforcement learning. Our\nalgorithm constrains the agent's action space to encourage behavior similar to\nthat in the replay buffer, while maintaining a distance from the manifold of\nthe orthonormal action space. The constraint preserves the simplicity of\nbatch-constrained, and provides an intuitive physical interpretation of\nextrapolation error. Empirically, we further demonstrate that our algorithm is\nrobustly trained and achieves competitive performance across standard\ncontinuous control benchmarks."}
{"id": "2509.19762", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19762", "abs": "https://arxiv.org/abs/2509.19762", "authors": ["Yuanxin Wang", "Pawel Filipczuk", "Anisha Garg", "Amaan Dhada", "Mohammad Hassanpour", "David Bick", "Ganesh Venkatesh"], "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning", "comment": null, "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by\ninternal model training and external agentic orchestration. However, this\nsynergy is often inefficient, as model verbosity and poor instruction following\nlead to wasted compute. We analyze this capability-cost trade-off and introduce\nan optimized reasoning workflow (\\cepo) that empowers smaller open-source\nmodels to outperform models multiple times their size. We will open-source this\nworkflow to enable further research. Our work demonstrates a clear path toward\nco-designing orchestration frameworks with the underlying model capabilities to\nunlock powerful reasoning in small-to-medium sized models."}
{"id": "2509.19789", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19789", "abs": "https://arxiv.org/abs/2509.19789", "authors": ["Carlo Bosio", "Greg Woelki", "Noureldin Hendy", "Nicholas Roy", "Byungsoo Kim"], "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving", "comment": "10 pages, 6 figures", "summary": "Human drivers focus only on a handful of agents at any one time. On the other\nhand, autonomous driving systems process complex scenes with numerous agents,\nregardless of whether they are pedestrians on a crosswalk or vehicles parked on\nthe side of the road. While attention mechanisms offer an implicit way to\nreduce the input to the elements that affect decisions, existing attention\nmechanisms for capturing agent interactions are quadratic, and generally\ncomputationally expensive. We propose RDAR, a strategy to learn per-agent\nrelevance -- how much each agent influences the behavior of the controlled\nvehicle -- by identifying which agents can be excluded from the input to a\npre-trained behavior model. We formulate the masking procedure as a Markov\nDecision Process where the action consists of a binary mask indicating agent\nselection. We evaluate RDAR on a large-scale driving dataset, and demonstrate\nits ability to learn an accurate numerical measure of relevance by achieving\ncomparable driving performance, in terms of overall progress, safety and\nperformance, while processing significantly fewer agents compared to a state of\nthe art behavior model."}
{"id": "2509.19783", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19783", "abs": "https://arxiv.org/abs/2509.19783", "authors": ["Jiexi Xu"], "title": "Agentic Metacognition: Designing a \"Self-Aware\" Low-Code Agent for Failure Prediction and Human Handoff", "comment": "7 pages, 2 tables", "summary": "The inherent non-deterministic nature of autonomous agents, particularly\nwithin low-code/no-code (LCNC) environments, presents significant reliability\nchallenges. Agents can become trapped in unforeseen loops, generate inaccurate\noutputs, or encounter unrecoverable failures, leading to user frustration and a\nbreakdown of trust. This report proposes a novel architectural pattern to\naddress these issues: the integration of a secondary, \"metacognitive\" layer\nthat actively monitors the primary LCNC agent. Inspired by human introspection,\nthis layer is designed to predict impending task failures based on a defined\nset of triggers, such as excessive latency or repetitive actions. Upon\npredicting a failure, the metacognitive agent proactively initiates a human\nhandoff, providing the user with a clear summary of the agent's \"thought\nprocess\" and a detailed explanation of why it could not proceed. An empirical\nanalysis of a prototype system demonstrates that this approach significantly\nincreases the overall task success rate. However, this performance gain comes\nwith a notable increase in computational overhead. The findings reframe human\nhandoffs not as an admission of defeat but as a core design feature that\nenhances system resilience, improves user experience, and builds trust by\nproviding transparency into the agent's internal state. The report discusses\nthe practical and ethical implications of this approach and identifies key\ndirections for future research."}
{"id": "2509.19921", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.19921", "abs": "https://arxiv.org/abs/2509.19921", "authors": ["Balazs Pejo", "Marcell Frank", "Krisztian Varga", "Peter Veliczky"], "title": "On the Fragility of Contribution Score Computation in Federated Learning", "comment": null, "summary": "This paper investigates the fragility of contribution evaluation in federated\nlearning, a critical mechanism for ensuring fairness and incentivizing\nparticipation. We argue that contribution scores are susceptible to significant\ndistortions from two fundamental perspectives: architectural sensitivity and\nintentional manipulation. First, we explore how different model aggregation\nmethods impact these scores. While most research assumes a basic averaging\napproach, we demonstrate that advanced techniques, including those designed to\nhandle unreliable or diverse clients, can unintentionally yet significantly\nalter the final scores. Second, we explore vulnerabilities posed by poisoning\nattacks, where malicious participants strategically manipulate their model\nupdates to inflate their own contribution scores or reduce the importance of\nother participants. Through extensive experiments across diverse datasets and\nmodel architectures, implemented within the Flower framework, we rigorously\nshow that both the choice of aggregation method and the presence of attackers\nare potent vectors for distorting contribution scores, highlighting a critical\nneed for more robust evaluation schemes."}
{"id": "2509.20021", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20021", "abs": "https://arxiv.org/abs/2509.20021", "authors": ["Tongtong Feng", "Xin Wang", "Yu-Gang Jiang", "Wenwu Zhu"], "title": "Embodied AI: From LLMs to World Models", "comment": "Accepted by IEEE CASM", "summary": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation."}
{"id": "2509.19924", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.19924", "abs": "https://arxiv.org/abs/2509.19924", "authors": ["Remo Sasso", "Michelangelo Conserva", "Dominik Jeurissen", "Paulo Rauber"], "title": "Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches", "comment": "16 pages, 7 figures. Accepted for presentation at the 39th Conference\n  on Neural Information Processing Systems (NeurIPS 2025) Workshop on the\n  Foundations of Reasoning in Language Models (FoRLM)", "summary": "Exploration in reinforcement learning (RL) remains challenging, particularly\nin sparse-reward settings. While foundation models possess strong semantic\npriors, their capabilities as zero-shot exploration agents in classic RL\nbenchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed\nbandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our\ninvestigation reveals a key limitation: while VLMs can infer high-level\nobjectives from visual input, they consistently fail at precise low-level\ncontrol: the \"knowing-doing gap\". To analyze a potential bridge for this gap,\nwe investigate a simple on-policy hybrid framework in a controlled, best-case\nscenario. Our results in this idealized setting show that VLM guidance can\nsignificantly improve early-stage sample efficiency, providing a clear analysis\nof the potential and constraints of using foundation models to guide\nexploration rather than for end-to-end control."}
{"id": "2509.20067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20067", "abs": "https://arxiv.org/abs/2509.20067", "authors": ["Wenliang Li", "Rui Yan", "Xu Zhang", "Li Chen", "Hongji Zhu", "Jing Zhao", "Junjun Li", "Mengru Li", "Wei Cao", "Zihang Jiang", "Wei Wei", "Kun Zhang", "Shaohua Kevin Zhou"], "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice."}
{"id": "2509.20214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20214", "abs": "https://arxiv.org/abs/2509.20214", "authors": ["Deokjae Lee", "Hyun Oh Song"], "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment", "comment": "NeurIPS 2025", "summary": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette."}
{"id": "2509.20095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20095", "abs": "https://arxiv.org/abs/2509.20095", "authors": ["Aymeric Vellinger", "Nemanja Antonic", "Elio Tuci"], "title": "From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms", "comment": "Contribution to the 9th International Symposium on Swarm Behavior and\n  Bio-Inspired Robotics 2025", "summary": "Swarm intelligence emerges from decentralised interactions among simple\nagents, enabling collective problem-solving. This study establishes a\ntheoretical equivalence between pheromone-mediated aggregation in \\celeg\\ and\nreinforcement learning (RL), demonstrating how stigmergic signals function as\ndistributed reward mechanisms. We model engineered nematode swarms performing\nforaging tasks, showing that pheromone dynamics mathematically mirror\ncross-learning updates, a fundamental RL algorithm. Experimental validation\nwith data from literature confirms that our model accurately replicates\nempirical \\celeg\\ foraging patterns under static conditions. In dynamic\nenvironments, persistent pheromone trails create positive feedback loops that\nhinder adaptation by locking swarms into obsolete choices. Through\ncomputational experiments in multi-armed bandit scenarios, we reveal that\nintroducing a minority of exploratory agents insensitive to pheromones restores\ncollective plasticity, enabling rapid task switching. This behavioural\nheterogeneity balances exploration-exploitation trade-offs, implementing\nswarm-level extinction of outdated strategies. Our results demonstrate that\nstigmergic systems inherently encode distributed RL processes, where\nenvironmental signals act as external memory for collective credit assignment.\nBy bridging synthetic biology with swarm robotics, this work advances\nprogrammable living systems capable of resilient decision-making in volatile\nenvironments."}
{"id": "2509.20241", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20241", "abs": "https://arxiv.org/abs/2509.20241", "authors": ["Felipe Oviedo", "Fiodar Kazhamiaka", "Esha Choukse", "Allen Kim", "Amy Luers", "Melanie Nakagawa", "Ricardo Bianchini", "Juan M. Lavista Ferres"], "title": "Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute", "comment": "A preprint version with DOI is available at Zenodo:\n  https://doi.org/10.5281/zenodo.17188770", "summary": "As AI inference scales to billions of queries and emerging reasoning and\nagentic workflows increase token demand, reliable estimates of per-query energy\nuse are increasingly important for capacity planning, emissions accounting, and\nefficiency prioritization. Many public estimates are inconsistent and overstate\nenergy use, because they extrapolate from limited benchmarks and fail to\nreflect efficiency gains achievable at scale. In this perspective, we introduce\na bottom-up methodology to estimate the per-query energy of large-scale LLM\nsystems based on token throughput. For models running on an H100 node under\nrealistic workloads, GPU utilization and PUE constraints, we estimate a median\nenergy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200\nbillion parameters). These results are consistent with measurements using\nproduction-scale configurations and show that non-production estimates and\nassumptions can overstate energy use by 4-20x. Extending to test-time scaling\nscenarios with 15x more tokens per typical query, the median energy rises 13x\nto 4.32 Wh, indicating that targeting efficiency in this regime will deliver\nthe largest fleet-wide savings. We quantify achievable efficiency gains at the\nmodel, serving platform, and hardware levels, finding individual median\nreductions of 1.5-3.5x in energy per query, while combined advances can\nplausibly deliver 8-20x reductions. To illustrate the system-level impact, we\nestimate the baseline daily energy use of a deployment serving 1 billion\nqueries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8\nGWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,\nsimilar to the energy footprint of web search at that scale. This echoes how\ndata centers historically tempered energy growth through efficiency gains\nduring the internet and cloud build-up."}
{"id": "2509.20175", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20175", "abs": "https://arxiv.org/abs/2509.20175", "authors": ["Lorenzo Giusti", "Ole Anton Werner", "Riccardo Taiello", "Matilde Carvalho Costa", "Emre Tosun", "Andrea Protani", "Marc Molina", "Rodrigo Lopes de Almeida", "Paolo Cacace", "Diogo Reis Santos", "Luigi Serio"], "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI", "comment": "18 pages, 4 figures", "summary": "We present Federation of Agents (FoA), a distributed orchestration framework\nthat transforms static multi-agent coordination into dynamic, capability-driven\ncollaboration. FoA introduces Versioned Capability Vectors (VCVs):\nmachine-readable profiles that make agent capabilities searchable through\nsemantic embeddings, enabling agents to advertise their capabilities, cost, and\nlimitations. Our aarchitecturecombines three key innovations: (1) semantic\nrouting that matches tasks to agents over sharded HNSW indices while enforcing\noperational constraints through cost-biased optimization, (2) dynamic task\ndecomposition where compatible agents collaboratively break down complex tasks\ninto DAGs of subtasks through consensus-based merging, and (3) smart clustering\nthat groups agents working on similar subtasks into collaborative channels for\nk-round refinement before synthesis. Built on top of MQTT,s publish-subscribe\nsemantics for scalable message passing, FoA achieves sub-linear complexity\nthrough hierarchical capability matching and efficient index maintenance.\nEvaluation on HealthBench shows 13x improvements over single-model baselines,\nwith clustering-enhanced laboration particularly effective for complex\nreasoning tasks requiring multiple perspectives. The system scales horizontally\nwhile maintaining consistent performance, demonstrating that semantic\norchestration with structured collaboration can unlock the collective\nintelligence of heterogeneous federations of AI agents."}
{"id": "2509.20244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20244", "abs": "https://arxiv.org/abs/2509.20244", "authors": ["Abhishek Sharma", "Anat Parush", "Sumit Wadhwa", "Amihai Savir", "Anne Guinard", "Prateek Srivastava"], "title": "Dynamic Lagging for Time-Series Forecasting in E-Commerce Finance: Mitigating Information Loss with A Hybrid ML Architecture", "comment": null, "summary": "Accurate forecasting in the e-commerce finance domain is particularly\nchallenging due to irregular invoice schedules, payment deferrals, and\nuser-specific behavioral variability. These factors, combined with sparse\ndatasets and short historical windows, limit the effectiveness of conventional\ntime-series methods. While deep learning and Transformer-based models have\nshown promise in other domains, their performance deteriorates under partial\nobservability and limited historical data. To address these challenges, we\npropose a hybrid forecasting framework that integrates dynamic lagged feature\nengineering and adaptive rolling-window representations with classical\nstatistical models and ensemble learners. Our approach explicitly incorporates\ninvoice-level behavioral modeling, structured lag of support data, and custom\nstability-aware loss functions, enabling robust forecasts in sparse and\nirregular financial settings. Empirical results demonstrate an approximate 5%\nreduction in MAPE compared to baseline models, translating into substantial\nfinancial savings. Furthermore, the framework enhances forecast stability over\nquarterly horizons and strengthens feature target correlation by capturing both\nshort- and long-term patterns, leveraging user profile attributes, and\nsimulating upcoming invoice behaviors. These findings underscore the value of\ncombining structured lagging, invoice-level closure modeling, and behavioral\ninsights to advance predictive accuracy in sparse financial time-series\nforecasting."}
{"id": "2509.20270", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20270", "abs": "https://arxiv.org/abs/2509.20270", "authors": ["Xingjian Kang", "Linda Vorberg", "Andreas Maier", "Alexander Katzmann", "Oliver Taubmann"], "title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent", "comment": null, "summary": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging."}
{"id": "2509.20290", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.20290", "abs": "https://arxiv.org/abs/2509.20290", "authors": ["Dayu Tan", "Jing Chen", "Xiaoping Zhou", "Yansen Su", "Chunhou Zheng"], "title": "PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction", "comment": "12page and 8 figures", "summary": "Infectious diseases continue to pose a serious threat to public health,\nunderscoring the urgent need for effective computational approaches to screen\nnovel anti-infective agents. Oligopeptides have emerged as promising candidates\nin antimicrobial research due to their structural simplicity, high\nbioavailability, and low susceptibility to resistance. Despite their potential,\ncomputational models specifically designed to predict associations between\noligopeptides and infectious diseases remain scarce. This study introduces a\nprompt-guided graph-based contrastive learning framework (PGCLODA) to uncover\npotential associations. A tripartite graph is constructed with oligopeptides,\nmicrobes, and diseases as nodes, incorporating both structural and semantic\ninformation. To preserve critical regions during contrastive learning, a\nprompt-guided graph augmentation strategy is employed to generate meaningful\npaired views. A dual encoder architecture, integrating Graph Convolutional\nNetwork (GCN) and Transformer, is used to jointly capture local and global\nfeatures. The fused embeddings are subsequently input into a multilayer\nperceptron (MLP) classifier for final prediction. Experimental results on a\nbenchmark dataset indicate that PGCLODA consistently outperforms\nstate-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and\nhyperparameter studies confirm the contribution of each module. Case studies\nfurther validate the generalization ability of PGCLODA and its potential to\nuncover novel, biologically relevant associations. These findings offer\nvaluable insights for mechanism-driven discovery and oligopeptide-based drug\ndevelopment. The source code of PGCLODA is available online at\nhttps://github.com/jjnlcode/PGCLODA."}
{"id": "2509.19363", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19363", "abs": "https://arxiv.org/abs/2509.19363", "authors": ["Zhuqi Wang", "Qinghe Zhang", "Zhuopei Cheng"], "title": "Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System", "comment": null, "summary": "Credit card fraud is assuming growing proportions as a major threat to the\nfinancial position of American household, leading to unpredictable changes in\nhousehold economic behavior. To solve this problem, in this paper, a new hybrid\nanalysis method is presented by using the Enhanced ANFIS. The model proposes\nseveral advances of the conventional ANFIS framework and employs a\nmulti-resolution wavelet decomposition module and a temporal attention\nmechanism. The model performs discrete wavelet transformations on historical\ntransaction data and macroeconomic indicators to generate localized economic\nshock signals. The transformed features are then fed into a deep fuzzy rule\nlibrary which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian\nmembership functions. The model proposes a temporal attention encoder that\nadaptively assigns weights to multi-scale economic behavior patterns,\nincreasing the effectiveness of relevance assessment in the fuzzy inference\nstage and enhancing the capture of long-term temporal dependencies and\nanomalies caused by fraudulent activities. The proposed method differs from\nclassical ANFIS which has fixed input-output relations since it integrates\nfuzzy rule activation with the wavelet basis selection and the temporal\ncorrelation weights via a modular training procedure. Experimental results show\nthat the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and\nconventional LSTM models."}
{"id": "2509.19517", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.19517", "abs": "https://arxiv.org/abs/2509.19517", "authors": ["Sai Teja Reddy Adapala"], "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning", "comment": null, "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap\nbetween their performance on static benchmarks and their fragility in dynamic,\ninformation-rich environments. While models excel at isolated tasks, the\ncomputational limits that govern their reasoning under cognitive load remain\npoorly understood. In this work, we introduce a formal theory of computational\ncognitive load, positing that extraneous, task-irrelevant information (Context\nSaturation) and interference from task-switching (Attentional Residue) are key\nmechanisms that degrade performance. We designed the Interleaved Cognitive\nEvaluation (ICE), a deconfounded benchmark to systematically manipulate these\nload factors on challenging multi-hop reasoning tasks. A comprehensive study (N\n= 10 replications per item across 200 questions) revealed significant\nperformance variations across five instruction-tuned models. Smaller\nopen-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)\nexhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all\nconditions, including clean controls, on this high-intrinsic-load task. In\ncontrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%\naccuracy in control conditions, with a statistically significant degradation\nunder context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These\nfindings provide preliminary evidence that cognitive load is a key contributor\nto reasoning failures, supporting theories of hallucination-as-guessing under\nuncertainty. We conclude that dynamic, cognitive-aware stress testing, as\nexemplified by the ICE benchmark, is essential for evaluating the true\nresilience and safety of advanced AI systems."}
{"id": "2509.19379", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.19379", "abs": "https://arxiv.org/abs/2509.19379", "authors": ["Returaj Burnwal", "Hriday Mehta", "Nirav Pravinbhai Bhatt", "Balaraman Ravindran"], "title": "Learning from Observation: A Survey of Recent Advances", "comment": null, "summary": "Imitation Learning (IL) algorithms offer an efficient way to train an agent\nby mimicking an expert's behavior without requiring a reward function. IL\nalgorithms often necessitate access to state and action information from expert\ndemonstrations. Although expert actions can provide detailed guidance,\nrequiring such action information may prove impractical for real-world\napplications where expert actions are difficult to obtain. To address this\nlimitation, the concept of learning from observation (LfO) or state-only\nimitation learning (SOIL) has recently gained attention, wherein the imitator\nonly has access to expert state visitation information. In this paper, we\npresent a framework for LfO and use it to survey and classify existing LfO\nmethods in terms of their trajectory construction, assumptions and algorithm's\ndesign choices. This survey also draws connections between several related\nfields like offline RL, model-based RL and hierarchical RL. Finally, we use our\nframework to identify open problems and suggest future research directions."}
{"id": "2509.19736", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19736", "abs": "https://arxiv.org/abs/2509.19736", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Jielin Qiu", "Zhiwei Liu", "Haolin Chen", "Shirley Kokane", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning", "comment": "28 Pages, 15 Figures, 6 Tables; Built upon latest UserBench release:\n  arXiv:2507.22034", "summary": "Reinforcement learning (RL) has shown promise in training agentic models that\nmove beyond static benchmarks to engage in dynamic, multi-turn interactions.\nYet, the ultimate value of such agents lies in their ability to assist users, a\nsetting where diversity and dynamics of user interaction pose challenges. In\nthis work, we propose UserRL, a unified framework for training and evaluating\nuser-centric abilities through standardized gym environments paired with\nsimulated users. We systematically vary turn-level reward assignment and\ntrajectory-level score calculation to analyze how different formulations affect\nlearning under the GRPO algorithm. Our experiments across Qwen3 models reveal\nthree key findings: (i) SFT cold start is critical for unlocking initial\ninteraction ability and enabling sustained RL improvements; (ii) deliberate\ntrajectory scoring yields more efficient and effective multi-turn interactions;\nand (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,\nopen-source simulators (e.g., Qwen3-32B) remain a cost-effective and\ntransferable option. Together, these results highlight that careful design of\nreward shaping and user simulation choice is as crucial as model scale, and\nestablish UserRL as a practical pathway for developing robust user-centric\nagentic models. All codes and data are public for future research."}
{"id": "2509.19485", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19485", "abs": "https://arxiv.org/abs/2509.19485", "authors": ["Hafijul Hoque Chowdhury", "Riad Ahmed Anonto", "Sourov Jajodia", "Suryadipta Majumdar", "Md. Shohrab Hossain"], "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs", "comment": "10 pages, accepted at PST 2025", "summary": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models."}
{"id": "2509.19512", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19512", "abs": "https://arxiv.org/abs/2509.19512", "authors": ["Charles Dansereau", "Junior-Samuel Lopez-Yepez", "Karthik Soma", "Antoine Fagette"], "title": "The Heterogeneous Multi-Agent Challenge", "comment": "7 pages. To Appear at ECAI 2025", "summary": "Multi-Agent Reinforcement Learning (MARL) is a growing research area which\ngained significant traction in recent years, extending Deep RL applications to\na much wider range of problems. A particularly challenging class of problems in\nthis domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where\nagents with different sensors, resources, or capabilities must cooperate based\non local information. The large number of real-world situations involving\nheterogeneous agents makes it an attractive research area, yet underexplored,\nas most MARL research focuses on homogeneous agents (e.g., a swarm of identical\nrobots). In MARL and single-agent RL, standardized environments such as ALE and\nSMAC have allowed to establish recognized benchmarks to measure progress.\nHowever, there is a clear lack of such standardized testbed for cooperative\nHeMARL. As a result, new research in this field often uses simple environments,\nwhere most algorithms perform near optimally, or uses weakly heterogeneous MARL\nenvironments."}
{"id": "2509.19599", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19599", "abs": "https://arxiv.org/abs/2509.19599", "authors": ["Danilo Trombino", "Vincenzo Pecorella", "Alessandro de Giulii", "Davide Tresoldi"], "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems", "comment": null, "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."}
{"id": "2509.19771", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19771", "abs": "https://arxiv.org/abs/2509.19771", "authors": ["Hyunwoo Kim", "Hyo Kyung Lee"], "title": "Frictional Q-Learning", "comment": null, "summary": "We draw an analogy between static friction in classical mechanics and\nextrapolation error in off-policy RL, and use it to formulate a constraint that\nprevents the policy from drifting toward unsupported actions. In this study, we\npresent Frictional Q-learning, a deep reinforcement learning algorithm for\ncontinuous control, which extends batch-constrained reinforcement learning. Our\nalgorithm constrains the agent's action space to encourage behavior similar to\nthat in the replay buffer, while maintaining a distance from the manifold of\nthe orthonormal action space. The constraint preserves the simplicity of\nbatch-constrained, and provides an intuitive physical interpretation of\nextrapolation error. Empirically, we further demonstrate that our algorithm is\nrobustly trained and achieves competitive performance across standard\ncontinuous control benchmarks."}
{"id": "2509.19789", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19789", "abs": "https://arxiv.org/abs/2509.19789", "authors": ["Carlo Bosio", "Greg Woelki", "Noureldin Hendy", "Nicholas Roy", "Byungsoo Kim"], "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving", "comment": "10 pages, 6 figures", "summary": "Human drivers focus only on a handful of agents at any one time. On the other\nhand, autonomous driving systems process complex scenes with numerous agents,\nregardless of whether they are pedestrians on a crosswalk or vehicles parked on\nthe side of the road. While attention mechanisms offer an implicit way to\nreduce the input to the elements that affect decisions, existing attention\nmechanisms for capturing agent interactions are quadratic, and generally\ncomputationally expensive. We propose RDAR, a strategy to learn per-agent\nrelevance -- how much each agent influences the behavior of the controlled\nvehicle -- by identifying which agents can be excluded from the input to a\npre-trained behavior model. We formulate the masking procedure as a Markov\nDecision Process where the action consists of a binary mask indicating agent\nselection. We evaluate RDAR on a large-scale driving dataset, and demonstrate\nits ability to learn an accurate numerical measure of relevance by achieving\ncomparable driving performance, in terms of overall progress, safety and\nperformance, while processing significantly fewer agents compared to a state of\nthe art behavior model."}
{"id": "2509.19924", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.19924", "abs": "https://arxiv.org/abs/2509.19924", "authors": ["Remo Sasso", "Michelangelo Conserva", "Dominik Jeurissen", "Paulo Rauber"], "title": "Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches", "comment": "16 pages, 7 figures. Accepted for presentation at the 39th Conference\n  on Neural Information Processing Systems (NeurIPS 2025) Workshop on the\n  Foundations of Reasoning in Language Models (FoRLM)", "summary": "Exploration in reinforcement learning (RL) remains challenging, particularly\nin sparse-reward settings. While foundation models possess strong semantic\npriors, their capabilities as zero-shot exploration agents in classic RL\nbenchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed\nbandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our\ninvestigation reveals a key limitation: while VLMs can infer high-level\nobjectives from visual input, they consistently fail at precise low-level\ncontrol: the \"knowing-doing gap\". To analyze a potential bridge for this gap,\nwe investigate a simple on-policy hybrid framework in a controlled, best-case\nscenario. Our results in this idealized setting show that VLM guidance can\nsignificantly improve early-stage sample efficiency, providing a clear analysis\nof the potential and constraints of using foundation models to guide\nexploration rather than for end-to-end control."}
{"id": "2509.20166", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20166", "abs": "https://arxiv.org/abs/2509.20166", "authors": ["Lauren Deason", "Adam Bali", "Ciprian Bejean", "Diana Bolocan", "James Crnkovich", "Ioana Croitoru", "Krishna Durai", "Chase Midler", "Calin Miron", "David Molnar", "Brad Moon", "Bruno Ostarcevic", "Alberto Peltea", "Matt Rosenberg", "Catalin Sandu", "Arthur Saputkin", "Sagar Shah", "Daniel Stan", "Ernest Szocs", "Shengye Wan", "Spencer Whitman", "Sven Krasser", "Joshua Saxe"], "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning", "comment": null, "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities."}
{"id": "2509.20190", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20190", "abs": "https://arxiv.org/abs/2509.20190", "authors": ["Tanmay Khule", "Stefan Marksteiner", "Jose Alguindigue", "Hannes Fuchs", "Sebastian Fischmeister", "Apurva Narayan"], "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation", "comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)", "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."}
{"id": "2509.20214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20214", "abs": "https://arxiv.org/abs/2509.20214", "authors": ["Deokjae Lee", "Hyun Oh Song"], "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment", "comment": "NeurIPS 2025", "summary": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette."}
{"id": "2509.20290", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.20290", "abs": "https://arxiv.org/abs/2509.20290", "authors": ["Dayu Tan", "Jing Chen", "Xiaoping Zhou", "Yansen Su", "Chunhou Zheng"], "title": "PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction", "comment": "12page and 8 figures", "summary": "Infectious diseases continue to pose a serious threat to public health,\nunderscoring the urgent need for effective computational approaches to screen\nnovel anti-infective agents. Oligopeptides have emerged as promising candidates\nin antimicrobial research due to their structural simplicity, high\nbioavailability, and low susceptibility to resistance. Despite their potential,\ncomputational models specifically designed to predict associations between\noligopeptides and infectious diseases remain scarce. This study introduces a\nprompt-guided graph-based contrastive learning framework (PGCLODA) to uncover\npotential associations. A tripartite graph is constructed with oligopeptides,\nmicrobes, and diseases as nodes, incorporating both structural and semantic\ninformation. To preserve critical regions during contrastive learning, a\nprompt-guided graph augmentation strategy is employed to generate meaningful\npaired views. A dual encoder architecture, integrating Graph Convolutional\nNetwork (GCN) and Transformer, is used to jointly capture local and global\nfeatures. The fused embeddings are subsequently input into a multilayer\nperceptron (MLP) classifier for final prediction. Experimental results on a\nbenchmark dataset indicate that PGCLODA consistently outperforms\nstate-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and\nhyperparameter studies confirm the contribution of each module. Case studies\nfurther validate the generalization ability of PGCLODA and its potential to\nuncover novel, biologically relevant associations. These findings offer\nvaluable insights for mechanism-driven discovery and oligopeptide-based drug\ndevelopment. The source code of PGCLODA is available online at\nhttps://github.com/jjnlcode/PGCLODA."}
