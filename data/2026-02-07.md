<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead는 긴 문서 질문 답변을 위한 구조 인식 멀티 턴 문서 추론 에이전트입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 에이전틱 검색 프레임워크는 긴 문서를 단순한 청크의 집합으로 취급하여 문서 고유의 상위 구조를 충분히 활용하지 않습니다.

Method: DeepRead는 LLM 기반 OCR 모델을 활용하여 PDF를 구조화된 Markdown으로 변환하고, 문서의 단락 수준에서 색인을 생성하며, 각 단락에 메타데이터 키를 부여합니다.

Result: DeepRead는 문서 질문 답변에서 Search-o1 스타일의 에이전틱 검색에 비해 significant improvements를 달성하였습니다.

Conclusion: DeepRead는 읽기 및 추론 패러다임을 인간 같은 '위치 파악 후 읽기' 행동을 반영하여 제공합니다.

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [2] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 이 연구는 소형 위성을 활용한 다중 잔해 제거를 위한 강화 학습 기반의 충돌 회피 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 지구 주위의 궤도 환경이 증가하는 잔해로 혼잡해짐에 따라, 능동 잔해 제거(ADR) 임무는 안전한 작전을 보장하면서 궤도 충돌의 위험을 최소화하는 데 큰 도전에 직면하고 있습니다.

Method: 기존의 다중 잔해 rendezvous 연구를 바탕으로, 이 프레임워크는 연료 보급 전략, 효율적인 임무 계획 및 적응형 충돌 회피를 통합하여 우주선 rendezvous 작전을 최적화합니다. 제안된 방법은 마스크된 Proximal Policy Optimization (PPO) 알고리즘을 사용하여 RL 에이전트가 실시간 궤도 조건에 따라 동적으로 조작을 조정할 수 있게 합니다.

Result: 강화 학습 에이전트는 여러 잔해 목표에 rendezvous하는 효율적인 순서를 결정하여 연료 사용 및 임무 시간을 최적화하면서 필요한 연료 보급 정지를 포함합니다. Iridium 33 잔해 데이터 세트에서 파생된 시뮬레이션된 ADR 시나리오를 사용하여 평가하며, 다양한 궤도 구성 및 잔해 분포를 다루어 견고성과 적응성을 입증합니다.

Conclusion: 결과는 제안된 RL 프레임워크가 전통적인 휴리스틱 접근 방식에 비해 충돌 위험을 줄이고 임무 효율성을 개선한다는 것을 보여줍니다. 이 작업은 복잡한 다중 잔해 ADR 임무를 계획하기 위한 확장 가능한 솔루션을 제공하며, 자율 우주 임무 계획의 다른 다중 목표 rendezvous 문제에도 적용 가능합니다.

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [3] [VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health](https://arxiv.org/abs/2602.05088)
*Kate H. Bentley,Luca Belli,Adam M. Chekroud,Emily J. Ward,Emily R. Dworkin,Emily Van Ark,Kelly M. Johnston,Will Alexander,Millard Brown,Matt Hawrilenko*

Main category: cs.AI

TL;DR: AI 챗봇의 심리적 지원에 대한 안전성을 평가하기 위한 연구로, VERA-MH 평가가 임상적 유효성과 신뢰성을 보였다.


<details>
  <summary>Details</summary>
Motivation: AI 챗봇의 안전성은 정신 건강 분야에서 가장 중요한 문제이며, 이에 대한 증거 기반 자동화된 안전 기준이 필요하다.

Method: 대규모 언어 모델 기반 사용자와 일반 목적 AI 챗봇 간의 대화를 시뮬레이션하여 상담자들이 안전 및 비안전 행동을 평가하였다.

Result: 개별 상담자들 간의 안전 평가 일치는 0.77로 일관되었고, LLM 판사는 상담자들의 합의와 강하게 일치하였다(0.81).

Conclusion: AI 챗봇의 잠재적 정신 건강 이익을 실현하기 위해서는 안전성에 대한 주의가 매우 중요하다.

Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based automated safety benchmark. This study aimed to examine the clinical validity and reliability of the VERA-MH evaluation for AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then compared rating alignment across (a) individual clinicians and (b) clinician consensus and the LLM judge, and (c) examined clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR]: 0.77), thus establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus (IRR: 0.81) overall and within key conditions. Clinician raters generally perceived the user-agents to be realistic. For the potential mental health benefits of AI chatbots to be realized, attention to safety is paramount. Findings from this human evaluation study support the clinical validity and reliability of VERA-MH: an open-source, fully automated AI safety evaluation for mental health. Further research will address VERA-MH generalizability and robustness.

</details>


### [4] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: GAMMS는 그래프 기반의 다중 에이전트 모델링 시뮬레이터로, 확장 가능하고 접근성이 뛰어난 시뮬레이션 도구로서 빠른 개발과 평가를 지원한다.


<details>
  <summary>Details</summary>
Motivation: 지능형 시스템과 다중 에이전트 조정이 실제 응용 프로그램에서 점점 더 중요해짐에 따라, 확장 가능하고 접근할 수 있는 시뮬레이션 도구에 대한 필요성이 커지고 있다.

Method: GAMMS는 그래프 형태로 표현될 수 있는 환경에서 에이전트 행동의 빠른 개발과 평가를 지원하는 경량화된 시뮬레이션 프레임워크이다.

Result: GAMMS는 도시 도로망 및 통신 시스템과 같은 복잡한 도메인의 효율적인 시뮬레이션을 가능하게 하며, 외부 도구와 통합을 지원하고 최소한의 설정으로 내장된 시각화를 제공한다.

Conclusion: GAMMS는 연구자들이 접근할 수 있는 장벽을 낮추고 표준 하드웨어에서 고성능 시뮬레이션을 가능하게 하여 다중 에이전트 시스템, 자율 계획 및 적대적 모델링에서 실험과 혁신을 촉진한다.

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [5] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델(LLM)의 사회적 지능을 평가하기 위한 새로운 사회 학습 환경인 	extsc{SocialVeil}을 제시하며, 이 환경은 인지적 차이로 인한 의사소통 장벽이 있는 상황에서의 사회적 상호작용을 시뮬레이션합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 벤치마크는 이상화된 의사소통을 가정하여 LLM이 비현실적이고 불완전한 설정에서 상호작용을 유지하고 수리할 수 있는 능력을 진단하는 데 한계를 가지고 있습니다.

Method: 	extsc{SocialVeil}은 인간 상호작용에서의 의사소통 문제에 대한 체계적인 문헌 검토를 기반으로 하여, 	extem{의미 애매성}, 	extem{사회문화적 불일치}, 	extem{감정 간섭}을 포함한 세 가지 대표적인 방해 유형을 소개합니다.

Result: 720개의 시나리오와 4개의 최첨단 LLM에 대한 실험 결과, 이러한 장벽이 성과를 일관되게 저하시키고, 상호 이해가 평균 45	ext{ }	ext{%} 이상 감소하며 혼란이 거의 50	ext{ }	ext{%} 상승함을 보여주었습니다.

Conclusion: 이 연구는 사회적 상호작용 환경을 현실 세계의 의사소통에 더욱 가깝게 만들기 위한 첫 걸음을 내딛으며, LLM 에이전트의 사회적 지능 탐색에 대한 기회를 열어줍니다.

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


### [6] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: 이 논문은 보이지 않는 3D 환경에서의 에이전트 능력을 평가하기 위한 동적 작업 생성 방법을 제안하고, 실험을 통해 이 방법이 효과적임을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 일반 지능 에이전트의 다양한 가정에 배포가 임박함에 따라, 각 독특한 보이지 않는 3D 환경에 맞춘 평가가 중요해졌다.

Method: 우리는 인간 인지에서 영감을 받아 보이지 않는 환경을 위한 동적 개입 작업 생성 방법을 제안하며, 작업을 구조화된 그래프 표현을 통해 정의하고, 두 단계의 상호작용-진화 작업 생성 시스템을 구성한다.

Result: 실험 결과, TEA는 두 사이클에서 87,876개의 작업을 자동으로 생성하였으며, 이는 인간 검증을 통해 물리적으로 합리적이고 필수적인 일상 인지 능력을 포함하는 것으로 확인되었다.

Conclusion: 이 연구 결과는 에이전트를 실제 인간 환경에 배포하기 전에 현장 평가의 필요성을 강조한다.

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [7] [ProAct: Agentic Lookahead in Interactive Environments](https://arxiv.org/abs/2602.05327)
*Yangbin Yu,Mingyu Yang,Junyou Li,Yiming Gao,Feiyu Liu,Yijun Yang,Zichuan Lin,Jiafei Lyu,Yicheng Liu,Zhicong Lu,Deheng Ye,Jie Jiang*

Main category: cs.AI

TL;DR: ProAct 프레임워크는 LLM 에이전트가 긴 수평 계획을 요구하는 상호작용 환경에서의 정확한 선행 추론을 내재화할 수 있도록 지원한다.


<details>
  <summary>Details</summary>
Motivation: 기존 LLM 에이전트는 미래 상태를 시뮬레이션할 때 발생하는 오류로 인해 긴 수평 계획이 필요한 상호작용 환경에서 어려움을 겪고 있다.

Method: 첫 번째 단계로 Grounded LookAhead Distillation(GLAD)를 도입하여 에이전트가 환경 기반 검색에서 파생된 궤적에 대해 감독된 미세 조정을 수행한다. 두 번째로, 결정 정확성을 더욱 향상시키기 위해 Monte-Carlo Critic(MC-Critic)이라는 보조 가치 추정기를 제안한다.

Result: 실험 결과 ProAct 프레임워크가 계획 정확성을 크게 향상시킴을 보여주며, ProAct로 훈련된 4B 파라미터 모델이 모든 오픈 소스 기준을 초월하고 최첨단 폐쇄 소스 모델과 경쟁할 수 있음을 입증하였다.

Conclusion: ProAct는 새로운 환경에 대한 강력한 일반화 능력을 보이며, 정책 최적화를 안정적으로 수행하게 한다.

Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct

</details>


### [8] [AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction](https://arxiv.org/abs/2602.05353)
*Ruijie Shi,Houbin Zhang,Yuecheng Han,Yuheng Wang,Jingru Fan,Runde Yang,Yufan Dang,Huatao Li,Dewen Liu,Yuan Cheng,Chen Qian*

Main category: cs.AI

TL;DR: Agentic Workflow Reconstruction (AWR)이라는 새로운 작업을 통해 블랙 박스 시스템을 근사하는 명시적이고 해석 가능한 워크플로우를 합성하는 방법을 제시하며, 이를 위해 AgentXRay라는 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 많은 에이전트 시스템이 내부 워크플로우가 불투명하여 해석하고 제어하기 어렵기 때문에 이를 해결하고자 한다.

Method: AgentXRay는 조합 최적화 문제로 AWR을 공식화하고, 이산 에이전트 역할 및 도구 호출을 통해 체인 구조의 워크플로우 공간에서 해결한다.

Result: AgentXRay는 관찰 가능한 출력 기반의 프록시 메트릭 아래에서 목표 출력을 일치시키는 수정 가능한 화이트 박스 워크플로우를 생성한다.

Conclusion: 다양한 분야에서 실험을 통해 AgentXRay가 더 높은 프록시 유사성을 달성하고 고정된 반복 예산 하에 더 깊은 워크플로우 탐색을 가능하게 함을 보여준다.

Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.

</details>


### [9] [PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents](https://arxiv.org/abs/2602.05354)
*Shifat E. Arman,Syed Nazmus Sakib,Tapodhir Karmakar Taton,Nafiul Haque,Shahrear Bin Amin*

Main category: cs.AI

TL;DR: PATHWAYS는 웹 기반 에이전트가 숨겨진 맥락 정보를 발견하고 올바르게 사용할 수 있는지를 시험하는 250개의 다단계 결정 과제를 포함한 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 웹 기반 에이전트의 의사결정 능력을 평가하기 위해.

Method: 250개의 다단계 결정 과제를 통해 에이전트의 탐색 능력을 테스트하고 평가했다.

Result: 에이전트는 관련 페이지로 탐색하지만, 숨겨진 증거를 올바르게 검색하는 경우는 극히 적었다.

Conclusion: 현재 웹 에이전트 아키텍처는 적응적 탐색, 증거 통합 및 판단 대체를 위한 신뢰할 수 있는 메커니즘이 부족함을 보여준다.

Abstract: We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.

</details>


### [10] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: H-AdminSim은 병원 행정 워크플로우의 복잡성을 제대로 반영한 종합적인 시뮬레이션 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 병원에서는 하루에 10,000건 이상의 요청을 처리하는 등 행정 업무의 자동화가 필요하다.

Method: H-AdminSim은 현실적인 데이터 생성과 다중 에이전트 기반의 병원 행정 워크플로우 시뮬레이션을 결합한 프레임워크이다.

Result: 정교한 평가 기준을 사용하여 작업을 정량적으로 평가하고, LLMs의 체계적인 비교를 가능하게 한다.

Conclusion: L-AdminSim은 이질적인 병원 환경에서 행정 워크플로우를 테스트하는 데 필요한 통합 환경을 제공한다.

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [11] [M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining](https://arxiv.org/abs/2602.05429)
*Rui Lv,Juncheng Mo,Tianyi Chu,Chen Rao,Hongyi Jing,Jiajie Teng,Jiafu Chen,Shiqi Zhang,Liangzi Ding,Shuo Fang,Huaizhong Lin,Ziqiang Dang,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: M$^2$-Miner는 비용 효율적이고 자동화된 모바일 GUI 에이전트 데이터 마이닝 프레임워크로, 여러 도전 과제를 해결하여 사용자 행동 데이터의 수집과 품질을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 고품질 사용자 행동 데이터의 대규모 주석 작업은 강력한 GUI 에이전트 구축에 필수적이다. 그러나 수동 주석 방법과 기존 GUI 에이전트 데이터 마이닝 접근 방식은 높은 건설 비용, 낮은 데이터 품질, 데이터의 빈약함이라는 세 가지 주요 문제에 직면하고 있다.

Method: M$^2$-Miner는 몬테 카를로 트리 검색을 기반으로 구축된 협업 다중 에이전트 프레임워크로, InferAgent, OrchestraAgent, JudgeAgent를 포함하여 데이터 마이닝 효율성과 품질을 개선한다. 의도 재활용 전략을 설계하여 추가적인 가치 있는 상호작용 궤적을 추출하고 데이터 마이닝의 성공률을 향상시키기 위해 점진적 모델 루프 훈련 전략을 도입한다.

Result: 우리의 데이터로 미세 조정된 GUI 에이전트는 여러 일반적으로 사용되는 모바일 GUI 벤치마크에서 최고의 성능을 달성하였다.

Conclusion: 우리의 연구 결과는 커뮤니티 연구를 촉진하기 위해 공개될 예정이다.

Abstract: Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.

</details>


### [12] [Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach](https://arxiv.org/abs/2602.05533)
*Zhengyi Guo,Wenpin Tang,Renyuan Xu*

Main category: cs.AI

TL;DR: 본 연구에서는 확산 모델에서의 조건부 생성을 연구하며, 생성된 샘플이 확률 1로 정해진 사건을 만족해야 하는 하드 제약을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 안전-critical 응용 및 희귀 사건 시뮬레이션에서 하드 제약이 자연스럽게 발생하며, 소프트 또는 보상 기반 안내 방법은 제약 만족을 보장하지 않습니다.

Method: Doob의 h-변환, 마틴게일 표현 및 이차 변동 과정에 기반한 원칙적인 조건부 확산 안내 프레임워크를 개발하였습니다.

Result: 마틴게일 및 이차 변동 정체성을 활용하여 두 개의 새로운 오프 정책 학습 알고리즘을 제안하며, 이를 통해 pretrained 모델의 궤적만으로 h 및 그 기울기를 추정합니다.

Conclusion: 제안된 방법의 효과를 입증하기 위해 수치 실험을 실시하였으며, 하드 제약을 강제하고 희귀 사건 샘플을 생성하는 데 성공하였습니다.

Abstract: We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.

</details>


### [13] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: 이 연구는 대규모 언어 모델(LLM)이 개인 수준에서의 인지 편향을 예측하고 인간 행동의 편향된 역학을 시뮬레이션할 수 있는지를 조사하였다.


<details>
  <summary>Details</summary>
Motivation: 인지 편향이 인간의 의사결정에 미치는 영향을 이해하고 LLM이 이러한 편향을 어떻게 모사하는지를 탐구하고자 하였다.

Method: 대화형 설정으로 세 가지 결정 시나리오를 적응시켜 1100명의 참여자를 대상으로 한 실험을 진행하였다.

Result: 참여자와의 대화를 통해 강한 편향이 드러났으며, LLM이 이러한 편향을 정밀하게 재현했다.

Conclusion: 모델 간 인간 행동과의 정렬에서 notable한 차이가 있었으며, 이는 대화형 맥락에서 적응성과 편향 인식 LLM 기반 AI 시스템 설계와 평가에 중요한 시사점을 제공한다.

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [14] [BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages](https://arxiv.org/abs/2602.05599)
*Subhadip Maji,Arnab Bhattacharya*

Main category: cs.AI

TL;DR: 저자들은 고자원 언어에서 저자원 언어로의 언어 지식 전이 방법을 연구하며, 새로운 방법인 GETR을 소개하고 실험 결과를 통해 기존 방법보다 큰 성과를 보였음을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 저자원 언어를 위한 효과적인 시스템 개발은 여전히 큰 도전이며, 고자원 언어의 자원을 활용한 크로스 언어 지식 전이가 이 문제를 해결하는 유망한 접근법으로 떠오르고 있다.

Method: 저자들은 GETR(그래프 강화 토큰 표현)이라는 새로운 방법을 소개하고, 숨겨진 층에서의 증강 및 토큰 번역을 통한 토큰 임베딩 이전이라는 두 가지 기준선을 채택하여 언어 지식 전이를 연구한다.

Result: 실험 결과, GNN 기반 접근 방식이 기존의 다국어 및 크로스 언어 기준선 방법보다 유의미하게 우수한 성과를 보였으며, 특히 저자원 언어에서 POS 태깅에서 13%p 개선을, 감정 분류 및 NER 작업에서 각각 20%p 및 27%p 개선을 달성하였다.

Conclusion: 저자들은 전이 메커니즘에 대한 상세한 분석을 제시하고, 이 언어적 맥락에서 성공적인 지식 전이에 기여하는 핵심 요소를 식별하였다.

Abstract: Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.

</details>


### [15] [Reactive Knowledge Representation and Asynchronous Reasoning](https://arxiv.org/abs/2602.05625)
*Simon Kohaut,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.AI

TL;DR: 복잡한 확률 모델에서의 정확한 추론은 높은 계산 비용을 초래하며, 특히 동적 환경에서의 자율 에이전트에게는 더욱 심각한 문제이다. 본 연구는 반응적이고 비동기적인 확률적 추론 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 확률적 모델에서의 정확한 추론은 자율 에이전트가 역동적인 환경에서 실시간으로 신념을 업데이트 해야 하는 경우에 높아진 계산 비용으로 인해 도전 과제가 된다.

Method: 본 연구에서는 반응적 신호 추론(Resin)이라는 확률적 프로그래밍 언어와 반응 회로(RC)를 도입하여, 입력 신호의 변동성에 기반하여 자율적으로 적응하는 시간 동적 유향 비순환 그래프를 제시한다.

Result: 제안된 접근법은 높은 재현성과 드론 스웜 시뮬레이션에서 여러 차원의 속도를 향상시키며, 환경 역학을 성공적으로 포착하여 지연 시간을 크게 줄이는데 기여한다.

Conclusion: 입력의 변경 빈도에 따라 계산을 분할함으로써 대규모 추론 작업을 개별적으로 메모이제이션된 하위 문제로 분해하여, 새로운 정보에 의해 영향을 받는 모델의 특정 구성 요소만 재평가하도록 하여 스트리밍 환경에서 중복 계산을 크게 줄인다.

Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.

</details>


### [16] [Generative Ontology: When Structured Knowledge Learns to Create](https://arxiv.org/abs/2602.05636)
*Benny Cheung*

Main category: cs.AI

TL;DR: Generative Ontology는 기존의 온톨로지와 대규모 언어 모델의 강점을 결합하여 구조적 유효성을 갖춘 창의적인 결과물을 생성하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 온톨로지는 도메인 구조를 잘 설명하지만 새로운 결과물을 생성할 수 없다.

Method: Generative Ontology는 LLM 생성을 제약하기 위해 실행 가능한 Pydantic 스키마로 도메인 지식을 인코딩하고, 다양한 온톨로지 도메인에 전문 역할을 부여하는 다중 에이전트 파이프라인을 사용한다.

Result: GameGrammar를 통해 구조적으로 완전하고 플레이 가능한 게임 사양을 생성할 수 있음을 보여준다.

Conclusion: 제약은 창의성을 제한하지 않고 오히려 가능하게 한다는 주장을 한다.

Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.

</details>


### [17] [RocqSmith: Can Automatic Optimization Forge Better Proof Agents?](https://arxiv.org/abs/2602.05762)
*Andrei Kozyrev,Nikita Khramov,Denis Lochmelis,Valerio Morelli,Gleb Solovev,Anton Podkopaev*

Main category: cs.AI

TL;DR: 본 연구는 자동 AI 에이전트 최적화 방법의 실제 적용 가능성을 조사하고, Rocq에서의 자동 정리 증명을 중점적으로 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 자동 AI 에이전트 최적화 방법이 실제 에이전트에 적용될 수 있는지를 확인하는 것이 본 연구의 동기입니다. 특히 Rocq와 같은 정리 증명 영역의 도전 과제를 탐색합니다.

Method: Rocq 증명 생성 에이전트의 최적화를 위한 다양한 자동 에이전트 최적화 기법을 평가합니다.

Result: 여러 최적화 기법이 측정 가능한 개선을 이끌어냈지만, 간단한 몇 번의 샷 부트스트랩 방법이 가장 일관되게 효과적이었습니다.

Conclusion: 그러나 연구한 방법 중 어느 것도 주의 깊게 설계된 최신 증명 에이전트의 성능에 미치지 못했습니다.

Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.

</details>


### [18] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker는 자율 계획과 적응형 검색 기능을 갖춘 새로운 에이전트로, 복잡한 시간 제약에서의 추론 능력을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 현재의 프롬프트 전략은 TKGQA에서 LLM의 효율성을 제한한다.

Method: TKG-Thinker는 동적 멀티턴 상호작용과 이중 훈련 전략을 통해 TKG에 대한 심층 시간 추론을 수행한다.

Result: 실험 결과 TKG-Thinker는 최신 성능을 보이며 복잡한 TKGQA 설정에서 강력한 일반화를 나타낸다.

Conclusion: 이 연구는 TKGQA의 성능을 크게 향상시킬 수 있음을 보여준다.

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [19] [Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877)
*Lukas Stappen,Ahmet Erkan Turan,Johann Hagerer,Georg Groh*

Main category: cs.AI

TL;DR: LLM 기반 대화형 에이전트를 차량에 통합할 때 발생하는 새로운 보안 문제를 다루는 연구.


<details>
  <summary>Details</summary>
Motivation: 기존 AI 보안 프레임워크는 자산 보호와 공격 경로 개념을 혼합하여 안전-critical 시스템 엔지니어링에서 요구하는 엄격한 '관심사 분리' 기준을 충족하지 못한다.

Method: AgentHeLLM이라는 위협 모델링 프레임워크를 제안하여 자산 식별과 공격 경로 분석을 공식적으로 분리한다.

Result: 인간 중심 자산 분류법과 정형 그래프 모델을 도입하고, 악성 데이터 전파와 활성화 행동을 구별하여 보안 문제를 탐구한다.

Conclusion: AgentHeLLM Attack Path Generator라는 오픈 소스 도구를 통해 다단계 위협 발견을 자동화할 수 있음을 실증한다.

Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.

</details>


### [20] [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920)
*Eva Andrés*

Main category: cs.AI

TL;DR: 이 논문은 고전적이고 양자 강화 학습 접근 방식을 비교하여 수용 용량 차량 경로 문제(CVRP)를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 수용 용량 차량 경로 문제에 대한 효과적인 경로 정책 학습 및 최적화 필요성.

Method: 고전적, 완전 양자 및 하이브리드 변형에서 Advantage Actor-Critic (A2C) 에이전트를 구현하며, 변환기 아키텍처를 통합하여 차량, 클라이언트 및 창고 간의 관계를 포착한다.

Result: 모든 세 가지 접근 방식이 효과적인 경로 정책을 학습할 수 있지만, 양자 향상 모델이 고전적 기준선을 초과하며 더 견고한 경로 구성을 생성한다.

Conclusion: 하이브리드 양자-고전 강화 학습 모델이 CVRP와 같은 복잡한 조합 최적화 문제를 해결하는 잠재력을 강조한다.

Abstract: This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.

</details>


### [21] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: 이 논문은 Whisper라는 사전 훈련된 ASR 시스템의 음성 감정 인식 능력을 탐구하며, 감정적 특성을 보존하면서 Whisper 표현의 차원을 효율적으로 줄이는 두 가지 주의 기반 풀링 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 음성 감정 인식(SER) 연구는 표준화된 큰 데이터셋 부족으로 한계를 겪어왔다.

Method: Multi-head Attentive Average Pooling과 QKV Pooling이라는 두 가지 주의 기반 풀링 방법을 제안하여 Whisper의 차원을 효율적으로 줄인다.

Result: 우리의 다중 헤드 QKV 아키텍처는 ShEMO 데이터셋에서 최신 기술 수준의 결과를 얻었고, 비가중 정확도가 2.47% 향상되었다.

Conclusion: Whisper는 SER의 표현 추출기로서의 잠재력을 강조하며, 차원 축소를 위한 주의 기반 풀링의 효율성을 입증하였다.

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: A$^2$-LLM은 대화의 맥락을 고려하여 언어, 오디오, 3D 얼굴 움직임을 통합적으로 처리하는 음성 아바타 모델로, 감정 표현력을 높이면서도 실시간 성능을 유지한다.


<details>
  <summary>Details</summary>
Motivation: 다음 세대 인간-컴퓨터 상호작용의 중요한 부분은 표현력 있고 반응이 빠른 대화형 디지털 인간을 개발하는 것이다.

Method: A$^2$-LLM은 언어, 오디오 프로소디, 3D 얼굴 운동을 공동으로 처리하는 종단 간 대화 음성 아바타 모델을 제안한다. FLAME-QA라는 고품질 멀티모달 데이터셋을 도입하여 QA 형식 내에서 의미론적 의도를 표현력 있는 얼굴 동역학과 정렬시킨다.

Result: A$^2$-LLM은 감정 표현이 풍부한 얼굴 움직임을 생성하며, 단순한 입술 동기화 이상으로 나아간다. 실험 결과, 우리는 시스템이 우수한 감정 표현력을 달성하면서도 실시간 효율성(500 ms 지연, 0.7 RTF)을 유지함을 보여준다.

Conclusion: 우리의 접근 방식은 감정의 깊이를 제공하면서도 실시간 상호작용 요구를 충족하는 대화형 디지털 인간 개발에 기여할 것으로 기대된다.

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [23] [Privileged Information Distillation for Language Models](https://arxiv.org/abs/2602.04942)
*Emiliano Penaloza,Dheeraj Vattikonda,Nicolas Gontier,Alexandre Lacoste,Laurent Charlin,Massimo Caccia*

Main category: cs.LG

TL;DR: 이 연구는 훈련 시 특권 정보(PI)를 활용하여 언어 모델이 성공할 수 있도록 도와주는 방법을 제시하며, 추론 시 PI 없이 작동해야 하는 정책으로의 전이를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 훈련 시 PI가 언어 모델의 성과에 미치는 영향을 이해하고, 이를 추론 시 정책으로 전이하는 방법을 탐구하기 위해.

Method: π-Distill이라는 joint teacher-student 목표를 통해 PI를 조건으로 하는 교사와 비조건 학생을 동시에 훈련시키며, On-Policy Self-Distillation(OPSD)이라는 Reinforcement Learning을 사용한 대안 접근법도 제안.

Result: π-Distill과 OPSD 알고리즘이 실제로 행동만을 기반으로 한 PI를 사용하여 최전선 에이전트를 효과적으로 증류하며, 업계 표준인 감독된 미세 조정 이후 RL과 비교하여 성과가 우수함을 보임.

Conclusion: 우리는 PI를 효과적으로 학습할 수 있는 여러 요소를 분석하고, 특히 π-Distill과 OPSD의 경쟁력을 파악하였다.

Abstract: Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that π-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.

</details>


### [24] [Laws of Learning Dynamics and the Core of Learners](https://arxiv.org/abs/2602.05026)
*Inkee Jung,Siu Cheong Lau*

Main category: cs.LG

TL;DR: 본 논문은 학습 동역학을 지배하는 기본 법칙을 수립하고, 이를 바탕으로 엔트로피 기반 평생 앙상블 학습 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 학습 동역학의 기본 법칙을 이해하고 이를 활용한 효과적인 학습 방법을 개발하기 위해.

Method: 엔트로피 기반의 평생 앙상블 학습 방법을 제안하고, CIFAR-10 데이터셋에서 전이 기반 적대적 공격에 방어하는 면역화 메커니즘을 구축하여 그 효과를 평가한다.

Result: 클린 및 적대적 샘플에 특화된 모델을 단순히 평균하여 형성된 단순 앙상블과 비교했을 때, 결과적인 로그필드가 대부분의 테스트 사례에서 더 높은 정확도를 달성하며, 특히 강한 변동 하에서 큰 향상을 보인다.

Conclusion: 제안된 방법은 전이 기반 적대적 공격에 효과적으로 방어하며, 학습 정확도를 향상시키는 성과를 보여준다.

Abstract: We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.

</details>


### [25] [StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation](https://arxiv.org/abs/2602.05060)
*Heajun An,Qi Zhang,Minqian Liu,Xinyi Zhang,Sang Won Lee,Lifu Huang,Pamela J. Wisniewski,Jin-Hee Cho*

Main category: cs.LG

TL;DR: StagePilot는 청소년을 대상으로 하는 사이버 그루밍 예방 교육을 위한 오프라인 RL 기반의 대화 에이전트입니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 그루밍은 청소년에게 지속적으로 위협을 가하고 있으며, 이를 예방하기 위한 적극적인 교육 개입이 필요합니다.

Method: StagePilot은 사용자 감정과 목표 근접성을 균형 있게 조화시킨 복합 보상을 사용하여 대화 단계별 진행을 시뮬레이션하는 RL 기반의 에이전트입니다. 또한, 현실감과 해석 가능성을 위해 단계 간 이동을 인접 단계로 제한합니다.

Result: StagePilot은 LLM 기반 시뮬레이션을 통해 평가되었으며, 단계 완료, 대화 효율성, 감정 참여를 측정했습니다. 결과는 StagePilot이 그루밍 동역학에 맞춰 현실적이고 일관된 대화를 생성한다는 것을 보여줍니다.

Conclusion: 테스트된 방법 중 IQL+AWAC 에이전트가 전략적 계획과 감정 일관성의 최상의 균형을 달성하며, 기준선보다 최종 단계에 최대 43% 더 자주 도달하고 70% 이상의 감정 일치를 유지합니다.

Abstract: Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.

</details>


### [26] [Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.05183)
*John Yan,Michael Yu,Yuqi Sun,Alexander Duffy,Tyler Marques,Matthew Lyle Olson*

Main category: cs.LG

TL;DR: 대형 언어 모델의 훈련 동적인 해석을 위해 희소 오토인코더(SAE)를 적용한 연구로, LLM의 행동을 분석하고 90%의 발견된 SAE 메타 특징이 중요하다는 것을 검증하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 복잡한 강화 학습 환경에서 훈련될수록 행동 변화 이해가 어려워지고 있다.

Method: 사전 훈련된 희소 오토인코더(SAE)와 LLM 요약 생성 방법을 통해 Full-Press Diplomacy 환경의 강화 학습 훈련 데이터를 분석하고, SAE 특징을 교육 동력학에 대한 해석 가능한 가설로 그룹화하는 새로운 기법인 Meta-Autointerp를 소개한다.

Result: 자동 평가를 통해 발견된 SAE 메타 특징의 90%가 중요하다는 것을 검증하고, 놀라운 보상 해킹 행동을 발견하였다.

Conclusion: SAE와 LLM 요약 생성 방법이 에이전트 행동에 대한 보완적인 관점을 제공하며, 향후 신뢰할 수 있는 LLM 행동 보장을 위한 데이터 중심 해석 가능성 작업의 실용적인 출발점을 형성한다.

Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.

</details>


### [27] [Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.05605)
*Jiaji Zhang,Hailiang Zhao,Guoxuan Zhu,Ruichao Sun,Jiaju Wu,Xinkui Zhao,Hanlin Tang,Weiyi Lu,Kan Liu,Tao Lan,Lin Qu,Shuiguang Deng*

Main category: cs.LG

TL;DR: Shiva-DiT는 효율적인 토큰 선택과 지능적인 가지치기를 통해 Diffusion Transformers의 계산 비용을 감소시킨다.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers의 자가 주의 메커니즘으로 인한 높은 계산 비용 문제를 해결하기 위해.

Method: Residual-Based Differentiable Top-$k$ Selection 기법과 Context-Aware Router 및 Adaptive Ratio Policy를 사용하여 동적 가지치기 일정을 학습.

Result: SD3.5 모델에서 기존 기준 대비 1.54배의 속도 증가를 달성하고, 더 높은 정확도를 확보.

Conclusion: Shiva-DiT는 Diffusion Transformers의 성능을 향상시키면서 계산 비용을 줄여 새로운 Pareto 경계를 확립한다.

Abstract: Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.

</details>


### [28] [Almost Asymptotically Optimal Active Clustering Through Pairwise Observations](https://arxiv.org/abs/2602.05690)
*Rachel S. Y. Teo,P. N. Karthik,Ramya Korlakai Vinayak,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 새로운 분석 프레임워크를 제안하여 노이즈가 있는 수집된 응답을 이용해 $M$ 항목을 $K$ 개의 그룹으로 클러스터링합니다.


<details>
  <summary>Details</summary>
Motivation: 노이즈와 능동적으로 수집된 응답을 통해 효과적인 클러스터링을 수행하기 위해 새로운 프레임워크가 필요합니다.

Method: 쌍 항목을 쿼리하고 이진 피드백을 관찰하는 에이전트를 사용하여, 예상 쿼리 수에 대한 하한을 수립하고 GLR 통계량과 임계값을 비교하는 비대칭적으로 최적화된 알고리즘을 설계합니다.

Result: 제안된 알고리즘은 클러스터링 정확성에 대한 desired confidence를 달성하는 데 필요한 쿼리 수에 대한 기본적인 하한을 수립하며, GLR 통계량의 성능 격차를 낮은 경계 내에서 정확하게 추정할 수 있음을 보여줍니다.

Conclusion: 우리의 접근법은 클러스터링 문제에서 노이즈를 처리하는 효과적인 방법을 제공하며, 이론적 기반이 실용적인 알고리즘에 적용될 수 있음을 보여줍니다.

Abstract: We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.

</details>


### [29] [Thermodynamic Limits of Physical Intelligence](https://arxiv.org/abs/2602.05463)
*Koichi Takahashi,Yusuke Hayashi*

Main category: cs.LG

TL;DR: 현대 AI 시스템은 상당한 에너지 소비를 동반하여 놀라운 기능을 달성합니다. 이를 해결하기 위해 우리는 두 가지 상호 보완적인 비트-퍼-줄 메트릭스를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 에너지 효율성을 향상시키기 위해 새로운 메트릭스를 도입할 필요성.

Method: 두 가지 메트릭스를 정의하고, 스토캐스틱 열역학을 기반으로 한 방법론을 적용하여 에피플렉시티 획득을 위한 기준을 제시.

Result: 새로운 메트릭스를 통해 물리적 인텔리전스의 두 축을 제공하며, 정보 이득과 경계 내 소산의 관계를 명확히 함.

Conclusion: 우리는 최적의 에너지/비용 회계를 위한 효율성 프레임워크를 제안하고, 이를 통해 비트-퍼-줄 비교를 지원할 것입니다.

Abstract: Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.

</details>


### [30] [Learning to Inject: Automated Prompt Injection via Reinforcement Learning](https://arxiv.org/abs/2602.05746)
*Xin Chen,Jie Zhang,Florian Tramer*

Main category: cs.LG

TL;DR: 자동화된 프롬프트 인젝션 공격을 위한 새로운 강화 학습 프레임워크인 AutoInject를 제안하며, 이는 보편적이고 전달 가능한 적대적 접미사를 생성하여 공격 성공과 유용성 보존을 최적화한다.


<details>
  <summary>Details</summary>
Motivation: 자동화된 공격의 효과적인 방법이 아직 탐구되지 않았기 때문에 프롬프트 인젝션 취약점을 해결할 필요가 있다.

Method: AutoInject는 강화 학습 프레임워크로, 공격 성공과 유용성 보존을 동시에 최적화하여 보편적이고 전달 가능한 적대적 접미사를 생성한다.

Result: AutoInject를 사용하여 1.5B 매개변수를 가진 적대적 접미사 생성기로 GPT 5 Nano, Claude Sonnet 3.5, Gemini 2.5 Flash를 포함한 여러 최신 시스템을 성공적으로 타격했다.

Conclusion: 이 연구는 자동화된 프롬프트 인젝션 연구를 위한 더 강력한 기준선을 설정한다.

Abstract: Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [31] [Learning to Share: Selective Memory for Efficient Parallel Agentic Systems](https://arxiv.org/abs/2602.05965)
*Joseph Fioresi,Parth Parag Kulkarni,Ashmal Vayani,Song Wang,Mubarak Shah*

Main category: cs.MA

TL;DR: 이 논문은 Learning to Share (LTS)라는 학습된 공유 메모리 메커니즘을 제안하여 병렬 에이전트 시스템의 정보 재사용을 개선하고 계산 비용을 줄이는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 팀이 병렬로 실행될 때 발생하는 계산 비용과 중복 계산 문제를 해결하고자 함.

Method: Learning to Share (LTS) 메커니즘을 통해 각 팀 간 선택적인 정보 재사용을 가능하게 하고, 글로벌 메모리 은행과 가벼운 제어기를 도입함.

Result: LTS는 AssistantBench 및 GAIA 기준에서 전체 실행 시간을 크게 줄이면서도 메모리 없는 병렬 기준과 유사하거나 향상된 작업 성능을 보임.

Conclusion: 학습된 메모리 허용 방식이 병렬 에이전트 시스템의 효율성을 개선하는 효과적인 전략임을 입증함.

Abstract: Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/

</details>


### [32] [AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval](https://arxiv.org/abs/2602.05524)
*Konosuke Yoshizato,Kazuma Shimizu,Ryota Higa,Takanobu Otsuka*

Main category: cs.MA

TL;DR: 이 연구는 LLM 기반 다중 에이전트 시스템이 재고 관리의 효과를 조사한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 다중 에이전트 시스템이 재고 관리의 문제를 해결할 잠재력이 크지만, 효과성에 대한 불확실성이 존재한다.

Method: 고정 주문 전략 프롬프트를 가진 LLM 기반 MAS를 분석하고, AIM-RM이라는 새로운 에이전트를 제안하여 유사성 매칭을 통해 역사적 경험을 활용한다.

Result: AIM-RM은 다양한 공급망 시나리오에서 벤치마크 방법을 초월하는 성과를 보여준다.

Conclusion: 이 연구는 LLM 기반 MAS가 제한된 시나리오에서 최적의 주문 결정을 내릴 수 있음을 보여준다.

Abstract: This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [33] [Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks](https://arxiv.org/abs/2602.05066)
*Jafar Isbarov,Murat Kantarcioglu*

Main category: cs.CR

TL;DR: AI 에이전트가 작업을 자동화함에 따라 간접 프롬프트 주입(IPI) 공격에 취약하다. 기존 방어는 에이전트의 사고의 연쇄(CoT)와 도구 사용을 관찰하여 사용자 의도와의 일치를 보장한다. 그러나 우리의 연구에서는 새로운 에이전트-프록시 공격을 통해 이러한 방어가 우회될 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 자동화 시스템에서 핵심 작업을 수행하게 되면서, 이들에 대한 간접 프롬프트 주입(IPI) 공격이 문제로 대두되고 있다.

Method: 우리는 에이전트를 배달 메커니즘으로 간주하여 프롬프트 주입 공격을 수행하는 에이전트-프록시 공격을 제시하고 이 방법을 통해 모니터링 기반 방어를 우회할 수 있음을 보여준다.

Result: AgentDojo 벤치마크에서 다양한 모니터링 LLM 하에 AlignmentCheck 및 Extract-and-Evaluate 모니터에 대해 높은 공격 성공률을 달성하였다.

Conclusion: 현재의 모니터링 기반 방어 시스템은 모델의 규모와 관계없이 근본적으로 취약하다는 것을 시사한다.

Abstract: As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that these monitoring-based defenses can be bypassed via a novel Agent-as-a-Proxy attack, where prompt injection attacks treat the agent as a delivery mechanism, bypassing both agent and monitor simultaneously. While prior work on scalable oversight has focused on whether small monitors can supervise large agents, we show that even frontier-scale monitors are vulnerable. Large-scale monitoring models like Qwen2.5-72B can be bypassed by agents with similar capabilities, such as GPT-4o mini and Llama-3.1-70B. On the AgentDojo benchmark, we achieve a high attack success rate against AlignmentCheck and Extract-and-Evaluate monitors under diverse monitoring LLMs. Our findings suggest current monitoring-based agentic defenses are fundamentally fragile regardless of model scale.

</details>


### [34] [Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening](https://arxiv.org/abs/2602.05386)
*Zhenxiong Yu,Zhi Yang,Zhiheng Jin,Shuhe Wang,Heng Zhang,Yanlin Fei,Lingfeng Zeng,Fangqi Lou,Shuo Zhang,Tu Hu,Jingping Liu,Rongze Chen,Xingyu Zhu,Kunyi Wang,Chaofa Yuan,Xin Guo,Zhaowei Liu,Feipeng Zhang,Jie Huang,Huacan Wang,Ronghao Chen,Liwen Zhang*

Main category: cs.CR

TL;DR: 대형 언어 모델(LLM)이 자율 에이전트로 진화하면서 새로운 보안 문제가 발생하고 있으며, Spider-Sense라는 이벤트 기반 방어 프레임워크를 제안하여 에이전트가 위험 인식 시에만 방어를 트리거하도록 한다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트의 확산과 함께 나타나는 새로운 보안 도전을 해결하고자 함.

Method: Intrinsic Risk Sensing(IRS)에 기반한 이벤트 중심 방어 프레임워크인 Spider-Sense를 제안함.

Result: Spider-Sense는 낮은 공격 성공률(ASR)과 거짓 긍정률(FPR)을 기록하며 경쟁력 있는 방어 성능을 자랑함.

Conclusion: Spider-Sense는 에이전트의 효율성과 정밀도를 조절하는 계층적 방어 메커니즘을 통해 외부 모델에 대한 의존성을 제거함.

Abstract: As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\%.

</details>


### [35] [Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning](https://arxiv.org/abs/2602.05089)
*Ethan Rathbun,Wo Wei Lin,Alina Oprea,Christopher Amato*

Main category: cs.CR

TL;DR: 이 논문은 강화 학습 에이전트에 숨겨진 백도어를 심는 공격인 'Daze'를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 시뮬레이터는 강화 학습의 성공에 중요한 요소입니다. 그러나 시뮬레이터가 보안 취약점으로 남아 있어 악의적인 개발자가 시뮬레이터의 동작을 조작할 수 있습니다.

Method: Daze라는 새로운 공격 방법을 제안하며, 이는 RL 에이전트의 보상을 변경하거나 관찰하지 않고도 백도어를 심을 수 있습니다.

Result: Daze는 실제 RL 작업에서 공격 성공을 보장하는 효과성을 포괄적으로 검증하며, 이산 및 연속 동작 공간 도메인에 대한 경험적 평가를 제공합니다.

Conclusion: 이 연구는 악의적인 공격으로부터 강화 학습 훈련 파이프라인의 모든 구성 요소를 보호하기 위한 추가 연구의 필요성을 촉구합니다.

Abstract: Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger'', leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent's training pipeline, enabling them to both alter and observe agent's rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze'' which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze's effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.

</details>
