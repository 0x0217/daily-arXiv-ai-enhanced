{"id": "2509.05149", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05149", "abs": "https://arxiv.org/abs/2509.05149", "authors": ["Huy Hung Ho", "Nhan Le Thanh"], "title": "Odoo-based Subcontract Inter-site Access Control Mechanism for Construction Projects", "comment": "7 pages, 2 figures, The 9th OISP Science and Technology Symposium", "summary": "In the era of Construction 4.0, the industry is embracing a new paradigm of\nlabor elasticity, driven by smart and flexible outsourcing and subcontracting\nstrategies. The increased reliance on specialized subcontractors enables\ncompanies to scale labor dynamically based on project demands. This adaptable\nworkforce model presents challenges in managing hierarchical integration and\ncoordinating inter-site collaboration. Our design introduces a subsystem\nintegrated into the Odoo ERP framework, employing a modular architecture to\nstreamline labor management, task tracking, and approval workflows. The system\nadopts a three-pronged approach to ensure synchronized data exchange between\ngeneral contractors and subcontractors, while maintaining both security and\noperational independence. The system features hybrid access control,\nthird-party integration for cross-domain communication, and role-based mapping\nalgorithm across sites. The system supports varying degrees of customization\nthrough a unified and consolidated attribute mapping center. This center\nleverages a tree-like index structure and Lagrange interpolation method to\nenhance the efficiency of role mapping. Demonstrations highlight practical\napplication in outsourcing, integration, and scalability scenarios, confirming\nthe system's robustness under high user volumes and in offline conditions.\nExperimental results further show improvements in database performance and\nworkflow adaptability to support a scalable, enterprise-level solution that\naligns with the evolving demands of smart construction management."}
{"id": "2509.05150", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05150", "abs": "https://arxiv.org/abs/2509.05150", "authors": ["Stefanos Vasileaidis", "Thanassis Giannetsos", "Matthias Schunter", "Bruno Crispo"], "title": "Reinforcing Secure Live Migration through Verifiable State Management", "comment": null, "summary": "Live migration of applications is a fundamental capability for enabling\nresilient computing in modern distributed systems. However, extending this\nfunctionality to trusted applications (TA) -- executing within Trusted\nExecution Environments (TEEs) -- introduces unique challenges such as secure\nstate preservation, integrity verification, replay and rollback prevention, and\nmitigation of unauthorized cloning of TAs. We present TALOS, a lightweight\nframework for verifiable state management and trustworthy application\nmigration. While our implementation is prototyped and evaluated using Intel SGX\nwith the Gramine LibOS and RISC-V Keystone (evidencing the framework's\nportability across diverse TEEs), its design is agnostic to the underlying TEE\narchitecture. Such agility is a necessity in today's network service mesh\n(collaborative computing across the continuum) where application workloads must\nbe managed across domain boundaries in a harmonized fashion. TALOS is built\naround the principle of minimizing trust assumptions: TAs are treated as\nuntrusted until explicitly verified, and the migration process does not rely on\na trusted third party. To ensure both the integrity and secure launch of the\nmigrated application, TALOS integrates memory introspection and control-flow\ngraph extraction, enabling robust verification of state continuity and\nexecution flow. Thereby achieving strong security guarantees while maintaining\nefficiency, making it suitable for decentralized settings."}
{"id": "2509.05161", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05161", "abs": "https://arxiv.org/abs/2509.05161", "authors": ["Abiodun Ganiyu", "Dara Ron", "Syed Rafiul Hussain", "Vijay K Shah"], "title": "Jamming Smarter, Not Harder: Exploiting O-RAN Y1 RAN Analytics for Efficient Interference", "comment": "8 pages, 7 figures", "summary": "The Y1 interface in O-RAN enables the sharing of RAN Analytics Information\n(RAI) between the near-RT RIC and authorized Y1 consumers, which may be\ninternal applications within the operator's trusted domain or external systems\naccessing data through a secure exposure function. While this visibility\nenhances network optimization and enables advanced services, it also introduces\na potential security risk -- a malicious or compromised Y1 consumer could\nmisuse analytics to facilitate targeted interference. In this work, we\ndemonstrate how an adversary can exploit the Y1 interface to launch selective\njamming attacks by passively monitoring downlink metrics. We propose and\nevaluate two Y1-aided jamming strategies: a clustering-based jammer leveraging\nDBSCAN for traffic profiling and a threshold-based jammer. These are compared\nagainst two baselines strategies -- always-on jammer and random jammer -- on an\nover-the-air LTE/5G O-RAN testbed. Experimental results show that in\nunconstrained jamming budget scenarios, the threshold-based jammer can closely\nreplicate the disruption caused by always-on jamming while reducing\ntransmission time by 27\\%. Under constrained jamming budgets, the\nclustering-based jammer proves most effective, causing up to an 18.1\\% bitrate\ndrop while remaining active only 25\\% of the time. These findings reveal a\ncritical trade-off between jamming stealthiness and efficiency, and illustrate\nhow exposure of RAN analytics via the Y1 interface can enable highly targeted,\nlow-overhead attacks, raising important security considerations for both\ncivilian and mission-critical O-RAN deployments."}
{"id": "2509.04505", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04505", "abs": "https://arxiv.org/abs/2509.04505", "authors": ["Somtochukwu Azie", "Yiping Meng"], "title": "The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management", "comment": "16 Pages", "summary": "The integration of Artificial Intelligence (AI) into construction project\nmanagement (CPM) is accelerating, with Large Language Models (LLMs) emerging as\naccessible decision-support tools. This study aims to critically evaluate the\nethical viability and reliability of LLMs when applied to the ethically\nsensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods\nresearch design was employed, involving the quantitative performance testing of\ntwo leading LLMs against twelve real-world ethical scenarios using a novel\nEthical Decision Support Assessment Checklist (EDSAC), and qualitative analysis\nof semi-structured interviews with 12 industry experts to capture professional\nperceptions. The findings reveal that while LLMs demonstrate adequate\nperformance in structured domains such as legal compliance, they exhibit\nsignificant deficiencies in handling contextual nuance, ensuring\naccountability, and providing transparent reasoning. Stakeholders expressed\nconsiderable reservations regarding the autonomous use of AI for ethical\njudgments, strongly advocating for robust human-in-the-loop oversight. To our\nknowledge, this is one of the first studies to empirically test the ethical\nreasoning of LLMs within the construction domain. It introduces the EDSAC\nframework as a replicable methodology and provides actionable recommendations,\nemphasising that LLMs are currently best positioned as decision-support aids\nrather than autonomous ethical agents."}
{"id": "2509.04537", "categories": ["cs.MA", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04537", "abs": "https://arxiv.org/abs/2509.04537", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem", "comment": null, "summary": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents."}
{"id": "2509.04575", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04575", "abs": "https://arxiv.org/abs/2509.04575", "authors": ["Minqi Jiang", "Andrei Lupu", "Yoram Bachrach"], "title": "Bootstrapping Task Spaces for Self-Improvement", "comment": null, "summary": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training."}
{"id": "2509.04642", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04642", "abs": "https://arxiv.org/abs/2509.04642", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "comment": "Technical Report by RELAI.ai", "summary": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix."}
{"id": "2509.04993", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04993", "abs": "https://arxiv.org/abs/2509.04993", "authors": ["Zheyan Qu", "Wenbo Wang", "Zitong Yu", "Boquan Sun", "Yang Li", "Xing Zhang"], "title": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration", "comment": "This paper has been accepted by IEEE Communications Magazine", "summary": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era."}
{"id": "2509.04699", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04699", "abs": "https://arxiv.org/abs/2509.04699", "authors": ["Wenhui Cui", "Christopher Sandino", "Hadi Pouransari", "Ran Liu", "Juri Minxha", "Ellen Zippi", "Aman Verma", "Anna Sedlackova", "Erdrin Azemi", "Behrooz Mahasseni"], "title": "CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals", "comment": null, "summary": "Hand gesture classification using high-quality structured data such as\nvideos, images, and hand skeletons is a well-explored problem in computer\nvision. Leveraging low-power, cost-effective biosignals, e.g. surface\nelectromyography (sEMG), allows for continuous gesture prediction on wearables.\nIn this paper, we demonstrate that learning representations from weak-modality\ndata that are aligned with those from structured, high-quality data can improve\nrepresentation quality and enables zero-shot classification. Specifically, we\npropose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and\npose representations, where we learn an EMG encoder that produces high-quality\nand pose-informative representations. We assess the gesture classification\nperformance of our model through linear probing and zero-shot setups. Our model\noutperforms emg2pose benchmark models by up to 21% on in-distribution gesture\nclassification and 72% on unseen (out-of-distribution) gesture classification."}
{"id": "2509.04646", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.04646", "abs": "https://arxiv.org/abs/2509.04646", "authors": ["Philippe J. Giabbanelli", "Ameeta Agrawal"], "title": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization", "comment": "Accepted at the AAAI 2025 Fall Symposium Series. November 6-8, 2025,\n  Arlington, VA, USA", "summary": "Modeling & Simulation (M&S) approaches such as agent-based models hold\nsignificant potential to support decision-making activities in health, with\nrecent examples including the adoption of vaccines, and a vast literature on\nhealthy eating behaviors and physical activity behaviors. These models are\npotentially usable by different stakeholder groups, as they support\npolicy-makers to estimate the consequences of potential interventions and they\ncan guide individuals in making healthy choices in complex environments.\nHowever, this potential may not be fully realized because of the models'\ncomplexity, which makes them inaccessible to the stakeholders who could benefit\nthe most. While Large Language Models (LLMs) can translate simulation outputs\nand the design of models into text, current approaches typically rely on\none-size-fits-all summaries that fail to reflect the varied informational needs\nand stylistic preferences of clinicians, policymakers, patients, caregivers,\nand health advocates. This limitation stems from a fundamental gap: we lack a\nsystematic understanding of what these stakeholders need from explanations and\nhow to tailor them accordingly. To address this gap, we present a step-by-step\nframework to identify stakeholder needs and guide LLMs in generating tailored\nexplanations of health simulations. Our procedure uses a mixed-methods design\nby first eliciting the explanation needs and stylistic preferences of diverse\nhealth stakeholders, then optimizing the ability of LLMs to generate tailored\noutputs (e.g., via controllable attribute tuning), and then evaluating through\na comprehensive range of metrics to further improve the tailored generation of\nsummaries."}
{"id": "2509.04731", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04731", "abs": "https://arxiv.org/abs/2509.04731", "authors": ["Brennen Hill"], "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents."}
{"id": "2509.04815", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.04815", "abs": "https://arxiv.org/abs/2509.04815", "authors": ["Wonseo Jang", "Dongjae Kim"], "title": "An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning", "comment": "8 pages, 8 figures", "summary": "Deep reinforcement learning (RL) models, despite their efficiency in learning\nan optimal policy in static environments, easily loses previously learned\nknowledge (i.e., catastrophic forgetting). It leads RL models to poor\nperformance in continual reinforcement learning (CRL) scenarios. To address\nthis, we present an arbitration control mechanism over an ensemble of RL\nagents. It is motivated by and closely aligned with how humans make decisions\nin a CRL context using an arbitration control of multiple RL agents in parallel\nas observed in the prefrontal cortex. We integrated two key ideas into our\nmodel: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have\ndiverse value functions and (2) an arbitration control that prioritizes agents\nwith higher reliability (i.e., less error) in recent trials. We propose a\nframework for CRL, an Arbitration Control for an Ensemble of Diversified DQN\nvariants (ACED-DQN). We demonstrate significant performance improvements in\nboth static and continual environments, supported by empirical evidence showing\nthe effectiveness of arbitration control over diversified DQNs during training.\nIn this work, we introduced a framework that enables RL agents to continuously\nlearn, with inspiration from the human brain."}
{"id": "2509.04731", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04731", "abs": "https://arxiv.org/abs/2509.04731", "authors": ["Brennen Hill"], "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents."}
{"id": "2509.04815", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.04815", "abs": "https://arxiv.org/abs/2509.04815", "authors": ["Wonseo Jang", "Dongjae Kim"], "title": "An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning", "comment": "8 pages, 8 figures", "summary": "Deep reinforcement learning (RL) models, despite their efficiency in learning\nan optimal policy in static environments, easily loses previously learned\nknowledge (i.e., catastrophic forgetting). It leads RL models to poor\nperformance in continual reinforcement learning (CRL) scenarios. To address\nthis, we present an arbitration control mechanism over an ensemble of RL\nagents. It is motivated by and closely aligned with how humans make decisions\nin a CRL context using an arbitration control of multiple RL agents in parallel\nas observed in the prefrontal cortex. We integrated two key ideas into our\nmodel: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have\ndiverse value functions and (2) an arbitration control that prioritizes agents\nwith higher reliability (i.e., less error) in recent trials. We propose a\nframework for CRL, an Arbitration Control for an Ensemble of Diversified DQN\nvariants (ACED-DQN). We demonstrate significant performance improvements in\nboth static and continual environments, supported by empirical evidence showing\nthe effectiveness of arbitration control over diversified DQNs during training.\nIn this work, we introduced a framework that enables RL agents to continuously\nlearn, with inspiration from the human brain."}
{"id": "2509.04966", "categories": ["cs.LG", "68T07", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.04966", "abs": "https://arxiv.org/abs/2509.04966", "authors": ["Arthur Bizzi", "Leonardo M. Moreira", "Márcio Marques", "Leonardo Mendonça", "Christian Júnior de Oliveira", "Vitor Balestro", "Lucas dos Santos Fernandez", "Daniel Yukimura", "Pavel Petrov", "João M. Pereira", "Tiago Novello", "Lucas Nissenbaum"], "title": "Neuro-Spectral Architectures for Causal Physics-Informed Networks", "comment": "24 pages, 10 figures", "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful neural\nframework for solving partial differential equations (PDEs). However, standard\nMLP-based PINNs often fail to converge when dealing with complex initial-value\nproblems, leading to solutions that violate causality and suffer from a\nspectral bias towards low-frequency components. To address these issues, we\nintroduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired\nby classical spectral methods, designed to solve linear and nonlinear PDEs with\nvariable coefficients. NeuSA learns a projection of the underlying PDE onto a\nspectral basis, leading to a finite-dimensional representation of the dynamics\nwhich is then integrated with an adapted Neural ODE (NODE). This allows us to\novercome spectral bias, by leveraging the high-frequency components enabled by\nthe spectral representation; to enforce causality, by inheriting the causal\nstructure of NODEs, and to start training near the target solution, by means of\nan initialization scheme based on classical methods. We validate NeuSA on\ncanonical benchmarks for linear and nonlinear wave equations, demonstrating\nstrong performance as compared to other architectures, with faster convergence,\nimproved temporal consistency and superior predictive accuracy. Code and\npretrained models will be released."}
{"id": "2509.04791", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04791", "abs": "https://arxiv.org/abs/2509.04791", "authors": ["Yuan Sui", "Yanming Zhang", "Yi Liao", "Yu Gu", "Guohua Tang", "Zhongqian Sun", "Wei Yang", "Bryan Hooi"], "title": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking", "comment": "arXiv admin note: text overlap with arXiv:2508.21365", "summary": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications."}
{"id": "2509.05091", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05091", "abs": "https://arxiv.org/abs/2509.05091", "authors": ["Matteo Bortoletto", "Yichao Zhou", "Lance Ying", "Tianmin Shu", "Andreas Bulling"], "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback", "comment": "Website at https://www.matteobortoletto.org/protom/", "summary": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users."}
{"id": "2509.05037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05037", "abs": "https://arxiv.org/abs/2509.05037", "authors": ["Noorul Wahab", "Ethar Alzaid", "Jiaqi Lv", "Adam Shephard", "Shan E Ahmed Raza"], "title": "ModalSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer", "comment": "6 pages, 1 figure, 2 tables", "summary": "Accurate prediction of time-to-event outcomes is a central challenge in\noncology, with significant implications for treatment planning and patient\nmanagement. In this work, we present ModaliSurv, a multimodal deep survival\nmodel utilising DeepHit with a projection layer and inter-modality\ncross-attention, which integrates heterogeneous patient data, including\nclinical, MRI, RNA-seq and whole-slide pathology features. The model is\ndesigned to capture complementary prognostic signals across modalities and\nestimate individualised time-to-biochemical recurrence in prostate cancer and\ntime-to-cancer recurrence in bladder cancer. Our approach was evaluated in the\ncontext of the CHIMERA Grand Challenge, across two of the three provided tasks.\nFor Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed\nframework achieved a concordance index (C-index) of 0.843 on 5-folds\ncross-validation and 0.818 on CHIMERA development set, demonstrating robust\ndiscriminatory ability. For Task 3 (bladder cancer recurrence prediction), the\nmodel obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on\ndevelopment set, highlighting its adaptability and potential for clinical\ntranslation. These results suggest that leveraging multimodal integration with\ndeep survival learning provides a promising pathway toward personalised risk\nstratification in prostate and bladder cancer. Beyond the challenge setting,\nour framework is broadly applicable to survival prediction tasks involving\nheterogeneous biomedical data."}
{"id": "2509.04809", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04809", "abs": "https://arxiv.org/abs/2509.04809", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "comment": "31 pages total", "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain."}
{"id": "2509.04642", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04642", "abs": "https://arxiv.org/abs/2509.04642", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "comment": "Technical Report by RELAI.ai", "summary": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix."}
{"id": "2509.04847", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04847", "abs": "https://arxiv.org/abs/2509.04847", "authors": ["Mukul Singh", "Arjun Radhakrishna", "Sumit Gulwani"], "title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory", "comment": "9 pages", "summary": "Language models are increasingly deployed in interactive online environments,\nfrom personal chat assistants to domain-specific agents, raising questions\nabout their cooperative and competitive behavior in multi-party settings. While\nprior work has examined language model decision-making in isolated or\nshort-term game-theoretic contexts, these studies often neglect long-horizon\ninteractions, human-model collaboration, and the evolution of behavioral\npatterns over time. In this paper, we investigate the dynamics of language\nmodel behavior in the iterated prisoner's dilemma (IPD), a classical framework\nfor studying cooperation and conflict. We pit model-based agents against a\nsuite of 240 well-established classical strategies in an Axelrod-style\ntournament and find that language models achieve performance on par with, and\nin some cases exceeding, the best-known classical strategies. Behavioral\nanalysis reveals that language models exhibit key properties associated with\nstrong cooperative strategies - niceness, provocability, and generosity while\nalso demonstrating rapid adaptability to changes in opponent strategy mid-game.\nIn controlled \"strategy switch\" experiments, language models detect and respond\nto shifts within only a few rounds, rivaling or surpassing human adaptability.\nThese results provide the first systematic characterization of long-term\ncooperative behaviors in language model agents, offering a foundation for\nfuture research into their role in more complex, mixed human-AI social\nenvironments."}
{"id": "2509.04731", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04731", "abs": "https://arxiv.org/abs/2509.04731", "authors": ["Brennen Hill"], "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents."}
{"id": "2509.04871", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04871", "abs": "https://arxiv.org/abs/2509.04871", "authors": ["Krittanon Kaewtawee", "Wachiravit Modecrua", "Krittin Pachtrachai", "Touchapon Kraisingkorn"], "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets for Telesales", "comment": "10 pages, 4 figures", "summary": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation."}
{"id": "2509.04871", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04871", "abs": "https://arxiv.org/abs/2509.04871", "authors": ["Krittanon Kaewtawee", "Wachiravit Modecrua", "Krittin Pachtrachai", "Touchapon Kraisingkorn"], "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets for Telesales", "comment": "10 pages, 4 figures", "summary": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation."}
{"id": "2509.04876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04876", "abs": "https://arxiv.org/abs/2509.04876", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Xiaofei Sun", "Keze Wang"], "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration", "comment": "Accepted at EMNLP 2025 (Long Paper)", "summary": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors."}
{"id": "2509.04926", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04926", "abs": "https://arxiv.org/abs/2509.04926", "authors": ["Barbara Gendron", "Gaël Guibon", "Mathieu D'aquin"], "title": "Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts", "comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and\n  applications)", "summary": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI."}
{"id": "2509.04926", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04926", "abs": "https://arxiv.org/abs/2509.04926", "authors": ["Barbara Gendron", "Gaël Guibon", "Mathieu D'aquin"], "title": "Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts", "comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and\n  applications)", "summary": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI."}
{"id": "2509.05263", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05263", "abs": "https://arxiv.org/abs/2509.05263", "authors": ["Yinglin Duan", "Zhengxia Zou", "Tongwei Gu", "Wei Jia", "Zhan Zhao", "Luyi Xu", "Xinzhu Liu", "Hao Jiang", "Kang Chen", "Shuang Qiu"], "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation", "comment": null, "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18"}
{"id": "2509.04979", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04979", "abs": "https://arxiv.org/abs/2509.04979", "authors": ["Rajesh Tembarai Krishnamachari", "Srividya Rajesh"], "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents", "comment": null, "summary": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web."}
{"id": "2509.05072", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05072", "abs": "https://arxiv.org/abs/2509.05072", "authors": ["Nir Sweed", "Hanit Hakim", "Ben Wolfson", "Hila Lifshitz", "Dafna Shahaf"], "title": "Finding your MUSE: Mining Unexpected Solutions Engine", "comment": null, "summary": "Innovators often exhibit cognitive fixation on existing solutions or nascent\nideas, hindering the exploration of novel alternatives. This paper introduces a\nmethodology for constructing Functional Concept Graphs (FCGs), interconnected\nrepresentations of functional elements that support abstraction, problem\nreframing, and analogical inspiration. Our approach yields large-scale,\nhigh-quality FCGs with explicit abstraction relations, overcoming limitations\nof prior work. We further present MUSE, an algorithm leveraging FCGs to\ngenerate creative inspirations for a given problem. We demonstrate our method\nby computing an FCG on 500K patents, which we release for further research."}
{"id": "2509.05091", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05091", "abs": "https://arxiv.org/abs/2509.05091", "authors": ["Matteo Bortoletto", "Yichao Zhou", "Lance Ying", "Tianmin Shu", "Andreas Bulling"], "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback", "comment": "Website at https://www.matteobortoletto.org/protom/", "summary": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users."}
{"id": "2509.05263", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05263", "abs": "https://arxiv.org/abs/2509.05263", "authors": ["Yinglin Duan", "Zhengxia Zou", "Tongwei Gu", "Wei Jia", "Zhan Zhao", "Luyi Xu", "Xinzhu Liu", "Hao Jiang", "Kang Chen", "Shuang Qiu"], "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation", "comment": null, "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18"}
{"id": "2509.04537", "categories": ["cs.MA", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04537", "abs": "https://arxiv.org/abs/2509.04537", "authors": ["Ryosuke Takata", "Atsushi Masumori", "Takashi Ikegami"], "title": "Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem", "comment": null, "summary": "We investigate the emergent social dynamics of Large Language Model (LLM)\nagents in a spatially extended El Farol Bar problem, observing how they\nautonomously navigate this classic social dilemma. As a result, the LLM agents\ngenerated a spontaneous motivation to go to the bar and changed their decision\nmaking by becoming a collective. We also observed that the LLM agents did not\nsolve the problem completely, but rather behaved more like humans. These\nfindings reveal a complex interplay between external incentives\n(prompt-specified constraints such as the 60% threshold) and internal\nincentives (culturally-encoded social preferences derived from pre-training),\ndemonstrating that LLM agents naturally balance formal game-theoretic\nrationality with social motivations that characterize human behavior. These\nfindings suggest that a new model of group decision making, which could not be\nhandled in the previous game-theoretic problem setting, can be realized by LLM\nagents."}
{"id": "2509.04993", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04993", "abs": "https://arxiv.org/abs/2509.04993", "authors": ["Zheyan Qu", "Wenbo Wang", "Zitong Yu", "Boquan Sun", "Yang Li", "Xing Zhang"], "title": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration", "comment": "This paper has been accepted by IEEE Communications Magazine", "summary": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era."}
