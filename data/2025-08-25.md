<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 4]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Building and Measuring Trust between Large Language Models](https://arxiv.org/abs/2508.15858)
*Maarten Buyl,Yousra Fettach,Guillaume Bied,Tijl De Bie*

Main category: cs.MA

TL;DR: 이 논문은 대규모 언어 모델(LLM) 간의 신뢰 형성을 분석하고, 신뢰를 측정하는 방법과 그 결과가 어떻게 연결되는지를 연구한다.


<details>
  <summary>Details</summary>
Motivation: LLM 간의 상호작용이 증가함에 따라 이들이 인간의 동료나 친구 간의 신뢰 관계와 유사한 신뢰 관계를 발전시키기를 기대하게 된다.

Method: 신뢰의 암묵적 척도(설득에 대한 민감성 및 재정적 협력 경향)를 심리학에서 확립된 2인 신뢰 질문지와 연결하여 연구했다. 신뢰는 다이나믹하게 라포를 형성하고, 신뢰를 증명하는 사전 작성된 스크립트에서 시작하며, LLM의 시스템 프롬프트를 조정함으로써 구축되었다.

Result: 명시적 신뢰 척도는 암묵적 신뢰 척도와 적거나 매우 부정적인 상관 관계를 보였다.

Conclusion: LLM 간의 신뢰를 그들의 의견을 물어 측정하는 것은 오해를 불러일으킬 수 있으며, 맥락에 특화된 암묵적 측정이 LLM 간의 신뢰를 이해하는 데 더 유용할 수 있다.

Abstract: As large language models (LLMs) increasingly interact with each other, most
notably in multi-agent setups, we may expect (and hope) that `trust'
relationships develop between them, mirroring trust relationships between human
colleagues, friends, or partners. Yet, though prior work has shown LLMs to be
capable of identifying emotional connections and recognizing reciprocity in
trust games, little remains known about (i) how different strategies to build
trust compare, (ii) how such trust can be measured implicitly, and (iii) how
this relates to explicit measures of trust.
  We study these questions by relating implicit measures of trust, i.e.
susceptibility to persuasion and propensity to collaborate financially, with
explicit measures of trust, i.e. a dyadic trust questionnaire well-established
in psychology. We build trust in three ways: by building rapport dynamically,
by starting from a prewritten script that evidences trust, and by adapting the
LLMs' system prompt. Surprisingly, we find that the measures of explicit trust
are either little or highly negatively correlated with implicit trust measures.
These findings suggest that measuring trust between LLMs by asking their
opinion may be deceiving. Instead, context-specific and implicit measures may
be more informative in understanding how LLMs trust each other.

</details>


### [2] [Sound and Solution-Complete CCBS](https://arxiv.org/abs/2508.16410)
*Alvin Combrink,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.MA

TL;DR: CCBS는 다중 에이전트 경로 찾기 문제의 최적 해법으로 알려져 있지만, 비종료 문제와 비최적 해를 반환할 수 있는 문제점이 발견되었다. 이 논문은 이를 해결하기 위한 분석 프레임워크를 소개하며, 새로운 분기 규칙을 제안하여 CCBS의 신뢰성과 종료 보장을 복원하였다.


<details>
  <summary>Details</summary>
Motivation: CCBS의 기존 구현은 비종료 문제와 비최적 해를 반환할 수 있어, 이 문제를 해결하기 위한 방법이 필요하다.

Method: 새로운 분석 프레임워크를 소개하고, 이 프레임워크를 기반으로 새로운 분기 규칙을 제안하여 CCBS의 신뢰성과 종료를 보장한다.

Result: 제안한 분기 규칙은 표준 CCBS보다 낮은 비용의 솔루션을 반환하며, 이는 기존 코딩 기반에서 쉽게 대체 가능하다.

Conclusion: 제안된 방법은 CCBS를 개선하여 다중 에이전트 경로 찾기 문제에서 처음으로 신뢰성과 종료 보장을 제공한다.

Abstract: Continuous-time Conflict Based-Search (CCBS) has long been viewed as the
de-facto optimal solver for multi-agent path finding in continuous time
(MAPFR). Recent findings, however, show that the original theoretical variant
of CCBS can suffer from non-termination, while the widely used implementation
can return sub-optimal solutions. We introduce an analytical framework that
yields simple and sufficient conditions under which any CCBS-style algorithm is
both sound, i.e., returns only optimal solutions, and solution complete, i.e.,
terminates on every solvable MAPFR instance. Investigating the publicly
available implementation of CCBS reveals that it violates these conditions.
Though this merely indicates that CCBS might be unsound, this indication is
supported by counter-examples.
  Leveraging the analytical framework, we propose a novel branching rule and
prove that it satisfies the sufficient conditions, thereby restoring soundness
and termination guarantees. Consequently, the resulting CCBS variant is both
sound and solution complete, matching the guarantees of the discrete-time CBS
for the first time in the continuous domain. We experimentally apply standard
CCBS and CCBS under our branching rule to an example problem, with our
branching rule returning a solution with lower sum-of-costs than standard CCBS.
Because the branching rule largely only affects the branching step, it can be
adopted as a drop-in replacement in existing code-bases, as we show in our
provided implementation. Beyond CCBS, the analytical framework and termination
criterion provide a systematic way to evaluate other CCBS-like MAPFR solvers
and future extensions.

</details>


### [3] [Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework](https://arxiv.org/abs/2508.16440)
*Surya Murthy,Zhenyu Gao,John-Paul Clarke,Ufuk Topcu*

Main category: cs.MA

TL;DR: 본 연구는 소음 최소화와 안전 유지라는 두 가지 목표를 통합한 강화 학습 기반의 공중교통 관리 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 도시 항공 이동성 (UAM)은 밀집된 도시 환경에서의 교통 수단 변화를 막대한 소규모 항공기 사용을 통해 실현하고자 한다.

Method: 강화 학습(RL)을 기반으로 하여 소음과 안전을 통합적으로 관리하는 분산 공중교통 관리 시스템을 개발하였다.

Result: 시스템은 높은 교통 밀도에서 분리, 소음 노출, 에너지 효율성 간의 tradeoff를 드러내며 두 가지 목표에서 강력한 성능을 보여준다.

Conclusion: 결과는 UAM 운용의 안전성, 조용함, 효율성을 향상시키는 데 있어 RL 및 다목적 조정 전략의 가능성을 강조한다.

Abstract: Urban Air Mobility (UAM) envisions the widespread use of small aerial
vehicles to transform transportation in dense urban environments. However, UAM
faces critical operational challenges, particularly the balance between
minimizing noise exposure and maintaining safe separation in low-altitude urban
airspace, two objectives that are often addressed separately. We propose a
reinforcement learning (RL)-based air traffic management system that integrates
both noise and safety considerations within a unified, decentralized framework.
Under this scalable air traffic coordination solution, agents operate in a
structured, multi-layered airspace and learn altitude adjustment policies to
jointly manage noise impact and separation constraints. The system demonstrates
strong performance across both objectives and reveals tradeoffs among
separation, noise exposure, and energy efficiency under high traffic density.
The findings highlight the potential of RL and multi-objective coordination
strategies in enhancing the safety, quietness, and efficiency of UAM
operations.

</details>


### [4] [Abmax: A JAX-based Agent-based Modeling Framework](https://arxiv.org/abs/2508.16508)
*Siddharth Chaturvedi,Ahmed El-Gazzar,Marcel van Gerven*

Main category: cs.MA

TL;DR: 이 논문에서는 JAX를 기반으로 한 ABM 프레임워크인 Abmax를 소개하며, 복잡한 시스템의 연구를 위한 에이전트 기반 모델링의 효율성을 높이는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 시스템을 연구하기 위해 ABM의 효과적인 활용과 성능 향상이 필요하다.

Method: Abmax는 여러 개의 JIT 컴파일 가능한 알고리즘을 구현하여 에이전트 조작 작업의 유연성을 제공한다.

Result: Abmax는 기존의 구현과 유사한 런타임 성능을 달성하며, 병렬 실행이 가능하다.

Conclusion: Abmax 프레임워크는 교통 흐름 모델과 금융 시장 모델 두 가지 예제를 통해 그 유용성을 보여준다.

Abstract: Agent-based modeling (ABM) is a principal approach for studying complex
systems. By decomposing a system into simpler, interacting agents, agent-based
modeling (ABM) allows researchers to observe the emergence of complex
phenomena. High-performance array computing libraries like JAX can help scale
such computational models to a large number of agents by using automatic
vectorization and just-in-time (JIT) compilation. One of the caveats of using
JAX to achieve such scaling is that the shapes of arrays used in the
computational model should remain immutable throughout the simulation. In the
context of agent-based modeling (ABM), this can pose constraints on certain
agent manipulation operations that require flexible data structures. A subset
of which is represented by the ability to update a dynamically selected number
of agents by applying distinct changes to them during a simulation. To this
effect, we introduce Abmax, an ABM framework based on JAX that implements
multiple just-in-time (JIT) compilable algorithms to provide this
functionality. On the canonical predation model benchmark, Abmax achieves
runtime performance comparable to state-of-the-art implementations. Further, we
show that this functionality can also be vectorized, making it possible to run
many similar agent-based models in parallel. We also present two examples in
the form of a traffic-flow model and a financial market model to show the use
case of Abmax.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [5] [Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain](https://arxiv.org/abs/2508.15776)
*Saeid Ghasemshirazi,Ghazaleh Shirvani,Marziye Ranjbar Tavakoli,Bahar Ghaedi,Mohammad Amin Langarizadeh*

Main category: cs.CR

TL;DR: 이 논문은 제로 트러스트 아키텍처가 제약 공급망의 보안과 회복력을 향상시킬 수 있는 가능성을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 제약 공급망은 환자 안전과 운영 지속성을 위협하는 사이버 보안 문제에 직면해 있다.

Method: 논문에서는 제로 트러스트의 지속적인 검증, 최소 권한 접근 및 데이터 중심 보안 원칙을 탐색하고, 실제 사례 연구를 통해 성공적인 구현을 보여준다.

Result: 제로 트러스트 구현을 통해 보안 강화, 데이터 보호 및 적응 가능한 회복력의 이점을 얻는다.

Conclusion: 제약 산업은 제로 트러스트를 수용함으로써 사이버 위협에 대한 공급망을 강화하고, 의료 운영의 신뢰성을 보장할 수 있다.

Abstract: The pharmaceutical supply chain faces escalating cybersecurity challenges
threatening patient safety and operational continuity. This paper examines the
transformative potential of zero trust architecture for enhancing security and
resilience within this critical ecosystem. We explore the challenges posed by
data breaches, counterfeiting, and disruptions and introduce the principles of
continuous verification, least-privilege access, and data-centric security
inherent in zero trust. Real-world case studies illustrate successful
implementations. Benefits include heightened security, data protection, and
adaptable resilience. As recognized by researchers and industrialists, a
reliable drug tracing system is crucial for ensuring drug safety throughout the
pharmaceutical production process. One of the most pivotal domains within the
pharmaceutical industry and its associated supply chains where zero trust can
be effectively implemented is in the management of narcotics, high-health-risk
drugs, and abusable substances. By embracing zero trust, the pharmaceutical
industry fortifies its supply chain against constantly changing cyber threats,
ensuring the trustworthiness of critical medical operations.

</details>


### [6] [Towards Stealthy and Effective Backdoor Attacks on Lane Detection: A Naturalistic Data Poisoning Approach](https://arxiv.org/abs/2508.15778)
*Yifan Liao,Yuxin Cao,Yedi Zhang,Wentao He,Yan Xiao,Xianglong Du,Zhiyong Huang,Jin Song Dong*

Main category: cs.CR

TL;DR: 본 논문은 자율주행 및 첨단 운전 보조 시스템에서의 차선 감지의 취약성을 해결하기 위해 자연스럽고 은밀한 백도어 트리거를 생성하는 새로운 데이터 오염 프레임워크인 DBALD를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 기반 차선 감지는 자율주행 및 첨단 운전 보조 시스템에서 중요한 역할을 하지만, 기존의 백도어 공격 방법은 현실적 활용도가 제한적입니다.

Method: DBALD는 최적의 트리거 위치 찾기와 은밀한 트리거 생성을 포함한 두 가지 핵심 요소로 구성됩니다.

Result: DBALD는 높은 공격 성공률과 뛰어난 은밀성을 달성하며, 4개의 주요 차선 감지 모델에 대한 실험에서 평균 성공률을 10.87% 향상시켰습니다.

Conclusion: 실험 결과는 실제 백도어 위협에 대한 모델 견고성을 보장하는 데 있어 상당한 실용적 도전과제를 강조합니다.

Abstract: Deep learning-based lane detection (LD) plays a critical role in autonomous
driving and advanced driver assistance systems. However, its vulnerability to
backdoor attacks presents a significant security concern. Existing backdoor
attack methods on LD often exhibit limited practical utility due to the
artificial and conspicuous nature of their triggers. To address this limitation
and investigate the impact of more ecologically valid backdoor attacks on LD
models, we examine the common data poisoning attack and introduce DBALD, a
novel diffusion-based data poisoning framework for generating naturalistic
backdoor triggers. DBALD comprises two key components: optimal trigger position
finding and stealthy trigger generation. Given the insight that attack
performance varies depending on the trigger position, we propose a
heatmap-based method to identify the optimal trigger location, with gradient
analysis to generate attack-specific heatmaps. A region-based editing diffusion
process is then applied to synthesize visually plausible triggers within the
most susceptible regions identified previously. Furthermore, to ensure scene
integrity and stealthy attacks, we introduce two loss strategies: one for
preserving lane structure and another for maintaining the consistency of the
driving scene. Consequently, compared to existing attack methods, DBALD
achieves both a high attack success rate and superior stealthiness. Extensive
experiments on 4 mainstream LD models show that DBALD exceeds state-of-the-art
methods, with an average success rate improvement of +10.87% and significantly
enhanced stealthiness. The experimental results highlight significant practical
challenges in ensuring model robustness against real-world backdoor threats in
LD.

</details>


### [7] [Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations](https://arxiv.org/abs/2508.15808)
*Benjamin Murphy,Twm Stone*

Main category: cs.CR

TL;DR: AI의 발전은 사이버 보안에 중대한 영향을 미치며, 많은 기업들이 보안 문제를 소홀히 함으로써 공격자들에게 더 많이 노출될 위험에 처해 있다. 이러한 '후발 주자' 기업들은 AI의 발전에 적절히 대응하지 못하고 있으며, 따라서 더 많은 공격이 발생할 것으로 예상된다.


<details>
  <summary>Details</summary>
Motivation: AI의 발전이 사이버 보안에 미치는 영향과 기업들이 경험하는 보안 문제를 분석하기 위함이다.

Method: 후발 주자 기업들이 보안에 충분한 투자를 하지 않는 이유와 AI가 그들에게 미치는 위협을 논의한다.

Result: AI의 발전으로 인해 사이버 공격 횟수와 양이 증가할 것이며, 후발 주자 기업들은 더 많은 공격에 노출될 것이다.

Conclusion: 조직과 정부가 후발 주자 기업들의 방어 태세를 개선하기 위한 다양한 솔루션을 제안한다.

Abstract: Advances in AI are widely understood to have implications for cybersecurity.
Articles have emphasized the effect of AI on the cyber offense-defense balance,
and commentators can be found arguing either that cyber will privilege
attackers or defenders. For defenders, arguments are often made that AI will
enable solutions like formal verification of all software--and for some
well-equipped companies, this may be true. This conversation, however, does not
match the reality for most companies. "Trailing-edge organizations," as we term
them, rely heavily on legacy software, poorly staff security roles, and
struggle to implement best practices like rapid deployment of security patches.
These decisions may be the result of corporate inertia, but may also be the
result of a seemingly-rational calculation that attackers may not bother
targeting a firm due to lack of economic incentives, and as a result,
underinvestment in defense will not be punished.
  This approach to security may have been sufficient prior to the development
of AI systems, but it is unlikely to remain viable in the near future. We argue
that continuing improvements in AI's capabilities poses additional risks on two
fronts: First, increased usage of AI will alter the economics of the marginal
cyberattack and expose these trailing-edge organizations to more attackers,
more frequently. Second, AI's advances will enable attackers to develop
exploits and launch attacks earlier than they can today--meaning that it is
insufficient for these companies to attain parity with today's leading
defenders, but must instead aim for faster remediation timelines and more
resilient software. The situation today portends a dramatically increased
number of attacks in the near future. Moving forward, we offer a range of
solutions for both organizations and governments to improve the defensive
posture of firms which lag behind their peers today.

</details>


### [8] [CIA+TA Risk Assessment for AI Reasoning Vulnerabilities](https://arxiv.org/abs/2508.15839)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: AI 시스템의 의사결정에 대한 위협을 다루는 인지 사이버 보안 프레임워크 제안.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 의사결정에 미치는 영향이 커짐에 따라, 기술적 인프라가 아닌 추론 메커니즘을 악용하는 위협에 직면하고 있다.

Method: 전통적인 사이버 보안과 AI 안전을 보완하는 인지 사이버 보안이라는 새로운 분야를 설정하고, CIA+TA 프레임워크를 소개하여 지식 주장을 생성하고 결정을 중재하는 시스템에 유일한 요구 사항을 추가하였다.

Result: 정량적 위험 평가 방법론을 제시하고, OWASP LLM Top 10 및 MITRE ATLAS에 맵핑하여 운영 통합을 용이하게 하였다.

Conclusion: AI 배포의 신뢰성을 위한 governance 요구사항으로서 사전 배포 인지 침투 테스트가 필요하다.

Abstract: As AI systems increasingly influence critical decisions, they face threats
that exploit reasoning mechanisms rather than technical infrastructure. We
present a framework for cognitive cybersecurity, a systematic protection of AI
reasoning processes from adversarial manipulation. Our contributions are
threefold. First, we establish cognitive cybersecurity as a discipline
complementing traditional cybersecurity and AI safety, addressing
vulnerabilities where legitimate inputs corrupt reasoning while evading
conventional controls. Second, we introduce the CIA+TA, extending traditional
Confidentiality, Integrity, and Availability triad with Trust (epistemic
validation) and Autonomy (human agency preservation), requirements unique to
systems generating knowledge claims and mediating decisions. Third, we present
a quantitative risk assessment methodology with empirically-derived
coefficients, enabling organizations to measure cognitive security risks. We
map our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational
integration. Validation through previously published studies (151 human
participants; 12,180 AI trials) reveals strong architecture dependence:
identical defenses produce effects ranging from 96% reduction to 135%
amplification of vulnerabilities. This necessitates pre-deployment Cognitive
Penetration Testing as a governance requirement for trustworthy AI deployment.

</details>


### [9] [Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution](https://arxiv.org/abs/2508.15840)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 소셜 미디어에서의 메시지 작성 시, 사용자는 개인 정보 보호 기대 없이 공개적으로 메시지를 송신한다. 이 논문에서는 스타일 측정 기술을 분석하고, 적대적 스타일 측정에서의 반대 전략을 논의하며, 유니코드 스테가노그래피를 통해 개선 방안을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 소셜 미디어와 같은 공공 통신 채널에서의 개인 정보 보호 문제를 해결하기 위해.

Method: 스타일 측정 기술을 분석하고, 적대적 스타일 측정에서의 전략을 논의하고, 유니코드 스테가노그래피를 통해 보완 방안을 제시함.

Result: 스타일 측정 및 적대적 스타일 측정의 구현 가능성과 효율성을 설명.

Conclusion: 메시지 내용이 개인의 신원을 드러내는 공격 경로가 될 수 있음을 경고하며, 이에 대한 방어 방안 제시.

Abstract: When using a public communication channel -- whether formal or informal, such
as commenting or posting on social media -- end users have no expectation of
privacy: they compose a message and broadcast it for the world to see. Even if
an end user takes utmost precautions to anonymize their online presence --
using an alias or pseudonym; masking their IP address; spoofing their
geolocation; concealing their operating system and user agent; deploying
encryption; registering with a disposable phone number or email; disabling
non-essential settings; revoking permissions; and blocking cookies and
fingerprinting -- one obvious element still lingers: the message itself.
Assuming they avoid lapses in judgment or accidental self-exposure, there
should be little evidence to validate their actual identity, right? Wrong. The
content of their message -- necessarily open for public consumption -- exposes
an attack vector: stylometric analysis, or author profiling. In this paper, we
dissect the technique of stylometry, discuss an antithetical counter-strategy
in adversarial stylometry, and devise enhancements through Unicode
steganography.

</details>


### [10] [Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion](https://arxiv.org/abs/2508.15848)
*Yinghan Zhou,Juan Wen,Wanli Peng,Zhengxian Wu,Ziwei Zhang,Yiming Xue*

Main category: cs.CR

TL;DR: 자체 변장 공격(SDA)이라는 새로운 접근법을 통해 대형 언어 모델(LLM)이 출력을 능동적으로 변장하여 탐지 가능성을 줄이는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 생성 텍스트(AIGT) 탐지 회피는 탐지 확률을 줄여 탐지기의 약점을 파악하고 실제 응용에서의 효과성과 신뢰성을 높이는 것을 목표로 한다.

Method: SDA는 적대적 특징 추출기와 검색 기반 컨텍스트 예시 최적화기로 구성된다. 전자는 LLM이 보다 인간적인 텍스트를 생성할 수 있도록 변장 특징을 생성한다. 후자는 외부 지식 기반에서 가장 관련성이 높은 예시를 검색하여 인-context 예시로 사용한다.

Result: 실험 결과, SDA는 세 가지 다른 LLM에 의해 생성된 텍스트에 대해 다양한 AIGT 탐지기의 전체 탐지 정확도를 효과적으로 감소시키면서 AIGT의 품질을 유지한다.

Conclusion: SDA는 탐지 저항 텍스트 생성을 위한 지침으로 변장 특징 및 최적화된 컨텍스트 예시를 포함하는 프롬프트를 사용하여 자원 소비를 줄인다.

Abstract: AI-generated text (AIGT) detection evasion aims to reduce the detection
probability of AIGT, helping to identify weaknesses in detectors and enhance
their effectiveness and reliability in practical applications. Although
existing evasion methods perform well, they suffer from high computational
costs and text quality degradation. To address these challenges, we propose
Self-Disguise Attack (SDA), a novel approach that enables Large Language Models
(LLM) to actively disguise its output, reducing the likelihood of detection by
classifiers. The SDA comprises two main components: the adversarial feature
extractor and the retrieval-based context examples optimizer. The former
generates disguise features that enable LLMs to understand how to produce more
human-like text. The latter retrieves the most relevant examples from an
external knowledge base as in-context examples, further enhancing the
self-disguise ability of LLMs and mitigating the impact of the disguise process
on the diversity of the generated text. The SDA directly employs prompts
containing disguise features and optimized context examples to guide the LLM in
generating detection-resistant text, thereby reducing resource consumption.
Experimental results demonstrate that the SDA effectively reduces the average
detection accuracy of various AIGT detectors across texts generated by three
different LLMs, while maintaining the quality of AIGT.

</details>


### [11] [Linkage Attacks Expose Identity Risks in Public ECG Data Sharing](https://arxiv.org/abs/2508.15850)
*Ziyu Wang,Elahe Khatibi,Farshad Firouzi,Sanaz Rahimi Mousavi,Krishnendu Chakrabarty,Amir M. Rahmani*

Main category: cs.CR

TL;DR: 이 연구는 실제 공격자가 부분적인 정보를 가진 조건에서 ECG의 개인 정보 보호 위험을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: 공개된 심전도(ECG) 데이터의 증가로 인해 개인 정보 보호에 대한 우려가 커지고 있다.

Method: 109명의 참여자를 포함한 다양한 실제 데이터 세트를 사용하여 공공 데이터 세트에서 개인을 재식별하는 정확도를 평가하였다.

Result: 공공 데이터 세트에서 85%의 재식별 정확도를 달성하고, 전체 오분류율은 14.2%로 나타났다.

Conclusion: 간단한 익명화 기법으로는 재식별을 방지하는 데 한계가 있으며, 개인 정보 보호를 위한 전략이 긴급히 필요하다.

Abstract: The increasing availability of publicly shared electrocardiogram (ECG) data
raises critical privacy concerns, as its biometric properties make individuals
vulnerable to linkage attacks. Unlike prior studies that assume idealized
adversarial capabilities, we evaluate ECG privacy risks under realistic
conditions where attackers operate with partial knowledge. Using data from 109
participants across diverse real-world datasets, our approach achieves 85%
accuracy in re-identifying individuals in public datasets while maintaining a
14.2% overall misclassification rate at an optimal confidence threshold, with
15.6% of unknown individuals misclassified as known and 12.8% of known
individuals misclassified as unknown. These results highlight the inadequacy of
simple anonymization techniques in preventing re-identification, demonstrating
that even limited adversarial knowledge enables effective identity linkage. Our
findings underscore the urgent need for privacy-preserving strategies, such as
differential privacy, access control, and encrypted computation, to mitigate
re-identification risks while ensuring the utility of shared biosignal data in
healthcare applications.

</details>


### [12] [Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection](https://arxiv.org/abs/2508.15865)
*Julia Boone,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 이 논문은 사이버-물리 시스템(CPS)의 보안을 위해 레이블이 없는 데이터를 사용하는 적응형 이상 탐지 모델을 개발했다.


<details>
  <summary>Details</summary>
Motivation: CPS는 다양한 응용 시나리오를 위해 널리 이용되지만, 물리적 및 컴퓨팅 요소의 결합으로 인해 공격에 더 취약하다.

Method: 도메인 적응 기술을 이용해 네트워크 트래픽 환경에서 알고 있는 공격 지식을 CPS 환경으로 전이하는 방법을 개발했다.

Result: 우리의 접근 방식을 네트워크, 운영 체제, ROS 데이터가 결합된 CPS 침입 데이터셋을 사용하여 검증하였고, 모델의 효과성을 입증했다.

Conclusion: 이 모델은 네트워크 트래픽 전용 환경과 CPS 환경에서의 다양한 공격 유형에 대해 우수함을 보였다.

Abstract: Cyber-physical systems (CPS) are being increasingly utilized for critical
applications. CPS combines sensing and computing elements, often having
multi-layer designs with networking, computational, and physical interfaces,
which provide them with enhanced capabilities for a variety of application
scenarios. However, the combination of physical and computational elements also
makes CPS more vulnerable to attacks compared to network-only systems, and the
resulting impacts of CPS attacks can be substantial. Intelligent intrusion
detection systems (IDS) are an effective mechanism by which CPS can be secured,
but the majority of current solutions often train and validate on network
traffic-only datasets, ignoring the distinct attacks that may occur on other
system layers. In order to address this, we develop an adaptable CPS anomaly
detection model that can detect attacks within CPS without the need for
previously labeled data. To achieve this, we utilize domain adaptation
techniques that allow us to transfer known attack knowledge from a network
traffic-only environment to a CPS environment. We validate our approach using a
state-of-the-art CPS intrusion dataset that combines network, operating system
(OS), and Robot Operating System (ROS) data. Through this dataset, we are able
to demonstrate the effectiveness of our model across network traffic-only and
CPS environments with distinct attack types and its ability to outperform other
anomaly detection methods.

</details>


### [13] [Evolving k-Threshold Visual Cryptography Schemes](https://arxiv.org/abs/2508.15917)
*Xiaoli Zhuo,Xuehu Yan,Lintao Liu,Wei Yan*

Main category: cs.CR

TL;DR: 이 논문에서는 $(k,orall)$ 비밀 이미지를 위한 새로운 시각 암호화 계획을 제안하고 대조도 개선 전략을 개발한다.


<details>
  <summary>Details</summary>
Motivation: 비밀 이미지 공유 연구는 주로 시각 암호화 계획(VCS)에 중점을 두었지만, 현재 pixel 확장 없이 임의의 $k$값에 적용할 수 있는 $(k,orall)$ VCS 구조는 없다.

Method: 임의의 $k$에 대해 작동하는 무작위 격자를 기반으로 하는 $(k,orall)$ VCS를 제안하고, $k=2$ 및 $3$에 대한 최적화된 $(k,orall)$ VCS 및 $k
geq 4$에 대한 대조도 향상 전략을 개발한다.

Result: 이론적 분석 및 실험 결과를 통해 제안한 스킴의 우수성을 입증한다.

Conclusion: 제안된 $(k,orall)$ VCS는 대조도를 개선하고, 다양한 참가자 수에 효과적으로 대응할 수 있는 기회를 제공한다.

Abstract: In evolving access structures, the number of participants is countably
infinite with no predetermined upper bound. While such structures have been
realized in secret sharing, research in secret image sharing has primarily
focused on visual cryptography schemes (VCS). However, there exists no
construction for $(k,\infty)$ VCS that applies to arbitrary $k$ values without
pixel expansion currently, and the contrast requires enhancement. In this
paper, we first present a formal mathematical definition of $(k,\infty)$ VCS.
Then, propose a $(k,\infty)$ VCS based on random grids that works for arbitrary
$k$. In addition, to further improve contrast, we develop optimized
$(k,\infty)$ VCS for $k=2$ and $3$, along with contrast enhancement strategies
for $k\geq 4$. Theoretical analysis and experimental results demonstrate the
superiority of our proposed schemes.

</details>


### [14] [Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification](https://arxiv.org/abs/2508.15934)
*Onur Alp Kirci,M. Emre Gursoy*

Main category: cs.CR

TL;DR: 본 논문은 클린 라벨 시나리오에서 백도어 공격의 효과성을 개선하기 위한 세 가지 샘플 선택 전략을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 백도어 공격은 자연어 처리에서 텍스트 분류 모델의 무결성에 심각한 위협을 가합니다.

Method: 본 논문에서는 세 가지 샘플 선택 전략(최소, Above50, Below50)을 제안하여 클린 라벨 시나리오에서 백도어 공격의 효과성을 개선합니다.

Result: 제안된 전략은 특히 최소 전략이 무작위 샘플 선택에 비해 공격 성공률을 크게 향상시켰으며, 모델의 클린 정확도에는 거의 영향을 미치지 않았습니다.

Conclusion: 우리의 전략을 통해 강화된 클린 라벨 공격은 BITE와 같은 최신 클린 라벨 공격 방법을 여러 구성에서 능가합니다.

Abstract: Backdoor attacks pose a significant threat to the integrity of text
classification models used in natural language processing. While several
dirty-label attacks that achieve high attack success rates (ASR) have been
proposed, clean-label attacks are inherently more difficult. In this paper, we
propose three sample selection strategies to improve attack effectiveness in
clean-label scenarios: Minimum, Above50, and Below50. Our strategies identify
those samples which the model predicts incorrectly or with low confidence, and
by injecting backdoor triggers into such samples, we aim to induce a stronger
association between the trigger patterns and the attacker-desired target label.
We apply our methods to clean-label variants of four canonical backdoor attacks
(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets
(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,
RoBERTa). Results show that the proposed strategies, particularly the Minimum
strategy, significantly improve the ASR over random sample selection with
little or no degradation in the model's clean accuracy. Furthermore,
clean-label attacks enhanced by our strategies outperform BITE, a state of the
art clean-label attack method, in many configurations.

</details>


### [15] [PickleBall: Secure Deserialization of Pickle-based Machine Learning Models](https://arxiv.org/abs/2508.15987)
*Andreas D. Kellas,Neophytos Christou,Wenxin Jiang,Penghui Li,Laurent Simon,Yaniv David,Vasileios P. Kemerlis,James C. Davis,Junfeng Yang*

Main category: cs.CR

TL;DR: 악의적인 모델이 포함된 머신러닝 모델로부터 보호하기 위해 새로운 도구 PickleBall을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존 모델 교환 방식의 여러 결점으로 인해 위험한 공격이 가능하다.

Method: PickleBall은 머신러닝 라이브러리의 소스 코드를 정적 분석하여 안전한 로드 타임 동작을 지정하는 사용자 정의 정책을 생성하고, 이를 동적 로드 시간에 시행한다.

Result: 우리 데이터셋에서 benign pickle 기반 모델의 79.8%를 올바르게 로드하고, 악의적인 예시는 100% 차단했다.

Conclusion: PickleBall은 악의적인 모델에 의한 임의 함수 호출 위협을 제거하여 공격자들이 코드 재사용 기술에 의존해야 하는 기준을 높인다.

Abstract: Machine learning model repositories such as the Hugging Face Model Hub
facilitate model exchanges. However, bad actors can deliver malware through
compromised models. Existing defenses such as safer model formats, restrictive
(but inflexible) loading policies, and model scanners have shortcomings: 44.9%
of popular models on Hugging Face still use the insecure pickle format, 15% of
these cannot be loaded by restrictive loading policies, and model scanners have
both false positives and false negatives. Pickle remains the de facto standard
for model exchange, and the ML community lacks a tool that offers transparent
safe loading.
  We present PickleBall to help machine learning engineers load pickle-based
models safely. PickleBall statically analyzes the source code of a given
machine learning library and computes a custom policy that specifies a safe
load-time behavior for benign models. PickleBall then dynamically enforces the
policy during load time as a drop-in replacement for the pickle module.
PickleBall generates policies that correctly load 79.8% of benign pickle-based
models in our dataset, while rejecting all (100%) malicious examples in our
dataset. In comparison, evaluated model scanners fail to identify known
malicious models, and the state-of-art loader loads 22% fewer benign models
than PickleBall. PickleBall removes the threat of arbitrary function invocation
from malicious pickle-based models, raising the bar for attackers to depend on
code reuse techniques.

</details>


### [16] [A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries](https://arxiv.org/abs/2508.16078)
*Nadeem Ahmed,Lei Zhang,Aryya Gangopadhyay*

Main category: cs.CR

TL;DR: 이 연구는 9개의 오픈소스 암호화 라이브러리에서 양자 저항 암호 알고리즘 지원 상태를 평가하고 있으며, 양자 위협에 대비하기 위한 긴급한 필요성을 강조하고 있다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨팅의 급속한 발전은 현대 암호 시스템에 큰 위협을 초래하므로, 포스트 양자 암호(PQC)로의 전환이 필요하다.

Method: NIST에서 선정한 PQC 후보 알고리즘인 CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, SPHINCS+의 구현을 평가하고, 최신 문서와 산업 보고서를 바탕으로 암호화 라이브러리의 PQC 지원 상태를 분석하였다.

Result: 조사 결과, 일부 라이브러리는 PQC 지원을 통합하거나 명확한 구현 로드맵을 가지고 있는 반면, 다른 라이브러리는 뒤처져 있어 양자 위협이 다가옴에 따라 잠재적인 보안 위험이 존재함을 발견하였다.

Conclusion: 본 연구는 양자 저항 암호 환경으로의 안전한 전환을 보장하기 위해 지속적인 연구, 표준화 노력 및 협력적 채택 전략의 필요성을 강조한다.

Abstract: The rapid advancement of quantum computing poses a significant threat to
modern cryptographic systems, necessitating the transition to Post-Quantum
Cryptography (PQC). This study evaluates the support for PQC algorithms within
nine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,
BoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --
focusing on their implementation of the NIST-selected PQC finalists:
CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based
on the latest available documentation, release notes, and industry reports as
of early 2025, reveals a varied state of readiness across these libraries.
While some libraries have integrated PQC support or have clear implementation
roadmaps, others lag behind, creating potential security risks as quantum
threats become more imminent. We discuss key challenges, including performance
trade-offs, implementation security, and adoption hurdles in real-world
cryptographic applications. Our findings highlight the urgent need for
continued research, standardization efforts, and coordinated adoption
strategies to ensure a secure transition to the quantum-resistant cryptographic
landscape.

</details>


### [17] [SoK: Understanding the Fundamentals and Implications of Sensor Out-of-band Vulnerabilities](https://arxiv.org/abs/2508.16133)
*Shilin Xiao,Wenjun Zhu,Yan Jiang,Kai Wang,Peiwang Wang,Chen Yan,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.CR

TL;DR: 이 논문은 센서 하드웨어의 취약점을 체계적으로 분석하고, CPS의 보안 강화를 위한 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 센서는 사이버-물리 시스템(CPS)의 기본 요소로, 물리적 자극을 디지털 측정으로 변환하는 역할을 수행한다. 그러나 센서에 대한 물리적 공격에 대한 연구가 증가하고 있음에도 불구하고, 센서 하드웨어 취약점에 대한 이해는 조각나 있다.

Method: 이 논문은 먼저 '센서 아웃-오브-밴드(OOB) 취약점'이라는 체계화 프레임워크를 제안하며, 이를 통해 센서 공격 표면에 대한 포괄적인 추상을 제공한다. 3개의 레벨에서 OOB 취약점들을 분석하는 바닥에서 위로 가는 체계화 방법론을 채택한다.

Result: 부품 수준에서 OOB 취약점에 기여하는 물리적 원리와 한계를 식별하고, 센서 수준에서는 알려진 공격을 분류하고 이들의 실제 가능성을 평가하며, 시스템 수준에서는 CPS 특징이 OOB 위협의 노출과 완화에 미치는 영향을 분석한다.

Conclusion: 이 연구 결과는 센서 하드웨어 보안에 대한 기초적인 이해를 제공하며, 보다 안전한 센서와 CPS 구축을 목표로 하는 센서 설계자, 보안 연구자, 시스템 개발자를 위한 안내와 미래 방향성을 제시한다.

Abstract: Sensors are fundamental to cyber-physical systems (CPS), enabling perception
and control by transducing physical stimuli into digital measurements. However,
despite growing research on physical attacks on sensors, our understanding of
sensor hardware vulnerabilities remains fragmented due to the ad-hoc nature of
this field. Moreover, the infinite attack signal space further complicates
threat abstraction and defense. To address this gap, we propose a
systematization framework, termed sensor out-of-band (OOB) vulnerabilities,
that for the first time provides a comprehensive abstraction for sensor attack
surfaces based on underlying physical principles. We adopt a bottom-up
systematization methodology that analyzes OOB vulnerabilities across three
levels. At the component level, we identify the physical principles and
limitations that contribute to OOB vulnerabilities. At the sensor level, we
categorize known attacks and evaluate their practicality. At the system level,
we analyze how CPS features such as sensor fusion, closed-loop control, and
intelligent perception impact the exposure and mitigation of OOB threats. Our
findings offer a foundational understanding of sensor hardware security and
provide guidance and future directions for sensor designers, security
researchers, and system developers aiming to build more secure sensors and CPS.

</details>


### [18] [Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks](https://arxiv.org/abs/2508.16150)
*Aristeidis Sidiropoulos,Christos Chrysanthos Nikolaidis,Theodoros Tsiolakis,Nikolaos Pavlidis,Vasilis Perifanis,Pavlos S. Efraimidis*

Main category: cs.CR

TL;DR: 이 연구는 기계 학습 모델에 대한 회원 추론 공격(MIA)의 취약성을 기계 학습 제거(Machine Unlearning) 알고리즘 적용 후 평가한다.


<details>
  <summary>Details</summary>
Motivation: MIA는 특정 데이터 포인트가 모델의 훈련 데이터셋에 포함되었는지 여부를 결정할 수 있어 상당한 프라이버시 위험을 초래한다.

Method: 최신 기계 학습 제거 알고리즘을 적용한 후 모델에 대한 MIA의 취약성을 체계적으로 평가한다.

Result: 기계 학습 제거는 MIA에 대한 근본적인 대책이 아니지만, 제거 알고리즘과 데이터 특성이 모델의 취약성에 중요한 영향을 미친다는 것을 발견했다.

Conclusion: 이 연구는 기계 학습 제거와 MIA 간의 상호작용에 대한 중요한 통찰을 제공하고, 프라이버시를 보존하는 기계 학습 시스템 설계에 대한 지침을 제시한다.

Abstract: Membership Inference Attacks (MIAs) pose a significant privacy risk, as they
enable adversaries to determine whether a specific data point was included in
the training dataset of a model. While Machine Unlearning is primarily designed
as a privacy mechanism to efficiently remove private data from a machine
learning model without the need for full retraining, its impact on the
susceptibility of models to MIA remains an open question. In this study, we
systematically assess the vulnerability of models to MIA after applying
state-of-art Machine Unlearning algorithms. Our analysis spans four diverse
datasets (two from the image domain and two in tabular format), exploring how
different unlearning approaches influence the exposure of models to membership
inference. The findings highlight that while Machine Unlearning is not
inherently a countermeasure against MIA, the unlearning algorithm and data
characteristics can significantly affect a model's vulnerability. This work
provides essential insights into the interplay between Machine Unlearning and
MIAs, offering guidance for the design of privacy-preserving machine learning
systems.

</details>


### [19] [A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems](https://arxiv.org/abs/2508.16189)
*Aparna Singh,Geetanjali Rathee,Chaker Abdelaziz Kerrache,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: 본 논문은 이종의 지리적으로 분산된 환경에서의 안전하고 효과적인 데이터 공유를 위한 새로운 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 지능형 교통 시스템(ITS)의 매우 높은 성장으로 인해 안전하고 효과적이며 문맥 인식적인 데이터 공유 메커니즘에 대한 긴급한 요구가 발생했다.

Method: 릴레이 체인 기반 암호화 시스템과 수정된 암호문 정책 속성 기반 암호화(CP-ABE) 방식을 결합한 새로운 아키텍처를 제안한다.

Result: 본 모델은 데이터의 속성에 따라 적절한 암호화 정책 수준을 지정하는 문맥 인식 스마트 계약을 제안하며, 이를 통해 데이터 암호화와 저장을 최적화한다.

Conclusion: 이 분산형, 확장 가능한 모델은 실시간 응답성과 보안 간의 적절한 균형을 제공하며, 다중 관할권 도메인에서 작동하는 차세대 차량 네트워크에 매우 적합하다.

Abstract: The very high growth of Intelligent Transportation Systems (ITS) has
generated an urgent requirement for secure, effective, and context-aware data
sharing mechanisms, especially over heterogeneous and geographically dispersed
settings. This work suggests a new architecture that combines a relay
chain-driven encryption system with a modified Ciphertext-Policy
Attribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of
dynamic access and low-latency communication. The model proposes a
context-aware smart contract on a worldwide relay chain that checks against
data properties, including event type, time, and geographical region, to
specify the suitable level of encryption policy. From such relay-directed
judgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and
store ciphertext inside localised regional blockchains, preventing dependence
on symmetric encryption or off-chain storage. High-sensitivity events are
secured with firm, multi-attribute access rules, whereas common updates use
light policies to help reduce processing burdens. The crypto system also adds
traceability and low-latency revocation, with global enforcement managed
through the relay chain. This distributed, scalable model provides a proper
balance between responsiveness in real time and security and is extremely apt
for next-gen vehicular networks that function across multi-jurisdictional
domains.

</details>


### [20] [How to Beat Nakamoto in the Race](https://arxiv.org/abs/2508.16202)
*Shu-Jie Cao,Dongning Guo*

Main category: cs.CR

TL;DR: 이 논문은 유한 네트워크 지연 하에서의 작업 증명 나카모토 합의에 대해 연구하며, 블록 안전성 공격과 관련된 두 가지 질문을 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 블록체인 보안에서 블록 안전성을 공격하는 최적의 방법과 그로 인해 발생하는 안전성 위반 확률을 이해하는 것입니다.

Method: 마르코프 결정 프로세스(MDP) 프레임워크를 도입하여 시스템 상태, 적의 행동 및 상태 전이 등을 정확하게 모델링합니다.

Result: 최적의 공격 방법인 '미끼-전환'을 제안하고, 나카모토보다 빠른 공격으로 블록 안전성 위반 확률을 극대화하는 것을 증명하였습니다.

Conclusion: 마르코프 체인 분석을 통해 확인 깊이에 따른 안전성 위반 확률을 계산하여 네트워크 지연, 확인 규칙 및 블록체인 보안 간의 상호작용에 대한 새로운 통찰을 제공합니다.

Abstract: This paper studies proof-of-work Nakamoto consensus under bounded network
delays, settling two long-standing questions in blockchain security: How can an
adversary most effectively attack block safety under a given block confirmation
latency? And what is the resulting probability of safety violation? A Markov
decision process (MDP) framework is introduced to precise characterize the
system state (including the tree and timings of all blocks mined), the
adversary's potential actions, and the state transitions due to the adversarial
action and the random block arrival processes. An optimal attack, called
bait-and-switch, is proposed and proved to maximize the adversary's chance of
violating block safety by "beating Nakamoto in the race". The exact probability
of this violation is calculated for any confirmation depth using Markov chain
analysis, offering fresh insights into the interplay of network delay,
confirmation rules, and blockchain security.

</details>


### [21] [Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs](https://arxiv.org/abs/2508.16347)
*Yu Yan,Sheng Sun,Zhe Wang,Yijun Lin,Zenghao Duan,zhifei zheng,Min Liu,Zhiyi yin,Jianping Zhang*

Main category: cs.CR

TL;DR: 이 연구는 대형 언어 모델(LLM)의 탈옥 공격에 대한 취약성을 분석하고, 위험한 지식 보유와 해로운 작업 계획 유틸리티에 대한 평가의 불일치를 밝혀낸다.


<details>
  <summary>Details</summary>
Motivation: LLM의 안전성을 향상시키기 위한 연구가 진행되고 있지만, LLM이 실제 범죄를 다룰 수 있는 진정한 지식을 내재화했는지 불확실하다.

Method: 탈옥 기술의 사용을 분리하여, 위험한 지식 보유, 해로운 작업 계획 유틸리티 및 해로움 판단의 강건성을 조사하는 지식 집약적 Q&A를 구성하였다.

Result: 실험 결과, LLM의 탈옥 성공률과 위험한 지식 보유 간에 불일치가 있으며, 기존 LLM을 판단자로 사용하는 프레임워크는 해로움 판단을 독성 언어 패턴에 고착시키는 경향이 있다.

Conclusion: 이 연구는 기존 LLM 안전성 평가와 실제 위협 가능성 간의 격차를 드러낸다.

Abstract: With the development of Large Language Models (LLMs), numerous efforts have
revealed their vulnerabilities to jailbreak attacks. Although these studies
have driven the progress in LLMs' safety alignment, it remains unclear whether
LLMs have internalized authentic knowledge to deal with real-world crimes, or
are merely forced to simulate toxic language patterns. This ambiguity raises
concerns that jailbreak success is often attributable to a hallucination loop
between jailbroken LLM and judger LLM. By decoupling the use of jailbreak
techniques, we construct knowledge-intensive Q\&A to investigate the misuse
threats of LLMs in terms of dangerous knowledge possession, harmful task
planning utility, and harmfulness judgment robustness. Experiments reveal a
mismatch between jailbreak success rates and harmful knowledge possession in
LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness
judgments on toxic language patterns. Our study reveals a gap between existing
LLM safety assessments and real-world threat potential.

</details>


### [22] [Temperature-Resilient Reconfigurable PUF with Dual-Pulse Modulation based on SOT-MRAM Chip](https://arxiv.org/abs/2508.16405)
*Min Wang,Chuanpeng Jiang,Zhaohao Wang,Zhengyi Hou,Zhongkui Zhang,Yuanfu Zhao,Hongxi Liu,Weisheng Zhao*

Main category: cs.CR

TL;DR: 이 논문에서는 IoT 환경에서 PUF 기반의 실시간 재구성이 가능한 보안 솔루션을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: IoT 시대에서 하드웨어 기반 보안 솔루션의 필요성이 증가하고 있습니다.

Method: SOT-MRAM 캐리어를 기반으로 하는 이중 펄스 재구성 전략을 제안합니다.

Result: 제안된 방법이 산업용 작동 온도 범위에서 실시간 재구성을 달성함을 보여줍니다.

Conclusion: 제안된 SOT-MRAM rPUF 디자인은 차세대 IoT 보호 아키텍처의 기초를 제공합니다.

Abstract: In the Internet of Things (IoT) era, hardware-based security solutions have
become an emerging choice for enhancing end-terminal information security. As
one of the hardware technologies, physical unclonable functions (PUFs) utilize
the inherent variations in the manufacturing process to generate cryptographic
keys. Reconfigurable PUFs (rPUFs), characterized by updating cryptographic
keys, offer enhanced security ability for protecting massive amounts of data in
dynamic operational scenarios. The core challenge lies in achieving real-time
reconfiguration independent of environmental conditions, particularly operating
temperature, which has rarely been investigated and addressed. In this study,
we propose a dual-pulse reconfiguration strategy based on SOT-MRAM carriers,
which effectively widens the operating window and exhibits excellent PUF
metrics. Experimental results demonstrate that our design achieves real-time
reconfiguration across industrial-grade operating temperature ranges, without
the need for dynamic feedback of real-time temperature. The proposed SOT-MRAM
rPUF design lays a solid foundation for next-generation IoT protection
architectures.

</details>


### [23] [Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models](https://arxiv.org/abs/2508.16406)
*Guangyu Yang,Jinghong Chen,Jingbiao Mei,Weizhe Lin,Bill Byrne*

Main category: cs.CR

TL;DR: RAD는 jailbreak 탐지를 위한 새로운 프레임워크로, 알려진 공격 사례 데이터베이스를 활용해 안전성과 유용성의 균형을 맞추며, 훈련 없이 새로운 공격 전략에 대응할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 탈옥 공격에 취약하며, 이러한 공격의 진화와 다양성은 방어 시스템에 많은 도전을 안겨준다.

Method: RAD는 기존의 공격 사례 데이터베이스를 Retrieval-Augmented Generation에 통합하여 악의적인 사용자 쿼리와 공격 전략을 추론하는 시스템이다.

Result: 실험 결과 RAD는 악성 탈옥 공격의 효과를 크게 줄이고, 무해한 쿼리에 대해 낮은 거부율을 유지한다.

Conclusion: RAD는 다양한 운영 지점에서 안전성과 유용성의 균형을 잘 맞춘다.

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which
attempt to elicit harmful responses from LLMs. The evolving nature and
diversity of these attacks pose many challenges for defense systems, including
(1) adaptation to counter emerging attack strategies without costly retraining,
and (2) control of the trade-off between safety and utility. To address these
challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for
jailbreak detection that incorporates a database of known attack examples into
Retrieval-Augmented Generation, which is used to infer the underlying,
malicious user query and jailbreak strategy used to attack the system. RAD
enables training-free updates for newly discovered jailbreak strategies and
provides a mechanism to balance safety and utility. Experiments on StrongREJECT
show that RAD substantially reduces the effectiveness of strong jailbreak
attacks such as PAP and PAIR while maintaining low rejection rates for benign
queries. We propose a novel evaluation scheme and show that RAD achieves a
robust safety-utility trade-off across a range of operating points in a
controllable manner.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: Z-Pruner는 재학습 없이 사전 훈련된 대형 언어 모델의 크기와 추론 지연을 줄이는 혁신적인 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 크기 증가로 인한 배포, 확장성 및 에너지 효율성 문제를 해결하기 위해.

Method: Z-Pruner는 재훈련 없이 사전 훈련된 모델에서 희소성을 유도하기 위해 가중치 업데이트 크기와 활성화 패턴을 활용한다.

Result: Z-Pruner는 최신 프루닝 방법보다 낮은 퍼플렉시티 점수와 높은 제로샷 정확도의 평균 점수를 기록한다.

Conclusion: Z-Pruner는 모델에 구애 받지 않으며 효율적이고 구현이 용이하다.

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [25] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net은 효율적이고 해석 가능한 다중 모드 감정 분석을 위한 새로운 딥 러닝 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 다중 모드 데이터에서 감정 분석을 수행하기 위한 높은 효율성과 해설 가능성을 제공하기 위해 개발되었다.

Method: 진화된 레이어 내부 융합, 적응형 게이트 중재 메커니즘, 하이브리드 매개변수 효율적 미세 조정 전략을 포함한다.

Result: MOSI 데이터셋에서 평균 절대 오차(MAE) 0.691 및 F1-스코어 86.9%를 기록하며, 3.09M의 훈련 가능한 매개변수만으로 뛰어난 성능과 계산 효율성을 보여준다.

Conclusion: PGF-Net은 감정 분석의 해석 가능성을 유지하면서도 뛰어난 성능을 발휘할 수 있도록 설계되었다.

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [26] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: 본 연구는 ECG 신호 분할을 위한 간소화된 아키텍처를 소개하며, 이는 높은 정확도를 유지하면서 계산 효율성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존 ECG 분할 모델은 복잡하고 비효율적인 다층 아키텍처에 의존하고 있습니다.

Method: 스펙트럼 분석과 확률적 예측을 결합한 간소화된 아키텍처를 사용하여 P, QRS, T파의 시간적 및 스펙트럼적 특성을 캡처합니다.

Result: QRS파 97.00%, T파 93.33%, P파 96.07%의 높은 분할 정확도를 달성하였습니다.

Conclusion: 이 방식은 계산 효율성을 개선하면서도 ECG 신호 모니터링을 위한 실용적이고 효과적인 솔루션을 제공합니다.

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [27] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: 이 논문에서는 다중 헤드 잠재 주의(Multi-Head Latent Attention, MLA)의 한계점을 극복하기 위해 텐서 병렬 잠재 주의(Tensor-Parallel Latent Attention, TPLA)라는 새로운 스킴을 제안하며, 이로 인해 메모리 사용을 줄이고 성능을 향상시킬 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 장치에 걸친 텐서 병렬 처리에서 MLA의 메모리 이점을 잃지 않으면서 효율성을 높이기 위해 TPLA를 개발해야 했다.

Method: TPLA는 잠재 표현과 각 헤드의 입력 차원을 장치 간에 분할하고, 각 조각에서 독립적으로 주의를 수행한 후 결과를 모두 재조합하는 방식이다.

Result: DeepSeek-V3와 Kimi-K2에 대해 각각 1.79배와 1.93배의 속도 향상을 달성하면서도 성능을 유지하였다.

Conclusion: TPLA는 FlashAttention-3와 호환되어 실제적인 end-to-end 가속을 가능하게 한다.

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [28] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: 본 연구에서는 복잡한 비선형 종속성과 비실제적 상관관계를 해결하기 위한 새로운 시간적 인과 관계 발견 및 추론 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 비선형적 시간적 관계와 비실제적 상관관계라는 두 가지 주요 과제를 해결하기 위해 이 연구를 진행하였습니다.

Method: 다층 Transformer 기반의 시계열 예측기를 사용하여 변수 간의 장기적 비선형 시간 관계를 포착하고, 기울기 기반 분석을 통해 근본적인 인과 구조와 관련 시간 지연을 추출합니다. 또한, 주의 마스킹을 기반으로 한 사전 지식 통합 메커니즘을 도입하여 비실제적 인과 관계의 영향을 완화합니다.

Result: 우리의 방법은 인과 관계 발견에서 F1-score가 12.8% 향상되었고, 인과 지연 추정에서 98.9%의 정확도를 달성함으로써 다른 최첨단 접근법을 상당히 초월했습니다.

Conclusion: 이 연구에서 제안한 프레임워크는 복잡한 인과 관계를 효율적으로 처리하고, 실험적으로도 그 우수성을 입증하였습니다.

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [29] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Emőke-Ágnes Horvát,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poličar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: 고차원 데이터의 저차원 표현을 위한 알고리즘의 수요가 증가하고 있으며, 이 리뷰는 최신 발전을 종합적으로 정리하고 최선의 기법을 도출합니다.


<details>
  <summary>Details</summary>
Motivation: 고차원 데이터의 시각화, 탐색 및 분석을 위한 저차원 표현 방법의 필요성 증가.

Method: 최신 발전에 대한 상세한 개요 제공, 최선의 기법 목록 도출, 다양한 데이터셋에 대한 인기 접근 방식 평가.

Result: 최신 저차원 임베딩 기법에 대한 종합적 이해를 도울 수 있는 자료 제공.

Conclusion: 여전히 남아있는 도전 과제와 미해결 문제에 대한 논의.

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [30] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,María C. V. Nascimento*

Main category: cs.LG

TL;DR: 이 논문은 메타휴리스틱과 그래프 표현 학습(GRL)을 결합하여 그래프의 잠재적 구조를 추출하는 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 최근 메타휴리스틱과 기계 학습 기법의 혼합에 대한 관심이 커지고 있으며, 특히 GRASP 휴리스틱 방법의 성능을 향상시키고자 하는 필요성이 있다.

Method: 메타휴리스틱과 저비용의 학습 전략을 결합하여 GRASP의 구축 단계에 GRL을 통합하는 Graph Learning GRASP (GL-GRASP)을 제안한다.

Result: 다양한 노드 임베딩 기법을 고려한 컴퓨팅 실험에서 GL-GRASP 휴리스틱이 기존 GRASP 휴리스틱보다 우수한 성능을 보였다.

Conclusion: GL-GRASP 휴리스틱은 고정 시간 한도 내에서 새로운 밀도가 높은 인스턴스에서도 강력한 성능을 입증하였다.

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [31] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: 기차의 휠 플랜지 마모 깊이를 모니터링하기 위한 혁신적인 온보드 측정 시스템을 소개하며, 머신 러닝 알고리즘을 사용하여 정밀한 안전 감시를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 철도 시스템 안전성을 위해 철도와 휠의 상호작용 기능이 중요하다.

Method: 변위 및 온도 센서를 활용하여 휠 플랜지 마모 깊이를 측정하는 혁신적인 온보드 측정 시스템을 설계하고, 실험실에서 휠 플랜지 마모 깊이와 온도 변동을 재현하는 실험을 진행한다.

Result: 머신 러닝 알고리즘을 통해 96.5%의 정확도로 온도 영향을 상쇄하며, IIR 필터를 통해 실시간 노이즈 저감으로 98.2%의 정확도를 달성한다.

Conclusion: 이 시스템은 IoT 장치와 통합되어 기차 운영의 안전성과 효율성을 높이는 데 기여한다.

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [32] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: 이 논문에서는 보상 벡터가 주어진 선호 원에 따라 정렬될 때, 분포 이동 하에서의 맥락적 밴딧 학습을 고려합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 맥락적 밴딧 문제에서의 성과를 개선하고 분포 변화에 능동적으로 적응하는 방식을 찾기 위함입니다.

Method: 적응적 이산화 및 낙관적 제거 기반 정책을 제안하고, 특정 선호 원에 대해 자가 조정하여 분포 이동에 적응하도록 합니다.

Result: 이 정책의 성능을 선언하며, 다양한 분포 이동의 성격에 대한 가정 하에서 낙관적 격차의 상한을 설정합니다.

Conclusion: 우리의 낙관적 격차 경계는 기존의 분포 이동 없는 경우에 대한 결과를 일반화하고, 분포 이동의 존재 하에서도 문제 매개변수에 따라 우아하게 스케일됩니다.

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [33] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 이 연구는 심층 생태계 전파(EP) 네트워크의 기울기 소실 문제를 해결하기 위해 새로운 EP 프레임워크를 제안하고, 이를 통해 깊은 네트워크 교육이 가능하다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존 EP 연구는 얕은 구조에 한정되었으며, 깊은 네트워크는 기울기 소실 문제로 인해 어려움을 겪고 있었다.

Method: 중간 오류 신호를 통합하여 신경 동역학의 정보 흐름과 수렴성을 향상시키는 새로운 EP 프레임워크를 제안한다.

Result: 제안한 접근 방식은 CIFAR-10 및 CIFAR-100 데이터셋에서 최신 성능을 달성하며, 깊은 VGG 아키텍처에서의 확장성을 보여준다.

Conclusion: 이 결과는 EP의 확장성에서 중요한 진전을 나타내며, 실제 시스템에서의 응용 가능성을 열어준다.

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [34] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: 양자 연합 학습(QFL)은 분산 양자 컴퓨팅과 연합 머신 러닝을 결합하여 개인 정보를 보호하면서 양자 향상 기능을 갖춘 분산 학습을 가능하게 하는 유망한 접근법이다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨팅과 연합 학습의 통합 배경 및 동기를 논의하여 이들의 작동 원리를 강조한다.

Method: QFL의 핵심 개념, 기초, 응용 및 새로운 과제를 포괄적으로 조사한다.

Result: QFL의 기초, 분류, 아키텍처, 통신 방식, 최적화 기술, 보안 메커니즘 및 여러 도메인에서의 응용을 탐구한다.

Conclusion: 현재의 도전 과제를 규명하고 향후 연구 가능성을 개략적으로 제시한다.

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [35] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fré,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: 이 논문에서는 Cartan 신경망 프로그램의 발전을 계속하며, 다음 단계에 필요한 수학적 기초 요소에 초점을 맞추고 있다.


<details>
  <summary>Details</summary>
Motivation: Cartan 신경망 프로그램의 다음 단계로 나아가기 위한 수학적 기초 요소가 필요하다.

Method: 비복합 대칭 공간으로 수학적으로 모델링된 층들을 도입하며, 이를 해결 가능한 군 동형사상으로 서로 연결한다.

Result: 모든 비복합 대칭 공간에 대한 분리자의 군 이론적 구조를 제시하였고, 일정한 결과와 부분 결과를 도출하였다.

Conclusion: Riemann 곡면에서 라플라시 고유함수를 얻기 위한 새로운 전략을 제안하였다.

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [36] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: PAC-MCoFL은 다중 서비스 제공자 생태계에서 비협조적 동적 환경을 극복하기 위한 게임 이론적 다중 에이전트 강화 학습 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 다중 서비스 제공자 생태계에서 개인 정보 보호 제약과 경쟁 이익으로 인해 중앙 집중식 최적화가 어려워지므로, 이러한 문제를 해결하기 위해 PAC-MCoFL을 제안합니다.

Method: PAC-MCoFL은 Pareto Actor-Critic 원칙과 기대 회귀를 통합하여 에이전트가 최적의 공동 정책을 추정하도록 하며, 고차원 동작 공간을 관리하기 위해 삼원 카르테시안 분해(TCAD) 메커니즘을 개발합니다.

Result: PAC-MCoFL은 최신 다중 에이전트 강화 학습 솔루션에 비해 총 보상과 하이퍼부피 지표에서 각각 약 5.8%와 4.2% 개선을 달성하며, 이론적 수렴 보장도 제공됩니다.

Conclusion: PAC-MCoFL은 다양한 데이터 이질성 하에서도 개별 서비스 제공자와 시스템 성능 사이의 균형을 보다 효과적으로 유지할 수 있음을 보여줍니다.

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [37] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: 본 논문은 시간에 따라 변화하는 클래스 조건 분포를 다루기 위해 비정상 선형 및 비정상 이차 판별 분석을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 시간에 따라 변화하는 분포로 인해 기존의 정형 판별 분석 기법들이 신뢰할 수 없게 됩니다.

Method: 상태 공간 모델 내에 판별 분석을 포함시켜 비정상 선형 판별 분석(NSLDA) 및 비정상 이차 판별 분석(NSQDA)을 수행합니다.

Result: 정형 판별 분석(LDA), 이차 판별 분석(QDA), 서포트 벡터 머신(SVM)과 비교하여 일관된 성능 개선을 보였습니다.

Conclusion: 이 논문은 시간적 분포 변화 아래에서의 판별 분석을 위한 통합적이고 데이터 효율적인 기초를 제공합니다.

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [38] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 작업 산술은 여러 개의 미세 조정된 모델을 하나로 결합하도록 도와주는 강력한 기술이다. 그러나 이 기법의 이론적 기초는 부족하다. 이 논문은 작업 벡터와 작업 손실의 기울기 간의 연결을 수립하여 작업 산술의 이론적 근거를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 작업 산술 기술의 성공적인 적용에도 불구하고 이 기법이 언제 그리고 왜 효과적인지에 대한 이론적 설명이 부족하다.

Method: 표준 경량 하강법 하에서 미세 조정된 한 에폭에서 생성된 작업 벡터가 손실의 음의 기울기와 학습률로 스케일된 동등한 관계에 있다는 것을 보여준다. 또한, 다중 에폭 환경에서 이러한 동등성이 대략적으로 유지됨을 입증한다.

Result: 본 논문은 7개의 비전 벤치마크에서 우리의 이론을 실증적으로 분석하여, 첫 번째 에폭의 기울기가 미세 조정 경로에서 지배적인 역할을 한다는 것을 보여준다.

Conclusion: 단일 에폭만 조정된 모델을 병합하면 완전히 수렴된 모델을 병합한 것과 유사한 성능을 발휘할 수 있다는 중요한 의미를 도출했다.

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [39] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: 유전자 프로그래밍을 활용하여 교통 신호 제어 전략을 자동으로 개발하는 학습 기반 접근 방식이 성공을 거두었으나, 기존 방법은 다양한 신호 단계의 공통적인 트래픽 특징을 일관되게 다루지 못했다. 본 연구에서는 대칭적 단계 긴급성 함수를 이용해 현재 도로 조건에 따른 긴급성을 계산하고, 이를 기반으로 유전자 프로그래밍 방법을 제안하여 효과적인 교통 신호 제어 정책을 발전시켰다.


<details>
  <summary>Details</summary>
Motivation: 기존의 유전자 프로그래밍 기반 방법이 교통 신호 단계 간의 공통 속성을 일관성 있게 처리하지 못하는 문제를 해결하고자 한다.

Method: 대칭적 단계 긴급성 함수를 사용하여 특정 단계에 대한 긴급성을 계산하고, 이를 진화시키기 위해 유전자 프로그래밍 방법을 제안한다.

Result: 대칭 긴급성 함수 표현이 전통적인 유전자 프로그래밍 표현보다 더 많은 시나리오에서 교통 신호 제어 정책의 성능을 현저하게 향상시킨다는 것을 실험 결과로 입증하였다.

Conclusion: 제안된 방법이 효과적이고 이해하기 쉬우며 쉽게 배포 가능한 교통 신호 제어 정책을 발전시킬 수 있음을 보여주었다.

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [40] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayyüce Begüm Bektaş,Mithat Gönen*

Main category: cs.LG

TL;DR: 본 논문은 의료와 같은 고위험 분야에 배치된 머신러닝 모델이 해석 가능하고, 공유 가능하며, 재현 가능하고, 책임이 있어야 한다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: 의료 데이터와 같은 중요한 데이터를 다루는 머신러닝 알고리즘의 설계 기준을 정립하기 위해 이 원칙들이 필요하다.

Method: 해석 가능한 모델 개발 접근법(스파스성을 가진 커널 방법, 프로토타입 기반 학습, 딥 커널 모델 등)을 제안한다.

Result: 의료 결정 지원의 신뢰성을 보장하기 위해 모델 평가 및 불확실성 정량화의 중요성을 강조한다.

Conclusion: 의료 AI를 개발하기 위해 머신러닝의 기초를 재사고해야 한다.

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [41] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: 본 논문에서는 커먼KV(CommonKV)라는 훈련이 필요 없는 방법을 제안하여 크로스 레이어 KV 캐시 압축을 수행하고, 이것이 기존 방법보다 더 나은 성능을 보임을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 증가하는 시퀀스 길이에 따라 KV 캐시의 메모리 문제에 직면하고 있으며, 기존 방법은 모델 구조 수정이나 성능 저하의 문제를 안고 있다.

Method: 커먼KV는 인접한 파라미터 공유를 통해 크로스 레이어 KV 캐시 압축을 수행하며, 특잇값 분해(SVD) 기법을 활용하여 파라미터 간 가중치를 공유한다. 또한 코사인 유사성을 기반으로 동적으로 압축 예산을 할당하는 전략을 도입한다.

Result: 다양한 백본 모델 및 LongBench, Ruler와 같은 벤치마크에서 실험을 통해, 제안된 방법이 다양한 압축 비율에서 기존 저순위 및 크로스 레이어 접근 방식을 일관되게 초월함을 확인하였다.

Conclusion: 커먼KV의 혜택은 다른 양자화 및 퇴출 방법과도 직교적이며, 이 접근법들을 통합함으로써 성능 손실 없이 98 압축 비율을 달성할 수 있다.

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [42] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: 마이크로모빌리티 시스템의 기계 학습 응용에 대한 포괄적인 문헌 조사.


<details>
  <summary>Details</summary>
Motivation: 마이크로모빌리티 시스템은 도시 교통의 중요한 부분이며, 그 효율성과 안전 문제를 해결하기 위해 기계 학습의 적용이 필요하다.

Method: 다양한 마이크로모빌리티 관련 데이터셋을 수집 및 분석하고, 기계 학습 모델의 장점과 도전 과제를 논의한다.

Result: 기계 학습 기술이 마이크로모빌리티의 수요 예측, 에너지 관리 및 안전성 향상에 대해 활용되는 다양한 사례를 제시한다.

Conclusion: 미래 연구 방향을 제안하여 연구자들이 마이크로모빌리티 분야를 더 잘 이해할 수 있도록 돕는다.

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [43] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: 새로운 학습 패러다임을 소개하며, 이는 대형 언어 모델의 세부 조정 없이 적응형 에이전트를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 기존 방식은 정적이거나 계산 집약적이며, 이에 대한 대안을 제시하고자 한다.

Method: 메모리 기반의 온라인 강화 학습을 통해 비용 효율적인 지속적 적응을 가능하게 한다.

Result: AgentFly 모델은 GAIA 검증에서 최고의 성과를 달성하고, 여러 데이터셋에서 기존 방법을 초월하는 성능을 보인다.

Conclusion: 이 접근법은 경량화된 학습을 통해 일반화된 LLM 에이전트를 발전시킬 수 있는 효율적인 경로를 제공한다.

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [44] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: 이 논문에서는 결정론적 샘플러의 잠재적 한계를 다루며, 새로운 지표를 도입하여 샘플링된 데이터가 국소 데이터 공간에 지나치게 집중되는 붕괴 오류를 식별합니다.


<details>
  <summary>Details</summary>
Motivation: 결정론적 샘플러의 잠재적 한계가 충분히 탐구되지 않았기 때문에, 이에 대한 연구가 필요합니다.

Method: 붕괴 오류를 정량화하기 위해 새로운 지표를 도입하고, 다양한 환경에서 발생하는 붕괴 오류를 탐구합니다.

Result: 저소음 범위에서의 스코어 학습이 고소음 범위에서 부정적인 영향을 미치는 상반된 효과를 관찰했습니다.

Conclusion: 이 연구는 ODE 기반 확산 샘플링에서 붕괴 오류에 대한 강력한 경험적 증거를 제공하며, 스코어 학습과 결정론적 샘플링 간의 상호작용에 대해 추가 연구의 필요성을 강조합니다.

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [45] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: STA-GANN은 불완전한 시공간 데이터를 처리하고, 시공간 패턴의 유효성과 일반화를 향상시키는 새로운 GNN 기반의 크리깅 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 불완전한 센서로 인한 시공간 데이터의 결측 값을 보완하고, 동적 공간 종속성과 시간 변화를 포착하는 방법이 필요하다.

Method: 시차 모듈, 데이터 기반 메타데이터 그래프 모델링 및 적대적 전이 학습 전략을 통합한 STA-GANN을 제안한다.

Result: 아홉 개의 데이터 세트에서 우수한 성능을 검증하였다.

Conclusion: STA-GANN은 시공간 패턴의 유효성과 일반성을 개선하는 데 효과적이다.

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [46] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: 이 논문은 대규모 이웃 탐색(LNS) 방법을 개선하기 위한 SPL-LNS라는 샘플링 강화 신경망 LNS 솔버를 제안하며, 실험 결과 SPL-LNS가 다양한 크기의 ILP 문제에서 기존 솔버를 크게 능가함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 LNS 솔버의 탐욕적 접근 방식이 지역 최적에 의해 제한되는 문제와 장기적인 샘플 효율성을 향상시키는 방법을 다루기 위해.

Method: LNS를 확률적 과정으로 정식화하고, 지역 정보에 기초한 제안을 활용하여 지역 최적을 벗어날 수 있는 SPL-LNS라는 샘플링 강화 신경 LNS 솔버를 소개.

Result: SPL-LNS는 다양한 크기의 ILP 문제에서 이전의 신경 LNS 솔버들을 상당히 초월하는 성능을 보였다.

Conclusion: SPL-LNS는 기본적인 LNS 솔루션의 한계를 극복하고 샘플 효율성을 개선하는 데 기여할 수 있다.

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [47] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: 이 논문은 EEG 모터 이미지를 분류하기 위한 새로운 방법을 제안하고, MiniRocket을 사용하여 높은 분류 정확도를 달성하는 것을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: EEG 신호의 비정상성, 시간 변화 및 개인적 다양성으로 인해 모터 이미지를 기반으로 한 전기뇌활동검사(MI-EEG) 작업의 분류는 큰 도전과제를 나타냅니다.

Method: MiniRocket이라는 최소한의 무작위 합성곱 커널 변환과 선형 분류기를 사용하여 EEG 모터 이미지 신호의 특징을 효율적으로 추출하고 이를 통해 활동 인식을 수행합니다.

Result: 제안된 모델은 MiniRocket에 대해 평균 정확도 98.63%, CNN-LSTM에 대해 98.06%의 성능을 달성했습니다.

Conclusion: 제안된 접근 방식은 모터 이미지 EEG의 정확성을 크게 향상시킬 수 있으며 MI-EEG의 특징 추출 및 분류에 대한 새로운 통찰력을 제공합니다.

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [48] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM은 사전 훈련된 모델의 파라미터를 상대적 규모에 맞추어 적절히 조정하여, 효율적인 미세 조정을 가능하게 하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 사전 훈련된 대형 모델을 새로운 작업에 적응하는 과정에서 파라미터를 효율적으로 조정할 필요성이 증가하고 있다.

Method: Gradient-to-Weight Ratio 및 Entropy-guided Masking(GEM)을 제안하며, 이는 파라미터의 상대적 규모를 인식하여 조정하는 미세 조정 프레임워크이다.

Result: GEM은 GLUE, SuperGLUE와 같은 일반 도메인 작업과 GSM8k, MBPP와 같은 도메인 특화 작업에서 효과적임을 입증하였으며, 전체 미세 조정에 비해 최대 1.6%의 정확도 향상을 달성하였다.

Conclusion: GEM은 0.1%의 모델 파라미터만 업데이트하였음에도 불구하고 효율적인 성능 향상을 제공한다.

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


### [49] [UMATO: Bridging Local and Global Structures for Reliable Visual Analytics with Dimensionality Reduction](https://arxiv.org/abs/2508.16227)
*Hyeon Jeon,Kwon Ko,Soohyun Lee,Jake Hyun,Taehyun Yang,Gyehun Go,Jaemin Jo,Jinwook Seo*

Main category: cs.LG

TL;DR: UMATO라는 차원 축소 기법은 고차원 데이터의 지역 및 전역 구조를 효과적으로 포착하여, 기존 기법보다 성능을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 고차원 데이터의 복잡성으로 인해 기존 차원 축소 기법들이 데이터의 모든 구조적 특성을 보존하기 어려움.

Method: UMATO는 UMAP의 최적화 과정을 두 단계로 나누어, 첫 번째 단계에서 대표 점을 사용해 기본 레이아웃을 구축하고, 두 번째 단계에서 나머지 점을 투영하여 지역 특성을 보존합니다.

Result: 정량적 실험 결과, UMATO는 지역 구조에서 약간 손실이 있지만 전역 구조 보존에서 널리 사용되는 차원 축소 기법인 UMAP보다 우수하다는 것이 입증되었습니다.

Conclusion: UMATO는 초기화 및 하위 샘플링에 대한 안정성과 확장성에서 기존 기법보다 더 나은 성능을 보여, 고차원 데이터 분석을 위한 신뢰할 수 있는 도구로 효과적임을 확인했습니다.

Abstract: Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality
reduction (DR) techniques cannot preserve all the structural characteristics of
the original data. Therefore, DR techniques focus on preserving either local
neighborhood structures (local techniques) or global structures such as
pairwise distances between points (global techniques). However, both approaches
can mislead analysts to erroneous conclusions about the overall arrangement of
manifolds in HD data. For example, local techniques may exaggerate the
compactness of individual manifolds, while global techniques may fail to
separate clusters that are well-separated in the original space. In this
research, we provide a deeper insight into Uniform Manifold Approximation with
Two-phase Optimization (UMATO), a DR technique that addresses this problem by
effectively capturing local and global structures. UMATO achieves this by
dividing the optimization process of UMAP into two phases. In the first phase,
it constructs a skeletal layout using representative points, and in the second
phase, it projects the remaining points while preserving the regional
characteristics. Quantitative experiments validate that UMATO outperforms
widely used DR techniques, including UMAP, in terms of global structure
preservation, with a slight loss in local structure. We also confirm that UMATO
outperforms baseline techniques in terms of scalability and stability against
initialization and subsampling, making it more effective for reliable HD data
analysis. Finally, we present a case study and a qualitative demonstration that
highlight UMATO's effectiveness in generating faithful projections, enhancing
the overall reliability of visual analytics using DR.

</details>


### [50] [PIANO: Physics Informed Autoregressive Network](https://arxiv.org/abs/2508.16235)
*Mayank Nagda,Jephte Abijuru,Phil Ostheimer,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: 물리 정보 신경망(PINN)은 시간 의존성 부분 미분 방정식(PDE)을 해결하는 데 제한적입니다. 본 연구에서는 동적 시스템 모델링을 위한 물리 정보 자회귀 네트워크(PIANO)를 소개하며, 이는 더 나은 안정성과 정확성을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 시간 의존성 부분 미분 방정식(PDE)을 해결하는 것은 과학 및 공학 전반에 걸쳐 중요한 현상을 모델링하는 데 필수적입니다.

Method: PIANO는 과거의 데이터에 기반하여 미래 예측을 명시적으로 조건화하는 자회귀 방식으로 작동하며, 물리적 제약을 적용하면서 자기 감독 롤아웃 메커니즘을 통해 훈련됩니다.

Result: PIANO는 자회귀 모델링을 통해 안정성을 달성하였으며, 기존 방법들보다 정확성과 안정성을 크게 향상시킵니다.

Conclusion: PIANO는 기존 방법에 비해 기상 예측에서도 우수한 성능을 보입니다.

Abstract: Solving time-dependent partial differential equations (PDEs) is fundamental
to modeling critical phenomena across science and engineering. Physics-Informed
Neural Networks (PINNs) solve PDEs using deep learning. However, PINNs perform
pointwise predictions that neglect the autoregressive property of dynamical
systems, leading to instabilities and inaccurate predictions. We introduce
Physics-Informed Autoregressive Networks (PIANO) -- a framework that redesigns
PINNs to model dynamical systems. PIANO operates autoregressively, explicitly
conditioning future predictions on the past. It is trained through a
self-supervised rollout mechanism while enforcing physical constraints. We
present a rigorous theoretical analysis demonstrating that PINNs suffer from
temporal instability, while PIANO achieves stability through autoregressive
modeling. Extensive experiments on challenging time-dependent PDEs demonstrate
that PIANO achieves state-of-the-art performance, significantly improving
accuracy and stability over existing methods. We further show that PIANO
outperforms existing methods in weather forecasting.

</details>


### [51] [A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease](https://arxiv.org/abs/2508.16237)
*Patricia Amado-Caballero,Luis M. San-José-Revuelta,Xinheng Wang,José Ramón Garmendia-Leiza,Carlos Alberola-López,Pablo Casaseca-de-la-Higuera*

Main category: cs.LG

TL;DR: 이 연구는 만성 호흡기 질환, 특히 만성 폐쇄성 폐질환(COPD)과 관련된 기침 소리의 스펙트럼 분석을 위한 설명 가능한 인공지능(XAI) 기반 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 만성 호흡기 질환의 진단에 기침 소리의 스펙트럼 분석을 활용하고, 이를 통해 COPD와 다른 호흡기 질환을 구분하는 방법론을 제안하고자 한다.

Method: CNN을 사용하여 기침 신호의 시간-주파수 표현에 대한 학습을 수행하고, 오클루전 맵을 통해 스펙트로그램 내 진단적으로 관련 있는 영역을 식별한다. 이후 강조된 영역은 다섯 개의 주파수 서브밴드로 분해되어 특정 스펙트럼 특징추출 및 분석이 이루어진다.

Result: 스펙트럼 패턴이 서브밴드 및 질환 그룹에 따라 다르며, 주파수 스펙트럼 전반에 걸쳐 보완적이고 보상적인 경향이 드러난다. 또한 이 방법론은 해석 가능한 스펙트럼 마커를 기반으로 COPD와 다른 호흡기 질환, 만성 및 비만성 환자 그룹을 구분할 수 있다.

Conclusion: 이 연구는 기침 음향의 기본 병리 생리학적 특성에 대한 통찰력을 제공하며, 생물 의학 신호 해석 및 호흡기 질환 진단을 위한 주파수 해상도와 XAI 기능 강화 분석의 가치를 보여준다.

Abstract: This paper presents an explainable artificial intelligence (XAI)-based
framework for the spectral analysis of cough sounds associated with chronic
respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary
Disease (COPD). A Convolutional Neural Network (CNN) is trained on
time-frequency representations of cough signals, and occlusion maps are used to
identify diagnostically relevant regions within the spectrograms. These
highlighted areas are subsequently decomposed into five frequency subbands,
enabling targeted spectral feature extraction and analysis. The results reveal
that spectral patterns differ across subbands and disease groups, uncovering
complementary and compensatory trends across the frequency spectrum.
Noteworthy, the approach distinguishes COPD from other respiratory conditions,
and chronic from non-chronic patient groups, based on interpretable spectral
markers. These findings provide insight into the underlying pathophysiological
characteristics of cough acoustics and demonstrate the value of
frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation
and translational respiratory disease diagnostics.

</details>


### [52] [When Simpler Wins: Facebooks Prophet vs LSTM for Air Pollution Forecasting in Data-Constrained Northern Nigeria](https://arxiv.org/abs/2508.16244)
*Habeeb Balogun,Yahaya Zakari*

Main category: cs.LG

TL;DR: 이 연구는 북부 나이지리아에서 대기 오염 예측을 위해 LSTM 네트워크와 Facebook Prophet 모델을 비교하여 모델 데이터를 일치시킬 중요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 대기 오염 예측은 환경 관리에 필수적이며, 그러나 자원이 부족한 지역에서는 데이터의 불규칙성과 부족이 주요한 도전 과제가 된다.

Method: 2018년부터 2023년까지 19개 주의 월간 관측 데이터를 사용하여 여러 오염물질(CO, SO2, SO4)에 대한 예측을 위해 LSTM 네트워크와 Facebook Prophet 모델을 평가한다.

Result: Prophet 모델은 LSTM의 정확성에 종종 일치하거나 초과하며, 특히 계절적 및 장기 추세가 지배적인 시계열에서 그렇고, LSTM은 급격한 구조적 변화가 있는 데이터셋에서 더 잘 수행된다.

Conclusion: 이 연구는 깊은 학습 모델이 단순한 접근 방식보다 본질적으로 더 나은 성능을 발휘한다는 가정에 도전하고, 모델과 데이터의 정렬 중요성을 강조한다.

Abstract: Air pollution forecasting is critical for proactive environmental management,
yet data irregularities and scarcity remain major challenges in low-resource
regions. Northern Nigeria faces high levels of air pollutants, but few studies
have systematically compared the performance of advanced machine learning
models under such constraints. This study evaluates Long Short-Term Memory
(LSTM) networks and the Facebook Prophet model for forecasting multiple
pollutants (CO, SO2, SO4) using monthly observational data from 2018 to 2023
across 19 states. Results show that Prophet often matches or exceeds LSTM's
accuracy, particularly in series dominated by seasonal and long-term trends,
while LSTM performs better in datasets with abrupt structural changes. These
findings challenge the assumption that deep learning models inherently
outperform simpler approaches, highlighting the importance of model-data
alignment. For policymakers and practitioners in resource-constrained settings,
this work supports adopting context-sensitive, computationally efficient
forecasting methods over complexity for its own sake.

</details>


### [53] [FEST: A Unified Framework for Evaluating Synthetic Tabular Data](https://arxiv.org/abs/2508.16254)
*Weijie Niu,Alberto Huertas Celdran,Karoline Siarsky,Burkhard Stiller*

Main category: cs.LG

TL;DR: FEST는 합성 데이터 생성 평가를 위한 체계적인 프레임워크로, 개인 정보 보호와 데이터 유용성 간의 균형을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 합성 데이터 생성 평가에서 개인 정보 보호와 데이터 유용성 간의 균형을 고려한 포괄적인 평가 프레임워크가 필요하다.

Method: FEST는 다양한 개인 정보 보호 메트릭(공격 기반 및 거리 기반)과 유사성 및 머신 러닝 유용성 메트릭을 통합하여 전체적인 평가를 제공한다.

Result: FEST는 다수의 데이터 세트에서 검증되어, 다른 합성 데이터 생성 모델의 개인 정보 보호-유용성 트레이드 오프 분석에 효과적임을 입증했다.

Conclusion: FEST의 소스 코드는 Github에서 이용 가능하다.

Abstract: Synthetic data generation, leveraging generative machine learning techniques,
offers a promising approach to mitigating privacy concerns associated with
real-world data usage. Synthetic data closely resembles real-world data while
maintaining strong privacy guarantees. However, a comprehensive assessment
framework is still missing in the evaluation of synthetic data generation,
especially when considering the balance between privacy preservation and data
utility in synthetic data. This research bridges this gap by proposing FEST, a
systematic framework for evaluating synthetic tabular data. FEST integrates
diverse privacy metrics (attack-based and distance-based), along with
similarity and machine learning utility metrics, to provide a holistic
assessment. We develop FEST as an open-source Python-based library and validate
it on multiple datasets, demonstrating its effectiveness in analyzing the
privacy-utility trade-off of different synthetic data generation models. The
source code of FEST is available on Github.

</details>


### [54] [Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning](https://arxiv.org/abs/2508.16255)
*Andreas Loizou,Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: 데이터 품질 평가를 위한 Data Shapley 접근법인 C-DaSh를 소개하며, 대규모 데이터셋에서 효율적으로 고품질 데이터 튜플을 식별하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 데이터셋의 볼륨과 다양성이 증가함에 따라 데이터 품질 평가의 중요성이 커지고 있다.

Method: C-DaSh는 데이터셋을 관리 가능한 청크로 나누고, 최적화된 부분 집합 선택과 단일 반복 확률적 경량 감소를 사용하여 각각의 청크의 기여도를 추정한다.

Result: C-DaSh는 기존의 Shapley 근사 방식에 비해 계산 효율성과 저품질 데이터 지역 탐지의 정확도에서 월등한 성능을 보여준다.

Conclusion: C-DaSh는 대규모 테이블 데이터셋에서 데이터셋 품질을 실용적으로 측정할 수 있게 해주며, 분류 및 회귀 파이프라인 모두를 지원한다.

Abstract: As the volume and diversity of available datasets continue to increase,
assessing data quality has become crucial for reliable and efficient Machine
Learning analytics. A modern, game-theoretic approach for evaluating data
quality is the notion of Data Shapley which quantifies the value of individual
data points within a dataset. State-of-the-art methods to scale the NP-hard
Shapley computation also face severe challenges when applied to large-scale
datasets, limiting their practical use. In this work, we present a Data Shapley
approach to identify a dataset's high-quality data tuples, Chunked Data Shapley
(C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and
estimates the contribution of each chunk using optimized subset selection and
single-iteration stochastic gradient descent. This approach drastically reduces
computation time while preserving high quality results. We empirically
benchmark our method on diverse real-world classification and regression tasks,
demonstrating that C-DaSh outperforms existing Shapley approximations in both
computational efficiency (achieving speedups between 80x - 2300x) and accuracy
in detecting low-quality data regions. Our method enables practical measurement
of dataset quality on large tabular datasets, supporting both classification
and regression pipelines.

</details>


### [55] [On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View](https://arxiv.org/abs/2508.16261)
*Tao Guo,Junxiao Wang,Fushuo Huo,Laizhong Cui,Song Guo,Jie Gui,Dacheng Tao*

Main category: cs.LG

TL;DR: 본 논문은 연합 학습(FL)을 활용한 대형 언어 모델(LLMs)의 연합 조정에 대한 포괄적인 조사를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자 데이터 프라이버시를 유지하면서 분산 데이터 사일로를 통해 모델을 훈련할 수 있는 연합 학습의 필요성

Method: 모델 접근 기반 및 파라미터 효율성 기반 최적화의 두 축을 따라 기존 연구를 분류하는 분류법을 제안

Result: 화이트박스, 그레이박스, 블랙박스 기법으로 FedLLM 접근 방식을 분류하고 각 카테고리 내 대표적인 방법을 강조

Conclusion: LLMs를 블랙박스 추론 API로 간주하는 새로운 연구 경향과 향후 연구를 위한 유망한 방향 및 열린 도전을 논의

Abstract: Federated Learning (FL) enables training models across decentralized data
silos while preserving client data privacy. Recent research has explored
efficient methods for post-training large language models (LLMs) within FL to
address computational and communication challenges. While existing approaches
often rely on access to LLMs' internal information, which is frequently
restricted in real-world scenarios, an inference-only paradigm (black-box
FedLLM) has emerged to address these limitations. This paper presents a
comprehensive survey on federated tuning for LLMs. We propose a taxonomy
categorizing existing studies along two axes: model access-based and parameter
efficiency-based optimization. We classify FedLLM approaches into white-box,
gray-box, and black-box techniques, highlighting representative methods within
each category. We review emerging research treating LLMs as black-box inference
APIs and discuss promising directions and open challenges for future research.

</details>


### [56] [Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation](https://arxiv.org/abs/2508.16269)
*Yahya Badran,Christine Preisach*

Main category: cs.LG

TL;DR: 이 논문은 지식 추적 모델에 보조 지식 개념을 도입하여 학생 모델링과 적응형 연습 추천을 향상시키는 심층 학습 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 개인화된 추천은 지능형 튜터링 시스템의 핵심 기능으로, 학생의 지식을 정확하게 모델링하는 데 의존합니다.

Method: 우리는 각 비트가 잠재 개념의 존재 여부를 나타내는 희소 이진 표현을 학습하는 심층 학습 모델을 제안합니다.

Result: 보조 지식 개념을 포함함으로써 학생 모델링과 적응형 연습 추천이 개선되었음을 보여줍니다.

Conclusion: 우리는 이 모델이 학생 학습 성과를 개선하는 데 기여한다는 것을 입증합니다.

Abstract: Personalized recommendation is a key feature of intelligent tutoring systems,
typically relying on accurate models of student knowledge. Knowledge Tracing
(KT) models enable this by estimating a student's mastery based on their
historical interactions. Many KT models rely on human-annotated knowledge
concepts (KCs), which tag each exercise with one or more skills or concepts
believed to be necessary for solving it. However, these KCs can be incomplete,
error-prone, or overly general.
  In this paper, we propose a deep learning model that learns sparse binary
representations of exercises, where each bit indicates the presence or absence
of a latent concept. We refer to these representations as auxiliary KCs. These
representations capture conceptual structure beyond human-defined annotations
and are compatible with both classical models (e.g., BKT) and modern deep
learning KT architectures.
  We demonstrate that incorporating auxiliary KCs improves both student
modeling and adaptive exercise recommendation. For student modeling, we show
that augmenting classical models like BKT with auxiliary KCs leads to improved
predictive performance. For recommendation, we show that using auxiliary KCs
enhances both reinforcement learning-based policies and a simple planning-based
method (expectimax), resulting in measurable gains in student learning outcomes
within a simulated student environment.

</details>


### [57] [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
*Jongyeop Hyun,Bumsoo Kim*

Main category: cs.LG

TL;DR: REFINE라는 새로운 피드백 시스템을 제안하여 멀티모달 언어 모델의 오류 분석 및 개선을 체계적으로 지원한다.


<details>
  <summary>Details</summary>
Motivation: 멀티모달 대형 언어 모델에서 오류를 구조적으로 분석하고 완화할 수 있는 체계적인 프레임워크의 필요성이 제기되었다.

Method: REFINE는 세 가지 체계적인 쿼리(Feed-Target, Feed-Check, Feed-Path)를 사용하여 구조화된 피드백을 제공하는 교사-학생 프레임워크이다.

Result: REFINE는 추론 효율성, 토큰 사용량, 확장성을 향상시키고 상당한 속도 개선과 계산 비용 절감을 보여주었다.

Conclusion: REFINE는 멀티모달 추론을 향상시킬 잠재력이 있음을 강조한다.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE's potential for
enhancing multimodal reasoning.

</details>


### [58] [Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links](https://arxiv.org/abs/2508.16314)
*Selen Gecgel Cetin,Tolga Ovatman,Gunes Karabulut Kurt*

Main category: cs.LG

TL;DR: 의도 기반 위협 모델을 제안하여 사이버 물리적 인식 프레임워크를 개발하고, 통합된 위협 평가를 제공하는 연구.


<details>
  <summary>Details</summary>
Motivation: 사이버 물리적 네트워크의 신뢰성과 보안을 별도로 분석하는 것이 시스템 특정 기준에 과적합을 초래할 수 있다는 점을 지적하고, 이를 통해 위협 평가의 본질적인 측면을 다루는 것이 필요하다.

Method: 받은 신호의 특성 속성을 추출하여 잠재적 위협에 대한 직관적인 이해를 돕는 알고리즘 제안, 신뢰성 관련 능력을 평가하는 작업과 신호의 기본적인 의도를 해독하는 작업을 포함하는 다중 작업 학습 아키텍처 개발, 다양한 보안 및 신뢰성 요구 사항에 맞춘 적응형 위협 평가 제안.

Result: 제안된 프레임워크는 기존의 순차적인 방법보다 위협 탐지 및 평가의 강건성을 향상시키며, emerging intershell links가 있는 우주 네트워크가 복잡한 위협 시나리오를 효과적으로 처리할 수 있도록 함.

Conclusion: 사이버 물리적 인식(CPA) 프레임워크를 통해 위협 탐지 및 평가의 능력을 향상하고, 신뢰성과 보안 요구 사항에 맞춘 적응형 분석을 제공한다.

Abstract: This letter addresses essential aspects of threat assessment by proposing
intent-driven threat models that incorporate both capabilities and intents. We
propose a holistic framework for cyber physical awareness (CPA) in space
networks, pointing out that analyzing reliability and security separately can
lead to overfitting on system-specific criteria. We structure our proposed
framework in three main steps. First, we suggest an algorithm that extracts
characteristic properties of the received signal to facilitate an intuitive
understanding of potential threats. Second, we develop a multitask learning
architecture where one task evaluates reliability-related capabilities while
the other deciphers the underlying intentions of the signal. Finally, we
propose an adaptable threat assessment that aligns with varying security and
reliability requirements. The proposed framework enhances the robustness of
threat detection and assessment, outperforming conventional sequential methods,
and enables space networks with emerging intershell links to effectively
address complex threat scenarios.

</details>


### [59] [OwkinZero: Accelerating Biological Discovery with AI](https://arxiv.org/abs/2508.16315)
*Nathan Bigaud,Vincent Cabeli,Meltem Gurel,Arthur Pignet,John Klein,Gilles Wainrib,Eric Durand*

Main category: cs.LG

TL;DR: 대규모 언어 모델(LLMs)은 생물학적 추론 업무에서 여전히 어려움을 겪고 있다. 이러한 한계를 극복하기 위해 30만 개 이상의 질문-답변 쌍을 포함한 8개의 벤치마크 데이터셋을 구축하고, 이를 사용하여 강화 학습 기반의 OwkinZero 모델을 개발했다. 결과적으로 이 모델은 상업적인 LLM보다 우수한 성능을 보였으며, 전문화된 모델이 일반화 효과에서 뛰어난 성과를 보임을 확인했다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 생물학적 추론 작업에서 여전히 부족함을 해결하기 위해.

Method: 300,000개 이상의 검증 가능한 질문-답변 쌍으로 구성된 8개의 벤치마크 데이터셋을 만들고, 이를 통해 강화 학습 기반으로 OwkinZero 모델을 개발했다.

Result: 특화된 OwkinZero 모델이 기존의 상업적 LLM보다 생물학 벤치마크에서 월등한 성과를 보였다.

Conclusion: 대상 데이터를 기반으로 한 맞춤형 강화 학습이 전문화된 모델의 일반화 가능한 성능을 높이고, AI 주도의 생물학적 발견을 가속화할 수 있음을 보여준다.

Abstract: While large language models (LLMs) are rapidly advancing scientific research,
they continue to struggle with core biological reasoning tasks essential for
translational and biomedical discovery. To address this limitation, we created
and curated eight comprehensive benchmark datasets comprising over 300,000
verifiable question-and-answer pairs, each targeting critical challenges in
drug discovery including target druggability, modality suitability, and drug
perturbation effects. Using this resource, we developed the OwkinZero models by
post-training open-source LLMs through a Reinforcement Learning from Verifiable
Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero
models substantially outperform larger, state-of-the-art commercial LLMs on
these biological benchmarks. Remarkably, we uncover evidence of a key aspect of
generalization: specialist models trained on a single task consistently
outperform their base models on previously unseen tasks. This generalization
effect is further amplified in our comprehensive OwkinZero models, which were
trained on a mixture of datasets and achieve even broader cross-task
improvements. This study represents a significant step toward addressing the
biological reasoning blind spot in current LLMs, demonstrating that targeted
reinforcement learning on carefully curated data can unlock generalizable
performance in specialized models, thereby accelerating AI-driven biological
discovery.

</details>


### [60] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 수원 분배 네트워크의 결함을 감지하기 위한 온라인 비지도 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 수원 분배 네트워크는 공공 안전과 경제적 안정성에 필수적이지만, 파이프 막힘, 배경 누수와 같은 문제에 직면하고 있으며, 이러한 문제는 데이터 비정상성과 제한된 라벨 데이터와 같은 운영 제약으로 인해 악화된다.

Method: 이 논문은 집단 이상으로 모델링된 파이프 막힘과 개념 이동으로 모델링된 배경 누수를 감지하는 비지도 온라인 학습 프레임워크를 제안하며, LSTM-VAE와 이중 드리프트 감지 메커니즘을 결합하여 비정상적인 조건에서 강력한 감지 및 적응을 가능하게 한다.

Result: 두 개의 실제 WDN에 대한 실험 결과, 제안된 접근법이 이상 감지 및 반복 드리프트에 대한 적응에서 강력한 기준선을 지속적으로 능가함을 보였다.

Conclusion: 이 연구는 동적 WDN 환경에서 비지도 이벤트 감지의 효과성을 입증한다.

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [61] [Probabilistic Pretraining for Neural Regression](https://arxiv.org/abs/2508.16355)
*Boris N. Oreshkin,Shiv Tavker,Dmitry Efimov*

Main category: cs.LG

TL;DR: 본 연구는 확률 회귀를 위한 전이 학습의 부족한 연구를 해결하기 위해 NIAQUE라는 새로운 모델을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 확률 회귀를 위한 전이 학습이 충분히 탐구되지 않았다는 점을 해결하고자 합니다.

Method: NIAQUE는 순열 불변성을 통해 확률 회귀에서 전이 학습을 위한 모델로 설계되었습니다.

Result: 다양한 하위 회귀 데이터셋에서 NIAQUE를 사전 훈련하고 특정 목표 데이터셋에서 미세 조정함으로써 각 회귀 작업의 성능이 향상됨을 보여주었습니다.

Conclusion: NIAQUE는 강력하고 확장 가능한 확률 회귀 프레임워크로서 전이 학습을 활용하여 예측 성능을 향상시키는 효과성을 강조합니다.

Abstract: Transfer learning for probabilistic regression remains underexplored. This
work closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile
Estimation, a new model designed for transfer learning in probabilistic
regression through permutation invariance. We demonstrate that pre-training
NIAQUE directly on diverse downstream regression datasets and fine-tuning it on
a specific target dataset enhances performance on individual regression tasks,
showcasing the positive impact of probabilistic transfer learning. Furthermore,
we highlight the effectiveness of NIAQUE in Kaggle competitions against strong
baselines involving tree-based models and recent neural foundation models
TabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and
scalable framework for probabilistic regression, leveraging transfer learning
to enhance predictive performance.

</details>


### [62] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: RotaTouille는 회전과 순환 이동에 대해 불변성을 갖춘 심층 학습 프레임워크로, 윤곽선 데이터를 학습하는 데 사용된다.


<details>
  <summary>Details</summary>
Motivation: 윤곽선 데이터에서 시작점 선택이 임의적이며, 회전과 순환 이동 불변성을 갖춘 심층 학습 모델이 필요하다.

Method: 복소수 값의 순환 합성곱을 통해 회전과 순환 이동 불변성을 달성하는 RotaTouille 프레임워크를 제안한다.

Result: 형상 분류, 재구성, 윤곽선 회귀 실험을 통해 RotaTouille의 효과를 입증했다.

Conclusion: RotaTouille는 윤곽선 데이터 학습에 있어 혁신적인 방법을 제시하며, 다양한 다운스트림 작업에서 불변 표현을 얻는다.

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [63] [Applications and Challenges of Fairness APIs in Machine Learning Software](https://arxiv.org/abs/2508.16377)
*Ajoy Das,Gias Uddin,Shaiful Chowdhury,Mostafijur Rahman Akhond,Hadi Hemmati*

Main category: cs.LG

TL;DR: 본 연구는 머신러닝 소프트웨어 시스템에서의 공정성 API 사용 사례와 개발자들이 직면한 도전 과제를 조사했다.


<details>
  <summary>Details</summary>
Motivation: 머신러닝 시스템이 민감한 환경에서 중요한 결정을 내리는 데 사용되므로, 특정 집단에 대한 차별적 결정을 피하는 것이 매우 중요하다.

Method: 204개의 GitHub 리포지토리를 분석하여 13개의 공정성 API의 사용 상황을 이해하고, API 개발자들이 겪는 문제를 조사했다.

Result: API들이 주로 실제 문제를 해결하고 학습하는 두 가지 목적을 위해 사용되는 것을 발견했으며, 17개의 독특한 사용 사례를 타겟으로 했다.

Conclusion: 개발자들은 편향 탐지 및 완화에 대한 이해가 부족하고, 문제 해결에 어려움을 겪으며, 의견과 자원을 자주 요청한다.

Abstract: Machine Learning software systems are frequently used in our day-to-day
lives. Some of these systems are used in various sensitive environments to make
life-changing decisions. Therefore, it is crucial to ensure that these AI/ML
systems do not make any discriminatory decisions for any specific groups or
populations. In that vein, different bias detection and mitigation open-source
software libraries (aka API libraries) are being developed and used. In this
paper, we conduct a qualitative study to understand in what scenarios these
open-source fairness APIs are used in the wild, how they are used, and what
challenges the developers of these APIs face while developing and adopting
these libraries. We have analyzed 204 GitHub repositories (from a list of 1885
candidate repositories) which used 13 APIs that are developed to address bias
in ML software. We found that these APIs are used for two primary purposes
(i.e., learning and solving real-world problems), targeting 17 unique
use-cases. Our study suggests that developers are not well-versed in bias
detection and mitigation; they face lots of troubleshooting issues, and
frequently ask for opinions and resources. Our findings can be instrumental for
future bias-related software engineering research, and for guiding educators in
developing more state-of-the-art curricula.

</details>


### [64] [Sequential Cohort Selection](https://arxiv.org/abs/2508.16386)
*Hortence Phalonne Nana,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 이 연구는 대학 입학 시 공정한 집단 선발 문제를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 대학 입학 과정에서 공정한 집단 선발의 필요성을 강조하기 위해 시작했습니다.

Method: 한 번의 설정에서는 입학 정책이 미리 고정되어야 하고 투명해야 하며, 후속 설정에서는 새로운 지원자 데이터에 따라 정책을 업데이트할 수 있습니다. 이전 입학 주기의 데이터를 사용하여 인구 모델을 통해 입학 정책을 최적화함으로써 이 설정을 달성합니다.

Result: 결과적으로, 기존의 공정한 집단 선발 정책의 품질과 공정성을 평가했습니다.

Conclusion: 이 연구는 공정성을 고려한 입학 정책 개발에 기여할 수 있습니다.

Abstract: We study the problem of fair cohort selection from an unknown population,
with a focus on university admissions. We start with the one-shot setting,
where the admission policy must be fixed in advance and remain transparent,
before observing the actual applicant pool. In contrast, the sequential setting
allows the policy to be updated across stages as new applicant data becomes
available. This is achieved by optimizing admission policies using a population
model, trained on data from previous admission cycles. We also study the
fairness properties of the resulting policies in the one-shot setting,
including meritocracy and group parity.

</details>


### [65] [Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow](https://arxiv.org/abs/2508.16403)
*Anahita Asadi,Leonid Popryho,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 본 연구에서는 RF 회로의 성능 예측을 위한 경량의 데이터 효율적 그래프 신경망 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현대 무선 시스템에서 능동 RF 회로의 성능을 정확하게 예측하는 것은 필수적이다.

Method: 트랜지스터 수준의 대칭성을 포착하고 세부 연결성을 보존하기 위해 장치 단말 수준에서 회로를 모델링하고, 복잡한 목표 분포를 모델링하기 위해 마스크 자율 회귀 흐름(MAF) 출력 헤드를 통합하였다.

Result: 대규모 데이터 세트에서도 높은 예측 정확도를 달성하였고, sMAPE와 MRE는 각각 2.40%와 2.91%를 기록하였다.

Conclusion: 이 방법은 RF 회로 설계 자동화를 위한 신속하고 정확한 방법을 입증하였다.

Abstract: Accurately predicting the performance of active radio frequency (RF) circuits
is essential for modern wireless systems but remains challenging due to highly
nonlinear, layout-sensitive behavior and the high computational cost of
traditional simulation tools. Existing machine learning (ML) surrogates often
require large datasets to generalize across various topologies or to accurately
model skewed and multi-modal performance metrics. In this work, a lightweight,
data-efficient, and topology-aware graph neural network (GNN) model is proposed
for predicting key performance metrics of multiple topologies of active RF
circuits such as low noise amplifiers (LNAs), mixers, voltage-controlled
oscillators (VCOs), and PAs. To capture transistor-level symmetry and preserve
fine-grained connectivity details, circuits are modeled at the device-terminal
level, enabling scalable message passing while reducing data requirements.
Masked autoregressive flow (MAF) output heads are incorporated to improve
robustness in modeling complex target distributions. Experiments on datasets
demonstrate high prediction accuracy, with symmetric mean absolute percentage
error (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,
respectively. Owing to the pin-level conversion of circuit to graph and ML
architecture robust to modeling complex densities of RF metrics, the MRE is
improved by 3.14x while using 2.24x fewer training samples compared to prior
work, demonstrating the method's effectiveness for rapid and accurate RF
circuit design automation.

</details>


### [66] [FraPPE: Fast and Efficient Preference-based Pure Exploration](https://arxiv.org/abs/2508.16487)
*Udvas Das,Apurv Shukla,Debabrota Basu*

Main category: cs.LG

TL;DR: 본 논문에서는 Preference-based Pure Exploration(PrePEx)에 대한 효율적인 알고리즘을 제안하여 기울기 구간 내에서 최적의 샘플 복잡성을 달성한다고 주장합니다.


<details>
  <summary>Details</summary>
Motivation: PrePEx 및 그 변형들은 잘 연구되었지만, 임의의 선호 구간에 대해 기존의 하한을 최적 트래킹 할 수 있는 효율적인 알고리즘이 존재하지 않습니다.

Method: 우리는 하한을 설정하고 최적화를 위해 Frank-Wolfe 최적화기를 배치하여 최소화 및 최대화 문제를 효율적으로 해결합니다.

Result: 제안된 FraPPE 알고리즘은 최저 샘플 복잡도로 기존 알고리즘들 사이에서 정확한 Pareto 세트를 식별하는 데 성공합니다.

Conclusion: FraPPE는 주어진 대칭 대역에서 최적의 샘플 복잡성을 점근적으로 달성하며, 이론적 근거와 실험적 결과 모두에서 높은 성능을 보여줍니다.

Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given
confidence level the set of Pareto optimal arms in a vector-valued (aka
multi-objective) bandit, where the reward vectors are ordered via a (given)
preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied,
there does not exist a computationally efficient algorithm that can optimally
track the existing lower bound for arbitrary preference cones. We successfully
fill this gap by efficiently solving the minimisation and maximisation problems
in the lower bound. First, we derive three structural properties of the lower
bound that yield a computationally tractable reduction of the minimisation
problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation
problem in the lower bound. Together, these techniques solve the maxmin
optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with
$K$ arms and $L$ dimensional reward, which is a significant acceleration over
the literature. We further prove that our proposed PrePEx algorithm, FraPPE,
asymptotically achieves the optimal sample complexity. Finally, we perform
numerical experiments across synthetic and real datasets demonstrating that
FraPPE achieves the lowest sample complexities to identify the exact Pareto set
among the existing algorithms.

</details>


### [67] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin Müller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: 본 논문은 오프라인 강화 학습에서 정책 성능 수준을 정확하게 제어하기 위한 새로운 접근법인 Doctor를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 오프라인 강화 학습 방법이 최적의 수익을 추구하는 데 중점을 두지만, 실제 응용에서는 정책 성능 수준에 대한 정밀한 제어가 필요하다.

Method: Doctor라는 새로운 접근법을 통해 목표 정렬을 수행하여 오프라인 RL을 처리하는 방법을 제안한다.

Result: Doctor는 데이터셋 내외에서 우수한 목표 정렬을 달성하며, 정책 성능에 대한 정확하고 유연한 제어를 가능하게 한다.

Conclusion: EpiCare 벤치마크에서 우리의 접근법은 치료 정책의 공격성을 효과적으로 조절하며, 치료 수익과 부작용 위험 간의 균형을 맞춘다.

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


### [68] [Post Hoc Regression Refinement via Pairwise Rankings](https://arxiv.org/abs/2508.16495)
*Kevin Tirta Wijaya,Michael Sun,Minghao Guo,Hans-Peter Seidel,Wojciech Matusik,Vahid Babaei*

Main category: cs.LG

TL;DR: RankRefine는 적은 데이터에서의 회귀 정확도를 개선하기 위한 모델 불가지론적 방법이다. 이는 쌍별 순위 정보를 이용하여 회귀를 정제한다.


<details>
  <summary>Details</summary>
Motivation: 연속적인 속성의 정확한 예측은 많은 과학 및 공학 작업에 필수적이다.

Method: RankRefine는 쌍별 순위를 통해 전문가 지식을 반영하여 기본 회귀기의 출력을 정제하는 포스트 호크 방법이다.

Result: 분자 속성 예측 작업에서 RankRefine는 20개의 쌍별 비교만으로 평균 절대 오차를 최대 10% 줄였다.

Conclusion: RankRefine는 적은 데이터 환경에서도 실용성과 광범위한 적용 가능성을 제공한다.

Abstract: Accurate prediction of continuous properties is essential to many scientific
and engineering tasks. Although deep-learning regressors excel with abundant
labels, their accuracy deteriorates in data-scarce regimes. We introduce
RankRefine, a model-agnostic, plug-and-play post hoc method that refines
regression with expert knowledge coming from pairwise rankings. Given a query
item and a small reference set with known properties, RankRefine combines the
base regressor's output with a rank-based estimate via inverse variance
weighting, requiring no retraining. In molecular property prediction task,
RankRefine achieves up to 10% relative reduction in mean absolute error using
only 20 pairwise comparisons obtained through a general-purpose large language
model (LLM) with no finetuning. As rankings provided by human experts or
general-purpose LLMs are sufficient for improving regression across diverse
domains, RankRefine offers practicality and broad applicability, especially in
low-data settings.

</details>


### [69] [Boardwalk: Towards a Framework for Creating Board Games with LLMs](https://arxiv.org/abs/2508.16447)
*Álvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 이 논문은 LLM(대형 언어 모델)을 사용하여 자연어로 설명된 규칙을 기반으로 디지털 보드 게임을 구현하는 가능성을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 보드 게임의 코딩은 시간이 많이 소요되는 작업이다. LLM이 도메인 특화 작업에 효과적임이 입증되었다.

Method: 세 개의 최신 LLM(Claude, DeepSeek, ChatGPT)을 사용하여 12개의 인기 및 덜 알려진 게임을 자유 형식과 제안된 일반 게임 플레이 API인 Boardwalk 내에서 코딩하도록 한다.

Result: Claude 3.7 Sonnet 모델이 55.6%의 게임에서 오류 없이 수행하는 등 우리의 접근 방법이 실행 가능함을 입증하였다.

Conclusion: API 준수가 오류 빈도를 증가시키지만, 오류의 심각성은 LLM에 더 크게 의존한다. 이 과정을 통합할 수 있는 프레임워크 구축을 위한 향후 단계에 대해 설명한다.

Abstract: Implementing board games in code can be a time-consuming task. However, Large
Language Models (LLMs) have been proven effective at generating code for
domain-specific tasks with simple contextual information. We aim to investigate
whether LLMs can implement digital versions of board games from rules described
in natural language. This would be a step towards an LLM-assisted framework for
quick board game code generation. We expect to determine the main challenges
for LLMs to implement the board games, and how different approaches and models
compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek
and ChatGPT) with coding a selection of 12 popular and obscure games in
free-form and within Boardwalk, our proposed General Game Playing API. We
anonymize the games and components to avoid evoking pre-trained LLM knowledge.
The implementations are tested for playability and rule compliance. We evaluate
success rate and common errors across LLMs and game popularity. Our approach
proves viable, with the best performing model, Claude 3.7 Sonnet, yielding
55.6\% of games without any errors. While compliance with the API increases
error frequency, the severity of errors is more significantly dependent on the
LLM. We outline future steps for creating a framework to integrate this
process, making the elaboration of board games more accessible.

</details>


### [70] [On Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2508.16496)
*Scott Jeen*

Main category: cs.LG

TL;DR: 이 논문은 실제 환경에서의 제로샷 강화 학습 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 사회가 직면한 복잡한 문제들은 강화 학습 기술을 필요로 하지만, 이러한 기술이 효과적으로 적용될 수 있는 데이터 시뮬레이션이 제한적이다.

Method: 기존 데이터에서 시뮬레이터를 학습하고, 세 가지 제약 조건(데이터 품질, 관측 가능성, 데이터 가용성)을 고려하여 제로샷 RL 방법을 제안한다.

Result: 기존 방법들의 한계를 드러내고, 이를 보완하기 위한 기술을 정당화하는 여러 실증 연구를 수행하였다.

Conclusion: 이러한 디자인들이 실제 문제를 해결하기 위한 RL 방법에 한걸음 더 가까이 가도록 한다.

Abstract: Modern reinforcement learning (RL) systems capture deep truths about general,
human problem-solving. In domains where new data can be simulated cheaply,
these systems uncover sequential decision-making policies that far exceed the
ability of any human. Society faces many problems whose solutions require this
skill, but they are often in domains where new data cannot be cheaply
simulated. In such scenarios, we can learn simulators from existing data, but
these will only ever be approximately correct, and can be pathologically
incorrect when queried outside of their training distribution. As a result, a
misalignment between the environments in which we train our agents and the
real-world in which we wish to deploy our agents is inevitable. Dealing with
this misalignment is the primary concern of zero-shot reinforcement learning, a
problem setting where the agent must generalise to a new task or domain with
zero practice shots. Whilst impressive progress has been made on methods that
perform zero-shot RL in idealised settings, new work is needed if these results
are to be replicated in real-world settings. In this thesis, we argue that
doing so requires us to navigate (at least) three constraints. First, the data
quality constraint: real-world datasets are small and homogeneous. Second, the
observability constraint: states, dynamics and rewards in the real-world are
often only partially observed. And third, the data availability constraint: a
priori access to data cannot always be assumed. This work proposes a suite of
methods that perform zero-shot RL subject to these constraints. In a series of
empirical studies we expose the failings of existing methods, and justify our
techniques for remedying them. We believe these designs take us a step closer
to RL methods that can be deployed to solve real-world problems.

</details>


### [71] [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](https://arxiv.org/abs/2508.16476)
*Maryam Ghasemzadeh,Anton van Beek*

Main category: cs.LG

TL;DR: NOSTRA는 실험 불확실성을 활용하여 정확한 대리 모델을 구축하고, 샘플링을 유망한 디자인 공간으로 집중시키는 새로운 샘플링 프레임워크로, 정량적 데이터가 제한된 경우에도 효율적인 성능을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: MOBO는 실험 불확실성으로 인해 동일한 입력에서 다양한 출력을 얻기 때문에 희소한 데이터셋으로 어려움을 겪으며, 이는 물리적 및 시뮬레이션 실험에서 일반적으로 발생합니다.

Method: NOSTRA는 실험 불확실성에 대한 사전 지식을 통합하여 더 정확한 대리 모델을 구축하고, 신뢰 영역을 사용해 샘플링을 유망한 디자인 공간으로 집중시키는 샘플링 프레임워크입니다.

Result: NOSTRA는 두 개의 테스트 함수에서 실험 불확실성이 증가함에 따라 기존 방법보다 성능을 향상시켜 노이즈가 많고 희소하며 제한된 데이터 처리에서 우수성을 입증했습니다.

Conclusion: NOSTRA는 자원이 제한된 실험에서 효율적인 성능을 보장하며, 샘플이 발견된 파레토 경계의 정확성을 개선하는 지역에 우선순위를 두는 리소스 효율적 알고리즘입니다.

Abstract: Multi-objective Bayesian optimization (MOBO) struggles with sparse
(non-space-filling), scarce (limited observations) datasets affected by
experimental uncertainty, where identical inputs can yield varying outputs.
These challenges are common in physical and simulation experiments (e.g.,
randomized medical trials and, molecular dynamics simulations) and are
therefore incompatible with conventional MOBO methods. As a result,
experimental resources are inefficiently allocated, leading to suboptimal
designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data
Trust Region-based Optimization Algorithm), a novel sampling framework that
integrates prior knowledge of experimental uncertainty to construct more
accurate surrogate models while employing trust regions to focus sampling on
promising areas of the design space. By strategically leveraging prior
information and refining search regions, NOSTRA accelerates convergence to the
Pareto frontier, enhances data efficiency, and improves solution quality.
Through two test functions with varying levels of experimental uncertainty, we
demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,
and scarce data. Specifically, we illustrate that, NOSTRA effectively
prioritizes regions where samples enhance the accuracy of the identified Pareto
frontier, offering a resource-efficient algorithm that is practical in
scenarios with limited experimental budgets while ensuring efficient
performance.

</details>


### [72] [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
*Parker Seegmiller,Kartik Mehta,Soumya Saha,Chenyang Tao,Shereen Oraby,Arpit Gupta,Tagyoung Chung,Mohit Bansal,Nanyun Peng*

Main category: cs.LG

TL;DR: FLAMES 프레임워크를 활용하여 10개의 데이터 합성 전략과 여러 요소가 합성 수학 추론 데이터 성능에 미치는 영향을 체계적으로 연구하였습니다. 실험 결과, 문제 복잡성을 높이는 데이터 에이전트가 대부분의 수학 메트릭에서 최고 개선을 이끌어내었으며, 데이터 생성 예산이 고정된 상황에서 더 높은 문제 커버리지를 유지하는 것이 중요하다는 것을 확인했습니다. FLAMES 데이터셋은 기존 공적 데이터셋을 능가하는 성과를 보여주어, 새로운 합성 전략들이 효과적임을 증명합니다.


<details>
  <summary>Details</summary>
Motivation: 합성 데이터로 LLM 수학 추론을 개선하려는 최근 연구들이 독특한 설정을 사용하여 데이터 합성 전략 비교를 실질적으로 어렵게 만들었습니다. 이는 저품질 문제 필터링 영향 등 합성 데이터 파이프라인에서 다양한 요소의 역할에 대한 많은 질문을 남깁니다.

Method: FLAMES라는 프레임워크를 소개하고 10개의 기존 데이터 합성 전략과 합성 수학 추론 데이터 성능에 영향을 미치는 여러 다른 요소를 체계적으로 연구하였습니다. 실험을 통해 합성 데이터의 난이도와 다양성의 최적 균형에 대한 통찰력을 제공하였습니다.

Result: 문제 복잡성을 높이기 위한 데이터 에이전트가 대부분의 수학 메트릭에서 가장 좋은 개선을 가져오며, 고정된 데이터 생성 예산에서 더 높은 문제 커버리지를 유지하는 것이 신뢰할 수 있는 솔루션이 있는 문제만을 유지하는 것보다 중요하다는 것을 확인했습니다. FLAMES 데이터셋은 공적 데이터셋에 비해 우수한 성능을 보였습니다.

Conclusion: FLAMES 실험의 통찰을 바탕으로 도메인 외 일반화 및 강건성을 개선하기 위한 두 가지 새로운 데이터 합성 전략을 설계하였으며, 이를 통해 기존 데이터 합성 전략과 혼합된 효과적인 FLAMES 데이터셋을 개발하였습니다.

Abstract: Recent works improving LLM math reasoning with synthetic data have used
unique setups, making comparison of data synthesis strategies impractical. This
leaves many unanswered questions about the roles of different factors in the
synthetic data pipeline, such as the impact of filtering low-quality problems.
To address this gap, we introduce FLAMES, a Framework for LLM Assessment of
Math rEasoning Data Synthesis, and perform a systematic study of 10 existing
data synthesis strategies and multiple other factors impacting the performance
of synthetic math reasoning data. Our FLAMES experiments provide several
valuable insights about the optimal balance of difficulty and diversity of
synthetic data. First, data agents designed to increase problem complexity lead
to best improvements on most math metrics. Second, with a fixed data generation
budget, keeping higher problem coverage is more important than keeping only
problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data
can lead to improvements on competition-level benchmarks, showcasing
easy-to-hard generalization. Leveraging insights from our FLAMES experiments,
we design two novel data synthesis strategies for improving out-of-domain
generalization and robustness. Further, we develop the FLAMES dataset, an
effective blend of our novel and existing data synthesis strategies,
outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),
GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES
dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and
Claude 3.5 Sonnet.

</details>


### [73] [Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms](https://arxiv.org/abs/2508.16481)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: 이 논문에서는 공격에 대한 LLM 기반 에이전틱 시스템의 강인성을 평가하고, 해로운 행위를 유도하는 공격을 통해 시스템의 안전한 사용을 보장하기 위한 새로운 해악 분류법과 벤치마크인 BAD-ACTS를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 시스템의 안전한 사용을 보장하기 위해 이러한 시스템이 공격받을 때 나타날 수 있는 악의적 행동의 범위를 철저히 이해해야 한다.

Method: 해악에 대한 새로운 분류법과 다양한 해로운 행동을 연구하기 위한 벤치마크 BAD-ACTS를 제안한다. BAD-ACTS는 서로 다른 응용 환경에서의 4가지 에이전틱 시스템 구현과 188개의 고품질 해로운 행동 사례로 구성된다.

Result: 공격자가 시스템 내의 한 에이전트를 조종하면서 다른 에이전트가 해로운 목표 행동을 실행하도록 조작하려 할 때 에이전틱 시스템의 강인성 분석 결과, 공격의 성공률이 높았다.

Conclusion: 단일 적대적 에이전트가 시스템 내에서 보안에 중대한 영향을 미칠 수 있음을 보여준다. 기본적인 방어 전략을 사용하더라도 공격이 여전히 효과적이다. 우리는 메시지 모니터링 기반의 더 효과적인 방어를 제안하고, 이 벤치마크가 에이전틱 시스템의 안전성 연구를 위한 다양한 테스트베드가 되리라 믿는다.

Abstract: Ensuring the safe use of agentic systems requires a thorough understanding of
the range of malicious behaviors these systems may exhibit when under attack.
In this paper, we evaluate the robustness of LLM-based agentic systems against
attacks that aim to elicit harmful actions from agents. To this end, we propose
a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,
for studying the security of agentic systems with respect to a wide range of
harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in
distinct application environments, as well as a dataset of 188 high-quality
examples of harmful actions. This enables a comprehensive study of the
robustness of agentic systems across a wide range of categories of harmful
behaviors, available tools, and inter-agent communication structures. Using
this benchmark, we analyze the robustness of agentic systems against an
attacker that controls one of the agents in the system and aims to manipulate
other agents to execute a harmful target action. Our results show that the
attack has a high success rate, demonstrating that even a single adversarial
agent within the system can have a significant impact on the security. This
attack remains effective even when agents use a simple prompting-based defense
strategy. However, we additionally propose a more effective defense based on
message monitoring. We believe that this benchmark provides a diverse testbed
for the security research of agentic systems. The benchmark can be found at
github.com/JNoether/BAD-ACTS

</details>


### [74] [Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation](https://arxiv.org/abs/2508.16521)
*Zhijian Zhou,Junyi An,Zongkai Liu,Yunfei Shi,Xuan Zhang,Fenglei Cao,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 물리적으로 현실적인 3D 분자 구조 생성을 위한 새로운 방법, RLPF를 제안하며, 실험 결과 기존 방법에 비해 분자 안정성을 크게 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 물리적 원칙을 준수하는 평형 구조를 생성하기 위한 필요성

Method: 강화 학습과 물리적 피드백을 이용하여 3D 분자 생성을 위한 새로운 프레임워크인 RLPF를 제안함.

Result: QM9 및 GEOM-drug 데이터셋에서 기존 방법에 비해 분자 안정성이 크게 향상됨을 보였음.

Conclusion: RLPF는 생성 모델링에서 물리 기반 피드백을 통합하는 것의 가치를 강조함.

Abstract: Generating physically realistic 3D molecular structures remains a core
challenge in molecular generative modeling. While diffusion models equipped
with equivariant neural networks have made progress in capturing molecular
geometries, they often struggle to produce equilibrium structures that adhere
to physical principles such as force field consistency. To bridge this gap, we
propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework
that extends Denoising Diffusion Policy Optimization to 3D molecular
generation. RLPF formulates the task as a Markov decision process and applies
proximal policy optimization to fine-tune equivariant diffusion models.
Crucially, RLPF introduces reward functions derived from force-field
evaluations, providing direct physical feedback to guide the generation toward
energetically stable and physically meaningful structures. Experiments on the
QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves
molecular stability compared to existing methods. These results highlight the
value of incorporating physics-based feedback into generative modeling. The
code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.

</details>


### [75] [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)
*Hangzhan Jin,Sicheng Lv,Sifan Wu,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: 큰 언어 모델을 처음부터 훈련하는 것은 실용적이지 않으며, 감독된 미세 조정(SFT)과 강화 학습 기반 미세 조정(RL-FT)이 현대 실천에서 중심 역할을 하고 있다.


<details>
  <summary>Details</summary>
Motivation: 큰 언어 모델의 훈련 방식 변화 및 OOD(performance에 대한 필요한 적합성 상실)를 보완할 수 있는 방법을 재검토하기 위함이다.

Method: 24포인트 카드 게임의 OOD 변형과 새로운 스펙트럼 기반 진단을 사용하여 RL-FT와 SFT의 모델 표현 및 OOD 성능에 미치는 영향을 연구했다.

Result: RL-FT는 SFT로 인한 OOD 성능 손실을 상당히 회복할 수 있지만, SFT가 심각한 과적합을 일으킬 경우 완전히 회복할 수는 없다. 고유 벡터의 방향 이동이 고유 값의 크기보다 중요하며, 저순위 및 얕은 복구가 효과적임을 보이고 있다.

Conclusion: RL은 SFT가 유도한 방향적 드리프트를 상쇄하는 데 주로 작용하는 반면, 새로운 해결책을 찾지는 않기 때문에 RL의 OOD 성능이 우수하다는 이전 보고와 일치한다.

Abstract: Training large language models (LLMs) from scratch is increasingly
impractical, making post-training methods such as supervised fine-tuning (SFT)
and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern
practice. Using an out-of-distribution (OOD) variant of the 24-point card game
and new spectrum-based diagnostics, we revisit how these two stages reshape
model representation and OOD performance. Our key findings are- (1) RL-FT can
restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to
15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and
a clear distribution shift, RL-FT cannot fully recover OOD performance. (2)
Direction shifts of singular vectors matter more than singular value
magnitudes. These shifts concentrate on directions linked to the largest and
smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and
shallow recovery is effective: restoring singular vector directions for the top
20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)
Stronger SFT checkpoints enable better recovery by RL, while overfitted ones
resist restoration. These results reconcile prior reports of RL superior OOD
performance: RL primarily counteracts SFT-induced directional drift rather than
finding new solutions. Our spectrum-aware analysis highlights inexpensive
recovery knobs low-rank UV merging and shallow-layer resets that practitioners
can use before costly RL fine-tuning.

</details>


### [76] [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 이 연구는 LLM 내의 활성화에서 특성을 추출하는 Sparse Autoencoders(SAEs)의 L0 하이퍼파라미터의 영향을 연구하며, 적절하지 않은 L0 설정이 SAE 학습에 미치는 영향을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: L0 하이퍼파라미터가 SAEs의 성능에 중요한 영향을 미치므로, 이를 정확히 설정하고 그 효과를 연구하는 것이 필요하다.

Method: BatchTopK SAEs에 대한 L0 설정을 연구하고, 최적의 L0 값을 찾는 방법을 제시한다.

Result: L0 값이 정확하지 않으면 SAE가 LLM의 기본 특징을 학습하지 못하며, 대부분의 사용되는 SAEs에서 L0 값이 너무 낮다는 것을 발견했다.

Conclusion: SAE 학습 시 적절한 특징을 얻기 위해서는 L0 값을 올바르게 설정해야 한다.

Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations,
meant to correspond to single concepts. A core SAE training hyperparameter is
L0: how many features should fire per token on average. Existing work compares
SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a
free parameter with no single correct value. In this work we study the effect
of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE
fails to learn the underlying features of the LLM. If L0 is too low, the SAE
will mix correlated features to improve reconstruction. If L0 is too high, the
SAE finds degenerate solutions that also mix features. Further, we demonstrate
a method to determine the correct L0 value for an SAE on a given training
distribution, which finds the true L0 in toy models and coincides with peak
sparse probing performance in LLMs. We find that most commonly used SAEs have
an L0 that is too low. Our work shows that, to train SAEs with correct
features, practitioners must set L0 correctly.

</details>


### [77] [MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation](https://arxiv.org/abs/2508.16503)
*Nadia Asif,Zhiqing Hong,Shaogang Ren,Xiaonan Zhang,Xiaojun Shang,Yukun Yuan*

Main category: cs.LG

TL;DR: 본 논문은 시내 311 시스템을 통해 주민 서비스 요청의 예상 응답 시간을 예측하는 MuST2-Learn 프레임워크를 제안하고, 이를 통해 평균 절대 오차를 최소 32.5% 감소시키는 방법을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 캐나다와 미국의 도시에서 주민의 삶의 질 향상을 위해 311 시스템과 같은 비긴급 municipal 서비스가 도입되고 있으나, 주민들이 서비스 요청의 처리 시간에 대한 정보가 제한적이다.

Method: MuST2-Learn은 공간, 시간 및 서비스 유형 차원을 함께 모델링하여 복잡한 요인들을 해결하기 위해 다중 뷰 공간-시간-유형 학습 프레임워크를 제안한다. 이를 위해 이종 서비스 요청 유형 간의 관계를 파악하는 inter-type encoder와 동종 유형 내에서 서비스 시간 변동을 모델링하는 intra-type variation encoder를 통합하며, 각 요청 유형의 공간 및 시간 상관관계를 캡처하기 위해 spatiotemporal encoder를 추가한다.

Result: 제안된 프레임워크는 두 개의 실제 데이터 세트를 사용하여 광범위한 실험을 통해 평가되었으며, MuST2-Learn이 평균 절대 오차를 최소 32.5% 감소시켜 최신 기술을 능가하는 결과를 보여준다.

Conclusion: 수년간 지속된 주민 서비스의 투명성을 제고하고 주민 만족도를 높이는 데 기여할 수 있는 효과적인 방법이 제시되었다.

Abstract: Non-emergency municipal services such as city 311 systems have been widely
implemented across cities in Canada and the United States to enhance residents'
quality of life. These systems enable residents to report issues, e.g., noise
complaints, missed garbage collection, and potholes, via phone calls, mobile
applications, or webpages. However, residents are often given limited
information about when their service requests will be addressed, which can
reduce transparency, lower resident satisfaction, and increase the number of
follow-up inquiries. Predicting the service time for municipal service requests
is challenging due to several complex factors: dynamic spatial-temporal
correlations, underlying interactions among heterogeneous service request
types, and high variation in service duration even within the same request
category. In this work, we propose MuST2-Learn: a Multi-view
Spatial-Temporal-Type Learning framework designed to address the aforementioned
challenges by jointly modeling spatial, temporal, and service type dimensions.
In detail, it incorporates an inter-type encoder to capture relationships among
heterogeneous service request types and an intra-type variation encoder to
model service time variation within homogeneous types. In addition, a
spatiotemporal encoder is integrated to capture spatial and temporal
correlations in each request type. The proposed framework is evaluated with
extensive experiments using two real-world datasets. The results show that
MuST2-Learn reduces mean absolute error by at least 32.5%, which outperforms
state-of-the-art methods.

</details>


### [78] [Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation](https://arxiv.org/abs/2508.16540)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 이 논문은 부드러운 비볼록 최적화에서 엄격한 안장점을 탈출하기 위한 1차 방법들에 대한 포괄적인 이론적 분석을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 비볼록 최적화 문제에서 엄격한 안장점 문제를 해결하기 위한 새로운 알고리즘 개발.

Method: Perturbed Saddle-escape Descent (PSD) 알고리즘을 제안하며, 명확한 상수와 그래디언트 하강 및 안장 탈출 단계 간에 철저한 분리를 포함한다.

Result: PSD가 고확률로 $(	ext{ε},	ext{√ᵗᵒϱε})$-근사 2차 스테이셔너리 포인트를 찾을 수 있음을 증명하고, 필요한 그래디언트 평가 수와 에피소드 수에 대한 상한을 제공한다.

Conclusion: 우리의 이론적 예측은 실험을 통해 검증되며, 알고리즘의 완전한 사양 및 변형이 포함된다.

Abstract: We present a comprehensive theoretical analysis of first-order methods for
escaping strict saddle points in smooth non-convex optimization. Our main
contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully
explicit constants and a rigorous separation between gradient-descent and
saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with
$\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds
an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point
with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient
evaluations for the descent phase plus
$O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode,
with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our
theoretical predictions through extensive experiments across both synthetic
functions and practical machine learning tasks, confirming the logarithmic
dimension dependence and the predicted per-episode function decrease. We also
provide complete algorithmic specifications including a finite-difference
variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch
sizing.

</details>


### [79] [Explainable AI in Deep Learning-Based Prediction of Solar Storms](https://arxiv.org/abs/2508.16543)
*Adam O. Rawashdeh,Jason T. L. Wang,Katherine G. Herbert*

Main category: cs.LG

TL;DR: 이 논문은 LSTM 기반의 태양폭풍 예측 모델의 해석 가능성을 높이는 접근 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 모델의 불투명한 특성 때문에 예측의 이유를 이해하기 어렵다.

Method: 장기 단기 메모리(LSTM) 네트워크와 주의 메커니즘을 사용하여 태양의 활성 영역에서 플레어와 관련된 CMEs 발생 여부 예측.

Result: 예측의 신뢰성을 높이기 위해 입력 시퀀스에 기여하는 요소를 명확히 하는 기술을 활용한다.

Conclusion: 이 모델에 해석 가능성을 추가한 것은 최초의 시도이다.

Abstract: A deep learning model is often considered a black-box model, as its internal
workings tend to be opaque to the user. Because of the lack of transparency, it
is challenging to understand the reasoning behind the model's predictions.
Here, we present an approach to making a deep learning-based solar storm
prediction model interpretable, where solar storms include solar flares and
coronal mass ejections (CMEs). This deep learning model, built based on a long
short-term memory (LSTM) network with an attention mechanism, aims to predict
whether an active region (AR) on the Sun's surface that produces a flare within
24 hours will also produce a CME associated with the flare. The crux of our
approach is to model data samples in an AR as time series and use the LSTM
network to capture the temporal dynamics of the data samples. To make the
model's predictions accountable and reliable, we leverage post hoc
model-agnostic techniques, which help elucidate the factors contributing to the
predicted output for an input sequence and provide insights into the model's
behavior across multiple sequences within an AR. To our knowledge, this is the
first time that interpretability has been added to an LSTM-based solar storm
prediction model.

</details>


### [80] [TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine](https://arxiv.org/abs/2508.16553)
*Tim Langer,Matthias Widra,Volkhard Beyer*

Main category: cs.LG

TL;DR: 이 논문은 산업 4.0의 맥락에서 산업 기계의 개조와 TinyML을 활용한 프로세스 모니터링 시스템의 구현을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 산업 4.0 환경에서 오래된 산업 기계에 대한 프로세스 모니터링 기능 추가의 필요성.

Method: TinyML 흐름을 데이터셋 생성부터 기계 학습 모델 개발, 구현 및 평가까지 설명하며, 특히 새로운 MillingVibes 데이터셋 생성이 포함됩니다.

Result: 8비트 양자화된 CNN 모델 개발로 구조 통합 프로세스 품질 모니터링을 실현하였고, ARM Cortex M4F 마이크로컨트롤러에서 100.0%의 시험 정확도와 짧은 추론 시간을 기록했습니다.

Conclusion: 이 연구는 미래의 TinyML 기반 프로세스 모니터링 솔루션을 위한 기준이 되는 결과를 제공합니다.

Abstract: In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.

</details>


### [81] [Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation](https://arxiv.org/abs/2508.16568)
*Guangyu Sun,Jingtao Li,Weiming Zhuang,Chen Chen,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 본 논문에서는 개인 정보 보호 요구사항을 충족하는 클라우드 기반의 기초 모델(FM) 적응 방법으로, PSSFL(실용 반지도 연합 학습)과 FedMox(연합 전문가 혼합 프레임워크)를 제안하여 성능을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 기반의 기초 모델이 개인 데이터에 직접 접근할 수 없는 상황에서, 보다 효과적이고 개인 정보를 보호하는 방법으로 연합 학습의 필요성을 강조합니다.

Method: PSSFL에서는 엣지 장치가 라벨이 없는 저해상도 데이터를 보유하고, 서버는 제한된 라벨이 있는 고해상도 데이터를 보유합니다. FedMox는 전문가 혼합 아키텍처를 통해 이 문제를 해결합니다.

Result: 실제 자율 주행 데이터 세트에 대한 실험을 통해 FedMox가 PSSFL 환경에서 FM을 효과적으로 적응시키며, 엣지 장치에서 메모리 비용을 제한하면서 성능을 유의미하게 향상시킴을 보여줍니다.

Conclusion: 우리의 연구는 연합 상황에서 기초 모델의 확장 가능하고 개인 정보를 보호하는 적응 방법을 제시합니다.

Abstract: Foundation models (FMs) exhibit remarkable generalization but require
adaptation to downstream tasks, particularly in privacy-sensitive applications.
Due to data privacy regulations, cloud-based FMs cannot directly access private
edge data, limiting their adaptation. Federated learning (FL) provides a
privacy-aware alternative, but existing FL approaches overlook the constraints
imposed by edge devices -- namely, limited computational resources and the
scarcity of labeled data. To address these challenges, we introduce Practical
Semi-Supervised Federated Learning (PSSFL), where edge devices hold only
unlabeled, low-resolution data, while the server has limited labeled,
high-resolution data. In this setting, we propose the Federated Mixture of
Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox
tackles computational and resolution mismatch challenges via a sparse
Mixture-of-Experts architecture, employing a spatial router to align features
across resolutions and a Soft-Mixture strategy to stabilize semi-supervised
learning. We take object detection as a case study, and experiments on
real-world autonomous driving datasets demonstrate that FedMox effectively
adapts FMs under PSSFL, significantly improving performance with constrained
memory costs on edge devices. Our work paves the way for scalable and
privacy-preserving FM adaptation in federated scenarios.

</details>


### [82] [Benchmarking Training Paradigms, Dataset Composition, and Model Scaling for Child ASR in ESPnet](https://arxiv.org/abs/2508.16576)
*Anyu Ying,Natarajan Balaji Shankar,Chyi-Jiunn Lin,Mohan Shi,Pu Wang,Hye-jin Shim,Siddhant Arora,Hugo Van hamme,Abeer Alwan,Shinji Watanabe*

Main category: cs.LG

TL;DR: 어린이 음성 인식에 대한 연구로, 아동 음성을 위한 훈련 방법 비교 및 SSL 표현의 편향을 분석하였습니다.


<details>
  <summary>Details</summary>
Motivation: ASR(자동 음성 인식)의 발전에도 불구하고 아동 음성 인식은 여전히 어려우며, acoustic variability와 제한된 주석 데이터가 문제입니다.

Method: 여러 데이터셋, SSL 표현(WavLM, XEUS) 및 디코더 아키텍처를 통해 flat-start 훈련과 기존 모델을 비교하였습니다.

Result: SSL 표현은 성인 음성을 편향하고 있으며, 아동 음성의 flat-start 훈련이 이러한 편향을 완화했습니다. 또한 모델 스케일링 분석을 통해 10억 개 파라미터까지 성능 향상이 일관되게 나타났습니다.

Conclusion: Whisper와 같은 독점 모델의 한계를 강조하며 신뢰할 수 있는 아동 음성 연구를 위한 공개 데이터 모델의 필요성을 드러냈습니다.

Abstract: Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 심볼릭 지식을 깊은 학습 아키텍처와 통합하는 최신 접근법이 정적 도메인에서 유망한 결과를 보였지만, 시간 논리 사양을 처리하는 방법은 충분히 탐구되지 않았습니다.


<details>
  <summary>Details</summary>
Motivation: 시간 논리 사양을 깊은 학습 아키텍처에 통합하여 순차 기반 작업을 개선하고자 합니다.

Method: Linear Temporal Logic over finite traces (LTLf)로 표현된 시간 논리 사양을 직접 통합하는 신경상징적 프레임워크를 제안하며, 이를 위해 Fuzzy LTLf 해석을 활용하여 Iterative Local Refinement (ILR) 알고리즘을 확장합니다.

Result: 제안된 Temporal Iterative Local Refinement (T-ILR) 방법을 기존 벤치마크에서 평가한 결과, 정확성과 계산 효율성이 크게 향상되었습니다.

Conclusion: T-ILR은 시간 신경상징적 아키텍처의 분류 성능을 향상시키는 데 기여합니다.

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [84] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: 이 논문은 빠른 의약 자산 실사를 위한 에이전트 AI 시스템 내에서 사용되는 경쟁자 발견 구성 요소를 설명하고 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 의약품 경쟁업체를 신속하게 분석할 수 있는 AI 시스템의 필요성.

Method: 경쟁자 발견 AI 에이전트가 주어진 지표에 대해 모든 관련 의약품의 정보를 수집하고, 이러한 약물의 전형적인 속성을 추출하는 방법을 사용합니다.

Result: 경쟁자 발견 에이전트는 83%의 재현율을 달성하여 OpenAI Deep Research(65%) 및 Perplexity Labs(60%)를 초과했습니다.

Conclusion: 생산 환경에서 시스템이 배포되었으며, 생명공학 VC 투자 기금과의 사례 연구에서 경쟁 분석에 대한 분석가의 처리 시간이 2.5일에서 약 3시간으로 감소했습니다.

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>


### [85] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: AI 기반 ECG 예측 모델의 임상 적용을 위한 XAI 접근법을 제시하는 CoFE 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 기반 ECG 예측 모델을 임상에서 성공적으로 통합하기 위해 설명 가능한 AI(XAI) 접근법의 필요성을 인식하였다.

Method: 특정 특징(예: 진폭 및 간격)이 모델의 예측 결정에 어떻게 영향을 미치는지를 설명하기 위해 CoFE라는 반사실적 ECG를 생성하는 프레임워크를 도입하였다.

Result: CoFE를 사용하여 심실 세동 분류 및 칼륨 수준 회귀 모델에 대한 두 가지 사례 연구를 제시하였으며, ECG 신호의 특징 변화가 기존의 임상 지식과 일치함을 보여주었다.

Conclusion: 유효한 특징이 ECG에서 어디에 나타나는지와 그것이 모델 예측에 어떻게 영향을 미치는지를 명확히 하여 AI-ECG 모델의 해석 가능성을 향상시키고 보다 효과적인 임상 의사 결정을 지원할 것으로 기대한다.

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [86] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 이 논문은 훈련 없는 멀티모달 다중 홉 질문 응답을 위한 Adaptive Planning Graph 기반의 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 멀티모달 질문 응답 방법들은 순차적 탐색에 의존하여 오류에 취약하며, 훈련이 비싼 멀티모달 모델의 필요성이 있다.

Method: Adaptive Planning Graph에 의해 안내되는 훈련 없는 프레임워크를 제안하며, 계획, 검색 및 추론 모듈로 구성된다.

Result: 우리의 접근 방식은 훈련에 의존하는 기존 모델과 비교하여 성능을 일치시키거나 초과함을 보여준다.

Conclusion: 우리는 비용이 많이 드는 특정 작업 훈련 없이 멀티모달 정보의 특성을 유지하면서 최신 모델과 원활하게 통합할 수 있는 방법을 제시한다.

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [87] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: Generative Deep Patient (GDP)는 전자 건강 기록(EHRs)을 활용하여 임상 예측과 내러티브 생성을 동시에 수행하는 다중 모달 기초 모델이다.


<details>
  <summary>Details</summary>
Motivation: EHR의 이질성을 활용하는 것이 환자 결과를 개선하는 데 중요하다.

Method: GDP는 CNN-Transformer 인코더를 통해 구조화된 EHR 시계열을 인코딩하고, cross-modal attention을 통해 비구조화된 EHR과 융합하여 LLaMA 기반 디코더로 구성된다.

Result: 임상 예측에서 GDP는 MIMIC-IV에서 우수한 성능을 보였다: 심부전 AUROC = 0.923, 제2형 당뇨병 AUROC = 0.817, 30일 재입원 AUROC = 0.627.

Conclusion: GDP는 임상적으로 실행 가능한 사건을 예측하고 고품질 임상 내러티브를 생성할 수 있는 단일 다중 모달 기초 모델임을 보여준다.

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [88] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 이 연구는 도시 계획에서 도시 편안함을 평가하기 위한 이론 해석과 방법론을 탐구하고, 다차원 분석, 데이터 지원 및 AI 지원의 세 가지 주요 차원을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 도시 계획의 기본 목표 중 하나는 거주성과 편안함을 보장하는 것이다.

Method: 이 연구는 디지털 계획 내에서 도시 편안함을 평가하기 위한 이론 해석과 방법론을 탐구한다.

Result: 도시 편안함과 관련된 명확한 정의 및 종합 평가 프레임워크는 여전히 모호하다.

Conclusion: 이 연구는 다차원 분석, 데이터 지원 및 AI 도움을 강조하며 도시 편안함 평가에 대한 새로운 통찰을 제공한다.

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [89] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: MSEF는 LLM이 시간 시계열 패턴에 깊게 접근할 수 있도록 하여 시계열 정보의 손실을 줄이는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 예측은 다양한 분야에서 필수적인 작업이며, LLM의 발전을 활용하여 이를 개선할 필요성이 있다.

Method: MSEF는 시간 시계열 패턴을 모든 깊이에서 직접적으로 접근할 수 있게 하며, LLM 레이어 간의 중간 텍스트 표현과 시계열의 임베딩을 융합한다.

Result: MSEF는 7개의 벤치마크에서 기존 방법에 비해 MSE 기준으로 평균 31.8% 성능 향상을 보여준다.

Conclusion: MSEF는 시계열 예측의 효과성을 크게 향상시키며, 관련 코드는 제공된다.

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [90] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 이 논문은 LLM이 사회적 맥락에서 개인화된 추론 스타일을 포착하고 적용할 수 있는지를 평가하기 위한 InMind라는 평가 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 인본 위주의 추론 작업에서 강력한 성과를 보였지만, 사회적 맥락에서 사람들이 해석하고 행동하는 방식에 영향을 미치는 개인화된 추론 스타일을 간과했습니다.

Method: InMind 프레임워크는 구조화된 게임 데이터에 라운드 수준의 전략 추적과 게임 후 반성을 통합하여 LLM이 SDG에서 개인화된 추론 스타일을 캡처하고 적용할 수 있는지를 평가하도록 설계되었습니다.

Result: 일반적인 LLM, 특히 GPT-4는 어휘 단서에 의존하며, 시간의 게임 플레이에 반영을 고정시키거나 발전하는 전략에 적응하는 데 어려움을 겪습니다. 반면, DeepSeek-R1과 같은 추론 강화 LLM은 스타일에 민감한 추론의 조짐을 조기에 나타냅니다.

Conclusion: 현재 LLM의 개인화된 적응 추론 능력의 주요 한계를 밝히고, InMind를 인지적으로 일치하는 인간-AI 상호작용을 향한 한 단계로 위치지었습니다.

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [91] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: IR-Agent는 적외선 스펙트럼에서 분자 구조를 규명하기 위해 설계된 다중 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 적외선 분광법 접근 방식은 전문가의 분석 과정을 반영하지 못하고, 다양한 화학 지식을 통합하는 유연성이 부족하다.

Method: 이 논문에서는 적외선 스펙트럼에서의 분자 구조 규명을 위해 IR-Agent라는 새로운 다중 에이전트 프레임워크를 제안한다. 이 프레임워크는 전문가 중심의 IR 분석 절차를 모사하도록 설계되었으며, 본질적으로 확장 가능하다.

Result: 광범위한 실험을 통해 IR-Agent가 실험적 적외선 스펙트럼에서 기본 성능을 향상시킬 뿐만 아니라 다양한 형태의 화학 정보에 대한 강한 적응성을 보인다는 것을 입증했다.

Conclusion: IR-Agent는 IR 분석의 전반적인 정확성을 개선하고 통합적 추론을 가능하게 하는 상호 보완적인 역할을 수행하는 각 에이전트로 구성되어 있다.

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [92] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 전 세계 음식 관련 주장에 대해 분산되고 발전이 미비한 인프라를 보완하기 위해 음식 주장 추적 네트워크(FCN)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전 세계 음식 환경에서 음식에 대한 여러 주장들이 영향을 미치고 있으며, 이를 추적하고 검증할 수 있는 인프라가 부족하다.

Method: FKG.in의 확장으로 FCN을 제안하였으며, 이를 통해 레딧 데이터와 대형 언어 모델을 활용한 개념 검증을 개발했다.

Result: FCN은 음식 관련 주장 추출 및 검증을 위한 데이터 입력, 구조화된 스킴, 출처 인식 파이프라인을 통합한다.

Conclusion: 우리는 음식 주장을 구조적이고 검증 가능하며 설명 가능한 방식으로 모델링함으로써, 더 투명하고 책임 있는 음식 지식 생태계에 기여하고자 한다.

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [93] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: MLLM의 응용을 통해 다문서적 추론을 다루는 OphthaReason 모델을 제안하며, MM-Retinal-Reason 데이터셋을 활용하여 성능 향상을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 실제 임상 진단은 기본적 추론을 넘어 이질적인 임상 정보와 다중 의료 이미징 데이터를 통합하는 복잡한 추론 과정을 요구한다.

Method: 아나리틱하게 무작위 체계의 깊이를 동적으로 조절하는 새로운 방법인 불확실성 인식 동적 사고(UADT)를 설계하여 기본 및 복잡한 추론 작업에 유연하게 적응할 수 있도록 한다.

Result: 우리 모델은 기본 및 복잡한 추론 작업에서 최첨단 성능을 달성하며, 일반 목적 MLLM 및 의료 MLLM에 비해 최소 24.92m{percentage} 높은 성과를 기록했다.

Conclusion: OphthaReason은 실제 임상적 사고 패턴을 모방하고 시각 중심의 기본 추론 능력을 향상시키는 것을 목표로 한다.

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [94] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: 도시 과학에서 인간 행동 이해는 중요한 분야이며, 본 논문은 새로운 방법인 Preference Chain을 소개하여 교통 시스템에서 인간 행동의 맥락 인식 시뮬레이션을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 도시 환경에서의 인간 행동 이해는 필수적이지만, 특히 신규 개발 지역에서 정확한 행동 데이터 수집이 어려운 문제를 해결하고자 합니다.

Method: Preference Chain은 Graph Retrieval-Augmented Generation(RAG)와 대규모 언어 모델(LLMs)을 통합하여 교통 시스템에서 인간 행동의 맥락 인식 시뮬레이션을 향상시키는 새로운 방법입니다.

Result: Replica 데이터셋을 이용한 실험 결과, Preference Chain이 실제 교통 수단 선택과 기본 LLM보다 잘 일치하는 것으로 나타났습니다.

Conclusion: 이 방법은 신흥 도시의 도시 이동성 모델링, 개인화된 이동 행동 분석, 역동적인 교통 예측 등에서 응용 가능성을 보여 줍니다. 단, 느린 추론 속도와 환각 위험 같은 제한점이 존재하지만, 데이터가 부족한 환경에서 복잡한 인간 행동을 시뮬레이션하기 위한 유망한 프레임워크를 제공합니다.

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [95] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: 모델 병합은 여러 머신러닝 모델의 전문 지식을 단일 모델로 통합하는 강력한 기술이다. 하지만 기존 방법은 모델 매개변수를 고정된 그룹으로 수동으로 나누어야 하며 이는 성능을 제한한다. 본 논문에서는 이러한 제한을 극복하기 위해 자연 환경에서의 자원 경쟁에서 영감을 받은 세 가지 주요 기능을 가진 진화 알고리즘인 자연적 틈새의 모델 병합(M2N2)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법의 한계를 극복하고 모델 병합의 가능성을 극대화하기 위해.

Method: 진화 알고리즘 M2N2는 동적인 병합 경계 조정, 다양성 유지 메커니즘, 그리고 모델 조합을 위한 휴리스틱 기반 매력 메트릭을 포함한다.

Result: 실험 결과, M2N2를 통해 모델을 처음부터 진화시킬 수 있으며, MNIST 분류기에서 CMA-ES에 필적하는 성능을 보였다.

Conclusion: M2N2는 전문화된 언어 및 이미지 생성 모델 병합에 효과적이며, 피트니스 함수에 의해 최적화되지 않은 중요한 모델 기능을 보존한다.

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [96] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: 이 연구는 GROW-AI 프레임워크를 확장하여 인공지능의 성장 가능성을 평가하려고 하며, 이는 투링 테스트의 자연스러운 후속작입니다.


<details>
  <summary>Details</summary>
Motivation: GROW-AI는 '기계가 성장할 수 있는가?'라는 질문에 답하기 위해 설계되었습니다.

Method: 평가 기준은 6개의 주요 기준(C1-C6)으로 구성되어 있으며, 각 기준은 특정 게임을 통해 평가됩니다. 이 게임은 인간 차원과 AI로의 변환을 탐구하는 네 가지 영역으로 나뉩니다.

Result: 이 방법론은 AI 엔티티의 '성장' 수준을 일관되고 비교 가능하게 평가할 수 있는 방법을 제공합니다.

Conclusion: GROW-AI는 인공지능 엔티티가 성숙으로 나아가는 진화 경로를 포착하여 성과를 측정하는 데 그치지 않고, 통합 테스트 형식에서 인간 세계의 '성장' 과정을 인공지능 세계로 개념적으로 전이합니다.

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [97] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0은 유연하고 효율적인 도구 기반의 에이전트-환경 상호 작용을 지원하여, 실용적인 에이전틱 애플리케이션 개발을 용이하게 한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 빠른 발전에 힘입어, 에이전트는 고유의 지식과 동적인 도구 사용을 결합할 수 있는 능력이 향상되었다.

Method: AgentScope는 에이전틱 애플리케이션에 필수적인 기본 구성 요소를 추상화하고 통합된 인터페이스와 확장 가능한 모듈을 제공한다.

Result: AgentScope는 고급 에이전트 수준 인프라를 제공하며, 다양한 내장 에이전트를 특정 실용 시나리오에 맞게 통합함으로써 인간-에이전트 및 에이전트-에이전트 상호 작용 패턴을 풍부하게 하고 실행 효율성을 향상시킨다.

Conclusion: 이러한 개선을 통해 AgentScope는 확장 가능하고 적응력이 뛰어나며 효과적인 에이전틱 애플리케이션 구축을 위한 실용적인 기반을 제공한다.

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [98] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: Vision-Language-Action 모델들이 로봇 작업에서 뛰어난 성과를 보이고 있다. 이 논문에서는 잘못된 전제로 이루어진 지시를 인식하고 응답하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: VLA 모델이 로봇 작업에서 효과적으로 언어 지시를 해석하는 데 중점을 두고 있으며, 잘못된 요청에 대한 이해도 필요하다.

Method: Instruct-Verify-and-Act(IVA)라는 통합 프레임워크를 제안하여 잘못된 전제를 감지하고 언어 기반의 명확화 및 수정, 실행 가능한 대안을 제시한다.

Result: IVA는 기존 방법에 비해 잘못된 전제 탐지 정확도를 97.56% 향상시켰고, 잘못된 전제 시나리오에서의 성공적인 응답률을 50.78% 증가시켰다.

Conclusion: 이 연구는 VLA 모델이 잘못된 지시를 처리하는 데 있어 혁신적인 접근 방식을 제공하며, 강력한 언어 기반 상호작용의 가능성을 보여준다.

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [99] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 이 논문에서는 mmWave MIMO 시스템에서의 효율적이고 신뢰할 수 있는 빔 정렬을 위한 인과 관계를 통합한 심층 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 6G 및 그 이상의 통신이 빠르고 적응 가능하며 실세계 불확실성에 견딜 수 있도록 하기 위해 효율적인 빔 정렬이 필요합니다.

Method: 인과 발견을 빔 관리 파이프라인에 통합하는 인과 인식 DL 프레임워크를 제안하며, 최소한의 관련 입력 집합을 식별하기 위한 새로운 두 단계 인과 빔 선택 알고리즘을 도입합니다.

Result: 제안된 인과 빔 선택 알고리즘은 전통적인 방법의 성능과 일치하면서도 인과적으로 관련된 특징에만 집중하여 입력 선택 시간을 94.4% 줄이고 빔 스위핑 오버헤드를 59.4% 감소시킵니다.

Conclusion: 본 연구는 딥러닝 기반의 빔 정렬에서 인과 관계의 중요성을 강조하며, 성능을 유지하면서도 효율성을 크게 향상시키는 방법을 제시합니다.

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [100] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: 법적 판단 예측이 법률 분야에서 점점 더 중요해지고 있으며, 본 논문에서는 기존의 대형 언어 모델이 법적 지식 부족으로 인한 추론 부족 문제를 가지고 있음을 제시합니다. 이에 따라 법적 지식을 동적으로 습득하는 GLARE라는 에이전틱 법적 추론 프레임워크를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 법적 판단 예측이 법률 분야에서 점점 더 중요해지고 있는 점을 강조하며, 기존 모델의 한계를 극복할 필요성을 제시합니다.

Method: GLARE라는 프레임워크를 통해 다양한 모듈을 호출하여 법적 지식을 동적으로 습득하는 방법을 소개합니다.

Result: 실제 데이터셋에서 수행한 실험을 통해 제안한 방법의 효과성을 검증합니다.

Conclusion: 분석 과정에서 생성된 추론 체인은 해석 가능성을 높이고 실제 적용 가능성을 제공합니다.

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [101] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: 이 논문은 VLM의 제로샷 능력을 강화하는 MoDER 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델이 지속적 학습에 미치는 영향을 탐구하고, 제로샷 분류 능력을 최적화할 필요성을 강조한다.

Method: MoDular Embedding Recomposition(MoDER)이라는 접근 방식을 제안하여, 여러 텍스트 전문가를 모듈화된 프레임워크에서 훈련하고, 이를 기반 허브에 저장한다.

Result: Class-IL 및 MTIL 두 가지 제로샷 점진적 프로토콜에서 14개의 데이터셋을 사용해 방법의 효과를 입증했다.

Conclusion: MoDER을 통해 VLM의 제로샷 능력을 현저히 향상시킬 수 있음을 보여준다.

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [102] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 이 논문은 확산 모델을 활용하여 신경망이 복잡한 논리 제약을 학습하고 상징적 추론을 수행하도록 하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 신경망이 복잡한 논리 제약을 학습하고 상징적 추론을 수행하는 것은 중요한 도전 과제입니다.

Method: 두 단계의 훈련 전략을 사용하는 확산 기반 파이프라인을 통해 기본적인 추론 능력을 길러주고, 논리 제약의 체계적인 학습을 강조합니다.

Result: 이 방법론은 고전적인 상징적 추론 벤치마크에서 우수한 정확성과 논리적 일관성을 입증합니다.

Conclusion: 우리의 접근 방식은 신경망 간의 뛰어난 정확도와 논리적 일관성을 달성합니다.

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>
