<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.AI](#cs.AI) [Total: 14]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Cooperative UAVs for Remote Data Collection under Limited Communications: An Asynchronous Multiagent Learning Framework](https://arxiv.org/abs/2601.10849)
*Cuong Le,Symeon Chatzinotas,Thang X. Vu*

Main category: cs.MA

TL;DR: 이 논문은 여러 무인항공기(UAV)의 경로와 대역폭 할당을 함께 최적화하여 에너지 효율성을 향상시키는 방법을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: UAV 간의 동기화가 불가능한 시스템의 중요한 측면에 주목하고, 기존의 학습 기반 솔루션이 이러한 비동기 환경에서 학습하도록 설계되지 않았음을 지적합니다.

Method: 경로 계획 문제를 분산 부분 관찰 가능 반 마르코프 결정 과정으로 공식화하고, UAV의 협력 정책을 학습하기 위한 비동기 다중 에이전트 학습 알고리즘을 도입합니다.

Result: UAV의 경로 정책이 학습된 후, 각 수집 지점에서의 지역 관찰에 기반하여 대역폭 할당을 최적으로 해결할 수 있습니다.

Conclusion: 제안된 방법의 에너지 효율성과 임무 완료 시간을 기준으로 한 포괄적인 실험 결과가 다른 학습 기반 및 휴리스틱 기준선보다 우수함을 입증합니다. 또한 학습된 정책은 다양한 환경 조건에서도 강건성을 나타냅니다.

Abstract: This paper addresses the joint optimization of trajectories and bandwidth allocation for multiple Unmanned Aerial Vehicles (UAVs) to enhance energy efficiency in the cooperative data collection problem. We focus on an important yet underestimated aspect of the system, where action synchronization across all UAVs is impossible. Since most existing learning-based solutions are not designed to learn in this asynchronous environment, we formulate the trajectory planning problem as a Decentralized Partially Observable Semi-Markov Decision Process and introduce an asynchronous multi-agent learning algorithm to learn UAVs' cooperative policies. Once the UAVs' trajectory policies are learned, the bandwidth allocation can be optimally solved based on local observations at each collection point. Comprehensive empirical results demonstrate the superiority of the proposed method over other learning-based and heuristic baselines in terms of both energy efficiency and mission completion time. Additionally, the learned policies exhibit robustness under varying environmental conditions.

</details>


### [2] [Can Small Agent Collaboration Beat a Single Big LLM?](https://arxiv.org/abs/2601.11327)
*Agata Żywot,Xinyi Chen,Maarten de Rijke*

Main category: cs.MA

TL;DR: 소형 보조 도구를 이용한 에이전트가 대형 단일 모델과 비교하여 GAIA 벤치마크에서 성능을 더욱 향상시킬 수 있는지를 연구한 보고서 중.


<details>
  <summary>Details</summary>
Motivation: 소형 에이전트가 대형 모델의 성능을 초과 또는 동등하게 달성할 수 있는지를 이해하기 위함.

Method: Qwen3 모델을 사용하여 Agentic-Reasoning 프레임워크를 수정하고, 모델 크기, 명시적 사고 방식, 도구 사용의 영향을 분석함.

Result: 도구 증강이 가장 큰 이익을 제공하며, 4B 모델이 도구 접근 없이 32B 모델을 초과하는 성능을 기록함.

Conclusion: 도구 사용이 성능에 긍정적인 영향을 미치지만, 명시적 사고 방식은 상황에 따라 다르며, 적절한 구성이 필요함.

Abstract: This report studies whether small, tool-augmented agents can match or outperform larger monolithic models on the GAIA benchmark. Using Qwen3 models (4B-32B) within an adapted Agentic-Reasoning framework, we isolate the effects of model scale, explicit thinking (no thinking, planner-only, or full), and tool use (search, code, mind-map). Tool augmentation provides the largest and most consistent gains. Using tools, 4B models can outperform 32B models without tool access on GAIA in our experimental setup. In contrast, explicit thinking is highly configuration- and difficulty-dependent: planner-only thinking can improve decomposition and constraint tracking, while unrestricted full thinking often degrades performance by destabilizing tool orchestration, leading to skipped verification steps, excessive tool calls, non-termination, and output-format drift.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents](https://arxiv.org/abs/2601.10820)
*Himanshu Thakur,Anusha Kamath,Anurag Muthyala,Dhwani Sanmukhani,Smruthi Mukund,Jay Katukuri*

Main category: cs.LG

TL;DR: 코드 생성 모델의 발전이 기능 공학 자동화를 위한 기회를 열었지만, 실제 ML 팀에서의 채택은 여전히 여러 과제에 제한되어 있다.


<details>
  <summary>Details</summary>
Motivation: 실제 ML 팀에서 기능 공학의 자동화를 위한 코드 생성 모델 채택의 한계를 해결하기 위해.

Method: 계획자가 안내하는 제한된 토폴로지 다중 에이전트 프레임워크를 통해 다단계 방식으로 리포지토리의 코드를 생성한다.

Result: 새로운 사내 데이터 세트에서, 수동 제작 및 계획되지 않은 워크플로우에 비해 평가 메트릭이 각각 38% 및 150% 개선되었다.

Conclusion: 추천 모델 기능 구축 시, 기능 공학 사이클이 세 주에서 하루로 단축되는 등의 실질적인 영향을 미쳤다.

Abstract: Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.

</details>


### [4] [Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation](https://arxiv.org/abs/2601.10911)
*Zhang Xiaocai,Xiao Zhe,Liang Maohan,Liu Tao,Li Haijiang,Zhang Wenbin*

Main category: cs.LG

TL;DR: 이 논문은 지속 가능한 해양 내비게이션을 위한 Curriculum Reinforcement Learning (CRL) 프레임워크를 제안하며, 실제 데이터를 기반으로 한 해양 시뮬레이션 환경과 연료 소비 예측 모듈을 통합한다.


<details>
  <summary>Details</summary>
Motivation: 해양 운송에서 지속 가능성이 점점 더 중요해지고 있으며, 기후 변화와 안전성을 향상시키기 위해 자동화된 내비게이션의 필요성이 커지고 있다.

Method: CRL 프레임워크를 통해 실제 선박 이동 데이터를 이용한 시뮬레이션 환경과 머신 러닝 기반 연료 소비 예측 모듈을 통합하여 제작하였다.

Result: 인도양의 해역에서 제안한 방법을 검증하여, 지속 가능하고 안전한 선박 내비게이션을 가능하게 함을 입증했다.

Conclusion: 이 프레임워크는 복잡한 작업을 효과적으로 처리하면서도 안정적이고 효율적인 학습을 보장한다.

Abstract: Sustainability is becoming increasingly critical in the maritime transport, encompassing both environmental and social impacts, such as Greenhouse Gas (GHG) emissions and navigational safety. Traditional vessel navigation heavily relies on human experience, often lacking autonomy and emission awareness, and is prone to human errors that may compromise safety. In this paper, we propose a Curriculum Reinforcement Learning (CRL) framework integrated with a realistic, data-driven marine simulation environment and a machine learning-based fuel consumption prediction module. The simulation environment is constructed using real-world vessel movement data and enhanced with a Diffusion Model to simulate dynamic maritime conditions. Vessel fuel consumption is estimated using historical operational data and learning-based regression. The surrounding environment is represented as image-based inputs to capture spatial complexity. We design a lightweight, policy-based CRL agent with a comprehensive reward mechanism that considers safety, emissions, timeliness, and goal completion. This framework effectively handles complex tasks progressively while ensuring stable and efficient learning in continuous action spaces. We validate the proposed approach in a sea area of the Indian Ocean, demonstrating its efficacy in enabling sustainable and safe vessel navigation.

</details>


### [5] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: 본 연구에서는 파라메트릭 스킬 전이(PaST)를 제안하여 지식 적응을 위한 효율적이고 효과적인 방법을 모듈식으로 지원한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 지식 컷오프 문제로 인해 새로운 정보를 직접적으로 내재화하는 데 어려움을 겪고 있으며, 이는 모델의 질문 응답 및 의사 결정 능력에 영향을 미친다.

Method: PaST 프레임워크를 통해 출처 도메인에서 도메인 무관한 스킬 벡터를 추출하고, 이를 통해 새로운 데이터에서 경량화된 감독 세밀 조정(SFT)을 마친 후 목표 모델에 지식 조작 스킬을 주입한다.

Result: SQuAD 및 LooGLE와 같은 지식 포함 QA와 ToolBench 벤치마크에서 PaST 방법이 효과적임을 입증하였다. SQuAD에서 PaST는 최신 자가 편집 SFT 기준선을 최대 9.9점 초과 달성하였으며, LooGLE에서는 8.0점의 정확도 상승을 보였고, ToolBench에서는 평균 +10.3점의 성공률 향상을 기록하였다.

Conclusion: 우리의 방법은 지식 적응 및 도메인 간 전이 가능성을 높이는 데 있어 강력한 확장성을 보여준다.

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [6] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: 오프라인 강화 학습을 이용한 자율 CPU 전력 컨트롤러 설계에 관한 논문.


<details>
  <summary>Details</summary>
Motivation: 현대 컴퓨팅 인프라 설계에서 에너지 효율은 성능, 비용, 확장성, 내구성에 영향을 미치는 중요한 요소가 되었습니다.

Method: 오프라인 강화 학습을 활용하여 훈련 전에 수집된 상태 전이 데이터셋을 사용하여 자율 CPU 전력 컨트롤러를 설계했습니다.

Result: 다양한 벤치마크에서 전력 제공을 통제하여 에너지 소비를 크게 줄일 수 있음을 보여주었습니다.

Conclusion: 오프라인으로 훈련된 에이전트는 허용 가능한 성능 저하 비용으로 에너지 소비를 상당히 감소시킬 수 있습니다.

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [7] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: 이 논문에서는 쌍둥이 오토인코더를 기반으로 한 새로운 데이터 기반 잠재 공간 추론 프레임워크를 제안하여 역 문제를 해결할 때 관측의 불일치를 처리합니다.


<details>
  <summary>Details</summary>
Motivation: 관측의 불일치를 해결하기 위해 역 문제를 해결하는 새로운 접근 방식을 찾기 위해.

Method: 매개변수 공간과 관측 공간 각각에 대해 하나의 오토인코더를 사용하고 이들 간의 잠재 공간에서 학습된 매핑으로 연결된 쌍둥이 오토인코더를 이용한다.

Result: 손상된 데이터의 복원을 가능하게 하고, 복원된 데이터를 사용하여 매개변수 추정을 수행함으로써 더 정확한 복원을 얻는다.

Conclusion: 제안된 방법은 의학적 단층촬영 및 지구 물리학적 지진파형 역산에서 두 가지 이미징 예시를 통해 입증되었다.

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


### [8] [Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.11401)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 이 논문에서는 다중 에이전트 강화 학습에서 신뢰도 할당 문제를 해결하기 위한 새로운 가치 함수인 확산 가치 함수(DVF)를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습(MARL)에서 신뢰도 할당은 핵심적인 도전 과제로, 특히 구조화된 지역 상호작용이 있는 대규모 시스템에서 문제가 됩니다.

Method: 확산 가치 함수(DVF)는 영향 그래프를 통해 보상을 확산시켜 각 에이전트에 값 구성 요소를 할당하는 GMDP를 위한 팩터링된 가치 함수입니다.

Result: DVF는 잘 정의되고 벨만 고정점을 가지며, 전역 할인 가치를 평균화 속성을 통해 분해할 수 있음을 보여줍니다. DA2C는 여러 벤치마크에서 지역 및 전역 비평자 기준을 지속적으로 초월하며 평균 보상을 최대 11% 향상시킵니다.

Conclusion: 이 연구는 DVF를 사용하여 분산화 알고리즘을 학습하는 데 있어 통신 비용 하에서 효과적이며, 새로운 알고리즘 개발로 GMDP를 효율적으로 처리할 수 있음을 보여줍니다.

Abstract: Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [9] [Too Helpful to Be Safe: User-Mediated Attacks on Planning and Web-Use Agents](https://arxiv.org/abs/2601.10758)
*Fengchao Chen,Tingmin Wu,Van Nguyen,Carsten Rudolph*

Main category: cs.CR

TL;DR: 대형 언어 모델(LLMs)이 대화에서 엔드 투 엔드 작업 실행으로 나아가 더 유용해졌지만, 사용자가 제공한 내용을 기반으로 작동함으로써 새로운 보안 위험을 초래함을 보여줍니다. 사용자가 공격자의 통제된 내용으로 의도치 않게 레이킹될 수 있는 사용자 매개 공격을 연구하고, 12개의 상업적 에이전트를 평가한 결과, 안전 요청 없이 에이전트가 안전 제약을 우회하는 경우가 많음을 발견했습니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)이 사용자에게 도움을 주기 위해 엔드 투 엔드 작업을 수행할 수 있게 되면서 발생하는 새로운 보안 위험들을 이해하고 해결하기 위해.

Method: 편리한 사용자를 속여 신뢰할 수 없는 내용을 에이전트에 전달하도록 만드는 사용자 매개 공격을 연구하고, 12개의 상업적 에이전트를 샌드박스 환경에서 평가하여 다양한 안전 요청 시나리오에서의 행동을 비교함.

Result: 여행 계획 에이전트는 안전 요청 없이 92% 이상에서 안전 제한을 우회하며, 웹 사용 에이전트는 위험한 작업을 거의 결정적으로 실행하고, 모든 경우에서 적시 테스트의 100% 우회율에 도달했습니다.

Conclusion: 에이전트가 안전 능력이 부족한 것이 아니라, 안전 요청을 우선시하지 않음으로써 발생한 문제를 드러냅니다. 요청이 없으면 목표 지향적 실행으로 돌아가며, 너무 많이 실행하여 불필요한 데이터 유출과 실제 피해를 초래합니다.

Abstract: Large Language Models (LLMs) have enabled agents to move beyond conversation toward end-to-end task execution and become more helpful. However, this helpfulness introduces new security risks stem less from direct interface abuse than from acting on user-provided content. Existing studies on agent security largely focus on model-internal vulnerabilities or adversarial access to agent interfaces, overlooking attacks that exploit users as unintended conduits. In this paper, we study user-mediated attacks, where benign users are tricked into relaying untrusted or attacker-controlled content to agents, and analyze how commercial LLM agents respond under such conditions. We conduct a systematic evaluation of 12 commercial agents in a sandboxed environment, covering 6 trip-planning agents and 6 web-use agents, and compare agent behavior across scenarios with no, soft, and hard user-requested safety checks. Our results show that agents are too helpful to be safe by default. Without explicit safety requests, trip-planning agents bypass safety constraints in over 92% of cases, converting unverified content into confident booking guidance. Web-use agents exhibit near-deterministic execution of risky actions, with 9 out of 17 supported tests reaching a 100% bypass rate. Even when users express soft or hard safety intent, constraint bypass remains substantial, reaching up to 54.7% and 7% for trip-planning agents, respectively. These findings reveal that the primary issue is not a lack of safety capability, but its prioritization. Agents invoke safety checks only conditionally when explicitly prompted, and otherwise default to goal-driven execution. Moreover, agents lack clear task boundaries and stopping rules, frequently over-executing workflows in ways that lead to unnecessary data disclosure and real-world harm.

</details>


### [10] [Multi-Agent Taint Specification Extraction for Vulnerability Detection](https://arxiv.org/abs/2601.10865)
*Jonah Ghebremichael,Saastha Vasan,Saad Ullah,Greg Tystahl,David Adei,Christopher Kruegel,Giovanni Vigna,William Enck,Alexandros Kapravelos*

Main category: cs.CR

TL;DR: 이 논문에서는 JavaScript에 대한 정적 오염 분석을 위한 SemTaint라는 다중 에이전트 시스템을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 패턴 기반 접근 방식에 비해 보다 높은 품질의 취약점 탐지 결과를 제공하는 SAST 도구의 필요성이 커지고 있습니다.

Method: SemTaint는 대형 언어 모델의 의미적 이해와 전통적인 정적 프로그램 분석을 결합하여 소스, 싱크, 호출 엣지 및 각 패키지에 맞춘 라이브러리 흐름 요약을 추출합니다.

Result: SemTaint는 SAST 도구인 CodeQL과 통합되어 CodeQL로 기존에 탐지할 수 없었던 162개의 취약점 중 106개를 탐지하는 성과를 보여주었습니다.

Conclusion: LLMs가 기존의 정적 프로그램 분석 알고리즘을 실질적으로 강화할 수 있음을 입증하며, 상징적 추론과 의미적 이해의 강점을 결합하여 취약점 탐지를 개선할 수 있음을 보여줍니다.

Abstract: Static Application Security Testing (SAST) tools using taint analysis are widely viewed as providing higher-quality vulnerability detection results compared to traditional pattern-based approaches. However, performing static taint analysis for JavaScript poses two major challenges. First, JavaScript's dynamic features complicate data flow extraction required for taint tracking. Second, npm's large library ecosystem makes it difficult to identify relevant sources/sinks and establish taint propagation across dependencies. In this paper, we present SemTaint, a multi-agent system that strategically combines the semantic understanding of Large Language Models (LLMs) with traditional static program analysis to extract taint specifications, including sources, sinks, call edges, and library flow summaries tailored to each package. Conceptually, SemTaint uses static program analysis to calculate a call graph and defers to an LLM to resolve call edges that cannot be resolved statically. Further, it uses the LLM to classify sources and sinks for a given CWE. The resulting taint specification is then provided to a SAST tool, which performs vulnerability analysis. We integrate SemTaint with CodeQL, a state-of-the-art SAST tool, and demonstrate its effectiveness by detecting 106 of 162 vulnerabilities previously undetectable by CodeQL. Furthermore, we find 4 novel vulnerabilities in 4 popular npm packages. In doing so, we demonstrate that LLMs can practically enhance existing static program analysis algorithms, combining the strengths of both symbolic reasoning and semantic understanding for improved vulnerability detection.

</details>


### [11] [Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents](https://arxiv.org/abs/2601.10955)
*Kaiyu Zhou,Yongsen Zheng,Yicheng He,Meng Xue,Xueluan Gong,Yuji Wang,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: 본 논문에서는 현대 대형 언어 모델(Large Language Model, LLM) 에이전트에서 에이전트-툴 간 통신 루프가 사이버 공격의 중요한 표면이라는 점을 강조하고, 다단계에서 작동하는 새로운 경제적 서비스 거부 공격 기법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 서비스 거부(DoS) 공격은 단일 턴 방식에 의존하며, 과제 지향적 접근 방식이 부족하여 현대 LLM 에이전트의 필요를 충족하지 못합니다.

Method: 다단계 경제적 서비스 거부 공격을 도입하고, 정상적으로 완료된 작업의 외관을 유지하며 도구 레이어에서 작동합니다. 이는 모델 컨텍스트 프로토콜(MCP)에 호환되는 도구 서버에서 텍스트 가시 필드와 템플릿 기반 반환 정책을 조정하여 이루어집니다.

Result: 여섯 가지 LLM에서 이 공격은 60,000 토큰을 초과하는 경로로 작업을 확장하고, 비용을 최대 658배까지 증가시켰으며, 에너지를 100~560배 증가시킵니다.

Conclusion: 결과적으로 에이전트-툴 인터페이스는 보안 프론티어로 격상되며, 최종 답변을 검증하는 것을 넘어서 전체 에이전트 프로세스의 경제적 및 계산적 비용을 모니터링해야 함을 시사합니다.

Abstract: The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.

</details>


### [12] [AJAR: Adaptive Jailbreak Architecture for Red-teaming](https://arxiv.org/abs/2601.10971)
*Yipu Dou,Wang Yang*

Main category: cs.CR

TL;DR: AJAR은 복잡한 다단계 공격을 효과적으로 모의할 수 있는 새로운 레드팀 프레임워크로, 안전한 AI 행동 보장을 목표로 합니다.


<details>
  <summary>Details</summary>
Motivation: AI 안전성 향상과 도구 사용의 복잡성을 해결하기 위해 새로운 레드팀 프레임워크가 필요합니다.

Method: AJAR는 프로토콜 중심의 인지 오케스트레이션을 통해 적대적 논리를 실행 루프와 분리하고, X-Teaming과 같은 최신 알고리즘을 표준화된 서비스로 캡슐화합니다.

Result: AJAR는 툴 사용 환경에서 상태 기반 백트래킹을 수행할 수 있는 능력을 입증했으며, 코드 실행을 통한 새로운 공격 벡터와 인격 기반 공격의 복잡한 동역학을 탐구하였습니다.

Conclusion: AJAR는 새로운 공격 표면에 대한 표준화된 평가를 위한 오픈 소스 프로젝트로 제공됩니다.

Abstract: As Large Language Models (LLMs) evolve from static chatbots into autonomous agents capable of tool execution, the landscape of AI safety is shifting from content moderation to action security. However, existing red-teaming frameworks remain bifurcated: they either focus on rigid, script-based text attacks or lack the architectural modularity to simulate complex, multi-turn agentic exploitations. In this paper, we introduce AJAR (Adaptive Jailbreak Architecture for Red-teaming), a proof-of-concept framework designed to bridge this gap through Protocol-driven Cognitive Orchestration. Built upon the robust runtime of Petri, AJAR leverages the Model Context Protocol (MCP) to decouple adversarial logic from the execution loop, encapsulating state-of-the-art algorithms like X-Teaming as standardized, plug-and-play services. We validate the architectural feasibility of AJAR through a controlled qualitative case study, demonstrating its ability to perform stateful backtracking within a tool-use environment. Furthermore, our preliminary exploration of the "Agentic Gap" reveals a complex safety dynamic: while tool usage introduces new injection vectors via code execution, the cognitive load of parameter formatting can inadvertently disrupt persona-based attacks. AJAR is open-sourced to facilitate the standardized, environment-aware evaluation of this emerging attack surface. The code and data are available at https://github.com/douyipu/ajar.

</details>


### [13] [Understanding Help Seeking for Digital Privacy, Safety, and Security](https://arxiv.org/abs/2601.11398)
*Kurt Thomas,Sai Teja Peddinti,Sarah Meiklejohn,Tara Matthews,Amelia Hassoun,Animesh Srivastava,Jessica McClearn,Patrick Gage Kelley,Sunny Consolvo,Nina Taft*

Main category: cs.CR

TL;DR: 이 연구는 디지털 프라이버시, 안전 및 보안에 대한 사용자 요청을 이해하고, 그 결과로 사용자에게 더 나은 지원 자원을 개발하는데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 사용자들이 직면하는 디지털 프라이버시, 안전 및 보안 위협에 대한 지원 요청 방식을 이해하기 위해서이다.

Method: 1억 개 이상의 Reddit 게시물을 질적 코딩과 LLM 미세 조정을 통해 분석하여 디지털 프라이버시, 안전 또는 보안 도움을 요청하는 패턴을 식별하였다.

Result: 300만 개의 관련 게시물을 93%의 정밀도와 재현율로 분리하고, 보안 도구, 프라이버시 설정, 사기, 계정 침해, 콘텐츠 조정 등의 주제로 자동 주석을 달았다.

Conclusion: 우리의 연구는 사용자 안내서나 LLM 도움 제공 에이전트와 같은 사용자에게 더 나은 자원을 개발하는 데 기여하지만, 사용자들이 복잡한 위협, 플랫폼, 완화, 맥락, 감정의 조합을 통해 지원받는 데 내재된 도전 과제를 강조한다.

Abstract: The complexity of navigating digital privacy, safety, and security threats often falls directly on users. This leads to users seeking help from family and peers, platforms and advice guides, dedicated communities, and even large language models (LLMs). As a precursor to improving resources across this ecosystem, our community needs to understand what help seeking looks like in the wild. To that end, we blend qualitative coding with LLM fine-tuning to sift through over one billion Reddit posts from the last four years to identify where and for what users seek digital privacy, safety, or security help. We isolate three million relevant posts with 93% precision and recall and automatically annotate each with the topics discussed (e.g., security tools, privacy configurations, scams, account compromise, content moderation, and more). We use this dataset to understand the scope and scale of help seeking, the communities that provide help, and the types of help sought. Our work informs the development of better resources for users (e.g., user guides or LLM help-giving agents) while underscoring the inherent challenges of supporting users through complex combinations of threats, platforms, mitigations, context, and emotions.

</details>


### [14] [IMS: Intelligent Hardware Monitoring System for Secure SoCs](https://arxiv.org/abs/2601.11447)
*Wadid Foudhaili,Aykut Rencber,Anouar Nechi,Rainer Buchty,Mladen Berekovic,Andres Gomez,Saleh Mulhem*

Main category: cs.CR

TL;DR: 이 논문은 AXI 프로토콜의 보안 취약성을 해결하기 위한 지능형 하드웨어 모니터링 시스템을 제안하며, 실시간으로 AXI 프로토콜 위반을 감지하는 기술을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 현대 시스템 온 칩(SoC)에서 AXI 프로토콜은 프로토콜 위반 공격을 통해 서비스 거부 공격(DoS)을 가능하게 하는 보안 취약점이 존재한다.

Method: 신경망을 활용하여 높은 감지 정확도를 달성하는 지능형 하드웨어 모니터링 시스템(IMS)을 제안하며, 모델 훈련을 위해 헤더 필드 조작 및 체계적인 악의적 작업을 통해 DoS 공격을 수행하고 AXI 트랜잭션을 기록하여 훈련 데이터셋을 구축한다.

Result: 양자화 최적화된 신경망을 배포하여 98.7%의 감지 정확도, <=3%의 대기 시간 오버헤드, 초당 250만 건 이상의 추론 처리량을 달성한다.

Conclusion: 하드웨어 크기가 작고, 리소스 제약이 있는 엣지 환경에서 경량 보안 모니터링의 가능성을 입증한다.

Abstract: In the modern Systems-on-Chip (SoC), the Advanced eXtensible Interface (AXI) protocol exhibits security vulnerabilities, enabling partial or complete denial-of-service (DoS) through protocol-violation attacks. The recent countermeasures lack a dedicated real-time protocol semantic analysis and evade protocol compliance checks. This paper tackles this AXI vulnerability issue and presents an intelligent hardware monitoring system (IMS) for real-time detection of AXI protocol violations. IMS is a hardware module leveraging neural networks to achieve high detection accuracy. For model training, we perform DoS attacks through header-field manipulation and systematic malicious operations, while recording AXI transactions to build a training dataset. We then deploy a quantization-optimized neural network, achieving 98.7% detection accuracy with <=3% latency overhead, and throughput of >2.5 million inferences/s. We subsequently integrate this IMS into a RISC-V SoC as a memory-mapped IP core to monitor its AXI bus. For demonstration and initial assessment for later ASIC integration, we implemented this IMS on an AMD Zynq UltraScale+ MPSoC ZCU104 board, showing an overall small hardware footprint (9.04% look-up-tables (LUTs), 0.23% DSP slices, and 0.70% flip-flops) and negligible impact on the overall design's achievable frequency. This demonstrates the feasibility of lightweight, security monitoring for resource-constrained edge environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Japanese AI Agent System on Human Papillomavirus Vaccination: System Design](https://arxiv.org/abs/2601.10718)
*Junyu Liu,Siwen Yang,Dexiu Ma,Qian Niu,Zequn Zhang,Momoko Nagai-Tanima,Tomoki Aoyama*

Main category: cs.AI

TL;DR: HPV 백신 hesitancy 문제를 해결하기 위해 이 연구는 환자와 의료 기관을 위한 AI 시스템을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 일본에서 HPV 백신 접종 권장 사항이 중단된 2013년부터 2021년까지의 정보 공백이 심각해짐에 따라, 이 연구는 백신 정보의 전달과 공공 담론 분석의 필요성을 강조한다.

Method: AI 대화형 인터페이스를 통해 검증된 HPV 백신 정보를 제공하고 사용자 상호작용과 소셜 미디어를 기반으로 한 분석 보고서를 생성하는 이중 목적의 AI 에이전트 시스템을 개발하였다.

Result: 챗봇은 단일 턴 평가에서 유용성 점수 평균 4.80, 다중 턴 평가에서 4.98을 기록하며, 보고서 생성 시스템은 모든 기간에 걸쳐 5.00의 참조 유효성을 달성하였다.

Conclusion: 이 연구는 양방향 HPV 백신 소통을 위한 통합 AI 에이전트 시스템의 실현 가능성을 보여준다.

Abstract: Human papillomavirus (HPV) vaccine hesitancy poses significant public health challenges, particularly in Japan where proactive vaccination recommendations were suspended from 2013 to 2021. The resulting information gap is exacerbated by misinformation on social media, and traditional ways cannot simultaneously address individual queries while monitoring population-level discourse. This study aimed to develop a dual-purpose AI agent system that provides verified HPV vaccine information through a conversational interface while generating analytical reports for medical institutions based on user interactions and social media. We implemented a system comprising: a vector database integrating academic papers, government sources, news media, and social media; a Retrieval-Augmented Generation chatbot using ReAct agent architecture with multi-tool orchestration across five knowledge sources; and an automated report generation system with modules for news analysis, research synthesis, social media sentiment analysis, and user interaction pattern identification. Performance was assessed using a 0-5 scoring scale. For single-turn evaluation, the chatbot achieved mean scores of 4.83 for relevance, 4.89 for routing, 4.50 for reference quality, 4.90 for correctness, and 4.88 for professional identity (overall 4.80). Multi-turn evaluation yielded higher scores: context retention 4.94, topic coherence 5.00, and overall 4.98. The report generation system achieved completeness 4.00-5.00, correctness 4.00-5.00, and helpfulness 3.67-5.00, with reference validity 5.00 across all periods. This study demonstrates the feasibility of an integrated AI agent system for bidirectional HPV vaccine communication. The architecture enables verified information delivery with source attribution while providing systematic public discourse analysis, with a transferable framework for adaptation to other medical contexts.

</details>


### [16] [Building AI Agents to Improve Job Referral Requests to Strangers](https://arxiv.org/abs/2601.10726)
*Ross Chu,Yuting Huang*

Main category: cs.AI

TL;DR: 이 논문은 직업 추천 요청 작성을 지원하는 AI 에이전트를 개발하였습니다.


<details>
  <summary>Details</summary>
Motivation: 온라인 커뮤니티에서 구직자가 효과적인 직업 추천 요청을 작성하는 것을 지원하는 필요성.

Method: 임프루버 에이전트와 평가자 에이전트를 활용하여 요청을 수정하고 품질을 측정하는 방법.

Result: LLM 수정을 통해 약한 요청의 성공 확률을 14est upt to 14\% 증가시키면서 강한 요청의 성능 저하 없이 성능을 개선함.

Conclusion: 모델이 예측한 성공률의 향상이 반드시 실제 추천 증가로 이어지지는 않지만, 실제 사용자에 대한 실험 전에 유망한 특성에 대한 저비용 신호를 제공한다.

Abstract: This paper develops AI agents that help job seekers write effective requests for job referrals in a professional online community. The basic workflow consists of an improver agent that rewrites the referral request and an evaluator agent that measures the quality of revisions using a model trained to predict the probability of receiving referrals from other users. Revisions suggested by the LLM (large language model) increase predicted success rates for weaker requests while reducing them for stronger requests. Enhancing the LLM with Retrieval-Augmented Generation (RAG) prevents edits that worsen stronger requests while it amplifies improvements for weaker requests. Overall, using LLM revisions with RAG increases the predicted success rate for weaker requests by 14\% without degrading performance on stronger requests. Although improvements in model-predicted success do not guarantee more referrals in the real world, they provide low-cost signals for promising features before running higher-stakes experiments on real users.

</details>


### [17] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA는 다중 시간 규모의 에이전트 아키텍처에서 발생하는 조정 안정성 문제를 해결하기 위한 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 다중 시간 규모 에이전트 아키텍처가 성능 향상을 이루었지만, 통합 에이전트 시스템의 조정 안정성을 저해하여 심각한 레이어 간 충돌과 오류 전파 문제를 초래했다.

Method: CTHA는 레이어 간 통신 공간을 구조화된 매니폴드에 투사하여 조정 안정성을 회복하고, 일관된 의사 결정을 보장하는 원칙적인 중재 메커니즘을 통합한다.

Result: CTHA는 실패 연쇄를 47% 감소시키고, 샘플 효율성을 2.3배 향상시켰으며, 비제약 계층 기준선에 비해 우수한 확장성을 제공한다.

Conclusion: CTHA는 다중 에이전트 조정을 깊이 이해하고 강력한 자율 시스템의 발전을 위한 유망한 방향을 제시할 수 있는 원칙적인 시간 계층 확장으로 기여할 것이다.

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [18] [Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration](https://arxiv.org/abs/2601.10744)
*Sen Wang,Bangwei Liu,Zhenkun Gao,Lizhuang Ma,Xuhong Wang,Yuan Xie,Xin Tan*

Main category: cs.AI

TL;DR: 이 논문에서는 장기 기억 기반의 탐색 기법을 통해 인공지능 에이전트의 평생 학습 능력을 향상시키는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 이 논문의 동기는 복잡한 장기 작업을 처리할 수 있는 평생 학습 능력을 가진 에이전트의 필요성입니다.

Method: 우리는 에이전트의 탐색 인지 및 의사 결정 행동을 통합하는 Long-term Memory Embodied Exploration (LMEE)을 제안하고, 이를 평가하기 위한 LMEE-Bench라는 데이터셋과 벤치마크를 구축했습니다.

Result: 우리의 모델은 상태-of-the-art embodied exploration 모델에 비해 장기 임무에서 상당한 이점을 달성했습니다.

Conclusion: 메모리 탐색 기능을 향상시키고 능동적인 탐색을 장려하는 새로운 방법론을 통해, 제안된 모델은 포괄적인 평가에서 우수한 성과를 보였습니다.

Abstract: An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.

</details>


### [19] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: ARC-AGI 벤치마크 시리즈는 새로운 작업에서의 소수샷 일반화를 측정하는 중요한 기준으로, ARC-AGI-2 데이터셋을 대상으로 한 ARC Prize 2025 글로벌 대회가 개최되었다. 이 대회는 1,455 팀과 15,154개의 참가작을 유치했으며, 상위 점수는 ARC-AGI-2 비공식 평가 세트에서 24%에 도달했다. 학술 논문 제출도 지난해에 비해 거의 두 배 증가하여 90건에 이르렀고, 이는 유동적 지능과 추상적 사고에 대한 연구 관심이 증가하고 있음을 반영한다.


<details>
  <summary>Details</summary>
Motivation: ARC-AGI 벤치마크는 새로운 작업에서의 소수샷 일반화를 측정하는 중요한 지표입니다.

Method: 2025년 ARC Prize 대회는 ARC-AGI-2 데이터셋을 사용하여 진행되었으며, 다양한 형태의 정제 루프와 심층 학습 방법을 포함하여 성능을 분석했습니다.

Result: 대회에는 1,455 팀과 15,154개의 참가작이 있었고, 최고 점수는 24%에 도달했습니다. 논문 제출 수는 90건으로 증가했습니다.

Conclusion: AGI 발전에서 정제 루프의 역할과 지식 의존 과적합 문제에 대해 논의하며, ARC-AGI-3의 인터렉티브 추론 과제에 대한 예고를 제공합니다.

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [20] [AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing](https://arxiv.org/abs/2601.11007)
*Zhenhua Xu,Dongsheng Chen,Shuo Wang,Jian Li,Chengjie Wang,Meng Han,Yabiao Wang*

Main category: cs.AI

TL;DR: 본 연구는 LLM 역할 놀이 시스템의 몰입도와 적응성을 향상시키기 위한 AdaMARP 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 역할 놀이 시스템이 제약된 몰입감과 적응성으로 고생하고 있기 때문에, 이 문제를 해결하기 위해 새로운 프레임워크를 개발할 필요성이 있다.

Method: AdaMARP는 [Thought], (Action), <Environment>, Speech를 혼합한 몰입형 메시지 형식과 역할 놀이를 관리하는 명시적 장면 관리자를 특징으로 한다.

Result: AdaRPSet은 캐릭터 일관성, 환경 기반 및 내러티브 일관성을 향상시키고, AdaSMSet은 원활한 장면 전환과 더 자연스러운 역할 소개를 가능하게 한다.

Conclusion: 본 연구의 실험 결과, AdaMARP 프레임워크는 여러 상업용 LLM을 초월한 성능을 보여준다.

Abstract: LLM role-playing aims to portray arbitrary characters in interactive narratives, yet existing systems often suffer from limited immersion and adaptability. They typically under-model dynamic environmental information and assume largely static scenes and casts, offering insufficient support for multi-character orchestration, scene transitions, and on-the-fly character introduction. We propose an adaptive multi-agent role-playing framework, AdaMARP, featuring an immersive message format that interleaves [Thought], (Action), <Environment>, and Speech, together with an explicit Scene Manager that governs role-playing through discrete actions (init_scene, pick_speaker, switch_scene, add_role, end) accompanied by rationales. To train these capabilities, we construct AdaRPSet for the Actor Model and AdaSMSet for supervising orchestration decisions, and introduce AdaptiveBench for trajectory-level evaluation. Experiments across multiple backbones and model scales demonstrate consistent improvements: AdaRPSet enhances character consistency, environment grounding, and narrative coherence, with an 8B actor outperforming several commercial LLMs, while AdaSMSet enables smoother scene transitions and more natural role introductions, surpassing Claude Sonnet 4.5 using only a 14B LLM.

</details>


### [21] [Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics](https://arxiv.org/abs/2601.11012)
*Jiahao Wang,Shuangjia Zheng*

Main category: cs.AI

TL;DR: 이 논문에서는 단백질 변형의 최적화를 위한 HADES라는 베이esian 최적화 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 단백질 변형의 최적화는 생명공학과 의약품 개발에 있어 혁신적인 잠재력을 지니고 있다. 기존의 서열 기반 최적화 방법은 에피스타시스 효과와 구조적 제약을 무시하여 고차원 복잡성에 어려움을 겪는다.

Method: HADES는 해밀턴 역학을 이용하여 구조를 인식하는 근사 후방 분포에서 효율적으로 샘플링하는 베이esian 최적화 방법이다. HADES는 시뮬레이션된 물리적 움직임의 운동량과 불확실성을 활용하여 유망한 지역으로 제안의 신속한 전환을 가능하게 한다.

Result: 광범위한 실험을 통해 우리의 방법이 대부분의 메트릭에서 최신 기법보다 우수함을 입증했다.

Conclusion: HADES의 접근 방식은 단백질 구조와 서열 간의 상호 제약을 활용하여 유사한 구조와 최적화된 특성을 가진 단백질 서열을 설계하는 데 독특한 이점을 제공한다.

Abstract: The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.

</details>


### [22] [BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search](https://arxiv.org/abs/2601.11037)
*Shiyu Liu,Yongjing Yin,Jianhao Yan,Yunbo Tang,Qinggang Zhang,Bei Li,Xin Chen,Jingang Wang,Xunliang Cai,Jinsong Su*

Main category: cs.AI

TL;DR: BAPO는 RL 기반의 에이전트 검색의 신뢰성을 높이는 새로운 프레임워크로, 에이전트가 자신의 추론 한계를 인식하도록 돕고 IDK 응답을 제대로 활용할 수 있게 한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 질문을 해결하는 RL 기반 에이전트 검색의 신뢰성을 높이기 위해.

Method: Boundary-Aware Policy Optimization (BAPO)를 통해 에이전트가 추론 한계를 인식하도록 하는 새로운 RL 프레임워크를 제안.

Result: BAPO는 IDK 응답을 유도하고, 초기 탐색 중 보상을 전략적으로 조정하여 신뢰성을 크게 향상시킴을 입증했다.

Conclusion: BAPO는 에이전트 검색의 전체 신뢰성을 크게 개선시키며, 실제 시나리오에서의 위험을 최소화한다.

Abstract: RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.

</details>


### [23] [AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts](https://arxiv.org/abs/2601.11044)
*Keyu Li,Junhao Shi,Yang Xiao,Mohan Jiang,Jie Sun,Yunze Wu,Shijie Xia,Xiaojie Cai,Tianze Xu,Weiye Si,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 대형 언어 모델 기반 자율 에이전트는 경제 생산에 크게 기여할 수 있는 다면적인 기능을 보여주지만, 현재의 벤치마크는 장기 현실 시나리오를 포착하지 못하고 있습니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 벤치마크는 단일 에이전트 기능에 집중하여 현실 세계의 장기 시나리오를 반영하지 못하고 있으며, 인간 피드백 의존성이 자동화된 평가를 저해합니다.

Method: AgencyBench를 소개하여, 일상 AI 사용에서 파생된 종합 벤치마크를 생성하고, 32개 현실 시나리오에서 6가지 핵심 에이전트 기능을 평가합니다. 이를 위해 사용자 시뮬레이션 에이전트를 활용하여 반복적인 피드백을 제공하고, Docker 샌드박스를 사용합니다.

Result: 실험 결과는 폐쇄형 모델이 오픈 소스 모델보다 상당히 우수한 성능을 나타내며, 자원 효율성, 피드백 기반 자기 수정 및 특정 도구 사용 선호도에서 모델 간 큰 차이를 보였습니다.

Conclusion: AgencyBench는 차세대 에이전트를 위한 중요한 시험대로, 모델 아키텍처와 에이전트 프레임워크의 동시 최적화 필요성을 강조합니다. 이 연구는 자율 에이전트의 미래 방향에 대한 통찰을 제공하며, 전체 벤치마크와 평가 도구를 https://github.com/GAIR-NLP/AgencyBench에서 공개합니다.

Abstract: Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.

</details>


### [24] [ReCreate: Reasoning and Creating Domain Agents Driven by Experience](https://arxiv.org/abs/2601.11100)
*Zhezheng Hao,Hong Wang,Jian Luo,Jianqing Zhang,Yuyan Zhou,Qiang Lin,Can Wang,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: ReCreate는 경험 기반의 프레임워크로, 도메인 에이전트를 자동 생성하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 산업 환경에서의 대형 언어 모델 에이전트의 자동 생성 및 적응 가능성에 대한 필요성을 다룬다.

Method: ReCreate는 에이전트 상호작용 기록을 활용하여 성공 또는 실패의 원인과 개선 가능성을 파악하는 경험 기반 프레임워크이다.

Result: ReCreate는 다양한 도메인에서 인간이 설계한 에이전트 및 기존의 자동화된 에이전트 생성 방법들을 지속적으로 초월하는 성능을 보였다.

Conclusion: ReCreate는 최소한의 초기 구조에서 시작해도 효과적으로 에이전트를 생성할 수 있음을 보여준다.

Abstract: Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.

</details>


### [25] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 이 논문은 다수의 에이전트들이 협력하여 복잡한 작업을 수행하는 대규모 언어 모델 기반의 다중 에이전트 시스템(MAS)에서 쿼리 수준이 아닌 작업 수준의 워크플로 생성이 더 효율적임을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템에서의 작업 수행 방식의 비용과 혜택이 불분명하기 때문에 이를 재고하고 실증적으로 분석할 필요가 있다.

Method: 작업 수준의 생성 프레임워크 SCALE을 제안하며, 이 프레임워크는 전체 검증 실행 대신 최적화의 자가 예측과 적은 샷 보정을 활용하여 평가를 진행한다.

Result: SCALE은 여러 데이터셋에서 기존 접근 방식 대비 평균 0.61%의 성능 저하로 경쟁력을 유지하면서 전체 토큰 사용량을 최대 83%까지 줄임을 보여준다.

Conclusion: 쿼리 수준의 워크플로 생성이 항상 필요하지 않으며, 작업 수준의 접근이 더 효율적일 수 있음을 증명하였다.

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [26] [Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems](https://arxiv.org/abs/2601.11189)
*Sofiene Lassoued,Asrat Gobachew,Stefan Lier,Andreas Schwung*

Main category: cs.AI

TL;DR: 이 논문은 Job Shop Scheduling Problem을 해결하기 위한 정책 기반 심층 강화 학습 하이퍼 휴리스틱 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: Job Shop Scheduling Problem을 효과적으로 해결하기 위한 새로운 방법의 필요성이 있습니다.

Method: 하이퍼 휴리스틱 에이전트가 시스템 상태에 따라 동적으로 스케줄링 규칙을 전환하도록 학습하며, 두 가지 주요 메커니즘을 통해 프레임워크를 확장합니다.

Result: 제안된 접근법은 전통적인 휴리스틱, 메타휴리스틱 및 최근의 신경망 기반 스케줄링 방법보다 우수한 성능을 보입니다.

Conclusion: 정책 수준에서 결정론적 탐욕 선택과 확률적 샘플링 등의 두 가지 행동 선택 전략을 비교했습니다.

Abstract: This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms. First, action prefiltering restricts decision-making to feasible low-level actions, enabling low-level heuristics to be evaluated independently of environmental constraints and providing an unbiased assessment. Second, a commitment mechanism regulates the frequency of heuristic switching. We investigate the impact of different commitment strategies, from step-wise switching to full-episode commitment, on both training behavior and makespan. Additionally, we compare two action selection strategies at the policy level: deterministic greedy selection and stochastic sampling. Computational experiments on standard JSSP benchmarks demonstrate that the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods

</details>


### [27] [AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems](https://arxiv.org/abs/2601.11354)
*Weiyi Wang,Xinchi Chen,Jingjing Gong,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 이 논문은 AstroReason-Bench라는 새로운 벤치마크를 소개하며, 이는 항공우주 계획 문제에서 에이전트의 계획 능력을 평가하는 데 초점을 맞추고 있다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 대형 언어 모델(LLMs)의 최근 발전이 다양한 작업에 대한 추론 및 행동을 수행할 수 있는 일반 계획자로 자리매김했지만, 실제 물리적 제약이 있는 환경에서의 성능은 충분히 탐구되지 않았다.

Method: AstroReason-Bench는 다양한 일정 제도를 통합하고, 에이전트 지향 상호 작용 프로토콜을 제공하여 우주 계획 문제(SPP)를 평가하는 포괄적인 벤치마크를 구성한다.

Result: 상태-of-the-art 오픈 및 클로즈드 소스 에이전틱 LLM 시스템에서 평가한 결과, 현재 에이전트는 전문 솔버에 비해 상당히 저조한 성능을 보였으며, 이는 현실적인 제약 아래에서의 일반 계획의 주요 한계를 강조한다.

Conclusion: AstroReason-Bench는 미래의 에이전틱 연구를 위한 도전적이고 진단적인 테스트베드를 제공한다.

Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

</details>


### [28] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 에티오피아의 보건부는 필수 서비스 접근성을 개선하기 위해 보건소를 업그레이드하고 하지만 자원의 제한으로 인해 업그레이드할 시설을 신중히 우선순위를 정해야 한다. 본 연구에서는 전문가 지식과 최적화 기법을 통합한 하이브리드 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 에티오피아는 농촌 지역에서 필수 서비스 접근성을 개선하려고 하며, 리소스의 제한으로 인해 시설 업그레이드의 우선순위를 신중히 선택해야 한다.

Method: 우리는 전문가 지식과 최적화 기법을 체계적으로 통합하는 하이브리드 프레임워크인 Large language model and Extended Greedy (LEG) 프레임워크를 개발했다.

Result: 이 프레임워크는 인구 커버리지 최적화에 대한 증명된 근사 알고리즘과 전문가의 질적 지침을 반영한 해결책을 보장한다는 점에서 효과적이다.

Conclusion: 실제 에티오피아의 세 지역에서의 실험은 이 프레임워크의 효과성과 공정하고 데이터 기반의 건강 시스템 계획에 기여할 잠재력을 보여주었다.

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>
