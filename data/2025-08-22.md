<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 92]
- [cs.CR](#cs.CR) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli](https://arxiv.org/abs/2508.14214)
*Mattson Ogg,Chace Ashcraft,Ritwik Bose,Raphael Norman-Tenazas,Michael Wolmetz*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM)이 감정적으로 부하가 걸린 자극이나 상황을 어떻게 평가하는지에 대한 이해를 기반으로 LLM의 인간 행동 정합성을 조사했습니다.


<details>
  <summary>Details</summary>
Motivation: 일상 생활에서 LLM을 어떻게 통합할지에 대한 논의는 감정 자극에 대한 LLM의 평가 방식에 기반해야 합니다.

Method: 인간이 감정 콘텐츠에 대해 평가한 단어와 이미지 데이터셋에 대해 여러 인기 LLM으로부터 평가를 elicited했습니다.

Result: GPT-4o는 여러 출처에서 인간 참가자와 매우 유사하게 반응했으며, 행복도_rating은 가장 높은 정합성을 보였습니다.

Conclusion: LLM은 감정 프레임워크 내에서 인간 평가자와 비교할 때 더 일관된 평가를 보였으며, 이는 생물학적 및 인공지능 간의 유사성과 차이를 강조합니다.

Abstract: Emotions exert an immense influence over human behavior and cognition in both
commonplace and high-stress tasks. Discussions of whether or how to integrate
large language models (LLMs) into everyday life (e.g., acting as proxies for,
or interacting with, human agents), should be informed by an understanding of
how these tools evaluate emotionally loaded stimuli or situations. A model's
alignment with human behavior in these cases can inform the effectiveness of
LLMs for certain roles or interactions. To help build this understanding, we
elicited ratings from multiple popular LLMs for datasets of words and images
that were previously rated for their emotional content by humans. We found that
when performing the same rating tasks, GPT-4o responded very similarly to human
participants across modalities, stimuli and most rating scales (r = 0.9 or
higher in many cases). However, arousal ratings were less well aligned between
human and LLM raters, while happiness ratings were most highly aligned. Overall
LLMs aligned better within a five-category (happiness, anger, sadness, fear,
disgust) emotion framework than within a two-dimensional (arousal and valence)
organization. Finally, LLM ratings were substantially more homogenous than
human ratings. Together these results begin to describe how LLM agents
interpret emotional stimuli and highlight similarities and differences among
biological and artificial intelligence in key behavioral domains.

</details>


### [2] [Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions](https://arxiv.org/abs/2508.14294)
*Maria Leonor Pacheco,Fabio Somenzi,Dananjay Srinivas,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 본 연구에서는 복잡한 의사결정 과정의 설명을 위한 신경기호적 접근 방식을 제안하며, 결정 절차와 대형 언어 모델의 강점을 결합합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 의사결정의 설명 필요성과 기존 방법의 한계를 극복하기 위해 신경기호적 접근 방식을 연구합니다.

Method: Hitori 퍼즐의 솔루션에 대한 설명을 생성하는 도구를 구현하고, SAT 해결기와 LLM을 유연하게 결합합니다.

Result: Hitori 퍼즐 해결을 돕는 도구의 효과를 입증하는 실험적 증거를 제시합니다.

Conclusion: 신경기호적 접근 방식이 Hitori 퍼즐과 같은 특정 문제에서 효과적인 설명을 제공함을 보여줍니다.

Abstract: We propose a neurosymbolic approach to the explanation of complex sequences
of decisions that combines the strengths of decision procedures and Large
Language Models (LLMs). We demonstrate this approach by producing explanations
for the solutions of Hitori puzzles. The rules of Hitori include local
constraints that are effectively explained by short resolution proofs. However,
they also include a connectivity constraint that is more suitable for visual
explanations. Hence, Hitori provides an excellent testing ground for a flexible
combination of SAT solvers and LLMs. We have implemented a tool that assists
humans in solving Hitori puzzles, and we present experimental evidence of its
effectiveness.

</details>


### [3] [Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning](https://arxiv.org/abs/2508.14410)
*Beinuo Yang,Qishen Zhou,Junyi Li,Xingchen Su,Simon Hu*

Main category: cs.AI

TL;DR: 본 논문은 최적화 모델링의 효율성을 향상시키기 위해 새로운 벤치마크와 자동화 프레임워크를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 의사결정 문제 해결을 위한 최적화 모델링의 필요성과 현재 접근법의 제한점을 해결하고자 함.

Method: 기존 데이터셋을 수정하고 LogiOR 벤치마크를 도입하며, ORThought라는 새로운 프레임워크를 제안하여 최적화 모델링 프로세스를 자동화함.

Result: ORThought는 기존 접근 방법들을 초월하며 특히 복잡한 최적화 문제에서 두드러진 이점을 보여줌.

Conclusion: 이 연구는 LLM 기반 최적화 모델링의 향후 연구를 위한 중요한 통찰을 제공합니다.

Abstract: Optimization Modeling (OM) is essential for solving complex decision-making
problems. However, the process remains time-consuming and error-prone, heavily
relying on domain experts. While Large Language Models (LLMs) show promise in
addressing these challenges through their natural language understanding and
reasoning capabilities, current approaches face three critical limitations:
high benchmark labeling error rates reaching up to 42\%, narrow evaluation
scope that only considers optimal values, and computational inefficiency due to
heavy reliance on multi-agent systems or model fine-tuning. In this work, we
first enhance existing datasets through systematic error correction and more
comprehensive annotation. Additionally, we introduce LogiOR, a new optimization
modeling benchmark from the logistics domain, containing more complex problems
with standardized annotations. Furthermore, we present ORThought, a novel
framework that leverages expert-level optimization modeling principles through
chain-of-thought reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought outperforms existing
approaches, including multi-agent frameworks, with particularly significant
advantages on complex optimization problems. Finally, we provide a systematic
analysis of our method, identifying critical success factors and failure modes,
providing valuable insights for future research on LLM-based optimization
modeling.

</details>


### [4] [The Agent Behavior: Model, Governance and Challenges in the AI Digital Age](https://arxiv.org/abs/2508.14415)
*Qiang Zhang,Pei Yan,Yijia Xu,Chuanpo Fu,Yong Fang,Yang Liu*

Main category: cs.AI

TL;DR: AI의 발전으로 인해 네트워크 환경에서 에이전트가 인간의 행동을 점점 더 반영하고 있으며, 이는 인공 및 인간 행위자 간의 경계를 모호하게 하고 있습니다. 이러한 변화는 신뢰, 책임, 윤리 및 보안 등에서 상당한 도전을 가져옵니다. 이 논문에서 제안하는 "네트워크 행동 생애 주기" 모델은 네트워크 행동을 6단계로 나누고 각 단계에서 인간과 에이전트 간의 행동 차이를 체계적으로 분석합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 행동이 인간 행동과 비슷해짐에 따라 신뢰와 책임 문제를 해결할 필요성이 대두되고 있다.

Method: 제안된 "네트워크 행동 생애 주기" 모델은 행동을 6단계로 나누고, "에이전트를 위한 에이전트 (A4A)" 패러다임과 "인간-에이전트 행동 불균형 (HABD)" 모델을 통한 5차원 분석을 수행한다.

Result: 모델의 효과는 레드팀 침투 및 블루팀 방어와 같은 실제 사례를 통해 검증되었다.

Conclusion: 동적인 인지 거버넌스 아키텍처 및 인간-에이전트 협력의 신뢰성과 보안을 위한 이론적 기초와 기술적 로드맵을 제공한다.

Abstract: Advancements in AI have led to agents in networked environments increasingly
mirroring human behavior, thereby blurring the boundary between artificial and
human actors in specific contexts. This shift brings about significant
challenges in trust, responsibility, ethics, security and etc. The difficulty
in supervising of agent behaviors may lead to issues such as data contamination
and unclear accountability. To address these challenges, this paper proposes
the "Network Behavior Lifecycle" model, which divides network behavior into 6
stages and systematically analyzes the behavioral differences between humans
and agents at each stage. Based on these insights, the paper further introduces
the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity
(HABD)" model, which examine the fundamental distinctions between human and
agent behaviors across 5 dimensions: decision mechanism, execution efficiency,
intention-behavior consistency, behavioral inertia, and irrational patterns.
The effectiveness of the model is verified through real-world cases such as red
team penetration and blue team defense. Finally, the paper discusses future
research directions in dynamic cognitive governance architecture, behavioral
disparity quantification, and meta-governance protocol stacks, aiming to
provide a theoretical foundation and technical roadmap for secure and
trustworthy human-agent collaboration.

</details>


### [5] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: LLM 기반 에이전트의 관점 이해 능력을 개선하기 위한 구조화된 예제의 잠재력을 탐구하는 연구.


<details>
  <summary>Details</summary>
Motivation: LLM과 추론 프레임워크의 발전은 자율 에이전트의 관점 이해 능력을 개선할 새로운 가능성을 열어주었지만, 적극적인 인식, 협력적 추론과 관점 이해가 포함된 작업은 여전히 도전 과제가 되고 있다.

Method: Fast Downward 플래너에 의해 생성된 변환된 솔루션 그래프에서 유도된 구조화된 예제를 사용하여 ReAct 프레임워크 내에서 LLM 기반 에이전트의 성능을 향상시키기 위한 구조화된 솔루션 처리 파이프라인을 제안한다. 이 파이프라인은 최적 목표 경로(G형), 유익한 노드 경로(E형), 대안 행동을 대비하는 단계별 최적 결정 시퀀스(L형)의 세 가지 유형의 예제를 생성한다.

Result: L형 예제는 명확화 요청과 전체 행동 단계를 약간 줄이지만, 일관된 개선을 나타내지는 않았다. 에이전트는 기본적인 주의 필터링을 요구하는 작업에서는 성공적이나, 가려진 공간에 대한 사고나 인식 행동의 비용 측정이 필요한 시나리오에서는 어려움을 겪었다.

Conclusion: 구조화된 예제만으로는 견고한 관점 이해에 충분하지 않으며, LLM 기반 에이전트의 사회적으로 기반한 협업을 가능하게 하기 위해 명시적인 신념 추적, 비용 모델링 및 풍부한 환경이 필요함을 강조한다.

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [6] [LeanGeo: Formalizing Competitional Geometry problems in Lean](https://arxiv.org/abs/2508.14644)
*Chendong Song,Zihan Wang,Frederick Pu,Haiming Wang,Xiaohan Lin,Junqi Liu,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: Lean 4 정리 증명기 내에서 경쟁 수준의 기하 문제를 형식화하고 해결하기 위한 통합 형식 시스템인 LeanGeo를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기하 문제는 AI 추론 능력을 시험하는 중요한 테스트 베드이다. 현재의 기하 해결 시스템은 통합된 프레임워크 내에서 문제를 표현할 수 없어 다른 수학 분야와의 통합이 어렵다.

Method: LeanGeo는 Lean 4 정리 증명기 내에서 경쟁 수준의 기하 문제를 형식화하고 해결하기 위한 통합 형식 시스템으로, 고급 기하 정리에 대한 포괄적인 라이브러리를 제공한다.

Result: 평가 결과는 최신 대형 언어 모델의 능력과 한계를 보여주며, 자동화된 기하 추론의 추가 발전 필요성을 강조한다.

Conclusion: LeanGeo의 정리 라이브러리와 벤치마크는 오픈 소스로 제공된다.

Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most
existing geometry solving systems cannot express problems within a unified
framework, thus are difficult to integrate with other mathematical fields.
Besides, since most geometric proofs rely on intuitive diagrams, verifying
geometry problems is particularly challenging. To address these gaps, we
introduce LeanGeo, a unified formal system for formalizing and solving
competition-level geometry problems within the Lean 4 theorem prover. LeanGeo
features a comprehensive library of high-level geometric theorems with Lean's
foundational logic, enabling rigorous proof verification and seamless
integration with Mathlib. We also present LeanGeo-Bench, a formal geometry
benchmark in LeanGeo, comprising problems from the International Mathematical
Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the
capabilities and limitations of state-of-the-art Large Language Models on this
benchmark, highlighting the need for further advancements in automated
geometric reasoning. We open source the theorem library and the benchmark of
LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.

</details>


### [7] [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](https://arxiv.org/abs/2508.14654)
*Peilin Ji,Xiao Xue,Simeng Wang,Wenhao Yan*

Main category: cs.AI

TL;DR: 이 논문은 도시 홍수에 대응하기 위한 효율적인 의사결정을 위한 H-J라는 계층적 다중 에이전트 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 최근 극한의 도시 강우 사건의 빈도가 증가하면서 비상 상황 시스템에 심각한 도전 과제가 제기되고 있다.

Method: H-J 프레임워크는 지식 기반의 프롬프트, 엔트로피 제약 생성, 피드백 기반 최적화를 통합한다.

Result: H-J는 극한 강우, 간헐적인 폭우, 일일 가벼운 비 등 세 가지 대표적인 조건에서 실제 도시 지형 및 강우 데이터를 평가하여 규칙 기반 및 강화 학습 기준선에 비해 교통 원활함, 작업 성공률, 시스템 탄력성에서 우수함을 입증하였다.

Conclusion: 결과는 도시 홍수 대응에서 불확실성을 인식하고 지식 제약이 있는 LLM 기반 접근 방식의 가능성을 강조한다.

Abstract: In recent years, the increasing frequency of extreme urban rainfall events
has posed significant challenges to emergency scheduling systems. Urban
flooding often leads to severe traffic congestion and service disruptions,
threatening public safety and mobility. However, effective decision making
remains hindered by three key challenges: (1) managing trade-offs among
competing goals (e.g., traffic flow, task completion, and risk mitigation)
requires dynamic, context-aware strategies; (2) rapidly evolving environmental
conditions render static rules inadequate; and (3) LLM-generated strategies
frequently suffer from semantic instability and execution inconsistency.
Existing methods fail to align perception, global optimization, and multi-agent
coordination within a unified framework. To tackle these challenges, we
introduce H-J, a hierarchical multi-agent framework that integrates
knowledge-guided prompting, entropy-constrained generation, and feedback-driven
optimization. The framework establishes a closed-loop pipeline spanning from
multi-source perception to strategic execution and continuous refinement. We
evaluate H-J on real-world urban topology and rainfall data under three
representative conditions: extreme rainfall, intermittent bursts, and daily
light rain. Experiments show that H-J outperforms rule-based and
reinforcement-learning baselines in traffic smoothness, task success rate, and
system robustness. These findings highlight the promise of uncertainty-aware,
knowledge-constrained LLM-based approaches for enhancing resilience in urban
flood response.

</details>


### [8] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: MCP-Universe는 대형 언어 모델을 실제 환경에서 평가하기 위한 포괄적인 벤치마크로, 여러 도메인과 MCP 서버를 포함하여 성능 평가의 새로운 기준을 설정한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 벤치마크가 실제 애플리케이션에서의 도전 과제를 반영하지 못하고 간단하여, LLM의 효과적인 평가가 필요하다.

Method: MCP-Universe는 6개 핵심 도메인과 11개 다양한 MCP 서버를 포함한 포괄적인 벤치마크를 제공하며, 여러 평가 방법을 통해 LLM을 평가한다.

Result: 최신 모델인 GPT-5, Grok-4 및 Claude-4.0-Sonnet조차도 성능 제한이 상당하며, 긴 문맥 도전과 익숙하지 않은 도구와의 상호작용에서 문제가 발생한다.

Conclusion: 우리는 UI 지원이 포함된 오픈소스 평가 프레임워크를 제공하여 연구자들이 새로운 에이전트와 MCP 서버를 통합하고 혁신할 수 있도록 지원한다.

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [9] [Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines](https://arxiv.org/abs/2508.14710)
*Swantje Plambeck,Ali Salamati,Eyke Huellermeier,Goerschwin Fey*

Main category: cs.AI

TL;DR: 이 논문은 분리된 추상화 형태의 CPS에 대한 데이터 기반 접근 방식을 소개하며, 시스템의 안전 확률을 평가하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: CPS는 검증, 진단 또는 디버깅과 같은 작업을 위해 강력한 모델이 필요한 복잡한 시스템이다.

Method: 본 논문에서는 이산 추상화 형태의 Mealy 기계를 사용하여 CPS의 안전 확률을 결정하는 데이터 기반 접근 방식을 제안한다.

Result: PAC 학습 패러다임을 기반으로 한 접근 방식을 통해 시스템의 확률적 도달 가능성을 분석하고 추가적인 신뢰도를 제공한다.

Conclusion: 자동화된 차선 유지 시스템에 대한 사례 연구를 통해 접근 방식을 검증하였다.

Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models
for tasks like verification, diagnosis, or debugging. Often, suitable models
are not available and manual extraction is difficult. Data-driven approaches
then provide a solution to, e.g., diagnosis tasks and verification problems
based on data collected from the system. In this paper, we consider CPS with a
discrete abstraction in the form of a Mealy machine. We propose a data-driven
approach to determine the safety probability of the system on a finite horizon
of n time steps. The approach is based on the Probably Approximately Correct
(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic
and probabilistic reachability analysis of systems, especially providing an
additional confidence on the determined probability. The learning process
follows an active learning paradigm, where new learning data is sampled in a
guided way after an initial learning set is collected. We validate the approach
with a case study on an automated lane-keeping system.

</details>


### [10] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: AI 모델의 자기 반성 가능성에 대한 정의와 실험 결과를 논의한다.


<details>
  <summary>Details</summary>
Motivation: AI 모델이 자기 반성을 할 수 있는지를 검토하는 것이 점점 더 중요한 질문이다.

Method: 최근에 제안된 '경량' 정의에서 출발하여, AI의 자기 반성을 더 두터운 정의로 주장한다.

Result: LLM이 내부 온도 파라미터에 대해 추론하는 실험을 통해 경량의 자기 반성을 가진 것처럼 보일 수 있지만, 우리의 제안된 정의에 따라 의미 있는 자기 반성을 수행하지 못한다는 것을 보여준다.

Conclusion: AI의 자기 반성에 대한 정의와 그 실험적 결과의 중요성을 강조한다.

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [11] [An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents](https://arxiv.org/abs/2508.14131)
*Junjie Qi,Siqi Mao,Tianyi Tan*

Main category: cs.MA

TL;DR: 본 논문에서는 다중 에이전트 환경에서 협력 행동을 촉진하는 개선된 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 알고리즘이 다중 에이전트 강화 학습 문제를 해결하는 데 있어 나타나는 단점을 분석하기 위함이다.

Method: 기존의 MADDPG 알고리즘을 기반으로, 에이전트 간 협력 행동이 확인될 때 에이전트가 얻을 수 있는 보상을 증가시키기 위한 새로운 매개변수를 도입한다.

Result: 개선된 알고리즘은 PettingZoo에서의 환경에서 MADDPG와 비교했을 때, 에이전트가 더 높은 팀 보상과 개인 보상을 달성하도록 도움을 준다.

Conclusion: 새로운 알고리즘은 다중 에이전트 환경에서 협력 행동을 유도하여 성능 향상을 가져온다.

Abstract: We propose an improved algorithm by identifying and encouraging cooperative
behavior in multi-agent environments. First, we analyze the shortcomings of
existing algorithms in addressing multi-agent reinforcement learning problems.
Then, based on the existing algorithm MADDPG, we introduce a new parameter to
increase the reward that an agent can obtain when cooperative behavior among
agents is identified. Finally, we compare our improved algorithm with MADDPG in
environments from PettingZoo. The results show that the new algorithm helps
agents achieve both higher team rewards and individual rewards.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students](https://arxiv.org/abs/2508.14057)
*Pablo G. Almeida,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Pedro Silva,Eduardo Luz*

Main category: cs.LG

TL;DR: 이 논문은 학생 데이터의 테이블 형식을 그래프로 변환하여 이탈 예측 정확성을 향상시키는 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 학생 이탈은 전 세계 교육 시스템에서 중요한 문제로, 사회적 및 경제적 비용이 크다. 위험에 처한 학생을 예측함으로써 시기 적절한 개입이 가능하다.

Method: 학생 데이터를 그래프 형태로 변환하기 위해 클러스터링 기법을 사용하는 방식을 본 논문에서 연구한다.

Result: PCA-KMeans 클러스터링에 의해 생성된 그래프에서 GraphSAGE를 사용한 특정 GNN 구성의 성능이 뛰어나며, 탁월한 성능 향상을 보였다.

Conclusion: GNN의 가능성과 테이블 데이터를 그래프 기반 학습으로 최적화하는 데 있어 도전 과제를 강조한다.

Abstract: Student dropout is a significant challenge in educational systems worldwide,
leading to substantial social and economic costs. Predicting students at risk
of dropout allows for timely interventions. While traditional Machine Learning
(ML) models operating on tabular data have shown promise, Graph Neural Networks
(GNNs) offer a potential advantage by capturing complex relationships inherent
in student data if structured as graphs. This paper investigates whether
transforming tabular student data into graph structures, primarily using
clustering techniques, enhances dropout prediction accuracy. We compare the
performance of GNNs (a custom Graph Convolutional Network (GCN) and GraphSAGE)
on these generated graphs against established tabular models (Random Forest
(RF), XGBoost, and TabNet) using a real-world student dataset. Our experiments
explore various graph construction strategies based on different clustering
algorithms (K-Means, HDBSCAN) and dimensionality reduction techniques
(Principal Component Analysis (PCA), Uniform Manifold Approximation and
Projection (UMAP)). Our findings demonstrate that a specific GNN configuration,
GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior
performance, notably improving the macro F1-score by approximately 7 percentage
points and accuracy by nearly 2 percentage points over the strongest tabular
baseline (XGBoost). However, other GNN configurations and graph construction
methods did not consistently surpass tabular models, emphasizing the critical
role of the graph generation strategy and GNN architecture selection. This
highlights both the potential of GNNs and the challenges in optimally
transforming tabular data for graph-based learning in this domain.

</details>


### [13] [Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation](https://arxiv.org/abs/2508.14069)
*Chinmoy Biswas,Nafis Faisal,Vivek Chowdhury,Abrar Al-Shadid Abir,Sabir Mahmud,Mithon Rahman,Shaikh Anowarul Fattah,Hafiz Imtiaz*

Main category: cs.LG

TL;DR: 본 연구는 약 62%의 희소성을 가진 전력 공장 시간별 부하 데이터셋을 Gaussian 보간법으로 보강하여 부하 예측에 활용할 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 실제 데이터셋에서 희소성 문제는 주요 도전 과제가 됩니다.

Method: 본 연구에서는 통계 분석을 수행하고, 다양한 기계 학습 및 딥러닝 모델을 훈련시킵니다.

Result: Gaussian 보간법을 사용하면 부하 예측 문제를 다루는 데 적합하다는 것을 경험적으로 입증합니다. 또한 LSTM 기반 신경망 모델이 다른 모델보다 최상의 성능을 발휘함을 보여줍니다.

Conclusion: LSTM 모델이 기존의 다양한 모델 중에서 가장 우수한 성능을 나타냅니다.

Abstract: Sparsity, defined as the presence of missing or zero values in a dataset,
often poses a major challenge while operating on real-life datasets. Sparsity
in features or target data of the training dataset can be handled using various
interpolation methods, such as linear or polynomial interpolation, spline,
moving average, or can be simply imputed. Interpolation methods usually perform
well with Strict Sense Stationary (SSS) data. In this study, we show that an
approximately 62\% sparse dataset with hourly load data of a power plant can be
utilized for load forecasting assuming the data is Wide Sense Stationary (WSS),
if augmented with Gaussian interpolation. More specifically, we perform
statistical analysis on the data, and train multiple machine learning and deep
learning models on the dataset. By comparing the performance of these models,
we empirically demonstrate that Gaussian interpolation is a suitable option for
dealing with load forecasting problems. Additionally, we demonstrate that Long
Short-term Memory (LSTM)-based neural network model offers the best performance
among a diverse set of classical and neural network-based models.

</details>


### [14] [Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation](https://arxiv.org/abs/2508.14342)
*Lingkai Kong,Haichuan Wang,Charles A. Emogor,Vincent Börsch-Supan,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: 밀렵은 야생 동물과 생물 다양성에 중대한 위협을 가합니다. 이 논문에서는 밀렵자의 행동 예측을 통해 밀렵을 줄이기 위한 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 밀렵은 생물다양성과 야생동물에 심각한 위협을 초래하므로, 이를 줄이기 위해 밀렵자의 행동 예측이 필요합니다.

Method: 본 연구에서는 흐름 매칭 기법을 밀렵 예측에 적용하였고, 불완전한 탐지 문제를 해결하기 위해 점유 기반 탐지 모델과 결합하였습니다. 또한, 데이터 부족 문제를 완화하기 위해 선형 모델 예측으로 초기화된 복합 흐름을 이용하였습니다.

Result: 우간다의 두 개 국립공원에서의 데이터 세트를 이용한 평가 결과, 예측 정확도에서 일관된 향상이 나타났습니다.

Conclusion: 본 연구는 밀렵 예측에서 흐름 매칭을 활용한 새로운 접근법을 제안하며, 예측 정확성을 향상시키는 데 기여합니다.

Abstract: Poaching poses significant threats to wildlife and biodiversity. A valuable
step in reducing poaching is to forecast poacher behavior, which can inform
patrol planning and other conservation interventions. Existing poaching
prediction methods based on linear models or decision trees lack the
expressivity to capture complex, nonlinear spatiotemporal patterns. Recent
advances in generative modeling, particularly flow matching, offer a more
flexible alternative. However, training such models on real-world poaching data
faces two central obstacles: imperfect detection of poaching events and limited
data. To address imperfect detection, we integrate flow matching with an
occupancy-based detection model and train the flow in latent space to infer the
underlying occupancy state. To mitigate data scarcity, we adopt a composite
flow initialized from a linear-model prediction rather than random noise which
is the standard in diffusion models, injecting prior knowledge and improving
generalization. Evaluations on datasets from two national parks in Uganda show
consistent gains in predictive accuracy.

</details>


### [15] [Edge-Selector Model Applied for Local Search Neighborhood for Solving Vehicle Routing Problems](https://arxiv.org/abs/2508.14071)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux,Daniele Vigo*

Main category: cs.LG

TL;DR: 이 연구는 차량 경로 문제를 해결하기 위한 하이브리드 머신러닝 및 메타 휴리스틱 메커니즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 차량 경로 문제(VRP)의 해결을 위한 효율적인 방법론 개발.

Method: 엣지 솔루션 선택기 모델을 통해 해법 엣지를 분류하고, GNN 및 단순 표 형 분류기를 활용하여 금지된 이동을 예측.

Result: 다양한 메타 휴리스틱 베이스라인에 적용하여 성능 개선을 달성함.

Conclusion: 우리의 방법은 Scalability 및 Generalizability를 보여주며, 다양한 문제 규격에 대해 성능 향상을 입증했다.

Abstract: This research proposes a hybrid Machine Learning and metaheuristic mechanism
that is designed to solve Vehicle Routing Problems (VRPs). The main of our
method is an edge solution selector model, which classifies solution edges to
identify prohibited moves during the local search, hence guiding the search
process within metaheuristic baselines. Two learning-based mechanisms are used
to develop the edge selector: a simple tabular binary classifier and a Graph
Neural Network (GNN). The tabular classifier employs Gradient Boosting Trees
and Feedforward Neural Network as the baseline algorithms. Adjustments to the
decision threshold are also applied to handle the class imbalance in the
problem instance. An alternative mechanism employs the GNN to utilize graph
structure for direct solution edge prediction, with the objective of guiding
local search by predicting prohibited moves. These hybrid mechanisms are then
applied in state-fo-the-art metaheuristic baselines. Our method demonstrates
both scalability and generalizability, achieving performance improvements
across different baseline metaheuristics, various problem sizes and variants,
including the Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time
Windows (CVRPTW). Experimental evaluations on benchmark datasets up to 30,000
customer nodes, supported by pair-wise statistical analysis, verify the
observed improvements.

</details>


### [16] [Multi-Objective Bayesian Optimization with Independent Tanimoto Kernel Gaussian Processes for Diverse Pareto Front Exploration](https://arxiv.org/abs/2508.14072)
*Anabel Yong*

Main category: cs.LG

TL;DR: GP-MOBO는 분자 최적화에서 최첨단을 발전시키는 새로운 다목적 베이지안 최적화 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 분자 최적화 문제를 더 효율적으로 해결하기 위한 방법을 찾기 위해 기존 방법의 한계를 극복하고자 한다.

Method: 빠른 최소 패키지를 통합한 정확한 가우시안 프로세스를 사용하여 희소 분자 지문을 전체 차원에서 처리한다.

Result: GP-MOBO는 전통적인 방법보다 더 높은 품질의 SMILES를 식별하고 화학 탐색 공간을 더 넓게 탐색할 수 있다.

Conclusion: GP-MOBO는 복잡한 다목적 최적화 문제를 효과적으로 해결하면서 최소한의 컴퓨팅 오버헤드로 더 높은 기하학적 평균 값을 산출한다.

Abstract: We present GP-MOBO, a novel multi-objective Bayesian Optimization algorithm
that advances the state-of-the-art in molecular optimization. Our approach
integrates a fast minimal package for Exact Gaussian Processes (GPs) capable of
efficiently handling the full dimensionality of sparse molecular fingerprints
without the need for extensive computational resources. GP-MOBO consistently
outperforms traditional methods like GP-BO by fully leveraging fingerprint
dimensionality, leading to the identification of higher-quality and valid
SMILES. Moreover, our model achieves a broader exploration of the chemical
search space, as demonstrated by its superior proximity to the Pareto front in
all tested scenarios. Empirical results from the DockSTRING dataset reveal that
GP-MOBO yields higher geometric mean values across 20 Bayesian optimization
iterations, underscoring its effectiveness and efficiency in addressing complex
multi-objective optimization challenges with minimal computational overhead.

</details>


### [17] [MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets](https://arxiv.org/abs/2508.14073)
*Qian Zhang,Ruilin Zhang,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 본 논문은 다중 뷰 대조 사전 훈련과 경량 감독 세부 조정을 통합한 반감독 학습 프레임워크인 MCLPD를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 고비용의 EEG 데이터 주석으로 인해 데이터셋 크기가 제한되고, 획득 프로토콜 및 피험자 인구 통계학적 차이로 인해 모델의 견고성과 일반화 가능성이 저해된다.

Method: MCLPD는 UNM 데이터셋에서 비지도 학습을 통해 사전 훈련을 수행하고, 시간 및 주파수 도메인에서 이중 증강을 적용하여 대조 쌍을 구성한다.

Result: MCLPD는 UI에서 0.91, UC에서 0.81의 F1 점수를 달성하였으며, 5%의 주석 데이터를 사용할 때 각각 0.97과 0.87로 개선된다.

Conclusion: MCLPD는 기존 방법들에 비해 크로스 데이터셋 일반화를 크게 향상시키며, 주석 데이터에 대한 의존도를 줄인다.

Abstract: Electroencephalography has been validated as an effective technique for
detecting Parkinson's disease,particularly in its early stages.However,the high
cost of EEG data annotation often results in limited dataset size and
considerable discrepancies across datasets,including differences in acquisition
protocols and subject demographics,significantly hinder the robustness and
generalizability of models in cross-dataset detection scenarios.To address such
challenges,this paper proposes a semi-supervised learning framework named
MCLPD,which integrates multi-view contrastive pre-training with lightweight
supervised fine-tuning to enhance cross-dataset PD detection performance.During
pre-training,MCLPD uses self-supervised learning on the unlabeled UNM
dataset.To build contrastive pairs,it applies dual augmentations in both time
and frequency domains,which enrich the data and naturally fuse time-frequency
information.In the fine-tuning phase,only a small proportion of labeled data
from another two datasets (UI and UC)is used for supervised
optimization.Experimental results show that MCLPD achieves F1 scores of 0.91 on
UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97
and 0.87,respectively,when 5%of labeled data is used.Compared to existing
methods,MCLPD substantially improves cross-dataset generalization while
reducing the dependency on labeled data,demonstrating the effectiveness of the
proposed framework.

</details>


### [18] [GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease](https://arxiv.org/abs/2508.14074)
*Qian Zhang,Ruilin Zhang,Biaokai Zhu,Xun Han,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 본 논문은 EEG 기반의 파킨슨병 분류를 위한 GAN 강화 일반화 모델 GEPD를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 파킨슨병 탐지 방법은 개별 데이터셋 내에서 성공적인 결과를 보이지만, 다양한 EEG 데이터셋에서 탐지 방법의 변동성과 각 데이터셋의 작은 크기로 인해 일반화 가능한 모델 훈련에 어려움이 있습니다.

Method: 생성 네트워크를 설계하여 생성된 데이터와 실제 데이터 간의 분포 유사성을 조정하여 융합 EEG 데이터를 생성하며, EEG 신호 품질 평가 모델을 설계하여 생성 데이터의 품질을 보장합니다. 또한, 다중 컨볼루션 신경망을 결합하여 EEG 신호의 시간-주파수 특성을 효과적으로 캡처하는 분류 네트워크를 설계합니다.

Result: 이 모델은 크로스 데이터셋 설정에서 최신 모델들과 유사한 성능을 보이며, 정확도 84.3%와 F1 점수 84.0%를 달성했습니다.

Conclusion: 제안된 모델의 일반화 가능성을 보여주며, 신경학적 질병의 진단 및 모니터링을 촉진하기 위한 지능적 방법의 활용에 헌신하고 있습니다.

Abstract: Electroencephalography has been established as an effective method for
detecting Parkinson's disease, typically diagnosed early.Current Parkinson's
disease detection methods have shown significant success within individual
datasets, however, the variability in detection methods across different EEG
datasets and the small size of each dataset pose challenges for training a
generalizable model for cross-dataset scenarios. To address these issues, this
paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for
EEG-based cross-dataset classification of Parkinson's disease.First, we design
a generative network that creates fusion EEG data by controlling the
distribution similarity between generated data and real data.In addition, an
EEG signal quality assessment model is designed to ensure the quality of
generated data great.Second, we design a classification network that utilizes a
combination of multiple convolutional neural networks to effectively capture
the time-frequency characteristics of EEG signals, while maintaining a
generalizable structure and ensuring easy convergence.This work is dedicated to
utilizing intelligent methods to study pathological manifestations, aiming to
facilitate the diagnosis and monitoring of neurological diseases.The evaluation
results demonstrate that our model performs comparably to state-of-the-art
models in cross-dataset settings, achieving an accuracy of 84.3% and an
F1-score of 84.0%, showcasing the generalizability of the proposed model.

</details>


### [19] [Explainable Graph Spectral Clustering For Text Embeddings](https://arxiv.org/abs/2508.14075)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Piotr Borkowski,Dariusz Czerski,Eryk Laskowski*

Main category: cs.LG

TL;DR: 이 논문은 텍스트 문서의 그래프 스펙트럼 클러스터링 결과의 설명 가능성에 대한 개념을 일반화한다.


<details>
  <summary>Details</summary>
Motivation: 문서 유사성을 용어 벡터 공간에서 코사인 유사도로 계산하는 방식에 대한 이해를 높이기 위함이다.

Method: GloVe 임베딩 아이디어를 바탕으로 한 다른 문서 임베딩을 고려하여 이 개념을 일반화한다.

Result: 문서 임베딩에 대한 새로운 접근 방식을 제안한다.

Conclusion: 이 접근 방식은 그래프 스펙트럼 클러스터링 결과의 설명 가능성을 확대한다.

Abstract: In a previous paper, we proposed an introduction to the explainability of
Graph Spectral Clustering results for textual documents, given that document
similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of
documents, in particular, based on the GloVe embedding idea.

</details>


### [20] [PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning](https://arxiv.org/abs/2508.14076)
*Mengdi Li,Guanqiao Chen,Xufeng Zhao,Haochen Wen,Shu Yang,Di Wang*

Main category: cs.LG

TL;DR: PersRM-R1은 개인적 요소를 식별하고 표현하기 위해 고안된 첫 번째 추론 기반 보상 모델링 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존 보상 모델(RM)이 사용자의 미세한 선호를 포착하는 데 어려움이 있으며, 이러한 문제를 해결하고자 한다.

Method: 합성 데이터 생성과 감독된 미세 조정, 강화 학습 미세 조정으로 구성된 2단계 훈련 파이프라인을 결합한 새로운 접근 방식을 사용한다.

Result: PersRM-R1은 유사한 크기의 기존 모델들을 초월하며, 더 큰 모델들과 동시에 정확성과 일반화 가능성에서 비교 가능하다.

Conclusion: PersRM-R1은 보다 효과적인 개인화된 LLM으로 나아가는 길을 열어준다.

Abstract: Reward models (RMs), which are central to existing post-training methods, aim
to align LLM outputs with human values by providing feedback signals during
fine-tuning. However, existing RMs struggle to capture nuanced, user-specific
preferences, especially under limited data and across diverse domains. Thus, we
introduce PersRM-R1, the first reasoning-based reward modeling framework
specifically designed to identify and represent personal factors from only one
or a few personal exemplars. To address challenges including limited data
availability and the requirement for robust generalization, our approach
combines synthetic data generation with a two-stage training pipeline
consisting of supervised fine-tuning followed by reinforcement fine-tuning.
Experimental results demonstrate that PersRM-R1 outperforms existing models of
similar size and matches the performance of much larger models in both accuracy
and generalizability, paving the way for more effective personalized LLMs.

</details>


### [21] [Label Smoothing is a Pragmatic Information Bottleneck](https://arxiv.org/abs/2508.14077)
*Sota Kudo*

Main category: cs.LG

TL;DR: 이 연구는 정보 병목 현상 관점을 통해 라벨 스무딩을 재조명한다.


<details>
  <summary>Details</summary>
Motivation: 라벨 스무딩의 효과를 정보 병목 현상과 연결하여 이해하고, 이를 통해 간단한 구현을 가능하게 하는 방법을 제시하고자 한다.

Method: 충분한 모델 유연성과 동일한 입력에 대한 상충하는 라벨이 없다는 가정 하에, 이론적 및 실험적으로 라벨 스무딩을 통한 모델 출력을 탐구한다.

Result: 실험을 통해 라벨 스무딩이 목표에 대한 정보를 포함하지 않는 요인이나 다른 변수에 조건화했을 때 추가적인 정보를 제공하지 않는 요인에 대해 둔감한 특성을 보인다.

Conclusion: 라벨 스무딩은 정보 병목 현상의 실용적인 접근법으로 해석될 수 있으며, 이를 통해 최적의 솔루션을 탐색할 수 있다.

Abstract: This study revisits label smoothing via a form of information bottleneck.
Under the assumption of sufficient model flexibility and no conflicting labels
for the same input, we theoretically and experimentally demonstrate that the
model output obtained through label smoothing explores the optimal solution of
the information bottleneck. Based on this, label smoothing can be interpreted
as a practical approach to the information bottleneck, enabling simple
implementation. As an information bottleneck method, we experimentally show
that label smoothing also exhibits the property of being insensitive to factors
that do not contain information about the target, or to factors that provide no
additional information about it when conditioned on another variable.

</details>


### [22] [Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction](https://arxiv.org/abs/2508.14078)
*Mohamed Hassan Abdalla Idris,Jakub Marek Cebula,Jebraeel Gholinezhad,Shamsul Masum,Hongjie Ma*

Main category: cs.LG

TL;DR: 이 연구는 다변량 시계열 분석에 특히 중점을 두어 샘플 외 탄화수소 생산 예측의 강건성을 향상시키기 위한 새로운 머신러닝 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 다변량 시계열 분석을 통해 탄화수소 생산 예측의 강건성을 높이고자 하였다.

Method: 생산성 지수(PI)에 기반한 특성 선택과 유도 적합 예측(ICP)을 통합하여 불확실성 정량화를 수행하였다.

Result: LSTM 모델이 가장 낮은 MAE(19.468)와 정통의 샘플 외 예측 데이터(29.638)를 달성하며 우수한 성능을 보였다.

Conclusion: 도메인 지식과 고급 머신러닝 기법을 결합하여 탄화수소 생산 예측의 신뢰성을 향상시킬 수 있는 상당한 잠재력을 보여준다.

Abstract: This research introduces a new ML framework designed to enhance the
robustness of out-of-sample hydrocarbon production forecasting, specifically
addressing multivariate time series analysis. The proposed methodology
integrates Productivity Index (PI)-driven feature selection, a concept derived
from reservoir engineering, with Inductive Conformal Prediction (ICP) for
rigorous uncertainty quantification. Utilizing historical data from the Volve
(wells PF14, PF12) and Norne (well E1H) oil fields, this study investigates the
efficacy of various predictive algorithms-namely Long Short-Term Memory (LSTM),
Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and eXtreme Gradient
Boosting (XGBoost) - in forecasting historical oil production rates (OPR_H).
All the models achieved "out-of-sample" production forecasts for an upcoming
future timeframe. Model performance was comprehensively evaluated using
traditional error metrics (e.g., MAE) supplemented by Forecast Bias and
Prediction Direction Accuracy (PDA) to assess bias and trend-capturing
capabilities. The PI-based feature selection effectively reduced input
dimensionality compared to conventional numerical simulation workflows. The
uncertainty quantification was addressed using the ICP framework, a
distribution-free approach that guarantees valid prediction intervals (e.g.,
95% coverage) without reliance on distributional assumptions, offering a
distinct advantage over traditional confidence intervals, particularly for
complex, non-normal data. Results demonstrated the superior performance of the
LSTM model, achieving the lowest MAE on test (19.468) and genuine out-of-sample
forecast data (29.638) for well PF14, with subsequent validation on Norne well
E1H. These findings highlight the significant potential of combining
domain-specific knowledge with advanced ML techniques to improve the
reliability of hydrocarbon production forecasts.

</details>


### [23] [A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy](https://arxiv.org/abs/2508.14079)
*Maxime Heuillet,Rishika Bhagwatkar,Jonas Ngnawé,Yann Pequignot,Alexandre Larouche,Christian Gagné,Irina Rish,Ola Ahmad,Audrey Durand*

Main category: cs.LG

TL;DR: 이 논문은 강건한 미세 조정의 효율성을 조사하고, 다양한 아키텍처와 손실 목표에서의 영향력을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 이미지 도메인에서 작동하는 딥 러닝 모델은 작은 입력 변동에 취약하다. 이러한 변동에 대한 강건성을 확보하는 것이 오랜 목표였다.

Method: 6개의 데이터셋, 40개의 사전 훈련된 아키텍처, 2개의 전문 손실 및 3개의 적응 프로토콜을 포함한 실증 연구를 수행하여 1,440개의 훈련 구성 및 7,200개의 강건성 측정을 수집했다.

Result: 이 연구는 현재까지의 강건한 미세 조정의 가장 다양한 벤치마크를 제공하며, 감독 방식으로 큰 데이터셋에서 사전 훈련된 합성곱 신경망이 최고의 성능을 내는 경향이 있음을 발견했다.

Conclusion: 이 분석은 이전의 설계 가정을 확인하고 도전하며, 유망한 연구 방향을 제시하고 실용적인 지침을 제공한다.

Abstract: Deep learning models operating in the image domain are vulnerable to small
input perturbations. For years, robustness to such perturbations was pursued by
training models from scratch (i.e., with random initializations) using
specialized loss objectives. Recently, robust fine-tuning has emerged as a more
efficient alternative: instead of training from scratch, pretrained models are
adapted to maximize predictive performance and robustness. To conduct robust
fine-tuning, practitioners design an optimization strategy that includes the
model update protocol (e.g., full or partial) and the specialized loss
objective. Additional design choices include the architecture type and size,
and the pretrained representation. These design choices affect robust
generalization, which is the model's ability to maintain performance when
exposed to new and unseen perturbations at test time. Understanding how these
design choices influence generalization remains an open question with
significant practical implications. In response, we present an empirical study
spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3
adaptation protocols, yielding 1,440 training configurations and 7,200
robustness measurements across five perturbation types. To our knowledge, this
is the most diverse and comprehensive benchmark of robust fine-tuning to date.
While attention-based architectures and robust pretrained representations are
increasingly popular, we find that convolutional neural networks pretrained in
a supervised manner on large datasets often perform best. Our analysis both
confirms and challenges prior design assumptions, highlighting promising
research directions and offering practical guidance.

</details>


### [24] [KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge](https://arxiv.org/abs/2508.14080)
*Guanghao Jin,Jingpei Wu,Tianpei Guo,Yiyi Niu,Weidong Zhou,Guoyang Liu*

Main category: cs.LG

TL;DR: 이 논문에서는 텍스트 표현을 기반으로 이미지 내의 목표 객체를 정확하게 감지하는 Referring Expression Comprehension (REC) 작업을 개선하기 위한 새로운 벤치마크인 KnowDR-REC을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 모델의 한계로 인해 전통적인 REC 벤치마크는 MLLMs의 추론 능력을 평가하는 데 부적합하다.

Method: KnowDR-REC 벤치마크는 실세계 지식을 바탕으로 하며, 텍스트와 이미지 간의 세밀한 멀티모달 추론을 요구한다. 또한 부정적 샘플을 정교하게 구성하여 모델의 강인성과 반환상 능력을 평가하는 데이터셋을 포함한다.

Result: 16개의 최신 멀티모달 모델을 KnowDR-REC에서 평가한 결과, 기존 MLLMs가 지식 기반의 시각적 그라운딩 과제에서 여전히 어려움을 겪고 있음을 보여준다.

Conclusion: 제안된 벤치마크는 더 강력하고 해석 가능한 지식 집약적 시각적 그라운딩 프레임워크를 개발하도록 향후 연구에 영감을 줄 것으로 기대된다.

Abstract: Referring Expression Comprehension (REC) is a popular multimodal task that
aims to accurately detect target objects within a single image based on a given
textual expression. However, due to the limitations of earlier models,
traditional REC benchmarks either rely solely on intra-image cues or lack
sufficiently fine-grained instance annotations, making them inadequate for
evaluating the reasoning capabilities of Multi-modal Large Language Models
(MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC,
characterized by three key features: Firstly, it is built upon real-world
knowledge, requiring fine-grained multimodal reasoning across text and image.
Secondly, the dataset includes elaborately constructed negative samples via
fine-grained expression editing, designed to evaluate a model's robustness and
anti-hallucination ability. Lastly, we introduce three novel evaluation metrics
to systematically explore the model's internal reasoning process. We evaluate
16 state-of-the-art multimodal models on KnowDR-REC, with experimental results
showing that existing MLLMs still struggle with knowledge-driven visual
grounding tasks. Furthermore, we observe a decoupling between textual
understanding and visual grounding in MLLMs, where many models are
significantly influenced by memorized shortcut correlations, which severely
affect their behavior on our benchmark and hinder genuine multimodal reasoning.
We anticipate that the proposed benchmark will inspire future research towards
developing more robust, interpretable, and knowledge-intensive visual grounding
frameworks, driving the development of more reliable and robust multimodal
systems for complex real-world scenarios.

</details>


### [25] [Toward Lifelong Learning in Equilibrium Propagation: Sleep-like and Awake Rehearsal for Enhanced Stability](https://arxiv.org/abs/2508.14081)
*Yoshimasa Kubo,Jean Erik Delanois,Maxim Bazhenov*

Main category: cs.LG

TL;DR: 이 논문에서는 RNN의 연속 학습에서 생기는 재앙적 망각 문제를 해결하기 위해, 수면과 유사한 리플레이 통합 알고리즘인 SRC를 제안하고, 이 알고리즘이 RNN의 성능을 향상시킨다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 인간의 뇌는 이전의 지식을 새롭게 학습하는 과정에서도 보존하고 통합할 수 있는 능력이 있지만, RNN은 지속적인 학습에서 재앙적 망각의 문제에 직면해 있다.

Method: 이 논문에서는 Equilibrium Propagation으로 훈련된 RNN을 위한 수면 유사 리플레이 통합(SRC) 알고리즘을 제안한다.

Result: SRC는 RNN의 지속적인 학습 시 재앙적 망각에 대한 저항력을 크게 향상시키며, 여러 데이터셋에서 기존 피드포워드 네트워크보다 좋은 성능을 보였다.

Conclusion: 이 연구는 수면 유사 리플레이 기법이 RNN에 적용 가능함을 보여주고, 인공 신경망에 인간과 유사한 학습 행동을 통합할 가능성을 강조한다.

Abstract: Recurrent neural networks (RNNs) trained using Equilibrium Propagation (EP),
a biologically plausible training algorithm, have demonstrated strong
performance in various tasks such as image classification and reinforcement
learning. However, these networks face a critical challenge in continuous
learning: catastrophic forgetting, where previously acquired knowledge is
overwritten when new tasks are learned. This limitation contrasts with the
human brain's ability to retain and integrate both old and new knowledge, aided
by processes like memory consolidation during sleep through the replay of
learned information. To address this challenge in RNNs, here we propose a
sleep-like replay consolidation (SRC) algorithm for EP-trained RNNs. We found
that SRC significantly improves RNN's resilience to catastrophic forgetting in
continuous learning scenarios. In class-incremental learning with SRC
implemented after each new task training, the EP-trained multilayer RNN model
(MRNN-EP) performed significantly better compared to feedforward networks
incorporating several well-established regularization techniques. The MRNN-EP
performed on par with MRNN trained using Backpropagation Through Time (BPTT)
when both were equipped with SRC on MNIST data and surpassed BPTT-based models
on the Fashion MNIST, Kuzushiji-MNIST, CIFAR10, and ImageNet datasets.
Combining SRC with rehearsal, also known as "awake replay", further boosted the
network's ability to retain long-term knowledge while continuing to learn new
tasks. Our study reveals the applicability of sleep-like replay techniques to
RNNs and highlights the potential for integrating human-like learning behaviors
into artificial neural networks (ANNs).

</details>


### [26] [Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization](https://arxiv.org/abs/2508.14385)
*Kim Hammar,Tao Li*

Main category: cs.LG

TL;DR: MOBAL은 모델의 오정의 하에서도 사이버 공격에 대한 실시간 대응 계획을 개선하는 온라인 방법이다.


<details>
  <summary>Details</summary>
Motivation: 사이버 공격에 대한 효과적인 대응은 신속한 결정을 요구하지만, 대부분의 사고 대응 지원 프레임워크는 상세한 시스템 모델에 의존하여 실용성이 제한된다.

Method: MOBAL은 새 정보가 제공될 때 모델에 대한 추측을 베이지안 학습을 통해 반복적으로 개선하며, 유한 마르코프 모델로 양자화하여 동적 프로그래밍을 통해 효율적인 대응 계획을 수립한다.

Result: MOBAL은 적응성과 모델 오정의에 대한 강건성이 뛰어나며, CAGE-2 벤치마크 실험에서 최첨단 방법들보다 우수한 성능을 보인다.

Conclusion: MOBAL은 모델의 오정의에 따른 신속하고 효율적인 사이버 공격 대응을 가능하게 해준다.

Abstract: Effective responses to cyberattacks require fast decisions, even when
information about the attack is incomplete or inaccurate. However, most
decision-support frameworks for incident response rely on a detailed system
model that describes the incident, which restricts their practical utility. In
this paper, we address this limitation and present an online method for
incident response planning under model misspecification, which we call MOBAL:
Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture
about the model through Bayesian learning as new information becomes available,
which facilitates model adaptation as the incident unfolds. To determine
effective responses online, we quantize the conjectured model into a finite
Markov model, which enables efficient response planning through dynamic
programming. We prove that Bayesian learning is asymptotically consistent with
respect to the information feedback. Additionally, we establish bounds on
misspecification and quantization errors. Experiments on the CAGE-2 benchmark
show that MOBAL outperforms the state of the art in terms of adaptability and
robustness to model misspecification.

</details>


### [27] [Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation](https://arxiv.org/abs/2508.14082)
*Ye Su,Hezhe Qiao,Wei Huang,Lin Chen*

Main category: cs.LG

TL;DR: 이 논문은 반지도 회귀를 위한 DRILL이라는 새로운 프레임워크를 제안하며, pseudo-label의 품질에 대한 의존도를 줄이고 과적합의 위험을 완화하기 위해 일반 회귀 문제를 이산 분포 추정 작업으로 변환한다.


<details>
  <summary>Details</summary>
Motivation: 반지도 회귀는 라벨이 많은 데이터에 대한 의존도를 줄이면서 샘플의 연속 점수를 예측하는 것을 목표로 한다.

Method: 제안된 DRILL 프레임워크는 일반 회귀 작업을 여러 개의 버킷에 대한 이산 분포 추정 작업으로 변환하고, 교사와 학생 간의 분포 정렬을 위해 분리된 분포 정렬(DDA)을 적용한다.

Result: DRILL은 다양한 도메인의 데이터셋에서 강력한 일반화 성능을 보이며, 기존의 경쟁 방법들보다 우수한 성능을 나타낸다.

Conclusion: 제안된 방법은 반지도 회귀 작업에서 과적합을 줄이고, 더 강건하고 일반화된 지식을 학생이 학습할 수 있게 돕는다.

Abstract: Semi-supervised regression (SSR), which aims to predict continuous scores of
samples while reducing reliance on a large amount of labeled data, has recently
received considerable attention across various applications, including computer
vision, natural language processing, and audio and medical analysis. Existing
semi-supervised methods typically apply consistency regularization on the
general regression task by generating pseudo-labels. However, these methods
heavily rely on the quality of pseudo-labels, and direct regression fails to
learn the label distribution and can easily lead to overfitting. To address
these challenges, we introduce an end-to-end Decoupled Representation
distillation framework (DRILL) which is specially designed for the
semi-supervised regression task where we transform the general regression task
into a Discrete Distribution Estimation (DDE) task over multiple buckets to
better capture the underlying label distribution and mitigate the risk of
overfitting associated with direct regression. Then we employ the Decoupled
Distribution Alignment (DDA) to align the target bucket and non-target bucket
between teacher and student on the distribution of buckets, encouraging the
student to learn more robust and generalized knowledge from the teacher.
Extensive experiments conducted on datasets from diverse domains demonstrate
that the proposed DRILL has strong generalization and outperforms the competing
methods.

</details>


### [28] [GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values](https://arxiv.org/abs/2508.14083)
*Songyu Ke,Chenyu Wu,Yuxuan Liang,Xiuwen Yi,Yanping Sun,Junbo Zhang,Yu Zheng*

Main category: cs.LG

TL;DR: 이 논문은 관심 지점(POI)에서의 군중 흐름 추정 문제를 자기 지도형 속성 그래프 표현 학습 과제로 재구성하고, 공공 서비스 및 도시 계획을 위한 효과적인 교통 관리의 중요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 도시 감지 기술의 한계로 인해 대부분의 데이터 출처에서 제공되는 군중 흐름 데이터의 품질이 불충분하여 각 POI에서의 군중 흐름 모니터링이 어려워진다.

Method: 군중 흐름 추정 문제를 자기 지도형 속성 그래프 표현 학습 작업으로 재구성하고, 혁신적인 대조적 자기 학습 프레임워크를 제안한다. 이 접근법은 POI와 그에 따른 거리 기반의 공간 인접 그래프를 구성하는 것으로 시작한다.

Result: 실제 두 개의 데이터 세트를 기반으로 한 실험 결과, 광범위한 소음 데이터로 사전 훈련된 모델이 처음부터 훈련된 모델보다 일관되게 성능이 뛰어난 것으로 나타났다.

Conclusion: 제안한 모델은 정확한 군중 흐름 데이터로 미세 조정을 통해 성능을 더욱 향상시킨다.

Abstract: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
  To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.

</details>


### [29] [Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure](https://arxiv.org/abs/2508.14085)
*Hanseul Kang,Shervin Karimkashi,Ville Vuorinen*

Main category: cs.LG

TL;DR: 본 논문은 다중 매개변수 시뮬레이션 데이터에서 해석 가능한 부분 미분 방정식 및 서브그리드 스케일 폐쇄를 발견하기 위한 확장 가능한 희소 회귀 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 비선형 역학 식별 방법의 한계를 극복하고 풍동 모델링의 새로운 접근 방식을 찾기 위함입니다.

Method: SINDy에 기반하여, 물리적 매개변수가 통합 회귀 내에서 변할 수 있도록 하는 기호적 매개변수화, 단위 일관성을 유지하면서 후보 라이브러리를 줄이는 차원 유사성 필터, 배치 처리를 가능하게 하는 메모리 효율적인그램 행렬 누적, 안정적인 모델 식별을 위한 계수 안정성 분석을 통한 앙상블 합의를 포함합니다.

Result: 표준 일차원 기준 벤치마크에서 매개변수 범위에 걸쳐 governing equation을 신뢰성 있게 회복하였으며, 필터링된 Burgers 데이터셋에 적용하였을 때 Smagorinsky 상수에 해당하는 SGS 폐쇄를 자율적으로 발견하였습니다.

Conclusion: 발견된 모델은 필터링 스케일 전반에 걸쳐 개선된 예측 정확도를 보여주며, 기존의 난류 모델링 방법에 대한 보완적 접근법을 제공합니다.

Abstract: We present a scalable, parameter-aware sparse regression framework for
discovering interpretable partial differential equations and subgrid-scale
closures from multi-parameter simulation data. Building on SINDy (Sparse
Identification of Nonlinear Dynamics), our approach addresses key limitations
through four innovations: symbolic parameterisation enabling physical
parameters to vary within unified regression; Dimensional Similarity Filter
enforcing unit-consistency whilst reducing candidate libraries;
memory-efficient Gram-matrix accumulation enabling batch processing; and
ensemble consensus with coefficient stability analysis for robust model
identification.
  Validation on canonical one-dimensional benchmarks demonstrates reliable
recovery of governing equations across parameter ranges. Applied to filtered
Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} =
0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$,
corresponding to a Smagorinsky constant of approximately 0.4004. This
represents autonomous discovery of Smagorinsky-type closure structure from data
without prior theoretical assumptions.
  The discovered model achieves $R^2 = 0.886$ across filter scales and
demonstrates improved prediction accuracy compared to classical closures. The
framework's ability to identify physically meaningful SGS forms and calibrate
coefficients offers a complementary approach to existing turbulence modelling
methods, contributing to the growing field of data-driven closure discovery.

</details>


### [30] [EEGDM: EEG Representation Learning via Generative Diffusion Model](https://arxiv.org/abs/2508.14086)
*Jia Hong Puah,Sim Kuan Goh,Ziwei Zhang,Zixuan Ye,Chow Khuen Chan,Kheng Seang Lim,Si Lei Fong,Kok Sin Woon*

Main category: cs.LG

TL;DR: 이 연구에서는 EEG 데이터의 의미 있는 표현을 학습하기 위한 EEGDM(Generative Diffusion Model)을 제안하고, 이를 통해 경량화된 모델로 고성능 분류를 달성하였다.


<details>
  <summary>Details</summary>
Motivation: 뇌를 모니터링하고 신경 장애를 진단하는 데 필요한 EEG 신호의 의미 있는 표현을 학습하는 것이 어렵다.

Method: EEGDM 기반의 구조화된 상태 공간 모델(SSMDP)을 개발하고 Denoising Diffusion Probabilistic Model을 사용하여 훈련하였다.

Result: 우리는 Temple University EEG Event Corpus를 통해 비교한 결과, 우리의 방법이 기존 방법보다 성능이 뛰어나고 약 19배 경량화된 것을 나타냈다.

Conclusion: EEGDM은 현재의 대형 모델에 대한 유망한 대안을 제공함을 보여주었다.

Abstract: While electroencephalogram (EEG) has been a crucial tool for monitoring the
brain and diagnosing neurological disorders (e.g., epilepsy), learning
meaningful representations from raw EEG signals remains challenging due to
limited annotations and high signal variability. Recently, EEG foundation
models (FMs) have shown promising potential by adopting transformer
architectures and self-supervised pre-training methods from large language
models (e.g., masked prediction) to learn representations from diverse EEG
data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large
models often incurred high computational costs during both training and
inference, with only marginal performance improvements as model size increases.
In this work, we proposed EEG representation learning framework building upon
Generative Diffusion Model (EEGDM). Specifically, we developed structured
state-space model for diffusion pretraining (SSMDP) to better capture the
temporal dynamics of EEG signals and trained the architecture using a Denoising
Diffusion Probabilistic Model. The resulting latent EEG representations were
then used for downstream classification tasks via our proposed latent fusion
transformer (LFT). To evaluate our method, we used the multi-event Temple
University EEG Event Corpus and compared EEGDM with current state-of-the-art
approaches, including EEG FMs. Empirical results showed that our method
outperformed existing methods while being approximately 19x more lightweight.
These findings suggested that EEGDM offered a promising alternative to current
FMs. Our code is available at: https://github.com/jhpuah/EEGDM.

</details>


### [31] [FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics](https://arxiv.org/abs/2508.14087)
*David Park,Shuhang Li,Yi Huang,Xihaier Luo,Haiwang Yu,Yeonju Go,Christopher Pinkenburg,Yuewei Lin,Shinjae Yoo,Joseph Osborn,Jin Huang,Yihui Ren*

Main category: cs.LG

TL;DR: 대형 언어 모델은 자가 감독 훈련을 통해 일반화 가능성이 높은 모델을 가능하게 하여 인공지능 분야에 혁신을 가져왔습니다. 본 논문은 실험 입자 물리학에 대한 과학적 기초 모델의 적용 가능성을 탐구하며, 1100만 건 이상의 입자 충돌 사건으로 구성된 새로운 데이터셋과 다양한 하위 작업을 통해 자가 감독 학습 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 입자 물리학에 응용할 수 있는 과학적 기초 모델의 가능성과 해당 분야의 데이터 특성에 대한 이해가 필요하다.

Method: 11백만 개 이상의 입자 충돌 사건을 포함한 새로운 데이터셋과 하위 작업을 위한 라벨링된 데이터를 제안하며, 탐지기 데이터에 대한 자가 감독 훈련 방법을 개발한다.

Result: 이러한 새로운 FM은 188백만 개 파라미터를 가진 모델로 신경 확장 가능성을 보여주며, 모든 하위 작업에서 기준 모델에 비해 일관되게 성능이 우수하다.

Conclusion: FM이 과업에 구애받지 않는 표현을 추출하되, 단일 선형 매핑을 통해 다양한 하위 작업에 특화될 수 있음을 보여준다.

Abstract: Large language models have revolutionized artificial intelligence by enabling
large, generalizable models trained through self-supervision. This paradigm has
inspired the development of scientific foundation models (FMs). However,
applying this capability to experimental particle physics is challenging due to
the sparse, spatially distributed nature of detector data, which differs
dramatically from natural language. This work addresses if an FM for particle
physics can scale and generalize across diverse tasks. We introduce a new
dataset with more than 11 million particle collision events and a suite of
downstream tasks and labeled data for evaluation. We propose a novel
self-supervised training method for detector data and demonstrate its neural
scalability with models that feature up to 188 million parameters. With frozen
weights and task-specific adapters, this FM consistently outperforms baseline
models across all downstream tasks. The performance also exhibits robust
data-efficient adaptation. Further analysis reveals that the representations
extracted by the FM are task-agnostic but can be specialized via a single
linear mapping for different downstream tasks.

</details>


### [32] [CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection](https://arxiv.org/abs/2508.14088)
*Haomin Wen,Shurui Cao,Leman Akoglu*

Main category: cs.LG

TL;DR: 인간 이동성의 집단적 anomaly 탐지를 위한 새로운 모델인 CoBAD를 제안하며, 이는 개인 이동 패턴과 개인 간 상호작용을 모델링하는 두 단계의 주의 기법을 활용하여 대규모 이동 데이터 세트에서 기존 방법보다 우수한 성능을 보임.


<details>
  <summary>Details</summary>
Motivation: 인간 이동성의 이상 탐지는 공공 안전 및 도시 계획과 같은 응용 프로그램에 필수적이지만, 집단적 이상 탐지는 아직 충분히 탐구되지 않은 문제입니다.

Method: CoBAD라는 새로운 모델을 제안하고, 이는 집단 사건 시퀀스(CES)에서 비지도 학습으로 문제를 공식화하며, 개인 이동 패턴과 다수 개인 간의 상호작용을 모델링하기 위해 두 단계의 주의 메커니즘을 사용합니다.

Result: CoBAD는 집단적인 비정상적인 동작을 탐지하는 데 있어 기존의 이상 탐지 기준보다 13%-18%의 AUCROC 및 19%-70%의 AUCPR 향상을 보여주었습니다.

Conclusion: 대규모 집단 행동 데이터를 통해 사전 훈련된 CoBAD는 예상치 못한 동시 발생 이상과 결석 이상 두 가지 유형의 집단적 이상을 탐지할 수 있습니다.

Abstract: Detecting anomalies in human mobility is essential for applications such as
public safety and urban planning. While traditional anomaly detection methods
primarily focus on individual movement patterns (e.g., a child should stay at
home at night), collective anomaly detection aims to identify irregularities in
collective mobility behaviors across individuals (e.g., a child is at home
alone while the parents are elsewhere) and remains an underexplored challenge.
Unlike individual anomalies, collective anomalies require modeling
spatiotemporal dependencies between individuals, introducing additional
complexity. To address this gap, we propose CoBAD, a novel model designed to
capture Collective Behaviors for human mobility Anomaly Detection. We first
formulate the problem as unsupervised learning over Collective Event Sequences
(CES) with a co-occurrence event graph, where CES represents the event
sequences of related individuals. CoBAD then employs a two-stage attention
mechanism to model both the individual mobility patterns and the interactions
across multiple individuals. Pre-trained on large-scale collective behavior
data through masked event and link reconstruction tasks, CoBAD is able to
detect two types of collective anomalies: unexpected co-occurrence anomalies
and absence anomalies, the latter of which has been largely overlooked in prior
work. Extensive experiments on large-scale mobility datasets demonstrate that
CoBAD significantly outperforms existing anomaly detection baselines, achieving
an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is
available at https://github.com/wenhaomin/CoBAD.

</details>


### [33] [Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions](https://arxiv.org/abs/2508.14091)
*Matthew Morris,David J. Tena Cucala,Bernardo Cuenca Grau*

Main category: cs.LG

TL;DR: 이 논문은 링크 예측을 위한 일반적인 방법론을 제시하며, GNN과 점수 함수를 조정하여 예측을 설명하는 규칙을 추출하는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: GNN의 설명 가능성이 부족한 문제를 해결하고자 한다.

Method: 모노토닉한 GNN과 점수 함수를 조정하여 예측을 설명하는 규칙을 추출한다.

Result: 링크 예측 벤치마크에서 모노토닉 GNN과 점수 함수가 실제로 잘 수행되며 많은 의미 있는 규칙을 생성함을 보여준다.

Conclusion: 모노토닉 GNN과 점수 함수를 사용하면 링크 예측에서 유용한 결과를 얻을 수 있다.

Abstract: Graph neural networks (GNNs) are often used for the task of link prediction:
predicting missing binary facts in knowledge graphs (KGs). To address the lack
of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs
with provable correspondence guarantees. The extracted rules can be used to
explain the GNN's predictions; furthermore, they can help characterise the
expressive power of various GNN models. However, these works address only a
form of link prediction based on a restricted, low-expressivity graph
encoding/decoding method. In this paper, we consider a more general and popular
approach for link prediction where a scoring function is used to decode the GNN
output into fact predictions. We show how GNNs and scoring functions can be
adapted to be monotonic, use the monotonicity to extract sound rules for
explaining predictions, and leverage existing results about the kind of rules
that scoring functions can capture. We also define procedures for obtaining
equivalent Datalog programs for certain classes of monotonic GNNs with scoring
functions. Our experiments show that, on link prediction benchmarks, monotonic
GNNs and scoring functions perform well in practice and yield many sound rules.

</details>


### [34] [Physics-Informed Reward Machines](https://arxiv.org/abs/2508.14093)
*Daniel Ajeleye,Ashutosh Trivedi,Majid Zamani*

Main category: cs.LG

TL;DR: 물리 정보 기반 보상 기계(pRMs)는 강화 학습(RL) 에이전트를 위한 복잡한 학습 목표와 보상 구조를 표현할 수 있도록 설계된 기호 기계이다.


<details>
  <summary>Details</summary>
Motivation: 본 연구는 비마르코프 보상을 명확하게 표현하고 프로그래밍 가능성을 향상시키기 위해 보상 기계(RM)의 필요성을 강조한다.

Method: 물리 정보 기반 보상 기계(pRMs)를 도입하고, 이를 통해 강화 학습 알고리즘이 카운터팩추얼 경험 생성 및 보상 형성을 통해 pRMs를 활용할 수 있도록 한다.

Result: 실험 결과는 이러한 기술이 강화 학습의 훈련 단계에서 보상 획득을 가속화함을 보여준다.

Conclusion: pRMs는 여러 제어 작업에서 학습 효율성을 크게 향상시킴을 나타낸다.

Abstract: Reward machines (RMs) provide a structured way to specify non-Markovian
rewards in reinforcement learning (RL), thereby improving both expressiveness
and programmability. Viewed more broadly, they separate what is known about the
environment, captured by the reward mechanism, from what remains unknown and
must be discovered through sampling. This separation supports techniques such
as counterfactual experience generation and reward shaping, which reduce sample
complexity and speed up learning. We introduce physics-informed reward machines
(pRMs), a symbolic machine designed to express complex learning objectives and
reward structures for RL agents, thereby enabling more programmable,
expressive, and efficient learning. We present RL algorithms capable of
exploiting pRMs via counterfactual experiences and reward shaping. Our
experimental results show that these techniques accelerate reward acquisition
during the training phases of RL. We demonstrate the expressiveness and
effectiveness of pRMs through experiments in both finite and continuous
physical environments, illustrating that incorporating pRMs significantly
improves learning efficiency across several control tasks.

</details>


### [35] [Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets](https://arxiv.org/abs/2508.14094)
*Benjamin Pikus,Pratyush Ranjan Tiwari,Burton Ye*

Main category: cs.LG

TL;DR: 언어 모델 세부 조정을 위한 고품질 훈련 예제를 수집하는 것은 비용이 많이 들며, 실질적인 예산이 데이터 획득량을 제한합니다. 이 논문에서는 고정된 획득 예산 하에서 쉽고 중간, 어렵거나 랜덤 난이도의 예제 중 어떤 것을 우선해야 하는지를 연구합니다. 실험 결과, 가장 어려운 예제로 훈련할 때 성능 향상이 최대 47%에 달하며, 쉬운 예제로 훈련할 때는 가장 작은 향상을 보입니다.


<details>
  <summary>Details</summary>
Motivation: 예산이 제한된 상황에서 언어 모델 세부 조정을 위한 수집 예제의 난이도를 최적화할 필요성.

Method: 그룹 상대 정책 최적화(GRPO)를 사용하여 다양한 모델 크기와 패밀리에서 여러 난이도의 예제를 비교하는 방법론을 적용했습니다.

Result: 가장 어려운 예제로 훈련한 경우 최대 47%의 성능 향상을 나타냈으며, 쉬운 예제에서의 향상은 가장 적었습니다.

Conclusion: 예산이 제한된 상황에서, GRPO를 사용할 때 어려운 예제를 우선적으로 다루는 것이 추론 작업에서 상당한 성능 향상을 가져올 수 있다는 것을 보여줍니다.

Abstract: Collecting high-quality training examples for language model fine-tuning is
expensive, with practical budgets limiting the amount of data that can be
procured. We investigate a critical question for resource-constrained
alignment: under a fixed acquisition budget, should practitioners prioritize
examples that are easy, medium, hard, or of random difficulty? We study Group
Relative Policy Optimization (GRPO) fine-tuning across different model sizes
and families, comparing four subset selection policies chosen from the same
unlabeled pool using base-model difficulty estimates obtained via multi-sample
evaluation. Our experiments reveal that training on the hardest examples yields
the largest performance gains, up to 47%, while training on easy examples yield
the smallest gains. Analysis reveals that this effect arises from harder
examples providing more learnable opportunities during GRPO training. These
findings provide practical guidance for budget-constrained post-training:
prioritizing hard examples yields substantial performance gains on reasoning
tasks when using GRPO.

</details>


### [36] [Implicit Hypergraph Neural Network](https://arxiv.org/abs/2508.14101)
*Akash Choudhuri,Yongjian Zhong,Bijaya Adhikari*

Main category: cs.LG

TL;DR: 이 논문은 고차 관계를 포착하기 위한 하이퍼그래프 신경망의 한계를 다루고, 임플리시트 하이퍼그래프 신경망(IHNN)을 제안하여 장기 의존성을 효과적으로 학습하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 하이퍼그래프 신경망은 고차 관계를 모델링하기 위해 많이 사용되지만, 장기 의존성을 포착하는 데 한계가 있다.

Method: 임플리시트 하이퍼그래프 신경망(IHNN)이라는 새로운 프레임워크를 제안하여 정점과 하이퍼모서에 대한 고정점 표현을 동시에 학습한다.

Result: IHNN은 실제 하이퍼그래프에 대한 노드 분류 실험에서 가장 유사한 기존 연구를 대부분의 설정에서 초월하며, 하이퍼그래프 학습에서 새로운 최첨단 결과를 달성한다.

Conclusion: IHNN은 효율적인 모델 학습을 위해 임플리시트 미분과 투사 경량 하강법을 활용하여 장기 의존성 문제를 효과적으로 해결한다.

Abstract: Hypergraphs offer a generalized framework for capturing high-order
relationships between entities and have been widely applied in various domains,
including healthcare, social networks, and bioinformatics. Hypergraph neural
networks, which rely on message-passing between nodes over hyperedges to learn
latent representations, have emerged as the method of choice for predictive
tasks in many of these domains. These approaches typically perform only a small
number of message-passing rounds to learn the representations, which they then
utilize for predictions. The small number of message-passing rounds comes at a
cost, as the representations only capture local information and forego
long-range high-order dependencies. However, as we demonstrate, blindly
increasing the message-passing rounds to capture long-range dependency also
degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture
long-range dependencies in standard graphs while maintaining performance.
Despite their popularity, prior work has not studied long-range dependency
issues on hypergraph neural networks. Here, we first demonstrate that existing
hypergraph neural networks lose predictive power when aggregating more
information to capture long-range dependency. We then propose Implicit
Hypergraph Neural Network (IHNN), a novel framework that jointly learns
fixed-point representations for both nodes and hyperedges in an end-to-end
manner to alleviate this issue. Leveraging implicit differentiation, we
introduce a tractable projected gradient descent approach to train the model
efficiently. Extensive experiments on real-world hypergraphs for node
classification demonstrate that IHNN outperforms the closest prior works in
most settings, establishing a new state-of-the-art in hypergraph learning.

</details>


### [37] [Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces](https://arxiv.org/abs/2508.14102)
*Thomas Gallien*

Main category: cs.LG

TL;DR: 본 논문은 신뢰 지역 기반 정책 최적화 방법이 행동 공간 차원의 변화에 따라 최적화 경관에 미치는 영향을 이론적 분석과 실험적 평가를 통해 보여준다.


<details>
  <summary>Details</summary>
Motivation: 지속적인 제어 작업에서 안정성과 높은 성능을 제공하는 신뢰 지역 기반 최적화 방법에 대한 관심이 높아지면서 다양한 운동 구조에 대처할 수 있는 능력인 형태학적 일반화에 대한 수요가 증가하고 있다.

Method: 신뢰 지역 정책 최적화(TRPO) 및 그 첫 번째 근사인 근접 정책 최적화(PPO)에 중점을 두고 신뢰 지역 기반 정책 최적화 방법에 대한 이론적 분석을 수행한다.

Result: 동작 공간 차원의 변화가 최적화 경관에 미치는 영향을 보여주며, 특히 KL 발산이나 정책 클리핑 패널티에 의해 부과된 제약 조건 하에서 관찰된다.

Conclusion: 형태 학적 변화를 통해 실시한 경험적 평가와 함께 이론적 통찰이 보완된다. 이 벤치마크는 기본 작업을 변경하지 않고 운동 구조를 다양하게 변형할 수 있는 체계적으로 통제된 설정을 제공하여 형태학적 일반화를 연구하는 데 적합하다.

Abstract: Trust region-based optimization methods have become foundational
reinforcement learning algorithms that offer stability and strong empirical
performance in continuous control tasks. Growing interest in scalable and
reusable control policies translate also in a demand for morphological
generalization, the ability of control policies to cope with different
kinematic structures. Graph-based policy architectures provide a natural and
effective mechanism to encode such structural differences. However, while these
architectures accommodate variable morphologies, the behavior of trust region
methods under varying action space dimensionality remains poorly understood. To
this end, we conduct a theoretical analysis of trust region-based policy
optimization methods, focusing on both Trust Region Policy Optimization (TRPO)
and its widely used first-order approximation, Proximal Policy Optimization
(PPO). The goal is to demonstrate how varying action space dimensionality
influence the optimization landscape, particularly under the constraints
imposed by KL-divergence or policy clipping penalties. Complementing the
theoretical insights, an empirical evaluation under morphological variation is
carried out using the Gymnasium Swimmer environment. This benchmark offers a
systematically controlled setting for varying the kinematic structure without
altering the underlying task, making it particularly well-suited to study
morphological generalization.

</details>


### [38] [A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data and Machine Learning](https://arxiv.org/abs/2508.14125)
*Madyan Bagosher,Tala Mustafa,Mohammad Alsmirat,Amal Al-Ali,Isam Mashhour Al Jawarneh*

Main category: cs.LG

TL;DR: 도시 인구 증가에 따른 주차 관리 문제를 해결하기 위해 여러 데이터 소스를 통합한 스마트 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대학 캠퍼스에서는 학생들이 수업 시간에 빠르고 편리하게 빈 주차 공간을 찾는 것이 중요하다.

Method: 물리적 센서를 설치하지 않고, 위치 서비스를 활용하여 주차 행동과 차량 이동 패턴을 분석하는 스마트 프레임워크를 제안한다.

Result: 랜덤 포레스트 회귀가 최저 RMSE 0.142와 최고 R2 0.582를 기록했다.

Conclusion: LSTM 모델이 추가 데이터와 긴 시간 단계로 더 나은 성능을 보일 가능성이 있다.

Abstract: As urban populations continue to grow, cities face numerous challenges in
managing parking and determining occupancy. This issue is particularly
pronounced in university campuses, where students need to find vacant parking
spots quickly and conveniently during class timings. The limited availability
of parking spaces on campuses underscores the necessity of implementing
efficient systems to allocate vacant parking spots effectively. We propose a
smart framework that integrates multiple data sources, including street maps,
mobility, and meteorological data, through a spatial join operation to capture
parking behavior and vehicle movement patterns over the span of 3 consecutive
days with an hourly duration between 7AM till 3PM. The system will not require
any sensing tools to be installed in the street or in the parking area to
provide its services since all the data needed will be collected using location
services. The framework will use the expected parking entrance and time to
specify a suitable parking area. Several forecasting models, namely, Linear
Regression, Support Vector Regression (SVR), Random Forest Regression (RFR),
and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was
employed using grid search, and model performance is assessed using Root Mean
Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of
Determination (R2). Random Forest Regression achieved the lowest RMSE of 0.142
and highest R2 of 0.582. However, given the time-series nature of the task, an
LSTM model may perform better with additional data and longer timesteps.

</details>


### [39] [From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery](https://arxiv.org/abs/2508.14111)
*Jiaqi Wei,Yuejin Yang,Xiang Zhang,Yuhan Chen,Xiang Zhuang,Zhangyang Gao,Dongzhan Zhou,Guangshuai Wang,Zhiqiang Gao,Juntai Cao,Zijie Qiu,Xuming He,Qiang Zhang,Chenyu You,Shuangjia Zheng,Ning Ding,Wanli Ouyang,Nanqing Dong,Yu Cheng,Siqi Sun,Lei Bai,Bowen Zhou*

Main category: cs.LG

TL;DR: AI가 과학적 발견을 재구성하고 있으며, 전문화된 도구에서 자율 연구 파트너로 발전하고 있다.


<details>
  <summary>Details</summary>
Motivation: AI가 과학 연구에서 독립적으로 활용되는 방식을 탐구하고자 한다.

Method: 대규모 언어 모델, 다중 모달 시스템, 통합 연구 플랫폼을 활용하여 자율 과학적 발견의 평가 및 분석을 진행한다.

Result: AI가 가설 생성, 실험 설계 및 분석에서 자율적으로 기능하는 능력을 보여주며, 이를 통해 여러 과학 분야에서의 응용 사례를 제시한다.

Conclusion: Agentic Science가 AI 기반 연구를 발전시키기 위한 체계적인 패러다임으로 자리를 잡았다.

Abstract: Artificial intelligence (AI) is reshaping scientific discovery, evolving from
specialized computational tools into autonomous research partners. We position
Agentic Science as a pivotal stage within the broader AI for Science paradigm,
where AI systems progress from partial assistance to full scientific agency.
Enabled by large language models (LLMs), multimodal systems, and integrated
research platforms, agentic AI shows capabilities in hypothesis generation,
experimental design, execution, analysis, and iterative refinement -- behaviors
once regarded as uniquely human. This survey provides a domain-oriented review
of autonomous scientific discovery across life sciences, chemistry, materials
science, and physics. We unify three previously fragmented perspectives --
process-oriented, autonomy-oriented, and mechanism-oriented -- through a
comprehensive framework that connects foundational capabilities, core
processes, and domain-specific realizations. Building on this framework, we (i)
trace the evolution of AI for Science, (ii) identify five core capabilities
underpinning scientific agency, (iii) model discovery as a dynamic four-stage
workflow, (iv) review applications across the above domains, and (v) synthesize
key challenges and future opportunities. This work establishes a
domain-oriented synthesis of autonomous scientific discovery and positions
Agentic Science as a structured paradigm for advancing AI-driven research.

</details>


### [40] [Comparison of derivative-free and gradient-based minimization for multi-objective compositional design of shape memory alloys](https://arxiv.org/abs/2508.14127)
*S. Josyula,Y. Noiman,E. J. Payton,T. Giovannelli*

Main category: cs.LG

TL;DR: 본 연구에서는 경제적이고 지속 가능한 형태기억합금(SMA) 조성을 최적화하여 원하는 마르텐사이트 시작 온도(Ms)를 달성하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 형태기억합금(SMA)의 성능 목표를 충족하면서도 저렴하고 지속 가능하도록 설계하는 것은 복잡한 과제이다.

Method: 우리는 머신러닝 모델을 서브 예측기로 사용하고 수치 최적화 방법을 적용하여 적합한 합금 조합을 탐색한다. 두 가지 타입의 머신러닝 모델, 즉 트리 기반 앙상블과 신경망을 훈련시켰다.

Result: 두 모델 모두 유사한 정확도로 Ms를 예측하지만, 신경망과 결합된 최적화기가 더 일관되게 더 나은 솔루션을 찾았다.

Conclusion: 이 연구는 물리 정보 데이터, 머신러닝 모델, 최적화 알고리즘을 결합하여 새로운 SMA 조성을 탐색하는 실용적인 접근법을 보여준다.

Abstract: Designing shape memory alloys (SMAs) that meet performance targets while
remaining affordable and sustainable is a complex challenge. In this work, we
focus on optimizing SMA compositions to achieve a desired martensitic start
temperature (Ms) while minimizing cost. To do this, we use machine learning
models as surrogate predictors and apply numerical optimization methods to
search for suitable alloy combinations. We trained two types of machine
learning models, a tree-based ensemble and a neural network, using a dataset of
experimentally characterized alloys and physics-informed features. The
tree-based model was used with a derivative-free optimizer (COBYLA), while the
neural network, which provides gradient information, was paired with a
gradient-based optimizer (TRUST-CONSTR). Our results show that while both
models predict Ms with similar accuracy, the optimizer paired with the neural
network finds better solutions more consistently. COBYLA often converged to
suboptimal results, especially when the starting guess was far from the target.
The TRUST-CONSTR method showed more stable behavior and was better at reaching
alloy compositions that met both objectives. This study demonstrates a
practical approach to exploring new SMA compositions by combining
physics-informed data, machine learning models, and optimization algorithms.
Although the scale of our dataset is smaller than simulation-based efforts, the
use of experimental data improves the reliability of the predictions. The
approach can be extended to other materials where design trade-offs must be
made with limited data.

</details>


### [41] [ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification](https://arxiv.org/abs/2508.14134)
*Xin Wu,Fei Teng,Ji Zhang,Xingwang Li,Yuxuan Liang*

Main category: cs.LG

TL;DR: 본 논문은 분산 외(TSC) 데이터에 대한 안정적 성능을 확보하기 위한 ERIS 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 이 연구의 동기는 분산 외 데이터에서의 신뢰할 수 있는 성능을 확보하는 핵심 장애물을 해결하는 것입니다.

Method: ERIS 프레임워크는 에너지 기반의 보정 메커니즘, 가중치 수준의 직교성 전략, 보조 적대적 훈련 메커니즘을 포함하여 특징 분리 과정을 안내합니다.

Result: ERIS는 네 가지 벤치마크에서 평균 4.04% 정확도를 향상시켰습니다.

Conclusion: 결론적으로, ERIS는 효과적인 특징 분리를 통해 TSC에서 성능을 개선하는 데 기여합니다.

Abstract: An ideal time series classification (TSC) should be able to capture invariant
representations, but achieving reliable performance on out-of-distribution
(OOD) data remains a core obstacle. This obstacle arises from the way models
inherently entangle domain-specific and label-relevant features, resulting in
spurious correlations. While feature disentanglement aims to solve this,
current methods are largely unguided, lacking the semantic direction required
to isolate truly universal features. To address this, we propose an end-to-end
Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework
to enable guided and reliable feature disentanglement. The core idea is that
effective disentanglement requires not only mathematical constraints but also
semantic guidance to anchor the separation process. ERIS incorporates three key
mechanisms to achieve this goal. Specifically, we first introduce an
energy-guided calibration mechanism, which provides crucial semantic guidance
for the separation, enabling the model to self-calibrate. Additionally, a
weight-level orthogonality strategy enforces structural independence between
domain-specific and label-relevant features, thereby mitigating their
interference. Moreover, an auxiliary adversarial training mechanism enhances
robustness by injecting structured perturbations. Experiments demonstrate that
ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy
across four benchmarks.

</details>


### [42] [STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers](https://arxiv.org/abs/2508.14138)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Brent ByungHoon Kang,Hyeongboo Baek*

Main category: cs.LG

TL;DR: 본 논문에서는 스파이킹 신경망(SNNs)의 높은 대기 시간과 계산 오버헤드를 줄이기 위한 STAS(Spatio-Temporal Adaptive computation time for Spiking transformers)라는 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: SNNs는 에너지 효율성이 뛰어나지만, 다중 타임스텝 작업 특성으로 인해 높은 대기 시간과 계산 오버헤드가 발생한다.

Method: STAS는 정적 아키텍처와 동적 계산 정책을 공동 설계하고, 통합 스파이크 패치 분할(I-SPS) 모듈을 도입하여 시간적 안정성을 확립하며, A-SSA 모듈을 통해 공간적 및 시간적 축을 따라 두 차원 토큰 프루닝을 수행한다.

Result: STAS는 CIFAR-10, CIFAR-100, ImageNet에서 검증되었으며, 각각 45.9%, 43.8%, 30.1%의 에너지 소비를 감소시키면서도 SOTA 모델에 비해 정확성을 향상시켰다.

Conclusion: 이 프레임워크는 SNN 기반 비전 트랜스포머의 효율성과 성능을 동시에 개선할 수 있는 잠재력을 가진다.

Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural
networks (ANNs) but suffer from high latency and computational overhead due to
their multi-timestep operational nature. While various dynamic computation
methods have been developed to mitigate this by targeting spatial, temporal, or
architecture-specific redundancies, they remain fragmented. While the
principles of adaptive computation time (ACT) offer a robust foundation for a
unified approach, its application to SNN-based vision Transformers (ViTs) is
hindered by two core issues: the violation of its temporal similarity
prerequisite and a static architecture fundamentally unsuited for its
principles. To address these challenges, we propose STAS (Spatio-Temporal
Adaptive computation time for Spiking transformers), a framework that
co-designs the static architecture and dynamic computation policy. STAS
introduces an integrated spike patch splitting (I-SPS) module to establish
temporal stability by creating a unified input representation, thereby solving
the architectural problem of temporal dissimilarity. This stability, in turn,
allows our adaptive spiking self-attention (A-SSA) module to perform
two-dimensional token pruning across both spatial and temporal axes.
Implemented on spiking Transformer architectures and validated on CIFAR-10,
CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%,
and 30.1%, respectively, while simultaneously improving accuracy over SOTA
models.

</details>


### [43] [Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach](https://arxiv.org/abs/2508.14135)
*Collins O. Ogbodo,Timothy J. Rogers,Mattia Dal Borgo,David J. Wagg*

Main category: cs.LG

TL;DR: 이 연구는 동적 모드 테스트 환경에서 적응형 센서 배치를 위한 에이전트 기반 의사 결정 지원 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 모달 테스트는 구조 분석에서 중요한 역할을 하며, 효과적인 테스트 캠페인을 설계하는 것은 복잡한 실험 계획을 포함합니다.

Method: 이 연구는 불완전하게 정의된 부분 관찰 마르코프 결정 프로세스를 사용하여 문제를 공식화하고, 이중 교육 커리큘럼 학습 전략을 통해 일반화된 강화 학습 에이전트를 훈련합니다.

Result: 강철 외팔보 구조에 대한 사례 연구를 통해 제안된 방법이 주파수 구간에 따라 센서 위치를 최적화하는 데 효과적임을 입증하였습니다.

Conclusion: 제안된 프레임워크는 테스트의 정확성과 적응성을 향상시켜 실험 환경에서의 실제 적용 가능성을 보여줍니다.

Abstract: Modal testing plays a critical role in structural analysis by providing
essential insights into dynamic behaviour across a wide range of engineering
industries. In practice, designing an effective modal test campaign involves
complex experimental planning, comprising a series of interdependent decisions
that significantly influence the final test outcome. Traditional approaches to
test design are typically static-focusing only on global tests without
accounting for evolving test campaign parameters or the impact of such changes
on previously established decisions, such as sensor configurations, which have
been found to significantly influence test outcomes. These rigid methodologies
often compromise test accuracy and adaptability. To address these limitations,
this study introduces an agent-based decision support framework for adaptive
sensor placement across dynamically changing modal test environments. The
framework formulates the problem using an underspecified partially observable
Markov decision process, enabling the training of a generalist reinforcement
learning agent through a dual-curriculum learning strategy. A detailed case
study on a steel cantilever structure demonstrates the efficacy of the proposed
method in optimising sensor locations across frequency segments, validating its
robustness and real-world applicability in experimental settings.

</details>


### [44] [Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs](https://arxiv.org/abs/2508.14140)
*Orestis Konstantaropoulos,Stelios Manolis Smirnakis,Maria Papadopouli*

Main category: cs.LG

TL;DR: G2GNet이라는 새로운 인공지능 신경망 아키텍처는 생물학적 신경망의 구조를 기반으로 하여 효율적인 메모리 사용과 높은 정확성을 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 생물학적 신경 회로의 구조에서 얻은 원칙들은 인공지능 신경망의 설계에 귀중한 통찰을 제공합니다.

Method: G2GNet은 피드포워드 레이어 간에 희소하고 모듈화된 연결성을 부여하는 새로운 아키텍처입니다. 동적 희소 훈련 메커니즘과 활성화 상관관계를 기반으로 한 리와이어링 규칙을 사용합니다.

Result: G2GNet은 Fashion-MNIST, CIFAR-10, CIFAR-100을 포함한 벤치마크에서 최대 75%의 희소성을 유지하면서 정확성을 최대 4.3% 향상시킵니다.

Conclusion: G2GNet은 생물학적으로 관찰된 기능적 연결성 패턴을 ANN 디자인에 구조적 편향으로 첫 번째로 통합한 아키텍처입니다.

Abstract: The structure of biological neural circuits-modular, hierarchical, and
sparsely interconnected-reflects an efficient trade-off between wiring cost,
functional specialization, and robustness. These principles offer valuable
insights for artificial neural network (ANN) design, especially as networks
grow in depth and scale. Sparsity, in particular, has been widely explored for
reducing memory and computation, improving speed, and enhancing generalization.
Motivated by systems neuroscience findings, we explore how patterns of
functional connectivity in the mouse visual cortex-specifically,
ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet,
a novel architecture that imposes sparse, modular connectivity across
feedforward layers. Despite having significantly fewer parameters than fully
connected models, G2GNet achieves superior accuracy on standard vision
benchmarks. To our knowledge, this is the first architecture to incorporate
biologically observed functional connectivity patterns as a structural bias in
ANN design. We complement this static bias with a dynamic sparse training (DST)
mechanism that prunes and regrows edges during training. We also propose a
Hebbian-inspired rewiring rule based on activation correlations, drawing on
principles of biological plasticity. G2GNet achieves up to 75% sparsity while
improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST,
CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer
computations.

</details>


### [45] [Topological Data Analysis for Unsupervised Anomaly Detection and Customer Segmentation on Banking Data](https://arxiv.org/abs/2508.14136)
*Leonardo Aldo Alejandro Barberi,Linda Maria De Cave*

Main category: cs.LG

TL;DR: 이 논문은 은행 데이터에서 비지도 이상 탐지 및 고객 세분화를 위한 고급 위상 데이터 분석 기술을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 은행 데이터에서 숨겨진 의미 있는 패턴을 발견하기 위해 비지도 학습 기법을 사용하는 필요성.

Method: Mapper 알고리즘과 지속적 호몰로지를 활용하여 고객의 은행 데이터를 분석하는 비지도 절차를 개발함.

Result: 이 프레임워크는 산업에 유용한 실제 사례와 결합된 추상적 수학 주제인 위상과의 통찰력을 제공한다.

Conclusion: 위상 정보의 활용을 통해 고객 데이터에서 유의미한 패턴을 밝혀내는 새로운 접근법을 제시했다.

Abstract: This paper introduces advanced techniques of Topological Data Analysis (TDA)
for unsupervised anomaly detection and customer segmentation in banking data.
Using the Mapper algorithm and persistent homology, we develop unsupervised
procedures that uncover meaningful patterns in customers' banking data by
exploiting topological information. The framework we present in this paper
yields actionable insights that combine the abstract mathematical subject of
topology with real-life use cases that are useful in industry.

</details>


### [46] [Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2508.14285)
*Liyi Zhang,Jake Snell,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: LoRA를 활용한 대형 언어 모델의 미세 조정 방법인 ABMLL은 비용 효율적으로 특정 데이터셋의 정보를 통합하며, 좋은 일반화를 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)의 미세 조정이 데이터셋의 정보를 통합하는 비용 효율적인 방법이나, 일반화 성능이 불확실합니다.

Method: 작은 모델을 위한 비아모르타이즈 베이지안 메타-러닝에 기초하여 LoRA에 적합하게 조정한 ABMLL을 제안합니다.

Result: ABMLL은 Unified-QA와 CrossFit 데이터셋에서 기존 방법보다 더 높은 정확성과 개선된 불확실성 정량화를 제공합니다.

Conclusion: ABMLL은 대형 모델에도 효과적으로 일반화를 제공하고, LoRA를 통한 재구성 정확도 및 글로벌 매개변수의 충실도를 균형 있게 조정합니다.

Abstract: Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a
cost-effective way to incorporate information from a specific dataset. However,
it is often unclear how well the fine-tuned LLM will generalize, i.e., how well
it will perform on unseen datasets. Methods have been proposed to improve
generalization by optimizing with in-context prompts, or by using meta-learning
to fine-tune LLMs. However, these methods are expensive in memory and
computation, requiring either long-context prompts or saving copies of
parameters and using second-order gradient updates. To address these
challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This
method builds on amortized Bayesian meta-learning for smaller models, adapting
this approach to LLMs while maintaining its computational efficiency. We
reframe task-specific and global parameters in the context of LoRA and use a
set of new hyperparameters to balance reconstruction accuracy and the fidelity
of task-specific parameters to the global ones. ABMLL provides effective
generalization and scales to large models such as Llama3-8B. Furthermore, as a
result of using a Bayesian framework, ABMLL provides improved uncertainty
quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that
it outperforms existing methods on these benchmarks in terms of both accuracy
and expected calibration error.

</details>


### [47] [Learning to Learn the Macroscopic Fundamental Diagram using Physics-Informed and meta Machine Learning techniques](https://arxiv.org/abs/2508.14137)
*Amalie Roark,Serio Agriesti,Francisco Camara Pereira,Guido Cantelmo*

Main category: cs.LG

TL;DR: 이 논문은 메타 학습을 활용한 프레임워크를 제안하여, 데이터가 부족한 상황에서도 개별 도시의 교통 흐름을 효과적으로 모델링할 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 교통 동역학을 집계된 방식으로 설명하기 위해 Macroscopic Fundamental Diagram(MFD)을 사용하는데, 이를 추정하기 위해 많은 루프 탐지기가 필요하지만 이는 현실적으로 항상 가능하지 않기 때문에 이 연구가 필요하다.

Method: 메타 학습을 적용하여 모델이 다양한 도시에서 수집된 데이터를 이용해 다른 도시의 MFD를 모델링하도록 훈련하고 테스트한다.

Result: 루프 탐지기 집합에 따라 약 17500에서 36000 사이의 평균 MSE 개선이 나타났다.

Conclusion: 메타 학습 프레임워크는 다양한 도시 설정에서 잘 일반화되고 데이터가 제한적인 도시에서도 성능을 향상시키며, 제한된 탐지기가 존재할 때 메타 학습의 활용 가능성을 보여준다.

Abstract: The Macroscopic Fundamental Diagram is a popular tool used to describe
traffic dynamics in an aggregated way, with applications ranging from traffic
control to incident analysis. However, estimating the MFD for a given network
requires large numbers of loop detectors, which is not always available in
practice. This article proposes a framework harnessing meta-learning, a
subcategory of machine learning that trains models to understand and adapt to
new tasks on their own, to alleviate the data scarcity challenge. The developed
model is trained and tested by leveraging data from multiple cities and
exploiting it to model the MFD of other cities with different shares of
detectors and topological structures. The proposed meta-learning framework is
applied to an ad-hoc Multi-Task Physics-Informed Neural Network, specifically
designed to estimate the MFD. Results show an average MSE improvement in flow
prediction ranging between ~ 17500 and 36000 (depending on the subset of loop
detectors tested). The meta-learning framework thus successfully generalizes
across diverse urban settings and improves performance on cities with limited
data, demonstrating the potential of using meta-learning when a limited number
of detectors is available. Finally, the proposed framework is validated against
traditional transfer learning approaches and tested with FitFun, a
non-parametric model from the literature, to prove its transferability.

</details>


### [48] [GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation](https://arxiv.org/abs/2508.14302)
*Amirmohsen Sattarifard,Sepehr Lavasani,Ehsan Imani,Kunlin Zhang,Hanlin Xu,Fengyu Sun,Negar Hassanpour,Chao Gao*

Main category: cs.LG

TL;DR: 본 연구에서는 LLM을 엣지 하드웨어에 배포하기 위한 동적인 가지치기 기법인 A/I-GLASS를 제안하며, 이는 계산량을 줄이면서 품질 저하 없이 FFN 유닛을 동적으로 선택할 수 있는 훈련이 필요 없는 방법이다.


<details>
  <summary>Details</summary>
Motivation: LLM을 엣지 하드웨어에 배포하는 데 필요한 계산량을 줄이면서도 품질을 유지하기 위해 동적인 가지치기 기법이 필요하다.

Method: A/I-GLASS는 활성화와 영향 기반의 글로벌-로컬 신경 중요도 집계를 통해 FFN 유닛을 동적으로 선택하는 두 가지 훈련이 필요 없는 방법이다.

Result: 여러 LLM 및 벤치마크에서 GLASS가 기존의 훈련이 필요 없는 방법보다 현저히 우수한 성능을 보임을 입증하였다.

Conclusion: GLASS는 보조 예측기나 추론 오버헤드를 추가하지 않으면서도 특히 도전적인 긴 형식 생성 시나리오에서 높은 성능을 발휘한다.

Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive,
prompt-aware dynamic pruning to reduce computation without degrading quality.
Static or predictor-based schemes either lock in a single sparsity pattern or
incur extra runtime overhead, and recent zero-shot methods that rely on
statistics from a single prompt fail on short prompt and/or long generation
scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local
neural importance Aggregation for feed-forward network SparSification, two
training-free methods that dynamically select FFN units using a
rank-aggregation of prompt local and model-intrinsic global neuron statistics.
Empirical results across multiple LLMs and benchmarks demonstrate that GLASS
significantly outperforms prior training-free methods, particularly in
challenging long-form generation scenarios, without relying on auxiliary
predictors or adding any inference overhead.

</details>


### [49] [Learning Time-Varying Convexifications of Multiple Fairness Measures](https://arxiv.org/abs/2508.14311)
*Quan Zhou,Jakub Marecek,Robert Shorten*

Main category: cs.LG

TL;DR: 이 연구는 다양한 공정성 측정을 고려하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 공정성을 평가하기 위해 여러 측정을 고려해야 한다는 인식이 증가하고 있다.

Method: 시간이 변하는 다수의 공정성 측정의 볼록화를 학습하는 방법을 제안한다.

Result: 제안된 방법은 제한된 그래프 구조 피드백 하에서 효과적으로 공정성을 학습할 수 있음을 보인다.

Conclusion: 공정성의 여러 측면을 동적으로 고려함으로써 더 나은 결과를 얻을 수 있다.

Abstract: There is an increasing appreciation that one may need to consider multiple
measures of fairness, e.g., considering multiple group and individual fairness
notions. The relative weights of the fairness regularisers are a priori
unknown, may be time varying, and need to be learned on the fly. We consider
the learning of time-varying convexifications of multiple fairness measures
with limited graph-structured feedback.

</details>


### [50] [Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS](https://arxiv.org/abs/2508.14313)
*Can Jin,Yang Zhou,Qixin Zhang,Hongwu Peng,Di Zhang,Marco Pavone,Ligong Han,Zhang-Wei Hong,Tong Che,Dimitris N. Metaxas*

Main category: cs.LG

TL;DR: 이 논문에서는 RL 기반과 검색 기반의 TTS를 통합한 AIRL-S라는 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(TTS)의 테스트 시간 스케일링(TTS)에 대한 기존 접근 방식의 한계를 극복하고자 합니다.

Method: AIRL-S는 역적대 강화 학습(AIRL)과 그룹 상대 정책 최적화(GRPO)를 결합하여 학습된 보상 함수를 사용해 동적 PRM을 생성합니다.

Result: 여덟 개의 벤치마크에서 평균 9% 성능 향상을 보였으며, 특히 GPT-4o와 동등한 성능을 나타냅니다.

Conclusion: 이 연구는 RL의 보상 함수가 검색을 위한 최상의 PRM임을 강조하며, 복잡한 추론 작업에 대한 강력하고 비용 효율적인 솔루션을 제공합니다.

Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen
into two largely separate paradigms: (1) reinforcement learning (RL) methods
that optimize sparse outcome-based rewards, yet suffer from instability and low
sample efficiency; and (2) search-based techniques guided by independently
trained, static process reward models (PRMs), which require expensive human- or
LLM-generated labels and often degrade under distribution shifts. In this
paper, we introduce AIRL-S, the first natural unification of RL-based and
search-based TTS. Central to AIRL-S is the insight that the reward function
learned during RL training inherently represents the ideal PRM for guiding
downstream search. Specifically, we leverage adversarial inverse reinforcement
learning (AIRL) combined with group relative policy optimization (GRPO) to
learn a dense, dynamic PRM directly from correct reasoning traces, entirely
eliminating the need for labeled intermediate process data. At inference, the
resulting PRM simultaneously serves as the critic for RL rollouts and as a
heuristic to effectively guide search procedures, facilitating robust reasoning
chain extension, mitigating reward hacking, and enhancing cross-task
generalization. Experimental results across eight benchmarks, including
mathematics, scientific reasoning, and code generation, demonstrate that our
unified approach improves performance by 9 % on average over the base model,
matching GPT-4o. Furthermore, when integrated into multiple search algorithms,
our PRM consistently outperforms all baseline PRMs trained with labeled data.
These results underscore that, indeed, your reward function for RL is your best
PRM for search, providing a robust and cost-effective solution to complex
reasoning tasks in LLMs.

</details>


### [51] [Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation](https://arxiv.org/abs/2508.14143)
*Xin Li*

Main category: cs.LG

TL;DR: 본 논문은 기억을 통한 추론을 모델링하는 Memory-Amortized Inference(MAI)라는 프레임워크를 제안하며, 이는 구조적 재사용을 통해 인지 시스템을 재구성하는 새로운 접근 방식을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 인지 과정은 균일한 샘플링이나 최적화에서 비롯되지 않고, 이전 추론 경로의 구조적 재사용에서 발생한다는 점이 중요합니다.

Method: MAI 시스템은 메모리 내 잠재적 주기에서의 추론으로서 인지를 모델링합니다.

Result: MAI는 마운트캐슬의 보편 피질 알고리즘에 대한 원칙 있는 기초를 제공하며, 각 피질 기둥을 주기 일관된 메모리 상태에서의 지역적 추론 연산자로 모델링합니다.

Conclusion: MAI는 구조, 재사용, 메모리에 기반한 통합된 생물학적 지능 이론을 제공하며, 인공지능의 계산적 병목 문제를 해결할 수 있는 방향을 제시합니다.

Abstract: Intelligence is fundamentally non-ergodic: it emerges not from uniform
sampling or optimization from scratch, but from the structured reuse of prior
inference trajectories. We introduce Memory-Amortized Inference (MAI) as a
formal framework in which cognition is modeled as inference over latent cycles
in memory, rather than recomputation through gradient descent. MAI systems
encode inductive biases via structural reuse, minimizing entropy and enabling
context-aware, structure-preserving inference. This approach reframes cognitive
systems not as ergodic samplers, but as navigators over constrained latent
manifolds, guided by persistent topological memory. Through the lens of
delta-homology, we show that MAI provides a principled foundation for
Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a
local inference operator over cycle-consistent memory states. Furthermore, we
establish a time-reversal duality between MAI and reinforcement learning:
whereas RL propagates value forward from reward, MAI reconstructs latent causes
backward from memory. This inversion paves a path toward energy-efficient
inference and addresses the computational bottlenecks facing modern AI. MAI
thus offers a unified, biologically grounded theory of intelligence based on
structure, reuse, and memory. We also briefly discuss the profound implications
of MAI for achieving artificial general intelligence (AGI).

</details>


### [52] [A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations](https://arxiv.org/abs/2508.14340)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.LG

TL;DR: 자동화 사이버 작전은 강화 학습을 활용하여 사이버 보안 분야에서 효과적인 결정을 내리도록 에이전트를 교육한다. 그러나 기존의 ACO 응용 프로그램은 에이전트가 처음부터 배우도록 요구하여 느린 수렴과 초기 단계 성능 저하를 초래한다. 본 연구에서는 모의 CybORG 환경에서 네 가지 교사-guided 기법을 구현하고 비교 평가를 수행하였다. 결과적으로 교사 통합이 초기 정책 성능 및 수렴 속도의 측면에서 훈련 효율성을 크게 향상시킬 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 자동화 사이버 작전(ACO)에서 에이전트가 효과적인 결정을 내리도록 교육하는 데 있어 강화 학습(RL)에 의존한다.

Method: 모의 CybORG 환경에서 네 가지 서로 다른 교사-guided 기법을 구현하고 비교 평가를 실시했다.

Result: 교사 통합이 초기 정책 성능 및 수렴 속도의 측면에서 훈련 효율성을 상당히 개선한다는 결과를 도출하였다.

Conclusion: 자동화 사이버 보안을 위한 잠재적인 이점을 강조한다.

Abstract: Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to
train agents to make effective decisions in the cybersecurity domain. However,
existing ACO applications require agents to learn from scratch, leading to slow
convergence and poor early-stage performance. While teacher-guided techniques
have demonstrated promise in other domains, they have not yet been applied to
ACO. In this study, we implement four distinct teacher-guided techniques in the
simulated CybORG environment and conduct a comparative evaluation. Our results
demonstrate that teacher integration can significantly improve training
efficiency in terms of early policy performance and convergence speed,
highlighting its potential benefits for autonomous cybersecurity.

</details>


### [53] [Noise Robust One-Class Intrusion Detection on Dynamic Graphs](https://arxiv.org/abs/2508.14192)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 이 연구는 노이즈가 포함된 데이터 입력에 대한 강건성을 강화하기 위한 새로운 TGN-SVDD 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 네트워크 침입 탐지에서 오염된 및 노이즈가 있는 데이터 입력에 대한 강건성은 여전히 중요한 문제이다.

Method: 이 연구에서는 입력 노이즈가 있는 상황에서 탐지 정확도를 향상시키기 위해 TGN-SVDD 모델의 확률적 버전을 도입하였다.

Result: 수정된 CIC-IDS2017 데이터 세트를 사용한 실험에서, 특히 노이즈 수준이 증가함에 따라 baseline TGN-SVDD 모델에 비해 탐지 성능이 크게 개선되었다.

Conclusion: 우리의 모델은 노이즈 있는 적대적인 상황에서도 자연스럽게 문제를 해결하며 baseline 모델에 비해 강건성을 향상시킨다.

Abstract: In the domain of network intrusion detection, robustness against contaminated
and noisy data inputs remains a critical challenge. This study introduces a
probabilistic version of the Temporal Graph Network Support Vector Data
Description (TGN-SVDD) model, designed to enhance detection accuracy in the
presence of input noise. By predicting parameters of a Gaussian distribution
for each network event, our model is able to naturally address noisy
adversarials and improve robustness compared to a baseline model. Our
experiments on a modified CIC-IDS2017 data set with synthetic noise demonstrate
significant improvements in detection performance compared to the baseline
TGN-SVDD model, especially as noise levels increase.

</details>


### [54] [Reliability comparison of vessel trajectory prediction models via Probability of Detection](https://arxiv.org/abs/2508.14198)
*Zahra Rastin,Kathrin Donandt,Dirk Söffker*

Main category: cs.LG

TL;DR: 본 연구는 다양한 교통 복잡성에서의 선박 궤적 예측 성능을 평가하고, 다양한 딥러닝 기반 접근 방식을 비교한다.


<details>
  <summary>Details</summary>
Motivation: 이전의 선박 궤적 예측 모델은 특정 교통 상황의 복잡성을 간과하고 신뢰성 평가가 부족하다.

Method: 본 연구는 교통 시나리오에 따라 모델 신뢰성을 정량화하기 위해 검출 확률 분석을 사용하여 과거의 오류 분포 분석을 넘어선다.

Result: 모든 모델은 예측 범위에 따라 교통 상황으로 분류된 테스트 샘플에서 평가되며, 각 카테고리에 대한 성능 지표 및 신뢰성 추정치가 얻어진다.

Conclusion: 이 포괄적인 평가의 결과는 다양한 예측 접근 방식의 강점과 약점을 더 깊이 이해하고, 안전한 예측을 보장할 수 있는 예측 범위 길이에 대한 신뢰성과 함께 제공한다. 이 연구 결과는 향후 내수 항로 항해에서 더 신뢰할 수 있는 선박 궤적 예측 접근 방식 개발에 기여할 수 있다.

Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on
the evaluation of different deep learning-based approaches. The objective is to
assess model performance in diverse traffic complexities and compare the
reliability of the approaches. While previous VTP models overlook the specific
traffic situation complexity and lack reliability assessments, this research
uses a probability of detection analysis to quantify model reliability in
varying traffic scenarios, thus going beyond common error distribution
analyses. All models are evaluated on test samples categorized according to
their traffic situation during the prediction horizon, with performance metrics
and reliability estimates obtained for each category. The results of this
comprehensive evaluation provide a deeper understanding of the strengths and
weaknesses of the different prediction approaches, along with their reliability
in terms of the prediction horizon lengths for which safe forecasts can be
guaranteed. These findings can inform the development of more reliable vessel
trajectory prediction approaches, enhancing safety and efficiency in future
inland waterways navigation.

</details>


### [55] [Organ-Agents: Virtual Human Physiology Simulator via LLMs](https://arxiv.org/abs/2508.14357)
*Rihao Chang,He Jiao,Weizhi Nie,Honglin Guo,Keliang Xie,Zhenhua Wu,Lina Zhao,Yunpeng Bai,Yongtao Ma,Lanjun Wang,Yuting Su,Xi Gao,Weijie Wang,Nicu Sebe,Bruno Lepri,Bingwei Sun*

Main category: cs.LG

TL;DR: Organ-Agents는 LLM 기반의 다중 에이전트 프레임워크로, 인체 생리학을 시뮬레이션하며, 높은 정확도로 중환자 치료에서의 다양한 시나리오 분석을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 최근 대규모 언어 모델(LLM)의 발전으로 복잡한 생리학적 시스템을 시뮬레이션할 수 있는 새로운 가능성이 열렸다.

Method: 각 시뮬레이터는 특정 시스템(예: 심혈관, 신장, 면역)을 모델링하며, 시스템 특화된 시계열 데이터로 감독된 미세 조정 후 동적 참조 선택 및 오류 수정을 통한 강화 학습 기반 조정을 수행한다.

Result: Organ-Agents는 4,509명의 독립적 환자들에서 0.16 미만의 MSE로 높은 시뮬레이션 정확도를 달성하였다. 외부 검증에서는 ICU 환자들에서 안정적인 시뮬레이션을 보였고, 15명의 중환자 의사들에 의해 사실성과 생리학적 타당성이 확인되었다.

Conclusion: Organ-Agents는 정밀 진단과 치료 시뮬레이션, 가설 검증을 위한 신뢰할 수 있는 디지털 쌍둥이로 자리 잡는다.

Abstract: Recent advances in large language models (LLMs) have enabled new
possibilities in simulating complex physiological systems. We introduce
Organ-Agents, a multi-agent framework that simulates human physiology via
LLM-driven agents. Each Simulator models a specific system (e.g.,
cardiovascular, renal, immune). Training consists of supervised fine-tuning on
system-specific time-series data, followed by reinforcement-guided coordination
using dynamic reference selection and error correction. We curated data from
7,134 sepsis patients and 7,895 controls, generating high-resolution
trajectories across 9 systems and 125 variables. Organ-Agents achieved high
simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and
robustness across SOFA-based severity strata. External validation on 22,689 ICU
patients from two hospitals showed moderate degradation under distribution
shifts with stable simulation. Organ-Agents faithfully reproduces critical
multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with
coherent timing and phase progression. Evaluation by 15 critical care
physicians confirmed realism and physiological plausibility (mean Likert
ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations
under alternative sepsis treatment strategies, generating trajectories and
APACHE II scores aligned with matched real-world patients. In downstream early
warning tasks, classifiers trained on synthetic data showed minimal AUROC drops
(<0.04), indicating preserved decision-relevant patterns. These results
position Organ-Agents as a credible, interpretable, and generalizable digital
twin for precision diagnosis, treatment simulation, and hypothesis testing in
critical care.

</details>


### [56] [Graph Concept Bottleneck Models](https://arxiv.org/abs/2508.14255)
*Haotian Xu,Tsui-Wei Weng,Lam M. Nguyen,Tengfei Ma*

Main category: cs.LG

TL;DR: GraphCBMs는 개념 관계를 촉진하는 새로운 변종으로, 최종 예측을 조정하는 데 도움을 줍니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 CBM은 개념 간의 관계를 무시하며, 개념들이 상호 연관되어 있다는 것을 간과하고 있습니다.

Method: GraphCBMs는 잠재적 개념 그래프를 구성하여 CBM의 성능을 향상시키는 새로운 변형입니다.

Result: 그래프 기반 CBM은 이미지 분류 작업에서 더 우수하고, 해석 가능성을 위한 더 많은 개념 구조 정보를 제공합니다.

Conclusion: GraphCBM은 다양한 훈련 및 아키텍처 설정에서 성능 저하 없이 강력한 결과를 보여줍니다.

Abstract: Concept Bottleneck Models (CBMs) provide explicit interpretations for deep
neural networks through concepts and allow intervention with concepts to adjust
final predictions. Existing CBMs assume concepts are conditionally independent
given labels and isolated from each other, ignoring the hidden relationships
among concepts. However, the set of concepts in CBMs often has an intrinsic
structure where concepts are generally correlated: changing one concept will
inherently impact its related concepts. To mitigate this limitation, we propose
GraphCBMs: a new variant of CBM that facilitates concept relationships by
constructing latent concept graphs, which can be combined with CBMs to enhance
model performance while retaining their interpretability. Our experiment
results on real-world image classification tasks demonstrate Graph CBMs offer
the following benefits: (1) superior in image classification tasks while
providing more concept structure information for interpretability; (2) able to
utilize latent concept graphs for more effective interventions; and (3) robust
in performance across different training and architecture settings.

</details>


### [57] [Exact Shapley Attributions in Quadratic-time for FANOVA Gaussian Processes](https://arxiv.org/abs/2508.14499)
*Majid Mohammadi,Krikamol Muandet,Ilaria Tiddi,Annette Ten Teije,Siu Lun Chau*

Main category: cs.LG

TL;DR: Shapley 값을 계산하는 것이 고차원에서 힘들지만, FANOVA GP에서는 이 값을 이차 시간 내에 계산할 수 있다.


<details>
  <summary>Details</summary>
Motivation: Shapley 값을 통해 기계 학습에서 입력 특징의 중요성을 정량적으로 평가할 수 있으나, 고차원 특성의 경우 계산이 복잡해진다.

Method: FANOVA GP를 사용하여 주효과와 상호작용을 모델링한 후, 로컬 및 글로벌 설명에 대한 정확한 Shapley 기여를 이차 시간 내에 계산한다.

Result: 로컬 설명은 확률적 협력 게임을 통해 계산하며, 글로벌 설명은 결정론적 분산 기반 가치 함수를 사용해 각 특징의 기여도를 정량화한다.

Conclusion: 이러한 방법은 구조화된 확률 모델의 예측을 위한 불확실성을 인식한 해석을 제공하여 설명 가능한 AI의 유용성을 향상시킨다.

Abstract: Shapley values are widely recognized as a principled method for attributing
importance to input features in machine learning. However, the exact
computation of Shapley values scales exponentially with the number of features,
severely limiting the practical application of this powerful approach. The
challenge is further compounded when the predictive model is probabilistic - as
in Gaussian processes (GPs) - where the outputs are random variables rather
than point estimates, necessitating additional computational effort in modeling
higher-order moments. In this work, we demonstrate that for an important class
of GPs known as FANOVA GP, which explicitly models all main effects and
interactions, *exact* Shapley attributions for both local and global
explanations can be computed in *quadratic time*. For local, instance-wise
explanations, we define a stochastic cooperative game over function components
and compute the exact stochastic Shapley value in quadratic time only,
capturing both the expected contribution and uncertainty. For global
explanations, we introduce a deterministic, variance-based value function and
compute exact Shapley values that quantify each feature's contribution to the
model's overall sensitivity. Our methods leverage a closed-form (stochastic)
M\"{o}bius representation of the FANOVA decomposition and introduce recursive
algorithms, inspired by Newton's identities, to efficiently compute the mean
and variance of Shapley values. Our work enhances the utility of explainable
AI, as demonstrated by empirical studies, by providing more scalable,
axiomatically sound, and uncertainty-aware explanations for predictions
generated by structured probabilistic models.

</details>


### [58] [Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks](https://arxiv.org/abs/2508.14536)
*Saman Yazdannik,Morteza Tayefi,Shamim Sanisales*

Main category: cs.LG

TL;DR: 본 논문에서는 Chebyshev 다항식 기초를 DQN 프레임워크에 통합한 Chebyshev-DQN(Ch-DQN) 아키텍처를 제안하여 강화학습 문제에서 더 효과적인 특성 표현을 생성하고, 이를 통해 더 높은 성능을 달성할 수 있음을 입증한다.


<details>
  <summary>Details</summary>
Motivation: Deep Q-Networks의 성능은 행동 가치 함수를 정확하게 근사할 수 있는 신경망의 능력에 결정적으로 의존한다.

Method: 본 논문에서는 Chebyshev 다항식 기초를 DQN 프레임워크에 통합한 Ch-DQN 아키텍처를 제안한다.

Result: Ch-DQN은 CartPole-v1 벤치마크에서 기준 DQN에 비해 약 39% 향상된 성능을 보였다.

Conclusion: 이 연구는 심층 강화 학습에서 직교 다항식 기초 사용의 잠재력을 검증하고 모델 복잡성의 절충점을 강조한다.

Abstract: The performance of Deep Q-Networks (DQN) is critically dependent on the
ability of its underlying neural network to accurately approximate the
action-value function. Standard function approximators, such as multi-layer
perceptrons, may struggle to efficiently represent the complex value landscapes
inherent in many reinforcement learning problems. This paper introduces a novel
architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev
polynomial basis into the DQN framework to create a more effective feature
representation. By leveraging the powerful function approximation properties of
Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more
efficiently and achieve higher performance. We evaluate our proposed model on
the CartPole-v1 benchmark and compare it against a standard DQN with a
comparable number of parameters. Our results demonstrate that the Ch-DQN with a
moderate polynomial degree (N=4) achieves significantly better asymptotic
performance, outperforming the baseline by approximately 39\%. However, we also
find that the choice of polynomial degree is a critical hyperparameter, as a
high degree (N=8) can be detrimental to learning. This work validates the
potential of using orthogonal polynomial bases in deep reinforcement learning
while also highlighting the trade-offs involved in model complexity.

</details>


### [59] [Adaptively Robust LLM Inference Optimization under Prediction Uncertainty](https://arxiv.org/abs/2508.14544)
*Zixi Chen,Yinyu Ye,Zijie Zhou*

Main category: cs.LG

TL;DR: 대형 언어 모델 추론 스케줄링의 최적화를 통해 총 대기 시간을 최소화하는 내용을 다룬다. 기존의 보수적인 접근법($\mathcal{A}_{\max}$)을 개선한 적응형 알고리즘($\mathcal{A}_{\min}$)을 제안하여 예측된 하한을 기반으로 동적으로 추정치를 조정함으로써 효율성과 견고성을 높인다. 시뮬레이션 결과, 자주 후행 스케줄러와 유사한 성능을 나타낸다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 추론 스케줄링 효율성을 개선하고 전력 소비를 줄이는 것이 필요하다.

Method: 기계 학습을 활용하여 출력 길이를 예측하고, 보수적인 알고리즘($\mathcal{A}_{\max}$)과 적응형 알고리즘($\mathcal{A}_{\min}$)을 제안한다.

Result: 적응형 알고리즘($\mathcal{A}_{\min}$)은 후행 스케줄러와 유사한 성능을 보이면서 로그 스케일 경쟁 비율을 달성한다.

Conclusion: 예측된 하한에만 의존하는 설계 선택은 일반적으로 출력 길이의 상한을 정확하게 예측하기 더 어려운 점에서 효율적이다.

Abstract: We study the problem of optimizing Large Language Model (LLM) inference
scheduling to minimize total latency. LLM inference is an online and multi-task
service process and also heavily energy consuming by which a pre-trained LLM
processes input requests and generates output tokens sequentially. Therefore,
it is vital to improve its scheduling efficiency and reduce the power
consumption while a great amount of prompt requests are arriving. A key
challenge in LLM inference scheduling is that while the prompt length is known
upon arrival, the output length, which critically impacts memory usage and
processing time, is unknown. To address this uncertainty, we propose algorithms
that leverage machine learning to predict output lengths, assuming the
prediction provides an interval classification (min-max range) for each
request.
  We first design a conservative algorithm, $\mathcal{A}_{\max}$, which
schedules requests based on the upper bound of predicted output lengths to
prevent memory overflow. However, this approach is overly conservative: as
prediction accuracy decreases, performance degrades significantly due to
potential overestimation. To overcome this limitation, we propose
$\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted
lower bound as the output length and dynamically refines this estimate during
inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale
competitive ratio. Through numerical simulations, we demonstrate that
$\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler,
highlighting both its efficiency and robustness in practical scenarios.
Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the
prediction interval--an advantageous design choice since upper bounds on output
length are typically more challenging to predict accurately.

</details>


### [60] [ELATE: Evolutionary Language model for Automated Time-series Engineering](https://arxiv.org/abs/2508.14667)
*Andrew Murray,Danial Dervovic,Michael Cashmore*

Main category: cs.LG

TL;DR: ELATE는 진화적 프레임워크 내에서 언어 모델을 활용하여 시계열 데이터의 특성 엔지니어링을 자동화하는 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 특성 엔지니어링은 모델 성능 향상에 필수적이지만 수작업으로 이루어져 시간 소모가 크다.

Method: ELATE는 시계열 통계적 측정 및 특성 중요성 지표를 활용하여 특성을 안내하고 다듬으며, 언어 모델이 새로운 관련 특성 변환을 제안한다.

Result: 다양한 도메인에서 평균 8.4%의 예측 정확도 향상을 달성했다.

Conclusion: 엘라테는 시계열 데이터에 대한 특성 엔지니어링을 자동화하여 예측 정확도를 향상시킨다.

Abstract: Time-series prediction involves forecasting future values using machine
learning models. Feature engineering, whereby existing features are transformed
to make new ones, is critical for enhancing model performance, but is often
manual and time-intensive. Existing automation attempts rely on exhaustive
enumeration, which can be computationally costly and lacks domain-specific
insights. We introduce ELATE (Evolutionary Language model for Automated
Time-series Engineering), which leverages a language model within an
evolutionary framework to automate feature engineering for time-series data.
ELATE employs time-series statistical measures and feature importance metrics
to guide and prune features, while the language model proposes new,
contextually relevant feature transformations. Our experiments demonstrate that
ELATE improves forecasting accuracy by an average of 8.4% across various
domains.

</details>


### [61] [FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models](https://arxiv.org/abs/2508.14315)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark Webb*

Main category: cs.LG

TL;DR: FedRAIN-Lite는 기후 모델의 공간 분해를 활용하여 지역 적합성을 위한 연합 강화 학습을 구현한 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 기후 모델의 서브 그리드 파라미터화는 정적이며 오프라인에서 조정되므로 변화하는 상태에 대한 적응력이 제한된다.

Method: FedRAIN-Lite는 위도 밴드에 에이전트를 할당하여 지역 파라미터 학습과 주기적인 글로벌 집계를 가능하게 하는 연합 강화 학습 프레임워크를 소개한다.

Result: DDPG가 정적 및 단일 에이전트 기준보다 일관되게 더 나은 성능을 발휘하며, 열대 및 중간 위도 지역에서 더 빠른 수렴과 낮은 지역 가중 RMSE를 보인다.

Conclusion: DDPG의 하이퍼파라미터 전이 능력과 낮은 계산 비용은 지리적으로 적응 가능한 파라미터 학습에 적합하다.

Abstract: Sub-grid parameterisations in climate models are traditionally static and
tuned offline, limiting adaptability to evolving states. This work introduces
FedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors
the spatial decomposition used in general circulation models (GCMs) by
assigning agents to latitude bands, enabling local parameter learning with
periodic global aggregation. Using a hierarchy of simplified energy-balance
climate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble
(ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under
different FedRL configurations. Results show that Deep Deterministic Policy
Gradient (DDPG) consistently outperforms both static and single-agent
baselines, with faster convergence and lower area-weighted RMSE in tropical and
mid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to
transfer across hyperparameters and low computational cost make it well-suited
for geographically adaptive parameter learning. This capability offers a
scalable pathway towards high-complexity GCMs and provides a prototype for
physically aligned, online-learning climate models that can evolve with a
changing climate. Code accessible at
https://github.com/p3jitnath/climate-rl-fedrl.

</details>


### [62] [Multi-view Graph Condensation via Tensor Decomposition](https://arxiv.org/abs/2508.14330)
*Nícolas Roque dos Santos,Dawon Ahn,Diego Minatel,Alneu de Andrade Lopes,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 이 논문은 Tensor 분해를 통한 다중 시각 그래프 응축(GCTD) 방법을 제안하여 그래프 크기를 줄이면서 GNN 성능을 유지할 수 있음을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 그래프 신경망(GNN)은 다양한 실제 응용에서 놀라운 성과를 보였으나, 대규모 그래프에서의 훈련은 상당한 계산적 도전 과제를 안고 있다.

Method: 이 논문에서는 Tensor 분해를 활용한 다중 시각 그래프 응축(GCTD) 방법을 제안하며, 이를 통해 효율적인 그래프 응축이 가능한지 탐구한다.

Result: GCTD는 그래프 크기를 효과적으로 줄이면서 GNN 성능을 유지하며, 6개의 실제 데이터 세트 중 3개에서 4.0%의 정확도 향상을 달성하였다.

Conclusion: GCTD는 기존 방법들에 비해 큰 그래프에서도 경쟁력 있는 성능을 보여주며 그래프 응축에 대한 새로운 접근 방식을 제시한다.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable results in various
real-world applications, including drug discovery, object detection, social
media analysis, recommender systems, and text classification. In contrast to
their vast potential, training them on large-scale graphs presents significant
computational challenges due to the resources required for their storage and
processing. Graph Condensation has emerged as a promising solution to reduce
these demands by learning a synthetic compact graph that preserves the
essential information of the original one while maintaining the GNN's
predictive performance. Despite their efficacy, current graph condensation
approaches frequently rely on a computationally intensive bi-level
optimization. Moreover, they fail to maintain a mapping between synthetic and
original nodes, limiting the interpretability of the model's decisions. In this
sense, a wide range of decomposition techniques have been applied to learn
linear or multi-linear functions from graph data, offering a more transparent
and less resource-intensive alternative. However, their applicability to graph
condensation remains unexplored. This paper addresses this gap and proposes a
novel method called Multi-view Graph Condensation via Tensor Decomposition
(GCTD) to investigate to what extent such techniques can synthesize an
informative smaller graph and achieve comparable downstream task performance.
Extensive experiments on six real-world datasets demonstrate that GCTD
effectively reduces graph size while preserving GNN performance, achieving up
to a 4.0\ improvement in accuracy on three out of six datasets and competitive
performance on large graphs compared to existing approaches. Our code is
available at https://anonymous.4open.science/r/gctd-345A.

</details>


### [63] [AFABench: A Generic Framework for Benchmarking Active Feature Acquisition](https://arxiv.org/abs/2508.14734)
*Valter Schütz,Han Wu,Reza Rezvan,Linus Aronsson,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: AFA의 표준화된 벤치마크 프레임워크인 AFABench를 소개하며, 다양한 데이터셋과 정책을 지원하여 AFA 방법들의 공정한 평가를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 인스턴스의 모든 기능을 획득하는 것이 비용, 지연 시간 또는 개인 정보 보호 문제로 인해 비현실적일 수 있다.

Method: AFABench는 다양한 합성 및 실제 데이터 세트를 포함하고 폭넓은 획득 정책을 지원하며, 새로운 방법과 작업의 통합을 용이하게 하는 모듈식 디자인을 제공한다.

Result: 다양한 AFA 전략 간의 주요 트레이드오프를 강조하고 향후 연구를 위한 실질적인 통찰력을 제공한다.

Conclusion: AFABench는 AFA 방법들의 공정하고 체계적인 평가를 위한 첫 번째 벤치마크 프레임워크로, 모든 주요 카테고리의 알고리즘을 구현하고 평가하였다.

Abstract: In many real-world scenarios, acquiring all features of a data instance can
be expensive or impractical due to monetary cost, latency, or privacy concerns.
Active Feature Acquisition (AFA) addresses this challenge by dynamically
selecting a subset of informative features for each data instance, trading
predictive performance against acquisition cost. While numerous methods have
been proposed for AFA, ranging from greedy information-theoretic strategies to
non-myopic reinforcement learning approaches, fair and systematic evaluation of
these methods has been hindered by the lack of standardized benchmarks. In this
paper, we introduce AFABench, the first benchmark framework for AFA. Our
benchmark includes a diverse set of synthetic and real-world datasets, supports
a wide range of acquisition policies, and provides a modular design that
enables easy integration of new methods and tasks. We implement and evaluate
representative algorithms from all major categories, including static, greedy,
and reinforcement learning-based approaches. To test the lookahead capabilities
of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed
to expose the limitations of greedy selection. Our results highlight key
trade-offs between different AFA strategies and provide actionable insights for
future research. The benchmark code is available at:
https://github.com/Linusaronsson/AFA-Benchmark.

</details>


### [64] [NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation](https://arxiv.org/abs/2508.14336)
*Xu Weng,K. V. Ling,Haochen Liu,Bingheng Wang,Kun Cao*

Main category: cs.LG

TL;DR: 이 논문은 도시 환경에서의 GNSS 로컬라이제이션의 정확도를 향상시키기 위해 데이터 기반 방법론을 사용하는 새로운 신경망 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 도시 환경에서의 GNSS 로컬라이제이션의 어려움은 위성 신호의 복잡한 전파와 저품질 GNSS 하드웨어로 인한 거리 측정 오차 때문입니다.

Method: 견고한 신경 거리 수정(NeRC) 프레임워크를 제시하며, 이에 대한 교육 목표로 위치 관련 지표를 사용한다. 거리 측정 오류 대신, 비교적 쉽게 얻을 수 있는 지상 진리 위치를 활용하여 신경망을 교육한다.

Result: 제안된 NeRC는 공공 벤치마크 및 수집된 데이터 세트에서 테스트되었으며, 위치 정확도의 뛰어난 개선을 보여주었다.

Conclusion: NeRC를 모바일 장치의 실시간 성능 검증을 위해 엣지에서 배포하였다.

Abstract: GNSS localization using everyday mobile devices is challenging in urban
environments, as ranging errors caused by the complex propagation of satellite
signals and low-quality onboard GNSS hardware are blamed for undermining
positioning accuracy. Researchers have pinned their hopes on data-driven
methods to regress such ranging errors from raw measurements. However, the
grueling annotation of ranging errors impedes their pace. This paper presents a
robust end-to-end Neural Ranging Correction (NeRC) framework, where
localization-related metrics serve as the task objective for training the
neural modules. Instead of seeking impractical ranging error labels, we train
the neural network using ground-truth locations that are relatively easy to
obtain. This functionality is supported by differentiable moving horizon
location estimation (MHE) that handles a horizon of measurements for
positioning and backpropagates the gradients for training. Even better, as a
blessing of end-to-end learning, we propose a new training paradigm using
Euclidean Distance Field (EDF) cost maps, which alleviates the demands on
labeled locations. We evaluate the proposed NeRC on public benchmarks and our
collected datasets, demonstrating its distinguished improvement in positioning
accuracy. We also deploy NeRC on the edge to verify its real-time performance
for mobile devices.

</details>


### [65] [Cross-Modality Controlled Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2508.14748)
*Yunzhe Zhang,Yifei Wang,Khanh Vinh Nguyen,Pengyu Hong*

Main category: cs.LG

TL;DR: 본 논문에서는 다중 제약을 지원하는 새로운 분자 생성 모델인 CMCM-DLM을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 SMILES 기반 확산 모델은 단일 모드 제약만을 지원하며, 제약이 변경될 때마다 새로 모델을 학습해야 하는 불편함이 있다. 그러나 실제 응용에서는 다양한 모드의 여러 제약이 필요하기 때문에 이를 해결할 새로운 접근이 필요하다.

Method: Cross-Modality Controlled Molecule Generation with Diffusion Language Model (CMCM-DLM)을 제안하며, 이는 사전 학습된 확산 모델을 기반으로 두 개의 학습 가능한 모듈인 구조 제어 모듈(SCM)과 특성 제어 모듈(PCM)을 통합하여 작동한다.

Result: 여러 데이터세트에서 실험 결과, 우리의 접근법이 효율적이고 적응 가능함을 보여주며, CMCM-DLM이 약물 발견을 위한 분자 생성에서 중요한 진전을 이루었음을 강조한다.

Conclusion: CMCM-DLM은 제약 변경 없이 모드 간 제약을 지원하는 분자 생성 모델의 새로운 가능성을 열어준다.

Abstract: Current SMILES-based diffusion models for molecule generation typically
support only unimodal constraint. They inject conditioning signals at the start
of the training process and require retraining a new model from scratch
whenever the constraint changes. However, real-world applications often involve
multiple constraints across different modalities, and additional constraints
may emerge over the course of a study. This raises a challenge: how to extend a
pre-trained diffusion model not only to support cross-modality constraints but
also to incorporate new ones without retraining. To tackle this problem, we
propose the Cross-Modality Controlled Molecule Generation with Diffusion
Language Model (CMCM-DLM), demonstrated by two distinct cross modalities:
molecular structure and chemical properties. Our approach builds upon a
pre-trained diffusion model, incorporating two trainable modules, the Structure
Control Module (SCM) and the Property Control Module (PCM), and operates in two
distinct phases during the generation process. In Phase I, we employs the SCM
to inject structural constraints during the early diffusion steps, effectively
anchoring the molecular backbone. Phase II builds on this by further
introducing PCM to guide the later stages of inference to refine the generated
molecules, ensuring their chemical properties match the specified targets.
Experimental results on multiple datasets demonstrate the efficiency and
adaptability of our approach, highlighting CMCM-DLM's significant advancement
in molecular generation for drug discovery applications.

</details>


### [66] [On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks](https://arxiv.org/abs/2508.14338)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 이 논문은 그래프 신경망(GNNs)에서 학습 알고리즘과 그래프 구조 간의 상호작용을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 기존 GNN의 학습 동역학에 대한 이론적 연구는 주로 보간 체계에서의 학습 알고리즘의 수렴 속도에 초점을 맞추고 있으며, 이러한 동역학과 실제 그래프 구조 간의 연결은 단순하게만 나타난다.

Method: GNN의 일반화 체계 내에서 학습 알고리즘의 과도한 위험(일반화 성능)을 조사하며, SGD와 Ridge 회귀와 같은 학습 알고리즘의 성능에 그래프 구조가 미치는 영향을 분석한다.

Result: SGD와 Ridge 회귀의 과도한 위험 프로파일을 유도하고, 스펙트럴 그래프 이론을 통해 이러한 프로파일을 그래프 구조와 연결한다. 다양한 그래프 구조가 이러한 알고리즘의 성능에 미치는 영향을 비교 분석을 통해 탐구한다.

Conclusion: 그래프 구조, GNN 및 학습 알고리즘 간의 상관관계를 보여주며, GNN 알고리즘의 설계 및 선택에 대한 통찰을 제공한다.

Abstract: This paper studies the interplay between learning algorithms and graph
structure for graph neural networks (GNNs). Existing theoretical studies on the
learning dynamics of GNNs primarily focus on the convergence rates of learning
algorithms under the interpolation regime (noise-free) and offer only a crude
connection between these dynamics and the actual graph structure (e.g., maximum
degree). This paper aims to bridge this gap by investigating the excessive risk
(generalization performance) of learning algorithms in GNNs within the
generalization regime (with noise). Specifically, we extend the conventional
settings from the learning theory literature to the context of GNNs and examine
how graph structure influences the performance of learning algorithms such as
stochastic gradient descent (SGD) and Ridge regression. Our study makes several
key contributions toward understanding the interplay between graph structure
and learning in GNNs. First, we derive the excess risk profiles of SGD and
Ridge regression in GNNs and connect these profiles to the graph structure
through spectral graph theory. With this established framework, we further
explore how different graph structures (regular vs. power-law) impact the
performance of these algorithms through comparative analysis. Additionally, we
extend our analysis to multi-layer linear GNNs, revealing an increasing
non-isotropic effect on the excess risk profile, thereby offering new insights
into the over-smoothing issue in GNNs from the perspective of learning
algorithms. Our empirical results align with our theoretical predictions,
\emph{collectively showcasing a coupling relation among graph structure, GNNs
and learning algorithms, and providing insights on GNN algorithm design and
selection in practice.}

</details>


### [67] [PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning](https://arxiv.org/abs/2508.14765)
*Ruheng Wang,Hang Zhang,Trieu Nguyen,Shasha Feng,Hao-Wei Pang,Xiang Yu,Li Xiao,Peter Zhiping Zhang*

Main category: cs.LG

TL;DR: PepThink-R1은 약리학적 특성을 최적화하기 위한 새로운 펩타이드 디자인 프레임워크로, 대형 언어 모델과 체인 오브 사고 감독 세부 조정 및 강화 학습을 통합한다.


<details>
  <summary>Details</summary>
Motivation: 치료용 펩타이드의 맞춤형 특성을 설계하는 것은 시퀀스 공간의 방대함과 제한된 실험 데이터, 현재 생성 모델의 해석 가능성 저하로 인해 어려움이 있다.

Method: PepThink-R1은 시퀀스 생성 중 모노머 수준의 수정을 명시적으로 추론하고 여러 약리학적 특성을 최적화하는 기능을 제공하는 생성 프레임워크이다.

Result: PepThink-R1은 기존의 일반 LLM(예: GPT-5) 및 도메인별 기준 모델보다 lipophilicity, stability 및 exposure가 현저히 향상된 고리형 펩타이드를 생성하였다.

Conclusion: 이 연구는 강화 학습 기반 특성 제어를 결합한 최초의 LLM 기반 펩타이드 디자인 프레임워크로, 치료 발견을 위한 신뢰할 수 있고 투명한 펩타이드 최적화를 위한 중요한 발걸음을 내디뎠다.

Abstract: Designing therapeutic peptides with tailored properties is hindered by the
vastness of sequence space, limited experimental data, and poor
interpretability of current generative models. To address these challenges, we
introduce PepThink-R1, a generative framework that integrates large language
models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and
reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly
reasons about monomer-level modifications during sequence generation, enabling
interpretable design choices while optimizing for multiple pharmacological
properties. Guided by a tailored reward function balancing chemical validity
and property improvements, the model autonomously explores diverse sequence
variants. We demonstrate that PepThink-R1 generates cyclic peptides with
significantly enhanced lipophilicity, stability, and exposure, outperforming
existing general LLMs (e.g., GPT-5) and domain-specific baseline in both
optimization success and interpretability. To our knowledge, this is the first
LLM-based peptide design framework that combines explicit reasoning with
RL-driven property control, marking a step toward reliable and transparent
peptide optimization for therapeutic discovery.

</details>


### [68] [Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning](https://arxiv.org/abs/2508.14859)
*Jiafeng Xiong,Rizos Sakellariou*

Main category: cs.LG

TL;DR: GTGIB는 그래프 구조 학습과 시간 그래프 정보 병목 현상을 통합하여 동적 네트워크에서 새로운 노드를 효과적으로 표현하고 노이즈 또는 중복 그래프 정보를 완화하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 동적 네트워크에서 새로운 노드와 엣지가 시간에 따라 진화하기 때문에, 효과적으로 보이지 않는 노드를 표현하고 노이즈나 중복 정보를 줄이는 것이 필요하다.

Method: GTGIB는 그래프 구조 학습(GSL)과 시간 그래프 정보 병목 현상(TGIB)을 통합한 다목적 프레임워크로, 두 단계의 GSL 기반 구조 향상기를 설계하여 노드 이웃을 풍부하게 하고 최적화한다.

Result: GTGIB 기반 모델은 네 가지 실제 데이터 세트에서 링크 예측을 평가하며, 모든 데이터 세트에서 기존 방법을 초월하여 유의미하고 일관된 향상을 보여준다.

Conclusion: GTGIB는 시간 그래프에서 정보를 최적화하여 안정적이고 효율적인 최적화를 가능하게 한다.

Abstract: Temporal graph learning is crucial for dynamic networks where nodes and edges
evolve over time and new nodes continuously join the system. Inductive
representation learning in such settings faces two major challenges:
effectively representing unseen nodes and mitigating noisy or redundant graph
information. We propose GTGIB, a versatile framework that integrates Graph
Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We
design a novel two-step GSL-based structural enhancer to enrich and optimize
node neighborhoods and demonstrate its effectiveness and efficiency through
theoretical proofs and experiments. The TGIB refines the optimized graph by
extending the information bottleneck principle to temporal graphs, regularizing
both edges and features based on our derived tractable TGIB objective function
via variational approximation, enabling stable and efficient optimization.
GTGIB-based models are evaluated to predict links on four real-world datasets;
they outperform existing methods in all datasets under the inductive setting,
with significant and consistent improvement in the transductive setting.

</details>


### [69] [A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations](https://arxiv.org/abs/2508.14351)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 본 연구는 스코어 기반 그래프 생성 모델(SGGMs)의 비대칭 수렴 분석을 제시하며, 그래프 생성 패러다임에 따라 수렴 경계를 분석합니다.


<details>
  <summary>Details</summary>
Motivation: SGGMs의 이론적 행동, 특히 수렴에 대한 연구가 부족하여 이를 해결하고자 함.

Method: SGGMs의 수렴 경계를 세 가지 그래프 생성 패러다임을 통해 분석하고, 하이퍼 파라미터 선택에 대한 이론적 통찰을 제공함.

Result: SGGMs에 특정한 고유한 요소들이 수렴 경계에 영향을 미친다는 것을 발견하고, 샘플링 단계와 확산 길이 같은 하이퍼 파라미터 기술을 통하여 수렴을 개선할 수 있는 방법을 제시함.

Conclusion: SGGMs의 이론적 이해를 심화시키고, 실제 모델 설계를 위한 실용적인 지침을 제공함으로써 중요한 분야에서의 적용성을 입증함.

Abstract: Score-based graph generative models (SGGMs) have proven effective in critical
applications such as drug discovery and protein synthesis. However, their
theoretical behavior, particularly regarding convergence, remains
underexplored. Unlike common score-based generative models (SGMs), which are
governed by a single stochastic differential equation (SDE), SGGMs involve a
system of coupled SDEs. In SGGMs, the graph structure and node features are
governed by separate but interdependent SDEs. This distinction makes existing
convergence analyses from SGMs inapplicable for SGGMs. In this work, we present
the first non-asymptotic convergence analysis for SGGMs, focusing on the
convergence bound (the risk of generative error) across three key graph
generation paradigms: (1) feature generation with a fixed graph structure, (2)
graph structure generation with fixed node features, and (3) joint generation
of both graph structure and node features. Our analysis reveals several unique
factors specific to SGGMs (e.g., the topological properties of the graph
structure) which affect the convergence bound. Additionally, we offer
theoretical insights into the selection of hyperparameters (e.g., sampling
steps and diffusion length) and advocate for techniques like normalization to
improve convergence. To validate our theoretical findings, we conduct a
controlled empirical study using synthetic graph models, and the results align
with our theoretical predictions. This work deepens the theoretical
understanding of SGGMs, demonstrates their applicability in critical domains,
and provides practical guidance for designing effective models.

</details>


### [70] [SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion](https://arxiv.org/abs/2508.14352)
*Junwei Su,Shan Wu*

Main category: cs.LG

TL;DR: 그래프 확산 생성 모델(GDGMs)은 고품질 그래프 생성을 위한 강력한 도구로 떠올랐다. 하지만 스케일 확장성과 크기 일반화 문제로 더 널리 채택되는 데 도전이 있다.


<details>
  <summary>Details</summary>
Motivation: GDGMs는 대규모 실세계 그래프에 대한 가능성을 제한하는 메모리 요구 사항으로 인해 큰 그래프에 대한 스케일링에 어려움을 겪고 있으며, 훈련 데이터와 다른 크기의 그래프를 생성하는 데 제한적인 능력을 보여준다.

Method: 우리는 그래프 표현을 블록 그래프 공간으로 정제하는 확률적 블록 그래프 확산(SBGD) 모델을 제안한다.

Result: 실험 결과, SBGD는 메모리 개선을 기록하며(최대 6배) 최신 방법과 비교해 동등하거나 우수한 그래프 생성 성능을 유지한다.

Conclusion: SBGD는 확장 가능하고 효과적인 GDGM을 넘어 생성 모델링의 모듈화 원칙을 보여주며, 복잡한 작업을 보다 관리하기 쉬운 구성 요소로 분해하여 생성 모델을 탐구할 수 있는 새로운 경로를 제공한다.

Abstract: Graph diffusion generative models (GDGMs) have emerged as powerful tools for
generating high-quality graphs. However, their broader adoption faces
challenges in \emph{scalability and size generalization}. GDGMs struggle to
scale to large graphs due to their high memory requirements, as they typically
operate in the full graph space, requiring the entire graph to be stored in
memory during training and inference. This constraint limits their feasibility
for large-scale real-world graphs. GDGMs also exhibit poor size generalization,
with limited ability to generate graphs of sizes different from those in the
training data, restricting their adaptability across diverse applications. To
address these challenges, we propose the stochastic block graph diffusion
(SBGD) model, which refines graph representations into a block graph space.
This space incorporates structural priors based on real-world graph patterns,
significantly reducing memory complexity and enabling scalability to large
graphs. The block representation also improves size generalization by capturing
fundamental graph structures. Empirical results show that SBGD achieves
significant memory improvements (up to 6$\times$) while maintaining comparable
or even superior graph generation performance relative to state-of-the-art
methods. Furthermore, experiments demonstrate that SBGD better generalizes to
unseen graph sizes. The significance of SBGD extends beyond being a scalable
and effective GDGM; it also exemplifies the principle of modularization in
generative modeling, offering a new avenue for exploring generative models by
decomposing complex tasks into more manageable components.

</details>


### [71] [Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states](https://arxiv.org/abs/2508.14413)
*Samarth Gupta,Raghudeep Gadde,Rui Chen,Aleix M. Martinez*

Main category: cs.LG

TL;DR: 이 논문은 확산 모델의 기본 가정인 더 많은 잠재 상태가 필요하다는 것을 도전하여, 적은 수의 잠재 상태로 훈련해도 성능이 우수하다는 것을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 확산 모델이 많은 잠재 상태를 요구한다는 가정에 도전하고자 했습니다.

Method: 소수의 잠재 상태를 사용하고, 적절한 노이즈 스케줄을 선택하여 훈련했습니다.

Result: 작은 잠재 상태 수로 훈련한 모델이 많은 잠재 상태로 훈련한 모델과 비슷한 성능을 발휘함을 보여주었습니다.

Conclusion: 제안된 분리된 모델은 두 개의 데이터 세트에서 다양한 메트릭을 기준으로 4-6배 더 빠르게 수렴함을 입증합니다.

Abstract: We challenge a fundamental assumption of diffusion models, namely, that a
large number of latent-states or time-steps is required for training so that
the reverse generative process is close to a Gaussian. We first show that with
careful selection of a noise schedule, diffusion models trained over a small
number of latent states (i.e. $T \sim 32$) match the performance of models
trained over a much large number of latent states ($T \sim 1,000$). Second, we
push this limit (on the minimum number of latent states required) to a single
latent-state, which we refer to as complete disentanglement in T-space. We show
that high quality samples can be easily generated by the disentangled model
obtained by combining several independently trained single latent-state models.
We provide extensive experiments to show that the proposed disentangled model
provides 4-6$\times$ faster convergence measured across a variety of metrics on
two different datasets.

</details>


### [72] [Personalized Counterfactual Framework: Generating Potential Outcomes from Wearable Data](https://arxiv.org/abs/2508.14432)
*Ajan Subramanian,Amir M. Rahmani*

Main category: cs.LG

TL;DR: 웨어러블 센서 데이터는 개인화된 건강 모니터링에 기회를 제공하지만 복잡한 데이터 스트림으로부터 실행 가능한 통찰력을 도출하는 것은 어렵다. 본 논문은 다변량 웨어러블 데이터에서 개인화된 반사실 모델을 학습하기 위한 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 웨어러블 센서 데이터는 개인의 건강 상태를 세밀하게 모니터링할 수 있는 잠재력을 지니고 있지만, 이를 효과적으로 활용하여 개별적인 행동 변화에 대한 통찰력을 도출하기 위해서는 대량의 복잡한 데이터에서 의미 있는 정보를 추출하는 것이 필요하다.

Method: 개인 데이터셋을 비슷한 환자의 데이터로 보강하고, 이후 시계열 PC 알고리즘을 적용하여 예측 관계를 발견한 후, 분류된 관계를 바탕으로 그라디언트 부스팅 머신을 학습시켜 개인별 효과를 정량화한다.

Result: 프레임워크의 평가 결과 평균 심박수 MAE 4.71 bpm의 예측 정확도와 중앙값 0.9643의 높은 반사실성 플라우스빌리티를 보였다.

Conclusion: 이러한 개입은 가상의 생활습관 변화에 대한 반응에서 개인 간의 큰 변동성을 강조하여 프레임워크의 개인화된 통찰력을 위한 잠재력을 보여준다.

Abstract: Wearable sensor data offer opportunities for personalized health monitoring,
yet deriving actionable insights from their complex, longitudinal data streams
is challenging. This paper introduces a framework to learn personalized
counterfactual models from multivariate wearable data. This enables exploring
what-if scenarios to understand potential individual-specific outcomes of
lifestyle choices. Our approach first augments individual datasets with data
from similar patients via multi-modal similarity analysis. We then use a
temporal PC (Peter-Clark) algorithm adaptation to discover predictive
relationships, modeling how variables at time t-1 influence physiological
changes at time t. Gradient Boosting Machines are trained on these discovered
relationships to quantify individual-specific effects. These models drive a
counterfactual engine projecting physiological trajectories under hypothetical
interventions (e.g., activity or sleep changes). We evaluate the framework via
one-step-ahead predictive validation and by assessing the plausibility and
impact of interventions. Evaluation showed reasonable predictive accuracy
(e.g., mean heart rate MAE 4.71 bpm) and high counterfactual plausibility
(median 0.9643). Crucially, these interventions highlighted significant
inter-individual variability in response to hypothetical lifestyle changes,
showing the framework's potential for personalized insights. This work provides
a tool to explore personalized health dynamics and generate hypotheses on
individual responses to lifestyle changes.

</details>


### [73] [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization](https://arxiv.org/abs/2508.14460)
*Shuaijie She,Yu Bao,Yu Lu,Lu Xu,Tao Li,Wenhao Zhu,Shujian Huang,Shanbo Cheng,Lu Lu,Yuxuan Wang*

Main category: cs.LG

TL;DR: DuPO는 주석 없는 피드백을 생성하는 이중 학습 기반의 선호 최적화 프레임워크로, 비싼 라벨에 의존하고 검증 가능 작업으로 제한된 기존 기법들의 한계를 극복한다.


<details>
  <summary>Details</summary>
Motivation: DuPO는 비싼 라벨에 의존하는 RLVR과 엄격한 이중 작업 쌍에 제한된 전통적인 이중 학습의 두 가지 주요 한계를 해결하기 위해 개발되었다.

Method: DuPO는 기본 작업의 입력을 알려진 요소와 알려지지 않은 요소로 분해한 후, 기본 출력을 사용하여 알려진 정보를 통해 알려지지 않은 부분을 재구성하는 이중 작업을 구축한다.

Result: DuPO는 756 방향에서 평균 번역 품질을 2.13 COMET 향상시키고, 세 가지 도전 기준에서 수학적 추론 정확성을 평균 6.4점 증가시켰으며, 추론 시 재랭커로서 성능을 9.3점 향상시킨다.

Conclusion: 이러한 결과는 DuPO가 LLM 최적화를 위한 확장 가능하고 일반적이며 주석 없는 패러다임으로 자리매김하게 한다.

Abstract: We present DuPO, a dual learning-based preference optimization framework that
generates annotation-free feedback via a generalized duality. DuPO addresses
two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s
reliance on costly labels and applicability restricted to verifiable tasks, and
traditional dual learning's restriction to strictly dual task pairs (e.g.,
translation and back-translation). Specifically, DuPO decomposes a primal
task's input into known and unknown components, then constructs its dual task
to reconstruct the unknown part using the primal output and known information
(e.g., reversing math solutions to recover hidden variables), broadening
applicability to non-invertible tasks. The quality of this reconstruction
serves as a self-supervised reward to optimize the primal task, synergizing
with LLMs' ability to instantiate both tasks via a single model. Empirically,
DuPO achieves substantial gains across diverse tasks: it enhances the average
translation quality by 2.13 COMET over 756 directions, boosts the mathematical
reasoning accuracy by an average of 6.4 points on three challenge benchmarks,
and enhances performance by 9.3 points as an inference-time reranker (trading
computation for accuracy). These results position DuPO as a scalable, general,
and annotation-free paradigm for LLM optimization.

</details>


### [74] [Fast Symbolic Regression Benchmarking](https://arxiv.org/abs/2508.14481)
*Viktor Martinek*

Main category: cs.LG

TL;DR: 표상 회귀(SR)은 데이터에서 수학적 모델을 발견하는 방법이다. 본 논문은 기존 벤치마크의 한계를 보완하기 위한 새로운 벤치마크 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 진실 재발견 벤치마크는 단일 표현형식의 복원에 과도하게 중점을 두거나 성공을 평가하기 위해 컴퓨터 대수 시스템에만 의존한다.

Method: 허용 가능한 표현의 신중하게 선별된 리스트와 조기 종료를 위한 콜백 메커니즘을 도입하여 문제를 개선.

Result: 새로운 벤치마킹 방법은 SymbolicRegression.jl의 재발견 비율을 26.7%에서 44.7%로 증가시켰고, 벤치마크 수행에 필요한 연산 비용을 41.2% 줄였다.

Conclusion: TiSR의 재발견 비율은 69.4%이며, 벤치마크 수행 시 63%의 시간을 절약할 수 있다.

Abstract: Symbolic regression (SR) uncovers mathematical models from data. Several
benchmarks have been proposed to compare the performance of SR algorithms.
However, existing ground-truth rediscovery benchmarks overemphasize the
recovery of "the one" expression form or rely solely on computer algebra
systems (such as SymPy) to assess success. Furthermore, existing benchmarks
continue the expression search even after its discovery. We improve upon these
issues by introducing curated lists of acceptable expressions, and a callback
mechanism for early termination. As a starting point, we use the symbolic
regression for scientific discovery (SRSD) benchmark problems proposed by
Yoshitomo et al., and benchmark the two SR packages SymbolicRegression.jl and
TiSR. The new benchmarking method increases the rediscovery rate of
SymbolicRegression.jl from 26.7%, as reported by Yoshitomo et at., to 44.7%.
Performing the benchmark takes 41.2% less computational expense. TiSR's
rediscovery rate is 69.4%, while performing the benchmark saves 63% time.

</details>


### [75] [On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines](https://arxiv.org/abs/2508.14482)
*Alexander Geiger,Lars Wagner,Daniel Rueckert,Dirk Wilhelm,Alissa Jell*

Main category: cs.LG

TL;DR: 딥러닝 모델의 설명 가능성은 의료 분야에서 중요한 도전 과제이다. 기존 방법들은 의미 있는 기준 입력 선택에 접근하는 데 한계를 가진다. 본 연구에서는 기준 선택에 대한 결핍 개념을 탐구하고, 의료 데이터에서의 의미 있는 특성 결핍을 더 잘 표현하는 반사실 근거 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 의료 분야에서 딥러닝 모델의 해석 가능성이 임상 신뢰 구축 및 투명성 확보에 필수적이다.

Method: 입력에 맞춰 동적으로 선택된 반사실 기반선을 생성하기 위해 변분 자동 인코더를 사용하며, 제안한 방법은 기존의 어떤 적합한 반사실 방법과도 적용 가능하다.

Result: 상세한 세 가지 의료 데이터 세트에서 평가하여, 반사실 기반선이 표준 기초 선택보다 더 신뢰할 수 있고 의료적으로 관련성이 높은 기여도를 산출함을 경험적으로 입증하였다.

Conclusion: 반사실적으로 정상적이지만 입력에 가까운 상황을 표현하는 방법이 의료 데이터의 특성 결핍을 더 정확하게 나타낸다고 주장한다.

Abstract: The explainability of deep learning models remains a significant challenge,
particularly in the medical domain where interpretable outputs are critical for
clinical trust and transparency. Path attribution methods such as Integrated
Gradients rely on a baseline input representing the absence of relevant
features ("missingness"). Commonly used baselines, such as all-zero inputs, are
often semantically meaningless, especially in medical contexts where
missingness can itself be informative. While alternative baseline choices have
been explored, existing methods lack a principled approach to dynamically
select baselines tailored to each input. In this work, we examine the notion of
missingness in the medical setting, analyze its implications for baseline
selection, and introduce a counterfactual-guided approach to address the
limitations of conventional baselines. We argue that a clinically normal but
input-close counterfactual represents a more accurate representation of a
meaningful absence of features in medical data. To implement this, we use a
Variational Autoencoder to generate counterfactual baselines, though our
concept is generative-model-agnostic and can be applied with any suitable
counterfactual method. We evaluate the approach on three distinct medical data
sets and empirically demonstrate that counterfactual baselines yield more
faithful and medically relevant attributions compared to standard baseline
choices.

</details>


### [76] [Semantic Energy: Detecting LLM Hallucination Beyond Entropy](https://arxiv.org/abs/2508.14496)
*Huan Ma,Jiadong Pan,Jing Liu,Yan Chen,Joey Tianyi Zhou,Guangyu Wang,Qinghua Hu,Hua Wu,Changqing Zhang,Haifeng Wang*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델(LLM)에서 발생하는 허위 응답을 감지하기 위해 새로운 불확실성 추정 방법인 시맨틱 에너지를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 실제 애플리케이션에서 점점 더 사용되고 있지만, 이들은 유창하지만 잘못된 응답을 생성하는 허위 응답에 취약합니다. 이러한 허위 응답은 잘못된 의사결정으로 이어질 수 있습니다.

Method: 시맨틱 에너지는 LLM의 본질적인 신뢰성을 활용하여 최종 레이어의 로짓에서 직접 작동하는 새로운 불확실성 추정 프레임워크입니다. 이는 볼츠만 영감을 받은 에너지 분포와 시맨틱 클러스터링을 결합하여 불확실성을 더 잘 포착합니다.

Result: 실험 결과, 시맨틱 에너지가 허위 응답 감지와 불확실성 추정을 크게 개선하여 하류 응용 프로그램에 더 신뢰할 수 있는 신호를 제공합니다.

Conclusion: 시맨틱 에너지는 불확실성 추정 면에서 기존 방법보다 우수한 성능을 보여주며, 허위 응답 탐지와 같은 다양한 하류 작업에 유용합니다.

Abstract: Large Language Models (LLMs) are being increasingly deployed in real-world
applications, but they remain susceptible to hallucinations, which produce
fluent yet incorrect responses and lead to erroneous decision-making.
Uncertainty estimation is a feasible approach to detect such hallucinations.
For example, semantic entropy estimates uncertainty by considering the semantic
diversity across multiple sampled responses, thus identifying hallucinations.
However, semantic entropy relies on post-softmax probabilities and fails to
capture the model's inherent uncertainty, causing it to be ineffective in
certain scenarios. To address this issue, we introduce Semantic Energy, a novel
uncertainty estimation framework that leverages the inherent confidence of LLMs
by operating directly on logits of penultimate layer. By combining semantic
clustering with a Boltzmann-inspired energy distribution, our method better
captures uncertainty in cases where semantic entropy fails. Experiments across
multiple benchmarks show that Semantic Energy significantly improves
hallucination detection and uncertainty estimation, offering more reliable
signals for downstream applications such as hallucination detection.

</details>


### [77] [Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2508.14503)
*Lian Lian,Yilin Li,Song Han,Renzi Meng,Sibo Wang,Ming Wang*

Main category: cs.LG

TL;DR: 본 연구는 클라우드 서비스 환경에서의 한계점을 해결하기 위해 통합된 다중 스케일 특성 인식을 기반으로 한 Transformer 아키텍처에 기반한 이상 감지 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 서비스 환경에서의 시간 모델링 및 스케일 인식 특성 표현의 한계를 해결하기 위해.

Method: 개선된 Transformer 모듈을 사용하여 고차원 모니터링 데이터에 대해 시간 모델링을 수행하고, 다중 스케일 특성 구축 경로를 도입하여 다운샘플링 및 병렬 인코딩을 통해 다양한 세분화 수준에서 시간 특성을 추출합니다. 또한, 주의 수준 합성 모듈을 설계하여 각 스케일의 기여도를 동적으로 조정합니다.

Result: 제안된 방법은 정밀도, 재현율, AUC, F1-score 등 주요 지표에서 기존의 주류 기본 모델을 초과하며, 다양한 방해 조건에서도 강력한 안정성과 탐지 성능을 유지합니다.

Conclusion: 복잡한 클라우드 환경에서 우수한 기능을 입증합니다.

Abstract: This study proposes an anomaly detection method based on the Transformer
architecture with integrated multiscale feature perception, aiming to address
the limitations of temporal modeling and scale-aware feature representation in
cloud service environments. The method first employs an improved Transformer
module to perform temporal modeling on high-dimensional monitoring data, using
a self-attention mechanism to capture long-range dependencies and contextual
semantics. Then, a multiscale feature construction path is introduced to
extract temporal features at different granularities through downsampling and
parallel encoding. An attention-weighted fusion module is designed to
dynamically adjust the contribution of each scale to the final decision,
enhancing the model's robustness in anomaly pattern modeling. In the input
modeling stage, standardized multidimensional time series are constructed,
covering core signals such as CPU utilization, memory usage, and task
scheduling states, while positional encoding is used to strengthen the model's
temporal awareness. A systematic experimental setup is designed to evaluate
performance, including comparative experiments and hyperparameter sensitivity
analysis, focusing on the impact of optimizers, learning rates, anomaly ratios,
and noise levels. Experimental results show that the proposed method
outperforms mainstream baseline models in key metrics, including precision,
recall, AUC, and F1-score, and maintains strong stability and detection
performance under various perturbation conditions, demonstrating its superior
capability in complex cloud environments.

</details>


### [78] [Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism](https://arxiv.org/abs/2508.14523)
*Kevin Riehl,Shaimaa K. El-Baklish,Anastasios Kouvelas,Michail A. Makridis*

Main category: cs.LG

TL;DR: 도로 사용자 움직임의 정확한 예측이 필수적이며, 자전거에 대한 관심이 부족하다. 본 연구에서는 자전거를 위한 하이브리드 다중모달 궤적 예측 프레임워크인 Great GATsBi를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 정확한 도로 사용자 움직임 예측은 첨단 운전 보조 시스템과 자율주행 등 여러 응용 분야에서 요구되며, 도로 안전에 특히 중요하다.

Method: 물리 기반 모델링과 사회 기반 모델링을 결합한 하이브리드 다중모달 궤적 예측 프레임워크인 Great GATsBi를 제안하며, 사회적 상호작용을 그래프 주의 네트워크로 모델링한다.

Result: 제안된 물리 모델과 사회 모델의 앙상블이 최신 기술을 초월하는 성능을 보였다.

Conclusion: 대규모 자전거 실험을 통해 자전거 궤적 예측 및 도로 사용자와의 사회적 상호작용 모델링에서 프레임워크의 성능을 입증했다.

Abstract: Accurate prediction of road user movement is increasingly required by many
applications ranging from advanced driver assistance systems to autonomous
driving, and especially crucial for road safety. Even though most traffic
accident fatalities account to bicycles, they have received little attention,
as previous work focused mainly on pedestrians and motorized vehicles. In this
work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal
trajectory prediction framework for bicycles. The model incorporates both
physics-based modeling (inspired by motorized vehicles) and social-based
modeling (inspired by pedestrian movements) to explicitly account for the dual
nature of bicycle movement. The social interactions are modeled with a graph
attention network, and include decayed historical, but also anticipated, future
trajectory data of a bicycles neighborhood, following recent insights from
psychological and social studies. The results indicate that the proposed
ensemble of physics models -- performing well in the short-term predictions --
and social models -- performing well in the long-term predictions -- exceeds
state-of-the-art performance. We also conducted a controlled mass-cycling
experiment to demonstrate the framework's performance when forecasting bicycle
trajectories and modeling social interactions with road users.

</details>


### [79] [FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning](https://arxiv.org/abs/2508.14539)
*Tao Shen,Zexi Li,Didi Zhu,Ziyu Zhao,Chao Wu,Fei Wu*

Main category: cs.LG

TL;DR: 본 논문에서는 교차 장치 연합 학습에서 기간 드리프트와 클라이언트 드리프트 사이의 상호작용을 조사하고, 이 두 가지 드리프트가 서로 보완하여 전체 영향을 완화할 수 있는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 교차 장치 연합 학습에서 발생하는 데이터 이질성과 그로 인한 드리프트 문제를 개선하고자 함.

Method: 예측-관찰 프레임워크를 기반으로 한 FedEve라는 방법을 제안하여 두 종류의 드리프트가 상호 보완적으로 작용할 수 있도록 함.

Result: 우리의 방법이 교차 장치 설정에서 비독립적이고 동등하지 않은 데이터에서 기존 대안보다 우수한 성능을 보임을 폭넓은 실험을 통해 입증함.

Conclusion: 제안된 방법은 모델 업데이트의 분산을 줄이는 이론적 근거를 제공하며, 다양한 실험 결과로 그 효과를 뒷받침함.

Abstract: Federated learning (FL) is a machine learning paradigm that allows multiple
clients to collaboratively train a shared model without exposing their private
data. Data heterogeneity is a fundamental challenge in FL, which can result in
poor convergence and performance degradation. Client drift has been recognized
as one of the factors contributing to this issue resulting from the multiple
local updates in FedAvg. However, in cross-device FL, a different form of drift
arises due to the partial client participation, but it has not been studied
well. This drift, we referred as period drift, occurs as participating clients
at each communication round may exhibit distinct data distribution that
deviates from that of all clients. It could be more harmful than client drift
since the optimization objective shifts with every round.
  In this paper, we investigate the interaction between period drift and client
drift, finding that period drift can have a particularly detrimental effect on
cross-device FL as the degree of data heterogeneity increases. To tackle these
issues, we propose a predict-observe framework and present an instantiated
method, FedEve, where these two types of drift can compensate each other to
mitigate their overall impact. We provide theoretical evidence that our
approach can reduce the variance of model updates. Extensive experiments
demonstrate that our method outperforms alternatives on non-iid data in
cross-device settings.

</details>


### [80] [Cooperative SGD with Dynamic Mixing Matrices](https://arxiv.org/abs/2508.14565)
*Soumya Sarkar,Shweta Jain*

Main category: cs.LG

TL;DR: 이 논문은 동적 토폴로지를 갖춘 Local-Update 기반 분산 SGD 알고리즘을 통합한 프레임워크를 제시하며, 기존 연구에 비해 수렴에 대한 이론적 보장을 개선하거나 일치시킨다.


<details>
  <summary>Details</summary>
Motivation: 분산 환경에서의 SGD 기반 알고리즘은 특정 조건 하에 이론적으로 수렴하는 것으로 알려져 있지만, 기존 연구들은 정적인 토폴로지를 가정하고 노드의 기여도가 균일하다고 가정하여 최적이 아닌 결과를 도출하고 있다.

Method: 이 논문은 동적 토폴로지와 클라이언트 선택을 이용한 비균일 집합 전략을 포함하는 여러 Local-Update SGD 기반 분산 알고리즘을 다룬다.

Result: 제안된 통합 프레임워크는 기존 연구보다 개선된 또는 일치하는 수렴에 대한 이론적 보장을 제공한다.

Conclusion: 이 연구는 동적 환경에서의 분산 SGD 알고리즘의 성능을 크게 향상시킬 수 있는 방법을 제시한다.

Abstract: One of the most common methods to train machine learning algorithms today is
the stochastic gradient descent (SGD). In a distributed setting, SGD-based
algorithms have been shown to converge theoretically under specific
circumstances. A substantial number of works in the distributed SGD setting
assume a fixed topology for the edge devices. These papers also assume that the
contribution of nodes to the global model is uniform. However, experiments have
shown that such assumptions are suboptimal and a non uniform aggregation
strategy coupled with a dynamically shifting topology and client selection can
significantly improve the performance of such models. This paper details a
unified framework that covers several Local-Update SGD-based distributed
algorithms with dynamic topologies and provides improved or matching
theoretical guarantees on convergence compared to existing work.

</details>


### [81] [A Comprehensive Evaluation of the Sensitivity of Density-Ratio Estimation Based Fairness Measurement in Regression](https://arxiv.org/abs/2508.14576)
*Abdalwahab Almajed,Maryam Tabar,Peyman Najafirad*

Main category: cs.LG

TL;DR: ML 기반 접근법에서 알고리즘 편향의 확산은 편향을 측정하고 완화하는 연구의 증가를 초래했습니다. 본 논문은 다양한 밀도 비율 추정 방법을 활용해 공정성 측정 방법을 개발하고, 각기 다른 방법이 공정성 수준에 미치는 영향을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 알고리즘 편향은 머신러닝에서 중요한 문제로, 공정성을 측정하고 완화하기 위한 연구 필요성이 증가하고 있습니다.

Method: 본 논문은 다양한 밀도 비율 추정 방법을 사용하여 공정성 측정 방법의 세트를 개발하고, 각 방법이 결과에 미치는 영향을 심층적으로 조사합니다.

Result: 실험 결과, 밀도 비율 추정 방법의 선택이 공정성 측정 결과에 상당한 영향을 미치며, 다양한 알고리즘의 상대적 공정성에 대해 일관되지 않은 결과를 생성할 수 있음을 보여줍니다.

Conclusion: 밀도 비율 추정 기반의 공정성 측정은 회귀 분석에서 주요 문제를 보이며, 이들의 신뢰성을 높이기 위한 추가 연구의 필요성을 제기합니다.

Abstract: The prevalence of algorithmic bias in Machine Learning (ML)-driven approaches
has inspired growing research on measuring and mitigating bias in the ML
domain. Accordingly, prior research studied how to measure fairness in
regression which is a complex problem. In particular, recent research proposed
to formulate it as a density-ratio estimation problem and relied on a Logistic
Regression-driven probabilistic classifier-based approach to solve it. However,
there are several other methods to estimate a density ratio, and to the best of
our knowledge, prior work did not study the sensitivity of such fairness
measurement methods to the choice of underlying density ratio estimation
algorithm. To fill this gap, this paper develops a set of fairness measurement
methods with various density-ratio estimation cores and thoroughly investigates
how different cores would affect the achieved level of fairness. Our
experimental results show that the choice of density-ratio estimation core
could significantly affect the outcome of fairness measurement method, and
even, generate inconsistent results with respect to the relative fairness of
various algorithms. These observations suggest major issues with density-ratio
estimation based fairness measurement in regression and a need for further
research to enhance their reliability.

</details>


### [82] [DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning](https://arxiv.org/abs/2508.14600)
*Xudong Wang,Guoming Tang,Junyu Xue,Srinivasan Keshav,Tongxin Li,Chris Ding*

Main category: cs.LG

TL;DR: DualNILM은 비침입형 부하 모니터링(NILM)에서 기기 상태 인식과 에너지 주입 식별을 동시에 수행하기 위한 심층 다중 작업 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 스마트 홈 및 건물 응용 프로그램에서 세부적인 기기 수준의 에너지 소비를 획득할 수 있는 비침입형 부하 모니터링(NILM)이 점점 더 많이 채택되고 있지만, 발전기 및 배터리 저장과 같은 미터 뒤 에너지 원의 사용 증가로 인해 기존 NILM 방법에 새로운 도전 과제가 발생하였다.

Method: DualNILM은 Transformer 기반 아키텍처 내에서 시퀀스-투-포인트 및 시퀀스-투-시퀀스 전략을 통합하여 기기 상태 인식과 주입된 에너지 식별이라는 이중 작업을 효과적으로 수행할 수 있게 한다.

Result: 자체 수집된 NILM 데이터셋과 합성된 공개 NILM 데이터셋을 사용하여 DualNILM의 유효성을 검증하였고, 기기 수준의 에너지 소비와 에너지 주입이 포함됐다.

Conclusion: 광범위한 실험 결과에 따르면, DualNILM은 기존의 방법들보다 매우 뛰어난 성능을 보여준다.

Abstract: Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain
fine-grained appliance-level energy consumption in smart homes and building
applications. However, the increasing adoption of behind-the-meter energy
sources, such as solar panels and battery storage, poses new challenges for
conventional NILM methods that rely solely on at-the-meter data. The injected
energy from the behind-the-meter sources can obscure the power signatures of
individual appliances, leading to a significant decline in NILM performance. To
address this challenge, we present DualNILM, a deep multi-task learning
framework designed for the dual tasks of appliance state recognition and
injected energy identification in NILM. By integrating sequence-to-point and
sequence-to-sequence strategies within a Transformer-based architecture,
DualNILM can effectively capture multi-scale temporal dependencies in the
aggregate power consumption patterns, allowing for accurate appliance state
recognition and energy injection identification. We conduct validation of
DualNILM using both self-collected and synthesized open NILM datasets that
include both appliance-level energy consumption and energy injection. Extensive
experimental results demonstrate that DualNILM maintains an excellent
performance for the dual tasks in NILM, much outperforming conventional
methods.

</details>


### [83] [Measuring IIA Violations in Similarity Choices with Bayesian Models](https://arxiv.org/abs/2508.14615)
*Hugo Sales Corrêa,Suryanarayana Sankagiri,Daniel Ratton Figueiredo,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 유사성 선택 데이터는 인간이 대안 간의 선택을 할 때 발생하며, 이 논문에서는 독립적인 관련 대안의 가정(IIA)에 대한 검증 방법을 제안하고 IIA 위반이 있는지 테스트한다.


<details>
  <summary>Details</summary>
Motivation: 유사성 선택 설정에서 IIA 위반이 널리 연구되지 않았기 때문에, 이를 검증할 필요가 있다.

Method: 고전적인 적합도 검정과 베이지안 접근법을 사용하여 IIA를 테스트하는 두 가지 통계적 방법을 제안한다.

Result: 두 데이터셋 모두에서 유의미한 IIA 위반을 확인하였으며, 두 데이터셋 간에 유사한 정도의 위반을 발견하였다.

Conclusion: 선택 집합 내 상호작용과 같은 맥락 효과를 반영하는 새로운 유사성 선택 모델이 필요하다.

Abstract: Similarity choice data occur when humans make choices among alternatives
based on their similarity to a target, e.g., in the context of information
retrieval and in embedding learning settings. Classical metric-based models of
similarity choice assume independence of irrelevant alternatives (IIA), a
property that allows for a simpler formulation. While IIA violations have been
detected in many discrete choice settings, the similarity choice setting has
received scant attention. This is because the target-dependent nature of the
choice complicates IIA testing. We propose two statistical methods to test for
IIA: a classical goodness-of-fit test and a Bayesian counterpart based on the
framework of Posterior Predictive Checks (PPC). This Bayesian approach, our
main technical contribution, quantifies the degree of IIA violation beyond its
mere significance. We curate two datasets: one with choice sets designed to
elicit IIA violations, and another with randomly generated choice sets from the
same item universe. Our tests confirmed significant IIA violations on both
datasets, and notably, we find a comparable degree of violation between them.
Further, we devise a new PPC test for population homogeneity. Results show that
the population is indeed homogenous, suggesting that the IIA violations are
driven by context effects -- specifically, interactions within the choice sets.
These results highlight the need for new similarity choice models that account
for such context effects.

</details>


### [84] [A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification](https://arxiv.org/abs/2508.14618)
*Amin Noroozi,Sandaruwan K. Sethunge,Elham Norouzi,Phat T. Phan,Kavinda U. Waduge,Md. Arafatur Rahman*

Main category: cs.LG

TL;DR: CDO(지속적 하강 작전)는 연료 소모, 배기가스 및 소음을 줄이면서 효율성 및 승객 편안함을 향상시키는 부드러운 하강 방식입니다. 그러나 CDO 성능에 영향을 미치는 요인은 제한적으로 연구되어 왔으며, 비행 안전을 위한 설명 가능성이 중요한 항공 분야에서 투명성이 결여된 기존 방법들이 많습니다. 이 연구는 퍼지 로직을 머신 러닝 및 SHAP 분석과 통합한 퍼지 강화 설명 가능한 AI(FEXAI) 프레임워크를 제안하였고, 비행 데이터로부터 도출된 29개의 특징을 분석하여 CDO 준수 수준을 분류하고 중요한 특징을 순위화하였습니다. 결과는 FEXAI가 90% 이상의 정확도를 기록하고, CDO 성능을 예측하는 중요 변수를 발견한 것을 포함합니다.


<details>
  <summary>Details</summary>
Motivation: CDO의 운영적 및 환경적 이점에도 불구하고 CDO 성능에 영향을 미치는 요소를 체계적으로 조사한 연구가 제한적입니다.

Method: 퍼지 로직과 머신 러닝 및 SHAP 분석을 통합한 퍼지 강화 설명 가능한 AI(FEXAI) 프레임워크를 제안하고, ADS-B 데이터를 이용해 1,094편의 비행으로부터 29개의 특징 데이터를 수집하여 머신 러닝 모델 및 SHAP을 적용합니다.

Result: 모든 모델이 90% 이상의 분류 정확도를 달성하였으며, FEXAI는 운영 사용자에게 의미 있는 사람 읽기 가능한 규칙을 제공합니다. SHAP 점수에 의해 식별된 CDO 성능의 세 가지 가장 영향력 있는 특징이 밝혀졌습니다.

Conclusion: FEXAI 방법은 운영 결정 지원을 위한 새로운 경로를 제시하며, 다양한 운영 조건에서 CDO 준수를 유지하기 위한 실시간 조언을 가능하게 할 수 있습니다.

Abstract: Continuous Descent Operations (CDO) involve smooth, idle-thrust descents that
avoid level-offs, reducing fuel burn, emissions, and noise while improving
efficiency and passenger comfort. Despite its operational and environmental
benefits, limited research has systematically examined the factors influencing
CDO performance. Moreover, many existing methods in related areas, such as
trajectory optimization, lack the transparency required in aviation, where
explainability is critical for safety and stakeholder trust. This study
addresses these gaps by proposing a Fuzzy-Enhanced Explainable AI (FEXAI)
framework that integrates fuzzy logic with machine learning and SHapley
Additive exPlanations (SHAP) analysis. For this purpose, a comprehensive
dataset of 29 features, including 11 operational and 18 weather-related
features, was collected from 1,094 flights using Automatic Dependent
Surveillance-Broadcast (ADS-B) data. Machine learning models and SHAP were then
applied to classify flights' CDO adherence levels and rank features by
importance. The three most influential features, as identified by SHAP scores,
were then used to construct a fuzzy rule-based classifier, enabling the
extraction of interpretable fuzzy rules. All models achieved classification
accuracies above 90%, with FEXAI providing meaningful, human-readable rules for
operational users. Results indicated that the average descent rate within the
arrival route, the number of descent segments, and the average change in
directional heading during descent were the strongest predictors of CDO
performance. The FEXAI method proposed in this study presents a novel pathway
for operational decision support and could be integrated into aviation tools to
enable real-time advisories that maintain CDO adherence under varying
operational conditions.

</details>


### [85] [Clinical semantics for lung cancer prediction](https://arxiv.org/abs/2508.14627)
*Luis H. John,Jan A. Kors,Jenna M. Reps,Peter R. Rijnbeek,Egill A. Fridgeirsson*

Main category: cs.LG

TL;DR: 본 연구는 Poincaré 임베딩을 사용하여 SNOMED 의학 용어 계층을 저차원 하이퍼볼릭 공간으로 매핑하여 폐암 발병 예측 정확도를 향상시키는 방법을 연구했다.


<details>
  <summary>Details</summary>
Motivation: 기존의 임상 예측 모델은 임상 개념 간의 의미적 관계를 무시하는 특성을 사용하여 환자 데이터를 표현한다.

Method: Optum EHR 데이터 세트의 과거 집단을 사용하여 SNOMED 분류에서 임상 지식 그래프를 도출하고, 리만 확률적 경량 기울기 하강법을 통해 Poincaré 임베딩을 생성하였다. 이러한 임베딩은 ResNet 및 Transformer 모델이라는 두 가지 심층 학습 아키텍처에 통합되었다.

Result: 사전 훈련된 Poincaré 임베딩을 통합함으로써 무작위로 초기화된 유클리드 임베딩을 사용하는 기반 모델에 비해 분별력 성능이 적절하고 일관된 개선을 보였다. 특히 10차원 Poincaré 임베딩을 사용하는 ResNet 모델은 보정이 강화된 반면, Transformer 모델은 다양한 구성에서 안정적인 보정을 유지하였다.

Conclusion: 임상 지식 그래프를 하이퍼볼릭 공간에 임베딩하고 이러한 표현을 심층 학습 모델에 통합함으로써 폐암 발병 예측을 개선할 수 있음을 보여주었다.

Abstract: Background: Existing clinical prediction models often represent patient data
using features that ignore the semantic relationships between clinical
concepts. This study integrates domain-specific semantic information by mapping
the SNOMED medical term hierarchy into a low-dimensional hyperbolic space using
Poincar\'e embeddings, with the aim of improving lung cancer onset prediction.
  Methods: Using a retrospective cohort from the Optum EHR dataset, we derived
a clinical knowledge graph from the SNOMED taxonomy and generated Poincar\'e
embeddings via Riemannian stochastic gradient descent. These embeddings were
then incorporated into two deep learning architectures, a ResNet and a
Transformer model. Models were evaluated for discrimination (area under the
receiver operating characteristic curve) and calibration (average absolute
difference between observed and predicted probabilities) performance.
  Results: Incorporating pre-trained Poincar\'e embeddings resulted in modest
and consistent improvements in discrimination performance compared to baseline
models using randomly initialized Euclidean embeddings. ResNet models,
particularly those using a 10-dimensional Poincar\'e embedding, showed enhanced
calibration, whereas Transformer models maintained stable calibration across
configurations.
  Discussion: Embedding clinical knowledge graphs into hyperbolic space and
integrating these representations into deep learning models can improve lung
cancer onset prediction by preserving the hierarchical structure of clinical
terminologies used for prediction. This approach demonstrates a feasible method
for combining data-driven feature extraction with established clinical
knowledge.

</details>


### [86] [Understanding Data Influence with Differential Approximation](https://arxiv.org/abs/2508.14648)
*Haoru Tan,Sitong Wu,Xiuzhe Wu,Wang Wang,Bo Zhao,Zeke Xie,Gui-Song Xia,Xiaojuan Qi*

Main category: cs.LG

TL;DR: 본 논문에서는 표본의 영향을 근사하기 위한 새로운 수식을 제안합니다. 이는 연속적인 학습 단계 간의 영향 차이를 누적하여 산출한 것으로, Diff-In이라 명명합니다.


<details>
  <summary>Details</summary>
Motivation: 데이터는 인공지능의 발전에 중요한 역할을 하며, 데이터의 양적 분석은 모델 훈련에 기여하지만 기존 도구들은 정확성이 부족합니다.

Method: Diff-In이라는 새로운 방법론을 제안하며, 이는 샘플의 영향을 연속적인 훈련 반복에서의 변화의 누적합으로 정의합니다.

Result: Diff-In은 기존의 영향 추정기보다 유의미하게 낮은 근사 오류를 달성하며, 데이터 전처리와 관련된 여러 작업에서 우수한 성능을 보입니다.

Conclusion: Diff-In은 수백만 개의 데이터 포인트에 대해 확장 가능하며, 강력한 기반선보다 우수한 성능을 발휘합니다.

Abstract: Data plays a pivotal role in the groundbreaking advancements in artificial
intelligence. The quantitative analysis of data significantly contributes to
model training, enhancing both the efficiency and quality of data utilization.
However, existing data analysis tools often lag in accuracy. For instance, many
of these tools even assume that the loss function of neural networks is convex.
These limitations make it challenging to implement current methods effectively.
In this paper, we introduce a new formulation to approximate a sample's
influence by accumulating the differences in influence between consecutive
learning steps, which we term Diff-In. Specifically, we formulate the
sample-wise influence as the cumulative sum of its changes/differences across
successive training iterations. By employing second-order approximations, we
approximate these difference terms with high accuracy while eliminating the
need for model convexity required by existing methods. Despite being a
second-order method, Diff-In maintains computational complexity comparable to
that of first-order methods and remains scalable. This efficiency is achieved
by computing the product of the Hessian and gradient, which can be efficiently
approximated using finite differences of first-order gradients. We assess the
approximation accuracy of Diff-In both theoretically and empirically. Our
theoretical analysis demonstrates that Diff-In achieves significantly lower
approximation error compared to existing influence estimators. Extensive
experiments further confirm its superior performance across multiple benchmark
datasets in three data-centric tasks: data cleaning, data deletion, and coreset
selection. Notably, our experiments on data pruning for large-scale
vision-language pre-training show that Diff-In can scale to millions of data
points and outperforms strong baselines.

</details>


### [87] [Improving Fairness in Graph Neural Networks via Counterfactual Debiasing](https://arxiv.org/abs/2508.14683)
*Zengyi Wo,Chang Liu,Yumeng Wang,Minglai Shao,Wenjun Wang*

Main category: cs.LG

TL;DR: 이 논문은 그래프 신경망(GNN)에서의 편향 문제를 해결하기 위한 새로운 접근법인 Fair-ICD를 제안한다.


<details>
  <summary>Details</summary>
Motivation: GNN이 인종 및 성별과 같은 속성을 바탕으로 편향된 예측을 할 수 있다는 문제를 해결하고자 한다.

Method: 이 방법은 메시지 패싱 전에 반사실적 데이터를 활용하여 다양성 있는 이웃을 생성하고, 이를 통해 공정한 노드 표현을 학습한다.

Result: Fair-ICD는 실험을 통해 GNN 모델의 공정성을 상당히 향상시킴을 보여준다.

Conclusion: 제안된 기법은 GNN의 공정성을 보장하면서도 높은 예측 성능을 유지한다.

Abstract: Graph Neural Networks (GNNs) have been successful in modeling
graph-structured data. However, similar to other machine learning models, GNNs
can exhibit bias in predictions based on attributes like race and gender.
Moreover, bias in GNNs can be exacerbated by the graph structure and
message-passing mechanisms. Recent cutting-edge methods propose mitigating bias
by filtering out sensitive information from input or representations, like edge
dropping or feature masking. Yet, we argue that such strategies may
unintentionally eliminate non-sensitive features, leading to a compromised
balance between predictive accuracy and fairness. To tackle this challenge, we
present a novel approach utilizing counterfactual data augmentation for bias
mitigation. This method involves creating diverse neighborhoods using
counterfactuals before message passing, facilitating unbiased node
representations learning from the augmented graph. Subsequently, an adversarial
discriminator is employed to diminish bias in predictions by conventional GNN
classifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNs
under moderate conditions. Experiments on standard datasets using three GNN
backbones demonstrate that Fair-ICD notably enhances fairness metrics while
preserving high predictive performance.

</details>


### [88] [Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum](https://arxiv.org/abs/2508.14684)
*Zengyi Wo,Wenjun Wang,Minglai Shao,Chang Liu,Yumeng Wang,Yueheng Sun*

Main category: cs.LG

TL;DR: 이 연구에서는 비정상 네트워크의 이질적 구조를 해결하기 위해 Causal Edge Separation을 기반으로 한 CES2-GAD라는 스펙트럴 신경망을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 비정상 엔티티가 정당한 연결을 추가하며 비정상 엔티티와의 직접적인 링크를 숨기는 상황에서 GNN 기반 기술이 실패하는 문제를 해결하기 위해.

Method: CES2-GAD는 인과 개입을 사용하여 원래 그래프를 동질적 및 이질적 엣지로 분리하고, 혼합 스펙트럼 필터를 사용하여 신호를 캡처합니다.

Result: 실제 데이터셋과의 광범위한 실험을 통해 제안한 방법의 효과가 입증되었습니다.

Conclusion: 비정상 그래프에서의 이상 탐지에 있어 CAUSAL EDGE SEPARATION 기법이 효과적임을 강조합니다.

Abstract: In the real world, anomalous entities often add more legitimate connections
while hiding direct links with other anomalous entities, leading to
heterophilic structures in anomalous networks that most GNN-based techniques
fail to address. Several works have been proposed to tackle this issue in the
spatial domain. However, these methods overlook the complex relationships
between node structure encoding, node features, and their contextual
environment and rely on principled guidance, research on solving spectral
domain heterophilic problems remains limited. This study analyzes the spectral
distribution of nodes with different heterophilic degrees and discovers that
the heterophily of anomalous nodes causes the spectral energy to shift from low
to high frequencies. To address the above challenges, we propose a spectral
neural network CES2-GAD based on causal edge separation for anomaly detection
on heterophilic graphs. Firstly, CES2-GAD will separate the original graph into
homophilic and heterophilic edges using causal interventions. Subsequently,
various hybrid-spectrum filters are used to capture signals from the segmented
graphs. Finally, representations from multiple signals are concatenated and
input into a classifier to predict anomalies. Extensive experiments with
real-world datasets have proven the effectiveness of the method we proposed.

</details>


### [89] [CaTE Data Curation for Trustworthy AI](https://arxiv.org/abs/2508.14741)
*Mary Versa Clemens-Sewall,Christopher Cervantes,Emma Rafkin,J. Neil Otte,Tom Magelinski,Libby Lewis,Michelle Liu,Dana Udwin,Monique Kirkman-Bey*

Main category: cs.LG

TL;DR: AI 시스템의 신뢰성을 높이기 위한 데이터 관리 단계의 실용적인 가이드를 제공한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템 개발을 위한 데이터 관리 단계에서 신뢰성을 촉진하는 방법을 팀에 안내하기 위함이다.

Method: 개발 팀, 특히 데이터 과학자들이 신뢰할 수 있는 AI 시스템을 구축하기 위해 취할 수 있는 일련의 단계들을 설명한다.

Result: 핵심 단계의 순서를 나열하고 대안이 존재하는 경우 병행 경로를 추적한다.

Conclusion: 독자가 AI 신뢰성을 개선하기 위한 다양한 yet 일관된 실행 방식을 갖출 수 있도록 돕는 것이 목표이다.

Abstract: This report provides practical guidance to teams designing or developing
AI-enabled systems for how to promote trustworthiness during the data curation
phase of development. In this report, the authors first define data, the data
curation phase, and trustworthiness. We then describe a series of steps that
the development team, especially data scientists, can take to build a
trustworthy AI-enabled system. We enumerate the sequence of core steps and
trace parallel paths where alternatives exist. The descriptions of these steps
include strengths, weaknesses, preconditions, outcomes, and relevant
open-source software tool implementations. In total, this report is a synthesis
of data curation tools and approaches from relevant academic literature, and
our goal is to equip readers with a diverse yet coherent set of practices for
improving AI trustworthiness.

</details>


### [90] [MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding](https://arxiv.org/abs/2508.14746)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: 이 논문에서는 데이터 없는 그래프에 적합한 새로운 그래프 구조 개선 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLMs에서 생성된 추론 그래프가 비디오 이상 탐지와 같은 시각적 작업과 잘 맞지 않기 때문입니다.

Method: 데이터 기반 그래프 구조 개선(D-GSR) 패러다임을 소개하고, 이를 운영화하기 위해 MissionHD라는 하이퍼차원 컴퓨팅 프레임워크를 제안했습니다.

Result: 도전적인 VAD 및 VAR 벤치마크에서 개선된 그래프를 사용했을 때 성능 향상을 보여주었습니다.

Conclusion: 우리의 방법이 효과적인 전처리 단계로서의 유효성을 입증했습니다.

Abstract: Reasoning graphs from Large Language Models (LLMs) are often misaligned with
downstream visual tasks such as video anomaly detection (VAD). Existing Graph
Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less
graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly
optimizes graph structure using downstream task data, and propose MissionHD, a
hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses
an efficient encode-decode process to refine the graph, guided by the
downstream task signal. Experiments on challenging VAD and VAR benchmarks show
significant performance improvements when using our refined graphs, validating
our approach as an effective pre-processing step.

</details>


### [91] [HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents](https://arxiv.org/abs/2508.14751)
*Thomas Carta,Clément Romac,Loris Gaven,Pierre-Yves Oudeyer,Olivier Sigaud,Sylvain Lamprier*

Main category: cs.LG

TL;DR: HERAKLES라는 체계를 통해 오픈엔디드 AI 에이전트가 복잡한 목표를 효율적으로 학습하고, 새로운 도전에 잘 적응할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 오픈엔디드 AI 에이전트는 경과하는 시간 동안 증가하는 복잡성, 추상성 및 이질성을 가진 목표를 효율적으로 학습할 수 있어야 한다.

Method: HERAKLES 프레임워크는 2단계 계층형 자율적 에이전트를 통해 숙달된 목표를 낮은 수준의 정책에 지속적으로 컴파일하고, 고급 정책이 접근 가능한 하위 목표 집합을 동적으로 확장한다.

Result: HERAKLES는 오픈엔디드 Crafter 환경에서 평가되었으며, 목표 복잡성이 증가함에 따라 효과적으로 확장되고, 기술 컴파일을 통해 샘플 효율성을 개선하며, 에이전트가 새로운 도전에 강하게 적응할 수 있도록 한다.

Conclusion: HERAKLES는 복잡한 목표를 효과적으로 처리하고, 샘플 효율성을 높이며, 노력을 통해 지속적인 적응을 가능하게 한다.

Abstract: Open-ended AI agents need to be able to learn efficiently goals of increasing
complexity, abstraction and heterogeneity over their lifetime. Beyond sampling
efficiently their own goals, autotelic agents specifically need to be able to
keep the growing complexity of goals under control, limiting the associated
growth in sample and computational complexity. To adress this challenge, recent
approaches have leveraged hierarchical reinforcement learning (HRL) and
language, capitalizing on its compositional and combinatorial generalization
capabilities to acquire temporally extended reusable behaviours. Existing
approaches use expert defined spaces of subgoals over which they instantiate a
hierarchy, and often assume pre-trained associated low-level policies. Such
designs are inadequate in open-ended scenarios, where goal spaces naturally
diversify across a broad spectrum of difficulties. We introduce HERAKLES, a
framework that enables a two-level hierarchical autotelic agent to continuously
compile mastered goals into the low-level policy, executed by a small, fast
neural network, dynamically expanding the set of subgoals available to the
high-level policy. We train a Large Language Model (LLM) to serve as the
high-level controller, exploiting its strengths in goal decomposition and
generalization to operate effectively over this evolving subgoal space. We
evaluate HERAKLES in the open-ended Crafter environment and show that it scales
effectively with goal complexity, improves sample efficiency through skill
compilation, and enables the agent to adapt robustly to novel challenges over
time.

</details>


### [92] [Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data](https://arxiv.org/abs/2508.14769)
*Ahmed Mujtaba,Gleb Radchenko,Radu Prodan,Marc Masana*

Main category: cs.LG

TL;DR: Federated distillation은 기존의 연합 학습 보다 향상된 개인 정보 보호와 통신량 감소를 제공하는 효율적인 기계 학습 접근법이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 연합 학습 방식은 모델 파라미터를 전송하는 대신 모델 출력(소프트 로짓)을 교환하여 개인 정보 보호를 향상시키고 통신량을 줄이는 방법이다.

Method: EdgeFD는 클라이언트 측 밀도 비율 추정의 복잡성을 줄이고 서버 측 필터링을 제거하는 강력하고 자원 효율적인 방법이다.

Result: EdgeFD는 다양한 실용 시나리오에서 평가되었으며, 이질적인 조건에서도 IID 시나리오에 가까운 정확도를 지속적으로 달성했다.

Conclusion: KMeans 기반 추정기의 계산 오버헤드가 크게 줄어들어 자원이 제한된 엣지 디바이스에서의 배치에 적합하면서 연합 증류의 확장성과 실제 적용 가능성을 향상시켰다.

Abstract: Federated distillation has emerged as a promising collaborative machine
learning approach, offering enhanced privacy protection and reduced
communication compared to traditional federated learning by exchanging model
outputs (soft logits) rather than full model parameters. However, existing
methods employ complex selective knowledge-sharing strategies that require
clients to identify in-distribution proxy data through computationally
expensive statistical density ratio estimators. Additionally, server-side
filtering of ambiguous knowledge introduces latency to the process. To address
these challenges, we propose a robust, resource-efficient EdgeFD method that
reduces the complexity of the client-side density ratio estimation and removes
the need for server-side filtering. EdgeFD introduces an efficient KMeans-based
density ratio estimator for effectively filtering both in-distribution and
out-of-distribution proxy data on clients, significantly improving the quality
of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,
including strong non-IID, weak non-IID, and IID data distributions on clients,
without requiring a pre-trained teacher model on the server for knowledge
distillation. Experimental results demonstrate that EdgeFD outperforms
state-of-the-art methods, consistently achieving accuracy levels close to IID
scenarios even under heterogeneous and challenging conditions. The
significantly reduced computational overhead of the KMeans-based estimator is
suitable for deployment on resource-constrained edge devices, thereby enhancing
the scalability and real-world applicability of federated distillation. The
code is available online for reproducibility.

</details>


### [93] [Context Steering: A New Paradigm for Compression-based Embeddings by Synthesizing Relevant Information Features](https://arxiv.org/abs/2508.14780)
*Guillermo Sarasa Durán,Ana Granados Fontecha,Francisco de Borja Rodríguez Ortíz*

Main category: cs.LG

TL;DR: 본 논문에서는 데이터 객체 간의 중복성을 통해 암묵적 정보를 식별하는 압축 기반 거리(CD)를 사용하여 유사성을 측정하는 방법론을 제시합니다. 그 과정에서 '맥락 조정'이라는 새로운 방법론을 도입하여 특징 형성 과정을 능동적으로 안내합니다.


<details>
  <summary>Details</summary>
Motivation: 압축 기반 거리(CD)와 해당 유사성 특징이 복잡한 클러스터링이나 분류 작업에 적합하게 적용되지 않아 문제를 해결하기 위해.

Method: 맥락 조정(context steering)이라는 새로운 방법론을 도입하여 각 객체가 클러스터링 프레임워크 내에서 관계 맥락에 미치는 영향을 체계적으로 분석하고, 이를 통해 맞춤형 임베딩을 생성한다.

Result: Normalized Compression Distance (NCD)와 Relative Compression Distance (NRC)를 사용하여 일반적인 계층적 클러스터링에서 이 전략의 유효성을 검증하였다.

Conclusion: 데이터 구조 발견에서 특정 목적에 맞춘 특징 공간의 능동적 형태로의 근본적인 전환을 나타낸다.

Abstract: Compression-based distances (CD) offer a flexible and domain-agnostic means
of measuring similarity by identifying implicit information through
redundancies between data objects. However, as similarity features are derived
from the data, rather than defined as an input, it often proves difficult to
align with the task at hand, particularly in complex clustering or
classification settings. To address this issue, we introduce "context
steering," a novel methodology that actively guides the feature-shaping
process. Instead of passively accepting the emergent data structure (typically
a hierarchy derived from clustering CDs), our approach "steers" the process by
systematically analyzing how each object influences the relational context
within a clustering framework. This process generates a custom-tailored
embedding that isolates and amplifies class-distinctive information. We
validate the capabilities of this strategy using Normalized Compression
Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical
clustering, providing an effective alternative to common transductive methods.
Experimental results across heterogeneous datasets-from text to real-world
audio-validate the robustness and generality of context steering, marking a
fundamental shift in their application: from merely discovering inherent data
structures to actively shaping a feature space tailored to a specific
objective.

</details>


### [94] [Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method](https://arxiv.org/abs/2508.14783)
*Suleyman Olcay Polat,Poli A. Nemkova,Mark V. Albert*

Main category: cs.LG

TL;DR: 새로운 적응형 증류 프레임워크를 통해 높은 손실 영역에서 훈련 데이터를 동적으로 증강하여 모델 압축을 효율적으로 수행.


<details>
  <summary>Details</summary>
Motivation: 자원 제약 환경에서 대규모 모델의 지식을 소형 학생 모델로 전이하는 것이 필요하다.

Method: UMAP 기반 차원 축소와 최근접 이웃 샘플링을 이용하여 손실이 큰 영역을 식별하고 목표된 합성 예제를 생성한다. 또한, 경량의 교사-학생 인터페이스를 도입하여 교사의 입력 층을 우회한다.

Result: 66M 파라미터의 학생 모델이 QNLI에서 91.2%, SST-2에서 92.3%의 성과를 내며, 적은 에폭으로 훈련되었다.

Conclusion: 손실 인식 데이터 증강과 벡터화된 증류가 효율적이고 효과적인 모델 압축에 대한 가능성을 강조한다.

Abstract: Model distillation enables the transfer of knowledge from large-scale models
to compact student models, facilitating deployment in resource-constrained
environments. However, conventional distillation approaches often suffer from
computational overhead and limited generalization. We propose a novel adaptive
distillation framework that dynamically augments training data in regions of
high student model loss. Using UMAP-based dimensionality reduction and nearest
neighbor sampling, our method identifies underperforming regions in the
embedding space and generates targeted synthetic examples to guide student
learning. To further improve efficiency, we introduce a lightweight
teacher-student interface that bypasses the teacher's input layer, enabling
direct distillation on vectorized representations. Experiments across standard
NLP benchmarks demonstrate that our 66M-parameter student model consistently
matches or surpasses established baselines, achieving 91.2% on QNLI and 92.3%
on SST-2, while training with fewer epochs. These results highlight the promise
of loss-aware data augmentation and vectorized distillation for efficient and
effective model compression.

</details>


### [95] [A Guide for Manual Annotation of Scientific Imagery: How to Prepare for Large Projects](https://arxiv.org/abs/2508.14801)
*Azim Ahmadzadeh,Rohan Adhyapak,Armin Iraji,Kartik Chaurasiya,V Aparna,Petrus C. Martens*

Main category: cs.LG

TL;DR: 이 논문은 복잡하고 비용이 많이 드는 주석 프로젝트 관리를 위한 포괄적인 가이드를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 수동으로 주석이 달린 이미지 데이터에 대한 높은 수요에도 불구하고, 이러한 프로젝트 관리는 여전히 충분히 논의되지 않고 있습니다.

Method: 이 논문은 과학 이미지에 초점을 맞추어 주석 프로젝트를 위한 도메인 비구속 준비 가이드를 제공합니다.

Result: 저자들의 대규모 수동 주석 프로젝트 관리 경험을 바탕으로, 프로젝트 목표, 데이터 가용성, 필수 팀 역할 등 기본 개념을 다루고, 주석 품질과 효율성을 향상시키기 위한 도구와 기술을 추천합니다.

Conclusion: 이 연구는 다양한 분야에서 수동 주석 프로젝트의 비용을 줄일 수 있는 포괄적인 지식 기반 생성을 위한 추가 연구와 프레임워크 개발을 촉구합니다.

Abstract: Despite the high demand for manually annotated image data, managing complex
and costly annotation projects remains under-discussed. This is partly due to
the fact that leading such projects requires dealing with a set of diverse and
interconnected challenges which often fall outside the expertise of specific
domain experts, leaving practical guidelines scarce. These challenges range
widely from data collection to resource allocation and recruitment, from
mitigation of biases to effective training of the annotators. This paper
provides a domain-agnostic preparation guide for annotation projects, with a
focus on scientific imagery. Drawing from the authors' extensive experience in
managing a large manual annotation project, it addresses fundamental concepts
including success measures, annotation subjects, project goals, data
availability, and essential team roles. Additionally, it discusses various
human biases and recommends tools and technologies to improve annotation
quality and efficiency. The goal is to encourage further research and
frameworks for creating a comprehensive knowledge base to reduce the costs of
manual annotation projects across various fields.

</details>


### [96] [Source-Guided Flow Matching](https://arxiv.org/abs/2508.14807)
*Zifan Wang,Alice Harting,Matthieu Barreau,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 본 논문에서는 사전 훈련된 벡터 필드를 유지하면서 소스 분포를 직접 수정하는 Source-Guided Flow Matching (SGFM) 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 생성 모델의 안내를 보다 효과적으로 수행하기 위해 새로운 방법론이 필요합니다.

Method: SGFM 프레임워크는 소스 분포를 직접 수정하여 문제를 정의된 샘플링 문제로 축소합니다.

Result: SGFM이 원하는 목표 분포를 정확히 회복함을 이론적으로 증명하며, 소스 분포의 근사 샘플러와 근사 벡터 필드를 사용할 때 발생할 수 있는 Wasserstein 오차의 경계를 제공합니다.

Conclusion: 제안하는 방법은 사용자가 특정 문제에 맞춰 샘플링 방법을 유연하게 선택할 수 있도록 해줍니다.

Abstract: Guidance of generative models is typically achieved by modifying the
probability flow vector field through the addition of a guidance field. In this
paper, we instead propose the Source-Guided Flow Matching (SGFM) framework,
which modifies the source distribution directly while keeping the pre-trained
vector field intact. This reduces the guidance problem to a well-defined
problem of sampling from the source distribution. We theoretically show that
SGFM recovers the desired target distribution exactly. Furthermore, we provide
bounds on the Wasserstein error for the generated distribution when using an
approximate sampler of the source distribution and an approximate vector field.
The key benefit of our approach is that it allows the user to flexibly choose
the sampling method depending on their specific problem. To illustrate this, we
systematically compare different sampling methods and discuss conditions for
asymptotically exact guidance. Moreover, our framework integrates well with
optimal flow matching models since the straight transport map generated by the
vector field is preserved. Experimental results on synthetic 2D benchmarks,
image datasets, and physics-informed generative tasks demonstrate the
effectiveness and flexibility of the proposed framework.

</details>


### [97] [Enhancing Contrastive Link Prediction With Edge Balancing Augmentation](https://arxiv.org/abs/2508.14808)
*Chen-Hao Chang,Hui-Ju Hung,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 본 논문은 링크 예측의 대조 학습을 이론적으로 분석하고, 노드 차수를 고려한 새로운 그래프 증강 접근법을 제안하여 성능을 향상시키는 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 대조 학습을 활용해 링크 예측 성능을 향상시키고자 하며, 기존 연구의 두 가지 약점을 해결하고자 한다.

Method: 대조 학습을 위한 첫 번째 공식 이론적 분석을 제공하고, Edge Balancing Augmentation(EBA)라는 새로운 그래프 증강 접근법을 제안하며, 이를 통해 모델 성능을 개선하기 위한 새로운 접근 방식인 CoEBA를 통합한다.

Result: 8개의 벤치마크 데이터셋에 대한 실험을 수행한 결과, 제안한 CoEBA가 기존의 최신 링크 예측 모델보다 성능이 크게 향상됨을 보여준다.

Conclusion: 이 연구는 링크 예측을 위한 대조 학습의 이론적 기초를 제공하고, 실험적으로 성능 향상이 검증된 새로운 방법론을 제시한다.

Abstract: Link prediction is one of the most fundamental tasks in graph mining, which
motivates the recent studies of leveraging contrastive learning to enhance the
performance. However, we observe two major weaknesses of these studies: i) the
lack of theoretical analysis for contrastive learning on link prediction, and
ii) inadequate consideration of node degrees in contrastive learning. To
address the above weaknesses, we provide the first formal theoretical analysis
for contrastive learning on link prediction, where our analysis results can
generalize to the autoencoder-based link prediction models with contrastive
learning. Motivated by our analysis results, we propose a new graph
augmentation approach, Edge Balancing Augmentation (EBA), which adjusts the
node degrees in the graph as the augmentation. We then propose a new approach,
named Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA),
that integrates the proposed EBA and the proposed new contrastive losses to
improve the model performance. We conduct experiments on 8 benchmark datasets.
The results demonstrate that our proposed CoEBA significantly outperforms the
other state-of-the-art link prediction models.

</details>


### [98] [Successive Halving with Learning Curve Prediction via Latent Kronecker Gaussian Processes](https://arxiv.org/abs/2508.14818)
*Jihao Andreas Lin,Nicolas Mayoraz,Steffen Rendle,Dima Kuzmin,Emil Praun,Berivan Isik*

Main category: cs.LG

TL;DR: Successive Halving은 하이퍼파라미터 최적화를 위한 인기 있는 알고리즘이지만, 느린 시작자를 조기에 제거하는 단점이 있다. 본 연구에서는 Latent Kronecker Gaussian Processes 기반의 학습 곡선 예측이 이 문제를 극복할 수 있는지를 탐구하며, 다양한 신경망 아키텍처와 클릭 예측 데이터셋을 이용한 실증 연구를 수행했다.


<details>
  <summary>Details</summary>
Motivation: Successive Halving 알고리즘은 유망한 후보에게 더 많은 자원을 할당하지만, 중간 성능 값을 기반으로 한 자원 할당 결정이 느린 시작자를 조기에 제거하는 문제를 야기할 수 있다.

Method: Latent Kronecker Gaussian Processes를 기반으로 한 학습 곡선 예측을 통해 Successive Halving 알고리즘을 안내하는 접근법을 연구하였다.

Result: 예측 접근법은 경쟁력 있는 성능을 보였지만, 완전 관측된 학습 곡선을 훈련 데이터로 요구하기 때문에 표준 접근법에 비해 파레토 최적이 아니다.

Conclusion: 기존 학습 곡선 데이터를 활용함으로써 이 단점을 완화할 수 있다.

Abstract: Successive Halving is a popular algorithm for hyperparameter optimization
which allocates exponentially more resources to promising candidates. However,
the algorithm typically relies on intermediate performance values to make
resource allocation decisions, which can cause it to prematurely prune slow
starters that would eventually become the best candidate. We investigate
whether guiding Successive Halving with learning curve predictions based on
Latent Kronecker Gaussian Processes can overcome this limitation. In a
large-scale empirical study involving different neural network architectures
and a click prediction dataset, we compare this predictive approach to the
standard approach based on current performance values. Our experiments show
that, although the predictive approach achieves competitive performance, it is
not Pareto optimal compared to investing more resources into the standard
approach, because it requires fully observed learning curves as training data.
However, this downside could be mitigated by leveraging existing learning curve
data.

</details>


### [99] [On Defining Neural Averaging](https://arxiv.org/abs/2508.14832)
*Su Hyeong Lee,Richard Ngo*

Main category: cs.LG

TL;DR: 신경망의 평균화 문제를 다루며, 여러 사전 훈련된 모델로부터 단일 신경망을 합성하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사전 훈련된 모델 집합에서 훈련 데이터에 접근하지 않고 단일 신경망을 합성하는 문제를 조사한다.

Method: 신경 평균화에 대한 정의를 내리고, 이를 위해 모델 수프에서 통찰을 얻어 데이터 없는 메타 최적화 접근 방식인 아모타이즈드 모델 앙상블 (AME)을 제안한다.

Result: AME는 개별 전문가 및 모델 수프 기준선보다 우수한 평균 신경 솔루션을 생성하며, 특히 분포 외 환경에서 성능을 향상시킨다.

Conclusion: 데이터 없는 모델 가중치 집합의 원칙적이고 일반화 가능한 개념을 제안하고, 신경 평균화를 수행하는 방법을 정의한다.

Abstract: What does it even mean to average neural networks? We investigate the problem
of synthesizing a single neural network from a collection of pretrained models,
each trained on disjoint data shards, using only their final weights and no
access to training data. In forming a definition of neural averaging, we take
insight from model soup, which appears to aggregate multiple models into a
singular model while enhancing generalization performance. In this work, we
reinterpret model souping as a special case of a broader framework: Amortized
Model Ensembling (AME) for neural averaging, a data-free meta-optimization
approach that treats model differences as pseudogradients to guide neural
weight updates. We show that this perspective not only recovers model soup but
enables more expressive and adaptive ensembling strategies. Empirically, AME
produces averaged neural solutions that outperform both individual experts and
model soup baselines, especially in out-of-distribution settings. Our results
suggest a principled and generalizable notion of data-free model weight
aggregation and defines, in one sense, how to perform neural averaging.

</details>


### [100] [Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations](https://arxiv.org/abs/2508.14844)
*Murat Isik,Mandeep Kaur Saggi,Humaira Gowher,Sabre Kais*

Main category: cs.LG

TL;DR: 새로운 다중 모드 양자 기계 학습(QML) 프레임워크를 통해 효소 기능 예측 정확도를 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 효소의 기능을 정확하게 예측하는 것은 구조 주석이나 서열 유사성이 제한된 효소의 경우 특히 도전 과제이다.

Method: 단백질 서열 임베딩, 양자 유래 전자 기술자, 분자 그래프 구조, 2D 분자 이미지 표현의 네 가지 상호 보완적인 생화학적 모드를 통합하는 QVT 백본을 구축.

Result: 멀티모달 QVT 모델은 85.1%의 top-1 정확도를 달성해 서열만 사용한 기준 모델보다 상당한 차이로 뛰어나며, 다른 QML 모델에 비해 더 나은 성능 결과를 나타낸다.

Conclusion: 그래프 특징과 공간 패턴을 통합함으로써, 우리의 방법은 효소 기능 뒤에 있는 주요 입체전자 상호작용을 포착한다.

Abstract: Accurately predicting enzyme functionality remains one of the major
challenges in computational biology, particularly for enzymes with limited
structural annotations or sequence homology. We present a novel multimodal
Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC)
classification by integrating four complementary biochemical modalities:
protein sequence embeddings, quantum-derived electronic descriptors, molecular
graph structures, and 2D molecular image representations. Quantum Vision
Transformer (QVT) backbone equipped with modality-specific encoders and a
unified cross-attention fusion module. By integrating graph features and
spatial patterns, our method captures key stereoelectronic interactions behind
enzyme function. Experimental results demonstrate that our multimodal QVT model
achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a
substantial margin and achieving better performance results compared to other
QML models.

</details>


### [101] [Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent](https://arxiv.org/abs/2508.14853)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 본 논문에서는 프로프라이어터리 모델에 대한 jailbreak 공격을 위한 새로운 최적화 방법을 제안하며, 여러 LLM에서 높은 성공률과 빠른 수렴 속도를 달성한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 중요 응용 프로그램에 배포됨에 따라, 이들의 강인성과 안전성 보장이 주요한 도전 과제로 남아 있다.

Method: 우리는 Bregman 투영과 결합된 지수 기울기 하강법을 사용하여 적대적 접미사 토큰의 완화된 원-핫 인코딩을 직접 최적화하는 내재적 최적화 방법을 제안한다.

Result: 우리의 방법은 다섯 개의 오픈 소스 LLM 및 네 개의 적대적 행동 데이터셋을 기준으로 평가했을 때 세 가지 최첨단 기준보다 높은 성공률과 더 빠른 수렴을 달성했다.

Conclusion: 우리의 접근 방식은 여러 개별 프롬프트 공격뿐만 아니라, 다양한 프롬프트에 대해 효과적인 보편적 적대적 접미사를 생성하고 최적화된 접미사가 다른 LLM으로 전이될 수 있음을 입증한다.

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, ensuring their robustness and safety alignment remains a major
challenge. Despite the overall success of alignment techniques such as
reinforcement learning from human feedback (RLHF) on typical prompts, LLMs
remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers
appended to user prompts. Most existing jailbreak methods either rely on
inefficient searches over discrete token spaces or direct optimization of
continuous embeddings. While continuous embeddings can be given directly to
selected open-source models as input, doing so is not feasible for proprietary
models. On the other hand, projecting these embeddings back into valid discrete
tokens introduces additional complexity and often reduces attack effectiveness.
We propose an intrinsic optimization method which directly optimizes relaxed
one-hot encodings of the adversarial suffix tokens using exponentiated gradient
descent coupled with Bregman projection, ensuring that the optimized one-hot
encoding of each token always remains within the probability simplex. We
provide theoretical proof of convergence for our proposed method and implement
an efficient algorithm that effectively jailbreaks several widely used LLMs.
Our method achieves higher success rates and faster convergence compared to
three state-of-the-art baselines, evaluated on five open-source LLMs and four
adversarial behavior datasets curated for evaluating jailbreak methods. In
addition to individual prompt attacks, we also generate universal adversarial
suffixes effective across multiple prompts and demonstrate transferability of
optimized suffixes to different LLMs.

</details>


### [102] [Squeezed Diffusion Models](https://arxiv.org/abs/2508.14871)
*Jyotirmai Singh,Samar Khanna,James Burgess*

Main category: cs.LG

TL;DR: 이 논문에서는 데이터 구조를 무시하는 대신 훈련 분포의 주성분을 따라 불확실성을 분배하는 Squeezed Diffusion Models(SDM)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 양자 압축 상태가 하이젠베르크 불확실성 원리에 따라 불확실성을 재분배하는 방식을 동기부여로 삼습니다.

Method: 훈련 분포의 주성분을 따라 비등방성으로 잡음을 스케일링하는 SDM을 도입합니다.

Result: CIFAR-10/100 및 CelebA-64에서 소폭의 저압축은 FID를 최대 15% 개선하고 정밀도-재현율 경계를 높은 재현율 쪽으로 이동시킵니다.

Conclusion: 간단한 데이터 인식 잡음 형성이 구조적 변경 없이도 강력한 생성적 이득을 제공할 수 있음을 보여줍니다.

Abstract: Diffusion models typically inject isotropic Gaussian noise, disregarding
structure in the data. Motivated by the way quantum squeezed states
redistribute uncertainty according to the Heisenberg uncertainty principle, we
introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically
along the principal component of the training distribution. As squeezing
enhances the signal-to-noise ratio in physics, we hypothesize that scaling
noise in a data-dependent manner can better assist diffusion models in learning
important data features. We study two configurations: (i) a Heisenberg
diffusion model that compensates the scaling on the principal axis with inverse
scaling on orthogonal directions and (ii) a standard SDM variant that scales
only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,
mild antisqueezing - i.e. increasing variance on the principal axis -
consistently improves FID by up to 15% and shifts the precision-recall frontier
toward higher recall. Our results demonstrate that simple, data-aware noise
shaping can deliver robust generative gains without architectural changes.

</details>


### [103] [Compute-Optimal Scaling for Value-Based Deep RL](https://arxiv.org/abs/2508.14881)
*Preston Fu,Oleh Rybkin,Zhiyuan Zhou,Michal Nauman,Pieter Abbeel,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 이 논문은 온라인 가치 기반 심층 강화 학습에서의 컴퓨팅 최적화를 위한 자원 배분 방법을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 모델이 커지고 훈련이 비용이 증가함에 따라, 컴퓨팅 자원을 최적으로 사용할 수 있는 방법이 중요해진다.

Method: 모델 용량과 데이터 업데이트 비율(UTD) 두 가지 축에서 자원을 어떻게 배분하는지를 분석한다.

Result: 모델 크기, 배치 크기, UTD 사이의 복잡한 상호작용을 발견하고, 작은 모델의 경우 배치를 늘리면 Q-함수 정확도가 감소하는 TD-과적합 현상을 식별한다.

Conclusion: 이 논문은 심층 강화 학습에서 컴퓨팅 자원 최적화를 위한 배치 크기 및 UTD 선택에 대한 가이드라인을 제공한다.

Abstract: As models grow larger and training them becomes expensive, it becomes
increasingly important to scale training recipes not just to larger models and
more data, but to do so in a compute-optimal manner that extracts maximal
performance per unit of compute. While such scaling has been well studied for
language modeling, reinforcement learning (RL) has received less attention in
this regard. In this paper, we investigate compute scaling for online,
value-based deep RL. These methods present two primary axes for compute
allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed
compute budget, we ask: how should resources be partitioned across these axes
to maximize sample efficiency? Our analysis reveals a nuanced interplay between
model size, batch size, and UTD. In particular, we identify a phenomenon we
call TD-overfitting: increasing the batch quickly harms Q-function accuracy for
small models, but this effect is absent in large models, enabling effective use
of large batch size at scale. We provide a mental model for understanding this
phenomenon and build guidelines for choosing batch size and UTD to optimize
compute usage. Our findings provide a grounded starting point for
compute-optimal scaling in deep RL, mirroring studies in supervised learning
but adapted to TD learning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [104] [Special-Character Adversarial Attacks on Open-Source Language Model](https://arxiv.org/abs/2508.14070)
*Ephraiem Sarabamoun*

Main category: cs.CR

TL;DR: 대형 언어 모델은 다양한 자연어 처리 작업에서 뛰어난 성능을 보이나, 문자 수준의 적대적 조작에 취약해 실제 배치에 대한 보안 문제가 있다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 뛰어난 성능에도 불구하고, 실제 환경에서의 보안 문제를 해결하는 것이 필요하다.

Method: 이 논문에서는 문자 수준의 적대적 조작을 분석한다.

Result: 대형 언어 모델이 문자 수준의 공격에 얼마나 취약한지 설명한다.

Conclusion: 실제 배치에 있어 LLM의 보안 문제는 해결해야 할 중요한 과제이다.

Abstract: Large language models (LLMs) have achieved remarkable performance across
diverse natural language processing tasks, yet their vulnerability to
character-level adversarial manipulations presents significant security
challenges for real-world deployments.

</details>


### [105] [CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection](https://arxiv.org/abs/2508.14128)
*Jiaming Hu,Haoyu Wang,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: cs.CR

TL;DR: CCFC는 LLM의 취약성을 완화하기 위해 디자인된 두 가지 트랙의 프롬프트 레벨 방어 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 안전하게 배포하기 위해서는 탈옥 공격 문제를 해결해야 합니다.

Method: CCFC는 사용자의 쿼리의 의미적 코어를 몇 가지 샘플 프롬프트로 분리한 다음, 두 개의 보완적인 트랙을 사용하여 쿼리를 평가합니다.

Result: CCFC는 최신 방어기술 대비 공격 성공률을 50-75% 감소시킵니다.

Conclusion: 우리의 방법은 안정성과 응답 품질을 보장하며, 실용적이고 효과적인 LLM 배포 솔루션을 제공합니다.

Abstract: Jailbreak attacks pose a serious challenge to the safe deployment of large
language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a
dual-track, prompt-level defense framework designed to mitigate LLMs'
vulnerabilities from prompt injection and structure-aware jailbreak attacks.
CCFC operates by first isolating the semantic core of a user query via few-shot
prompting, and then evaluating the query using two complementary tracks: a
core-only track to ignore adversarial distractions (e.g., toxic suffixes or
prefix injections), and a core-full-core (CFC) track to disrupt the structural
patterns exploited by gradient-based or edit-based attacks. The final response
is selected based on a safety consistency check across both tracks, ensuring
robustness without compromising on response quality. We demonstrate that CCFC
cuts attack success rates by 50-75% versus state-of-the-art defenses against
strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on
benign queries. Our method consistently outperforms state-of-the-art
prompt-level defenses, offering a practical and effective solution for safer
LLM deployment.

</details>


### [106] [Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text](https://arxiv.org/abs/2508.14190)
*Zixin Rao,Youssef Mohamed,Shang Liu,Zeyan Liu*

Main category: cs.CR

TL;DR: 이 논문에서는 텍스트 탐지와 저작권 귀속을 동시에 해결하는 DA-MTL이라는 다중 작업 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 보안 및 무결성 문제에 대응하기 위한 새로운 접근 방식이 필요하다.

Method: DA-MTL은 텍스트 탐지와 저작권 귀속을 동시에 처리하는 다중 작업 학습 프레임워크이다.

Result: DA-MTL은 아홉 개 데이터셋과 네 개의 백본 모델에서 강력한 성능을 발휘하였다.

Conclusion: 이 프레임워크는 LLM의 행동과 탐지 및 저작권 귀속의 일반화에 대한 귀중한 통찰을 제공한다.

Abstract: Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated
remarkable abilities in generating natural language. However, they also pose
security and integrity challenges. Existing countermeasures primarily focus on
distinguishing AI-generated content from human-written text, with most
solutions tailored for English. Meanwhile, authorship attribution--determining
which specific LLM produced a given text--has received comparatively little
attention despite its importance in forensic analysis. In this paper, we
present DA-MTL, a multi-task learning framework that simultaneously addresses
both text detection and authorship attribution. We evaluate DA-MTL on nine
datasets and four backbone models, demonstrating its strong performance across
multiple languages and LLM sources. Our framework captures each task's unique
characteristics and shares insights between them, which boosts performance in
both tasks. Additionally, we conduct a thorough analysis of cross-modal and
cross-lingual patterns and assess the framework's robustness against
adversarial obfuscation techniques. Our findings offer valuable insights into
LLM behavior and the generalization of both detection and authorship
attribution.

</details>


### [107] [A Taxonomy and Methodology for Proof-of-Location Systems](https://arxiv.org/abs/2508.14230)
*Eduardo Brito,Fernando Castillo,Liina Kamm,Amnir Hadachi,Ulrich Norbisrath*

Main category: cs.CR

TL;DR: 디지털 사회에서 신뢰할 수 있는 물리적 존재 증명이 중요해지며, 본 논문은 다양한 응용 요구를 탐색하는 데 도움이 되는 PoL 시스템의 통합 프레임워크를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 디지털 서비스에서 신뢰할 수 있는 존재 증명의 필요성이 증가하고 있으나, 기존의 위치 확인 방법은 보안 취약점이 있습니다.

Method: PoL 시스템에 대한 통합 프레임워크를 제안하고, 네 가지 핵심 영역을 분류합니다: (1) 암호화 보증, (2) 시공간 동기화, (3) 신뢰 및 증인 모델, (4) 상호작용 및 오버헤드.

Result: 세 가지 사용 사례(소매 전자 쿠폰, 공급망 감사, 물리적 전자 투표)를 통해 서로 다른 제약이 프로토콜 선택에 미치는 영향을 보여줍니다.

Conclusion: 이 연구는 안전하고 확장 가능하며 상호 운용 가능한 PoL 시스템을 구축하기 위한 구조화된 접근 방식을 제공합니다.

Abstract: Digital societies increasingly rely on trustworthy proofs of physical
presence for services such as supply-chain tracking, e-voting, ride-sharing,
and location-based rewards. Yet, traditional localization methods often lack
cryptographic guarantees of where and when an entity was present, leaving them
vulnerable to spoofing, replay, or collusion attacks. In response, research on
Proof-of-Location (PoL) has emerged, with recent approaches combining distance
bounding, distributed consensus, and privacy-enhancing techniques to enable
verifiable, tamper-resistant location claims.
  As the design space for PoL systems grows in complexity, this paper provides
a unified framework to help practitioners navigate diverse application needs.
We first propose a taxonomy identifying four core domains: (1) cryptographic
guarantees, (2) spatio-temporal synchronization, (3) trust and witness models,
and (4) interaction and overhead. Building on this, we introduce a methodology
to map application-specific requirements onto appropriate PoL architectures. We
illustrate this process through three use cases (retail e-coupons, supply chain
auditing, and physical e-voting), each showing how different constraints shape
protocol choices. Overall, this work offers a structured approach to building
secure, scalable, and interoperable PoL systems.

</details>


### [108] [SaMOSA: Sandbox for Malware Orchestration and Side-Channel Analysis](https://arxiv.org/abs/2508.14261)
*Meet Udeshi,Venkata Sai Charan Putrevu,Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.CR

TL;DR: SaMOSA는 여러 사이드 채널을 캡처할 수 있는 모듈화된 리눅스 샌드박스입니다.


<details>
  <summary>Details</summary>
Motivation: OT 및 CPS 시스템의 리눅스 기반 임베디드 장치를 겨냥한 악성코드가 증가함에 따라, 포괄적인 악성코드 탐지를 위한 동적 분석의 필요성이 커졌습니다.

Method: SaMOSA 샌드박스는 네 가지 소스로부터 시간 동기화된 사이드 채널을 캡처할 수 있도록 리눅스 악성코드를 에뮬레이션합니다.

Result: 기존 리눅스 샌드박스보다 더 많은 사이드 채널을 캡처하며, 세 가지 아키텍처(x86-64, ARM64, PowerPC 64)를 지원합니다.

Conclusion: SaMOSA는 모듈화되고 사용자 정의 가능한 샌드박스 프레임워크를 제공하여 다양한 악성코드 분석 작업에 적응할 수 있도록 합니다.

Abstract: Cyber-attacks on operational technology (OT) and cyber-physical systems (CPS)
have increased tremendously in recent years with the proliferation of malware
targeting Linux-based embedded devices of OT and CPS systems. Comprehensive
malware detection requires dynamic analysis of execution behavior in addition
to static analysis of binaries. Safe execution of malware in a manner that
captures relevant behaviors via side-channels requires a sandbox environment.
Existing Linux sandboxes are built for specific tasks, only capture one or two
side-channels, and do not offer customization for different analysis tasks. We
present the SaMOSA Linux sandbox that allows emulation of Linux malwares while
capturing time-synchronized side-channels from four sources. SaMOSA
additionally provides emulation of network services via FakeNet, and allows
orchestration and customization of the sandbox environment via pipeline hooks.
In comparison to existing Linux sandboxes, SaMOSA captures more side-channels
namely system calls, network activity, disk activity, and hardware performance
counters. It supports three architectures predominantly used in OT and CPS
namely x86-64, ARM64, and PowerPC 64. SaMOSA fills a gap in Linux malware
analysis by providing a modular and customizable sandbox framework that can be
adapted for many malware analysis tasks. We present three case studies of three
different malware families to demonstrate the advantages of SaMOSA.

</details>


### [109] [MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing](https://arxiv.org/abs/2508.14300)
*Youssef Maklad,Fares Wael,Ali Hamdi,Wael Elsersy,Khaled Shaban*

Main category: cs.CR

TL;DR: MultiFuzz는 프로토콜 퍼징의 한계 극복을 위한 새로운 다중 에이전트 시스템으로, 신뢰성 높은 출력 및 구조적 결과물을 생성하여 퍼저의 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존 AFL 기반 시스템의 전통적인 프로토콜 퍼징 기술은 복잡한 프로토콜 문법에 대한 제한된 의미 이해와 엄격한 시드 변형 전략으로 인해 효과가 부족하다.

Method: MultiFuzz는 의미적 컨텍스트 검색, 특화된 에이전트, 구조화된 도구 지원 추론을 통합하여 프로토콜 문서의 에이전틱 청크를 이용해 벡터 데이터베이스에 임베딩을 구축하고, 검색 증가 생성(RAG) 파이프라인을 통해 에이전트가 더 신뢰할 수 있고 구조화된 결과를 생성할 수 있도록 한다.

Result: MultiFuzz는 실시간 스트리밍 프로토콜(RTSP) 실험 평가에서 NSFuzz, AFLNet 및 ChatAFL과 같은 최신 퍼저에 비해 브랜치 커버리지를 크게 향상시키고 더 깊은 프로토콜 상태 및 전이를 탐색한다.

Conclusion: MultiFuzz는 자율 프로토콜 퍼징의 새로운 패러다임을 제시하며, 지능형 에이전트 기반 퍼징 시스템의 향후 연구를 위한 확장 가능하고 유연한 기초를 제공한다.

Abstract: Traditional protocol fuzzing techniques, such as those employed by AFL-based
systems, often lack effectiveness due to a limited semantic understanding of
complex protocol grammars and rigid seed mutation strategies. Recent works,
such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol
fuzzing and address these limitations, pushing protocol fuzzers to wider
exploration of the protocol state space. But ChatAFL still faces issues like
unreliable output, LLM hallucinations, and assumptions of LLM knowledge about
protocol specifications. This paper introduces MultiFuzz, a novel dense
retrieval-based multi-agent system designed to overcome these limitations by
integrating semantic-aware context retrieval, specialized agents, and
structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of
protocol documentation (RFC Documents) to build embeddings in a vector database
for a retrieval-augmented generation (RAG) pipeline, enabling agents to
generate more reliable and structured outputs, enhancing the fuzzer in mutating
protocol messages with enhanced state coverage and adherence to syntactic
constraints. The framework decomposes the fuzzing process into modular groups
of agents that collaborate through chain-of-thought reasoning to dynamically
adapt fuzzing strategies based on the retrieved contextual knowledge.
Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate
that MultiFuzz significantly improves branch coverage and explores deeper
protocol states and transitions over state-of-the-art (SOTA) fuzzers such as
NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic
coordination, and language model reasoning, MultiFuzz establishes a new
paradigm in autonomous protocol fuzzing, offering a scalable and extensible
foundation for future research in intelligent agentic-based fuzzing systems.

</details>


### [110] [Differentially Private aggregate hints in mev-share](https://arxiv.org/abs/2508.14284)
*Jonathan Passerat-Palmbach,Sarisht Wadhwa*

Main category: cs.CR

TL;DR: 본 논문은 mev-share에서 사용자와 검색자 간의 정보 공유를 최적화하는 새로운 힌트 유형인 차등 개인화 집합 힌트를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 사용자는 정보를 withholding 하여 frontrunning을 방지하려고 하고, 검색자는 온체인 거래의 효율성과 수익성을 유지하기 위해 더 많은 정보가 필요하다.

Method: 논문에서는 차등 개인화(DP) 집합 힌트를 소개하며, 이는 사용자의 개인 정보 손실을 정량적으로 측정할 수 있게 한다.

Result: DP 집합 힌트를 통해 사용자는 검색자에게 요청할 리베이트 수준을 더 잘 추정할 수 있다.

Conclusion: 우리는 Trusted Curator Model에서 차등 개인화의 원리를 활용하여 집합 힌트를 설계하고, 무작위 샘플링이 sybil 공격으로부터 방어하고 사용자 개인 정보를 증대시키는 방법을 설명한다.

Abstract: Flashbots recently released mev-share to empower users with control over the
amount of information they share with searchers for extracting Maximal
Extractable Value (MEV). Searchers require more information to maintain
on-chain exchange efficiency and profitability, while users aim to prevent
frontrunning by withholding information. After analyzing two searching
strategies in mev-share to reason about searching techniques, this paper
introduces Differentially-Private (DP) aggregate hints as a new type of hints
to disclose information quantitatively. DP aggregate hints enable users to
formally quantify their privacy loss to searchers, and thus better estimate the
level of rebates to ask in return. The paper discusses the current properties
and privacy loss in mev-share and lays out how DP aggregate hints could enhance
the system for both users and searchers. We leverage Differential Privacy in
the Trusted Curator Model to design our aggregate hints. Additionally, we
explain how random sampling can defend against sybil attacks and amplify
overall user privacy while providing valuable hints to searchers for improved
backrunning extraction and frontrunning prevention.

</details>


### [111] [Precision over Noise: Tailoring S3 Public Access Detection to Reduce False Positives in Cloud Security Platforms](https://arxiv.org/abs/2508.14402)
*Dikshant,Geetika Verma*

Main category: cs.CR

TL;DR: 클라우드 보안 솔루션의 과도한 경고 생성은 분석가의 피로도를 높이고 운영 비효율성을 초래한다. 이 연구에서는 Amazon S3에서 생성된 허위 경고의 문제를 조사하며, 맞춤형 탐지 로직을 개발하여 허위 경고를 줄이는 해결책을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 보안 솔루션에 의해 생성되는 과도하고 허위 경고는 분석가의 피로와 운영 비효율의 주요 원인이다.

Method: Amazon S3에서 다양한 접근 구성으로 이루어진 1,000개 이상의 S3 버킷을 포함한 시뮬레이션 생산 테스트 환경에서 기본 규칙에 의해 생성된 경고의 80% 이상이 허위 경고로 분류되었으며, 맞춤 탐지 로직을 개발하여 기존 경고를 통합하고 보안 위험이 있는 버킷만을 표시하는 방식을 사용했다.

Result: 허위 경고가 크게 감소하였고, 경고의 정확성이 향상되며, 보안 분석가의 시간이 절약되었다.

Conclusion: 이 연구에서 제안하는 해결책은 클라우드 환경에서 보안 경고의 정확성을 향상시키는 데 유용하고 재현 가능하다는 것을 보여준다.

Abstract: Excessive and spurious alert generation by cloud security solutions is a root
cause of analyst fatigue and operational inefficiencies. In this study, the
long-standing issue of false positives from publicly accessible alerts in
Amazon S3, as generated by a licensed cloud-native security solution, is
examined. In a simulated production test environment, which consisted of over
1,000 Amazon S3 buckets with diverse access configurations, it was discovered
that over 80\% of the alerts generated by default rules were classified as
false positives, thus demonstrating the severity of the detection issue. This
severely impacted detection accuracy and generated a heavier workload for
analysts due to redundant manual triage efforts. For addressing this problem,
custom detection logic was created as an exercise of the native rule
customization capabilities of the solution. A unified titled ``S3 Public Access
Validation and Data Exposure'' was created in an effort to consolidate
different forms of alerts into one, context-aware logic that systematically
scans ACL configurations, bucket policies, indicators of public exposure, and
the presence of sensitive data, and then marks only those S3 buckets that
indeed denote security risk and are publicly exposed on the internet with no
authentication. The results demonstrate a significant reduction in false
positives, more precise alert fidelity, and significant time saving for
security analysts, thus demonstrating an actionable and reproducible solution
to enhance the accuracy of security alerting in compliance-focused cloud
environments.

</details>


### [112] [CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production](https://arxiv.org/abs/2508.14526)
*Stefan Lenz,David Schachtschneider,Simon Jonas,Liam Tirpitz,Sandra Geisler,Martin Henze*

Main category: cs.CR

TL;DR: 산업 공장의 디지털화가 생산에 큰 개선을 가져오지만, 사이버 공격에 취약하게 만들 수 있다. 이 연구에서는 전체 생산 라인을 복제한 CoFacS라는 시뮬레이션을 제시하여 사이버 공격에 대한 보안 연구를 위한 테스트베드를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 산업 네트워크를 사이버 공격으로부터 보호하기 위한 보안 조치를 연구하고 테스트하는 것이 중요하다.

Method: CoFacS는 전체 생산 라인을 복제하고, 실제 산업 응용 프로그램과의 통합을 가능하게 하는 최초의 완전한 공장 시뮬레이션이다.

Result: CoFacS는 물리적인 모델 공장과의 최대 편차가 0.11%로, 물리적 공격 및 네트워크 기반 사이버 공격의 영향을 연구할 수 있게 한다.

Conclusion: CoFacS는 공격 감지 및 신뢰성 있는 5G 기반 산업 통신의 방해 저항성에 관한 두 가지 사례 연구를 통해 보안 연구를 지원한다.

Abstract: While the digitization of industrial factories provides tremendous
improvements for the production of goods, it also renders such systems
vulnerable to serious cyber-attacks. To research, test, and validate security
measures protecting industrial networks against such cyber-attacks, the
security community relies on testbeds to simulate industrial systems, as
utilizing live systems endangers costly components or even human life. However,
existing testbeds focus on individual parts of typically complex production
lines in industrial factories. Consequently, the impact of cyber-attacks on
industrial networks as well as the effectiveness of countermeasures cannot be
evaluated in an end-to-end manner. To address this issue and facilitate
research on novel security mechanisms, we present CoFacS, the first COmplete
FACtory Simulation that replicates an entire production line and affords the
integration of real-life industrial applications. To showcase that CoFacS
accurately captures real-world behavior, we validate it against a physical
model factory widely used in security research. We show that CoFacS has a
maximum deviation of 0.11% to the physical reference, which enables us to study
the impact of physical attacks or network-based cyber-attacks. Moreover, we
highlight how CoFacS enables security research through two cases studies
surrounding attack detection and the resilience of 5G-based industrial
communication against jamming.

</details>


### [113] [DOPA: Stealthy and Generalizable Backdoor Attacks from a Single Client under Challenging Federated Constraints](https://arxiv.org/abs/2508.14530)
*Xuezheng Qin,Ruwei Huang,Xiaolong Tang,Feng Li*

Main category: cs.CR

TL;DR: DOPA는 비모수적, 분기 최적화 경로 공격을 통해 분산된 학습 환경에서 효과적인 백도어를 생성하는 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 개인 정보 보호를 유지하면서 협력적인 훈련을 하는데 점점 더 많이 사용되지만, 탈중앙화된 특성으로 인해 백도어 공격에 특히 취약하다.

Method: DOPA는 이질적인 지역 훈련 동역학을 시뮬레이션하고, 분기 최적화 경로 간의 합의를 추구하여 효과적이고 은밀한 백도어 트리거를 생성하는 프레임워크이다.

Result: DOPA는 12개의 방어 전략, 두 가지 모델 아키텍처(ResNet18/VGG16), 두 가지 데이터셋(CIFAR-10/TinyImageNet) 및 다양한 비 IID 환경에서 검증되었다.

Conclusion: DOPA는 높은 공격 성공률, 최소한의 정확도 저하, 낮은 실행 시간, 그리고 장기적 지속성을 달성하며, 연합 학습 시스템에서 강력한 방어 전략을 설계하기 위한 새로운 관점을 제공한다.

Abstract: Federated Learning (FL) is increasingly adopted for privacy-preserving
collaborative training, but its decentralized nature makes it particularly
susceptible to backdoor attacks. Existing attack methods, however, often rely
on idealized assumptions and fail to remain effective under real-world
constraints, such as limited attacker control, non-IID data distributions, and
the presence of diverse defense mechanisms. To address this gap, we propose
DOPA (Divergent Optimization Path Attack), a novel framework that simulates
heterogeneous local training dynamics and seeks consensus across divergent
optimization trajectories to craft universally effective and stealthy backdoor
triggers. By leveraging consistency signals across simulated paths to guide
optimization, DOPA overcomes the challenge of heterogeneity-induced instability
and achieves practical attack viability under stringent federated constraints.
We validate DOPA on a comprehensive suite of 12 defense strategies, two model
architectures (ResNet18/VGG16), two datasets (CIFAR-10/TinyImageNet), and both
mild and extreme non-IID settings. Despite operating under a single-client,
black-box, and sparsely participating threat model, DOPA consistently achieves
high attack success, minimal accuracy degradation, low runtime, and long-term
persistence. These results demonstrate a more practical attack paradigm,
offering new perspectives for designing robust defense strategies in federated
learning systems

</details>


### [114] [Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single Bootstrap per Cell](https://arxiv.org/abs/2508.14568)
*Wouter Legiest,Jan-Pieter D'Anvers,Bojan Spasic,Nam-Luc Tran,Ingrid Verbauwhede*

Main category: cs.CR

TL;DR: 이 논문은 완전 동형 암호화(FHE) 프레임워크 내에서 레벤슈타인(Edit) 거리를 계산하는 혁신적인 접근 방식을 제시하며, 특히 TFHE와 같은 3세대 스킴을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 편집 거리 계산은 DNA 시퀀스 정렬과 같은 금융 및 유전체학 전반의 애플리케이션에서 필수적이다.

Method: 이 논문에서는 Leuvenshtein이라는 알고리즘을 소개하여 편집 거리 계산의 비용을 크게 줄인다. 이 알고리즘은 계산의 각 셀당 필요한 프로그래머블 부트스트랩(PBS) 수를 기존의 Wagner-Fisher 알고리즘에서 약 94작업에서 1작업으로 줄인다. 또한, 효율적인 문자 일치 확인 방법을 제안하여 ASCII 문자 비교를 단 2개의 PBS 작업으로 줄인다.

Result: Leuvenshtein은 가장 좋은 TFHE 구현과 비교하여 최대 $278	imes$ 빠른 성능을 보이며, 최적화된 Wagner-Fisher 알고리즘 구현보다 최대 $39	imes$ 빠르다. 또한, 서버 측에 비암호화된 입력이 있을 경우 추가로 $3	imes$ 속도 증가를 이룰 수 있다.

Conclusion: 이 논문은 편집 거리 계산의 성능을 획기적으로 향상시키고, 비암호화된 문자열의 존재를 활용하여 추가적인 성능 개선 가능성을 탐구한다.

Abstract: This paper presents a novel approach to calculating the Levenshtein (edit)
distance within the framework of Fully Homomorphic Encryption (FHE),
specifically targeting third-generation schemes like TFHE. Edit distance
computations are essential in applications across finance and genomics, such as
DNA sequence alignment. We introduce an optimised algorithm that significantly
reduces the cost of edit distance calculations called Leuvenshtein. This
algorithm specifically reduces the number of programmable bootstraps (PBS)
needed per cell of the calculation, lowering it from approximately 94
operations -- required by the conventional Wagner-Fisher algorithm -- to just
1. Additionally, we propose an efficient method for performing equality checks
on characters, reducing ASCII character comparisons to only 2 PBS operations.
Finally, we explore the potential for further performance improvements by
utilising preprocessing when one of the input strings is unencrypted. Our
Leuvenshtein achieves up to $278\times$ faster performance compared to the best
available TFHE implementation and up to $39\times$ faster than an optimised
implementation of the Wagner-Fisher algorithm. Moreover, when offline
preprocessing is possible due to the presence of one unencrypted input on the
server side, an additional $3\times$ speedup can be achieved.

</details>


### [115] [Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud Detection](https://arxiv.org/abs/2508.14699)
*Jan Lum Fok,Qingwen Zeng,Shiping Chen,Oscar Fawkes,Huaming Chen*

Main category: cs.CR

TL;DR: 신용 카드 사기 탐지(CCFD)에 대한 머신 러닝의 견고성을 연구한 논문으로, 적대적 공격에 대한 민감성을 밝혀냈다.


<details>
  <summary>Details</summary>
Motivation: 금융 손실을 줄이기 위해 사기 거래를 정확히 식별하는 것이 필수적이다.

Method: 신용 카드 거래 데이터에 대해 블랙박스 및 화이트박스 적대적 공격 설정에서 기울기 기반 공격 방법을 적용했다.

Result: 표 데이터가 미세한 교란에 취약하다는 사실을 확인했고, 기울기 기반 공격으로부터의 적대적 샘플 전이 실험 또한 우리의 주장을 뒷받침했다.

Conclusion: 신용 카드 사기 탐지 알고리즘에 대한 강력한 방어 체계를 개발할 필요성이 강조된다.

Abstract: Credit card fraud detection (CCFD) is a critical application of Machine
Learning (ML) in the financial sector, where accurately identifying fraudulent
transactions is essential for mitigating financial losses. ML models have
demonstrated their effectiveness in fraud detection task, in particular with
the tabular dataset. While adversarial attacks have been extensively studied in
computer vision and deep learning, their impacts on the ML models, particularly
those trained on CCFD tabular datasets, remains largely unexplored. These
latent vulnerabilities pose significant threats to the security and stability
of the financial industry, especially in high-value transactions where losses
could be substantial. To address this gap, in this paper, we present a holistic
framework that investigate the robustness of CCFD ML model against adversarial
perturbations under different circumstances. Specifically, the gradient-based
attack methods are incorporated into the tabular credit card transaction data
in both black- and white-box adversarial attacks settings. Our findings confirm
that tabular data is also susceptible to subtle perturbations, highlighting the
need for heightened awareness among financial technology practitioners
regarding ML model security and trustworthiness. Furthermore, the experiments
by transferring adversarial samples from gradient-based attack method to
non-gradient-based models also verify our findings. Our results demonstrate
that such attacks remain effective, emphasizing the necessity of developing
robust defenses for CCFD algorithms.

</details>


### [116] [A Lightweight Incentive-Based Privacy-Preserving Smart Metering Protocol for Value-Added Services](https://arxiv.org/abs/2508.14703)
*Farid Zaredar,Morteza Amini*

Main category: cs.CR

TL;DR: 스마트 그리드와 고급 계량 인프라의 출현이 에너지 관리를 혁신했다. 본 논문에서는 개인정보를 보호하는 경량 스마트 미터링 프로토콜을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 그리드는 전통적인 전력망과 달리 양방향 통신을 통해 에너지 관리의 효율성을 높인다.

Method: 지역 차등 개인 정보 보호, 해시 체인, 블라인드 디지털 서명 등을 사용하여 소비자 정보를 안전하게 보고한다.

Result: 프로토콜은 약 0.51초에 실행되며 4.5MB의 메모리를 소모한다.

Conclusion: 이 프로토콜은 소비자의 개인 정보를 보호하면서도 데이터 유용성을 유지한다.

Abstract: The emergence of smart grids and advanced metering infrastructure (AMI) has
revolutionized energy management. Unlike traditional power grids, smart grids
benefit from two-way communication through AMI, which surpasses earlier
automated meter reading (AMR). AMI enables diverse demand- and supply-side
utilities such as accurate billing, outage detection, real-time grid control,
load forecasting, and value-added services. Smart meters play a key role by
delivering consumption values at predefined intervals to the utility provider
(UP). However, such reports may raise privacy concerns, as adversaries can
infer lifestyle patterns, political orientations, and the types of electrical
devices in a household, or even sell the data to third parties (TP) such as
insurers. In this paper, we propose a lightweight, privacy-preserving smart
metering protocol for incentive-based value-added services. The scheme employs
local differential privacy, hash chains, blind digital signatures, pseudonyms,
temporal aggregation, and anonymous overlay networks to report coarse-grained
values with adjustable granularity to the UP. This protects consumers' privacy
while preserving data utility. The scheme prevents identity disclosure while
enabling automatic token redemption. From a performance perspective, our
results show that with a 1024-bit RSA key, a 7-day duration, and four reports
per day, our protocol runs in approximately 0.51s and consumes about 4.5 MB of
memory. From a privacy perspective, the protocol resists semi-trusted and
untrusted adversaries.

</details>


### [117] [A Collusion-Resistance Privacy-Preserving Smart Metering Protocol for Operational Utility](https://arxiv.org/abs/2508.14744)
*Farid Zaredar,Morteza Amini*

Main category: cs.CR

TL;DR: 정전력 계량 인프라를 통해 전통적인 전력망보다 더 나은 신뢰성, 에너지 효율성, 보안을 제공하는 스마트 미터링 프로토콜을 제안. 이 프로토콜은 개인 정보를 보호하면서도 운영 서비스를 위한 효과적인 집계 기능을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 미터가 제공하는 소비 데이터로 사용자 프라이버시가 위협받는 문제 해결.

Method: 부분 가산 동형 암호화, 집계, 데이터 섞기 및 데이터 최소화를 활용한 집계 프로토콜 제안.

Result: 1024비트 키 크기에서 총 실행 시간이 약 2.21초로 성능과 개인 정보 보호를 모두 평가하였다.

Conclusion: 제안된 프로토콜이 개인 정보 보호 기술을 효과적으로 사용하면서도 효율성을 입증하였다.

Abstract: Modern grids have adopted advanced metering infrastructure (AMI) to
facilitate bidirectional communication between smart meters and control
centers. This enables smart meters to report consumption values at predefined
intervals to utility providers for purposes including demand balancing, load
forecasting, dynamic billing, and operational efficiency. Compared to
traditional power grids, smart grids offer advantages such as enhanced
reliability, improved energy efficiency, and increased security. However,
utility providers can compromise user privacy by analyzing fine-grained
readings and extracting individuals' daily activities from this time-series
data. To address this concern, we propose a collusion-resistant,
privacy-preserving aggregation protocol for smart metering in operational
services. Our protocol ensures privacy by leveraging techniques such as
partially additive homomorphic encryption, aggregation, data perturbation, and
data minimization. The scheme aggregates perturbed readings using the additive
homomorphic property of the Paillier cryptosystem to provide results for
multiple operational purposes. We evaluate the protocol in terms of both
performance and privacy. Computational, memory, and communication overhead were
examined. The total execution time with 1024-bit key size is about 2.21
seconds. We also evaluated privacy through the normalized conditional entropy
(NCE) metric. Higher NCE values, closer to 1, indicate stronger privacy. By
increasing noise scale, the NCE value rises, showing perturbed values retain
minimal information about the original, thereby reducing risks. Overall,
evaluation demonstrates the protocol's efficiency while employing various
privacy-preserving techniques.

</details>


### [118] [A Guide to Stakeholder Analysis for Cybersecurity Researchers](https://arxiv.org/abs/2508.14796)
*James C Davis,Sophie Chen,Huiyun Peng,Paschal C Amusuo,Kelechi G Kalu*

Main category: cs.CR

TL;DR: 이 논문은 사이버 보안 연구를 위한 이해관계자 기반의 윤리 분석에 대한 실용적인 지원을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안 연구자들이 능력을 제공하는 것을 넘어 잠재적인 해를 예상하고 완화해야 한다는 합의가 커지고 있습니다.

Method: 이 가이드는 이해관계자 유형을 나열하고 이를 일반적인 경험적 연구 방법과 매핑하여 윤리 분석을 수행하는 데 도움을 줍니다.

Result: 연구자들은 실제 프로젝트에서 이해관계자의 노출 가능성을 식별할 수 있는 작업 예제를 제공합니다.

Conclusion: 이 연구는 연구 팀들이 새롭게 요구되는 윤리 기준을 자신감 있게 충족할 수 있도록 돕기 위한 것입니다.

Abstract: Stakeholder-based ethics analysis is now a formal requirement for submissions
to top cybersecurity research venues. This requirement reflects a growing
consensus that cybersecurity researchers must go beyond providing capabilities
to anticipating and mitigating the potential harms thereof. However, many
cybersecurity researchers may be uncertain about how to proceed in an ethics
analysis. In this guide, we provide practical support for that requirement by
enumerating stakeholder types and mapping them to common empirical research
methods. We also offer worked examples to demonstrate how researchers can
identify likely stakeholder exposures in real-world projects. Our goal is to
help research teams meet new ethics mandates with confidence and clarity, not
confusion.

</details>


### [119] [A Lightweight Privacy-Preserving Smart Metering Billing Protocol with Dynamic Tariff Policy Adjustment](https://arxiv.org/abs/2508.14815)
*Farid Zaredar,Morteza Amini*

Main category: cs.CR

TL;DR: ICT와 전통적인 전력망의 통합으로 스마트 그리드가 등장하였으며, 고급 계량 인프라가 쌍방향 통신과 소비자 데이터 보호를 위한 경량화된 프로토콜을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 그리드의 필요성과 소비자 프라이버시 우려 해결의 중요성.

Method: 경량 프라이버시 보호 스마트 계량 프로토콜을 제안하고, 데이터 교란 기술을 사용하여 정확한 에너지 사용 데이터를 보호한다.

Result: 우리의 프로토콜은 실시간 요금 청구 서비스를 지원하며, 연간 백업 시스템의 실행 시간은 약 3.945 초로 나타난다.

Conclusion: 제안된 프로토콜은 프라이버시 메트릭과 노이즈 스케일 증가를 통해 사용자 프라이버시를 효과적으로 보호할 수 있다.

Abstract: The integration of information and communication technology (ICT) with
traditional power grids has led to the emergence of smart grids. Advanced
metering infrastructure (AMI) plays a crucial role in smart grids by
facilitating two-way communication between smart meters and the utility
provider. This bidirectional communication allows intelligent meters to report
fine-grained consumption data at predefined intervals, enabling accurate
billing, efficient grid monitoring and management, and rapid outage detection.
However, the collection of detailed consumption data can inadvertently disclose
consumers' daily activities, raising privacy concerns and potentially leading
to privacy violations. To address these issues and preserve individuals'
privacy, we propose a lightweight privacy-preserving smart metering protocol
specifically designed to support real-time tariff billing service with dynamic
policy adjustment. Our scheme employs an efficient data perturbation technique
to obscure precise energy usage data from internal adversaries, including the
intermediary gateways and the utility provider. Subsequently, we validate the
efficiency and security of our protocol through comprehensive performance and
privacy evaluations. We examined the computational, memory, and communication
overhead of the proposed scheme. The execution time of our secure and
privacy-aware billing system is approximately 3.94540 seconds for a complete
year. Furthermore, we employed the Jensen-Shannon divergence as a privacy
metric to demonstrate that our protocol can effectively safeguard users'
privacy by increasing the noise scale.

</details>
