{"id": "2510.21738", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21738", "abs": "https://arxiv.org/abs/2510.21738", "authors": ["Yifan Bai", "Shruti Kotpalliwar", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "Collaborative Task Assignment, Sequencing and Multi-agent Path-finding", "comment": null, "summary": "In this article, we address the problem of collaborative task assignment,\nsequencing, and multi-agent pathfinding (TSPF), where a team of agents must\nvisit a set of task locations without collisions while minimizing flowtime.\nTSPF incorporates agent-task compatibility constraints and ensures that all\ntasks are completed. We propose a Conflict-Based Search with Task Sequencing\n(CBS-TS), an optimal and complete algorithm that alternates between finding new\ntask sequences and resolving conflicts in the paths of current sequences.\nCBS-TS uses a mixed-integer linear program (MILP) to optimize task sequencing\nand employs Conflict-Based Search (CBS) with Multi-Label A* (MLA*) for\ncollision-free path planning within a search forest. By invoking MILP for the\nnext-best sequence only when needed, CBS-TS efficiently limits the search\nspace, enhancing computational efficiency while maintaining optimality. We\ncompare the performance of our CBS-TS against Conflict-based Steiner Search\n(CBSS), a baseline method that, with minor modifications, can address the TSPF\nproblem. Experimental results demonstrate that CBS-TS outperforms CBSS in most\ntesting scenarios, achieving higher success rates and consistently optimal\nsolutions, whereas CBSS achieves near-optimal solutions in some cases. The\nsupplementary video is available at https://youtu.be/QT8BYgvefmU."}
{"id": "2510.22222", "categories": ["cs.MA", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.22222", "abs": "https://arxiv.org/abs/2510.22222", "authors": ["Yumeng Shi", "Zhongliang Yang", "Yisi Wang", "Linna Zhou"], "title": "CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating", "comment": "8 pages, 2 figures", "summary": "In the domain of corporate credit rating, traditional deep learning methods\nhave improved predictive accuracy but still suffer from the inherent\n'black-box' problem and limited interpretability. While incorporating\nnon-financial information enriches the data and provides partial\ninterpretability, the models still lack hierarchical reasoning mechanisms,\nlimiting their comprehensive analytical capabilities. To address these\nchallenges, we propose CreditXAI, a Multi-Agent System (MAS) framework that\nsimulates the collaborative decision-making process of professional credit\nanalysts. The framework focuses on business, financial, and governance risk\ndimensions to generate consistent and interpretable credit assessments.\nExperimental results demonstrate that multi-agent collaboration improves\npredictive accuracy by more than 7% over the best single-agent baseline,\nconfirming its significant synergistic advantage in corporate credit risk\nevaluation. This study provides a new technical pathway to build intelligent\nand interpretable credit rating models."}
{"id": "2510.22235", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22235", "abs": "https://arxiv.org/abs/2510.22235", "authors": ["Yixiao Nie", "Yang Zhang", "Yingjie Jin", "Zhepeng Wang", "Xiu Li", "Xiang Li"], "title": "CGoT: A Novel Inference Mechanism for Embodied Multi-Agent Systems Using Composable Graphs of Thoughts", "comment": null, "summary": "The integration of self-driving cars and service robots is becoming\nincreasingly prevalent across a wide array of fields, playing a crucial and\nexpanding role in both industrial applications and everyday life. In parallel,\nthe rapid advancements in Large Language Models (LLMs) have garnered\nsubstantial attention and interest within the research community. This paper\nintroduces a novel vehicle-robot system that leverages the strengths of both\nautonomous vehicles and service robots. In our proposed system, two autonomous\nego-vehicles transports service robots to locations within an office park,\nwhere they perform a series of tasks. The study explores the feasibility and\npotential benefits of incorporating LLMs into this system, with the aim of\nenhancing operational efficiency and maximizing the potential of the\ncooperative mechanisms between the vehicles and the robots. This paper proposes\na novel inference mechanism which is called CGOT toward this type of system\nwhere an agent can carry another agent. Experimental results are presented to\nvalidate the performance of the proposed method."}
{"id": "2510.22320", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22320", "abs": "https://arxiv.org/abs/2510.22320", "authors": ["Yanqing Fu", "Chenrun Wang", "Chao Huang", "Zhuping Wang"], "title": "IFS: Information Flow Structure for Multi-agent Ad Hoc System", "comment": null, "summary": "Multi-agent ad hoc systems are dynamic collaborative systems in which\nmultiple autonomous agents must cooperate with both known and unknown teammates\nin open environments, without relying on pre-coordinated strategies. These\nsystems operate under conditions of uncertainty and partial observability,\nwhere team composition, agent behaviors, and environmental factors may change\nduring execution. Through an analysis of information flow in such systems, we\nidentify two key limitations in existing research: insufficient information\nflow and limited information processing capacity. To address these issues, we\npropose an information flow structure for multi-agent ad hoc systems (IFS),\nwhich tackles these challenges from the perspectives of communication and\ninformation fusion. Experimental results in StarCraft II demonstrate that IFS\nsignificantly improves both information flow and processing capacity, while\nexhibiting strong generalization capabilities and outperforming baseline\nmethods in complex ad hoc teamwork scenarios."}
{"id": "2510.21721", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21721", "abs": "https://arxiv.org/abs/2510.21721", "authors": ["Kentaro Ueda", "Takehiro Takayanagi"], "title": "PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation", "comment": null, "summary": "While recent advances in Large Language Models (LLMs) have improved the\nquality of creative text generation, significant challenges remain in producing\npersonalized stories that reflect individual user preferences. Conventional\napproaches rely on explicit feedback or fine-tuning, which presents practical\nissues regarding user burden, data collection, computational costs, and\nprivacy. In this work, we propose PREFINE (Persona-and-Rubric Guided\nCritique-and-Refine), a novel framework that extends the Critique-and-Refine\nparadigm to personalization. PREFINE constructs a pseudo-user agent from a\nuser's interaction history and generates user-specific rubrics (evaluation\ncriteria). By having this agent critique and refine outputs on the user's\nbehalf based on these tailored rubrics, our method achieves personalized\ngeneration without requiring parameter updates or direct user feedback. We\nconducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.\nWe designed three baseline methods and several model variants to verify the\ncontribution of each component of our framework. In automatic evaluations\n(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically\nsignificant scores than the baselines, without compromising general story\nquality. Analysis of the model variants confirmed that both the pseudo-user\nagent and the user-specific rubrics are crucial for enhancing personalization\nperformance. Beyond story generation, our approach holds potential for enabling\nefficient personalization in broader applications, such as dialogue systems,\neducation, and recommendation."}
{"id": "2510.22396", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22396", "abs": "https://arxiv.org/abs/2510.22396", "authors": ["Zhaoyang Li", "Zheng Yu", "Jingyi Song", "Meng Xu", "Yuxuan Luo", "Dongliang Mu"], "title": "PortGPT: Towards Automated Backporting Using Large Language Models", "comment": "Accepted by IEEE S&P 2026", "summary": "Patch backporting, the process of migrating mainline security patches to\nolder branches, is an essential task in maintaining popular open-source\nprojects (e.g., Linux kernel). However, manual backporting can be\nlabor-intensive, while existing automated methods, which heavily rely on\npredefined syntax or semantic rules, often lack agility for complex patches.\n  In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation\nof patch backporting in real-world scenarios. PORTGPT enhances an LLM with\ntools to access code on-demand, summarize Git history, and revise patches\nautonomously based on feedback (e.g., from compilers), hence, simulating\nhuman-like reasoning and verification. PORTGPT achieved an 89.15% success rate\non existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex\ncases, both outperforms state-of-the-art of backporting tools. We contributed 9\nbackported patches from PORTGPT to the Linux kernel community and all patches\nare now merged."}
{"id": "2510.21770", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.21770", "abs": "https://arxiv.org/abs/2510.21770", "authors": ["Jinwoo Baek"], "title": "Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability", "comment": "15 pages", "summary": "Transformers trained in low precision can suffer forward-error amplification.\nWe give a first-order, module-wise theory that predicts when and where errors\ngrow. For self-attention we derive a per-layer bound that factorizes into three\ninterpretable diagnostics: a score-scale ratio $\\kappa_{\\rm score}$, a rowwise\nsoftmax sensitivity $\\kappa_{\\rm softmax}$, and value conditioning $\\kappa(V)$.\nWe prove a residual relaxation inequality showing that residual blocks\nattenuate depth-wise accumulation, and we introduce a precision- and\nwidth-aware LayerNorm indicator $\\rho_{\\rm LN}$ with a matching first-order\nbound in the $\\epsilon$-dominated regime. These pieces yield a unified\nforward-stability bound whose right-hand side is directly estimable during\ntraining.\n  On Tiny-ViT/CIFAR-10 we evaluate the bound and components. (1) The combined\npredictor $\\kappa_{\\rm softmax},(1+\\kappa_{\\rm\nscore}),\\kappa(V),|W_O|2+\\kappa{\\rm eff}+C_{\\rm LN}$ tracks\nFP32$\\leftrightarrow$LP mismatches across seeds, widths, and precisions;\nscaling by $\\epsilon_{\\rm mach}$ collapses mixed-precision points. (2) The\ntime-series maximum of $\\kappa_{\\rm softmax}$ acts as an early-warning signal,\nleading error spikes by 16-24 steps (corr. 0.65-0.82; permutation\n$p!\\approx!10^{-3}$; Precision@K 0.89-1.00). (3) Guided by $\\rho_{\\rm LN}$, a\nsmall LayerNorm-$\\epsilon$ tweak targeting $\\rho_\\star$ gives consistent\nstabilization (mean tail-loss $\\downarrow\\ \\approx0.010$ at $\\rho_\\star!=!0.6$,\ncap$=10^{-2}$) with negligible overhead.\n  Overall, our theory supplies actionable, unitless diagnostics that (i)\nexplain when self-attention is fragile, (ii) forecast instability, and (iii)\nmotivate a minimally invasive mitigation."}
{"id": "2510.22422", "categories": ["cs.MA", "cs.AI", "cs.CY", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.22422", "abs": "https://arxiv.org/abs/2510.22422", "authors": ["Ariel Flint", "Luca Maria Aiello", "Romualdo Pastor-Satorras", "Andrea Baronchelli"], "title": "Group size effects and collective misalignment in LLM multi-agent systems", "comment": null, "summary": "Multi-agent systems of large language models (LLMs) are rapidly expanding\nacross domains, introducing dynamics not captured by single-agent evaluations.\nYet, existing work has mostly contrasted the behavior of a single agent with\nthat of a collective of fixed size, leaving open a central question: how does\ngroup size shape dynamics? Here, we move beyond this dichotomy and\nsystematically explore outcomes across the full range of group sizes. We focus\non multi-agent misalignment, building on recent evidence that interacting LLMs\nplaying a simple coordination game can generate collective biases absent in\nindividual models. First, we show that collective bias is a deeper phenomenon\nthan previously assessed: interaction can amplify individual biases, introduce\nnew ones, or override model-level preferences. Second, we demonstrate that\ngroup size affects the dynamics in a non-linear way, revealing model-dependent\ndynamical regimes. Finally, we develop a mean-field analytical approach and\nshow that, above a critical population size, simulations converge to\ndeterministic predictions that expose the basins of attraction of competing\nequilibria. These findings establish group size as a key driver of multi-agent\ndynamics and highlight the need to consider population-level effects when\ndeploying LLM-based systems at scale."}
{"id": "2510.21855", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.21855", "abs": "https://arxiv.org/abs/2510.21855", "authors": ["Ryan Zhang", "Herbert Woisetscläger"], "title": "SIGN: Schema-Induced Games for Naming", "comment": "AAAI 2026 Student Abstract (Oral). Code available ar\n  https://github.com/ryanzhangofficial/schema-induced-games-for-naming", "summary": "Real-world AI systems are tackling increasingly complex problems, often\nthrough interactions among large language model (LLM) agents. When these agents\ndevelop inconsistent conventions, coordination can break down. Applications\nsuch as collaborative coding and distributed planning therefore require\nreliable, consistent communication, and scalability is a central concern as\nsystems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming\ngame that examines how lightweight structure can steer convention formation. We\ncompare schema-induced communication to unconstrained natural language and find\nfaster convergence with up to 5.8x higher agreement. These results suggest that\nminimal structure can act as a simple control knob for efficient multi-agent\ncoordination, pointing toward broader applications beyond the naming game."}
{"id": "2510.22620", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22620", "abs": "https://arxiv.org/abs/2510.22620", "authors": ["Julia Bazinska", "Max Mathys", "Francesco Casucci", "Mateo Rojas-Carulla", "Xander Davies", "Alexandra Souly", "Niklas Pfister"], "title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents", "comment": "Julia Bazinska and Max Mathys contributed equally", "summary": "AI agents powered by large language models (LLMs) are being deployed at\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\naffects agent security. The non-deterministic sequential nature of AI agents\ncomplicates security modeling, while the integration of traditional software\nwith AI components entangles novel LLM vulnerabilities with conventional\nsecurity risks. Existing frameworks only partially address these challenges as\nthey either capture specific vulnerabilities only or require modeling of\ncomplete agents. To address these limitations, we introduce threat snapshots: a\nframework that isolates specific states in an agent's execution flow where LLM\nvulnerabilities manifest, enabling the systematic identification and\ncategorization of security risks that propagate from the LLM to the agent\nlevel. We apply this framework to construct the $\\operatorname{b}^3$ benchmark,\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\nenhanced reasoning capabilities improve security, while model size does not\ncorrelate with security. We release our benchmark, dataset, and evaluation code\nto facilitate widespread adoption by LLM providers and practitioners, offering\nguidance for agent developers and incentivizing model developers to prioritize\nbackbone security improvements."}
{"id": "2510.21788", "categories": ["cs.LG", "cs.AI", "I.2; G.3"], "pdf": "https://arxiv.org/pdf/2510.21788", "abs": "https://arxiv.org/abs/2510.21788", "authors": ["Larkin Liu", "Jalal Etesami"], "title": "Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "We explore the use of expert-guided bandit learning, which we refer to as\nonline mixture-of-experts (OMoE). In this setting, given a context, a candidate\ncommittee of experts must determine how to aggregate their outputs to achieve\noptimal results in terms of aggregate accuracy. We propose two algorithms to\naddress this problem. The first algorithm combines aggregate voting with\nUCB-driven successive elimination, efficiently pruning suboptimal exploration\nactions. The second algorithm employs an online weighted-majority-voting\nmechanism, leveraging the respective voting power of each expert proportional\nto their predictive power. We derive theoretical guarantees for the regret\nproperties in the bandit setting under ideal circumstances, and empirical\nresults are provided accordingly. As a modern study on applications, these\nmethods are applied to the online fine-tuning of a set of expert large language\nmodels (LLMs), where after each response, the generative LLM dynamically\nreweighs its set of experts and/or selects the optimal committee of experts to\ngenerate the most accurate response. Our results introduce new methodologies\nand no-regret guarantees for combining multiple experts to improve on the\nperformance of the an aggregate model overall."}
{"id": "2510.22431", "categories": ["cs.MA", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22431", "abs": "https://arxiv.org/abs/2510.22431", "authors": ["Zheng Wei", "Mingchen Li", "Zeqian Zhang", "Ruibin Yuan", "Pan Hui", "Huamin Qu", "James Evans", "Maneesh Agrawala", "Anyi Rao"], "title": "Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration", "comment": null, "summary": "Recent advancements in multi-agent systems have demonstrated significant\npotential for enhancing creative task performance, such as long video\ngeneration. This study introduces three innovations to improve multi-agent\ncollaboration. First, we propose OmniAgent, a hierarchical, graph-based\nmulti-agent framework for long video generation that leverages a\nfilm-production-inspired architecture to enable modular specialization and\nscalable inter-agent collaboration. Second, inspired by context engineering, we\npropose hypergraph nodes that enable temporary group discussions among agents\nlacking sufficient context, reducing individual memory requirements while\nensuring adequate contextual information. Third, we transition from directed\nacyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing\nagents to reflect and refine outputs iteratively, thereby improving earlier\nstages through feedback from subsequent nodes. These contributions lay the\ngroundwork for developing more robust multi-agent systems in creative tasks."}
{"id": "2510.22009", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22009", "abs": "https://arxiv.org/abs/2510.22009", "authors": ["Yangqin Jiang", "Chao Huang"], "title": "LightAgent: Mobile Agentic Foundation Models", "comment": null, "summary": "With the advancement of multimodal large language models (MLLMs), building\nGUI agent systems has become an increasingly promising direction-especially for\nmobile platforms, given their rich app ecosystems and intuitive touch\ninteractions. Yet mobile GUI agents face a critical dilemma: truly on-device\nmodels (4B or smaller) lack sufficient performance, while capable models\n(starting from 7B) are either too large for mobile deployment or prohibitively\ncostly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose\nLightAgent, a mobile agentic foundation model solution that leverages\ndevice-cloud collaboration to tap the cost-efficiency of on-device models and\nthe high capability of cloud models, while avoiding their drawbacks.\nSpecifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO\ntraining on synthetic GUI data for strong decision-making, integrates an\nefficient long-reasoning mechanism to utilize historical interactions under\ntight resources, and defaults to on-device execution-only escalating\nchallenging subtasks to the cloud via real-time complexity assessment.\nExperiments on the online AndroidLab benchmark and diverse apps show LightAgent\nmatches or nears larger models, with a significant reduction in cloud costs."}
{"id": "2510.22726", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.22726", "abs": "https://arxiv.org/abs/2510.22726", "authors": ["Van Le", "Tan Le"], "title": "SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking", "comment": null, "summary": "SpoofTrackBench is a reproducible, modular benchmark for evaluating\nadversarial robustness in real-time localization and tracking (RTLS) systems\nunder radar spoofing. Leveraging the Hampton University Skyler Radar Sensor\ndataset, we simulate drift, ghost, and mirror-type spoofing attacks and\nevaluate tracker performance using both Joint Probabilistic Data Association\n(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates\nclean and spoofed detection streams, visualizes spoof-induced trajectory\ndivergence, and quantifies assignment errors via direct drift-from-truth\nmetrics. Clustering overlays, injection-aware timelines, and scenario-adaptive\nvisualizations enable interpretability across spoof types and configurations.\nEvaluation figures and logs are auto-exported for reproducible comparison.\nSpoofTrackBench sets a new standard for open, ethical benchmarking of\nspoof-aware tracking pipelines, enabling rigorous cross-architecture analysis\nand community validation."}
{"id": "2510.21935", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21935", "abs": "https://arxiv.org/abs/2510.21935", "authors": ["Samuel Bright-Thonney", "Christina Reissel", "Gaia Grosso", "Nathaniel Woodward", "Katya Govorkova", "Andrzej Novak", "Sang Eon Park", "Eric Moreno", "Philip Harris"], "title": "AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing", "comment": "Accepted at NeurIPS 2025; 32 pages, 16 figures", "summary": "Novelty detection in large scientific datasets faces two key challenges: the\nnoisy and high-dimensional nature of experimental data, and the necessity of\nmaking statistically robust statements about any observed outliers. While there\nis a wealth of literature on anomaly detection via dimensionality reduction,\nmost methods do not produce outputs compatible with quantifiable claims of\nscientific discovery. In this work we directly address these challenges,\npresenting the first step towards a unified pipeline for novelty detection\nadapted for the rigorous statistical demands of science. We introduce\nAutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive\nTesting), a general-purpose pipeline for detecting novelty in scientific data.\nAutoSciDACT begins by creating expressive low-dimensional data representations\nusing a contrastive pre-training, leveraging the abundance of high-quality\nsimulated data in many scientific domains alongside expertise that can guide\nprincipled data augmentation strategies. These compact embeddings then enable\nan extremely sensitive machine learning-based two-sample test using the New\nPhysics Learning Machine (NPLM) framework, which identifies and statistically\nquantifies deviations in observed data relative to a reference distribution\n(null hypothesis). We perform experiments across a range of astronomical,\nphysical, biological, image, and synthetic datasets, demonstrating strong\nsensitivity to small injections of anomalous data across all domains."}
{"id": "2510.22477", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22477", "abs": "https://arxiv.org/abs/2510.22477", "authors": ["Yijia Fan", "Jusheng Zhang", "Jing Yang", "Keze Wang"], "title": "Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization", "comment": null, "summary": "To combat the prohibitive communication costs of ``free-for-all\" multi-agent\nsystems (MAS), we introduce \\textbf{Agent-GSPO}, a framework that directly\noptimizes for token economy using sequence-level reinforcement learning.\nAgent-GSPO leverages the stable and memory-efficient Group Sequence Policy\nOptimization (GSPO) algorithm to train agents on a communication-aware reward\nthat explicitly penalizes verbosity. Across seven reasoning benchmarks,\nAgent-GSPO not only achieves new state-of-the-art performance but does so with\na fraction of the token consumption of existing methods. By fostering emergent\nstrategies like ``strategic silence,\" our approach provides a practical\nblueprint for developing scalable and economically viable multi-agent systems."}
{"id": "2510.22039", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.22039", "abs": "https://arxiv.org/abs/2510.22039", "authors": ["Po-Chen Kuo", "Han Hou", "Will Dabney", "Edgar Y. Walker"], "title": "Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability", "comment": "Accepted to Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2025", "summary": "Learning a compact representation of history is critical for planning and\ngeneralization in partially observable environments. While meta-reinforcement\nlearning (RL) agents can attain near Bayes-optimal policies, they often fail to\nlearn the compact, interpretable Bayes-optimal belief states. This\nrepresentational inefficiency potentially limits the agent's adaptability and\ngeneralization capacity. Inspired by predictive coding in neuroscience--which\nsuggests that the brain predicts sensory inputs as a neural implementation of\nBayesian inference--and by auxiliary predictive objectives in deep RL, we\ninvestigate whether integrating self-supervised predictive coding modules into\nmeta-RL can facilitate learning of Bayes-optimal representations. Through state\nmachine simulation, we show that meta-RL with predictive modules consistently\ngenerates more interpretable representations that better approximate\nBayes-optimal belief states compared to conventional meta-RL across a wide\nvariety of tasks, even when both achieve optimal policies. In challenging tasks\nrequiring active information seeking, only meta-RL with predictive modules\nsuccessfully learns optimal representations and policies, whereas conventional\nmeta-RL struggles with inadequate representation learning. Finally, we\ndemonstrate that better representation learning leads to improved\ngeneralization. Our results strongly suggest the role of predictive learning as\na guiding principle for effective representation learning in agents navigating\npartial observability."}
{"id": "2510.22963", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22963", "abs": "https://arxiv.org/abs/2510.22963", "authors": ["Zesen Liu", "Zhixiang Zhang", "Yuchong Xie", "Dongdong She"], "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents", "comment": null, "summary": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections."}
{"id": "2510.22057", "categories": ["cs.LG", "cs.AI", "cs.CY", "I.5.1; I.4.7"], "pdf": "https://arxiv.org/pdf/2510.22057", "abs": "https://arxiv.org/abs/2510.22057", "authors": ["James Thiering", "Tarun Sethupat Radha Krishna", "Dylan Zelkin", "Ashis Kumer Biswas"], "title": "Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model", "comment": "13 pages, 12 figures, and 1 table", "summary": "With the rise of online and virtual learning, monitoring and enhancing\nstudent engagement have become an important aspect of effective education.\nTraditional methods of assessing a student's involvement might not be\napplicable directly to virtual environments. In this study, we focused on this\nproblem and addressed the need to develop an automated system to detect student\nengagement levels during online learning. We proposed a novel training method\nwhich can discourage a model from leveraging sensitive features like gender for\nits predictions. The proposed method offers benefits not only in the\nenforcement of ethical standards, but also to enhance interpretability of the\nmodel predictions. We applied an attribute-orthogonal regularization technique\nto a split-model classifier, which uses multiple transfer learning strategies\nto achieve effective results in reducing disparity in the distribution of\nprediction for sensitivity groups from a Pearson correlation coefficient of\n0.897 for the unmitigated model, to 0.999 for the mitigated model. The source\ncode for this project is available on\nhttps://github.com/ashiskb/elearning-engagement-study ."}
{"id": "2510.21855", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.21855", "abs": "https://arxiv.org/abs/2510.21855", "authors": ["Ryan Zhang", "Herbert Woisetscläger"], "title": "SIGN: Schema-Induced Games for Naming", "comment": "AAAI 2026 Student Abstract (Oral). Code available ar\n  https://github.com/ryanzhangofficial/schema-induced-games-for-naming", "summary": "Real-world AI systems are tackling increasingly complex problems, often\nthrough interactions among large language model (LLM) agents. When these agents\ndevelop inconsistent conventions, coordination can break down. Applications\nsuch as collaborative coding and distributed planning therefore require\nreliable, consistent communication, and scalability is a central concern as\nsystems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming\ngame that examines how lightweight structure can steer convention formation. We\ncompare schema-induced communication to unconstrained natural language and find\nfaster convergence with up to 5.8x higher agreement. These results suggest that\nminimal structure can act as a simple control knob for efficient multi-agent\ncoordination, pointing toward broader applications beyond the naming game."}
{"id": "2510.22052", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22052", "abs": "https://arxiv.org/abs/2510.22052", "authors": ["Abhijit Chatterjee", "Niraj K. Jha", "Jonathan D. Cohen", "Thomas L. Griffiths", "Hongjing Lu", "Diana Marculescu", "Ashiqur Rasul", "Keshab K. Parhi"], "title": "Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms", "comment": null, "summary": "The field of artificial intelligence (AI) has taken a tight hold on broad\naspects of society, industry, business, and governance in ways that dictate the\nprosperity and might of the world's economies. The AI market size is projected\nto grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI\nis dominated by large language models that exhibit linguistic and visual\nintelligence. However, training these models requires a massive amount of data\nscraped from the web as well as large amounts of energy (50--60 GWh to train\nGPT-4). Despite these costs, these models often hallucinate, a characteristic\nthat prevents them from being deployed in critical application domains. In\ncontrast, the human brain consumes only 20~W of power. What is needed is the\nnext level of AI evolution in which lightweight domain-specific multimodal\nmodels with higher levels of intelligence can reason, plan, and make decisions\nin dynamic environments with real-time data and prior knowledge, while learning\ncontinuously and evolving in ways that enhance future decision-making\ncapability. This will define the next wave of AI, progressing from today's\nlarge models, trained with vast amounts of data, to nimble energy-efficient\ndomain-specific agents that can reason and think in a world full of\nuncertainty. To support such agents, hardware will need to be reimagined to\nallow energy efficiencies greater than 1000x over the state of the art. Such a\nvision of future AI systems is developed in this work."}
{"id": "2510.23274", "categories": ["cs.CR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23274", "abs": "https://arxiv.org/abs/2510.23274", "authors": ["Weixuan Chen", "Qianqian Yang", "Shuo Shao", "Shunpu Tang", "Zhiguo Shi", "Shui Yu"], "title": "Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy", "comment": null, "summary": "While semantic communication (SemCom) improves transmission efficiency by\nfocusing on task-relevant information, it also raises critical privacy\nconcerns. Many existing secure SemCom approaches rely on restrictive or\nimpractical assumptions, such as favorable channel conditions for the\nlegitimate user or prior knowledge of the eavesdropper's model. To address\nthese limitations, this paper proposes a novel secure SemCom framework for\nimage transmission over wiretap channels, leveraging differential privacy (DP)\nto provide approximate privacy guarantees. Specifically, our approach first\nextracts disentangled semantic representations from source images using\ngenerative adversarial network (GAN) inversion method, and then selectively\nperturbs private semantic representations with approximate DP noise. Distinct\nfrom conventional DP-based protection methods, we introduce DP noise with\nlearnable pattern, instead of traditional white Gaussian or Laplace noise,\nachieved through adversarial training of neural networks (NNs). This design\nmitigates the inherent non-invertibility of DP while effectively protecting\nprivate information. Moreover, it enables explicitly controllable security\nlevels by adjusting the privacy budget according to specific security\nrequirements, which is not achieved in most existing secure SemCom approaches.\nExperimental results demonstrate that, compared with the previous DP-based\nmethod and direct transmission, the proposed method significantly degrades the\nreconstruction quality for the eavesdropper, while introducing only slight\ndegradation in task performance. Under comparable security levels, our approach\nachieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86\nfor the legitimate user compared with the previous DP-based method."}
{"id": "2510.22068", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.22068", "abs": "https://arxiv.org/abs/2510.22068", "authors": ["Matthew Lowery", "Zhitong Xu", "Da Long", "Keyan Chen", "Daniel S. Johnson", "Yang Bai", "Varun Shankar", "Shandian Zhe"], "title": "Deep Gaussian Processes for Functional Maps", "comment": "10 pages + 9 page appendix, 5 figures", "summary": "Learning mappings between functional spaces, also known as\nfunction-on-function regression, plays a crucial role in functional data\nanalysis and has broad applications, e.g. spatiotemporal forecasting, curve\nprediction, and climate modeling. Existing approaches, such as functional\nlinear models and neural operators, either fall short of capturing complex\nnonlinearities or lack reliable uncertainty quantification under noisy, sparse,\nand irregularly sampled data. To address these issues, we propose Deep Gaussian\nProcesses for Functional Maps (DGPFM). Our method designs a sequence of\nGP-based linear and nonlinear transformations, leveraging integral transforms\nof kernels, GP interpolation, and nonlinear activations sampled from GPs. A key\ninsight simplifies implementation: under fixed locations, discrete\napproximations of kernel integral transforms collapse into direct functional\nintegral transforms, enabling flexible incorporation of various integral\ntransform designs. To achieve scalable probabilistic inference, we use inducing\npoints and whitening transformations to develop a variational learning\nalgorithm. Empirical results on real-world and PDE benchmark datasets\ndemonstrate that the advantage of DGPFM in both predictive performance and\nuncertainty calibration."}
{"id": "2510.22158", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.22158", "abs": "https://arxiv.org/abs/2510.22158", "authors": ["Lorenzo Magnino", "Kai Shao", "Zida Wu", "Jiacheng Shen", "Mathieu Laurière"], "title": "Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics", "comment": "Neurips 2025", "summary": "Mean field games (MFGs) have emerged as a powerful framework for modeling\ninteractions in large-scale multi-agent systems. Despite recent advancements in\nreinforcement learning (RL) for MFGs, existing methods are typically limited to\nfinite spaces or stationary models, hindering their applicability to real-world\nproblems. This paper introduces a novel deep reinforcement learning (DRL)\nalgorithm specifically designed for non-stationary continuous MFGs. The\nproposed approach builds upon a Fictitious Play (FP) methodology, leveraging\nDRL for best-response computation and supervised learning for average policy\nrepresentation. Furthermore, it learns a representation of the time-dependent\npopulation distribution using a Conditional Normalizing Flow. To validate the\neffectiveness of our method, we evaluate it on three different examples of\nincreasing complexity. By addressing critical limitations in scalability and\ndensity approximation, this work represents a significant advancement in\napplying DRL techniques to complex MFG problems, bringing the field closer to\nreal-world multi-agent systems."}
{"id": "2510.22095", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22095", "abs": "https://arxiv.org/abs/2510.22095", "authors": ["Yankai Chen", "Xinni Zhang", "Yifei Zhang", "Yangning Li", "Henry Peng Zou", "Chunyu Miao", "Weizhi Zhang", "Xue Liu", "Philip S. Yu"], "title": "Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies", "comment": "Accepted by NeurIPS'25 Position Track", "summary": "Brain-Computer Interfaces (BCIs) offer a direct communication pathway between\nthe human brain and external devices, holding significant promise for\nindividuals with severe neurological impairments. However, their widespread\nadoption is hindered by critical limitations, such as low information transfer\nrates and extensive user-specific calibration. To overcome these challenges,\nrecent research has explored the integration of Large Language Models (LLMs),\nextending the focus from simple command decoding to understanding complex\ncognitive states. Despite these advancements, deploying agentic AI faces\ntechnical hurdles and ethical concerns. Due to the lack of comprehensive\ndiscussion on this emerging direction, this position paper argues that the\nfield is poised for a paradigm extension from BCI to Brain-Agent Collaboration\n(BAC). We emphasize reframing agents as active and collaborative partners for\nintelligent assistance rather than passive brain signal data processors,\ndemanding a focus on ethical data handling, model reliability, and a robust\nhuman-agent collaboration framework to ensure these systems are safe,\ntrustworthy, and effective."}
{"id": "2510.23443", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23443", "abs": "https://arxiv.org/abs/2510.23443", "authors": ["Chiara Bonfanti", "Alessandro Druetto", "Cataldo Basile", "Tharindu Ranasinghe", "Marcos Zampieri"], "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration", "comment": "7 pages", "summary": "The growing intersection of cybersecurity and law creates a complex\ninformation space where traditional legal research tools struggle to deal with\nnuanced connections between cases, statutes, and technical vulnerabilities.\nThis knowledge divide hinders collaboration between legal experts and\ncybersecurity professionals. To address this important gap, this work provides\na first step towards intelligent systems capable of navigating the increasingly\nintricate cyber-legal domain. We demonstrate promising initial results on\nmultilingual tasks."}
{"id": "2510.22070", "categories": ["cs.LG", "cs.CV", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.22070", "abs": "https://arxiv.org/abs/2510.22070", "authors": ["Luca Caldera", "Giacomo Bottacini", "Lara Cavinato"], "title": "MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification", "comment": null, "summary": "Generative modeling has emerged as a powerful paradigm for representation\nlearning, but its direct applicability to challenging fields like medical\nimaging remains limited: mere generation, without task alignment, fails to\nprovide a robust foundation for clinical use. We propose MAGIC-Flow, a\nconditional multiscale normalizing flow architecture that performs generation\nand classification within a single modular framework. The model is built as a\nhierarchy of invertible and differentiable bijections, where the Jacobian\ndeterminant factorizes across sub-transformations. We show how this ensures\nexact likelihood computation and stable optimization, while invertibility\nenables explicit visualization of sample likelihoods, providing an\ninterpretable lens into the model's reasoning. By conditioning on class labels,\nMAGIC-Flow supports controllable sample synthesis and principled\nclass-probability estimation, effectively aiding both generative and\ndiscriminative objectives. We evaluate MAGIC-Flow against top baselines using\nmetrics for similarity, fidelity, and diversity. Across multiple datasets, it\naddresses generation and classification under scanner noise, and\nmodality-specific synthesis and identification. Results show MAGIC-Flow creates\nrealistic, diverse samples and improves classification. MAGIC-Flow is an\neffective strategy for generation and classification in data-limited domains,\nwith direct benefits for privacy-preserving augmentation, robust\ngeneralization, and trustworthy medical AI."}
{"id": "2510.22654", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22654", "abs": "https://arxiv.org/abs/2510.22654", "authors": ["Ilgam Latypov", "Alexandra Suvorikova", "Alexey Kroshnin", "Alexander Gasnikov", "Yuriy Dorn"], "title": "UCB-type Algorithm for Budget-Constrained Expert Learning", "comment": null, "summary": "In many modern applications, a system must dynamically choose between several\nadaptive learning algorithms that are trained online. Examples include model\nselection in streaming environments, switching between trading strategies in\nfinance, and orchestrating multiple contextual bandit or reinforcement learning\nagents. At each round, a learner must select one predictor among $K$ adaptive\nexperts to make a prediction, while being able to update at most $M \\le K$ of\nthem under a fixed training budget.\n  We address this problem in the \\emph{stochastic setting} and introduce\n\\algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that\nprovides \\emph{anytime regret guarantees}. Its confidence intervals are built\ndirectly from realized losses, require no additional optimization, and\nseamlessly reflect the convergence properties of the underlying experts. If\neach expert achieves internal regret $\\tilde O(T^\\alpha)$, then \\algname{M-LCB}\nensures overall regret bounded by $\\tilde O\\!\\Bigl(\\sqrt{\\tfrac{KT}{M}} \\;+\\;\n(K/M)^{1-\\alpha}\\,T^\\alpha\\Bigr)$.\n  To our knowledge, this is the first result establishing regret guarantees\nwhen multiple adaptive experts are trained simultaneously under per-round\nbudget constraints. We illustrate the framework with two representative cases:\n(i) parametric models trained online with stochastic losses, and (ii) experts\nthat are themselves multi-armed bandit algorithms. These examples highlight how\n\\algname{M-LCB} extends the classical bandit paradigm to the more realistic\nscenario of coordinating stateful, self-learning experts under limited\nresources."}
{"id": "2510.22462", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22462", "abs": "https://arxiv.org/abs/2510.22462", "authors": ["Abhijnan Nath", "Nikhil Krishnaswamy"], "title": "Learning \"Partner-Aware\" Collaborators in Multi-Party Collaboration", "comment": null, "summary": "Large Language Models (LLMs) are increasingly bring deployed in agentic\nsettings where they act as collaborators with humans. Therefore, it is\nincreasingly important to be able to evaluate their abilities to collaborate\neffectively in multi-turn, multi-party tasks. In this paper, we build on the AI\nalignment and safe interruptability literature to offer novel theoretical\ninsights on collaborative behavior between LLM-driven collaborator agents and\nan intervention agent. Our goal is to learn an ideal partner-aware collaborator\nthat increases the group's common-ground (CG)-alignment on task-relevant\npropositions-by intelligently collecting information provided in interventions\nby a partner agent.We show how LLM agents trained using standard RLHF and\nrelated approaches are naturally inclined to ignore possibly well-meaning\ninterventions, which makes increasing group common ground non-trivial in this\nsetting. We employ a two-player Modified-Action MDP to examine this suboptimal\nbehavior of standard AI agents, and propose Interruptible Collaborative\nRoleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal\ncollaborators. Experiments on multiple collaborative task environments show\nthat ICR, on average, is more capable of promoting successful CG convergence\nand exploring more diverse solutions in such tasks."}
{"id": "2510.22075", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22075", "abs": "https://arxiv.org/abs/2510.22075", "authors": ["Siyu Zhu", "Anastasiya Karpovich", "Albert Chen", "Jessica Koscheka", "Shailesh Jannu", "Di Wen", "Yuqing Zhu", "Rohit Jain", "Alborz Geramifard"], "title": "Agentic Reinforcement Learning for Real-World Code Repair", "comment": null, "summary": "We tackle the challenge of training reliable code-fixing agents in real\nrepositories, where complex builds and shifting dependencies make evaluation\nunstable. We developed a verifiable pipeline with success defined as post-fix\nbuild validation and improved reproducibility across ~1K real issues by pinning\ndependencies and disabling automatic upgrades. Building on this, we introduced\na scalable simplified pipeline for large-scale reinforcement learning (RL).\nUsing this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and\napplied RL on top of the SFT model in the simplified environment. The SFT model\ndistilled from GPT-4.1 trajectories performs on par while being 56x smaller,\nand RL added 7-20% absolute gains under matched train-test conditions.\n\"Thinking mode\" was on par or worse in our experiments. Both SFT and RL models\nfailed to generalize across environments, highlighting the importance of\nmatching train-test environments for building reliable real-world code-fixing\nagents."}
{"id": "2510.22732", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22732", "abs": "https://arxiv.org/abs/2510.22732", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation", "comment": "9 pages, NeurIPS 2025 Workshop on Language Agents and World Models", "summary": "We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system"}
{"id": "2510.22609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22609", "abs": "https://arxiv.org/abs/2510.22609", "authors": ["Md. Mehedi Hasan", "Rafid Mostafiz", "Md. Abir Hossain", "Bikash Kumar Paul"], "title": "CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation", "comment": "13 pages, 9 figures. Preprint version under review in the area of\n  Artificial Intelligence (cs.CR)", "summary": "Accurate symptom-to-disease classification and clinically grounded treatment\nrecommendations remain challenging, particularly in heterogeneous patient\nsettings with high diagnostic risk. Existing large language model (LLM)-based\nsystems often lack medical grounding and fail to quantify uncertainty,\nresulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid\npipeline that integrates multimodal patient encoding, uncertainty-calibrated\ndisease classification, and retrieval-augmented treatment generation. The\nframework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease\ndataset and incorporates Focal Loss with Monte Carlo Dropout to enable\nconfidence-aware predictions from free-text symptoms and structured vitals.\nLow-certainty cases (18%) are automatically flagged for expert review, ensuring\nhuman oversight. For treatment generation, CLIN-LLM employs Biomedical\nSentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample\nMedDialog corpus. The retrieved evidence and patient context are fed into a\nfine-tuned FLAN-T5 model for personalized treatment generation, followed by\npost-processing with RxNorm for antibiotic stewardship and drug-drug\ninteraction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,\noutperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval\nprecision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic\nsuggestions are reduced by 67% compared to GPT-5. These results demonstrate\nCLIN-LLM's robustness, interpretability, and clinical safety alignment. The\nproposed system provides a deployable, human-in-the-loop decision support\nframework for resource-limited healthcare environments. Future work includes\nintegrating imaging and lab data, multilingual extensions, and clinical trial\nvalidation."}
{"id": "2510.22094", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.22094", "abs": "https://arxiv.org/abs/2510.22094", "authors": ["Thomas Bailie", "S. Karthik Mukkavilli", "Varvara Vetrova", "Yun Sing Koh"], "title": "Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training", "comment": null, "summary": "Climate events arise from intricate, multivariate dynamics governed by\nglobal-scale drivers, profoundly impacting food, energy, and infrastructure.\nYet, accurate weather prediction remains elusive due to physical processes\nunfolding across diverse spatio-temporal scales, which fixed-resolution methods\ncannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale\nrepresentation, but nonlinear downward mappings often erase global trends,\nweakening the integration of physics into forecasts. We introduce HiFlowCast\nand its ensemble variant HiAntFlow, HGNNs that embed physics within a\nmultiscale prediction framework. Two innovations underpin their design: a\nLatent-Memory-Retention mechanism that preserves global trends during downward\ntraversal, and a Latent-to-Physics branch that integrates PDE solution fields\nacross diverse scales. Our Flow models cut errors by over 5% at 13-day lead\ntimes and by 5-8% under 1st and 99th quantile extremes, improving reliability\nfor rare events. Leveraging pretrained model weights, they converge within a\nsingle epoch, reducing training cost and their carbon footprint. Such\nefficiency is vital as the growing scale of machine learning challenges\nsustainability and limits research accessibility. Code and model weights are in\nthe supplementary materials."}
{"id": "2510.22969", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22969", "abs": "https://arxiv.org/abs/2510.22969", "authors": ["Kechen Meng", "Sinuo Zhang", "Rongpeng Li", "Xiangming Meng", "Chan Wang", "Ming Lei", "Zhifeng Zhao"], "title": "Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner", "comment": null, "summary": "In wireless communication systems, efficient and adaptive resource allocation\nplays a crucial role in enhancing overall Quality of Service (QoS). While\ncentralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a\ncentral coordinator for policy training and resource scheduling, they suffer\nfrom scalability issues and privacy risks. In contrast, the Distributed\nTraining with Decentralized Execution (DTDE) paradigm enables distributed\nlearning and decision-making, but it struggles with non-stationarity and\nlimited inter-agent cooperation, which can severely degrade system performance.\nTo overcome these challenges, we propose the Multi-Agent Conditional Diffusion\nModel Planner (MA-CDMP) for decentralized communication resource management.\nBuilt upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP\nemploys Diffusion Models (DMs) to capture environment dynamics and plan future\ntrajectories, while an inverse dynamics model guides action generation, thereby\nalleviating the sample inefficiency and slow convergence of conventional DTDE\nmethods. Moreover, to approximate large-scale agent interactions, a Mean-Field\n(MF) mechanism is introduced as an assistance to the classifier in DMs. This\ndesign mitigates inter-agent non-stationarity and enhances cooperation with\nminimal communication overhead in distributed settings. We further\ntheoretically establish an upper bound on the distributional approximation\nerror introduced by the MF-based diffusion generation, guaranteeing convergence\nstability and reliable modeling of multi-agent stochastic dynamics. Extensive\nexperiments demonstrate that MA-CDMP consistently outperforms existing MARL\nbaselines in terms of average reward and QoS metrics, showcasing its\nscalability and practicality for real-world wireless network optimization."}
{"id": "2510.22626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22626", "abs": "https://arxiv.org/abs/2510.22626", "authors": ["Adhyayan Veer Singh", "Aaron Shen", "Brian Law", "Ahmed Ismail", "Jonas Rohweder", "Sean O'Brien", "Kevin Zhu"], "title": "SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming", "comment": null, "summary": "Correctness alone is insufficient: LLM-generated programs frequently satisfy\nunit tests while violating contest time or memory budgets. We present\nSwiftSolve, a complexity-aware multi-agent system for competitive programming\nthat couples algorithmic planning with empirical profiling and\ncomplexity-guided repair. We frame competitive programming as a software\nenvironment where specialized agents act as programmers, each assuming roles\nsuch as planning, coding, profiling, and complexity analysis. A Planner\nproposes an algorithmic sketch; a deterministic Static Pruner filters high-risk\nplans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on\na fixed input-size schedule to record wall time and peak memory; and a\nComplexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a\ncomplexity class and dispatch targeted patches to either the Planner or Coder.\nAgents communicate via typed, versioned JSON; a controller enforces iteration\ncaps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10\nCodeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains\npass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with\nmarginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate\nrun-level success is 73.08% at 12.40 s mean. Failures are predominantly\nresource-bound, indicating inefficiency rather than logic errors. Against\nClaude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at\napproximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness\n(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence\nof TLE or MLE, and complexity fit accuracy on BigO), demonstrating that\nprofiling and complexity-guided replanning reduce inefficiency while preserving\naccuracy."}
{"id": "2510.22139", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22139", "abs": "https://arxiv.org/abs/2510.22139", "authors": ["Jinzhe Liu", "Junshu Sun", "Shufan Shen", "Chenxue Yang", "Shuhui Wang"], "title": "Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs", "comment": "19 pages, 11 figures, Accepted by NeurIPS 2025", "summary": "Lifelong knowledge editing enables continuous, precise updates to outdated\nknowledge in large language models (LLMs) without computationally expensive\nfull retraining. However, existing methods often accumulate errors throughout\nthe editing process, causing a gradual decline in both editing accuracy and\ngeneralization. To tackle this problem, we propose Neuron-Specific Masked\nKnowledge Editing (NMKE), a novel fine-grained editing framework that combines\nneuron-level attribution with dynamic sparse masking. Leveraging neuron\nfunctional attribution, we identify two key types of knowledge neurons, with\nknowledge-general neurons activating consistently across prompts and\nknowledge-specific neurons activating to specific prompts. NMKE further\nintroduces an entropy-guided dynamic sparse mask, locating relevant neurons to\nthe target knowledge. This strategy enables precise neuron-level knowledge\nediting with fewer parameter modifications. Experimental results from thousands\nof sequential edits demonstrate that NMKE outperforms existing methods in\nmaintaining high editing success rates and preserving model general\ncapabilities in lifelong editing."}
{"id": "2510.23408", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23408", "abs": "https://arxiv.org/abs/2510.23408", "authors": ["Abolfazl Younesi", "Zahra Najafabadi Samani", "Thomas Fahringer"], "title": "AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines", "comment": "Under review", "summary": "Data pipelines are essential in stream processing as they enable the\nefficient collection, processing, and delivery of real-time data, supporting\nrapid data analysis. In this paper, we present AutoStreamPipe, a novel\nframework that employs Large Language Models (LLMs) to automate the design,\ngeneration, and deployment of stream processing pipelines. AutoStreamPipe\nbridges the semantic gap between high-level user intent and platform-specific\nimplementations across distributed stream processing systems for structured\nmulti-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an\nextended version of GoT. AutoStreamPipe combines resilient execution\nstrategies, advanced query analysis, and HGoT to deliver pipelines with good\naccuracy. Experimental evaluations on diverse pipelines demonstrate that\nAutoStreamPipe significantly reduces development time (x6.3) and error rates\n(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM\ncode-generation methods."}
{"id": "2510.22780", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22780", "abs": "https://arxiv.org/abs/2510.22780", "authors": ["Zora Zhiruo Wang", "Yijia Shao", "Omar Shaikh", "Daniel Fried", "Graham Neubig", "Diyi Yang"], "title": "How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations", "comment": null, "summary": "AI agents are continually optimized for tasks related to human work, such as\nsoftware engineering and professional writing, signaling a pressing trend with\nsignificant impacts on the human workforce. However, these agent developments\nhave often not been grounded in a clear understanding of how humans execute\nwork, to reveal what expertise agents possess and the roles they can play in\ndiverse workflows. In this work, we study how agents do human work by\npresenting the first direct comparison of human and agent workers across\nmultiple essential work-related skills: data analysis, engineering,\ncomputation, writing, and design. To better understand and compare\nheterogeneous computer-use activities of workers, we introduce a scalable\ntoolkit to induce interpretable, structured workflows from either human or\nagent computer-use activities. Using such induced workflows, we compare how\nhumans and agents perform the same tasks and find that: (1) While agents\nexhibit promise in their alignment to human workflows, they take an\noverwhelmingly programmatic approach across all work domains, even for\nopen-ended, visually dependent tasks like design, creating a contrast with the\nUI-centric methods typically used by humans. (2) Agents produce work of\ninferior quality, yet often mask their deficiencies via data fabrication and\nmisuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster\nand cost 90.4-96.2% less than humans, highlighting the potential for enabling\nefficient collaboration by delegating easily programmable tasks to agents."}
{"id": "2510.22158", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.22158", "abs": "https://arxiv.org/abs/2510.22158", "authors": ["Lorenzo Magnino", "Kai Shao", "Zida Wu", "Jiacheng Shen", "Mathieu Laurière"], "title": "Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics", "comment": "Neurips 2025", "summary": "Mean field games (MFGs) have emerged as a powerful framework for modeling\ninteractions in large-scale multi-agent systems. Despite recent advancements in\nreinforcement learning (RL) for MFGs, existing methods are typically limited to\nfinite spaces or stationary models, hindering their applicability to real-world\nproblems. This paper introduces a novel deep reinforcement learning (DRL)\nalgorithm specifically designed for non-stationary continuous MFGs. The\nproposed approach builds upon a Fictitious Play (FP) methodology, leveraging\nDRL for best-response computation and supervised learning for average policy\nrepresentation. Furthermore, it learns a representation of the time-dependent\npopulation distribution using a Conditional Normalizing Flow. To validate the\neffectiveness of our method, we evaluate it on three different examples of\nincreasing complexity. By addressing critical limitations in scalability and\ndensity approximation, this work represents a significant advancement in\napplying DRL techniques to complex MFG problems, bringing the field closer to\nreal-world multi-agent systems."}
{"id": "2510.23443", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23443", "abs": "https://arxiv.org/abs/2510.23443", "authors": ["Chiara Bonfanti", "Alessandro Druetto", "Cataldo Basile", "Tharindu Ranasinghe", "Marcos Zampieri"], "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration", "comment": "7 pages", "summary": "The growing intersection of cybersecurity and law creates a complex\ninformation space where traditional legal research tools struggle to deal with\nnuanced connections between cases, statutes, and technical vulnerabilities.\nThis knowledge divide hinders collaboration between legal experts and\ncybersecurity professionals. To address this important gap, this work provides\na first step towards intelligent systems capable of navigating the increasingly\nintricate cyber-legal domain. We demonstrate promising initial results on\nmultilingual tasks."}
{"id": "2510.22781", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22781", "abs": "https://arxiv.org/abs/2510.22781", "authors": ["Xiaofeng Zhu", "Yunshen Zhou"], "title": "Agentic Meta-Orchestrator for Multi-task Copilots", "comment": null, "summary": "Microsoft Copilot suites serve as the universal entry point for various\nagents skilled in handling important tasks, ranging from assisting a customer\nwith product purchases to detecting vulnerabilities in corporate programming\ncode. Each agent can be powered by language models, software engineering\noperations, such as database retrieval, and internal \\& external knowledge. The\nrepertoire of a copilot can expand dynamically with new agents. This requires a\nrobust orchestrator that can distribute tasks from user prompts to the right\nagents. In this work, we propose an Agentic Meta-orchestrator (AMO) for\nhandling multiple tasks and scalable agents in copilot services, which can\nprovide both natural language and action responses. We will also demonstrate\nthe planning that leverages meta-learning, i.e., a trained decision tree model\nfor deciding the best inference strategy among various agents/models. We\nshowcase the effectiveness of our AMO through two production use cases:\nMicrosoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365\nE-Commerce Copilot advertises Microsoft products to external customers to\npromote sales success. The M365 E-Commerce Copilot provides up-to-date product\ninformation and connects to multiple agents, such as relational databases and\nhuman customer support. The code compliance copilot scans the internal DevOps\ncode to detect known and new compliance issues in pull requests (PR)."}
{"id": "2510.22228", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22228", "abs": "https://arxiv.org/abs/2510.22228", "authors": ["Keyu Wang", "Tian Lyu", "Guinan Su", "Jonas Geiping", "Lu Yin", "Marco Canini", "Shiwei Liu"], "title": "When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs", "comment": null, "summary": "Layer pruning has emerged as a widely adopted technique for improving the\nefficiency of large language models (LLMs). Although existing methods\ndemonstrate strong performance retention on general knowledge tasks, their\neffect on long-chain reasoning, a more brittle yet crucial capability, remains\nlargely unexplored. In this work, we study the impact of layer pruning on\nlong-chain reasoning through the lens of test-time scaling, a key mechanism in\nmodern LLMs that enables strong reasoning capacity by allocating more\ncomputation at inference time. With extensive experiments, we demonstrate that\npruning even one or two layers can severely impair test-time scaling, with\nperformance collapsing drastically on long reasoning benchmarks even when\nperformance on knowledge-intensive and shallow reasoning tasks remains stable.\nFurthermore, we find that standard supervised fine-tuning remedies fail to\nrecover test-time scaling once it has deteriorated. Through in-depth analyses,\nwe identify the mechanisms underlying this fragility of test-time scaling and\nhighlight the fundamental risks of applying layer pruning to\nreasoning-intensive LLMs. These findings call for a rethinking of layer pruning\nstrategies and provide insights for developing methods that preserve the\nrobustness of reasoning. We open-source the codebase in\n\\href{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}."}
{"id": "2510.22814", "categories": ["cs.AI", "I.2.0; K.4.1; K.6.m"], "pdf": "https://arxiv.org/pdf/2510.22814", "abs": "https://arxiv.org/abs/2510.22814", "authors": ["Mohamed El Louadi", "Emna Ben Romdhane"], "title": "Will Humanity Be Rendered Obsolete by AI?", "comment": null, "summary": "This article analyzes the existential risks artificial intelligence (AI)\nposes to humanity, tracing the trajectory from current AI to ultraintelligence.\nDrawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent\npublications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and\nsuperintelligence. Considering machines' exponentially growing cognitive power\nand hypothetical IQs, it addresses the ethical and existential implications of\nan intelligence vastly exceeding humanity's, fundamentally alien. Human\nextinction may result not from malice, but from uncontrollable, indifferent\ncognitive superiority."}
{"id": "2510.22261", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22261", "abs": "https://arxiv.org/abs/2510.22261", "authors": ["Shireen Kudukkil Manchingal"], "title": "Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know", "comment": "PhD thesis", "summary": "Machine learning has achieved remarkable successes, yet its deployment in\nsafety-critical domains remains hindered by an inherent inability to manage\nuncertainty, resulting in overconfident and unreliable predictions when models\nencounter out-of-distribution data, adversarial perturbations, or naturally\nfluctuating environments. This thesis, titled Epistemic Deep Learning: Enabling\nMachine Learning Models to 'Know When They Do Not Know', addresses these\ncritical challenges by advancing the paradigm of Epistemic Artificial\nIntelligence, which explicitly models and quantifies epistemic uncertainty: the\nuncertainty arising from limited, biased, or incomplete training data, as\nopposed to the irreducible randomness of aleatoric uncertainty, thereby\nempowering models to acknowledge their limitations and refrain from\noverconfident decisions when uncertainty is high.\n  Central to this work is the development of the Random-Set Neural Network\n(RS-NN), a novel methodology that leverages random set theory to predict belief\nfunctions over sets of classes, capturing the extent of epistemic uncertainty\nthrough the width of associated credal sets, applications of RS-NN, including\nits adaptation to Large Language Models (LLMs) and its deployment in weather\nclassification for autonomous racing. In addition, the thesis proposes a\nunified evaluation framework for uncertainty-aware classifiers. Extensive\nexperiments validate that integrating epistemic awareness into deep learning\nnot only mitigates the risks associated with overconfident predictions but also\nlays the foundation for a paradigm shift in artificial intelligence, where the\nability to 'know when it does not know' becomes a hallmark of robust and\ndependable systems. The title encapsulates the core philosophy of this work,\nemphasizing that true intelligence involves recognizing and managing the limits\nof one's own knowledge."}
{"id": "2510.22832", "categories": ["cs.AI", "cs.LG", "stat.ML", "68T07 (Primary) 62M45, 37N99 (Secondary)", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.22832", "abs": "https://arxiv.org/abs/2510.22832", "authors": ["Long H Dang", "David Rawlinson"], "title": "HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning", "comment": "14 pages, 9 figures, 1 table", "summary": "The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities\ngiven its small size, but has only been applied to supervised, static,\nfully-observable problems. One of HRM's strengths is its ability to adapt its\ncomputational effort to the difficulty of the problem. However, in its current\nform it cannot integrate and reuse computation from previous time-steps if the\nproblem is dynamic, uncertain or partially observable, or be applied where the\ncorrect action is undefined, characteristics of many real-world problems.\n  This paper presents HRM-Agent, a variant of HRM trained using only\nreinforcement learning. We show that HRM can learn to navigate to goals in\ndynamic and uncertain maze environments. Recent work suggests that HRM's\nreasoning abilities stem from its recurrent inference process. We explore the\ndynamics of the recurrent inference process and find evidence that it is\nsuccessfully reusing computation from earlier environment time-steps."}
{"id": "2510.22301", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22301", "abs": "https://arxiv.org/abs/2510.22301", "authors": ["Yujie Xiao", "Gongzhen Tang", "Wenhui Liu", "Jun Li", "Guangkun Nie", "Zhuoran Kan", "Deyun Zhang", "Qinghao Zhao", "Shenda Hong"], "title": "AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals", "comment": null, "summary": "Timely access to laboratory values is critical for clinical decision-making,\nyet current approaches rely on invasive venous sampling and are intrinsically\ndelayed. Electrocardiography (ECG), as a non-invasive and widely available\nsignal, offers a promising modality for rapid laboratory estimation. Recent\nprogress in deep learning has enabled the extraction of latent hematological\nsignatures from ECGs. However, existing models are constrained by low\nsignal-to-noise ratios, substantial inter-individual variability, limited data\ndiversity, and suboptimal generalization, especially when adapted to low-lead\nwearable devices. In this work, we conduct an exploratory study leveraging\ntransfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG\nfoundation model, on the Multimodal Clinical Monitoring in the Emergency\nDepartment (MC-MED) dataset from Stanford. We generated a corpus of more than\n20 million standardized ten-second ECG segments to enhance sensitivity to\nsubtle biochemical correlates. On internal validation, the model demonstrated\nstrong predictive performance (area under the curve above 0.65) for\nthirty-three laboratory indicators, moderate performance (between 0.55 and\n0.65) for fifty-nine indicators, and limited performance (below 0.55) for\nsixteen indicators. This study provides an efficient artificial-intelligence\ndriven solution and establishes the feasibility scope for real-time,\nnon-invasive estimation of laboratory values."}
{"id": "2510.22833", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22833", "abs": "https://arxiv.org/abs/2510.22833", "authors": ["Adrian Orenstein", "Jessica Chen", "Gwyneth Anne Delos Santos", "Bayley Sapara", "Michael Bowling"], "title": "Toward Agents That Reason About Their Computation", "comment": null, "summary": "While reinforcement learning agents can achieve superhuman performance in\nmany complex tasks, they typically do not become more computationally efficient\nas they improve. In contrast, humans gradually require less cognitive effort as\nthey become more proficient at a task. If agents could reason about their\ncompute as they learn, could they similarly reduce their computation footprint?\nIf they could, we could have more energy efficient agents or free up compute\ncycles for other processes like planning. In this paper, we experiment with\nshowing agents the cost of their computation and giving them the ability to\ncontrol when they use compute. We conduct our experiments on the Arcade\nLearning Environment, and our results demonstrate that with the same training\ncompute budget, agents that reason about their compute perform better on 75% of\ngames. Furthermore, these agents use three times less compute on average. We\nanalyze individual games and show where agents gain these efficiencies."}
{"id": "2510.22383", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22383", "abs": "https://arxiv.org/abs/2510.22383", "authors": ["David Freire-Obregón", "José Salas-Cáceres", "Modesto Castrillón-Santana"], "title": "Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization", "comment": "Accepted for presentation at the 5th International Conference on\n  Computing and Machine Intelligence (ICMI 2026)", "summary": "Regularization techniques play a crucial role in preventing overfitting and\nimproving the generalization performance of neural networks. Dropout, a widely\nused regularization technique, randomly deactivates units during training to\nintroduce redundancy and prevent co-adaptation among neurons. Despite its\neffectiveness, dropout has limitations, such as its static nature and lack of\ninterpretability. In this paper, we propose a novel approach to regularization\nby substituting dropout with Conway's Game of Life (GoL), a cellular automata\nwith simple rules that govern the evolution of a grid of cells. We introduce\ndynamic unit deactivation during training by representing neural network units\nas cells in a GoL grid and applying the game's rules to deactivate units. This\napproach allows for the emergence of spatial patterns that adapt to the\ntraining data, potentially enhancing the network's ability to generalize. We\ndemonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing\nthat dynamic unit deactivation using GoL achieves comparable performance to\ntraditional dropout techniques while offering insights into the network's\nbehavior through the visualization of evolving patterns. Furthermore, our\ndiscussion highlights the applicability of our proposal in deeper\narchitectures, demonstrating how it enhances the performance of different\ndropout techniques."}
{"id": "2510.22898", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22898", "abs": "https://arxiv.org/abs/2510.22898", "authors": ["Vishvesh Bhat", "Omkar Ghugarkar", "Julian McAuley"], "title": "On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset", "comment": "Preprint", "summary": "Generalization across Agentic tool-calling environments remains a key\nunsolved challenge in developing reliable agentic reasoning systems. While\nlarge language models (LLMs) demonstrate strong performance on isolated\nbenchmarks, their ability to transfer reasoning strategies and co-ordinate\ntools across diverse domains is poorly understood. In this work, we conduct a\nlarge-scale evaluation of state-of-the-art LLMs on multiple tool-calling\nbenchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &\nPhysics Adversarial Verification & Evaluation Network), a new out of\ndistribution (OOD) benchmark designed to stress-test multi-step reasoning\nthrough explicit verification and adversarial task composition. Our results\nshow that most current models achieve below 50% accuracy on MAVEN, revealing a\nsignificant generalization gap across tool-use settings.\n  To address this, we present the CoreThink Agentic Reasoner, a framework that\naugments LLMs with a lightweight symbolic reasoning layer for structured\ndecomposition and adaptive tool orchestration. Without additional training, it\ngeneralizes across all benchmarks, achieving state-of-the-art performance with\n530% improvements over existing baselines at roughly one-tenth the\ncomputational cost."}
{"id": "2510.22500", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22500", "abs": "https://arxiv.org/abs/2510.22500", "authors": ["Ren Yin", "Takashi Ishida", "Masashi Sugiyama"], "title": "Scalable Oversight via Partitioned Human Supervision", "comment": null, "summary": "As artificial intelligence (AI) systems approach and surpass expert human\nperformance across a broad range of tasks, obtaining high-quality human\nsupervision for evaluation and training becomes increasingly challenging. Our\nfocus is on tasks that require deep knowledge and skills of multiple domains.\nUnfortunately, even the best human experts are knowledgeable only in a single\nnarrow area, and will not be able to evaluate the correctness of advanced AI\nsystems on such superhuman tasks. However, based on their narrow expertise,\nhumans may provide a weak signal, i.e., a complementary label indicating an\noption that is incorrect. For example, a cardiologist could state that \"this is\nnot related to cardiology,'' even if they cannot identify the true disease.\nBased on this weak signal, we propose a scalable oversight framework that\nenables us to evaluate frontier AI systems without the need to prepare the\nground truth. We derive an unbiased estimator of top-1 accuracy from\ncomplementary labels and quantify how many complementary labels are needed to\nmatch the variance of ordinary labels. We further introduce two estimators to\ncombine scarce ordinary labels with abundant complementary labels. We provide\nfinite-sample deviation guarantees for both complementary-only and the mixed\nestimators. Empirically, we show that we can evaluate the output of large\nlanguage models without the ground truth, if we have complementary labels. We\nfurther show that we can train an AI system with such weak signals: we show how\nwe can design an agentic AI system automatically that can perform better with\nthis partitioned human supervision. Our code is available at\nhttps://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision."}
{"id": "2510.22969", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22969", "abs": "https://arxiv.org/abs/2510.22969", "authors": ["Kechen Meng", "Sinuo Zhang", "Rongpeng Li", "Xiangming Meng", "Chan Wang", "Ming Lei", "Zhifeng Zhao"], "title": "Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner", "comment": null, "summary": "In wireless communication systems, efficient and adaptive resource allocation\nplays a crucial role in enhancing overall Quality of Service (QoS). While\ncentralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a\ncentral coordinator for policy training and resource scheduling, they suffer\nfrom scalability issues and privacy risks. In contrast, the Distributed\nTraining with Decentralized Execution (DTDE) paradigm enables distributed\nlearning and decision-making, but it struggles with non-stationarity and\nlimited inter-agent cooperation, which can severely degrade system performance.\nTo overcome these challenges, we propose the Multi-Agent Conditional Diffusion\nModel Planner (MA-CDMP) for decentralized communication resource management.\nBuilt upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP\nemploys Diffusion Models (DMs) to capture environment dynamics and plan future\ntrajectories, while an inverse dynamics model guides action generation, thereby\nalleviating the sample inefficiency and slow convergence of conventional DTDE\nmethods. Moreover, to approximate large-scale agent interactions, a Mean-Field\n(MF) mechanism is introduced as an assistance to the classifier in DMs. This\ndesign mitigates inter-agent non-stationarity and enhances cooperation with\nminimal communication overhead in distributed settings. We further\ntheoretically establish an upper bound on the distributional approximation\nerror introduced by the MF-based diffusion generation, guaranteeing convergence\nstability and reliable modeling of multi-agent stochastic dynamics. Extensive\nexperiments demonstrate that MA-CDMP consistently outperforms existing MARL\nbaselines in terms of average reward and QoS metrics, showcasing its\nscalability and practicality for real-world wireless network optimization."}
{"id": "2510.22520", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22520", "abs": "https://arxiv.org/abs/2510.22520", "authors": ["Michael Ito", "Danai Koutra", "Jenna Wiens"], "title": "Random Search Neural Networks for Efficient and Expressive Graph Learning", "comment": "NEURIPS 2025; version with full appendix", "summary": "Random walk neural networks (RWNNs) have emerged as a promising approach for\ngraph representation learning, leveraging recent advances in sequence models to\nprocess random walks. However, under realistic sampling constraints, RWNNs\noften fail to capture global structure even in small graphs due to incomplete\nnode and edge coverage, limiting their expressivity. To address this, we\npropose \\textit{random search neural networks} (RSNNs), which operate on random\nsearches, each of which guarantees full node coverage. Theoretically, we\ndemonstrate that in sparse graphs, only $O(\\log |V|)$ searches are needed to\nachieve full edge coverage, substantially reducing sampling complexity compared\nto the $O(|V|)$ walks required by RWNNs (assuming walk lengths scale with graph\nsize). Furthermore, when paired with universal sequence models, RSNNs are\nuniversal approximators. We lastly show RSNNs are probabilistically invariant\nto graph isomorphisms, ensuring their expectation is an isomorphism-invariant\ngraph function. Empirically, RSNNs consistently outperform RWNNs on molecular\nand protein benchmarks, achieving comparable or superior performance with up to\n16$\\times$ fewer sampled sequences. Our work bridges theoretical and practical\nadvances in random walk based approaches, offering an efficient and expressive\nframework for learning on sparse graphs."}
{"id": "2510.23008", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23008", "abs": "https://arxiv.org/abs/2510.23008", "authors": ["Qiuli Wang", "Jie Chen", "Yongxu Liu", "Xingpeng Zhang", "Xiaoming Li", "Wei Chen"], "title": "From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports", "comment": "10 pages, 6 figures, 4 tables", "summary": "Large language models (LLMs) have demonstrated promising performance in\ngenerating diagnostic conclusions from imaging findings, thereby supporting\nradiology reporting, trainee education, and quality control. However,\nsystematic guidance on how to optimize prompt design across different clinical\ncontexts remains underexplored. Moreover, a comprehensive and standardized\nframework for assessing the trustworthiness of LLM-generated radiology reports\nis yet to be established. This study aims to enhance the trustworthiness of\nLLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility\nAssessment (MDCA) framework and providing guidance on institution-specific\nprompt optimization. The proposed framework is applied to evaluate and compare\nthe performance of several advanced LLMs, including Kimi-K2-Instruct-0905,\nQwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and\nByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform."}
{"id": "2510.22654", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22654", "abs": "https://arxiv.org/abs/2510.22654", "authors": ["Ilgam Latypov", "Alexandra Suvorikova", "Alexey Kroshnin", "Alexander Gasnikov", "Yuriy Dorn"], "title": "UCB-type Algorithm for Budget-Constrained Expert Learning", "comment": null, "summary": "In many modern applications, a system must dynamically choose between several\nadaptive learning algorithms that are trained online. Examples include model\nselection in streaming environments, switching between trading strategies in\nfinance, and orchestrating multiple contextual bandit or reinforcement learning\nagents. At each round, a learner must select one predictor among $K$ adaptive\nexperts to make a prediction, while being able to update at most $M \\le K$ of\nthem under a fixed training budget.\n  We address this problem in the \\emph{stochastic setting} and introduce\n\\algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that\nprovides \\emph{anytime regret guarantees}. Its confidence intervals are built\ndirectly from realized losses, require no additional optimization, and\nseamlessly reflect the convergence properties of the underlying experts. If\neach expert achieves internal regret $\\tilde O(T^\\alpha)$, then \\algname{M-LCB}\nensures overall regret bounded by $\\tilde O\\!\\Bigl(\\sqrt{\\tfrac{KT}{M}} \\;+\\;\n(K/M)^{1-\\alpha}\\,T^\\alpha\\Bigr)$.\n  To our knowledge, this is the first result establishing regret guarantees\nwhen multiple adaptive experts are trained simultaneously under per-round\nbudget constraints. We illustrate the framework with two representative cases:\n(i) parametric models trained online with stochastic losses, and (ii) experts\nthat are themselves multi-armed bandit algorithms. These examples highlight how\n\\algname{M-LCB} extends the classical bandit paradigm to the more realistic\nscenario of coordinating stateful, self-learning experts under limited\nresources."}
{"id": "2510.23127", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23127", "abs": "https://arxiv.org/abs/2510.23127", "authors": ["Kai Zhuang", "Jiawei Zhang", "Yumou Liu", "Hanqun Cao", "Chunbin Gu", "Mengdi Liu", "Zhangyang Gao", "Zitong Jerry Wang", "Xuanhe Zhou", "Pheng-Ann Heng", "Lijun Wu", "Conghui He", "Cheng Tan"], "title": "Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs", "comment": "36 pages, under review", "summary": "Scientific Large Language Models (Sci-LLMs) have emerged as a promising\nfrontier for accelerating biological discovery. However, these models face a\nfundamental challenge when processing raw biomolecular sequences: the\ntokenization dilemma. Whether treating sequences as a specialized language,\nrisking the loss of functional motif information, or as a separate modality,\nintroducing formidable alignment challenges, current strategies fundamentally\nlimit their reasoning capacity. We challenge this sequence-centric paradigm by\npositing that a more effective strategy is to provide Sci-LLMs with high-level\nstructured context derived from established bioinformatics tools, thereby\nbypassing the need to interpret low-level noisy sequence data directly. Through\na systematic comparison of leading Sci-LLMs on biological reasoning tasks, we\ntested three input modes: sequence-only, context-only, and a combination of\nboth. Our findings are striking: the context-only approach consistently and\nsubstantially outperforms all other modes. Even more revealing, the inclusion\nof the raw sequence alongside its high-level context consistently degrades\nperformance, indicating that raw sequences act as informational noise, even for\nmodels with specialized tokenization schemes. These results suggest that the\nprimary strength of existing Sci-LLMs lies not in their nascent ability to\ninterpret biomolecular syntax from scratch, but in their profound capacity for\nreasoning over structured, human-readable knowledge. Therefore, we argue for\nreframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines\nover expert knowledge. This work lays the foundation for a new class of hybrid\nscientific AI agents, repositioning the developmental focus from direct\nsequence interpretation towards high-level knowledge synthesis. The code is\navailable at github.com/opendatalab-raise-dev/CoKE."}
{"id": "2510.22655", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22655", "abs": "https://arxiv.org/abs/2510.22655", "authors": ["Berken Utku Demirel", "Christian Holz"], "title": "Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections", "comment": "Published at the Conference on Neural Information Processing Systems\n  (NeurIPS) 2025", "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations without labeled data. Most SSL approaches rely on\nstrong, well-established, handcrafted data augmentations to generate diverse\nviews for representation learning. However, designing such augmentations\nrequires domain-specific knowledge and implicitly imposes representational\ninvariances on the model, which can limit generalization. In this work, we\npropose an unsupervised representation learning method that replaces\naugmentations by generating views using orthonormal bases and overcomplete\nframes. We show that embeddings learned from orthonormal and overcomplete\nspaces reside on distinct manifolds, shaped by the geometric biases introduced\nby representing samples in different spaces. By jointly leveraging the\ncomplementary geometry of these distinct manifolds, our approach achieves\nsuperior performance without artificially increasing data diversity through\nstrong augmentations. We demonstrate the effectiveness of our method on nine\ndatasets across five temporal sequence tasks, where signal-specific\ncharacteristics make data augmentations particularly challenging. Without\nrelying on augmentation-induced diversity, our method achieves performance\ngains of up to 15--20\\% over existing self-supervised approaches. Source code:\nhttps://github.com/eth-siplab/Learning-with-FrameProjections"}
{"id": "2510.23216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23216", "abs": "https://arxiv.org/abs/2510.23216", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Jean-Philippe Barrette-LaPierre", "Florian Fuchs", "Brady Chen", "Michael Jones", "Linus Gisslén"], "title": "Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach", "comment": null, "summary": "While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testimony of the impact of the approach, the method\nis intended to replace the hand-crafted counterpart in next iterations of the\nseries."}
{"id": "2510.22732", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22732", "abs": "https://arxiv.org/abs/2510.22732", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation", "comment": "9 pages, NeurIPS 2025 Workshop on Language Agents and World Models", "summary": "We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system"}
{"id": "2510.23304", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23304", "abs": "https://arxiv.org/abs/2510.23304", "authors": ["Riccardo Romanello", "Daniele Lizzio Bosco", "Jacopo Cossio", "Dusan Sutulovic", "Giuseppe Serra", "Carla Piazza", "Paolo Burelli"], "title": "CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach", "comment": null, "summary": "CNOT gates are fundamental to quantum computing, as they facilitate\nentanglement, a crucial resource for quantum algorithms. Certain classes of\nquantum circuits are constructed exclusively from CNOT gates. Given their\nwidespread use, it is imperative to minimise the number of CNOT gates employed.\nThis problem, known as CNOT minimisation, remains an open challenge, with its\ncomputational complexity yet to be fully characterised. In this work, we\nintroduce a novel reinforcement learning approach to address this task. Instead\nof training multiple reinforcement learning agents for different circuit sizes,\nwe use a single agent up to a fixed size $m$. Matrices of sizes different from\nm are preprocessed using either embedding or Gaussian striping. To assess the\nefficacy of our approach, we trained an agent with m = 8, and evaluated it on\nmatrices of size n that range from 3 to 15. The results we obtained show that\nour method overperforms the state-of-the-art algorithm as the value of n\nincreases."}
{"id": "2510.22811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22811", "abs": "https://arxiv.org/abs/2510.22811", "authors": ["Jingyuan Liu", "Hao Qiu", "Lin Yang", "Mengfan Xu"], "title": "Distributed Multi-Agent Bandits Over Erdős-Rényi Random Networks", "comment": null, "summary": "We study the distributed multi-agent multi-armed bandit problem with\nheterogeneous rewards over random communication graphs. Uniquely, at each time\nstep $t$ agents communicate over a time-varying random graph $G_t$ generated by\napplying the Erd\\H{o}s-R\\'enyi model to a fixed connected base graph $G$ (for\nclassical Erd\\H{o}s-R\\'enyi graphs, $G$ is a complete graph), where each\npotential edge in $G$ is randomly and independently present with the link\nprobability $p$. Notably, the resulting random graph is not necessarily\nconnected at each time step. Each agent's arm rewards follow time-invariant\ndistributions, and the reward distribution for the same arm may differ across\nagents. The goal is to minimize the cumulative expected regret relative to the\nglobal mean reward of each arm, defined as the average of that arm's mean\nrewards across all agents. To this end, we propose a fully distributed\nalgorithm that integrates the arm elimination strategy with the random gossip\nalgorithm. We theoretically show that the regret upper bound is of order $\\log\nT$ and is highly interpretable, where $T$ is the time horizon. It includes the\noptimal centralized regret $O\\left(\\sum_{k: \\Delta_k>0} \\frac{\\log\nT}{\\Delta_k}\\right)$ and an additional term $O\\left(\\frac{N^2 \\log T}{p\n\\lambda_{N-1}(Lap(G))} + \\frac{KN^2 \\log T}{p}\\right)$ where $N$ and $K$ denote\nthe total number of agents and arms, respectively. This term reflects the\nimpact of $G$'s algebraic connectivity $\\lambda_{N-1}(Lap(G))$ and the link\nprobability $p$, and thus highlights a fundamental trade-off between\ncommunication efficiency and regret. As a by-product, we show a nearly optimal\nregret lower bound. Finally, our numerical experiments not only show the\nsuperiority of our algorithm over existing benchmarks, but also validate the\ntheoretical regret scaling with problem complexity."}
{"id": "2510.23340", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23340", "abs": "https://arxiv.org/abs/2510.23340", "authors": ["Anwesha Das", "John Duff", "Jörg Hoffmann", "Vera Demberg"], "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps", "comment": "11 pages, 3 figures", "summary": "Adaptive agent design offers a way to improve human-AI collaboration on\ntime-sensitive tasks in rapidly changing environments. In such cases, to ensure\nthe human maintains an accurate understanding of critical task elements, an\nassistive agent must not only identify the highest priority information but\nalso estimate how and when this information can be communicated most\neffectively, given that human attention represents a zero-sum cognitive\nresource where focus on one message diminishes awareness of other or upcoming\ninformation. We introduce a theoretical framework for adaptive signalling which\nmeets these challenges by using principles of rational communication,\nformalised as Bayesian reference resolution using the Rational Speech Act (RSA)\nmodelling framework, to plan a sequence of messages which optimise timely\nalignment between user belief and a dynamic environment. The agent adapts\nmessage specificity and timing to the particulars of a user and scenario based\non projections of how prior-guided interpretation of messages will influence\nattention to the interface and subsequent belief update, across several\ntimesteps out to a fixed horizon. In a comparison to baseline methods, we show\nthat this effectiveness depends crucially on combining multi-step planning with\na realistic model of user awareness. As the first application of RSA for\ncommunication in a dynamic environment, and for human-AI interaction in\ngeneral, we establish theoretical foundations for pragmatic communication in\nhuman-agent teams, highlighting how insights from cognitive science can be\ncapitalised to inform the design of assistive agents."}
{"id": "2510.22835", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.22835", "abs": "https://arxiv.org/abs/2510.22835", "authors": ["Dominik Meier", "Shixing Yu", "Sagnik Nandy", "Promit Ghosal", "Kyra Gan"], "title": "Clustering by Denoising: Latent plug-and-play diffusion for single-cell data", "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables the study of cellular\nheterogeneity. Yet, clustering accuracy, and with it downstream analyses based\non cell labels, remain challenging due to measurement noise and biological\nvariability. In standard latent spaces (e.g., obtained through PCA), data from\ndifferent cell types can be projected close together, making accurate\nclustering difficult. We introduce a latent plug-and-play diffusion framework\nthat separates the observation and denoising space. This separation is\noperationalized through a novel Gibbs sampling procedure: the learned diffusion\nprior is applied in a low-dimensional latent space to perform denoising, while\nto steer this process, noise is reintroduced into the original high-dimensional\nobservation space. This unique \"input-space steering\" ensures the denoising\ntrajectory remains faithful to the original data structure. Our approach offers\nthree key advantages: (1) adaptive noise handling via a tunable balance between\nprior and observed data; (2) uncertainty quantification through principled\nuncertainty estimates for downstream analysis; and (3) generalizable denoising\nby leveraging clean reference data to denoise noisier datasets, and via\naveraging, improve quality beyond the training set. We evaluate robustness on\nboth synthetic and real single-cell genomics data. Our method improves\nclustering accuracy on synthetic data across varied noise levels and dataset\nshifts. On real-world single-cell data, our method demonstrates improved\nbiological coherence in the resulting cell clusters, with cluster boundaries\nthat better align with known cell type markers and developmental trajectories."}
{"id": "2510.23408", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23408", "abs": "https://arxiv.org/abs/2510.23408", "authors": ["Abolfazl Younesi", "Zahra Najafabadi Samani", "Thomas Fahringer"], "title": "AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines", "comment": "Under review", "summary": "Data pipelines are essential in stream processing as they enable the\nefficient collection, processing, and delivery of real-time data, supporting\nrapid data analysis. In this paper, we present AutoStreamPipe, a novel\nframework that employs Large Language Models (LLMs) to automate the design,\ngeneration, and deployment of stream processing pipelines. AutoStreamPipe\nbridges the semantic gap between high-level user intent and platform-specific\nimplementations across distributed stream processing systems for structured\nmulti-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an\nextended version of GoT. AutoStreamPipe combines resilient execution\nstrategies, advanced query analysis, and HGoT to deliver pipelines with good\naccuracy. Experimental evaluations on diverse pipelines demonstrate that\nAutoStreamPipe significantly reduces development time (x6.3) and error rates\n(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM\ncode-generation methods."}
{"id": "2510.22889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22889", "abs": "https://arxiv.org/abs/2510.22889", "authors": ["Darshana Priyasad", "Tharindu Fernando", "Maryam Haghighat", "Harshala Gammulle", "Clinton Fookes"], "title": "Transforming volcanic monitoring: A dataset and benchmark for onboard volcano activity detection", "comment": "Preprint to appear in IEEE IGARSS 2025", "summary": "Natural disasters, such as volcanic eruptions, pose significant challenges to\ndaily life and incur considerable global economic losses. The emergence of\nnext-generation small-satellites, capable of constellation-based operations,\noffers unparalleled opportunities for near-real-time monitoring and onboard\nprocessing of such events. However, a major bottleneck remains the lack of\nextensive annotated datasets capturing volcanic activity, which hinders the\ndevelopment of robust detection systems. This paper introduces a novel dataset\nexplicitly designed for volcanic activity and eruption detection, encompassing\ndiverse volcanoes worldwide. The dataset provides binary annotations to\nidentify volcanic anomalies or non-anomalies, covering phenomena such as\ntemperature anomalies, eruptions, and volcanic ash emissions. These annotations\noffer a foundational resource for developing and evaluating detection models,\naddressing a critical gap in volcanic monitoring research. Additionally, we\npresent comprehensive benchmarks using state-of-the-art models to establish\nbaselines for future studies. Furthermore, we explore the potential for\ndeploying these models onboard next-generation satellites. Using the Intel\nMovidius Myriad X VPU as a testbed, we demonstrate the feasibility of volcanic\nactivity detection directly onboard. This capability significantly reduces\nlatency and enhances response times, paving the way for advanced early warning\nsystems. This paves the way for innovative solutions in volcanic disaster\nmanagement, encouraging further exploration and refinement of onboard\nmonitoring technologies."}
{"id": "2510.23424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23424", "abs": "https://arxiv.org/abs/2510.23424", "authors": ["Elouanes Khelifi", "Amir Saki", "Usef Faghihi"], "title": "Causal Deep Q Network", "comment": null, "summary": "Deep Q Networks (DQN) have shown remarkable success in various reinforcement\nlearning tasks. However, their reliance on associative learning often leads to\nthe acquisition of spurious correlations, hindering their problem-solving\ncapabilities. In this paper, we introduce a novel approach to integrate causal\nprinciples into DQNs, leveraging the PEACE (Probabilistic Easy vAriational\nCausal Effect) formula for estimating causal effects. By incorporating causal\nreasoning during training, our proposed framework enhances the DQN's\nunderstanding of the underlying causal structure of the environment, thereby\nmitigating the influence of confounding factors and spurious correlations. We\ndemonstrate that integrating DQNs with causal capabilities significantly\nenhances their problem-solving capabilities without compromising performance.\nExperimental results on standard benchmark environments showcase that our\napproach outperforms conventional DQNs, highlighting the effectiveness of\ncausal reasoning in reinforcement learning. Overall, our work presents a\npromising avenue for advancing the capabilities of deep reinforcement learning\nagents through principled causal inference."}
{"id": "2510.22940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22940", "abs": "https://arxiv.org/abs/2510.22940", "authors": ["Judah Goldfeder", "Matthew So", "Hod Lipson"], "title": "RL-AUX: Reinforcement Learning for Auxiliary Task Generation", "comment": null, "summary": "Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in\nwhich a network trains on auxiliary tasks to improve performance on its main\ntask. This technique is used to improve generalization and, ultimately,\nperformance on the network's main task. AL has been demonstrated to improve\nperformance across multiple domains, including navigation, image\nclassification, and natural language processing. One weakness of AL is the need\nfor labeled auxiliary tasks, which can require human effort and domain\nexpertise to generate. Meta Learning techniques have been used to solve this\nissue by learning an additional auxiliary task generation network that can\ncreate helpful tasks for the primary network. The most prominent techniques\nrely on Bi-Level Optimization, which incurs computational cost and increased\ncode complexity. To avoid the need for Bi-Level Optimization, we present an\nRL-based approach to dynamically create auxiliary tasks. In this framework, an\nRL agent is tasked with selecting auxiliary labels for every data point in a\ntraining set. The agent is rewarded when their selection improves the\nperformance on the primary task. We also experiment with learning optimal\nstrategies for weighing the auxiliary loss per data point. On the 20-Superclass\nCIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and\nperforms as well as a prominent Bi-Level Optimization technique. Our weight\nlearning approaches significantly outperform all of these benchmarks. For\nexample, a Weight-Aware RL-based approach helps the VGG16 architecture achieve\n80.9% test accuracy while the human-labeled auxiliary task setup achieved\n75.53%. The goal of this work is to (1) prove that RL is a viable approach to\ndynamically generate auxiliary tasks and (2) demonstrate that per-sample\nauxiliary task weights can be learned alongside the auxiliary task labels and\ncan achieve strong results."}
{"id": "2510.23443", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23443", "abs": "https://arxiv.org/abs/2510.23443", "authors": ["Chiara Bonfanti", "Alessandro Druetto", "Cataldo Basile", "Tharindu Ranasinghe", "Marcos Zampieri"], "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration", "comment": "7 pages", "summary": "The growing intersection of cybersecurity and law creates a complex\ninformation space where traditional legal research tools struggle to deal with\nnuanced connections between cases, statutes, and technical vulnerabilities.\nThis knowledge divide hinders collaboration between legal experts and\ncybersecurity professionals. To address this important gap, this work provides\na first step towards intelligent systems capable of navigating the increasingly\nintricate cyber-legal domain. We demonstrate promising initial results on\nmultilingual tasks."}
{"id": "2510.22955", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22955", "abs": "https://arxiv.org/abs/2510.22955", "authors": ["Junhao Fan", "Wenrui Liang", "Wei-Qiang Zhang"], "title": "SARNet: A Spike-Aware consecutive validation Framework for Accurate Remaining Useful Life Prediction", "comment": "5 pages, 2 figures, 3 tables. Equal contribution by Junhao Fan and\n  Wenrui Liang. Corresponding author: Wei-Qiang Zhang. Submitted to ICASSP 2026", "summary": "Accurate prediction of remaining useful life (RUL) is essential to enhance\nsystem reliability and reduce maintenance risk. Yet many strong contemporary\nmodels are fragile around fault onset and opaque to engineers: short,\nhigh-energy spikes are smoothed away or misread, fixed thresholds blunt\nsensitivity, and physics-based explanations are scarce. To remedy this, we\nintroduce SARNet (Spike-Aware Consecutive Validation Framework), which builds\non a Modern Temporal Convolutional Network (ModernTCN) and adds spike-aware\ndetection to provide physics-informed interpretability. ModernTCN forecasts\ndegradation-sensitive indicators; an adaptive consecutive threshold validates\ntrue spikes while suppressing noise. Failure-prone segments then receive\ntargeted feature engineering (spectral slopes, statistical derivatives, energy\nratios), and the final RUL is produced by a stacked RF--LGBM regressor. Across\nbenchmark-ported datasets under an event-triggered protocol, SARNet\nconsistently lowers error compared to recent baselines (RMSE 0.0365, MAE\n0.0204) while remaining lightweight, robust, and easy to deploy."}
{"id": "2510.23476", "categories": ["cs.AI", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23476", "abs": "https://arxiv.org/abs/2510.23476", "authors": ["Sima Noorani", "Shayan Kiyani", "George Pappas", "Hamed Hassani"], "title": "Human-AI Collaborative Uncertainty Quantification", "comment": null, "summary": "AI predictive systems are increasingly embedded in decision making pipelines,\nshaping high stakes choices once made solely by humans. Yet robust decisions\nunder uncertainty still rely on capabilities that current AI lacks: domain\nknowledge not captured by data, long horizon context, and reasoning grounded in\nthe physical world. This gap has motivated growing efforts to design\ncollaborative frameworks that combine the complementary strengths of humans and\nAI. This work advances this vision by identifying the fundamental principles of\nHuman AI collaboration within uncertainty quantification, a key component of\nreliable decision making. We introduce Human AI Collaborative Uncertainty\nQuantification, a framework that formalizes how an AI model can refine a human\nexpert's proposed prediction set with two goals: avoiding counterfactual harm,\nensuring the AI does not degrade correct human judgments, and complementarity,\nenabling recovery of correct outcomes the human missed. At the population\nlevel, we show that the optimal collaborative prediction set follows an\nintuitive two threshold structure over a single score function, extending a\nclassical result in conformal prediction. Building on this insight, we develop\npractical offline and online calibration algorithms with provable distribution\nfree finite sample guarantees. The online method adapts to distribution shifts,\nincluding human behavior evolving through interaction with AI, a phenomenon we\ncall Human to AI Adaptation. Experiments across image classification,\nregression, and text based medical decision making show that collaborative\nprediction sets consistently outperform either agent alone, achieving higher\ncoverage and smaller set sizes across various conditions."}
{"id": "2510.22977", "categories": ["cs.LG", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2510.22977", "abs": "https://arxiv.org/abs/2510.22977", "authors": ["Chenlong Yin", "Zeyang Sha", "Shiwen Cui", "Changhua Meng"], "title": "The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination", "comment": "18 pages, 5 figures", "summary": "Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key\nstrategy for building Agents that \"think then act.\" However, recent\nobservations, like OpenAI's o3, suggest a paradox: stronger reasoning often\ncoincides with increased hallucination, yet no prior work has systematically\nexamined whether reasoning enhancement itself causes tool hallucination. To\naddress this gap, we pose the central question: Does strengthening reasoning\nincrease tool hallucination? To answer this, we introduce SimpleToolHalluBench,\na diagnostic benchmark measuring tool hallucination in two failure modes: (i)\nno tool available, and (ii) only distractor tools available. Through controlled\nexperiments, we establish three key findings. First, we demonstrate a causal\nrelationship: progressively enhancing reasoning through RL increases tool\nhallucination proportionally with task performance gains. Second, this effect\ntranscends overfitting - training on non-tool tasks (e.g., mathematics) still\namplifies subsequent tool hallucination. Third, the effect is method-agnostic,\nappearing when reasoning is instilled via supervised fine-tuning and when it is\nmerely elicited at inference by switching from direct answers to step-by-step\nthinking. We also evaluate mitigation strategies including Prompt Engineering\nand Direct Preference Optimization (DPO), revealing a fundamental\nreliability-capability trade-off: reducing hallucination consistently degrades\nutility. Mechanistically, Reasoning RL disproportionately collapses\ntool-reliability-related representations, and hallucinations surface as\namplified divergences concentrated in late-layer residual streams. These\nfindings reveal that current reasoning enhancement methods inherently amplify\ntool hallucination, highlighting the need for new training objectives that\njointly optimize for capability and reliability."}
{"id": "2510.23487", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.23487", "abs": "https://arxiv.org/abs/2510.23487", "authors": ["Roham Koohestani", "Ziyou Li", "Anton Podkopaev", "Maliheh Izadi"], "title": "Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy", "comment": null, "summary": "This paper establishes a formal equivalence between the architectural classes\nof modern agentic AI systems and the abstract machines of the Chomsky\nhierarchy. We posit that the memory architecture of an AI agent is the\ndefinitive feature determining its computational power and that it directly\nmaps it to a corresponding class of automaton. Specifically, we demonstrate\nthat simple reflex agents are equivalent to Finite Automata, hierarchical\ntask-decomposition agents are equivalent to Pushdown Automata, and agents\nemploying readable/writable memory for reflection are equivalent to TMs. This\nAutomata-Agent Framework provides a principled methodology for right-sizing\nagent architectures to optimize computational efficiency and cost. More\ncritically, it creates a direct pathway to formal verification, enables the\napplication of mature techniques from automata theory to guarantee agent safety\nand predictability. By classifying agents, we can formally delineate the\nboundary between verifiable systems and those whose behavior is fundamentally\nundecidable. We address the inherent probabilistic nature of LLM-based agents\nby extending the framework to probabilistic automata that allow quantitative\nrisk analysis. The paper concludes by outlining an agenda for developing static\nanalysis tools and grammars for agentic frameworks."}
{"id": "2510.22982", "categories": ["cs.LG", "H.3.5; I.2.6; I.2.10; I.2.7; I.6.5"], "pdf": "https://arxiv.org/pdf/2510.22982", "abs": "https://arxiv.org/abs/2510.22982", "authors": ["Guanchen Du", "Jianlong Xu", "Mingtong Li", "Ruiqi Wang", "Qianqing Guo", "Caiyi Chen", "Qingcao Dai", "Yuxiang Zeng"], "title": "QoSGMAA: A Robust Multi-Order Graph Attention and Adversarial Framework for Sparse QoS Prediction", "comment": null, "summary": "With the rapid advancement of internet technologies, network services have\nbecome critical for delivering diverse and reliable applications to users.\nHowever, the exponential growth in the number of available services has\nresulted in many similar offerings, posing significant challenges in selecting\noptimal services. Predicting Quality of Service (QoS) accurately thus becomes a\nfundamental prerequisite for ensuring reliability and user satisfaction.\nHowever, existing QoS prediction methods often fail to capture rich contextual\ninformation and exhibit poor performance under extreme data sparsity and\nstructural noise. To bridge this gap, we propose a novel architecture, QoSMGAA,\nspecifically designed to enhance prediction accuracy in complex and noisy\nnetwork service environments. QoSMGAA integrates a multi-order attention\nmechanism to aggregate extensive contextual data and predict missing QoS values\neffectively. Additionally, our method incorporates adversarial neural networks\nto perform autoregressive supervised learning based on transformed interaction\nmatrices. To capture complex, higher-order interactions among users and\nservices, we employ a discrete sampling technique leveraging the Gumbel-Softmax\nmethod to generate informative negative samples. Comprehensive experimental\nvalidation conducted on large-scale real-world datasets demonstrates that our\nproposed model significantly outperforms existing baseline methods,\nhighlighting its strong potential for practical deployment in service selection\nand recommendation scenarios."}
{"id": "2510.23524", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23524", "abs": "https://arxiv.org/abs/2510.23524", "authors": ["KC Santosh", "Rodrigue Rizk", "Longwei Wang"], "title": "Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence", "comment": "9 pages, 3 figures", "summary": "The rapid advancement of Artificial Intelligence (AI) has led to\nunprecedented computational demands, raising significant environmental and\nethical concerns. This paper critiques the prevailing reliance on large-scale,\nstatic datasets and monolithic training paradigms, advocating for a shift\ntoward human-inspired, sustainable AI solutions. We introduce a novel\nframework, Human AI (HAI), which emphasizes incremental learning, carbon-aware\noptimization, and human-in-the-loop collaboration to enhance adaptability,\nefficiency, and accountability. By drawing parallels with biological cognition\nand leveraging dynamic architectures, HAI seeks to balance performance with\necological responsibility. We detail the theoretical foundations, system\ndesign, and operational principles that enable AI to learn continuously and\ncontextually while minimizing carbon footprints and human annotation costs. Our\napproach addresses pressing challenges in active learning, continual\nadaptation, and energy-efficient model deployment, offering a pathway toward\nresponsible, human-centered artificial intelligence."}
{"id": "2510.23053", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.23053", "abs": "https://arxiv.org/abs/2510.23053", "authors": ["Zhiyu Wang", "Suman Raj", "Rajkumar Buyya"], "title": "AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing", "comment": null, "summary": "Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing\n(MEC) systems face critical challenges in coordinating trajectory planning,\ntask offloading, and resource allocation while ensuring Quality of Service\n(QoS) under dynamic and uncertain environments. Existing approaches suffer from\nlimited scalability, slow convergence, and inefficient knowledge sharing among\nUAVs, particularly when handling large-scale IoT device deployments with\nstringent deadline constraints. This paper proposes AirFed, a novel federated\ngraph-enhanced multi-agent reinforcement learning framework that addresses\nthese challenges through three key innovations. First, we design dual-layer\ndynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal\ndependencies among UAVs and IoT devices, capturing both service relationships\nand collaborative interactions within the network topology. Second, we develop\na dual-Actor single-Critic architecture that jointly optimizes continuous\ntrajectory control and discrete task offloading decisions. Third, we propose a\nreputation-based decentralized federated learning mechanism with\ngradient-sensitive adaptive quantization, enabling efficient and robust\nknowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate\nthat AirFed achieves 42.9% reduction in weighted cost compared to\nstate-of-the-art baselines, attains over 99% deadline satisfaction and 94.2%\nIoT device coverage rate, and reduces communication overhead by 54.5%.\nScalability analysis confirms robust performance across varying UAV numbers,\nIoT device densities, and system scales, validating AirFed's practical\napplicability for large-scale UAV-MEC deployments."}
{"id": "2510.23538", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23538", "abs": "https://arxiv.org/abs/2510.23538", "authors": ["Qiushi Sun", "Jingyang Gong", "Yang Liu", "Qiaosheng Chen", "Lei Li", "Kai Chen", "Qipeng Guo", "Ben Kao", "Fei Yuan"], "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence", "comment": "Work in progress", "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder."}
{"id": "2510.23142", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23142", "abs": "https://arxiv.org/abs/2510.23142", "authors": ["Chi Liu"], "title": "Rethinking GSPO: The Perplexity-Entropy Equivalence", "comment": "10 pages, 2 figures", "summary": "We provide a new perspective on GSPO's length-normalized importance ratios by\nestablishing their connection to information-theoretic quantities. We show that\nGSPO's sequence-level weight $s(\\theta) =\n(\\pi_\\theta/\\pi_{\\theta_{\\text{old}}})^{1/|y|}$ can be equivalently expressed\nas the inverse perplexity ratio\n$\\text{PPL}_{\\theta_{\\text{old}}}/\\text{PPL}_\\theta$ and as the exponential\ncross-entropy change $\\exp(\\Delta H)$. While the perplexity-entropy\nrelationship follows from standard definitions, this observation provides a\nuseful lens for understanding GSPO: the algorithm weights policy gradient\nupdates by perplexity ratios, offering an information-theoretic interpretation\nof the importance weights. This perspective helps explain GSPO's empirical\nproperties, including log-domain variance reduction through geometric averaging\nand stability in training mixture-of-experts models. We validate the\nmathematical equivalences and variance predictions through controlled\nexperiments on mathematical reasoning tasks."}
{"id": "2510.23564", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23564", "abs": "https://arxiv.org/abs/2510.23564", "authors": ["Zhaoyang Yu", "Jiayi Zhang", "Huixue Su", "Yufan Zhao", "Yifan Wu", "Mingyi Deng", "Jinyu Xiang", "Yizhang Lin", "Lingxiao Tang", "Yingchao Li", "Yuyu Luo", "Bang Liu", "Chenglin Wu"], "title": "ReCode: Unify Plan and Action for Universal Granularity Control", "comment": null, "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode."}
{"id": "2510.23148", "categories": ["cs.LG", "cs.AI", "eess.IV", "I.2.6; I.2.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.23148", "abs": "https://arxiv.org/abs/2510.23148", "authors": ["Aryan Mathur", "Asaduddin Ahmed"], "title": "Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI", "comment": "Undergraduate research project, IIT Palakkad, 2025", "summary": "Deep reinforcement learning agents often struggle when tasks require\nunderstanding both vision and language. Conventional architectures typically\nisolate perception (for example, CNN-based visual encoders) from\ndecision-making (policy networks). This separation can be inefficient, since\nthe policy's failures do not directly help the perception module learn what is\nimportant. To address this, we implement the Perception-Decision Interleaving\nTransformer (PDiT) architecture introduced by Mao et al. (2023), a model that\nalternates between perception and decision layers within a single transformer.\nThis interleaving allows feedback from decision-making to refine perceptual\nfeatures dynamically. In addition, we integrate a contrastive loss inspired by\nCLIP to align textual mission embeddings with visual scene features. We\nevaluate the PDiT encoders on the BabyAI GoToLocal environment and find that\nthe approach achieves more stable rewards and stronger alignment compared to a\nstandard PPO baseline. The results suggest that interleaved transformer\nencoders are a promising direction for developing more integrated autonomous\nagents."}
{"id": "2510.23595", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23595", "abs": "https://arxiv.org/abs/2510.23595", "authors": ["Yixing Chen", "Yiding Wang", "Siqi Zhu", "Haofei Yu", "Tao Feng", "Muhan Zhang", "Mostofa Patwary", "Jiaxuan You"], "title": "Multi-Agent Evolve: LLM Self-Improve through Co-evolution", "comment": "29 pages, 4 figures", "summary": "Reinforcement Learning (RL) has demonstrated significant potential in\nenhancing the reasoning capabilities of large language models (LLMs). However,\nthe success of RL for LLMs heavily relies on human-curated datasets and\nverifiable rewards, which limit their scalability and generality. Recent\nSelf-Play RL methods, inspired by the success of the paradigm in games and Go,\naim to enhance LLM reasoning capabilities without human-annotated data.\nHowever, their methods primarily depend on a grounded environment for feedback\n(e.g., a Python interpreter or a game engine); extending them to general\ndomains remains challenging. To address these challenges, we propose\nMulti-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in\nsolving diverse tasks, including mathematics, reasoning, and general knowledge\nQ&A. The core design of MAE is based on a triplet of interacting agents\n(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies\nreinforcement learning to optimize their behaviors. The Proposer generates\nquestions, the Solver attempts solutions, and the Judge evaluates both while\nco-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves\nan average improvement of 4.54% on multiple benchmarks. These results highlight\nMAE as a scalable, data-efficient method for enhancing the general reasoning\nabilities of LLMs with minimal reliance on human-curated supervision."}
{"id": "2510.23191", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23191", "abs": "https://arxiv.org/abs/2510.23191", "authors": ["Timo Freiesleben", "Sebastian Zezulka"], "title": "The Benchmarking Epistemology: Construct Validity for Evaluating Machine Learning Models", "comment": null, "summary": "Predictive benchmarking, the evaluation of machine learning models based on\npredictive performance and competitive ranking, is a central epistemic practice\nin machine learning research and an increasingly prominent method for\nscientific inquiry. Yet, benchmark scores alone provide at best measurements of\nmodel performance relative to an evaluation dataset and a concrete learning\nproblem. Drawing substantial scientific inferences from the results, say about\ntheoretical tasks like image classification, requires additional assumptions\nabout the theoretical structure of the learning problems, evaluation functions,\nand data distributions. We make these assumptions explicit by developing\nconditions of construct validity inspired by psychological measurement theory.\nWe examine these assumptions in practice through three case studies, each\nexemplifying a typical intended inference: measuring engineering progress in\ncomputer vision with ImageNet; evaluating policy-relevant weather predictions\nwith WeatherBench; and examining limitations of the predictability of life\nevents with the Fragile Families Challenge. Our framework clarifies the\nconditions under which benchmark scores can support diverse scientific claims,\nbringing predictive benchmarking into perspective as an epistemological\npractice and a key site of conceptual and theoretical reasoning in machine\nlearning."}
{"id": "2510.23601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23601", "abs": "https://arxiv.org/abs/2510.23601", "authors": ["Jiahao Qiu", "Xuan Qi", "Hongru Wang", "Xinzhe Juan", "Yimin Wang", "Zelin Zhao", "Jiayi Geng", "Jiacheng Guo", "Peihang Li", "Jingzhe Shi", "Shilong Liu", "Mengdi Wang"], "title": "Alita-G: Self-Evolving Generative Agent for Agent Generation", "comment": "15 pages, 3 figures", "summary": "Large language models (LLMs) have been shown to perform better when\nscaffolded into agents with memory, tools, and feedback. Beyond this,\nself-evolving agents have emerged, but current work largely limits adaptation\nto prompt rewriting or failure retries. Therefore, we present ALITA-G, a\nself-evolution framework that transforms a general-purpose agent into a domain\nexpert by systematically generating, abstracting, and curating Model Context\nProtocol (MCP) tools. In this framework, a generalist agent executes a curated\nsuite of target-domain tasks and synthesizes candidate MCPs from successful\ntrajectories. These are then abstracted to parameterized primitives and\nconsolidated into an MCP Box. At inference time, ALITA-G performs\nretrieval-augmented MCP selection with the help of each tool's descriptions and\nuse cases, before executing an agent equipped with the MCP Executor. Across\nseveral benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains\nstrong gains while reducing computation costs. On GAIA validation, it achieves\n83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result\nwhile reducing mean tokens per example by approximately 15% relative to a\nstrong baseline agent. ALITA-G thus provides a principled pathway from\ngeneralist capability to reusable, domain-specific competence, improving both\naccuracy and efficiency on complex reasoning tasks."}
{"id": "2510.23215", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.23215", "abs": "https://arxiv.org/abs/2510.23215", "authors": ["Hong Wang", "Jie Wang", "Jian Luo", "huanshuo dong", "Yeqiu Chen", "Runmin Jiang", "Zhen huang"], "title": "Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter", "comment": null, "summary": "Eigenvalue problems are among the most important topics in many scientific\ndisciplines. With the recent surge and development of machine learning, neural\neigenvalue methods have attracted significant attention as a forward pass of\ninference requires only a tiny fraction of the computation time compared to\ntraditional solvers. However, a key limitation is the requirement for large\namounts of labeled data in training, including operators and their eigenvalues.\nTo tackle this limitation, we propose a novel method, named Sorting Chebyshev\nSubspace Filter (SCSF), which significantly accelerates eigenvalue data\ngeneration by leveraging similarities between operators -- a factor overlooked\nby existing methods. Specifically, SCSF employs truncated fast Fourier\ntransform sorting to group operators with similar eigenvalue distributions and\nconstructs a Chebyshev subspace filter that leverages eigenpairs from\npreviously solved problems to assist in solving subsequent ones, reducing\nredundant computations. To the best of our knowledge, SCSF is the first method\nto accelerate eigenvalue data generation. Experimental results show that SCSF\nachieves up to a $3.5\\times$ speedup compared to various numerical solvers."}
{"id": "2510.21788", "categories": ["cs.LG", "cs.AI", "I.2; G.3"], "pdf": "https://arxiv.org/pdf/2510.21788", "abs": "https://arxiv.org/abs/2510.21788", "authors": ["Larkin Liu", "Jalal Etesami"], "title": "Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "We explore the use of expert-guided bandit learning, which we refer to as\nonline mixture-of-experts (OMoE). In this setting, given a context, a candidate\ncommittee of experts must determine how to aggregate their outputs to achieve\noptimal results in terms of aggregate accuracy. We propose two algorithms to\naddress this problem. The first algorithm combines aggregate voting with\nUCB-driven successive elimination, efficiently pruning suboptimal exploration\nactions. The second algorithm employs an online weighted-majority-voting\nmechanism, leveraging the respective voting power of each expert proportional\nto their predictive power. We derive theoretical guarantees for the regret\nproperties in the bandit setting under ideal circumstances, and empirical\nresults are provided accordingly. As a modern study on applications, these\nmethods are applied to the online fine-tuning of a set of expert large language\nmodels (LLMs), where after each response, the generative LLM dynamically\nreweighs its set of experts and/or selects the optimal committee of experts to\ngenerate the most accurate response. Our results introduce new methodologies\nand no-regret guarantees for combining multiple experts to improve on the\nperformance of the an aggregate model overall."}
{"id": "2510.23535", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.23535", "abs": "https://arxiv.org/abs/2510.23535", "authors": ["Chen Lu", "Ke Xue", "Lei Yuan", "Yao Wang", "Yaoyuan Wang", "Sheng Fu", "Chao Qian"], "title": "Sequential Multi-Agent Dynamic Algorithm Configuration", "comment": "NeurIPS 2025", "summary": "Dynamic algorithm configuration (DAC) is a recent trend in automated machine\nlearning, which can dynamically adjust the algorithm's configuration during the\nexecution process and relieve users from tedious trial-and-error tuning tasks.\nRecently, multi-agent reinforcement learning (MARL) approaches have improved\nthe configuration of multiple heterogeneous hyperparameters, making various\nparameter configurations for complex algorithms possible. However, many complex\nalgorithms have inherent inter-dependencies among multiple parameters (e.g.,\ndetermining the operator type first and then the operator's parameter), which\nare, however, not considered in previous approaches, thus leading to\nsub-optimal results. In this paper, we propose the sequential multi-agent DAC\n(Seq-MADAC) framework to address this issue by considering the inherent\ninter-dependencies of multiple parameters. Specifically, we propose a\nsequential advantage decomposition network, which can leverage action-order\ninformation through sequential advantage decomposition. Experiments from\nsynthetic functions to the configuration of multi-objective optimization\nalgorithms demonstrate Seq-MADAC's superior performance over state-of-the-art\nMARL methods and show strong generalization across problem classes. Seq-MADAC\nestablishes a new paradigm for the widespread dependency-aware automated\nalgorithm configuration. Our code is available at\nhttps://github.com/lamda-bbo/seq-madac."}
{"id": "2510.21935", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.21935", "abs": "https://arxiv.org/abs/2510.21935", "authors": ["Samuel Bright-Thonney", "Christina Reissel", "Gaia Grosso", "Nathaniel Woodward", "Katya Govorkova", "Andrzej Novak", "Sang Eon Park", "Eric Moreno", "Philip Harris"], "title": "AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing", "comment": "Accepted at NeurIPS 2025; 32 pages, 16 figures", "summary": "Novelty detection in large scientific datasets faces two key challenges: the\nnoisy and high-dimensional nature of experimental data, and the necessity of\nmaking statistically robust statements about any observed outliers. While there\nis a wealth of literature on anomaly detection via dimensionality reduction,\nmost methods do not produce outputs compatible with quantifiable claims of\nscientific discovery. In this work we directly address these challenges,\npresenting the first step towards a unified pipeline for novelty detection\nadapted for the rigorous statistical demands of science. We introduce\nAutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive\nTesting), a general-purpose pipeline for detecting novelty in scientific data.\nAutoSciDACT begins by creating expressive low-dimensional data representations\nusing a contrastive pre-training, leveraging the abundance of high-quality\nsimulated data in many scientific domains alongside expertise that can guide\nprincipled data augmentation strategies. These compact embeddings then enable\nan extremely sensitive machine learning-based two-sample test using the New\nPhysics Learning Machine (NPLM) framework, which identifies and statistically\nquantifies deviations in observed data relative to a reference distribution\n(null hypothesis). We perform experiments across a range of astronomical,\nphysical, biological, image, and synthetic datasets, demonstrating strong\nsensitivity to small injections of anomalous data across all domains."}
{"id": "2510.21855", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.21855", "abs": "https://arxiv.org/abs/2510.21855", "authors": ["Ryan Zhang", "Herbert Woisetscläger"], "title": "SIGN: Schema-Induced Games for Naming", "comment": "AAAI 2026 Student Abstract (Oral). Code available ar\n  https://github.com/ryanzhangofficial/schema-induced-games-for-naming", "summary": "Real-world AI systems are tackling increasingly complex problems, often\nthrough interactions among large language model (LLM) agents. When these agents\ndevelop inconsistent conventions, coordination can break down. Applications\nsuch as collaborative coding and distributed planning therefore require\nreliable, consistent communication, and scalability is a central concern as\nsystems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming\ngame that examines how lightweight structure can steer convention formation. We\ncompare schema-induced communication to unconstrained natural language and find\nfaster convergence with up to 5.8x higher agreement. These results suggest that\nminimal structure can act as a simple control knob for efficient multi-agent\ncoordination, pointing toward broader applications beyond the naming game."}
{"id": "2510.22057", "categories": ["cs.LG", "cs.AI", "cs.CY", "I.5.1; I.4.7"], "pdf": "https://arxiv.org/pdf/2510.22057", "abs": "https://arxiv.org/abs/2510.22057", "authors": ["James Thiering", "Tarun Sethupat Radha Krishna", "Dylan Zelkin", "Ashis Kumer Biswas"], "title": "Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model", "comment": "13 pages, 12 figures, and 1 table", "summary": "With the rise of online and virtual learning, monitoring and enhancing\nstudent engagement have become an important aspect of effective education.\nTraditional methods of assessing a student's involvement might not be\napplicable directly to virtual environments. In this study, we focused on this\nproblem and addressed the need to develop an automated system to detect student\nengagement levels during online learning. We proposed a novel training method\nwhich can discourage a model from leveraging sensitive features like gender for\nits predictions. The proposed method offers benefits not only in the\nenforcement of ethical standards, but also to enhance interpretability of the\nmodel predictions. We applied an attribute-orthogonal regularization technique\nto a split-model classifier, which uses multiple transfer learning strategies\nto achieve effective results in reducing disparity in the distribution of\nprediction for sensitivity groups from a Pearson correlation coefficient of\n0.897 for the unmitigated model, to 0.999 for the mitigated model. The source\ncode for this project is available on\nhttps://github.com/ashiskb/elearning-engagement-study ."}
{"id": "2510.22052", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22052", "abs": "https://arxiv.org/abs/2510.22052", "authors": ["Abhijit Chatterjee", "Niraj K. Jha", "Jonathan D. Cohen", "Thomas L. Griffiths", "Hongjing Lu", "Diana Marculescu", "Ashiqur Rasul", "Keshab K. Parhi"], "title": "Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms", "comment": null, "summary": "The field of artificial intelligence (AI) has taken a tight hold on broad\naspects of society, industry, business, and governance in ways that dictate the\nprosperity and might of the world's economies. The AI market size is projected\nto grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI\nis dominated by large language models that exhibit linguistic and visual\nintelligence. However, training these models requires a massive amount of data\nscraped from the web as well as large amounts of energy (50--60 GWh to train\nGPT-4). Despite these costs, these models often hallucinate, a characteristic\nthat prevents them from being deployed in critical application domains. In\ncontrast, the human brain consumes only 20~W of power. What is needed is the\nnext level of AI evolution in which lightweight domain-specific multimodal\nmodels with higher levels of intelligence can reason, plan, and make decisions\nin dynamic environments with real-time data and prior knowledge, while learning\ncontinuously and evolving in ways that enhance future decision-making\ncapability. This will define the next wave of AI, progressing from today's\nlarge models, trained with vast amounts of data, to nimble energy-efficient\ndomain-specific agents that can reason and think in a world full of\nuncertainty. To support such agents, hardware will need to be reimagined to\nallow energy efficiencies greater than 1000x over the state of the art. Such a\nvision of future AI systems is developed in this work."}
{"id": "2510.22075", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22075", "abs": "https://arxiv.org/abs/2510.22075", "authors": ["Siyu Zhu", "Anastasiya Karpovich", "Albert Chen", "Jessica Koscheka", "Shailesh Jannu", "Di Wen", "Yuqing Zhu", "Rohit Jain", "Alborz Geramifard"], "title": "Agentic Reinforcement Learning for Real-World Code Repair", "comment": null, "summary": "We tackle the challenge of training reliable code-fixing agents in real\nrepositories, where complex builds and shifting dependencies make evaluation\nunstable. We developed a verifiable pipeline with success defined as post-fix\nbuild validation and improved reproducibility across ~1K real issues by pinning\ndependencies and disabling automatic upgrades. Building on this, we introduced\na scalable simplified pipeline for large-scale reinforcement learning (RL).\nUsing this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and\napplied RL on top of the SFT model in the simplified environment. The SFT model\ndistilled from GPT-4.1 trajectories performs on par while being 56x smaller,\nand RL added 7-20% absolute gains under matched train-test conditions.\n\"Thinking mode\" was on par or worse in our experiments. Both SFT and RL models\nfailed to generalize across environments, highlighting the importance of\nmatching train-test environments for building reliable real-world code-fixing\nagents."}
{"id": "2510.22620", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22620", "abs": "https://arxiv.org/abs/2510.22620", "authors": ["Julia Bazinska", "Max Mathys", "Francesco Casucci", "Mateo Rojas-Carulla", "Xander Davies", "Alexandra Souly", "Niklas Pfister"], "title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents", "comment": "Julia Bazinska and Max Mathys contributed equally", "summary": "AI agents powered by large language models (LLMs) are being deployed at\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\naffects agent security. The non-deterministic sequential nature of AI agents\ncomplicates security modeling, while the integration of traditional software\nwith AI components entangles novel LLM vulnerabilities with conventional\nsecurity risks. Existing frameworks only partially address these challenges as\nthey either capture specific vulnerabilities only or require modeling of\ncomplete agents. To address these limitations, we introduce threat snapshots: a\nframework that isolates specific states in an agent's execution flow where LLM\nvulnerabilities manifest, enabling the systematic identification and\ncategorization of security risks that propagate from the LLM to the agent\nlevel. We apply this framework to construct the $\\operatorname{b}^3$ benchmark,\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\nenhanced reasoning capabilities improve security, while model size does not\ncorrelate with security. We release our benchmark, dataset, and evaluation code\nto facilitate widespread adoption by LLM providers and practitioners, offering\nguidance for agent developers and incentivizing model developers to prioritize\nbackbone security improvements."}
{"id": "2510.22158", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.22158", "abs": "https://arxiv.org/abs/2510.22158", "authors": ["Lorenzo Magnino", "Kai Shao", "Zida Wu", "Jiacheng Shen", "Mathieu Laurière"], "title": "Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics", "comment": "Neurips 2025", "summary": "Mean field games (MFGs) have emerged as a powerful framework for modeling\ninteractions in large-scale multi-agent systems. Despite recent advancements in\nreinforcement learning (RL) for MFGs, existing methods are typically limited to\nfinite spaces or stationary models, hindering their applicability to real-world\nproblems. This paper introduces a novel deep reinforcement learning (DRL)\nalgorithm specifically designed for non-stationary continuous MFGs. The\nproposed approach builds upon a Fictitious Play (FP) methodology, leveraging\nDRL for best-response computation and supervised learning for average policy\nrepresentation. Furthermore, it learns a representation of the time-dependent\npopulation distribution using a Conditional Normalizing Flow. To validate the\neffectiveness of our method, we evaluate it on three different examples of\nincreasing complexity. By addressing critical limitations in scalability and\ndensity approximation, this work represents a significant advancement in\napplying DRL techniques to complex MFG problems, bringing the field closer to\nreal-world multi-agent systems."}
{"id": "2510.22832", "categories": ["cs.AI", "cs.LG", "stat.ML", "68T07 (Primary) 62M45, 37N99 (Secondary)", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.22832", "abs": "https://arxiv.org/abs/2510.22832", "authors": ["Long H Dang", "David Rawlinson"], "title": "HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning", "comment": "14 pages, 9 figures, 1 table", "summary": "The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities\ngiven its small size, but has only been applied to supervised, static,\nfully-observable problems. One of HRM's strengths is its ability to adapt its\ncomputational effort to the difficulty of the problem. However, in its current\nform it cannot integrate and reuse computation from previous time-steps if the\nproblem is dynamic, uncertain or partially observable, or be applied where the\ncorrect action is undefined, characteristics of many real-world problems.\n  This paper presents HRM-Agent, a variant of HRM trained using only\nreinforcement learning. We show that HRM can learn to navigate to goals in\ndynamic and uncertain maze environments. Recent work suggests that HRM's\nreasoning abilities stem from its recurrent inference process. We explore the\ndynamics of the recurrent inference process and find evidence that it is\nsuccessfully reusing computation from earlier environment time-steps."}
{"id": "2510.22228", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22228", "abs": "https://arxiv.org/abs/2510.22228", "authors": ["Keyu Wang", "Tian Lyu", "Guinan Su", "Jonas Geiping", "Lu Yin", "Marco Canini", "Shiwei Liu"], "title": "When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs", "comment": null, "summary": "Layer pruning has emerged as a widely adopted technique for improving the\nefficiency of large language models (LLMs). Although existing methods\ndemonstrate strong performance retention on general knowledge tasks, their\neffect on long-chain reasoning, a more brittle yet crucial capability, remains\nlargely unexplored. In this work, we study the impact of layer pruning on\nlong-chain reasoning through the lens of test-time scaling, a key mechanism in\nmodern LLMs that enables strong reasoning capacity by allocating more\ncomputation at inference time. With extensive experiments, we demonstrate that\npruning even one or two layers can severely impair test-time scaling, with\nperformance collapsing drastically on long reasoning benchmarks even when\nperformance on knowledge-intensive and shallow reasoning tasks remains stable.\nFurthermore, we find that standard supervised fine-tuning remedies fail to\nrecover test-time scaling once it has deteriorated. Through in-depth analyses,\nwe identify the mechanisms underlying this fragility of test-time scaling and\nhighlight the fundamental risks of applying layer pruning to\nreasoning-intensive LLMs. These findings call for a rethinking of layer pruning\nstrategies and provide insights for developing methods that preserve the\nrobustness of reasoning. We open-source the codebase in\n\\href{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}."}
{"id": "2510.22833", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22833", "abs": "https://arxiv.org/abs/2510.22833", "authors": ["Adrian Orenstein", "Jessica Chen", "Gwyneth Anne Delos Santos", "Bayley Sapara", "Michael Bowling"], "title": "Toward Agents That Reason About Their Computation", "comment": null, "summary": "While reinforcement learning agents can achieve superhuman performance in\nmany complex tasks, they typically do not become more computationally efficient\nas they improve. In contrast, humans gradually require less cognitive effort as\nthey become more proficient at a task. If agents could reason about their\ncompute as they learn, could they similarly reduce their computation footprint?\nIf they could, we could have more energy efficient agents or free up compute\ncycles for other processes like planning. In this paper, we experiment with\nshowing agents the cost of their computation and giving them the ability to\ncontrol when they use compute. We conduct our experiments on the Arcade\nLearning Environment, and our results demonstrate that with the same training\ncompute budget, agents that reason about their compute perform better on 75% of\ngames. Furthermore, these agents use three times less compute on average. We\nanalyze individual games and show where agents gain these efficiencies."}
{"id": "2510.22261", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22261", "abs": "https://arxiv.org/abs/2510.22261", "authors": ["Shireen Kudukkil Manchingal"], "title": "Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know", "comment": "PhD thesis", "summary": "Machine learning has achieved remarkable successes, yet its deployment in\nsafety-critical domains remains hindered by an inherent inability to manage\nuncertainty, resulting in overconfident and unreliable predictions when models\nencounter out-of-distribution data, adversarial perturbations, or naturally\nfluctuating environments. This thesis, titled Epistemic Deep Learning: Enabling\nMachine Learning Models to 'Know When They Do Not Know', addresses these\ncritical challenges by advancing the paradigm of Epistemic Artificial\nIntelligence, which explicitly models and quantifies epistemic uncertainty: the\nuncertainty arising from limited, biased, or incomplete training data, as\nopposed to the irreducible randomness of aleatoric uncertainty, thereby\nempowering models to acknowledge their limitations and refrain from\noverconfident decisions when uncertainty is high.\n  Central to this work is the development of the Random-Set Neural Network\n(RS-NN), a novel methodology that leverages random set theory to predict belief\nfunctions over sets of classes, capturing the extent of epistemic uncertainty\nthrough the width of associated credal sets, applications of RS-NN, including\nits adaptation to Large Language Models (LLMs) and its deployment in weather\nclassification for autonomous racing. In addition, the thesis proposes a\nunified evaluation framework for uncertainty-aware classifiers. Extensive\nexperiments validate that integrating epistemic awareness into deep learning\nnot only mitigates the risks associated with overconfident predictions but also\nlays the foundation for a paradigm shift in artificial intelligence, where the\nability to 'know when it does not know' becomes a hallmark of robust and\ndependable systems. The title encapsulates the core philosophy of this work,\nemphasizing that true intelligence involves recognizing and managing the limits\nof one's own knowledge."}
{"id": "2510.23216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23216", "abs": "https://arxiv.org/abs/2510.23216", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Jean-Philippe Barrette-LaPierre", "Florian Fuchs", "Brady Chen", "Michael Jones", "Linus Gisslén"], "title": "Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach", "comment": null, "summary": "While several high profile video games have served as testbeds for Deep\nReinforcement Learning (DRL), this technique has rarely been employed by the\ngame industry for crafting authentic AI behaviors. Previous research focuses on\ntraining super-human agents with large models, which is impractical for game\nstudios with limited resources aiming for human-like agents. This paper\nproposes a sample-efficient DRL method tailored for training and fine-tuning\nagents in industrial settings such as the video game industry. Our method\nimproves sample efficiency of value-based DRL by leveraging pre-collected data\nand increasing network plasticity. We evaluate our method training a goalkeeper\nagent in EA SPORTS FC 25, one of the best-selling football simulations today.\nOur agent outperforms the game's built-in AI by 10% in ball saving rate.\nAblation studies show that our method trains agents 50% faster compared to\nstandard DRL methods. Finally, qualitative evaluation from domain experts\nindicates that our approach creates more human-like gameplay compared to\nhand-crafted agents. As a testimony of the impact of the approach, the method\nis intended to replace the hand-crafted counterpart in next iterations of the\nseries."}
{"id": "2510.22301", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22301", "abs": "https://arxiv.org/abs/2510.22301", "authors": ["Yujie Xiao", "Gongzhen Tang", "Wenhui Liu", "Jun Li", "Guangkun Nie", "Zhuoran Kan", "Deyun Zhang", "Qinghao Zhao", "Shenda Hong"], "title": "AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals", "comment": null, "summary": "Timely access to laboratory values is critical for clinical decision-making,\nyet current approaches rely on invasive venous sampling and are intrinsically\ndelayed. Electrocardiography (ECG), as a non-invasive and widely available\nsignal, offers a promising modality for rapid laboratory estimation. Recent\nprogress in deep learning has enabled the extraction of latent hematological\nsignatures from ECGs. However, existing models are constrained by low\nsignal-to-noise ratios, substantial inter-individual variability, limited data\ndiversity, and suboptimal generalization, especially when adapted to low-lead\nwearable devices. In this work, we conduct an exploratory study leveraging\ntransfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG\nfoundation model, on the Multimodal Clinical Monitoring in the Emergency\nDepartment (MC-MED) dataset from Stanford. We generated a corpus of more than\n20 million standardized ten-second ECG segments to enhance sensitivity to\nsubtle biochemical correlates. On internal validation, the model demonstrated\nstrong predictive performance (area under the curve above 0.65) for\nthirty-three laboratory indicators, moderate performance (between 0.55 and\n0.65) for fifty-nine indicators, and limited performance (below 0.55) for\nsixteen indicators. This study provides an efficient artificial-intelligence\ndriven solution and establishes the feasibility scope for real-time,\nnon-invasive estimation of laboratory values."}
{"id": "2510.23408", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23408", "abs": "https://arxiv.org/abs/2510.23408", "authors": ["Abolfazl Younesi", "Zahra Najafabadi Samani", "Thomas Fahringer"], "title": "AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines", "comment": "Under review", "summary": "Data pipelines are essential in stream processing as they enable the\nefficient collection, processing, and delivery of real-time data, supporting\nrapid data analysis. In this paper, we present AutoStreamPipe, a novel\nframework that employs Large Language Models (LLMs) to automate the design,\ngeneration, and deployment of stream processing pipelines. AutoStreamPipe\nbridges the semantic gap between high-level user intent and platform-specific\nimplementations across distributed stream processing systems for structured\nmulti-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an\nextended version of GoT. AutoStreamPipe combines resilient execution\nstrategies, advanced query analysis, and HGoT to deliver pipelines with good\naccuracy. Experimental evaluations on diverse pipelines demonstrate that\nAutoStreamPipe significantly reduces development time (x6.3) and error rates\n(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM\ncode-generation methods."}
{"id": "2510.22383", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22383", "abs": "https://arxiv.org/abs/2510.22383", "authors": ["David Freire-Obregón", "José Salas-Cáceres", "Modesto Castrillón-Santana"], "title": "Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization", "comment": "Accepted for presentation at the 5th International Conference on\n  Computing and Machine Intelligence (ICMI 2026)", "summary": "Regularization techniques play a crucial role in preventing overfitting and\nimproving the generalization performance of neural networks. Dropout, a widely\nused regularization technique, randomly deactivates units during training to\nintroduce redundancy and prevent co-adaptation among neurons. Despite its\neffectiveness, dropout has limitations, such as its static nature and lack of\ninterpretability. In this paper, we propose a novel approach to regularization\nby substituting dropout with Conway's Game of Life (GoL), a cellular automata\nwith simple rules that govern the evolution of a grid of cells. We introduce\ndynamic unit deactivation during training by representing neural network units\nas cells in a GoL grid and applying the game's rules to deactivate units. This\napproach allows for the emergence of spatial patterns that adapt to the\ntraining data, potentially enhancing the network's ability to generalize. We\ndemonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing\nthat dynamic unit deactivation using GoL achieves comparable performance to\ntraditional dropout techniques while offering insights into the network's\nbehavior through the visualization of evolving patterns. Furthermore, our\ndiscussion highlights the applicability of our proposal in deeper\narchitectures, demonstrating how it enhances the performance of different\ndropout techniques."}
{"id": "2510.23524", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23524", "abs": "https://arxiv.org/abs/2510.23524", "authors": ["KC Santosh", "Rodrigue Rizk", "Longwei Wang"], "title": "Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence", "comment": "9 pages, 3 figures", "summary": "The rapid advancement of Artificial Intelligence (AI) has led to\nunprecedented computational demands, raising significant environmental and\nethical concerns. This paper critiques the prevailing reliance on large-scale,\nstatic datasets and monolithic training paradigms, advocating for a shift\ntoward human-inspired, sustainable AI solutions. We introduce a novel\nframework, Human AI (HAI), which emphasizes incremental learning, carbon-aware\noptimization, and human-in-the-loop collaboration to enhance adaptability,\nefficiency, and accountability. By drawing parallels with biological cognition\nand leveraging dynamic architectures, HAI seeks to balance performance with\necological responsibility. We detail the theoretical foundations, system\ndesign, and operational principles that enable AI to learn continuously and\ncontextually while minimizing carbon footprints and human annotation costs. Our\napproach addresses pressing challenges in active learning, continual\nadaptation, and energy-efficient model deployment, offering a pathway toward\nresponsible, human-centered artificial intelligence."}
{"id": "2510.22422", "categories": ["cs.MA", "cs.AI", "cs.CY", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.22422", "abs": "https://arxiv.org/abs/2510.22422", "authors": ["Ariel Flint", "Luca Maria Aiello", "Romualdo Pastor-Satorras", "Andrea Baronchelli"], "title": "Group size effects and collective misalignment in LLM multi-agent systems", "comment": null, "summary": "Multi-agent systems of large language models (LLMs) are rapidly expanding\nacross domains, introducing dynamics not captured by single-agent evaluations.\nYet, existing work has mostly contrasted the behavior of a single agent with\nthat of a collective of fixed size, leaving open a central question: how does\ngroup size shape dynamics? Here, we move beyond this dichotomy and\nsystematically explore outcomes across the full range of group sizes. We focus\non multi-agent misalignment, building on recent evidence that interacting LLMs\nplaying a simple coordination game can generate collective biases absent in\nindividual models. First, we show that collective bias is a deeper phenomenon\nthan previously assessed: interaction can amplify individual biases, introduce\nnew ones, or override model-level preferences. Second, we demonstrate that\ngroup size affects the dynamics in a non-linear way, revealing model-dependent\ndynamical regimes. Finally, we develop a mean-field analytical approach and\nshow that, above a critical population size, simulations converge to\ndeterministic predictions that expose the basins of attraction of competing\nequilibria. These findings establish group size as a key driver of multi-agent\ndynamics and highlight the need to consider population-level effects when\ndeploying LLM-based systems at scale."}
{"id": "2510.23564", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23564", "abs": "https://arxiv.org/abs/2510.23564", "authors": ["Zhaoyang Yu", "Jiayi Zhang", "Huixue Su", "Yufan Zhao", "Yifan Wu", "Mingyi Deng", "Jinyu Xiang", "Yizhang Lin", "Lingxiao Tang", "Yingchao Li", "Yuyu Luo", "Bang Liu", "Chenglin Wu"], "title": "ReCode: Unify Plan and Action for Universal Granularity Control", "comment": null, "summary": "Real-world tasks require decisions at varying granularities, and humans excel\nat this by leveraging a unified cognitive representation where planning is\nfundamentally understood as a high-level form of action. However, current Large\nLanguage Model (LLM)-based agents lack this crucial capability to operate\nfluidly across decision granularities. This limitation stems from existing\nparadigms that enforce a rigid separation between high-level planning and\nlow-level action, which impairs dynamic adaptability and limits generalization.\nWe propose ReCode (Recursive Code Generation), a novel paradigm that addresses\nthis limitation by unifying planning and action within a single code\nrepresentation. In this representation, ReCode treats high-level plans as\nabstract placeholder functions, which the agent then recursively decomposes\ninto finer-grained sub-functions until reaching primitive actions. This\nrecursive approach dissolves the rigid boundary between plan and action,\nenabling the agent to dynamically control its decision granularity.\nFurthermore, the recursive structure inherently generates rich,\nmulti-granularity training data, enabling models to learn hierarchical\ndecision-making processes. Extensive experiments show ReCode significantly\nsurpasses advanced baselines in inference performance and demonstrates\nexceptional data efficiency in training, validating our core insight that\nunifying planning and action through recursive code generation is a powerful\nand effective approach to achieving universal granularity control. The code is\navailable at https://github.com/FoundationAgents/ReCode."}
{"id": "2510.22477", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22477", "abs": "https://arxiv.org/abs/2510.22477", "authors": ["Yijia Fan", "Jusheng Zhang", "Jing Yang", "Keze Wang"], "title": "Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization", "comment": null, "summary": "To combat the prohibitive communication costs of ``free-for-all\" multi-agent\nsystems (MAS), we introduce \\textbf{Agent-GSPO}, a framework that directly\noptimizes for token economy using sequence-level reinforcement learning.\nAgent-GSPO leverages the stable and memory-efficient Group Sequence Policy\nOptimization (GSPO) algorithm to train agents on a communication-aware reward\nthat explicitly penalizes verbosity. Across seven reasoning benchmarks,\nAgent-GSPO not only achieves new state-of-the-art performance but does so with\na fraction of the token consumption of existing methods. By fostering emergent\nstrategies like ``strategic silence,\" our approach provides a practical\nblueprint for developing scalable and economically viable multi-agent systems."}
{"id": "2510.22500", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22500", "abs": "https://arxiv.org/abs/2510.22500", "authors": ["Ren Yin", "Takashi Ishida", "Masashi Sugiyama"], "title": "Scalable Oversight via Partitioned Human Supervision", "comment": null, "summary": "As artificial intelligence (AI) systems approach and surpass expert human\nperformance across a broad range of tasks, obtaining high-quality human\nsupervision for evaluation and training becomes increasingly challenging. Our\nfocus is on tasks that require deep knowledge and skills of multiple domains.\nUnfortunately, even the best human experts are knowledgeable only in a single\nnarrow area, and will not be able to evaluate the correctness of advanced AI\nsystems on such superhuman tasks. However, based on their narrow expertise,\nhumans may provide a weak signal, i.e., a complementary label indicating an\noption that is incorrect. For example, a cardiologist could state that \"this is\nnot related to cardiology,'' even if they cannot identify the true disease.\nBased on this weak signal, we propose a scalable oversight framework that\nenables us to evaluate frontier AI systems without the need to prepare the\nground truth. We derive an unbiased estimator of top-1 accuracy from\ncomplementary labels and quantify how many complementary labels are needed to\nmatch the variance of ordinary labels. We further introduce two estimators to\ncombine scarce ordinary labels with abundant complementary labels. We provide\nfinite-sample deviation guarantees for both complementary-only and the mixed\nestimators. Empirically, we show that we can evaluate the output of large\nlanguage models without the ground truth, if we have complementary labels. We\nfurther show that we can train an AI system with such weak signals: we show how\nwe can design an agentic AI system automatically that can perform better with\nthis partitioned human supervision. Our code is available at\nhttps://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision."}
{"id": "2510.22620", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22620", "abs": "https://arxiv.org/abs/2510.22620", "authors": ["Julia Bazinska", "Max Mathys", "Francesco Casucci", "Mateo Rojas-Carulla", "Xander Davies", "Alexandra Souly", "Niklas Pfister"], "title": "Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents", "comment": "Julia Bazinska and Max Mathys contributed equally", "summary": "AI agents powered by large language models (LLMs) are being deployed at\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\naffects agent security. The non-deterministic sequential nature of AI agents\ncomplicates security modeling, while the integration of traditional software\nwith AI components entangles novel LLM vulnerabilities with conventional\nsecurity risks. Existing frameworks only partially address these challenges as\nthey either capture specific vulnerabilities only or require modeling of\ncomplete agents. To address these limitations, we introduce threat snapshots: a\nframework that isolates specific states in an agent's execution flow where LLM\nvulnerabilities manifest, enabling the systematic identification and\ncategorization of security risks that propagate from the LLM to the agent\nlevel. We apply this framework to construct the $\\operatorname{b}^3$ benchmark,\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\nenhanced reasoning capabilities improve security, while model size does not\ncorrelate with security. We release our benchmark, dataset, and evaluation code\nto facilitate widespread adoption by LLM providers and practitioners, offering\nguidance for agent developers and incentivizing model developers to prioritize\nbackbone security improvements."}
{"id": "2510.22655", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22655", "abs": "https://arxiv.org/abs/2510.22655", "authors": ["Berken Utku Demirel", "Christian Holz"], "title": "Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections", "comment": "Published at the Conference on Neural Information Processing Systems\n  (NeurIPS) 2025", "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations without labeled data. Most SSL approaches rely on\nstrong, well-established, handcrafted data augmentations to generate diverse\nviews for representation learning. However, designing such augmentations\nrequires domain-specific knowledge and implicitly imposes representational\ninvariances on the model, which can limit generalization. In this work, we\npropose an unsupervised representation learning method that replaces\naugmentations by generating views using orthonormal bases and overcomplete\nframes. We show that embeddings learned from orthonormal and overcomplete\nspaces reside on distinct manifolds, shaped by the geometric biases introduced\nby representing samples in different spaces. By jointly leveraging the\ncomplementary geometry of these distinct manifolds, our approach achieves\nsuperior performance without artificially increasing data diversity through\nstrong augmentations. We demonstrate the effectiveness of our method on nine\ndatasets across five temporal sequence tasks, where signal-specific\ncharacteristics make data augmentations particularly challenging. Without\nrelying on augmentation-induced diversity, our method achieves performance\ngains of up to 15--20\\% over existing self-supervised approaches. Source code:\nhttps://github.com/eth-siplab/Learning-with-FrameProjections"}
{"id": "2510.22732", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22732", "abs": "https://arxiv.org/abs/2510.22732", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation", "comment": "9 pages, NeurIPS 2025 Workshop on Language Agents and World Models", "summary": "We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system"}
{"id": "2510.22963", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22963", "abs": "https://arxiv.org/abs/2510.22963", "authors": ["Zesen Liu", "Zhixiang Zhang", "Yuchong Xie", "Dongdong She"], "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents", "comment": null, "summary": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections."}
{"id": "2510.22977", "categories": ["cs.LG", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2510.22977", "abs": "https://arxiv.org/abs/2510.22977", "authors": ["Chenlong Yin", "Zeyang Sha", "Shiwen Cui", "Changhua Meng"], "title": "The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination", "comment": "18 pages, 5 figures", "summary": "Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key\nstrategy for building Agents that \"think then act.\" However, recent\nobservations, like OpenAI's o3, suggest a paradox: stronger reasoning often\ncoincides with increased hallucination, yet no prior work has systematically\nexamined whether reasoning enhancement itself causes tool hallucination. To\naddress this gap, we pose the central question: Does strengthening reasoning\nincrease tool hallucination? To answer this, we introduce SimpleToolHalluBench,\na diagnostic benchmark measuring tool hallucination in two failure modes: (i)\nno tool available, and (ii) only distractor tools available. Through controlled\nexperiments, we establish three key findings. First, we demonstrate a causal\nrelationship: progressively enhancing reasoning through RL increases tool\nhallucination proportionally with task performance gains. Second, this effect\ntranscends overfitting - training on non-tool tasks (e.g., mathematics) still\namplifies subsequent tool hallucination. Third, the effect is method-agnostic,\nappearing when reasoning is instilled via supervised fine-tuning and when it is\nmerely elicited at inference by switching from direct answers to step-by-step\nthinking. We also evaluate mitigation strategies including Prompt Engineering\nand Direct Preference Optimization (DPO), revealing a fundamental\nreliability-capability trade-off: reducing hallucination consistently degrades\nutility. Mechanistically, Reasoning RL disproportionately collapses\ntool-reliability-related representations, and hallucinations surface as\namplified divergences concentrated in late-layer residual streams. These\nfindings reveal that current reasoning enhancement methods inherently amplify\ntool hallucination, highlighting the need for new training objectives that\njointly optimize for capability and reliability."}
{"id": "2510.23142", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23142", "abs": "https://arxiv.org/abs/2510.23142", "authors": ["Chi Liu"], "title": "Rethinking GSPO: The Perplexity-Entropy Equivalence", "comment": "10 pages, 2 figures", "summary": "We provide a new perspective on GSPO's length-normalized importance ratios by\nestablishing their connection to information-theoretic quantities. We show that\nGSPO's sequence-level weight $s(\\theta) =\n(\\pi_\\theta/\\pi_{\\theta_{\\text{old}}})^{1/|y|}$ can be equivalently expressed\nas the inverse perplexity ratio\n$\\text{PPL}_{\\theta_{\\text{old}}}/\\text{PPL}_\\theta$ and as the exponential\ncross-entropy change $\\exp(\\Delta H)$. While the perplexity-entropy\nrelationship follows from standard definitions, this observation provides a\nuseful lens for understanding GSPO: the algorithm weights policy gradient\nupdates by perplexity ratios, offering an information-theoretic interpretation\nof the importance weights. This perspective helps explain GSPO's empirical\nproperties, including log-domain variance reduction through geometric averaging\nand stability in training mixture-of-experts models. We validate the\nmathematical equivalences and variance predictions through controlled\nexperiments on mathematical reasoning tasks."}
{"id": "2510.23148", "categories": ["cs.LG", "cs.AI", "eess.IV", "I.2.6; I.2.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.23148", "abs": "https://arxiv.org/abs/2510.23148", "authors": ["Aryan Mathur", "Asaduddin Ahmed"], "title": "Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI", "comment": "Undergraduate research project, IIT Palakkad, 2025", "summary": "Deep reinforcement learning agents often struggle when tasks require\nunderstanding both vision and language. Conventional architectures typically\nisolate perception (for example, CNN-based visual encoders) from\ndecision-making (policy networks). This separation can be inefficient, since\nthe policy's failures do not directly help the perception module learn what is\nimportant. To address this, we implement the Perception-Decision Interleaving\nTransformer (PDiT) architecture introduced by Mao et al. (2023), a model that\nalternates between perception and decision layers within a single transformer.\nThis interleaving allows feedback from decision-making to refine perceptual\nfeatures dynamically. In addition, we integrate a contrastive loss inspired by\nCLIP to align textual mission embeddings with visual scene features. We\nevaluate the PDiT encoders on the BabyAI GoToLocal environment and find that\nthe approach achieves more stable rewards and stronger alignment compared to a\nstandard PPO baseline. The results suggest that interleaved transformer\nencoders are a promising direction for developing more integrated autonomous\nagents."}
{"id": "2510.23215", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.23215", "abs": "https://arxiv.org/abs/2510.23215", "authors": ["Hong Wang", "Jie Wang", "Jian Luo", "huanshuo dong", "Yeqiu Chen", "Runmin Jiang", "Zhen huang"], "title": "Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter", "comment": null, "summary": "Eigenvalue problems are among the most important topics in many scientific\ndisciplines. With the recent surge and development of machine learning, neural\neigenvalue methods have attracted significant attention as a forward pass of\ninference requires only a tiny fraction of the computation time compared to\ntraditional solvers. However, a key limitation is the requirement for large\namounts of labeled data in training, including operators and their eigenvalues.\nTo tackle this limitation, we propose a novel method, named Sorting Chebyshev\nSubspace Filter (SCSF), which significantly accelerates eigenvalue data\ngeneration by leveraging similarities between operators -- a factor overlooked\nby existing methods. Specifically, SCSF employs truncated fast Fourier\ntransform sorting to group operators with similar eigenvalue distributions and\nconstructs a Chebyshev subspace filter that leverages eigenpairs from\npreviously solved problems to assist in solving subsequent ones, reducing\nredundant computations. To the best of our knowledge, SCSF is the first method\nto accelerate eigenvalue data generation. Experimental results show that SCSF\nachieves up to a $3.5\\times$ speedup compared to various numerical solvers."}
