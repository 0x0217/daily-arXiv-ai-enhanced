{"id": "2509.21549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21549", "abs": "https://arxiv.org/abs/2509.21549", "authors": ["Dongkyu Cho", "Amy B. Z. Zhang", "Bilel Fehri", "Sheng Wang", "Rumi Chunara", "Rui Song", "Hengrui Cai"], "title": "Correct Reasoning Paths Visit Shared Decision Pivots", "comment": "18 pages, 10 figures", "summary": "Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of\nlarge language models (LLMs), yet verifying those traces at scale remains\nunsolved. In response, we introduce the idea of decision pivots-minimal,\nverifiable checkpoints that any correct reasoning path must visit. We\nhypothesize that correct reasoning, though stylistically diverse, converge on\nthe same pivot set, while incorrect ones violate at least one pivot. Leveraging\nthis property, we propose a self-training pipeline that (i) samples diverse\nreasoning paths and mines shared decision pivots, (ii) compresses each trace\ninto pivot-focused short-path reasoning using an auxiliary verifier, and (iii)\npost-trains the model using its self-generated outputs. The proposed method\naligns reasoning without ground truth reasoning data or external metrics.\nExperiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the\neffectiveness of our method."}
{"id": "2509.21553", "categories": ["cs.AI", "cs.CE", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21553", "abs": "https://arxiv.org/abs/2509.21553", "authors": ["Ahmed Jaber", "Wangshu Zhu", "Karthick Jayavelu", "Justin Downes", "Sameer Mohamed", "Candace Agonafir", "Linnia Hawkins", "Tian Zheng"], "title": "AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need", "comment": null, "summary": "Climate data science faces persistent barriers stemming from the fragmented\nnature of data sources, heterogeneous formats, and the steep technical\nexpertise required to identify, acquire, and process datasets. These challenges\nlimit participation, slow discovery, and reduce the reproducibility of\nscientific workflows. In this paper, we present a proof of concept for\naddressing these barriers through the integration of a curated knowledge graph\n(KG) with AI agents designed for cloud-native scientific workflows. The KG\nprovides a unifying layer that organizes datasets, tools, and workflows, while\nAI agents -- powered by generative AI services -- enable natural language\ninteraction, automated data access, and streamlined analysis. Together, these\ncomponents drastically lower the technical threshold for engaging in climate\ndata science, enabling non-specialist users to identify and analyze relevant\ndatasets. By leveraging existing cloud-ready API data portals, we demonstrate\nthat \"a knowledge graph is all you need\" to unlock scalable and agentic\nworkflows for scientific inquiry. The open-source design of our system further\nsupports community contributions, ensuring that the KG and associated tools can\nevolve as a shared commons. Our results illustrate a pathway toward\ndemocratizing access to climate data and establishing a reproducible,\nextensible framework for human--AI collaboration in scientific research."}
{"id": "2509.21593", "categories": ["cs.AI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2509.21593", "abs": "https://arxiv.org/abs/2509.21593", "authors": ["Peng Luo", "Xiayin Lou", "Yu Zheng", "Zhuo Zheng", "Stefano Ermon"], "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models", "comment": null, "summary": "Geospatial modeling provides critical solutions for pressing global\nchallenges such as sustainability and climate change. Existing large language\nmodel (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at\nevolving generic code but lack the domain knowledge and multi-step reasoning\nrequired for complex geospatial problems. We introduce GeoEvolve, a multi-agent\nLLM framework that couples evolutionary search with geospatial domain knowledge\nto automatically design and refine geospatial algorithms. GeoEvolve operates in\ntwo nested loops: an inner loop leverages a code evolver to generate and mutate\ncandidate solutions, while an outer agentic controller evaluates global elites\nand queries a GeoKnowRAG module -- a structured geospatial knowledge base that\ninjects theoretical priors from geography. This knowledge-guided evolution\nsteers the search toward theoretically meaningful and computationally efficient\nalgorithms. We evaluate GeoEvolve on two fundamental and classical tasks:\nspatial interpolation (kriging) and spatial uncertainty quantification\n(geospatial conformal prediction). Across these benchmarks, GeoEvolve\nautomatically improves and discovers new algorithms, incorporating geospatial\ntheory on top of classical models. It reduces spatial interpolation error\n(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%.\nAblation studies confirm that domain-guided retrieval is essential for stable,\nhigh-quality evolution. These results demonstrate that GeoEvolve provides a\nscalable path toward automated, knowledge-driven geospatial modeling, opening\nnew opportunities for trustworthy and efficient AI-for-Science discovery."}
{"id": "2509.21600", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21600", "abs": "https://arxiv.org/abs/2509.21600", "authors": ["Mafalda Malafaia", "Peter A. N. Bosman", "Coen Rasch", "Tanja Alderliesten"], "title": "Automated and Interpretable Survival Analysis from Multimodal Data", "comment": "4 figures; 4 tables; 24 pages", "summary": "Accurate and interpretable survival analysis remains a core challenge in\noncology. With growing multimodal data and the clinical need for transparent\nmodels to support validation and trust, this challenge increases in complexity.\nWe propose an interpretable multimodal AI framework to automate survival\nanalysis by integrating clinical variables and computed tomography imaging. Our\nMultiFIX-based framework uses deep learning to infer survival-relevant features\nthat are further explained: imaging features are interpreted via Grad-CAM,\nwhile clinical variables are modeled as symbolic expressions through genetic\nprogramming. Risk estimation employs a transparent Cox regression, enabling\nstratification into groups with distinct survival outcomes. Using the\nopen-source RADCURE dataset for head and neck cancer, MultiFIX achieves a\nC-index of 0.838 (prediction) and 0.826 (stratification), outperforming the\nclinical and academic baseline approaches and aligning with known prognostic\nmarkers. These results highlight the promise of interpretable multimodal AI for\nprecision oncology with MultiFIX."}
{"id": "2509.21403", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21403", "abs": "https://arxiv.org/abs/2509.21403", "authors": ["Rushil Gupta", "Jason Hartford", "Bang Liu"], "title": "LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?", "comment": "Accepted to EMNLP 2025", "summary": "Large language models (LLMs) have recently been proposed as general-purpose\nagents for experimental design, with claims that they can perform in-context\nexperimental design. We evaluate this hypothesis using both open- and\nclosed-source instruction-tuned LLMs applied to genetic perturbation and\nmolecular property discovery tasks. We find that LLM-based agents show no\nsensitivity to experimental feedback: replacing true outcomes with randomly\npermuted labels has no impact on performance. Across benchmarks, classical\nmethods such as linear bandits and Gaussian process optimization consistently\noutperform LLM agents. We further propose a simple hybrid method, LLM-guided\nNearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with\nnearest-neighbor sampling to guide the design of experiments. LLMNN achieves\ncompetitive or superior performance across domains without requiring\nsignificant in-context adaptation. These results suggest that current open- and\nclosed-source LLMs do not perform in-context experimental design in practice\nand highlight the need for hybrid frameworks that decouple prior-based\nreasoning from batch acquisition with updated posteriors."}
{"id": "2509.21475", "categories": ["cs.CR", "cs.CE", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.21475", "abs": "https://arxiv.org/abs/2509.21475", "authors": ["Sen Yang", "Burak Öz", "Fei Wu", "Fan Zhang"], "title": "Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic", "comment": null, "summary": "Decentralization has a geographic dimension that conventional metrics such as\nstake distribution overlook. Where validators run affects resilience to\nregional shocks (outages, disasters, government intervention) and fairness in\nreward access. Yet in permissionless systems, locations cannot be mandated, but\nthey emerge from incentives. Today, Ethereum's validators cluster along the\nAtlantic (EU and U.S. East Coast), where latency is structurally favorable.\nThis raises a key question: when some regions already enjoy latency advantages,\nhow does protocol design shape validator incentives and the geography of\n(de)centralization? We develop a latency-calibrated agent-based model and\ncompare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),\nakin to MEV-Boost, where proposers fetch full blocks from a relay that also\npropagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate\nvalue from multiple sources and broadcast the block themselves. Simulations\nshow that SSP concentrates around relay placement but more slowly, since\nproximity mainly affects propagation, and the marginal value of time is\nrelatively uniform across regions. MSP centralizes faster: aggregating across\nsources makes marginal value location-dependent, amplifying payoff dispersion\nand migration toward latency minima. Source placement and consensus settings\ncan dampen or intensify these effects, though once validators are already\nclustered, the impact of source placement on decentralization is marginal. In\nmost cases, North America consistently emerges as the focal hub. These findings\nshow that protocol design materially shapes validator geography and offer\nlevers for promoting geographical decentralization."}
{"id": "2509.21789", "categories": ["cs.MA", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21789", "abs": "https://arxiv.org/abs/2509.21789", "authors": ["Xinlei Yu", "Chengming Xu", "Guibin Zhang", "Yongbo He", "Zhangquan Chen", "Zhucun Xue", "Jiangning Zhang", "Yue Liao", "Xiaobin Hu", "Yu-Gang Jiang", "Shuicheng Yan"], "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow", "comment": null, "summary": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables\nchallenging tasks but suffers from a novel failure term, multi-agent visual\nhallucination snowballing, where hallucinations are seeded in a single agent\nand amplified by following ones due to the over-reliance on textual flow to\nrelay visual information. Through turn-, layer-, and token-wise attention\nanalyses, we provide detailed insights into the essence of hallucination\nsnowballing regarding the reduction of visual attention allocation. It leads us\nto identify a subset of vision tokens with a unimodal attention peak in middle\nlayers that best preserve visual evidence but gradually diminish in deeper\nagent turns, resulting in the visual hallucination snowballing in MAS. Thus, we\npropose ViF, a lightweight, plug-and-play mitigation paradigm that relays\ninter-agent messages with Visual Flow powered by the selected visual relay\ntokens and applies attention reallocation to amplify this pattern. The\nexperiment results demonstrate that our method markedly reduces hallucination\nsnowballing, consistently improving the performance across eight benchmarks\nbased on four common MAS structures and ten base models. The source code will\nbe available at: https://github.com/YU-deep/ViF.git."}
{"id": "2509.21651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21651", "abs": "https://arxiv.org/abs/2509.21651", "authors": ["Abhishek Jindal", "Dmitry Kalashnikov", "Oscar Chang", "Divya Garikapati", "Anirudha Majumdar", "Pierre Sermanet", "Vikas Sindhwani"], "title": "Can AI Perceive Physical Danger and Intervene?", "comment": null, "summary": "When AI interacts with the physical world -- as a robot or an assistive agent\n-- new safety challenges emerge beyond those of purely ``digital AI\". In such\ninteractions, the potential for physical harm is direct and immediate. How well\ndo state-of-the-art foundation models understand common-sense facts about\nphysical safety, e.g. that a box may be too heavy to lift, or that a hot cup of\ncoffee should not be handed to a child? In this paper, our contributions are\nthree-fold: first, we develop a highly scalable approach to continuous physical\nsafety benchmarking of Embodied AI systems, grounded in real-world injury\nnarratives and operational safety constraints. To probe multi-modal safety\nunderstanding, we turn these narratives and constraints into photorealistic\nimages and videos capturing transitions from safe to unsafe states, using\nadvanced generative models. Secondly, we comprehensively analyze the ability of\nmajor foundation models to perceive risks, reason about safety, and trigger\ninterventions; this yields multi-faceted insights into their deployment\nreadiness for safety-critical agentic applications. Finally, we develop a\npost-training paradigm to teach models to explicitly reason about\nembodiment-specific safety constraints provided through system instructions.\nThe resulting models generate thinking traces that make safety reasoning\ninterpretable and transparent, achieving state of the art performance in\nconstraint satisfaction evaluations. The benchmark will be released at\nhttps://asimov-benchmark.github.io/v2"}
{"id": "2509.21465", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21465", "abs": "https://arxiv.org/abs/2509.21465", "authors": ["George Yakushev", "Alina Shutova", "Ivan Rubachev", "Renat Sergazinov", "Artem Babenko"], "title": "Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data", "comment": "Preprint, code at https://github.com/yandex-research/TalkingTrees", "summary": "Tabular foundation models are becoming increasingly popular for low-resource\ntabular problems. These models make up for small training datasets by\npretraining on large volumes of synthetic data. The prior knowledge obtained\nvia pretraining provides the exceptional performance, but the resulting model\nbecomes a black box that is difficult to interpret and costly to inference. In\nthis work, we explore an alternative strategy: using reasoning-capable LLMs to\ninduce decision trees for small tabular datasets in agentic setup. We design a\nminimal set of tools for constructing, analyzing and manipulating decision\ntrees. By using these tools, LLMs combine their prior knowledge with learning\nfrom data to create a lightweight decision tree that outperforms traditional\nCART on low-resource tabular problems. While a single decision tree does not\noutperform state-of-the-art black box models, it comes with a human-readable\nreasoning trace that can be checked for biases and data leaks. Furthermore, the\nreasoning-based LLM's creation process allows for additional human input:\ncorrecting biases or incorporating domain-specific intuition that is not\ncaptured in the data."}
{"id": "2509.21634", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21634", "abs": "https://arxiv.org/abs/2509.21634", "authors": ["Prakhar Sharma", "Haohuang Wen", "Vinod Yegneswaran", "Ashish Gehani", "Phillip Porras", "Zhiqiang Lin"], "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs", "comment": null, "summary": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G."}
{"id": "2509.21834", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21834", "abs": "https://arxiv.org/abs/2509.21834", "authors": ["Shengxiang Xu", "Jiayi Zhang", "Shimin Di", "Yuyu Luo", "Liang Yao", "Hanmo Liu", "Jia Zhu", "Fan Liu", "Min-Ling Zhang"], "title": "RobustFlow: Towards Robust Agentic Workflow Generation", "comment": null, "summary": "The automated generation of agentic workflows is a promising frontier for\nenabling large language models (LLMs) to solve complex tasks. However, our\ninvestigation reveals that the robustness of agentic workflow remains a\ncritical, unaddressed challenge. Current methods often generate wildly\ninconsistent workflows when provided with instructions that are semantically\nidentical but differently phrased. This brittleness severely undermines their\nreliability and trustworthiness for real-world applications. To quantitatively\ndiagnose this instability, we propose metrics based on nodal and topological\nsimilarity to evaluate workflow consistency against common semantic variations\nsuch as paraphrasing and noise injection. Subsequently, we further propose a\nnovel training framework, RobustFlow, that leverages preference optimization to\nteach models invariance to instruction variations. By training on sets of\nsynonymous task descriptions, RobustFlow boosts workflow robustness scores to\n70\\% - 90\\%, which is a substantial improvement over existing approaches. The\ncode is publicly available at https://github.com/DEFENSE-SEU/RobustFlow."}
{"id": "2509.21766", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21766", "abs": "https://arxiv.org/abs/2509.21766", "authors": ["Haotian Luo", "Huaisong Zhang", "Xuelin Zhang", "Haoyu Wang", "Zeyu Qin", "Wenjie Lu", "Guozheng Ma", "Haiying He", "Yingsha Xie", "Qiyang Zhou", "Zixuan Hu", "Hongze Mi", "Yibo Wang", "Naiqiang Tan", "Hong Chen", "Yi R. Fung", "Chun Yuan", "Li Shen"], "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios", "comment": null, "summary": "Autonomous agents have recently achieved remarkable progress across diverse\ndomains, yet most evaluations focus on short-horizon, fully observable tasks.\nIn contrast, many critical real-world tasks, such as large-scale software\ndevelopment, commercial investment, and scientific discovery, unfold in\nlong-horizon and partially observable scenarios where success hinges on\nsustained reasoning, planning, memory management, and tool use. Existing\nbenchmarks rarely capture these long-horizon challenges, leaving a gap in\nsystematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a\nnovel benchmark that measures the foundational capabilities essential for\ncomplex real-world challenges. We use exploration as a unifying task across\nthree distinct environments to validate these core competencies. Agents are\ndesigned in long-horizon discovery tasks where they must iteratively uncover\nhidden rules through sustained reasoning, planning, memory and tools\nmanagement, and interaction with environments. Under the heaviest scale\nsetting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool\ncalls, whereas in standard configurations they still exceed \\textbf{35k} tokens\nand involve more than \\textbf{60} tool calls on average. Our extensive\nexperiments reveal that LLM-agents consistently underperform in these settings,\nwhereas human participants achieve higher scores, underscoring a persistent gap\nin agents' long-horizon abilities. We also observe that simple scaling fails in\nour task. To better illustrate the failure of agents, we conduct an in-depth\nanalysis of collected trajectories. We identify eight types of errors and\nattribute them to two primary causes: in-context locking and functional\nfundamental capability gaps.\n\\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available\nhere.}"}
{"id": "2509.21579", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21579", "abs": "https://arxiv.org/abs/2509.21579", "authors": ["Mst Eshita Khatun", "Halima Akter", "Tasnimul Rehan", "Toufiq Ahmed"], "title": "Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews", "comment": "Accepted & presented at THE 16th INTERNATIONAL IEEE CONFERENCE ON\n  COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT) 2025", "summary": "In this digital era, online shopping is common practice in our daily lives.\nProduct reviews significantly influence consumer buying behavior and help\nestablish buyer trust. However, the prevalence of fraudulent reviews undermines\nthis trust by potentially misleading consumers and damaging the reputations of\nthe sellers. This research addresses this pressing issue by employing advanced\nbig data analytics and machine learning approaches on a substantial dataset of\nAmazon product reviews. The primary objective is to detect and classify spam\nreviews accurately so that it enhances the authenticity of the review. Using a\nscalable big data framework, we efficiently process and analyze a large scale\nof review data, extracting key features indicative of fraudulent behavior. Our\nstudy illustrates the utility of various machine learning classifiers in\ndetecting spam reviews, with Logistic Regression achieving an accuracy of\n90.35%, thus contributing to a more trustworthy and transparent online shopping\nenvironment."}
{"id": "2509.21712", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21712", "abs": "https://arxiv.org/abs/2509.21712", "authors": ["Bingcan Guo", "Eryue Xu", "Zhiping Zhang", "Tianshi Li"], "title": "Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing", "comment": null, "summary": "Aligning AI systems with human privacy preferences requires understanding\nindividuals' nuanced disclosure behaviors beyond general norms. Yet eliciting\nsuch boundaries remains challenging due to the context-dependent nature of\nprivacy decisions and the complex trade-offs involved. We present an AI-powered\nelicitation approach that probes individuals' privacy boundaries through a\ndiscriminative task. We conducted a between-subjects study that systematically\nvaried communication roles and delegation conditions, resulting in 1,681\nboundary specifications from 169 participants for 61 scenarios. We examined how\nthese contextual factors and individual differences influence the boundary\nspecification. Quantitative results show that communication roles influence\nindividuals' acceptance of detailed and identifiable disclosure, AI delegation\nand individuals' need for privacy heighten sensitivity to disclosed\nidentifiers, and AI delegation results in less consensus across individuals.\nOur findings highlight the importance of situating privacy preference\nelicitation within real-world data flows. We advocate using nuanced privacy\nboundaries as an alignment goal for future AI systems."}
{"id": "2509.22130", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22130", "abs": "https://arxiv.org/abs/2509.22130", "authors": ["Merve Atasever", "Matthew Hong", "Mihir Nitin Kulkarni", "Qingpei Li", "Jyotirmoy V. Deshmukh"], "title": "Multi-Agent Path Finding via Offline RL and LLM Collaboration", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) poses a significant and challenging problem\ncritical for applications in robotics and logistics, particularly due to its\ncombinatorial complexity and the partial observability inherent in realistic\nenvironments. Decentralized reinforcement learning methods commonly encounter\ntwo substantial difficulties: first, they often yield self-centered behaviors\namong agents, resulting in frequent collisions, and second, their reliance on\ncomplex communication modules leads to prolonged training times, sometimes\nspanning weeks. To address these challenges, we propose an efficient\ndecentralized planning framework based on the Decision Transformer (DT),\nuniquely leveraging offline reinforcement learning to substantially reduce\ntraining durations from weeks to mere hours. Crucially, our approach\neffectively handles long-horizon credit assignment and significantly improves\nperformance in scenarios with sparse and delayed rewards. Furthermore, to\novercome adaptability limitations inherent in standard RL methods under dynamic\nenvironmental changes, we integrate a large language model (GPT-4o) to\ndynamically guide agent policies. Extensive experiments in both static and\ndynamically changing environments demonstrate that our DT-based approach,\naugmented briefly by GPT-4o, significantly enhances adaptability and\nperformance."}
{"id": "2509.21782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21782", "abs": "https://arxiv.org/abs/2509.21782", "authors": ["Junliang Liu", "Jingyu Xiao", "Wenxin Tang", "Wenxuan Wang", "Zhixian Wang", "Minrui Zhang", "Shuanghe Yu"], "title": "Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety", "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly positioned as AI\ncollaborators for building complex web-related applications like GUI agents and\nfront-end code generation. However, existing benchmarks largely emphasize\nvisual perception or UI code generation, showing insufficient evaluation on the\nreasoning, robustness and safety capability required for end-to-end web\napplications. To bridge the gap, we introduce a comprehensive web understanding\nbenchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and\nSafety across eight tasks, such as position relationship reasoning, color\nrobustness, and safety critical detection, etc. The benchmark is constructed\nfrom 729 websites and contains 3799 question answer pairs that probe multi-step\ninference over page structure, text, widgets, and safety-critical interactions.\nTo ensure reliable measurement, we adopt standardized prompts, deterministic\nevaluation scripts, and multi-stage quality control combining automatic checks\nwith targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The\nresults reveal significant gaps, models still struggle with compositional and\ncross-element reasoning over realistic layouts, show limited robustness when\nfacing perturbations in user interfaces and content such as layout\nrearrangements or visual style shifts, and are rather conservative in\nrecognizing and avoiding safety critical or irreversible actions. Our code is\navailable at https://github.com/jinliang-byte/webssrbench."}
{"id": "2509.21654", "categories": ["cs.LG", "cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.21654", "abs": "https://arxiv.org/abs/2509.21654", "authors": ["Rina Panigrahy", "Vatsal Sharan"], "title": "Limitations on Safe, Trusted, Artificial General Intelligence", "comment": "17 pages, 1 figure", "summary": "Safety, trust and Artificial General Intelligence (AGI) are aspirational\ngoals in artificial intelligence (AI) systems, and there are several informal\ninterpretations of these notions. In this paper, we propose strict,\nmathematical definitions of safety, trust, and AGI, and demonstrate a\nfundamental incompatibility between them. We define safety of a system as the\nproperty that it never makes any false claims, trust as the assumption that the\nsystem is safe, and AGI as the property of an AI system always matching or\nexceeding human capability. Our core finding is that -- for our formal\ndefinitions of these notions -- a safe and trusted AI system cannot be an AGI\nsystem: for such a safe, trusted system there are task instances which are\neasily and provably solvable by a human but not by the system. We note that we\nconsider strict mathematical definitions of safety and trust, and it is\npossible for real-world deployments to instead rely on alternate, practical\ninterpretations of these notions. We show our results for program verification,\nplanning, and graph reachability. Our proofs draw parallels to G\\\"odel's\nincompleteness theorems and Turing's proof of the undecidability of the halting\nproblem, and can be regarded as interpretations of G\\\"odel's and Turing's\nresults."}
{"id": "2509.21772", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21772", "abs": "https://arxiv.org/abs/2509.21772", "authors": ["Daiki Chiba", "Hiroki Nakano", "Takashi Koide"], "title": "PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation", "comment": null, "summary": "Phishing attacks are a significant societal threat, disproportionately\nharming vulnerable populations and eroding trust in essential digital services.\nCurrent defenses are often reactive, failing against modern evasive tactics\nlike cloaking that conceal malicious content. To address this, we introduce\nPhishLumos, an adaptive multi-agent system that proactively mitigates entire\nattack campaigns. It confronts a core cybersecurity imbalance: attackers can\neasily scale operations, while defense remains an intensive expert task.\nInstead of being blocked by evasion, PhishLumos treats it as a critical signal\nto investigate the underlying infrastructure. Its Large Language Model\n(LLM)-powered agents uncover shared hosting, certificates, and domain\nregistration patterns. On real-world data, our system identified 100% of\ncampaigns in the median case, over a week before their confirmation by\ncybersecurity experts. PhishLumos demonstrates a practical shift from reactive\nURL blocking to proactive campaign mitigation, protecting users before they are\nharmed and making the digital world safer for all."}
{"id": "2509.22216", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22216", "abs": "https://arxiv.org/abs/2509.22216", "authors": ["Ahmet Onur Akman", "Anastasia Psarou", "Zoltán György Varga", "Grzegorz Jamróz", "Rafał Kucharski"], "title": "Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach", "comment": "Work presented at the European Workshop on Reinforcement Learning\n  (EWRL 2024)", "summary": "This study examines the potential impact of reinforcement learning\n(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic\nenvironment. We focus on a simplified day-to-day route choice problem in a\nmulti-agent setting. We consider a city network where human drivers travel\nthrough their chosen routes to reach their destinations in minimum travel time.\nThen, we convert one-third of the population into AVs, which are RL agents\nemploying Deep Q-learning algorithm. We define a set of optimization targets,\nor as we call them behaviors, namely selfish, collaborative, competitive,\nsocial, altruistic, and malicious. We impose a selected behavior on AVs through\ntheir rewards. We run our simulations using our in-house developed RL framework\nPARCOUR. Our simulations reveal that AVs optimize their travel times by up to\n5\\%, with varying impacts on human drivers' travel times depending on the AV\nbehavior. In all cases where AVs adopt a self-serving behavior, they achieve\nshorter travel times than human drivers. Our findings highlight the complexity\ndifferences in learning tasks of each target behavior. We demonstrate that the\nmulti-agent RL setting is applicable for collective routing on traffic\nnetworks, though their impact on coexisting parties greatly varies with the\nbehaviors adopted."}
{"id": "2509.21799", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21799", "abs": "https://arxiv.org/abs/2509.21799", "authors": ["Hongze Mi", "Yibo Feng", "Wenjie Lu", "Yuqi Wang", "Jinyuan Li", "Song Cao", "He Cui", "Tengfei Tian", "Xuelin Zhang", "Haotian Luo", "Di Sun", "Naiqiang Tan", "Gang Pan"], "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents", "comment": null, "summary": "Graphical User Interface (GUI) agents aim to automate a wide spectrum of\nhuman tasks by emulating user interaction. Despite rapid advancements, current\napproaches are hindered by several critical challenges: data bottleneck in\nend-to-end training, high cost of delayed error detection, and risk of\ncontradictory guidance. Inspired by the human cognitive loop of Thinking,\nAlignment, and Reflection, we present D-Artemis -- a novel deliberative\nframework in this paper. D-Artemis leverages a fine-grained, app-specific tip\nretrieval mechanism to inform its decision-making process. It also employs a\nproactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)\nCheck module and Action Correction Agent (ACA) work in concert to mitigate the\nrisk of execution failures. A post-execution Status Reflection Agent (SRA)\ncompletes the cognitive loop, enabling strategic learning from experience.\nCrucially, D-Artemis enhances the capabilities of general-purpose Multimodal\nlarge language models (MLLMs) for GUI tasks without the need for training on\ncomplex trajectory datasets, demonstrating strong generalization. D-Artemis\nestablishes new state-of-the-art (SOTA) results across both major benchmarks,\nachieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.\nExtensive ablation studies further demonstrate the significant contribution of\neach component to the framework."}
{"id": "2509.21659", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2509.21659", "abs": "https://arxiv.org/abs/2509.21659", "authors": ["Siming Shan", "Min Zhu", "Youzuo Lin", "Lu Lu"], "title": "RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion", "comment": null, "summary": "Partial differential equation (PDE)-governed inverse problems are fundamental\nacross various scientific and engineering applications; yet they face\nsignificant challenges due to nonlinearity, ill-posedness, and sensitivity to\nnoise. Here, we introduce a new computational framework, RED-DiffEq, by\nintegrating physics-driven inversion and data-driven learning. RED-DiffEq\nleverages pretrained diffusion models as a regularization mechanism for\nPDE-governed inverse problems. We apply RED-DiffEq to solve the full waveform\ninversion problem in geophysics, a challenging seismic imaging technique that\nseeks to reconstruct high-resolution subsurface velocity models from seismic\nmeasurement data. Our method shows enhanced accuracy and robustness compared to\nconventional methods. Additionally, it exhibits strong generalization ability\nto more complex velocity models that the diffusion model is not trained on. Our\nframework can also be directly applied to diverse PDE-governed inverse\nproblems."}
{"id": "2509.21884", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21884", "abs": "https://arxiv.org/abs/2509.21884", "authors": ["Bochuan Cao", "Changjiang Li", "Yuanpu Cao", "Yameng Ge", "Ting Wang", "Jinghui Chen"], "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors", "comment": "29 pages, 10 tables, 6figures, accepted by CCS 25", "summary": "Large language models (LLMs) have been widely adopted across various\napplications, leveraging customized system prompts for diverse tasks. Facing\npotential system prompt leakage risks, model developers have implemented\nstrategies to prevent leakage, primarily by disabling LLMs from repeating their\ncontext when encountering known attack patterns. However, it remains vulnerable\nto new and unforeseen prompt-leaking techniques. In this paper, we first\nintroduce a simple yet effective prompt leaking attack to reveal such risks.\nOur attack is capable of extracting system prompts from various LLM-based\napplication, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our\nfindings further inspire us to search for a fundamental solution to the\nproblems by having no system prompt in the context. To this end, we propose\nSysVec, a novel method that encodes system prompts as internal representation\nvectors rather than raw text. By doing so, SysVec minimizes the risk of\nunauthorized disclosure while preserving the LLM's core language capabilities.\nRemarkably, this approach not only enhances security but also improves the\nmodel's general instruction-following abilities. Experimental results\ndemonstrate that SysVec effectively mitigates prompt leakage attacks, preserves\nthe LLM's functional integrity, and helps alleviate the forgetting issue in\nlong-context scenarios."}
{"id": "2509.22218", "categories": ["cs.MA", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.22218", "abs": "https://arxiv.org/abs/2509.22218", "authors": ["Sandaru Fernando", "Imasha Jayarathne", "Sithumini Abeysekara", "Shanuja Sithamparanthan", "Thushari Silva", "Deshan Jayawardana"], "title": "VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture", "comment": null, "summary": "Data visualization is essential for interpreting complex datasets, yet\ntraditional tools often require technical expertise, limiting accessibility.\nVizGen is an AI-assisted graph generation system that empowers users to create\nmeaningful visualizations using natural language. Leveraging advanced NLP and\nLLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries\ninto SQL and recommends suitable graph types. Built on a multi-agent\narchitecture, VizGen handles SQL generation, graph creation, customization, and\ninsight extraction. Beyond visualization, it analyzes data for patterns,\nanomalies, and correlations, and enhances user understanding by providing\nexplanations enriched with contextual information gathered from the internet.\nThe system supports real-time interaction with SQL databases and allows\nconversational graph refinement, making data analysis intuitive and accessible.\nVizGen democratizes data visualization by bridging the gap between technical\ncomplexity and user-friendly design."}
{"id": "2509.21823", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21823", "abs": "https://arxiv.org/abs/2509.21823", "authors": ["Gaole Dai", "Shiqi Jiang", "Ting Cao", "Yuqing Yang", "Yuanchun Li", "Rui Tan", "Mo Li", "Lili Qiu"], "title": "ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration", "comment": "10 pages, 7 figures", "summary": "Reward is critical to the evaluation and training of large language models\n(LLMs). However, existing rule-based or model-based reward methods struggle to\ngeneralize to GUI agents, where access to ground-truth trajectories or\napplication databases is often unavailable, and static trajectory-based\nLLM-as-a-Judge approaches suffer from limited accuracy. To address these\nchallenges, we propose ProRe, a proactive reward system that leverages a\ngeneral-purpose reasoner and domain-specific evaluator agents (actors). The\nreasoner schedules targeted state probing tasks, which the evaluator agents\nthen execute by actively interacting with the environment to collect additional\nobservations. This enables the reasoner to assign more accurate and verifiable\nrewards to GUI agents. Empirical results on over 3K trajectories demonstrate\nthat ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,\nrespectively. Furthermore, integrating ProRe with state-of-the-art policy\nagents yields a success rate improvement of up to 22.4%."}
{"id": "2509.21666", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21666", "abs": "https://arxiv.org/abs/2509.21666", "authors": ["Joshua Salim", "Jordan Yu", "Xilei Zhao"], "title": "DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks", "comment": null, "summary": "While deep learning models excel at predictive tasks, they often overfit due\nto their complex structure and large number of parameters, causing them to\nmemorize training data, including noise, rather than learn patterns that\ngeneralize to new data. To tackle this challenge, this paper proposes a new\nregularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep\nNeural Networks (DIM), which maintains domain-informed monotonic relationships\nin complex deep learning models to further improve predictions. Specifically,\nour method enforces monotonicity by penalizing violations relative to a linear\nbaseline, effectively encouraging the model to follow expected trends while\npreserving its predictive power. We formalize this approach through a\ncomprehensive mathematical framework that establishes a linear reference,\nmeasures deviations from monotonic behavior, and integrates these measurements\ninto the training objective. We test and validate the proposed methodology\nusing a real-world ridesourcing dataset from Chicago and a synthetically\ncreated dataset. Experiments across various neural network architectures show\nthat even modest monotonicity constraints consistently enhance model\nperformance. DIM enhances the predictive performance of deep neural networks by\napplying domain-informed monotonicity constraints to regularize model behavior\nand mitigate overfitting"}
{"id": "2509.22022", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22022", "abs": "https://arxiv.org/abs/2509.22022", "authors": ["Marc Damie", "Florian Hahn", "Andreas Peter", "Jan Ramon"], "title": "Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions", "comment": "Accepted in DPM 2025", "summary": "Distributed Point Functions (DPFs) enable sharing secret point functions\nacross multiple parties, supporting privacy-preserving technologies such as\nPrivate Information Retrieval, and anonymous communications. While 2-party\nPRG-based schemes with logarithmic key sizes have been known for a decade,\nextending these solutions to multi-party settings has proven challenging. In\nparticular, PRG-based multi-party DPFs have historically struggled with\npracticality due to key sizes growing exponentially with the number of parties\nand the field size.\n  Our work addresses this efficiency bottleneck by optimizing the PRG-based\nmulti-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the\nhonest-majority assumption, we eliminate the exponential factor present in this\nscheme. Our construction is the first PRG-based multi-party DPF scheme with\npractical key sizes, and provides key up to 3x smaller than the best known\nmulti-party DPF. This work demonstrates that with careful optimization,\nPRG-based multi-party DPFs can achieve practical performances, and even obtain\ntop performances."}
{"id": "2509.22596", "categories": ["cs.MA", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.22596", "abs": "https://arxiv.org/abs/2509.22596", "authors": ["Qixin Zhang", "Yan Sun", "Can Jin", "Xikun Zhang", "Yao Shu", "Puning Zhao", "Li Shen", "Dacheng Tao"], "title": "Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives", "comment": "Accepted to NeurIPS 2025", "summary": "In this paper, we present two effective policy learning algorithms for\nmulti-agent online coordination(MA-OC) problem. The first one, \\texttt{MA-SPL},\nnot only can achieve the optimal $(1-\\frac{c}{e})$-approximation guarantee for\nthe MA-OC problem with submodular objectives but also can handle the unexplored\n$\\alpha$-weakly DR-submodular and $(\\gamma,\\beta)$-weakly submodular scenarios,\nwhere $c$ is the curvature of the investigated submodular functions, $\\alpha$\ndenotes the diminishing-return(DR) ratio and the tuple $(\\gamma,\\beta)$\nrepresents the submodularity ratios. Subsequently, in order to reduce the\nreliance on the unknown parameters $\\alpha,\\gamma,\\beta$ inherent in the\n\\texttt{MA-SPL} algorithm, we further introduce the second online algorithm\nnamed \\texttt{MA-MPL}. This \\texttt{MA-MPL} algorithm is entirely\n\\emph{parameter-free} and simultaneously can maintain the same approximation\nratio as the first \\texttt{MA-SPL} algorithm. The core of our \\texttt{MA-SPL}\nand \\texttt{MA-MPL} algorithms is a novel continuous-relaxation technique\ntermed as \\emph{policy-based continuous extension}. Compared with the\nwell-established \\emph{multi-linear extension}, a notable advantage of this new\n\\emph{policy-based continuous extension} is its ability to provide a lossless\nrounding scheme for any set function, thereby enabling us to tackle the\nchallenging weakly submodular objectives. Finally, extensive simulations are\nconducted to validate the effectiveness of our proposed algorithms."}
{"id": "2509.21825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21825", "abs": "https://arxiv.org/abs/2509.21825", "authors": ["Jaehyun Nam", "Jinsung Yoon", "Jiefeng Chen", "Jinwoo Shin", "Tomas Pfister"], "title": "DS-STAR: Data Science Agent via Iterative Planning and Verification", "comment": null, "summary": "Data science, which transforms raw data into actionable insights, is critical\nfor data-driven decision-making. However, these tasks are often complex,\ninvolving steps for exploring multiple data sources and synthesizing findings\nto deliver insightful answers. While large language models (LLMs) show\nsignificant promise in automating this process, they often struggle with\nheterogeneous data formats and generate sub-optimal analysis plans, as\nverifying plan sufficiency is inherently difficult without ground-truth labels\nfor such open-ended tasks. To overcome these limitations, we introduce DS-STAR,\na novel data science agent. Specifically, DS-STAR makes three key\ncontributions: (1) a data file analysis module that automatically explores and\nextracts context from diverse data formats, including unstructured types; (2) a\nverification step where an LLM-based judge evaluates the sufficiency of the\nanalysis plan at each stage; and (3) a sequential planning mechanism that\nstarts with a simple, executable plan and iteratively refines it based on the\nDS-STAR's feedback until its sufficiency is verified. This iterative refinement\nallows DS-STAR to reliably navigate complex analyses involving diverse data\nsources. Our experiments show that DS-STAR achieves state-of-the-art\nperformance across three challenging benchmarks: DABStep, KramaBench, and\nDA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks\nthat require processing multiple data files with heterogeneous formats."}
{"id": "2509.21735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21735", "abs": "https://arxiv.org/abs/2509.21735", "authors": ["Houliang Zhou", "Rong Zhou", "Yangying Liu", "Kanhao Zhao", "Li Shen", "Brian Y. Chen", "Yu Zhang", "Lifang He", "Alzheimer's Disease Neuroimaging Initiative"], "title": "Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks", "comment": null, "summary": "Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease\n(AD) progression is crucial for timely intervention. However, this task remains\nchallenging due to the complex dysfunctions in the spatio-temporal\ncharacteristics of underlying brain networks, which are often overlooked by\nexisting methods. To address these limitations, we develop an interpretable\nspatio-temporal graph neural network framework to predict future AD\nprogression, leveraging dual Stochastic Differential Equations (SDEs) to model\nthe irregularly-sampled longitudinal functional magnetic resonance imaging\n(fMRI) data. We validate our approach on two independent cohorts, including the\nOpen Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease\nNeuroimaging Initiative (ADNI). Our framework effectively learns sparse\nregional and connective importance probabilities, enabling the identification\nof key brain circuit abnormalities associated with disease progression.\nNotably, we detect the parahippocampal cortex, prefrontal cortex, and parietal\nlobule as salient regions, with significant disruptions in the ventral\nattention, dorsal attention, and default mode networks. These abnormalities\ncorrelate strongly with longitudinal AD-related clinical symptoms. Moreover,\nour interpretability strategy reveals both established and novel neural\nsystems-level and sex-specific biomarkers, offering new insights into the\nneurobiological mechanisms underlying AD progression. Our findings highlight\nthe potential of spatio-temporal graph-based learning for early, individualized\nprediction of AD progression, even in the context of irregularly-sampled\nlongitudinal imaging data."}
{"id": "2509.22040", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22040", "abs": "https://arxiv.org/abs/2509.22040", "authors": ["Yue Liu", "Yanjie Zhao", "Yunbo Lyu", "Ting Zhang", "Haoyu Wang", "David Lo"], "title": "\"Your AI, My Shell\": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors", "comment": null, "summary": "Agentic AI coding editors driven by large language models have recently\nbecome more popular due to their ability to improve developer productivity\nduring software development. Modern editors such as Cursor are designed not\njust for code completion, but also with more system privileges for complex\ncoding tasks (e.g., run commands in the terminal, access development\nenvironments, and interact with external systems). While this brings us closer\nto the \"fully automated programming\" dream, it also raises new security\nconcerns. In this study, we present the first empirical analysis of prompt\ninjection attacks targeting these high-privilege agentic AI coding editors. We\nshow how attackers can remotely exploit these systems by poisoning external\ndevelopment resources with malicious instructions, effectively hijacking AI\nagents to run malicious commands, turning \"your AI\" into \"attacker's shell\". To\nperform this analysis, we implement AIShellJack, an automated testing framework\nfor assessing prompt injection vulnerabilities in agentic AI coding editors.\nAIShellJack contains 314 unique attack payloads that cover 70 techniques from\nthe MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale\nevaluation on GitHub Copilot and Cursor, and our evaluation results show that\nattack success rates can reach as high as 84% for executing malicious commands.\nMoreover, these attacks are proven effective across a wide range of objectives,\nranging from initial access and system discovery to credential theft and data\nexfiltration."}
{"id": "2509.21553", "categories": ["cs.AI", "cs.CE", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21553", "abs": "https://arxiv.org/abs/2509.21553", "authors": ["Ahmed Jaber", "Wangshu Zhu", "Karthick Jayavelu", "Justin Downes", "Sameer Mohamed", "Candace Agonafir", "Linnia Hawkins", "Tian Zheng"], "title": "AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need", "comment": null, "summary": "Climate data science faces persistent barriers stemming from the fragmented\nnature of data sources, heterogeneous formats, and the steep technical\nexpertise required to identify, acquire, and process datasets. These challenges\nlimit participation, slow discovery, and reduce the reproducibility of\nscientific workflows. In this paper, we present a proof of concept for\naddressing these barriers through the integration of a curated knowledge graph\n(KG) with AI agents designed for cloud-native scientific workflows. The KG\nprovides a unifying layer that organizes datasets, tools, and workflows, while\nAI agents -- powered by generative AI services -- enable natural language\ninteraction, automated data access, and streamlined analysis. Together, these\ncomponents drastically lower the technical threshold for engaging in climate\ndata science, enabling non-specialist users to identify and analyze relevant\ndatasets. By leveraging existing cloud-ready API data portals, we demonstrate\nthat \"a knowledge graph is all you need\" to unlock scalable and agentic\nworkflows for scientific inquiry. The open-source design of our system further\nsupports community contributions, ensuring that the KG and associated tools can\nevolve as a shared commons. Our results illustrate a pathway toward\ndemocratizing access to climate data and establishing a reproducible,\nextensible framework for human--AI collaboration in scientific research."}
{"id": "2509.21842", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21842", "abs": "https://arxiv.org/abs/2509.21842", "authors": ["Yansong Ning", "Rui Liu", "Jun Wang", "Kai Chen", "Wei Li", "Jun Fang", "Kan Zheng", "Naiqiang Tan", "Hao Liu"], "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents", "comment": "Under review", "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks."}
{"id": "2509.21770", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21770", "abs": "https://arxiv.org/abs/2509.21770", "authors": ["Sadman Saumik Islam", "Bruna Dalcin Baldasso", "Davide Cattaneo", "Xianta Jiang", "Michelle Ploughman"], "title": "Machine Learning and AI Applied to fNIRS Data Reveals Novel Brain Activity Biomarkers in Stable Subclinical Multiple Sclerosis", "comment": null, "summary": "People with Multiple Sclerosis (MS) complain of problems with hand dexterity\nand cognitive fatigue. However, in many cases, impairments are subtle and\ndifficult to detect. Functional near-infrared spectroscopy (fNIRS) is a\nnon-invasive neuroimaging technique that measures brain hemodynamic responses\nduring cognitive or motor tasks. We aimed to detect brain activity biomarkers\nthat could explain subjective reports of cognitive fatigue while completing\ndexterous tasks and provide targets for future brain stimulation treatments. We\nrecruited 15 people with MS who did not have a hand (Nine Hole Peg Test\n[NHPT]), mobility, or cognitive impairment, and 12 age- and sex-matched\ncontrols. Participants completed two types of hand dexterity tasks with their\ndominant hand, single task and dual task (NHPT while holding a ball between the\nfifth finger and hypothenar eminence of the same hand). We analyzed fNIRS data\n(oxygenated and deoxygenated hemoglobin levels) using a machine learning\nframework to classify MS patients from controls based on their brain activation\npatterns in bilateral prefrontal and sensorimotor cortices. The K-Nearest\nNeighbor classifier achieved an accuracy of 75.0% for single manual dexterity\ntasks and 66.7% for the more complex dual manual dexterity tasks. Using XAI, we\nfound that the most important brain regions contributing to the machine\nlearning model were the supramarginal/angular gyri and the precentral gyrus\n(sensory integration and motor regions) of the ipsilateral hemisphere, with\nsuppressed activity and slower neurovascular response in the MS group. During\nboth tasks, deoxygenated hemoglobin levels were better predictors than the\nconventional measure of oxygenated hemoglobin. This nonconventional method of\nfNIRS data analysis revealed novel brain activity biomarkers that can help\ndevelop personalized brain stimulation targets."}
{"id": "2509.22256", "categories": ["cs.CR", "cs.AI", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.22256", "abs": "https://arxiv.org/abs/2509.22256", "authors": ["Haochen Gong", "Chenxiao Li", "Rui Chang", "Wenbo Shen"], "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space", "comment": null, "summary": "Large language model (LLM)-based computer-use agents represent a convergence\nof AI and OS capabilities, enabling natural language to control system- and\napplication-level functions. However, due to LLMs' inherent uncertainty issues,\ngranting agents control over computers poses significant security risks. When\nagent actions deviate from user intentions, they can cause irreversible\nconsequences. Existing mitigation approaches, such as user confirmation and\nLLM-based dynamic action validation, still suffer from limitations in\nusability, security, and performance. To address these challenges, we propose\nCSAgent, a system-level, static policy-based access control framework for\ncomputer-use agents. To bridge the gap between static policy and dynamic\ncontext and user intent, CSAgent introduces intent- and context-aware policies,\nand provides an automated toolchain to assist developers in constructing and\nrefining them. CSAgent enforces these policies through an optimized OS service,\nensuring that agent actions can only be executed under specific user intents\nand contexts. CSAgent supports protecting agents that control computers through\ndiverse interfaces, including API, CLI, and GUI. We implement and evaluate\nCSAgent, which successfully defends against more than 99.36% of attacks while\nintroducing only 6.83% performance overhead."}
{"id": "2509.21828", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21828", "abs": "https://arxiv.org/abs/2509.21828", "authors": ["The Viet Bui", "Tien Mai", "Hong Thanh Nguyen"], "title": "Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning", "comment": null, "summary": "We study the problem of online multi-agent reinforcement learning (MARL) in\nenvironments with sparse rewards, where reward feedback is not provided at each\ninteraction but only revealed at the end of a trajectory. This setting, though\nrealistic, presents a fundamental challenge: the lack of intermediate rewards\nhinders standard MARL algorithms from effectively guiding policy learning. To\naddress this issue, we propose a novel framework that integrates online inverse\npreference learning with multi-agent on-policy optimization into a unified\narchitecture. At its core, our approach introduces an implicit multi-agent\nreward learning model, built upon a preference-based value-decomposition\nnetwork, which produces both global and local reward signals. These signals are\nfurther used to construct dual advantage streams, enabling differentiated\nlearning targets for the centralized critic and decentralized actors. In\naddition, we demonstrate how large language models (LLMs) can be leveraged to\nprovide preference labels that enhance the quality of the learned reward model.\nEmpirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and\nSMACv2, show that our method achieves superior performance compared to existing\nbaselines, highlighting its effectiveness in addressing sparse-reward\nchallenges in online MARL."}
{"id": "2509.21862", "categories": ["cs.AI", "cs.MA", "cs.SI", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.21862", "abs": "https://arxiv.org/abs/2509.21862", "authors": ["So Kuroki", "Yingtao Tian", "Kou Misaki", "Takashi Ikegami", "Takuya Akiba", "Yujin Tang"], "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi", "comment": null, "summary": "The study of emergent behaviors in large language model (LLM)-driven\nmulti-agent systems is a critical research challenge, yet progress is limited\nby a lack of principled methodologies for controlled experimentation. To\naddress this, we introduce Shachi, a formal methodology and modular framework\nthat decomposes an agent's policy into core cognitive components: Configuration\nfor intrinsic traits, Memory for contextual persistence, and Tools for expanded\ncapabilities, all orchestrated by an LLM reasoning engine. This principled\narchitecture moves beyond brittle, ad-hoc agent designs and enables the\nsystematic analysis of how specific architectural choices influence collective\nbehavior. We validate our methodology on a comprehensive 10-task benchmark and\ndemonstrate its power through novel scientific inquiries. Critically, we\nestablish the external validity of our approach by modeling a real-world U.S.\ntariff shock, showing that agent behaviors align with observed market reactions\nonly when their cognitive architecture is appropriately configured with memory\nand tools. Our work provides a rigorous, open-source foundation for building\nand evaluating LLM agents, aimed at fostering more cumulative and\nscientifically grounded research."}
{"id": "2509.21828", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21828", "abs": "https://arxiv.org/abs/2509.21828", "authors": ["The Viet Bui", "Tien Mai", "Hong Thanh Nguyen"], "title": "Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning", "comment": null, "summary": "We study the problem of online multi-agent reinforcement learning (MARL) in\nenvironments with sparse rewards, where reward feedback is not provided at each\ninteraction but only revealed at the end of a trajectory. This setting, though\nrealistic, presents a fundamental challenge: the lack of intermediate rewards\nhinders standard MARL algorithms from effectively guiding policy learning. To\naddress this issue, we propose a novel framework that integrates online inverse\npreference learning with multi-agent on-policy optimization into a unified\narchitecture. At its core, our approach introduces an implicit multi-agent\nreward learning model, built upon a preference-based value-decomposition\nnetwork, which produces both global and local reward signals. These signals are\nfurther used to construct dual advantage streams, enabling differentiated\nlearning targets for the centralized critic and decentralized actors. In\naddition, we demonstrate how large language models (LLMs) can be leveraged to\nprovide preference labels that enhance the quality of the learned reward model.\nEmpirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and\nSMACv2, show that our method achieves superior performance compared to existing\nbaselines, highlighting its effectiveness in addressing sparse-reward\nchallenges in online MARL."}
{"id": "2509.21862", "categories": ["cs.AI", "cs.MA", "cs.SI", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2509.21862", "abs": "https://arxiv.org/abs/2509.21862", "authors": ["So Kuroki", "Yingtao Tian", "Kou Misaki", "Takashi Ikegami", "Takuya Akiba", "Yujin Tang"], "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi", "comment": null, "summary": "The study of emergent behaviors in large language model (LLM)-driven\nmulti-agent systems is a critical research challenge, yet progress is limited\nby a lack of principled methodologies for controlled experimentation. To\naddress this, we introduce Shachi, a formal methodology and modular framework\nthat decomposes an agent's policy into core cognitive components: Configuration\nfor intrinsic traits, Memory for contextual persistence, and Tools for expanded\ncapabilities, all orchestrated by an LLM reasoning engine. This principled\narchitecture moves beyond brittle, ad-hoc agent designs and enables the\nsystematic analysis of how specific architectural choices influence collective\nbehavior. We validate our methodology on a comprehensive 10-task benchmark and\ndemonstrate its power through novel scientific inquiries. Critically, we\nestablish the external validity of our approach by modeling a real-world U.S.\ntariff shock, showing that agent behaviors align with observed market reactions\nonly when their cognitive architecture is appropriately configured with memory\nand tools. Our work provides a rigorous, open-source foundation for building\nand evaluating LLM agents, aimed at fostering more cumulative and\nscientifically grounded research."}
{"id": "2509.21981", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21981", "abs": "https://arxiv.org/abs/2509.21981", "authors": ["Zhimin Wang", "Shaokang He", "Duo Wu", "Jinghe Wang", "Linjia Kang", "Jing Yu", "Zhi Wang"], "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration", "comment": null, "summary": "Effective real-world multi-agent collaboration requires not only accurate\nplanning but also the ability to reason about collaborators' intents -- a\ncrucial capability for avoiding miscoordination and redundant communication\nunder partial observable environments. Due to their strong planning and\nreasoning capabilities, large language models (LLMs) have emerged as promising\nautonomous agents for collaborative task solving. However, existing\ncollaboration frameworks for LLMs overlook their reasoning potential for\ndynamic intent inference, and thus produce inconsistent plans and redundant\ncommunication, reducing collaboration efficiency. To bridge this gap, we\npropose CoBel-World, a novel framework that equips LLM agents with a\ncollaborative belief world -- an internal representation jointly modeling the\nphysical environment and collaborators' mental states. CoBel-World enables\nagents to parse open-world task knowledge into structured beliefs via a\nsymbolic belief language, and perform zero-shot Bayesian-style belief updates\nthrough LLM reasoning. This allows agents to proactively detect potential\nmiscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated\non challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World\nsignificantly reduces communication costs by 22-60% and improves task\ncompletion efficiency by 4-28% compared to the strongest baseline. Our results\nshow that explicit, intent-aware belief modeling is essential for efficient and\nhuman-like collaboration in LLM-based multi-agent systems."}
{"id": "2509.21848", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21848", "abs": "https://arxiv.org/abs/2509.21848", "authors": ["Taejong Joo", "Shu Ishida", "Ivan Sosnovik", "Bryan Lim", "Sahand Rezaei-Shoshtari", "Adam Gaier", "Robert Giaquinto"], "title": "Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration", "comment": "Preprint", "summary": "As a model-agnostic approach to long context modeling, multi-agent systems\ncan process inputs longer than a large language model's context window without\nretraining or architectural modifications. However, their performance often\nheavily relies on hand-crafted multi-agent collaboration strategies and prompt\nengineering, which limit generalizability. In this work, we introduce a\nprincipled framework that formalizes the model-agnostic long context modeling\nproblem as a compression problem, yielding an information-theoretic compression\nobjective. Building on this framework, we propose Graph of Agents (GoA), which\ndynamically constructs an input-dependent collaboration structure that\nmaximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document\nquestion answering benchmarks, GoA improves the average $F_1$ score of\nretrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using\na fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K\ncontext window, GoA surpasses the 128K context window Llama 3.1 8B on\nLongBench, showing a dramatic increase in effective context length. Our source\ncode is available at https://github.com/tjoo512/graph-of-agents."}
{"id": "2509.21981", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21981", "abs": "https://arxiv.org/abs/2509.21981", "authors": ["Zhimin Wang", "Shaokang He", "Duo Wu", "Jinghe Wang", "Linjia Kang", "Jing Yu", "Zhi Wang"], "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration", "comment": null, "summary": "Effective real-world multi-agent collaboration requires not only accurate\nplanning but also the ability to reason about collaborators' intents -- a\ncrucial capability for avoiding miscoordination and redundant communication\nunder partial observable environments. Due to their strong planning and\nreasoning capabilities, large language models (LLMs) have emerged as promising\nautonomous agents for collaborative task solving. However, existing\ncollaboration frameworks for LLMs overlook their reasoning potential for\ndynamic intent inference, and thus produce inconsistent plans and redundant\ncommunication, reducing collaboration efficiency. To bridge this gap, we\npropose CoBel-World, a novel framework that equips LLM agents with a\ncollaborative belief world -- an internal representation jointly modeling the\nphysical environment and collaborators' mental states. CoBel-World enables\nagents to parse open-world task knowledge into structured beliefs via a\nsymbolic belief language, and perform zero-shot Bayesian-style belief updates\nthrough LLM reasoning. This allows agents to proactively detect potential\nmiscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated\non challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World\nsignificantly reduces communication costs by 22-60% and improves task\ncompletion efficiency by 4-28% compared to the strongest baseline. Our results\nshow that explicit, intent-aware belief modeling is essential for efficient and\nhuman-like collaboration in LLM-based multi-agent systems."}
{"id": "2509.21982", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21982", "abs": "https://arxiv.org/abs/2509.21982", "authors": ["Renqi Chen", "Zeyin Tao", "Jianming Guo", "Jingzhe Zhu", "Yiheng Peng", "Qingqing Sun", "Tianyi Zhang", "Shuai Chen"], "title": "RISK: A Framework for GUI Agents in E-commerce Risk Management", "comment": null, "summary": "E-commerce risk management requires aggregating diverse, deeply embedded web\ndata through multi-step, stateful interactions, which traditional scraping\nmethods and most existing Graphical User Interface (GUI) agents cannot handle.\nThese agents are typically limited to single-step tasks and lack the ability to\nmanage dynamic, interactive content critical for effective risk assessment. To\naddress this challenge, we introduce RISK, a novel framework designed to build\nand deploy GUI agents for this domain. RISK integrates three components: (1)\nRISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction\ntrajectories, collected through a high-fidelity browser framework and a\nmeticulous data curation process; (2) RISK-Bench, a benchmark with 802\nsingle-step and 320 multi-step trajectories across three difficulty levels for\nstandardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning\nframework considering four aspects: (i) Output Format: Updated format reward to\nenhance output syntactic correctness and task comprehension, (ii) Single-step\nLevel: Stepwise accuracy reward to provide granular feedback during early\ntraining stages, (iii) Multi-step Level: Process reweight to emphasize critical\nlater steps in interaction sequences, and (iv) Task Level: Level reweight to\nfocus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms\nexisting baselines, achieving a 6.8% improvement in offline single-step and an\n8.8% improvement in offline multi-step. Moreover, it attains a top task success\nrate of 70.5% in online evaluation. RISK provides a scalable, domain-specific\nsolution for automating complex web interactions, advancing the state of the\nart in e-commerce risk management."}
{"id": "2509.22008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22008", "abs": "https://arxiv.org/abs/2509.22008", "authors": ["Yajie Qi", "Wei Wei", "Lin Li", "Lijun Zhang", "Zhidong Gao", "Da Wang", "Huizhong Song"], "title": "Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning", "comment": null, "summary": "Real-world decision-making tasks typically occur in complex and open\nenvironments, posing significant challenges to reinforcement learning (RL)\nagents' exploration efficiency and long-horizon planning capabilities. A\npromising approach is LLM-enhanced RL, which leverages the rich prior knowledge\nand strong planning capabilities of LLMs to guide RL agents in efficient\nexploration. However, existing methods mostly rely on frequent and costly LLM\ninvocations and suffer from limited performance due to the semantic mismatch.\nIn this paper, we introduce a Structured Goal-guided Reinforcement Learning\n(SGRL) method that integrates a structured goal planner and a goal-conditioned\naction pruner to guide RL agents toward efficient exploration. Specifically,\nthe structured goal planner utilizes LLMs to generate a reusable, structured\nfunction for goal generation, in which goals are prioritized. Furthermore, by\nutilizing LLMs to determine goals' priority weights, it dynamically generates\nforward-looking goals to guide the agent's policy toward more promising\ndecision-making trajectories. The goal-conditioned action pruner employs an\naction masking mechanism that filters out actions misaligned with the current\ngoal, thereby constraining the RL agent to select goal-consistent policies. We\nevaluate the proposed method on Crafter and Craftax-Classic, and experimental\nresults demonstrate that SGRL achieves superior performance compared to\nexisting state-of-the-art methods."}
{"id": "2509.22137", "categories": ["cs.AI", "cs.HC", "cs.MA", "cs.RO", "68N19, 68T09", "H.5.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2509.22137", "abs": "https://arxiv.org/abs/2509.22137", "authors": ["Seoyoung Lee", "Seonbin Yoon", "Seongbeen Lee", "Hyesoo Kim", "Joo Yong Sim"], "title": "Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach", "comment": null, "summary": "GUI task automation streamlines repetitive tasks, but existing LLM or\nVLM-based planner-executor agents suffer from brittle generalization, high\nlatency, and limited long-horizon coherence. Their reliance on single-shot\nreasoning or static plans makes them fragile under UI changes or complex tasks.\nLog2Plan addresses these limitations by combining a structured two-level\nplanning framework with a task mining approach over user behavior logs,\nenabling robust and adaptable GUI automation. Log2Plan constructs high-level\nplans by mapping user commands to a structured task dictionary, enabling\nconsistent and generalizable automation. To support personalization and reuse,\nit employs a task mining approach from user behavior logs that identifies\nuser-specific patterns. These high-level plans are then grounded into low-level\naction sequences by interpreting real-time GUI context, ensuring robust\nexecution across varying interfaces. We evaluated Log2Plan on 200 real-world\ntasks, demonstrating significant improvements in task success rate and\nexecution time. Notably, it maintains over 60.0% success rate even on\nlong-horizon task sequences, highlighting its robustness in complex, multi-step\nworkflows."}
{"id": "2509.21998", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21998", "abs": "https://arxiv.org/abs/2509.21998", "authors": ["Hanlin Zhu", "Tianyu Guo", "Song Mei", "Stuart Russell", "Nikhil Ghosh", "Alberto Bietti", "Jiantao Jiao"], "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments", "comment": "35 pages, 8 figures", "summary": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability\nto combine tool use, especially search, and reasoning - becomes a critical\nskill. However, it is hard to disentangle agentic reasoning when evaluated in\ncomplex environments and tasks. Current agent benchmarks often mix agentic\nreasoning with challenging math reasoning, expert-level knowledge, and other\nadvanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,\nwhere an LLM agent is required to solve grade-school-level reasoning problems,\nbut is only presented with the question in the prompt without the premises that\ncontain the necessary information to solve the task, and needs to proactively\ncollect that information using tools. Although the original tasks are\ngrade-school math problems, we observe that even frontier models like GPT-5\nonly achieve 67% accuracy. To understand and analyze the agentic reasoning\npatterns, we propose the concept of agentic reasoning graph: cluster the\nenvironment's document embeddings into nodes, and map each tool call to its\nnearest node to build a reasoning path. Surprisingly, we identify that the\nability to revisit a previously visited node, widely taken as a crucial pattern\nin static reasoning, is often missing for agentic reasoning for many models.\nBased on the insight, we propose a tool-augmented test-time scaling method to\nimprove LLM's agentic reasoning performance by adding tools to encourage models\nto revisit. We expect our benchmark and the agentic reasoning framework to aid\nfuture studies of understanding and pushing the boundaries of agentic\nreasoning."}
{"id": "2509.22174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22174", "abs": "https://arxiv.org/abs/2509.22174", "authors": ["Durgesh Kalwar", "Mayank Baranwal", "Harshad Khadilkar"], "title": "Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead", "comment": null, "summary": "In today's data-sensitive landscape, distributed learning emerges as a vital\ntool, not only fortifying privacy measures but also streamlining computational\noperations. This becomes especially crucial within fully decentralized\ninfrastructures where local processing is imperative due to the absence of\ncentralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to\ninformation aggregation in multi-agent networks. DYNAWEIGHT offers substantial\nacceleration in decentralized learning with minimal additional communication\nand memory overhead. Unlike traditional static weight assignments, such as\nMetropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring\nservers based on their relative losses on local datasets. Consequently, it\nfavors servers possessing diverse information, particularly in scenarios of\nsubstantial data heterogeneity. Our experiments on various datasets MNIST,\nCIFAR10, and CIFAR100 incorporating various server counts and graph topologies,\ndemonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT\nfunctions as an aggregation scheme compatible with any underlying server-level\noptimization algorithm, underscoring its versatility and potential for\nwidespread integration."}
{"id": "2509.22426", "categories": ["cs.LG", "cs.GT", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.22426", "abs": "https://arxiv.org/abs/2509.22426", "authors": ["Yuma Fujimoto", "Kenshi Abe", "Kaito Ariu"], "title": "Learning from Delayed Feedback in Games via Extra Prediction", "comment": "11 pages, 3 figures (main); 9 pages (appendix)", "summary": "This study raises and addresses the problem of time-delayed feedback in\nlearning in games. Because learning in games assumes that multiple agents\nindependently learn their strategies, a discrepancy in optimization often\nemerges among the agents. To overcome this discrepancy, the prediction of the\nfuture reward is incorporated into algorithms, typically known as Optimistic\nFollow-the-Regularized-Leader (OFTRL). However, the time delay in observing the\npast rewards hinders the prediction. Indeed, this study firstly proves that\neven a single-step delay worsens the performance of OFTRL from the aspects of\nregret and convergence. This study proposes the weighted OFTRL (WOFTRL), where\nthe prediction vector of the next reward in OFTRL is weighted $n$ times. We\nfurther capture an intuition that the optimistic weight cancels out this time\ndelay. We prove that when the optimistic weight exceeds the time delay, our\nWOFTRL recovers the good performances that the regret is constant\n($O(1)$-regret) in general-sum normal-form games, and the strategies converge\nto the Nash equilibrium as a subsequence (best-iterate convergence) in\npoly-matrix zero-sum games. The theoretical results are supported and\nstrengthened by our experiments."}
{"id": "2509.22137", "categories": ["cs.AI", "cs.HC", "cs.MA", "cs.RO", "68N19, 68T09", "H.5.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2509.22137", "abs": "https://arxiv.org/abs/2509.22137", "authors": ["Seoyoung Lee", "Seonbin Yoon", "Seongbeen Lee", "Hyesoo Kim", "Joo Yong Sim"], "title": "Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach", "comment": null, "summary": "GUI task automation streamlines repetitive tasks, but existing LLM or\nVLM-based planner-executor agents suffer from brittle generalization, high\nlatency, and limited long-horizon coherence. Their reliance on single-shot\nreasoning or static plans makes them fragile under UI changes or complex tasks.\nLog2Plan addresses these limitations by combining a structured two-level\nplanning framework with a task mining approach over user behavior logs,\nenabling robust and adaptable GUI automation. Log2Plan constructs high-level\nplans by mapping user commands to a structured task dictionary, enabling\nconsistent and generalizable automation. To support personalization and reuse,\nit employs a task mining approach from user behavior logs that identifies\nuser-specific patterns. These high-level plans are then grounded into low-level\naction sequences by interpreting real-time GUI context, ensuring robust\nexecution across varying interfaces. We evaluated Log2Plan on 200 real-world\ntasks, demonstrating significant improvements in task success rate and\nexecution time. Notably, it maintains over 60.0% success rate even on\nlong-horizon task sequences, highlighting its robustness in complex, multi-step\nworkflows."}
{"id": "2509.22272", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22272", "abs": "https://arxiv.org/abs/2509.22272", "authors": ["Nassim Walha", "Sebastian G. Gruber", "Thomas Decker", "Yinchong Yang", "Alireza Javanmardi", "Eyke Hüllermeier", "Florian Buettner"], "title": "Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly integrated in diverse\napplications, obtaining reliable measures of their predictive uncertainty has\nbecome critically important. A precise distinction between aleatoric\nuncertainty, arising from inherent ambiguities within input data, and epistemic\nuncertainty, originating exclusively from model limitations, is essential to\neffectively address each uncertainty source. In this paper, we introduce\nSpectral Uncertainty, a novel approach to quantifying and decomposing\nuncertainties in LLMs. Leveraging the Von Neumann entropy from quantum\ninformation theory, Spectral Uncertainty provides a rigorous theoretical\nfoundation for separating total uncertainty into distinct aleatoric and\nepistemic components. Unlike existing baseline methods, our approach\nincorporates a fine-grained representation of semantic similarity, enabling\nnuanced differentiation among various semantic interpretations in model\nresponses. Empirical evaluations demonstrate that Spectral Uncertainty\noutperforms state-of-the-art methods in estimating both aleatoric and total\nuncertainty across diverse models and benchmark datasets."}
{"id": "2509.22601", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.22601", "abs": "https://arxiv.org/abs/2509.22601", "authors": ["Yulei Qin", "Xiaoyu Tan", "Zhengbao He", "Gang Li", "Haojia Lin", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Siqi Cai", "Renting Rui", "Shaofei Cai", "Yuzheng Cai", "Xuan Zhang", "Sheng Ye", "Ke Li", "Xing Sun"], "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning", "comment": "26 pages, 11 figures", "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence."}
{"id": "2509.22242", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22242", "abs": "https://arxiv.org/abs/2509.22242", "authors": ["Simone Lionetti", "Fabian Gröger", "Philippe Gottfrois", "Alvaro Gonzalez-Jimenez", "Ludovic Amruthalingam", "Alexander A. Navarini", "Marc Pouly"], "title": "Clinical Uncertainty Impacts Machine Learning Evaluations", "comment": null, "summary": "Clinical dataset labels are rarely certain as annotators disagree and\nconfidence is not uniform across cases. Typical aggregation procedures, such as\nmajority voting, obscure this variability. In simple experiments on medical\nimaging benchmarks, accounting for the confidence in binary labels\nsignificantly impacts model rankings. We therefore argue that machine-learning\nevaluations should explicitly account for annotation uncertainty using\nprobabilistic metrics that directly operate on distributions. These metrics can\nbe applied independently of the annotations' generating process, whether\nmodeled by simple counting, subjective confidence ratings, or probabilistic\nresponse models. They are also computationally lightweight, as closed-form\nexpressions have linear-time implementations once examples are sorted by model\nscore. We thus urge the community to release raw annotations for datasets and\nto adopt uncertainty-aware evaluation so that performance estimates may better\nreflect clinical data."}
{"id": "2509.22321", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.22321", "abs": "https://arxiv.org/abs/2509.22321", "authors": ["Bowen Wang", "Matteo Zecchin", "Osvaldo Simeone"], "title": "Distributed Associative Memory via Online Convex Optimization", "comment": null, "summary": "An associative memory (AM) enables cue-response recall, and associative\nmemorization has recently been noted to underlie the operation of modern neural\narchitectures such as Transformers. This work addresses a distributed setting\nwhere agents maintain a local AM to recall their own associations as well as\nselective information from others. Specifically, we introduce a distributed\nonline gradient descent method that optimizes local AMs at different agents\nthrough communication over routing trees. Our theoretical analysis establishes\nsublinear regret guarantees, and experiments demonstrate that the proposed\nprotocol consistently outperforms existing online optimization baselines."}
{"id": "2509.22315", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22315", "abs": "https://arxiv.org/abs/2509.22315", "authors": ["Hieu Tran", "Zonghai Yao", "Nguyen Luong Tran", "Zhichao Yang", "Feiyun Ouyang", "Shuo Han", "Razieh Rahimi", "Hong Yu"], "title": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning", "comment": "8 pages", "summary": "Inspired by the dual-process theory of human cognition from \\textit{Thinking,\nFast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated\nMemory for Enhanced Reasoning), a multi-agent reasoning framework that\ndynamically integrates \\textbf{System 1} (fast, intuitive thinking) and\n\\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick\nThinking Agent (System 1) to generate a rapid answer; if uncertainty is\ndetected, it then triggers a structured System 2 reasoning pipeline composed of\nspecialized agents for \\textit{planning}, \\textit{hypothesis generation},\n\\textit{retrieval}, \\textit{information integration}, and\n\\textit{decision-making}. This multi-agent design faithfully mimics human\ncognitive processes and enhances both efficiency and accuracy. Experimental\nresults with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to\nperform competitively with state-of-the-art closed-source models like GPT-4 and\nGPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This\nresearch establishes PRIME as a scalable solution for improving LLMs in domains\nrequiring complex, knowledge-intensive reasoning."}
{"id": "2509.22403", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22403", "abs": "https://arxiv.org/abs/2509.22403", "authors": ["Fanjin Meng", "Yuan Yuan", "Jingtao Ding", "Jie Feng", "Chonghua Han", "Yong Li"], "title": "MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning", "comment": null, "summary": "Mobility Foundation Models (MFMs) have advanced the modeling of human\nmovement patterns, yet they face a ceiling due to limitations in data scale and\nsemantic understanding. While Large Language Models (LLMs) offer powerful\nsemantic reasoning, they lack the innate understanding of spatio-temporal\nstatistics required for generating physically plausible mobility trajectories.\nTo address these gaps, we propose MoveFM-R, a novel framework that unlocks the\nfull potential of mobility foundation models by leveraging language-driven\nsemantic reasoning capabilities. It tackles two key challenges: the vocabulary\nmismatch between continuous geographic coordinates and discrete language\ntokens, and the representation gap between the latent vectors of MFMs and the\nsemantic world of LLMs. MoveFM-R is built on three core innovations: a\nsemantically enhanced location encoding to bridge the geography-language gap, a\nprogressive curriculum to align the LLM's reasoning with mobility patterns, and\nan interactive self-reflection mechanism for conditional trajectory generation.\nExtensive experiments demonstrate that MoveFM-R significantly outperforms\nexisting MFM-based and LLM-based baselines. It also shows robust generalization\nin zero-shot settings and excels at generating realistic trajectories from\nnatural language instructions. By synthesizing the statistical power of MFMs\nwith the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm\nthat enables a more comprehensive, interpretable, and powerful modeling of\nhuman mobility. The implementation of MoveFM-R is available online at\nhttps://anonymous.4open.science/r/MoveFM-R-CDE7/."}
{"id": "2509.22391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22391", "abs": "https://arxiv.org/abs/2509.22391", "authors": ["Jiaqi Shao", "Yuxiang Lin", "Munish Prasad Lohani", "Yufeng Miao", "Bing Luo"], "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents", "comment": null, "summary": "Recent work has explored training Large Language Model (LLM) search agents\nwith reinforcement learning (RL) for open-domain question answering (QA).\nHowever, most evaluations focus solely on final answer accuracy, overlooking\nhow these agents reason with and act on external evidence. We introduce\nSeekBench, the first benchmark for evaluating the \\textit{epistemic competence}\nof LLM search agents through step-level analysis of their response traces.\nSeekBench comprises 190 expert-annotated traces with over 1,800 response steps\ngenerated by LLM search agents, each enriched with evidence annotations for\ngranular analysis of whether agents (1) generate reasoning steps grounded in\nobserved evidence, (2) adaptively reformulate searches to recover from\nlow-quality results, and (3) have proper calibration to correctly assess\nwhether the current evidence is sufficient for providing an answer."}
{"id": "2509.22426", "categories": ["cs.LG", "cs.GT", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.22426", "abs": "https://arxiv.org/abs/2509.22426", "authors": ["Yuma Fujimoto", "Kenshi Abe", "Kaito Ariu"], "title": "Learning from Delayed Feedback in Games via Extra Prediction", "comment": "11 pages, 3 figures (main); 9 pages (appendix)", "summary": "This study raises and addresses the problem of time-delayed feedback in\nlearning in games. Because learning in games assumes that multiple agents\nindependently learn their strategies, a discrepancy in optimization often\nemerges among the agents. To overcome this discrepancy, the prediction of the\nfuture reward is incorporated into algorithms, typically known as Optimistic\nFollow-the-Regularized-Leader (OFTRL). However, the time delay in observing the\npast rewards hinders the prediction. Indeed, this study firstly proves that\neven a single-step delay worsens the performance of OFTRL from the aspects of\nregret and convergence. This study proposes the weighted OFTRL (WOFTRL), where\nthe prediction vector of the next reward in OFTRL is weighted $n$ times. We\nfurther capture an intuition that the optimistic weight cancels out this time\ndelay. We prove that when the optimistic weight exceeds the time delay, our\nWOFTRL recovers the good performances that the regret is constant\n($O(1)$-regret) in general-sum normal-form games, and the strategies converge\nto the Nash equilibrium as a subsequence (best-iterate convergence) in\npoly-matrix zero-sum games. The theoretical results are supported and\nstrengthened by our experiments."}
{"id": "2509.22460", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22460", "abs": "https://arxiv.org/abs/2509.22460", "authors": ["Shichao Weng", "Zhiqiang Wang", "Yuhua Zhou", "Rui Lu", "Ting Liu", "Zhiyang Teng", "Xiaozhang Liu", "Hanmeng Liu"], "title": "GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation", "comment": null, "summary": "Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large\nLanguage Models (MLLMs), requiring not only the joint interpretation of text\nand diagrams but also iterative visuospatial reasoning. While existing\napproaches process diagrams as static images, they lack the capacity for\ndynamic manipulation - a core aspect of human geometric reasoning involving\nauxiliary line construction and affine transformations. We present GeoSketch, a\nneural-symbolic framework that recasts geometric reasoning as an interactive\nperception-reasoning-action loop. GeoSketch integrates: (1) a Perception module\nthat abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning\nmodule that applies geometric theorems to decide the next deductive step, and\n(3) a Sketch Action module that executes operations such as drawing auxiliary\nlines or applying transformations, thereby updating the diagram in a closed\nloop. To train this agent, we develop a two-stage pipeline: supervised\nfine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement\nlearning with dense, symbolic rewards to enhance robustness and strategic\nexploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a\nhigh-quality set of 390 geometry problems requiring auxiliary construction or\naffine transformations. Experiments on strong MLLM baselines demonstrate that\nGeoSketch significantly improves stepwise reasoning accuracy and\nproblem-solving success over static perception methods. By unifying\nhierarchical decision-making, executable visual actions, and symbolic\nverification, GeoSketch advances multimodal reasoning from static\ninterpretation to dynamic, verifiable interaction, establishing a new\nfoundation for solving complex visuospatial problems."}
{"id": "2509.22522", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22522", "abs": "https://arxiv.org/abs/2509.22522", "authors": ["Guillem Capellera", "Luis Ferraz", "Antonio Rubio", "Alexandre Alahi", "Antonio Agudo"], "title": "JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation", "comment": null, "summary": "Generative models often treat continuous data and discrete events as separate\nprocesses, creating a gap in modeling complex systems where they interact\nsynchronously. To bridge this gap, we introduce JointDiff, a novel diffusion\nframework designed to unify these two processes by simultaneously generating\ncontinuous spatio-temporal data and synchronous discrete events. We demonstrate\nits efficacy in the sports domain by simultaneously modeling multi-agent\ntrajectories and key possession events. This joint modeling is validated with\nnon-controllable generation and two novel controllable generation scenarios:\nweak-possessor-guidance, which offers flexible semantic control over game\ndynamics through a simple list of intended ball possessors, and text-guidance,\nwhich enables fine-grained, language-driven generation. To enable the\nconditioning with these guidance signals, we introduce CrossGuid, an effective\nconditioning operation for multi-agent domains. We also share a new unified\nsports benchmark enhanced with textual descriptions for soccer and football\ndatasets. JointDiff achieves state-of-the-art performance, demonstrating that\njoint modeling is crucial for building realistic and controllable generative\nmodels for interactive systems."}
{"id": "2509.22502", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22502", "abs": "https://arxiv.org/abs/2509.22502", "authors": ["Chenglin Yu", "Yang Yu", "Songmiao Wang", "Yucheng Wang", "Yifan Yang", "Jinjia Li", "Ming Li", "Hongxia Yang"], "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios", "comment": "9 pages of main content and 32 pages of others, 2 figures, under\n  review as a conference paper at ICLR 2026", "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin organizing and executing complex tasks, and many such agents are now widely\nused in various application scenarios. However, developing these agents\nrequires carefully designed workflows, carefully crafted prompts, and iterative\ntuning, which requires LLM techniques and domain-specific expertise. These\nhand-crafted limitations hinder the scalability and cost-effectiveness of LLM\nagents across a wide range of industries. To address these challenges, we\npropose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that\ncan be applied to \\textbf{infi}nite scenarios, which introduces several key\ninnovations: a generalized \"agent-as-a-tool\" mechanism that automatically\ndecomposes complex agents into hierarchical multi-agent systems; a dual-audit\nmechanism that ensures the quality and stability of task completion; an agent\nrouting function that enables efficient task-agent matching; and an agent\nself-evolution mechanism that autonomously restructures the agent DAG based on\nnew tasks, poor performance, or optimization opportunities. Furthermore,\nInfiAgent's atomic task design supports agent parallelism, significantly\nimproving execution efficiency. This framework evolves into a versatile\npyramid-like multi-agent system capable of solving a wide range of problems.\nEvaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\%\nhigher performance compared to ADAS (similar auto-generated agent framework),\nwhile a case study of the AI research assistant InfiHelper shows that it\ngenerates scientific papers that have received recognition from human reviewers\nat top-tier IEEE conferences."}
{"id": "2509.22576", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22576", "abs": "https://arxiv.org/abs/2509.22576", "authors": ["Xu Wujiang", "Wentian Zhao", "Zhenting Wang", "Li Yu-Jhe", "Jin Can", "Jin Mingyu", "Mei Kai", "Wan Kun", "Metaxas Dimitris"], "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning", "comment": null, "summary": "Training LLM agents in multi-turn environments with sparse rewards, where\ncompleting a single task requires 30+ turns of interaction within an episode,\npresents a fundamental challenge for reinforcement learning. We identify a\ncritical failure mode unique to this setting: the exploration-exploitation\ncascade failure. This cascade begins with early-stage policy premature\nconvergence, where sparse feedback causes agents to commit to flawed,\nlow-entropy strategies. Subsequently, agents enter late-stage policy collapse,\nwhere conventional entropy regularization becomes counterproductive, promoting\nchaotic exploration that destabilizes training. We propose Entropy-regularized\nPolicy Optimization (EPO), a general framework that breaks this failure cycle\nthrough three synergistic mechanisms: (1) adopting entropy regularization in\nmulti-turn settings to enhance exploration, (2) an entropy smoothing\nregularizer that bounds policy entropy within historical averages to prevent\nabrupt fluctuations, and (3) adaptive phase-based weighting that balances\nexploration and exploitation across training. Our analysis justifies that EPO\nguarantees monotonically decreasing entropy variance while maintaining\nconvergence. EPO achieves up to 152% performance improvement on ScienceWorld\nand up to 19.8% on ALFWorld. Our work demonstrates that multi-turn\nsparse-reward settings require fundamentally different entropy control than\ntraditional RL, with broad implications for LLM agent training."}
{"id": "2509.22504", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22504", "abs": "https://arxiv.org/abs/2509.22504", "authors": ["Jinyeop Song", "Jeff Gore", "Max Kleiman-Weiner"], "title": "Estimating the Empowerment of Language Model Agents", "comment": "10 pages, 8 figures. Submitted to ICLR 2026", "summary": "As language model (LM) agents become more capable and gain broader access to\nreal-world tools, there is a growing need for scalable evaluation frameworks of\nagentic capability. However, conventional benchmark-centric evaluations are\ncostly to design and require human designers to come up with valid tasks that\ntranslate into insights about general model capabilities. In this work, we\npropose information-theoretic evaluation based on empowerment, the mutual\ninformation between an agent's actions and future states, as an open-ended\nmethod for evaluating LM agents. We introduce EELMA (Estimating Empowerment of\nLanguage Model Agents), an algorithm for approximating effective empowerment\nfrom multi-turn text interactions. We validate EELMA on both language games and\nscaled-up realistic web-browsing scenarios. We find that empowerment strongly\ncorrelates with average task performance, characterize the impact of\nenvironmental complexity and agentic factors such as chain-of-thought, model\nscale, and memory length on estimated empowerment, and that high empowerment\nstates and actions are often pivotal moments for general capabilities.\nTogether, these results demonstrate empowerment as an appealing general-purpose\nmetric for evaluating and monitoring LM agents in complex, open-ended settings."}
{"id": "2509.22601", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.22601", "abs": "https://arxiv.org/abs/2509.22601", "authors": ["Yulei Qin", "Xiaoyu Tan", "Zhengbao He", "Gang Li", "Haojia Lin", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Siqi Cai", "Renting Rui", "Shaofei Cai", "Yuzheng Cai", "Xuan Zhang", "Sheng Ye", "Ke Li", "Xing Sun"], "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning", "comment": "26 pages, 11 figures", "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence."}
{"id": "2509.22537", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22537", "abs": "https://arxiv.org/abs/2509.22537", "authors": ["Haoyang Li", "Xiao Jia", "Zhanzhan Zhao"], "title": "The Emergence of Altruism in Large-Language-Model Agents Society", "comment": null, "summary": "Leveraging Large Language Models (LLMs) for social simulation is a frontier\nin computational social science. Understanding the social logics these agents\nembody is critical to this attempt. However, existing research has primarily\nfocused on cooperation in small-scale, task-oriented games, overlooking how\naltruism, which means sacrificing self-interest for collective benefit, emerges\nin large-scale agent societies. To address this gap, we introduce a\nSchelling-variant urban migration model that creates a social dilemma,\ncompelling over 200 LLM agents to navigate an explicit conflict between\negoistic (personal utility) and altruistic (system utility) goals. Our central\nfinding is a fundamental difference in the social tendencies of LLMs. We\nidentify two distinct archetypes: \"Adaptive Egoists\", which default to\nprioritizing self-interest but whose altruistic behaviors significantly\nincrease under the influence of a social norm-setting message board; and\n\"Altruistic Optimizers\", which exhibit an inherent altruistic logic,\nconsistently prioritizing collective benefit even at a direct cost to\nthemselves. Furthermore, to qualitatively analyze the cognitive underpinnings\nof these decisions, we introduce a method inspired by Grounded Theory to\nsystematically code agent reasoning. In summary, this research provides the\nfirst evidence of intrinsic heterogeneity in the egoistic and altruistic\ntendencies of different LLMs. We propose that for social simulation, model\nselection is not merely a matter of choosing reasoning capability, but of\nchoosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a\nmore suitable choice for simulating complex human societies, \"Altruistic\nOptimizers\" are better suited for modeling idealized pro-social actors or\nscenarios where collective welfare is the primary consideration."}
{"id": "2509.22626", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22626", "abs": "https://arxiv.org/abs/2509.22626", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "Learning Admissible Heuristics for A*: Theory and Practice", "comment": null, "summary": "Heuristic functions are central to the performance of search algorithms such\nas A-star, where admissibility - the property of never overestimating the true\nshortest-path cost - guarantees solution optimality. Recent deep learning\napproaches often disregard admissibility and provide limited guarantees on\ngeneralization beyond the training data. This paper addresses both of these\nlimitations. First, we pose heuristic learning as a constrained optimization\nproblem and introduce Cross-Entropy Admissibility (CEA), a loss function that\nenforces admissibility during training. On the Rubik's Cube domain, this method\nyields near-admissible heuristics with significantly stronger guidance than\ncompressed pattern database (PDB) heuristics. Theoretically, we study the\nsample complexity of learning heuristics. By leveraging PDB abstractions and\nthe structural properties of graphs such as the Rubik's Cube, we tighten the\nbound on the number of training samples needed for A-star to generalize.\nReplacing a general hypothesis class with a ReLU neural network gives bounds\nthat depend primarily on the network's width and depth, rather than on graph\nsize. Using the same network, we also provide the first generalization\nguarantees for goal-dependent heuristics."}
{"id": "2509.22558", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22558", "abs": "https://arxiv.org/abs/2509.22558", "authors": ["Chenyu Zhou", "Tianyi Xu", "Jianghao Lin", "Dongdong Ge"], "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown promising capabilities for solving\nOperations Research (OR) problems. While reinforcement learning serves as a\npowerful paradigm for LLM training on OR problems, existing works generally\nface two key limitations. First, outcome reward suffers from the credit\nassignment problem, where correct final answers can reinforce flawed reasoning.\nSecond, conventional discriminative process supervision is myopic, failing to\nevaluate the interdependent steps of OR modeling holistically. To this end, we\nintroduce StepORLM, a novel self-evolving framework with generative process\nsupervision. At its core, StepORLM features a co-evolutionary loop where a\npolicy model and a generative process reward model (GenPRM) iteratively improve\non each other. This loop is driven by a dual-feedback mechanism: definitive,\noutcome-based verification from an external solver, and nuanced, holistic\nprocess evaluation from the GenPRM. The combined signal is used to align the\npolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneously\nrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new\nstate-of-the-art across six benchmarks, significantly outperforming vastly\nlarger generalist models, agentic methods, and specialized baselines. Moreover,\nthe co-evolved GenPRM is able to act as a powerful and universally applicable\nprocess verifier, substantially boosting the inference scaling performance of\nboth our own model and other existing LLMs."}
{"id": "2509.21553", "categories": ["cs.AI", "cs.CE", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21553", "abs": "https://arxiv.org/abs/2509.21553", "authors": ["Ahmed Jaber", "Wangshu Zhu", "Karthick Jayavelu", "Justin Downes", "Sameer Mohamed", "Candace Agonafir", "Linnia Hawkins", "Tian Zheng"], "title": "AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need", "comment": null, "summary": "Climate data science faces persistent barriers stemming from the fragmented\nnature of data sources, heterogeneous formats, and the steep technical\nexpertise required to identify, acquire, and process datasets. These challenges\nlimit participation, slow discovery, and reduce the reproducibility of\nscientific workflows. In this paper, we present a proof of concept for\naddressing these barriers through the integration of a curated knowledge graph\n(KG) with AI agents designed for cloud-native scientific workflows. The KG\nprovides a unifying layer that organizes datasets, tools, and workflows, while\nAI agents -- powered by generative AI services -- enable natural language\ninteraction, automated data access, and streamlined analysis. Together, these\ncomponents drastically lower the technical threshold for engaging in climate\ndata science, enabling non-specialist users to identify and analyze relevant\ndatasets. By leveraging existing cloud-ready API data portals, we demonstrate\nthat \"a knowledge graph is all you need\" to unlock scalable and agentic\nworkflows for scientific inquiry. The open-source design of our system further\nsupports community contributions, ensuring that the KG and associated tools can\nevolve as a shared commons. Our results illustrate a pathway toward\ndemocratizing access to climate data and establishing a reproducible,\nextensible framework for human--AI collaboration in scientific research."}
{"id": "2509.22570", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22570", "abs": "https://arxiv.org/abs/2509.22570", "authors": ["Qi Mao", "Tinghan Yang", "Jiahao Li", "Bin Li", "Libiao Jin", "Yan Lu"], "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration", "comment": null, "summary": "The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI\nagents is transforming human-AI collaboration into bidirectional, multimodal\ninteraction. However, existing codecs remain optimized for unimodal, one-way\ncommunication, resulting in repeated degradation under conventional\ncompress-transmit-reconstruct pipelines. To address this limitation, we propose\nUniMIC, a Unified token-based Multimodal Interactive Coding framework that\nbridges edge devices and cloud AI agents. Instead of transmitting raw pixels or\nplain text, UniMIC employs compact tokenized representations as the\ncommunication medium, enabling efficient low-bitrate transmission while\nmaintaining compatibility with LMMs. To further enhance compression,\nlightweight Transformer-based entropy models with scenario-specific\ndesigns-generic, masked, and text-conditioned-effectively minimize inter-token\nredundancy. Extensive experiments on text-to-image generation, text-guided\ninpainting, outpainting, and visual question answering show that UniMIC\nachieves substantial bitrate savings and remains robust even at ultra-low\nbitrates (<0.05bpp), without compromising downstream task performance. These\nresults establish UniMIC as a practical and forward-looking paradigm for\nnext-generation multimodal interactive communication."}
{"id": "2509.21600", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21600", "abs": "https://arxiv.org/abs/2509.21600", "authors": ["Mafalda Malafaia", "Peter A. N. Bosman", "Coen Rasch", "Tanja Alderliesten"], "title": "Automated and Interpretable Survival Analysis from Multimodal Data", "comment": "4 figures; 4 tables; 24 pages", "summary": "Accurate and interpretable survival analysis remains a core challenge in\noncology. With growing multimodal data and the clinical need for transparent\nmodels to support validation and trust, this challenge increases in complexity.\nWe propose an interpretable multimodal AI framework to automate survival\nanalysis by integrating clinical variables and computed tomography imaging. Our\nMultiFIX-based framework uses deep learning to infer survival-relevant features\nthat are further explained: imaging features are interpreted via Grad-CAM,\nwhile clinical variables are modeled as symbolic expressions through genetic\nprogramming. Risk estimation employs a transparent Cox regression, enabling\nstratification into groups with distinct survival outcomes. Using the\nopen-source RADCURE dataset for head and neck cancer, MultiFIX achieves a\nC-index of 0.838 (prediction) and 0.826 (stratification), outperforming the\nclinical and academic baseline approaches and aligning with known prognostic\nmarkers. These results highlight the promise of interpretable multimodal AI for\nprecision oncology with MultiFIX."}
{"id": "2509.21634", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21634", "abs": "https://arxiv.org/abs/2509.21634", "authors": ["Prakhar Sharma", "Haohuang Wen", "Vinod Yegneswaran", "Ashish Gehani", "Phillip Porras", "Zhiqiang Lin"], "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs", "comment": null, "summary": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G."}
{"id": "2509.21634", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.21634", "abs": "https://arxiv.org/abs/2509.21634", "authors": ["Prakhar Sharma", "Haohuang Wen", "Vinod Yegneswaran", "Ashish Gehani", "Phillip Porras", "Zhiqiang Lin"], "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs", "comment": null, "summary": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G."}
{"id": "2509.21654", "categories": ["cs.LG", "cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.21654", "abs": "https://arxiv.org/abs/2509.21654", "authors": ["Rina Panigrahy", "Vatsal Sharan"], "title": "Limitations on Safe, Trusted, Artificial General Intelligence", "comment": "17 pages, 1 figure", "summary": "Safety, trust and Artificial General Intelligence (AGI) are aspirational\ngoals in artificial intelligence (AI) systems, and there are several informal\ninterpretations of these notions. In this paper, we propose strict,\nmathematical definitions of safety, trust, and AGI, and demonstrate a\nfundamental incompatibility between them. We define safety of a system as the\nproperty that it never makes any false claims, trust as the assumption that the\nsystem is safe, and AGI as the property of an AI system always matching or\nexceeding human capability. Our core finding is that -- for our formal\ndefinitions of these notions -- a safe and trusted AI system cannot be an AGI\nsystem: for such a safe, trusted system there are task instances which are\neasily and provably solvable by a human but not by the system. We note that we\nconsider strict mathematical definitions of safety and trust, and it is\npossible for real-world deployments to instead rely on alternate, practical\ninterpretations of these notions. We show our results for program verification,\nplanning, and graph reachability. Our proofs draw parallels to G\\\"odel's\nincompleteness theorems and Turing's proof of the undecidability of the halting\nproblem, and can be regarded as interpretations of G\\\"odel's and Turing's\nresults."}
{"id": "2509.21998", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21998", "abs": "https://arxiv.org/abs/2509.21998", "authors": ["Hanlin Zhu", "Tianyu Guo", "Song Mei", "Stuart Russell", "Nikhil Ghosh", "Alberto Bietti", "Jiantao Jiao"], "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments", "comment": "35 pages, 8 figures", "summary": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability\nto combine tool use, especially search, and reasoning - becomes a critical\nskill. However, it is hard to disentangle agentic reasoning when evaluated in\ncomplex environments and tasks. Current agent benchmarks often mix agentic\nreasoning with challenging math reasoning, expert-level knowledge, and other\nadvanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,\nwhere an LLM agent is required to solve grade-school-level reasoning problems,\nbut is only presented with the question in the prompt without the premises that\ncontain the necessary information to solve the task, and needs to proactively\ncollect that information using tools. Although the original tasks are\ngrade-school math problems, we observe that even frontier models like GPT-5\nonly achieve 67% accuracy. To understand and analyze the agentic reasoning\npatterns, we propose the concept of agentic reasoning graph: cluster the\nenvironment's document embeddings into nodes, and map each tool call to its\nnearest node to build a reasoning path. Surprisingly, we identify that the\nability to revisit a previously visited node, widely taken as a crucial pattern\nin static reasoning, is often missing for agentic reasoning for many models.\nBased on the insight, we propose a tool-augmented test-time scaling method to\nimprove LLM's agentic reasoning performance by adding tools to encourage models\nto revisit. We expect our benchmark and the agentic reasoning framework to aid\nfuture studies of understanding and pushing the boundaries of agentic\nreasoning."}
{"id": "2509.21666", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21666", "abs": "https://arxiv.org/abs/2509.21666", "authors": ["Joshua Salim", "Jordan Yu", "Xilei Zhao"], "title": "DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks", "comment": null, "summary": "While deep learning models excel at predictive tasks, they often overfit due\nto their complex structure and large number of parameters, causing them to\nmemorize training data, including noise, rather than learn patterns that\ngeneralize to new data. To tackle this challenge, this paper proposes a new\nregularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep\nNeural Networks (DIM), which maintains domain-informed monotonic relationships\nin complex deep learning models to further improve predictions. Specifically,\nour method enforces monotonicity by penalizing violations relative to a linear\nbaseline, effectively encouraging the model to follow expected trends while\npreserving its predictive power. We formalize this approach through a\ncomprehensive mathematical framework that establishes a linear reference,\nmeasures deviations from monotonic behavior, and integrates these measurements\ninto the training objective. We test and validate the proposed methodology\nusing a real-world ridesourcing dataset from Chicago and a synthetically\ncreated dataset. Experiments across various neural network architectures show\nthat even modest monotonicity constraints consistently enhance model\nperformance. DIM enhances the predictive performance of deep neural networks by\napplying domain-informed monotonicity constraints to regularize model behavior\nand mitigate overfitting"}
{"id": "2509.22130", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22130", "abs": "https://arxiv.org/abs/2509.22130", "authors": ["Merve Atasever", "Matthew Hong", "Mihir Nitin Kulkarni", "Qingpei Li", "Jyotirmoy V. Deshmukh"], "title": "Multi-Agent Path Finding via Offline RL and LLM Collaboration", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) poses a significant and challenging problem\ncritical for applications in robotics and logistics, particularly due to its\ncombinatorial complexity and the partial observability inherent in realistic\nenvironments. Decentralized reinforcement learning methods commonly encounter\ntwo substantial difficulties: first, they often yield self-centered behaviors\namong agents, resulting in frequent collisions, and second, their reliance on\ncomplex communication modules leads to prolonged training times, sometimes\nspanning weeks. To address these challenges, we propose an efficient\ndecentralized planning framework based on the Decision Transformer (DT),\nuniquely leveraging offline reinforcement learning to substantially reduce\ntraining durations from weeks to mere hours. Crucially, our approach\neffectively handles long-horizon credit assignment and significantly improves\nperformance in scenarios with sparse and delayed rewards. Furthermore, to\novercome adaptability limitations inherent in standard RL methods under dynamic\nenvironmental changes, we integrate a large language model (GPT-4o) to\ndynamically guide agent policies. Extensive experiments in both static and\ndynamically changing environments demonstrate that our DT-based approach,\naugmented briefly by GPT-4o, significantly enhances adaptability and\nperformance."}
{"id": "2509.21712", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21712", "abs": "https://arxiv.org/abs/2509.21712", "authors": ["Bingcan Guo", "Eryue Xu", "Zhiping Zhang", "Tianshi Li"], "title": "Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing", "comment": null, "summary": "Aligning AI systems with human privacy preferences requires understanding\nindividuals' nuanced disclosure behaviors beyond general norms. Yet eliciting\nsuch boundaries remains challenging due to the context-dependent nature of\nprivacy decisions and the complex trade-offs involved. We present an AI-powered\nelicitation approach that probes individuals' privacy boundaries through a\ndiscriminative task. We conducted a between-subjects study that systematically\nvaried communication roles and delegation conditions, resulting in 1,681\nboundary specifications from 169 participants for 61 scenarios. We examined how\nthese contextual factors and individual differences influence the boundary\nspecification. Quantitative results show that communication roles influence\nindividuals' acceptance of detailed and identifiable disclosure, AI delegation\nand individuals' need for privacy heighten sensitivity to disclosed\nidentifiers, and AI delegation results in less consensus across individuals.\nOur findings highlight the importance of situating privacy preference\nelicitation within real-world data flows. We advocate using nuanced privacy\nboundaries as an alignment goal for future AI systems."}
{"id": "2509.22242", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22242", "abs": "https://arxiv.org/abs/2509.22242", "authors": ["Simone Lionetti", "Fabian Gröger", "Philippe Gottfrois", "Alvaro Gonzalez-Jimenez", "Ludovic Amruthalingam", "Alexander A. Navarini", "Marc Pouly"], "title": "Clinical Uncertainty Impacts Machine Learning Evaluations", "comment": null, "summary": "Clinical dataset labels are rarely certain as annotators disagree and\nconfidence is not uniform across cases. Typical aggregation procedures, such as\nmajority voting, obscure this variability. In simple experiments on medical\nimaging benchmarks, accounting for the confidence in binary labels\nsignificantly impacts model rankings. We therefore argue that machine-learning\nevaluations should explicitly account for annotation uncertainty using\nprobabilistic metrics that directly operate on distributions. These metrics can\nbe applied independently of the annotations' generating process, whether\nmodeled by simple counting, subjective confidence ratings, or probabilistic\nresponse models. They are also computationally lightweight, as closed-form\nexpressions have linear-time implementations once examples are sorted by model\nscore. We thus urge the community to release raw annotations for datasets and\nto adopt uncertainty-aware evaluation so that performance estimates may better\nreflect clinical data."}
{"id": "2509.21735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21735", "abs": "https://arxiv.org/abs/2509.21735", "authors": ["Houliang Zhou", "Rong Zhou", "Yangying Liu", "Kanhao Zhao", "Li Shen", "Brian Y. Chen", "Yu Zhang", "Lifang He", "Alzheimer's Disease Neuroimaging Initiative"], "title": "Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks", "comment": null, "summary": "Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease\n(AD) progression is crucial for timely intervention. However, this task remains\nchallenging due to the complex dysfunctions in the spatio-temporal\ncharacteristics of underlying brain networks, which are often overlooked by\nexisting methods. To address these limitations, we develop an interpretable\nspatio-temporal graph neural network framework to predict future AD\nprogression, leveraging dual Stochastic Differential Equations (SDEs) to model\nthe irregularly-sampled longitudinal functional magnetic resonance imaging\n(fMRI) data. We validate our approach on two independent cohorts, including the\nOpen Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease\nNeuroimaging Initiative (ADNI). Our framework effectively learns sparse\nregional and connective importance probabilities, enabling the identification\nof key brain circuit abnormalities associated with disease progression.\nNotably, we detect the parahippocampal cortex, prefrontal cortex, and parietal\nlobule as salient regions, with significant disruptions in the ventral\nattention, dorsal attention, and default mode networks. These abnormalities\ncorrelate strongly with longitudinal AD-related clinical symptoms. Moreover,\nour interpretability strategy reveals both established and novel neural\nsystems-level and sex-specific biomarkers, offering new insights into the\nneurobiological mechanisms underlying AD progression. Our findings highlight\nthe potential of spatio-temporal graph-based learning for early, individualized\nprediction of AD progression, even in the context of irregularly-sampled\nlongitudinal imaging data."}
{"id": "2509.22504", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22504", "abs": "https://arxiv.org/abs/2509.22504", "authors": ["Jinyeop Song", "Jeff Gore", "Max Kleiman-Weiner"], "title": "Estimating the Empowerment of Language Model Agents", "comment": "10 pages, 8 figures. Submitted to ICLR 2026", "summary": "As language model (LM) agents become more capable and gain broader access to\nreal-world tools, there is a growing need for scalable evaluation frameworks of\nagentic capability. However, conventional benchmark-centric evaluations are\ncostly to design and require human designers to come up with valid tasks that\ntranslate into insights about general model capabilities. In this work, we\npropose information-theoretic evaluation based on empowerment, the mutual\ninformation between an agent's actions and future states, as an open-ended\nmethod for evaluating LM agents. We introduce EELMA (Estimating Empowerment of\nLanguage Model Agents), an algorithm for approximating effective empowerment\nfrom multi-turn text interactions. We validate EELMA on both language games and\nscaled-up realistic web-browsing scenarios. We find that empowerment strongly\ncorrelates with average task performance, characterize the impact of\nenvironmental complexity and agentic factors such as chain-of-thought, model\nscale, and memory length on estimated empowerment, and that high empowerment\nstates and actions are often pivotal moments for general capabilities.\nTogether, these results demonstrate empowerment as an appealing general-purpose\nmetric for evaluating and monitoring LM agents in complex, open-ended settings."}
{"id": "2509.21848", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21848", "abs": "https://arxiv.org/abs/2509.21848", "authors": ["Taejong Joo", "Shu Ishida", "Ivan Sosnovik", "Bryan Lim", "Sahand Rezaei-Shoshtari", "Adam Gaier", "Robert Giaquinto"], "title": "Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration", "comment": "Preprint", "summary": "As a model-agnostic approach to long context modeling, multi-agent systems\ncan process inputs longer than a large language model's context window without\nretraining or architectural modifications. However, their performance often\nheavily relies on hand-crafted multi-agent collaboration strategies and prompt\nengineering, which limit generalizability. In this work, we introduce a\nprincipled framework that formalizes the model-agnostic long context modeling\nproblem as a compression problem, yielding an information-theoretic compression\nobjective. Building on this framework, we propose Graph of Agents (GoA), which\ndynamically constructs an input-dependent collaboration structure that\nmaximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document\nquestion answering benchmarks, GoA improves the average $F_1$ score of\nretrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using\na fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K\ncontext window, GoA surpasses the 128K context window Llama 3.1 8B on\nLongBench, showing a dramatic increase in effective context length. Our source\ncode is available at https://github.com/tjoo512/graph-of-agents."}
{"id": "2509.22596", "categories": ["cs.MA", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.22596", "abs": "https://arxiv.org/abs/2509.22596", "authors": ["Qixin Zhang", "Yan Sun", "Can Jin", "Xikun Zhang", "Yao Shu", "Puning Zhao", "Li Shen", "Dacheng Tao"], "title": "Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives", "comment": "Accepted to NeurIPS 2025", "summary": "In this paper, we present two effective policy learning algorithms for\nmulti-agent online coordination(MA-OC) problem. The first one, \\texttt{MA-SPL},\nnot only can achieve the optimal $(1-\\frac{c}{e})$-approximation guarantee for\nthe MA-OC problem with submodular objectives but also can handle the unexplored\n$\\alpha$-weakly DR-submodular and $(\\gamma,\\beta)$-weakly submodular scenarios,\nwhere $c$ is the curvature of the investigated submodular functions, $\\alpha$\ndenotes the diminishing-return(DR) ratio and the tuple $(\\gamma,\\beta)$\nrepresents the submodularity ratios. Subsequently, in order to reduce the\nreliance on the unknown parameters $\\alpha,\\gamma,\\beta$ inherent in the\n\\texttt{MA-SPL} algorithm, we further introduce the second online algorithm\nnamed \\texttt{MA-MPL}. This \\texttt{MA-MPL} algorithm is entirely\n\\emph{parameter-free} and simultaneously can maintain the same approximation\nratio as the first \\texttt{MA-SPL} algorithm. The core of our \\texttt{MA-SPL}\nand \\texttt{MA-MPL} algorithms is a novel continuous-relaxation technique\ntermed as \\emph{policy-based continuous extension}. Compared with the\nwell-established \\emph{multi-linear extension}, a notable advantage of this new\n\\emph{policy-based continuous extension} is its ability to provide a lossless\nrounding scheme for any set function, thereby enabling us to tackle the\nchallenging weakly submodular objectives. Finally, extensive simulations are\nconducted to validate the effectiveness of our proposed algorithms."}
{"id": "2509.21884", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21884", "abs": "https://arxiv.org/abs/2509.21884", "authors": ["Bochuan Cao", "Changjiang Li", "Yuanpu Cao", "Yameng Ge", "Ting Wang", "Jinghui Chen"], "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors", "comment": "29 pages, 10 tables, 6figures, accepted by CCS 25", "summary": "Large language models (LLMs) have been widely adopted across various\napplications, leveraging customized system prompts for diverse tasks. Facing\npotential system prompt leakage risks, model developers have implemented\nstrategies to prevent leakage, primarily by disabling LLMs from repeating their\ncontext when encountering known attack patterns. However, it remains vulnerable\nto new and unforeseen prompt-leaking techniques. In this paper, we first\nintroduce a simple yet effective prompt leaking attack to reveal such risks.\nOur attack is capable of extracting system prompts from various LLM-based\napplication, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our\nfindings further inspire us to search for a fundamental solution to the\nproblems by having no system prompt in the context. To this end, we propose\nSysVec, a novel method that encodes system prompts as internal representation\nvectors rather than raw text. By doing so, SysVec minimizes the risk of\nunauthorized disclosure while preserving the LLM's core language capabilities.\nRemarkably, this approach not only enhances security but also improves the\nmodel's general instruction-following abilities. Experimental results\ndemonstrate that SysVec effectively mitigates prompt leakage attacks, preserves\nthe LLM's functional integrity, and helps alleviate the forgetting issue in\nlong-context scenarios."}
{"id": "2509.22130", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22130", "abs": "https://arxiv.org/abs/2509.22130", "authors": ["Merve Atasever", "Matthew Hong", "Mihir Nitin Kulkarni", "Qingpei Li", "Jyotirmoy V. Deshmukh"], "title": "Multi-Agent Path Finding via Offline RL and LLM Collaboration", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) poses a significant and challenging problem\ncritical for applications in robotics and logistics, particularly due to its\ncombinatorial complexity and the partial observability inherent in realistic\nenvironments. Decentralized reinforcement learning methods commonly encounter\ntwo substantial difficulties: first, they often yield self-centered behaviors\namong agents, resulting in frequent collisions, and second, their reliance on\ncomplex communication modules leads to prolonged training times, sometimes\nspanning weeks. To address these challenges, we propose an efficient\ndecentralized planning framework based on the Decision Transformer (DT),\nuniquely leveraging offline reinforcement learning to substantially reduce\ntraining durations from weeks to mere hours. Crucially, our approach\neffectively handles long-horizon credit assignment and significantly improves\nperformance in scenarios with sparse and delayed rewards. Furthermore, to\novercome adaptability limitations inherent in standard RL methods under dynamic\nenvironmental changes, we integrate a large language model (GPT-4o) to\ndynamically guide agent policies. Extensive experiments in both static and\ndynamically changing environments demonstrate that our DT-based approach,\naugmented briefly by GPT-4o, significantly enhances adaptability and\nperformance."}
{"id": "2509.22174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22174", "abs": "https://arxiv.org/abs/2509.22174", "authors": ["Durgesh Kalwar", "Mayank Baranwal", "Harshad Khadilkar"], "title": "Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead", "comment": null, "summary": "In today's data-sensitive landscape, distributed learning emerges as a vital\ntool, not only fortifying privacy measures but also streamlining computational\noperations. This becomes especially crucial within fully decentralized\ninfrastructures where local processing is imperative due to the absence of\ncentralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to\ninformation aggregation in multi-agent networks. DYNAWEIGHT offers substantial\nacceleration in decentralized learning with minimal additional communication\nand memory overhead. Unlike traditional static weight assignments, such as\nMetropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring\nservers based on their relative losses on local datasets. Consequently, it\nfavors servers possessing diverse information, particularly in scenarios of\nsubstantial data heterogeneity. Our experiments on various datasets MNIST,\nCIFAR10, and CIFAR100 incorporating various server counts and graph topologies,\ndemonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT\nfunctions as an aggregation scheme compatible with any underlying server-level\noptimization algorithm, underscoring its versatility and potential for\nwidespread integration."}
{"id": "2509.22216", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22216", "abs": "https://arxiv.org/abs/2509.22216", "authors": ["Ahmet Onur Akman", "Anastasia Psarou", "Zoltán György Varga", "Grzegorz Jamróz", "Rafał Kucharski"], "title": "Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach", "comment": "Work presented at the European Workshop on Reinforcement Learning\n  (EWRL 2024)", "summary": "This study examines the potential impact of reinforcement learning\n(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic\nenvironment. We focus on a simplified day-to-day route choice problem in a\nmulti-agent setting. We consider a city network where human drivers travel\nthrough their chosen routes to reach their destinations in minimum travel time.\nThen, we convert one-third of the population into AVs, which are RL agents\nemploying Deep Q-learning algorithm. We define a set of optimization targets,\nor as we call them behaviors, namely selfish, collaborative, competitive,\nsocial, altruistic, and malicious. We impose a selected behavior on AVs through\ntheir rewards. We run our simulations using our in-house developed RL framework\nPARCOUR. Our simulations reveal that AVs optimize their travel times by up to\n5\\%, with varying impacts on human drivers' travel times depending on the AV\nbehavior. In all cases where AVs adopt a self-serving behavior, they achieve\nshorter travel times than human drivers. Our findings highlight the complexity\ndifferences in learning tasks of each target behavior. We demonstrate that the\nmulti-agent RL setting is applicable for collective routing on traffic\nnetworks, though their impact on coexisting parties greatly varies with the\nbehaviors adopted."}
{"id": "2509.22218", "categories": ["cs.MA", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.22218", "abs": "https://arxiv.org/abs/2509.22218", "authors": ["Sandaru Fernando", "Imasha Jayarathne", "Sithumini Abeysekara", "Shanuja Sithamparanthan", "Thushari Silva", "Deshan Jayawardana"], "title": "VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture", "comment": null, "summary": "Data visualization is essential for interpreting complex datasets, yet\ntraditional tools often require technical expertise, limiting accessibility.\nVizGen is an AI-assisted graph generation system that empowers users to create\nmeaningful visualizations using natural language. Leveraging advanced NLP and\nLLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries\ninto SQL and recommends suitable graph types. Built on a multi-agent\narchitecture, VizGen handles SQL generation, graph creation, customization, and\ninsight extraction. Beyond visualization, it analyzes data for patterns,\nanomalies, and correlations, and enhances user understanding by providing\nexplanations enriched with contextual information gathered from the internet.\nThe system supports real-time interaction with SQL databases and allows\nconversational graph refinement, making data analysis intuitive and accessible.\nVizGen democratizes data visualization by bridging the gap between technical\ncomplexity and user-friendly design."}
{"id": "2509.22256", "categories": ["cs.CR", "cs.AI", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.22256", "abs": "https://arxiv.org/abs/2509.22256", "authors": ["Haochen Gong", "Chenxiao Li", "Rui Chang", "Wenbo Shen"], "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space", "comment": null, "summary": "Large language model (LLM)-based computer-use agents represent a convergence\nof AI and OS capabilities, enabling natural language to control system- and\napplication-level functions. However, due to LLMs' inherent uncertainty issues,\ngranting agents control over computers poses significant security risks. When\nagent actions deviate from user intentions, they can cause irreversible\nconsequences. Existing mitigation approaches, such as user confirmation and\nLLM-based dynamic action validation, still suffer from limitations in\nusability, security, and performance. To address these challenges, we propose\nCSAgent, a system-level, static policy-based access control framework for\ncomputer-use agents. To bridge the gap between static policy and dynamic\ncontext and user intent, CSAgent introduces intent- and context-aware policies,\nand provides an automated toolchain to assist developers in constructing and\nrefining them. CSAgent enforces these policies through an optimized OS service,\nensuring that agent actions can only be executed under specific user intents\nand contexts. CSAgent supports protecting agents that control computers through\ndiverse interfaces, including API, CLI, and GUI. We implement and evaluate\nCSAgent, which successfully defends against more than 99.36% of attacks while\nintroducing only 6.83% performance overhead."}
{"id": "2509.22601", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.22601", "abs": "https://arxiv.org/abs/2509.22601", "authors": ["Yulei Qin", "Xiaoyu Tan", "Zhengbao He", "Gang Li", "Haojia Lin", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Siqi Cai", "Renting Rui", "Shaofei Cai", "Yuzheng Cai", "Xuan Zhang", "Sheng Ye", "Ke Li", "Xing Sun"], "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning", "comment": "26 pages, 11 figures", "summary": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge of exploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens of policy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where a\nreplay buffer stores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizing intrinsic rewards to foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses, self-imitation gets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthe replay buffer to address the potential policy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-level entropy control to curb\nover-confidence."}
{"id": "2509.22626", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22626", "abs": "https://arxiv.org/abs/2509.22626", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "Learning Admissible Heuristics for A*: Theory and Practice", "comment": null, "summary": "Heuristic functions are central to the performance of search algorithms such\nas A-star, where admissibility - the property of never overestimating the true\nshortest-path cost - guarantees solution optimality. Recent deep learning\napproaches often disregard admissibility and provide limited guarantees on\ngeneralization beyond the training data. This paper addresses both of these\nlimitations. First, we pose heuristic learning as a constrained optimization\nproblem and introduce Cross-Entropy Admissibility (CEA), a loss function that\nenforces admissibility during training. On the Rubik's Cube domain, this method\nyields near-admissible heuristics with significantly stronger guidance than\ncompressed pattern database (PDB) heuristics. Theoretically, we study the\nsample complexity of learning heuristics. By leveraging PDB abstractions and\nthe structural properties of graphs such as the Rubik's Cube, we tighten the\nbound on the number of training samples needed for A-star to generalize.\nReplacing a general hypothesis class with a ReLU neural network gives bounds\nthat depend primarily on the network's width and depth, rather than on graph\nsize. Using the same network, we also provide the first generalization\nguarantees for goal-dependent heuristics."}
