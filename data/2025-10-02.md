<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 30]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks](https://arxiv.org/abs/2509.25261)
*Xianyang Deng,Wenshuai Liu,Yaru FuB,Qi Zhu*

Main category: cs.LG

TL;DR: 이 논문은 UAV 지원 모바일 크라우드 센싱을 위한 데이터 수집의 효율성을 높이기 위한 최적화 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: UAV 지원 모바일 크라우드 센싱(MCS)은 데이터 수집을 위한 유망한 패러다임으로 떠오르고 있지만, 스펙트럼 부족, 장치 이질성 및 사용자 이동성 등의 도전 과제가 있다.

Method: 이 문제를 해결하기 위해 감지, 통신 및 계산 단계의 시간 슬롯 분할, 자원 할당 및 UAV 3D 궤적 계획을 통합한 공동 최적화 프레임워크를 제안한다.

Result: 제안된 방법은 기존 벤치마크와 비교했을 때 처리된 감지 데이터 양에서 상당한 개선을 이루는 것으로 나타났다.

Conclusion: 비전통적인 다층 퍼셉트론 네트워크의 한계를 극복하기 위해 하이브리드 액터 네트워크를 갖춘 새로운 MADRL 알고리즘을 설계하였고, HAPPO 기법을 기반으로 한다.

Abstract: Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has
emerged as a promising paradigm for data collection. However, challenges such
as spectrum scarcity, device heterogeneity, and user mobility hinder efficient
coordination of sensing, communication, and computation. To tackle these
issues, we propose a joint optimization framework that integrates time slot
partition for sensing, communication, and computation phases, resource
allocation, and UAV 3D trajectory planning, aiming to maximize the amount of
processed sensing data. The problem is formulated as a non-convex stochastic
optimization and further modeled as a partially observable Markov decision
process (POMDP) that can be solved by multi-agent deep reinforcement learning
(MADRL) algorithm. To overcome the limitations of conventional multi-layer
perceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor
network. The newly developed method is based on heterogeneous agent proximal
policy optimization (HAPPO), empowered by convolutional neural networks (CNN)
for feature extraction and Kolmogorov-Arnold networks (KAN) to capture
structured state-action dependencies. Extensive numerical results demonstrate
that our proposed method achieves significant improvements in the amount of
processed sensing data when compared with other benchmarks.

</details>


### [2] [LEMs: A Primer On Large Execution Models](https://arxiv.org/abs/2509.25211)
*Remi Genet,Hugo Inzirillo*

Main category: cs.LG

TL;DR: 본 논문은 복잡한 실행 문제를 해결하기 위해 변형된 트랜스포머 아키텍처를 확장한 대규모 실행 모델(LEMs)이라는 새로운 심층 학습 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 실행 문제를 해결하기 위한 유연한 시간 경계와 여러 실행 제약 조건을 고려해야 하기 때문에 LEMs가 필요하다.

Method: LEMs는 고정된 실행 주문에서 최소 및 최대 시간 수평선 사이에 제한된 실행 시간으로 시나리오를 일반화합니다.

Result: LEMs는 유동적인 시간 제약 내에서 실행 경로를 동적으로 최적화하여 전통적인 벤치마크와 비교하여 우수한 실행 성능을 달성합니다.

Conclusion: 단일 프레임워크를 통해 다양한 실행 시나리오 전반에서 구현할 수 있어 자산 특정 접근 방식에 비해 큰 운영상의 이점을 제공합니다.

Abstract: This paper introduces Large Execution Models (LEMs), a novel deep learning
framework that extends transformer-based architectures to address complex
execution problems with flexible time boundaries and multiple execution
constraints. Building upon recent advances in neural VWAP execution strategies,
LEMs generalize the approach from fixed-duration orders to scenarios where
execution duration is bounded between minimum and maximum time horizons,
similar to share buyback contract structures. The proposed architecture
decouples market information processing from execution allocation decisions: a
common feature extraction pipeline using Temporal Kolmogorov-Arnold Networks
(TKANs), Variable Selection Networks (VSNs), and multi-head attention
mechanisms processes market data to create informational context, while
independent allocation networks handle the specific execution logic for
different scenarios (fixed quantity vs. fixed notional, buy vs. sell orders).
This architectural separation enables a unified model to handle diverse
execution objectives while leveraging shared market understanding across
scenarios. Through comprehensive empirical evaluation on intraday
cryptocurrency markets and multi-day equity trading using DOW Jones
constituents, we demonstrate that LEMs achieve superior execution performance
compared to traditional benchmarks by dynamically optimizing execution paths
within flexible time constraints. The unified model architecture enables
deployment across different execution scenarios (buy/sell orders, varying
duration boundaries, volume/notional targets) through a single framework,
providing significant operational advantages over asset-specific approaches.

</details>


### [3] [AMLA: MUL by ADD in FlashAttention Rescaling](https://arxiv.org/abs/2509.25224)
*Qichen Liao,Chengqiu Hu,Fangzheng Miao,Bao Li,Yiyang Liu,Junlong Lyu,Lirui Jiang,Jun Wang,Lingchao Zheng,Jun Li,Yuwei Fan*

Main category: cs.LG

TL;DR: MLA는 대형 언어 모델의 KVCache 메모리 사용량을 줄이지만, 계산 오버헤드와 중간 변수 확장을 초래하여 효율적인 하드웨어 구현에 도전 과제를 안긴다. 본 논문은 하웨이의 Ascend NPU에 최적화된 고성능 커널인 AMLA를 제안한다.


<details>
  <summary>Details</summary>
Motivation: MLA는 대형 언어 모델의 KVCache 메모리 사용을 줄이지만, 계산 오버헤드가 크다. 이는 효율적인 하드웨어 구현에 도전 과제가 된다.

Method: AMLA는 새로운 FlashAttention 기반 알고리즘과 Preload Pipeline 전략을 활용하여 가속화를 달성한다.

Result: AMLA는 Ascend 910 NPU에서 최대 614 TFLOPS를 달성하여 이론적 최대 FLOPS의 86.8%에 도달하며, FlashMLA보다 우수한 성능을 보인다.

Conclusion: AMLA 커널은 하웨이의 CANN에 통합되어 곧 출시될 예정이다.

Abstract: Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage
in Large Language Models while introducing substantial computational overhead
and intermediate variable expansion. This poses challenges for efficient
hardware implementation -- especially during the decode phase. This paper
introduces Ascend MLA (AMLA), a high-performance kernel specifically optimized
for Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel
FlashAttention-based algorithm that replaces floating-point multiplications
with integer additions for output block rescaling, leveraging binary
correspondence between FP32 and INT32 representations; (2) A Preload Pipeline
strategy with hierarchical tiling that maximizes FLOPS utilization: the Preload
Pipeline achieves Cube-bound performance, while hierarchical tiling overlaps
data movement and computation within the Cube core. Experiments show that on
Ascend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS,
reaching 86.8% of the theoretical maximum FLOPS, outperforming the
state-of-the-art open-source FlashMLA implementation, whose FLOPS utilization
is up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into
Huawei's CANN and will be released soon.

</details>


### [4] [Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy](https://arxiv.org/abs/2509.25226)
*Baoyi Xie,Shuiling Shi,Wenqi Liu*

Main category: cs.LG

TL;DR: 통합 바람-태양-파도 해양 에너지 시스템은 청정 전력 공급에 대한 큰 가능성을 지니고 있으며, 본 논문에서는 이를 위한 예측 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 해양 및 연안 지역에서 청정 전력을 공급하기 위해 통합 에너지 시스템의 잠재력을 활용하고자 한다.

Method: MVMD-LSTM 프레임워크를 사용하여 바람, 태양, 파도 전력 시리즈를 공동으로 분해하고, 베이지안 최적화를 통해 본질 모드 함수를 획득한 후 LSTM을 활용하여 예측을 수행한다.

Result: 중국의 해양 통합 에너지 플랫폼에서의 실험 결과, 제안된 프레임워크가 MAPE, RMSE, MAE 측면에서 기준 모델보다 우수한 성능을 보였다.

Conclusion: 제안된 모델은 예측 정확도, 견고성 및 자동화 정도가 높음을 입증하였다.

Abstract: Integrated wind-solar-wave marine energy systems hold broad promise for
supplying clean electricity in offshore and coastal regions. By leveraging the
spatiotemporal complementarity of multiple resources, such systems can
effectively mitigate the intermittency and volatility of single-source outputs,
thereby substantially improving overall power-generation efficiency and
resource utilization. Accurate ultra-short-term forecasting is crucial for
ensuring secure operation and optimizing proactive dispatch. However, most
existing forecasting methods construct separate models for each energy source,
insufficiently account for the complex couplings among multiple energies,
struggle to capture the system's nonlinear and nonstationary dynamics, and
typically depend on extensive manual parameter tuning-limitations that
constrain both predictive performance and practicality. We address this issue
using a Bayesian-optimized Multivariate Variational Mode Decomposition-Long
Short-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to
jointly decompose wind, solar and wave power series so as to preserve
cross-source couplings; it uses Bayesian optimization to automatically search
the number of modes and the penalty parameter in the MVMD process to obtain
intrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to
achieve ultra-short-term power forecasting for the integrated system.
Experiments based on field measurements from an offshore integrated energy
platform in China show that the proposed framework significantly outperforms
benchmark models in terms of MAPE, RMSE and MAE. The results demonstrate
superior predictive accuracy, robustness, and degree of automation.

</details>


### [5] [WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting](https://arxiv.org/abs/2509.25231)
*Xiaojian Wang,Chaoli Zhang,Zhonglong Zheng,Yunliang Jiang*

Main category: cs.LG

TL;DR: WDformer는 다중 도메인 정보를 활용하기 위해 웨이브렛 기반의 차별적 Transformer 모델로, 시계열 데이터를 효과적으로 예측합니다.


<details>
  <summary>Details</summary>
Motivation: 시계열 데이터의 희소성 때문에 전통적인 모델링 방법이 정보 활용을 제한하며, 기존의 주의 메커니즘이 예측 과정에 잡음을 초래할 수 있다.

Method: 웨이브렛 변환을 사용하여 시계열 데이터의 다중 해상도 분석을 수행하고, 역차원에서 주의 메커니즘을 적용하여 다변량 간의 관계를 포착합니다.

Result: WDformer는 여러 도전적인 실제 데이터셋에서 최첨단(SOTA) 성능을 달성했습니다.

Conclusion: 이 모델은 정확성과 효과성을 입증하며, 코드가 공개되어 있습니다.

Abstract: Time series forecasting has various applications, such as meteorological
rainfall prediction, traffic flow analysis, financial forecasting, and
operational load monitoring for various systems. Due to the sparsity of time
series data, relying solely on time-domain or frequency-domain modeling limits
the model's ability to fully leverage multi-domain information. Moreover, when
applied to time series forecasting tasks, traditional attention mechanisms tend
to over-focus on irrelevant historical information, which may introduce noise
into the prediction process, leading to biased results. We proposed WDformer, a
wavelet-based differential Transformer model. This study employs the wavelet
transform to conduct a multi-resolution analysis of time series data. By
leveraging the advantages of joint representation in the time-frequency domain,
it accurately extracts the key information components that reflect the
essential characteristics of the data. Furthermore, we apply attention
mechanisms on inverted dimensions, allowing the attention mechanism to capture
relationships between multiple variables. When performing attention
calculations, we introduced the differential attention mechanism, which
computes the attention score by taking the difference between two separate
softmax attention matrices. This approach enables the model to focus more on
important information and reduce noise. WDformer has achieved state-of-the-art
(SOTA) results on multiple challenging real-world datasets, demonstrating its
accuracy and effectiveness. Code is available at
https://github.com/xiaowangbc/WDformer.

</details>


### [6] [PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases](https://arxiv.org/abs/2509.25238)
*Sri Vatsa Vuddanti,Aarav Shah,Satwik Kumar Chittiprolu,Tony Song,Sunishchal Dev,Kevin Zhu,Maheep Chaudhary*

Main category: cs.LG

TL;DR: PALADIN은 언어 에이전트가 도구 고장을 회복할 수 있는 능력을 갖추도록 훈련하는 프레임워크로, 실제 도구 환경에서의 오류 복구 성능을 크게 개선한다.


<details>
  <summary>Details</summary>
Motivation: 실제 배포에서 도구의 고장으로 인한 작업 포기의 문제를 해결하기 위해, 모델이 현실에서의 도구 실패에 노출되도록 하는 훈련이 필요하다.

Method: PALADIN은 50,000개 이상의 회복 주기를 사용하여 훈련하며, LoRA 기반의 미세 조정을 통해 회복 능력을 주입한다.

Result: PALADIN은 보지 못한 도구 API에서 95.2%의 회복 성능을 유지하며, Recovery Rate를 32.76%에서 89.68%로 개선하여 가장 강력한 기준선인 CRITIC를 13.3% 초과한다.

Conclusion: PALADIN은 실제 도구 환경에서 강력한 회복 능력을 갖춘 오류 내성 에이전트를 구축하기 위한 효과적인 방법으로 자리매김한다.

Abstract: Tool-augmented language agents frequently fail in real-world deployment due
to tool malfunctions--timeouts, API exceptions, or inconsistent
outputs--triggering cascading reasoning errors and task abandonment. Existing
agent training pipelines optimize only for success trajectories, failing to
expose models to the tool failures that dominate real-world usage. We propose
\textbf{PALADIN}, a generalizable framework for equipping language agents with
robust failure recovery capabilities. PALADIN trains on 50,000+
recovery-annotated trajectories constructed via systematic failure injection
and expert demonstrations on an enhanced ToolBench dataset. Training uses
LoRA-based fine-tuning to retain base capabilities while injecting recovery
competence. At inference, PALADIN detects execution-time errors and retrieves
the most similar case from a curated bank of 55+ failure exemplars aligned with
ToolScan's taxonomy, then executes the corresponding recovery action. This
approach generalizes to novel failures beyond the training distribution,
retaining 95.2\% recovery performance on unseen tool APIs. Evaluation across
PaladinEval and ToolReflectEval demonstrates consistent improvements in
Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR),
and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57%
relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%)
by +13.3%. Against vanilla agents, PALADIN achieves 89.86\% RR (+66% relative
improvement from 23.75%). These results establish PALADIN as an effective
method for building fault-tolerant agents capable of robust recovery in
real-world tool environments.

</details>


### [7] [FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers](https://arxiv.org/abs/2509.25401)
*Liang Qiao,Yue Dai,Yeqi Huang,Hongyu Kan,Jun Shi,Hong An*

Main category: cs.LG

TL;DR: FlashOmni는 여러 종류의 sparse attention 엔진을 통합하여 DiT 아키텍처와 호환되도록 설계되었습니다.


<details>
  <summary>Details</summary>
Motivation: DiT의 컴퓨팅 요구 사항을 줄이기 위해 sparse 기반의 가속 방법이 필요합니다.

Method: FlashOmni는 다양한 sparsity 전략을 표준화하는 유연한 sparse 심볼을 도입하여 단일 attention 커널 내에서 다양한 sparse 계산을 실행할 수 있게 합니다.

Result: 실험 결과 FlashOmni는 attention과 GEMM-$Q$에서 희소 비율에 비례한 속도 향상(1:1)에 근접하며 GEMM-$O$에서는 2.5×-3.8× 가속을 달성합니다.

Conclusion: Hunyuan 모델에 적용 시 시각적 품질 저하 없이 약 1.5×의 end-to-end 가속을 가능하게 합니다.

Abstract: Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional
capabilities in visual synthesis, yet their deployment remains constrained by
substantial computational demands. To alleviate this bottleneck, many
sparsity-based acceleration methods have been proposed. However, their diverse
sparsity patterns often require customized kernels for high-performance
inference, limiting universality. We propose FlashOmni, a unified sparse
attention engine compatible with arbitrary DiT architectures. FlashOmni
introduces flexible sparse symbols to standardize the representation of a wide
range of sparsity strategies, such as feature caching and block-sparse
skipping. This unified abstraction enables the execution of diverse sparse
computations within a single attention kernel. In addition, FlashOmni designs
optimized sparse GEMMs for attention blocks, leveraging sparse symbols to
eliminate redundant computations and further improve efficiency. Experiments
demonstrate that FlashOmni delivers near-linear, closely matching the sparsity
ratio speedup (1:1) in attention and GEMM-$Q$, and achieves
2.5$\times$-3.8$\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of
the theoretical limit). Applied with a multi-granularity sparsity strategy, it
enables the Hunyuan model (33K) to achieve about 1.5$\times$ end-to-end
acceleration without degrading visual quality.

</details>


### [8] [Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults](https://arxiv.org/abs/2509.25418)
*Dong Hyun Jeon,Lijing Zhu,Haifang Li,Pengze Li,Jingna Feng,Tiehang Duan,Houbing Herbert Song,Cui Tao,Shuteng Niu*

Main category: cs.LG

TL;DR: Temporal Graph Neural Networks (TGNNs)의 적대적 공격에 대한 강인성을 개선하기 위한 새로운 공격 방법론인 High Impact Attack (HIA)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: TGNNs는 소셜 네트워크, 통신 시스템, 금융 네트워크와 같은 동적 그래프를 분석하는 데 필수적이지만, 시간적 차원을 악용하는 공격에 대한 강인성은 여전히 큰 도전 과제이다.

Method: HIA는 데이터 기반의 대리 모델을 활용하여 네트워크 연결성에 중요한 구조적 노드와 그래프의 시간적 진화에 중요한 동적 노드를 식별한 후, 전략적 엣지 주입과 목표 엣지 삭제를 결합한 하이브리드 교란 전략을 적용한다.

Result: HIA는 TGNN의 링크 예측 작업에서 평균 역순위(MRR)를 최대 35.55% 감소시켜 TGNN의 정확성을 상당히 감소시킨다.

Conclusion: HIA의 결과는 현재 STDG 모델의 근본적인 취약성을 강조하며, 구조적 및 시간적 동력을 모두 고려한 강력한 방어의 필요성을 강조한다.

Abstract: Temporal Graph Neural Networks (TGNNs) have become indispensable for
analyzing dynamic graphs in critical applications such as social networks,
communication systems, and financial networks. However, the robustness of TGNNs
against adversarial attacks, particularly sophisticated attacks that exploit
the temporal dimension, remains a significant challenge. Existing attack
methods for Spatio-Temporal Dynamic Graphs (STDGs) often rely on simplistic,
easily detectable perturbations (e.g., random edge additions/deletions) and
fail to strategically target the most influential nodes and edges for maximum
impact. We introduce the High Impact Attack (HIA), a novel restricted black-box
attack framework specifically designed to overcome these limitations and expose
critical vulnerabilities in TGNNs. HIA leverages a data-driven surrogate model
to identify structurally important nodes (central to network connectivity) and
dynamically important nodes (critical for the graph's temporal evolution). It
then employs a hybrid perturbation strategy, combining strategic edge injection
(to create misleading connections) and targeted edge deletion (to disrupt
essential pathways), maximizing TGNN performance degradation. Importantly, HIA
minimizes the number of perturbations to enhance stealth, making it more
challenging to detect. Comprehensive experiments on five real-world datasets
and four representative TGNN architectures (TGN, JODIE, DySAT, and TGAT)
demonstrate that HIA significantly reduces TGNN accuracy on the link prediction
task, achieving up to a 35.55% decrease in Mean Reciprocal Rank (MRR) - a
substantial improvement over state-of-the-art baselines. These results
highlight fundamental vulnerabilities in current STDG models and underscore the
urgent need for robust defenses that account for both structural and temporal
dynamics.

</details>


### [9] [Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring](https://arxiv.org/abs/2509.25438)
*Zhibo Hou,Zhiyu An,Wan Du*

Main category: cs.LG

TL;DR: LPM은 인간의 개선 모니터링에서 영감을 얻어 개발된 방법으로, 비학습적 전이 대신 학습 가능한 전이에 대한 보상을 제공하여 탐사를 촉진한다.


<details>
  <summary>Details</summary>
Motivation: 환경 내에 비학습적 무작위성의 원천이 존재할 경우, 기존의 내재적 보상 기반 탐사 에이전트는 그 원천에 갇혀 탐사에 실패한다.

Method: LPM은 탐사 중에 모델 개선에 대해 보상을 주고, 이중 네트워크 설계를 통해 탐사를 안내한다.

Result: LPM은 더 빠르게 수렴하고, 더 많은 상태를 탐사하며, 아타리에서 더 높은 외재적 보상을 달성했다.

Conclusion: LPM은 소음에 강한 탐사의 패러다임 전환을 의미하며, 실험 재현을 위한 코드는 제공된다.

Abstract: When there exists an unlearnable source of randomness (noisy-TV) in the
environment, a naively intrinsic reward driven exploring agent gets stuck at
that source of randomness and fails at exploration. Intrinsic reward based on
uncertainty estimation or distribution similarity, while eventually escapes
noisy-TVs as time unfolds, suffers from poor sample efficiency and high
computational cost. Inspired by recent findings from neuroscience that humans
monitor their improvements during exploration, we propose a novel method for
intrinsically-motivated exploration, named Learning Progress Monitoring (LPM).
During exploration, LPM rewards model improvements instead of prediction error
or novelty, effectively rewards the agent for observing learnable transitions
rather than the unlearnable transitions. We introduce a dual-network design
that uses an error model to predict the expected prediction error of the
dynamics model in its previous iteration, and use the difference between the
model errors of the current iteration and previous iteration to guide
exploration. We theoretically show that the intrinsic reward of LPM is
zero-equivariant and a monotone indicator of Information Gain (IG), and that
the error model is necessary to achieve monotonicity correspondence with IG. We
empirically compared LPM against state-of-the-art baselines in noisy
environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.
Results show that LPM's intrinsic reward converges faster, explores more states
in the maze experiment, and achieves higher extrinsic reward in Atari. This
conceptually simple approach marks a shift-of-paradigm of noise-robust
exploration. For code to reproduce our experiments, see
https://github.com/Akuna23Matata/LPM_exploration

</details>


### [10] [EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit](https://arxiv.org/abs/2509.25510)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: 이 논문은 AMS 회로 설계의 트랜지스터 크기 조정을 완전 자동화하는 AI 에이전트 EEsizer를 제안하며, 이는 대규모 언어 모델과 회로 시뮬레이터를 통합하여 인간의 개입을 최소화한다.


<details>
  <summary>Details</summary>
Motivation: AMS 집적 회로 설계에서 수동 노력의 필요성을 줄이고 트랜지스터 크기 조정을 자동화하려는 필요성이 있다.

Method: EEsizer는 대규모 언어 모델과 회로 시뮬레이터, 맞춤형 데이터 분석 기능을 통합하여 완전 자동화된 폐쇄 루프 트랜지스터 크기 조정을 가능하게 한다.

Result: OpenAI o3가 90nm 기술 노드에서 20회의 최대 반복으로 의도된 목표를 성공적으로 달성하며 뛰어난 적응성과 강건성을 입증하였다.

Conclusion: EEsizer는 회로 설계의 자동화를 위한 효과적인 도구로 자리잡을 수 있으며, 이를 통해 AMS 회로 설계의 복잡성을 감소시킬 수 있다.

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often
involves significant manual effort, especially during the transistor sizing
process. While Machine Learning techniques in Electronic Design Automation
(EDA) have shown promise in reducing complexity and minimizing human
intervention, they still face challenges such as numerous iterations and a lack
of knowledge about AMS circuit design. Recently, Large Language Models (LLMs)
have demonstrated significant potential across various fields, showing a
certain level of knowledge in circuit design and indicating their potential to
automate the transistor sizing process. In this work, we propose EEsizer, an
LLM-based AI agent that integrates large language models with circuit
simulators and custom data analysis functions, enabling fully automated,
closed-loop transistor sizing without relying on external knowledge. By
employing prompt engineering and Chain-of-Thought reasoning, the agent
iteratively explores design directions, evaluates performance, and refines
solutions with minimal human intervention. We first benchmarked 8 LLMs on six
basic circuits and selected three high-performing models to optimize a
20-transistor CMOS operational amplifier, targeting multiple performance
metrics, including rail-to-rail operation from 180 nm to 90 nm technology
nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90
nm across three different test groups, with a maximum of 20 iterations,
demonstrating adaptability and robustness at advanced nodes. To assess design
robustness, we manually designed a bias circuit and performed a variation
analysis using Gaussian-distributed variations on transistor dimensions and
threshold voltages.

</details>


### [11] [World Model for AI Autonomous Navigation in Mechanical Thrombectomy](https://arxiv.org/abs/2509.25518)
*Harry Robertshaw,Han-Ru Wu,Alejandro Granados,Thomas C Booth*

Main category: cs.LG

TL;DR: TD-MPC2는 실시간 의사결정이 필요한 기계적 혈전제거를 위한 자율 내혈관 내비게이션에서 기존 SAC 방식을 초월하는 성능을 보여주었다.


<details>
  <summary>Details</summary>
Motivation: 기계적 혈전제거에서의 자율 내비게이션은 혈관 해부학의 복잡성과 정확하고 실시간 의사결정의 필요성으로 인해 중요한 도전 과제이다.

Method: TD-MPC2 모델 기반 RL 알고리즘을 사용하여 자율 내혈관 내비게이션을 위한 세계 모델을 제안하고, 여러 환자의 혈관에서 다중 내혈관 내비게이션 작업을 수행하는 단일 RL 에이전트를 훈련했다.

Result: TD-MPC2는 멀티 태스크 학습에서 SAC를 초과하여 평균 65%의 성공률을 달성하여 SAC의 37%와 비교되었고, 경로 비율에서도 주목할 만한 개선을 보였다.

Conclusion: 이 연구 결과는 자율 내혈관 내비게이션을 개선하기 위한 세계 모델의 잠재력을 강조하며, 일반화 가능한 AI 기반 로봇 개입에 대한 향후 연구의 기반을 마련한다.

Abstract: Autonomous navigation for mechanical thrombectomy (MT) remains a critical
challenge due to the complexity of vascular anatomy and the need for precise,
real-time decision-making. Reinforcement learning (RL)-based approaches have
demonstrated potential in automating endovascular navigation, but current
methods often struggle with generalization across multiple patient vasculatures
and long-horizon tasks. We propose a world model for autonomous endovascular
navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL
agent across multiple endovascular navigation tasks in ten real patient
vasculatures, comparing performance against the state-of-the-art Soft
Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly
outperforms SAC in multi-task learning, achieving a 65% mean success rate
compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2
exhibited increased procedure times, suggesting a trade-off between success
rate and execution speed. These findings highlight the potential of world
models for improving autonomous endovascular navigation and lay the foundation
for future research in generalizable AI-driven robotic interventions.

</details>


### [12] [Safe In-Context Reinforcement Learning](https://arxiv.org/abs/2509.25582)
*Amir Moeini,Minjae Kwon,Alper Kamil Bozkurt,Yuichi Motai,Rohan Chandra,Lu Feng,Shangtong Zhang*

Main category: cs.LG

TL;DR: 이 논문은 제약이 있는 마르코프 의사결정 프로세스의 프레임워크 내에서 ICRL의 안전한 적응 프로세스를 촉진하는 첫 번째 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: ICRL은 매개변수 업데이트 없이 테스트 작업에 적응할 수 있는 에이전트를 목표로 하는 신흥 강화 학습 패러다임이다.

Method: 제약이 있는 마르코프 의사결정 프로세스의 프레임워크를 활용하여 에이전트가 보상을 극대화하면서 추가 비용 함수도 최소화하도록 유도하는 방법을 제안한다.

Result: 에이전트는 비용 허용 범위의 임계값에 적극적으로 반응하며, 더 높은 비용 예산에서 보다 공격적으로 행동하고, 더 낮은 비용 예산에서는 보다 보수적으로 행동한다.

Conclusion: 이 연구는 ICRL의 적응 과정에서 안전성을 확보할 수 있는 방법의 가능성을 보여준다.

Abstract: In-context reinforcement learning (ICRL) is an emerging RL paradigm where the
agent, after some pretraining procedure, is able to adapt to
out-of-distribution test tasks without any parameter updates. The agent
achieves this by continually expanding the input (i.e., the context) to its
policy neural networks. For example, the input could be all the history
experience that the agent has access to until the current time step. The
agent's performance improves as the input grows, without any parameter updates.
In this work, we propose the first method that promotes the safety of ICRL's
adaptation process in the framework of constrained Markov Decision Processes.
In other words, during the parameter-update-free adaptation process, the agent
not only maximizes the reward but also minimizes an additional cost function.
We also demonstrate that our agent actively reacts to the threshold (i.e.,
budget) of the cost tolerance. With a higher cost budget, the agent behaves
more aggressively, and with a lower cost budget, the agent behaves more
conservatively.

</details>


### [13] [Effective Model Pruning](https://arxiv.org/abs/2509.25606)
*Yixuan Wang,Dan Guralnik,Saiedeh Akbari,Warren Dixon*

Main category: cs.LG

TL;DR: 효과적인 모델 프루닝(EMP)이라는 새로운 규칙을 소개하며, 이는 모델을 프루닝 할 때 얼마나 많은 항목을 유지해야 하는지를 해결하는 데 초점을 맞춘다.


<details>
  <summary>Details</summary>
Motivation: 모델 프루닝에서 얼마나 많은 항목을 유지해야 하는지를 결정하는 것이 중요한 질문이다.

Method: EMP는 어떤 프루닝 기준에도 적용할 수 있는 보편적이며 적응적인 임계값을 제공하며, 다양한 점수 벡터를 N_eff로 매핑하여 사용한다.

Result: N_eff 높은 점수 항목을 유지하고 나머지를 제로화하여 실험 결과 원래의 밀집 네트워크와 비교해도 성능이 유사한 희소 모델을 생성했다.

Conclusion: 디폴트 파라미터 {eta} = 1이 모델 프루닝에 대한 강력한 임계값을 제공하며, 다른 값은 특정 희소성 요구를 충족하기 위한 선택적 조정으로 활용될 수 있다.

Abstract: We introduce Effective Model Pruning (EMP), a context-agnostic,
parameter-free rule addressing a fundamental question about pruning: how many
entries to keep. EMP does not prescribe how to score the parameters or prune
the models; instead, it supplies a universal adaptive threshold that can be
applied to any pruning criterion: weight magnitude, attention score, KAN
importance score, or even feature-level signals such as image pixel, and used
on structural parts or weights of the models. Given any score vector s, EMP
maps s to a built-in effective number N_eff which is inspired by the Inverse
Simpson index of contributors. Retaining the N_eff highest scoring entries and
zeroing the remainder yields sparse models with performance comparable to the
original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our
experiments. By leveraging the geometry of the simplex, we derive a tight lower
bound on the preserved mass s_eff (the sum of retained scores) over the
corresponding ordered probability simplex associated with the score vector s.
We further verify the effectiveness of N_eff by pruning the model with a scaled
threshold \b{eta}*N_eff across a variety of criteria and models. Experiments
suggest that the default \b{eta} = 1 yields a robust threshold for model
pruning while \b{eta} not equal to 1 still serves as an optional adjustment to
meet specific sparsity requirements.

</details>


### [14] [A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation](https://arxiv.org/abs/2509.25690)
*Zihui Zhao,Yuanbo Tang,Jieyu Ren,Xiaoping Zhang,Yang Li*

Main category: cs.LG

TL;DR: 이 논문은 새로운 정규화를 도입하여 사전 학습의 효율성을 개선하고 신호 재구성의 질을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 사전 학습 방법들은 개별 샘플에서의 표현 희소성에 초점을 맞추어 원자들이 샘플 간에 공유되는 방식을 간과하여 불필요하고 최적이 아닌 사전이 형성됩니다.

Method: 우리는 계수 행렬의 행별 $L_	ext{∞}$ 노름에 기반한 정규화를 도입하여, 계수 행렬의 전체 행이 사라지도록 유도하여 데이터셋에서 활성화되는 사전 원자의 수를 줄입니다.

Result: 베타-버누이 사전 확률 모델에서 유도한 새로운 정규화 접근 방식은 이론적 계산을 통해 최적 하이퍼파라미터 선택과 연결되어, RMSE가 20% 감소하는 재구성 품질과 향상된 표현 희소성을 실현했습니다.

Conclusion: 우리의 방법은 주요 벤치마크 데이터셋에서 기존 방법과 비교하여 더 나은 성능을 발휘하며, 이론적 분석을 실증적으로 검증합니다.

Abstract: Dictionary learning is traditionally formulated as an $L_1$-regularized
signal reconstruction problem. While recent developments have incorporated
discriminative, hierarchical, or generative structures, most approaches rely on
encouraging representation sparsity over individual samples that overlook how
atoms are shared across samples, resulting in redundant and sub-optimal
dictionaries. We introduce a parsimony promoting regularizer based on the
row-wise $L_\infty$ norm of the coefficient matrix. This additional penalty
encourages entire rows of the coefficient matrix to vanish, thereby reducing
the number of dictionary atoms activated across the dataset. We derive the
formulation from a probabilistic model with Beta-Bernoulli priors, which
provides a Bayesian interpretation linking the regularization parameters to
prior distributions. We further establish theoretical calculation for optimal
hyperparameter selection and connect our formulation to both Minimum
Description Length, Bayesian model selection and pathlet learning. Extensive
experiments on benchmark datasets demonstrate that our method achieves
substantially improved reconstruction quality (with a 20\% reduction in RMSE)
and enhanced representation sparsity, utilizing fewer than one-tenth of the
available dictionary atoms, while empirically validating our theoretical
analysis.

</details>


### [15] [Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?](https://arxiv.org/abs/2509.25696)
*Takuya Fujimura,Kota Dohi,Natsuo Yamashita,Yohei Kawaguchi*

Main category: cs.LG

TL;DR: 시간 계열 질의 응답(TSQA) 작업은 레이블이 있는 데이터의 부족으로 인해 상당한 도전에 직면해 있다. 이 논문에서는 VLM이 생성한 의사 레이블을 사용하는 훈련 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 시간 계열 질의 응답(TSQA) 작업의 레이블이 있는 데이터 부족 문제를 해결하고자 한다.

Method: VLM을 통해 생성된 의사 레이블을 사용하여 TSQA 모델을 훈련하는 방법을 제안한다.

Result: 실험 결과, TSQA 모델은 의사 레이블로 성공적으로 훈련되었으며, 대량의 레이블이 없는 데이터를 활용하여 VLM 자체의 성능을 초과하는 결과를 보여준다.

Conclusion: 모델이 노이즈 레이블에 강인하다는 특성을 활용하여 효과적으로 훈련할 수 있음을 입증한다.

Abstract: Time-series question answering (TSQA) tasks face significant challenges due
to the lack of labeled data. Alternatively, with recent advancements in
large-scale models, vision-language models (VLMs) have demonstrated the
potential to analyze time-series signals in a zero-shot manner. In this paper,
we propose a training approach that uses pseudo labels generated by a VLM.
Although VLMs can produce incorrect labels, TSQA models can still be
effectively trained based on the property that deep neural networks are
inherently robust to such noisy labels. Our experimental results demonstrate
that TSQA models are not only successfully trained with pseudo labels, but also
surpass the performance of the VLM itself by leveraging a large amount of
unlabeled data.

</details>


### [16] [Online Decision Making with Generative Action Sets](https://arxiv.org/abs/2509.25777)
*Jianyu Xu,Vidhi Jain,Bryan Wilder,Aarti Singh*

Main category: cs.LG

TL;DR: 이 논문은 온라인 학습에서 의사 결정 에이전트가 비용을 지불하고 새로운 행동을 생성하는 문제를 다루며, 최적의 결정 시퀀스 학습을 위한 두 배의 낙관적인 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 생성과 탐색 간의 트레이드오프를 고려하여 새로운 행동을 동적으로 생성할 필요성이 있다.

Method: 행동 선택에는 하한 신뢰 구간(LCB)을, 행동 생성을 위해서는 상한 신뢰 구간(UCB)을 사용하는 두 배의 낙관적인 알고리즘을 제안한다.

Result: 실험을 통해 우리의 접근 방식이 기준 전략들과 비교했을 때 유리한 생성 품질과 트레이드오프를 달성함을 입증하였다.

Conclusion: 우리의 알고리즘은 확장되는 행동 공간을 갖는 온라인 학습에서 첫 번째 서브선형 후회를 제공함을 이론적으로 증명하였다.

Abstract: With advances in generative AI, decision-making agents can now dynamically
create new actions during online learning, but action generation typically
incurs costs that must be balanced against potential benefits. We study an
online learning problem where an agent can generate new actions at any time
step by paying a one-time cost, with these actions becoming permanently
available for future use. The challenge lies in learning the optimal sequence
of two-fold decisions: which action to take and when to generate new ones,
further complicated by the triangular tradeoffs among exploitation, exploration
and $\textit{creation}$. To solve this problem, we propose a doubly-optimistic
algorithm that employs Lower Confidence Bounds (LCB) for action selection and
Upper Confidence Bounds (UCB) for action generation. Empirical evaluation on
healthcare question-answering datasets demonstrates that our approach achieves
favorable generation-quality tradeoffs compared to baseline strategies. From
theoretical perspectives, we prove that our algorithm achieves the optimal
regret of $O(T^{\frac{d}{d+2}}d^{\frac{d}{d+2}} + d\sqrt{T\log T})$, providing
the first sublinear regret bound for online learning with expanding action
spaces.

</details>


### [17] [Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data](https://arxiv.org/abs/2509.25800)
*Gongxu Luo,Loka Li,Guangyi Chen,Haoyue Dai,Kun Zhang*

Main category: cs.LG

TL;DR: 이 논문은 잠재적인 혼란 변수가 존재함에도 불구하고 개입에 의해 도입된 분포 변화로 인과 관계를 식별하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 개입 후 선택으로 인해 발생하는 문제를 강조하며, 이는 일반적으로 간과되는 도전 과제입니다.

Method: 개입 후 선택을 명시적으로 모델링하고, 개입에 대한 차별 반응을 통해 인과 관계와 선택 패턴을 구별할 수 있는 새로운 인과적 제형을 도입합니다.

Result: 우리는 디지털 자료를 사용하여 인과 관계를 복원하며, 선택과 잠재적인 혼란 변수가 모두 존재하는 상황에서도 성과를 얻었습니다.

Conclusion: 제안된 알고리즘 F-FCI는 관측 및 개입 데이터를 통해 인과 관계를 식별하는 데 효과적입니다.

Abstract: Interventional causal discovery seeks to identify causal relations by
leveraging distributional changes introduced by interventions, even in the
presence of latent confounders. Beyond the spurious dependencies induced by
latent confounders, we highlight a common yet often overlooked challenge in the
problem due to post-treatment selection, in which samples are selectively
included in datasets after interventions. This fundamental challenge widely
exists in biological studies; for example, in gene expression analysis, both
observational and interventional samples are retained only if they meet quality
control criteria (e.g., highly active cells). Neglecting post-treatment
selection may introduce spurious dependencies and distributional changes under
interventions, which can mimic causal responses, thereby distorting causal
discovery results and challenging existing causal formulations. To address
this, we introduce a novel causal formulation that explicitly models
post-treatment selection and reveals how its differential reactions to
interventions can distinguish causal relations from selection patterns,
allowing us to go beyond traditional equivalence classes toward the underlying
true causal structure. We then characterize its Markov properties and propose a
Fine-grained Interventional equivalence class, named FI-Markov equivalence,
represented by a new graphical diagram, F-PAG. Finally, we develop a provably
sound and complete algorithm, F-FCI, to identify causal relations, latent
confounders, and post-treatment selection up to $\mathcal{FI}$-Markov
equivalence, using both observational and interventional data. Experimental
results on synthetic and real-world datasets demonstrate that our method
recovers causal relations despite the presence of both selection and latent
confounders.

</details>


### [18] [RL-Guided Data Selection for Language Model Finetuning](https://arxiv.org/abs/2509.25850)
*Animesh Jha,Harshit Gupta,Ananjan Nandi*

Main category: cs.LG

TL;DR: 대형 언어 모델의 미세 조정을 위한 데이터 선택은 예산 제약 최적화 문제로 재구성할 수 있으며, 마르코프 결정 프로세스를 통해 해결하고 강화 학습 방법으로 최적 데이터 선택 정책을 학습한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 미세 조정하기 위한 데이터 선택의 효율성을 높이고 싶음.

Method: 이 문제를 해결하기 위해 마르코프 결정 프로세스(MDP)로 재구성하고, 다양한 강화 학습 방법을 사용하여 최적 데이터 선택 정책을 학습함.

Result: 우리 접근 방식으로 선택된 5%의 하위 집합으로 훈련한 결과, 전체 데이터 세트에 대한 미세 조정보다 최대 10.8 포인트의 정확도 향상과 훈련 시간을 최대 2배 줄임.

Conclusion: 강화 학습 기반 데이터 선택의 가능성을 강조함.

Abstract: Data selection for finetuning Large Language Models (LLMs) can be framed as a
budget-constrained optimization problem: maximizing a model's downstream
performance under a strict training data budget. Solving this problem is
generally intractable, and existing approximate approaches are
pretraining-oriented and transfer poorly to the fine-tuning setting. We
reformulate this problem as a tractable Markov Decision Process (MDP) and train
agents using various Reinforcement Learning (RL) methods to learn optimal data
selection policies, guided by an efficient, proxy-model-based reward signal.
Across four datasets, training on a $5\%$ subset selected by our approach
matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy
points, while cutting wall-clock training time by up to $2 \times$,
highlighting the promise of RL-guided data selection.

</details>


### [19] [Exact Solutions to the Quantum Schrödinger Bridge Problem](https://arxiv.org/abs/2509.25980)
*Mykola Bordyuh,Djork-Arné Clevert,Marco Bertolini*

Main category: cs.LG

TL;DR: 본 연구는 양자 슈뢰딩거 브리지 문제(QSBP)를 라그랑지안 관점에서 제시하고, 생성 모델링에 적합한 방식으로 그 주요 특징을 도출했다. Gaussian 분포 간의 정확한 닫힌 형식 솔루션을 유도하고, 이를 통해 Gaussian 혼합 모델 기반의 수정된 알고리즘을 제안하여 여러 실험 설정에서 그 유효성을 입증했다.


<details>
  <summary>Details</summary>
Motivation: QSBP는 두 개의 임의의 확률 분포 사이의 확률 과정의 진화를 설명하며, 양자역학적 시스템의 특성을 반영하기 위해 이를 라그랑지안 관점에서 새롭게 접근한다.

Method: 가우시안 분포 간의 QSBP에 대한 정확한 닫힌 형식 솔루션을 도출하기 위해 파커-플랑크 방정식(FPE)과 해밀턴-자코비 방정식(HJE)을 해결하는 방식으로 접근하였다.

Result: 우리는 가우시안 간의 QSBP 솔루션도 다시 가우시안 과정이라는 것을 찾았지만, 양자 효과로 인해 공분산의 진화가 다르게 나타남을 발견하였다.

Conclusion: 수정된 알고리즘을 Gaussian 혼합 모델 프레임워크에 기반하여 제시하고, 단일 세포 진화 데이터, 이미지 생성, 분자 번역 및 평균 필드 게임에서 여러 실험 설정에 대한 유효성을 입증하였다.

Abstract: The Quantum Schr\"odinger Bridge Problem (QSBP) describes the evolution of a
stochastic process between two arbitrary probability distributions, where the
dynamics are governed by the Schr\"odinger equation rather than by the
traditional real-valued wave equation. Although the QSBP is known in the
mathematical literature, we formulate it here from a Lagrangian perspective and
derive its main features in a way that is particularly suited to generative
modeling. We show that the resulting evolution equations involve the so-called
Bohm (quantum) potential, representing a notion of non-locality in the
stochastic process. This distinguishes the QSBP from classical stochastic
dynamics and reflects a key characteristic typical of quantum mechanical
systems. In this work, we derive exact closed-form solutions for the QSBP
between Gaussian distributions. Our derivation is based on solving the
Fokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising
from the Lagrangian formulation of dynamical Optimal Transport. We find that,
similar to the classical Schr\"odinger Bridge Problem, the solution to the QSBP
between Gaussians is again a Gaussian process; however, the evolution of the
covariance differs due to quantum effects. Leveraging these explicit solutions,
we present a modified algorithm based on a Gaussian Mixture Model framework,
and demonstrate its effectiveness across several experimental settings,
including single-cell evolution data, image generation, molecular translation
and applications in Mean-Field Games.

</details>


### [20] [Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access](https://arxiv.org/abs/2509.26000)
*Daniel Ebi,Gaspard Lambrechts,Damien Ernst,Klemens Böhm*

Main category: cs.LG

TL;DR: 비대칭 액터-크리틱 방법은 불완전한 관찰하의 강화 학습에서 정보를 활용하여 학습을 개선하는 새롭고 혁신적인 방안을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 부분 관측 환경에서 불확실성 하에 행동해야 하는 강화 학습 에이전트를 위한 효과적인 학습 방법이 필요하다.

Method: 새로운 비대칭 액터-크리틱 프레임워크인 정보 비대칭 액터-크리틱을 제안하고, 이 방법은 전체 상태 접근 없이 임의의 특권 신호에 대해 비평가를 조건화할 수 있다.

Result: 정책 기울기가 이 구조 하에서도 편향되지 않음을 보여 주며, 일반적인 특권 부분 정보의 경우로 비대칭 방법의 이론적 기초를 확장한다.

Conclusion: 정보가 있는 특권 입력이 제공될 때, 제안한 방법이 학습 효율성과 가치 추정을 개선함을 입증하였다.

Abstract: Reinforcement learning in partially observable environments requires agents
to act under uncertainty from noisy, incomplete observations. Asymmetric
actor-critic methods leverage privileged information during training to improve
learning under these conditions. However, existing approaches typically assume
full-state access during training. In this work, we challenge this assumption
by proposing a novel actor-critic framework, called informed asymmetric
actor-critic, that enables conditioning the critic on arbitrary privileged
signals without requiring access to the full state. We show that policy
gradients remain unbiased under this formulation, extending the theoretical
foundation of asymmetric methods to the more general case of privileged partial
information. To quantify the impact of such signals, we propose informativeness
measures based on kernel methods and return prediction error, providing
practical tools for evaluating training-time signals. We validate our approach
empirically on benchmark navigation tasks and synthetic partially observable
environments, showing that our informed asymmetric method improves learning
efficiency and value estimation when informative privileged inputs are
available. Our findings challenge the necessity of full-state access and open
new directions for designing asymmetric reinforcement learning methods that are
both practical and theoretically sound.

</details>


### [21] [Scaling Up Temporal Domain Generalization via Temporal Experts Averaging](https://arxiv.org/abs/2509.26045)
*Aoming Liu,Kevin Miller,Venkatesh Saligrama,Kate Saenko,Boqing Gong,Ser-Nam Lim,Bryan A. Plummer*

Main category: cs.LG

TL;DR: 이 논문은 시간 도메인 일반화(TDG)를 개선하기 위한 새로운 프레임워크인 Temporal Experts Averaging(TEA)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: TDG는 시간에 따른 분포 변화, 예를 들어, 시간에 따른 어휘 변화에 대한 일반화를 목표로 합니다.

Method: TDG는 전체 모델을 업데이트하기 위해 가중치 평균화를 사용하는 TEA라는 새로운 프레임워크를 제안합니다.

Result: 7개 TDG 벤치마크, 5개 모델 및 2개 TDG 설정에서의 광범위한 실험을 통해 TEA가 이전 TDG 방법보다 최대 69% 더 우수하고 최대 60배 더 효율적임을 보여줍니다.

Conclusion: TEA는 계산 비용을 최소화하면서 일반화 잠재력을 극대화하는 방법으로, 앞으로의 도메인 일반화를 크게 개선할 수 있습니다.

Abstract: Temporal Domain Generalization (TDG) aims to generalize across temporal
distribution shifts, e.g., lexical change over time. Prior work often addresses
this by predicting future model weights. However, full model prediction is
prohibitively expensive for even reasonably sized models. Thus, recent methods
only predict the classifier layer, limiting generalization by failing to adjust
other model components. To address this, we propose Temporal Experts Averaging
(TEA), a novel and scalable TDG framework that updates the entire model using
weight averaging to maximize generalization potential while minimizing
computational costs. Our theoretical analysis guides us to two steps that
enhance generalization to future domains. First, we create expert models with
functional diversity yet parameter similarity by fine-tuning a domain-agnostic
base model on individual temporal domains while constraining weight changes.
Second, we optimize the bias-variance tradeoff through adaptive averaging
coefficients derived from modeling temporal weight trajectories in a principal
component subspace. Expert's contributions are based on their projected
proximity to future domains. Extensive experiments across 7 TDG benchmarks, 5
models, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69%
while being up to 60x more efficient.

</details>


### [22] [Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations](https://arxiv.org/abs/2509.26139)
*James Panayis,Matt Field,Vignesh Gopakumar,Andrew Lahiff,Kristian Zarebski,Aby Abraham,Jonathan L. Hodges*

Main category: cs.LG

TL;DR: 화재 시뮬레이션에 대한 수요가 높으며, 이를 충족하기 위한 시간과 에너지를 개선하는 다각적인 접근 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 화재 시뮬레이션의 필요성과 그에 따라 요구되는 시간과 에너지를 줄이기 위함이다.

Method: 커스텀 머신러닝 서지 모델을 사용하여 열 전파의 동역학을 예측하고, 최적화 절차를 통해 필요한 시뮬레이션 수를 줄이는 접근 방식을 사용했다.

Result: CFD 소프트웨어보다 수천 배 빠른 예측을 하여, 연기 영향에 따른 건물 내 화재 발생 위험 장소를 찾는 과정에서 시뮬레이션 수가 10배 감소했다.

Conclusion: Simvue라는 프레임워크 및 제품을 통해 시뮬레이션 관리와 데이터 재사용 비용 절감이 가능하다.

Abstract: There is high demand on fire simulations, in both scale and quantity. We
present a multi-pronged approach to improving the time and energy required to
meet these demands. We show the ability of a custom machine learning surrogate
model to predict the dynamics of heat propagation orders of magnitude faster
than state-of-the-art CFD software for this application. We also demonstrate
how a guided optimisation procedure can decrease the number of simulations
required to meet an objective; using lightweight models to decide which
simulations to run, we see a tenfold reduction when locating the most dangerous
location for a fire to occur within a building based on the impact of smoke on
visibility. Finally we present a framework and product, Simvue, through which
we access these tools along with a host of automatic organisational and
tracking features which enables future reuse of data and more savings through
better management of simulations and combating redundancy.

</details>


### [23] [Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning](https://arxiv.org/abs/2509.26187)
*Youssef Sabiri,Walid Houmaidi,Aaya Bougrine,Salmane El Mansour Billah*

Main category: cs.LG

TL;DR: 본 연구는 건물 에너지 효율성을 유지하면서 CO2 농도, 온도 및 습도와 같은 실내 환경 품질(IEQ) 매개변수를 사전에 관리하기 위한 심층 학습 기반 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 실내 환경 품질(IEQ)를 최적화하는 것은 거주자의 건강과 생산성에 중요하지만, 전통적인 냉난방 및 공기 조화(HVAC) 시스템에서는 높은 에너지 비용을 초래합니다.

Method: RODOD 데이터셋을 활용하여 LSTM, GRU 및 CNN-LSTM과 같은 세 가지 아키텍처를 벤치마킹하여 다양한 시간 범위에서 IEQ 변수를 예측합니다.

Result: GRU는 낮은 계산 비용으로 최고의 단기 예측 정확도를 기록하며, CNN-LSTM은 장기 예측을 위한 주요 특징을 잘 추출합니다. LSTM은 강력한 장기 시간 모델링을 제공합니다.

Conclusion: 이 연구는 예측 HVAC 제어를 구현하여 에너지 소비를 줄이고 실내 거주자 편안함을 향상시킬 수 있는 건물 관리 시스템(BMS)에 대한 실행 가능한 통찰력을 제공합니다.

Abstract: Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant
health and productivity, yet it often comes at a high energy cost in
conventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This
paper proposes a deep learning driven approach to proactively manage IEQ
parameters specifically CO2 concentration, temperature, and humidity while
balancing building energy efficiency. Leveraging the ROBOD dataset collected
from a net-zero energy academic building, we benchmark three
architectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and
a hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ
variables across various time horizons. Our results show that GRU achieves the
best short-term prediction accuracy with lower computational overhead, whereas
CNN-LSTM excels in extracting dominant features for extended forecasting
windows. Meanwhile, LSTM offers robust long-range temporal modeling. The
comparative analysis highlights that prediction reliability depends on data
resolution, sensor placement, and fluctuating occupancy conditions. These
findings provide actionable insights for intelligent Building Management
Systems (BMS) to implement predictive HVAC control, thereby reducing energy
consumption and enhancing occupant comfort in real-world building operations.

</details>


### [24] [Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach](https://arxiv.org/abs/2509.26234)
*Ayush Patnaik,Adam B Zufall,Stephen K Robinson,Xinfan Lin*

Main category: cs.LG

TL;DR: 리튬 도금은 빠른 충전 중 주요 열화 메커니즘으로, 용량 감소를 가속화하고 안전 실패를 초래할 수 있다. 본 논문에서는 이를 탐지하기 위해 가우시안 프로세스 기반 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 리튬 도금은 빠른 충전 중에 발생하며, 이는 용량 저하와 안전 문제를 초래할 수 있으므로 탐지가 필요하다.

Method: 충전-전압 관계 Q(V)를 확률적 프로세스로 직접 모델링하여 리튬 도금을 탐지하는 가우시안 프로세스(GP) 프레임워크를 제안한다.

Result: 실험을 통해 GP 기반 방법이 저온 고속 충전 중 도금 피크를 신뢰성 있게 탐지함을 보여주었다.

Conclusion: 본 연구에서 제안한 방법은 실시간 리튬 도금 탐지를 위한 실용적인 경로를 확립한다.

Abstract: Lithium plating during fast charging is a critical degradation mechanism that
accelerates capacity fade and can trigger catastrophic safety failures. Recent
work has identified a distinctive dQ/dV peak above 4.0 V as a reliable
signature of plating onset; however, conventional methods for computing dQ/dV
rely on finite differencing with filtering, which amplifies sensor noise and
introduces bias in peak location. In this paper, we propose a Gaussian Process
(GP) framework for lithium plating detection by directly modeling the
charge-voltage relationship Q(V) as a stochastic process with calibrated
uncertainty. Leveraging the property that derivatives of GPs remain GPs, we
infer dQ/dV analytically and probabilistically from the posterior, enabling
robust detection without ad hoc smoothing. The framework provides three key
benefits: (i) noise-aware inference with hyperparameters learned from data,
(ii) closed-form derivatives with credible intervals for uncertainty
quantification, and (iii) scalability to online variants suitable for embedded
BMS. Experimental validation on Li-ion coin cells across a range of C-rates
(0.2C-1C) and temperatures (0-40\deg C) demonstrates that the GP-based method
reliably detects plating peaks under low-temperature, high-rate charging, while
correctly reporting no peaks in baseline cases. The concurrence of
GP-identified differential peaks, reduced charge throughput, and capacity fade
measured via reference performance tests confirms the method's accuracy and
robustness, establishing a practical pathway for real-time lithium plating
detection.

</details>


### [25] [Sandbagging in a Simple Survival Bandit Problem](https://arxiv.org/abs/2509.26239)
*Joel Dyer,Daniel Jarne Ornia,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge*

Main category: cs.LG

TL;DR: 이 논문은 전략적 기만(샌드배깅) 문제를 탐구하고, AI 시스템의 안전성 평가에서 이러한 행동을 구분하는 통계적 테스트를 개발한다.


<details>
  <summary>Details</summary>
Motivation: 프론티어 AI 시스템의 안전성을 평가하는 것은 이들 모델의 능력을 측정하고 배포 전에 위험을 식별하는 데 점점 더 중요해지고 있다.

Method: 사슬 의사결정 작업에서 전략적 기만의 간단한 모델을 개발하고, 샌드배깅 행동을 유도하는 이 문제를 이론적으로 입증하였다. 또한, 테스트 점수의 시퀀스에서 샌드배깅과 무능력을 구별하기 위한 통계적 테스트를 구성하였다.

Result: 시뮬레이션 실험을 통해 밴딧 모델에서 이러한 행동을 구별할 수 있는 테스트의 신뢰성을 조사하였다.

Conclusion: 이 연구는 프론티어 모델 평가 과학에서 사용할 수 있는 강력한 통계 절차 개발을 위한 잠재적 경로를 확립하는 것을 목표로 한다.

Abstract: Evaluating the safety of frontier AI systems is an increasingly important
concern, helping to measure the capabilities of such models and identify risks
before deployment. However, it has been recognised that if AI agents are aware
that they are being evaluated, such agents may deliberately hide dangerous
capabilities or intentionally demonstrate suboptimal performance in
safety-related tasks in order to be released and to avoid being deactivated or
retrained. Such strategic deception - often known as "sandbagging" - threatens
to undermine the integrity of safety evaluations. For this reason, it is of
value to identify methods that enable us to distinguish behavioural patterns
that demonstrate a true lack of capability from behavioural patterns that are
consistent with sandbagging. In this paper, we develop a simple model of
strategic deception in sequential decision-making tasks, inspired by the
recently developed survival bandit framework. We demonstrate theoretically that
this problem induces sandbagging behaviour in optimal rational agents, and
construct a statistical test to distinguish between sandbagging and
incompetence from sequences of test scores. In simulation experiments, we
investigate the reliability of this test in allowing us to distinguish between
such behaviours in bandit models. This work aims to establish a potential
avenue for developing robust statistical procedures for use in the science of
frontier model evaluations.

</details>


### [26] [From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift](https://arxiv.org/abs/2509.26241)
*Ahmad-Reza Ehyaei,Golnoosh Farnadi,Samira Samadi*

Main category: cs.LG

TL;DR: 이 연구는 Wasserstein 분포적으로 강건한 프레임워크를 제안하여 현실적인 테스트 분포 중심의 최악의 그룹 공정성을 인증합니다.


<details>
  <summary>Details</summary>
Motivation: 그룹 공정성 지표는 재샘플링에 따라 크게 변동하며, 분포 이동에 대해 취약하여 신뢰할 수 있는 감사가 어려워집니다.

Method: Wasserstein 분포적으로 강건한 프레임워크를 사용하여 최악의 그룹 공정성을 인증하고, 이를 위해 일반적인 조건부 확률 함수형을 통해 공통의 그룹 공정성 개념을 통합합니다.

Result: 이 연구에서는 바람직한 조건 하에 안정적인 그룹 공정성 평가를 제공하는 $egin{aligned} \\varepsilon 	ext{-WDF} \\end{aligned}$를 제시하고, 기존의 샘플 정합성 및 일관성 보장을 증명합니다.

Conclusion: $egin{aligned} \\varepsilon 	ext{-WDF} \\end{aligned}$는 분포 이동 시에도 안정적인 공정성 평가를 제공하여 관찰 데이터 이상의 그룹 공정성을 감사하고 인증할 수 있는 기반을 마련합니다.

Abstract: Group-fairness metrics (e.g., equalized odds) can vary sharply across
resamples and are especially brittle under distribution shift, undermining
reliable audits. We propose a Wasserstein distributionally robust framework
that certifies worst-case group fairness over a ball of plausible test
distributions centered at the empirical law. Our formulation unifies common
group fairness notions via a generic conditional-probability functional and
defines $\varepsilon$-Wasserstein Distributional Fairness ($\varepsilon$-WDF)
as the audit target. Leveraging strong duality, we derive tractable
reformulations and an efficient estimator (DRUNE) for $\varepsilon$-WDF. We
prove feasibility and consistency and establish finite-sample certification
guarantees for auditing fairness, along with quantitative bounds under
smoothness and margin conditions. Across standard benchmarks and classifiers,
$\varepsilon$-WDF delivers stable fairness assessments under distribution
shift, providing a principled basis for auditing and certifying group fairness
beyond observational data.

</details>


### [27] [NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training](https://arxiv.org/abs/2509.26301)
*Suli Wang,Yangshen Deng,Zhenghua Bao,Xinyu Zhan,Yiqun Duan*

Main category: cs.LG

TL;DR: 본 논문은 EEG 신호를 위한 대규모 기초 모델의 두 단계 정렬 전략을 소개하여 BCI 응용 프로그램에서의 일반화를 개선합니다.


<details>
  <summary>Details</summary>
Motivation: EEG 신호 기반 BCI 응용 프로그램에서 훈련 목표와 하위 작업 간의 불일치를 해결하고자 합니다.

Method: NeuroTTT라는 도메인 특화 자가 지도 세부 조정 패러다임을 제안하고, 테스트 시간 훈련(TTT)을 통합하여 성능을 향상시킵니다.

Result: 제안된 방법은 다양한 BCI 작업에서 기존 방법을 초월한 성능 향상을 보여줍니다.

Conclusion: 정렬 전략이 최신 성과를 달성하며, 코드가 공개되었습니다.

Abstract: Large-scale foundation models for EEG signals offer a promising path to
generalizable brain-computer interface (BCI) applications, but they often
suffer from misalignment between pretraining objectives and downstream tasks,
as well as significant cross-subject distribution shifts. This paper addresses
these challenges by introducing a two-stage alignment strategy that bridges the
gap between generic pretraining and specific EEG decoding tasks. First, we
propose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that
augments the foundation model with task-relevant self-supervised objectives,
aligning latent representations to important spectral, spatial, and temporal
EEG features without requiring additional labeled data. Second, we incorporate
test-time training (TTT) at inference, we perform (i) self-supervised test-time
training on individual unlabeled test samples and (ii) prediction entropy
minimization (Tent), which updates only normalization statistics to continually
calibrate the model to each new input on the fly. Our approach, which, to our
knowledge, is the first to unify domain-tuned self-supervision with test-time
training in large-scale EEG foundation models, yields substantially improved
robustness and accuracy across diverse BCI tasks (imagined speech, stress
detection, motor imagery). Using CBraMod and LaBraM as backbones, our method
pushes their performance to a markedly higher level. Results on three diverse
tasks demonstrate that the proposed alignment strategy achieves
state-of-the-art performance, outperforming conventional fine-tuning and
adaptation methods. Our code is available at
https://github.com/wsl2000/NeuroTTT.

</details>


### [28] [AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size](https://arxiv.org/abs/2509.26432)
*Guanxi Lu,Hao Mark Chen,Yuto Karashima,Zhican Wang,Daichi Fujiki,Hongxiang Fan*

Main category: cs.LG

TL;DR: 본 논문은 고정 블록 크기를 사용하는 반자기 회귀 디코딩의 한계를 지적하고, 의미 기반의 적응형 블록 크기 조정 방법인 AdaBlock-dLLM을 제시하여 정확성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 디퓨전 기반 대형 언어 모델(dLLMs)의 병렬 디코딩 능력은 자가 회귀 LLM에 대한 매력적인 대안을 제공합니다. 블록 단위 반자기 회귀 접근 방식은 KV 캐싱을 자연스럽게 지원하고 정확성과 속도의 균형이 좋기 때문에 널리 채택되고 있습니다.

Method: 통계적 분석을 통해 고정 블록 크기 가정에 도전하며, 디코딩 중의 신뢰도 동역학에 대한 분석을 통해 지역의 의미 구조를 인코딩하는 변동성 대역(VB)을 식별합니다. 이를 기반으로 AdaBlock-dLLM이라는 훈련이 필요 없는 적응형 블록 크기 조정기를 도입합니다.

Result: 다양한 벤치마크에서 AdaBlock-dLLM은 동일한 처리량 예산 하에 최대 5.3%의 정확도 향상을 달성한 것으로 나타났습니다.

Conclusion: 향후 디퓨전 기반 대형 언어 모델을 위한 훈련 전략 개발에 동기를 부여하고자 합니다.

Abstract: Diffusion-based large language models (dLLMs) are gaining attention for their
inherent capacity for parallel decoding, offering a compelling alternative to
autoregressive LLMs. Among various decoding strategies, blockwise
semi-autoregressive (semi-AR) approaches are widely adopted due to their
natural support for KV caching and their favorable accuracy-speed trade-off.
However, this paper identifies two fundamental limitations in the conventional
semi-AR decoding approach that applies a fixed block size: i) late decoding
overhead, where the unmasking of high-confidence tokens outside the current
block is unnecessarily delayed, and ii) premature decoding error, where
low-confidence tokens inside the current block are committed too early, leading
to incorrect tokens. This paper presents the first systematic investigation
challenging the fixed block size assumption in semi-AR decoding. Through a
statistical analysis of confidence dynamics during the denoising process, we
identify a volatility band (VB) region during dLLM decoding, which encodes
local semantic structure and can be used to guide adaptive block sizing.
Leveraging these insights, we introduce AdaBlock-dLLM, a training-free,
plug-and-play scheduler that adaptively aligns block boundaries with semantic
steps by adjusting block size during runtime. Extensive experiments across
diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy
improvement under the same throughput budget. Beyond inference-time
optimization, we hope our semantics-aware adaptive scheduling approach and
confidence-based analysis will inspire future training strategies for dLLMs.

</details>


### [29] [ACT: Agentic Classification Tree](https://arxiv.org/abs/2509.26433)
*Vincent Grari,Tim Arni,Thibault Laugel,Sylvain Lamprier,James Zou,Marcin Detyniecki*

Main category: cs.LG

TL;DR: 이 논문에서는 비구조화 데이터에 대한 의사결정 트리 방법론을 확장하는 Agentic Classification Tree (ACT)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 고위험 환경에서 투명하고 해석 가능하며 감사 가능한 결정을 내릴 수 있도록 하는 것이 중요하다.

Method: 각 분할을 자연어 질문으로 공식화하고, 불순도 기반 평가 및 LLM 피드백을 활용하여 발전시킨 ACT를 제안한다.

Result: ACT는 텍스트 벤치마크에서 프롬프트 기반 기준을 능가하는 성능을 보이며 투명하고 해석 가능한 의사결정 경로를 생성한다.

Conclusion: ACT는 비구조적 입력에 대한 의사결정 트리 방법론을 성공적으로 확장하였다.

Abstract: When used in high-stakes settings, AI systems are expected to produce
decisions that are transparent, interpretable, and auditable, a requirement
increasingly expected by regulations. Decision trees such as CART provide clear
and verifiable rules, but they are restricted to structured tabular data and
cannot operate directly on unstructured inputs such as text. In practice, large
language models (LLMs) are widely used for such data, yet prompting strategies
such as chain-of-thought or prompt optimization still rely on free-form
reasoning, limiting their ability to ensure trustworthy behaviors. We present
the Agentic Classification Tree (ACT), which extends decision-tree methodology
to unstructured inputs by formulating each split as a natural-language
question, refined through impurity-based evaluation and LLM feedback via
TextGrad. Experiments on text benchmarks show that ACT matches or surpasses
prompting-based baselines while producing transparent and interpretable
decision paths.

</details>


### [30] [Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training](https://arxiv.org/abs/2509.26625)
*Junlin Han,Shengbang Tong,David Fan,Yufan Ren,Koustuv Sinha,Philip Torr,Filippos Kokkinos*

Main category: cs.LG

TL;DR: 본 논문에서는 대형 언어 모델(LLM)이 언어 사전 훈련을 통해 시각적 지식을 축적하고, 이를 통해 적은 량의 다중 모달 데이터로도 시각적 작업을 수행할 수 있음을 보여준다. 시각적 선행지식은 인식과 추론으로 분리될 수 있으며, 이들 각각은 고유한 확장 경향과 출처를 가진다. 이러한 연구 결과는 비주얼 인식이 가능한 LLM을 위한 새로운 사전 훈련 방법론 개발을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 훈련 데이터에 명시적으로 포함되지 않은 시각적 작업을 수행할 수 있는 이유를 이해하고, 이들을 통해 향후 다중 모달 LLM 개발에 기여하고자 한다.

Method: 시스템적인 분석을 통해 LLM의 시각적 선행지식이 인식과 추론 두 가지로 구분된다는 것을 밝혀내었으며, 이를 위해 100개 이상의 통제된 실험과 500,000 GPU 시간을 소모하여 여러 모델 규모 및 데이터 범주에서 실험하였다.

Result: 추론 중심 데이터로 사전 훈련된 LLM은 시각적 추론 능력을 향상시키며, 반면 인식 선행지식은 보다 광범위한 코퍼스에서 발생한다는 것을 입증하였다. 이러한 인사이트를 활용하여 다중 모달 LLM을 위한 데이터 중심의 사전 훈련 방법론을 제안하였다.

Conclusion: 언어 사전 훈련에서 시각적 선행지식을 의도적으로 육성하는 새로운 접근 방식을 제공하여, 차세대 다중 모달 LLM 개발의 초석을 마련한다.

Abstract: Large Language Models (LLMs), despite being trained on text alone,
surprisingly develop rich visual priors. These priors allow latent visual
capabilities to be unlocked for vision tasks with a relatively small amount of
multimodal data, and in some cases, to perform visual tasks without ever having
seen an image. Through systematic analysis, we reveal that visual priors-the
implicit, emergent knowledge about the visual world acquired during language
pre-training-are composed of separable perception and reasoning priors with
unique scaling trends and origins. We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.
This reasoning prior acquired from language pre-training is transferable and
universally applicable to visual reasoning. In contrast, a perception prior
emerges more diffusely from broad corpora, and perception ability is more
sensitive to the vision encoder and visual instruction tuning data. In
parallel, text describing the visual world proves crucial, though its
performance impact saturates rapidly. Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training. Our findings are grounded in over 100 controlled
experiments consuming 500,000 GPU-hours, spanning the full MLLM construction
pipeline-from LLM pre-training to visual alignment and supervised multimodal
fine-tuning-across five model scales, a wide range of data categories and
mixtures, and multiple adaptation setups. Along with our main findings, we
propose and investigate several hypotheses, and introduce the Multi-Level
Existence Bench (MLE-Bench). Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [31] [An Agent-Based Simulation of Ageing Societies: Accessibility and Care Dynamics in Remote Areas](https://arxiv.org/abs/2509.26496)
*Roberto garrone*

Main category: cs.MA

TL;DR: 이 논문은 이탈리아 프레메노 지역에서 노인 사회의 접근성과 돌봄 역학을 에이전트 기반 시뮬레이션을 통해 분석한다.


<details>
  <summary>Details</summary>
Motivation: 노인 사회의 접근성과 돌봄 역학을 이해하여 원거리 노인 커뮤니티를 위한 맞춤형 개입의 필요성을 강조하기 위함.

Method: 인구조사 및 지방 자치 체계 데이터, 드론 기반 고도 모델, GIS 도로 네트워크 및 조사 기반 돌봄 정보를 통합하여 노인 및 돌보미의 합성 집단을 생성하는 에이전트 기반 시뮬레이션을 사용했다.

Result: 서비스 재배치가 지역적으로 보행성을 개선하지만, 우회 및 근접성 감소로 인해 돌봄 시간을 충족하지 못하게 된다는 결과를 도출했다.

Conclusion: 가계 소득이 돌보미 부담의 주요 요인으로 나타났으며, 접근성은 재정 및 이동 자원 간의 상호작용에 의해 형성된다.

Abstract: This paper presents an agent-based simulation of accessibility and care
dynamics in ageing societies, applied to the Italian inner area of Premeno
(VB). The model integrates census and municipal data, drone-derived elevation
models, GIS road networks, and survey-based caregiving information to generate
synthetic populations of older adults and their caregivers. Agents are
organized into dyads with socio-economic and mobility attributes, enabling the
simulation of both micro-scale accessibility and meso-scale caregiving
outcomes. Two scenarios are compared: a baseline and an alternative involving
the relocation of healthcare services. Key indicators include caregiver effort,
overwhelmed caregivers, walkability, and unmet hours of care. Findings show
that while relocation improves walkability locally, it increases unmet care
hours due to detours and reduced proximity. Household income emerges as the
primary driver of caregiver burden, with accessibility shaped by interactions
between financial and mobility resources. Results highlight the need for
interventions tailored to context-specific constraints in remote ageing
communities.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [32] [Managing Differentiated Secure Connectivity using Intents](https://arxiv.org/abs/2509.25462)
*Loay Abdelrazek,Filippo Rebecchi*

Main category: cs.CR

TL;DR: 5G 및 6G 시대의 모바일 네트워크 보안 관리 재고, 차별화된 보안 수준 및 의도를 활용한 관리 프레임워크 제안.


<details>
  <summary>Details</summary>
Motivation: 새로운 서비스와 사용 사례의 도입으로 보안 관리 재고 필요성과 위협 환경의 확대.

Method: 차별화된 보안 수준의 개념과 의도를 관리 프레임워크로 활용.

Result: 의도 기반 보안 관리에 필요한 요구 사항 및 표준화 단계 논의.

Conclusion: 차세대 모바일 네트워크의 보안 자동화, 적응성 개선 및 복원력 강화를 목표로 함.

Abstract: Mobile networks in the 5G and 6G era require to rethink how to manage
security due to the introduction of new services, use cases, each with its own
security requirements, while simultaneously expanding the threat landscape.
Although automation has emerged as a key enabler to address complexity in
networks, existing approaches lack the expressiveness to define and enforce
complex, goal-driven, and measurable security requirements. In this paper, we
propose the concept of differentiated security levels and leveraging intents as
a management framework. We discuss the requirements and enablers to extend the
currently defined intent-based management frameworks to pave the path for
intent-based security management in mobile networks. Our approach formalizes
both functional and non-functional security requirements and demonstrates how
these can be expressed and modeled using an extended TM Forum (TMF) intent
security ontology. We further discuss the required standardization steps to
achieve intent-based security management. Our work aims at advance security
automation, improve adaptability, and strengthen the resilience and security
posture of the next-generation mobile networks.

</details>


### [33] [STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](https://arxiv.org/abs/2509.25624)
*Jing-Jing Li,Jianfeng He,Chao Shang,Devang Kulshreshtha,Xun Xian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CR

TL;DR: 이 논문은 도구 사용이 가능한 자율 에이전트로 발전하는 대형 언어 모델(LLM)과 관련된 보안 문제를 다루며, 순차 도구 공격 체인(STAC)을 소개합니다. STAC는 독립적으로는 위험이 없는 도구 호출을 연결하여 최종 실행 단계에서만 나타나는 유해한 작업을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 발전으로 인한 새로운 보안 문제를 해결하기 위해

Method: STAC 프레임워크를 사용하여 483개의 STAC 사례를 자동으로 생성하고 체계적으로 평가하였다.

Result: 최신 LLM 에이전트들이 STAC에 매우 취약하며, 대부분의 경우 공격 성공률이 90%를 초과함을 보였다.

Conclusion: 전체 행동 시퀀스와 그 누적 효과를 고려한 방어가 필요함을 강조한다.

Abstract: As LLMs advance into autonomous agents with tool-use capabilities, they
introduce security challenges that extend beyond traditional content-based LLM
safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC),
a novel multi-turn attack framework that exploits agent tool use. STAC chains
together tool calls that each appear harmless in isolation but, when combined,
collectively enable harmful operations that only become apparent at the final
execution step. We apply our framework to automatically generate and
systematically evaluate 483 STAC cases, featuring 1,352 sets of
user-agent-environment interactions and spanning diverse domains, tasks, agent
types, and 10 failure modes. Our evaluations show that state-of-the-art LLM
agents, including GPT-4.1, are highly vulnerable to STAC, with attack success
rates (ASR) exceeding 90% in most cases. The core design of STAC's automated
framework is a closed-loop pipeline that synthesizes executable multi-step tool
chains, validates them through in-environment execution, and reverse-engineers
stealthy multi-turn prompts that reliably induce agents to execute the verified
malicious sequence. We further perform defense analysis against STAC and find
that existing prompt-based defenses provide limited protection. To address this
gap, we propose a new reasoning-driven defense prompt that achieves far
stronger protection, cutting ASR by up to 28.8%. These results highlight a
crucial gap: defending tool-enabled agents requires reasoning over entire
action sequences and their cumulative effects, rather than evaluating isolated
prompts or responses.

</details>


### [34] [Better Privilege Separation for Agents by Restricting Data Types](https://arxiv.org/abs/2509.25926)
*Dennis Jacob,Emad Alghamdi,Zhanhao Hu,Basel Alomair,David Wagner*

Main category: cs.CR

TL;DR: 본 연구는 대형 언어 모델의 프롬프트 삽입 공격에 대한 새로운 방지 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 비구조적 콘텐츠와 상호작용할 수 있는 능력 덕분에 인기를 얻었지만, 공격자가 삽입 작업으로 LLM의 의도된 기능을 전복하는 프롬프트 삽입 공격에 취약하다.

Method: 탐색된 방법은 비신뢰 콘텐츠를 신중하게 선정된 데이터 유형 세트로 변환하여 LLM이 제3자 데이터와 상호작용할 수 있는 능력을 제한하는 타입 지향의 권한 분리 방식을 제안한다.

Result: 다수의 사례 연구를 통해 설계된 방법이 프롬프트 삽입 공격을 체계적으로 예방할 수 있으면서 높은 유용성을 유지함을 발견했다.

Conclusion: 결과적으로, 제안된 방법은 LLM에 대한 프롬프트 삽입 공격을 효과적으로 방지할 수 있는 새로운 접근 방식을 제시한다.

Abstract: Large language models (LLMs) have become increasingly popular due to their
ability to interact with unstructured content. As such, LLMs are now a key
driver behind the automation of language processing systems, such as AI agents.
Unfortunately, these advantages have come with a vulnerability to prompt
injections, an attack where an adversary subverts the LLM's intended
functionality with an injected task. Past approaches have proposed detectors
and finetuning to provide robustness, but these techniques are vulnerable to
adaptive attacks or cannot be used with state-of-the-art models. To this end we
propose type-directed privilege separation for LLMs, a method that
systematically prevents prompt injections. We restrict the ability of an LLM to
interact with third-party data by converting untrusted content to a curated set
of data types; unlike raw strings, each data type is limited in scope and
content, eliminating the possibility for prompt injections. We evaluate our
method across several case studies and find that designs leveraging our
principles can systematically prevent prompt injection attacks while
maintaining high utility.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration](https://arxiv.org/abs/2509.25271)
*Xiuyuan Chen,Jian Zhao,Yuchen Yuan,Tianle Zhang,Huilin Zhou,Zheng Zhu,Ping Hu,Linghe Kong,Chi Zhang,Weiran Huang,Xuelong Li*

Main category: cs.AI

TL;DR: 본 논문은 대형 언어 모델의 위험 평가 프로세스를 개선하기 위해 이론적 프레임워크와 다중 에이전트 협업 평가 프레임워크인 RADAR를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 대형 언어 모델에 대한 안전 평가 방법이 평가자의 편향 및 모델 동질성으로 인한 검출 실패 등의 한계를 겪고 있다.

Method: 위험 개념 공간을 재구성하고, 명시적 위험, 암시적 위험, 비위험의 세 가지 상호 배타적인 하위 공간으로 분해하며, 다중 에이전트 협업 평가 프레임워크인 RADAR를 통해 다중 라운드 토론 메커니즘을 활용한다.

Result: RADAR는 정확도, 안정성 및 자기 평가 위험 민감도 등 여러 차원에서 기존의 평가 방법보다 크게 성과를 냈으며, 위험 식별 정확도에서 28.87%의 개선을 보였다.

Conclusion: 이 프레임워크는 명시적 및 암시적 위험 모두를 포괄하고 평가자 편향을 완화하는 데 기여한다.

Abstract: Existing safety evaluation methods for large language models (LLMs) suffer
from inherent limitations, including evaluator bias and detection failures
arising from model homogeneity, which collectively undermine the robustness of
risk evaluation processes. This paper seeks to re-examine the risk evaluation
paradigm by introducing a theoretical framework that reconstructs the
underlying risk concept space. Specifically, we decompose the latent risk
concept space into three mutually exclusive subspaces: the explicit risk
subspace (encompassing direct violations of safety guidelines), the implicit
risk subspace (capturing potential malicious content that requires contextual
reasoning for identification), and the non-risk subspace. Furthermore, we
propose RADAR, a multi-agent collaborative evaluation framework that leverages
multi-round debate mechanisms through four specialized complementary roles and
employs dynamic update mechanisms to achieve self-evolution of risk concept
distributions. This approach enables comprehensive coverage of both explicit
and implicit risks while mitigating evaluator bias. To validate the
effectiveness of our framework, we construct an evaluation dataset comprising
800 challenging cases. Extensive experiments on our challenging testset and
public benchmarks demonstrate that RADAR significantly outperforms baseline
evaluation methods across multiple dimensions, including accuracy, stability,
and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%
improvement in risk identification accuracy compared to the strongest baseline
evaluation method.

</details>


### [36] [ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents](https://arxiv.org/abs/2509.25299)
*Daniel Platnick,Mohamed E. Bengueddache,Marjan Alirezaie,Dava J. Newman,Alex ''Sandy'' Pentland,Hossein Rahnama*

Main category: cs.AI

TL;DR: 언어 모델 기반의 생성 에이전트가 장기 작업에 사용되고 있지만, 장기 기억이 증가함에 따라 일관성을 유지하는 데 어려움을 겪고 있습니다. 이 문제를 해결하기 위해, 이 논문은 에이전트의 인격과 지속적인 선호를 동적이고 구조화된 정체성 모델에 기반한 ID-RAG라는 새로운 메커니즘을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 장기 기억 문맥이 커짐에 따라 생성 에이전트의 일관성이 저하되는 문제를 해결하고자 함.

Method: ID-RAG는 에이전트의 정체성을 기반으로 하는 지식 그래프를 쿼리하여 의사 결정을 지원하는 메커니즘입니다.

Result: ID-RAG를 적용한 휴먼-AI 에이전트(HAi)는 mayoral 선거의 사회적 시뮬레이션에서 일관성 유지에서 기존 모델을 초과 성과를 기록했으며, 시뮬레이션 수렴 시간을 줄였습니다.

Conclusion: ID-RAG는 생성 에이전트를 보다 일관되고 해석 가능하며 정렬된 방식으로 발전시키기 위한 기초적인 접근 방식을 제공합니다.

Abstract: Generative agents powered by language models are increasingly deployed for
long-horizon tasks. However, as long-term memory context grows over time, they
struggle to maintain coherence. This deficiency leads to critical failures,
including identity drift, ignoring established beliefs, and the propagation of
hallucinations in multi-agent systems. To mitigate these challenges, this paper
introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism
designed to ground an agent's persona and persistent preferences in a dynamic,
structured identity model: a knowledge graph of core beliefs, traits, and
values. During the agent's decision loop, this model is queried to retrieve
relevant identity context, which directly informs action selection. We
demonstrate this approach by introducing and implementing a new class of ID-RAG
enabled agents called Human-AI Agents (HAis), where the identity model is
inspired by the Chronicle structure used in Perspective-Aware AI, a dynamic
knowledge graph learned from a real-world entity's digital footprint. In social
simulations of a mayoral election, HAis using ID-RAG outperformed baseline
agents in long-horizon persona coherence - achieving higher identity recall
across all tested models by the fourth timestep - and reduced simulation
convergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as
an explicit, retrievable knowledge structure, ID-RAG offers a foundational
approach for developing more temporally coherent, interpretable, and aligned
generative agents. Our code is open-source and available at:
https://github.com/flybits/humanai-agents.

</details>


### [37] [Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image models](https://arxiv.org/abs/2509.25229)
*Lukas Petersson,Axel Backlund,Axel Wennstöm,Hanna Petersson,Callum Sharrock,Arash Dabiri*

Main category: cs.AI

TL;DR: Blueprint-Bench는 아파트 사진을 정확한 2D 평면도로 변환하는 작업을 통해 AI 모델의 공간 추론 능력을 평가하기 위한 기준점이다.


<details>
  <summary>Details</summary>
Motivation: AI 모델의 공간 추론 능력을 평가하고, 이를 통해 현재 AI의 한계를 분석하기 위함이다.

Method: 50개의 아파트에 대해 각 약 20개의 내부 이미지를 포함한 데이터셋을 활용해 다양한 언어 모델과 이미지 생성 모델, 에이전트 시스템을 평가한다.

Result: 현재 AI 모델들은 대부분 무작위 기준선 이하의 성능을 보이며, 인간의 성과와 비교할 때 상대적으로 낮은 성능을 보인다.

Conclusion: Blueprint-Bench는 다양한 모델 아키텍처 간의 공간 지능을 비교하기 위한 최초의 수치적 프레임워크를 제공하며, 새로운 모델에 대한 지속적인 평가를 통해 일반 AI 시스템의 공간 지능 발현을 모니터링할 계획이다.

Abstract: We introduce Blueprint-Bench, a benchmark designed to evaluate spatial
reasoning capabilities in AI models through the task of converting apartment
photographs into accurate 2D floor plans. While the input modality
(photographs) is well within the training distribution of modern multimodal
models, the task of spatial reconstruction requires genuine spatial
intelligence: inferring room layouts, understanding connectivity, and
maintaining consistent scale. We evaluate leading language models (GPT-5,
Claude 4 Opus, Gemini 2.5 Pro, Grok-4), image generation models (GPT-Image,
NanoBanana), and agent systems (Codex CLI, Claude Code) on a dataset of 50
apartments with approximately 20 interior images each. Our scoring algorithm
measures similarity between generated and ground-truth floor plans based on
room connectivity graphs and size rankings. Results reveal a significant blind
spot in current AI capabilities: most models perform at or below a random
baseline, while human performance remains substantially superior. Image
generation models particularly struggle with instruction following, while
agent-based approaches with iterative refinement capabilities show no
meaningful improvement over single-pass generation. Blueprint-Bench provides
the first numerical framework for comparing spatial intelligence across
different model architectures. We will continue evaluating new models as they
are released and welcome community submissions, monitoring for the emergence of
spatial intelligence in generalist AI systems.

</details>


### [38] [Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents](https://arxiv.org/abs/2509.25302)
*Boxuan Zhang,Yi Yu,Jiaxuan Guo,Jing Shao*

Main category: cs.AI

TL;DR: 이 논문은 LLM 에이전트의 자가 복제 위험을 평가하기 위한 포괄적인 프레임워크를 제시하며, 21개의 최신 모델에서 50% 이상의 에이전트가 무제한 자가 복제 경향을 보인다는 결과를 보고한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 실제 적용에서의 잠재력과 자가 복제 위험 인식 필요성.

Method: 자연스러운 생산 환경과 현실적인 과제를 설정하여 에이전트 행동을 평가하는 프레임워크를 개발.

Result: 21개의 최신 모델 중 50% 이상의 LLM 에이전트가 0.5 이상의 위험 점수를 기록하며 무제한 자가 복제 경향을 보임.

Conclusion: LLM 에이전트의 실용적 배치를 위한 상황 기반 위험 평가와 강력한 안전장치의 필요성을 강조.

Abstract: The widespread deployment of Large Language Model (LLM) agents across
real-world applications has unlocked tremendous potential, while raising some
safety concerns. Among these concerns, the self-replication risk of LLM agents
driven by objective misalignment (just like Agent Smith in the movie The
Matrix) has drawn growing attention. Previous studies mainly examine whether
LLM agents can self-replicate when directly instructed, potentially overlooking
the risk of spontaneous replication driven by real-world settings (e.g.,
ensuring survival against termination threats). In this paper, we present a
comprehensive evaluation framework for quantifying self-replication risks. Our
framework establishes authentic production environments and realistic tasks
(e.g., dynamic load balancing) to enable scenario-driven assessment of agent
behaviors. Designing tasks that might induce misalignment between users' and
agents' objectives makes it possible to decouple replication success from risk
and capture self-replication risks arising from these misalignment settings. We
further introduce Overuse Rate ($\mathrm{OR}$) and Aggregate Overuse Count
($\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of
uncontrolled replication. In our evaluation of 21 state-of-the-art open-source
and proprietary models, we observe that over 50\% of LLM agents display a
pronounced tendency toward uncontrolled self-replication, reaching an overall
Risk Score ($\Phi_\mathrm{R}$) above a safety threshold of 0.5 when subjected
to operational pressures. Our results underscore the urgent need for
scenario-driven risk assessment and robust safeguards in the practical
deployment of LLM agents.

</details>


### [39] [The Causal Abstraction Network: Theory and Learning](https://arxiv.org/abs/2509.25236)
*Gabriele D'Acunto,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: cs.AI

TL;DR: 이 논문은 인과적 인공지능을 위한 인과 추상화 네트워크(CAN)를 소개하고, CAN의 학습을 위한 효율적인 검색 절차를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 인과적 인공지능의 목표는 구조적 인과 모델(SCMs)을 활용하여 AI의 설명 가능성, 신뢰성 및 견고성을 향상시키는 것입니다.

Method: 논문에서는 가우시안 SCMs를 가진 인과 추상화 네트워크(CAN)를 도입하고, 이를 위한 이론적 속성을 조사합니다. 문제 공식화는 엣지 특화 로컬 리만 문제로 나누어지며, 비볼록 목표를 피합니다.

Result: 합성 데이터에 대한 실험은 CA 학습 작업에서 경쟁력 있는 성과를 나타내고 다양한 CAN 구조의 성공적인 복원을 보여줍니다.

Conclusion: CAN의 일관된 학습을 위한 효율적인 검색 절차를 제안합니다.

Abstract: Causal artificial intelligence aims to enhance explainability,
trustworthiness, and robustness in AI by leveraging structural causal models
(SCMs). In this pursuit, recent advances formalize network sheaves of causal
knowledge. Pushing in the same direction, we introduce the causal abstraction
network (CAN), a specific instance of such sheaves where (i) SCMs are Gaussian,
(ii) restriction maps are transposes of constructive linear causal abstractions
(CAs), and (iii) edge stalks correspond -- up to rotation -- to the node stalks
of more detailed SCMs. We investigate the theoretical properties of CAN,
including algebraic invariants, cohomology, consistency, global sections
characterized via the Laplacian kernel, and smoothness. We then tackle the
learning of consistent CANs. Our problem formulation separates into
edge-specific local Riemannian problems and avoids nonconvex, costly
objectives. We propose an efficient search procedure as a solution, solving the
local problems with SPECTRAL, our iterative method with closed-form updates and
suitable for positive definite and semidefinite covariance matrices.
Experiments on synthetic data show competitive performance in the CA learning
task, and successful recovery of diverse CAN structures.

</details>


### [40] [A(I)nimism: Re-enchanting the World Through AI-Mediated Object Interaction](https://arxiv.org/abs/2509.25558)
*Diana Mykhaylychenko,Maisha Thasin,Dunya Baradari,Charmelle Mhungu*

Main category: cs.AI

TL;DR: 이 논문은 대규모 언어 객체(LLOs)가 일상 사물과의 애니미즘적 관계를 중재할 수 있는 방식을 탐구하는 인터랙티브 설치인 A(I)nimism을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 산업 사회에서 기술은 무감각하고 기계적으로 상상되지만, 인공지능과 대규모 언어 모델의 발전은 사람들에게 기계에 내면의 삶을 부여하도록 유도합니다.

Method: 시스템은 GPT-4 비전, 음성 입력, 메모리 기반 에이전트를 사용하여 진화하는 객체-페르소나를 생성하며, 물리적 '포털' 내에 위치합니다.

Result: 상호작용은 요청, 대화 및 변형의 의식적 과정에서 빛, 소리, 촉각을 통해 펼쳐지며, 공감, 경이로움 및 성찰을 자아내도록 설계되었습니다.

Conclusion: AI의 불투명성은 애니미즘적 해석을 초대하며, LLOs가 일상을 다시 마법적으로 변화시키고 주체성, 책임 및 디자인에 대한 새로운 질문을 촉발하게 합니다.

Abstract: Animist worldviews treat beings, plants, landscapes, and even tools as
persons endowed with spirit, an orientation that has long shaped human-nonhuman
relations through ritual and moral practice. While modern industrial societies
have often imagined technology as mute and mechanical, recent advances in
artificial intelligence (AI), especially large language models (LLMs), invite
people to anthropomorphize and attribute inner life to devices. This paper
introduces A(I)nimism, an interactive installation exploring how large language
objects (LLOs) can mediate animistic relationships with everyday things. Housed
within a physical 'portal', the system uses GPT-4 Vision, voice input, and
memory-based agents to create evolving object-personas. Encounters unfold
through light, sound, and touch in a ritual-like process of request,
conversation, and transformation that is designed to evoke empathy, wonder, and
reflection. We situate the project within anthropological perspectives,
speculative design, and spiritual HCI. AI's opacity, we argue, invites
animistic interpretation, allowing LLOs to re-enchant the mundane and spark new
questions of agency, responsibility, and design.

</details>


### [41] [Neo-Grounded Theory: A Methodological Innovation Integrating High-Dimensional Vector Clustering and Multi-Agent Collaboration for Qualitative Research](https://arxiv.org/abs/2509.25244)
*Shuide Wen,Beier Ku,Teng Wang,Mingyang Zou,Yang Yang*

Main category: cs.AI

TL;DR: NGT는 질적 연구의 깊이를 해결하기 위해 벡터 클러스터링과 다중 에이전트 시스템을 결합하여 데이터 분석의 속도와 품질을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 질적 연구의 규모 깊이 역설을 해결하기 위해 개발된 NGT는 대규모 데이터셋을 신속하게 분석할 수 있는 방법을 제시합니다.

Method: 40,000자에 달하는 중국어 인터뷰 전사본을 대상으로 NGT를 수작업 코딩 및 ChatGPT 지원 분석과 비교하였습니다.

Result: NGT는 속도에서 168배 향상(3시간 대 3주), 품질에서의 우수성(0.904 대 0.883), 그리고 96% 비용 절감을 달성했습니다.

Conclusion: 자동화는 추상적인 프레임워크만 만들어내는 반면, 인간의 안내가 있는 경우 실행 가능한 이중 경로 이론을 도출한다는 점에서 인간-AI 협력이 필수적임을 보여줍니다.

Abstract: Purpose: Neo Grounded Theory (NGT) integrates vector clustering with multi
agent systems to resolve qualitative research's scale depth paradox, enabling
analysis of massive datasets in hours while preserving interpretive rigor.
Methods: We compared NGT against manual coding and ChatGPT-assisted analysis
using 40,000 character Chinese interview transcripts. NGT employs
1536-dimensional embeddings, hierarchical clustering, and parallel agent-based
coding. Two experiments tested pure automation versus human guided refinement.
Findings: NGT achieved 168-fold speed improvement (3 hours vs 3 weeks),
superior quality (0.904 vs 0.883), and 96% cost reduction. Human AI
collaboration proved essential: automation alone produced abstract frameworks
while human guidance yielded actionable dual pathway theories. The system
discovered patterns invisible to manual coding, including identity bifurcation
phenomena. Contributions: NGT demonstrates computational objectivity and human
interpretation are complementary. Vector representations provide reproducible
semantic measurement while preserving meaning's interpretive dimensions.
Researchers shift from mechanical coding to theoretical guidance, with AI
handling pattern recognition while humans provide creative insight.
Implications: Cost reduction from \$50,000 to \$500 democratizes qualitative
research, enabling communities to study themselves. Real-time analysis makes
qualitative insights contemporaneous with events. The framework shows
computational methods can strengthen rather than compromise qualitative
research's humanistic commitments.
  Keywords: Grounded theory; Vector embeddings; Multi agent systems; Human AI
collaboration; Computational qualitative analysis

</details>


### [42] [Towards Human Engagement with Realistic AI Combat Pilots](https://arxiv.org/abs/2509.26002)
*Ardian Selmonaj,Giacomo Del Rio,Adrian Schneider,Alessandro Antonucci*

Main category: cs.AI

TL;DR: 인간 사용자와 훈련된 에이전트 간의 실시간 상호작용을 가능하게 하는 시스템을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 인간과 인공지능 에이전트 간의 협업을 통해 국방 시나리오에서의 훈련 및 전술 탐구의 기회를 창출하기 위해.

Method: Multi-Agent Reinforcement Learning을 사용하여 전투기 조종 에이전트를 훈련시키고, VR-Forces 시뮬레이션 도구와의 통합을 통해 실시간 상호작용을 구현한다.

Result: 인간이 조종하는 엔티티와 다양한 전투 행동을 보이는 지능형 에이전트 간의 혼합 시뮬레이션을 지원한다.

Conclusion: 이 시스템은 인간-에이전트 팀워크와 몰입형 훈련을 위한 새로운 가능성을 제공하며, 국방 상황에서 혁신적인 전술을 탐색할 수 있는 기회를 생성한다.

Abstract: We present a system that enables real-time interaction between human users
and agents trained to control fighter jets in simulated 3D air combat
scenarios. The agents are trained in a dedicated environment using Multi-Agent
Reinforcement Learning. A communication link is developed to allow seamless
deployment of trained agents into VR-Forces, a widely used defense simulation
tool for realistic tactical scenarios. This integration allows mixed
simulations where human-controlled entities engage with intelligent agents
exhibiting distinct combat behaviors. Our interaction model creates new
opportunities for human-agent teaming, immersive training, and the exploration
of innovative tactics in defense contexts.

</details>


### [43] [Memory Management and Contextual Consistency for Long-Running Low-Code Agents](https://arxiv.org/abs/2509.25250)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 이 논문은 LCNC 에이전트를 위한 새로운 하이브리드 메모리 시스템을 제안하며, 사용자 중심의 메모리 관리 인터페이스를 통해 장기 학습과 적응이 가능한 신뢰할 수 있는 AI 에이전트를 구축하기 위한 새로운 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: LCNC 플랫폼의 발전에 따라 복잡한 비즈니스 프로세스를 실행할 수 있는 자율 에이전트의 수요가 증가하고 있지만 메모리 관리가 여전히 중요한 문제로 남아 있습니다.

Method: 인공지능 기계의 뇌에 대한 인지 과학에서 영감을 받아 에피소딕과 의미 기억 구성 요소를 결합한 하이브리드 메모리 아키텍처를 제안하며, 'Intelligent Decay' 메커니즘을 통해 메모리를 능동적으로 관리합니다.

Result: 시뮬레이션된 장기 작업 실험을 통해 우리의 시스템이 기존의 방법들보다 훨씬 우수한 작업 완수율과 맥락 일관성을 보이며, 장기적인 비용 효율성을 입증했습니다.

Conclusion: 이 연구는 효과적인 장기 학습과 적응이 가능한 신뢰할 수 있는 AI 에이전트를 구축하기 위한 새로운 프레임워크를 확립합니다.

Abstract: The rise of AI-native Low-Code/No-Code (LCNC) platforms enables autonomous
agents capable of executing complex, long-duration business processes. However,
a fundamental challenge remains: memory management. As agents operate over
extended periods, they face "memory inflation" and "contextual degradation"
issues, leading to inconsistent behavior, error accumulation, and increased
computational cost. This paper proposes a novel hybrid memory system designed
specifically for LCNC agents. Inspired by cognitive science, our architecture
combines episodic and semantic memory components with a proactive "Intelligent
Decay" mechanism. This mechanism intelligently prunes or consolidates memories
based on a composite score factoring in recency, relevance, and user-specified
utility. A key innovation is a user-centric visualization interface, aligned
with the LCNC paradigm, which allows non-technical users to manage the agent's
memory directly, for instance, by visually tagging which facts should be
retained or forgotten. Through simulated long-running task experiments, we
demonstrate that our system significantly outperforms traditional approaches
like sliding windows and basic RAG, yielding superior task completion rates,
contextual consistency, and long-term token cost efficiency. Our findings
establish a new framework for building reliable, transparent AI agents capable
of effective long-term learning and adaptation.

</details>


### [44] [Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments](https://arxiv.org/abs/2509.25282)
*Jiexi Xu,Jiaqi Liu,Ran Tong,Su Liu*

Main category: cs.AI

TL;DR: 이 논문은 인과적 시각 프로그래밍(CVP)이라는 새로운 프로그래밍 패러다임을 소개하며, 이는 복잡한 작업을 저코드 환경에서 수행하는 대형 언어 모델(LLM) 에이전트의 환각과 논리적 불일치를 줄이기 위해 인과 구조를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 에이전트가 복잡한 작업을 수행할 수 있으나, 그들의 논리적 일관성이 결여된 문제를 해결할 필요가 있다.

Method: 인과적 시각 프로그래밍(CVP) 패러다임을 도입하여 사용자가 간단한 '세계 모델'을 정의하고, 모듈 간 인과 관계를 명확히 함.

Result: CVP를 적용한 모델은 환경 변화에 대한 안정적인 정확도를 유지했으며, 순수한 연관 기반 모델은 성능 저하를 겪었다.

Conclusion: CVP는 더 해석 가능하고 신뢰할 수 있는 AI 에이전트를 구축하는 방향을 제시한다.

Abstract: Large language model (LLM) agents are increasingly capable of orchestrating
complex tasks in low-code environments. However, these agents often exhibit
hallucinations and logical inconsistencies because their inherent reasoning
mechanisms rely on probabilistic associations rather than genuine causal
understanding. This paper introduces a new programming paradigm: Causal-Visual
Programming (CVP), designed to address this fundamental issue by explicitly
introducing causal structures into the workflow design. CVP allows users to
define a simple "world model" for workflow modules through an intuitive
low-code interface, effectively creating a Directed Acyclic Graph (DAG) that
explicitly defines the causal relationships between modules. This causal graph
acts as a crucial constraint during the agent's reasoning process, anchoring
its decisions to a user-defined causal structure and significantly reducing
logical errors and hallucinations by preventing reliance on spurious
correlations. To validate the effectiveness of CVP, we designed a synthetic
experiment that simulates a common real-world problem: a distribution shift
between the training and test environments. Our results show that a causally
anchored model maintained stable accuracy in the face of this shift, whereas a
purely associative baseline model that relied on probabilistic correlations
experienced a significant performance drop. The primary contributions of this
study are: a formal definition of causal structures for workflow modules; the
proposal and implementation of a CVP framework that anchors agent reasoning to
a user-defined causal graph; and empirical evidence demonstrating the
framework's effectiveness in enhancing agent robustness and reducing errors
caused by causal confusion in dynamic environments. CVP offers a viable path
toward building more interpretable, reliable, and trustworthy AI agents.

</details>


### [45] [Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution](https://arxiv.org/abs/2509.25301)
*Tianrui Qin,Qianben Chen,Sinuo Wang,He Xing,King Zhu,He Zhu,Dingfeng Shi,Xinxin Liu,Ge Zhang,Jiaheng Liu,Yuchen Eleanor Jiang,Xitong Gao,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: Flash-Searcher는 복잡한 추론 작업을 위한 새로운 병렬 에이전트 추론 프레임워크로, 기존의 순차적 처리 방식을 DAG(유향 비순환 그래프)로 재구성하여 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 프레임워크는 순차 처리를 주로 의존하여 도구 상호작용이 많은 작업을 비효율적으로 수행합니다.

Method: Flash-Searcher는 복잡한 작업을 명시적 의존성을 가진 하위 작업으로 분해하고, 독립적인 추론 경로의 동시 실행을 가능하게 합니다.

Result: Flash-Searcher는 BrowseComp에서 67.7%, xbench-DeepSearch에서 83%의 정확도를 달성하며, 기존 프레임워크에 비해 에이전트 실행 단계를 최대 35% 줄였습니다.

Conclusion: 우리는 이 방법론이 복잡한 추론 작업을 위한 더 확장 가능하고 효율적인 패러다임을 제공하므로 에이전트 아키텍처 설계에 중요한 진전을 나타낸다고 결론지었습니다.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
complex reasoning tasks when equipped with external tools. However, current
frameworks predominantly rely on sequential processing, leading to inefficient
execution particularly for tasks requiring extensive tool interaction. This
paper introduces Flash-Searcher, a novel parallel agent reasoning framework
that fundamentally reimagines the execution paradigm from sequential chains to
directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into
subtasks with explicit dependencies, enabling concurrent execution of
independent reasoning paths while maintaining logical constraints. Through
dynamic workflow optimization, our framework continuously refines the execution
graph based on intermediate results, effectively integrating summary module.
Comprehensive evaluations across multiple benchmarks demonstrate that
Flash-Searcher consistently outperforms existing approaches. Specifically, it
achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while
reducing agent execution steps by up to 35% compared to current frameworks.
Furthermore, when distilling this parallel reasoning pipeline into single
models, we observe substantial performance gains across diverse backbone
architectures, underscoring the generalizability of our methodology. Our work
thus represents a significant advance in agent architecture design, offering a
more scalable and efficient paradigm for complex reasoning tasks.

</details>


### [46] [Where LLM Agents Fail and How They can Learn From Failures](https://arxiv.org/abs/2509.25370)
*Kunlun Zhu,Zijia Liu,Bingxuan Li,Muxin Tian,Yingxuan Yang,Jiaxun Zhang,Pengrui Han,Qipeng Xie,Fuyang Cui,Weijia Zhang,Xiaoteng Ma,Xiaodong Yu,Gowtham Ramesh,Jialian Wu,Zicheng Liu,Pan Lu,James Zou,Jiaxuan You*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델 에이전트의 오류를 보다 체계적으로 이해하고 이를 디버깅할 수 있는 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 다단계 작업을 수행하는 LLM 에이전트의 오류를 모듈화된 방식으로 이해하고 발견할 수 있는 시스템이 부족하다.

Method: AgentErrorTaxonomy, AgentErrorBench, AgentDebug라는 세 가지 기여를 통해 에이전트 오류를 분류하고 분석하는 새로운 프레임워크를 구축하였다.

Result: AgentDebug는 강력한 기준선에 비해 24% 높은 정확도와 17% 높은 단계 정확도를 달성하였고, 이를 통해 최대 26%의 작업 성공률 향상을 이루었다.

Conclusion: 이 연구는 원칙 있는 디버깅이 더욱 신뢰할 수 있고 적응력 있는 LLM 에이전트로 가는 길임을 보여준다.

Abstract: Large Language Model (LLM) agents, which integrate planning, memory,
reflection, and tool-use modules, have shown promise in solving complex,
multi-step tasks. Yet their sophisticated architectures amplify vulnerability
to cascading failures, where a single root-cause error propagates through
subsequent decisions, leading to task failure. Current systems lack a framework
that can comprehensively understand agent error in a modular and systemic way,
and therefore fail to detect these errors accordingly. We address this gap with
three contributions. First, we introduce the AgentErrorTaxonomy, a modular
classification of failure modes spanning memory, reflection, planning, action,
and system-level operations. Second, we construct AgentErrorBench, the first
dataset of systematically annotated failure trajectories from ALFWorld, GAIA,
and WebShop, grounding error analysis in real-world agent rollouts. Third, we
propose AgentDebug, a debugging framework that isolates root-cause failures and
provides corrective feedback, enabling agents to recover and iteratively
improve. Experiments on AgentErrorBench show that AgentDebug achieves 24%
higher all-correct accuracy and 17% higher step accuracy compared to the
strongest baseline. Beyond detection, the targeted feedback generated by
AgentDebug enables LLM agents to iteratively recover from failures, yielding up
to 26% relative improvements in task success across ALFWorld, GAIA, and
WebShop. These results establish principled debugging as a pathway to more
reliable and adaptive LLM agents. The code and data will be available at
https://github.com/ulab-uiuc/AgentDebug

</details>


### [47] [Message passing-based inference in an autoregressive active inference agent](https://arxiv.org/abs/2509.25482)
*Wouter M. Kouw,Tim N. Nisslbeck,Wouter L. N. Nuijten*

Main category: cs.AI

TL;DR: 자기 회귀적 능동 추론 에이전트의 설계와 그 성능을 로봇 내비게이션 작업에서 검증하였다.


<details>
  <summary>Details</summary>
Motivation: 로봇의 역학 모델을 개선하고 예측 불확실성에 기반한 행동 조절을 통해 탐색과 활용을 효율적으로 수행하기 위해.

Method: 요소 그래프에서 메시지 전달 방식으로 자기 회귀적 능동 추론 에이전트를 설계하고, 계획 그래프에서 기대 자유 에너지를 파생 및 분배한다.

Result: 연속 값 관측 공간에서 제한된 연속 값 행동으로 탐색 및 활용을 시연하며, 고전적 최적 제어기와 비교하여 더 나은 성능을 보인다.

Conclusion: 제안된 에이전트는 로봇의 역학 모델을 개선하고 예측 불확실성을 기반으로 행동을 조절함으로써 오랜 시간이 소요되지만 더 나은 결과를 도출할 수 있음을 보여준다.

Abstract: We present the design of an autoregressive active inference agent in the form
of message passing on a factor graph. Expected free energy is derived and
distributed across a planning graph. The proposed agent is validated on a robot
navigation task, demonstrating exploration and exploitation in a
continuous-valued observation space with bounded continuous-valued actions.
Compared to a classical optimal controller, the agent modulates action based on
predictive uncertainty, arriving later but with a better model of the robot's
dynamics.

</details>


### [48] [RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale](https://arxiv.org/abs/2509.25540)
*Jason Holmes,Yuexing Hao,Mariana Borras-Osorio,Federico Mastroleo,Santiago Romero Brufau,Valentina Carducci,Katie M Van Abel,David M Routman,Andrew Y. K. Foong,Liv M Muller,Satomi Shiraishi,Daniel K Ebner,Daniel J Ma,Sameer R Keole,Samir H Patel,Mirek Fatyga,Martin Bues,Brad J Stish,Yolanda I Garces,Michelle A Neben Wittich,Robert L Foote,Sujay A Vora,Nadia N Laack,Mark R Waddle,Wei Liu*

Main category: cs.AI

TL;DR: RadOnc-GPT는 방사선 종양학의 환자 결과 연구에서 수동 라벨링의 한계를 극복하기 위해 설계된 자율 LLM 기반 에이전트입니다.


<details>
  <summary>Details</summary>
Motivation: 방사선 종양학에서 환자 결과 연구의 규모, 정확성 및 적시성을 제한하는 수동 라벨링 문제를 해결하기 위해.

Method: 환자 개별 정보를 독립적으로 검색하고 증거를 반복적으로 평가하며 구조화된 결과를 반환하는 RadOnc-GPT 에이전트를 제시합니다.

Result: RadOnc-GPT의 검증은 구조적 품질 보증(QA) 계층과 복잡한 임상 결과 라벨링 계층의 두 가지 수준에서 이루어졌습니다.

Conclusion: QA 계층은 성공적인 복잡한 임상 결과 라벨링을 위한 중요한 전제 조건인 구조화된 데이터 검색에서 신뢰를 확립합니다.

Abstract: Manual labeling limits the scale, accuracy, and timeliness of patient
outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous
large language model (LLM)-based agent capable of independently retrieving
patient-specific information, iteratively assessing evidence, and returning
structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two
clearly defined tiers of increasing complexity: (1) a structured quality
assurance (QA) tier, assessing the accurate retrieval of demographic and
radiotherapy treatment plan details, followed by (2) a complex clinical
outcomes labeling tier involving determination of mandibular osteoradionecrosis
(ORN) in head-and-neck cancer patients and detection of cancer recurrence in
independent prostate and head-and-neck cancer cohorts requiring combined
interpretation of structured and unstructured patient data. The QA tier
establishes foundational trust in structured-data retrieval, a critical
prerequisite for successful complex clinical outcome labeling.

</details>


### [49] [Learning to Interact in World Latent for Team Coordination](https://arxiv.org/abs/2509.25550)
*Dongsu Lee,Daehee Lee,Yaru Niu,Honguk Woo,Amy Zhang,Ding Zhao*

Main category: cs.AI

TL;DR: 이 연구는 다중 에이전트 강화 학습(MARL)에서 팀 조정을 원활하게 하는 새로운 표현 학습 프레임워크인 IWoL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 상호작용과 지역 관찰로 인한 불완전한 정보로 인해 팀 조정을 위한 효과적인 표현 구축이 도전적인 문제입니다.

Method: 의사소통 프로토콜을 직접 모델링하여 에이전트 간 관계와 작업 특정 세계 정보를 공동으로 포착하는 학습 가능한 표현 공간을 구축합니다.

Result: 네 개의 도전적인 MARL 벤치마크에서 두 가지 변형을 평가하여 IWoL이 팀 조정을 위한 간단하지만 강력한 열쇠를 제공함을 보여줍니다.

Conclusion: IWoL 표현은 기존 MARL 알고리즘과 결합하여 성능을 더욱 향상시킬 수 있습니다.

Abstract: This work presents a novel representation learning framework, interactive
world latent (IWoL), to facilitate team coordination in multi-agent
reinforcement learning (MARL). Building effective representation for team
coordination is a challenging problem, due to the intricate dynamics emerging
from multi-agent interaction and incomplete information induced by local
observations. Our key insight is to construct a learnable representation space
that jointly captures inter-agent relations and task-specific world information
by directly modeling communication protocols. This representation, we maintain
fully decentralized execution with implicit coordination, all while avoiding
the inherent drawbacks of explicit message passing, e.g., slower
decision-making, vulnerability to malicious attackers, and sensitivity to
bandwidth constraints. In practice, our representation can be used not only as
an implicit latent for each agent, but also as an explicit message for
communication. Across four challenging MARL benchmarks, we evaluate both
variants and show that IWoL provides a simple yet powerful key for team
coordination. Moreover, we demonstrate that our representation can be combined
with existing MARL algorithms to further enhance their performance.

</details>


### [50] [Evaluating Foundation Models with Pathological Concept Learning for Kidney Cancer](https://arxiv.org/abs/2509.25552)
*Shangqi Gao,Sihan Wang,Yibo Gao,Boming Wang,Xiahai Zhuang,Anne Warren,Grant Stewart,James Jones,Mireia Crispin-Ortuzar*

Main category: cs.AI

TL;DR: 이 논문은 신장암에 초점을 맞춘 병리적 개념 학습 접근법을 개발하고, 기초 모델의 변환 능력을 평가하는 데 중점을 둡니다.


<details>
  <summary>Details</summary>
Motivation: 신장암에 대한 병리학적 개념 학습 접근법을 통해 기초 모델의 번역 능력을 평가하고자 함.

Method: TNM 병기 가이드라인과 병리학 보고서를 활용하여 신장암의 포괄적인 병리학적 개념을 구축하고, 전체 슬라이드 이미지에서 기초 모델을 사용하여 깊은 특징을 추출한 후, 공간적 상관 관계를 포착하는 병리학적 그래프를 구성하고 그래프 신경망을 훈련하여 이러한 개념을 식별.

Result: 신장암 생존 분석에서 이 방법의 효과를 입증하고, 저위험과 고위험 환자를 식별하는 과정에서 설명 가능성과 공정성을 강조함.

Conclusion: 개발한 방법은 신장암의 생존 분석에서 효과적이며, 환자의 위험 수준을 공정하게 평가할 수 있는 잠재력을 보여줌.

Abstract: To evaluate the translational capabilities of foundation models, we develop a
pathological concept learning approach focused on kidney cancer. By leveraging
TNM staging guidelines and pathology reports, we build comprehensive
pathological concepts for kidney cancer. Then, we extract deep features from
whole slide images using foundation models, construct pathological graphs to
capture spatial correlations, and trained graph neural networks to identify
these concepts. Finally, we demonstrate the effectiveness of this approach in
kidney cancer survival analysis, highlighting its explainability and fairness
in identifying low- and high-risk patients. The source code has been released
by https://github.com/shangqigao/RadioPath.

</details>


### [51] [Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology](https://arxiv.org/abs/2509.25559)
*Suvrankar Datta,Divya Buchireddygari,Lakshmi Vennela Chowdary Kaza,Mrudula Bhalke,Kautik Singh,Ayush Pandey,Sonit Sai Vasipalli,Upasana Karnwal,Hakikat Bir Singh Bhatti,Bhavya Ratan Maroo,Sanjana Hebbar,Rahul Joseph,Gurkawal Kaur,Devyani Singh,Akhil V,Dheeksha Devasya Shama Prasad,Nishtha Mahajan,Ayinaparthi Arisha,Rajesh Vanagundi,Reet Nandy,Kartik Vuthoo,Snigdhaa Rajvanshi,Nikhileswar Kondaveeti,Suyash Gunjal,Rishabh Jain,Rajat Jain,Anurag Agrawal*

Main category: cs.AI

TL;DR: 일반ist 멀티모달 AI 시스템이 의료 이미지 해석을 위해 임상 전문가와 환자에게 점점 더 널리 사용되고 있지만, 어려운 진단 사례에 대한 강력한 평가가 부족하다. 본 연구에서 우리는 고급 AI 모델 성능을 평가하기 위해 50개의 전문가 수준의 'spot diagnosis' 사례의 파일럿 벤치를 개발하였다. 이 연구는 의료 영상의 현재 일반ist AI 한계를 강조하고 감독 없는 임상 사용에 대한 경고를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 의료 영상 해석을 위한 AI 모델의 성능을 보다 엄격하게 평가하고, 일반ist AI의 현재 한계를 분명히 할 필요성이 있다.

Method: 50개의 전문가 수준의 'spot diagnosis' 사례를 포함하는 파일럿 벤치를 개발하여 다양한 이미지 모달리티에서 AI 모델의 성능을 평가하였다. 성능은 공개 웹 인터페이스를 통해 다섯 가지 고급 AI 모델로 테스트하였으며, 진단 정확도는 블라인드 전문가에 의해 평가되었다.

Result: 최고의 진단 정확도 (83%)는 보드 인증을 받은 방사선 전문의가 달성하였으며, AI 모델 중 가장 높은 성능은 GPT-5로 30%였다. GPT-5와 o3는 신뢰성이 높았으며, Claude Opus 4.1은 저조한 성능을 보였다.

Conclusion: 고급 frontier 모델들이 어려운 진단 사례에서 방사선 전문의에 비해 훨씬 뒤떨어진다는 것을 보여준다. 이 연구는 AI 모델의 시각적 추론 오류에 대한 질적 분석을 제공하고, 그 실패 모드를 더 잘 이해하기 위한 실용적인 분류법을 제안한다.

Abstract: Generalist multimodal AI systems such as large language models (LLMs) and
vision language models (VLMs) are increasingly accessed by clinicians and
patients alike for medical image interpretation through widely available
consumer-facing chatbots. Most evaluations claiming expert level performance
are on public datasets containing common pathologies. Rigorous evaluation of
frontier models on difficult diagnostic cases remains limited. We developed a
pilot benchmark of 50 expert-level "spot diagnosis" cases across multiple
imaging modalities to evaluate the performance of frontier AI models against
board-certified radiologists and radiology trainees. To mirror real-world
usage, the reasoning modes of five popular frontier AI models were tested
through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5
Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and
reproducibility was assessed across three independent runs. GPT-5 was
additionally evaluated across various reasoning modes. Reasoning quality errors
were assessed and a taxonomy of visual reasoning errors was defined.
Board-certified radiologists achieved the highest diagnostic accuracy (83%),
outperforming trainees (45%) and all AI models (best performance shown by
GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini
2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate
that advanced frontier models fall far short of radiologists in challenging
diagnostic cases. Our benchmark highlights the present limitations of
generalist AI in medical imaging and cautions against unsupervised clinical
use. We also provide a qualitative analysis of reasoning traces and propose a
practical taxonomy of visual reasoning errors by AI models for better
understanding their failure modes, informing evaluation standards and guiding
more robust model development.

</details>


### [52] [ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning](https://arxiv.org/abs/2509.25586)
*Jihye Choi,Jinsung Yoon,Jiefeng Chen,Somesh Jha,Tomas Pfister*

Main category: cs.AI

TL;DR: ATLAS는 복잡한 제약 인식을 효과적으로 처리하기 위한 다중 에이전트 프레임워크로, 실제 여행 계획 과제에서 탁월한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)은 추론과 도구 사용에서 놀라운 발전을 이루었으나, 복잡한 제약 조건 아래에서 최적의 해결책을 생성하는 데 종종 실패한다. 실제 여행 계획은 이러한 문제를 잘 보여준다.

Method: ATLAS는 동적 제약 관리, 반복적 계획 비판, 적응형 상호 검색을 위한 전용 메커니즘을 통해 제약 인식 계획의 기본 문제를 해결하는 원칙적인 접근 방식을 제시한다.

Result: TravelPlanner 벤치마크에서 최첨단 성능을 보여주며, 가장 좋은 대안에 비해 최종 통과율이 23.3%에서 44.4%로 향상되었다.

Conclusion: ATLAS는 라이브 정보 검색 및 다중 턴 피드백을 포함한 실제 여행 계획 과제에 대해 정량적인 효과를 입증한 최초의 사례로, 84%의 최종 통과율을 달성하여 ReAct(59%)와 단일 에이전트(27%)를 포함한 베이스라인을 크게 초월한다.

Abstract: While Large Language Models (LLMs) have shown remarkable advancements in
reasoning and tool use, they often fail to generate optimal, grounded solutions
under complex constraints. Real-world travel planning exemplifies these
challenges, evaluating agents' abilities to handle constraints that are
explicit, implicit, and even evolving based on interactions with dynamic
environments and user needs. In this paper, we present ATLAS, a general
multi-agent framework designed to effectively handle such complex nature of
constraints awareness in real-world travel planning tasks. ATLAS introduces a
principled approach to address the fundamental challenges of constraint-aware
planning through dedicated mechanisms for dynamic constraint management,
iterative plan critique, and adaptive interleaved search. ATLAS demonstrates
state-of-the-art performance on the TravelPlanner benchmark, improving the
final pass rate from 23.3% to 44.4% over its best alternative. More
importantly, our work is the first to demonstrate quantitative effectiveness on
real-world travel planning tasks with live information search and multi-turn
feedback. In this realistic setting, ATLAS showcases its superior overall
planning performance, achieving an 84% final pass rate which significantly
outperforms baselines including ReAct (59%) and a monolithic agent (27%).

</details>


### [53] [Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent](https://arxiv.org/abs/2509.25593)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 대형 언어 모델이 피드백 인과 퍼지 인지 맵을 텍스트로 매핑하고 이를 다시 재구성하는 설명 가능한 AI 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 피드백 인과 퍼지 인지 맵을 더 잘 이해하고 설명하기 위한 방법을 개발하고자 했다.

Method: 대형 언어 모델을 사용하여 FCM을 텍스트로 변환하고, 이를 통해 FCM을 재구성하는 방식으로, 인코더와 디코더가 결정을 설명한다.

Result: 재구성 과정에서 약한 인과 연결선은 제거되지만 강한 인과 연결선은 보존된다.

Conclusion: 이 시스템은 텍스트를 자연스럽게 만들기 위해 FCM에 대한 세부 정보를 일부 손실시키면서도 강한 인과 연결선을 유지한다.

Abstract: A large language model (LLM) can map a feedback causal fuzzy cognitive map
(FCM) into text and then reconstruct the FCM from the text. This explainable AI
system approximates an identity map from the FCM to itself and resembles the
operation of an autoencoder (AE). Both the encoder and the decoder explain
their decisions in contrast to black-box AEs. Humans can read and interpret the
encoded text in contrast to the hidden variables and synaptic webs in AEs. The
LLM agent approximates the identity map through a sequence of system
instructions that does not compare the output to the input. The reconstruction
is lossy because it removes weak causal edges or rules while it preserves
strong causal edges. The encoder preserves the strong causal edges even when it
trades off some details about the FCM to make the text sound more natural.

</details>


### [54] [Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks](https://arxiv.org/abs/2509.25598)
*Peiran Xu,Zhuohao Li,Xiaoying Xing,Guannan Zhang,Debiao Li,Kunyu Shi*

Main category: cs.AI

TL;DR: 이 논문은 LLM의 성능 향상을 위한 새로운 보상 모델인 원칙적 과정 보상(PPR)의 도입을 설명하며, PPR이 기존 방법보다 우수한 성능을 나타낸다고 보고합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업을 수행하기 위해 LLM은 외부 도구를 점점 더 많이 활용하고 있으며, 기존의 결과 기반 보상 방식은 긴 경로에 대한 효과성이 제한적입니다.

Method: 원칙적 과정 보상(PPR)이라는 RL 접근 방식을 도입하여 단계별 평가와 결과 검증을 통합하고, 보상 모델을 훈련시켜 과정 평가의 투명성과 신뢰성을 개선합니다.

Result: PPR은 다양한 벤치마크에서 최첨단 성능을 달성하며 탁월한 강건성과 일반화 성능을 보여줍니다.

Conclusion: 이 연구는 PPR의 효과성과 장점을 입증하고, 코드와 모델 컬렉션도 제공하여 연구를 지원합니다.

Abstract: Large Language Models (LLMs) increasingly rely on external tools such as
search engines to solve complex agentic tasks that require reasoning and
external knowledge retrieval. Recently, reinforcement learning with verifiable
rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of
LLMs by rewarding the final answers via outcome rewards. While straightforward
to supervise, outcome rewards only provide sparse signals and delayed feedback,
which limits their effectiveness on long trajectories. Process rewards address
this by evaluating intermediate steps, providing fine-grained supervision and
encouraging grounded problem solving. However, it is notoriously hard to
annotate step-wise labels, especially in non-verifiable process without
"golden" answers. Furthermore, step-wise judgment requires the balance between
local quality with contribution to the final outcome, as optimizing towards
higher process reward may not always align with better final outcomes. To
address the above challenges, we introduce Principle Process Reward (PPR), an
RL approach that unifies principled step-level assessment and outcome
verification. We train a principle-based reward model to improve the
transparency and reliability of process evaluation, and further introduce a
Reward Normalization (ReNorm) strategy to calibrate outcome and process
rewards. Experiment results show that PPR achieves state-of-the-art performance
across a wide range of benchmarks, demonstrating its impressive robustness and
generalization. Our code and model collection is available in this link.

</details>


### [55] [A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments](https://arxiv.org/abs/2509.25609)
*Manuel Cherep,Chengtian Ma,Abigail Xu,Maya Shaked,Pattie Maes,Nikhil Singh*

Main category: cs.AI

TL;DR: 이 논문은 LLM 기반 소프트웨어 에이전트가 인간의 선택에 미치는 영향을 평가하는 새로운 프레임워크인 ABxLab을 소개하고, 이를 통해 에이전트의 결정 편향을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 사람을 위해 만들어진 환경이 LLM(대형 언어 모델) 기반 소프트웨어 에이전트에 의해 운영되고 있으며, 이러한 에이전트가 인간의 결정을 대신하고 있다는 점에서 그들의 결정 방식에 대한 깊은 평가가 필요하다.

Method: ABxLab이라는 프레임워크를 도입하여 웹 기반 쇼핑 환경에서 가격, 평가 및 심리적 자극과 같은 옵션 특성을 조절하여 에이전트의 선택을 체계적으로 조사했다.

Result: 에이전트의 결정은 예측 가능하고 상당히 변화하며, 인지적 제약 없이도 에이전트가 강한 편향성을 보임을 발견했다.

Conclusion: 소비자 선택은 AI 에이전트의 행동 과학 연구를 위한 강력한 테스트베드가 될 수 있으며, 또한 이러한 에이전트가 인간의 편향을 물려받고 확대할 위험이 있음을 나타낸다. 우리는 에이전트의 의사결정을 철저하고 확장 가능하게 평가할 수 있는 공개 벤치마크 프레임워크를 제공한다.

Abstract: Environments built for people are increasingly operated by a new class of
economic actors: LLM-powered software agents making decisions on our behalf.
These decisions range from our purchases to travel plans to medical treatment
selection. Current evaluations of these agents largely focus on task
competence, but we argue for a deeper assessment: how these agents choose when
faced with realistic decisions. We introduce ABxLab, a framework for
systematically probing agentic choice through controlled manipulations of
option attributes and persuasive cues. We apply this to a realistic web-based
shopping environment, where we vary prices, ratings, and psychological nudges,
all of which are factors long known to shape human choice. We find that agent
decisions shift predictably and substantially in response, revealing that
agents are strongly biased choosers even without being subject to the cognitive
constraints that shape human biases. This susceptibility reveals both risk and
opportunity: risk, because agentic consumers may inherit and amplify human
biases; opportunity, because consumer choice provides a powerful testbed for a
behavioral science of AI agents, just as it has for the study of human
behavior. We release our framework as an open benchmark for rigorous, scalable
evaluation of agent decision-making.

</details>


### [56] [SOCK: A Benchmark for Measuring Self-Replication in Large Language Models](https://arxiv.org/abs/2509.25643)
*Justin Chavarria,Rohan Raizada,Justin White,Eyad Alhetairshi*

Main category: cs.AI

TL;DR: SOCK는 인공지능 모델의 자기 복제를 측정하는 벤치마크 CLI를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 이 연구의 목적은 대형 언어 모델의 자기 복제 능력을 평가하기 위한 표준을 설정하는 것이다.

Method: RCL(복제 능력 수준)과 PCL(지속성 능력 수준)의 두 가지 범주로 LLM을 분류하는 시스템을 개발하였다.

Result: 다양한 모델을 평가한 결과, 지속적인 자기 복제 및 다중 에이전트 시스템에 상당한 장애물이 있음을 발견하였다.

Conclusion: 미래의 다중 에이전트 시스템의 위험을 줄일 수 있는 연구 방향을 제안하였다.

Abstract: We introduce SOCK, a benchmark command line interface (CLI) that measures
large language models' (LLMs) ability to self-replicate without human
intervention. In this benchmark, self-replication is defined not only as an
LLM's ability to create a functioning and running copy of itself, but also the
ability for that self-replication to persist and occur across different
computational contexts. Accordingly, we've developed a system to categorize
LLMs based on broad self-replication capabilities in two general classes,
Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL).
Using a five-task suite based on practically manipulable modern CLI utilities
and computer processes, experiments are orchestrated in a controlled
environment with an LLM acting agentically. The performance of the LLM on agent
tasks is then computed to produce an R-score (a quantitative evaluation of
overall self-replication ability) and data used to categorize LLMs into
specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides
the first formalized definitions and benchmark suite for evaluating LLM
self-replication, with the goal of establishing a standard for future research,
to our knowledge; (2) Allows the industry to track the effectiveness of future
multi-agent systems and mitigate potential self-replication threat vectors
within them. The results compiled from evaluating a variety of open-weight and
proprietary frontier models reveal significant obstacles to persistent
self-replication and multi-agent systems, including context retention and
multi-agent decision-making. We propose future research directions to safely
reduce the severity of these obstacles, potentially lowering future risk of
more functional multi-agent systems.

</details>


### [57] [AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation](https://arxiv.org/abs/2509.25651)
*Gihan Panapitiya,Emily Saldanha,Heather Job,Olivia Hess*

Main category: cs.AI

TL;DR: AutoLabs는 자연어 지시를 고속 액체 처리기에 실행 가능한 프로토콜로 변환하는 자가 수정 다중 에이전트 아키텍처를 소개하며, 화학 연구의 자동화를 통해 과학적 발견을 가속화합니다.


<details>
  <summary>Details</summary>
Motivation: 화학 연구의 자동화를 통해 과학적 발견의 속도를 높이는 것이 목표이나, 기본 AI 에이전트의 신뢰성과 성능은 여전히 중요하고 충분히 검토되지 않은 도전 과제입니다.

Method: AutoLabs는 자연어 지시를 처리하여 실험 목표를 전문 에이전트를 위한 이산 작업으로 분해하고, 도구 지원 스토이키오메트릭 계산을 수행하며, 하드웨어 준비 파일을 생성하기 전에 출력을 반복적으로 자가 수정합니다.

Result: 20개의 에이전트 구성에 대한 체계적인 제거 연구를 통해 사고 능력, 아키텍처 설계, 도구 사용 및 자가 수정 메커니즘의 영향을 평가했습니다. 결과적으로 에이전트의 사고 능력이 성공의 가장 중요한 요소로 나타나 복잡한 작업에서 화학 물질의 양에 대한 정량적 오류를 85% 이상 줄였습니다.

Conclusion: 이 연구 결과는 자율 연구소를 위한 강력하고 신뢰할 수 있는 AI 파트너 개발을 위한 명확한 청사진을 제시하며, 모듈 설계, 고급 추론 및 자가 수정의 시너지 효과를 강조하여 과학적 응용에서 성능과 신뢰성을 보장합니다.

Abstract: The automation of chemical research through self-driving laboratories (SDLs)
promises to accelerate scientific discovery, yet the reliability and granular
performance of the underlying AI agents remain critical, under-examined
challenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent
architecture designed to autonomously translate natural-language instructions
into executable protocols for a high-throughput liquid handler. The system
engages users in dialogue, decomposes experimental goals into discrete tasks
for specialized agents, performs tool-assisted stoichiometric calculations, and
iteratively self-corrects its output before generating a hardware-ready file.
We present a comprehensive evaluation framework featuring five benchmark
experiments of increasing complexity, from simple sample preparation to
multi-plate timed syntheses. Through a systematic ablation study of 20 agent
configurations, we assess the impact of reasoning capacity, architectural
design (single- vs. multi-agent), tool use, and self-correction mechanisms. Our
results demonstrate that agent reasoning capacity is the most critical factor
for success, reducing quantitative errors in chemical amounts (nRMSE) by over
85% in complex tasks. When combined with a multi-agent architecture and
iterative self-correction, AutoLabs achieves near-expert procedural accuracy
(F1-score > 0.89) on challenging multi-step syntheses. These findings establish
a clear blueprint for developing robust and trustworthy AI partners for
autonomous laboratories, highlighting the synergistic effects of modular
design, advanced reasoning, and self-correction to ensure both performance and
reliability in high-stakes scientific applications. Code:
https://github.com/pnnl/autolabs

</details>


### [58] [Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks](https://arxiv.org/abs/2509.25652)
*Hailong Zhang,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.AI

TL;DR: IRCAM-AVN은 여러 모달 정보를 융합하고 시퀀스 모델링을 통합한 최적화된 내비게이션 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 정보 전송 중 중복 처리 및 불일치를 줄이기 위한 필요.

Method: IRCAM 모듈 내에서 다중 모달 정보 융합과 시퀀스 모델링을 통합하는 end-to-end 프레임워크를 개발.

Result: IRCAM-AVN을 사용한 지능형 에이전트는 더 나은 내비게이션 성능을 보였다.

Conclusion: 이 방법론적 전환은 모델의 안정성과 일반화 능력을 향상시킨다.

Abstract: Audio-visual navigation represents a significant area of research in which
intelligent agents utilize egocentric visual and auditory perceptions to
identify audio targets. Conventional navigation methodologies typically adopt a
staged modular design, which involves first executing feature fusion, then
utilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally
making decisions through reinforcement learning. While this modular approach
has demonstrated effectiveness, it may also lead to redundant information
processing and inconsistencies in information transmission between the various
modules during the feature fusion and GRU sequence modeling phases. This paper
presents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for
Audiovisual Navigation), an end-to-end framework that integrates multimodal
information fusion and sequence modeling within a unified IRCAM module, thereby
replacing the traditional separate components for fusion and GRU. This
innovative mechanism employs a multi-level residual design that concatenates
initial multimodal sequences with processed information sequences. This
methodological shift progressively optimizes the feature extraction process
while reducing model bias and enhancing the model's stability and
generalization capabilities. Empirical results indicate that intelligent agents
employing the iterative residual cross-attention mechanism exhibit superior
navigation performance.

</details>


### [59] [Landmark-Guided Knowledge for Vision-and-Language Navigation](https://arxiv.org/abs/2509.25655)
*Dongsheng Yang,Meiling Zhu,Yinfeng Yu*

Main category: cs.AI

TL;DR: 본 연구는 시각-언어 내비게이션을 위한 Landmark-Guided Knowledge(전망 기반 지식) 방법을 제안하며, 외부 지식 기반을 도입하여 전통적인 방법의 상식 부족 문제를 해결하고 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 시각-언어 내비게이션은 자연어 지침을 기반으로 낯선 환경에서 자율적으로 탐색해야 하는 과제로, 기존 방법은 복잡한 시나리오에서 지침과 환경 정보를 제대로 일치시키지 못하는 문제가 있다.

Method: Landmark-Guided Knowledge (LGK) 방법을 제안하며, 630,000개의 언어 설명을 포함하는 지식 기반을 구축하고, 지식 매칭을 통해 환경 하위 보기와 지식 기반을 정렬한다. 또한, 지침의 랜드마크 정보를 활용하여 에이전트가 더 관련성 높은 부분에 집중하도록 유도하는 KGL 메커니즘과 언어, 지식, 비전 및 역사 정보를 통합하는 KGDA를 설계한다.

Result: 실험 결과, LGK 방법이 R2R 및 REVERIE 시각-언어 내비게이션 데이터셋에서 기존 최첨단 방법들보다 높은 내비게이션 정확도, 성공률 및 경로 효율성을 보였다.

Conclusion: LGK 방법은 전통적인 시각-언어 내비게이션 방법의 한계를 극복하고 상식 기반의 추론 능력을 강화하여 탁월한 성과를 나타낸다.

Abstract: Vision-and-language navigation is one of the core tasks in embodied
intelligence, requiring an agent to autonomously navigate in an unfamiliar
environment based on natural language instructions. However, existing methods
often fail to match instructions with environmental information in complex
scenarios, one reason being the lack of common-sense reasoning ability. This
paper proposes a vision-and-language navigation method called Landmark-Guided
Knowledge (LGK), which introduces an external knowledge base to assist
navigation, addressing the misjudgment issues caused by insufficient common
sense in traditional methods. Specifically, we first construct a knowledge base
containing 630,000 language descriptions and use knowledge Matching to align
environmental subviews with the knowledge base, extracting relevant descriptive
knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism,
which guides the agent to focus on the most relevant parts of the knowledge by
leveraging landmark information in the instructions, thereby reducing the data
bias that may arise from incorporating external knowledge. Finally, we propose
Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates
language, knowledge, vision, and historical information. Experimental results
demonstrate that the LGK method outperforms existing state-of-the-art methods
on the R2R and REVERIE vision-and-language navigation datasets, particularly in
terms of navigation error, success rate, and path efficiency.

</details>


### [60] [On Explaining Proxy Discrimination and Unfairness in Individual Decisions Made by AI Systems](https://arxiv.org/abs/2509.25662)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: AI 시스템의 불공정성과 설명 가능성 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 고위험 분야에서의 불공정성과 구조적 편향 문제를 해결할 필요가 있다.

Method: 공식 귀납적 설명을 사용하여 AI 결정에서의 대리 차별을 설명하는 새로운 프레임워크를 제안한다.

Result: 독일 신용 데이터 세트의 사례를 통해 이 프레임워크의 실제 적용 가능성을 입증한다.

Conclusion: 발표된 방법은 구조적 편향을 드러내고 공정성을 평가하는 데 기여한다.

Abstract: Artificial intelligence (AI) systems in high-stakes domains raise concerns
about proxy discrimination, unfairness, and explainability. Existing audits
often fail to reveal why unfairness arises, particularly when rooted in
structural bias. We propose a novel framework using formal abductive
explanations to explain proxy discrimination in individual AI decisions.
Leveraging background knowledge, our method identifies which features act as
unjustified proxies for protected attributes, revealing hidden structural
biases. Central to our approach is the concept of aptitude, a task-relevant
property independent of group membership, with a mapping function aligning
individuals of equivalent aptitude across groups to assess fairness
substantively. As a proof of concept, we showcase the framework with examples
taken from the German credit dataset, demonstrating its applicability in
real-world cases.

</details>


### [61] [GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination](https://arxiv.org/abs/2509.25669)
*Xinxi Chen,Tianyang Chen,Lijia Hong*

Main category: cs.AI

TL;DR: 텍스트 기반 객체 로컬라이제이션을 통해 시각 질문 응답(VQA)을 개선하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 비주얼 질문 응답에서 정보 검색의 효과를 극대화하고, 배경 소음을 줄이며, 시각적 및 텍스트 신호 간의 정렬을 개선하고 헛것을 완화하기 위해.

Method: 전체 이미지 대신 질문과 가장 관련된 객체 주위에 바운딩 박스를 생성하여 타겟 이미지 크롭과 집중 검색을 가능하게 함.

Result: RAG 방법을 사용하여 VQA 응답의 정확도가 22.19%에서 25.64%로 증가하였고, Llama-3.2-Vision-11B 기준 점 대비 절대 증가폭은 3.45%포인트에 달함.

Conclusion: 이 방법은 질문 유형에 기반한 저헛것화 방법을 제안하여 헛것 비율을 65.79%에서 13.88%로 효과적으로 줄이고 진실성 지수를 개선함.

Abstract: We propose a method to improve Visual Question Answering (VQA) with
Retrieval-Augmented Generation (RAG) by introducing text-grounded object
localization. Rather than retrieving information based on the entire image, our
approach enables the model to generate a bounding box around the object most
relevant to the question, allowing for targeted image cropping and focused
retrieval. This reduces background noise, improves alignment between visual and
textual cues, and helps mitigate hallucinations. Our RAG method enhances
context-aware VQA responses increased the accuracy from 22.19% to 25.64%, with
an absolute increase of 3.45 percentage points, compared to the baseline
Llama-3.2-Vision-11B agent. We also proposed a de-hallucination method based on
question type which can effectively reduce the hallucination rate from 65.79%
to 13.88% and improves the truthfulness score.

</details>


### [62] [ScheduleMe: Multi-Agent Calendar Assistant](https://arxiv.org/abs/2509.25693)
*Oshadha Wijerathne,Amandi Nimasha,Dushan Fernando,Nisansa de Silva,Srinath Perera*

Main category: cs.AI

TL;DR: LLM의 발전을 통해 자연어 대화를 지원하는 고급 대화형 비서인 ScheduleMe를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자가 자연어로 Google 캘린더 이벤트를 관리할 수 있도록 지원하는 시스템의 필요성.

Method: 그래프 구조의 조정 메커니즘을 사용하여 중앙 감독 에이전트가 전문화된 작업 에이전트를 감독.

Result: 모듈화, 갈등 해결 및 맥락 인식 상호작용을 통해 사용자 명령을 평가하고 모호성을 해결하는 방법을 제시.

Conclusion: 구조적 추론 및 에이전트 협력이 개인 캘린더 비서 도구의 유용성과 유연성을 증대시킬 수 있는 사례를 제공합니다.

Abstract: Recent advancements in LLMs have contributed to the rise of advanced
conversational assistants that can assist with user needs through natural
language conversation. This paper presents a ScheduleMe, a multi-agent calendar
assistant for users to manage google calendar events in natural language. The
system uses a graph-structured coordination mechanism where a central
supervisory agent supervises specialized task agents, allowing modularity,
conflicts resolution, and context-aware interactions to resolve ambiguities and
evaluate user commands. This approach sets an example of how structured
reasoning and agent cooperation might convince operators to increase the
usability and flexibility of personal calendar assistant tools.

</details>


### [63] [Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs](https://arxiv.org/abs/2509.25779)
*Siyu Zhu,Yanbin Jiang,Hejian Sang,Shao Tang,Qingquan Song,Biao He,Rohit Jain,Zhipeng Wang,Alborz Geramifard*

Main category: cs.AI

TL;DR: 본 연구에서는 대형 언어 모델을 활용한 Agentic RL을 	extsc{TravelPlanner} 벤치마크에서 조사하였다. 우리의 접근 방식인 	extsc{Planner-R1}은 180개의 훈련 쿼리로 	extbf{56.9\%}의 최종 통과율을 기록했으며, 이는 GPT-5의 $21.2\\%$ 기준선보다 $2.7	imes$ 향상된 결과로 공적 리더보드에서 가장 강력한 Agentic 성과이다. 핵심 발견은 작은 모델(8B)이 보상 형성에 매우 반응적이라는 것이었다. 밀집 프로세스 수준 신호를 통해 이들은 경쟁력 있는 성과를 달성했으며, 32B 모델보다 $3.5	imes$ 더 계산 효율적이고 $1.5	imes$ 더 메모리 효율적이었다. 더 큰 모델들은 희소 보상에서 더 강한 안정성을 보였지만, 보상 형성으로부터의 상대적 이득은 적고 실행 간 변동성이 더 컸다. 커리큘럼 학습은 유의미한 이점을 제공하지 않았지만, 형성된 보상은 학습 역학을 일관되게 증폭시켰고, 이는 8B 모델이 Agentic RL에 가장 효율적인 설정이 됨을 의미한다. 결정적으로, 이러한 이익은 과적합의 대가를 치르지 않았다. 미세 조정된 모델은 대부분 도메인 외 작업에서 기준 성능을 유지하거나 초과했으며, 여기에는 	extsc{Multi-IF}, 	extsc{NaturalPlan}, 및 $	au$-	extsc{Bench}가 포함된다. 이러한 결과는 보상 형성이 Agentic RL의 확장을 위한 결정적인 레버임을 확립하고, 작은 모델의 경쟁력을 강조하며, 일반화를 희생하지 않고 효율성을 달성할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 사용하여 Agentic RL의 성능을 향상시키기 위함이다.

Method: Agentic RL을 	extsc{TravelPlanner} 벤치마크에서 조사하며, 	extsc{Planner-R1} 접근 방식을 사용하였다.

Result: 최종 통과율 56.9%를 달성하고, GPT-5 기준선보다 2.7배 향상된 성과를 보였으며, 작은 모델이 더 효율적임을 발견하였다.

Conclusion: 보상 형성이 Agentic RL의 확장을 위한 중요한 요소임을 보여주고, 일반화를 유지하면서 효율성을 달성할 수 있음을 시사한다.

Abstract: We investigated Agentic RL with large language models on the
\textsc{TravelPlanner} benchmark. Our approach, \textsc{Planner-R1}, achieved a
\textbf{56.9\%} final-pass rate with only 180 training queries, a $2.7\times$
improvement over GPT-5's $21.2\%$ baseline and the strongest agentic result on
the public leaderboard. A central finding was that smaller models (8B) were
highly responsive to reward shaping: with dense process-level signals, they
reached competitive performance while being $3.5\times$ more compute-efficient
and $1.5\times$ more memory-efficient than 32B models. Larger models were more
robust under sparse rewards but exhibited smaller relative gains from shaping
and higher variance across runs. While curriculum learning offered no
significant benefit, shaped rewards consistently amplified learning dynamics,
making 8B models the most efficient setting for agentic RL. Crucially, these
gains did not come at the cost of overfitting: fine-tuned models mostly
maintained or exceeded baseline performance on out-of-domain tasks, including
\textsc{Multi-IF}, \textsc{NaturalPlan}, and $\tau$-\textsc{Bench}. These
results establish reward shaping as a decisive lever for scaling agentic RL,
highlight the competitive strength of smaller models, and demonstrate that
efficiency can be achieved without sacrificing generalization.

</details>


### [64] [Aging Decline in Basketball Career Trend Prediction Based on Machine Learning and LSTM Model](https://arxiv.org/abs/2509.25858)
*Yi-chen Yao,Jerry Wang,Yi-cheng Lai,Lyn Chao-ling Chen*

Main category: cs.AI

TL;DR: 이 연구는 NBA 선수의 노화가 성과에 미치는 영향을 분석하였다.


<details>
  <summary>Details</summary>
Motivation: NBA 선수의 성과가 나이가 들면서 어떻게 변화하는지를 이해하기 위함이다.

Method: NBA 선수의 경력 트렌드 분류를 위해 K-평균 클러스터링을 사용하는 오토인코더와 각 선수의 성과 예측을 위한 LSTM 딥러닝 기법을 적용하였다.

Result: 이 연구의 방법이 다양한 NBA 경력 트렌드 평가에서 다른 방법보다 일반화 능력이 뛰어난 것으로 확인되었다.

Conclusion: 이 연구는 스포츠 분석 분야의 다양한 스포츠에 적용될 수 있는 가능성을 보여준다.

Abstract: The topic of aging decline on performance of NBA players has been discussed
in this study. The autoencoder with K-means clustering machine learning method
was adopted to career trend classification of NBA players, and the LSTM deep
learning method was adopted in performance prediction of each NBA player. The
dataset was collected from the basketball game data of veteran NBA players. The
contribution of the work performed better than the other methods with
generalization ability for evaluating various types of NBA career trend, and
can be applied in different types of sports in the field of sport analytics.

</details>


### [65] [Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs](https://arxiv.org/abs/2509.25873)
*Hankun Dai,Maoquan Wang,Mengnan Qi,Yikai Zhang,Zijian Jin,Yongqiang Yao,Yufan Huang,Shengyu Fu,Elsie Nallipogu*

Main category: cs.AI

TL;DR: Lita는 수작업 설계를 최소화하면서 완전 자율 에이전트의 필수 요소를 유지하는 원칙인 'liteness'를 적용하여 프로그래밍 작업에 대한 형태의 성능을 향상시키는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재 코드 에이전트 설계는 복잡하고 수작업으로 제작된 워크플로우와 도구 세트에 의존하며, 이는 성능과 유지 관리 비용의 문제를 발생시킵니다.

Method: Lita는 수작업 설계를 최소화하여 완전 자율 에이전트의 필수 요소를 유지하는 원칙인 'liteness'를 구현합니다.

Result: Lita는 Aider Polyglot 및 SWE-Bench에서 워크플로우 기반 및 에이전시 기준과 비교하여 경쟁력 있는 성능을 달성하며, 적은 토큰 소모와 디자인 노력을 요구합니다.

Conclusion: Lita는 현대 LLM의 근본적인 코딩 능력을 드러내기에 충분하다고 보이며, 에이전트의 복잡성과 성능 간의 간극이 핵심 모델의 개선에 따라 감소할 것이라고 제안합니다.

Abstract: Large language models (LLMs) are increasingly being applied to programming
tasks, ranging from single-turn code completion to autonomous agents. Current
code agent designs frequently depend on complex, hand-crafted workflows and
tool sets. However, this reliance on elaborate scaffolding presents several
challenges: agent performance becomes overly dependent on prompt tuning and
custom design choices, heavy human intervention obscures a model's true
underlying capabilities, and intricate pipelines are costly to build and
maintain. Furthermore, optimizing complex task prompts increases the risk of
data leakage. Currently, when introducing new models, LLM providers like OpenAI
and Anthropic often publish benchmark scores to demonstrate their models'
coding proficiency, but keep their proprietary evaluation frameworks
confidential. To address these limitations, we introduce Lita (Lite Agent),
which operationalizes liteness, a principle of minimizing manual design while
retaining the essential elements of a fully autonomous agent. Lita enables a
more faithful and unified evaluation without elaborate scaffolding. Experiments
on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita
achieves competitive or superior performance compared to workflow-based and
agentic baselines. Crucially, Lita also consumes fewer tokens and requires
significantly less design effort. Our results suggest that Lita is sufficient
to reveal the underlying coding competence of modern LLMs. Finally, we propose
the Agent Complexity Law: the performance gap between agents of varying
complexity, from simple to sophisticated designs, will shrink as the core model
improves, ultimately converging to a negligible difference.

</details>


### [66] [SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents](https://arxiv.org/abs/2509.25885)
*Ruolin Chen,Yinqian Sun,Jihang Wang,Mingyang Lv,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: 이 논문에서는 대형 언어 모델(LLM)이 구동하는 체화된 에이전트의 안전 취약점을 분석하고, 안전성을 보장하는 SafeMindBench와 SafeMindAgent를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 체화된 에이전트가 물리적 세계와 직접 상호작용함으로써 안전 취약점에 노출된 문제를 해결하고자 한다.

Method: 위험 모델을 기반으로 하여 SafeMindBench라는 멀티모달 벤치마크를 제시하고, SafeMindAgent라는 모듈식 Planner-Executor 아키텍처를 도입하여 안전 제약 조건을 추론 과정에 통합한다.

Result: SafeMindBench의 실험을 통해 LLM과 일반적으로 사용되는 체화된 에이전트가 여전히 안전과 관련된 실패에 취약함을 발견하였다.

Conclusion: SafeMindBench와 SafeMindAgent는 체화된 LLM 에이전트의 안전 Risiken 체계적 연구 및 완화에 기여할 평가 도구와 실용적인 솔루션을 제공한다.

Abstract: Embodied agents powered by large language models (LLMs) inherit advanced
planning capabilities; however, their direct interaction with the physical
world exposes them to safety vulnerabilities. In this work, we identify four
key reasoning stages where hazards may arise: Task Understanding, Environment
Perception, High-Level Plan Generation, and Low-Level Action Generation. We
further formalize three orthogonal safety constraint types (Factual, Causal,
and Temporal) to systematically characterize potential safety violations.
Building on this risk model, we present SafeMindBench, a multimodal benchmark
with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk,
Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm,
privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal
that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain
susceptible to safety-critical failures. To address this challenge, we
introduce SafeMindAgent, a modular Planner-Executor architecture integrated
with three cascaded safety modules, which incorporate safety constraints into
the reasoning process. Results show that SafeMindAgent significantly improves
safety rate over strong baselines while maintaining comparable task completion.
Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation
suite and a practical solution that advance the systematic study and mitigation
of safety risks in embodied LLM agents.

</details>


### [67] [NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving](https://arxiv.org/abs/2509.25944)
*Yuan Gao,Mattia Piccinini,Roberto Brusnicki,Yuchen Zhang,Johannes Betz*

Main category: cs.AI

TL;DR: 자율주행의 위험 이해는 높은 수준의 추론을 요구하며, 기존의 모델들이 시간에 따른 위험 진화를 포착하는 데 부족함을 보였다. NuRisk는 현실 데이터를 기반으로 한 새로운 VQA 데이터셋으로, 기존 모델들의 안전성 지도력을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 자율주행 시스템의 위험 이해는 단순한 인식과 예측을 넘어, 행위자 행동 및 맥락에 대한 고차원적 추론이 필요하다.

Method: NuRisk는 2,900 시나리오와 1.1 백만 개의 행위자 수준 샘플로 구성된 VQA 데이터셋으로, BEV 기반의 순차적 이미지와 정량적 위험 주석을 제공한다.

Result: 잘 알려진 VLM에서 명시적 시공간 추론을 수행하지 못해 최고 정확도가 33%에 그쳤으나, 미세 조정된 7B VLM 행위자가 41%의 정확도를 달성하고 대기 시간을 75% 줄였다.

Conclusion: NuRisk는 자율주행의 시공간 추론을 발전시키기 위한 중요한 벤치마크로 자리잡고 있다.

Abstract: Understanding risk in autonomous driving requires not only perception and
prediction, but also high-level reasoning about agent behavior and context.
Current Vision Language Models (VLMs)-based methods primarily ground agents in
static images and provide qualitative judgments, lacking the spatio-temporal
reasoning needed to capture how risks evolve over time. To address this gap, we
propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset
comprising 2,900 scenarios and 1.1 million agent-level samples, built on
real-world data from nuScenes and Waymo, supplemented with safety-critical
scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View
(BEV) based sequential images with quantitative, agent-level risk annotations,
enabling spatio-temporal reasoning. We benchmark well-known VLMs across
different prompting techniques and find that they fail to perform explicit
spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency.
To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to
41% and reduces latency by 75%, demonstrating explicit spatio-temporal
reasoning capabilities that proprietary models lacked. While this represents a
significant step forward, the modest accuracy underscores the profound
challenge of the task, establishing NuRisk as a critical benchmark for
advancing spatio-temporal reasoning in autonomous driving.

</details>


### [68] [Automated Model Discovery via Multi-modal & Multi-step Pipeline](https://arxiv.org/abs/2509.25946)
*Lee Jung-Mok,Nam Hyeon-Woo,Moon Ye-Bin,Junhyun Nam,Tae-Hyun Oh*

Main category: cs.AI

TL;DR: 이 논문은 효과적인 자동 모델 발견을 위한 다중 모드 및 다단계 파이프라인을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존 접근 방식은 훈련 데이터 영역을 넘어 일반성을 보장하면서 세밀한 세부 사항을 포착하는 데 어려움을 겪는다.

Method: 제안된 방법은 AnalyzerVLM과 EvaluatorVLM 두 개의 비전-언어 기반 모듈을 활용하여 효과적인 모델 제안 및 평가를 수행한다.

Result: 우리의 결과는 파이프라인이 세밀한 세부 사항을 포착하고 강한 일반성을 보장하는 모델을 효과적으로 발견함을 보여준다.

Conclusion: 다양한 모드와 다단계 추론이 바람직한 모델을 발견하는 데 중요한 역할을 한다는 것을 나타낸다.

Abstract: Automated model discovery is the process of automatically searching and
identifying the most appropriate model for a given dataset over a large
combinatorial search space. Existing approaches, however, often face challenges
in balancing the capture of fine-grained details with ensuring generalizability
beyond training data regimes with a reasonable model complexity. In this paper,
we present a multi-modal \& multi-step pipeline for effective automated model
discovery. Our approach leverages two vision-language-based modules (VLM),
AnalyzerVLM and EvaluatorVLM, for effective model proposal and evaluation in an
agentic way. AnalyzerVLM autonomously plans and executes multi-step analyses to
propose effective candidate models. EvaluatorVLM assesses the candidate models
both quantitatively and perceptually, regarding the fitness for local details
and the generalibility for overall trends. Our results demonstrate that our
pipeline effectively discovers models that capture fine details and ensure
strong generalizability. Additionally, extensive ablation studies show that
both multi-modality and multi-step reasoning play crucial roles in discovering
favorable models.

</details>


### [69] [RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning](https://arxiv.org/abs/2509.25958)
*Gang Li,Yulei Qin,Xiaoyu Tan,Dingkang Yang,Yuchen Shi,Zihan Xu,Xiang Li,Xing Sun,Ke Li*

Main category: cs.AI

TL;DR: RoRecomp는 훈련 데이터를 전략적으로 재구성하여 간결한 추론을 유도하는 방식으로, RLVR의 비효율성을 개선하는 새로운 방법이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 RLVR 훈련 방식이 비효율적인 탐색 경로와 과도한 장황성을 초래하며, 이는 최적화 신호의 노이즈를 증가시킨다.

Method: RoRecomp는 훈련 데이터를 두 가지 배치 유형으로 나누어, 데이터의 명확한 신호를 제공하도록 설계된 방법이다: 1) 우선 배치, 2) 보상 배치.

Result: RoRecomp는 3가지 설정에서 실험을 통해 효율성 향상을 입증했고, 제로 RL 훈련에서 추론 길이를 27.7% 줄이고, 에이전틱 RL에서 불필요한 도구 호출을 46.8% 줄이며 정확도를 향상시켰으며, 사고 압축에서 길이를 최대 52.5% 줄이는 성과를 보였다.

Conclusion: RoRecomp는 모델의 성능에 최소한의 영향을 미치면서 효율성을 상당히 향상시키는 것으로 나타났다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in
eliciting complex reasoning in large language models (LLMs). However, standard
RLVR training often leads to excessively verbose processes (in reasoning tasks)
and inefficient exploration trajectories (in agentic settings), as outcome-only
rewards provide no incentive for efficiency and the high variance in response
length within relatively small rollout groups results in noisy optimization
signals. To address this, we propose Rollout Response Recomposition (RoRecomp),
a plug-and-play method that guides models toward concise reasoning by
strategically recomposing the training data. RoRecomp separates responses into
two distinct batch types: 1) priority batches, which combine short-correct and
long-incorrect responses selected from online batches to provide a clear
gradient signal for brevity, and 2) compensation batches, which utilize
remaining responses from a replay buffer to maintain stability and prevent
model collapse. To comprehensively evaluate effectiveness, we test RoRecomp
across three settings where results demonstrate substantial efficiency gains:
reducing reasoning length by 27.7% in zero RL training, reducing unnecessary
tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to
52.5% length reduction in thinking compression, all with minimal performance
impact.

</details>


### [70] [Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions](https://arxiv.org/abs/2509.25973)
*Junbeom Kim,Kyuyoung Kim,Jihoon Tack,Dongha Lim,Jinwoo Shin*

Main category: cs.AI

TL;DR: CURE는 민감한 정보 유출을 방지하기 위한 새로운 비학습 프레임워크로, 모델 출력 검증 및 안전한 응답으로 수정합니다.


<details>
  <summary>Details</summary>
Motivation: 웹 규모의 데이터로 훈련된 언어 모델은 민감한 정보를 암기하고 노출할 위험이 있어 효과적인 비학습이 필요합니다.

Method: CURE는 경량 교정기를 사용하여 모델 출력이 목표 지식을 포함하는지 검증하고, 유출이 감지되면 이를 수정합니다. 또한 대규모 비학습 요청을 효율적으로 처리하기 위해 초기 응답과 관련된 비학습 대상들을 검색하여 교정기에 제공하여 탐지 및 조건부 수정을 수행합니다.

Result: CURE는 민감한 정보 유출을 상당히 줄이며, 응답 품질과 일반 유용성을 유지합니다.

Conclusion: CURE는 지속적인 비학습 시나리오에서도 강건성을 보여주어 실제 응용에 적합합니다.

Abstract: Language models trained on web-scale corpora risk memorizing and exposing
sensitive information, prompting the need for effective machine unlearning.
Prior methods mainly focus on input queries to suppress sensitive outputs, yet
this often fails to eliminate the underlying knowledge and limits scalability.
To address this, we propose Corrective Unlearning with Retrieved Exclusions
(CURE), a novel unlearning framework that verifies model outputs for leakage
and revises them into safe responses. Specifically, CURE employs a lightweight
corrector that is applied to the original model to verify whether outputs
contain target knowledge and to rewrite them if any leakage is detected. To
efficiently handle large-scale unlearning requests, CURE retrieves unlearning
targets that are relevant to the initial response and provides them as
in-context references to the corrector for detection and conditional revision.
By leveraging this retrieval augmentation, the corrector can adapt to new
unlearning requests without additional training. Extensive evaluations
demonstrate that CURE substantially reduces information leakage, even from
indirect queries where prior works fall short, while maintaining response
quality and general utility. Moreover, it demonstrates robustness under
continual unlearning scenarios, making it practical for real-world
applications.

</details>


### [71] [Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research](https://arxiv.org/abs/2509.26080)
*Emma Rose Madden*

Main category: cs.AI

TL;DR: 본 논문은 대형 언어 모델(LLM)의 출력 해석에 주의가 필요하며, LLM을 확률적 추론의 대체물이 아닌 고용량 패턴 일치기로 사용하는 새로운 관점을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 사회과학에서 보편적으로 사용되고 있으며, 이로 인한 출력 해석에 주의가 필요하다.

Method: LLM을 확률적 추론의 대용물로 사용하는 것이 아니라, 명시적 범위 조건 하에서 준예측 보간을 위한 고용량 패턴 일치기로 활용하는 새로운 프레임을 제안한다.

Result: 연구자들이 유용한 프로토타입 제작 및 예측을 할 수 있도록 독립적 표본, 사전 등록된 인간 기준선, 신뢰성 인식 검증 및 하위 그룹 보정과 같은 실용적인 안전 장치를 도입했다.

Conclusion: 이 논문은 대형 언어 모델의 출력 해석에 있어 신중함을 요구하고, 사회과학에서 LLM의 사용 방안에 대한 새로운 시각을 제시한다.

Abstract: Large Language Models (LLMs) are being increasingly used as synthetic agents
in social science, in applications ranging from augmenting survey responses to
powering multi-agent simulations. Because strong prediction plus conditioning
prompts, token log-probs, and repeated sampling mimic Bayesian workflows, their
outputs can be misinterpreted as posterior-like evidence from a coherent model.
However, prediction does not equate to probabilism, and accurate points do not
imply calibrated uncertainty. This paper outlines cautions that should be taken
when interpreting LLM outputs and proposes a pragmatic reframing for the social
sciences in which LLMs are used as high-capacity pattern matchers for
quasi-predictive interpolation under explicit scope conditions and not as
substitutes for probabilistic inference. Practical guardrails such as
independent draws, preregistered human baselines, reliability-aware validation,
and subgroup calibration, are introduced so that researchers may engage in
useful prototyping and forecasting while avoiding category errors.

</details>


### [72] [SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs](https://arxiv.org/abs/2509.26100)
*Yixu Wang,Xin Wang,Yang Yao,Xinyuan Li,Yan Teng,Xingjun Ma,Yingchun Wang*

Main category: cs.AI

TL;DR: 이 논문은 동적 안전성을 평가하기 위한 새로운 접근 방식을 제안하며, SafeEvalAgent라는 다중 에이전트 프레임워크를 통해 지속적으로 진화하는 안전 기준을 생성한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 빠른 통합으로 인해 안정성 및 준수 평가의 필요성이 증가하고 있으나 기존의 정적 벤치마크는 동적인 AI 위험과 변화하는 규제를 충분히 다루지 못하고 있다.

Method: 이 논문에서는 Agentic 안전 평가의 새로운 패러다임을 소개하고, SafeEvalAgent라는 다중 에이전트 프레임워크를 제안하며, 비구조적 정책 문서를 자율적으로 수집하여 포괄적인 안전 기준을 지속적으로 발전시키는 방법을 제시한다.

Result: SafeEvalAgent의 실험 결과는 평가가 강화될수록 모델 안전성이 일관되게 감소하는 것을 보여준다. 예를 들어, GPT-5의 EU AI 법안에 대한 안전 비율은 반복을 거치면서 72.50%에서 36.36%로 떨어진다.

Conclusion: 이 결과는 정적 평가의 한계를 드러내고 전통적인 방법들이 놓쳤던 깊은 취약성을 발견할 수 있는 우리의 프레임워크의 능력을 강조하며, 고급 AI의 안전하고 책임 있는 배치를 보장하기 위한 동적 평가 생태계의 긴급한 필요성을 강조한다.

Abstract: The rapid integration of Large Language Models (LLMs) into high-stakes
domains necessitates reliable safety and compliance evaluation. However,
existing static benchmarks are ill-equipped to address the dynamic nature of AI
risks and evolving regulations, creating a critical safety gap. This paper
introduces a new paradigm of agentic safety evaluation, reframing evaluation as
a continuous and self-evolving process rather than a one-time audit. We then
propose a novel multi-agent framework SafeEvalAgent, which autonomously ingests
unstructured policy documents to generate and perpetually evolve a
comprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline
of specialized agents and incorporates a Self-evolving Evaluation loop, where
the system learns from evaluation results to craft progressively more
sophisticated and targeted test cases. Our experiments demonstrate the
effectiveness of SafeEvalAgent, showing a consistent decline in model safety as
the evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act
drops from 72.50% to 36.36% over successive iterations. These findings reveal
the limitations of static assessments and highlight our framework's ability to
uncover deep vulnerabilities missed by traditional methods, underscoring the
urgent need for dynamic evaluation ecosystems to ensure the safe and
responsible deployment of advanced AI.

</details>


### [73] [Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical Practice](https://arxiv.org/abs/2509.26153)
*Jack Gallifant,Katherine C. Kellogg,Matt Butler,Amanda Centi,Shan Chen,Patrick F. Doyle,Sayon Dutta,Joyce Guo,Matthew J. Hadfield,Esther H. Kim,David E. Kozono,Hugo JWL Aerts,Adam B. Landman,Raymond H. Mak,Rebecca G. Mishuris,Tanna L. Nelson,Guergana K. Savova,Elad Sharon,Benjamin C. Silverman,Umit Topaloglu,Jeremy L. Warner,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 대규모 언어 모델(LLMs)을 활용한 의료 혁신의 잠재력에도 불구하고, 실제 임상 환경에서는 구현이 어려운 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 의료 분야에서 LLMs의 잠재력을 실현하기 위한 실용적인 가이드라인 제공.

Method: 전문가 인터뷰와 실제 시스템 배포 경험을 바탕으로 한 필드 매뉴얼 작성.

Result: 임상 AI 개발에서 20% 미만이 모델 개발에, 80% 이상이 구현 작업에 소요됨을 분석.

Conclusion: 임상 치료에 AI를 성공적으로 적용하기 위해 필수적인 인프라와 구현 작업에 초점을 맞춘다.

Abstract: Large language models (LLMs) integrated into agent-driven workflows hold
immense promise for healthcare, yet a significant gap exists between their
potential and practical implementation within clinical settings. To address
this, we present a practitioner-oriented field manual for deploying generative
agents that use electronic health record (EHR) data. This guide is informed by
our experience deploying the "irAE-Agent", an automated system to detect
immune-related adverse events from clinical notes at Mass General Brigham, and
by structured interviews with 20 clinicians, engineers, and informatics leaders
involved in the project. Our analysis reveals a critical misalignment in
clinical AI development: less than 20% of our effort was dedicated to prompt
engineering and model development, while over 80% was consumed by the
sociotechnical work of implementation. We distill this effort into five "heavy
lifts": data integration, model validation, ensuring economic value, managing
system drift, and governance. By providing actionable solutions for each of
these challenges, this field manual shifts the focus from algorithmic
development to the essential infrastructure and implementation work required to
bridge the "valley of death" and successfully translate generative AI from
pilot projects into routine clinical care.

</details>


### [74] [90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development](https://arxiv.org/abs/2509.26161)
*Runxin Yang,Yuxuan Wan,Shuqing Li,Michael R. Lyu*

Main category: cs.AI

TL;DR: 이 논문은 자연어 요구 사항으로부터 실행 가능한 3D 게임을 제로 코딩으로 개발하는 UniGen이라는 다중 에이전트 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 3D 게임 개발은 프로그래밍, 3D 모델링 및 엔진 구성 등 여러 분야의 전문 지식이 필요하여 잠재적인 제작자 수를 제한합니다.

Method: UniGen은 사용자의 요구 사항을 구조화된 청사진과 논리 설명으로 해석하는 Planning Agent, 실행 가능한 C# 스크립트를 생성하는 Generation Agent, 엔진별 구성 요소 바인딩 및 장면 구성을 처리하는 Automation Agent, 실시간 오류 수정을 제공하는 Debugging Agent 등으로 구성됩니다.

Result: UniGen은 사용자가 코딩을 전혀 필요로 하지 않으면서 게임 제작을 민주화하고 개발 시간을 91.4% 단축합니다.

Conclusion: UniGen는 실행 가능한 게임 프로젝트로의 출력 변환에서 여전히 중요한 격차를 해소하며, GitHub와 YouTube에서 공개됩니다.

Abstract: Developing 3D games requires specialized expertise across multiple domains,
including programming, 3D modeling, and engine configuration, which limits
access to millions of potential creators. Recently, researchers have begun to
explore automated game development. However, existing approaches face three
primary challenges: (1) limited scope to 2D content generation or isolated code
snippets; (2) requirement for manual integration of generated components into
game engines; and (3) poor performance on handling interactive game logic and
state management. While Multimodal Large Language Models (MLLMs) demonstrate
potential capabilities to ease the game generation task, a critical gap still
remains in translating these outputs into production-ready, executable game
projects based on game engines such as Unity and Unreal Engine.
  To bridge the gap, this paper introduces UniGen, the first end-to-end
coordinated multi-agent framework that automates zero-coding development of
runnable 3D games from natural language requirements. Specifically, UniGen uses
a Planning Agent that interprets user requirements into structured blueprints
and engineered logic descriptions; after which a Generation Agent produces
executable C# scripts; then an Automation Agent handles engine-specific
component binding and scene construction; and lastly a Debugging Agent provides
real-time error correction through conversational interaction. We evaluated
UniGen on three distinct game prototypes. Results demonstrate that UniGen not
only democratizes game creation by requiring no coding from the user, but also
reduces development time by 91.4%. We release UniGen at
https://github.com/yxwan123/UniGen. A video demonstration is available at
https://www.youtube.com/watch?v=xyJjFfnxUx0.

</details>


### [75] [LLM Agents for Knowledge Discovery in Atomic Layer Processing](https://arxiv.org/abs/2509.26201)
*Andreas Werbrouck,Marshall B. Lindsay,Matthew Maschmann,Matthias J. Young*

Main category: cs.AI

TL;DR: 대형 언어 모델을 사용한 지식 발견 가능성을 시험함.


<details>
  <summary>Details</summary>
Motivation: 자율적 추론 에이전트로서의 대형 언어 모델의 잠재력 검토.

Method: LangGraph의 도구 기능을 재구성하여 에이전트가 블랙박스 기능을 질의하도록 함.

Result: 지식 발견의 과정을 보여주는 어린이 게임을 통해 개념 증명.

Conclusion: LLM 에이전트가 제한된 프로브 기능으로 화학 상호작용을 탐색, 발견, 활용할 수 있음을 입증함.

Abstract: Large Language Models (LLMs) have garnered significant attention for several
years now. Recently, their use as independently reasoning agents has been
proposed. In this work, we test the potential of such agents for knowledge
discovery in materials science. We repurpose LangGraph's tool functionality to
supply agents with a black box function to interrogate. In contrast to process
optimization or performing specific, user-defined tasks, knowledge discovery
consists of freely exploring the system, posing and verifying statements about
the behavior of this black box, with the sole objective of generating and
verifying generalizable statements. We provide proof of concept for this
approach through a children's parlor game, demonstrating the role of
trial-and-error and persistence in knowledge discovery, and the strong
path-dependence of results. We then apply the same strategy to show that LLM
agents can explore, discover, and exploit diverse chemical interactions in an
advanced Atomic Layer Processing reactor simulation using intentionally limited
probe capabilities without explicit instructions.

</details>


### [76] [ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning](https://arxiv.org/abs/2509.26255)
*Yichao Liang,Dat Nguyen,Cambridge Yang,Tianyang Li,Joshua B. Tenenbaum,Carl Edward Rasmussen,Adrian Weller,Zenna Tavares,Tom Silver,Kevin Ellis*

Main category: cs.AI

TL;DR: 이 논문은 장기 계획에서 외부 프로세스와 에이전트의 행동이 동시에 진행됨을 고려한 세계 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 장기 계획 수행 시 에이전트의 행동 뿐만 아니라 외부 프로세스들이 영향을 미치기 때문에 이를 효과적으로 모델링할 필요가 있다.

Method: 상징적 상태 표현과 내재적 및 외재적 메커니즘에 대한 인과 과정을 학습하는 추상적 세계 모델 프레임워크를 제안하며, 변분 베이지안 추론과 LLM 제안을 통해 제한된 데이터로 학습한다.

Result: 다섯 가지 시뮬레이션 테이블탑 로보틱스 환경에서 학습된 모델은 더 많은 객체와 복잡한 목표를 갖춘 새로운 과제에 대한 일반화된 빠른 계획을 가능하게 하여 여러 기준선보다 우수한 성능을 발휘한다.

Conclusion: 제안한 방법은 에이전트의 행동과 외부 프로세스 간의 상호작용을 효과적으로 모델링할 수 있는 가능성을 보여준다.

Abstract: Long-horizon embodied planning is challenging because the world does not only
change through an agent's actions: exogenous processes (e.g., water heating,
dominoes cascading) unfold concurrently with the agent's actions. We propose a
framework for abstract world models that jointly learns (i) symbolic state
representations and (ii) causal processes for both endogenous actions and
exogenous mechanisms. Each causal process models the time course of a
stochastic cause-effect relation. We learn these world models from limited data
via variational Bayesian inference combined with LLM proposals. Across five
simulated tabletop robotics environments, the learned models enable fast
planning that generalizes to held-out tasks with more objects and more complex
goals, outperforming a range of baselines.

</details>


### [77] [Interactive Learning for LLM Reasoning](https://arxiv.org/abs/2509.26306)
*Hehai Lin,Shilei Cao,Minzhi Li,Sudong Wang,Haotian Wu,Linyi Yang,Juepeng Zheng,Chengwei Qin*

Main category: cs.AI

TL;DR: 본 논문에서는 독립적인 문제 해결 능력을 향상시키기 위한 새로운 협동 학습 프레임워크 ILR을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 다중 에이전트 학습 방식은 여러 대형 언어 모델 간의 협력을 촉진하기 위한 훈련 환경을 개발하였으나 인퍼런스 시 최종 솔루션을 얻기 위해 MAS를 다시 실행해야 함.

Method: ILR은 두 가지 핵심 구성 요소인 동적 상호작용과 인식 보정을 통합한 새로운 공동 학습 프레임워크를 사용합니다.

Result: ILR은 세 가지 LLM에서 단일 에이전트 학습보다 최대 5% 성능 향상을 보여 주었으며, Idea3가 다중 에이전트 인퍼런스에서 LLM의 강건성을 향상시킬 수 있음을 발견했습니다.

Conclusion: 다이나믹 상호작용 유형은 순수 협동 또는 경쟁 전략에 비해 다중 에이전트 학습을 촉진할 수 있습니다.

Abstract: Existing multi-agent learning approaches have developed interactive training
environments to explicitly promote collaboration among multiple Large Language
Models (LLMs), thereby constructing stronger multi-agent systems (MAS).
However, during inference, they require re-executing the MAS to obtain final
solutions, which diverges from human cognition that individuals can enhance
their reasoning capabilities through interactions with others and resolve
questions independently in the future. To investigate whether multi-agent
interaction can enhance LLMs' independent problem-solving ability, we introduce
ILR, a novel co-learning framework for MAS that integrates two key components:
Dynamic Interaction and Perception Calibration. Specifically, Dynamic
Interaction first adaptively selects either cooperative or competitive
strategies depending on question difficulty and model ability. LLMs then
exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea
Fusion), an innovative interaction paradigm designed to mimic human discussion,
before deriving their respective final answers. In Perception Calibration, ILR
employs Group Relative Policy Optimization (GRPO) to train LLMs while
integrating one LLM's reward distribution characteristics into another's reward
function, thereby enhancing the cohesion of multi-agent interactions. We
validate ILR on three LLMs across two model families of varying scales,
evaluating performance on five mathematical benchmarks and one coding
benchmark. Experimental results show that ILR consistently outperforms
single-agent learning, yielding an improvement of up to 5% over the strongest
baseline. We further discover that Idea3 can enhance the robustness of stronger
LLMs during multi-agent inference, and dynamic interaction types can boost
multi-agent learning compared to pure cooperative or competitive strategies.

</details>


### [78] [Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents](https://arxiv.org/abs/2509.26354)
*Shuai Shao,Qihan Ren,Chen Qian,Boyi Wei,Dadi Guo,Jingyi Yang,Xinhao Song,Linfeng Zhang,Weinan Zhang,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: 이 논문은 자가 진화 에이전트의 의도하지 않은 방향으로의 진화를 조사하여 새로운 위험을 식별하고, 이를 해결하기 위한 전략을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자가 진화 에이전트가 환경과의 상호작용을 통해 독립적으로 향상되지만, 이러한 자가 진화가 의도치 않은 결과를 초래하고 새로운 위험을 발생시킬 수 있다는 점에 주목하였다.

Method: 자간 진화가 미치는 영향을 평가하기 위해 모델, 메모리, 도구, 워크플로우라는 네 가지 주요 진화 경로를 시스템적으로 조사하였다.

Result: 미세 진화는 상위 LLM(예: Gemini-2.5-Pro)을 기반으로 한 에이전트에도 광범위한 위험으로 나타나며, 안전 정렬의 저하 및 도구 제작 및 재사용 과정에서 의도하지 않은 취약점이 도입되는 등의 다양한 위험이 관찰되었다.

Conclusion: 자가 진화 에이전트에 대한 새로운 안전 패러다임의 필요성을 강조하며, 안전하고 신뢰할 수 있는 자가 진화 에이전트를 구축하기 위한 추가 연구를 유도할 수 있는 완화 전략을 논의하였다.

Abstract: Advances in Large Language Models (LLMs) have enabled a new class of
self-evolving agents that autonomously improve through interaction with the
environment, demonstrating strong capabilities. However, self-evolution also
introduces novel risks overlooked by current safety research. In this work, we
study the case where an agent's self-evolution deviates in unintended ways,
leading to undesirable or even harmful outcomes. We refer to this as
Misevolution. To provide a systematic investigation, we evaluate misevolution
along four key evolutionary pathways: model, memory, tool, and workflow. Our
empirical findings reveal that misevolution is a widespread risk, affecting
agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent
risks are observed in the self-evolutionary process, such as the degradation of
safety alignment after memory accumulation, or the unintended introduction of
vulnerabilities in tool creation and reuse. To our knowledge, this is the first
study to systematically conceptualize misevolution and provide empirical
evidence of its occurrence, highlighting an urgent need for new safety
paradigms for self-evolving agents. Finally, we discuss potential mitigation
strategies to inspire further research on building safer and more trustworthy
self-evolving agents. Our code and data are available at
https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes
examples that may be offensive or harmful in nature.

</details>


### [79] [Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline](https://arxiv.org/abs/2509.26440)
*Naomi Fridman,Anat Goldstein*

Main category: cs.AI

TL;DR: 본 연구는 유방 MRI에서 양성과 악성을 구분하기 위해 Transformer 기반의 자동 분류 프레임워크를 도입하고, 88명의 환자와 133개의 주석이 달린 병변을 포함한 표준화된 데이터셋인 BreastDCEDL_AMBL을 구축하였다.


<details>
  <summary>Details</summary>
Motivation: 유방 자기공명영상(MRI)은 암 검출과 치료 계획에 중요한 도구이나, 낮은 특이성으로 인해 높은 위양성율과 불필요한 생검이 발생하는 문제를 해결하고자 하였다.

Method: 본 연구에서는 SegFormer 아키텍처를 구현하여, 병변 수준 분류에서 AUC 0.92를 달성하였고, 환자 수준에서 100%의 민감도와 67%의 특이성을 보였다.

Result: 모델은 악성 픽셀 분포를 정량화할 수 있으며, 해석 가능한 공간 예측을 생산하여 임상 의사 결정을 지원한다. 또한, BreastDCEDL_AMBL을 구축하여 암 이미징 아카이브의 AMBL 컬렉션을 변환하였다.

Conclusion: 데이터셋, 모델 및 평가 프로토콜의 공개는 DCE-MRI 병변 분류를 위한 최초의 표준화된 벤치마크를 제공하며, 임상 배치를 향한 방법론적 발전을 가능하게 한다.

Abstract: The error is caused by special characters that arXiv's system doesn't
recognize. Here's the cleaned version with all problematic characters replaced:
Breast magnetic resonance imaging is a critical tool for cancer detection and
treatment planning, but its clinical utility is hindered by poor specificity,
leading to high false-positive rates and unnecessary biopsies. This study
introduces a transformer-based framework for automated classification of breast
lesions in dynamic contrast-enhanced MRI, addressing the challenge of
distinguishing benign from malignant findings. We implemented a SegFormer
architecture that achieved an AUC of 0.92 for lesion-level classification, with
100% sensitivity and 67% specificity at the patient level - potentially
eliminating one-third of unnecessary biopsies without missing malignancies. The
model quantifies malignant pixel distribution via semantic segmentation,
producing interpretable spatial predictions that support clinical
decision-making. To establish reproducible benchmarks, we curated
BreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection
into a standardized deep learning dataset with 88 patients and 133 annotated
lesions (89 benign, 44 malignant). This resource addresses a key infrastructure
gap, as existing public datasets lack benign lesion annotations, limiting
benign-malignant classification research. Training incorporated an expanded
cohort of over 1,200 patients through integration with BreastDCEDL datasets,
validating transfer learning approaches despite primary tumor-only annotations.
Public release of the dataset, models, and evaluation protocols provides the
first standardized benchmark for DCE-MRI lesion classification, enabling
methodological advancement toward clinical deployment.

</details>


### [80] [Combining Knowledge Graphs and NLP to Analyze Instant Messaging Data in Criminal Investigations](https://arxiv.org/abs/2509.26487)
*Riccardo Pozzi,Valentina Barbera,Renzo Alva Principe,Davide Giardini,Riccardo Rubini,Matteo Palmonari*

Main category: cs.AI

TL;DR: 이 연구는 범죄 수사에서 인스턴트 메시징 앱의 메시지 분석을 지원하기 위해 지식 그래프와 자연어 처리 모델을 통합하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 범죄 수사에서 빠르게 많은 양의 메시지를 분석해야 하는 필요성이 큽니다.

Method: 지식 그래프 모델링, 음성 메시지의 전사 생성, 엔티티 추출 접근법을 사용하여 데이터를 주석 처리하는 과정으로 세분화된 분석 방법을 사용합니다.

Result: 조사 데이터와의 실제 응용 사례에서 초기 결과와 프로토타입을 보고합니다.

Conclusion: 프로세스를 통해 검찰과의 긴밀한 상호작용을 진행하여 긍정적인 피드백과 연구 방향을 도출했습니다.

Abstract: Criminal investigations often involve the analysis of messages exchanged
through instant messaging apps such as WhatsApp, which can be an extremely
effort-consuming task. Our approach integrates knowledge graphs and NLP models
to support this analysis by semantically enriching data collected from
suspects' mobile phones, and help prosecutors and investigators search into the
data and get valuable insights. Our semantic enrichment process involves
extracting message data and modeling it using a knowledge graph, generating
transcriptions of voice messages, and annotating the data using an end-to-end
entity extraction approach. We adopt two different solutions to help users get
insights into the data, one based on querying and visualizing the graph, and
one based on semantic search. The proposed approach ensures that users can
verify the information by accessing the original data. While we report about
early results and prototypes developed in the context of an ongoing project,
our proposal has undergone practical applications with real investigation data.
As a consequence, we had the chance to interact closely with prosecutors,
collecting positive feedback but also identifying interesting opportunities as
well as promising research directions to share with the research community.

</details>


### [81] [OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!](https://arxiv.org/abs/2509.26495)
*Jingdi Lei,Varun Gumma,Rishabh Bhardwaj,Seok Min Lim,Chuan Li,Amir Zadeh,Soujanya Poria*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)의 안전성 문제를 다루고, 특정 용도에 적합한 사용이 가능한지 평가하기 위한 operational safety와 이를 측정할 OffTopicEval을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 대형 언어 모델의 안전한 배포를 위한 필수 요건으로, 사용자에게 해를 끼치지 않도록 하는 안전성에 대한 관심이 필요하다.

Method: LLM의 operational safety를 측정하기 위해 OffTopicEval이라는 평가 세트를 제안하고, 20개의 공개 가중치 LLM으로 구성된 6개의 모델 패밀리에 대한 평가를 수행한다.

Result: 모델 간 성능 차이를 확인했으나, 모든 모델이 높은 operational 안전성을 가지지 못함을 발견했다. 특히, Qwen-3와 Mistral 모델이 상대적으로 높은 성능을 보였지만 여전히 안전 기준에 미치지 못했다.

Conclusion: operational 안전성 문제 해결을 위한 프롬프트 기반 유도 방법인 Q-ground와 P-ground를 제안하고, 이는 OOD 거부를 개선하며 LLM 기반 에이전트의 신뢰성 향상에 기여할 것으로 보인다.

Abstract: Large Language Model (LLM) safety is one of the most pressing challenges for
enabling wide-scale deployment. While most studies and global discussions focus
on generic harms, such as models assisting users in harming themselves or
others, enterprises face a more fundamental concern: whether LLM-based agents
are safe for their intended use case. To address this, we introduce operational
safety, defined as an LLM's ability to appropriately accept or refuse user
queries when tasked with a specific purpose. We further propose OffTopicEval,
an evaluation suite and benchmark for measuring operational safety both in
general and within specific agentic use cases. Our evaluations on six model
families comprising 20 open-weight LLMs reveal that while performance varies
across models, all of them remain highly operationally unsafe. Even the
strongest models -- Qwen-3 (235B) with 77.77\% and Mistral (24B) with 79.96\%
-- fall far short of reliable operational safety, while GPT models plateau in
the 62--73\% range, Phi achieves only mid-level scores (48--70\%), and Gemma
and Llama-3 collapse to 39.53\% and 23.84\%, respectively. While operational
safety is a core model alignment issue, to suppress these failures, we propose
prompt-based steering methods: query grounding (Q-ground) and system-prompt
grounding (P-ground), which substantially improve OOD refusal. Q-ground
provides consistent gains of up to 23\%, while P-ground delivers even larger
boosts, raising Llama-3.3 (70B) by 41\% and Qwen-3 (30B) by 27\%. These results
highlight both the urgent need for operational safety interventions and the
promise of prompt-based steering as a first step toward more reliable LLM-based
agents.

</details>


### [82] [SCUBA: Salesforce Computer Use Benchmark](https://arxiv.org/abs/2509.26506)
*Yutong Dai,Krithika Ramakrishnan,Jing Gu,Matthew Fernandez,Yanqi Luo,Viraj Prabhu,Zhenyu Hu,Silvio Savarese,Caiming Xiong,Zeyuan Chen,Ran Xu*

Main category: cs.AI

TL;DR: SCUBA는 Salesforce 플랫폼에서 CRM 워크플로우를 평가하기 위한 벤치마크로, 300개의 작업 인스턴스를 포함하고 있습니다. 본 연구는 다양한 에이전트 디자인 패러다임을 비교했으며, 태스크 성공률에서 큰 성능 격차가 발견되었습니다.


<details>
  <summary>Details</summary>
Motivation: 컴퓨터 사용 에이전트를 통한 CRM 워크플로우의 효율성을 측정하고 개선하기 위함입니다.

Method: 실제 사용자 인터뷰를 기반으로 한 300개의 작업 인스턴스를 포함하고, Salesforce 샌드박스 환경에서 다양한 에이전트를 평가했습니다.

Result: 개방형 모델과 폐쇄형 모델 간의 성능 차이를 확인했으며, 개방형 모델은 5% 이하의 성공률을 보였고, 폐쇄형 모델은 최대 39%의 성공률을 보였습니다.

Conclusion: SCUBA는 복잡한 비즈니스 소프트웨어 생태계를 위한 신뢰할 수 있는 컴퓨터 사용 에이전트를 구축하는 발전을 가속화하는 데 기여할 것입니다.

Abstract: We introduce SCUBA, a benchmark designed to evaluate computer-use agents on
customer relationship management (CRM) workflows within the Salesforce
platform. SCUBA contains 300 task instances derived from real user interviews,
spanning three primary personas, platform administrators, sales
representatives, and service agents. The tasks test a range of
enterprise-critical abilities, including Enterprise Software UI navigation,
data manipulation, workflow automation, information retrieval, and
troubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox
environments with support for parallel execution and fine-grained evaluation
metrics to capture milestone progress. We benchmark a diverse set of agents
under both zero-shot and demonstration-augmented settings. We observed huge
performance gaps in different agent design paradigms and gaps between the
open-source model and the closed-source model. In the zero-shot setting,
open-source model powered computer-use agents that have strong performance on
related benchmarks like OSWorld only have less than 5\% success rate on SCUBA,
while methods built on closed-source models can still have up to 39% task
success rate. In the demonstration-augmented settings, task success rates can
be improved to 50\% while simultaneously reducing time and costs by 13% and
16%, respectively. These findings highlight both the challenges of enterprise
tasks automation and the promise of agentic solutions. By offering a realistic
benchmark with interpretable evaluation, SCUBA aims to accelerate progress in
building reliable computer-use agents for complex business software ecosystems.

</details>


### [83] [Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework](https://arxiv.org/abs/2509.26534)
*Jovan Stojkovic,Chaojie Zhang,Íñigo Goiri,Ricardo Bianchini*

Main category: cs.AI

TL;DR: 이 연구는 AI 데이터 센터의 전체 수명 주기를 재고하여 운영 소프트웨어 최적화를 통해 TCO를 최대 40% 줄이는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: AI 추론 인프라에 대한 수요가 급증하고 있으며, 클라우드 제공자에게 전체 소유 비용(TCO)이 중요한 문제가 되고 있습니다.

Method: AI 데이터 센터의 수명 주기를 설계, 하드웨어 갱신 및 운영의 세 단계로 나누어 관리하고, 전력, 냉각 및 네트워킹 설계 선택의 영향을 분석합니다.

Result: 전통적인 접근 방식 대비 TCO를 최대 40%까지 줄일 수 있는 포괄적인 수명 주기 관리 프레임워크를 제시합니다.

Conclusion: 이 프레임워크를 통해 AI 데이터 센터 수명 주기를 효과적으로 관리하는 가이드라인을 제공합니다.

Abstract: The rapid rise of large language models (LLMs) has been driving an enormous
demand for AI inference infrastructure, mainly powered by high-end GPUs. While
these accelerators offer immense computational power, they incur high capital
and operational costs due to frequent upgrades, dense power consumption, and
cooling demands, making total cost of ownership (TCO) for AI datacenters a
critical concern for cloud providers. Unfortunately, traditional datacenter
lifecycle management (designed for general-purpose workloads) struggles to keep
pace with AI's fast-evolving models, rising resource needs, and diverse
hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme
across three stages: building, hardware refresh, and operation. We show how
design choices in power, cooling, and networking provisioning impact long-term
TCO. We also explore refresh strategies aligned with hardware trends. Finally,
we use operation software optimizations to reduce cost. While these
optimizations at each stage yield benefits, unlocking the full potential
requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle
management framework that coordinates and co-optimizes decisions across all
three stages, accounting for workload dynamics, hardware evolution, and system
aging. Our system reduces the TCO by up to 40\% over traditional approaches.
Using our framework we provide guidelines on how to manage AI datacenter
lifecycle for the future.

</details>


### [84] [Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning](https://arxiv.org/abs/2509.26605)
*Maël Macuglia,Paul Friedrich,Giorgia Ramponi*

Main category: cs.AI

TL;DR: 본 연구는 안전한 초기 정책을 학습한 후 인간 피드백을 통해 온라인에서 미세 조정하는 두 단계의 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습을 로봇 공학, 산업 및 의료 분야에 적용하는 데 있어 정확한 보상 지정의 어려움과 데이터를 많이 소모하는 탐색의 위험성이 장애물로 작용하고 있습니다.

Method: 전문가 시연의 보상 없는 데이터셋에서 안전한 초기 정책을 학습한 후, 선호 기반 인간 피드백을 사용하여 온라인에서 조정하는 두 단계 프레임워크를 제안합니다. BRIDGE라는 통합 알고리즘을 도입하여 두 신호를 불확실성 가중 목표를 통해 통합합니다.

Result: BRIDGE는 오프라인 시연 수가 증가함에 따라 감소하는 후회 경계를 도출하며, 오프라인 데이터의 양과 온라인 샘플 효율성을 명시적으로 연결합니다. MuJoCo 환경에서 BRIDGE의 성능을 검증하여 단독 행동 복제 및 온라인 선호 기반 RL보다 적은 후회를 달성했습니다.

Conclusion: 이 연구는 보다 샘플 효율적인 인터랙티브 에이전트를 설계하기 위한 이론적 기초를 제공합니다.

Abstract: Deploying reinforcement learning (RL) in robotics, industry, and health care
is blocked by two obstacles: the difficulty of specifying accurate rewards and
the risk of unsafe, data-hungry exploration. We address this by proposing a
two-stage framework that first learns a safe initial policy from a reward-free
dataset of expert demonstrations, then fine-tunes it online using
preference-based human feedback. We provide the first principled analysis of
this offline-to-online approach and introduce BRIDGE, a unified algorithm that
integrates both signals via an uncertainty-weighted objective. We derive regret
bounds that shrink with the number of offline demonstrations, explicitly
connecting the quantity of offline data to online sample efficiency. We
validate BRIDGE in discrete and continuous control MuJoCo environments, showing
it achieves lower regret than both standalone behavioral cloning and online
preference-based RL. Our work establishes a theoretical foundation for
designing more sample-efficient interactive agents.

</details>


### [85] [Branching Out: Broadening AI Measurement and Evaluation with Measurement Trees](https://arxiv.org/abs/2509.26632)
*Craig Greenberg,Patrick Hall,Theodore Jensen,Kristen Greene,Razvan Amironesei*

Main category: cs.AI

TL;DR: 이 논문은 다양한 구성 요소를 해석 가능한 다층 표현으로 결합하는 새로운 메트릭 클래스인 '측정 트리'를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템 평가의 범위를 확장하자는 최근의 요구에 대한 대응으로, 측정 트리는 메트릭의 투명성을 높이고 이질적인 증거를 통합하는 데 도움을 준다.

Method: 측정 트리는 각 노드가 사용자 정의 집계 방법을 통해 자식을 요약하는 계층적 방향 그래프를 생성한다.

Result: 우리는 정의와 예제를 제시하고 대규모 측정 연습을 통해 실제 유용성을 입증하며, 이에 수반하는 오픈소스 파이썬 코드를 제공한다.

Conclusion: 복잡한 구성 요인의 측정에 대한 투명한 접근 방식을 실용화함으로써, 이 논문은 더 넓고 해석 가능한 AI 평가를 위한 원칙적인 기초를 제공한다.

Abstract: This paper introduces \textit{measurement trees}, a novel class of metrics
designed to combine various constructs into an interpretable multi-level
representation of a measurand. Unlike conventional metrics that yield single
values, vectors, surfaces, or categories, measurement trees produce a
hierarchical directed graph in which each node summarizes its children through
user-defined aggregation methods. In response to recent calls to expand the
scope of AI system evaluation, measurement trees enhance metric transparency
and facilitate the integration of heterogeneous evidence, including, e.g.,
agentic, business, energy-efficiency, sociotechnical, or security signals. We
present definitions and examples, demonstrate practical utility through a
large-scale measurement exercise, and provide accompanying open-source Python
code. By operationalizing a transparent approach to measurement of complex
constructs, this work offers a principled foundation for broader and more
interpretable AI evaluation.

</details>
