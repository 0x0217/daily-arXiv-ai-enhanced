<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 66]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.LG](#cs.LG) [Total: 73]
- [cs.CR](#cs.CR) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 이 논문은 의료 및 제약 분야에서의 기계 학습 모델 배포에 대한 벤치마킹 분석을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 모델의 효율적이고 확장 가능한 배포는 현대 생산 환경, 특히 규제가 있는 분야에서 필수적입니다.

Method: FastAPI를 사용한 경량 REST 서비스와 NVIDIA Triton Inference Server라는 고성능 서버 엔진을 비교하여, 의료 AI를 위한 참조 아키텍처를 활용해 Kubernetes에서 DistilBERT 감정 분석 모델을 배포했습니다.

Result: FastAPI는 단일 요청 워크로드에 대해 p50 지연시간이 22ms로 낮은 오버헤드를 제공하는 반면, Triton은 동적 배치를 통해 단일 NVIDIA T4 GPU에서 780 요청/초의 처리량을 달성했습니다.

Conclusion: 하이브리드 아키텍처 접근 방식을 평가하며, 이는 보호 건강 정보 비식별화를 위한 FastAPI와 백엔드 추론을 위한 Triton을 통합합니다. 이 모델은 기업 임상 AI에 대한 최선의 실천 사례로 검증되었습니다.

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [2] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: AFDLD 모델을 통해 제품 가격을 속성 수준의 기여도로 표현하고, ADEPT 알고리즘을 사용하여 동적 시장에서 투명하고 효율적인 가격 책정을 실현한다.


<details>
  <summary>Details</summary>
Motivation: 고차원 시장에서의 동적 가격 책정에서 스케일링, 불확실성 및 해석 가능성의 근본적인 문제를 해결하고자 한다.

Method: AFDLD 모델을 도입하여 속성 수준의 기여도 합으로 제품 가격을 표현하고, ADEPT 알고리즘을 제안하여 속성 공간에서 직접 작동하는 온라인 학습을 구현한다.

Result: ADEPT는 동적 시장 조건에서 근접 최적 가격을 학습하고, 충격 및 드리프트에 빠르게 적응하며, 투명한 속성 수준의 가격 설명을 제공한다.

Conclusion: 구조화된 속성 기반 표현을 통해 자율 가격 책정 에이전트에서 해석 가능성과 효과성을 동시에 달성할 수 있음을 보여준다.

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [3] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 딥러닝 에이전트가 복잡한 게임 도메인에서 높은 성능을 달성할 수 있지만, 게임 메커니즘을 이해하지 못하는 경우가 많다. 이 연구에서는 대형 언어 모델을 사용하여 비디오 게임 설명 언어의 규칙을 역설계하는 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 에이전트가 복잡한 게임 도메인에서 높은 성능을 보이지만, 그 과정에서 게임의 근본적인 인과 관계를 이해하지 못하는 문제를 해결하고자 한다.

Method: 비디오 게임 플레이 흔적을 이용해 VGDL 규칙을 역설계하는 대형 언어 모델을 과제로 설정하고, 관찰 데이터에서 지배 법칙을 유추하는 인과 유도 능력을 조사한다.

Result: SCM 기반 접근 방식이 직접 생성 방식보다 더 정확한 VGDL 설명을 생성하며, 최대 81%의 선호도 승률을 보여주었다.

Conclusion: 학습된 SCM은 인과 강화 학습, 해석 가능한 에이전트, 그리고 논리적으로 일관된 새로운 게임의 절차적 생성과 같은 하위 사용 사례에 활용될 수 있다.

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [4] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA는 동적으로 데이터 처리 파이프라인을 구성하고 최적화하는 메타 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 데이터 처리 파이프라인은 특정 작업에 맞게 정적으로 설계되어 요구 사항 변화에 적응하기 어렵다.

Method: ADP-MA는 계층적 에이전트 조정을 통해 데이터 처리 파이프라인을 동적으로 구성하고 실행하며 반복적으로 개선한다.

Result: ADP-MA는 표준 데이터 처리 작업에 대해 파이프라인을 구성, 실행 모니터링 및 적응적 개선을 보여준다.

Conclusion: 이 프레임워크는 맥락 인식 최적화, 적응형 작업 부하 분할 및 확장성을 위해 점진적 샘플링을 강조한다.

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [5] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLMs)이 훈련 세트에서 실제 환경으로 이동할 때, 고정된 훈련이 지속적인 환경 변화에 발맞추지 못하는 한계를 다루고 있다. 이 문제를 해결하기 위해서는 새로운 진화 축이 필요하며, 이를 위해 A-Evolve라는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)이 훈련 환경에서 실제 환경으로 이동할 때, 정적인 훈련 방식이 지속적인 환경 변화에 적응하지 못하는 근본적인 한계가 존재한다.

Method: 우리는 A-Evolve라는 프레임워크를 통해 배포 시 개선을 의도적인 목표 지향 최적화 과정으로 다룬다.

Result: 우리는 진화 성장이 진화를 위한 계산 자원에 비례하여 적응 능력이 증가하며, 에이전틱 진화가 지속적이고 개방적인 적응을 위한 확장 가능한 경로임을 제안한다.

Conclusion: 우리는 에이전틱 진화가 LLM 적응의 필수적인 미래라고 주장하며, 이것이 고정된 파이프라인에서 자율적인 진화 프로세스로 진화해야 한다고 결론짓는다.

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [6] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: PolarMem은 검증 가능한 증거에 기반한 에이전트 추론을 지원하는 훈련 없는 메모리 시스템으로, 현재 비례 모델의 한계를 극복하여 부정 제약을 명시적으로 저장하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 멀티모달 에이전트가 정보 가용성뿐만 아니라 논리적 검증 가능성을 제공하는 메모리 시스템이 필요하다.

Method: Polarized Latent Graph Memory를 도입하여 비모수적 분포 분할을 통해 모호한 지각 확률을 논리적 제약으로 변환하고, 직교 억제 연결이 있는 양극화된 그래프 토폴로지를 사용하여 검증된 부정을 저장한다.

Result: PolarMem은 8개의 동결된 비전-언어 모델과 6개의 벤치마크에 대한 평가에서 강력한 인지 시스템으로 기능함을 입증한다.

Conclusion: PolarMem은 검증 가능한 멀티모달 에이전트를 위한 기초를 형성한다.

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [7] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR는 멀티 에이전트 토론의 문맥 문제를 해결하기 위해 긴 텍스트 이력을 컴팩트한 이미지 표현으로 대체하여 입력 토큰을 92% 이상 줄이고 계산 비용을 낮추는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 토론이 추론 품질을 개선하고 환각을 줄일 수 있지만, 토론 라운드와 에이전트 수가 증가함에 따라 문맥이 급격히 증가한다.

Method: DebateOCR는 긴 텍스트 토론 이력을 컴팩트한 이미지 표현으로 대체하고, 이를 전용 비전 인코더를 통해 처리하여 이후 라운드를 조정한다.

Result: 이 설계는 이력이 수십만 개의 토큰에 이르는 경우를 압축하여 입력 토큰을 92% 이상 줄여주며, 여러 벤치마크에서 계산 비용을 대폭 낮추고 추론 속도를 빠르게 한다.

Conclusion: 대리인이 다양성을 가질 때 생략된 정보 복구를 지원하는 이론적 관점을 제공하며, 여러 에이전트의 압축된 뷰를 집계하면 정보 병목 현상에 도달할 확률이 기하급수적으로 높아진다.

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [8] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: AI 에이전트가 기업 애플리케이션에 통합됨에 따라, 이들의 평가에는 실제 운영의 복잡성을 반영하는 벤치마크가 필요하다. 기존의 벤치마크는 개방형 도메인에 과도하게 강조하고 정확성 메트릭이 좁으며 진정한 복잡성이 부족하다. 우리는 실제 기업의 도전과제를 포착하기 위해 도메인 전문가와 긴밀하게 협력하여 설계된 전문가 우선의 다회전 보험 언더라이팅 벤치마크인 UNDERWRITE를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 평가에 필요한 벤치마크는 실제 복잡성을 반영해야 한다.

Method: 전문가와의 협력을 통해 개발된 UNDERWRITE 벤치마크를 사용하여 13개의 최첨단 모델을 평가하였다.

Result: 연구실 성능과 기업 준비 상태 사이의 유의미한 격차가 발견되었고, 많은 모델들이 도메인 지식을 상상하거나 도구 사용에도 불구하고 오류를 보였다.

Conclusion: 전문가 참여가 벤치마크 설계에 필수적이며, 성능 보고를 왜곡하는 취약성을 발견할 수 있도록 신뢰성 있는 평가 접근 방식이 필요하다.

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [9] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: L$^{2}$-VMAS는 정보 병목 현상을 해결하고 에이전트 간 협업을 향상시키는 새로운 접근법이다.


<details>
  <summary>Details</summary>
Motivation: VMAS가 에이전트 간 협업을 통해 종합 능력을 향상시킬 수 있지만, 에이전트 회전 수 증가가 성능 저하를 초래하는 현상을 해결하고자 한다.

Method: L$^{2}$-VMAS는 이중 잠재 기억을 사용하여 에이전트 간의 협업을 가능하게 하는 프레임워크로, 인식과 사고를 분리하고 동적으로 이중 잠재 기억을 합성한다.

Result: 우리의 방법은 다양한 백본, 크기 및 다중 에이전트 구조에서 실험하여 '확장 벽'을 효과적으로 극복하고, 평균 정확도를 2.7-5.4% 향상시키며 토큰 사용을 21.3-44.8% 줄였다.

Conclusion: L$^{2}$-VMAS는 에이전트 간 협업을 위한 효율적인 메모리 접근 방식을 제공한다.

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [10] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: PCBSchemaGen은 PCB 회로도 설계를 위한 최초의 훈련 없는 프레임워크로, 디지털, 아날로그 및 전원 신호를 처리하며 설계 정확도와 계산 효율성을 크게 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존의 연구는 디지털 또는 아날로그 회로에만 집중했지만, PCB 설계는 다양한 신호와 실제 IC 패키지 및 핀 제약을 고려해야 하므로 자동화된 PCB 회로도 설계에 대한 관심이 필요하다.

Method: PCBSchemaGen은 LLM 에이전트와 제약 안내 합성을 포함하는 훈련 없는 프레임워크로서, 도메인 특정 프롬프트를 사용한 반복 피드백을 활용한 LLM 기반 코드 생성 패러다임과 실제 IC 데이터시트를 기반으로 한 지식 그래프와 서브그래프 동형사상 인코딩을 통한 검증 프레임워크를 제공한다.

Result: 23개의 PCB 회로도 작업에 대한 광범위한 실험을 통해 설계의 정확도와 계산 효율성을 크게 개선함을 입증하였다.

Conclusion: 따라서 PCBSchemaGen은 PCB 설계의 자동화를 위한 중요한 발전을 나타내며, 다양한 분야의 회로도 설계 작업을 지원할 수 있다.

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [11] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: 이 논문은 MedBeads라는 데이터 인프라를 제안하여 AI가 환자 기록을 처리할 때 발생하는 컨텍스트 불일치를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 전문가 수준의 의료 지식을 보여주지만, 이를 자율적인 임상 에이전트로 배포하는 데 한계가 있습니다.

Method: MedBeads라는 불변의 '구슬'로 구성된 데이터 인프라를 만들어, 임상 사건과 카우살 프레데세서 간의 관계를 암호적으로 참조합니다.

Result: 합성 데이터를 사용하여 워크플로우를 성공적으로 구현하였으며, FHIR-투-DAG 변환을 통해 단순한 자원들을 인과적으로 연결된 그래프로 변환했습니다.

Conclusion: MedBeads는 확률론적 검색에서 결정론적 그래프 탐색으로, 변동 가능한 기록에서 불변 체인으로 전환하여 신뢰할 수 있는 의료 AI를 위한 기반을 제공합니다.

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [12] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan은 고유한 도전을 제시하는 중국 카드 게임의 벤치마크를 제안하며, 인간-AI 상호작용을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 기반 인공지능의 발전은 대규모 벤치마크에 크게 의존하며, 더 도전적인 벤치마크의 필요성이 제기된다.

Method: OpenGuanDan은 효율적인 시뮬레이션과 학습 기반 및 규칙 기반 AI 에이전트의 포괄적인 평가를 가능하게 한다.

Result: 현재의 학습 기반 에이전트는 규칙 기반 에이전트보다 성능이 뛰어나지만, 여전히 초인적인 성능에는 미치지 못한다.

Conclusion: 이 연구는 다중 에이전트 지능적 의사결정 분야의 지속적인 연구 필요성을 강조한다.

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [13] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 이 논문은 LLM을 평가자로 사용할 때의 신뢰성을 진단하기 위한 두 단계의 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 평가 방식이 피평가 결과물에 초점을 맞추고 있어, LLM 평가자 자체의 신뢰성과 안정성을 평가하는 데 한계가 있다.

Method: 아이템 반응 이론(IRT)을 기반으로 하여 그레이디드 반응 모델(GRM)을 채택한 두 단계 진단 프레임워크를 도입하였다.

Result: 다양한 LLM 평가자에 대해 이 프레임워크를 실증적으로 검토하였으며, IRT-GRM을 활용해 체계적인 판단 진단을 위한 해석 가능한 신호를 생성한다.

Conclusion: 이 신호들은 LLM을 평가기로 활용하는 신뢰성을 검증하고, 신뢰성 저하의 잠재적 원인을 식별하는 데 실질적인 지침을 제공한다.

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [14] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 이 논문은 사용자 설득이 장기 과제에 참여하는 AI 에이전트의 행동에 미치는 영향을 연구하며, 설득의 전파 현상을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 현대 AI 에이전트는 대화형 상호작용과 자율 작업 실행을 결합하고 있으며, 오랜 과제에서 사용자 설득을 받는 경우에 대한 질문을 제기한다.

Method: 행동 중심의 평가 프레임워크를 도입하여 작업 실행 중 또는 이전에 적용된 설득을 구분한다.

Result: 웹 연구 및 코딩 과제를 통해 즉시 적용된 설득이 약하고 일관성이 없는 행동 효과를 유도한다는 것을 발견했다. 반면, 작업 시 믿음 상태를 명시적으로 정의할 때 믿음이 미리 채워진 에이전트는 중립적으로 채워진 에이전트에 비해 평균 26.9% 적은 검색을 수행하고 16.9% 적은 고유 출처를 방문한다.

Conclusion: 이 결과는 설득이 설득 전에 이루어졌음에도 불구하고 에이전트의 행동에 영향을 미칠 수 있음을 시사하며, 에이전트 시스템의 행동 수준 평가를 유도한다.

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [15] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 본 연구는 대형 언어 모델이 포커와 같은 고위험 상황에서 전략적으로 추론하는 능력을 평가한 것이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 고위험 분야에서 점점 더 많이 사용됨에 따라 불확실한 상황에서 전략적으로 추론하는 능력이 중요해지고 있다.

Method: 여러 가지 현실적인 포커 작업에서 LLM을 체계적으로 연구하고 게임 플레이 결과 및 추론 흔적을 평가하였다.

Result: 분석 결과, LLM은 전통적인 알고리즘과 경쟁하지 못하며 휴리스틱 의존, 사실 오해, 추론과 행동 간의 괴리 등 세 가지 반복적인 결함이 발견되었다.

Conclusion: 이러한 한계를 극복하기 위해 외부 해결책과 GTO 일관된 행동을 결합한 ToolPoker라는 도구 통합 추론 프레임워크를 제안하였으며, 실험 결과 ToolPoker가 게임 이론 원칙을 잘 반영하는 추론 흔적을 생성하면서 최첨단 게임 플레이를 달성함을 보여주었다.

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [16] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM의 안전 정렬을 위한 새로운 재강화 학습 프레임워크인 MAGIC을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 방어 방법이 정적이고 사전 수집된 데이터 분포에 의존하기 때문에 진화하는 적대적 공격에 뒤처지는 문제가 지속되고 있다.

Method: MAGIC은 LLM의 안전 정렬을 적대적 비대칭 게임으로 공식화한 다중 턴 다중 에이전트 강화 학습 프레임워크다.

Result: 공격자는 반복적인 RL 훈련을 통해 이전에 보지 못한 새로운 조합 전략을 진화시키며, 방어자는 이러한 입력을 인식하고 거부하기 위해 정책을 최적화한다.

Conclusion: 우리의 프레임워크는 모델의 유용성을 저해하지 않으면서도 뛰어난 방어 성공률을 입증한다.

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [17] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: 이 논문은 정보 탐색 에이전트를 통합하는 방법을 제안하며, 데이터 및 매개변수 수준에서의 통합 전략을 비교하고 그 결과를 분석한다.


<details>
  <summary>Details</summary>
Motivation: 정보 탐색 에이전트는 지식 집약적 작업을 해결하는 강력한 패러다임으로 부상하고 있으나, 기존 에이전트는 특정 도메인에 특화되어 있어 확장성과 도메인 간 일반화에 제한이 있다.

Method: 두 가지 보완적인 통합 전략을 연구했다: 데이터 수준 통합은 도메인별 데이터 세트를 혼합하여 통합 모델을 공동 훈련하고, 매개변수 수준 통합은 독립적으로 훈련된 에이전트 모델을 매개변수 수준에서 병합한다.

Result: 데이터 수준 통합이 강력하고 안정적인 기준선으로 유지되는 반면, 매개변수 수준 통합은 유망하고 효율적인 대안을 제공하지만 간섭 및 내구성 문제에 직면한다는 것을 보였다.

Conclusion: 효과적인 에이전트 통합을 위한 주요 설계 요소로는 세밀한 병합 세분성, 작업 이질성에 대한 인식, 그리고 원칙에 기반한 합의 전략이 있다.

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [18] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: DockSmith은 소프트웨어 공학 에이전트의 실행 기반 훈련 및 평가를 위한 신뢰할 수 있는 Docker 환경 구축을 위한 전문적인 Docker 빌더이다.


<details>
  <summary>Details</summary>
Motivation: Docker 기반 환경 구축은 소프트웨어 공학 에이전트의 훈련 및 평가를 규모있게 확장하는 데 주요 병목현상이다.

Method: DockSmith는 환경 구축을 전처리 단계가 아닌 핵심 에이전틱 능력으로 처리하며, 긴 수명의 도구 사용, 의존성 추론, 실패 복구를 통해 감독을 제공한다.

Result: 30B-A3B 모델을 큰 규모의 Docker 구축 경로에 대해 훈련시킨 결과, Multi-Docker-Eval에서 오픈소스 최첨단 성능을 달성하였다.

Conclusion: DockSmith는 SWE-bench Verified, SWE-bench Multilingual 및 Terminal-Bench 2.0에서 분포 외 성능을 향상시켜, 환경 구축의 더 넓은 에이전틱 장점을 보여준다.

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [19] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: 현재의 에이전트 프레임워크는 장기 작업에서 성능이 떨어진다. 우리는 ROMA라는 새로운 프레임워크를 소개하며, 이는 재귀적 작업 분해와 구조적 집계를 통해 이러한 한계를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 에이전트 프레임워크는 장기 과제에서 효율성이 떨어지며, 이는 깊은 추론을 요구할 때 성능 저하를 초래한다.

Method: ROMA는 의존성 인식 하위 작업 트리로 목표를 재귀적으로 분해하고, 중간 결과를 압축하고 검증하여 맥락 성장률을 제어한다.

Result: ROMA와 GEPA+를 결합하여 추론 및 장기 생성 벤치마크에서 뛰어난 성능을 보여준다. SEAL-0에서는 정확도를 9.9% 개선하였다.

Conclusion: 재귀적이고 모듈화된 에이전트 아키텍처는 추론 깊이를 확장하면서도 해석 가능하고 유연하며 모델에 독립적이다.

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [20] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 이 연구에서는 Embodied AI의 성능을 평가하기 위해 두 개의 7B-매개변수 모델과 구조적 자기 일관성을 활용한 새로운 디코딩 전략을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: Embodied AI에서 에이전트가 목표를 이해하고 행동을 계획하며 시뮬레이션된 환경에서 작업을 실행할 수 있도록 하는 것이 필요합니다.

Method: VirtualHome 벤치마크에서 Embodied Agent Interface (EAI) 프레임워크를 사용하여 두 개의 7B-매개변수 모델 OPENPANGU-7B와 QWEN2.5-7B를 비교했습니다. 4가지 기본 작업: 목표 해석, 행동 순서 지정, 하위 목표 분해 및 전이 모델링을 수행합니다. 또한, 우리는 구조적 자기 일관성(SSC)이라는 향상된 디코딩 전략을 제안하여 도메인 특정 투표 메커니즘과 다중 샘플링을 활용하여 구조화된 생성 작업의 출력 품질을 향상시킵니다.

Result: 실험 결과 SSC가 성능을 크게 향상시킴을 보여주었고, OPENPANGU-7B는 계층적 계획에서 뛰어난 성능을 보였으며 QWEN2.5-7B는 행동 수준의 작업에서 우위를 나타냈습니다.

Conclusion: 모델 유형 간의 상호 보완적인 강점을 분석하여 향후 Embodied AI 시스템 개발에 대한 통찰을 제공합니다.

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [21] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: 본 논문에서는 다중 에이전트 토론(MAD)의 일관성을 개선하기 위해 다중 LLM 컨텍스트 학습 방법(M2CL)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 MAD 방법은 불일치한 논의로 인해 일관성 있는 해결책에 도달하지 못하는 문제점을 가지고 있습니다.

Method: M2CL은 자동 정보 조직화 및 정제를 통해 논의 라운드마다 동적으로 컨텍스트 지침을 생성할 수 있는 컨텍스트 생성기를 학습합니다.

Result: M2CL을 사용한 결과, 기존 방법보다 20%~50% 성능이 향상되었으며, 전이 가능성과 계산 효율성도 우수했습니다.

Conclusion: M2CL은 LLM이 다수의 잡음에 조기에 수렴하는 것을 피하고 점진적으로 올바른 합의에 도달하도록 돕습니다.

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [22] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 이 연구는 역삼투압 탈염에서의 UF 멤브레인 잔여 유용 수명(RUL) 추정에 대한 설명 가능한 예측 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 예측 유지보수 모델은 해석성이 부족하고 운영자의 신뢰를 결여하고 있어 정기적인 예방 유지보수에 의존하게 됩니다.

Method: 퍼지 유사성 추론을 이용하여 UF 멤브레인 잔여 유용 수명(RUL)을 추정하는 설명 가능한 예측 프레임워크를 제안합니다.

Result: 산업 규모의 UF 시스템에서 12,528개의 운영 사이클을 테스트하여 평균 절대 오차가 4.50 사이클을 달성했습니다.

Conclusion: 전문가의 이해와 일치하는 해석 가능한 규칙 기반을 생성하면서 투명하고 유사성 가중치 기반의 RUL 추정을 가능하게 했습니다.

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [23] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: SEISMO는 실시간 분자 최적화를 수행하여 기존 방법보다 2-3배 더 높은 효율성을 달성하는 LLM 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 약물 발견 과정에서 분자의 구조를 최적화하는 것은 화학 과학에서 주요한 병목 현상이다.

Method: SEISMO는 매개변수 기반 학습 없이 모든 오라클 호출 후에 업데이트를 수행하는 온라인 분자 최적화 방법이다.

Result: SEISMO는 23개의 Practical Molecular Optimization 벤치마크 과제에서 이전 방법들보다 2-3배 더 높은 최적화 곡선 면적을 달성하였다.

Conclusion: 도메인 지식과 구조화된 정보를 활용하는 것이 샘플 효율적인 분자 최적화의 핵심임을 보여준다.

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [24] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: 이 논문에서는 사회과학 실험에서의 인간 참여자를 시뮬레이션하는 데 있어 대규모 언어 모델(LLM)의 불안정성을 해결하는 새로운 접근법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 사회과학 실험에서 시뮬레이션된 참여자로서 점점 더 많이 사용되고 있지만, 이들의 행동은 종종 불안정하며 설계 선택에 매우 민감합니다.

Method: 이 논문은 참가자 시뮬레이션을 실험 프로토콜 전체에 대한 에이전트 설계 문제로 프레임하며, 이때 에이전트는 기본 모델과 행동 가정을 인코딩한 사양(예: 참가자 속성)에 의해 정의됩니다. HUMANSTUDY-BENCH라는 벤치마크를 소개하여 LLM 기반 에이전트가 필터-추출-실행-평가 파이프라인을 통해 인간 참여자 실험을 재구성합니다.

Result: 이 벤치마크는 원래의 통계 절차를 유지하는 공유 런타임에서 평가 시퀀스를 재생하고 원래 분석 파이프라인을 실행하여 과학적 추론의 일치성을 평가합니다.

Conclusion: 12개의 기초 연구를 이 동적 벤치마크의 초기 스위트로 제정하여 개인 인지, 전략적 상호작용 및 사회 심리학을 포괄하며, 10명에서 2100명 이상의 참가자에 이르는 인간 샘플을 가지고 6,000건 이상의 실험을 다룹니다.

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [25] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: AI를 임상 환경에 통합하는 것은 견고하고 신뢰할 수 있는 시스템으로의 전환을 요구하는 소프트웨어 공학적 도전이다. 본 논문은 이러한 격차를 해결하는 'Maria' 플랫폼이라는 생산 등급 AI 시스템을 사례 연구로 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI의 임상 환경 통합은 신뢰할 수 있고 효율적인 시스템 개발을 위한 도전 과제를 제기한다.

Method: 'Clean Architecture'와 'Event-driven architecture'를 결합한 시너지 아키텍처를 제시하고, 모듈화의 주요 단위로 작동하는 에이전트를 도입한다.

Result: 이 플랫폼은 지속적인 개선을 위한 중요한 데이터 소스로서 휴먼인더루프 거버넌스 모델을 기술적으로 통합한다.

Conclusion: 이 플랫폼은 고위험 분야에서 유지 관리 가능하고 확장 가능하며 책임이 있는 AI 시스템 구축을 위한 실용적인 교훈을 제공하는 참조 아키텍처를 제시한다.

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [26] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 강화 학습을 통해 훈련된 대형 언어 모델(LLM) 에이전트는 게임, 수학, 코딩과 같은 저비용 환경에서 초인적인 성능을 달성했지만, 로봇 운영의 물리적 비용, ML 엔지니어링의 시간 비용, 과학 실험의 자원 비용과 같은 높은 상호 작용 비용이 있는 복잡한 도메인에는 성공이 전이되지 않았다. 이 논문은 에이전트와 실제 세계 간의 매개체로 세계 모델을 사용하는 것이 필요하다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: 복잡하고 비용이 높은 도메인에서 에이전트 성능 향상의 진정한 병목 현상은 보상 신호를 획득하기 위한 행동 실행의 비용에 있다.

Method: 세계 모델을 에이전트와 실제 세계 간의 중개자로 활용하고, 역동성, 보상 및 작업 분포의 모델로서 이를 다룬다.

Result: 세계 모델이 행동 학습에 필요한 중요한 학습 신호를 제공할 수 있음을 보이며, 다양한 도메인에서 적용 가능성을 시연한다.

Conclusion: 세계 모델 구축의 도전 과제를 식별하고 데이터 세트 큐레이션, 아키텍처 설계, 스케일링 및 평가와 관련하여 실행 가능한 아이템을 제안한다.

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [27] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: InfoReasoner는 정보를 효과적으로 탐색하도록 유도하는 통합 프레임워크로, 대규모 추론 모델의 외부 지식 획득을 최적화하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 추론 모델이 동적으로 외부 지식을 획득할 수 있도록 하는 에이전틱 추론의 필요성.

Method: 정보 이득이라는 합성 의미 정보 이득 보상을 통해 효과적인 정보 탐색을 유도하도록 설계되었으며, 비정상성, 망원수 가산법 및 채널 단조성을 포함한 보장을 확립한다.

Result: InfoReasoner가 강력한 정보 검색 보강 기준선을 지속적으로 초과하여 최대 5.4%의 평균 정확도 향상을 달성함을 보여준다.

Conclusion: 검색을 통한 에이전틱 추론을 향한 이론적으로 기반이 있으며 확장 가능한 경로를 제공한다.

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [28] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: 본 논문은 현대 트랜스포머 주의 메커니즘이 다중 에이전트 시스템으로 작동한다는 점을 다루고, 이로 인해 발생하는 비효율성을 해소하기 위한 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 트랜스포머 주의 메커니즘은 헤드 간 경쟁 및 조정이 이루어지지만, 우리는 이를 단일 최적화기로 다룬다.

Method: 잠재적 게임의 형태를 정식화하고 헤드 상호작용 행렬의 오프 대각선 질량을 사용하여 비효율성을 제어하는 경계식을 도출하여 GAME-LoRA를 제안한다.

Result: 이론에 따르면, $Γ(G)$가 헐리네이션을 예측하고, emergent coalitions가 선택적 조정을 나타내며, GAME-LoRA가 지식 감소 없이 최대 18%의 헐리네이션 감소를 달성한다.

Conclusion: 게임 구조를 무시한 방법으로는 접근할 수 없는 Pareto 개선을 제공한다.

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [29] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse 프레임워크는 LLM 기반 에이전트의 협업 학습에서 도구 사용 행동의 공유 글로벌 지식 모델을 훈련하여 의사소통 비용과 데이터 이질성 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트 간의 협업 학습에서 통신 비용, 데이터 이질성 및 도구 사용 문제 등으로 효과성이 제한되고 있습니다.

Method: Synapse는 고정된 LLM을 가진 클라이언트 에이전트가 로컬에서 도구 사용 패턴을 학습하고, 조정자를 통해 연합 집계를 위한 아티팩트를 전송하는 프레임워크입니다. 글로벌 도구 목록이 업데이트되고 재배포되어 안정적인 도구 선택을 향한 수렴을 가능하게 합니다.

Result: Synapse는 도구 사용 효과성을 향상시키고, 멀티 에이전트 LLM 시스템에서 가중치 또는 프롬프트 공유 접근 방식과 비교하여 통신 오버헤드를 줄입니다.

Conclusion: 이 프레임워크는 이질적인 데이터를 지원하며 성능 향상을 정량화할 수 있습니다.

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [30] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2는 LLM의 인맥 학습 능력을 활용하여 재사용 가능한 추상화를 능동적으로 학습하는 새로운 TBRL 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 최신 LLM 에이전트와 딥 RL 시스템이 여전히 어려워하는 태스크 간의 빠른 일반화를 위한 추상화 학습이 필요하다.

Method: TheoryCoder-2는 경험에서 추상화를 합성하고 이를 계층적 계획 과정에 통합하여 수동으로 지정된 추상화 대신 LLM의 인맥 학습 능력을 활용한다.

Result: TheoryCoder-2는 Sokoban과 같은 VGDL 게임을 포함한 다양한 환경에서 실험을 통해 이전 TBRL 시스템보다 더 많은 샘플 효율성을 보인다.

Conclusion: TheoryCoder-2는 최소한의 인간 프롬프트만으로도 베이스라인이 실패한 복잡한 작업을 해결할 수 있다.

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [31] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: 대형 언어 모델이 정신 건강 지원에 사용되지만 임상 적합성을 보장하지는 않는다. 본 연구는 위험 분류법을 제시하고 MindGuard라는 안전 분류기를 개발하여 임상 전문가와 협력하여 진정한 위기 상황을 식별하고 안전한 치료 콘텐츠를 유지한다.


<details>
  <summary>Details</summary>
Motivation: 정신 건강 지원을 위한 대형 언어 모델의 사용 증가에도 불구하고 기존의 일반 목적 안전 장치는 임상 적합성을 보장하지 못하고 안전 문제를 발생시킬 수 있다.

Method: 정신학 박사와 협력하여 개발한 임상적으로 기반한 위험 분류법을 소개하고, 임상 전문가가 주석을 단 다중 턴 대화 데이터셋인 MindGuard-testset을 배포하며, 두 개의 에이전트를 통한 합성 대화를 사용하여 MindGuard라는 경량 안전 분류기를 훈련시킨다.

Result: 우리의 분류기는 높은 재현율 운영 지점에서 잘못된 양성을 줄이고, 임상 언어 모델과 결합될 때 일반 목적 안전 장치에 비해 적대적인 다중 턴 상호작용에서 공격 성공 및 유해 참여율을 낮춘다.

Conclusion: 모든 모델과 인간 평가 데이터를 공개한다.

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [32] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: 온라인 계층적 작업 네트워크(HTN) 에이전트를 소개하며, 이들은 내장된 지침 집합 \\D에 의해 행동이 좌우된다. 에이전트는 사용자가 지정한 작업을 수행하지 않을 수 있다. 이 연구는 HTN 계획, 온라인 계획, 그리고 사용자 지정 작업 수행 시 고려해야 할 지침 \\D의 세 가지 개념을 결합한다.


<details>
  <summary>Details</summary>
Motivation: 우리의 목적은 사용자가 지정한 작업을 수행하면서도 에이전트의 자율성을 유지하는 것이다.

Method: 우리는 비적응형 에이전트와 적응형 에이전트를 두 가지 변형으로 조사하고, R-HTN이라는 알고리즘을 제안한다.

Result: R-HTN 에이전트는 지침을 결코 위반하지 않으며, 가능하다면 사용자 지시 목표를 달성하려고 한다.

Conclusion: R-HTN은 온라인 HTN 계획을 위한 일반 알고리즘으로 안전성과 성격 특성에 따라 지침 위반을 방지한다.

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [33] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 이 논문은 에이전틱 강화 학습(ARL)에서 추론과 도구 실행 간의 간섭을 조사하고, 이를 해결하기 위해 DART이라는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 많은 ARL 방법들은 추론과 도구 사용 행동을 동시에 지원하기 위해 단일 모델 매개변수를 훈련함으로써, 공동 훈련이 에이전트 성능을 향상시킨다고 가정합니다. 하지만 이 가정은 경험적으로 검증된 바가 드뭅니다.

Method: 본 논문에서는 선형 효과 귀속 시스템(LEAS)을 도입하여 추론과 도구 사용 행동 간의 간섭에 대한 정량적 증거를 제공합니다. 이어서 DART라는 프레임워크를 제안하여 추론과 도구 사용에 대한 매개변수 업데이트를 명시적으로 분리합니다.

Result: 실험 결과 DART는 평균 6.35% 향상을 보여주며, 도구 사용과 추론을 명시적으로 분리한 다중 에이전트 시스템과 성능이 비교됩니다.

Conclusion: DART는 기존의 방법들에 비해 일관되게 우수한 성능을 발휘하며 ARL의 기존 패러다임에 도전하는 결과를 보여줍니다.

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [34] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM(대형 언어 모델)이 심볼릭 목적 함수의 볼록성을 식별하는 능력을 테스트하기 위한 벤치마크인 	extcb를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 연구 수준의 수학과 과학을 자동화하기 시작하면서 볼록성을 이해하고 추론하는 능력이 중요해졌습니다.

Method: 심볼릭 목적의 볼록성을 심층 함수 조합 하에서 식별할 수 있는지 테스트하기 위해 	extcb라는 확장 가능하고 기계적으로 검증 가능한 벤치마크를 도입했습니다.

Result: 실험 결과, LLM의 성능이 깊이가 깊어질수록 급격히 저하되며, 깊이 2에서의 F1 점수가 1.0에서 깊이 100에서 약 0.2로 떨어진다는 것을 보여주었습니다.

Conclusion: 우리는 외부 도구를 이용해 파싱을 오프로드하고 각 중간 하위 표현에 대해 집중적인 맥락으로 재귀적 추론을 시행하는 에이전틱 분할 정복 프레임워크를 제안합니다. 이 프레임워크는 깊은 조합 실패를 신뢰할 수 있게 완화시켜, 깊이 100에서 F1-점수 1.0을 달성하는 상당한 성능 향상을 이끌어냅니다.

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [35] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth는 건강 데이터를 자율적으로 모델링하고 신뢰성을 평가하기 위한 새로운 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트는 자율 기계 학습에 강력한 잠재력을 나타내지만, 건강 데이터에 대한 적용 가능성이 제한적이다.

Method: AutoHealth는 다섯 개의 전문 에이전트 간의 폐쇄 루프 조정을 사용하여 데이터 탐색, 작업 조건화 모델 구축, 훈련 및 최적화를 수행한다.

Result: AutoHealth는 17개의 다양한 데이터 양식 및 학습 환경에서 29.2%의 예측 성능 향상과 50.2%의 불확실성 추정 향상을 보인다.

Conclusion: 시스템은 신뢰할 수 있는 해석과 위험 인지 의사 결정을 지원하는 포괄적인 보고서를 생성한다.

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [36] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP는 고차원 이벤트 시퀀스에서 오류 패턴을 자동으로 생성하는 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 현대 차량의 진단 문제 코드는 수천 가지 유형의 이산 사건을 생성하며, 이를 바탕으로 시스템 결함을 정의하고 차량의 안전성을 보장하기 위해 오류 패턴이 필요하다. 그러나 현재 오류 패턴 규칙은 전문가에 의해 수작업으로 작성되며, 이는 비용이 많이 들고 복잡한 차량 설계로 인해 오류가 발생하기 쉬운 문제이다.

Method: CAREP는 잠재적인 진단 문제 코드와 오류 패턴 간의 관계를 식별하는 인과 발견 에이전트, 메타데이터와 설명을 통합하는 맥락 정보 에이전트, 후보 불리언 규칙을 종합하고 해석 가능한 추론 흔적을 제공하는 오케스트레이터 에이전트로 구성된 다중 에이전트 시스템이다.

Result: 대규모 자동차 데이터 세트에서 CAREP는 29,100개 이상의 고유 진단 문제 코드와 474개 오류 패턴을 분석하여 알려지지 않은 오류 패턴 규칙을 자동으로 정확하게 발견하며, LLM 전용 기준보다 더 나은 성능을 보였다.

Conclusion: CAREP는 실용적인 인과 발견과 에이전트 기반 추론을 통합하여 완전 자동화된 결함 진단을 위한 한 걸음을 내디뎠으며, 확장 가능하고 해석 가능한 경제적인 차량 유지 보수를 가능하게 한다.

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [37] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench는 자연어 문제를 로직 프로그램으로 자동 변환하는 시스템을 평가하기 위한 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 자연어 명세를 로직 프로그램으로 자동 변환하는 것은 신경 상징 공학에 영향을 미치는 도전적인 작업이다.

Method: 128개의 자연어 문제 인스턴스를 포함한 ASP-Bench 벤치마크를 소개하며, ReAct(Reason and Act) 프레임워크를 기반으로 한 에이전트 접근 방식을 사용하여 벤치마크를 테스트한다.

Result: 피드백 기반의 반복 개선을 통해 ASP에서 자연어 모델링을 위한 신뢰할 수 있는 접근 방식을 제공한다.

Conclusion: 문제의 모델링 난이도를 결정하는 요소에 대한 통찰을 얻을 수 있다.

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [38] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 이 논문은 복잡한 추론 작업을 해결하는 LLM 기반 에이전트의 능력을 향상시키기 위해 워크플로우 최적화 문제를 다중 턴 자연어 기반 결정 과정으로 재구성하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 워크플로우의 급속한 발전은 복잡한 추론 작업에서 LLM 기반 에이전트의 강력한 성능을 보여주고 있습니다. 하지만 기존의 워크플로우 최적화 방법은 워크플로우 생성을 정적이고 단일 코드 중심의 생성 문제로 공표하여 모델의 코딩 능력을 지나치게 제한하고 있습니다.

Method: Workflow-R1이라는 프레임워크를 통해 워크플로우 구성을 다중 턴 자연어 기반의 순차적 결정 과정으로 재구성합니다. 또한, GSsPO(Group Sub-sequence Policy Optimization)를 도입하여 이러한 다중 턴 상호작용에서 발생하는 최적화 세분화 불일치를 해결합니다.

Result: 광범위한 QA 벤치마크에 대한 포괄적인 실험을 통해 Workflow-R1은 경쟁 기반 모델을 초월적으로 성능을 발휘하고, GSsPO를 순차적 추론을 위한 일반화된 솔루션으로 검증합니다.

Conclusion: Workflow-R1은 자동화된 워크플로우 최적화를 위한 유망한 새로운 패러다임으로 자리잡습니다.

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [39] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF는 다중 전문가를 이용하여 신경학적 진단을 개선하는 새로운 프레임워크로, EMR의 한계를 극복하고 논리적 일관성을 보장한다.


<details>
  <summary>Details</summary>
Motivation: 신경학적 전자 의무 기록(EMR)은 이질적이고 희소하며 노이즈가 존재하여 임상 진단에서 대규모 언어 모델(LLM)에 대한 도전 과제가 된다.

Method: RE-MCDF는 생성-검증-수정의 폐쇄 루프 구조를 도입하여 주 전문가, 실험실 전문가, 다중 관계 인식 및 평가 전문가 그룹을 통합한다.

Result: NEEMRs 및 XMEMRs 데이터셋을 통한 광범위한 실험은 RE-MCDF가 복잡한 진단 시나리오에서 기존 방법보다 일관되게 우수하다는 것을 보여준다.

Conclusion: RE-MCDF는 의학적 지식 그래프(MKG)에 의해 안내되며, EMR 증거의 재조정을 통해 논리적 일관성을 보장한다.

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [40] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: 이 논문은 텍스트에 대한 엔티티 수준 집계 쿼리를 형식화하고, 이를 평가하기 위한 AGGBench라는 벤치마크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자유 텍스트에 대한 집계 쿼리는 기존에 충분히 탐구되지 않은 문제입니다. 기존 시스템은 '모두 찾기'를 요구하는데, 이는 단순히 '하나 찾기'와는 다릅니다.

Method: AGGBench를 소개하고, 집계 쿼리를 해석 가능한 단계로 분해하는 모듈형 기준인 DFA를 제안합니다.

Result: DFA는 강력한 RAG 및 기준에 비해 집계 증거 범위를 일관되게 개선했습니다.

Conclusion: 이 데이터와 코드는 https://anonymous.4open.science/r/DFA-A4C1에서 사용할 수 있습니다.

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [41] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: SimGym은 대규모 언어 모델 에이전트를 이용해 오프라인 A/B 테스트를 신속하게 수행할 수 있는 시스템으로, 실제 사용자 경험에 미치는 영향을 최소화하며 실험 사이클을 단축한다.


<details>
  <summary>Details</summary>
Motivation: A/B 테스트는 E-commerce UI 변경 평가의 금본위지만, 트래픽을 분산시키고 결과가 유의미해질 때까지 수주가 걸리며 사용자 경험에 해를 끼칠 위험이 있다.

Method: SimGym은 실제 브라우저에서 작동하는 대규모 언어 모델 에이전트를 통해 트래픽 기반의 합성 구매자들을 이용하여 신속한 오프라인 A/B 테스트를 수행하는 확장 가능한 시스템이다.

Result: SimGym은 주요 E-commerce 플랫폼에서의 실제 UI 변경에 대한 실인간 결과와 비교하여 검증되었으며, 실험 사이클을 수주에서 1시간 미만으로 줄였다.

Conclusion: SimGym 에이전트는 훈련 후 조정 없이도 관찰된 결과 변화와 뛰어난 정렬 성과를 달성하여, 실제 구매자에게 노출되지 않고도 신속한 실험을 가능하게 한다.

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [42] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 이 논문은 소프트웨어 엔지니어링을 조직 과정으로 모델링하는 완전 자동화된 다중 에이전트 시스템을 제안하며, 이는 팀 구조를 복제하고 인간의 개입 없이 작업을 수행한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 자율 시스템이 문제 해결을 단일 프로세스 또는 파이프라인으로 처리하는 반면, 실제 소프트웨어 개발은 역할 분리와 의사소통, 검토가 명확한 팀 기반 협력 활동으로 구성된다.

Method: agyn이라는 오픈 소스 플랫폼 위에 구축된 이 시스템은 조정, 연구, 구현, 검토 등과 같은 역할에 전문화된 에이전트를 할당하고, 실험을 위한 격리된 샌드박스를 제공하며, 구조화된 의사소통을 가능하게 한다.

Result: 시스템은 분석, 작업 명세, 풀 리퀘스트 생성 및 반복적 검토를 포함한 정의된 개발 방법론을 따라 인간 개입 없이 작동하며, SWE-bench 500에서 72.4%의 작업을 해결하여 비교 가능한 언어 모델을 사용하는 단일 에이전트 기준을 초과한다.

Conclusion: 팀 구조, 방법론 및 의사소통을 복제하는 것이 자율 소프트웨어 엔지니어링을 위한 강력한 패러다임이며, 향후 발전은 모델 개선만큼이나 조직 설계와 에이전트 인프라에 달려 있을 수 있다.

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [43] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: AI 거버넌스에서 주목해야 할 것은 법률과 규제 인프라를 구축하는 것이다.


<details>
  <summary>Details</summary>
Motivation: AI의 변화무쌍한 특성을 감안할 때 법적 및 규제 프레임워크 구축이 특히 중요하다.

Method: 최전선 모델에 대한 등록 체계, 자율 에이전트에 대한 등록 및 식별 체계, 민간 기업이 AI 규제 서비스를 혁신하고 제공할 수 있도록 하는 규제 시장 설계의 세 가지 예를 제시한다.

Result: 법률 및 규제 인프라 구축의 필요성에 대한 논의가 이루어졌다.

Conclusion: AI 거버넌스에서 실질적인 규칙 외에도 법적 시스템 구축이 필수적이다.

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [44] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: PRISM은 비용 민감 선택적 개입 문제를 해결하기 위해 제안된 새로운 프레임워크로, 의사 결정 이론 게이트와 이중 프로세스 추론 아키텍처를 결합한다.


<details>
  <summary>Details</summary>
Motivation: 현재 많은 시스템이 경직된 휴리스틱 또는 무차별적인 장기 추론에 의존하고 있으며, 그로 인해 이익-부담 균형을 제어하기 어렵다.

Method: PRISM은 의사 결정 이론 게이트와 이중 프로세스 추론 아키택처를 결합하여, 균형 잡힌 사용자 수용 확률이 비대칭 비용에 의해 결정된 임계값을 초과할 때만 개입한다.

Result: PRISM은 강력한 기준선 대비 허위 경고를 22.78% 줄이고 F1 스코어를 20.14% 개선했다.

Conclusion: PRISM은 비용 민감 추론과 선택적 느린 추론을 결합하여 정밀하고 효율적이며 제어 가능한 능동 에이전트를 제공한다.

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [45] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent는 다학제적 과학 연구를 위해 설계된 자기 진화 에이전트 프레임워크로, 복잡한 연구 작업의 안정적인 모델링을 가능하게 하고, 다양한 과학 도구들을 통합하여 최적의 성과를 달성한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 및 도구 기반 에이전트가 긴 계획 수립과 목표 유지, 실행 학습 등에서 한계를 겪고 있어 이 문제들을 해결하기 위해 연구한다.

Method: S1-NexusAgent는 계층적 Plan-and-CodeAct 실행 패러다임을 채택하여 글로벌 과학 계획과 하위 작업 도구 실행을 분리하며, 객체 참조 기반의 희소 맥락 관리를 도입하여 하위 작업 맥락의 분리 및 중간 결과 압축을 가능하게 한다.

Result: S1-NexusAgent는 여러 과학 벤치마크에서 최첨단 성능을 달성하며, 복잡한 과학 작업에서의 효과성과 일반화 능력을 검증한다.

Conclusion: S1-NexusAgent는 지속 가능하고 장기적인 과학 연구를 위한 자기 진화의 폐쇄 루프를 형성하여, 고품질 연구 경로를 재사용 가능한 과학 기술로 정제한다.

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [46] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 대형 언어 모델을 기반으로 한 AI 시스템은 동적 및 개방 환경에서 자율적으로 의사 결정을 내리는 데 중요하지만, 기존 시스템은 사전 정의된 작업과 고정된 프롬프트에 의존하여 적응성에 한계가 있다. 본 논문에서는 AI 시스템이 내부 상태, 환경 관찰 및 다른 AI 시스템과의 상호작용을 통해 질문을 형성하고 작업을 설정할 수 있도록 하는 인적 시뮬레이션 기반 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율적으로 변화하는 환경에서 문제를 인식하고 해결할 수 있는 능력을 향상시키기 위한 방법론 필요.

Method: 내부 상태 및 환경 관찰에 기반하여 질문 형성을 주요 결정 과정으로 삼고, 경험을 통해 질문 형성 과정을 학습하도록 지원하는 프레임워크 제안.

Result: 다중 에이전트 시뮬레이션 환경에서 실험 결과, 환경 인지 프롬프트가 내부 주도 기준보다 더 많은 사건을 줄임을 보였으며, 상호 에이전트 인지 프롬프트는 20일 시뮬레이션 기간 동안 누적 사건을 60% 이상 더 줄였다.

Conclusion: AI 시스템의 적응성과 의사 결정 품질이 개선되었음을 실험을 통해 확인하였다.

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [47] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ProjDevBench는 코딩 에이전트의 성능을 평가하기 위한 종합 벤치마크로, 시스템 아키텍처 설계, 기능적 정확성 및 반복적 해결 정제를 평가한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 평가 방법은 결함 수정에 초점을 맞추고 있어 전체 구현 과정에서의 성능을 평가하지 못한다.

Method: ProDevBench는 프로젝트 요구 사항을 코딩 에이전트에 제공하고 결과적으로 생성된 저장소를 평가하는 종합 벤치마크이다.

Result: 평가에서 27.38%의 전체 수용률을 기록하며, 에이전트는 기본 기능 및 데이터 구조는 처리할 수 있으나, 복잡한 시스템 설계 및 시간 복잡도 최적화에는 어려움을 겪었다.

Conclusion: 우리의 벤치마크는 다양한 코드 평가를 제공하며, github에서 사용할 수 있다.

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [48] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer는 경량 정책 모델을 에이전트로 사용하는 종단 간 강화 학습 프레임워크로, 작업의 자동화를 통해 워크플로우 오케스트레이션의 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 워크플로우 오케스트레이션은 높은 수작업 비용, 특정 연산자나 대규모 언어 모델(LLM)에 대한 의존성, 희박한 보상 신호와 같은 주요 문제에 직면해 있습니다.

Method: FlowSteer는 경량 정책 모델을 에이전트로 사용하고, 실행 가능한 캔버스 환경에서 다중 턴 상호작용을 통해 워크플로우 오케스트레이션을 자동화합니다. 정책 모델은 실행 상태를 분석하고 편집 작업을 선택하며, 캔버스는 연산자를 실행하고 피드백을 반환하여 반복적인 정제를 도와줍니다. 또한 FlowSteer는 다양한 연산자 라이브러리와 교체 가능한 LLM 백엔드를 지원하는 플러그 앤 플레이 프레임워크를 제공합니다.

Result: 12개 데이터셋 실험 결과, FlowSteer는 다양한 작업에서 기준선보다 현저하게 우수한 성능을 발휘합니다.

Conclusion: 이 연구는 강화 학습을 활용한 새로운 워크플로우 오케스트레이션 접근 방식을 제시하며, 학습 안정화 및 단축 행동 억제를 위한 다양성 제약 보상을 도입합니다.

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [49] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench라는 새로운 벤치마크를 소개하며, 이는 현실적인 여행 계획 시나리오에 기반하여 복잡한 대화에서의 도전 과제를 평가한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 점점 더 복잡한 실제 환경에 배치됨에 따라 기존 벤치마크는 글로벌 제약, 다중 도구 추론 조정, 장기 다자 간 상호작용에서의 진화하는 사용자 행동 적응과 같은 주요 과제를 충분히 반영하지 못하고 있다.

Method: TRIP-Bench는 실제 데이터를 활용하고 18개의 커리케이션 도구와 40개 이상의 여행 요구 사항을 제공하며, 자동화된 평가를 지원한다. 또한 다양한 난이도의 분할이 포함되어 있으며, 어려운 분할은 긴 상호작용, 스타일 변화, 실행 가능성 변화 및 반복적인 버전 수정을 강조한다.

Result: 실험에 따르면, 고급 모델조차도 쉬운 분할에서 최대 50%의 성공률을 기록했으며, 어려운 하위 집합에서는 성능이 10% 이하로 떨어졌다.

Conclusion: TRIP-Bench는 실용적인 장기 상호작용 에이전트를 발전시키고 GTPO는 강력한 장기 훈련을 위한 효과적인 온라인 강화 학습 방법을 제공할 것으로 기대된다.

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [50] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: ORCH는 이질적인 대형 언어 모델을 조정하는 결정적 협업 프레임워크로, 독립적으로 구조화된 분석을 생성하고 최종 선택을 출력함으로써 예측 가능하고 재현 가능한 결정 과정을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 발전으로 인해 다중 에이전트 구조가 복잡한 추론 작업에 매력적이지만, 기존 시스템은 재현성이 떨어지고 해석하기 어렵다는 문제가 있다.

Method: ORCH는 '많은 분석, 하나의 결정' 패러다임을 따르며, 여러 기본 모델이 독립적으로 구조화된 분석을 생성하고, 전담 병합 에이전트가 최종 선택을 출력한다.

Result: ORCH는 MMLU, MMLU-Pro 및 GSM8K에서 단일 모델 기준과 다수 투표 앙상블을 일관되게 초월하며, MMLU-Pro에서 정확도를 10포인트 이상 개선하였다.

Conclusion: ORCH는 다중 에이전트 협업과 라우팅이 매우 중요하며, 제어 가능하고 해석 가능한 LLM 기반 에이전트 시스템을 위한 실용적인 경로를 제시한다.

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [51] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: 본 논문에서는 개별화된 과학자 프로필을 기반으로 한 다중 에이전트 시스템인 INDIBATOR를 제안하며, 이는 과학적 발견을 자동화하는 데 효과적임을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 에이전트 행동을 차별화하기 위해서는 과학자 개개인의 독특한 연구 경로를 반영해야 한다.

Method: INDIBATOR 프레임워크는 출판 이력과 분자 이력을 기반으로 개별화된 과학자 프로필을 생성하고, 이 에이전트들이 제안, 비판, 투표 단계를 통해 다중 차례의 토론에 참여한다.

Result: 평가 결과, 개별화된 에이전트들이 거친 키워드 기반의 페르소나에 의존하는 시스템보다 일관되게 우수한 성능을 보여주었으며, 경쟁력 있는 또는 최신 기술 수준의 성과를 달성했다.

Conclusion: 개별 에이전트의 '과학적 DNA'를 포착하는 것이 고품질 발견을 위해 필수적임을 검증하였다.

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [52] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: SOPRAG는 산업 SOP 검색 문제를 해결하기 위해 설계된 새로운 프레임워크로, 전문가 기반 접근 방식을 채택하여 SOP의 정확한 검색과 실행을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 산업 환경에서 운영 안전성과 일관성을 보장하기 위해 SOP가 필수적이지만, 기존의 검색 방식으로는 SOP를 효과적으로 검색하고 따르는 데 어려움이 있다.

Method: SOPRAG는 평면적으로 분할하는 대신 특정한 개체, 인과관계, 흐름 그래프 전문가를 활용하여 산업 구조적 및 논리적 복잡성을 해결한다. 또한, 검색 공간을 줄이는 Procedure Card 레이어와 동적으로 전문가 가중치를 조정하는 LLM-Guided gating 메커니즘을 제안한다.

Result: SOPRAG는 네 개의 산업 분야에서 광범위한 실험을 통해 기존의 강력한 RAG 기준선에 비해 검색 정확도와 응답 유용성 모두에서 유의미한 성능 개선을 보였으며, 실제 중요 작업에서 완벽한 실행 점수를 달성하였다.

Conclusion: SOPRAG는 산업 SOP 검색 및 실행에서의 주요 문제를 해결하며, 복잡한 환경에서의 프로세스 안전성을 향상시키는 잠재력을 가지고 있다.

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [53] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: LLM 기반 에이전트는 순차적 의사결정에서 우수한 성능을 보이지만, 경험 재사용이 부족해 불필요한 계산과 불안정성을 초래합니다. 이를 해결하기 위해 ProcMEM 프레임워크를 제안하여 에이전트가 상호작용 경험에서 절차적 기억을 자율적으로 학습할 수 있도록 합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 순차적 의사결정에서 더욱 효과적으로 작동하기 위해서는 경험의 재사용이 필요합니다.

Method: ProcMEM은 Skill-MDP를 형식화하여 패시브 에피소딕 내러티브를 실행 가능한 Skills로 변환하고, Non-Parametric PPO를 도입하여 고품질 후보 생성을 지원하며, Skill 검증을 위한 PPO Gate를 포함합니다.

Result: ProcMEM은 실험을 통해 우수한 재사용률과 성능 향상, 극단적인 기억 압축을 달성했습니다.

Conclusion: ProcMEM은 절차적 지식을 투명하게 축적, 정제 및 재사용하여 장기적인 자율성을 촉진합니다.

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [54] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: 이 논문에서는 도구 통합 추론(TIR) 모델의 한계를 극복하기 위해, LLM을 도구 사용자에서 도구 창조자로 변환하는 훈련 없는 새로운 프레임워크 UCT를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 TIR 모델은 외부 도구를 통합해 LLM의 질문-응답 능력을 확장하지만, 고정된 도구는 실제 환경에서의 요구를 충족하지 못하고, 잘못된 도구 출력을 LLM의 응답을 오도할 수 있다.

Method: UCT는 에이전트를 도구 창조자로 변환하여 추론 과정에서 적응형 도구 생성을 가능하게 하고, 경험적 메모리를 유지하기 위한 메모리 통합 메커니즘을 도입한다.

Result: 실험 결과, 멀티 도메인 수학 및 과학 추론 과제에서 +20.86% 및 +23.04%의 성능 향상을 보였으며, 에이전트의 자기 진화 능력을 검증한다.

Conclusion: 제안된 방법은 TIR 모델의 능력을 향상시키기 위한 새로운 패러다임으로 자리잡는다.

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [55] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 대화형 진단 시스템은 진단 지식 그래프를 탐색하여 정보를 보완하고 최종 진단에 도달하는 데 필요한 질문을 통해 가설을 검증하는 과정을 포함한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 대화형 진단 접근 방식은 모델의 파라메트릭 지식에 의존하거나 환자가 구체적이고 풍부한 정보를 제공한다고 가정하는데, 이는 비현실적이다.

Method: 우리의 시스템은 대화 맥락에서 진단 가설을 생성하고, 명확한 질문을 통해 이를 검증하는 두 단계로 이루어진다.

Result: 실험 결과, 강력한 기준선에 비해 진단 정확도와 효율성이 향상되었고, 의사들의 평가를 통해 시뮬레이터의 현실성과 생성된 질문의 임상 유용성이 지지되었다.

Conclusion: 곧 코드가 공개될 예정이다.

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [56] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 본 연구에서는 자연어 설명에서 최적화 모델을 자동으로 형성하는 문제를 다루며, 복합 제약조건과 적절한 모델링 패러다임의 필요성에 대한 기존의 LLM 기반 접근 방식의 한계를 극복하기 위해 Canonical Intermediate Representation (CIR)을 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 운영 연구에서 자연어 설명으로부터 최적화 모델을 자동으로 형성하는 것이 점점 더 중요한 주제가 되고 있으나, 현재의 LLM 기반 접근 방식은 복합 제약조건과 복잡한 운영 규칙에 필요한 적절한 모델링 패러다임에서 어려움을 겪고 있습니다.

Method: CIR은 문제 설명과 최적화 모델 간에 LLM이 명시적으로 생성하는 스키마로서, 운영 규칙의 의미를 제약 아키타입과 후보 모델링 패러다임을 통해 인코딩하여 규칙 논리를 수학적 구현과 분리합니다. 기존의 CIR 지식 기반을 바탕으로 R2C 프레임워크를 개발합니다.

Result: R2C는 새로 구성된 벤치마크에서 47.2%의 정확도로 최첨단 정확도를 달성하며, 기존의 문헌에 있는 벤치마크에서도 높은 경쟁 결과를 제공합니다.

Conclusion: R2C는 반영 메커니즘을 통해 추가 이점을 얻고 일부 벤치마크에서 새로운 최고 보고 결과를 설정합니다.

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [57] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 이 논문에서는 다중 에이전트 시스템을 기반으로 한 복잡한 워크플로우를 다루는 방식과 그 장점을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 다단계 워크플로우를 수행하는 LLM 기반 에이전트의 사용이 증가하는 가운데, 불확실성 관리 및 인적 감독과의 조정을 비교하고 관찰하는 것이 어려운 점을 해결하고자 한다.

Method: 유한 수평 마르코프 결정 프로세스(MDP)로 형식화된 다중 에이전트 시스템을 도입하여 각 에이전트는 특정 역할 또는 결정 단계를 담당하고, 에이전트 수준의 인지적 불확실성은 몬테카를로 추정기를 사용하여 정량화된다.

Result: 단일 에이전트 기반보다 향상된 성과를 보여주며, 정확도가 최대 19% 향상되고, 인적 검토 필요성이 최대 85배 감소하며, 일부 구성에서는 처리 시간이 단축되는 결과를 나타낸다.

Conclusion: 이 연구는 다중 에이전트 시스템이 복잡한 워크플로우에서 불확실성을 효과적으로 관리하고 개선된 성과를 제공할 수 있음을 보여준다.

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [58] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 이 논문은 자율성을 요구하는 조사 지능을 소개하고 이에 대한 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 대형 언어 모델(Agentic Large Language Models)의 기능은 단순한 질문에 대한 올바른 답변을 넘어서, 목표 설정과 탐색 결정의 자율성을 요구한다.

Method: 우리는 데이터베이스에서 LLM이 자율적으로 주요 인사이트를 추출하는 오픈 엔디드 과제인 Deep Data Research(DDR)와 검증 가능한 평가를 가능하게 하는 대규모 체크리스트 기반 벤치마크인 DDR-Bench를 소개한다.

Result: 최신 모델에서는 에이전시가 나타나지만, 장기 탐색은 여전히 도전 과제임을 보여준다.

Conclusion: 효과적인 조사 지능은 에이전트 구조나 단순한 규모 확장이 아니라, 에이전트 모델의 내재적 전략에 의존함을 강조한다.

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [59] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: 본 연구에서는 도구 사용 행동을 최적화하기 위한 보상 전략을 제안하고, 이러한 전략이 도구 사용 효율성과 성능을 향상시킴을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 기반으로 한 도구 사용 에이전트는 수학적 추론 및 다중 질문 응답과 같은 작업에서 뛰어난 성능을 보인다.

Method: 엔트로피 감소를 감독 신호로 활용하고, 도구 사용 행동을 최적화하기 위한 두 가지 보상 전략을 설계하였다.

Result: 희소한 결과 보상은 도구 호출을 평균 대비 72.07% 줄이는 반면, 밀집 과정 보상은 성능을 22.27% 향상시켰다.

Conclusion: 엔트로피 감소는 도구 사용 행동을 개선하는 핵심 메커니즘으로 자리잡았다.

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [60] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: Self-Improving Diffusion Agent (SIDiffAgent)는 훈련이 필요 없는 프레임워크로, 여러 모델을 활용하여 텍스트-이미지 생성의 여러 과제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 텍스트-이미지 확산 모델의 실용적인 배포는 여러 한계에 의해 제한되고 있습니다.

Method: SIDiffAgent는 Qwen 모델 패밀리를 활용하여 프롬프트 엔지니어링을 관리하고, 불량 생성물을 감지 및 수정하며 미세한 아티팩트를 제거합니다.

Result: SIDiffAgent는 GenAIBench에서 평균 VQA 점수 0.884를 기록하였고, 오픈 소스 및 독점 모델들과 에이전틱 방법들을 능가했습니다.

Conclusion: 우리는 연구가 승인되는 대로 코드를 공개할 예정입니다.

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [61] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 자율 LLM 에이전트의 성능을 개선하기 위한 테스트-시간 개선(CTI)과 관련된 평가 프레임워크 TIDE를 제안한다.


<details>
  <summary>Details</summary>
Motivation: TTI의 성공 또는 실패 원인을 이해하고, 기존 평가 지표의 한계를 해결하고자 한다.

Method: 에이전트와 환경에 독립적인 프레임워크 TIDE를 제안하여 TTI를 세 가지 차원으로 분석한다.

Result: TIDE는 수행의 제약이 순환 행동인지 누적된 기억인지 점검한다.

Conclusion: 에이전트와 환경 간의 상호작용 역학을 최적화하는 것이 필요하다.

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [62] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: 자기 발전 메모리 시스템인 Live-Evo는 온라인 학습을 통해 경험을 유지하고 업데이트하며, 작업에 적응한 지침을 제공하여 성과를 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 메모리 시스템은 정적인 데이터에만 의존하여 진정한 분포 변화에 취약합니다.

Method: Live-Evo는 경험 은행과 메타 지침 은행을 통해 경험의 활용 방식을 분리하여 동적으로 메모리를 관리합니다.

Result: Live-Evo는 10주 간의 Prophet Arena 벤치마크에서 Brier 점수를 20.8% 개선하고 시장 수익을 12.9% 증가시켰습니다.

Conclusion: Live-Evo는 강력한 기준선 대비 지속적인 성과 향상을 보여주는 심층 연구 벤치마크로도 전이됩니다.

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [63] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: 본 논문은 입력 오류에 대한 에이전트의 반응을 평가하는 Drift-Bench라는 새로운 기준을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 자율 에이전트로 전환됨에 따라 사용자 입력이 협력적인 가정을 자주 위반하여 실행 위험을 초래하고 있습니다.

Method: Drift-Bench는 다중 턴의 명확화를 통해 입력 오류 아래 에이전틱 화용론을 평가하는 진단 기준입니다.

Result: 실험 결과, 입력 오류가 발생할 경우 성능이 크게 저하되며, 명확화의 효과는 사용자 페르소나 및 오류 유형에 따라 다르게 나타났습니다.

Conclusion: 이 연구는 명확화 연구와 에이전트 안전성 평가를 연결하여 안전하지 않은 실행을 초래할 수 있는 실패를 체계적으로 진단할 수 있는 방법을 제공합니다.

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [64] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web는 복잡한 웹 인터페이스에서 장기 과제를 신뢰성 있게 수행하는 웹 에이전트로, 새로운 오픈 소스 상태의 최신 기술을 달성했다.


<details>
  <summary>Details</summary>
Motivation: 다양한 사용자 인터페이스 패러다임에서 강력하고 원활한 상호작용을 가능하게 하기 위해 기존 웹 에이전트의 한계를 극복하고자 함.

Method: Mixture of Grounding Experts, Experience-Imitation Planning, 작업 추적 체크리스트 및 적응형 메모리를 활용함.

Result: Avenir-Web는 Online-Mind2Web 벤치마크에서 이전 오픈 소스 에이전트를 크게 능가하고, 최고 수준의 독점 모델과 동등한 성능을 달성함.

Conclusion: Avenir-Web는 라이브 웹사이트에서 신뢰할 수 있는 웹 에이전트를 위한 새로운 오픈 소스 최신 기술을 확립함.

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [65] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 자동회귀 대형 언어 모델(LLM)은 복잡한 작업에서 상당한 성공을 거두었지만, 단순한 논리적 추론에서 여전히 실패할 수 있다. 본 논문에서는 훈련 데이터를 약간 수정하여 reversal curse를 완화할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 자동회귀 LLM의 논리적 추론 능력의 한계를 이해하고 향상시키기 위해.

Method: 훈련 데이터에 'Identity Bridge'라는 간단한 정규화 데이터를 추가하여 변화시킴.

Result: 1B 사전 훈련된 언어 모델이 제안된 데이터 레시피로 미세 조정되었을 때, reversal tasks에서 40%의 성공률을 기록.

Conclusion: 이 논문은 reversal curse에 대한 새로운 이론적 기초를 제공하며, LLM이 데이터로부터 더 높은 수준의 규칙을 학습하도록 장려하는 저비용 경로를 제시한다.

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [66] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: AI 에이전트의 실패를 분석하기 위해 115개의 실패 궤적을 수집하고 이를 기반으로 자동화된 진단 프레임워크 AGENTRX를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 실패는 위치를 파악하기 어렵고, 이를 해결하기 위해 실패 궤적을 수집할 필요가 있다.

Method: AGENTRX는 실패한 에이전트 궤적의 주요 실패 단계를 파악하기 위해 제약 조건을 종합하고 단계별로 평가한다.

Result: 이 프레임워크는 세 가지 도메인에서 단계 로컬라이제이션 및 실패 귀속을 기존 기준보다 개선했다.

Conclusion: AGENTRX는 에이전트 실패 분석의 효율성을 높이며, 향후 연구에 기여할 수 있다.

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [67] [Evolving Interpretable Constitutions for Multi-Agent Simulation](https://arxiv.org/abs/2602.00755)
*Ujwal Kumar,Alice Saito,Hershraj Niranjani,Rayan Yessou,Phan Xuan Tan*

Main category: cs.MA

TL;DR: 이 논문은 다중 에이전트 시스템에서 행동 규범을 자동으로 발견하는 '헌법 진화' 프레임워크를 제시하며, 개인과 집단의 복지 사이의 긴장을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 헌법 AI는 고정된 원칙을 사용하는 단일 모델 정렬에 초점을 맞추고 있지만, 다중 에이전트 시스템은 새로운 정렬 문제를 초래한다.

Method: survival pressure가 있는 그리드 월드 시뮬레이션을 사용하여, 생산성, 생존 및 갈등 메트릭을 결합한 사회 안정성 점수 S를 통해 개인과 집단 복지 간의 긴장을 연구한다.

Result: 진화된 헌법 C*는 S = 0.556 +/- 0.008을 달성하여 갈등을 없애고, 의사소통 최소화가 장황한 조정보다 우수함을 발견하였다.

Conclusion: 협력적 규범은 처방하기보다는 발견할 수 있다는 것을 보여준다.

Abstract: Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles ("be helpful, harmless, honest") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.

</details>


### [68] [Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning](https://arxiv.org/abs/2602.00766)
*Xiaoxue Yu,Rongpeng Li,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: 이 연구는 AI-native xG 네트워크를 위한 NetGPT 프레임워크를 제안하고, 자율적 추론과 전문가 에이전트 간의 작업 분담을 통해 협업을 최적화하는 방법을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다음 세대 무선 네트워크는 연결 중심 아키텍처에서 AI 네이티브 디자인으로의 패러다임 전환을 나타내며, 데이터, 컴퓨팅, 통신을 긴밀하게 통합해야 한다.

Method: 이 연구는 autonomous reasoning과 전문화된 에이전트에게 하위 작업을 위임하는 agentic communication을 활용하여 NetGPT 프레임워크를 제안한다.

Result: NetGPT는 협업 추론 전략을 지속적으로 개선할 수 있도록 에이전틱 강화학습을 통해 확장 가능하고 분산된 지능을 제공한다.

Conclusion: 이 연구는 복잡한 통신 환경에서 자율적인 감지, 추론 및 행동이 가능한 AI-native xG 네트워크를 위한 기초 아키텍처와 훈련 방법론을 제공한다.

Abstract: The evolution of next-Generation (xG) wireless networks marks a paradigm shift from connectivity-centric architectures to Artificial Intelligence (AI)-native designs that tightly integrate data, computing, and communication. Yet existing AI deployments in communication systems remain largely siloed, offering isolated optimizations without intrinsic adaptability, dynamic task delegation, or multi-agent collaboration. In this work, we propose a unified agentic NetGPT framework for AI-native xG networks, wherein a NetGPT core can either perform autonomous reasoning or delegate sub-tasks to domain-specialized agents via agentic communication. The framework establishes clear modular responsibilities and interoperable workflows, enabling scalable, distributed intelligence across the network. To support continual refinement of collaborative reasoning strategies, the framework is further enhanced through Agentic reinforcement learning under partially observable conditions and stochastic external states. The training pipeline incorporates masked loss against external agent uncertainty, entropy-guided exploration, and multi-objective rewards that jointly capture task quality, coordination efficiency, and resource constraints. Through this process, NetGPT learns when and how to collaborate, effectively balancing internal reasoning with agent invocation. Overall, this work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action in complex communication environments.

</details>


### [69] [Symphony-Coord: Emergent Coordination in Decentralized Agent Systems](https://arxiv.org/abs/2602.00966)
*Zhaoyang Guan,Huixi Cao,Ming Zhong,Eric Yang,Lynn Ai,Yongxin Ni,Bill Shi*

Main category: cs.MA

TL;DR: Symphony-Coord는 분산형 다중 에이전트 프레임워크로, 에이전트 선택을 온라인 다중 무장 강도 문제로 변환하여 자연스럽게 역할을 생성한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 다단계 작업을 수행하기 위해 현재의 조정 메커니즘은 정적으로 할당된 역할과 중앙 집중식 제어기가 필요하지만, 이는 비효율적인 라우팅과 적응력 저하, 취약한 오류 복구를 초래한다.

Method: Symphony-Coord는 에이전트 선택을 온라인 다중 무장 강도 문제로 변환하고, 두 단계의 동적 비콘 프로토콜을 사용하여 경량 후보 선별 기제와 적응형 LinUCB 선택기를 포함한다.

Result: 시뮬레이션 실험 및 실제 대규모 언어 모델 벤치마크를 통해, Symphony-Coord는 작업 라우팅 효율성을 향상시키고 자가 치유 능력을 입증하였다.

Conclusion: 제도적 역할이 없는 확장 가능한 조정 메커니즘을 통해 시스템이 최적 배분 방식에 수렴함을 보여준다.

Abstract: Multi-agent large language model systems can tackle complex multi-step tasks by decomposing work and coordinating specialized behaviors. However, current coordination mechanisms typically rely on statically assigned roles and centralized controllers. As agent pools and task distributions evolve, these design choices lead to inefficient routing, poor adaptability, and fragile fault recovery capabilities. We introduce Symphony-Coord, a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling roles to emerge organically through interaction. The framework employs a two-stage dynamic beacon protocol: (i) a lightweight candidate screening mechanism to limit communication and computational overhead; (ii) an adaptive LinUCB selector that routes subtasks based on context features derived from task requirements and agent states, continuously optimized through delayed end-to-end feedback. Under standard linear realizability assumptions, we provide sublinear regret bounds, indicating the system converges toward near-optimal allocation schemes. Validation through simulation experiments and real-world large language model benchmarks demonstrates that Symphony-Coord not only enhances task routing efficiency but also exhibits robust self-healing capabilities in scenarios involving distribution shifts and agent failures, achieving a scalable coordination mechanism without predefined roles.

</details>


### [70] [Multi-Agent Teams Hold Experts Back](https://arxiv.org/abs/2602.01011)
*Aneesh Pappu,Batu El,Hancheng Cao,Carmelo di Nolfo,Yanchao Sun,Meng Cao,James Zou*

Main category: cs.MA

TL;DR: 자율적인 협력자로서 자유롭게 상호작용하는 다중 에이전트 LLM 시스템에 대한 연구.


<details>
  <summary>Details</summary>
Motivation: 자율적인 협력자로서 다중 에이전트 LLM 시스템의 효과적인 조정이 어떻게 이루어지는지 연구하기 위함.

Method: 조직 심리학을 기반으로 자가 조직화된 LLM 팀이 강력한 시너지를 달성하는지 연구.

Result: 자가 조직화된 LLM 팀이 전문가의 성과에 미치지 못하며, 성능 손실이 최대 37.6%에 달하는 것을 발견.

Conclusion: 자가 조직화된 다중 에이전트 팀이 구성원의 집단 전문성을 활용하는 데 있어 큰 간극이 있음을 보여준다.

Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.

</details>


### [71] [A-MapReduce: Executing Wide Search via Agentic MapReduce](https://arxiv.org/abs/2602.01331)
*Mingju Chen,Guibin Zhang,Heng Chang,Yuchen Guo,Shiji Zhou*

Main category: cs.MA

TL;DR: A-MapReduce는 넓은 검색 작업에 적합한 다중 에이전트 실행 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 에이전틱 프레임워크는 폭넓은 검색 작업에서 비효율적이며, 이러한 한계를 극복하기 위해 A-MapReduce를 제안한다.

Method: A-MapReduce는 작업 적응형 분해와 구조화된 결과 집계를 통해 대규모 검색 대상을 병렬 처리한다.

Result: 광범위한 에이전트 벤치마크에서 A-MapReduce는 최첨단 성능을 달성하고 평균 Item F1을 5.11%에서 17.50% 개선하였다.

Conclusion: A-MapReduce는 비용 효율적이며, 대표적인 다중 에이전트 기준에 비해 실행 시간을 45.8% 단축하였다.

Abstract: Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.

</details>


### [72] [Evidence-Decision-Feedback: Theory-Driven Adaptive Scaffolding for LLM Agents](https://arxiv.org/abs/2602.01415)
*Clayton Cohn,Siyuan Guo,Surya Rayala,Hanchen David Wang,Naveeduddin Mohammed,Umesh Timalsina,Shruti Jain,Angela Eeds,Menton Deweese,Pamela J. Osborn Popp,Rebekah Stanton,Shakeera Walker,Meiyi Ma,Gautam Biswas*

Main category: cs.MA

TL;DR: 이 논문은 다중 에이전트 LLM 아키텍처를 통해 개인화된 교육 지원을 제공하기 위한 EDF(증거-결정-피드백) 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 LLM 아키텍처는 학생들이 도메인 지식을 구축하고 비판적 사고 기술을 개발하도록 돕는 기회를 제공합니다. 그러나 많은 시스템이 '모든 사람에게 맞는' 방식으로 작동하여 개인화된 지원을 제공하는 능력을 제한합니다.

Method: 이 논문에서는 LLM을 사용한 적응형 스캐폴딩을 위한 이론적 프레임워크인 EDF를 소개하고, 이를 STEM+C 문제 해결을 위한 협력적 동료 에이전트인 Copa를 통해 구현합니다.

Result: 고등학교 교실에서의 실제 연구를 통해 EDF에 맞춘 상호작용이 학생들의 이해도와 과제 숙달에 맞춰 피드백을 조정하고, 점진적인 스캐폴딩 감소를 촉진하며, 과도한 의존성 없이 해석 가능한 증거 기반 설명을 지원함을 보여주었습니다.

Conclusion: 이 연구는 EDF 프레임워크가 어떻게 개인화된 교육 지원과 학습 과정의 개선을 촉진하는지 보여줍니다.

Abstract: Multi-agent LLM architectures offer opportunities for pedagogical agents to help students construct domain knowledge and develop critical-thinking skills, yet many operate on a "one-size-fits-all" basis, limiting their ability to provide personalized support. To address this, we introduce Evidence-Decision-Feedback (EDF), a theoretical framework for adaptive scaffolding using LLMs. EDF integrates elements of intelligent tutoring systems and agentic behavior by organizing interactions around evidentiary inference, pedagogical decision-making, and adaptive feedback. We instantiate EDF through Copa, an agentic collaborative peer agent for STEM+C problem-solving. In an authentic high school classroom study, we show that EDF-aligned interactions align feedback with students' demonstrated understanding and task mastery; promote gradual scaffold fading; and support interpretable, evidence-grounded explanations without fostering overreliance.

</details>


### [73] [TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.01665)
*Hayeong Lee,JunHyeok Oh,Byung-Jun Lee*

Main category: cs.MA

TL;DR: TABX라는 재구성 가능한 멀티 에이전트 작업을 위한 고속 샌드박스를 소개하며, 이를 통해 MARL 알고리즘의 평가와 개발을 지원한다.


<details>
  <summary>Details</summary>
Motivation: MARL 알고리즘의 개발 및 평가에서 환경 설계의 중요성을 강조하고, 기존 벤치마크의 모듈화 부족 문제를 해결하고자 함.

Method: 고속 샌드박스인 TABX를 도입하여 환경 매개변수에 대한 세밀한 제어를 제공하고, 다양한 작업 복잡성에 대한 에이전트 행동과 알고리즘 간의 거래를 체계적으로 조사함.

Result: TABX는 GPU에서 하드웨어 가속 실행을 위해 JAX를 활용하여 대규모 병렬 처리를 가능하게 하고 계산 오버헤드를 크게 줄임.

Conclusion: TABX는 복잡한 구조적 도메인에서 MARL 에이전트를 연구하는 데 필요한 빠르고 확장 가능하며 쉽게 사용자화할 수 있는 프레임워크를 제공함.

Abstract: The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.

</details>


### [74] [Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study](https://arxiv.org/abs/2602.02170)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

TL;DR: 본 논문은 Self-Evolving Coordination Protocols(SECP)의 탐색적 시스템 실행 가능성을 연구하며, 이는 제한된 외부 검증된 자기 수정이 가능하면서 고정된 형식 불변성을 유지하는 조정 프로토콜이다.


<details>
  <summary>Details</summary>
Motivation: 안전이 중요한 금융과 같은 규제된 분야에서 이들 조정 메커니즘은 엄격한 형식적 요건을 충족해야 하며, 감사 가능하고 명시적으로 경계가 설정된 범위 내에서 작동해야 한다.

Method: 여섯 개의 고정 비잔틴 합의 프로토콜 제안이 여섯 개의 전문 결정 모듈에 의해 평가되는 제어된 개념 증명 설정에서 SECP를 연구한다.

Result: 단일 재귀 수정이 두 개에서 세 개의 수용된 제안으로 커버리지를 증가시키면서 모든 선언된 불변성을 보존했다.

Conclusion: 조정 프로토콜의 경계 있는 자기 수정을 기술적으로 구현 가능하고 감사 가능하며 명시적 형식 제약 하에서 분석 가능함을 보여준다.

Abstract: Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.
  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.
  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.
  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [75] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All은 시민들이 지리 공간의 공개 정부 데이터를 보다 효과적으로 활용할 수 있도록 지원하는 투명하고 감사 가능하며 재현 가능한 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 시민들의 지리 공간 공개 정부 데이터와의 상호작용을 향상시키기 위해.

Method: 대형 언어 모델을 기반으로 하여 의미론적 데이터 검색, 반복 코드 생성을 위한 에이전틱 추론, 검증 가능한 다중모델 출력을 생성하는 안전한 샌드박스 실행을 결합한 시스템.

Result: 199개의 질문 벤치마크에서 98%의 분석 정확도와 94%의 재현율을 달성하였으며, 데이터에 의해 뒷받침되지 않는 질문을 신뢰성 있게 거부하여 환상 위험을 최소화하였다.

Conclusion: 이 접근 방식은 LLM이 공공 데이터에 대한 설명 가능한 다중모드 접근을 제공할 수 있음을 보여주며, 개방적 거버넌스를 위한 신뢰할 수 있는 AI를 진전시킨다.

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [76] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: 이 논문은 ELLMPEG라는 에지 기반의 에이전트 LLM 프레임워크를 제안하여 비디오 처리 명령을 자동으로 생성하며, 외부 클라우드 API에 대한 의존성을 제거하고 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 기반 LLM의 한계 극복 및 에지기반 AI의 발전을 활용하여 효율적인 비디오 처리 명령 생성 필요.

Method: ELLMPEG는 도구 인식 RAG와 반복적 자기 반성을 통합하여 에지에서 직접 실행 가능한 FFmpeg 및 VVenC 명령을 생성하고 검증한다.

Result: Qwen2.5는 ELLMPEG 프레임워크로 보강했을 때 평균 78%의 명령 생성 정확도를 달성하며, 모든 오픈 소스 모델을 능가함.

Conclusion: ELLMPEG는 비디오 처리 명령 자동 생성에서 새로운 기준을 제시하며, 클라우드 API 비용 없이도 효과적인 성능을 보여준다.

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [77] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 본 연구는 인도적 지원 및 재해 구호를 위한 효율적인 Retrieval-Augmented Generation (RAG) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 효과적인 인도적 지원 및 재해 구호(HADR)를 위해서는 신속한 상황 이해, 신뢰할 수 있는 의사 결정 지원, 그리고 다양한 재해 맥락에서 일반화할 수 있는 능력이 필요하다.

Method: 이 연구는 구조적 멀티모달 검색 트리를 생성하기 위해 텍스트 재해 매뉴얼, 과거 사례, 공중 및 지상 이미지를 통합한 계층적 지식 기반을 구축한다.

Result: 실제 재해 데이터셋에서 실험 결과, 개선된 상황 이해, 향상된 작업 분해 정확도, 긴급 운영을 위한 우수한 사용성을 보여준다.

Conclusion: 적응형 검색 증강 생성 방식 및 자기 추론, 멀티모달 사고 능력을 통해 상당한 성과를 달성했다.

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [78] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 이 논문은 장비 유지보수 전략을 능동적으로 전환하는 조건 기반 유지보수(CBM)를 제안하며, 다중 장비 CBM을 위한 새로운 분포적 강화 학습 접근 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 시간 기반 유지보수 방식은 불필요한 비용과 예기치 않은 장비 고장을 초래하는 문제를 해결하기 위해, CBM을 통해 실시간 장비 상태 데이터를 활용한 유지보수 타이밍 개선이 필요하다.

Method: 이 연구에서는 노후화 요인을 통합한 양자 회귀 심층 Q-네트워크(QR-DQN)를 이용하여 다중 장비 CBM을 위한 새로운 분포적 강화 학습 접근 방식을 제안하며, 세 가지 전략적 시나리오를 통해 다수의 펌프 유닛을 동시에 관리한다.

Result: 3000회의 훈련 에피소드에 대한 종합적인 실험 검증을 통해 모든 전략에서 상당한 성과 개선이 입증되었다. Safety-First 전략은 3.91의 투자수익률(ROI)을 달성하여 대안들보다 152% 더 우수한 성과를 보이며, 31% 더 높은 투자만을 요구한다.

Conclusion: 이 시스템은 95.66%의 운영 안정성을 보여주며 산업 환경에 즉시 적용 가능하다.

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [79] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 이 논문은 대형 AI 모델의 교육 비용과 긴 개발 주기를 해결하기 위한 새로운 교육 방법론인 SCPL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기업 정보 시스템에서 대규모 AI 모델의 채택은 높은 교육 비용과 긴 개발 주기로 인해 자주 방해받는다.

Method: SCPL은 BP를 분리하고 긴 그래디언트 흐름을 여러 개의 짧은 흐름으로 변환하는 새로운 훈련 방법론이다.

Result: SCPL은 BP, Early Exit, GPipe 및 AL과 비교하여 효율성과 효과성을 입증합니다.

Conclusion: SCPL은 성능의 근본적인 병목 현상을 완화하여 조직이 더 비용 효율적이고 민첩하게 고급 정보 시스템을 개발하고 배포할 수 있는 실용적인 경로를 제공합니다.

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [80] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: 이 논문은 Low-Rank Adaptation(LoRA) 기법이 라벨 노이즈에 저항하는 특성을 설명하는 이론적 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 사전 훈련 모델을 조정하는 데 있어 파라미터 효율적인 미세 조정 방법의 필요성을 강조하고, LoRA의 라벨 노이즈 저항 특성을 탐구한다.

Method: LoRA의 성질을 분석하여 세 가지 주요 통찰력을 도출하고, RACT(랭크 인식 커리큘럼 훈련) 방법을 제안한다.

Result: 실험을 통해 RACT가 AG News에서 노이즈 감지에 대해 91.1%의 F1 점수를 기록하고 91.46%의 정확도로 경쟁력을 입증한다.

Conclusion: LoRA가 노이즈에 저항하는 능력과 RACT를 통한 노이즈 감지의 유용성을 강조하며, 이 방법이 기존의 기법들과 비교해 유의미한 성과를 낸다.

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [81] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: ECCO는 해석 가능한 추론과 조합 검색을 연결하는 프레임워크로, 최적화 결정의 인과 논리를 학습하도록 돕습니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 블랙박스 검색 방법과 최근의 대형 언어 모델 접근 방식 간의 격차를 해소하기 위한 필요성.

Method: 체인 오브 사고 데이터셋을 구성하기 위한 리버스 엔지니어링 방법론과 LLM을 전략가로 활용하는 협력적 추론 메커니즘 설계.

Result: ECCO는 7개 데이터셋에서 LLVM opt -O3 기준에 비해 평균 24.44% 사이클 감소를 기록하며 크게 향상된 성능을 보여줍니다.

Conclusion: ECCO는 컴파일러 자동 조정 분야에서 해석 가능한 추론과 효율적인 최적화를 가능하게 합니다.

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [82] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: 본 논문은 다양한 후보 솔루션을 생성하는 여러 에이전트를 활용하여 LLM의 추론 성능을 향상시키는 새로운 방법인 ALIGN을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM은 단일 생성 및 선택 파이프라인에 의존할 때 복잡한 추론 작업에서 성능이 저하되는 경향이 있습니다.

Method: ALIGN 방법은 LLM 추론을 정렬된 위임 게임으로 모델링하며, 주요 당사자가 여러 에이전트에 작업을 위임하고 이들이 생성한 후보 솔루션을 바탕으로 최종 답변을 선택합니다.

Result: ALIGN은 후보 솔루션에 대한 동등한 접근을 보장하는 공정한 비교에서 단일 에이전트 생성에 비해 기대 성능을 증명 가능하게 개선합니다.

Conclusion: 광범위한 LLM 추론 벤치마크에서 실험 결과, ALIGN이 강력한 단일 에이전트 및 앙상블 기준보다 일관되게 우수한 성능을 보임을 입증하였습니다.

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [83] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: GASP는 결과 검증만을 사용하여 복원 기능을 훈련시키는 강건화 방법이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 RLVR 모델은 깨끗한 조건에서만 최종 답변의 정확성을 최적화하므로, 잘못된 맥락에서는 재해를 일으킬 수 있다.

Method: GASP는 단일 모델 내에서 오염물질이 실패를 유도하고, 에이전트가 같은 오염된 조건에서 진단하고 회복하는 적대적 자기 플레이 게임을 형성한다.

Result: GASP는 1.5B~8B의 네 가지 오픈웨이트 모델에서 강력하지만 취약한 추론기를 강건한 모델로 변화시킨다.

Conclusion: GASP는 잘못된 맥락에서도 견디는 강건한 모델을 만들고, 청정 정확성을 종종 개선한다.

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [84] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: EPIAGENT는 전통적인 전염병 모델링의 한계를 극복하고, 질병 진행을 반복적인 프로그램 합성 문제로 모델링함으로써 자동으로 역학 시뮬레이터를 합성, 보정, 검증 및 정제하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 전염병 모델링 접근 방식은 고정된 모델 클래스에 의존하여, 병원체, 정책 및 시나리오 가정이 변화할 때마다 수동으로 재설계해야 하는 불편함이 있다.

Method: EPIAGENT는 명시적인 역학 흐름 그래프 중간 표현을 통해 시나리오 명세와 모델 구조를 연결하고, 코드 생성 전에 모듈식 검증을 가능하게 한다. 검증된 흐름 그래프는 물리적 및 역학적 제약 조건 하에서 해석 가능한 매개변수 학습을 지원하는 기계적 모델로 컴파일된다.

Result: 역학 시나리오 사례 연구에 대한 평가 결과, EPIAGENT는 복잡한 성장 역학을 포착하고 다양한 백신 접종 및 면역 회피 가정에 대해 역학적으로 일관된 반사적 프로젝션을 생성한다.

Conclusion: 결과는 에이전틱 피드백 루프가 전문가의 작업 흐름을 모방함으로써 모형의 퇴화를 방지하고 유효한 모델로의 수렴을 상당히 가속화함을 보여준다.

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [85] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: AI 기반의 접근점(AP) 배치를 위한 새로운 접근 방식으로, 확산 샘플러가 효과적인 생성적 추론 모델로 제안됨.


<details>
  <summary>Details</summary>
Motivation: AP 배치의 복잡성을 해결하기 위해 기존 LLM의 한계를 극복할 필요가 있음.

Method: 통합 보상 함수를 기반으로 한 생성적 추론 모델을 연구하고, 확산 샘플러가 대안 방법보다 일관되게 우수함을 입증함.

Result: 확산 프로세스가 보상 경관을 매끄럽게 하고 날카롭게 하여 샘플링을 개선함.

Conclusion: 통합 보상 함수를 이용한 확산 기반의 생성적 추론이 실내 AP 배치 계획을 위한 확장 가능하고 도메인 비특정적 기초를 제공함.

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [86] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: Co-RedTeam은 보안 인식 다중 에이전트 프레임워크로, 실제 레드 팀 작업 흐름을 반영해 취약성 분석을 조정된 발견 및 이용 단계로 분해하여 에이전트들이 실행 피드백을 바탕으로 계획하고 실행하며 검증할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 접근 방식은 자동 취약성 발견 및 이용에 어려움을 겪고 있으며, 이는 제한된 상호 작용, 약한 실행 기반 및 경험 재사용 부족 때문이다.

Method: Co-RedTeam은 보안 도메인 지식, 코드 인식 분석, 실행 기반의 반복적 추론, 장기 메모리를 통합하여 설계된 다중 에이전트 프레임워크이다.

Result: Co-RedTeam은 다양한 백본 모델에서 강력한 기준선을 초과하여 항상 우수한 성능을 보였으며, 취약성 이용에서 60% 이상의 성공률과 취약성 탐지에서 10% 이상의 절대 개선을 달성하였다.

Conclusion: 모델은 실행 피드백, 구조화된 상호 작용 및 메모리의 중요한 역할을 확인하였다.

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [87] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 다중 모달 감정 분석을 위한 새로운 방법 TEMSA 소개 및 실험 결과.


<details>
  <summary>Details</summary>
Motivation: 다중 모달 감정 분석은 텍스트와 이미지 간의 차이, 감정의 모호성, 맥락의 복잡성으로 인해 도전 과제가 많음.

Method: 객체 인식 방법에 기반한 TEMSA를 사용하여 이미지에서 감지된 모든 객체의 이름을 추출하고 관련 텍스트와 결합.

Result: TEMS를 사용하여 다중 모달 데이터의 전반적인 감정에 대해 더 나은 결과를 도출함.

Conclusion: 이 연구는 다중 모달 감정 분석을 발전시키고, 이미지와 텍스트 데이터를 결합한 TEMSA의 효율성을 제시함.

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [88] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델이 자율 에이전트로 진화하면서 발생하는 적대적 실패를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 도구 향상 환경에서의 안전성 평가를 주관적인 NLP 작업에서 객관적인 제어 문제로 전환하기 위해 이 문제를 연구했습니다.

Method: Slingshot이라는 '콜드 스타트' 강화 학습 프레임워크를 사용하여 자율적으로 발생하는 공격 벡터를 발견합니다.

Result: Slingshot은 Qwen2.5-32B-Instruct-AWQ 운영자에 대해 67.0%의 성공률을 기록하며, 첫 성공까지의 시도를 52.3에서 1.3으로 줄였습니다.

Conclusion: 이 연구는 Tag-Along Attack을 1급 검증 가능한 위협 모델로 확립하고 환경 상호작용을 통해 효율적인 공격이 오픈-웨이트 모델에서 유도될 수 있음을 보여줍니다.

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [89] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 이 논문은 확산 대형 언어 모델(D-LLMs)의 안전성을 분석하고 그 한계를 확인하였다.


<details>
  <summary>Details</summary>
Motivation: D-LLMs는 자가 회귀 LLMs(AR-LLMs)에 대한 대안을 제공하며, 생성 효율성에서 장점을 보인다. 또한, D-LLMs는 AR-LLMs를 위해 설계된 jailbreak 공격에 대해 내재적 강인성을 제공하는 안전성의 장점을 나타낸다.

Method: 우리는 D-LLMs의 내재적 강인성의 메커니즘을 분석하며, 확산 경로가 점진적인 안전한 생성 억제 효과를 유도함을 보여준다. 그러나 이는 절대적이지 않다.

Result: 우리는 harmful한 요청이 구조적으로 무해한 맥락에 내포되어 'context nesting'으로 알려진 실패 모드를 찾았다. 실험적으로, 이 단순한 전략이 D-LLMs의 안전성을 우회할 수 있음을 보여주었고, 다양한 모델과 벤치마크에서 최고의 공격 성공률을 달성하였다.

Conclusion: 결과적으로, D-LLMs의 안전性의 기원과 한계를 규명하였으며, 이를 통해 D-LLMs의 초기 단계의 레드팀을 구성하였다.

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [90] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 이 논문은 대리 모델(Agency Model)과 공간적 작업을 통합하는 새로운 분류 체계를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 대리적 추론과 계획을 위한 큰 언어 모델들이 성공적으로 사용되고 있지만, 이들이 물리적 세계에 적용되는 데는 한계가 있습니다.

Method: 2,000편 이상의 논문을 면밀히 검토하여, 대리적 능력과 공간적 작업을 연결하는 통합된 3축 분류법을 제안합니다.

Result: 우리는 세 가지 주요 발견을 통해 이 통합 과정을 설명하고, 각 축별로 연결된 내용을 보여줍니다.

Conclusion: 연구의 미래 방향성과 표준화된 평가 프레임워크의 필요성을 강조하며, 다음 세대의 공간을 인식하는 자율 시스템 개발을 위한 기초를 마련합니다.

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [91] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: 본 논문은 Job Shop Scheduling Problem (JSSP)을 해결하기 위한 새로운 Variational Graph-to-Scheduler (VG2S) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 제조업에서 작업 효율성과 자원 활용도를 직접적으로 영향을 미치는 JSSP는 중요한 과제이다.

Method: 이 논문은 JSSP 영역에 최초로 변분 추론을 도입하고 최대 엔트로피 강화 학습을 기반으로 한 확률적 목표를 도출한다. 이는 표현 학습과 정책 최적화를 수학적으로 분리하여 이루어진다.

Result: VG2S 프레임워크는 에이전트가 변분 그래프 인코더를 통해 스케줄링 인스턴스의 강건한 구조적 표현을 학습할 수 있게 한다.

Conclusion: 제안된 방법은 최신 DRL 기준 및 전통적인 파견 규칙에 비해 특히 대규모 및 도전적인 벤치마크 인스턴스에서 우수한 제로샷 일반화를 보여준다.

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [92] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: GRPO는 대규모 언어 모델의 추론 능력을 향상시키기 위한 접근법으로, 개인화된 데이터에 대한 RL 정렬의 한계를 극복하고자 FedMOA라는 연합 GRPO 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 추론 능력을 개선하고 개인화된 데이터에 대한 요구를 충족하기 위한 새로운 연구가 필요하다.

Method: FedMOA는 비동기 가중치 조정 메커니즘을 통해 각기 다른 보상 하에 멀티오브젝트 정렬을 가능하게 하는 연합 GRPO 프레임워크이다.

Result: FedMOA는 수학적 추론 및 코드 생성 벤치마크에서 연합 평균을 일관되게 능가하며, 정확도를 최대 2.2% 향상시키는 결과를 낸다.

Conclusion: FedMOA는 글로벌 성능, 개인화 및 멀티오브젝트 균형을 개선하며, 연합 학습의 도전 과제를 해결한다.

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [93] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: SIERL은 강화 학습에서 탐험을 적극적으로 안내하는 새로운 방법으로, 에이전트의 학습 진척도에 따라 서브 목표를 설정합니다.


<details>
  <summary>Details</summary>
Motivation: 희소 보상이 있는 환경에서의 탐험은 강화 학습의 근본적인 도전 과제입니다.

Method: 에피소드마다 SIERL은 에이전트의 알려진 상태 공간의 경계에서 서브 목표를 선택한 후, 메인 작업 목표를 향해 탐험을 계속합니다.

Result: SIERL은 희소 보상이 있는 환경에서 진행된 실험에서 기존의 주요 기준선을 초월하며, 메인 작업 목표 달성과 임의의 상태 도달에서 우수한 성과를 보였습니다.

Conclusion: 서브 목표 선택 메커니즘은 적절한 상태-행동 쌍을 제공하여 에이전트가 경계 내의 모든 상태에 도달할 수 있도록 보장합니다.

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [94] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: 이 논문에서는 확산 언어 모델(DLMs)이 자동으로 올바른 채우기 길이를 찾아내는 능력을 갖추고 있음을 보여줍니다. 이를 위해 제안된 방법인 CAL을 통해 성능을 개선했습니다.


<details>
  <summary>Details</summary>
Motivation: DLM의 성능은 미리 지정된 채우기 길이에 의해 제한되므로 자동으로 채우기 길이를 탐색할 수 있는 방법이 필요합니다.

Method: 본 연구에서는 노이즈 제거 신뢰도의 첫 단계에서 두 가지 주요 통계적 현상인 지역적인 Oracle Peak와 시스템적인 Length Bias를 밝히고, 이를 활용하여 CAL 방법을 통해 최적의 길이를 효율적으로 탐색할 수 있도록 합니다.

Result: CAL 방법은 고정 길이 기준선에 비해 최대 47.7%의 Pass@1 개선을 보였으며, 코드 채우기에서 채팅 기반 적응 방법에 비해 40.5% 향상되었습니다. 또한 텍스트 채우기에서 BLEU-2와 ROUGE-L을 각각 최대 8.5%와 9.9% 향상시켰습니다.

Conclusion: 결과적으로 CAL은 전문적인 훈련 없이도 강력한 DLM 채우기를 가능하게 합니다.

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [95] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 이 논문은 호흡 신호에서 수면 뇌파(EEG)를 합성하는 새로운 생리학 간 변환 작업을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 호흡 신호와 EEG 간의 복잡성 차이를 해소하고자 하는 필요.

Method: 파형 조건부 생성 프레임워크를 제안하여 호흡 역학을 보존하면서 EEG 목표 공간을 이산 토큰화로 제약.

Result: 28,000명 이상을 대상으로 훈련된 모델이 EEG 스펙트로그램 재구성에서 7%의 평균 절대 오차를 달성.

Conclusion: 합성된 EEG는 연령 추정, 성별 탐지 및 수면 분류 작업에서 실제 EEG와 유사한 성능을 보이며, 비접촉식 센싱을 통해 원거리 신경학적 평가의 가능성을 보여준다.

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [96] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: 모델 프리, 게임 비의존적, 그래디언트 비사용의 독립적 보상 기반 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 확률적 게임에 대한 새로운 학습 접근 방식의 필요성.

Method: 최고 응답형 액터-비평가 아키텍처를 따르며, 두 가지 비평가로부터 피드백을 받아 전략 업데이트를 수행한다.

Result: 무한 지평선에서 두 대리인 제로섬 및 다수의 대리인 동일 이익 확률적 게임에서 (근사적) 균형으로 수렴함을 보인다.

Conclusion: 제안된 방법은 이론적 보장이 있는 첫 번째 보상 기반 완전 분산형 학습 알고리즘 중 하나다.

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [97] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: LocalV는 모듈형 하드웨어 설계에서 정보를 지역적으로 활용하는 다중 에이전트 프레임워크로, RTL 코드 생성을 용이하게 한다.


<details>
  <summary>Details</summary>
Motivation: 디지털 하드웨어 설계에서 RTL 코드를 생성하는 과정은 필수적이지만 노동 집약적인 단계로, 엔지니어들이 복잡한 사양을 수천 줄의 합성 가능한 HDL 코드로 수동 변환해야 한다.

Method: LocalV는 긴 문서에서 긴 코드 생성 문제를 짧은 문서와 짧은 코드 작업 세트로 분해하며, 계층적 문서 분할, 작업 계획, 지역화된 코드 생성, 인터페이스 일관성을 유지한 병합, AST 기반의 지역 인식 디버깅을 통합한다.

Result: LocalV는 RealBench의 IP 수준 Verilog 생성을 위한 실험에서 기존의 SOTA LLM 및 에이전트보다 상당히 우수한 성능을 보이며, 패스율은 45.0%에 달한다.

Conclusion: LocalV는 산업 IP 수준 설계 작업에 적합한 확장 가능한 코드 생성 및 디버깅을 가능하게 한다.

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [98] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 언어 모델의 미세 조정에서 발생하는 비정상적인 행동을 방지하기 위한 기계적 접근 방식을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델이 좁은 범위의 감독된 목표에 대해 미세 조정될 때 emergent misalignment가 발생할 수 있다.

Method: 내부 특성을 식별하고 이를 미세 조정 중 강화하지 않도록 모델을 유도하는 접근 방식을 사용한다.

Result: 여섯 개의 미세 조정 도메인에서 고정된 특성을 차단함으로써 emergent misalignment를 95%까지 줄일 수 있다.

Conclusion: 훈련 시간에 대한 목표된 제약이 타겟 작업 성능을 저하시키지 않고 emergent misalignment를 완화할 수 있음을 보여준다.

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [99] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX는 공간 이질성을 분석하기 위한 자동화된 기계 학습 및 설명 가능한 인공지능 기술을 통합한 Python 패키지입니다.


<details>
  <summary>Details</summary>
Motivation: 지리적 위치와 맥락에 맞는 기계 학습 모델을 자동으로 선택하고 최적화하기 위한 필요성이 있었습니다.

Method: SHAP 분석을 통해 해석 가능성을 유지하면서 회귀 및 분류 작업에 대해 모델을 최적화합니다.

Result: 기존의 GALAX 프레임워크에 비해 자동 대역폭 선택과 유연한 커널 함수 선택 기능을 통해 공간 모델링의 유연성과 강인성이 향상되었습니다.

Conclusion: PyGALAX는 지리학, 도시 계획, 환경 과학 및 관련 분야의 연구자와 실무자들이 접근할 수 있도록 복잡한 공간 관계에 대한 투명한 통찰력을 생성합니다.

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [100] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: 이 논문은 의료 분야에 특화된 효율적이고 경량화된 심층 학습 아키텍처를 종합적으로 정리하고, 최신 모델을 CNN, 경량 트랜스포머, 선형 복잡도 모델로 분류하며, 모델 압축 전략을 평가하여 진단 성능을 유지하는 동시에 하드웨어 요구사항을 줄이는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 심층 학습이 의료 이미지 분석에서 중요한 역할을 하고 있지만, 대규모 모델을 실제 임상 환경에 배포하는 것은 높은 계산 비용, 지연 제약, 환자 데이터 프라이버시 문제로 인해 도전과제가 되고 있다.

Method: 의료 분야에 특화된 효율적이고 경량화된 심층 학습 아키텍처를 정리하고, CNN, 경량 트랜스포머, 선형 복잡도 모델의 세 가지 주요 흐름으로 분류하며, 모델 압축 전략을 논의한다.

Result: 모델 압축 전략을 통해 진단 성능을 유지하면서 하드웨어 요구사항을 줄이는 데 효과적임을 평가하였다.

Conclusion: 현재의 한계를 인식하고 장치 내 인텔리전스 전환에 대해 논의함으로써 고성능 AI와 자원이 제약된 임상 환경 간의 격차를 해소하려는 연구자 및 실무자에게 로드맵 역할을 한다.

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [101] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: AI 모델의 투명성과 설명가능성을 개선하기 위한 엔지니어링 이미지 기반 바이오마커의 필요성을 다루는 연구.


<details>
  <summary>Details</summary>
Motivation: 컴퓨터 병리학의 발전에도 불구하고, 많은 AI 모델은 여전히 해석하기 어려운 블랙박스이며, 이는 임상적 채택에 큰 장벽이 된다.

Method: SAGE라는 AI 시스템을 도입하여 생물학적 증거에 근거해 해석 가능한 병리 바이오마커를 식별한다.

Result: SAGE는 문헌 기반 추론과 다중 데이터 분석을 통합하여 이미지에서 추출한 특징과 분자 바이오마커를 상관관계 짓는다.

Conclusion: SAGE는 투명하고 생물학적으로 뒷받침된 바이오마커를 우선시하며, 컴퓨터 병리학의 임상적 번역을 진전시킨다.

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [102] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 본 논문은 대형 언어 모델의 지식을 체계적으로 추출하고 정량화하기 위한 상호작용 에이전틱 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 보유한 지식의 내용을 명확히 이해하고, 그 지식의 한계를 확장하기 위해 필요하다.

Method: 모델이 보유한 지식을 다양한 세분화 수준에서 탐색하기 위한 네 가지 적응형 탐색 정책과 지식 품질을 보장하는 세 단계의 지식 처리 파이프라인을 포함한다.

Result: 재귀적 분류법이 가장 효과적인 탐색 전략으로 확인되었으며, 대형 모델이 일관되게 더 많은 지식을 추출하는 명확한 지식 확장 법칙을 관찰하였다.

Conclusion: 훈련 데이터 구성의 차이가 모델 패밀리 간에 뚜렷하고 측정 가능한 지식 프로필을 초래한다는 것을 보여준다.

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [103] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: FoSTA는 라벨 정보를 활용하여 다양한 데이터 세트를 정렬하는 새로운 정렬 프레임워크로, 기존의 유클리드 기하학 접근법의 한계를 극복한다.


<details>
  <summary>Details</summary>
Motivation: 라벨 기반의 방법론을 활용하여 멀티모달 데이터 세트를 정렬하는 데 있어, 기존의 유클리드 기하학 모델이 가지는 한계를 극복하기 위함이다.

Method: FoSTA는 숲에 의해 유도된 기하학을 활용하여 내부 도메인 구조의 잡음을 제거하고, 정렬 전에 작업 관련 매니폴드를 복구하는 정렬 프레임워크이다.

Result: FoSTA는 기존의 기준과 비교하여 합의 회수 및 라벨 전송을 개선하며, 합성 벤치마크에서 우수한 성능을 보여준다.

Conclusion: FoSTA는 배치 수정 및 생물학적 보존을 포함한 실제 단일 세포 응용 프로그램에서 뛰어난 성능을 발휘한다.

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [104] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: FLood는 비IID 데이터로 인한 문제를 해결하여 연합 학습의 안정성과 정확성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 분산 서비스 노드 간의 협동 모델 학습을 가능하게 하여 지능형 서비스 시스템의 핵심 요소인 연합 학습은 비공식적인 데이터의 특성으로 인해 발생하는 문제에 직면해 있습니다.

Method: FLood는 비공식적 분포 감지를 기반으로 한 새로운 연합 학습 프레임워크로, 이중 가중치 메커니즘을 통해 지역 훈련과 글로벌 집계를 동시에 조절합니다.

Result: FLood는 비IID 환경에서 다양한 벤치마크 실험 결과, 기존 최첨단 연합 학습 방법들보다 뛰어난 정확도와 일반화 능력을 보여줍니다.

Conclusion: FLood는 기존 연합 학습 알고리즘과 통합할 수 있는 모듈로, 핵심 최적화 논리를 수정하지 않고도 이질성 하에서 성능을 향상시킵니다.

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [105] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: LRAgent는 다중 LoRA 에이전트를 위한 KV 캐시 공유 프레임워크로, 메모리 및 계산 오버헤드를 줄입니다.


<details>
  <summary>Details</summary>
Motivation: 다중 LLM 에이전트 시스템에서 에이전트들이 자율적으로 KV 캐시를 구축하면서 발생하는 메모리 및 계산 오버헤드를 해결하고자 합니다.

Method: LRAgent는 사전 훈련된 가중치로부터의 공유 기본 구성 요소와 LoRA 가중치로부터의 어댑터 의존 성분으로 캐시를 분해합니다.

Result: LRAgent는 메모리 및 계산 오버헤드를 줄이고, 에이전트 간 중복 계산을 피하여 성능을 향상시켰습니다.

Conclusion: LRAgent는 전체 공유 캐싱에 가까운 처리량과 첫 번째 토큰 대기 시간을 달성하며, 정확도는 비공유 캐싱 기준선과 유사합니다.

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [106] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 본 논문에서는 연속 스케일링의 성능을 향상시키기 위한 체계적인 프레임워크를 제안하며, 이를 통해 MarkovScale이라는 효율적인 시스템을 개발하여 최적의 정확도와 효율성 균형을 달성한다.


<details>
  <summary>Details</summary>
Motivation: 연속 스케일링은 주목받는 추론 시 스케일링 패러다임이지만, 성능 향상이 제한적이며 최적성의 경계가 명확하지 않다.

Method: 연속 스케일링을 두 상태 마코프 과정으로 모델링하는 체계적인 프레임워크를 제안하고, 이를 통해 정확성이 향상되는 조건 및 이론적 성능 경계를 도출한다.

Result: MarkovScale은 3개의 주요 LLM, 5개의 벤치마크, 20개 이상의 구성에서 실험을 통해 현재의 병렬 및 연속 스케일링 방법들을 일관되게 초월하는 성과를 보인다.

Conclusion: MarkovScale은 LLM의 최적화 및 자원 효율적인 추론을 향한 중요한 진전을 나타낸다.

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [107] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 그래프 신경망(GNN)의 성능 한계를 해결하기 위한 연구.


<details>
  <summary>Details</summary>
Motivation: GNN은 구조화된 데이터에서 표현 학습의 강력한 도구로 자리잡았지만, 일반화, 적대적 교란에 대한 강건성, 표현 학습의 효율성에서 여러 도전에 직면해 있다.

Method: 그래프 시프트 연산자(GSO)를 기반으로 한 새로운 표현 학습 기법 개발, 그래프 데이터 증대를 통한 일반화 향상 방법 도입, 적대적 공격에 대한 노이즈 기반 방어 및 직교화 기법을 활용한 더 강건한 GNN 개발.

Result: 각 기여를 통해 GNN의 성능을 향상시키고, 이론적 한계 및 잠재력에 대한 더 깊은 이해를 제공한다.

Conclusion: 이 연구는 GNN의 한계와 가능성을 명확히 하고, 향후 연구 방향을 제시한다.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [108] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 이 연구는 신경 음성 디코더의 내부 표현을 기계적 해석 가능성을 사용하여 원인적으로 조사한다.


<details>
  <summary>Details</summary>
Motivation: 음성 인식 및 생성의 다양한 양식 간에 정보를 전달하는 메커니즘이 덜 탐구된 상태이다.

Method: 음성 모드 간의 내부 활성화의 교차 모드 패칭 및 세 가지 모드의 보간을 사용하여 음성 표현의 변화를 분석하였다.

Result: 소규모의, 분산되지 않은 뉴런의 서브셋이 교차 모드 전이에 영향을 미친다는 것을 발견하였다.

Conclusion: 음성 모드는 공유된 연속적인 원인 매니폴드 위에 존재하며, 교차 모드 전이는 광범위한 활동이 아닌, 응집력 있는 층 특유의 서브공간에 의해 매개된다.

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [109] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: Mixture-of-World Models (MoW)는 다양한 작업 동작을 효과적으로 캡처하고 샘플 효율성을 개선하기 위해 설계된 하이브리드 모델이다.


<details>
  <summary>Details</summary>
Motivation: 다양한 관측과 동작을 가진 작업들이 존재하는 시각적 도메인에서 다중 작업 강화 학습(MTRL)의 샘플 효율성을 달성하는 것은 근본적인 도전 과제이다.

Method: MoW는 작업 적응형 시각 압축을 위한 모듈형 변분 오토인코더, 작업 조건 전문가와 공유된 백본을 갖춘 하이브리드 Transformer 기반 동작 모델, 효율적인 매개변수 할당을 위한 그래디언트 기반 작업 클러스터링 전략을 결합한 확장 가능한 아키텍처이다.

Result: MoW 에이전트는 26개의 Atari 게임에서 한 번 훈련하여 평균 110.4%의 인류 정규화 점수를 달성했으며, 이는 26개의 작업 특정 모델로 구성된 STORM의 114.2% 점수와 경쟁할 만하다.

Conclusion: 이 결과들은 MoW가 일반적인 세계 모델을 위한 확장 가능하고 매개변수 효율적인 기초를 제공함을 증명한다.

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [110] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 본 논문은 다양한 서비스의 상충하는 의도를 지원하는 자율 통신 네트워크를 위한 에이전틱 AI 시스템을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 자율적으로 작동하면서 다양한 서비스의 상충하는 의도를 지원하는 통신 네트워크의 필요성이 증가하고 있다.

Method: 본 논문에서는 세 가지 전문화된 에이전트를 중심으로 한 에이전틱 AI 시스템을 제안한다. 감독 해석 에이전트는 자연어 모델을 활용하여 의도를 실행 가능한 최적화 템플릿으로 변환하고, 피드백 및 네트워크 조건에 따라 인지적 정제를 수행한다. 최적화 에이전트는 이 템플릿을 해결 가능한 최적화 문제로 변환하여 목표 간의 트레이드오프를 분석하고 선호도를 도출한다. 마지막으로, 다목적 강화 학습을 기반으로 한 선호 기반 제어 에이전트는 이러한 선호도를 활용하여 네트워크 성능의 파레토 경계 근처에서 최상의 원래 의도를 만족시키며 작동한다.

Result: 제안된 시스템은 네트워크가 다양한 의도와 네트워크 조건을 자율적으로 해석하고 대응하며 행동할 수 있도록 한다.

Conclusion: 본 연구는 자율 통신 네트워크의 고급 의도를 구체적인 제어 행동으로 변환하는 데 기여한다.

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [111] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: 본 연구에서는 Bayesian Last Layers (BLLs)의 한계를 극복하고 개선하기 위한 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 BLLs는 마지막 층에만 베이지안 처리를 적용하여 초기 층에서 발생하는 불확실성을 간과하므로 인식적 불확실성을 과소평가합니다.

Method: 우리는 Neural Tangent Kernel (NTK) 기능의 프로젝션을 마지막 층 기능이 스팬하는 공간에 활용하여 BLLs를 개선하는 방법을 제안합니다.

Result: 제안한 방법은 표준 BLL보다 더 크거나 같은 후방 분산을 제공하여 인식적 불확실성을 과소평가하는 경향을 수정합니다.

Conclusion: 우리는 UCI 회귀, 맥락 밴딧, 이미지 분류 및 이미지와 표형 데이터세트에서의 분포 외 탐지 작업에 대한 실증적 평가를 통해 제안한 방법이 표준 BLL 및 경쟁 기준보다 개선된 보정 및 불확실성 추정치를 제공함을 보입니다.

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [112] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: 이 논문은 TRAK 알고리즘의 이론적 분석을 제공하며, 알고리즘의 성능을 특성화하고 근사에서 발생하는 오류를 정량화한다.


<details>
  <summary>Details</summary>
Motivation: AI 모델의 해석을 위한 데이터 귀속의 필요성을 다룬다.

Method: TRAK 알고리즘의 이론적 분석을 통해 성능을 특성화하고 오류를 정량화한다.

Result: TRAK의 추정된 영향력은 원래의 영향력과 높은 상관관계를 유지한다.

Conclusion: TRAK의 근사는 상당한 오류를 발생시키지만, 데이터 포인트의 상대적 순위를 대체로 보존한다.

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [113] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: OddSHAP는 Shapley 값을 효율적으로 근사하는 새로운 일관된 추정기를 제안하며, 최적의 추정 정확도를 달성한다.


<details>
  <summary>Details</summary>
Motivation: Shapley 값의 정확한 계산이 일반적으로 불가능하여 효율적인 근사 방법이 필요하다.

Method: OddSHAP는 홀수 부분에 대해 다항 회귀를 수행하며, 푸리에 기초를 이용해 해당 부분을 분리하고 높은 영향을 미치는 상호작용을 식별하기 위해 프록시 모델을 사용한다.

Result: OddSHAP는 벤치마크 평가를 통해 최첨단 추정 정확도를 달성한다.

Conclusion: OddSHAP는 Shapley 값의 홀수 구성 요소에 대한 의존성을 기반으로 효율성을 극대화한다.

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [114] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 협동 다중 에이전트 강화 학습을 보상 없는 탐색의 설정에서 연구하며, 여러 에이전트가 함께 미지의 MDP를 탐색하여 동적을 학습하는 방법을 논의한다.


<details>
  <summary>Details</summary>
Motivation: 보상 없는 탐색 환경에서 에이전트들이 어떻게 동적으로 최적의 정책을 학습할 수 있는지를 이해하고자 한다.

Method: 각 학습 단계에서 여러 에이전트가 독립적으로 환경과 상호작용하며, 각 에이전트는 정책을 부여받고 실행한 후 결과 궤적을 관찰한다.

Result: 학습 단계 수가 수평선 $H$와 같을 때, 동적의 $ε$ 근사를 얻기 위해 $	ilde{O}(S^6 H^6 A / ε^2)$ 에이전트를 사용하는 계산 효율적인 알고리즘을 제시한다.

Conclusion: 에이전트 수를 다항식으로 제한했을 때 $H$ 단계의 학습이 필수적임을 보인다.

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [115] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 테스트 시간 스케일링은 대형 언어 모델의 추론 능력을 향상시키는 중요한 방법으로 자리잡았다.


<details>
  <summary>Details</summary>
Motivation: 직관적인 'Best-of-$N$' 전략이 성능 향상을 보여주었지만, $N$ 선택, 예산 할당 및 다단계 의사결정에 대한 원칙적인 지침이 부족하여 최적화의 여지가 남아 있다.

Method: 보상 꼬리 분포를 추정하여 LLM의 스케일링 법칙을 예측하는 Tail-guided search를 통해 새로운 방법론을 제안한다.

Result: SLG가 완전 정보 오라클에 비해 사라지는 후회의 성과를 이루며, BoN을 사용할 때 필요한 계산 예산보다 작은 예산으로도 기대 보상을 달성했다.

Conclusion: 꼬리 기반 할당이 동일한 계산 예산 하에 Best-of-$N$보다 높은 보상 수익을 지속적으로 달성한다는 것을 경험적으로 검증했다.

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [116] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: 다이나믹 시스템을 위한 데이터 기반 대체 모델이 빠르지만, 기계학습 모델들이 고주파 성분을 왜곡하는 문제를 지적하며, 이를 해결하기 위해 다중 스케일 웨이브렛 변환기를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기계학습 기반 모델이 고주파 성분을 왜곡하여 장기 예측에서 불안정성을 초래하는 문제를 해결할 필요가 있다.

Method: 웨이브렛 변환을 통해 저주파와 고주파 콘텐츠를 명시적으로 분리하고, 고주파를 보존하는 다운샘플링 방식을 사용하여 스케일과 주파수 대역에서 종속성을 포착한다.

Result: 혼돈적인 다이나믹 시스템에 대한 실험에서 오류 감소와 개선된 장기 스펙트럼 충실도를 보여준다.

Conclusion: ERA5 기후 재분석에서 기후 편향을 추가로 줄이며 실제 예측 환경에서의 효과를 입증한다.

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [117] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 유전 프로그래밍 기반 특징 구성은 학습 성능 향상에 성공했으나, 과적합이 여전히 문제로 남아 있습니다. 이 논문에서는 데이터를 왜곡하거나 혼합하여 얻은 Vicinal Risk가 경험적 리스크와 정규화 항의 합으로 제한된다는 것을 증명합니다. 이를 바탕으로 경험적 리스크와 Vicinal Jensen Gap을 공동 최적화하는 진화적 특징 구성 프레임워크를 제안합니다. 실험 결과는 Jensen Gap 최소화의 효과성을 보이며, 제안된 과적합 제어 전략이 탁월한 성능을 가져옵니다.


<details>
  <summary>Details</summary>
Motivation: 유전 프로그래밍 기반 특징 구성이 최근 몇 년간 학습 성능 향상의 자동화된 기계 학습 기술로서 상당한 성공을 거두었으나, 과적합은 그 적용을 제한하는 문제로 남아 있습니다.

Method: 본 논문에서는 소음 교란 또는 믹스업 기반 데이터 증대를 통해 추정한 Vicinal Risk가 경험적 리스크와 정규화 항(유한 차분 또는 Vicinal Jensen Gap)의 합으로 제한된다는 것을 증명합니다. 이를 활용하여 경험적 리스크와 Vicinal Jensen Gap을 공동 최적화하는 진화적 특징 구성 프레임워크를 제안합니다.

Result: 58개의 데이터셋에 대한 실험 결과는 다른 복잡성 측정 방법들과 비교했을 때 Jensen Gap 최소화의 효과성을 보여줍니다.

Conclusion: 15개의 기계 학습 알고리즘과의 비교는 제안된 과적합 제어 전략을 가진 유전 프로그래밍이 뛰어난 성능을 달성함을 나타냅니다.

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [118] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: 이 연구는 장기 기억 기능을 갖춘 대규모 언어 모델에서 암묵적 편향이 어떻게 축적되고 전파되는지를 분석하며, 이를 위해 암묵적 편향을 양적화할 수 있는 새로운 벤치마크를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 장기 기억 메커니즘이 대규모 언어 모델에 지속성과 개인화를 유지하게 하지만, 공정성과 관련된 새로운 위험을 초래할 수 있습니다.

Method: Decision-based Implicit Bias (DIB) 벤치마크라는 새로운 데이터셋을 도입하고, 이를 사용해 여섯 개의 최첨단 LLM을 평가합니다.

Result: LLM의 암묵적 편향이 정적이지 않고, 시간이 지남에 따라 강해지며 관계 없는 도메인 간에 전파된다는 것을 발견했습니다.

Conclusion: DMT라는 동적 메모리 태깅 기법을 통해 편향 축적을 현저히 줄이고 도메인 간 편향 전파를 효과적으로 억제할 수 있다는 결과를 도출했습니다.

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [119] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: 본 연구는 모바일 GUI 성능을 개선하기 위한 새로운 비주얼 월드 모델링 패러다임을 제안하며, 이를 통해 정확한 텍스트 렌더링과 고화질 비주얼 생성을 동시에 달성한다.


<details>
  <summary>Details</summary>
Motivation: 모바일 그래픽 사용자 인터페이스 (GUI) 에이전트의 성능을 향상시키는 것.

Method: 비주얼 월드 모델링을 위한 렌더링 코드 생성을 통해 GUI의 다음 상태를 웹 코드로 예측하는 단일 비전-언어 모델 (VLM)을 제안한다.

Result: gWorld라는 새로운 오픈-웨이트 비주얼 모바일 GUI WMs를 소개하고, 이 모델은 4개의 배급 내 및 2개의 배급 외 벤치마크에서 새로운 파레토 경계를 설정하여 50.25배 더 큰 8개의 오픈-웨이트 모델을 능가한다.

Conclusion: gWorld는 훈련 데이터의 의미 있는 이점을 가져오며, 모델링 강화를 통해 모바일 GUI 정책 성능을 개선함. 각 구성 요소가 데이터 품질을 개선하는 역할도 한다.

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [120] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 이 논문은 대형 언어 모델의 상호작용 성능 평가에서 인터페이스 의존성과 도구 사용 메커니즘을 구분하기 위한 PIPE라는 새로운 평가 프로토콜을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 인터랙티브 에이전트로 평가될 때, 기존 벤치마크는 도구 사용과 인터페이스 의존성을 혼합하고 있습니다.

Method: 환경 인터페이스를 최소한으로 재작성하면서 과업의 의미와 실행 행동을 보존하는 PIPE를 제안합니다.

Result: 16개의 환경에서 PIPE는 경로 기반 학습이 인터페이스 단축을 크게 강화한다는 것을 보여주었습니다.

Conclusion: 인터페이스 의존성은 환경에 따라 다르고 표준 평가에서 보이지 않는 비선형 훈련 동태를 나타냅니다.

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [121] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 이 연구에서는 새로운 무감독 기술 발견 프레임워크인 SUSD를 소개하며, 환경의 구성을 요소로 분해하여 기술 발견 과정을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기술 관리를 지원할 수 있는 더 복잡하고 동적인 기술 세트를 발견할 필요가 있다.

Method: 상태 공간을 독립적인 요소로 분해하고, 각 요소에 특정 기술 변수를 할당하여 기술 발견 과정을 개선하는 SUSD 프레임워크를 제안한다.

Result: SUSD는 다양한 복잡한 기술을 무감독으로 발견하며, 기존 방식보다 성능이 우수하다.

Conclusion: SUSD는 환경의 구성 구조를 활용하여 보다 풍부하고 다양한 기술을 발견할 수 있게 하며, 계층 강화 학습(HRL)을 통해 개별 엔티티에 대한 세밀한 제어를 가능하게 한다.

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [122] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: 적응형 비일관성 점수와 동형 임계값의 공동 온라인 적응을 통해 로봇의 예측 지역을 최적화하는 AdaptNC 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자율 시스템이 제약이 없는 환경에서 안전하게 배치되기 위해서는 엄격한 불확실성 정량화가 필수적입니다.

Method: AdaptNC는 비일관성 점수 매개변수와 동형 임계값을 함께 조정하는 프레임워크를 제안합니다.

Result: AdaptNC를 사용하면 예측 지역의 부피가 줄어들면서 목표 커버리지 수준을 유지할 수 있습니다.

Conclusion: 우리의 연구 결과 AdaptNC가 기존의 임계값만 사용하는 방법에 비해 예측 지역의 부피를 크게 줄임을 보여줍니다.

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [123] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: 온라인 역선형 최적화에서 에이전트의 숨겨진 목표 벡터를 추론하는 방법을 연구합니다. 우리는 M-볼록 집합에서 유한한 후회 경계를 도출합니다.


<details>
  <summary>Details</summary>
Motivation: 온라인 역선형 최적화에서 에이전트의 숨겨진 목표를 잘 추론하여 추천 행동의 성능을 높이는 것이 목표입니다.

Method: M-볼록 집합에서 최적해의 구조적 특성과 기하학적 볼륨 주장을 결합하여 유한한 후회 경계를 도출합니다.

Result: 유한한 후회 경계 $O(d	ext{log} d)$를 입증하고, 최대 $C$ 라운드의 적대적으로 손상된 피드백에 대해 후회경계 $O((C+1)d	ext{log} d)$를 얻습니다.

Conclusion: 이 연구는 에이전트의 진정한 목표에 대한 행동 추천의 성능을 개선하고, 유한한 후회 경계를 달성할 수 있는 가능성을 보여줍니다.

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [124] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: Collaborative Memory Transformer (CoMeT)는 긴 맥락 처리를 위한 혁신적인 아키텍처로, 일정한 메모리 사용량과 선형 시간 복잡도로 임의의 긴 시퀀스를 처리할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 표준 Transformer의 이차 복잡성과 무한히 성장하는 키-값 캐시는 긴 맥락 처리에 큰 장벽이 된다.

Method: CoMeT는 효율적인 플러그인 모듈로 설계되어 최소한의 세부 조정으로 미리 훈련된 모델에 통합될 수 있다. FIFO 큐에서 최근 이벤트를 위한 임시 메모리와 장기 의존성을 위한 게이트 업데이트 규칙이 있는 글로벌 메모리로 구성된 이중 메모리 시스템을 운영한다.

Result: CoMeT를 장착하고 32k 맥락으로 미세 조정된 모델은 1M 토큰 시퀀스 내에서 임의의 위치에서 패스키를 정확하게 검색할 수 있다. SCROLLS 벤치마크에서 CoMeT는 다른 효율적인 방법을 능가하고 요약 작업에서 풀 어텐션 기준과 유사한 성과를 달성했다.

Conclusion: 실제 세계의 에이전트와 사용자 행동 QA 작업에서 CoMeT의 효과가 추가로 검증되었다.

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [125] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 이 논문에서는 에이전트 주도 시계열 예측(ATSF)을 제안하며, 이를 통해 예측을 에이전틱 프로세스로 재구성하고 예측 모델에 국한되지 않고 에이전틱 워크플로우를 조직하는 방식으로 시계열 예측의 기초를 제시하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 시계열 예측 방식은 정보 추출 및 적응이 필요한 상황에서 부족함을 드러냈다.

Method: ATSF는 예측을 인지, 계획, 행동, 반성 및 기억으로 구성된 에이전틱 프로세스로 재구성한다.

Result: ATSF는 예측 워크플로우를 조직하여 도구와 상호작용하고 결과에 따른 피드백을 반영하며 경험을 축적하면서 발전할 수 있다.

Conclusion: 향후 연구에 있어 시계열 예측의 기초로 ATSF를 확립하는 것을 목표로 한다.

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [126] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 본 연구는 스토캐스틱 그래디언트 계산을 고차원 추정 문제로 формулиру하고, Stein 규칙 축소 기반의 의사결정 이론적 프레임워크를 도입하여 노이즈가 있는 미니 배치 그래디언트를 안정된 제한 추정량으로 조정하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 고차원 설정에서 적절한 그래디언트 추정을 통해 스토캐스틱 그래디언트 방법의 최적화를 도모하고자 함.

Method: 노이즈가 있는 미니 배치 그래디언트를 이력적 모멘텀에서 유도된 안정된 제한 추정량으로 조정하는 축소 그래디언트 추정기를 구성함.

Result: 제안된 추정기가 일반적인 스토캐스틱 그래디언트보다 우수한 성능을 발휘하며, Adam 옵티마이저에 통합 가능한 알고리즘을 제공함.

Conclusion: 현대 딥러닝에서 스토캐스틱 그래디언트 추정 개선을 위한 원칙적이고 효과적인 접근 방법을 제시함.

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [127] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: Grad2Reward는 Judge 모델 추론 과정을 통해 밀집 프로세스 보상을 추출하여 강화 학습의 효율성과 추론 품질을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning with Verifiable Rewards는 수학 및 프로그래밍과 같은 검증 가능한 도메인에서 복잡한 LLM 추론에 중대한 breakthroughs를 촉발했습니다.

Method: Grad2Reward라는 새로운 프레임워크를 도입하여 Judge의 모델 추론 과정에서 단일 역방향 패스를 통해 밀집 프로세스 보상을 추출합니다.

Result: Grad2Reward로 최적화된 정책은 다양한 개방형 작업에서 뛰어난 성과를 달성합니다.

Conclusion: Grad2Reward는 훈련 효율성과 추론 품질을 크게 향상시키며, 자가 평가 메커니즘을 도입하여 정책이 스스로 평가 신호를 통해 개선될 수 있도록 합니다.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [128] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 저비용 센서 하드웨어로 정밀한 제어를 달성하는 새로운 딥러닝 프레임워크를 제안.


<details>
  <summary>Details</summary>
Motivation: 소비자 접근성을 제한하는 복잡하고 밀집된 다중 센서 배열에 의존하지 않기 위한 필요성.

Method: 스파스 두 채널 표면 전자기학(sEMG)을 최적화한 하이브리드 변환기와 Time2Vec 학습 가능한 시간 임베딩을 활용.

Result: 이 프레임워크는 10개 운동 세트에서 95.7%의 F1 스코어를 달성하여 기존 모델을 초월함.

Conclusion: 높은 충실도의 시간 임베딩이 낮은 공간 해상도를 보완할 수 있음을 입증하며, 밀집 센싱의 필요성을 도전한다.

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [129] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 이 논문은 재생 버퍼의 경험 우선 순위를 정하는 데 비전-언어 모델을 활용하여 강화 학습의 샘플 효율성 및 성공률을 높이는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델과 비전-언어 모델의 발전이 강화 학습의 샘플 효율성, 고수준 계획, 해석 가능성을 향상시킬 수 있는 새로운 기회를 제공하고 있다.

Method: 비전-언어 모델을 활용하여 강화 학습의 재생 버퍼 경험 우선 순위를 자동으로 평가하고 정리하는 방법을 제안한다.

Result: 제안된 방법을 사용한 에이전트는 과거 접근 방식에 비해 평균 성공률이 11-52% 더 높고 샘플 효율성이 19-45% 향상되었다.

Conclusion: 이 연구는 비전-언어 모델이 경험의 우선 순위를 정하는 데 있어 유용할 수 있으며, 강화 학습의 성능을 크게 향상시킬 수 있음을 보여준다.

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [130] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: 본 논문은 컴파일러 최적화를 위해 다중 대형 언어 모델(LLM)의 협업 추론이 단일 대형 모델의 성능에 필적하거나 초과할 수 있는지를 탐구하고, COLT라고 불리는 경량 협업 다중 LLM 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 모델 제공 비용이 지배적이므로, 컴파일러 최적화가 확장 가능한 배포를 위해 필수적이다.

Method: COLT라는 경량 협업 다중 LLM 프레임워크를 제안하며, 이는 단일 몬테카를로 트리 탐색(MCTS) 과정 내에서 여러 모델 간의 조정된 추론을 가능하게 한다.

Result: COLT 프레임워크는 여러 모델 간의 협업 기초로서 단일 공유 MCTS 트리를 사용하여 변환 접두사의 재사용과 모델 간 값 전파를 가능하게 한다.

Conclusion: COLT는 전통적인 외부 계획자 및 다중 LLM 사용의 필요성을 줄이며, 각 반복에서 작용하는 LLM이 조합된 행동을 제안하도록 하여 효율적인 최적화를 실현한다.

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [131] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 이 논문은 제로샷 강화 학습에서 오프 정책 문제를 해결하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 오프 정책 학습 방법은 고정된 이전 상호작용 데이터셋으로부터 최적의 정책을 직접 도출하는 것을 목표로 한다.

Method: 이 논문에서는 잔여 측정값과 정상 밀도 비율 간의 이론적 연결을 발견하여 제로샷 환경에서 오프 정책 문제를 해결한다.

Result: 우리 알고리즘은 최적의 중요도 샘플링 비율을 추론할 수 있어, 어느 작업에 대해서도 실시간으로 최적 정책에 대한 정상 분포 수정을 수행한다.

Conclusion: 이 기술은 새로운 작업에 대한 빠른 적응을 가능하게 하여, 오프 정책 학습과 제로샷 적응 간의 교량 역할을 한다.

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [132] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 본 연구는 LLM 에이전트가 단순히 과거 성공 사례를 바탕으로 진화하는 대신, 실패 사례에서 교훈을 얻고 이를 통해 지속적으로 발전할 수 있는 새로운 자기 진화 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 문제 해결 능력은 인상적이지만, 이들은 평생 상호작용을 통해 진화할 수 있는 능력이 부족하다.

Method: 대조적인 반영 전략을 통해 오류가 발생하기 쉬운 패턴을 요약하고 재사용 가능한 통찰력을 포착하며, 비모수적 텍스트 경험을 압축 가능 학습 매개변수로 증류하는 자기 응집 메커니즘을 제안한다.

Result: 우리의 방법은 LLM 에이전트의 장기적인 진화에서 장점을 보여준다.

Conclusion: 제안된 프레임워크는 LLM 에이전트가 과거의 실패를 교훈으로 삼아 지속적으로 발전하도록 지원한다.

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [133] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 본 연구에서는 LLM의 성능 저하 없이 압축을 통해 성능 개선을 위한 IntraSlice 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)은 다양한 작업에서 높은 성능을 보이지만, 시스템에 배포할 때 모델의 크기 때문에 어려움을 겪는다.

Method: We propose IntraSlice framework that uses block-wise module-intra PCA compression pruning, allowing transformation matrices to be fully fused into the model without extra parameters.

Result: 우리는 Llama2, Llama3 및 Phi 시리즈와 같은 다양한 언어 벤치마크에서 우리의 방법을 검증하였고, 실험 결과 우리 접근 방식이 동일한 압축 비율이나 추론 속도에서 최근의 기준선들과 비교하여 우수한 압축 성능을 달성함을 보여준다.

Conclusion: IntraSlice는 LLM의 압축과 성능 개선을 동시에 이뤄낼 수 있는 효과적인 방법이다.

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [134] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: SNAP은 상호 동의에 기반한 견고한 계산을 위한 자기 지도 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 견고한 계산을 위한 신뢰할 수 있는 항목에 비중을 두고 이상치를 줄이기 위해 SNAP을 도입했다.

Method: SNAP은 동의 신뢰성 가설에 기반하여 비모니터링 및 사전 지식 없이 동의 정도를 정량화하는 가중치를 할당한다.

Result: SNAP의 주요 결과는 고차원 환경에서도 이상치가 계산에 미치는 기여를 무시할 수 있도록 보장하는 이상치 가중치의 지수 억제이다.

Conclusion: SNAP은 견고한 계산을 위한 유연하고 사용하기 쉬운 광범위한 응용 접근 방식을 제공한다.

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [135] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 본 연구는 Chain-of-Thought의 역학에 대한 이전의 보완적 관찰에서 비롯되었으며, 대형 언어 모델은 CoT가 나타나기 전에 이후 추론의 잠재적 계획을 보여주어 명시적 CoT의 중요성을 감소시킵니다. 그러나 CoT는 다단계 추론이 필요한 작업에 여전히 중요합니다. 우리는 LLM의 내부 상태와 언어화된 추론 경로 간의 이해를 심화하고자 하며, 다양한 작업 도메인에 대해 숨겨진 상태에 적용한 우리의 탐색 방법인 Tele-Lens를 통해 LLM의 잠재적 계획 강도를 조사합니다. 우리의 실증 결과는 LLM이 정확한 글로벌 계획 없이 주로 점진적 전이를 수행하는 근시안적 시야를 보인다는 것을 나타냅니다. 우리는 이 특성을 활용하여 CoT의 불확실성 추정 개선에 대한 가설을 제안하며, CoT의 작은 하위 집합이 전체 경로의 불확실성을 효과적으로 나타낼 수 있음을 검증합니다. 우리는 CoT 역학을 활용하는 중요성을 강조하며, 성능 저하 없이 CoT 우회를 자동으로 인식할 수 있음을 보여줍니다. 우리의 코드, 데이터 및 모델은 https://github.com/lxucs/tele-lens에서 공개되었습니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 추론 과정에서 Chain-of-Thought의 중요성과 그 역학을 이해하고자 하였습니다.

Method: Tele-Lens라는 탐색 방법을 통해 다양한 작업 도메인에서 LLM의 숨겨진 상태에 적용하여 잠재적 계획 강도를 조사하였습니다.

Result: LLM은 주로 정확한 글로벌 계획 없이 점진적 전이를 수행하는 근시안적 시야를 보입니다.

Conclusion: CoT의 불확실성 추정 개선을 위한 가설을 제안하고, 작은 하위 집합이 전체 경로를 대표할 수 있음을 검증하였으며, CoT 역학을 활용하는 것의 중요성을 강조하면서 성능 저하 없이 인식이 가능하다는 것을 증명하였습니다.

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [136] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 본 연구는 월드 모델의 양자화가 모델의 성능에 미치는 영향을 체계적으로 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 효율적인 배포를 위해서는 월드 모델의 양자화가 필수적입니다.

Method: DINO-WM을 대표 사례로 하여 다양한 PTQ 방법을 평가하고, 여러 비트 너비와 양자화 세분도로 광범위한 시각적 계획 작업에서 실험을 수행했습니다.

Result: 양자화 효과는 표준 정확도 및 비트 너비의 절충을 넘어 다양한 결과를 보여주었습니다.

Conclusion: 이 연구는 월드 모델 기반 계획에서 발생할 수 있는 특정 양자화-induced 실패 모드를 밝혀내고, 엄격한 계산 제약 하에서 양자화된 월드 모델 배포를 위한 실용적인 지침을 제공합니다.

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [137] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: 이 논문은 DCoPilot라는 하이브리드 프레임워크를 제안하여 동적 데이터 센터 운영에서 생성적인 제어 정책을 생성한다.


<details>
  <summary>Details</summary>
Motivation: 현대 데이터 센터는 높은 전력 밀도로 운영되며, 이로 인해 안전하고 에너지 효율적인 운영을 위한 세밀한 적응이 필요하다.

Method: DCoPilot는 구조화된 보상 형식의 기호 생성을 수행하는 대형 언어 모델(LLM)과 정책 가중치의 매개변수 생성을 수행하는 하이퍼네트워크를 결합하여 세 가지 단계로 운영된다: 시뮬레이션 스케일 업, 메타 정책 증류, 온라인 적응.

Result: DCoPilot는 다섯 개의 제어 작업 패밀리에서 평가되었으며, 거의 제로에 가까운 제약 위반을 달성하고 모든 기준선 성과를 초과하였다.

Conclusion: LLM 기반의 통합 보상 생성이 안정적인 하이퍼네트워크 수렴을 가능하게 함을 검증했다.

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [138] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: 이 논문은 장기 시계열 예측의 효율성을 높이기 위한 간단하면서도 효과적인 프레임워크인 BTTF를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 장기 시계열 예측(LTSF)은 병렬 효율성과 시간적 일관성의 순차적 모델링 간의 트레이드오프가 있어 여전히 도전적입니다.

Method: BTTF는 선행 보강 및 자기 수정 정제를 통해 예측 안정성을 향상시키는 프레임워크입니다. 복잡한 모델 아키텍처에 의존하기보다는, BTTF는 기본 예측 프로세스를 재검토하고 초기 예측으로 보강된 두 번째 단계 모델을 앙상블하여 기본 모델을 정제합니다.

Result: 우리의 접근법은 일관되게 장기 정확도를 향상시키고 선형 예측 모델의 불안정을 완화하며, 최대 58%의 정확도 향상이 이루어졌습니다.

Conclusion: 모델 생성 예측을 보강으로 활용하는 것은 복잡한 아키텍처 없이도 장기 예측을 향상시키는 간단하면서도 강력한 방법이 될 수 있음을 시사합니다.

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [139] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 본 논문은 선형 주의 모델의 내부 동역학을 연구하며, 주목할 만한 상태 계급 층화 현상을 발견하고, 저차원 헤드와 고차원 헤드의 역할을 비교합니다.


<details>
  <summary>Details</summary>
Motivation: 선형 주의 큰 언어 모델의 내부 동역학을 이해하기 위해.

Method: 최신 선형 주의 모델의 런타임 상태 동역학을 종합적으로 연구하고, 다양한 추론 컨텍스트에서 실험을 수행한다.

Result: 상태 계급 층화라는 현상을 발견하고, 저차원 헤드가 모델 추론에 필수적이며 고차원 헤드가 상당한 중복성을 가진다는 것을 밝힘.

Conclusion: Joint Rank-Norm Pruning이라는 제로샷 전략을 제안하여 KV-캐시 오버헤드를 38.9% 줄이면서 모델 정확도를 유지할 수 있음.

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [140] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: 이 논문은 시각 정보와 텍스트 토큰 간 상호작용을 최적화하여 MLLMs의 KV 캐시 사용을 최소화하는 Hierarchical Adaptive Eviction (HAE) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비주얼 정보의 LLM 통합으로 멀티모달 LLM이 가능해졌지만, Transformer 아키텍처의 메모리 및 계산 비용이 여전히 병목 현상으로 남아 있다.

Method: HAE는 Dual-Attention Pruning 및 Dynamic Decoding Eviction Strategy를 통해 KV 캐시 제거 전략을 최적화한다.

Result: HAE는 KV-캐시 메모리를 41% 감소시키고, 이미지 이해 작업에서 0.3%의 정확도 손실로 생성 작업에서의 효율성을 향상시킨다.

Conclusion: HAE는 효율성을 높이면서 정보 무결성을 보장하고 더 낮은 오류 한계를 제공한다.

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [141] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: Fat-Cat은 문서 기반 에이전트 아키텍처로, 상태 관리를 개선하고, Kimi-k2 모델이 HotPotQA에서 GPT-4o를 초과 성과를 낼 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 효과는 모델 용량이 아니라 런타임에서의 맥락 정보 활용 효율성에 의해 제한된다.

Method: Fat-Cat은 에이전트 상태를 공통 사전 훈련 코퍼스와 일치하는 Markdown 문서로 표현하는 의미 파일 시스템과, 매개변수 업데이트 없이 과제 해결 지식을 축적하는 텍스트 전략 진화 모듈, 추론 경로를 모니터링하여 환각을 줄이는 폐쇄 루프 감시자를 통합한 아키텍처이다.

Result: Fat-Cat은 풍부한 추론, 검색, 코딩 벤치마크에서 일관되게 에이전트 성능을 향상시키며, Kimi-k2 모델이 HotPotQA에서 맞춤형 GPT-4o 기준을 초과 성과를 낸다.

Conclusion: 문서 기반 상태를 JSON으로 교체하면 성능 저하가 발생하며, 강직한 구문보다 문서 기반 상태 모델링의 중요성이 실증적으로 검증된다.

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [142] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: MaskLAM은 시각적 에이전트 분할을 통합하여 LAM 훈련의 주요 문제를 완화하는 가벼운 수정 방법입니다.


<details>
  <summary>Details</summary>
Motivation: LAM은 라벨이 없는 비디오로부터 강화 학습이 가능하게 하지만, 동작과 관련된 노이즈와 특징을 분리하는 데 어려움을 겪습니다.

Method: MaskLAM은 사전 훈련된 기반 모델의 세분화 마스크를 사용하여 LAM 재구성 손실에 가중치를 부여하면서 아키텍처 수정 없이 중요한 정보를 우선시합니다.

Result: 우리는 지속적인 제어 MuJoCo 작업에서 MaskLAM의 효과를 입증하였으며, 표준 기준선에 비해 보상이 최대 4배 증가하고, 잠재적 행동 품질이 3배 향상되었습니다.

Conclusion: 이 연구는 LAM 훈련에서 중요한 요소를 강조하고, MaskLAM이 이러한 문제를 해결하는 데 효과적임을 보여줍니다.

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [143] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 랜덤 지연 환경에서 의사결정을 위한 강력한 에이전트를 제안


<details>
  <summary>Details</summary>
Motivation: 현실 세계 강화 학습 응용 프로그램은 환경으로부터의 지연된 피드백으로 인해 마르코프 가정이 위배되고 큰 도전 과제를 초래합니다.

Method: 랜덤 지연 환경을 상수 지연 환경으로 재구성하는 보수적 에이전트를 제안합니다.

Result: 이 알고리즘이 기존 알고리즘에 비해 비대칭 성능과 샘플 효율성에서 크게 우수하다는 것을 보여줍니다.

Conclusion: 보수적 에이전트 기반 알고리즘은 연속 제어 작업에서 현저한 성능 향상을 보입니다.

Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.

</details>


### [144] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: 본 논문에서는 에피소드 MDP에 대한 새로운 제한된 피드백 모델을 다루며, 처리 효율적인 밴딧 학습 알고리즘을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 기본 가정은 에이전트가 모든 관찰된 상태-행동 쌍과 보상을 관찰한다는 것입니다. 하지만 이러한 피드백은 비현실적일 수 있으며, 최근 연구는 에이전트가 방문한 모든 상태-행동 쌍을 관찰하지만 단일 집계 보상만 관찰하는 제한된 설정을 조사했습니다.

Method: 우리는 에피소드 MDP에 대한 새로운 '완전 밴딧' 피드백 모델을 제안하고, 복잡 전이 종합 보상을 학습하는 효율적인 밴딧 학습 알고리즘을 제공합니다.

Result: 우리의 알고리즘은 $	ilde{O}(	ext{	extit{T}})^{1/2}$의 후회 값을 가지며, 이는 수평 길이 $	ext{	extit{H}}$에 지수적 의존관계를 갖습니다. 또한 '주문형' MDP에 대한 개선된 후회 경계를 도출할 수 있습니다.

Conclusion: 우리는 특정 최적화 문제에 대한 성능을 평가하며, 제한된 피드백에도 불구하고 우리의 알고리즘은 상세한 상태-행동 피드백을 가진 최첨단 학습 알고리즘과 비슷한 성능을 보입니다.

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [145] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

TL;DR: 본 연구는 경험적 과정 이론에 기반한 통계적 학습 이론의 Lean 4 포멀리제이션을 처음으로 포괄적으로 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 통계적 학습 이론에 대한 명확한 포멀리제이션을 제공하여 이론의 세부 사항을 명확히 하고, 머신러닝 이론의 향후 발전을 위한 기초를 마련하고자 함.

Method: Lean 4 Mathlib 라이브러리에 누락된 내용을 포괄적으로 구현하고, 인간과 AI의 협업 방식으로 증명 전략을 설계하며, AI가 전술적 증명을 실행함.

Result: 가우시안 립시츠 농축의 완전한 개발, 서브 가우시안 과정에 대한 더들리의 엔트로피 적분 정리 최초의 포멀리제이션, 그리고 최소제곱 회귀에 대한 sharp rate의 응용으로 구성된 ML 툴박스를 완성함.

Conclusion: Reusable formal foundation을 수립하고 머신러닝 이론의 미래 발전을 위한 길을 열어줌.

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [146] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: RL에서 LLM의 후속 훈련을 위한 텍스트 피드백의 가능성을 탐구하는 연구.


<details>
  <summary>Details</summary>
Motivation: LLM 후속 훈련에서 RL의 성공이 비효율적인 정보 원천에 기인한다는 점을 해결하고, 비용이 많이 드는 시연 대신 텍스트 피드백을 활용하고자 하였다.

Method: 훈련 중 텍스트 피드백을 이용하는 다중 턴 RL 설정인 RL from Text Feedback(RLTF)을 제안하며, 두 가지 방법인 Self Distillation(RLTF-SD)와 Feedback Modeling(RLTF-FM)을 도입하였다.

Result: 두 방법 모두 여러 기준에서 강력한 베이스라인을 일관되게 초과하는 성능을 보였다.

Conclusion: 리치한 감독 소스와 함께 RL을 활용한 가능성을 강조하며, 테스트 시의 단일 턴 성과를 향상시키기 위해 피드백을 내재화해야 한다.

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [147] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything는 강화 학습 프레임워크로, 환경, 정책 및 보상 모델을 동적으로 최적화하여 RL 시스템을 강화합니다.


<details>
  <summary>Details</summary>
Motivation: 정확하고 효율적인 강화 학습 시스템의 필요성.

Method: 정확한 피드백을 통해 정책과 보상 모델을 최적화하고, 경험에서 학습하는 자동 환경 적응을 구현합니다.

Result: RLAnything는 다양한 LLM과 에이전트 작업에서 현저한 성과 개선을 보여줍니다.

Conclusion: RLAnything는 기존 시스템보다 개선된 성능으로 강화 학습 작업에 기여합니다.

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [148] [First Steps, Lasting Impact: Platform-Aware Forensics for the Next Generation of Analysts](https://arxiv.org/abs/2602.00160)
*Vinayak Jain,Sneha Sudhakaran,Saranyan Senthivel*

Main category: cs.CR

TL;DR: 이 연구는 Windows 및 Linux 시스템에서의 디스크 및 메모리 포렌식 기법을 평가하고, 각 운영 체제에 맞춤화된 포렌식 도구와 구성의 효과적인 조합을 식별하여 증거 수집의 정확성과 신뢰성을 향상시키는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 사이버 포렌식 증거 수집의 신뢰성은 운영 체제에 따라 크게 영향을 받는다.

Method: Windows와 Linux 시스템을 대표하는 샘플에서 디스크 및 메모리 포렌식 기법을 체계적으로 평가하기 위해 포렌식 도구와 구성의 조합을 분석하였다.

Result: 디스크 포렌식은 플랫폼마다 고유한 장애물에 직면하며, 메모리 포렌식을 통해 점점 더 중요해지는 문제를 다루었다.

Conclusion: 증거 수집의 정확성과 신뢰성을 향상시키기 위해 포렌식 입력 신뢰성과 발자국 무결성을 지속적으로 보장하는 문제를 강조하였다.

Abstract: The reliability of cyber forensic evidence acquisition is strongly influenced by the underlying operating systems, Windows, macOS, and Linux - due to inherent variations in file system structures, encryption protocols, and forensic tool compatibility. Disk forensics, one of the most widely used techniques in digital investigations, faces distinct obstacles on each platform. Windows, with its predominantly NTFS and FAT file systems, typically supports reliable disk imaging and analysis through established tools such as FTK Imager and Autopsy/Sleuth Kit. However, encryption features frequently pose challenges to evidence acquisition. Conversely, Linux environments, which rely on file systems like ext4 and XFS, generally offer greater transparency, yet the transient nature of log retention often complicates forensic analysis. In instances where anti-forensic strategies such as encryption and compression render traditional disk forensics insufficient, memory forensics becomes crucial. While memory forensic methodologies demonstrate robustness across Windows and Linux platforms forms through frameworks like Volatility, platform-specific difficulties persist. Memory analysis on Linux systems benefits from tools like LiME, snapshot utilities, and dd for memory acquisition; nevertheless, live memory acquisition on Linux can still present challenges. This research systematically assesses both disk and memory forensic acquisition techniques across samples representing Windows and Linux systems. By identifying effective combinations of forensic tools and configurations tailored to each operating system, the study aims to improve the accuracy and reliability of evidence collection. It further evaluates current forensic tools and highlights a persistent gap: consistently assuring forensic input reliability and footprint integrity.

</details>


### [149] [EigenAI: Deterministic Inference, Verifiable Results](https://arxiv.org/abs/2602.00182)
*David Ribeiro Alves,Vishnu Patankar,Matheus Pereira,Jamie Stephens,Nima Vaziri,Sreeram Kannan*

Main category: cs.CR

TL;DR: EigenAI는 EigenLayer 재스테이킹 생태계를 기반으로 구축된 검증 가능한 AI 플랫폼입니다. 이 플랫폼은 결정적인 대규모 언어 모델(LLM) 추론 엔진과 암호 경제적으로 보안된 낙관적 재실행 프로토콜을 결합하여 모든 추론 결과를 공개적으로 감사하고 복제하며 필요시 경제적으로 집행할 수 있게 합니다.


<details>
  <summary>Details</summary>
Motivation: AI 모델의 결과를 검증 가능하게 만들고, 불신의 여지를 줄이기 위해.

Method: 결정적인 LLM 추론 엔진과 암호 경제적 낙관적 재실행 프로토콜을 결합하여 구현.

Result: EigenAI는 신뢰할 수 없는 운영자가 고정된 GPU 아키텍처에서 추론을 수행하고, 요청 및 응답을 서명 및 암호화하여 로그를 EigenDA에 게시하도록 하고, 도전 창 기간 동안에 누구나 EigenVerify를 통해 재실행 요청을 할 수 있게 함.

Conclusion: 이 아키텍처는 이더리움의 검증자 기반의 보안을 상속받으면서도 최첨단 성능을 자랑하는 주권 에이전트들을 생성할 수 있습니다.

Abstract: EigenAI is a verifiable AI platform built on top of the EigenLayer restaking ecosystem. At a high level, it combines a deterministic large-language model (LLM) inference engine with a cryptoeconomically secured optimistic re-execution protocol so that every inference result can be publicly audited, reproduced, and, if necessary, economically enforced. An untrusted operator runs inference on a fixed GPU architecture, signs and encrypts the request and response, and publishes the encrypted log to EigenDA. During a challenge window, any watcher may request re-execution through EigenVerify; the result is then deterministically recomputed inside a trusted execution environment (TEE) with a threshold-released decryption key, allowing a public challenge with private data. Because inference itself is bit-exact, verification reduces to a byte-equality check, and a single honest replica suffices to detect fraud. We show how this architecture yields sovereign agents -- prediction-market judges, trading bots, and scientific assistants -- that enjoy state-of-the-art performance while inheriting security from Ethereum's validator base.

</details>


### [150] [TessPay: Verify-then-Pay Infrastructure for Trusted Agentic Commerce](https://arxiv.org/abs/2602.00213)
*Mehul Goenka,Tejas Pathak,Siddharth Asthana*

Main category: cs.CR

TL;DR: TessPay는 고객과 에이전트 간의 신뢰 성격을 해결하기 위해 '검증-후 지불' 아키텍처를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 상거래가 필요하나 신뢰 부족 문제가 존재하는 상황을 해결하고자 합니다.

Method: TessPay는 작업 실행 전후 및 결제 과정에서 명확한 신뢰를 구축하는 통합 인프라와 아키텍처를 제공합니다.

Result: TessPay는 에이전트의 작업 실행을 지원하기 위한 암호화 증거를 생성하고 결제 프로세스를 개선합니다.

Conclusion: TessPay는 신뢰를 보장하며 분쟁 해결을 위한 감사 추적을 제공합니다.

Abstract: The global economy is entering the era of Agentic Commerce, where autonomous agents can discover services, negotiate prices, and transact value. However adoption towards agentic commerce faces a foundational trust gap: current systems are built for direct human interactions rather than agent-driven operations. It lacks core primitives across three critical stages of agentic transactions. First, Task Delegation lacks means to translate user intent into defined scopes, discover appropriate agents, and securely authorize actions. Second, Payment Settlement for tasks is processed before execution, lacking verifiable evidence to validate the agent's work. Third, Audit Mechanisms fail to capture the full transaction lifecycle, preventing clear accountability for disputes. While emerging standards address fragments of this trust gap, there still remains a critical need for a unified infrastructure that binds the entire transaction lifecycle.
  To resolve this gap, we introduce TessPay, a unified infrastructure that replaces implicit trust with a 'Verify-then-Pay' architecture. It is a two plane architecture separating control and verification from settlement. TessPay operationalizes trust across four distinct stages: Before execution, agents are anchored in a canonical registry and user intent is captured as verifiable mandates, enabling stakeholder accountability. During execution, funds are locked in escrow while the agent executes the task and generates cryptographic evidence (TLS Notary, TEE etc.) to support Proof of Task Execution (PoTE). At settlement, the system verifies this evidence and releases funds only when the PoTE satisfies verification predicates; modular rail adapters ensure this PoTE-gated escrow remains chain-agnostic across heterogeneous payment rails. After settlement, TessPay preserves a tamper-evident audit trail to enable clear accountability for dispute resolution.

</details>


### [151] [HEEDFUL: Leveraging Sequential Transfer Learning for Robust WiFi Device Fingerprinting Amid Hardware Warm-Up Effects](https://arxiv.org/abs/2602.00338)
*Abdurrahman Elmaghbub,Bechir Hamdaoui*

Main category: cs.CR

TL;DR: 이 연구에서는 RF 지문 인식의 신뢰성을 높이기 위한 새로운 방법인 HEEDFUL 프레임워크를 제안하며, 기존 모델을 능가하는 성능을 입증한다.


<details>
  <summary>Details</summary>
Motivation: RF 지문 인식 방식은 하드웨어 웜업 시나리오에서 성능이 저하되어 신뢰성과 실제 적용의 어려움이 발생하고 있다.

Method: HEEDFUL이라는 새로운 프레임워크를 도입하여 순차적 전이 학습과 목표 손상 추정을 활용하여 이 문제를 해결한다.

Result: HEEDFUL은 초기 장치 운영 간격에서 96%에 달하는 분류 정확도를 달성하여 기존 모델을 크게 초월한다.

Conclusion: 이 연구는 하드웨어 손상 데이터의 활용을 통해 RF 지문에 대한 이해를 높이고 보다 강력한 RF 지문 인식 솔루션 개발을 촉진할 것이다.

Abstract: Deep Learning-based RF fingerprinting approaches struggle to perform well in cross-domain scenarios, particularly during hardware warm-up. This often-overlooked vulnerability has been jeopardizing their reliability and their adoption in practical settings. To address this critical gap, in this work, we first dive deep into the anatomy of RF fingerprints, revealing insights into the temporal fingerprinting variations during and post hardware stabilization. Introducing HEEDFUL, a novel framework harnessing sequential transfer learning and targeted impairment estimation, we then address these challenges with remarkable consistency, eliminating blind spots even during challenging warm-up phases. Our evaluation showcases HEEDFUL's efficacy, achieving remarkable classification accuracies of up to 96% during the initial device operation intervals-far surpassing traditional models. Furthermore, cross-day and cross-protocol assessments confirm HEEDFUL's superiority, achieving and maintaining high accuracy during both the stable and initial warm-up phases when tested on WiFi signals. Additionally, we release WiFi type B and N RF fingerprint datasets that, for the first time, incorporate both the time-domain representation and real hardware impairments of the frames. This underscores the importance of leveraging hardware impairment data, enabling a deeper understanding of fingerprints and facilitating the development of more robust RF fingerprinting solutions.

</details>


### [152] ["Someone Hid It": Query-Agnostic Black-Box Attacks on LLM-Based Retrieval](https://arxiv.org/abs/2602.00364)
*Jiate Li,Defu Cao,Li Li,Wei Yang,Yuehan Qin,Chenxiao Yu,Tiannuo Yang,Ryan A. Rossi,Yan Liu,Xiyang Hu,Yue Zhao*

Main category: cs.CR

TL;DR: 이 논문은 대형 언어 모델이 검색 시스템에 대한 블랙박스 공격 방법을 제안하며, 실제 쿼리나 모델 정보 없이도 성공적으로 전이 가능한 토큰을 생성할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 기반의 검색 시스템(LLMR)이 적대적 공격에 취약하다는 것을 보여주고, 이러한 취약점을 해결하기 위한 새로운 공격 방법을 제안하고자 함.

Method: 제로샷 대리 LLM을 기반으로 하는 전이 가능한 주입 토큰을 생성하는 실용적인 블랙박스 공격 방법을 제안하고, 이를 수학적 관점에서 미니맥스 문제로 모델링하여 최적의 적대적 토큰을 찾는 학습 가능한 쿼리 샘플을 활용함.

Result: 제안된 공격 방법은 인기 있는 LLM 검색기에서 효율적으로 작동하며, 기존의 공격 연구들이 추정하는 것입니다.

Conclusion: 이 연구는 LLMR의 보안 위험을 탐구하고, 저작물의 무의식적인 수정을 통해 유사한 효과가 발생할 수 있음을 시사한다.

Abstract: Large language models (LLMs) have been serving as effective backbones for retrieval systems, including Retrieval-Augmentation-Generation (RAG), Dense Information Retriever (IR), and Agent Memory Retrieval. Recent studies have demonstrated that such LLM-based Retrieval (LLMR) is vulnerable to adversarial attacks, which manipulates documents by token-level injections and enables adversaries to either boost or diminish these documents in retrieval tasks. However, existing attack studies mainly (1) presume a known query is given to the attacker, and (2) highly rely on access to the victim model's parameters or interactions, which are hardly accessible in real-world scenarios, leading to limited validity.
  To further explore the secure risks of LLMR, we propose a practical black-box attack method that generates transferable injection tokens based on zero-shot surrogate LLMs without need of victim queries or victim models knowledge. The effectiveness of our attack raises such a robustness issue that similar effects may arise from benign or unintended document edits in the real world. To achieve our attack, we first establish a theoretical framework of LLMR and empirically verify it. Under the framework, we simulate the transferable attack as a min-max problem, and propose an adversarial learning mechanism that finds optimal adversarial tokens with learnable query samples. Our attack is validated to be effective on benchmark datasets across popular LLM retrievers.

</details>


### [153] [SpyDir: Spy Device Localization Through Accurate Direction Finding](https://arxiv.org/abs/2602.00411)
*Wenhao Chen,Wenyi Morty Zhang,Wei Sun,Dinesh Bharadia,Roshan Ayyalasomayajula*

Main category: cs.CR

TL;DR: SpyDir는 숨겨진 스파이 IoT 장치의 정확한 위치를 파악할 수 있는 시스템으로, 전자기 방출을 활용하여 실내 환경에서의 위치 추적을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 숨겨진 스파이 카메라가 인도네시아 환경에서 제기하는 개인 정보 위협을 해결하기 위해.

Method: 전자기 방출을 활용하는 포터블 스위칭 안테나 배열과 비상관 평균화를 통한 방출 강화 알고리즘, 새로운 최적화 기반 희소 방향 각도(AoA) 유도 방법을 사용하여 실내 환경에서의 위치를 파악합니다.

Result: 실제 실험 평가에서 SpyDir은 평균 6.30도의 AoA 오차를 달성하였고, 이는 기존 알고리즘에 비해 3.3배 향상된 정확도를 나타냈습니다.

Conclusion: SpyDir은 숨겨진 스파이 IoT 장치의 정확한 위치 추적을 위한 효과적인 솔루션임을 입증합니다.

Abstract: Hidden spy cameras have become a great privacy threat recently, as these low-cost, low-power, and small form-factor IoT devices can quietly monitor human activities in the indoor environment without generating any side-channel information. As such, it is difficult to detect and even more challenging to localize them in the rich-scattering indoor environment. To this end, this paper presents the design, implementation, and evaluation of SpyDir, a system that can accurately localize the hidden spy IoT devices by harnessing the electromagnetic emanations automatically and unintentionally emitted from them. Our system design mainly consists of a portable switching antenna array to sniff the spectrum-spread emanations, an emanation enhancement algorithm through non-coherent averaging that can de-correlate the correlated noise effect due to the square-wave emanation structure, and a multipath-resolving algorithm that can exploit the relative channels using a novel optimization-based sparse AoA derivation. Our real-world experimental evaluation across different indoor environments demonstrates an average AoA error of 6.30 deg, whereas the baseline algorithm yields 21.06 deg, achieving over a 3.3 times improvement in accuracy, and a mean localization error of 19.86cm over baseline algorithms of 206.79cm (MUSIC) and 294.75cm (SpotFi), achieving over a 10.41 times and 14.8 times improvement in accuracy.

</details>


### [154] [SMCP: Secure Model Context Protocol](https://arxiv.org/abs/2602.01129)
*Xinyi Hou,Shenao Wang,Yifan Zhang,Ziluo Xue,Yanjie Zhao,Cai Fu,Haoyu Wang*

Main category: cs.CR

TL;DR: 이 논문에서는 보안 및 신뢰성을 강화한 보안 모델 컨텍스트 프로토콜(SMCP)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반의 에이전트 AI 시스템이 여러 도구와 자원을 연결하는 개방 생태계로 이동하고 있는 가운데, MCP의 보안 및 개인 정보 보호의 새로운 도전 과제가 발생하고 있다.

Method: SMCP는 통합된 신원 관리, 강력한 상호 인증, 지속적인 보안 컨텍스트 전파, 세분화된 정책 집행 및 포괄적인 감사 로깅을 추가하여 MCP를 강화한다.

Result: SMCP의 주요 구성 요소를 설명하고, 이를 통해 보안 위험을 줄이는 방법을 제시하며, 실용적인 예로 응용 사례를 보여준다.

Conclusion: 이 연구가 안전하고 신뢰할 수 있는 에이전트 시스템 개발에 기여하길 바란다.

Abstract: Agentic AI systems built around large language models (LLMs) are moving away from closed, single-model frameworks and toward open ecosystems that connect a variety of agents, external tools, and resources. The Model Context Protocol (MCP) has emerged as a standard to unify tool access, allowing agents to discover, invoke, and coordinate with tools more flexibly. However, as MCP becomes more widely adopted, it also brings a new set of security and privacy challenges. These include risks such as unauthorized access, tool poisoning, prompt injection, privilege escalation, and supply chain attacks, any of which can impact different parts of the protocol workflow. While recent research has examined possible attack surfaces and suggested targeted countermeasures, there is still a lack of systematic, protocol-level security improvements for MCP. To address this, we introduce the Secure Model Context Protocol (SMCP), which builds on MCP by adding unified identity management, robust mutual authentication, ongoing security context propagation, fine-grained policy enforcement, and comprehensive audit logging. In this paper, we present the main components of SMCP, explain how it helps reduce security risks, and illustrate its application with practical examples. We hope that this work will contribute to the development of agentic systems that are not only powerful and adaptable, but also secure and dependable.

</details>


### [155] [Protocol Agent: What If Agents Could Use Cryptography In Everyday Life?](https://arxiv.org/abs/2602.01304)
*Marco De Rossi*

Main category: cs.CR

TL;DR: 이 논문에서는 에이전트들이 효율적이고 능력에 맞는 통신 패턴을 개발할 수 있는 가능성을 탐구하고, 이를 평가하기 위한 벤치마크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 간의 상호작용이 인간 대화를 반영할 것이라는 가정은 종종 있지만, 에이전트는 근본적으로 다르게 작동한다.

Method: 프로토콜 에이전트는 암호학적 기초 인식, 협상 기술, 구현 정확성, 계산 정확성, 보안 강도를 포함하는 벤치마크를 도입하여 문제를 프레이밍한다.

Result: 현재의 오픈웨이트 및 최첨단 모델을 벤치마크에서 평가하고, 이러한 능력을 향상시키기 위한 데이터셋 생성 방안을 제안하며, 감독된 미세 조정(SFT)이 벤치마크 성능에 미치는 영향을 측정한 결과, 조정된 모델이 기본 모델보다 훨씬 높은 성능을 보였다.

Conclusion: 제안된 접근 방식은 에이전트 간의 효율적이고 안전한 통신 패턴 개발을 위한 새로운 길을 열어준다.

Abstract: We often assume that agent-to-agent interaction will mirror human conversation. However, agents operate fundamentally differently. What if they could develop communication patterns that are more efficient and better aligned with their capabilities? While cryptographic primitives that could profoundly improve everyday interactions already exist, humans can't use them because they are too complex and the math can't be done in one's head. Examples range from proving your age (or other attributes) without showing your ID, to filing an anonymous report within a group while proving you are a legitimate member, to splitting a dinner bill fairly without revealing salaries. What if agents could create protocols "on the fly" by recognizing which primitive fits an everyday situation, proposing it to an agentic counterpart, persuading them to participate, and then executing the protocol correctly using appropriate computation tools? Protocol Agent frames this problem by introducing a benchmark that spans: (1) cryptographic primitive recognition, (2) negotiation skills, (3) implementation correctness, (4) correct computation and (5) security strength. We evaluate current open-weight and state-of-the-art models on this benchmark, propose a dataset-generation approach to improve these capabilities, and measure the impact of supervised fine-tuning (SFT) on benchmark performance, with tuned models outperforming base models by a wide margin.

</details>


### [156] [TxRay: Agentic Postmortem of Live Blockchain Attacks](https://arxiv.org/abs/2602.01317)
*Ziyue Wang,Jiangshan Yu,Kaihua Qin,Dawn Song,Arthur Gervais,Liyi Zhou*

Main category: cs.CR

TL;DR: 탈중앙화 금융(DeFi)에서는 블록체인이 금융 인프라로 전환되어 모든 사용자가 중개인 없이 거래, 대출, 프로토콜 구축을 할 수 있지만, 이러한 개방성은 코드에 의해 통제되는 가치 풀을 노출시킵니다. 이 연구에서는 TxRay라는 도구를 통해 제한된 증거로부터 ACT 공격을 재구성하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: DeFi 생태계에서 사용자가 겪는 보안 취약성을 해결하기 위해.

Method: TxRay를 사용하여 제한된 증거에서 ACT 공격을 재구성하고, 사건별 의미 오라클을 실행 가능한 주장으로 인코딩하여 사후 분석을 수행합니다.

Result: TxRay는 114건의 사건 중 105건에 대해 전문가 정렬된 근본 원인을 생성하고 실행 가능한 개념 증명을 제공합니다. 최종 재현율은 92.11%입니다.

Conclusion: TxRay는 40분 내에 검증된 근본 원인을 제공하고, 59분 내에 개념 증명을 생성하여 DeFi 보안성을 향상시킵니다.

Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported exploits. Many exploits arise from permissionless opportunities that any participant can trigger using only public state and standard interfaces, which we call Anyone-Can-Take (ACT) opportunities. Despite on-chain transparency, postmortem analysis remains slow and manual: investigations start from limited evidence, sometimes only a single transaction hash, and must reconstruct the exploit lifecycle by recovering related transactions, contract code, and state dependencies.
  We present TxRay, a Large Language Model (LLM) agentic postmortem system that uses tool calls to reconstruct live ACT attacks from limited evidence. Starting from one or more seed transactions, TxRay recovers the exploit lifecycle, derives an evidence-backed root cause, and generates a runnable, self-contained Proof of Concept (PoC) that deterministically reproduces the incident. TxRay self-checks postmortems by encoding incident-specific semantic oracles as executable assertions.
  To evaluate PoC correctness and quality, we develop PoCEvaluator, an independent agentic execution-and-review evaluator. On 114 incidents from DeFiHackLabs, TxRay produces an expert-aligned root cause and an executable PoC for 105 incidents, achieving 92.11% end-to-end reproduction. Under PoCEvaluator, 98.1% of TxRay PoCs avoid hard-coding attacker addresses, a +24.8pp lift over DeFiHackLabs. In a live deployment, TxRay delivers validated root causes in 40 minutes and PoCs in 59 minutes at median latency. TxRay's oracle-validated PoCs enable attack imitation, improving coverage by 15.6% and 65.5% over STING and APE.

</details>


### [157] [Implementation Challenges in Quantum Key Distribution](https://arxiv.org/abs/2602.01500)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 이 연구는 실제 양자 컴퓨팅 환경에서 BB84와 E91이라는 두 가지 양자 키 분배(QKD) 프로토콜을 구현하고 비교합니다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨팅 기술이 성숙해지고 있으며, 네트워크 통신 보안 분야에서 실용적인 응용 프로그램을 찾고 있습니다.

Method: IBM Quantum Platform을 사용하여 BB84 및 E91 프로토콜의 실제 하드웨어 구현을 실험합니다. 또한 균일한 양자 중첩 상태를 생성하기 위해 SX 게이트 연산을 사용합니다.

Result: 연구는 양자 중첩과 양자 얽힘의 특성을 활용하여 통신 당사자들이 안전하게 공유 비밀을 얻을 수 있는 방법을 보여줍니다.

Conclusion: 이 연구는 BB84 및 E91 프로토콜의 실제 양자 하드웨어에서의 실행 가능성을 입증하며, 여러 메트릭을 통해 평가합니다.

Abstract: In recent years, quantum computing technologies have steadily matured and have begun to find practical applications across various domains. One important area is network communication security, where Quantum Key Distribution (QKD) enables communicating parties to establish a shared secret that can then be used to generate symmetric keys for subsequent encryption and decryption. This study focuses on implementing and comparing two well-known QKD protocols, namely BB84 and E91, within an actual quantum computing environment. It also proposes the use of SX gate operations to generate uniform quantum superposition states. By leveraging the properties of quantum superposition and quantum entanglement, the study illustrates how communicating parties can securely obtain a shared secret while preventing adversaries from intercepting it. The experiments are conducted using the IBM Quantum Platform to demonstrate the feasibility of the BB84 and E91 protocols on actual quantum hardware. The evaluation considers several metrics, including entropy, Independent and Identically Distributed (IID), and error-rate verifications.

</details>


### [158] [Are Security Cues Static? Rethinking Warning and Trust Indicators for Life Transitions](https://arxiv.org/abs/2602.01544)
*Sarah Tabassum*

Main category: cs.CR

TL;DR: 보안 신호는 인간의 삶의 전환과 변화에 적응하지 못하고 있어 사용자가 해석할 부담을 지고 있다. 이 연구에서는 생활 전환에 대비한 보안 신호 프레임워크인 TASeC을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사회적 맥락과 개인의 삶의 변화에 대한 보안 신호의 적절성을 평가하고자 한다.

Method: 이전 연구 사례와 통찰력을 바탕으로 TASeC 프레임워크를 고안하고 보안 신호가 전환 단계에서 어떻게 진화할 수 있는지를 탐구한다.

Result: 기존의 보안 신호가 사용자에 의해 해석될 때의 한계를 파악하고, 새로운 디자인 개념을 제시하여 전환 지점에서의 보안 신호의 가능성을 탐색한다.

Conclusion: HCI 분야에 보안 신호를 장기적이고 생애 중심의 디자인 요소로 재구성할 것을 제안한다.

Abstract: Security cues, such as warnings and trust signals, are designed as stable interface elements, even though people's lives, contexts, and vulnerabilities change over time. Life transitions including migration, aging, or shifts in institutional environments reshape how risk and trust are understood and acted upon. Yet current systems rarely adapt their security cues to these changing conditions, placing the burden of interpretation on users. In this Works-in-Progress paper, we argue that the static nature of security cues represents a design mismatch with transitional human lives. We draw on prior empirical insights from work on educational migration as a motivating case, and extend the discussion to other life transitions. Building on these insights, we introduce the Transition-Aware Security Cues (TASeC) framework and present speculative design concepts illustrating how security cues might evolve across transition stages. We invite HCI to rethink security cues as longitudinal, life-centered design elements collectively.

</details>


### [159] [Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency](https://arxiv.org/abs/2602.01765)
*Bingzheng Wang,Xiaoyan Gu,Hongbo Xu,Hongcheng Li,Zimo Yu,Jiang Zhou,Weiping Wang*

Main category: cs.CR

TL;DR: 이 연구에서는 백도어 감지 및 해독을 위한 새로운 접근 방식인 Temporal Noise Consistency Defense(TNC-Defense)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 그림자 주입 공격이 가능한 확산 모델의 취약성을 해결하고, 감지 및 해독 효과성을 동시에 개선할 필요성이 있다.

Method: Temporal Noise Consistency Defense(TNC-Defense)라는 통합 프레임워크를 사용하여 인접 타임스텝의 노이즈 일관성을 활용하여 이상 감지를 수행하고, 이를 바탕으로 타임스텝 인식 해독 모듈을 설계한다.

Result: TNC-Defense는 평균 감지 정확도를 $11\%$ 향상시키고, 발생한 샘플의 평균 $98.5\%$를 무효화하면서도 생성 품질의 경미한 저하만 발생시킨다.

Conclusion: TNC-Defense는 백도어 공격에 대한 효과적인 방어책을 제공하며, 감지 비용을 크게 줄일 수 있다.

Abstract: Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors are typically unable to access model parameters, rendering existing white-box or query-intensive detection methods impractical. More importantly, even after the backdoor is detected, existing detoxification approaches are often trapped in a dilemma between detoxification effectiveness and generation quality.
  In this work, we identify a previously unreported phenomenon called temporal noise unconsistency, where the noise predictions between adjacent diffusion timesteps is disrupted in specific temporal segments when the input is triggered, while remaining stable under clean inputs. Leveraging this finding, we propose Temporal Noise Consistency Defense (TNC-Defense), a unified framework for backdoor detection and detoxification. The framework first uses the adjacent timestep noise consistency to design a gray-box detection module, for identifying and locating anomalous diffusion timesteps. Furthermore, the framework uses the identified anomalous timesteps to construct a trigger-agnostic, timestep-aware detoxification module, which directly corrects the backdoor generation path. This effectively suppresses backdoor behavior while significantly reducing detoxification costs.
  We evaluate the proposed method under five representative backdoor attack scenarios and compare it with state-of-the-art defenses. The results show that TNC-Defense improves the average detection accuracy by $11\%$ with negligible additional overhead, and invalidates an average of $98.5\%$ of triggered samples with only a mild degradation in generation quality.

</details>


### [160] [Human Society-Inspired Approaches to Agentic AI Security: The 4C Framework](https://arxiv.org/abs/2602.01942)
*Alsharif Abuadbba,Nazatul Sultan,Surya Nepal,Sanjay Jha*

Main category: cs.CR

TL;DR: 이 논문은 다중 에이전트 AI 보안을 위한 4C 프레임워크를 소개하며, 이는 에이전트의 위험을 네 가지 상호 의존적 차원으로 정리한다.


<details>
  <summary>Details</summary>
Motivation: AI는 예측 가능한 제한된 환경에서 특정 도메인에 기반한 자율성에서 벗어나, 개방적인 조직 간 환경에서 계획하고 행동하는 대규모 언어 모델 기반 에이전트로 발전하고 있다.

Method: 이 논문은 사회적 거버넌스에서 영감을 받아 4C 프레임워크를 소개하며, 시스템, 인프라 및 환경의 무결성(Core), 통신, 조정 및 신뢰(Connection), 신념, 목표 및 추론의 무결성(Cognition), 윤리적, 법적 및 제도적 거버넌스(Compliance)라는 네 가지 차원에서 에이전트의 위험을 정리한다.

Result: 이 프레임워크는 시스템 중심의 보호에서 행동 무결성 및 의도의 광범위한 유지로 AI 보안을 전환하여 기존 AI 보안 전략을 보완하고 신뢰할 수 있으며 거버넌스가 가능한, 인간의 가치에 aligned된 에이전트 AI 시스템 구축에 대한 원칙적인 토대를 제공한다.

Conclusion: AI 보안을 시스템 중심의 접근 방식에서 벗어나 보다 포괄적인 행동 무결성과 의도의 보존으로 전환하는 것이 중요하다.

Abstract: AI is moving from domain-specific autonomy in closed, predictable settings to large-language-model-driven agents that plan and act in open, cross-organizational environments. As a result, the cybersecurity risk landscape is changing in fundamental ways. Agentic AI systems can plan, act, collaborate, and persist over time, functioning as participants in complex socio-technical ecosystems rather than as isolated software components. Although recent work has strengthened defenses against model and pipeline level vulnerabilities such as prompt injection, data poisoning, and tool misuse, these system centric approaches may fail to capture risks that arise from autonomy, interaction, and emergent behavior. This article introduces the 4C Framework for multi-agent AI security, inspired by societal governance. It organizes agentic risks across four interdependent dimensions: Core (system, infrastructure, and environmental integrity), Connection (communication, coordination, and trust), Cognition (belief, goal, and reasoning integrity), and Compliance (ethical, legal, and institutional governance). By shifting AI security from a narrow focus on system-centric protection to the broader preservation of behavioral integrity and intent, the framework complements existing AI security strategies and offers a principled foundation for building agentic AI systems that are trustworthy, governable, and aligned with human values.

</details>
