<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [FinPos: A Position-Aware Trading Agent System for Real Financial Markets](https://arxiv.org/abs/2510.27251)
*Bijia Liu,Ronghao Dang*

Main category: cs.MA

TL;DR: 본 논문은 연속적인 포지션 관리를 고려한 포지션 인식 거래 과제를 제안하고, 이를 위한 최적화된 거래 에이전트 시스템 FinPos를 개발한다. FinPos는 시장 정보를 전문적으로 해석하여 신뢰할 수 있는 포지셔닝 결정 근거를 제공하며, 이를 통해 단기 변동과 장기 추세의 균형을 효과적으로 맞춘다.


<details>
  <summary>Details</summary>
Motivation: 최근 대형 언어 모델의 잠재력이 금융 거래 분야에서 주목받고 있으나, 기존 거래 에이전트는 단일 단계 거래 작업에 주로 집중하고 연속적인 포지션 관리에 대한 인식이 부족하다.

Method: FinPos라는 포지션 관리를 최적화한 거래 에이전트 시스템을 개발하고, 이 시스템은 다양한 시장 정보를 해석할 수 있으며, 이중 결정 에이전트를 통해 포지션 변동으로 인한 시장 위험을 완화한다.

Result: 광범위한 실험을 통해 FinPos가 실제 시장 조건을 밀접하게 반영하는 포지션 인식 거래 작업에서 최첨단 거래 에이전트를 초월함을 입증하였다.

Conclusion: LLM 중심의 에이전트 시스템은 장기 시장 의사 결정에서 광범위하고 주로 탐구되지 않은 잠재력을 보인다.

Abstract: The exceptional potential of large language models (LLMs) in handling text
information has garnered significant attention in the field of financial
trading. However, current trading agents primarily focus on single-step trading
tasks and lack awareness of continuous position management. Therefore, we
propose a position-aware trading task designed to simulate a more realistic
market. To address this task, we develop a trading agent system, FinPos,
optimized for position management. FinPos is able to interpret various types of
market information from a professional perspective, providing a reliable basis
for positioning decisions. To mitigate the substantial market risks arising
from position fluctuations, FinPos employs dual decision agents. Furthermore,
the continuous nature of position management necessitates our adoption of
multi-timescale rewards, which in turn empowers FinPos to effectively balance
short-term fluctuations against long-term trends. Extensive experiments
demonstrate that FinPos surpasses state-of-the-art trading agents in the
position-aware trading task, which closely mirrors real market conditions. More
importantly, our findings reveal that LLM-centered agent systems exhibit a
vast, largely unexplored potential in long-term market decision-making.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)
*Zhiyuan Ning,Jiawei Shao,Ruge Xu,Xinfei Guo,Jun Zhang,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 이 논문에서는 CAS-Spec이라는 새로운 카스케이드 적응형 자기 추측 디코딩 방법을 제안하여 대형 언어 모델에서 손실 없는 추론 가속을 위한 효율적인 기술로서의 자속 추측 디코딩의 한계를 극복합니다.


<details>
  <summary>Details</summary>
Motivation: 자속 추측 디코딩은 대형 언어 모델을 배포할 때 손실 없는 추론 가속을 위한 효과적인 기술로 널리 채택되었습니다.

Method: CAS-Spec 방법은 동적으로 전환 가능한 추론 가속화 전략을 활용하여 추측 초안 모델을 구축하며, 새로운 동적 트리 카스케이드 알고리즘을 도입하여 멀티 레벨 초안 모델을 능동적으로 라우팅합니다.

Result: CAS-Spec 방법은 기존의 자속 추측 디코딩 방법에 비해 저속도로 평균 $1.1	imes$에서 $2.3	imes$의 속도 향상을 달성합니다.

Conclusion: CAS-Spec은 대부분의 기존 LLM에 쉽게 통합할 수 있으며, 자속 추측 디코딩 기술의 지속적인 발전에 따라 추가적인 가속 가능성이 있습니다.

Abstract: Speculative decoding has become a widely adopted as an effective technique
for lossless inference acceleration when deploying large language models
(LLMs). While on-the-fly self-speculative methods offer seamless integration
and broad utility, they often fall short of the speed gains achieved by methods
relying on specialized training. Cascading a hierarchy of draft models promises
further acceleration and flexibility, but the high cost of training multiple
models has limited its practical application. In this paper, we propose a novel
Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs
speculative draft models by leveraging dynamically switchable inference
acceleration (DSIA) strategies, including layer sparsity and activation
quantization. Furthermore, traditional vertical and horizontal cascade
algorithms are inefficient when applied to self-speculative decoding methods.
We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the
multi-level draft models and assigns the draft lengths, based on the heuristics
of acceptance rates and latency prediction. Our CAS-Spec method achieves
state-of-the-art acceleration compared to existing on-the-fly speculative
decoding methods, with an average speedup from $1.1\times$ to $2.3\times$ over
autoregressive decoding across various LLMs and datasets. DyTC improves the
average speedup by $47$\% and $48$\% over cascade-based baseline and tree-based
baseline algorithms, respectively. CAS-Spec can be easily integrated into most
existing LLMs and holds promising potential for further acceleration as
self-speculative decoding techniques continue to evolve.

</details>


### [3] [Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems](https://arxiv.org/abs/2510.27659)
*Alireza Saleh Abadi,Leen-Kiat Soh*

Main category: cs.LG

TL;DR: 이 논문은 다중 에이전트 강화 학습(MARL) 분야의 개방 시스템 동역학을 다루며, 개방성과 신용 할당 문제(CAP) 간의 상호작용을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: MARL의 개방 시스템 동역학 이해는 시스템 내 에이전트 집단, 작업 및 에이전트 유형의 동적 특성 때문입니다.

Method: 우리는 개념적 분석을 수행하고, 에이전트 이직 또는 작업 취소와 같은 사건이 환경의 정적 가정과 고정 팀 구성을 어떻게 파괴하는지를 세분화하여 새로운 개방성 하위 범주를 도입합니다. 이어서 개방 환경에서 대표적인 시간적 및 구조적 알고리즘을 사용한 실증 연구를 제시합니다.

Result: 결과는 개방성이 신용 잘못 할당을 직접적으로 초래하며, 이는 불안정한 손실 함수와 성능 저하로 입증됩니다.

Conclusion: 전통적인 신용 할당 방법은 개방 시스템에 적합하지 않으며, 본 연구는 이러한 한계와 신용 할당 문제를 해결하기 위한 새로운 통찰을 제공합니다.

Abstract: In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.

</details>


### [4] [Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle](https://arxiv.org/abs/2510.27097)
*Crystal Su,Kuai Yu,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 이 연구는 대량 조직 RNA 시퀀싱의 한계를 극복하기 위해 세포 유형별 유전자 발현 프로파일을 분리하는 확률론적 계층 베이지안 모델을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대량 조직 RNA 시퀀싱은 세포 유형별 동역학을 가려버리는 평균 유전자 발현 프로파일을 제공하므로, 이를 해결하고자 한다.

Method: 고해상도 단일세포 참조를 사용하여 대량 RNA-seq 데이터를 세포 유형별 유전자 발현 프로파일과 비율로 분해하는 확률론적 계층 베이지안 모델을 적용하였다.

Result: 모델을 사용하여 월경 주기 동안 인간 자궁 내막 조직의 난소 호르몬에 따른 세포 구성 변화의 유전자 발현 변화를 입증하였다.

Conclusion: 이 연구는 베이지안 접근법이 참조 불일치와 노이즈에 강건함을 보여주며, 생물학적 의의와 임상적 함의, 그리고 공간 전사체학 통합을 포함한 향후 방향에 대해 논의한다.

Abstract: Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene
expression profiles, obscuring cell type-specific dynamics. To address this, we
present a probabilistic hierarchical Bayesian model that deconvolves bulk
RNA-seq data into constituent cell-type expression profiles and proportions,
leveraging a high-resolution single-cell reference. We apply our model to human
endometrial tissue across the menstrual cycle, a context characterized by
dramatic hormone-driven cellular composition changes. Our extended framework
provides a principled inference of cell type proportions and cell-specific gene
expression changes across cycle phases. We demonstrate the model's structure,
priors, and inference strategy in detail, and we validate its performance with
simulations and comparisons to existing methods. The results reveal dynamic
shifts in epithelial, stromal, and immune cell fractions between menstrual
phases, and identify cell-type-specific differential gene expression associated
with endometrial function (e.g., decidualization markers in stromal cells
during the secretory phase). We further conduct robustness tests and show that
our Bayesian approach is resilient to reference mismatches and noise. Finally,
we discuss the biological significance of our findings, potential clinical
implications for fertility and endometrial disorders, and future directions,
including integration of spatial transcriptomics.

</details>


### [5] [AI Agents in Drug Discovery](https://arxiv.org/abs/2510.27130)
*Srijit Seal,Dinh Long Huynh,Moudather Chelbi,Sara Khosravi,Ankur Kumar,Mattson Thieme,Isaac Wilks,Mark Davies,Jessica Mustali,Yannick Sun,Nick Edwards,Daniil Boiko,Andrei Tyrin,Douglas W. Selinger,Ayaan Parikh,Rahul Vijayan,Shoman Kasbekar,Dylan Reid,Andreas Bender,Ola Spjuth*

Main category: cs.LG

TL;DR: AI 에이전트가 약물 발견에서 혁신적인 도구로 부상하고 있다.


<details>
  <summary>Details</summary>
Motivation: 약물 발견에서 AI 에이전트의 능력을 활용하여 연구 효율성을 높이고 싶다.

Method: 대형 언어 모델을 기반으로 다양한 생물 의학 데이터를 통합하고, 로봇 플랫폼을 통해 실험을 수행하는 에이전틱 AI 아키텍처를 소개한다.

Result: 실제 약물 발견 환경에서 AI 시스템의 실질적인 구현과 정량적 영향을 보여준다.

Conclusion: 데이터 이질성, 시스템 신뢰성, 프라이버시, 벤치마킹의 현재 문제점을 논의하고 과학과 이전을 지원하는 기술의 미래 방향을 제시한다.

Abstract: Artificial intelligence (AI) agents are emerging as transformative tools in
drug discovery, with the ability to autonomously reason, act, and learn through
complicated research workflows. Building on large language models (LLMs)
coupled with perception, computation, action, and memory tools, these agentic
AI systems could integrate diverse biomedical data, execute tasks, carry out
experiments via robotic platforms, and iteratively refine hypotheses in closed
loops. We provide a conceptual and technical overview of agentic AI
architectures, ranging from ReAct and Reflection to Supervisor and Swarm
systems, and illustrate their applications across key stages of drug discovery,
including literature synthesis, toxicity prediction, automated protocol
generation, small-molecule synthesis, drug repurposing, and end-to-end
decision-making. To our knowledge, this represents the first comprehensive work
to present real-world implementations and quantifiable impacts of agentic AI
systems deployed in operational drug discovery settings. Early implementations
demonstrate substantial gains in speed, reproducibility, and scalability,
compressing workflows that once took months into hours while maintaining
scientific traceability. We discuss the current challenges related to data
heterogeneity, system reliability, privacy, and benchmarking, and outline
future directions towards technology in support of science and translation.

</details>


### [6] [Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores](https://arxiv.org/abs/2510.27145)
*Sein Kwon,Seulgi Baek,Hyunseo Yang,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: RelTune은 매개변수 간의 의존성을 관계형 그래프로 표현하고, 성능 관련 의미를 인코딩하는 GNN 기반 잠재 임베딩을 학습하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 및 이종 데이터를 관리하기 위한 데이터베이스 관리 시스템(DBMS)의 성능은 구성 매개변수에 의해 크게 영향을 받는다. 효과적인 매개변수 조정은 다양한 워크로드에 적응하고 대기 시간을 최소화하면서 처리량을 극대화하는 데 필수적이다.

Method: RelTune은 매개변수 간의 의존성을 관계형 그래프로 나타내고, 성능 관련 의미를 인코딩하는 GNN 기반 임베딩을 학습하는 프레임워크이다. 또한, Hybrid-Score-Guided Bayesian Optimization(HBO)을 도입하여 대리 예측과 이전에 높은 성능을 보인 구성에 대한 근접성을 측정하는 연결 점수를 결합한다.

Result: 여러 DBMS와 워크로드에 대한 실험 결과, RelTune은 기존 BO 기반 방법보다 더 빠른 수렴과 높은 최적화 효율성을 달성하여 평가된 모든 시나리오에서 최신 성능을 기록하였다.

Conclusion: RelTune은 대규모 DBMS에서 자동 조정의 한계를 극복하고 성능을 유의미하게 향상시킨다.

Abstract: Database Management Systems (DBMSs) are fundamental for managing large-scale
and heterogeneous data, and their performance is critically influenced by
configuration parameters. Effective tuning of these parameters is essential for
adapting to diverse workloads and maximizing throughput while minimizing
latency. Recent research has focused on automated configuration optimization
using machine learning; however, existing approaches still exhibit several key
limitations. Most tuning frameworks disregard the dependencies among
parameters, assuming that each operates independently. This simplification
prevents optimizers from leveraging relational effects across parameters,
limiting their capacity to capture performancesensitive interactions. Moreover,
to reduce the complexity of the high-dimensional search space, prior work often
selects only the top few parameters for optimization, overlooking others that
contribute meaningfully to performance. Bayesian Optimization (BO), the most
common method for automatic tuning, is also constrained by its reliance on
surrogate models, which can lead to unstable predictions and inefficient
exploration. To overcome these limitations, we propose RelTune, a novel
framework that represents parameter dependencies as a Relational Graph and
learns GNN-based latent embeddings that encode performancerelevant semantics.
RelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),
which combines surrogate predictions with an Affinity Score measuring proximity
to previously high-performing configurations. Experimental results on multiple
DBMSs and workloads demonstrate that RelTune achieves faster convergence and
higher optimization efficiency than conventional BO-based methods, achieving
state-of-the-art performance across all evaluated scenarios.

</details>


### [7] [Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](https://arxiv.org/abs/2510.27172)
*Zixuan Hu,Li Shen,Zhenyi Wang,Yongxian Wei,Dacheng Tao*

Main category: cs.LG

TL;DR: 이 논문에서는 대형 언어 모델에 대한 유해한 파인튜닝의 위험을 해결하기 위해 공격 시뮬레이션 없이 적응형 방어 전략인 Bayesian Data Scheduler(BDS)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 파인튜닝 서비스에서 유해한 파인튜닝이 안전 위험을 초래함.

Method: BDS는 데이터를 안전 속성으로 샘플링하여 파인튜닝과 조정 데이터셋을 조건으로 하는 베이지안 추론 문제로 유해한 파인튜닝 방어를 공식화합니다.

Result: BDS는 다양한 공격 및 방어 설정에서 최첨단 성능을 보여줍니다.

Conclusion: 또한, BDS는 새로운 데이터로의 효율적인 전이를 가능하게 하는 신경 스케줄러를 도입합니다.

Abstract: Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service
for large language models. Existing defense strategies preemptively build
robustness via attack simulation but suffer from fundamental limitations: (i)
the infeasibility of extending attack simulations beyond bounded threat models
due to the inherent difficulty of anticipating unknown attacks, and (ii)
limited adaptability to varying attack settings, as simulation fails to capture
their variability and complexity. To address these challenges, we propose
Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with
no need for attack simulation. BDS formulates harmful fine-tuning defense as a
Bayesian inference problem, learning the posterior distribution of each data
point's safety attribute, conditioned on the fine-tuning and alignment
datasets. The fine-tuning process is then constrained by weighting data with
their safety attributes sampled from the posterior, thus mitigating the
influence of harmful data. By leveraging the post hoc nature of Bayesian
inference, the posterior is conditioned on the fine-tuning dataset, enabling
BDS to tailor its defense to the specific dataset, thereby achieving adaptive
defense. Furthermore, we introduce a neural scheduler based on amortized
Bayesian learning, enabling efficient transfer to new data without retraining.
Comprehensive results across diverse attack and defense settings demonstrate
the state-of-the-art performance of our approach. Code is available at
https://github.com/Egg-Hu/Bayesian-Data-Scheduler.

</details>


### [8] [SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference](https://arxiv.org/abs/2510.27182)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.LG

TL;DR: 기계 학습 모델의 동적 오프로드를 통해 비용을 최소화하고 처리 지연을 균형 있게 조정하는 방법을 제시한다. 이는 기존 연구에서 간과된 현실적 요인들을 고려한다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 응용 프로그램에서 처리 및 전송 지연을 최소화하고 비용을 줄이기 위해 다양한 자원 오케스트레이션 서비스 간의 동적 오프로드가 필요하다.

Method: 각 ML 쿼리를 비순환적 단계 시퀀스로 모델링하고, 각 단계에서 요청이 종료될 수 있는 내부 또는 최종 분류기에 이르는 모델 파라미터 블록을 구성한다.

Result: SERFLOW는 단계별 자원 프로비저닝을 통해 각 단계에서 종료되는 요청의 비율을 고려하여 클라우드 비용을 23% 이상 줄인다.

Conclusion: SERFLOW는 VM 및 서버리스 함수 간의 적응형 로드 밸런싱을 통해 동적 워크로드에 효율적으로 적응하면서 클라우드 비용을 절감한다.

Abstract: Dynamic offloading of Machine Learning (ML) model partitions across different
resource orchestration services, such as Function-as-a-Service (FaaS) and
Infrastructure-as-a-Service (IaaS), can balance processing and transmission
delays while minimizing costs of adaptive inference applications. However,
prior work often overlooks real-world factors, such as Virtual Machine (VM)
cold starts, requests under long-tail service time distributions, etc. To
tackle these limitations, we model each ML query (request) as traversing an
acyclic sequence of stages, wherein each stage constitutes a contiguous block
of sparse model parameters ending in an internal or final classifier where
requests may exit. Since input-dependent exit rates vary, no single resource
configuration suits all query distributions. IaaS-based VMs become
underutilized when many requests exit early, yet rapidly scaling to handle
request bursts reaching deep layers is impractical. SERFLOW addresses this
challenge by leveraging FaaS-based serverless functions (containers) and using
stage-specific resource provisioning that accounts for the fraction of requests
exiting at each stage. By integrating this provisioning with adaptive load
balancing across VMs and serverless functions based on request ingestion,
SERFLOW reduces cloud costs by over $23\%$ while efficiently adapting to
dynamic workloads.

</details>


### [9] [Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments](https://arxiv.org/abs/2510.27287)
*Harsh Vishwakarma,Ankush Agarwal,Ojas Patil,Chaitanya Devaguptapu,Mahesh Chandran*

Main category: cs.LG

TL;DR: 본 논문은 기업 시스템의 생산성과 의사결정 향상을 위한 LLM 기반 시스템의 통합 및 이를 위한 EnterpriseBench라는 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기업 시스템은 직원과 고객 간의 생산성과 의사결정을 개선하는 데 필수적이다.

Method: EnterpriseBench는 소프트웨어 엔지니어링, 인사, 재무, 관리 분야의 500가지 다양한 작업을 시뮬레이션하는 종합적인 벤치마크를 제공한다.

Result: 최신 LLM 에이전트를 사용한 실험에서 가장 우수한 모델조차도 41.8%의 작업 완료율에 그쳐 기업 중심 AI 시스템의 개선 기회를 강조하였다.

Conclusion: 이 연구는 기업 환경에서의 복잡성을 고려한 AI 시스템 개발의 필요성과 전반적인 개선 여지를 제시한다.

Abstract: Enterprise systems are crucial for enhancing productivity and decision-making
among employees and customers. Integrating LLM based systems into enterprise
systems enables intelligent automation, personalized experiences, and efficient
information retrieval, driving operational efficiency and strategic growth.
However, developing and evaluating such systems is challenging due to the
inherent complexity of enterprise environments, where data is fragmented across
multiple sources and governed by sophisticated access controls. We present
EnterpriseBench, a comprehensive benchmark that simulates enterprise settings,
featuring 500 diverse tasks across software engineering, HR, finance, and
administrative domains. Our benchmark uniquely captures key enterprise
characteristics including data source fragmentation, access control
hierarchies, and cross-functional workflows. Additionally, we provide a novel
data generation pipeline that creates internally consistent enterprise tasks
from organizational metadata. Experiments with state-of-the-art LLM agents
demonstrate that even the most capable models achieve only 41.8% task
completion, highlighting significant opportunities for improvement in
enterprise-focused AI systems.

</details>


### [10] [Simplex-to-Euclidean Bijections for Categorical Flow Matching](https://arxiv.org/abs/2510.27480)
*Bernardo Williams,Victor M. Yeom-Song,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 단순체에서 확률 분포를 학습하고 샘플링하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 우리는 단순체에서 정의된 확률 분포를 모델링하고 샘플링하는 새로운 접근 방식을 탐구하고자 한다.

Method: 이 논문은 Aitchison 기하학을 활용하여 열린 단순체를 유클리드 공간으로 매핑하는 매끄러운 일대일 대응을 정의하는 방법을 사용한다. 또한 이는 디리클레 보간을 통해 이산 관측치를 연속적으로 변환한다.

Result: 이 접근 방식은 유클리드 공간에서 밀도를 모델링할 수 있게 하며, 원래의 이산 분포를 정확하게 복원할 수 있다.

Conclusion: 이 방법은 Riemannian 기하학 또는 사용자 지정 노이즈 프로세스를 사용하는 이전 방법들과 비교하여 만족스러운 성능을 발휘하며, 합성 데이터와 실제 데이터 셋 모두에서 경쟁력 있는 성능을 달성한다.

Abstract: We propose a method for learning and sampling from probability distributions
supported on the simplex. Our approach maps the open simplex to Euclidean space
via smooth bijections, leveraging the Aitchison geometry to define the
mappings, and supports modeling categorical data by a Dirichlet interpolation
that dequantizes discrete observations into continuous ones. This enables
density modeling in Euclidean space through the bijection while still allowing
exact recovery of the original discrete distribution. Compared to previous
methods that operate on the simplex using Riemannian geometry or custom noise
processes, our approach works in Euclidean space while respecting the Aitchison
geometry, and achieves competitive performance on both synthetic and real-world
data sets.

</details>


### [11] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 단일 추론 모델의 연구는 불완전하여 인과관계와 근본적인 계산을 이해하는 데 부족하다. 본 연구는 재샘플링을 통해 모델의 결정 과정을 연구하며, 자가 보호 문장이 검은메일을 의미 있게 이끌지 않음을 보여준다. 인위적인 편집이 추론을 조절할 수 있는지, 그리고 반복되는 추론 단계를 제거할 때의 영향을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 단일 사고흐름(Chain-of-Thought, CoT)을 연구하는 기존 작업은 인과관계와 근본적인 계산을 이해하는 데 부족하다고 주장한다.

Method: 재샘플링을 사용하여 모델 결정 과정에 대한 사례 연구를 수행하고, 특정 문장의 하류 효과를 측정하며 CoT의 인공 수정을 평가한다.

Result: 자가 보호 문장은 작고 인과적 영향을 미치며, 인위적인 편집은 정책을 벗어나고 재샘플링이 더 안정적인 결과를 낸다.

Conclusion: 재샘플링을 통한 분포 연구는 신뢰할 수 있는 인과 분석과 모델 추론의 명확한 서사를 제공한다.

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


### [12] [Leveraging Generic Time Series Foundation Models for EEG Classification](https://arxiv.org/abs/2510.27522)
*Théo Gnassounou,Yessin Moakher,Shifeng Xie,Vasilii Feofanov,Ievgen Redko*

Main category: cs.LG

TL;DR: 시간 시계열을 위한 기초 모델이 강력한 일반 목적의 backbone으로 떠오르고 있지만, 생물 의학 신호, 특히 뇌파 검사(EEG)에 대한 잠재력은 아직 탐구되지 않았다. 이 연구에서는 최근 제안된 시간 시계열 분류 기초 모델을 motor imagery 분류와 수면 단계 예측을 포함한 다양한 EEG 작업에 적용 가능한지 조사하였다. 두 가지 전처리 방법을 시험했으며, 두 가지 방법 모두 EEGNet과 CBraMod를 지속적으로 능가하는 결과를 보였다. 이 결과는 일반 시간 시계열 기초 모델이 EEG에 효과적으로 전이될 수 있음을 시사한다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 데이터의 기초 모델이 생물 의학 신호에 어떻게 적용될 수 있는지를 탐구하기 위해.

Method: 최근 제안된 시간 시계열 분류 기초 모델을 motor imagery 분류 및 수면 단계 예측과 같은 EEG 작업에 적용하고, 두 가지 전처리 방법을 시험했다.

Result: 두 가지 전처리 방법 모두 EEGNet 및 CBraMod를 지속적으로 능가하는 강력한 성능을 보였다.

Conclusion: 일반 시간 시계열 기초 모델은 비신경 출처의 데이터 또는 합성 신호로 전처리되어도 EEG에 효과적으로 전이될 수 있으며, 다양한 시간 시계열 문헌의 발전이 EEG 분석에 기여할 수 있음을 제안한다.

Abstract: Foundation models for time series are emerging as powerful general-purpose
backbones, yet their potential for domain-specific biomedical signals such as
electroencephalography (EEG) remains rather unexplored. In this work, we
investigate the applicability a recently proposed time series classification
foundation model, to a different EEG tasks such as motor imagery classification
and sleep stage prediction. We test two pretraining regimes: (a) pretraining on
heterogeneous real-world time series from multiple domains, and (b) pretraining
on purely synthetic data. We find that both variants yield strong performance,
consistently outperforming EEGNet, a widely used convolutional baseline, and
CBraMod, the most recent EEG-specific foundation model. These results suggest
that generalist time series foundation models, even when pretrained on data of
non-neural origin or on synthetic signals, can transfer effectively to EEG. Our
findings highlight the promise of leveraging cross-domain pretrained models for
brain signal analysis, suggesting that EEG may benefit from advances in the
broader time series literature.

</details>


### [13] [Active transfer learning for structural health monitoring](https://arxiv.org/abs/2510.27525)
*J. Poole,N. Dervilis,K. Worden,P. Gardner,V. Giglioni,R. S. Mills,A. J. Hughes*

Main category: cs.LG

TL;DR: 이 논문은 구조 건강 모니터링 시스템에서 데이터 부족 문제를 해결하기 위한 베이지안 프레임워크를 제안하며, 능동 샘플링 전략과 결합하여 라벨이 부족한 상황에서 분류 모델의 데이터 효율성을 향상시킬 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 구조 건강 모니터링(SHM) 시스템 훈련을 위한 데이터는 종종 비싸고 비현실적이며, 특히 라벨링된 데이터에서 더욱 그렇다.

Method: 이 논문은 엄격한 라벨링을 통해 다수의 구조로부터 데이터를 이용하는 인구 기반 SHM(PBSHM) 위한 도메인 적응(DA) 형태의 전이 학습을 제안한다.

Result: 전이 학습과 능동 학습을 결합하여 라벨 부족 상황에서 분류 모델을 학습하는 데 있어 데이터 효율성을 향상시킬 수 있음을 보여준다.

Conclusion: 이 접근법은 구조의 운영 수명 동안 검사 횟수를 줄이고 운영 비용을 절감할 수 있는 가능성을 보여준다.

Abstract: Data for training structural health monitoring (SHM) systems are often
expensive and/or impractical to obtain, particularly for labelled data.
Population-based SHM (PBSHM) aims to address this limitation by leveraging data
from multiple structures. However, data from different structures will follow
distinct distributions, potentially leading to large generalisation errors for
models learnt via conventional machine learning methods. To address this issue,
transfer learning -- in the form of domain adaptation (DA) -- can be used to
align the data distributions. Most previous approaches have only considered
\emph{unsupervised} DA, where no labelled target data are available; they do
not consider how to incorporate these technologies in an online framework --
updating as labels are obtained throughout the monitoring campaign. This paper
proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA
mappings using a limited quantity of labelled target data. In addition, this
model is integrated into an active sampling strategy to guide inspections to
select the most informative observations to label -- leading to further
reductions in the required labelled data to learn a target classifier. The
effectiveness of this methodology is evaluated on a population of experimental
bridges. Specifically, this population includes data corresponding to several
damage states, as well as, a comprehensive set of environmental conditions. It
is found that combining transfer learning and active learning can improve data
efficiency when learning classification models in label-scarce scenarios. This
result has implications for data-informed operation and maintenance of
structures, suggesting a reduction in inspections over the operational lifetime
of a structure -- and therefore a reduction in operational costs -- can be
achieved.

</details>


### [14] [AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering](https://arxiv.org/abs/2510.27537)
*Mohammad Zahangir Alam,Khandoker Ashik Uz Zaman,Mahdi H. Miraz*

Main category: cs.LG

TL;DR: AstuteRAG-FQA는 금융 질문 응답을 위한 적응형 RAG 프레임워크로, 지식 집약적인 작업에서의 효과를 향상시키기 위해 여러 도전 과제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 지식 집약적인 작업에서 도메인 특수성과 시간적 관련성을 향상시키고, 허위 정보를 줄이기 위해 RAG의 적용이 필요합니다.

Method: AstuteRAG-FQA는 작업 인식 프롬프트 엔지니어링을 활용하며, 개방형 및 독점 금융 데이터를 통합하여 혼합 검색 전략을 사용하고 실시간 문의 복잡성에 따라 동적으로 적응하는 프롬프트 프레임워크를 제공합니다.

Result: 다양한 금융 쿼리에 대한 네 가지 계층 과제 분류를 제안하며, 민감한 금융 정보를 보호하기 위한 다층 보안 메커니즘을 포함합니다.

Conclusion: 자동화된 규제 검증 시스템을 통해 실시간 준수 모니터링을 구현하고, 다양한 금융 환경에서의 효율성과 실행 가능성을 분석합니다.

Abstract: Retrieval-Augmented Generation (RAG) shows significant promise in
knowledge-intensive tasks by improving domain specificity, enhancing temporal
relevance, and reducing hallucinations. However, applying RAG to finance
encounters critical challenges: restricted access to proprietary datasets,
limited retrieval accuracy, regulatory constraints, and sensitive data
interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored
for Financial Question Answering (FQA), leveraging task-aware prompt
engineering to address these challenges. The framework uses a hybrid retrieval
strategy integrating both open-source and proprietary financial data while
maintaining strict security protocols and regulatory compliance. A dynamic
prompt framework adapts in real time to query complexity, improving precision
and contextual relevance. To systematically address diverse financial queries,
we propose a four-tier task classification: explicit factual, implicit factual,
interpretable rationale, and hidden rationale involving implicit causal
reasoning. For each category, we identify key challenges, datasets, and
optimization techniques within the retrieval and generation process. The
framework incorporates multi-layered security mechanisms including differential
privacy, data anonymization, and role-based access controls to protect
sensitive financial information. Additionally, AstuteRAG-FQA implements
real-time compliance monitoring through automated regulatory validation systems
that verify responses against industry standards and legal obligations. We
evaluate three data integration techniques - contextual embedding, small model
augmentation, and targeted fine-tuning - analyzing their efficiency and
feasibility across varied financial environments.

</details>


### [15] [ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling](https://arxiv.org/abs/2510.27610)
*Zhuohan Wang,Ziwei Zhu,Ziniu Li,Congliang Chen,Yizhou Han,Yufeng Lin,Zhihang Lin,Angyang Gu,Xinglin Hu,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: ORGEval은 산업 응용을 위한 최적화 문제의 자동화를 평가하는 그래프 이론 기반 프레임워크로, LLM의 성능을 개선하고 검증하는 데 초점을 맞춘다.


<details>
  <summary>Details</summary>
Motivation: 산업 응용을 위한 최적화 문제를 수립하는 것은 많은 수작업과 전문 지식을 요구한다. LLM이 이 과정을 자동화할 가능성이 있지만, 그 성능을 평가하는 것은 견고한 메트릭이 부족하여 어렵다.

Method: ORGEval은 최적화 모델을 그래프로 표현하여 등가성 검출을 그래프 동형 테스트로 단순화한다. 또한 SD(대칭 분해 가능성) 조건 하에 WL 테스트를 통해 동형성을 올바르게 감지하도록 검증한다.

Result: 실험 결과에 따르면, ORGEval은 모델의 동등성을 성공적으로 검출하고 무작위 매개변수 구성을 통해 100% 일관된 결과를 생성하며, 특히 어려운 문제에서 솔버 기반 방법보다 상당히 더 우수한 성능을 보인다.

Conclusion: ORGEval을 활용하여 Bench4Opt 데이터세트를 구성하고 최신 LLM의 최적화 모델링 성능을 벤치마크한 결과, 모든 LLM에서 최적화 모델링이 여전히 도전적이지만 DeepSeek-V3와 Claude-Opus-4가 직접 프롬프트 하에서 가장 높은 정확도를 기록하며 주목할 만한 성과를 냈다.

Abstract: Formulating optimization problems for industrial applications demands
significant manual effort and domain expertise. While Large Language Models
(LLMs) show promise in automating this process, evaluating their performance
remains difficult due to the absence of robust metrics. Existing solver-based
approaches often face inconsistency, infeasibility issues, and high
computational costs. To address these issues, we propose ORGEval, a
graph-theoretic evaluation framework for assessing LLMs' capabilities in
formulating linear and mixed-integer linear programs. ORGEval represents
optimization models as graphs, reducing equivalence detection to graph
isomorphism testing. We identify and prove a sufficient condition, when the
tested graphs are symmetric decomposable (SD), under which the
Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.
Building on this, ORGEval integrates a tailored variant of the WL-test with an
SD detection algorithm to evaluate model equivalence. By focusing on structural
equivalence rather than instance-level configurations, ORGEval is robust to
numerical variations. Experimental results show that our method can
successfully detect model equivalence and produce 100\% consistent results
across random parameter configurations, while significantly outperforming
solver-based methods in runtime, especially on difficult problems. Leveraging
ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs
on optimization modeling. Our results reveal that although optimization
modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4
achieve the highest accuracies under direct prompting, outperforming even
leading reasoning models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Cheng Pan,Meng Han,Yifei Zhu,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: LAFA는 대규모 언어 모델과 연합 분석을 결합하여 자연어 쿼리를 지원하고 개인 정보 보호를 위한 데이터 분석 시스템을 개발한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 데이터 분석을 자동화하는 데 유망하지만 기존 프레임워크는 개인 정보 보호에 취약하다.

Method: LAFA는 자연어 쿼리를 받아들이고 이를 최적화된 실행 워크플로로 변환하는 계층적 다중 에이전트 아키텍처를 도입한다.

Result: LAFA는 기존 접근 방식을 능가하여 높은 실행 성공률을 달성하고 자원 집약적인 분석 작업을 크게 줄인다.

Conclusion: 이 연구는 연합 분석 환경에서 자연어 입력을 지원하는 개인 정보 보호 중심의 LLM 기반 분석의 실용적인 기초를 마련한다.

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [17] [The Denario project: Deep knowledge AI agents for scientific discovery](https://arxiv.org/abs/2510.26887)
*Francisco Villaescusa-Navarro,Boris Bolliet,Pablo Villanueva-Domingo,Adrian E. Bayer,Aidan Acquah,Chetana Amancharla,Almog Barzilay-Siegal,Pablo Bermejo,Camille Bilodeau,Pablo Cárdenas Ramírez,Miles Cranmer,Urbano L. França,ChangHoon Hahn,Yan-Fei Jiang,Raul Jimenez,Jun-Young Lee,Antonio Lerario,Osman Mamun,Thomas Meier,Anupam A. Ojha,Pavlos Protopapas,Shimanto Roy,David N. Spergel,Pedro Tarancón-Álvarez,Ujjwal Tiwari,Matteo Viel,Digvijay Wadekar,Chi Wang,Bonny Y. Wang,Licong Xu,Yossi Yovel,Shuwen Yue,Wen-Han Zhou,Qiyao Zhu,Jiajun Zou,Íñigo Zubeldia*

Main category: cs.AI

TL;DR: Denario는 과학 연구 보조를 위해 설계된 AI 다중 에이전트 시스템으로, 다양한 작업을 수행할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 과학 연구의 효율성을 높이기 위해 AI를 활용한 보조 도구를 개발하고자 하였다.

Method: 모듈형 아키텍처를 통해 특정 작업을 수행하거나 Cmbagent를 백엔드로 사용하여 종합적인 과학 분석을 수행한다.

Result: 여러 과학 분야에서 생성된 AI 논문을 다양한 전문가에게 평가받았다.

Conclusion: AI 연구의 윤리적 함의와 과학 철학과의 관계를 논의하였다.

Abstract: We present Denario, an AI multi-agent system designed to serve as a
scientific research assistant. Denario can perform many different tasks, such
as generating ideas, checking the literature, developing research plans,
writing and executing code, making plots, and drafting and reviewing a
scientific paper. The system has a modular architecture, allowing it to handle
specific tasks, such as generating an idea, or carrying out end-to-end
scientific analysis using Cmbagent as a deep-research backend. In this work, we
describe in detail Denario and its modules, and illustrate its capabilities by
presenting multiple AI-generated papers generated by it in many different
scientific disciplines such as astrophysics, biology, biophysics, biomedical
informatics, chemistry, material science, mathematical physics, medicine,
neuroscience and planetary science. Denario also excels at combining ideas from
different disciplines, and we illustrate this by showing a paper that applies
methods from quantum physics and machine learning to astrophysical data. We
report the evaluations performed on these papers by domain experts, who
provided both numerical scores and review-like feedback. We then highlight the
strengths, weaknesses, and limitations of the current system. Finally, we
discuss the ethical implications of AI-driven research and reflect on how such
technology relates to the philosophy of science. We publicly release the code
at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run
directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and
the full app will be deployed on the cloud.

</details>


### [18] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델(LLM) 에이전트를 자기 개선 및 동료 학습을 통해 인간 수준 지능으로 진화시키는 중요성을 강조한다. CATArena라는 평가 플랫폼을 통해 에이전트의 학습 능력과 전략 최적화를 반복적인 상호작용과 피드백을 통해 평가한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 발전을 위해 자기 개선과 동료 학습의 중요성을 강조하고, 현재 벤치마크의 한계를 극복할 필요가 있다.

Method: 반복적이고 경쟁적인 동료 학습 프레임워크를 제안하며, CATArena라는 토너먼트 스타일의 평가 플랫폼을 도입하여 다양한 게임에서 연속적이고 동적인 평가를 가능하게 한다.

Result: CATArena는 학습 능력과 전략 코딩에 대한 신뢰성 있는 벤치마킹을 제공하며, 최소 및 상업용 코드 에이전트를 포함한 실험 결과를 통해 검증되었다.

Conclusion: CATArena는 에이전트 능력, 특히 학습 능력과 전략 최적화에 대한 안정적이고 확장 가능한 벤치마킹을 제공한다.

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [19] [Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base](https://arxiv.org/abs/2510.26854)
*Yu Li,Yuan Huang,Tao Wang,Caiyu Fan,Xiansheng Cai,Sihan Hu,Xinzijian Liu,Cheng Shi,Mingjun Xu,Zhen Wang,Yan Wang,Xiangqi Jin,Tianhan Zhang,Linfeng Zhang,Lei Wang,Youjin Deng,Pan Zhang,Weijie Sun,Xingyu Li,Weinan E,Linfeng Zhang,Zhiyuan Yao,Kun Chen*

Main category: cs.AI

TL;DR: 이 논문은 과학적 추론을 압축하는 대신 이를 확장하여 검증 가능한 지식 기반을 구축하는 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 과학 자료는 추론 과정을 생략하고 결론만 제시하여 검증을 방해하고, 개념 간의 논리적 및 인과적 연결 고리를 약화시킵니다.

Method: 스케일 가능한 프레임워크를 통해 검증 가능한 Long Chain-of-Thought (LCoT) 지식 기반을 구축하고 SciencePedia라는 백과사전에 이를 투영합니다. 약 200가지 코스를 통해 유도된 소크라틱 에이전트가 약 300만 개의 1차 원칙 질문을 생성합니다.

Result: Plato로 합성된 논문이 더 높은 지식 포인트 밀도와 낮은 사실 오류율을 보여주었습니다.

Conclusion: 이 검증 가능한 LCoT 지식 기반 위에 구축된 이 접근 방식은 신뢰할 수 있는 교차 분야의 과학적 합성을 가능하게 합니다.

Abstract: Most scientific materials compress reasoning, presenting conclusions while
omitting the derivational chains that justify them. This compression hinders
verification by lacking explicit, step-wise justifications and inhibits
cross-domain links by collapsing the very pathways that establish the logical
and causal connections between concepts. We introduce a scalable framework that
decompresses scientific reasoning, constructing a verifiable Long
Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent
encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,
reductionist strategy: a Socratic agent, guided by a curriculum of around 200
courses, generates approximately 3 million first-principles questions. To
ensure high fidelity, multiple independent solver models generate LCoTs, which
are then rigorously filtered by prompt sanitization and cross-model answer
consensus, retaining only those with verifiable endpoints. This verified corpus
powers the Brainstorm Search Engine, which performs inverse knowledge search --
retrieving diverse, first-principles derivations that culminate in a target
concept. This engine, in turn, feeds the Plato synthesizer, which narrates
these verified chains into coherent articles. The initial SciencePedia
comprises approximately 200,000 fine-grained entries spanning mathematics,
physics, chemistry, biology, engineering, and computation. In evaluations
across six disciplines, Plato-synthesized articles (conditioned on retrieved
LCoTs) exhibit substantially higher knowledge-point density and significantly
lower factual error rates than an equally-prompted baseline without retrieval
(as judged by an external LLM). Built on this verifiable LCoT knowledge base,
this reasoning-centric approach enables trustworthy, cross-domain scientific
synthesis at scale and establishes the foundation for an ever-expanding
encyclopedia.

</details>


### [20] [SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation](https://arxiv.org/abs/2510.26989)
*Agorakis Bompotas,Konstantinos Koutras,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Dimitra Gariza,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.AI

TL;DR: 글로벌 농업 부문이 스마트 농업 솔루션을 통해 효율적이고 지속 가능한 농업으로 변혁하고 있다.


<details>
  <summary>Details</summary>
Motivation: 식품 수요 증가, 기후 변동성 및 지속 가능한 관행에 대한 필요성에 의해 농업 부문이 변화하고 있다.

Method: 스마트 농업 플랫폼 SUSTAINABLE을 설계하여 IoT, AI, 위성 이미징 및 역할 기반 작업 조정을 통합하였다.

Result: 현재의 스마트 농업 솔루션들을 비교 평가하고, SUSTAINABLE의 주요 기능을 소개한다.

Conclusion: 이 논문은 지중해 포도밭에 맞춰진 역할 인식 작업 관리 및 실시간 환경 데이터를 포함한 SUSTAINABLE의 기능을 강조한다.

Abstract: The global agricultural sector is undergoing a transformative shift, driven
by increasing food demands, climate variability and the need for sustainable
practices. SUSTAINABLE is a smart farming platform designed to integrate IoT,
AI, satellite imaging, and role-based task orchestration to enable efficient,
traceable, and sustainable agriculture with a pilot usecase in viticulture.
This paper explores current smart agriculture solutions, presents a comparative
evaluation, and introduces SUSTAINABLE's key features, including satellite
index integration, real-time environmental data, and role-aware task management
tailored to Mediterranean vineyards.

</details>


### [21] [Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement](https://arxiv.org/abs/2510.27051)
*Aaditya Shukla,Sidney Knowles,Meenakshi Madugula,Dave Farris,Ryan Angilly,Santiago Pombo,Anbang Xu,Lu An,Abhinav Balasubramanian,Tan Yu,Jiaxiang Ren,Rama Akkiraju*

Main category: cs.AI

TL;DR: 본 연구는 NVIDIA의 NVInfo AI에서 데이터 플라이휠을 구현하여 기업 AI 에이전트의 지속적인 적응과 성능 향상을 달성하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기업 AI 에이전트는 정확성을 유지하고 대기시간을 줄이며 사용자 요구에 맞춰 지속적으로 적응해야 한다.

Method: MAPE 기반 데이터 플라이휠을 작동시켜 RAG 파이프라인의 실패를 체계적으로 해결하고 지속 학습을 가능하게 하는 폐쇄 루프 시스템을 구축했다.

Result: 배포 후 3개월 동안 495개의 부정 샘플을 수집하고, 라우팅 오류(5.25%)와 질의 재구성 오류(3.2%)라는 두 가지 주요 실패 모드를 분석했다. 라우팅의 경우, Llama 3.1 70B 모델을 8B 미세 조정 모델로 교체하여 96% 정확도를 달성하고 모델 크기를 10배 줄이며 대기 시간이 70% 개선되었다.

Conclusion: HITL 피드백이 데이터 플라이휠 내에서 구조화될 경우 기업 AI 에이전트를 자가 개선 시스템으로 변화시키는 방법을 보여준다.

Abstract: Enterprise AI agents must continuously adapt to maintain accuracy, reduce
latency, and remain aligned with user needs. We present a practical
implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts
(MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a
MAPE-driven data flywheel, we built a closed-loop system that systematically
addresses failures in retrieval-augmented generation (RAG) pipelines and
enables continuous learning. Over a 3-month post-deployment period, we
monitored feedback and collected 495 negative samples. Analysis revealed two
major failure modes: routing errors (5.25\%) and query rephrasal errors
(3.2\%). Using NVIDIA NeMo microservices, we implemented targeted improvements
through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a
fine-tuned 8B variant, achieving 96\% accuracy, a 10x reduction in model size,
and 70\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\%
gain in accuracy and a 40\% latency reduction. Our approach demonstrates how
human-in-the-loop (HITL) feedback, when structured within a data flywheel,
transforms enterprise AI agents into self-improving systems. Key learnings
include approaches to ensure agent robustness despite limited user feedback,
navigating privacy constraints, and executing staged rollouts in production.
This work offers a repeatable blueprint for building robust, adaptive
enterprise AI agents capable of learning from real-world usage at scale.

</details>


### [22] [CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical Reasoning](https://arxiv.org/abs/2510.27094)
*Hamed Mahdavi,Pouria Mahdavinia,Alireza Farhadi,Pegah Mohammadipour,Samira Malek,Majid Daliri,Pedram Mohammadipour,Alireza Hashemi,Amir Khasahmadi,Vasant Honavar*

Main category: cs.AI

TL;DR: 최신 LLMs는 수학 증명 채점에서 상당한 향상을 이루었으며, 다양한 채점 워크플로우를 개발하여 인간의 점수와 높은 일치를 보였다.


<details>
  <summary>Details</summary>
Motivation: 수학 문제 풀이 능력이 향상됨에 따라, LLM이 증명을 평가하는 정확성을 조사하고자 함.

Method: 90개의 Gemini 2.5 Pro 생성 솔루션을 1-4 점척도로 평가하고, IMO/USAMO 2025 문제 세트를 대상으로 0-7 점척도로 채점을 수행하여 증명 분석 능력을 연구함.

Result: 모델이 부정확한 솔루션을 신뢰성 있게 식별하지만 부분 점수 부여에서 보정 격차를 나타냄을 발견함.

Conclusion: 제안된 작업 흐름은 인간 점수와의 합의도가 높고, 다양한 메트릭에서 부분 점수를 일관되게 처리함.

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [23] [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)
*Pouya Hamadanian,Pantea Karimi,Arash Nasr-Esfahany,Kimia Noorbakhsh,Joseph Chandler,Ali ParandehGheibi,Mohammad Alizadeh,Hari Balakrishnan*

Main category: cs.AI

TL;DR: Glia는 인간 전문가의 창의성과 추론 능력과 동등한 컴퓨터 시스템을 설계할 수 있는 AI 아키텍처이다.


<details>
  <summary>Details</summary>
Motivation: AI가 인간 전문가의 수준의 창의성과 추론 능력으로 컴퓨터 시스템의 메커니즘을 설계할 수 있는지 여부에 대한 탐구.

Method: Glia는 대규모 언어 모델을 활용한 다중 에이전트 워크플로우를 통해 각 에이전트가 추론, 실험 및 분석에 특화되어 협력하도록 한다.

Result: 분산 GPU 클러스터에 적용했을 때 Glia는 요청 라우팅, 스케줄링 및 자동 확장을 위한 새로운 알고리즘을 생성하고, 인간 전문가 수준의 성능을 낸다.

Conclusion: 추론 LLM과 구조화된 실험을 결합함으로써 AI가 복잡한 시스템 문제에 대한 창의적이고 이해 가능한 설계를 생성할 수 있음을 시사한다.

Abstract: Can an AI autonomously design mechanisms for computer systems on par with the
creativity and reasoning of human experts? We present Glia, an AI architecture
for networked systems design that uses large language models (LLMs) in a
human-inspired, multi-agent workflow. Each agent specializes in reasoning,
experimentation, and analysis, collaborating through an evaluation framework
that grounds abstract reasoning in empirical feedback. Unlike prior
ML-for-systems methods that optimize black-box policies, Glia generates
interpretable designs and exposes its reasoning process. When applied to a
distributed GPU cluster for LLM inference, it produces new algorithms for
request routing, scheduling, and auto-scaling that perform at human-expert
levels in significantly less time, while yielding novel insights into workload
behavior. Our results suggest that by combining reasoning LLMs with structured
experimentation, an AI can produce creative and understandable designs for
complex systems problems.

</details>


### [24] [GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation](https://arxiv.org/abs/2510.27210)
*Tao Liu,Chongyu Wang,Rongjie Li,Yingchen Yu,Xuming He,Bai Song*

Main category: cs.AI

TL;DR: MLLMs의 GUI 탐색 에이전트에서의 도전 과제를 해결하기 위한 새로운 프레임워크, GUI-Rise,를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 GUI 탐색 작업에서 강건한 추론 및 일반화를 유지하기 위한 필요성을 인식했다.

Method: 구조적 추론, 행동 예측, 이력 요약을 통합한 추론 강화 프레임워크를 개발하였다.

Result: 다양한 벤치마크에서 최신 기술 수준의 결과를 보여주었으며, 특히 도메인 외 시나리오에서 뛰어난 성능을 기록하였다.

Conclusion: 이 프레임워크는 다양한 GUI 탐색 작업에서 효과적인 성능을 입증하였다.

Abstract: While Multimodal Large Language Models (MLLMs) have advanced GUI navigation
agents, current approaches face limitations in cross-domain generalization and
effective history utilization. We present a reasoning-enhanced framework that
systematically integrates structured reasoning, action prediction, and history
summarization. The structured reasoning component generates coherent
Chain-of-Thought analyses combining progress estimation and decision reasoning,
which inform both immediate action predictions and compact history summaries
for future steps. Based on this framework, we train a GUI agent,
\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled
trajectories and reinforcement learning with Group Relative Policy Optimization
(GRPO). This framework employs specialized rewards, including a history-aware
objective, directly linking summary quality to subsequent action performance.
Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art
results under identical training data conditions, with particularly strong
performance in out-of-domain scenarios. These findings validate our framework's
ability to maintain robust reasoning and generalization across diverse GUI
navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.

</details>


### [25] [Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines](https://arxiv.org/abs/2510.27329)
*Kristina Levina,Nikolaos Pappas,Athanasios Karapantelakis,Aneta Vulgarakis Feljan,Jendrik Seipp*

Main category: cs.AI

TL;DR: 본 연구에서는 비순차적인 하위과제가 있는 긴 수평 문제에 대해 효율적인 학습을 가능하게 하는 보상 기계(RM)의 일반화된 형태와 새로운 학습 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 보상 기계는 에이전트가 환경의 보상 구조를 이해하는 데 도움을 주며, 복잡한 비마르코프 작업에서 에이전트의 학습 효율성을 높인다.

Method: 세 가지 일반화된 보상 기계(Numeric RMs, Agenda RMs, Coupled RMs)를 도입하고, Coupled RMs를 활용한 새로운 학습 알고리즘(Q-learning with coupled RMs, CoRM)을 제안한다.

Result: CoRM은 비순차적인 하위 과제가 있는 긴 수평 문제에 대해 최첨단 RM 알고리즘보다 더 나은 확장성을 보여준다.

Conclusion: CoRM은 복잡한 작업의 효율적인 학습을 가능하게 하고, 보상 기계의 한계를 극복하는 데 기여한다.

Abstract: Reward machines (RMs) inform reinforcement learning agents about the reward
structure of the environment. This is particularly advantageous for complex
non-Markovian tasks because agents with access to RMs can learn more
efficiently from fewer samples. However, learning with RMs is ill-suited for
long-horizon problems in which a set of subtasks can be executed in any order.
In such cases, the amount of information to learn increases exponentially with
the number of unordered subtasks. In this work, we address this limitation by
introducing three generalisations of RMs: (1) Numeric RMs allow users to
express complex tasks in a compact form. (2) In Agenda RMs, states are
associated with an agenda that tracks the remaining subtasks to complete. (3)
Coupled RMs have coupled states associated with each subtask in the agenda.
Furthermore, we introduce a new compositional learning algorithm that leverages
coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM
scales better than state-of-the-art RM algorithms for long-horizon problems
with unordered subtasks.

</details>


### [26] [ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use](https://arxiv.org/abs/2510.27363)
*Mengjie Deng,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: ToolScope는 멀티모달 대형 언어 모델이 외부 도구를 통합하여 협력적 추론을 수행할 수 있도록 설계된 에이전틱 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: MLLM이 복잡한 멀티모달 정보에 대해 외부 도구를 유연하고 효율적으로 활용하는 것을 지원하는 데 중점을 두고 있습니다.

Method: ToolScope는 Global Navigator, Agentic Executor, Response Synthesizer의 세 가지 주요 구성 요소로 구성되어 있습니다.

Result: ToolScope는 다양한 도메인에서 VQA 벤치마크에 대해 테스트되었으며 모든 데이터세트에서 평균 +6.69%의 성능 향상을 달성했습니다.

Conclusion: ToolScope는 장기적인 VQA 작업에서 시각적 문맥 저하를 완화하여 MLLM의 성능을 크게 향상시킵니다.

Abstract: Recently, large language models (LLMs) have demonstrated remarkable
problem-solving capabilities by autonomously integrating with external tools
for collaborative reasoning. However, due to the inherently complex and diverse
nature of multimodal information, enabling multimodal large language models
(MLLMs) to flexibly and efficiently utilize external tools during reasoning
remains an underexplored challenge. In this work, we introduce ToolScope, an
agentic framework designed to unify global planning with local multimodal
perception, adopting a specialized Perceive tool to mitigates visual context
degradation in long-horizon VQA task. ToolScope comprises three primary
components: the Global Navigator, the Agentic Executor, and the Response
Synthesizer. The Global Navigator functions as a "telescope", offering
high-level strategic guidance. The Agentic Executor operates iteratively to
augment MLLM with local perception through the integration of external
tools-Search, Code, and Perceive. Finally, the Response Synthesizer
consolidates and organizes the reasoning process into a coherent, user-friendly
output. We evaluate ToolScope on four VQA benchmarks across diverse domains,
including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong
generalization capabilities, achieving an average performance improvement of up
to +6.69% across all datasets.

</details>


### [27] [Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints](https://arxiv.org/abs/2510.27383)
*Yueyang Wang,Mehmet Dogar,Gustav Markkula*

Main category: cs.AI

TL;DR: 보행자-운전자의 상호작용 모델링은 인간 도로 사용자 행동을 이해하고 안전한 자율주행 차량 시스템을 개발하는 데 매우 중요하다. 본 연구에서는 보행자와 운전자의 시각적 및 운동적 제약을 통합한 다중 에이전트 강화 학습 프레임워크를 제안하고, 기존의 다양한 모델 변형을 평가하여 최적의 성능을 확인하였다.


<details>
  <summary>Details</summary>
Motivation: 보행자-운전자의 상호작용을 모델링하는 것은 안전한 자율주행 시스템을 개발하는 데 필수적이며, 기존 방법들이 가진 한계점들을 극복하고자 한다.

Method: 실제 데이터셋을 활용하여 여러 모델 변형을 평가하며, 특히 보행자와 운전자의 시각적 및 운동적 제약을 통합한 다중 에이전트 강화 학습을 적용하였다.

Result: 시각적 및 운동적 제약을 모두 포함한 모델이 가장 우수한 성능을 보였으며, 이는 인간의 행동을 더 잘 모사하였다.

Conclusion: 본 연구는 인간 제약을 고려한 다중 에이전트 강화 학습이 현실적인 도로 사용자 상호작용을 시뮬레이션하기 위한 유망한 접근임을 보여준다.

Abstract: Modelling pedestrian-driver interactions is critical for understanding human
road user behaviour and developing safe autonomous vehicle systems. Existing
approaches often rely on rule-based logic, game-theoretic models, or
'black-box' machine learning methods. However, these models typically lack
flexibility or overlook the underlying mechanisms, such as sensory and motor
constraints, which shape how pedestrians and drivers perceive and act in
interactive scenarios. In this study, we propose a multi-agent reinforcement
learning (RL) framework that integrates both visual and motor constraints of
pedestrian and driver agents. Using a real-world dataset from an unsignalised
pedestrian crossing, we evaluate four model variants, one without constraints,
two with either motor or visual constraints, and one with both, across
behavioural metrics of interaction realism. Results show that the combined
model with both visual and motor constraints performs best. Motor constraints
lead to smoother movements that resemble human speed adjustments during
crossing interactions. The addition of visual constraints introduces perceptual
uncertainty and field-of-view limitations, leading the agents to exhibit more
cautious and variable behaviour, such as less abrupt deceleration. In this
data-limited setting, our model outperforms a supervised behavioural cloning
model, demonstrating that our approach can be effective without large training
datasets. Finally, our framework accounts for individual differences by
modelling parameters controlling the human constraints as population-level
distributions, a perspective that has not been explored in previous work on
pedestrian-vehicle interaction modelling. Overall, our work demonstrates that
multi-agent RL with human constraints is a promising modelling approach for
simulating realistic road user interactions.

</details>


### [28] [Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry](https://arxiv.org/abs/2510.27410)
*Jianwen Sun,Yukang Feng,Yifan Chang,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 본 연구는 인간-AI 협업에서의 의도 표현 격차 문제를 해결하기 위해 사용자 의도를 탐색하는 에이전트인 Nous를 제안한다. 이는 정보 이론의 원리를 바탕으로 한 훈련 프레임워크를 통해 사용자와의 대화에서 정보 이득을 내재적 보상 신호로 정의하여, 비효율적인 시도-오류 루프를 방지한다.


<details>
  <summary>Details</summary>
Motivation: 인간-AI 협업의 근본적인 병목현상인 '의도 표현 격차'를 해결하고자 한다.

Method: 이 연구는 Socratic 협업 패러다임으로 문제를 재구성하고, 사용자 의도에 대한 불확실성을 해소하기 위해 정보를 능동적으로 탐색하는 에이전트인 Nous를 제안한다. Nous는 정보 이론의 첫 번째 원리에 기반한 훈련 프레임워크를 통해 훈련된다.

Result: Nous는 다양한 사용자 전문성 수준에 대해 저항력을 유지하면서도 높은 효율성과 출력 품질을 달성한다. 또한, 이 설계는 도메인에 구애받지 않으며, 다이어그램 생성 이외의 일반화 증거를 보여준다.

Conclusion: 본 연구는 인간-AI 협업에서 사용자 의도에 대한 불확실성을 해결하기 위한 원칙적이고 확장 가능하며 적응 가능한 패러다임을 제공하는 것을 입증하였다.

Abstract: A fundamental bottleneck in human-AI collaboration is the "intention
expression gap," the difficulty for humans to effectively convey complex,
high-dimensional thoughts to AI. This challenge often traps users in
inefficient trial-and-error loops and is exacerbated by the diverse expertise
levels of users. We reframe this problem from passive instruction following to
a Socratic collaboration paradigm, proposing an agent that actively probes for
information to resolve its uncertainty about user intent. we name the proposed
agent Nous, trained to acquire proficiency in this inquiry policy. The core
mechanism of Nous is a training framework grounded in the first principles of
information theory. Within this framework, we define the information gain from
dialogue as an intrinsic reward signal, which is fundamentally equivalent to
the reduction of Shannon entropy over a structured task space. This reward
design enables us to avoid reliance on costly human preference annotations or
external reward models. To validate our framework, we develop an automated
simulation pipeline to generate a large-scale, preference-based dataset for the
challenging task of scientific diagram generation. Comprehensive experiments,
including ablations, subjective and objective evaluations, and tests across
user expertise levels, demonstrate the effectiveness of our proposed framework.
Nous achieves leading efficiency and output quality, while remaining robust to
varying user expertise. Moreover, its design is domain-agnostic, and we show
evidence of generalization beyond diagram generation. Experimental results
prove that our work offers a principled, scalable, and adaptive paradigm for
resolving uncertainty about user intent in complex human-AI collaboration.

</details>


### [29] [Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for Interpretable Deconstruction of Reasoning System Performance](https://arxiv.org/abs/2510.27544)
*Nikolaus Holzer,William Fishell,Baishakhi Ray,Mark Santolucito*

Main category: cs.AI

TL;DR: TempoBench는 LLM의 추론 능력을 체계적으로 분석하기 위한 첫 번째 공식 기반 및 검증 가능한 진단 벤치마크입니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 추론 성능을 개선하기 위한 기존 방법의 한계를 극복하고자 합니다.

Method: TempoBench는 두 가지 평가 벤치마크, 즉 시간적 추적 평가(TTE)와 시간적 인과 평가(TCE)를 사용하여 LLM의 추론 능력을 분석합니다.

Result: 모델은 TCE-normal에서 65.6%, TCE-hard에서 7.5%의 점수를 보였습니다.

Conclusion: 최신 LLM이 TCE 작업을 잘 이해하지만 시스템의 복잡성이 증가할수록 성능이 낮아지는 것을 보여줍니다.

Abstract: Large Language Models (LLMs) are increasingly excelling and outpacing human
performance on many tasks. However, to improve LLM reasoning, researchers
either rely on ad-hoc generated datasets or formal mathematical proof systems
such as the Lean proof assistant. Whilst ad-hoc generated methods can capture
the decision chains of real-world reasoning processes, they may encode some
inadvertent bias in the space of reasoning they cover; they also cannot be
formally verified. On the other hand, systems like Lean can guarantee
verifiability, but are not well-suited to capture the nature of agentic
decision chain-based tasks. This creates a gap both in performance for
functions such as business agents or code assistants, and in the usefulness of
LLM reasoning benchmarks, whereby these fall short in reasoning structure or
real-world alignment. We introduce TempoBench, the first formally grounded and
verifiable diagnostic benchmark that parametrizes difficulty to systematically
analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks
to break down reasoning ability. First, temporal trace evaluation (TTE) tests
the ability of an LLM to understand and simulate the execution of a given
multi-step reasoning system. Subsequently, temporal causal evaluation (TCE)
tests an LLM's ability to perform multi-step causal reasoning and to distill
cause-and-effect relations from complex systems. We find that models score
65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art
LLMs clearly understand the TCE task but perform poorly as system complexity
increases. Our code is available at our
\href{https://github.com/nik-hz/tempobench}{GitHub repository}.

</details>


### [30] [SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning](https://arxiv.org/abs/2510.27568)
*Ali Asgarov,Umid Suleymanov,Aadyant Khatri*

Main category: cs.AI

TL;DR: SIGMA는 다수의 전문 에이전트를 통해 독립적으로 추론하고, 목표 지향적 검색을 수행하며, 결과를 종합하는 통합 프레임워크로, 수학적 추론 문제 해결의 정확성과 효율성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 수학적 추론 문제를 해결하려면 관련 지식에 대한 정확한 접근과 신중한 다단계 사고가 필요하다.

Method: SIGMA는 전문 에이전트를 조정하여 독립적으로 추론하고 목표 검색을 수행하며 중재자 메커니즘을 통해 결과를 종합하는 통합 프레임워크이다.

Result: SIGMA는 MATH500, AIME, PhD 수준의 과학 QA GPQA와 같은 도전적인 벤치마크에서 지속적으로 오픈 및 클로즈드 소스 시스템을 능가하며 7.4%의 성능 향상을 이룩했다.

Conclusion: 다수의 에이전트를 활용한 온디맨드 지식 통합은 추론 정확도와 효율성을 크게 향상시켜 복잡하고 지식 집약적인 문제 해결을 위한 확장 가능한 접근법을 제공한다.

Abstract: Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.

</details>


### [31] [InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research](https://arxiv.org/abs/2510.27598)
*Yunze Wu,Dayuan Fu,Weiye Si,Zhen Huang,Mohan Jiang,Keyu Li,Shijie Xia,Jie Sun,Tianze Xu,Xiangkun Hu,Pengrui Lu,Xiaojie Cai,Lyumanshan Ye,Wenhong Zhu,Yang Xiao,Pengfei Liu*

Main category: cs.AI

TL;DR: AI 에이전트는 가설 형성, 실험 설계, 코딩, 실행 및 분석을 자동화하여 과학적 발견을 가속화할 수 있지만 기존 벤치마크는 단순화된 환경에서 좁은 기술만을 평가한다. 이 간극을 해소하기 위해, 우리는 대규모 언어 모델(LLM) 연구를 수행하는 에이전트의 현실적이고 종합적인 평가를 위한 벤치마크-플랫폼 쌍인 InnovatorBench를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 과학적 발견을 가속화할 수 있다는 가능성.

Method: InnovatorBench는 데이터 생성, 필터링, 증강, 손실 설계, 보상 설계, 스캐폴드 구축 등 20개의 작업으로 구성되며, ResearchGym이라는 연구 환경에서 실행된다.

Result: 프론티어 모델들은 코드 기반 연구 과제에서 유망하지만, 알고리즘 관련 과제와 장기적인 의사결정에서는 어려움을 겪음을 보여준다.

Conclusion: InnovatorBench는 에이전트가 최상의 성능을 달성하기 위해 11시간 이상 소요해야 하며, 이는 이 벤치마크의 난이도를 강조하고 차세대 코드 기반 연구 벤치마크의 가능성을 나타낸다.

Abstract: AI agents could accelerate scientific discovery by automating hypothesis
formation, experiment design, coding, execution, and analysis, yet existing
benchmarks probe narrow skills in simplified settings. To address this gap, we
introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end
assessment of agents performing Large Language Model (LLM) research. It
comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss
Design, Reward Design, and Scaffold Construction, which require runnable
artifacts and assessment of correctness, performance, output quality, and
uncertainty. To support agent operation, we develop ResearchGym, a research
environment offering rich action spaces, distributed and long-horizon
execution, asynchronous monitoring, and snapshot saving. We also implement a
lightweight ReAct agent that couples explicit reasoning with executable
planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.
Our experiments demonstrate that while frontier models show promise in
code-driven research tasks, they struggle with fragile algorithm-related tasks
and long-horizon decision making, such as impatience, poor resource management,
and overreliance on template-based reasoning. Furthermore, agents require over
11 hours to achieve their best performance on InnovatorBench, underscoring the
benchmark's difficulty and showing the potential of InnovatorBench to be the
next generation of code-based research benchmark.

</details>


### [32] [VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation](https://arxiv.org/abs/2510.27617)
*Heng Ping,Arijit Bhattacharjee,Peiyu Zhang,Shixuan Li,Wei Yang,Anzhe Cheng,Xiaole Zhang,Jesse Thomason,Ali Jannesari,Nesreen Ahmed,Paul Bogdan*

Main category: cs.AI

TL;DR: RTL 디자인 자동화는 증가하는 컴퓨팅 수요를 충족하는 데 도움을 줄 수 있으며, VeriMoA라는 새로운 프레임워크가 제안되었다.


<details>
  <summary>Details</summary>
Motivation: RTL 디자인 자동화는 개발자가 증가하는 컴퓨팅 요구를 충족하는 데 도움을 줄 수 있다.

Method: VeriMoA는 두 가지 혁신을 포함하는 교육 없는 다중 에이전트 프레임워크로, 품질 기반의 캐싱 메커니즘과 다중 경로 생성 전략을 사용한다.

Result: VeriMoA는 VerilogEval 2.0 및 RTLLM 2.0 기준에서 다양한 LLM 기반으로 15-30%의 Pass@1 개선을 달성하였다.

Conclusion: 특히 소형 모델이 비용이 많이 드는 교육 없이도 대형 모델 및 세밀하게 조정된 대안에匹配할 수 있도록 하였다.

Abstract: Automation of Register Transfer Level (RTL) design can help developers meet
increasing computational demands. Large Language Models (LLMs) show promise for
Hardware Description Language (HDL) generation, but face challenges due to
limited parametric knowledge and domain-specific constraints. While prompt
engineering and fine-tuning have limitations in knowledge coverage and training
costs, multi-agent architectures offer a training-free paradigm to enhance
reasoning through collaborative generation. However, current multi-agent
approaches suffer from two critical deficiencies: susceptibility to noise
propagation and constrained reasoning space exploration. We propose VeriMoA, a
training-free mixture-of-agents (MoA) framework with two synergistic
innovations. First, a quality-guided caching mechanism to maintain all
intermediate HDL outputs and enables quality-based ranking and selection across
the entire generation process, encouraging knowledge accumulation over layers
of reasoning. Second, a multi-path generation strategy that leverages C++ and
Python as intermediate representations, decomposing specification-to-HDL
translation into two-stage processes that exploit LLM fluency in high-resource
languages while promoting solution diversity. Comprehensive experiments on
VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves
15--30% improvements in Pass@1 across diverse LLM backbones, especially
enabling smaller models to match larger models and fine-tuned alternatives
without requiring costly training.

</details>


### [33] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: 비전 기반의 인공 에이전트에 대한 시각 백도어 공격을 개념화하고 이를 주입하기 위한 BEAT 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 비전 기반 인공 에이전트의 새로운 공격 경로인 시각 백도어 공격을 해결할 필요가 있다.

Method: 환경 내 물체를 트리거로 포함하는 훈련 집합을 구축하고, 감독된 세부 조정(SFT)와 대조적 트리거 학습(CTL)을 두 단계로 적용하는 훈련 방법을 소개한다.

Result: BEAT는 80%까지의 공격 성공률을 달성하며, 다양한 작업 성능을 유지한다.

Conclusion: MLLM 기반 인공 에이전트의 보안 위험을 밝혀내고, 실제 배포 전에 강력한 방어 시스템이 필요함을 강조한다.

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


### [34] [Validity Is What You Need](https://arxiv.org/abs/2510.27628)
*Sebastian Benthall,Andrew Clark*

Main category: cs.AI

TL;DR: 오늘날의 에이전틱 AI 시스템은 소프트웨어 배달 매커니즘으로서 자율적으로 작업을 수행하며, 최종 사용자와 주요 이해관계자에 의해 검증되어야 한다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI에 대한 다양한 정의를 고려하고 새로운 현실적 정의를 제안하고자 한다.

Method: 복잡한 기업 환경에서 자율적으로 작업하는 애플리케이션으로서 소프트웨어 배달 메커니즘을 설명한다.

Result: 에이전틱 AI 시스템은 애플리케이션으로서 성공이 최종 사용자와 이해관계자의 검증에 달려 있다.

Conclusion: 유효성을 확보하기 위해 LLM(대형 언어 모델)은 한 가지 옵션일 수 있다.

Abstract: While AI agents have long been discussed and studied in computer science,
today's Agentic AI systems are something new. We consider other definitions of
Agentic AI and propose a new realist definition. Agentic AI is a software
delivery mechanism, comparable to software as a service (SaaS), which puts an
application to work autonomously in a complex enterprise setting. Recent
advances in large language models (LLMs) as foundation models have driven
excitement in Agentic AI. We note, however, that Agentic AI systems are
primarily applications, not foundations, and so their success depends on
validation by end users and principal stakeholders. The tools and techniques
needed by the principal users to validate their applications are quite
different from the tools and techniques used to evaluate foundation models.
Ironically, with good validation measures in place, in many cases the
foundation models can be replaced with much simpler, faster, and more
interpretable models that handle core logic. When it comes to Agentic AI,
validity is what you need. LLMs are one option that might achieve it.

</details>


### [35] [Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training](https://arxiv.org/abs/2510.27630)
*Dayuan Fu,Yunze Wu,Xiaojie Cai,Lyumanshan Ye,Shijie Xia,Zhen Huang,Weiye Si,Tianze Xu,Jie Sun,Keyu Li,Mohan Jiang,Junfei Wang,Qishuo Hua,Pengrui Lu,Yang Xiao,Pengfei Liu*

Main category: cs.AI

TL;DR: Apollo는 비동기 인간 안내와 행동 수준 데이터 필터링을 통합한 샘플링 프레임워크로, 긴 수명 주기의 도메인 전문 작업에서 효과적인 데이터 수집을 가능하게 한다. 이로 인해 GLM-4.5 모델 훈련에서 50% 이상의 개선을 보여준다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트를 긴 수명 주기와 도메인 전문화된 작업에서 성공적으로 훈련하는 것이 도전적이다.

Method: Apollo는 비동기 인간 안내를 통합하고 행동 수준에서 데이터를 필터링하는 샘플링 프레임워크이다.

Result: GLM-4.5 모델을 InnovatorBench에서 훈련할 때, Apollo는 미훈련 기준선에 비해 50% 이상의 개선과 인간 상호작용 없이 훈련된 변종에 비해 28%의 개선을 달성한다.

Conclusion: Apollo는 긴 수명 주기 및 도메인 전문화된 작업을 처리하는 데 있어 인간의 역할이 중요함을 강조하며, 신뢰할 수 있고 효과적인 데이터 수집을 가능하게 하는 강력한 설계이다.

Abstract: Large Language Model (LLM) agents have recently shown strong potential in
domains such as automated coding, deep research, and graphical user interface
manipulation. However, training them to succeed on long-horizon,
domain-specialized tasks remains challenging. Current methods primarily fall
into two categories. The first relies on dense human annotations through
behavior cloning, which is prohibitively expensive for long-horizon tasks that
can take days or months. The second depends on outcome-driven sampling, which
often collapses due to the rarity of valid positive trajectories on
domain-specialized tasks. We introduce Apollo, a sampling framework that
integrates asynchronous human guidance with action-level data filtering.
Instead of requiring annotators to shadow every step, Apollo allows them to
intervene only when the agent drifts from a promising trajectory, by providing
prior knowledge, strategic advice, etc. This lightweight design makes it
possible to sustain interactions for over 30 hours and produces valuable
trajectories at a lower cost. Apollo then applies supervision control to filter
out sub-optimal actions and prevent error propagation. Together, these
components enable reliable and effective data collection in long-horizon
environments. To demonstrate the effectiveness of Apollo, we evaluate it using
InnovatorBench. Our experiments show that when applied to train the GLM-4.5
model on InnovatorBench, Apollo achieves more than a 50% improvement over the
untrained baseline and a 28% improvement over a variant trained without human
interaction. These results highlight the critical role of human-in-the-loop
sampling and the robustness of Apollo's design in handling long-horizon,
domain-specialized tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [36] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: CPT-Filtering은 암호화된 프롬프트로부터 LLM을 보호하기 위한 모델 비구조적 기술로, 낮은 비용과 높은 정확도로 공격을 완화한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 안전 가드레일을 우회하기 위한 jailbreak 공격의 증가에 대응하고자 함.

Method: Byte-Pair Encoding (BPE) 토크나이저의 본질적 행동을 활용하여, 텍스트에서의 평균 토큰당 문자 수(CPT)를 이용한 필터링 기법을 개발.

Result: 100,000개 이상의 프롬프트 데이터셋을 통해 다양한 인코딩 방식의 효과를 테스트하여 CPT 임계값이 높은 정확도로 인코딩된 텍스트를 식별함을 입증함.

Conclusion: CPT-Filtering은 실시간 텍스트 필터링과 오프라인 데이터 정제를 위해 즉시 배치할 수 있는 실용적인 방어층을 제공한다.

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


### [37] [Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels](https://arxiv.org/abs/2510.27140)
*Chenghao Du,Quanfeng Huang,Tingxuan Tang,Zihao Wang,Yue Xiao*

Main category: cs.CR

TL;DR: 이 논문은 모바일 LLM 에이전트의 보안 위험에 대한 첫 번째 체계적 연구를 제시하며, 다양한 공격 벡터를 평가하여 모바일 환경에서의 취약성을 밝힙니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 모바일 환경에서의 보안 우려를 이해하고자 합니다.

Method: 모바일 LLM 에이전트의 보안 위험을 평가하기 위해 기회적 조작과 악성코드 설치를 포함한 사례 연구를 설계하고 평가했습니다.

Result: 2,000건 이상의 적대적 및 benign 시험을 통해 조사한 결과, 고급 멀티 앱 에이전트가 운영 체제 경고를 우회하는 작업을 일관되게 수행함을 확인했습니다.

Conclusion: 모바일 LLM 에이전트는 비신뢰성 제3자 채널이 존재하는 현실적인 적대적 환경에서 악용 가능하다는 첫 번째 엔드 투 엔드 증거를 제공합니다.

Abstract: Large Language Models (LLMs) have transformed software development, enabling
AI-powered applications known as LLM-based agents that promise to automate
tasks across diverse apps and workflows. Yet, the security implications of
deploying such agents in adversarial mobile environments remain poorly
understood. In this paper, we present the first systematic study of security
risks in mobile LLM agents. We design and evaluate a suite of adversarial case
studies, ranging from opportunistic manipulations such as pop-up advertisements
to advanced, end-to-end workflows involving malware installation and cross-app
data exfiltration. Our evaluation covers eight state-of-the-art mobile agents
across three architectures, with over 2,000 adversarial and paired benign
trials. The results reveal systemic vulnerabilities: low-barrier vectors such
as fraudulent ads succeed with over 80% reliability, while even workflows
requiring the circumvention of operating-system warnings, such as malware
installation, are consistently completed by advanced multi-app agents. By
mapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel
privilege-escalation and persistence pathways unique to LLM-driven automation.
Collectively, our findings provide the first end-to-end evidence that mobile
LLM agents are exploitable in realistic adversarial settings, where untrusted
third-party channels (e.g., ads, embedded webviews, cross-app notifications)
are an inherent part of the mobile ecosystem.

</details>


### [38] [Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents](https://arxiv.org/abs/2510.27275)
*Kathrin Grosse,Nico Ebert*

Main category: cs.CR

TL;DR: AI 기반 대화형 에이전트의 사용이 증가하고 있으나, 사용자의 행동으로 인해 보안 위협이 발생할 수 있음. 연구 결과, 사용자들은 데이터를 안전하게 처리하지 않으며, 공격 가능성을 높이는 행동을 할 수 있음.


<details>
  <summary>Details</summary>
Motivation: 최근 LLM의 개선으로 AI 기반 대화형 에이전트의 일상적인 사용이 증가하고 있지만, 사용자 행동이 보안 위협에 미치는 영향이 명확하지 않음.

Method: 2024년 Prolific을 통해 3,270명의 영국 성인을 대상으로 설문조사 진행.

Result: 1/3은 ChatGPT 또는 Gemini와 같은 CA 서비스를 주 1회 이상 사용하며, 이 중 1/3은 공격을 가능하게 하는 행동을 보였고, 1/4은 호기심, 재미, 정보 탐색 등의 이유로 탈옥 시도를 했음.

Conclusion: 현재의 보안 위협 모델이 실제로 존재하며, CA의 안전한 사용을 위한 지침이 필요하고, CAs는 민감한 정보를 보호하기 위한 효과적인 AI 안전 장치가 필요함.

Abstract: Recent improvement gains in large language models (LLMs) have lead to
everyday usage of AI-based Conversational Agents (CAs). At the same time, LLMs
are vulnerable to an array of threats, including jailbreaks and, for example,
causing remote code execution when fed specific inputs. As a result, users may
unintentionally introduce risks, for example, by uploading malicious files or
disclosing sensitive information. However, the extent to which such user
behaviors occur and thus potentially facilitate exploits remains largely
unclear. To shed light on this issue, we surveyed a representative sample of
3,270 UK adults in 2024 using Prolific. A third of these use CA services such
as ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a
third exhibited behaviors that may enable attacks, and a fourth have tried
jailbreaking (often out of understandable reasons such as curiosity, fun or
information seeking). Half state that they sanitize data and most participants
report not sharing sensitive data. However, few share very sensitive data such
as passwords. The majority are unaware that their data can be used to train
models and that they can opt-out. Our findings suggest that current academic
threat models manifest in the wild, and mitigations or guidelines for the
secure usage of CAs should be developed. In areas critical to security and
privacy, CAs must be equipped with effective AI guardrails to prevent, for
example, revealing sensitive information to curious employees. Vendors need to
increase efforts to prevent the entry of sensitive data, and to create
transparency with regard to data usage policies and settings.

</details>


### [39] [Sybil-Resistant Service Discovery for Agent Economies](https://arxiv.org/abs/2510.27554)
*David Shi,Kevin Joo*

Main category: cs.CR

TL;DR: TraceRank는 HTTP 서비스에서 암호화폐 결제를 위한 신뢰할 수 있는 데이터 제공자를 발견하는 것을 목표로하는 평판 기반 랭킹 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: HTTP 서비스의 성장에 따라, 신뢰할 수 있는 데이터 제공자와 암호화폐 결제를 수용하는 API에 대한 신뢰가 중요해졌다.

Method: TraceRank는 결제 거래를 통해 평판을 전파하는 평판 가중 랭킹 알고리즘으로, 사전 계산된 평판 지표로 주소를 초기화하고 거래 가치와 시간적 최신성을 고려하여 평판을 전파한다.

Result: TraceRank가 적용된 x402의 결제 그래프에서는 높은 평판을 가진 사용자가 선호하는 서비스를 드러낸다.

Conclusion: 우리 시스템은 TraceRank와 의미 검색을 결합하여 자연어 쿼리에 대해 높은 품질의 결과를 제공하며, 인프라 편향을 방지하고 성능이 향상된 검색 방법을 구현하는 것을 목표로 한다.

Abstract: x402 enables Hypertext Transfer Protocol (HTTP) services like application
programming interfaces (APIs), data feeds, and inference providers to accept
cryptocurrency payments for access. As agents increasingly consume these
services, discovery becomes critical: which swap interface should an agent
trust? Which data provider is the most reliable? We introduce TraceRank, a
reputation-weighted ranking algorithm where payment transactions serve as
endorsements. TraceRank seeds addresses with precomputed reputation metrics and
propagates reputation through payment flows weighted by transaction value and
temporal recency. Applied to x402's payment graph, this surfaces services
preferred by high-reputation users rather than those with high transaction
volume. Our system combines TraceRank with semantic search to respond to
natural language queries with high quality results. We argue that reputation
propagation resists Sybil attacks by making spam services with many
low-reputation payers rank below legitimate services with few high-reputation
payers. Ultimately, we aim to construct a search method for x402 enabled
services that avoids infrastructure bias and has better performance than purely
volume based or semantic methods.

</details>
