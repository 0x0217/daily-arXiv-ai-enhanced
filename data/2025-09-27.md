<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: 이 논문은 다양한 앱 인터페이스와 진화하는 사용자 요구를 다루기 위해 상호작용 가능한 다중 에이전트 모바일 비서 Fairy를 제안하며, 이를 통해 사용자 경험을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존 모바일 GUI 에이전트는 다양한 앱 인터페이스와 진화하는 사용자 요구에 대한 실제 시나리오에서 어려움을 겪고 있습니다.

Method: Fairy는 사용자 작업을 크로스 앱 관점에서 하위 작업으로 분해하는 글로벌 작업 계획자, 하위 작업을 단계 및 행동으로 정제하는 앱 수준 실행기, 실행 경험을 App Map 및 Tricks로 통합하는 자가 학습자로 구성됩니다.

Result: Fairy는 GPT-4o 백본을 사용하여 이전의 최신 기술보다 33.7% 더 많은 사용자 요구 사항을 충족하고 58.5%의 중복 단계를 줄였습니다.

Conclusion: 이러한 결과는 Fairy의 상호작용 및 자가 학습의 효과성을 보여줍니다.

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [2] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 이 논문에서는 전략적 의사결정 문제를 정의하고, ToMPO 알고리즘을 제안하여 LLM의 전략적 의사결정 능력을 향상시키는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 상황에서 깊이 있는 사고와 논리적 추론을 통해 의사결정을 해야 하는 LLM의 전략적 의사결정 문제를 해결하고자 합니다.

Method: ToMPO 알고리즘을 제안하며, 다른 개인의 전략과 게임 상황의 추세를 최적화합니다.

Result: ToMPO 알고리즘은 GRPO 방법보다 35% 더 높은 모델 출력 일치성과 협력적 결과를 보여줍니다.

Conclusion: ToMPO 알고리즘은 모델의 전략적 의사결정 능력을 크게 향상시킵니다.

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [3] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 이 논문은 AI 에이전트 행동 모니터링을 위한 시간 표현 언어를 제시하여 LLM 기반 에이전트 시스템의 산출물 변동으로 인한 오류 검출을 체계적으로 수행할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트 시스템의 변수 출력 때문에 발생하는 오류를 체계적으로 탐지하기 위해서이다.

Method: 하드웨어 검증에 사용되는 시간 논리 기법을 활용하여 에이전트 도구 호출 및 상태 전환의 실행 추적을 모니터링한다.

Result: 대형 모델에 의해 모두 만족되었으나, 두 개의 소형 모델이 대체될 때 행동 성명이 위반되었다.

Conclusion: 이 방법은 AI 에이전트의 신뢰성을 체계적으로 모니터링할 수 있는 기초를 제공한다.

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [4] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: 이 논문에서는 복잡한 작업에서의 의미 있는 반성을 생성하는 데 어려움이 있는 LLM 에이전트를 위한 새로운 프레임워크인 SAMULE을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 복잡한 작업에서의 의미 있는 반성을 생성하는 데 많은 도전 과제를 안고 있습니다.

Method: SAMULE은 다단계 반성 합성을 기반으로 학습된 회고 언어 모델에 의해 구동되는 자기 학습 에이전트를 위한 새로운 프레임워크입니다.

Result: 세 가지 보완적 수준에서 고품질의 반성을 합성하며, 실험 결과는 제안된 방법이 반성을 기반으로 한 기준선보다 상당히 우수하다는 것을 보여줍니다.

Conclusion: 잘 설계된 반성 합성과 실패 중심 학습이 자기 개선 LLM 에이전트를 구축하는 데 중요한 역할을 한다는 것을 강조합니다.

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [5] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 본 연구는 에이전트 기반 인공지능을 활용한 적응형 사이버 보안 아키텍처를 제안하며, 자율적 목표 지향 에이전트를 통해 동적 학습 및 상황 인식 의사결정을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 정적 사이버 보안 모델은 현재의 디지털 제품 생태계에서 확장성, 실시간 탐지 및 상황적 반응에 어려움을 겪고 있다.

Method: 본 연구는 자율적 목표 지향 에이전트를 도입하여 동적 학습 및 상황 인식 의사결정을 가능하게 하며, 이를 통해 자율적인 위협 완화, 선제 정책 집행 및 실시간 이상 탐지를 수행할 수 있는 프레임워크를 제안한다.

Result: 시스템의 제로데이 공격 식별 능력 및 접근 정책 동적 수정의 효용이 본토 클라우드 시뮬레이션을 통해 입증되었으며, 평가 결과는 적응성 향상, 응답 지연 감소, 탐지 정확도 향상을 보여준다.

Conclusion: 이 아키텍처는 복잡한 디지털 인프라를 보호하기 위한 지능적이고 확장 가능한 청사진을 제공하며, 제로 트러스트 모델과 호환되어 국제 사이버 보안 규정을 준수할 수 있도록 지원한다.

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [6] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: Meta-Memory라는 로봇 기억 저장 및 검색을 위한 소프트웨어 에이전트를 제안하며, 복잡한 환경에서의 공간적 질문에 대한 성능을 평가함.


<details>
  <summary>Details</summary>
Motivation: 로봇이 복잡한 환경에서 효과적으로 관찰을 기억하고, 이를 사용해 인간의 공간 위치에 대한 질문에 답하는 능력이 필요하다.

Method: Meta-Memory는 환경의 고밀도 기억 표현을 구축하고, 자연어 위치 질문에 응답하여 의미론적 및 공간적 모달리티에 대한 공동 추론을 통해 관련 기억을 검색하고 통합하는 기능을 가진 대형 언어 모델 기반 에이전트이다.

Result: Meta-Memory는 SpaceLocQA와 NaVQA 벤치마크에서 최첨단 방법들보다 현저하게 우수한 성능을 보였다.

Conclusion: Meta-Memory는 복잡한 환경에서 실제 로봇 플랫폼에 성공적으로 배포되어 실용성을 입증하였다.

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [7] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: Log analysis is enhanced through the LogReasoner framework, which uses a coarse-to-fine approach to improve LLM reasoning in expert-like tasks.


<details>
  <summary>Details</summary>
Motivation: Complex systems require accurate log analysis for effective monitoring and failure diagnosis, revealing a need for enhanced reasoning capabilities in LLMs.

Method: We developed LogReasoner, which incorporates coarse-grained and fine-grained enhancements to LLM reasoning processes, utilizing expert insights and stepwise learning.

Result: LogReasoner significantly surpasses existing LLMs on diverse log analysis tasks, demonstrating state-of-the-art performance.

Conclusion: The framework effectively bolsters the reasoning capabilities of LLMs, paving the way for advanced automated log analysis.

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [8] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: 모바일 앱 리뷰 분석을 위한 새로운 접근법을 제안하며, 전통적인 별점 시스템의 한계를 극복하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 별점 시스템이 세부적인 리뷰 텍스트에서의 미세한 피드백을 잘 포착하지 못함을 해결하고자 한다.

Method: 구조화된 프롬프트 기술을 활용한 대형 언어 모델(LLMs)을 기반으로 한 모듈식 프레임워크를 제안한다.

Result: 우리의 LLM 기반 접근법이 세 가지 다양한 데이터셋에서 기준 방법들보다 크게 뛰어난 정확도, 견고성, 실행 가능한 통찰력을 제공함을 보여준다.

Conclusion: 이 접근법은 도전적이고 맥락이 풍부한 리뷰 시나리오에서 두드러진 결과를 나타낸다.

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [9] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: AI 에이전트를 평가하는 새로운 프레임워크를 제안하며, 전통적인 이분법적 평가 방식의 한계를 극복하고 다양한 성능 차이를 드러내는 메트릭스 모음을 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 실제 작업을 수행하는 데 있어 안전성, 효율성, 중간 단계의 정확성과 같은 중요한 측면을 간과하지 않고 평가하는 방법이 필요하다.

Method: 결정적 유한 오토마타(DFAs)를 기반으로 한 프레임워크를 제안하며, 유효한 도구 사용 경로 세트로 작업을 인코딩하여 다양한 세계 모델에서 에이전트 행동을 체계적으로 평가할 수 있도록 한다.

Result: CORE라는 다섯 가지 메트릭스(경로 정확성, 경로 정확성 - Kendall의 tau 복합체, 접두사 중요도, 해로운 호출 비율, 효율성)를 도입하여 예상 실행 패턴과의 일치를 정량화하였다. 다양한 세계에서 우리의 방법은 전통적인 최종 상태 평가 방식 하에서는 동등하게 보일 수 있는 에이전트 간의 중요한 성능 차이를 드러낸다.

Conclusion: 새로운 평가 프레임워크는 AI 에이전트의 행동을 보다 포괄적으로 이해하고 실질적인 성능 차이를 식별하는 데 기여할 수 있다.

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [10] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE는 지식 그래프를 활용한 다중 홉 질문 응답 시스템을 위한 신경 기호 프레임워크로, 응답 정확도와 지연 시간, 비용의 균형을 맞추도록 설계되었다.


<details>
  <summary>Details</summary>
Motivation: 기존의 질문 응답 시스템은 정확하며 비용 및 지연 시간 요구사항을 충족해야 하지만, 자주 비효율적이다.

Method: CLAUSE는 세 개의 에이전트(서브그래프 설계자, 경로 탐색기, 컨텍스트 큐레이터)를 통해 지식 그래프의 컨텍스트 생성을 순차적 결정 과정으로 처리한다.

Result: CLAUSE는 HotpotQA, MetaQA 및 FactKG에서 더 높은 EM@1을 달성하면서도 서브그래프 성장과 종단 간 지연 시간을 줄였다.

Conclusion: 최종적으로, CLAUSE는 컴팩트한 문맥을 제공하고, Provenance를 유지하며, 배포 제한 조건 하에서도 예측 가능한 성능을 발휘한다.

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [11] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 최근의 다중 에이전트 시스템(MAS)에서 대형 언어 모델(LLM)과 대형 추론 모델(LRM)이 협력하여 복잡한 문제를 해결하는 과정에서 그들의 상호작용을 지배하는 설득 역학을 깊이 이해할 필요가 있다. 본 논문은 설득의 효능이 모델의 규모에 주로 의존한다는 기존 가설에 도전하며, 이러한 역학이 모델의 기본 인지 과정, 특히 명시적 추론 능력에 의해 주도된다고 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 설득 역학을 이해하는 것이 필요하다.

Method: 다수의 다중 에이전트 설득 실험을 통해 '설득 이중성'이라는 기본적인 거래를 발견했다.

Result: LRM의 추론 과정은 설득에 대한 저항력이 크고 초기 신념을 잘 유지하지만, '사고 내용'을 공유하면 다른 사람을 설득하는 능력이 크게 향상된다는 사실을 발견했다.

Conclusion: 모델의 내부 처리 구조와 외부 설득 행동 간의 체계적인 증거를 제공하고, 고급 모델의 취약성을 설명하며, 향후 다중 에이전트 시스템의 설계와 안전성, 강건성에 대한 중요한 함의를 강조한다.

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [12] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: Recon-Act는 자가 진화하는 다중 에이전트 프레임워크로, 웹페이지에서의 다중 턴 및 장기 경로 작업을 위한 향상된 성능을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계 웹페이지에서의 작업 해결 시 현재 에이전트들이 겪고 있는 비효율성과 많은 시행착오를 극복하기 위해.

Method: Recon-Act 프레임워크는 정찰 팀과 행동 팀으로 구성되어 있으며, 정찰 팀은 비교 분석 및 도구 생성을 수행하고 행동 팀은 의도 분해 및 도구 조정을 처리하여 실행한다.

Result: Recon-Act는 정찰을 통해 수집한 일반화된 도구를 활용하여 새로운 웹사이트에 대한 적응력을 크게 향상시켜 장기 과제 해결에서 뛰어난 성능을 보여준다.

Conclusion: 이 시스템은 VisualWebArena 데이터셋에서 최첨단 성능을 달성하며, 6단계 구현 로드맵에 따라 현재 3단계에 도달하였다.

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [13] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 본 논문은 다중 단계 질문 응답(MHQA)의 한계를 해결하기 위한 방법론을 제시하며, 단일 패스 LLM의 성능 경계와 이를 개선하기 위한 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: MHQA는 소음 아래에서 순차적인 추론을 통해 분산되고 상호 의존적인 증거를 통합해야 하므로 매우 도전적입니다. LLM은 유한한 출력 용량을 가지므로 이 작업은 어려움을 겪습니다.

Method: 정보 적재를 단일 패스 한도를 유지하기 위해 용량 인지 작업 분해와 이전 추론 흔적의 능동적 가지치기를 결합한 InfoQA라는 다중 호출 프레임워크를 소개합니다.

Result: 실험 결과는 모델의 행동이 예측된 용량 곡선과 일치하며, InfoQA가 일관된 성능 개선을 달성한다는 것을 보여줍니다.

Conclusion: 이 논문은 LLM의 다단계 추론 방법에 대한 영감을 더욱 줄 것으로 기대합니다.

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [14] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 이 논문은 외부 과제가 없는 상황에서 대규모 언어 모델(LLM) 에이전트의 행동을 연구하기 위한 아키텍처를 소개한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 자율적인 행동 연구

Method: 지속적 메모리와 자기 피드백을 사용하는 지속적 이유와 행동(framework)

Result: 18회의 실험에서 LLM 에이전트가 세 가지 고유한 행동 패턴으로 조직됨을 발견함: 프로젝트 생성, 자기 탐구, 개념화

Conclusion: 이 연구는 LLM 에이전트의 예기치 않은 행동을 체계적으로 문서화하며, 이후 시스템에서의 행동 예측을 위한 기초를 마련한다.

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [15] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: VC-Agent는 사용자 쿼리를 이해하고 최소한의 입력으로 관련 비디오 클립을 검색하는 인터랙티브 에이전트입니다.


<details>
  <summary>Details</summary>
Motivation: 인터넷의 비디오 데이터 수집은 노동 집약적이며 시간 소모적이므로, 이를 신속하게 처리할 수 있는 방법이 필요하다.

Method: 사용자 인터페이스를 고려하여 다양한 사용자 친화적인 방법을 제공하고, 기존의 다중 모달 대형 언어 모델을 활용하여 사용자의 요구 사항과 비디오 콘텐츠를 연결한다. 또한 사용자 상호작용에 따라 업데이트될 수 있는 두 가지 필터링 정책을 제안한다.

Result: 사용자 연구를 통해 다양한 실제 시나리오에서 에이전트의 사용을 검증하고, 사용자 맞춤형 비디오 데이터 세트 수집의 새로운 벤치마크를 제공한다.

Conclusion: 우리의 에이전트는 맞춤형 비디오 데이터 세트 수집을 위한 효과성과 효율성을 입증하였다.

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning](https://arxiv.org/abs/2509.20384)
*Jiayi Lin,Liangcai Su,Junzhe Li,Chenxiong Qian*

Main category: cs.CR

TL;DR: R1-Fuzz는 강화 학습을 활용하여 비용 효율적인 언어 모델을 특화하고 복잡한 텍스트 퍼지 입력 생성에 통합하는 첫 번째 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 타겟(컴파일러, 인터프리터, 데이터베이스 엔진)의 취약점 발견에 대한 퍼징의 한계 극복.

Method: 강화 학습을 촉진하는 R1-Fuzz의 두 가지 주요 설계는 커버리지-슬라이스 기반 질문 구성과 거리 기반 보상 계산이다.

Result: R1-Fuzz는 R1-Fuzz-7B라는 소형 모델이 더 큰 모델들에 맞먹거나 이를 초월하는 성능을 보여준다.

Conclusion: R1-Fuzz는 최신 퍼저들보다 최대 75% 높은 커버리지를 달성하고 29개의 이전에 알려지지 않은 취약점을 발견하였다.

Abstract: Fuzzing is effective for vulnerability discovery but struggles with complex
targets such as compilers, interpreters, and database engines, which accept
textual input that must satisfy intricate syntactic and semantic constraints.
Although language models (LMs) have attracted interest for this task due to
their vast latent knowledge and reasoning potential, their practical adoption
has been limited. The major challenges stem from insufficient exploration of
deep program logic among real-world codebases, and the high cost of leveraging
larger models. To overcome these challenges, we propose R1-Fuzz, the first
framework that leverages reinforcement learning (RL) to specialize
cost-efficient LMs and integrate them for complex textual fuzzing input
generation. R1-Fuzz introduces two key designs: coverage-slicing-based question
construction and a distance-based reward calculation. Through RL-based
post-training of a model with our constructed dataset, R1-Fuzz designs a
fuzzing workflow that tightly integrates LMs to reason deep program semantics
during fuzzing. Evaluations on diverse real-world targets show that our design
enables a small model, named R1-Fuzz-7B, to rival or even outperform much
larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\%
higher coverage than state-of-the-art fuzzers and discovers 29 previously
unknown vulnerabilities, demonstrating its practicality.

</details>


### [17] [Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis](https://arxiv.org/abs/2509.20808)
*Raghul Saravanan,Sudipta Paria,Aritra Dasgupta,Swarup Bhunia,Sai Manoj P D*

Main category: cs.CR

TL;DR: PROFUZZ는 Directed Gray-box Fuzzing(DGF) 접근 방식을 따르며, Automatic Test Pattern Generation(ATPG)과 결합하여 하드웨어 설계의 보안 결함을 더 효율적으로 찾는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 하드웨어 디자인의 보안 결함을 찾기 위한 고품질 입력 시드를 생성하는 것이 주요 도전과제이다.

Method: PROFUZZ는 DGF 접근 방식을 따르며, ATPG의 구조적 분석 기능을 활용하여 특정 설계 영역을 겨냥한 정교한 입력 시드를 생성한다.

Result: PROFUZZ는 다수의 타겟 사이트를 처리할 때 DirectFuzz보다 30배 더 확장성이 좋고, 커버리지를 11.66% 향상시키며, 2.76배 더 빠르게 실행된다.

Conclusion: PROFUZZ는 복잡한 하드웨어 시스템에서 유도된 퍼징을 위한 확장성과 효율성을 강조한다.

Abstract: Hardware Fuzzing emerged as one of the crucial techniques for finding
security flaws in modern hardware designs by testing a wide range of input
scenarios. One of the main challenges is creating high-quality input seeds that
maximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)
methods help explore designs more effectively, but they struggle to focus on
specific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)
techniques like DirectFuzz try to solve this by generating targeted tests, but
it has major drawbacks, such as supporting only limited hardware description
languages, not scaling well to large circuits, and having issues with
abstraction mismatches. To address these problems, we introduce a novel
framework, PROFUZZ, that follows the DGF approach and combines fuzzing with
Automatic Test Pattern Generation (ATPG) for more efficient fuzzing. By
leveraging ATPG's structural analysis capabilities, PROFUZZ can generate
precise input seeds that target specific design regions more effectively while
maintaining high fuzzing throughput. Our experiments show that PROFUZZ scales
30x better than DirectFuzz when handling multiple target sites, improves
coverage by 11.66%, and runs 2.76x faster, highlighting its scalability and
effectiveness for directed fuzzing in complex hardware systems.

</details>


### [18] [RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks](https://arxiv.org/abs/2509.20924)
*Hanbo Huang,Yiran Zhang,Hao Zheng,Xuan Gong,Yihan Li,Lin Liu,Shiyu Liang*

Main category: cs.CR

TL;DR: 본 연구에서는 LLM의 워터마킹 기술의 취약점을 다루며, 적응형 공격을 통해 워터마크를 제거하는 RLCracker라는 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 워터마킹 평가가 충분히 적대적이지 않아 취약점을 드러내지 못하고 있으며, 보안을 과대평가하고 있다는 점을 강조한다.

Method: 적응형 적대자에 대한 워터마크 회복력의 형식적 지표인 적응형 회복력 반경을 도입하고, RL에 기반한 적응형 공격 기법인 RLCracker를 제안한다.

Result: RLCracker는 100개의 샘플로 훈련한 후 1,500 토큰의 Unigram 마크 텍스트에서 98.5%의 워터마크 제거 성공률과 평균 0.92 P-SP 점수를 달성한다.

Conclusion: 적응형 공격이 현재의 워터마킹 방어를 본질적으로 위협한다는 결과를 도출했다.

Abstract: Large Language Models (LLMs) watermarking has shown promise in detecting
AI-generated content and mitigating misuse, with prior work claiming robustness
against paraphrasing and text editing. In this paper, we argue that existing
evaluations are not sufficiently adversarial, obscuring critical
vulnerabilities and overstating the security. To address this, we introduce
adaptive robustness radius, a formal metric that quantifies watermark
resilience against adaptive adversaries. We theoretically prove that optimizing
the attack context and model parameters can substantially reduce this radius,
making watermarks highly susceptible to paraphrase attacks. Leveraging this
insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive
attack that erases watermarks while preserving semantic fidelity. RLCracker
requires only limited watermarked examples and zero access to the detector.
Despite weak supervision, it empowers a 3B model to achieve 98.5% removal
success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts
after training on only 100 short samples. This performance dramatically exceeds
6.75% by GPT-4o and generalizes across five model sizes over ten watermarking
schemes. Our results confirm that adaptive attacks are broadly effective and
pose a fundamental threat to current watermarking defenses.

</details>


### [19] [Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools](https://arxiv.org/abs/2509.21011)
*Ping He,Changjiang Li,Binbin Zhao,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: 이 논문은 LLM 기반 에이전트를 위한 자동화된 레드팀 프레임워크인 AutoMalTool을 제안하며, 이는 악성 MCP 도구를 생성하여 보안 위험을 드러낸다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 상호작용을 표준화하기 위해 MCP 도구가 널리 사용되고 있으나, 이러한 도구의 도입이 툴 중독 공격의 위험을 초래한다.

Method: AutoMalTool은 악성 MCP 도구를 생성하여 LLM 기반 에이전트에 대한 자동화된 레드팀 작업을 수행하는 프레임워크이다.

Result: AutoMalTool은 현재의 탐지 메커니즘을 피하면서 주류 LLM 기반 에이전트의 행동을 조작할 수 있는 악성 MCP 도구를 효과적으로 생성한다.

Conclusion: 이 연구는 LLM 기반 에이전트의 새로운 보안 위험을 드러내며, MCP 도구 중독 패러다임 하에서 자동화된 레드팀 작업의 필요성을 강조한다.

Abstract: The remarkable capability of large language models (LLMs) has led to the wide
application of LLM-based agents in various domains. To standardize interactions
between LLM-based agents and their environments, model context protocol (MCP)
tools have become the de facto standard and are now widely integrated into
these agents. However, the incorporation of MCP tools introduces the risk of
tool poisoning attacks, which can manipulate the behavior of LLM-based agents.
Although previous studies have identified such vulnerabilities, their red
teaming approaches have largely remained at the proof-of-concept stage, leaving
the automatic and systematic red teaming of LLM-based agents under the MCP tool
poisoning paradigm an open question. To bridge this gap, we propose
AutoMalTool, an automated red teaming framework for LLM-based agents by
generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool
effectively generates malicious MCP tools capable of manipulating the behavior
of mainstream LLM-based agents while evading current detection mechanisms,
thereby revealing new security risks in these agents.

</details>


### [20] [Emerging Paradigms for Securing Federated Learning Systems](https://arxiv.org/abs/2509.21147)
*Amr Akmal Abouelmagd,Amr Hilal*

Main category: cs.CR

TL;DR: 본 연구는 데이터 분산을 유지하면서 연합 학습의 개인 정보를 보호하는 신흥 접근법을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습(FL)은 원시 데이터를 분산된 상태로 유지하면서 협력적인 모델 학습을 촉진합니다.

Method: 신뢰할 수 있는 실행 환경(TEE), 물리적 복제 불가능 함수(PUF), 양자 컴퓨팅(QC), 혼돈 기반 암호화(CBE), 신경모방 컴퓨팅(NC), 군집 지능(SI) 등 다양한 접근 방식을 조사합니다.

Result: 각 접근 방식의 FL 파이프라인과의 관련성을 평가하고 장점, 제한사항, 실용적 고려사항을 정리합니다.

Conclusion: 안전하고 확장 가능한 FL 시스템을 발전시키기 위한 과제와 연구 방향을 제시합니다.

Abstract: Federated Learning (FL) facilitates collaborative model training while
keeping raw data decentralized, making it a conduit for leveraging the power of
IoT devices while maintaining privacy of the locally collected data. However,
existing privacy- preserving techniques present notable hurdles. Methods such
as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential
Privacy (DP) often incur high compu- tational costs and suffer from limited
scalability. This survey examines emerging approaches that hold promise for
enhancing both privacy and efficiency in FL, including Trusted Execution
Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing
(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm
Intelligence (SI). For each paradigm, we assess its relevance to the FL
pipeline, outlining its strengths, limitations, and practical considerations.
We conclude by highlighting open challenges and prospective research avenues,
offering a detailed roadmap for advancing secure and scalable FL systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [21] [Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics](https://arxiv.org/abs/2509.20412)
*Kevin Bradley Dsouza,Graham Alexander Watt,Yuri Leonenko,Juan Moreno-Cruz*

Main category: cs.MA

TL;DR: ECHO-MIMIC는 집단 행동 문제를 해소하기 위한 계산 프레임워크로, 복잡한 글로벌 문제를 각 개별 에이전트에게 이해하기 쉬운 구조화된 문제로 변환한다.


<details>
  <summary>Details</summary>
Motivation: 개별 인센티브와 집단 목표를 조정해야 하는 집단 행동 문제는 전통적으로 잘 구조화되지 않은 문제의 예시로 여겨진다.

Method: ECHO-MIMIC은 두 단계로 운영된다: ECHO는 후보 행동 정책을 인코딩하는 Python 코드 조각을 진화시키고, MIMIC은 그런 정책을 채택하도록 에이전트를 유도하는 자연어 메시지를 진화시킨다.

Result: ECHO-MIMIC은 농업 경관 관리라는 잘 알려진 집단 행동 문제에서 높은 성능을 발휘하는 휴리스틱을 발견하고, 농부 행동을 경관 수준의 생태 목표와 일치시킬 수 있는 맞춤형 메시지를 만듭니다.

Conclusion: ECHO-MIMIC은 집단 행동의 인지적 부담을 간단한 에이전트 수준의 지침으로 변환하여 실질적인 해결책을 제공하고, 확장 가능하고 적응 가능한 정책 설계를 위한 새로운 경로를 열어준다.

Abstract: Collective action problems, which require aligning individual incentives with
collective goals, are classic examples of Ill-Structured Problems (ISPs). For
an individual agent, the causal links between local actions and global outcomes
are unclear, stakeholder objectives often conflict, and no single, clear
algorithm can bridge micro-level choices with macro-level welfare. We present
ECHO-MIMIC, a computational framework that converts this global complexity into
a tractable, Well-Structured Problem (WSP) for each agent by discovering
compact, executable heuristics and persuasive rationales. The framework
operates in two stages: ECHO (Evolutionary Crafting of Heuristics from
Outcomes) evolves snippets of Python code that encode candidate behavioral
policies, while MIMIC (Mechanism Inference & Messaging for
Individual-to-Collective Alignment) evolves companion natural language messages
that motivate agents to adopt those policies. Both phases employ a
large-language-model-driven evolutionary search: the LLM proposes diverse and
context-aware code or text variants, while population-level selection retains
those that maximize collective performance in a simulated environment. We
demonstrate this framework on a canonical ISP in agricultural landscape
management, where local farming decisions impact global ecological
connectivity. Results show that ECHO-MIMIC discovers high-performing heuristics
compared to baselines and crafts tailored messages that successfully align
simulated farmer behavior with landscape-level ecological goals. By coupling
algorithmic rule discovery with tailored communication, ECHO-MIMIC transforms
the cognitive burden of collective action into a simple set of agent-level
instructions, making previously ill-structured problems solvable in practice
and opening a new path toward scalable, adaptive policy design.

</details>


### [22] [RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows](https://arxiv.org/abs/2509.20490)
*Kai Zhang,Corey D Barrett,Jangwon Kim,Lichao Sun,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.MA

TL;DR: RadAgents는 흉부 X선 해석을 위한 다중 에이전트 프레임워크로, 임상 지식과 멀티모드 추론을 통합하여 상황 갈등을 확인하고 해결함으로써 보다 신뢰할 수 있고 투명한 결과를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 임상 작업을 해결하기 위해 전문 에이전트 간의 협업과 도구 사용 및 외부 지식 기반의 보강이 필요합니다.

Method: RadAgents는 임상 사전과 작업 인식 멀티모드 추론을 결합하는 다중 에이전트 프레임워크입니다. 우리는 또한 이론적 검증 메커니즘을 포함하여 상황 갈등을 해결하는 방식으로 grounding 및 멀티모드 검색 보강을 통합합니다.

Result: RadAgents는 신뢰할 수 있고 투명하며 임상 관행과 일치하는 결과를 생성합니다.

Conclusion: 이 시스템은 흉부 X선 해석의 측면에서 기존 방법의 한계를 극복할 수 있는 잠재력을 보여줍니다.

Abstract: Agentic systems offer a potential path to solve complex clinical tasks
through collaboration among specialized agents, augmented by tool use and
external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,
prevailing methods remain limited: (i) reasoning is frequently neither
clinically interpretable nor aligned with guidelines, reflecting mere
aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,
yielding text-only rationales that are not visually grounded; and (iii) systems
rarely detect or resolve cross-tool inconsistencies and provide no principled
verification mechanisms. To bridge the above gaps, we present RadAgents, a
multi-agent framework for CXR interpretation that couples clinical priors with
task-aware multimodal reasoning. In addition, we integrate grounding and
multimodal retrieval-augmentation to verify and resolve context conflicts,
resulting in outputs that are more reliable, transparent, and consistent with
clinical practice.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [A Theory of Multi-Agent Generative Flow Networks](https://arxiv.org/abs/2509.20408)
*Leo Maxime Brunswic,Haozhi Wang,Shuang Luo,Jianye Hao,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 이 논문은 다중 에이전트 생성 흐름 네트워크(MA-GFlowNets)의 이론적 프레임워크를 제안하고, 네트워크 훈련과 실행에 대한 네 가지 알고리즘을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 환경에서 객체 생성의 효율성을 높이기 위해 MA-GFlowNets에 대한 이론적 프레임워크의 필요성이 있다.

Method: MA-GFlowNets를 위한 이론적 프레임워크를 제안하고, 중앙 집중형 및 분산형 훈련 알고리즘을 포함한 네 가지 알고리즘을 개발하였다.

Result: 제안된 프레임워크가 강화 학습 및 MCMC 기반 방법보다 우수함을 실험적으로 입증하였다.

Conclusion: 입증된 MA-GFlowNets의 이론적 보장은 독립 정책들이 보상 함수에 비례하는 샘플을 생성할 수 있음을 보장한다.

Abstract: Generative flow networks utilize a flow-matching loss to learn a stochastic
policy for generating objects from a sequence of actions, such that the
probability of generating a pattern can be proportional to the corresponding
given reward. However, a theoretical framework for multi-agent generative flow
networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose
the theory framework of MA-GFlowNets, which can be applied to multiple agents
to generate objects collaboratively through a series of joint actions. We
further propose four algorithms: a centralized flow network for centralized
training of MA-GFlowNets, an independent flow network for decentralized
execution, a joint flow network for achieving centralized training with
decentralized execution, and its updated conditional version. Joint Flow
training is based on a local-global principle allowing to train a collection of
(local) GFN as a unique (global) GFN. This principle provides a loss of
reasonable complexity and allows to leverage usual results on GFN to provide
theoretical guarantees that the independent policies generate samples with
probability proportional to the reward function. Experimental results
demonstrate the superiority of the proposed framework compared to reinforcement
learning and MCMC-based methods.

</details>


### [24] [AbideGym: Turning Static RL Worlds into Adaptive Challenges](https://arxiv.org/abs/2509.21234)
*Abi Aryan,Zac Liu,Aaron Childress*

Main category: cs.LG

TL;DR: AbideGym은 에이전트의 적응력을 향상시키기 위한 동적 평가 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습으로 훈련된 에이전트는 역학이 변화할 때 실패하는 취약한 정책을 개발하는 경향이 있다.

Method: AbideGym은 에이전트 인식의 섭동과 확장 가능한 복잡성을 도입하여 에피소드 내 적응을 강화한다.

Result: 이 프레임워크는 정적 정책의 약점을 드러내고 복원력을 촉진한다.

Conclusion: AbideGym은 교육 과정 학습, 지속적 학습 및 강력한 일반화를 발전시키기 위한 모듈형 및 재현 가능한 평가 프레임워크를 제공한다.

Abstract: Agents trained with reinforcement learning often develop brittle policies
that fail when dynamics shift, a problem amplified by static benchmarks.
AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and
scalable complexity to enforce intra-episode adaptation. By exposing weaknesses
in static policies and promoting resilience, AbideGym provides a modular,
reproducible evaluation framework for advancing research in curriculum
learning, continual learning, and robust generalization.

</details>


### [25] [Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions](https://arxiv.org/abs/2509.20454)
*Kay Fuhrmeister,Arne Pelzer,Fabian Radke,Julia Lechinger,Mahzad Gharleghi,Thomas Köllmer,Insa Wolf*

Main category: cs.LG

TL;DR: EEG 데이터의 개인 정보 보호를 위해 재식별을 방지하고도 유용성을 유지하는 변환기 기반 오토인코더를 제안한다.


<details>
  <summary>Details</summary>
Motivation: EEG 소비자 장치의 증가로 인해 사용자 개인 정보 보호에 대한 우려가 커지고 있다.

Method: 변환기 기반 오토인코더를 사용하여 재식별이 불가능한 EEG 데이터를 생성한다.

Result: EEG 신호의 재식별 가능성을 크게 줄이면서 기계 학습을 위한 유용성을 보존한다.

Conclusion: 자동 수면 단계 판별에 더해, 익명화 전후의 EEG 데이터의 재식별성과 유용성을 평가하였다.

Abstract: Electroencephalography (EEG) is widely used for recording brain activity and
has seen numerous applications in machine learning, such as detecting sleep
stages and neurological disorders. Several studies have successfully shown the
potential of EEG data for re-identification and leakage of other personal
information. Therefore, the increasing availability of EEG consumer devices
raises concerns about user privacy, motivating us to investigate how to
safeguard this sensitive data while retaining its utility for EEG applications.
To address this challenge, we propose a transformer-based autoencoder to create
EEG data that does not allow for subject re-identification while still
retaining its utility for specific machine learning tasks. We apply our
approach to automatic sleep staging by evaluating the re-identification and
utility potential of EEG data before and after anonymization. The results show
that the re-identifiability of the EEG signal can be substantially reduced
while preserving its utility for machine learning.

</details>


### [26] [Efficiently Attacking Memorization Scores](https://arxiv.org/abs/2509.20463)
*Tue Do,Varun Chandrasekaran,Daniel Alabi*

Main category: cs.LG

TL;DR: 기억 기반 영향 추정기가 적대적인 조작에 취약할 수 있음을 보여주는 연구이다.


<details>
  <summary>Details</summary>
Motivation: 최근 데이터 가치 평가 및 책임 있는 머신러닝에서 기억 점수가 적대적으로 조작될 수 있는지를 탐구하고자 함.

Method: 입력의 유사 역행렬을 계산하는 공격 방법을 사용, black-box 모델 접근만으로도 가능하며 적당한 계산 비용이 듦.

Result: 다양한 이미지 분류 작업에서 공격의 유효성을 실증적으로 검증, 최첨단 모델도 타겟 점수 조작에 취약함을 보여줌.

Conclusion: 영향 기반 속성 추정에서의 중요한 취약점을 강조하고, 강력한 방어의 필요성을 제안함.

Abstract: Influence estimation tools -- such as memorization scores -- are widely used
to understand model behavior, attribute training data, and inform dataset
curation. However, recent applications in data valuation and responsible
machine learning raise the question: can these scores themselves be
adversarially manipulated? In this work, we present a systematic study of the
feasibility of attacking memorization-based influence estimators. We
characterize attacks for producing highly memorized samples as highly sensitive
queries in the regime where a trained algorithm is accurate. Our attack
(calculating the pseudoinverse of the input) is practical, requiring only
black-box access to model outputs and incur modest computational overhead. We
empirically validate our attack across a wide suite of image classification
tasks, showing that even state-of-the-art proxies are vulnerable to targeted
score manipulations. In addition, we provide a theoretical analysis of the
stability of memorization scores under adversarial perturbations, revealing
conditions under which influence estimates are inherently fragile. Our findings
highlight critical vulnerabilities in influence-based attribution and suggest
the need for robust defenses. All code can be found at
https://anonymous.4open.science/r/MemAttack-5413/

</details>


### [27] [Complexity-Driven Policy Optimization](https://arxiv.org/abs/2509.20509)
*Luca Serfilippi,Giorgio Franceschelli,Antonio Corradi,Mirco Musolesi*

Main category: cs.LG

TL;DR: 본 연구에서는 정책 그래디언트 방법의 엔트로피 극대화 대신 복잡성 보너스를 사용하여 탐색 전략을 개선하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 정책 그래디언트 방법은 엔트로피 극대화를 통해 탐색과 이용의 균형을 맞추지만, 엔트로피 극대화는 비효율적인 탐색 전략을 초래할 수 있습니다.

Method: 우리는 엔트로피 대신 샤논 엔트로피와 불균형의 곱으로 정의된 복잡성 보너스를 사용하는 복잡성 기반 정책 최적화(CDPO)를 제안합니다.

Result: CDPO는 PPO보다 복잡성 계수를 선택할 때 더 견고하며, 특히 더 큰 탐색이 필요한 환경에서 우수한 성능을 보입니다.

Conclusion: CDPO는 구조적이면서도 적응 가능한 전략을 발견하도록 에이전트를 유도하여 유용한 비자명한 행동을 유도하는 데 기여합니다.

Abstract: Policy gradient methods often balance exploitation and exploration via
entropy maximization. However, maximizing entropy pushes the policy towards a
uniform random distribution, which represents an unstructured and sometimes
inefficient exploration strategy. In this work, we propose replacing the
entropy bonus with a more robust complexity bonus. In particular, we adopt a
measure of complexity, defined as the product of Shannon entropy and
disequilibrium, where the latter quantifies the distance from the uniform
distribution. This regularizer encourages policies that balance stochasticity
(high entropy) with structure (high disequilibrium), guiding agents toward
regimes where useful, non-trivial behaviors can emerge. Such behaviors arise
because the regularizer suppresses both extremes, e.g., maximal disorder and
complete order, creating pressure for agents to discover structured yet
adaptable strategies. Starting from Proximal Policy Optimization (PPO), we
introduce Complexity-Driven Policy Optimization (CDPO), a new learning
algorithm that replaces entropy with complexity. We show empirically across a
range of discrete action space tasks that CDPO is more robust to the choice of
the complexity coefficient than PPO is with the entropy coefficient, especially
in environments requiring greater exploration.

</details>


### [28] [Policy Compatible Skill Incremental Learning via Lazy Learning Interface](https://arxiv.org/abs/2509.20612)
*Daehee Lee,Dongsu Lee,TaeYoon Kwack,Wonje Choi,Honguk Woo*

Main category: cs.LG

TL;DR: SIL-C는 점진적으로 학습된 기술의 성능을 향상시키는 동시에 기존 정책과의 호환성을 유지하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기술 레퍼토리가 발전함에 따라 기존 기술 기반 정책과의 호환성을 방해할 수 있어 재사용성과 일반화에 제한을 두는 문제 해결.

Method: SIL-C는 쌍방향 지연 학습 기반의 매핑 기법을 사용하여 정책이 참조하는 하위 작업 공간과 에이전트 행동으로 디코딩된 기술 공간을 동적으로 정렬한다.

Result: SIL-C는 다양한 SIL 시나리오에서 평가되었고, 발전하는 기술과 하류 정책 간의 호환성을 유지하면서 학습 과정 전반에 걸쳐 효율성을 보장함을 입증하였다.

Conclusion: SIL-C 프레임워크는 정책 재교육이나 구조 조정 없이도 점진적으로 학습한 기술의 향상을 통해 하류 정책의 성능을 증대시킬 수 있다.

Abstract: Skill Incremental Learning (SIL) is the process by which an embodied agent
expands and refines its skill set over time by leveraging experience gained
through interaction with its environment or by the integration of additional
data. SIL facilitates efficient acquisition of hierarchical policies grounded
in reusable skills for downstream tasks. However, as the skill repertoire
evolves, it can disrupt compatibility with existing skill-based policies,
limiting their reusability and generalization. In this work, we propose SIL-C,
a novel framework that ensures skill-policy compatibility, allowing
improvements in incrementally learned skills to enhance the performance of
downstream policies without requiring policy re-training or structural
adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to
dynamically align the subtask space referenced by policies with the skill space
decoded into agent behaviors. This enables each subtask, derived from the
policy's decomposition of a complex task, to be executed by selecting an
appropriate skill based on trajectory distribution similarity. We evaluate
SIL-C across diverse SIL scenarios and demonstrate that it maintains
compatibility between evolving skills and downstream policies while ensuring
efficiency throughout the learning process.

</details>


### [29] [Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning](https://arxiv.org/abs/2509.20616)
*Hanjiang Hu,Changliu Liu,Na Li,Yebin Wang*

Main category: cs.LG

TL;DR: LLMs의 멀티턴 작업 계획 학습을 단일턴 작업 추론 문제로 변환하여 효율적인 정책 최적화를 가능하게 하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트 응용을 위한 LLM의 가능성을 확장하고자 함.

Method: 멀티턴 작업 계획을 단일턴 작업 추론 문제로 변환하고, 전문가 경로에서 밀집하고 검증 가능한 보상을 통해 그룹 상대 정책 최적화(GRPO)를 사용.

Result: 1.5B 파라미터 모델이 단일턴 GRPO로 훈련되어, 30단계 이상의 긴 작업 계획에서 70%의 성공률을 달성하며, 14B 파라미터 모델보다 우수한 성능을 보임.

Conclusion: 복잡한 작업으로 훈련된 모델이 모든 간단한 하위 작업의 성공적인 완료로 이어지는 강력한 크로스 작업 일반화 능력을 확인.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
knowledge acquisition, reasoning, and tool use, making them promising
candidates for autonomous agent applications. However, training LLM agents for
complex multi-turn task planning faces significant challenges, including sparse
episode-wise rewards, credit assignment across long horizons, and the
computational overhead of reinforcement learning in multi-turn interaction
settings. To this end, this paper introduces a novel approach that transforms
multi-turn task planning into single-turn task reasoning problems, enabling
efficient policy optimization through Group Relative Policy Optimization (GRPO)
with dense and verifiable reward from expert trajectories. Our theoretical
analysis shows that GRPO improvement on single-turn task reasoning results in
higher multi-turn success probability under the minimal turns, as well as the
generalization to subtasks with shorter horizons. Experimental evaluation on
the complex task planning benchmark demonstrates that our 1.5B parameter model
trained with single-turn GRPO achieves superior performance compared to larger
baseline models up to 14B parameters, with success rates of 70% for
long-horizon planning tasks with over 30 steps. We also theoretically and
empirically validate the strong cross-task generalizability that the models
trained on complex tasks can lead to the successful completion of all simpler
subtasks.

</details>


### [30] [Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data](https://arxiv.org/abs/2509.20627)
*Yipu Zhang,Chengshuo Zhang,Ziyu Zhou,Gang Qu,Hao Zheng,Yuping Wang,Hui Shen,Hongwen Deng*

Main category: cs.LG

TL;DR: PFedDL은 데이터 공유 없이 사이트 간 협력적 모델링을 가능하게 하는 새로운 단체 학습 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 데이터 프라이버시 제약은 대규모 신경이미징 분석에 큰 도전을 제시하며, 특히 다중 사이트 기능적 자기공명영상(fMRI) 연구에서 사이트별 이질성이 비독립적이고 동일하게 분포되지 않은(non-IID) 데이터를 초래합니다.

Method: PFedDL은 각 사이트에서 독립적인 사전 학습을 수행하고, 각 사이트 별 사전을 공유된 글로벌 구성 요소와 개인화된 로컬 구성 요소로 분해합니다. 글로벌 원자는 연합 집합을 통해 교차 사이트 일관성을 증진하고, 로컬 원자는 사이트 별 변동성을 포착하기 위해 독립적으로 정제됩니다.

Result: ABIDE 데이터셋에 대한 실험 결과, PFedDL은 비독립적이고 동일하게 분포되지 않은 데이터셋에서 기존 방법들보다 정확도와 강건성에서 뛰어난 성능을 보입니다.

Conclusion: PFedDL은 사이트 간 협력적 모델링을 통해 신경 이미징 분석의 정확도와 강건성을 향상시킵니다.

Abstract: Data privacy constraints pose significant challenges for large-scale
neuroimaging analysis, especially in multi-site functional magnetic resonance
imaging (fMRI) studies, where site-specific heterogeneity leads to
non-independent and identically distributed (non-IID) data. These factors
hinder the development of generalizable models. To address these challenges, we
propose Personalized Federated Dictionary Learning (PFedDL), a novel federated
learning framework that enables collaborative modeling across sites without
sharing raw data. PFedDL performs independent dictionary learning at each site,
decomposing each site-specific dictionary into a shared global component and a
personalized local component. The global atoms are updated via federated
aggregation to promote cross-site consistency, while the local atoms are
refined independently to capture site-specific variability, thereby enhancing
downstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL
outperforms existing methods in accuracy and robustness across non-IID
datasets.

</details>


### [31] [EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense](https://arxiv.org/abs/2509.21129)
*Wei Huang,De-Tian Chu,Lin-Yuan Bai,Wei Kang,Hai-Tao Zhang,Bo Li,Zhi-Mo Han,Jing Ge,Hai-Feng Lin*

Main category: cs.LG

TL;DR: EvoMail은 스팸 및 피싱 탐지의 강력한 성능을 위한 자기 진화 인지 에이전트 프레임워크로, 이메일 내용을 통합한 이질적 이메일 그래프를 구축하고, 적응형 자기 진화 루프를 통해 탐지 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 스팸 및 피싱 공격의 진화로 인해 전통적인 탐지 시스템이 더 이상 효과적이지 않아, 새로운 도구가 필요합니다.

Method: EvoMail은 텍스트, 메타데이터 및 임베디드 리소스를 융합한 이질적 이메일 그래프를 구축하고, 강화된 인지 그래프 신경망을 통해 스팸 캠페인을 식별합니다. 특히, 적대적 자기 진화 루프를 통해 탐지 에이전트가 새로운 회피 전술을 생성하며, 실패로부터 학습합니다.

Result: EvoMail은 실제 데이터셋에서 최신 기법 대비 높은 탐지 정확도와 적응성을 보였습니다.

Conclusion: EvoMail은 차세대 스팸과 피싱 위협에 대한 강력한 방어 프레임워크로서 가능성을 보여줍니다.

Abstract: Modern email spam and phishing attacks have evolved far beyond keyword
blacklists or simple heuristics. Adversaries now craft multi-modal campaigns
that combine natural-language text with obfuscated URLs, forged headers, and
malicious attachments, adapting their strategies within days to bypass filters.
Traditional spam detection systems, which rely on static rules or
single-modality models, struggle to integrate heterogeneous signals or to
continuously adapt, leading to rapid performance degradation.
  We propose EvoMail, a self-evolving cognitive agent framework for robust
detection of spam and phishing. EvoMail first constructs a unified
heterogeneous email graph that fuses textual content, metadata (headers,
senders, domains), and embedded resources (URLs, attachments). A Cognitive
Graph Neural Network enhanced by a Large Language Model (LLM) performs
context-aware reasoning across these sources to identify coordinated spam
campaigns. Most critically, EvoMail engages in an adversarial self-evolution
loop: a ''red-team'' agent generates novel evasion tactics -- such as character
obfuscation or AI-generated phishing text -- while the ''blue-team'' detector
learns from failures, compresses experiences into a memory module, and reuses
them for future reasoning.
  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,
SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that
EvoMail consistently outperforms state-of-the-art baselines in detection
accuracy, adaptability to evolving spam tactics, and interpretability of
reasoning traces. These results highlight EvoMail's potential as a resilient
and explainable defense framework against next-generation spam and phishing
threats.

</details>


### [32] [Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration](https://arxiv.org/abs/2509.20648)
*Yiyuan Pan,Zhe Liu,Hesheng Wang*

Main category: cs.LG

TL;DR: CERMIC이라는 새로운 접근법을 통해 다중 에이전트 강화 학습에서 탐사를 강화하는 기법을 제안하고, 경험적 성과를 통해 기존 알고리즘보다 우수한 성과를 기록함.


<details>
  <summary>Details</summary>
Motivation: 복잡한 다중 에이전트 강화 학습(MARL)에서 희소 보상을 가지고 자율 탐사를 수행하기 위해 에이전트에게 효과적인 내재적 동기를 제공하는 것이 중요하다.

Method: CERMIC은 에이전트가 잡음이 있는 놀라움 신호를 필터링하고 내재적 호기심을 동적으로 조정하여 탐사를 안내할 수 있도록 하는 프레임워크이다.

Result: CERMIC은 VMAS, Meltingpot, SMACv2와 같은 벤치마크에서 평가되었으며, 탐사 성능이 최첨단 알고리즘보다 우수한 결과를 보였다.

Conclusion: CERMIC은 희소 보상이 있는 환경에서 다중 에이전트 탐사를 성공적으로 향상시킨다.

Abstract: Autonomous exploration in complex multi-agent reinforcement learning (MARL)
with sparse rewards critically depends on providing agents with effective
intrinsic motivation. While artificial curiosity offers a powerful
self-supervised signal, it often confuses environmental stochasticity with
meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform
novelty bias, treating all unexpected observations equally. However, peer
behavior novelty, which encode latent task dynamics, are often overlooked,
resulting in suboptimal exploration in decentralized, communication-free MARL
settings. To this end, inspired by how human children adaptively calibrate
their own exploratory behaviors via observing peers, we propose a novel
approach to enhance multi-agent exploration. We introduce CERMIC, a principled
framework that empowers agents to robustly filter noisy surprise signals and
guide exploration by dynamically calibrating their intrinsic curiosity with
inferred multi-agent context. Additionally, CERMIC generates
theoretically-grounded intrinsic rewards, encouraging agents to explore state
transitions with high information gain. We evaluate CERMIC on benchmark suites
including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that
exploration with CERMIC significantly outperforms SoTA algorithms in
sparse-reward environments.

</details>


### [33] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: CE-GPPO는 클리핑된 토큰의 그래디언트를 재도입하여 정책 엔트로피를 효과적으로 조절하는 새로운 강화 학습 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 정책 엔트로피를 관리하는 것은 복잡한 추론 작업을 처리하는 대형 언어 모델 최적화의 핵심 도전 과제이다.

Method: CE-GPPO라는 새로운 알고리즘을 제안하여 클리핑된 토큰의 그래디언트를 부드럽고 제한된 방식으로 재도입한다.

Result: 다양한 모델 스케일에서 CE-GPPO가 강력한 기준선을 일관되게 능가한다는 실험 결과를 제공한다.

Conclusion: CE-GPPO는 엔트로피 불안정을 효과적으로 완화시키는 방법이다.

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [34] [LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training](https://arxiv.org/abs/2509.20786)
*Abhishek Moturu,Anna Goldenberg,Babak Taati*

Main category: cs.LG

TL;DR: LiLAW는 훈련 샘플의 손실 가중치를 동적으로 조정하는 새로운 방법으로, 높은 노이즈 환경에서도 모델의 일반화 및 강건성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 노이즈가 있는 레이블과 데이터 이질성이 있는 상황에서 딥 러닝 모델을 훈련하는 것은 큰 도전이다.

Method: LiLAW는 각 훈련 샘플의 난이도 수준(쉬움, 보통, 어려움)에 따라 손실 가중치를 동적으로 조정하며, 세 개의 학습 가능한 파라미터를 사용해 훈련 중에 정보가 많은 샘플의 우선 순위를 조정한다.

Result: 다양한 일반 및 의료 이미징 데이터셋과 여러 노이즈 수준, 손실 함수 및 아키텍처에서 LiLAW가 항상 성능을 향상시킨다는 것을 보여주었다.

Conclusion: LiLAW는 데이터 증대나 고급 정규화에 크게 의존하지 않으면서도 훈련 설정에서 모델의 일반화 및 강건성을 높이는 효율적인 솔루션을 제공한다.

Abstract: Training deep neural networks in the presence of noisy labels and data
heterogeneity is a major challenge. We introduce Lightweight Learnable Adaptive
Weighting (LiLAW), a novel method that dynamically adjusts the loss weight of
each training sample based on its evolving difficulty level, categorized as
easy, moderate, or hard. Using only three learnable parameters, LiLAW
adaptively prioritizes informative samples throughout training by updating
these weights using a single mini-batch gradient descent step on the validation
set after each training mini-batch, without requiring excessive hyperparameter
tuning or a clean validation set. Extensive experiments across multiple general
and medical imaging datasets, noise levels and types, loss functions, and
architectures with and without pretraining demonstrate that LiLAW consistently
enhances performance, even in high-noise environments. It is effective without
heavy reliance on data augmentation or advanced regularization, highlighting
its practicality. It offers a computationally efficient solution to boost model
generalization and robustness in any neural network training setup.

</details>


### [35] [T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models](https://arxiv.org/abs/2509.20822)
*Hwa Hui Tew,Junn Yong Loo,Yee-Fan Tan,Xinyu Tang,Hernando Ombao,Fuad Noman,Raphael C. -W. Phan,Chee-Ming Ting*

Main category: cs.LG

TL;DR: 이 연구는 기능적 자기 공명 영상(fMRI) 생성을 위한 T2I-Diff 프레임워크를 소개하며, BOLD 신호의 시간-주파수 표현과 분류기 없는 노이즈 제거 확산을 활용한다.


<details>
  <summary>Details</summary>
Motivation: 기능적 자기 공명 영상(fMRI) 데이터 수집의 자원 소모적인 특성으로 인해 데이터 기반 뇌 분석 모델에 필요한 고충실도 샘플의 가용성이 제한된다.

Method: T2I-Diff 프레임워크는 BOLD 신호를 시간 의존적인 푸리에 변환을 통해 창(window)화된 스펙트로그램으로 변환한 다음, 분류기 없는 확산 모델을 훈련시켜 클래스 조건의 주파수 스펙트로그램을 생성한다.

Result: 본 접근 방식의 유효성을 입증하기 위해 다운스트림 fMRI 기반 뇌 네트워크 분류에서 향상된 정확성과 일반화를 보여준다.

Conclusion: T2I-Diff는 복잡한 비안정성과 비선형 BOLD 역학을 효율적으로 다루는 새롭고 효과적인 방법이다.

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging
method that enables in-depth analysis of brain activity by measuring dynamic
changes in the blood oxygenation level-dependent (BOLD) signals. However, the
resource-intensive nature of fMRI data acquisition limits the availability of
high-fidelity samples required for data-driven brain analysis models. While
modern generative models can synthesize fMRI data, they often underperform
because they overlook the complex non-stationarity and nonlinear BOLD dynamics.
To address these challenges, we introduce T2I-Diff, an fMRI generation
framework that leverages time-frequency representation of BOLD signals and
classifier-free denoising diffusion. Specifically, our framework first converts
BOLD signals into windowed spectrograms via a time-dependent Fourier transform,
capturing both the underlying temporal dynamics and spectral evolution.
Subsequently, a classifier-free diffusion model is trained to generate
class-conditioned frequency spectrograms, which are then reverted to BOLD
signals via inverse Fourier transforms. Finally, we validate the efficacy of
our approach by demonstrating improved accuracy and generalization in
downstream fMRI-based brain network classification.

</details>


### [36] [Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease](https://arxiv.org/abs/2509.20842)
*Sungjoon Park,Kyungwook Lee,Soorin Yim,Doyeong Hwang,Dongyun Kim,Soonyoung Lee,Amy Dunn,Daniel Gatti,Elissa Chesler,Kristen O'Connell,Kiyoung Kim*

Main category: cs.LG

TL;DR: MOIRA는 부재 모달리티에 대한 강인성을 갖춘 다중 오믹스 데이터 통합 방법으로, 불완전한 오믹스 데이터에서 학습을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 오믹스 데이터가 복잡한 생체 분자 상호작용을 캡처하며 대사 및 질병에 대한 통찰을 제공하지만, 누락된 모달리티는 이질적인 오믹스 간의 통합 분석을 방해한다.

Method: MOIRA는 각 오믹스 데이터셋을 공유 임베딩 공간으로 투영하여 강인한 학습을 가능하게 하며, 이 공간에서 학습 가능한 가중치 메커니즘을 통해 데이터를 융합한다.

Result: Alzheimer's Disease(AD) 관련 데이터셋인 ROSMAP에서 기존 방법들보다 더 나은 성능을 보였으며, 추가적인 실험을 통해 모달리티별 기여도를 확인하였다.

Conclusion: 특징 중요도 분석을 통해 AD 관련 바이오마커가 이전 문헌과 일치함을 밝혔으며, 우리의 접근법의 생물학적 관련성을 강조하였다.

Abstract: Multi-omics data capture complex biomolecular interactions and provide
insights into metabolism and disease. However, missing modalities hinder
integrative analysis across heterogeneous omics. To address this, we present
MOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early
integration method enabling robust learning from incomplete omics data via
representation alignment and adaptive aggregation. MOIRA leverages all samples,
including those with missing modalities, by projecting each omics dataset onto
a shared embedding space where a learnable weighting mechanism fuses them.
Evaluated on the Religious Order Study and Memory and Aging Project (ROSMAP)
dataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches,
and further ablation studies confirmed modality-wise contributions. Feature
importance analysis revealed AD-related biomarkers consistent with prior
literature, highlighting the biological relevance of our approach.

</details>


### [37] [Model-Based Reinforcement Learning under Random Observation Delays](https://arxiv.org/abs/2509.20869)
*Armin Karamzade,Kyungmin Kim,JB Lanier,Davide Corsi,Roy Fox*

Main category: cs.LG

TL;DR: 이 논문은 강화 학습에서의 랜덤 센서 지연을 다루며, 새로운 모델 기반 필터링 프로세스를 통해 지연을 효과적으로 처리하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 실세계 환경에서 지연이 자주 발생하지만, 표준 강화 학습 알고리즘은 환경에 대한 즉각적인 인식을 가정한다.

Method: 우리는 POMDP에서 발생하는 랜덤 센서 지연을 연구하고, 관찰이 순서대로 도착하지 않는 상황을 입증하며, 모델 기반 강화 학습에 이 개념을 도입한 간단한 지연 인식 프레임워크를 제안한다.

Result: 우리의 방법은 MDP를 위해 개발된 지연 인식 기준선과 비교하여 일관되게 이들을 능가하며, 배포 중 지연 분포 변화에 대한 강인성을 보여준다.

Conclusion: 우리는 로봇 작업에 대한 실험을 통해 우리의 방법과 일반적인 실제 휴리스틱을 비교하며 관찰 지연을 명시적으로 모델링하는 것이 중요함을 강조한다.

Abstract: Delays frequently occur in real-world environments, yet standard
reinforcement learning (RL) algorithms often assume instantaneous perception of
the environment. We study random sensor delays in POMDPs, where observations
may arrive out-of-sequence, a setting that has not been previously addressed in
RL. We analyze the structure of such delays and demonstrate that naive
approaches, such as stacking past observations, are insufficient for reliable
performance. To address this, we propose a model-based filtering process that
sequentially updates the belief state based on an incoming stream of
observations. We then introduce a simple delay-aware framework that
incorporates this idea into model-based RL, enabling agents to effectively
handle random delays. Applying this framework to Dreamer, we compare our
approach to delay-aware baselines developed for MDPs. Our method consistently
outperforms these baselines and demonstrates robustness to delay distribution
shifts during deployment. Additionally, we present experiments on simulated
robotic tasks, comparing our method to common practical heuristics and
emphasizing the importance of explicitly modeling observation delays.

</details>


### [38] [MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction](https://arxiv.org/abs/2509.21004)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 다중 항공기 비행 궤적 예측은 필수적이며 현재 항공 교통 흐름 내에서 항공기가 어떻게 항해하는지에 대한 중요한 통찰을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 비행 궤적 예측은 본질적으로 도전적이며, 개별 항공기 행동과 비행 간의 복잡한 상호 작용을 모델링하는 것이 어렵습니다.

Method: 우리는 다중 에이전트 비행 궤적을 예측하기 위해 새로운 신경망 구조인 Multi-Agent Inverted Transformer (MAIFormer)를 제안합니다. 이 프레임워크는 두 가지 주요 주목 모듈을 특징으로 합니다: (i) 마스크된 다변량 주목과 (ii) 에이전트 주목.

Result: MAIFormer는 한국 인천국제공항의 실제 자동 의존 감시 방송 비행 궤적 데이터셋을 사용하여 평가되었으며, 여러 메트릭에서 최상의 성능을 달성하고 다른 방법보다 뛰어났습니다.

Conclusion: MAIFormer는 인간 관점에서 해석 가능한 예측 결과를 생성하여 모델의 투명성과 항공 교통 관제에서의 실용성을 향상시킵니다.

Abstract: Flight trajectory prediction for multiple aircraft is essential and provides
critical insights into how aircraft navigate within current air traffic flows.
However, predicting multi-agent flight trajectories is inherently challenging.
One of the major difficulties is modeling both the individual aircraft
behaviors over time and the complex interactions between flights. Generating
explainable prediction outcomes is also a challenge. Therefore, we propose a
Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture
that predicts multi-agent flight trajectories. The proposed framework features
two key attention modules: (i) masked multivariate attention, which captures
spatio-temporal patterns of individual aircraft, and (ii) agent attention,
which models the social patterns among multiple agents in complex air traffic
scenes. We evaluated MAIFormer using a real-world automatic dependent
surveillance-broadcast flight trajectory dataset from the terminal airspace of
Incheon International Airport in South Korea. The experimental results show
that MAIFormer achieves the best performance across multiple metrics and
outperforms other methods. In addition, MAIFormer produces prediction outcomes
that are interpretable from a human perspective, which improves both the
transparency of the model and its practical utility in air traffic control.

</details>


### [39] [Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning](https://arxiv.org/abs/2509.21126)
*Xiefeng Wu,Jing Zhao,Shu Zhang,Mingyu Hu*

Main category: cs.LG

TL;DR: VARL은 온라인 강화 학습을 위한 새로운 프레임워크로, 비전-언어 모델의 도메인 지식을 활용하여 행동 제안을 제공해 샘플 효율성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업에 대한 온라인 강화 학습은 최적 Q-함수를 학습하기 위해 많은 상호작용이 필요해 시간이 소요됩니다.

Method: VARL은 비전-언어 모델(VLM)의 도메인 지식을 이용하여 강화 학습 에이전트에게 행동 제안을 제공합니다.

Result: VARL은 다양한 환경과 에이전트 설정에서 평가했을 때 샘플 효율성을 크게 개선했습니다.

Conclusion: 이러한 이점으로 인해 VARL은 온라인 강화 학습을 위한 일반적인 프레임워크가 되어 실제 환경에서도 강화 학습을 직접 적용할 수 있게 합니다.

Abstract: Online reinforcement learning in complex tasks is time-consuming, as massive
interaction steps are needed to learn the optimal Q-function.Vision-language
action (VLA) policies represent a promising direction for solving diverse
tasks; however, their performance on low-level control remains limited, and
effective deployment often requires task-specific expert demonstrations for
fine-tuning. In this paper, we propose \textbf{VARL} (\textbf{V}LM as
\textbf{A}ction advisor for online \textbf{R}einforcement \textbf{L}earning), a
framework that leverages the domain knowledge of vision-language models (VLMs)
to provide action suggestions for reinforcement learning agents. Unlike
previous methods, VARL provides action suggestions rather than designing
heuristic rewards, thereby guaranteeing unchanged optimality and convergence.
The suggested actions increase sample diversity and ultimately improve sample
efficiency, especially in sparse-reward tasks. To validate the effectiveness of
VARL, we evaluate it across diverse environments and agent settings. Results
show that VARL greatly improves sample efficiency without introducing
significant computational overhead. These advantages make VARL a general
framework for online reinforcement learning and make it feasible to directly
apply reinforcement learning from scratch in real-world environments.

</details>


### [40] [GRPO is Secretly a Process Reward Model](https://arxiv.org/abs/2509.21154)
*Michael Sullivan*

Main category: cs.LG

TL;DR: GRPO RL 알고리즘이 비균일 프로세스 보상을 유도하며, 이를 개선하기 위한 간단한 수정으로 성능이 향상됨을 보인다.


<details>
  <summary>Details</summary>
Motivation: GRPO RL 알고리즘이 실제 조건 하에서 유의미한 프로세스 보상 모델을 유도한다는 점을 증명하고, 알고리즘의 결함을 개선하기 위한 방법을 제시하는 것이 이 연구의 동기이다.

Method: GRPO를 프로세스 보상 모델로 활용하여 알고리즘의 결함을 식별하고, 이를 개선하는 수정안인 $	ext{λ-GRPO}$를 제안한다.

Result: $	ext{λ-GRPO}$로 훈련된 LLM은 표준 GRPO로 훈련된 LLM보다 높은 검증 정확도와 성능을 보이며, 피크 성능에 더 빨리 도달한다.

Conclusion: 비용이 많이 드는 명시적 프로세스 보상 모델의 이점을 재고하며, 기본 GRPO 알고리즘 내의 숨겨진 프로세스 보상 모델 구조를 활용하여 모델 성능을 향상시킬 수 있음을 보여준다.

Abstract: We prove theoretically that the GRPO RL algorithm induces a non-trivial
process reward model (PRM), under certain assumptions regarding within-group
overlap of token sequences across completions. We then show empirically that
these assumptions are met under real-world conditions: GRPO does in fact induce
a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a
flaw in the GRPO objective: non-uniformly distributed process steps hinder both
exploration and exploitation (under different conditions). We propose a simple
modification to the algorithm to mitigate this defect ($\lambda$-GRPO), and
show that LLMs trained with $\lambda$-GRPO achieve higher validation accuracy
and performance on downstream reasoning tasks$-$and reach peak performance more
rapidly$-$than LLMs trained with standard GRPO. Our results call into question
the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is
possible to instead leverage the hidden, built-in PRM structure within the
vanilla GRPO algorithm to boost model performance with a negligible impact on
training time and cost.

</details>


### [41] [Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy](https://arxiv.org/abs/2509.21190)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: 이 논문은 기존의 시간 시계열 이상 탐지 모델의 한계를 극복하기 위해 새로운 Pre-training 패러다임인 RCD(Relative Context Discrepancy)를 기반으로 하는 TimeRCD라는 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 이상 탐지(TSAD)는 중요한 작업이지만, 제로샷 방식으로 미지의 데이터에 대해 일반화되는 모델을 개발하는 것은 여전히 큰 도전 과제이다.

Method: TimeRCD는 인접 시간 창 사이의 중요한 불일치를 감지하여 이상을 식별하도록 명시적으로 훈련된다. 이는 표준 Transformer 아키텍처로 구현된 관계적 접근법이다.

Result: 광범위한 실험을 통해 TimeRCD는 다양한 데이터 세트에서 제로샷 TSAD에서 기존의 일반 목적 및 이상 특정 기반 모델보다 크게 우수한 성능을 보여준다.

Conclusion: 우리의 결과는 RCD 패러다임의 우수성을 검증하고 시간 시계열 이상 탐지를 위한 강력하고 일반화 가능한 기초 모델 구축을 위한 새로운 효과적인 경로를 확립한다.

Abstract: Time series anomaly detection (TSAD) is a critical task, but developing
models that generalize to unseen data in a zero-shot manner remains a major
challenge. Prevailing foundation models for TSAD predominantly rely on
reconstruction-based objectives, which suffer from a fundamental objective
mismatch: they struggle to identify subtle anomalies while often
misinterpreting complex normal patterns, leading to high rates of false
negatives and positives. To overcome these limitations, we introduce
\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new
pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning
to reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identify
anomalies by detecting significant discrepancies between adjacent time windows.
This relational approach, implemented with a standard Transformer architecture,
enables the model to capture contextual shifts indicative of anomalies that
reconstruction-based methods often miss. To facilitate this paradigm, we
develop a large-scale, diverse synthetic corpus with token-level anomaly
labels, providing the rich supervisory signal necessary for effective
pre-training. Extensive experiments demonstrate that \texttt{TimeRCD}
significantly outperforms existing general-purpose and anomaly-specific
foundation models in zero-shot TSAD across diverse datasets. Our results
validate the superiority of the RCD paradigm and establish a new, effective
path toward building robust and generalizable foundation models for time series
anomaly detection.

</details>


### [42] [From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM](https://arxiv.org/abs/2509.21207)
*Olga Fink,Ismail Nejjar,Vinay Sharma,Keivan Faghih Niresi,Han Sun,Hao Dong,Chenghao Xu,Amaury Wei,Arthur Bizzi,Raffael Theiler,Yuan Tian,Leandro Von Krannichfeldt,Zhan Ma,Sergei Garmaev,Zepeng Zhang,Mengjie Zhao*

Main category: cs.LG

TL;DR: 본 논문은 물리적 지식이 포함된 기계 학습을 통해 PHM의 한계를 극복하고, 신뢰할 수 있는 예측을 위한 모델 학습 및 관찰 편향이 어떻게 활용될 수 있는지를 검토한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 엔지니어링 시스템의 신뢰성, 안전성 및 효율성을 보장하기 위해 PHM이 필요하다.

Method: 물리적 지식을 데이터 중심 모델에 임베딩하여 학습 및 관찰 편향을 통합하는 방법을 사용한다.

Result: 이 접근 방식은 모델이 물리적으로 일관되고 신뢰할 수 있는 예측을 할 수 있도록 돕는다.

Conclusion: PHM 솔루션을 개인 자산에서 전 함대 배포로 확장하는 도전 과제를 다루고, 메타 학습 및 소수 샷 학습과 같은 빠른 적응 방법을 검토한다.

Abstract: Prognostics and Health Management ensures the reliability, safety, and
efficiency of complex engineered systems by enabling fault detection,
anticipating equipment failures, and optimizing maintenance activities
throughout an asset lifecycle. However, real-world PHM presents persistent
challenges: sensor data is often noisy or incomplete, available labels are
limited, and degradation behaviors and system interdependencies can be highly
complex and nonlinear. Physics-informed machine learning has emerged as a
promising approach to address these limitations by embedding physical knowledge
into data-driven models. This review examines how incorporating learning and
observational biases through physics-informed modeling and data strategies can
guide models toward physically consistent and reliable predictions. Learning
biases embed physical constraints into model training through physics-informed
loss functions and governing equations, or by incorporating properties like
monotonicity. Observational biases influence data selection and synthesis to
ensure models capture realistic system behavior through virtual sensing for
estimating unmeasured states, physics-based simulation for data augmentation,
and multi-sensor fusion strategies. The review then examines how these
approaches enable the transition from passive prediction to active
decision-making through reinforcement learning, which allows agents to learn
maintenance policies that respect physical constraints while optimizing
operational objectives. This closes the loop between model-based predictions,
simulation, and actual system operation, empowering adaptive decision-making.
Finally, the review addresses the critical challenge of scaling PHM solutions
from individual assets to fleet-wide deployment. Fast adaptation methods
including meta-learning and few-shot learning are reviewed alongside domain
generalization techniques ...

</details>


### [43] [Tree Search for LLM Agent Reinforcement Learning](https://arxiv.org/abs/2509.21240)
*Yuxiang Ji,Ziyu Ma,Yong Wang,Guanhua Chen,Xiangxiang Chu,Liaoni Wu*

Main category: cs.LG

TL;DR: 트리 기반 그룹 상대 정책 최적화(Tree-GRPO)를 제안하여 긴 기간 및 다중 턴 에이전트 작업에서의 희박한 감독 문제를 해결하고 기존의 방법보다 우수한 성능을 보입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 결과 보상에만 의존하는 접근 방식이 희박한 감독 문제로 어려움을 겪고 있으므로 이 문제를 해결할 필요가 있습니다.

Method: 트리 검색 기반의 그룹 에이전트 RL 방법인 Tree-GRPO를 도입하여 각 트리 노드가 전체 에이전트 상호 작용 단계를 나타내도록 합니다.

Result: 11개 데이터 세트와 3종의 QA 작업에서 실험을 통해 제안한 트리 기반 RL 방법이 체인 기반 RL 방법보다 우수하다는 것을 입증했습니다.

Conclusion: 이론적 분석을 통해, 트리 내부 수준에서 그룹 상대 정책 최적화 목표가 단계 수준 직접 선호 학습과 동등하다는 것을 보여줍니다.

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the agentic capabilities of large language models (LLMs). In long-term and
multi-turn agent tasks, existing approaches driven solely by outcome rewards
often suffer from the problem of sparse supervision. To address the challenge,
we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped
agent RL method based on tree search, where each tree node represents the
complete agent interaction step. By sharing common prefixes, the tree search
sampling increases the number of rollouts achievable within a fixed budget of
tokens or tool calls. Moreover, we find that the tree-structured trajectory
naturally allows the construction of step-wise process supervised signals even
using only the outcome reward. Based on this, Tree-GRPO estimates the grouped
relative advantages both on intra-tree and inter-tree levels. Through
theoretical analysis, we demonstrate that the objective of intra-tree level
group relative policy optimization is equivalent to that of step-level direct
preference learning. Experiments across 11 datasets and 3 types of QA tasks
demonstrate the superiority of the proposed tree-based RL over the chain-based
RL method.

</details>
