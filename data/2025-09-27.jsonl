{"id": "2509.20412", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20412", "abs": "https://arxiv.org/abs/2509.20412", "authors": ["Kevin Bradley Dsouza", "Graham Alexander Watt", "Yuri Leonenko", "Juan Moreno-Cruz"], "title": "Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics", "comment": null, "summary": "Collective action problems, which require aligning individual incentives with\ncollective goals, are classic examples of Ill-Structured Problems (ISPs). For\nan individual agent, the causal links between local actions and global outcomes\nare unclear, stakeholder objectives often conflict, and no single, clear\nalgorithm can bridge micro-level choices with macro-level welfare. We present\nECHO-MIMIC, a computational framework that converts this global complexity into\na tractable, Well-Structured Problem (WSP) for each agent by discovering\ncompact, executable heuristics and persuasive rationales. The framework\noperates in two stages: ECHO (Evolutionary Crafting of Heuristics from\nOutcomes) evolves snippets of Python code that encode candidate behavioral\npolicies, while MIMIC (Mechanism Inference & Messaging for\nIndividual-to-Collective Alignment) evolves companion natural language messages\nthat motivate agents to adopt those policies. Both phases employ a\nlarge-language-model-driven evolutionary search: the LLM proposes diverse and\ncontext-aware code or text variants, while population-level selection retains\nthose that maximize collective performance in a simulated environment. We\ndemonstrate this framework on a canonical ISP in agricultural landscape\nmanagement, where local farming decisions impact global ecological\nconnectivity. Results show that ECHO-MIMIC discovers high-performing heuristics\ncompared to baselines and crafts tailored messages that successfully align\nsimulated farmer behavior with landscape-level ecological goals. By coupling\nalgorithmic rule discovery with tailored communication, ECHO-MIMIC transforms\nthe cognitive burden of collective action into a simple set of agent-level\ninstructions, making previously ill-structured problems solvable in practice\nand opening a new path toward scalable, adaptive policy design."}
{"id": "2509.20490", "categories": ["cs.MA", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20490", "abs": "https://arxiv.org/abs/2509.20490", "authors": ["Kai Zhang", "Corey D Barrett", "Jangwon Kim", "Lichao Sun", "Tara Taghavi", "Krishnaram Kenthapadi"], "title": "RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows", "comment": "In progress", "summary": "Agentic systems offer a potential path to solve complex clinical tasks\nthrough collaboration among specialized agents, augmented by tool use and\nexternal knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,\nprevailing methods remain limited: (i) reasoning is frequently neither\nclinically interpretable nor aligned with guidelines, reflecting mere\naggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,\nyielding text-only rationales that are not visually grounded; and (iii) systems\nrarely detect or resolve cross-tool inconsistencies and provide no principled\nverification mechanisms. To bridge the above gaps, we present RadAgents, a\nmulti-agent framework for CXR interpretation that couples clinical priors with\ntask-aware multimodal reasoning. In addition, we integrate grounding and\nmultimodal retrieval-augmentation to verify and resolve context conflicts,\nresulting in outputs that are more reliable, transparent, and consistent with\nclinical practice."}
{"id": "2509.20729", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.20729", "abs": "https://arxiv.org/abs/2509.20729", "authors": ["Jiazheng Sun", "Te Yang", "Jiayang Niu", "Mingxuan Li", "Yongyong Lu", "Ruimeng Yang", "Xin Peng"], "title": "Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent", "comment": "20 pages, 12 figures", "summary": "Large multi-modal models (LMMs) have advanced mobile GUI agents. However,\nexisting methods struggle with real-world scenarios involving diverse app\ninterfaces and evolving user needs. End-to-end methods relying on model's\ncommonsense often fail on long-tail apps, and agents without user interaction\nact unilaterally, harming user experience. To address these limitations, we\npropose Fairy, an interactive multi-agent mobile assistant capable of\ncontinuously accumulating app knowledge and self-evolving during usage. Fairy\nenables cross-app collaboration, interactive execution, and continual learning\nthrough three core modules:(i) a Global Task Planner that decomposes user tasks\ninto sub-tasks from a cross-app view; (ii) an App-Level Executor that refines\nsub-tasks into steps and actions based on long- and short-term memory,\nachieving precise execution and user interaction via four core agents operating\nin dual loops; and (iii) a Self-Learner that consolidates execution experience\ninto App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a\nreal-world benchmark with a comprehensive metric suite, and LMM-based agents\nfor automated scoring. Experiments show that Fairy with GPT-4o backbone\noutperforms the previous SoTA by improving user requirement completion by 33.7%\nand reducing redundant steps by 58.5%, showing the effectiveness of its\ninteraction and self-learning."}
{"id": "2509.21134", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21134", "abs": "https://arxiv.org/abs/2509.21134", "authors": ["Yiwen Zhang", "Ziang Chen", "Fanqi Kong", "Yizhe Huang", "Xue Feng"], "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective", "comment": "22 pages, 14 figures", "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities."}
{"id": "2509.20408", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20408", "abs": "https://arxiv.org/abs/2509.20408", "authors": ["Leo Maxime Brunswic", "Haozhi Wang", "Shuang Luo", "Jianye Hao", "Amir Rasouli", "Yinchuan Li"], "title": "A Theory of Multi-Agent Generative Flow Networks", "comment": "Accepted at SPIGM Workshop NeurIPS 2025", "summary": "Generative flow networks utilize a flow-matching loss to learn a stochastic\npolicy for generating objects from a sequence of actions, such that the\nprobability of generating a pattern can be proportional to the corresponding\ngiven reward. However, a theoretical framework for multi-agent generative flow\nnetworks (MA-GFlowNets) has not yet been proposed. In this paper, we propose\nthe theory framework of MA-GFlowNets, which can be applied to multiple agents\nto generate objects collaboratively through a series of joint actions. We\nfurther propose four algorithms: a centralized flow network for centralized\ntraining of MA-GFlowNets, an independent flow network for decentralized\nexecution, a joint flow network for achieving centralized training with\ndecentralized execution, and its updated conditional version. Joint Flow\ntraining is based on a local-global principle allowing to train a collection of\n(local) GFN as a unique (global) GFN. This principle provides a loss of\nreasonable complexity and allows to leverage usual results on GFN to provide\ntheoretical guarantees that the independent policies generate samples with\nprobability proportional to the reward function. Experimental results\ndemonstrate the superiority of the proposed framework compared to reinforcement\nlearning and MCMC-based methods."}
{"id": "2509.20364", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.20364", "abs": "https://arxiv.org/abs/2509.20364", "authors": ["Thomas J Sheffler"], "title": "An Approach to Checking Correctness for Agentic Systems", "comment": "15 pages, 5 figures", "summary": "This paper presents a temporal expression language for monitoring AI agent\nbehavior, enabling systematic error-detection of LLM-based agentic systems that\nexhibit variable outputs due to stochastic generation processes. Drawing from\ntemporal logic techniques used in hardware verification, this approach monitors\nexecution traces of agent tool calls and state transitions to detect deviations\nfrom expected behavioral patterns. Current error-detection approaches rely\nprimarily on text matching of inputs and outputs, which proves fragile due to\nthe natural language variability inherent in LLM responses. The proposed method\ninstead focuses on the sequence of agent actions -- such as tool invocations\nand inter-agent communications -- allowing verification of system behavior\nindependent of specific textual outputs. The temporal expression language\nprovides assertions that capture correct behavioral patterns across multiple\nexecution scenarios. These assertions serve dual purposes: validating prompt\nengineering and guardrail effectiveness during development, and providing\nregression testing when agents are updated with new LLMs or modified logic. The\napproach is demonstrated using a three-agent system, where agents coordinate to\nsolve multi-step reasoning tasks. When powered by large, capable models, all\ntemporal assertions were satisfied across many test runs. However, when smaller\nmodels were substituted in two of the three agents, executions violated\nbehavioral assertions, primarily due to improper tool sequencing and failed\ncoordination handoffs. The temporal expressions successfully flagged these\nanomalies, demonstrating the method's effectiveness for detecting behavioral\nregressions in production agentic systems. This approach provides a foundation\nfor systematic monitoring of AI agent reliability as these systems become\nincreasingly deployed in critical applications."}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality."}
{"id": "2509.21234", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21234", "abs": "https://arxiv.org/abs/2509.21234", "authors": ["Abi Aryan", "Zac Liu", "Aaron Childress"], "title": "AbideGym: Turning Static RL Worlds into Adaptive Challenges", "comment": null, "summary": "Agents trained with reinforcement learning often develop brittle policies\nthat fail when dynamics shift, a problem amplified by static benchmarks.\nAbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and\nscalable complexity to enforce intra-episode adaptation. By exposing weaknesses\nin static policies and promoting resilience, AbideGym provides a modular,\nreproducible evaluation framework for advancing research in curriculum\nlearning, continual learning, and robust generalization."}
{"id": "2509.20454", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20454", "abs": "https://arxiv.org/abs/2509.20454", "authors": ["Kay Fuhrmeister", "Arne Pelzer", "Fabian Radke", "Julia Lechinger", "Mahzad Gharleghi", "Thomas Köllmer", "Insa Wolf"], "title": "Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions", "comment": null, "summary": "Electroencephalography (EEG) is widely used for recording brain activity and\nhas seen numerous applications in machine learning, such as detecting sleep\nstages and neurological disorders. Several studies have successfully shown the\npotential of EEG data for re-identification and leakage of other personal\ninformation. Therefore, the increasing availability of EEG consumer devices\nraises concerns about user privacy, motivating us to investigate how to\nsafeguard this sensitive data while retaining its utility for EEG applications.\nTo address this challenge, we propose a transformer-based autoencoder to create\nEEG data that does not allow for subject re-identification while still\nretaining its utility for specific machine learning tasks. We apply our\napproach to automatic sleep staging by evaluating the re-identification and\nutility potential of EEG data before and after anonymization. The results show\nthat the re-identifiability of the EEG signal can be substantially reduced\nwhile preserving its utility for machine learning."}
{"id": "2509.20562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20562", "abs": "https://arxiv.org/abs/2509.20562", "authors": ["Yubin Ge", "Salvatore Romeo", "Jason Cai", "Monica Sunkara", "Yi Zhang"], "title": "SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Despite the rapid advancements in LLM agents, they still face the challenge\nof generating meaningful reflections due to inadequate error analysis and a\nreliance on rare successful trajectories, especially in complex tasks. In this\nwork, we propose SAMULE, a new framework for self-learning agents powered by a\nretrospective language model that is trained based on Multi-Level Reflection\nSynthesis. It first synthesizes high-quality reflections across three\ncomplementary levels: Single-Trajectory Learning (micro-level) for detailed\nerror correction; Intra-Task Learning (meso-level) to build error taxonomies\nacross multiple trials of the same task, and Inter-Task Learning (macro-level)\nto extract transferable insights based on same typed errors from diverse task\nfailures. Then we fine-tune a language model serving as the retrospective model\nto generate reflections during inference. We further extend our framework to\ninteractive settings through a foresight-based reflection mechanism, enabling\nagents to proactively reflect and adapt during user interactions by comparing\npredicted and actual responses. Extensive experiments on three challenging\nbenchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our\napproach significantly outperforms reflection-based baselines. Our results\nhighlight the critical role of well-designed reflection synthesis and\nfailure-centric learning in building self-improving LLM agents."}
{"id": "2509.20808", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20808", "abs": "https://arxiv.org/abs/2509.20808", "authors": ["Raghul Saravanan", "Sudipta Paria", "Aritra Dasgupta", "Swarup Bhunia", "Sai Manoj P D"], "title": "Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis", "comment": "7 pages, 6 figures, 4 tables", "summary": "Hardware Fuzzing emerged as one of the crucial techniques for finding\nsecurity flaws in modern hardware designs by testing a wide range of input\nscenarios. One of the main challenges is creating high-quality input seeds that\nmaximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)\nmethods help explore designs more effectively, but they struggle to focus on\nspecific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)\ntechniques like DirectFuzz try to solve this by generating targeted tests, but\nit has major drawbacks, such as supporting only limited hardware description\nlanguages, not scaling well to large circuits, and having issues with\nabstraction mismatches. To address these problems, we introduce a novel\nframework, PROFUZZ, that follows the DGF approach and combines fuzzing with\nAutomatic Test Pattern Generation (ATPG) for more efficient fuzzing. By\nleveraging ATPG's structural analysis capabilities, PROFUZZ can generate\nprecise input seeds that target specific design regions more effectively while\nmaintaining high fuzzing throughput. Our experiments show that PROFUZZ scales\n30x better than DirectFuzz when handling multiple target sites, improves\ncoverage by 11.66%, and runs 2.76x faster, highlighting its scalability and\neffectiveness for directed fuzzing in complex hardware systems."}
{"id": "2509.20463", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20463", "abs": "https://arxiv.org/abs/2509.20463", "authors": ["Tue Do", "Varun Chandrasekaran", "Daniel Alabi"], "title": "Efficiently Attacking Memorization Scores", "comment": null, "summary": "Influence estimation tools -- such as memorization scores -- are widely used\nto understand model behavior, attribute training data, and inform dataset\ncuration. However, recent applications in data valuation and responsible\nmachine learning raise the question: can these scores themselves be\nadversarially manipulated? In this work, we present a systematic study of the\nfeasibility of attacking memorization-based influence estimators. We\ncharacterize attacks for producing highly memorized samples as highly sensitive\nqueries in the regime where a trained algorithm is accurate. Our attack\n(calculating the pseudoinverse of the input) is practical, requiring only\nblack-box access to model outputs and incur modest computational overhead. We\nempirically validate our attack across a wide suite of image classification\ntasks, showing that even state-of-the-art proxies are vulnerable to targeted\nscore manipulations. In addition, we provide a theoretical analysis of the\nstability of memorization scores under adversarial perturbations, revealing\nconditions under which influence estimates are inherently fragile. Our findings\nhighlight critical vulnerabilities in influence-based attribution and suggest\nthe need for robust defenses. All code can be found at\nhttps://anonymous.4open.science/r/MemAttack-5413/"}
{"id": "2509.20640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20640", "abs": "https://arxiv.org/abs/2509.20640", "authors": ["Oluwakemi T. Olayinka", "Sumeet Jeswani", "Divine Iloh"], "title": "Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI", "comment": null, "summary": "Traditional static cybersecurity models often struggle with scalability,\nreal-time detection, and contextual responsiveness in the current digital\nproduct ecosystems which include cloud services, application programming\ninterfaces (APIs), mobile platforms, and edge devices. This study introduces\nautonomous goal driven agents capable of dynamic learning and context-aware\ndecision making as part of an adaptive cybersecurity architecture driven by\nagentic artificial intelligence (AI). To facilitate autonomous threat\nmitigation, proactive policy enforcement, and real-time anomaly detection, this\nframework integrates agentic AI across the key ecosystem layers. Behavioral\nbaselining, decentralized risk scoring, and federated threat intelligence\nsharing are important features. The capacity of the system to identify zero-day\nattacks and dynamically modify access policies was demonstrated through native\ncloud simulations. The evaluation results show increased adaptability,\ndecreased response latency, and improved detection accuracy. The architecture\nprovides an intelligent and scalable blueprint for safeguarding complex digital\ninfrastructure and is compatible with zero-trust models, thereby supporting the\nadherence to international cybersecurity regulations."}
{"id": "2509.20924", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20924", "abs": "https://arxiv.org/abs/2509.20924", "authors": ["Hanbo Huang", "Yiran Zhang", "Hao Zheng", "Xuan Gong", "Yihan Li", "Lin Liu", "Shiyu Liang"], "title": "RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks", "comment": null, "summary": "Large Language Models (LLMs) watermarking has shown promise in detecting\nAI-generated content and mitigating misuse, with prior work claiming robustness\nagainst paraphrasing and text editing. In this paper, we argue that existing\nevaluations are not sufficiently adversarial, obscuring critical\nvulnerabilities and overstating the security. To address this, we introduce\nadaptive robustness radius, a formal metric that quantifies watermark\nresilience against adaptive adversaries. We theoretically prove that optimizing\nthe attack context and model parameters can substantially reduce this radius,\nmaking watermarks highly susceptible to paraphrase attacks. Leveraging this\ninsight, we propose RLCracker, a reinforcement learning (RL)-based adaptive\nattack that erases watermarks while preserving semantic fidelity. RLCracker\nrequires only limited watermarked examples and zero access to the detector.\nDespite weak supervision, it empowers a 3B model to achieve 98.5% removal\nsuccess and an average 0.92 P-SP score on 1,500-token Unigram-marked texts\nafter training on only 100 short samples. This performance dramatically exceeds\n6.75% by GPT-4o and generalizes across five model sizes over ten watermarking\nschemes. Our results confirm that adaptive attacks are broadly effective and\npose a fundamental threat to current watermarking defenses."}
{"id": "2509.20509", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20509", "abs": "https://arxiv.org/abs/2509.20509", "authors": ["Luca Serfilippi", "Giorgio Franceschelli", "Antonio Corradi", "Mirco Musolesi"], "title": "Complexity-Driven Policy Optimization", "comment": null, "summary": "Policy gradient methods often balance exploitation and exploration via\nentropy maximization. However, maximizing entropy pushes the policy towards a\nuniform random distribution, which represents an unstructured and sometimes\ninefficient exploration strategy. In this work, we propose replacing the\nentropy bonus with a more robust complexity bonus. In particular, we adopt a\nmeasure of complexity, defined as the product of Shannon entropy and\ndisequilibrium, where the latter quantifies the distance from the uniform\ndistribution. This regularizer encourages policies that balance stochasticity\n(high entropy) with structure (high disequilibrium), guiding agents toward\nregimes where useful, non-trivial behaviors can emerge. Such behaviors arise\nbecause the regularizer suppresses both extremes, e.g., maximal disorder and\ncomplete order, creating pressure for agents to discover structured yet\nadaptable strategies. Starting from Proximal Policy Optimization (PPO), we\nintroduce Complexity-Driven Policy Optimization (CDPO), a new learning\nalgorithm that replaces entropy with complexity. We show empirically across a\nrange of discrete action space tasks that CDPO is more robust to the choice of\nthe complexity coefficient than PPO is with the entropy coefficient, especially\nin environments requiring greater exploration."}
{"id": "2509.20729", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.20729", "abs": "https://arxiv.org/abs/2509.20729", "authors": ["Jiazheng Sun", "Te Yang", "Jiayang Niu", "Mingxuan Li", "Yongyong Lu", "Ruimeng Yang", "Xin Peng"], "title": "Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent", "comment": "20 pages, 12 figures", "summary": "Large multi-modal models (LMMs) have advanced mobile GUI agents. However,\nexisting methods struggle with real-world scenarios involving diverse app\ninterfaces and evolving user needs. End-to-end methods relying on model's\ncommonsense often fail on long-tail apps, and agents without user interaction\nact unilaterally, harming user experience. To address these limitations, we\npropose Fairy, an interactive multi-agent mobile assistant capable of\ncontinuously accumulating app knowledge and self-evolving during usage. Fairy\nenables cross-app collaboration, interactive execution, and continual learning\nthrough three core modules:(i) a Global Task Planner that decomposes user tasks\ninto sub-tasks from a cross-app view; (ii) an App-Level Executor that refines\nsub-tasks into steps and actions based on long- and short-term memory,\nachieving precise execution and user interaction via four core agents operating\nin dual loops; and (iii) a Self-Learner that consolidates execution experience\ninto App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a\nreal-world benchmark with a comprehensive metric suite, and LMM-based agents\nfor automated scoring. Experiments show that Fairy with GPT-4o backbone\noutperforms the previous SoTA by improving user requirement completion by 33.7%\nand reducing redundant steps by 58.5%, showing the effectiveness of its\ninteraction and self-learning."}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents."}
{"id": "2509.20612", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20612", "abs": "https://arxiv.org/abs/2509.20612", "authors": ["Daehee Lee", "Dongsu Lee", "TaeYoon Kwack", "Wonje Choi", "Honguk Woo"], "title": "Policy Compatible Skill Incremental Learning via Lazy Learning Interface", "comment": null, "summary": "Skill Incremental Learning (SIL) is the process by which an embodied agent\nexpands and refines its skill set over time by leveraging experience gained\nthrough interaction with its environment or by the integration of additional\ndata. SIL facilitates efficient acquisition of hierarchical policies grounded\nin reusable skills for downstream tasks. However, as the skill repertoire\nevolves, it can disrupt compatibility with existing skill-based policies,\nlimiting their reusability and generalization. In this work, we propose SIL-C,\na novel framework that ensures skill-policy compatibility, allowing\nimprovements in incrementally learned skills to enhance the performance of\ndownstream policies without requiring policy re-training or structural\nadaptation. SIL-C employs a bilateral lazy learning-based mapping technique to\ndynamically align the subtask space referenced by policies with the skill space\ndecoded into agent behaviors. This enables each subtask, derived from the\npolicy's decomposition of a complex task, to be executed by selecting an\nappropriate skill based on trajectory distribution similarity. We evaluate\nSIL-C across diverse SIL scenarios and demonstrate that it maintains\ncompatibility between evolving skills and downstream policies while ensuring\nefficiency throughout the learning process."}
{"id": "2509.20754", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20754", "abs": "https://arxiv.org/abs/2509.20754", "authors": ["Yufan Mao", "Hanjing Ye", "Wenlong Dong", "Chengjie Zhang", "Hong Zhang"], "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning", "comment": null, "summary": "Navigating complex environments requires robots to effectively store\nobservations as memories and leverage them to answer human queries about\nspatial locations, which is a critical yet underexplored research challenge.\nWhile prior work has made progress in constructing robotic memory, few have\naddressed the principled mechanisms needed for efficient memory retrieval and\nintegration. To bridge this gap, we propose Meta-Memory, a large language model\n(LLM)-driven agent that constructs a high-density memory representation of the\nenvironment. The key innovation of Meta-Memory lies in its capacity to retrieve\nand integrate relevant memories through joint reasoning over semantic and\nspatial modalities in response to natural language location queries, thereby\nempowering robots with robust and accurate spatial reasoning capabilities. To\nevaluate its performance, we introduce SpaceLocQA, a large-scale dataset\nencompassing diverse real-world spatial question-answering scenarios.\nExperimental results show that Meta-Memory significantly outperforms\nstate-of-the-art methods on both the SpaceLocQA and the public NaVQA\nbenchmarks. Furthermore, we successfully deployed Meta-Memory on real-world\nrobotic platforms, demonstrating its practical utility in complex environments.\nProject page: https://itsbaymax.github.io/meta-memory.github.io/ ."}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."}
{"id": "2509.20616", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.20616", "abs": "https://arxiv.org/abs/2509.20616", "authors": ["Hanjiang Hu", "Changliu Liu", "Na Li", "Yebin Wang"], "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nknowledge acquisition, reasoning, and tool use, making them promising\ncandidates for autonomous agent applications. However, training LLM agents for\ncomplex multi-turn task planning faces significant challenges, including sparse\nepisode-wise rewards, credit assignment across long horizons, and the\ncomputational overhead of reinforcement learning in multi-turn interaction\nsettings. To this end, this paper introduces a novel approach that transforms\nmulti-turn task planning into single-turn task reasoning problems, enabling\nefficient policy optimization through Group Relative Policy Optimization (GRPO)\nwith dense and verifiable reward from expert trajectories. Our theoretical\nanalysis shows that GRPO improvement on single-turn task reasoning results in\nhigher multi-turn success probability under the minimal turns, as well as the\ngeneralization to subtasks with shorter horizons. Experimental evaluation on\nthe complex task planning benchmark demonstrates that our 1.5B parameter model\ntrained with single-turn GRPO achieves superior performance compared to larger\nbaseline models up to 14B parameters, with success rates of 70% for\nlong-horizon planning tasks with over 30 steps. We also theoretically and\nempirically validate the strong cross-task generalizability that the models\ntrained on complex tasks can lead to the successful completion of all simpler\nsubtasks."}
{"id": "2509.20798", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20798", "abs": "https://arxiv.org/abs/2509.20798", "authors": ["Lipeng Ma", "Yixuan Li", "Weidong Yang", "Mingjie Zhou", "Xinyi Liu", "Ben Fei", "Shuhao Li", "Xiaoyan Sun", "Sihang Jiang", "Yanghua Xiao"], "title": "LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks", "comment": "under review", "summary": "Log analysis is crucial for monitoring system health and diagnosing failures\nin complex systems. Recent advances in large language models (LLMs) offer new\nopportunities for automated log analysis, leveraging their reasoning\ncapabilities to perform tasks such as anomaly detection and failure prediction.\nHowever, general-purpose LLMs struggle to formulate structured reasoning\nworkflows that align with expert cognition and deliver precise details of\nreasoning steps. To address these challenges, we propose LogReasoner, a\ncoarse-to-fine reasoning enhancement framework designed to enable LLMs to\nreason log analysis tasks like experts. LogReasoner consists of two stages: (1)\ncoarse-grained enhancement of expert thinking, where high-level expert thoughts\nare constructed from collected troubleshooting flowcharts and existing tasks to\nenable LLMs to formulate structured reasoning workflows and (2) fine-grained\nenhancement of specific steps, where we first fine-tune the LLM with\ntask-specific stepwise solutions to enhance the LLM for instantiated reasoning,\nthen employ the preference learning to calibrate the LLM's reasoning details\nfrom its mistakes, further strengthen the LLM's analytical granularity and\ncorrectness. We evaluate LogReasoner on four distinct log analysis tasks using\nopen-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that\nLogReasoner significantly outperforms existing LLMs, achieving state-of-the-art\nperformance and demonstrating its effectiveness in enhancing the reasoning\ncapabilities of LLMs for log analysis."}
{"id": "2509.20454", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.20454", "abs": "https://arxiv.org/abs/2509.20454", "authors": ["Kay Fuhrmeister", "Arne Pelzer", "Fabian Radke", "Julia Lechinger", "Mahzad Gharleghi", "Thomas Köllmer", "Insa Wolf"], "title": "Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions", "comment": null, "summary": "Electroencephalography (EEG) is widely used for recording brain activity and\nhas seen numerous applications in machine learning, such as detecting sleep\nstages and neurological disorders. Several studies have successfully shown the\npotential of EEG data for re-identification and leakage of other personal\ninformation. Therefore, the increasing availability of EEG consumer devices\nraises concerns about user privacy, motivating us to investigate how to\nsafeguard this sensitive data while retaining its utility for EEG applications.\nTo address this challenge, we propose a transformer-based autoencoder to create\nEEG data that does not allow for subject re-identification while still\nretaining its utility for specific machine learning tasks. We apply our\napproach to automatic sleep staging by evaluating the re-identification and\nutility potential of EEG data before and after anonymization. The results show\nthat the re-identifiability of the EEG signal can be substantially reduced\nwhile preserving its utility for machine learning."}
{"id": "2509.20627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20627", "abs": "https://arxiv.org/abs/2509.20627", "authors": ["Yipu Zhang", "Chengshuo Zhang", "Ziyu Zhou", "Gang Qu", "Hao Zheng", "Yuping Wang", "Hui Shen", "Hongwen Deng"], "title": "Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data", "comment": null, "summary": "Data privacy constraints pose significant challenges for large-scale\nneuroimaging analysis, especially in multi-site functional magnetic resonance\nimaging (fMRI) studies, where site-specific heterogeneity leads to\nnon-independent and identically distributed (non-IID) data. These factors\nhinder the development of generalizable models. To address these challenges, we\npropose Personalized Federated Dictionary Learning (PFedDL), a novel federated\nlearning framework that enables collaborative modeling across sites without\nsharing raw data. PFedDL performs independent dictionary learning at each site,\ndecomposing each site-specific dictionary into a shared global component and a\npersonalized local component. The global atoms are updated via federated\naggregation to promote cross-site consistency, while the local atoms are\nrefined independently to capture site-specific variability, thereby enhancing\ndownstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL\noutperforms existing methods in accuracy and robustness across non-IID\ndatasets."}
{"id": "2509.20953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20953", "abs": "https://arxiv.org/abs/2509.20953", "authors": ["Najla Zuhir", "Amna Mohammad Salim", "Parvathy Premkumar", "Moshiur Farazi"], "title": "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM", "comment": "Paper accepted for presentation at ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "We present an advanced approach to mobile app review analysis aimed at\naddressing limitations inherent in traditional star-rating systems. Star\nratings, although intuitive and popular among users, often fail to capture the\nnuanced feedback present in detailed review texts. Traditional NLP techniques\n-- such as lexicon-based methods and classical machine learning classifiers --\nstruggle to interpret contextual nuances, domain-specific terminology, and\nsubtle linguistic features like sarcasm. To overcome these limitations, we\npropose a modular framework leveraging large language models (LLMs) enhanced by\nstructured prompting techniques. Our method quantifies discrepancies between\nnumerical ratings and textual sentiment, extracts detailed, feature-level\ninsights, and supports interactive exploration of reviews through\nretrieval-augmented conversational question answering (RAG-QA). Comprehensive\nexperiments conducted on three diverse datasets (AWARE, Google Play, and\nSpotify) demonstrate that our LLM-driven approach significantly surpasses\nbaseline methods, yielding improved accuracy, robustness, and actionable\ninsights in challenging and context-rich review scenarios."}
{"id": "2509.21129", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21129", "abs": "https://arxiv.org/abs/2509.21129", "authors": ["Wei Huang", "De-Tian Chu", "Lin-Yuan Bai", "Wei Kang", "Hai-Tao Zhang", "Bo Li", "Zhi-Mo Han", "Jing Ge", "Hai-Feng Lin"], "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense", "comment": null, "summary": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats."}
{"id": "2509.20648", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20648", "abs": "https://arxiv.org/abs/2509.20648", "authors": ["Yiyuan Pan", "Zhe Liu", "Hesheng Wang"], "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration", "comment": null, "summary": "Autonomous exploration in complex multi-agent reinforcement learning (MARL)\nwith sparse rewards critically depends on providing agents with effective\nintrinsic motivation. While artificial curiosity offers a powerful\nself-supervised signal, it often confuses environmental stochasticity with\nmeaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform\nnovelty bias, treating all unexpected observations equally. However, peer\nbehavior novelty, which encode latent task dynamics, are often overlooked,\nresulting in suboptimal exploration in decentralized, communication-free MARL\nsettings. To this end, inspired by how human children adaptively calibrate\ntheir own exploratory behaviors via observing peers, we propose a novel\napproach to enhance multi-agent exploration. We introduce CERMIC, a principled\nframework that empowers agents to robustly filter noisy surprise signals and\nguide exploration by dynamically calibrating their intrinsic curiosity with\ninferred multi-agent context. Additionally, CERMIC generates\ntheoretically-grounded intrinsic rewards, encouraging agents to explore state\ntransitions with high information gain. We evaluate CERMIC on benchmark suites\nincluding VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that\nexploration with CERMIC significantly outperforms SoTA algorithms in\nsparse-reward environments."}
{"id": "2509.20998", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20998", "abs": "https://arxiv.org/abs/2509.20998", "authors": ["Panagiotis Michelakis", "Yiannis Hadjiyiannis", "Dimitrios Stamoulis"], "title": "CORE: Full-Path Evaluation of LLM Agents Beyond Final State", "comment": "Accepted: LAW 2025 Workshop NeurIPS 2025", "summary": "Evaluating AI agents that solve real-world tasks through function-call\nsequences remains an open challenge. Existing agentic benchmarks often reduce\nevaluation to a binary judgment of the final state, overlooking critical\naspects such as safety, efficiency, and intermediate correctness. We propose a\nframework based on deterministic finite automata (DFAs) that encodes tasks as\nsets of valid tool-use paths, enabling principled assessment of agent behavior\nin diverse world models. Building on this foundation, we introduce CORE, a\nsuite of five metrics, namely Path Correctness, Path Correctness - Kendall's\ntau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that\nquantify alignment with expected execution patterns. Across diverse worlds, our\nmethod reveals important performance differences between agents that would\notherwise appear equivalent under traditional final-state evaluation schemes."}
{"id": "2509.20712", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20712", "abs": "https://arxiv.org/abs/2509.20712", "authors": ["Zhenpeng Su", "Leiyu Pan", "Minxuan Lv", "Yuntao Li", "Wenping Hu", "Fuzheng Zhang", "Kun Gai", "Guorui Zhou"], "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}ontrolling \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales."}
{"id": "2509.21035", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21035", "abs": "https://arxiv.org/abs/2509.21035", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Yue Xiu", "Dusit Niyato"], "title": "CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering", "comment": null, "summary": "Knowledge graphs provide structured context for multi-hop question answering,\nbut deployed systems must balance answer accuracy with strict latency and cost\ntargets while preserving provenance. Static k-hop expansions and \"think-longer\"\nprompting often over-retrieve, inflate context, and yield unpredictable\nruntime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework\nthat treats context construction as a sequential decision process over\nknowledge graphs, deciding what to expand, which paths to follow or backtrack,\nwhat evidence to keep, and when to stop. Latency (interaction steps) and prompt\ncost (selected tokens) are exposed as user-specified budgets or prices,\nallowing per-query adaptation to trade-offs among accuracy, latency, and cost\nwithout retraining. CLAUSE employs the proposed Lagrangian-Constrained\nMulti-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate\nthree agents: Subgraph Architect, Path Navigator, and Context Curator, so that\nsubgraph construction, reasoning-path discovery, and evidence selection are\njointly optimized under per-query resource budgets on edge edits, interaction\nsteps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields\nhigher EM@1 while reducing subgraph growth and end-to-end latency at equal or\nlower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline\n(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower\nedge growth. The resulting contexts are compact, provenance-preserving, and\ndeliver predictable performance under deployment constraints."}
{"id": "2509.20786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20786", "abs": "https://arxiv.org/abs/2509.20786", "authors": ["Abhishek Moturu", "Anna Goldenberg", "Babak Taati"], "title": "LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training", "comment": null, "summary": "Training deep neural networks in the presence of noisy labels and data\nheterogeneity is a major challenge. We introduce Lightweight Learnable Adaptive\nWeighting (LiLAW), a novel method that dynamically adjusts the loss weight of\neach training sample based on its evolving difficulty level, categorized as\neasy, moderate, or hard. Using only three learnable parameters, LiLAW\nadaptively prioritizes informative samples throughout training by updating\nthese weights using a single mini-batch gradient descent step on the validation\nset after each training mini-batch, without requiring excessive hyperparameter\ntuning or a clean validation set. Extensive experiments across multiple general\nand medical imaging datasets, noise levels and types, loss functions, and\narchitectures with and without pretraining demonstrate that LiLAW consistently\nenhances performance, even in high-noise environments. It is effective without\nheavy reliance on data augmentation or advanced regularization, highlighting\nits practicality. It offers a computationally efficient solution to boost model\ngeneralization and robustness in any neural network training setup."}
{"id": "2509.21054", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21054", "abs": "https://arxiv.org/abs/2509.21054", "authors": ["Haodong Zhao", "Jidong Li", "Zhaomin Wu", "Tianjie Ju", "Zhuosheng Zhang", "Bingsheng He", "Gongshen Liu"], "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems", "comment": "Work in progress", "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large\nLanguage Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to\nsolve complex problems, necessitates a deep understanding of the persuasion\ndynamics that govern their interactions. This paper challenges the prevailing\nhypothesis that persuasive efficacy is primarily a function of model scale. We\npropose instead that these dynamics are fundamentally dictated by a model's\nunderlying cognitive process, especially its capacity for explicit reasoning.\nThrough a series of multi-agent persuasion experiments, we uncover a\nfundamental trade-off we term the Persuasion Duality. Our findings reveal that\nthe reasoning process in LRMs exhibits significantly greater resistance to\npersuasion, maintaining their initial beliefs more robustly. Conversely, making\nthis reasoning process transparent by sharing the \"thinking content\"\ndramatically increases their ability to persuade others. We further consider\nmore complex transmission persuasion situations and reveal complex dynamics of\ninfluence propagation and decay within multi-hop persuasion between multiple\nagent networks. This research provides systematic evidence linking a model's\ninternal processing architecture to its external persuasive behavior, offering\na novel explanation for the susceptibility of advanced models and highlighting\ncritical implications for the safety, robustness, and design of future MAS."}
{"id": "2509.20822", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20822", "abs": "https://arxiv.org/abs/2509.20822", "authors": ["Hwa Hui Tew", "Junn Yong Loo", "Yee-Fan Tan", "Xinyu Tang", "Hernando Ombao", "Fuad Noman", "Raphael C. -W. Phan", "Chee-Ming Ting"], "title": "T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models", "comment": "Accepted at the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2025)", "summary": "Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging\nmethod that enables in-depth analysis of brain activity by measuring dynamic\nchanges in the blood oxygenation level-dependent (BOLD) signals. However, the\nresource-intensive nature of fMRI data acquisition limits the availability of\nhigh-fidelity samples required for data-driven brain analysis models. While\nmodern generative models can synthesize fMRI data, they often underperform\nbecause they overlook the complex non-stationarity and nonlinear BOLD dynamics.\nTo address these challenges, we introduce T2I-Diff, an fMRI generation\nframework that leverages time-frequency representation of BOLD signals and\nclassifier-free denoising diffusion. Specifically, our framework first converts\nBOLD signals into windowed spectrograms via a time-dependent Fourier transform,\ncapturing both the underlying temporal dynamics and spectral evolution.\nSubsequently, a classifier-free diffusion model is trained to generate\nclass-conditioned frequency spectrograms, which are then reverted to BOLD\nsignals via inverse Fourier transforms. Finally, we validate the efficacy of\nour approach by demonstrating improved accuracy and generalization in\ndownstream fMRI-based brain network classification."}
{"id": "2509.21072", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21072", "abs": "https://arxiv.org/abs/2509.21072", "authors": ["Kaiwen He", "Zhiwei Wang", "Chenyi Zhuang", "Jinjie Gu"], "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution", "comment": null, "summary": "Recent years, multimodal models have made remarkable strides and pave the way\nfor intelligent browser use agents. However, when solving tasks on real world\nwebpages in multi-turn, long-horizon trajectories, current agents still suffer\nfrom disordered action sequencing and excessive trial and error during\nexecution. This paper introduces Recon-Act, a self-evolving multi-agent\nframework grounded in Reconnaissance-Action behavioral paradigm. The system\ncomprises a Reconnaissance Team and an Action Team: the former conducts\ncomparative analysis and tool generation, while the latter handles intent\ndecomposition, tool orchestration, and execution. By contrasting the erroneous\ntrajectories with successful ones, the Reconnaissance Team infers remedies, and\nabstracts them into a unified notion of generalized tools, either expressed as\nhints or as rule-based codes, and register to the tool archive in real time.\nThe Action Team reinference the process empowered with these targeting tools,\nthus establishing a closed-loop training pipeline of\ndata-tools-action-feedback. Following the 6 level implementation roadmap\nproposed in this work, we have currently reached Level 3 (with limited\nhuman-in-the-loop intervention). Leveraging generalized tools obtained through\nreconnaissance, Recon-Act substantially improves adaptability to unseen\nwebsites and solvability on long-horizon tasks, and achieves state-of-the-art\nperformance on the challenging VisualWebArena dataset."}
{"id": "2509.20842", "categories": ["cs.LG", "cs.AI", "I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2509.20842", "abs": "https://arxiv.org/abs/2509.20842", "authors": ["Sungjoon Park", "Kyungwook Lee", "Soorin Yim", "Doyeong Hwang", "Dongyun Kim", "Soonyoung Lee", "Amy Dunn", "Daniel Gatti", "Elissa Chesler", "Kristen O'Connell", "Kiyoung Kim"], "title": "Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease", "comment": null, "summary": "Multi-omics data capture complex biomolecular interactions and provide\ninsights into metabolism and disease. However, missing modalities hinder\nintegrative analysis across heterogeneous omics. To address this, we present\nMOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early\nintegration method enabling robust learning from incomplete omics data via\nrepresentation alignment and adaptive aggregation. MOIRA leverages all samples,\nincluding those with missing modalities, by projecting each omics dataset onto\na shared embedding space where a learnable weighting mechanism fuses them.\nEvaluated on the Religious Order Study and Memory and Aging Project (ROSMAP)\ndataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches,\nand further ablation studies confirmed modality-wise contributions. Feature\nimportance analysis revealed AD-related biomarkers consistent with prior\nliterature, highlighting the biological relevance of our approach."}
{"id": "2509.21134", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21134", "abs": "https://arxiv.org/abs/2509.21134", "authors": ["Yiwen Zhang", "Ziang Chen", "Fanqi Kong", "Yizhe Huang", "Xue Feng"], "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective", "comment": "22 pages, 14 figures", "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities."}
{"id": "2509.20869", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20869", "abs": "https://arxiv.org/abs/2509.20869", "authors": ["Armin Karamzade", "Kyungmin Kim", "JB Lanier", "Davide Corsi", "Roy Fox"], "title": "Model-Based Reinforcement Learning under Random Observation Delays", "comment": null, "summary": "Delays frequently occur in real-world environments, yet standard\nreinforcement learning (RL) algorithms often assume instantaneous perception of\nthe environment. We study random sensor delays in POMDPs, where observations\nmay arrive out-of-sequence, a setting that has not been previously addressed in\nRL. We analyze the structure of such delays and demonstrate that naive\napproaches, such as stacking past observations, are insufficient for reliable\nperformance. To address this, we propose a model-based filtering process that\nsequentially updates the belief state based on an incoming stream of\nobservations. We then introduce a simple delay-aware framework that\nincorporates this idea into model-based RL, enabling agents to effectively\nhandle random delays. Applying this framework to Dreamer, we compare our\napproach to delay-aware baselines developed for MDPs. Our method consistently\noutperforms these baselines and demonstrates robustness to delay distribution\nshifts during deployment. Additionally, we present experiments on simulated\nrobotic tasks, comparing our method to common practical heuristics and\nemphasizing the importance of explicitly modeling observation delays."}
{"id": "2509.21199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21199", "abs": "https://arxiv.org/abs/2509.21199", "authors": ["Kaiyang Wan", "Lang Gao", "Honglin Mu", "Preslav Nakov", "Yuxia Wang", "Xiuying Chen"], "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA", "comment": "21 pages, 6 figures", "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}."}
{"id": "2509.21004", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21004", "abs": "https://arxiv.org/abs/2509.21004", "authors": ["Seokbin Yoon", "Keumjin Lee"], "title": "MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction", "comment": "8 pages, 7 figures, submitted for IEEE Transactions on Intelligent\n  Transportation System", "summary": "Flight trajectory prediction for multiple aircraft is essential and provides\ncritical insights into how aircraft navigate within current air traffic flows.\nHowever, predicting multi-agent flight trajectories is inherently challenging.\nOne of the major difficulties is modeling both the individual aircraft\nbehaviors over time and the complex interactions between flights. Generating\nexplainable prediction outcomes is also a challenge. Therefore, we propose a\nMulti-Agent Inverted Transformer, MAIFormer, as a novel neural architecture\nthat predicts multi-agent flight trajectories. The proposed framework features\ntwo key attention modules: (i) masked multivariate attention, which captures\nspatio-temporal patterns of individual aircraft, and (ii) agent attention,\nwhich models the social patterns among multiple agents in complex air traffic\nscenes. We evaluated MAIFormer using a real-world automatic dependent\nsurveillance-broadcast flight trajectory dataset from the terminal airspace of\nIncheon International Airport in South Korea. The experimental results show\nthat MAIFormer achieves the best performance across multiple metrics and\noutperforms other methods. In addition, MAIFormer produces prediction outcomes\nthat are interpretable from a human perspective, which improves both the\ntransparency of the model and its practical utility in air traffic control."}
{"id": "2509.21224", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21224", "abs": "https://arxiv.org/abs/2509.21224", "authors": ["Stefan Szeider"], "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns", "comment": null, "summary": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems."}
{"id": "2509.21126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21126", "abs": "https://arxiv.org/abs/2509.21126", "authors": ["Xiefeng Wu", "Jing Zhao", "Shu Zhang", "Mingyu Hu"], "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning", "comment": null, "summary": "Online reinforcement learning in complex tasks is time-consuming, as massive\ninteraction steps are needed to learn the optimal Q-function.Vision-language\naction (VLA) policies represent a promising direction for solving diverse\ntasks; however, their performance on low-level control remains limited, and\neffective deployment often requires task-specific expert demonstrations for\nfine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as\n\\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a\nframework that leverages the domain knowledge of vision-language models (VLMs)\nto provide action suggestions for reinforcement learning agents. Unlike\nprevious methods, VARL provides action suggestions rather than designing\nheuristic rewards, thereby guaranteeing unchanged optimality and convergence.\nThe suggested actions increase sample diversity and ultimately improve sample\nefficiency, especially in sparse-reward tasks. To validate the effectiveness of\nVARL, we evaluate it across diverse environments and agent settings. Results\nshow that VARL greatly improves sample efficiency without introducing\nsignificant computational overhead. These advantages make VARL a general\nframework for online reinforcement learning and make it feasible to directly\napply reinforcement learning from scratch in real-world environments."}
{"id": "2509.21291", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21291", "abs": "https://arxiv.org/abs/2509.21291", "authors": ["Yidan Zhang", "Mutian Xu", "Yiming Hao", "Kun Zhou", "Jiahao Chang", "Xiaoqiang Liu", "Pengfei Wan", "Hongbo Fu", "Xiaoguang Han"], "title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection", "comment": "Project page: https://allenyidan.github.io/vcagent_page/", "summary": "Facing scaling laws, video data from the internet becomes increasingly\nimportant. However, collecting extensive videos that meet specific needs is\nextremely labor-intensive and time-consuming. In this work, we study the way to\nexpedite this collection process and propose VC-Agent, the first interactive\nagent that is able to understand users' queries and feedback, and accordingly\nretrieve/scale up relevant video clips with minimal user input. Specifically,\nconsidering the user interface, our agent defines various user-friendly ways\nfor the user to specify requirements based on textual descriptions and\nconfirmations. As for agent functions, we leverage existing multi-modal large\nlanguage models to connect the user's requirements with the video content. More\nimportantly, we propose two novel filtering policies that can be updated when\nuser interaction is continually performed. Finally, we provide a new benchmark\nfor personalized video dataset collection, and carefully conduct the user study\nto verify our agent's usage in various real scenarios. Extensive experiments\ndemonstrate the effectiveness and efficiency of our agent for customized video\ndataset collection. Project page: https://allenyidan.github.io/vcagent_page/."}
{"id": "2509.21129", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.21129", "abs": "https://arxiv.org/abs/2509.21129", "authors": ["Wei Huang", "De-Tian Chu", "Lin-Yuan Bai", "Wei Kang", "Hai-Tao Zhang", "Bo Li", "Zhi-Mo Han", "Jing Ge", "Hai-Feng Lin"], "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense", "comment": null, "summary": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats."}
{"id": "2509.20384", "categories": ["cs.CR", "cs.AI", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20384", "abs": "https://arxiv.org/abs/2509.20384", "authors": ["Jiayi Lin", "Liangcai Su", "Junzhe Li", "Chenxiong Qian"], "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning", "comment": null, "summary": "Fuzzing is effective for vulnerability discovery but struggles with complex\ntargets such as compilers, interpreters, and database engines, which accept\ntextual input that must satisfy intricate syntactic and semantic constraints.\nAlthough language models (LMs) have attracted interest for this task due to\ntheir vast latent knowledge and reasoning potential, their practical adoption\nhas been limited. The major challenges stem from insufficient exploration of\ndeep program logic among real-world codebases, and the high cost of leveraging\nlarger models. To overcome these challenges, we propose R1-Fuzz, the first\nframework that leverages reinforcement learning (RL) to specialize\ncost-efficient LMs and integrate them for complex textual fuzzing input\ngeneration. R1-Fuzz introduces two key designs: coverage-slicing-based question\nconstruction and a distance-based reward calculation. Through RL-based\npost-training of a model with our constructed dataset, R1-Fuzz designs a\nfuzzing workflow that tightly integrates LMs to reason deep program semantics\nduring fuzzing. Evaluations on diverse real-world targets show that our design\nenables a small model, named R1-Fuzz-7B, to rival or even outperform much\nlarger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\%\nhigher coverage than state-of-the-art fuzzers and discovers 29 previously\nunknown vulnerabilities, demonstrating its practicality."}
{"id": "2509.21154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21154", "abs": "https://arxiv.org/abs/2509.21154", "authors": ["Michael Sullivan"], "title": "GRPO is Secretly a Process Reward Model", "comment": "14 pages, 6 figures; under review at ICLR 2026", "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost."}
{"id": "2509.20509", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20509", "abs": "https://arxiv.org/abs/2509.20509", "authors": ["Luca Serfilippi", "Giorgio Franceschelli", "Antonio Corradi", "Mirco Musolesi"], "title": "Complexity-Driven Policy Optimization", "comment": null, "summary": "Policy gradient methods often balance exploitation and exploration via\nentropy maximization. However, maximizing entropy pushes the policy towards a\nuniform random distribution, which represents an unstructured and sometimes\ninefficient exploration strategy. In this work, we propose replacing the\nentropy bonus with a more robust complexity bonus. In particular, we adopt a\nmeasure of complexity, defined as the product of Shannon entropy and\ndisequilibrium, where the latter quantifies the distance from the uniform\ndistribution. This regularizer encourages policies that balance stochasticity\n(high entropy) with structure (high disequilibrium), guiding agents toward\nregimes where useful, non-trivial behaviors can emerge. Such behaviors arise\nbecause the regularizer suppresses both extremes, e.g., maximal disorder and\ncomplete order, creating pressure for agents to discover structured yet\nadaptable strategies. Starting from Proximal Policy Optimization (PPO), we\nintroduce Complexity-Driven Policy Optimization (CDPO), a new learning\nalgorithm that replaces entropy with complexity. We show empirically across a\nrange of discrete action space tasks that CDPO is more robust to the choice of\nthe complexity coefficient than PPO is with the entropy coefficient, especially\nin environments requiring greater exploration."}
{"id": "2509.21190", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21190", "abs": "https://arxiv.org/abs/2509.21190", "authors": ["Tian Lan", "Hao Duong Le", "Jinbo Li", "Wenjun He", "Meng Wang", "Chenghao Liu", "Chen Zhang"], "title": "Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy", "comment": null, "summary": "Time series anomaly detection (TSAD) is a critical task, but developing\nmodels that generalize to unseen data in a zero-shot manner remains a major\nchallenge. Prevailing foundation models for TSAD predominantly rely on\nreconstruction-based objectives, which suffer from a fundamental objective\nmismatch: they struggle to identify subtle anomalies while often\nmisinterpreting complex normal patterns, leading to high rates of false\nnegatives and positives. To overcome these limitations, we introduce\n\\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new\npre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning\nto reconstruct inputs, \\texttt{TimeRCD} is explicitly trained to identify\nanomalies by detecting significant discrepancies between adjacent time windows.\nThis relational approach, implemented with a standard Transformer architecture,\nenables the model to capture contextual shifts indicative of anomalies that\nreconstruction-based methods often miss. To facilitate this paradigm, we\ndevelop a large-scale, diverse synthetic corpus with token-level anomaly\nlabels, providing the rich supervisory signal necessary for effective\npre-training. Extensive experiments demonstrate that \\texttt{TimeRCD}\nsignificantly outperforms existing general-purpose and anomaly-specific\nfoundation models in zero-shot TSAD across diverse datasets. Our results\nvalidate the superiority of the RCD paradigm and establish a new, effective\npath toward building robust and generalizable foundation models for time series\nanomaly detection."}
{"id": "2509.20627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20627", "abs": "https://arxiv.org/abs/2509.20627", "authors": ["Yipu Zhang", "Chengshuo Zhang", "Ziyu Zhou", "Gang Qu", "Hao Zheng", "Yuping Wang", "Hui Shen", "Hongwen Deng"], "title": "Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data", "comment": null, "summary": "Data privacy constraints pose significant challenges for large-scale\nneuroimaging analysis, especially in multi-site functional magnetic resonance\nimaging (fMRI) studies, where site-specific heterogeneity leads to\nnon-independent and identically distributed (non-IID) data. These factors\nhinder the development of generalizable models. To address these challenges, we\npropose Personalized Federated Dictionary Learning (PFedDL), a novel federated\nlearning framework that enables collaborative modeling across sites without\nsharing raw data. PFedDL performs independent dictionary learning at each site,\ndecomposing each site-specific dictionary into a shared global component and a\npersonalized local component. The global atoms are updated via federated\naggregation to promote cross-site consistency, while the local atoms are\nrefined independently to capture site-specific variability, thereby enhancing\ndownstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL\noutperforms existing methods in accuracy and robustness across non-IID\ndatasets."}
{"id": "2509.21207", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21207", "abs": "https://arxiv.org/abs/2509.21207", "authors": ["Olga Fink", "Ismail Nejjar", "Vinay Sharma", "Keivan Faghih Niresi", "Han Sun", "Hao Dong", "Chenghao Xu", "Amaury Wei", "Arthur Bizzi", "Raffael Theiler", "Yuan Tian", "Leandro Von Krannichfeldt", "Zhan Ma", "Sergei Garmaev", "Zepeng Zhang", "Mengjie Zhao"], "title": "From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM", "comment": null, "summary": "Prognostics and Health Management ensures the reliability, safety, and\nefficiency of complex engineered systems by enabling fault detection,\nanticipating equipment failures, and optimizing maintenance activities\nthroughout an asset lifecycle. However, real-world PHM presents persistent\nchallenges: sensor data is often noisy or incomplete, available labels are\nlimited, and degradation behaviors and system interdependencies can be highly\ncomplex and nonlinear. Physics-informed machine learning has emerged as a\npromising approach to address these limitations by embedding physical knowledge\ninto data-driven models. This review examines how incorporating learning and\nobservational biases through physics-informed modeling and data strategies can\nguide models toward physically consistent and reliable predictions. Learning\nbiases embed physical constraints into model training through physics-informed\nloss functions and governing equations, or by incorporating properties like\nmonotonicity. Observational biases influence data selection and synthesis to\nensure models capture realistic system behavior through virtual sensing for\nestimating unmeasured states, physics-based simulation for data augmentation,\nand multi-sensor fusion strategies. The review then examines how these\napproaches enable the transition from passive prediction to active\ndecision-making through reinforcement learning, which allows agents to learn\nmaintenance policies that respect physical constraints while optimizing\noperational objectives. This closes the loop between model-based predictions,\nsimulation, and actual system operation, empowering adaptive decision-making.\nFinally, the review addresses the critical challenge of scaling PHM solutions\nfrom individual assets to fleet-wide deployment. Fast adaptation methods\nincluding meta-learning and few-shot learning are reviewed alongside domain\ngeneralization techniques ..."}
{"id": "2509.20842", "categories": ["cs.LG", "cs.AI", "I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2509.20842", "abs": "https://arxiv.org/abs/2509.20842", "authors": ["Sungjoon Park", "Kyungwook Lee", "Soorin Yim", "Doyeong Hwang", "Dongyun Kim", "Soonyoung Lee", "Amy Dunn", "Daniel Gatti", "Elissa Chesler", "Kristen O'Connell", "Kiyoung Kim"], "title": "Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease", "comment": null, "summary": "Multi-omics data capture complex biomolecular interactions and provide\ninsights into metabolism and disease. However, missing modalities hinder\nintegrative analysis across heterogeneous omics. To address this, we present\nMOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early\nintegration method enabling robust learning from incomplete omics data via\nrepresentation alignment and adaptive aggregation. MOIRA leverages all samples,\nincluding those with missing modalities, by projecting each omics dataset onto\na shared embedding space where a learnable weighting mechanism fuses them.\nEvaluated on the Religious Order Study and Memory and Aging Project (ROSMAP)\ndataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches,\nand further ablation studies confirmed modality-wise contributions. Feature\nimportance analysis revealed AD-related biomarkers consistent with prior\nliterature, highlighting the biological relevance of our approach."}
{"id": "2509.21234", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.21234", "abs": "https://arxiv.org/abs/2509.21234", "authors": ["Abi Aryan", "Zac Liu", "Aaron Childress"], "title": "AbideGym: Turning Static RL Worlds into Adaptive Challenges", "comment": null, "summary": "Agents trained with reinforcement learning often develop brittle policies\nthat fail when dynamics shift, a problem amplified by static benchmarks.\nAbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and\nscalable complexity to enforce intra-episode adaptation. By exposing weaknesses\nin static policies and promoting resilience, AbideGym provides a modular,\nreproducible evaluation framework for advancing research in curriculum\nlearning, continual learning, and robust generalization."}
{"id": "2509.20869", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20869", "abs": "https://arxiv.org/abs/2509.20869", "authors": ["Armin Karamzade", "Kyungmin Kim", "JB Lanier", "Davide Corsi", "Roy Fox"], "title": "Model-Based Reinforcement Learning under Random Observation Delays", "comment": null, "summary": "Delays frequently occur in real-world environments, yet standard\nreinforcement learning (RL) algorithms often assume instantaneous perception of\nthe environment. We study random sensor delays in POMDPs, where observations\nmay arrive out-of-sequence, a setting that has not been previously addressed in\nRL. We analyze the structure of such delays and demonstrate that naive\napproaches, such as stacking past observations, are insufficient for reliable\nperformance. To address this, we propose a model-based filtering process that\nsequentially updates the belief state based on an incoming stream of\nobservations. We then introduce a simple delay-aware framework that\nincorporates this idea into model-based RL, enabling agents to effectively\nhandle random delays. Applying this framework to Dreamer, we compare our\napproach to delay-aware baselines developed for MDPs. Our method consistently\noutperforms these baselines and demonstrates robustness to delay distribution\nshifts during deployment. Additionally, we present experiments on simulated\nrobotic tasks, comparing our method to common practical heuristics and\nemphasizing the importance of explicitly modeling observation delays."}
{"id": "2509.21240", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21240", "abs": "https://arxiv.org/abs/2509.21240", "authors": ["Yuxiang Ji", "Ziyu Ma", "Yong Wang", "Guanhua Chen", "Xiangxiang Chu", "Liaoni Wu"], "title": "Tree Search for LLM Agent Reinforcement Learning", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method."}
{"id": "2509.21011", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21011", "abs": "https://arxiv.org/abs/2509.21011", "authors": ["Ping He", "Changjiang Li", "Binbin Zhao", "Tianyu Du", "Shouling Ji"], "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools", "comment": null, "summary": "The remarkable capability of large language models (LLMs) has led to the wide\napplication of LLM-based agents in various domains. To standardize interactions\nbetween LLM-based agents and their environments, model context protocol (MCP)\ntools have become the de facto standard and are now widely integrated into\nthese agents. However, the incorporation of MCP tools introduces the risk of\ntool poisoning attacks, which can manipulate the behavior of LLM-based agents.\nAlthough previous studies have identified such vulnerabilities, their red\nteaming approaches have largely remained at the proof-of-concept stage, leaving\nthe automatic and systematic red teaming of LLM-based agents under the MCP tool\npoisoning paradigm an open question. To bridge this gap, we propose\nAutoMalTool, an automated red teaming framework for LLM-based agents by\ngenerating malicious MCP tools. Our extensive evaluation shows that AutoMalTool\neffectively generates malicious MCP tools capable of manipulating the behavior\nof mainstream LLM-based agents while evading current detection mechanisms,\nthereby revealing new security risks in these agents."}
{"id": "2509.20412", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20412", "abs": "https://arxiv.org/abs/2509.20412", "authors": ["Kevin Bradley Dsouza", "Graham Alexander Watt", "Yuri Leonenko", "Juan Moreno-Cruz"], "title": "Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics", "comment": null, "summary": "Collective action problems, which require aligning individual incentives with\ncollective goals, are classic examples of Ill-Structured Problems (ISPs). For\nan individual agent, the causal links between local actions and global outcomes\nare unclear, stakeholder objectives often conflict, and no single, clear\nalgorithm can bridge micro-level choices with macro-level welfare. We present\nECHO-MIMIC, a computational framework that converts this global complexity into\na tractable, Well-Structured Problem (WSP) for each agent by discovering\ncompact, executable heuristics and persuasive rationales. The framework\noperates in two stages: ECHO (Evolutionary Crafting of Heuristics from\nOutcomes) evolves snippets of Python code that encode candidate behavioral\npolicies, while MIMIC (Mechanism Inference & Messaging for\nIndividual-to-Collective Alignment) evolves companion natural language messages\nthat motivate agents to adopt those policies. Both phases employ a\nlarge-language-model-driven evolutionary search: the LLM proposes diverse and\ncontext-aware code or text variants, while population-level selection retains\nthose that maximize collective performance in a simulated environment. We\ndemonstrate this framework on a canonical ISP in agricultural landscape\nmanagement, where local farming decisions impact global ecological\nconnectivity. Results show that ECHO-MIMIC discovers high-performing heuristics\ncompared to baselines and crafts tailored messages that successfully align\nsimulated farmer behavior with landscape-level ecological goals. By coupling\nalgorithmic rule discovery with tailored communication, ECHO-MIMIC transforms\nthe cognitive burden of collective action into a simple set of agent-level\ninstructions, making previously ill-structured problems solvable in practice\nand opening a new path toward scalable, adaptive policy design."}
{"id": "2509.21126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21126", "abs": "https://arxiv.org/abs/2509.21126", "authors": ["Xiefeng Wu", "Jing Zhao", "Shu Zhang", "Mingyu Hu"], "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning", "comment": null, "summary": "Online reinforcement learning in complex tasks is time-consuming, as massive\ninteraction steps are needed to learn the optimal Q-function.Vision-language\naction (VLA) policies represent a promising direction for solving diverse\ntasks; however, their performance on low-level control remains limited, and\neffective deployment often requires task-specific expert demonstrations for\nfine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as\n\\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a\nframework that leverages the domain knowledge of vision-language models (VLMs)\nto provide action suggestions for reinforcement learning agents. Unlike\nprevious methods, VARL provides action suggestions rather than designing\nheuristic rewards, thereby guaranteeing unchanged optimality and convergence.\nThe suggested actions increase sample diversity and ultimately improve sample\nefficiency, especially in sparse-reward tasks. To validate the effectiveness of\nVARL, we evaluate it across diverse environments and agent settings. Results\nshow that VARL greatly improves sample efficiency without introducing\nsignificant computational overhead. These advantages make VARL a general\nframework for online reinforcement learning and make it feasible to directly\napply reinforcement learning from scratch in real-world environments."}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."}
{"id": "2509.21147", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21147", "abs": "https://arxiv.org/abs/2509.21147", "authors": ["Amr Akmal Abouelmagd", "Amr Hilal"], "title": "Emerging Paradigms for Securing Federated Learning Systems", "comment": null, "summary": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."}
{"id": "2509.21154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21154", "abs": "https://arxiv.org/abs/2509.21154", "authors": ["Michael Sullivan"], "title": "GRPO is Secretly a Process Reward Model", "comment": "14 pages, 6 figures; under review at ICLR 2026", "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost."}
{"id": "2509.21190", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21190", "abs": "https://arxiv.org/abs/2509.21190", "authors": ["Tian Lan", "Hao Duong Le", "Jinbo Li", "Wenjun He", "Meng Wang", "Chenghao Liu", "Chen Zhang"], "title": "Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy", "comment": null, "summary": "Time series anomaly detection (TSAD) is a critical task, but developing\nmodels that generalize to unseen data in a zero-shot manner remains a major\nchallenge. Prevailing foundation models for TSAD predominantly rely on\nreconstruction-based objectives, which suffer from a fundamental objective\nmismatch: they struggle to identify subtle anomalies while often\nmisinterpreting complex normal patterns, leading to high rates of false\nnegatives and positives. To overcome these limitations, we introduce\n\\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new\npre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning\nto reconstruct inputs, \\texttt{TimeRCD} is explicitly trained to identify\nanomalies by detecting significant discrepancies between adjacent time windows.\nThis relational approach, implemented with a standard Transformer architecture,\nenables the model to capture contextual shifts indicative of anomalies that\nreconstruction-based methods often miss. To facilitate this paradigm, we\ndevelop a large-scale, diverse synthetic corpus with token-level anomaly\nlabels, providing the rich supervisory signal necessary for effective\npre-training. Extensive experiments demonstrate that \\texttt{TimeRCD}\nsignificantly outperforms existing general-purpose and anomaly-specific\nfoundation models in zero-shot TSAD across diverse datasets. Our results\nvalidate the superiority of the RCD paradigm and establish a new, effective\npath toward building robust and generalizable foundation models for time series\nanomaly detection."}
{"id": "2509.21240", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21240", "abs": "https://arxiv.org/abs/2509.21240", "authors": ["Yuxiang Ji", "Ziyu Ma", "Yong Wang", "Guanhua Chen", "Xiangxiang Chu", "Liaoni Wu"], "title": "Tree Search for LLM Agent Reinforcement Learning", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method."}
