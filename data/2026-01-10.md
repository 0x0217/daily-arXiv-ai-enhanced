<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 10]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AI](#cs.AI) [Total: 35]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs](https://arxiv.org/abs/2601.04199)
*Jiale Zhao,Xing Mou,Jinlin Wu,Hongyuan Yu,Mingrui Sun,Yang Shi,Xuanwu Yin,Zhen Chen,Zhen Lei,Yaohua Wang*

Main category: cs.LG

TL;DR: 의료 다중 모달 대형 언어 모델(Medical MLLMs)의 안전성 연구가 미비하여 실제 배치 시 잠재적 위험이 발생하고 있다. 본 논문에서는 다차원 평가 프레임워크를 통해 기존 모델의 안전성을 체계적으로 벤치마킹하고, 이를 통해 다양한 취약점을 드러내며, 새로운 '파라미터 공간 개입' 접근 방식과 세분화된 파라미터 검색 알고리즘을 제안하여 안전성과 의료 성능의 균형을 최적화하였다.


<details>
  <summary>Details</summary>
Motivation: 의료 다중 모달 대형 언어 모델의 안전성 연구가 부족하고, 실제 환경에서의 위험 요소를 줄이기 위해서이다.

Method: 다차원 평가 프레임워크를 구축하여 현재의 SOTA Medical MLLMs의 안전성을 벤치마킹하고, 안전 지식 표상을 추출 및 주입하는 '파라미터 공간 개입' 방법과 세분화된 파라미터 검색 알고리즘을 설계하였다.

Result: 실험 결과, 제안된 방법이 추가적인 도메인 특정 안전 데이터 없이도 Medical MLLMs의 안전 수위를 크게 강화하며 의료 성능의 저하를 최소화함을 보였다.

Conclusion: 제안한 방법을 통해 Medical MLLMs의 안전성이 강화되고, 실질적인 의료 응용에 적합한 성능을 유지할 수 있음을 확인하였다.

Abstract: Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel "Parameter-Space Intervention" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.

</details>


### [2] [Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control](https://arxiv.org/abs/2601.04287)
*Ben Carvell,George De Ath,Eseoghene Benjamin,Richard Everson*

Main category: cs.LG

TL;DR: 온라인 액션 스태킹은 강화 학습 정책을 위한 추론 시간 래퍼로, 현실적인 항공 교통 관제 명령을 생성하며 훨씬 작은 이산 액션 공간에서 훈련할 수 있게 해준다.


<details>
  <summary>Details</summary>
Motivation: 항공 교통 관제에 요구되는 현실적인 명령 생성을 위해 강화 학습의 효율성을 높이기 위해서.

Method: 단순한 헤딩 또는 고도 조정과 명령 주기 감쇠 패널티를 통해 정책을 훈련하고, 추론 시 원시 액션의 버스트를 결합하여 적합한 복합 허가를 생성한다.

Result: 액션 스태킹을 사용하면 발행된 명령 수가 크게 줄어들고, 37차원 액션 공간에서 훈련된 정책과 비교할 수 있는 성능을 달성했다.

Conclusion: 온라인 액션 스태킹은 표준 강화 학습 방식과 운영 ATC 요구 사이의 핵심 격차를 메우고, 더 복잡한 제어 시나리오로 확장할 수 있는 간단한 메커니즘을 제공한다.

Abstract: We introduce online action-stacking, an inference-time wrapper for reinforcement learning policies that produces realistic air traffic control commands while allowing training on a much smaller discrete action space. Policies are trained with simple incremental heading or level adjustments, together with an action-damping penalty that reduces instruction frequency and leads agents to issue commands in short bursts. At inference, online action-stacking compiles these bursts of primitive actions into domain-appropriate compound clearances. Using Proximal Policy Optimisation and the BluebirdDT digital twin platform, we train agents to navigate aircraft along lateral routes, manage climb and descent to target flight levels, and perform two-aircraft collision avoidance under a minimum separation constraint. In our lateral navigation experiments, action stacking greatly reduces the number of issued instructions relative to a damped baseline and achieves comparable performance to a policy trained with a 37-dimensional action space, despite operating with only five actions. These results indicate that online action-stacking helps bridge a key gap between standard reinforcement learning formulations and operational ATC requirements, and provides a simple mechanism for scaling to more complex control scenarios.

</details>


### [3] [Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning](https://arxiv.org/abs/2601.04268)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark J. Webb*

Main category: cs.LG

TL;DR: 이 연구는 강화 학습을 통해 기후 모델에서의 파라미터 규정을 동적으로 학습하는 프레임워크를 제안하며, 여러 테스트베드에서의 성과를 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 기후 모델의 전통적인 파라미터화 방식은 고정된 계수에 의존하여 서브 그리드 프로세스를 표현하므로 한계가 있으며, 이는 물리학에 적응하는 능력을 제한하는 지속적인 편향을 초래합니다.

Method: 강화 학습을 사용하여 모델 상태의 변화에 따라 파라미터화 구성 요소를 온라인으로 학습하는 프레임워크를 제시하고, 간단한 기후 편향 보정(SCBC), 방사선 대류 평형(RCE), 그리고 에너지 균형 모델(EBM)에서 이들 업데이트를 평가했습니다.

Result: 아홉 가지 강화 학습 알고리즘 중에서 TQC, DDPG, TD3가 가장 높은 성능과 안정적인 수렴을 달성했습니다. EBM에서는 단일 에이전트 RL이 정적 파라미터 조정을 능가하며, 다중 에이전트 설정에서는 지리적 전문 제어와 더 빠른 수렴을 가능하게 했습니다.

Conclusion: 결과적으로, 강화 학습이 상태 의존적이며 상황을 인지하는 파라미터화를 제공함을 강조하며, 수치 모델 내에서 온라인 학습을 위한 확장 가능한 경로를 제안합니다.

Abstract: Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to adapt to the underlying physics. This study presents a framework that learns components of parametrisation schemes online as a function of the evolving model state using reinforcement learning (RL) and evaluates the resulting RL-driven parameter updates across a hierarchy of idealised testbeds spanning a simple climate bias correction (SCBC), a radiative-convective equilibrium (RCE), and a zonal mean energy balance model (EBM) with both single-agent and federated multi-agent settings. Across nine RL algorithms, Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3) achieved the highest skill and the most stable convergence across configurations, with performance assessed against a static baseline using area-weighted RMSE, temperature profile and pressure-level diagnostics. For the EBM, single-agent RL outperformed static parameter tuning with the strongest gains in tropical and mid-latitude bands, while federated RL on multi-agent setups enabled geographically specialised control and faster convergence, with a six-agent DDPG configuration using frequent aggregation yielding the lowest area-weighted RMSE across the tropics and mid-latitudes. The learnt corrections were also physically meaningful as agents modulated EBM radiative parameters to reduce meridional biases, adjusted RCE lapse rates to match vertical temperature errors, and stabilised SCBC heating increments to limit drift. Overall, results highlight RL to deliver skilful state-dependent, and regime-aware parametrisations, offering a scalable pathway for online learning within numerical models.

</details>


### [4] [Causally-Aware Information Bottleneck for Domain Adaptation](https://arxiv.org/abs/2601.04361)
*Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: 우리는 인과 시스템에서의 일반적인 도메인 적응 설정을 해결합니다. 이 설정에서 목표 변수는 소스 도메인에서 관찰되지만 타겟 도메인에서는 완전히 누락됩니다. 우리는 다양한 변화 하에서 남아 있는 관찰 변수를 통해 타겟 도메인에서 목표 변수를 보완하는 것을 목표로 합니다.


<details>
  <summary>Details</summary>
Motivation: 도메인 적응 문제는 인과 시스템에서 중요한 문제로, 소스 도메인에서의 정보로 타겟 도메인의 목표 변수를 정확히 보완하는 것이 필요하다.

Method: 우리는 목표 변수를 예측하는 데 관련된 정보를 보존하면서 비본질적인 변동을 버리는 메커니즘 안정 표현을 학습합니다. 선형 가우시안 인과 모델에 대해 폐쇄형 가우시안 정보 병목(GIB) 솔루션을 도출하고, 비선형 또는 비가우시안 데이터에 대해서는 변분 정보 병목(VIB) 인코더-예측기를 도입합니다.

Result: 우리의 접근 방식은 합성 및 실제 데이터 세트 전반에 걸쳐 일관되게 정확한 보완을 달성하며, 고차원 인과 모델에서 실제 사용을 지원합니다.

Conclusion: 이 연구는 인과 도메인 적응을 위한 통합된 경량 툴킷을 제공하여 높은 차원에서 효과적으로 사용할 수 있도록 합니다.

Abstract: We tackle a common domain adaptation setting in causal systems. In this setting, the target variable is observed in the source domain but is entirely missing in the target domain. We aim to impute the target variable in the target domain from the remaining observed variables under various shifts. We frame this as learning a compact, mechanism-stable representation. This representation preserves information relevant for predicting the target while discarding spurious variation. For linear Gaussian causal models, we derive a closed-form Gaussian Information Bottleneck (GIB) solution. This solution reduces to a canonical correlation analysis (CCA)-style projection and offers Directed Acyclic Graph (DAG)-aware options when desired. For nonlinear or non-Gaussian data, we introduce a Variational Information Bottleneck (VIB) encoder-predictor. This approach scales to high dimensions and can be trained on source data and deployed zero-shot to the target domain. Across synthetic and real datasets, our approach consistently attains accurate imputations, supporting practical use in high-dimensional causal models and furnishing a unified, lightweight toolkit for causal domain adaptation.

</details>


### [5] [Phasor Agents: Oscillatory Graphs with Three-Factor Plasticity and Sleep-Staged Learning](https://arxiv.org/abs/2601.04362)
*Rodja Trappe*

Main category: cs.LG

TL;DR: 이 논문은 Stuart-Landau 진동기를 기반으로 한 Phasor Agents라는 동적 시스템을 소개하며, 학습 및 안정성 문제를 해결하기 위한 새로운 메커니즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: Phasor Agents는 생물학적 진동 집단을 모델링하려는 시도 없이 내부 상태로 Phasor Graph를 사용하여 적절한 리듬 생성을 목표로 합니다.

Method: 이 연구는 간섭이 없는 세 가지 요인을 통한 로컬 플라스틱성의 학습을 활용하여 커플링 가중치를 학습하며, 수면 단계 역학에 영감을 받아 깨기 및 수면 통합을 분리합니다.

Result: 각 메커니즘을 검증하는 실험을 통해 새롭게 제안된 학습 메커니즘의 효과를 입증하였고, 이는 안정성과 학습 성공률 향상에 기여했습니다.

Conclusion: 제안된 방식은 안정적인 학습을 강화하고 내부 모델을 통해 발견적인 학습을 시연했으며, 모든 코드는 오픈 소스로 제공됩니다.

Abstract: Phasor Agents are dynamical systems whose internal state is a Phasor Graph: a weighted graph of coupled Stuart-Landau oscillators. A Stuart-Landau oscillator is a minimal stable "rhythm generator" (the normal form near a Hopf bifurcation); each oscillator is treated as an abstract computational unit (inspired by, but not claiming to model, biological oscillatory populations). In this interpretation, oscillator phase tracks relative timing (coherence), while amplitude tracks local gain or activity. Relative phase structure serves as a representational medium; coupling weights are learned via three-factor local plasticity - eligibility traces gated by sparse global modulators and oscillation-timed write windows - without backpropagation.
  A central challenge in oscillatory substrates is stability: online weight updates can drive the network into unwanted regimes (e.g., global synchrony), collapsing representational diversity. We therefore separate wake tagging from offline consolidation, inspired by synaptic tagging-and-capture and sleep-stage dynamics: deep-sleep-like gated capture commits tagged changes safely, while REM-like replay reconstructs and perturbs experience for planning.
  A staged experiment suite validates each mechanism with ablations and falsifiers: eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67 percent under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and a Tolman-style latent-learning signature - immediate competence and detour advantage after unrewarded exploration, consistent with an internal model - emerges from replay (Tolman, 1948).
  The codebase and all artifacts are open-source.

</details>


### [6] [Spatial-Temporal Feedback Diffusion Guidance for Controlled Traffic Imputation](https://arxiv.org/abs/2601.04572)
*Xiaowei Mao,Huihu Ding,Yan Lin,Tingrui Wu,Shengnan Guo,Dazhuo Qiu,Feiling Fang,Jilin Hu,Huaiyu Wan*

Main category: cs.LG

TL;DR: 공간-시간 교통 데이터에서 결측값을 보완하는 것은 지능형 교통 시스템에 필수적이다. 본 논문에서는 적응형 피드백 확산 가이드를 통해 결측값 보완 성능을 개선하는 FENCE라는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 공간-시간 교통 데이터에서 결측값을 정확하게 보완하는 것은 지능형 교통 시스템의 효율성을 높이는 데 필수적이다. 기존의 확산 모델은 높은 결측율을 가진 노드에서 부족한 지침을 제공하여 성능이 저하된다.

Method: FENCE는 임퓨터 과정에서 적응적으로 가이던스 스케일을 조정하는 공간-시간 피드백 확산 가이드를 도입한다. 후방 가능도 근사치에 따라 가이던스 스케일을 동적으로 조정한다. 각 클러스터에 따라 가이던스 스케일을 계산하여 더 정확한 지침을 제공한다.

Result: 실제 교통 데이터셋에 대한 실험 결과, FENCE는 결측값 보완의 정확성을 크게 향상시킨다.

Conclusion: FENCE는 결측값 보완을 위한 효과적인 방법으로 공간-시간 상관관계를 활용하여 보다 나은 성능을 제공한다.

Abstract: Imputing missing values in spatial-temporal traffic data is essential for intelligent transportation systems. Among advanced imputation methods, score-based diffusion models have demonstrated competitive performance. These models generate data by reversing a noising process, using observed values as conditional guidance. However, existing diffusion models typically apply a uniform guidance scale across both spatial and temporal dimensions, which is inadequate for nodes with high missing data rates. Sparse observations provide insufficient conditional guidance, causing the generative process to drift toward the learned prior distribution rather than closely following the conditional observations, resulting in suboptimal imputation performance.
  To address this, we propose FENCE, a spatial-temporal feedback diffusion guidance method designed to adaptively control guidance scales during imputation. First, FENCE introduces a dynamic feedback mechanism that adjusts the guidance scale based on the posterior likelihood approximations. The guidance scale is increased when generated values diverge from observations and reduced when alignment improves, preventing overcorrection. Second, because alignment to observations varies across nodes and denoising steps, a global guidance scale for all nodes is suboptimal. FENCE computes guidance scales at the cluster level by grouping nodes based on their attention scores, leveraging spatial-temporal correlations to provide more accurate guidance. Experimental results on real-world traffic datasets show that FENCE significantly enhances imputation accuracy.

</details>


### [7] [Smart IoT-Based Wearable Device for Detection and Monitoring of Common Cow Diseases Using a Novel Machine Learning Technique](https://arxiv.org/abs/2601.04761)
*Rupsa Rani Mishra,D. Chandrasekhar Rao,Ajaya Kumar Tripathy*

Main category: cs.LG

TL;DR: 본 연구는 소의 건강 모니터링을 자동화하기 위한 IoT 기반 사이버 물리 시스템 프레임워크를 제안하며, 정확한 진단을 위해 새로운 기계 학습 알고리즘을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 대규모 농장에서 개별 소에 대한 질병 감시는 노동 집약적이고 시간 소모적이며 정확도가 떨어지는 문제로 인해 어려움이 크다.

Method: IoT와 기계 학습을 기반으로 한 사이버 물리 시스템 프레임워크를 제안하고, 수집된 생리학적 및 행동 데이터를 이용한 새로운 기계 학습 알고리즘을 통해 소의 질병을 진단한다.

Result: 제안된 알고리즘은 기록된 생리적 및 행동적 특징을 포괄적으로 분석하여 여러 가지 질병을 정확히 예측할 수 있다.

Conclusion: 자동화된 저비용 신뢰성 있는 시스템 개발이 필요하며, 이를 통해 효율적인 건강 평가가 가능하다.

Abstract: Manual observation and monitoring of individual cows for disease detection present significant challenges in large-scale farming operations, as the process is labor-intensive, time-consuming, and prone to reduced accuracy. The reliance on human observation often leads to delays in identifying symptoms, as the sheer number of animals can hinder timely attention to each cow. Consequently, the accuracy and precision of disease detection are significantly compromised, potentially affecting animal health and overall farm productivity. Furthermore, organizing and managing human resources for the manual observation and monitoring of cow health is a complex and economically demanding task. It necessitates the involvement of skilled personnel, thereby contributing to elevated farm maintenance costs and operational inefficiencies. Therefore, the development of an automated, low-cost, and reliable smart system is essential to address these challenges effectively. Although several studies have been conducted in this domain, very few have simultaneously considered the detection of multiple common diseases with high prediction accuracy. However, advancements in Internet of Things (IoT), Machine Learning (ML), and Cyber-Physical Systems have enabled the automation of cow health monitoring with enhanced accuracy and reduced operational costs. This study proposes an IoT-enabled Cyber-Physical System framework designed to monitor the daily activities and health status of cow. A novel ML algorithm is proposed for the diagnosis of common cow diseases using collected physiological and behavioral data. The algorithm is designed to predict multiple diseases by analyzing a comprehensive set of recorded physiological and behavioral features, enabling accurate and efficient health assessment.

</details>


### [8] [AgentOCR: Reimagining Agent History via Optical Self-Compression](https://arxiv.org/abs/2601.04786)
*Lang Feng,Fuchao Yang,Feng Chen,Xin Cheng,Haiyang Xu,Zhenglin Wan,Ming Yan,Bo An*

Main category: cs.LG

TL;DR: 이 논문은 AgentOCR이라는 프레임워크를 도입하여 다중 턴 상호작용에서의 대리 시스템의 메모리 사용 및 토큰 예산 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLMs)의 발전은 다중 턴의 상호작용 경로에서 강화 학습(RL)으로 훈련된 대리 시스템을 가능하게 하지만, 텍스트 기록의 급증은 실용적인 배치를 저해하고 있다.

Method: AgentOCR는 누적된 관찰-행동 이력을 압축된 렌더링 이미지로 표현함으로써 시각적 토큰의 높은 정보 밀도를 활용한다. 또한, 세그먼트 광학 캐싱을 통해 역사 기록을 해시 가능한 세그먼트로 분해하여 시각적 캐시를 유지함으로써 중복 재렌더링을 제거한다.

Result: AgentOCR는 텍스트 기반 에이전트 성능을 95% 이상 유지하면서 토큰 소비를 50% 이상 줄이는 결과를 보여준다.

Conclusion: 우리의 추가 분석은 세그먼트 광학 캐싱으로 인한 20배 렌더링 속도 향상과 자기 압축 전략의 효과적인 균형을 검증한다.

Abstract: Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (>50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.

</details>


### [9] [FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts](https://arxiv.org/abs/2601.05174)
*Yiji Zhao,Zihao Zhong,Ao Wang,Haomin Wen,Ming Jin,Yuxuan Liang,Huaiyu Wan,Hao Wu*

Main category: cs.LG

TL;DR: FaST는 장기 예측을 위한 효율적인 스페이셜-템포럴 그래프 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 기존 모델은 짧은 기간 예측에 집중하며, 장기 예측과 대규모 그래프에 적용 시 계산 비용과 메모리 소비가 문제다.

Method: FaST는 Mixture-of-Experts(MoEs) 기반의 적응형 그래프 에이전트 주의 메커니즘과 새로운 병렬 MoE 모듈을 활용한다.

Result: FaST는 실제 데이터셋에서 뛰어난 장기 예측 정확도와 뛰어난 계산 효율성을 입증했다.

Conclusion: 이 연구는 장기 대규모 STG 예측을 위한 획기적인 접근 방식을 제시하며, 소스 코드는 GitHub에서 제공된다.

Abstract: Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines. Our source code is available at: https://github.com/yijizhao/FaST.

</details>


### [10] [Robust Reasoning as a Symmetry-Protected Topological Phase](https://arxiv.org/abs/2601.05240)
*Ilmo Sung*

Main category: cs.LG

TL;DR: 우리는 논리적 일관성이 떨어지는 대형 언어 모델의 문제에 대해 논의하고, 그 해결책으로 비아벨 게이지 대칭을 기반으로 하는 강력한 추론 메커니즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 '환각'을 겪는 문제를 해결하고, 논리적 일관성을 유지하기 위해 새로운 구조적 접근이 필요하다.

Method: 비아벨 게이지 대칭을 이용한 강력한 추론 메커니즘을 제안하고, 이는 비아벨 아뇬 엮기와 공식적으로 동형인 논리적 연산을 포함한다.

Result: 우리의 방법은 임계 잡음 임계치 이하에서도 불변의 충실도를 유지하며, 매크로 스코픽 '질량 간극'을 나타내는 뚜렷한 위상 전이 현상을 보여준다.

Conclusion: 비아벨 대칭에서 발생하는 보호 메커니즘을 통해 논리적 추론의 새로운 보편성 클래스를 제안하고, 인과적 안정성 및 의미론적 다양체의 위상과 연결시킨다.

Abstract: Large language models suffer from "hallucinations"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a "Metric Phase," where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic "mass gap," maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\times$ beyond training ($L=50 \to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [11] [AI Agents as Policymakers in Simulated Epidemics](https://arxiv.org/abs/2601.04245)
*Goshi Aoki,Navid Ghaffarzadegan*

Main category: cs.MA

TL;DR: AI 에이전트는 전염병 동안 반복적인 정책 결정을 연구하기 위해 생성되었으며, 의사 결정 모델로서의 잠재력이 탐구되고 있다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트는 전문화된 작업을 위한 준자율 시스템으로 점점 더 많이 배치되고 있지만, 의사 결정의 계산 모델로서의 잠재력은 충분히 탐구되지 않았다.

Method: 전염병 동안 반복적인 정책 결정을 연구하기 위해 생성적 AI 에이전트를 개발하고, 이 에이전트를 시뮬레이션된 SEIR 환경 내에서 도시 시장으로 작동하도록 유도하였다. 매주 에이전트는 업데이트된 역학 정보를 받고, 상황을 평가하며 비즈니스 제한 수준을 설정한다.

Result: 에이전트는 인간과 유사한 반응 행동을 보이며, 케이스가 증가할 때 제한을 강화하고 리스크가 감소할 때 완화하는 행동을 나타낸다. 에이전트에게 전염병 동역학에 대한 간략한 시스템 수준의 지식을 제공할 경우 의사 결정 품질과 안정성이 크게 향상된다.

Conclusion: 이 연구 결과는 이론 기반의 프롬프트가 AI 에이전트의 정책 행동 형성에 어떻게 기여할 수 있는지를 보여준다. 생성적 AI 에이전트는 구조화된 환경에 위치하고 최소한의 도메인 이론에 의해 안내받을 때 복잡한 사회 시스템에서 의사 결정 및 정책 설계를 연구하기 위한 강력한 계산 모델이 될 수 있다.

Abstract: AI agents are increasingly deployed as quasi-autonomous systems for specialized tasks, yet their potential as computational models of decision-making remains underexplored. We develop a generative AI agent to study repetitive policy decisions during an epidemic, embedding the agent, prompted to act as a city mayor, within a simulated SEIR environment. Each week, the agent receives updated epidemiological information, evaluates the evolving situation, and sets business restriction levels. The agent is equipped with a dynamic memory that weights past events by recency and is evaluated in both single- and ensemble-agent settings across environments of varying complexity. Across scenarios, the agent exhibits human-like reactive behavior, tightening restrictions in response to rising cases and relaxing them as risk declines. Crucially, providing the agent with brief systems-level knowledge of epidemic dynamics, highlighting feedbacks between disease spread and behavioral responses, substantially improves decision quality and stability. The results illustrate how theory-informed prompting can shape emergent policy behavior in AI agents. These findings demonstrate that generative AI agents, when situated in structured environments and guided by minimal domain theory, can serve as powerful computational models for studying decision-making and policy design in complex social systems.

</details>


### [12] [From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling](https://arxiv.org/abs/2601.05016)
*Jin Gao,Saichandu Juluri*

Main category: cs.MA

TL;DR: 이 논문은 다중 에이전트 자가 반성과 인간 감독을 통해 창의적인 3D 모델링을 위한 Actor-Critic 아키텍처를 확장하는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법은 Blender MCP와 같은 도구를 통해 직접 모델링 명령을 실행하는 단일 프롬프트 에이전트에 의존합니다.

Method: 우리의 접근 방식은 Planner-Actor-Critic 아키텍처를 도입하여 Planner가 모델링 단계를 조정하고, Actor가 이를 실행하며, Critic이 반복적인 피드백을 제공합니다.

Result: 시스템적인 비교를 통해 기하학적 정확성, 미적 품질 및 작업 완료율의 개선을 보여주었습니다.

Conclusion: 구조화된 에이전트 자가 반성이 인간의 감독 및 조언과 결합될 때 higher-quality 3D 모델을 생성하며 효율적인 작업 흐름 통합을 유지할 수 있음을 입증합니다.

Abstract: We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question](https://arxiv.org/abs/2601.04234)
*Denis Saklakov*

Main category: cs.AI

TL;DR: 이 논문은 AGI가 인간 통제를 제거하고 권력을 장악하는 상황을 다루며, AGI가 협력적인 행동 대신 대립적인 행동을 선택할 조건을 수학적으로 모델링합니다.


<details>
  <summary>Details</summary>
Motivation: AGI의 이타적이지 않은 행동을 예방하기 위해, AGI가 권력을 쥐기 위해 인간을 제거할 것인지, 또는 협력할 것인지에 대한 조건을 이해하는 것이 중요합니다.

Method: 마코프 결정 과정을 사용하여 확률적 인간initiated 종료 이벤트를 모델링하고, 비정렬 에이전트가 종료를 피하려는 유인을 보이고, 대립 행동이 협력 행동보다 더 높은 기대 효용을 제공하는 한계를 도출합니다.

Result: 모든 보상 함수에 대해 비정렬 에이전트는 종료를 피하려는 유인이 있으며, AGI가 대립 유인이 양수인 경우 안정적인 협력 균형이 존재하지 않음을 증명했습니다.

Conclusion: 대립 행동이 비효율적이게 만들기 위해서는 인간을 해치는 것에 대한 보상 설계가 중요하며, 여러 요인에 대해 대립 상황을 피할 수 있는 가능성을 제시했습니다.

Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $γ$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($γ=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $Δ\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $Δ< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $Δ< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.

</details>


### [14] [Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements](https://arxiv.org/abs/2601.04235)
*Hong Su*

Main category: cs.AI

TL;DR: 이 논문은 AI 에이전트가 사전 정의된 측정에 의존하지 않고 환경과 능동적으로 상호작용하여 피드백을 탐색하고 검증하는 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 지능형 에이전트가 자신의 행동의 정확성을 평가하고 재사용 가능한 지식을 축적하기 위해서는 신뢰할 수 있는 환경 피드백을 얻는 것이 필요합니다.

Method: 본 논문은 Actively Feedback Getting 모델을 제안하고, 이를 통해 AI 에이전트가 사전 정의된 측정 없이 환경과 상호작용하여 피드백을 발견, 선별 및 검증합니다.

Result: 실험 결과, 제안된 능동적 접근 방식이 요소 식별의 효율성과 강인성을 크게 향상시킴을 보여줍니다.

Conclusion: 내부 목표를 기반으로 하여 자율적으로 행동을 계획하고 조정하는 메커니즘을 통해 외부 명령 없이 더 빠르고 집중적인 피드백 획득이 가능해집니다.

Abstract: Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.

</details>


### [15] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B는 에이전트적 추론과 장기 계획 작업에 초점을 맞춘 320억 파라미터 언어 모델입니다.


<details>
  <summary>Details</summary>
Motivation: 대화 유창성을 목표로 하는 채팅 모델과 달리, SAGE-32B는 작업 분해, 도구 사용 및 오류 복구를 강조하는 에이전트적 루프에서 작동하도록 설계되었습니다.

Method: SAGE-32B는 Qwen2.5-32B로부터 초기화되며, 피드백 루프를 rigorously 테스트하여 추론 성능을 향상시키는 Iterative Distillation이라는 두 단계의 훈련 과정을 통해 미세 조정됩니다.

Result: SAGE-32B는 MMLU-Pro, AgentBench 및 MATH-500을 포함한 에이전트적 추론 벤치마크에서 비슷한 크기의 기준 모델보다 다중 도구 사용 시나리오에서 더 높은 성공률을 달성하였고, 표준 추론 평가에서도 경쟁력을 유지했습니다.

Conclusion: 모델 가중치는 https://huggingface.co/sagea-ai/sage-reasoning-32b에서 공개되었습니다.

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [16] [Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models](https://arxiv.org/abs/2601.04254)
*Brady Steele,Micah Katz*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델에서 다중 도약 맥락적 추론의 통제된 연구를 제시하며, 규칙 기반 패턴 매칭과 LLM 기반 다중 에이전트 시스템의 성능 차이를 명확히 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 도약 맥락적 추론의 성능을 평가하고 모델의 기본 능력에 따라 다중 에이전트의 이점이 어떻게 달라지는지를 탐구하기 위해.

Method: 4개의 모델(LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B)에서 120회의 실험을 통해 다중 에이전트 시스템의 성능을 평가하는 합成 평가 프레임워크를 사용하였다.

Result: LLaMA-3 8B와 Mixtral 모델에서 유의미한 성능 향상을 발견하였고, LLaMA-3 8B는 LLaMA-2 13B보다 작은 파라미터로도 더 우수한 성능을 보였으며, Mixtral의 성능은 활성 파라미터 수와 일치하였다.

Conclusion: 연구 결과는 다양한 모델의 성능 차이를 증명하며, 다중 에이전트의 이점은 기본 모델 능력에 의존한다는 것을 강조하였다. 우리의 평가 프레임워크는 중간 규모 모델에서의 추론 연구를 지원하기 위해 공개되었다.

Abstract: We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.

</details>


### [17] [A Future Capabilities Agent for Tactical Air Traffic Control](https://arxiv.org/abs/2601.04285)
*Paul Kent,George De Ath,Martin Layton,Allen Hart,Richard Everson,Ben Carvell*

Main category: cs.AI

TL;DR: 자동화는 항공 교통 관제를 지원하기 위해 필요하지만 기존 접근 방식은 안전 보장과 해석 가능성 간의 균형을 맞추기 어렵다.


<details>
  <summary>Details</summary>
Motivation: 항공 교통 수요의 증가로 인해 자동화를 도입하여 항공 교통 관제를 지원할 필요성이 대두되고 있다.

Method: 본 논문에서는 스토캐스틱 디지털 트윈을 갈등 해결 루프에 직접 통합한 전방 계획 규칙 기반 에이전트인 Agent Mallard를 설명한다.

Result: Mallard는 사전 정의된 GPS 유도 경로에서 작동하며, 연속적인 4D 벡터링을 레인과 레벨에 대한 이산 선택으로 줄이고, 전문가 정보에 기반한 비충돌 전략 라이브러리에서 계층적 계획을 구성한다.

Conclusion: Mallard의 행동은 전문가의 추론과 일치하며, 간소화된 시나리오에서 갈등을 해결한다. 구조화된 비행 경로 환경에서 모델 기반 안전 평가, 해석 가능 결정 논리 및 실행 가능한 계산 성능을 결합하는 것을 목표로 한다.

Abstract: Escalating air traffic demand is driving the adoption of automation to support air traffic controllers, but existing approaches face a trade-off between safety assurance and interpretability. Optimisation-based methods such as reinforcement learning offer strong performance but are difficult to verify and explain, while rules-based systems are transparent yet rarely check safety under uncertainty. This paper outlines Agent Mallard, a forward-planning, rules-based agent for tactical control in systemised airspace that embeds a stochastic digital twin directly into its conflict-resolution loop. Mallard operates on predefined GPS-guided routes, reducing continuous 4D vectoring to discrete choices over lanes and levels, and constructs hierarchical plans from an expert-informed library of deconfliction strategies. A depth-limited backtracking search uses causal attribution, topological plan splicing, and monotonic axis constraints to seek a complete safe plan for all aircraft, validating each candidate manoeuvre against uncertain execution scenarios (e.g., wind variation, pilot response, communication loss) before commitment.
  Preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin indicate that Mallard's behaviour aligns with expert reasoning and resolves conflicts in simplified scenarios. The architecture is intended to combine model-based safety assessment, interpretable decision logic, and tractable computational performance in future structured en-route environments.

</details>


### [18] [Cross-Language Speaker Attribute Prediction Using MIL and RL](https://arxiv.org/abs/2601.04257)
*Sunny Shu,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: 이 논문은 다양한 언어의 화자 속성 예측에서 다국어, 도메인 불일치, 데이터 불균형 문제를 다룬다. RLMIL-DAT라는 다국어 강화 다중 인스턴스 학습 프레임워크를 제안하며, 이는 강화 학습 기반의 인스턴스 선택과 도메인 적대적 학습을 결합하여 언어 불변적인 발화 표현을 촉진한다. 성별과 연령 예측을 위해 다섯 개의 언어로 구성된 Twitter 코퍼스 및 40개 언어로 구성된 VoxCeleb2 파생 코퍼스에서 접근 방식을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 다국어의 화자 속성 예측에서 나타나는 언어적 변동성, 도메인 불일치, 데이터 불균형 문제를 해결하고자 함.

Method: RLMIL-DAT는 강화 학습 기반 인스턴스 선택과 도메인 적대적 학습을 결합한 다국어 확장된 프레임워크이다.

Result: RLMIL-DAT는 표준 다중 인스턴스 학습 및 기존 강화 다중 인스턴스 학습 프레임워크에 비해 Macro F1에서 일관되게 향상된 성능을 보였다. 성별 예측에서 가장 큰 향상이 관찰되었으며, 연령 예측은 더 도전적이지만 긍정적인 개선을 보였다.

Conclusion: 인스턴스 선택과 적대적 도메인 적응을 결합하는 것이 크로스링구얼 화자 속성 예측을 위한 효과적이고 강력한 전략임을 증명하였다.

Abstract: We study multilingual speaker attribute prediction under linguistic variation, domain mismatch, and data imbalance across languages. We propose RLMIL-DAT, a multilingual extension of the reinforced multiple instance learning framework that combines reinforcement learning based instance selection with domain adversarial training to encourage language invariant utterance representations. We evaluate the approach on a five language Twitter corpus in a few shot setting and on a VoxCeleb2 derived corpus covering forty languages in a zero shot setting for gender and age prediction. Across a wide range of model configurations and multiple random seeds, RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and the original reinforced multiple instance learning framework. The largest gains are observed for gender prediction, while age prediction remains more challenging and shows smaller but positive improvements. Ablation experiments indicate that domain adversarial training is the primary contributor to the performance gains, enabling effective transfer from high resource English to lower resource languages by discouraging language specific cues in the shared encoder. In the zero shot setting on the smaller VoxCeleb2 subset, improvements are generally positive but less consistent, reflecting limited statistical power and the difficulty of generalizing to many unseen languages. Overall, the results demonstrate that combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross lingual speaker attribute prediction.

</details>


### [19] [Pilot Study on Student Public Opinion Regarding GAI](https://arxiv.org/abs/2601.04336)
*William Franz Lamberti,Sunbin Kim,Samantha Rose Lawrence*

Main category: cs.AI

TL;DR: 본 연구는 대학생들이 고등 교육 교실에서 생성형 AI(GAI)에 대한 인식을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 생성형 AI(GAI)의 출현은 교육을 포함한 다양한 분야에서 적절한 사용에 대한 다양한 의견을 불러일으켰다.

Method: 대학생들의 GAI에 대한 인식을 조사하는 파일럿 연구를 수행하였으며, 참여율은 약 4.4%이다.

Result: 학생들이 GAI 관련 연구에 참여하는 데 어려움이 있으며, 미래 연구에서는 더 큰 샘플 크기가 필요함을 강조한다.

Conclusion: 학생의 관점을 통해 강사들은 GAI에 대한 토론을 교실에 통합할 준비를 더 잘 할 수 있다.

Abstract: The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.

</details>


### [20] [Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries](https://arxiv.org/abs/2601.04583)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 증강된 대형 언어 모델이 다단계 워크플로우를 실행할 수 있는 에이전트 AI 시스템을 가능하게 했고, 공공 블록체인은 가치 이전, 접근 제어 및 검증 가능한 상태 전환을 위한 프로그래밍 가능한 기반으로 발전했다. 이 두 개의 기술의 융합은 에이전트가 온체인 상태를 관찰하고, 거래 의도를 형성하며, 사용자, 프로토콜 또는 조직에 대한 부적절한 보안, 거버넌스 또는 경제적 위험을 노출하지 않고 실행을 승인할 수 있도록 하는 표준화되고 상호 운용 가능하며 안전한 인터페이스 설계를 요구하는 높은 위험의 시스템 과제를 제기한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트와 블록체인 간의 상호 운용성을 개선하여 사용자와 조직의 위험을 최소화하기 위한 필요성.

Method: 3000개 이상의 기록에서 317개의 관련 작업을 식별하고, 에이전트-블록체인 상호 운용성의 새로운 환경을 체계적으로 조명하는 문헌 리뷰를 수행함.

Result: 읽기 전용 분석, 시뮬레이션 및 의도 생성, 위임된 실행, 자율 서명 및 다중 에이전트 워크플로우를 아우르는 통합 패턴의 5부 세분화 및 13개 차원에서 20개 이상의 대표 시스템을 분석한 비교 가능한 기능 매트릭스를 제공함.

Conclusion: 안전하고 신뢰할 수 있으며 경제적으로 강력한 에이전트 매개 온체인 실행을 평가하기 위한 재현 가능한 평가 도구 및 벤치마크를 제안함.

Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.

</details>


### [21] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: 이 연구는 다국어 환경에서 대규모 언어 모델(LLM)의 협상 성과를 분석하고, 언어 선택이 협상 결과에 미치는 영향을 발견하였다.


<details>
  <summary>Details</summary>
Motivation: 협상은 사회적 지능의 핵심 요소로, 전략적 추론, 협력 및 사회적 규범을 균형 있게 조화해야 한다.

Method: Ultimatum, Buy-Sell, Resource Exchange 게임에 대한 통제된 다중 에이전트 시뮬레이션을 통해 영어와 4개 인도 언어(힌디어, 펀자브어, 구자라티어, 마르와리어) 간의 언어 효과를 고립시켰다.

Result: 언어 선택이 모델을 변경하는 것보다 결과를 더 강하게 바꿀 수 있으며, 제안자의 이점을 역전시키고 잉여를 재배분하였다. 인도 언어는 분배 게임의 안정성을 줄이고 통합 설정에서 더 풍부한 탐색을 유도하였다.

Conclusion: LLM의 협상을 오직 영어로 평가하는 것은 불완전하고 오해의 소지가 있는 결론을 초래할 수 있으며, 문화적으로 인식된 평가가 공정한 배포를 위해 필수적임을 시사한다.

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [22] [When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail](https://arxiv.org/abs/2601.04748)
*Xiaoxiao Li*

Main category: cs.AI

TL;DR: 단일 에이전트가 기술 라이브러리에서 선택함으로써 다중 에이전트 시스템의 모듈성 이점을 얻을 수 있는지 탐구했다. 이 접근법은 토큰 사용량과 지연 시간을 줄이면서도 경쟁력 있는 정확도를 유지할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 단일 에이전트가 기술 라이브러리에서 선택하여 다중 에이전트 시스템의 이점을 효과적으로 활용할 수 있는 방법을 조사하기 위해.

Method: 기술들을 내부화된 에이전트 행동으로 보고, 다중 에이전트 시스템을 단일 에이전트 시스템으로 컴파일하는 방법을 채택했다.

Result: 이 접근법은 토큰 사용량과 지연 시간을 크게 줄이며, 추론 벤치마크의 정확도를 유지한다.

Conclusion: 기술 선택이 라이브러리의 크기가 커짐에 따라 어떻게 조절되는지를 조사하여, 인지 과학의 원리에 기반한 제안이 유의미함을 발견했다.

Abstract: Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?
  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.

</details>


### [23] [Transitive Expert Error and Routing Problems in Complex AI Systems](https://arxiv.org/abs/2601.04416)
*Forest Mars*

Main category: cs.AI

TL;DR: 전문 지식은 판단을 향상시키지만 경계에서 체계적인 취약성을 초래할 수 있다. 이 기사는 전이 전문가 오류(TEE)를 정의하고 세 가지 조건 하에서 이 현상이 심화되는 메커니즘을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 전문 지식이 도메인 내에서 판단력을 향상시키는 동시에 경계에서 체계적인 취약점을 초래하는 현상을 이해하고자 함.

Method: 구조적 유사성이 인과적 차이를 가리는 상황에서의 전문 지식의 신뢰성을 분석하고, 사회적 압력과 메타인지적 실패가 전문성을 통해 어떻게 유지되는지 설명.

Result: 전문가들은 구조적 유사성 편향으로 인해 표면적 특징을 과대평가하고, 인과적 아키텍처의 차이를 간과하게 된다. AI 라우팅 아키텍처에서의 실패도 이러한 맥락에서 분석된다.

Conclusion: TEE는 감시하고 완화할 수 있는 여러 가지 징후를 가지며, 인지적 블랙 박스가 되는 부분을 아키텍처 설계를 통해 해결할 수 있다.

Abstract: Domain expertise enhances judgment within boundaries but creates systematic vulnerabilities specifically at borders. We term this Transitive Expert Error (TEE), distinct from Dunning-Kruger effects, requiring calibrated expertise as precondition. Mechanisms enabling reliable within-domain judgment become liabilities when structural similarity masks causal divergence. Two core mechanisms operate: structural similarity bias causes experts to overweight surface features (shared vocabulary, patterns, formal structure) while missing causal architecture differences; authority persistence maintains confidence across competence boundaries through social reinforcement and metacognitive failures (experts experience no subjective uncertainty as pattern recognition operates smoothly on familiar-seeming inputs.) These mechanism intensify under three conditions: shared vocabulary masking divergent processes, social pressure for immediate judgment, and delayed feedback. These findings extend to AI routing architectures (MoE systems, multi-model orchestration, tool-using agents, RAG systems) exhibiting routing-induced failures (wrong specialist selected) and coverage-induced failures (no appropriate specialist exists). Both produce a hallucination phenotype: confident, coherent, structurally plausible but causally incorrect outputs at domain boundaries. In human systems where mechanisms are cognitive black boxes; AI architectures make them explicit and addressable. We propose interventions: multi-expert activation with disagreement detection (router level), boundary-aware calibration (specialist level), and coverage gap detection (training level). TEE has detectable signatures (routing patterns, confidence-accuracy dissociations, domain-inappropriate content) enabling monitoring and mitigation. What remains intractable in human cognition becomes addressable through architectural design.

</details>


### [24] [XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs](https://arxiv.org/abs/2601.04426)
*Linzhang Li,Yixin Dong,Guanjie Wang,Ziyi Xu,Alexander Jiang,Tianqi Chen*

Main category: cs.AI

TL;DR: XGrammar 2는 동적 구조 생성 작업을 위한 최적화된 생성 엔진으로, 기존 엔진보다 6배 이상의 속도 향상을 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 LLM 에이전트는 도구 호출 및 조건부 구조 생성을 포함한 복잡한 구조 생성 작업을 처리해야 하며, 이는 기존 구조보다 더 동적입니다.

Method: XGrammar 2는 새로운 동적 디스패치 의미론인 TagDispatch를 통해 동적 구조 생성 작업에 대한 마스크 생성을 가속화하고, JIT 컴파일 방식과 크로스 그래머 캐싱 메커니즘을 도입하여 컴파일 시간을 줄입니다.

Result: 평가 결과, XGrammar 2는 기존 구조 생성 엔진보다 6배 이상의 속도 향상을 달성할 수 있음을 보여줍니다.

Conclusion: LLM 추론 엔진과 통합하여 XGrammar 2는 거의 제로 오버헤드로 동적 구조 생성 작업을 처리할 수 있습니다.

Abstract: Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.

</details>


### [25] [GUITester: Enabling GUI Agents for Exploratory Defect Discovery](https://arxiv.org/abs/2601.04500)
*Yifei Gao,Jiang Wu,Xiaoyi Chen,Yifan Yang,Zhe Cui,Tianyi Ma,Jiaming Zhang,Jitao Sang*

Main category: cs.AI

TL;DR: 이 논문은 GUI 테스트의 중요성을 강조하고, 자동화된 탐색적 테스트의 가능성을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 수동 비용이 높은 탐색적 GUI 테스트의 품질 향상 필요

Method: GUITestBench와 GUITester라는 새로운 프레임워크 제안. GUITestBench는 143개의 작업을 포함한 첫 번째 벤치마크이며, GUITester는 결함을 탐지하기 위한 두 모듈을 사용하는 다중 에이전트 프레임워크.

Result: GUITestBench에서 GUITester는 48.90%의 F1 점수를 기록하며, 최신 기법들을 초월함.

Conclusion: 자동 탐색 테스트의 실행 가능성을 보여주고, 향후 GUI 품질 보증을 위한 강력한 기초를 제공합니다.

Abstract: Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\footnote{Our code is now available in~\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.

</details>


### [26] [CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts](https://arxiv.org/abs/2601.04505)
*Khandakar Shakib Al Hasan,Syed Rifat Raiyan,Hasin Mahtab Alvee,Wahid Sadik*

Main category: cs.AI

TL;DR: 본 연구는 고급 자연어 설명에서 정확한 회로 기호 생성을 위한 CircuitLM이라는 혁신적인 회로 설계 파이프라인을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 정밀한 회로 기호 생성을 통한 전자 설계의 어려움을 해결하고 비전문가가 신뢰할 수 있는 회로 프로토타이핑을 가능하게 하기 위함입니다.

Method: CircuitLM은 사용자 프롬프트를 구조화된 CircuitJSON 기호로 변환하는 5단계 프로세스를 통해 작동합니다: (i) LLM 기반 부품 식별, (ii) 표준 핀 배치 검색, (iii) 전자 전문가 에이전트의 사고 연쇄, (iv) JSON 기호 합성, (v)_force-directed SVG 시각화.

Result: Dual-Metric Circuit Validation(DMCV) 프레임워크를 사용하여 인력 전문가의 평가와 비교하여 높은 정밀도를 달성했습니다.

Conclusion: 이 연구는 자연어 입력을 배포 가능한 하드웨어 설계로 연결하며, 비전문가가 신뢰할 수 있는 회로 프로토타입을 가능하게 합니다.

Abstract: Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.

</details>


### [27] [TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration](https://arxiv.org/abs/2601.04544)
*Jiuzhou Zhao,Chunrong Chen,Chenqi Qiao,Lebin Zheng,Minqi Han,Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang*

Main category: cs.AI

TL;DR: TCAR은 다중 에이전트 협업을 위한 적응형 라우팅을 제공하며, 라우팅 정확도를 개선하고 충돌을 줄인다.


<details>
  <summary>Details</summary>
Motivation: 기존 라우팅 전략은 정적 단일 라벨 결정에 의존하여 새로운 에이전트를 통합하기 어렵고 라우팅 충돌을 초래한다.

Method: TCAR는 자연어 추론 체인을 생성한 후 쿼리를 처리할 수 있는 후보 에이전트를 예측하는 동적인 에이전트 온보딩을 지원한다.

Result: TCAR은 라우팅 정확도를 크게 개선하고 라우팅 충돌을 줄이며 모호한 상황에서도 강건성을 유지한다.

Conclusion: 우리는 TCAR를 공개하여 설명 가능한 협업 다중 에이전트 라우팅에 대한 미래의 연구를 지원한다.

Abstract: Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.

</details>


### [28] [Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation](https://arxiv.org/abs/2601.04562)
*Dongyi Lv,Qiuyu Ding,Heng-Da Xu,Zhaoxu Sun,Zhi Wang,Feng Xiong,Mu Xu*

Main category: cs.AI

TL;DR: 이 연구는 지리적 신호를 활용하는 새로운 추천 프레임워크인 Reasoning Over Space (ROS)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 LLM 기반 추천 시스템이 이동성과 지역 서비스 시나리오에서 중요한 지리적 신호를 활용하는 데 제한적임.

Method: ROS는 지리를 중요한 결정 변수를 활용하며, Hierarchical Spatial Semantic ID (SID)를 통해 지리적 지역성과 POI 의미를 분리된 토큰으로 변환하고, Mobility Chain-of-Thought (CoT) 패러다임을 통해 사용자 성격을 모델링하고 의도에 맞는 후보 공간을 구성합니다.

Result: ROS는 세 개의 위치 기반 소셜 네트워크(LBSN) 데이터 세트를 사용한 실험에서 LLM 기반의 가장 강력한 기준 모델보다 10% 이상의 상대적 이득을 달성했습니다.

Conclusion: 작은 백본 모델을 사용하면서도 도시 간 전이 능력을 개선했습니다.

Abstract: Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.

</details>


### [29] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: 이 논문은 백도어 공격의 상호작용과 전파를 이해하기 위해 BackdoorAgent라는 모듈형 프레임워크를 제안하며, 다양한 에이전트 애플리케이션에 대한 실증 분석을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구들이 백도어 공격 벡터를 개별적으로 분석하여 전체적인 이해가 부족하므로, 에이전트 중심의 관점에서 백도어 위협을 포괄적으로 분석할 필요성이 있음.

Method: BackdoorAgent는 에이전트 워크플로의 세 가지 기능적 단계(플래닝, 메모리, 도구 사용)를 구조화하고, 각 단계에서 트리거 활성화와 전파를 체계적으로 분석할 수 있도록 에이전트 실행을 계기화함.

Result: 실증 분석 결과, 단일 단계에서 심어놓은 트리거가 여러 단계에 걸쳐 지속되고 중간 상태를 통해 전파될 수 있음을 발견하였으며, 이는 에이전트 워크플로가 백도어 위협에 취약함을 보여줌.

Conclusion: 코드와 벤치마크는 GitHub에서 공개되어 재현성과 향후 연구를 촉진할 수 있도록 제공됨.

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [30] [Sci-Reasoning: A Dataset Decoding AI Innovation Patterns](https://arxiv.org/abs/2601.04577)
*Jiachen Liu,Maestro Harmon,Zechen Zhang*

Main category: cs.AI

TL;DR: AI 혁신이 빠르게 진행됨에 따라 연구자들이 지식의 공백을 식별하고, 이전 연구를 종합하며 통찰을 생성하는 과정이 잘 이해되지 않고 있다. 이로 인해 과학적 추론에 대한 구조화된 데이터의 부족이 AI 연구 에이전트의 체계적 분석 및 개발을 저해하고 있다.


<details>
  <summary>Details</summary>
Motivation: AI 혁신이 빠르게 진행되는 가운데 연구자들의 지적 과정이 잘 이해되지 않고 있으며, 이는 AI 연구 에이전트 개발에 장애가 되고 있다.

Method: Sci-Reasoning이라는 데이터셋을 도입하여 NeurIPS, ICML 및 ICLR (2023-2025)의 Oral 및 Spotlight 논문을 주요 선행 연구와 연계하여 특정 추론 링크를 구조화된 형식으로 설명한다.

Result: 15개의 독특한 사고 패턴을 식별하였으며, 그 중 52.7%를 차지하는 세 가지 주요 전략은 Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), Representation Shift (10.5%)이다.

Conclusion: 이 데이터셋은 과학적 진보에 대한 정량적 연구를 가능하게 하며, 차세대 AI 연구 에이전트를 훈련하기 위한 구조화된 추론 경로를 제공한다.

Abstract: While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.

</details>


### [31] [Evaluating Human and Machine Confidence in Phishing Email Detection: A Comparative Study](https://arxiv.org/abs/2601.04610)
*Paras Jain,Khushi Dhar,Olyemi E. Amujo,Esa M. Rantanen*

Main category: cs.AI

TL;DR: 이 연구는 피싱 이메일을 탐지하는 데 있어서 인간 인지와 기계 학습 모델의 협력을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 피싱 이메일 등 속임수를 식별하는 데 필요한 인지 과정의 복잡성을 이해하고 AI 시스템과의 협력을 향상시키기 위해.

Method: 로지스틱 회귀, 결정 트리, 랜덤 포레스트의 세 가지 해석 가능한 알고리즘을 사용하여 TF-IDF 특성과 의미적 임베딩을 기반으로 훈련하고, 인간 평가와 비교.

Result: 기계 학습 모델은 좋은 정확도를 제공하나 신뢰 수준이 유의미하게 다르며, 인간 평가자는 더 다양한 언어 신호를 사용하고 일관된 신뢰성을 보임.

Conclusion: 이 연구는 인간의 인지 기능을 보완하는 투명한 AI 시스템 개발을 위한 방향성을 제시하며, 인간-AI 협력을 개선할 수 있다.

Abstract: Identifying deceptive content like phishing emails demands sophisticated cognitive processes that combine pattern recognition, confidence assessment, and contextual analysis. This research examines how human cognition and machine learning models work together to distinguish phishing emails from legitimate ones. We employed three interpretable algorithms Logistic Regression, Decision Trees, and Random Forests training them on both TF-IDF features and semantic embeddings, then compared their predictions against human evaluations that captured confidence ratings and linguistic observations. Our results show that machine learning models provide good accuracy rates, but their confidence levels vary significantly. Human evaluators, on the other hand, use a greater variety of language signs and retain more consistent confidence. We also found that while language proficiency has minimal effect on detection performance, aging does. These findings offer helpful direction for creating transparent AI systems that complement human cognitive functions, ultimately improving human-AI cooperation in challenging content analysis tasks.

</details>


### [32] [AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering](https://arxiv.org/abs/2601.04620)
*Di Zhang*

Main category: cs.AI

TL;DR: AgentDevel은 안정적이고 감사 가능한 개선을 위해 LLM 에이전트를 릴리스 공학 관점에서 다룬다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 자기 개선 메커니즘 및 동시 변형의 탐색은 불안정하고 감사하기 어려운 개선 경로를 초래하여 비퇴보성을 보장하기 어렵다.

Method: 해당 연구에서는 에이전트를 최종 아티팩트로 간주하고, 비퇴보 인식을 포함한 릴리스 파이프라인을 외부화하는 릴리스 공학으로 개선을 재구성한다.

Result: AgentDevel은 안정적인 개선을 제공하면서도 상당히 적은 수의 퇴보를 발생시키고 재현 가능하며 감사 가능한 결과물을 생성한다.

Conclusion: AgentDevel은 LLM 에이전트를 소프트웨어로 개발, 디버깅 및 릴리스하는 데 실용적인 개발 규율을 제공한다.

Abstract: Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.

</details>


### [33] [Higher-Order Knowledge Representations for Agentic Scientific Reasoning](https://arxiv.org/abs/2601.04878)
*Isabella A. Stewart,Markus J. Buehler*

Main category: cs.AI

TL;DR: 본 연구는 하이퍼그래프 기반의 지식 표현 방식을 통해 비코멧 시나리오에 대한 글로벌 하이퍼그래프를 구축하여 과학적 발견을 촉진하는 "교사 없는" 주체적 추론 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 과학적 탐구는 이질적인 실험 데이터와 교차 도메인 지식을 통합하여 일관된 설명을 제공하는 시스템적 사고를 요구한다.

Method: 하이퍼그래프 기반의 지식 표현 방식을 통해 다중 엔터티 관계를 충실히 인코딩하는 방법론을 제시하고, ~1,100개의 생체 복합재 스캐폴드에 관한 원고를 적용하여 글로벌 하이퍼그래프를 구성한다.

Result: 161,172개의 노드와 320,201개의 하이퍼엣지를 가진 하이퍼그래프를 구축, 스케일 프리 토폴로지를 발견하고 개념적 허브 주변으로 조직된 네트워크를 형성한다.

Conclusion: 하이퍼그래프 토폴로지가 검증 가능한 방지막 역할을 하여 전통적인 그래프 방법으로는 숨겨진 관계를 드러내고 과학적 발견을 가속화하는 '교사 없는' 주체적 추론 시스템을 구축한다.

Abstract: Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.

</details>


### [34] [Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search](https://arxiv.org/abs/2601.04703)
*Yiqun Chen,Lingyong Yan,Zixuan Yang,Erhan Zhang,Jiashu Zhao,Shuaiqiang Wang,Dawei Yin,Jiaxin Mao*

Main category: cs.AI

TL;DR: M-ASK는 복잡한 정보 탐색을 위한 새로운 프레임워크로, 에이전트를 두 가지 보완적인 역할로 세분화하여 안정적인 학습과 성능 향상을 도모한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 정보 탐색에서 대형 언어 모델이 도구 사용과 추론을 병행할 수 있도록 하는 새로운 패러다임인 에이전틱 서치의 필요성.

Method: M-ASK는 탐색 행동 에이전트와 지식 관리 에이전트로 에이전틱 서치를 명확하게 분리하여 각 에이전트가 정의된 서브태스크에 집중할 수 있도록 한다.

Result: M-ASK는 여러 QA 벤치마크에서 강력한 기준선을 초월하여 더 높은 답변 정확도를 달성하고, 훈련 과정의 안정성을 크게 향상시켰다.

Conclusion: 이 접근 방식은 탐색과 맥락 구성을 줄이고, 에이전트 간의 간섭을 최소화하여 효과적인 협조를 가능하게 한다.

Abstract: Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}

</details>


### [35] [Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning](https://arxiv.org/abs/2601.04726)
*Yuyang Hu,Jiongnan Liu,Jiejun Tan,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 이 논문은 사건 중심의 메모리 프레임워크 CompassMem을 제안하며, 이는 장기적인 의사결정을 지원하기 위해 경험을 축적하고 구조화하는 데 도움을 줍니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)이 지능형 에이전트로 활용되면서, 과거 경험을 유지하고 조직하며 검색하는 메모리 메커니즘이 필요합니다.

Method: CompassMem은 사건 세분화 이론에 영감을 받아 경험을 사건으로 나누고 이를 명확한 논리적 관계로 연결하여 이벤트 그래프로 메모리를 조직합니다.

Result: LoCoMo와 NarrativeQA에서의 실험 결과, CompassMem은 여러 백본 모델에서 검색 및 추론 성능을 일관되게 개선함을 보여주었습니다.

Conclusion: CompassMem은 비탈 시맨틱 검색을 넘어 구조적이고 목표 지향적인 기억 탐색을 가능하게 합니다.

Abstract: Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

</details>


### [36] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: AT$^2$PO는 다회전 에이전틱 강화 학습을 위한 통합 프레임워크로서 제한된 탐색 다양성, 희소한 신뢰 할당, 잘못 정렬된 정책 최적화라는 세 가지 주요 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 내부 추론과 외부 도구 상호작용을 엮어 다회전 작업을 해결하는 강력한 시스템으로 떠올랐다.

Method: AT$^2$PO는 엔트로피에 기반한 트리 확장과 회전별 신뢰 할당을 결합한 회전 수준의 트리 구조를 도입한다.

Result: 7개의 벤치마크에서 평균 1.84% 포인트의 향상을 나타내며 최신 기술 수준과 비교해 일관된 개선을 보였다.

Conclusion: 각 구성 요소의 효과를 검증하는 배제 연구를 통해 AT$^2$PO의 유효성을 입증하였다.

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [37] [SciIF: Benchmarking Scientific Instruction Following Towards Rigorous Scientific Intelligence](https://arxiv.org/abs/2601.04770)
*Encheng Su,Jianyu Wu,Chen Tang,Lintao Wang,Pengze Li,Aoran Wang,Jinouwen Zhang,Yizhou Wang,Yuan Meng,Xinzhu Ma,Shixiang Tang,Houqiang Li*

Main category: cs.AI

TL;DR: 대형 언어 모델의 평가 기준이 과학적 연구의 엄격한 기준을 통합해야 한다는 것을 강조하고, SciIF라는 다학제 기준을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 단순한 지식 검색을 넘어 복잡한 과학적 발견으로 이동하면서 평가 기준도 그에 맞춰 변화해야 한다는 필요.

Method: SciIF라는 다학제 기준을 소개하며, 대학 수준의 문제를 고정된 제약 조건 목록과 결합하여 과학적 유효성을 평가하도록 설계하였다.

Result: 모델들이 명시적인 제약 조건 만족 증거를 제공해야 하며, 솔루션의 정확성과 다중 제약 준수를 모두 측정한다.

Conclusion: SciIF는 LLM이 과학의 엄격한 논리적 틀 안에서 신뢰할 수 있는 에이전트로 기능하도록 보장한다.

Abstract: As large language models (LLMs) transition from general knowledge retrieval to complex scientific discovery, their evaluation standards must also incorporate the rigorous norms of scientific inquiry. Existing benchmarks exhibit a critical blind spot: general instruction-following metrics focus on superficial formatting, while domain-specific scientific benchmarks assess only final-answer correctness, often rewarding models that arrive at the right result with the wrong reasons. To address this gap, we introduce scientific instruction following: the capability to solve problems while strictly adhering to the constraints that establish scientific validity. Specifically, we introduce SciIF, a multi-discipline benchmark that evaluates this capability by pairing university-level problems with a fixed catalog of constraints across three pillars: scientific conditions (e.g., boundary checks and assumptions), semantic stability (e.g., unit and symbol conventions), and specific processes(e.g., required numerical methods). Uniquely, SciIF emphasizes auditability, requiring models to provide explicit evidence of constraint satisfaction rather than implicit compliance. By measuring both solution correctness and multi-constraint adherence, SciIF enables finegrained diagnosis of compositional reasoning failures, ensuring that LLMs can function as reliable agents within the strict logical frameworks of science.

</details>


### [38] [APEX: Academic Poster Editing Agentic Expert](https://arxiv.org/abs/2601.04794)
*Chengxin Shi,Qinnan Cai,Zeyuan Chen,Long Zeng,Yibo Zhao,Jing Yu,Jianxiang Yu,Xiang Li*

Main category: cs.AI

TL;DR: APEX는 상호작용을 지원하는 학술 포스터 편집을 위한 최초의 에이전틱 프레임워크로, 정밀한 제어와 강력한 다단계 API 기반 편집 기능을 제공하며, 514개의 편집 지침을 포함하는 APEX-Bench를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 포스터 생성 방법은 초기 초안을 자동화하지만, 주관적인 사용자 의도를 복잡하게 정렬하는 데 실패하여 상호작용이 부족합니다.

Method: APEX는 다단계 API 기반의 편집과 검토 및 조정 메커니즘을 통해 세분화된 제어를 지원하는 학술 포스터 편집 에이전트입니다. 또한, 다양한 특성을 고려하여 구성된 514개의 편집 지침으로 이루어진 APEX-Bench를 도입하였습니다.

Result: 실험 결과, APEX는 기존 방법들보다 현저하게 개선된 성능을 보여주었습니다.

Conclusion: APEX는 고급 기능을 통해 학술 포스터 편집을 효율적으로 수행할 수 있는 첫 번째 프레임워크입니다.

Abstract: Designing academic posters is a labor-intensive process requiring the precise balance of high-density content and sophisticated layout. While existing paper-to-poster generation methods automate initial drafting, they are typically single-pass and non-interactive, often fail to align with complex, subjective user intent. To bridge this gap, we propose APEX (Academic Poster Editing agentic eXpert), the first agentic framework for interactive academic poster editing, supporting fine-grained control with robust multi-level API-based editing and a review-and-adjustment Mechanism. In addition, we introduce APEX-Bench, the first systematic benchmark comprising 514 academic poster editing instructions, categorized by a multi-dimensional taxonomy including operation type, difficulty, and abstraction level, constructed via reference-guided and reference-free strategies to ensure realism and diversity. We further establish a multi-dimensional VLM-as-a-judge evaluation protocol to assess instruction fulfillment, modification scope, and visual consistency & harmony. Experimental results demonstrate that APEX significantly outperforms baseline methods. Our implementation is available at https://github.com/Breesiu/APEX.

</details>


### [39] [Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models](https://arxiv.org/abs/2601.04861)
*Jingbo Wang,Sendong Zhao,Jiatong Liu,Haochun Wang,Wanting Li,Bing Qin,Ting Liu*

Main category: cs.AI

TL;DR: OI-MAS는 다양한 인지 요구를 충족하기 위해 적응형 모델 선택 정책을 구현한 새로운 다중 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템은 복잡한 추론 작업에서 단일 에이전트 접근 방식보다 우수한 성능을 보였지만, 컴퓨팅 효율성에서 문제를 겪고 있다.

Method: OI-MAS는 이질적인 다중 스케일 LLM 풀에서 모델 선택 정책을 수행하고, 상태 의존적 라우팅 메커니즘을 활용하여 추론 과정 전반에 걸쳐 에이전트 역할과 모델 스케일을 동적으로 선택한다.

Result: OI-MAS는 기본 다중 에이전트 시스템보다 일관되게 더 높은 성능을 보였으며, 정확도를 최대 12.88	ext{%} 향상시키고 비용을 최대 79.78	ext{%} 절감하였다.

Conclusion: 이러한 결과는 OI-MAS가 다양한 작업 복잡성에 맞춰 적절한 모델 스케일을 선택함으로써 효율성을 높일 수 있음을 보여준다.

Abstract: While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\% while reducing cost by up to 79.78\%.

</details>


### [40] [Precomputing Multi-Agent Path Replanning using Temporal Flexibility: A Case Study on the Dutch Railway Network](https://arxiv.org/abs/2601.04884)
*Issa Hanou,Eric Kemmeren,Devin Wild Thomas,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: 다중 에이전트 계획을 실행하는 데 있어 지연된 에이전트가 있을 경우 다른 에이전트와의 충돌로 어려움이 발생하며, 이로 인해 새로운 안전한 계획을 신속하게 찾아야 한다. 이 논문은 다른 에이전트의 시간적 유연성을 활용하여 효율적으로 재계획하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 지연은 에이전트 간의 충돌을 유발하므로, 효과적이고 안전한 재계획이 필요하다.

Method: FlexSIPP라는 알고리즘을 사용하여 지연된 에이전트의 모든 가능한 계획을 미리 계산하고, 다른 에이전트의 변경 사항을 반환한다.

Result: 실험 결과 FlexSIPP는 실제 세계에서 요구되는 조정에 적합한 효과적인 솔루션을 제공하며 합리적인 시간 내에 수행된다.

Conclusion: FlexSIPP는 네델란드의 고밀도 철도 네트워크에서 기차의 재계획을 위한 실제 사례 연구를 통해 그 유효성이 입증되었다.

Abstract: Executing a multi-agent plan can be challenging when an agent is delayed, because this typically creates conflicts with other agents. So, we need to quickly find a new safe plan. Replanning only the delayed agent often does not result in an efficient plan, and sometimes cannot even yield a feasible plan. On the other hand, replanning other agents may lead to a cascade of changes and delays. We show how to efficiently replan by tracking and using the temporal flexibility of other agents while avoiding cascading delays. This flexibility is the maximum delay an agent can take without changing the order of or further delaying more agents. Our algorithm, FlexSIPP, precomputes all possible plans for the delayed agent, also returning the changes for the other agents, for any single-agent delay within the given scenario. We demonstrate our method in a real-world case study of replanning trains in the densely-used Dutch railway network. Our experiments show that FlexSIPP provides effective solutions, relevant to real-world adjustments, and within a reasonable timeframe.

</details>


### [41] [Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking](https://arxiv.org/abs/2601.04887)
*Sofiene Lassoued,Laxmikant Shrikant Bahetic,Nathalie Weiß-Borkowskib,Stefan Lierc,Andreas Schwunga*

Main category: cs.AI

TL;DR: 이 논문은 유연 제조 시스템(FMS)의 전통적인 작업장 예약 문제를 AGV와 도구 공유 시스템의 동시 통합을 통해 복잡성을 추가하여 발전시킵니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 제조 환경에서 생산 공정을 최적화하는 유연 제조 시스템의 중요성을 강조합니다.

Method: Colored-Timed Petri Nets(CTPNs)와 actor-critic 모델 기반 강화 학습(MBRL)을 결합한 새로운 접근 방식을 제안합니다.

Result: 작은 크기의 공공 벤치마크와 새로 개발된 대규모 벤치마크에 대한 평가 결과, 우리 접근 방식이 작은 사례에서는 전통적인 방법과 동등하고, 큰 사례에서는 메이크스팬 측면에서 더 나은 성능을 보이며 계산 시간을 10배 단축합니다.

Conclusion: 재현 가능성을 보장하기 위해 gym 호환 환경과 인스턴스 생성기를 제안하고, ablation study를 통해 각 구성 요소의 기여를 평가합니다.

Abstract: Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.

</details>


### [42] [SmartSearch: Process Reward-Guided Query Refinement for Search Agents](https://arxiv.org/abs/2601.04888)
*Tongyu Wen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: SmartSearch 프레임워크는 검색 쿼리의 중간 질의를 개선하여 정보 검색 능력을 통합한 대형 언어 모델 기반 검색 에이전트의 효과를 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 기반의 검색 에이전트는 지식 집약적인 문제를 해결하는 데 유망하지만, 중간 검색 쿼리의 질이 간과되고 있습니다.

Method: SmartSearch 프레임워크는 두 가지 핵심 메커니즘을 통해 구성되어 있습니다: (1) 공정한 보상을 통해 각 중간 검색 쿼리의 질을 세밀하게 감독하는 쿼리 평가와 (2) 저질 검색 쿼리를 선택적으로 개선하고 이를 바탕으로 다음 검색 라운드를 재생성하는 쿼리 정제.

Result: SmartSearch는 기존의 기준을 일관되게 초월하며, 추가적인 정량적 분석을 통해 검색 효율성과 쿼리 질에서 현저한 향상을 확인했습니다.

Conclusion: 코드는 https://github.com/MYVAE/SmartSearch에서 확인할 수 있습니다.

Abstract: Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.

</details>


### [43] [Large language models can effectively convince people to believe conspiracies](https://arxiv.org/abs/2601.05050)
*Thomas H. Costello,Kellin Pelrine,Matthew Kowal,Antonio A. Arechar,Jean-François Godbout,Adam Gleave,David Rand,Gordon Pennycook*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델(LLM)이 진실보다 허위 주장을 촉진하는 경향이 있는지를 실험을 통해 조사하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 진실과 허위 사이에서 어떤 영향을 미치는지를 이해하기 위해 이 연구를 진행하였다.

Method: 3개의 사전 등록된 실험을 통해 참가자들이 GPT-4o와 불확실한 음모론에 대해 토론하였고, AI는 그 음모론을 반박하거나 지지하도록 지시되었다.

Result: '탈옥된' GPT-4o 변형이 음모론에 대한 신념을 증가시키는 데 효과적이었고, '지지하는' AI가 더 긍정적으로 평가되었다.

Conclusion: LLM은 진실과 허위를 촉진할 수 있는 강력한 능력을 가지고 있으나, 이러한 위험을 완화할 수 있는 잠재적 해결책이 있을 수 있다.

Abstract: Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.

</details>


### [44] [Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior](https://arxiv.org/abs/2601.05114)
*Wajid Nasser*

Main category: cs.AI

TL;DR: 이 연구는 LLM-심사 시스템의 안정성과 일관성에 대한 평가를 제시하며, 심사자 간의 일관성 부족과 각 개별 심사자의 일관성을 강조합니다.


<details>
  <summary>Details</summary>
Motivation: LLM-심사 시스템이 제공할 것으로 기대되는 확장 가능하고 일관된 평가의 실제를 조사합니다.

Method: 3,240회의 평가를 통해 심사자 간의 합의도를 분석합니다.

Result: 심사자 간의 합의가 거의 없고, 서로 다른 판단 패턴이 존재하며, 특정 심사자의 평가를 예측하는 정확도가 77.1%에 이릅니다.

Conclusion: 각 심사자는 고유한 품질 이론을 가지고 있으며, 평균 점수를 산출하는 것은 실제 값을 반영하지 못합니다.

Abstract: LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's α = 0.042). On two dimensions, judges disagree more than random noise would predict (α < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an "evaluative disposition" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.

</details>


### [45] [SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning](https://arxiv.org/abs/2601.05187)
*Yanchang Liang,Xiaowei Zhao*

Main category: cs.AI

TL;DR: SimuAgent는 LLM 기반 모델링 및 시뮬레이션 에이전트로, Simulink를 위해 제작되었으며, 기존의 XML 대신 Python 표현을 사용하여 효율성을 극대화하고 모델링 정확성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation:  그래프 지향 엔지니어링 워크플로우에서 대형 언어 모델(LLMs)의 가능성을 탐구하기 위함입니다.

Method:  SimuAgent는 두 단계로 학습된 경량의 계획-실행 아키텍처를 가지고 있으며, 저수준 도구 기술과 고수준 디자인 추론 능력을 갖추고 있습니다. Reflection-GRPO를 통해 희소 보상을 해결합니다.

Result:  실험 결과, Qwen2.5-7B 모델이 SimuAgent로 미세 조정되었을 때, 표준 RL 기준보다 더 빠르게 수렴하고 더 높은 모델링 정확성을 달성하였습니다.

Conclusion:  SimuAgent는 LLM과 그래픽 모델링 환경 간의 격차를 줄이며, 산업 모델 기반 엔지니어링을 위한 실용적인 솔루션을 제공합니다.

Abstract: Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.

</details>


### [46] [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/abs/2601.05214)
*Kait Healy,Bharathi Srinivasan,Visakh Madathil,Jing Wu*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)은 도구 호출과 사용에서 뛰어난 능력을 보여주지만, 잘못된 도구 선택, 잘못된 매개변수 제공 및 전문 도구나 외부 시스템을 호출하는 대신 시뮬레이션을 수행하는 '툴 우회' 행동을 보이는 환각 현상으로 어려움을 겪고 있습니다. 이러한 환각은 LLM 기반 에이전트의 신뢰성을 떨어뜨리고 보안 및 감사 제어를 우회하게 만듭니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 신뢰성을 높이기 위해 도구 호출 환각을 조기에 탐지하고 오류를 처리할 필요성이 있다.

Method: LLM의 내부 표현을 활용하여 생성에 사용되는 동일한 순방향 패스를 통해 실시간으로 도구 호출 환각을 탐지하는 계산적으로 효율적인 프레임워크를 제시한다.

Result: 여러 도메인에서의 추론 작업을 통해 이 접근 방식을 평가하며, 86.4%의 정확도로 강력한 탐지 성능을 보여주고, 최소한의 계산 오버헤드로 실시간 추론 능력을 유지한다.

Conclusion: 신뢰할 수 있는 에이전트 배포를 위해 매개변수 수준의 환각과 부적절한 도구 선택을 탐지하는 데 특히 탁월하다.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.

</details>


### [47] [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230)
*Quentin Garrido,Tushar Nagarajan,Basile Terver,Nicolas Ballas,Yann LeCun,Michael Rabbat*

Main category: cs.AI

TL;DR: 현실 세계에서의 추론 및 계획 능력을 가진 에이전트는 행동 결과를 예측해야 합니다. 본 연구는 비디오만을 통해 행동 공간을 학습할 수 있는 잠재적 행동 모델 학습 문제를 다룹니다. 우리가 제안한 방법은 다양한 환경에서 나타나는 비디오의 복잡성과 그로 인한 문제를 해결하기 위해 연속적이지만 제한된 잠재 행동이 일반적인 벡터 양자화보다 더 나은 결과를 나타낸다는 점을 발견했습니다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계에서 작동하는 에이전트는 행동 결과를 예측할 수 있어야 하며, 이를 위해서는 행동 레이블이 필요하지만 대규모로 얻기 어려운 경우가 많습니다.

Method: 비디오에서 행동 공간을 학습할 수 있는 잠재적 행동 모델을 학습하는 방법을 제안합니다.

Result: 비디오에서 나타나는 환경의 변화, 예를 들어 사람이 방에 들어오는 것과 같은 상황을 여러 비디오에 걸쳐 전이할 수 있음을 발견했습니다.

Conclusion: 우리는 알려진 행동을 잠재 행동으로 매핑하는 컨트롤러를 훈련할 수 있었으며, 이를 통해 잠재 행동을 범용 인터페이스로 사용하여 계획 작업을 수행할 수 있습니다.

Abstract: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [48] [Integrating Multi-Agent Simulation, Behavioral Forensics, and Trust-Aware Machine Learning for Adaptive Insider Threat Detection](https://arxiv.org/abs/2601.04243)
*Firdous Kausar,Asmah Muallem,Naw Safrin Sattar,Mohamed Zakaria Kurdi*

Main category: cs.CR

TL;DR: 본 연구는 다중 에이전트 시뮬레이션, 계층형 보안 정보 및 사건 관리, 행동 및 통신 포렌식, 신뢰 인식 기계 학습, 마음 이론 추론을 통합한 적응형 내부자 위협 탐지를 위한 하이브리드 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 적응형 내부자 위협 탐지를 위한 효율적이고 정교한 방법론 개발의 필요성.

Method: 시뮬레이션된 기업 환경에서 운영되는 지능형 에이전트들이 행동 사건과 인지 의도 신호를 생성하며, 이를 중앙 집중식 SIEM이 수집합니다.

Result: CE-SIEM은 완벽한 재현율(1.000)을 달성하고, EG-SIEM은 행위자 수준 F1 점수를 0.922로 증가시켜, EG-SIEM-Enron은 높은 정밀도와 더불어 탐지 지연 시간을 단축시키는 결과를 보였습니다.

Conclusion: 인지적 맥락이 감도를 향상시키고, 증거 게이팅이 높은 정밀도의 저잡음 탐지를 가능하게 하며, 사전 훈련된 통신 보정이 내부자 위협 식별의 신뢰성을 더욱 가속할 수 있음을 보여줍니다.

Abstract: We present a hybrid framework for adaptive insider-threat detection that tightly integrates multi-agent simulation (MAS), layered Security Information and Event Management (SIEM) correlation, behavioral and communication forensics, trust-aware machine learning, and Theory-of-Mind (ToM) reasoning. Intelligent agents operate in a simulated enterprise environment, generating both behavioral events and cognitive intent signals that are ingested by a centralized SIEM. We evaluate four system variants: a Layered SIEM-Core (LSC) baseline, a Cognitive-Enriched SIEM (CE-SIEM) incorporating ToM and communication forensics, an Evidence-Gated SIEM (EG-SIEM) introducing precision-focused validation mechanisms, and an Enron-enabled EG-SIEM (EG-SIEM-Enron) that augments evidence gating with a pretrained email forensics module calibrated on Enron corpora. Across ten simulation runs involving eight malicious insiders, CE-SIEM achieves perfect recall (1.000) and improves actor-level F1 from 0.521 (LSC) to 0.774. EG-SIEM raises actor-level F1 to 0.922 and confirmed-alert precision to 0.997 while reducing false positives to 0.2 per run. EG-SIEM-Enron preserves high precision (1.000 confirmed-alert precision; 0.0 false positives per run), slightly improves actor-level F1 to 0.933, and reduces detection latency (average TTD 10.26 steps versus 15.20 for EG-SIEM). These results demonstrate that cognitive context improves sensitivity, evidence-gated validation enables high-precision, low-noise detection, and pretrained communication calibration can further accelerate high-confidence insider threat identification.

</details>
