<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 12]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Verifier-Constrained Flow Expansion for Discovery Beyond the Data](https://arxiv.org/abs/2602.15984)
*Riccardo De Santi,Kimon Protopapas,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 본 논문에서는 사전 훈련된 흐름 모델이 데이터 분포를 넘어 유효한 설계를 샘플링할 수 있도록 하는 방법을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 흐름 및 확산 모델은 제한된 데이터를 기반으로 훈련되어, 유효한 설계 공간의 일부만을 커버합니다. 이는 과학적 발견 애플리케이션에 있어 본질적인 한계입니다.

Method: 사전 훈련된 흐름 모델을 적응시키기 위해 검증자(예: 원자 결합 검사기)에 접근하여 고 데이터 가용성 지역을 넘어 밀도를 확장하는 알고리즘 프레임워크를 제안합니다.

Result: 흐름 확장기(FE)는 검증자 제약 하의 엔트로피 최대화를 통해 두 가지 문제를 해결하는 분명한 미러 하강 방식입니다.

Conclusion: 우리의 방법은 생소한 환경과 분자 설계 작업에서 사전 훈련된 흐름 모델의 유효성을 유지하면서 원형 다양성을 증가시키는 능력을 입증하였습니다.

Abstract: Flow and diffusion models are typically pre-trained on limited available data (e.g., molecular samples), covering only a fraction of the valid design space (e.g., the full molecular space). As a consequence, they tend to generate samples from only a narrow portion of the feasible domain. This is a fundamental limitation for scientific discovery applications, where one typically aims to sample valid designs beyond the available data distribution. To this end, we address the challenge of leveraging access to a verifier (e.g., an atomic bonds checker), to adapt a pre-trained flow model so that its induced density expands beyond regions of high data availability, while preserving samples validity. We introduce formal notions of strong and weak verifiers and propose algorithmic frameworks for global and local flow expansion via probability-space optimization. Then, we present Flow Expander (FE), a scalable mirror descent scheme that provably tackles both problems by verifier-constrained entropy maximization over the flow process noised state space. Next, we provide a thorough theoretical analysis of the proposed method, and state convergence guarantees under both idealized and general assumptions. Ultimately, we empirically evaluate our method on both illustrative, yet visually interpretable settings, and on a molecular design task showcasing the ability of FE to expand a pre-trained flow model increasing conformer diversity while preserving validity.

</details>


### [2] [Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16196)
*Emile Anand,Richard Hoffmann,Sarah Liaw,Adam Wierman*

Main category: cs.LG

TL;DR: 이 논문에서는 이질적인 상호작용을 가진 대규모 에이전트 집단을 위한 스케일 가능한 협력적 다중 에이전트 강화 학습을 위한 그래폰 평균장 하위 샘플링 프레임워크인 GMFS를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 상호작용 에이전트 집단을 조정하는 것은 다중 에이전트 강화 학습(MARL)에서 중요한 도전 과제로, 에이전트 수가 증가함에 따라 Joint State-Action Space의 크기가 기하급수적으로 증가합니다.

Method: 우리는 상호작용 강도에 따라 κ개의 에이전트를 하위 샘플링하여 그래폰 가중 평균장을 근사하고, 표본 복잡성은 poly(κ)이고 최적성 간격은 O(1/√κ)인 정책을 학습합니다.

Result: 로봇 조정에서 수치 시뮬레이션을 통해 이론을 검증하며, GMFS가 거의 최적의 성능을 달성한다는 것을 보여줍니다.

Conclusion: GMFS는 이질적인 에이전트 상호작용을 고려한 스케일 가능한 협력적 MARL을 가능하게 합니다.

Abstract: Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.

</details>


### [3] [Feature-based morphological analysis of shape graph data](https://arxiv.org/abs/2602.16120)
*Murad Hossen,Demetrio Labate,Nicolas Charon*

Main category: cs.LG

TL;DR: 이 논문은 2D 또는 3D 공간에 포함된 기하학적 네트워크의 형태 그래프 데이터 세트를 통계적으로 분석하기 위한 계산 파이프라인을 제안하고 시연한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 추상 그래프와는 달리, 데이터의 연결 구조의 변화를 파악하고 구별하는 것뿐만 아니라 네트워크 가지의 기하학적 차이를 분석하는 것이 목적이다.

Method: 특히 선별되고 명시적인 일련의 위상적, 기하학적, 방향적 특성을 추출하는 방법에 의존하며, 이는 주요 불변성 특성을 만족하도록 설계되었다.

Result: 그 결과로 생성된 특성 표현은 형태 그래프 집단에 대한 그룹 비교, 군집화 및 분류와 같은 작업에 활용된다.

Conclusion: 이 표현의 효과는 도시 도로/거리 네트워크, 신경 추적 및 아스트로사이트 이미지를 포함한 여러 실제 데이터 세트에서 평가되었으며, 결과는 여러 대체 방법과 비교된다.

Abstract: This paper introduces and demonstrates a computational pipeline for the statistical analysis of shape graph datasets, namely geometric networks embedded in 2D or 3D spaces. Unlike traditional abstract graphs, our purpose is not only to retrieve and distinguish variations in the connectivity structure of the data but also geometric differences of the network branches. Our proposed approach relies on the extraction of a specifically curated and explicit set of topological, geometric and directional features, designed to satisfy key invariance properties. We leverage the resulting feature representation for tasks such as group comparison, clustering and classification on cohorts of shape graphs. The effectiveness of this representation is evaluated on several real-world datasets including urban road/street networks, neuronal traces and astrocyte imaging. These results are benchmarked against several alternative methods, both feature-based and not.

</details>


### [4] [HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents](https://arxiv.org/abs/2602.16165)
*Jiangweizhi Peng,Yuanxin Liu,Ruida Zhou,Charles Fleming,Zhaoran Wang,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: Hierarchical Plan-Execute RL 프레임워크인 HiPER는 고수준 계획과 저수준 실행을 분리하여 상호작용 에이전트의 다단계 의사결정을 지원합니다.


<details>
  <summary>Details</summary>
Motivation: LLM을 다단계 의사결정을 위한 상호작용 에이전트로 훈련시키는 것이 어렵고, 특히 보상이 드물고 지연되는 장기적인 작업에서 문제를 겪고 있습니다.

Method: HiPER는 정책을 고수준 계획자와 저수준 실행자로 분리하고, 계층적 이점 추정(HAE) 기법을 도입하여 계획 및 실행 레벨에서 신용을 효과적으로 할당합니다.

Result: HiPER는 ALFWorld에서 97.4", WebShop에서 83.3\

Conclusion: 이러한 결과는 다단계 LLM 에이전트의 확장 가능한 RL 훈련을 위한 명시적 계층적 분해의 중요성을 강조합니다.

Abstract: Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.
  We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.
  Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\% success on ALFWorld and 83.3\% on WebShop with Qwen2.5-7B-Instruct (+6.6\% and +8.3\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.

</details>


### [5] [A Scalable Approach to Solving Simulation-Based Network Security Games](https://arxiv.org/abs/2602.16564)
*Michael Lanier,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: MetaDOAR는 경량 메타 컨트롤러로, 커다란 사이버 네트워크 환경에서 스케일 가능한 다중 에이전트 강화 학습을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 네트워크 환경에서 효율적인 강화 학습을 위한 필요성이 있다.

Method: MetaDOAR는 구조적 임베딩을 기반으로 압축된 상태 프로젝션을 학습하고, 선택된 장치에 대해 집중적인 빔 검색을 수행하는 전통적인 저수준 액터와 협력한다.

Result: MetaDOAR는 대규모 네트워크 토폴로지에서 기존 최첨단 기법보다 높은 플레이어 보상률을 달성했다.

Conclusion: 이 연구는 대규모 네트워크 의사결정 문제를 위한 효율적인 계층 정책 학습을 위한 실용적이고 이론적으로 동기화된 경로를 제공한다.

Abstract: We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.

</details>


### [6] [HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs](https://arxiv.org/abs/2602.16336)
*Samira Nazari,Mohammad Saeed Almasi,Mahdi Taheri,Ali Azarpeyvand,Ali Mokhtari,Ali Mahani,Christian Herglotz*

Main category: cs.LG

TL;DR: HAWX는 다양한 DNN 추상화 수준에서 다중 수준 민감도 점수를 활용하여 이질적인 AxC 블록의 선택적 통합을 안내하는 하드웨어 인식 확장 탐색 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: DNN의 성능을 최적화하고 다양한 하드웨어 아키텍처에 적합한 방법을 찾기 위해.

Method: HAWX는 DNN의 여러 추상화 수준에서 다중 수준의 민감도 점수를 계산하고, 이를 통해 후보 구성의 평가를 가속화하는 예측 모델을 사용한다.

Result: HAWX는 LeNet-5에 대해 2개의 후보 근사 블록을 활용한 레이어 수준 탐색에서 23배 이상의 속도 향상을 달성했으며, 필터 수준 탐색에서는 3*106배 이상의 속도 향상을 달성하였다.

Conclusion: HAWX는 다양한 DNN 벤치마크와 하드웨어 아키텍처에서 효율적으로 작동하며, 네트워크 크기에 따라 효율성 이점을 기하급수적으로 증가시킨다.

Abstract: This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, power, and area, HAWX accelerates the evaluation of candidate configurations, achieving over 23* speedup in a layer-level search with two candidate approximate blocks and more than (3*106)* speedup at the filter-level search only for LeNet-5, while maintaining accuracy comparable to exhaustive search. Experiments across state-of-the-art DNN benchmarks such as VGG-11, ResNet-18, and EfficientNetLite demonstrate that the efficiency benefits of HAWX scale exponentially with network size. The HAWX hardware-aware search algorithm supports both spatial and temporal accelerator architectures, leveraging either off-the-shelf approximate components or customized designs.

</details>


### [7] [Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks](https://arxiv.org/abs/2602.16357)
*Sarkis Ter Martirosyan,Xinyue Huang,David Qin,Anthony Yu,Stanislav Emelianov*

Main category: cs.LG

TL;DR: SPOI-AE는 비선형성을 가정하지 않고 sPA 이미징과 스펙트럼 분리 문제를 해결하여 생리학적 과정에 대한 정보를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: sPA 이미지에서 크로모포어의 상대 농도를 정확하게 추정하면 생리학적 과정에 대한 많은 정보를 얻을 수 있지만, 비선형성과 ill-posedness로 인해 추정이 어렵습니다.

Method: Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE)를 제안하며, 비선형성을 가정하지 않고 sPA 광학 역전 및 스펙트럼 분리 문제를 해결합니다.

Result: SPOI-AE는 전통적인 알고리즘보다 입력 sPA 픽셀을 더 잘 재구성하고, 광학 매개변수, 크로모포어 농도, 조직의 산소 포화 비율에 대한 생물학적으로 일관된 추정치를 제공합니다.

Conclusion: SPOI-AE의 분리 정확도는 시뮬레이션한 마우스 림프절 팬텀의 실제 값과 비교하여 검증되었습니다.

Abstract: Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.

</details>


### [8] [Improved Bounds for Reward-Agnostic and Reward-Free Exploration](https://arxiv.org/abs/2602.16363)
*Oran Ridel,Alon Cohen*

Main category: cs.LG

TL;DR: 이 연구는 에피소드 유한 지평선 마르코프 결정 과정(MDP)에서 보상 없는 탐색과 보상 무관 탐색을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트가 외부 보상을 관찰하지 않고도 알려지지 않은 환경을 탐색할 수 있도록 하는 것이 이 연구의 동기이다.

Method: 새로운 알고리즘을 제안하며, 이 알고리즘은 신중하게 설계된 보상을 가진 온라인 학습 절차를 통해 탐색 정책을 구성한다.

Result: 제안된 알고리즘은 보상으로부터 $ε$-최적 정책을 계산하기 위한 정확한 동역학 추정에 필요한 데이터 수집을 가능하게 한다.

Conclusion: 보상 없는 탐색을 위한 엄밀한 하한선을 정립하여 알려진 상한선과 하한선의 간극을 좁혔다.

Abstract: We study reward-free and reward-agnostic exploration in episodic finite-horizon Markov decision processes (MDPs), where an agent explores an unknown environment without observing external rewards. Reward-free exploration aims to enable $ε$-optimal policies for any reward revealed after exploration, while reward-agnostic exploration targets $ε$-optimality for rewards drawn from a small finite class. In the reward-agnostic setting, Li, Yan, Chen, and Fan achieve minimax sample complexity, but only for restrictively small accuracy parameter $ε$. We propose a new algorithm that significantly relaxes the requirement on $ε$. Our approach is novel and of technical interest by itself. Our algorithm employs an online learning procedure with carefully designed rewards to construct an exploration policy, which is used to gather data sufficient for accurate dynamics estimation and subsequent computation of an $ε$-optimal policy once the reward is revealed. Finally, we establish a tight lower bound for reward-free exploration, closing the gap between known upper and lower bounds.

</details>


### [9] [Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study](https://arxiv.org/abs/2602.16523)
*Gerhard Stenzel,Isabella Debelic,Michael Kölle,Tobias Rohe,Leo Sünkel,Julian Hager,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 이 연구는 강화 학습을 통해 이산 게이트 선택에서 매개변수화된 양자 상태 준비로 확장된 지향 양자 회로 합성을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 양자 회로 합성을 통해 양자 컴퓨팅의 효율성을 높이고자 하며, 특히 지속적인 단일 큐비트 회전을 통해 보다 복잡한 양자 상태를 준비하는 것을 연구한다.

Method: 하나의 에이전트가 게이트 유형, 관련 큐비트, 회전 각도를 동시에 선택하는 일단계 방식과, 이산 회로를 제안한 후 아담을 이용해 회전 각도를 최적화하는 이단계 방식을 비교한다.

Result: PPO는 안정적인 하이퍼파라미터에서 성공적으로 정책을 학습하여 83%에서 99%의 계산 기반 상태와 61%에서 77%의 벨 상태를 재구성하였지만, 두 단계를 사용할 경우 정확도 향상은 미미하고 실행 시간이 세 배 증가하였다.

Conclusion: 고정된 계산 예산에서의 실용성을 고려할 때, 일단계 PPO 정책을 추천하며, 구체적인 합성 회로를 제공하고 고전적 변분 기준선과 대조를 통해 더 나은 확장성을 위한 경로를 제시한다.

Abstract: We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \(R_x\), \(R_y\), and \(R_z\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \(λ\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \(5\times10^{-4}\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \(10^{-4}\)). Both approaches reliably reconstruct computational basis states (between 83\% and 99\% success) and Bell states (between 61\% and 77\% success). However, scalability saturates for \(λ\) of approximately three to four and does not extend to ten-qubit targets even at \(λ=2\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.

</details>


### [10] [MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models](https://arxiv.org/abs/2602.16573)
*Antonios Tziorvas,George S. Theodoropoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 이 논문에서는 도시 수요 예측의 중요성을 강조하고, 기울기 부스팅 모델을 이용한 새로운 예측 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 도시 수요 예측은 지능형 교통 시스템 내에서 경로 최적화, 배차 및 혼잡 관리에 필수적입니다.

Method: 두 가지 기울기 부스팅 모델 변형을 제안하며, 하나는 분류를 위한 것이고 다른 하나는 회귀를 위한 것입니다. 이 모델들은 5분에서 1시간까지 다양한 시간 범위에서 수요 예측을 생성할 수 있습니다.

Result: 우리는 실제 데이터 세트를 사용하여 우리의 접근 방법을 최신 기술 및 생성적 AI 기반 모델과 비교했습니다.

Conclusion: 우리의 방법론은 도시 마이크로 모빌리티 관리에 대한 새로운 통찰력을 제공하여, 급속한 도시화에 따른 도전 과제를 극복하는 데 기여합니다.

Abstract: Urban demand forecasting plays a critical role in optimizing routing, dispatching, and congestion management within Intelligent Transportation Systems. By leveraging data fusion and analytics techniques, traffic demand forecasting serves as a key intermediate measure for identifying emerging spatial and temporal demand patterns. In this paper, we tackle this challenge by proposing two gradient boosting model variations, one for classiffication and one for regression, both capable of generating demand forecasts at various temporal horizons, from 5 minutes up to one hour. Our overall approach effectively integrates temporal and contextual features, enabling accurate predictions that are essential for improving the efficiency of shared (micro-) mobility services. To evaluate its effectiveness, we utilize open shared mobility data derived from e-scooter and e-bike networks in five metropolitan areas. These real-world datasets allow us to compare our approach with state-of-the-art methods as well as a Generative AI-based model, demonstrating its effectiveness in capturing the complexities of modern urban mobility. Ultimately, our methodology offers novel insights on urban micro-mobility management, helping to tackle the challenges arising from rapid urbanization and thus, contributing to more sustainable, efficient, and livable cities.

</details>


### [11] [A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models](https://arxiv.org/abs/2602.16626)
*SungJun Cho,Chetan Gohil,Rukuang Huang,Oiwi Parker Jones,Mark W. Woolrich*

Main category: cs.LG

TL;DR: 자연어 처리의 최근 성공이 신경영상 데이터에 대한 대규모 기초 모델에 대한 관심을 증가시켰다. 본 연구에서는 MEG 데이터에 적용된 트랜스포머 기반 대형 신경영상 모델에 대해 샘플 수준의 토큰화 전략을 체계적으로 평가하였다.


<details>
  <summary>Details</summary>
Motivation: 신경영상 데이터에 대한 대규모 기초 모델에 대한 관심이 증가하고 있으며, 이들 모델은 신경 시간 시계열 데이터를 이산화해야 한다.

Method: MEG 데이터에 적용된 트랜스포머 기반 대형 신경영상 모델에 대해 학습 가능 및 비학습 가능 토크나이저의 신호 재구성 충실도와 기초 모델링 성능에 미치는 영향을 비교하였다. 학습 가능한 토크나이저에 대해 새로운 오토인코더 기반 접근 방식을 제안하였다.

Result: 학습 가능 및 비학습 가능 이산화 방식 모두 높은 재구성 정확도를 달성하였으며, 대부분의 평가 기준에서 유사한 성능을 보였다.

Conclusion: 간단한 고정 샘플 수준의 토큰화 전략을 신경 기초 모델 개발에 사용할 수 있음을 시사한다.

Abstract: Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.

</details>


### [12] [Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes](https://arxiv.org/abs/2602.16629)
*Ethan Blaser,Jiuqi Wang,Shangtong Zhang*

Main category: cs.LG

TL;DR: 이 논문은 평균 보상에 대한 강화 학습의 수렴성을 다루며, 특히 지역 시계 없이도 수렴성을 보장하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 수렴 보장 방법은 지역 시계에 의존하며 실무에서는 사용되지 않는다.

Method: 표준 감소 학습률을 사용하여 지역 시계 없이 어떤 n에 대해서도 온 정책 n-단계 차등 TD의 거의 확실한 수렴성을 증명하고, 오프 정책 n-단계 차등 TD가 지역 시계 없이 수렴하는 세 가지 충분조건을 도출한다.

Result: 온 정책 n-단계 차등 TD의 거의 확실한 수렴을 보였고, 오프 정책에 대한 수렴 조건을 제시했다.

Conclusion: 이 결과는 차등 TD의 이론적 기초를 강화하고 실제 구현에 대한 수렴 분석을 더 가깝게 만든다.

Abstract: The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [13] [From Tool Orchestration to Code Execution: A Study of MCP Design Choices](https://arxiv.org/abs/2602.15945)
*Yuval Felendler,Parth A. Gandhi,Idan Habler,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: MCP는 이질적인 실행 환경에서 도구를 발견하고 선택하며 조정할 수 있는 통합 플랫폼을 제공하지만, 도구 호출 증가로 인한 조정 오버헤드 및 상태 관리 단편화 문제가 발생한다. CE-MCP는 이러한 문제를 해결하기 위해 코드 실행을 기본 기능으로 포함시킨 새로운 설계로, 복잡한 워크플로우를 통합하여 실행하지만, 공격 표면이 크게 확장되는 보안 취약점도 동반한다. 이 연구는 이와 같은 보안 갭을 해결하기 위해 MAESTRO 프레임워크를 적용하고 공격 클래스와 위험을 분석하여 방어 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: MCP 기반 시스템은 이질적인 실행 환경에서 도구를 효율적으로 관리할 수 있는 필요성이 증가하고 있다. 그러나 기존의 도구별 호출 방식은 조정 오버헤드를 증가시키고, 상태 관리의 단편화를 초래하여 대규모 환경에서의 운영을 제한한다.

Method: CE-MCP는 코드 실행을 첫 번째급 기능으로 포함하여 복잡한 워크플로우를 단일 프로그램으로 통합하여 격리된 실행 환경 내에서 실행하도록 한다. MCP-Bench 프레임워크를 활용해 10개 대표 서버에서 작업 동작, 도구 활용 패턴, 실행 지연 및 프로토콜 효율성을 평가한다.

Result: 실험 결과 CE-MCP는 토큰 사용과 실행 지연을 획기적으로 줄이고, 대규모 환경에서의 효율성을 개선하지만, 공격 표면을 크게 확장하는 것으로 나타났다. 이를 통해 MAESTRO 프레임워크를 적용한 보안 갭을 파악하고, 특정 코드 실행 위협도 포함하는16개의 공격 클래스를 식별했다.

Conclusion: 이 연구는 생산 준비가 된 실행 가능한 에이전트 워크플로우에서 확장성 및 보안 간 균형을 맞추기 위한 체계적인 로드맵을 제공한다.

Abstract: Model Context Protocols (MCPs) provide a unified platform for agent systems to discover, select, and orchestrate tools across heterogeneous execution environments. As MCP-based systems scale to incorporate larger tool catalogs and multiple concurrently connected MCP servers, traditional tool-by-tool invocation increases coordination overhead, fragments state management, and limits support for wide-context operations. To address these scalability challenges, recent MCP designs have incorporated code execution as a first-class capability, an approach called Code Execution MCP (CE-MCP). This enables agents to consolidate complex workflows, such as SQL querying, file analysis, and multi-step data transformations, into a single program that executes within an isolated runtime environment. In this work, we formalize the architectural distinction between context-coupled (traditional) and context-decoupled (CE-MCP) models, analyzing their fundamental scalability trade-offs. Using the MCP-Bench framework across 10 representative servers, we empirically evaluate task behavior, tool utilization patterns, execution latency, and protocol efficiency as the scale of connected MCP servers and available tools increases, demonstrating that while CE-MCP significantly reduces token usage and execution latency, it introduces a vastly expanded attack surface. We address this security gap by applying the MAESTRO framework, identifying sixteen attack classes across five execution phases-including specific code execution threats such as exception-mediated code injection and unsafe capability synthesis. We validate these vulnerabilities through adversarial scenarios across multiple LLMs and propose a layered defense architecture comprising containerized sandboxing and semantic gating. Our findings provide a rigorous roadmap for balancing scalability and security in production-ready executable agent workflows.

</details>


### [14] [Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes](https://arxiv.org/abs/2602.16109)
*Srikumar Nayak,James Walmesley*

Main category: cs.CR

TL;DR: 새로운 연합 학습 프레임워크인 FedGraph-AGI를 통해 다국적 재정 네트워크에서의 내부 위협을 탐지할 수 있으며, 이는 AGI 추론과 그래프 신경망을 결합하여 개인 정보를 보호하고, 10개 관할권에서 50,000건의 거래 데이터셋으로 실험한 결과 92.3%의 정확도를 기록하였다.


<details>
  <summary>Details</summary>
Motivation: 국경을 넘는 내부 위협은 정부 재정 계획에 중대한 도전 과제를 제기하며, 특히 여러 관할권에서 분산되고 개인 정보가 민감한 데이터를 다룰 때 더욱 그렇다.

Method: FedGraph-AGI는 개인 정보 보호를 위해 AGI 추론과 그래프 신경망을 통합한 새로운 연합 학습 프레임워크이다. 이 접근 방식은 데이터 주권을 보존하는 연합 그래프 신경망, 이질적인 관할권을 위한 혼합 전문가 집합(MoE) 집계, 그래프 데이터에서 인과 추론을 수행하는 대규모 행동 모델(LAM)을 통한 AGI 기반 추론을 결합한다.

Result: FedGraph-AGI는 10개 관할권에서 50,000건의 거래 데이터셋에 대한 실험을 통해 92.3%의 정확도를 달성하며, 연합 기초(Base)보다 크게 향상된 86.1%, 중앙 집중 방식보다 높은 84.7%의 성과를 보였다. AGI 추론은 6.8%의 개선에 기여하고 MoE는 4.4%를 추가하였다.

Conclusion: 이 시스템은 내부 위협 감지를 위한 AGI 추론과 연합 그래프 학습의 첫 번째 통합을 나타내며, 개인 정보를 보호하는 국경을 넘는 정보 공유를 위한 새로운 방향을 열어준다.

Abstract: Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.

</details>


### [15] [Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents](https://arxiv.org/abs/2602.16520)
*Doron Shavit*

Main category: cs.CR

TL;DR: RLM-JB는 재귀 언어 모델을 기반으로 하여 효율적인 Jailbreak 탐지 프레임워크를 제공하며, 여러 LLM 백엔드에서 높은 탐지 효과성을 유지하면서도 정밀도와 낮은 위양성률을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델에 대한 jailbreak 프롬프트의 위협이 증가하고 있으며, 특히 신뢰할 수 없는 콘텐츠에서 도구를 실행하는 에이전트 시스템에서 문제가 되고 있습니다.

Method: RLM-JB는 입력을 변환하고, 숨겨진 부분에 대해 작업자 모델을 쿼리하며, 검토 가능한 결정을 위해 증거를 집계하는 경계 분석 프로그램을 조정하는 루트 모델을 갖춘 재귀 언어 모델 기반의 엔드 투 엔드 jailbreak 탐지 프레임워크입니다. 이 프레임워크는 의심스러운 입력을 정상화하고 디오브푸스케이션하며, 텍스트를 분할하고 병렬 청크 스크리닝을 수행하는 등의 절차적 방법으로 탐지를 처리합니다.

Result: RLM-JB는 AutoDAN 스타일의 적대적 입력에서 세 가지 LLM 백엔드에 걸쳐 높은 탐지 효과성(ASR/재현율 92.5-98.0%)을 달성하고, 매우 높은 정밀도(98.99-100%)와 낮은 위양성률(0.0-2.0%)을 유지합니다.

Conclusion: 스크리닝 백엔드가 변경됨에 따라 실용적인 민감도-특이성 트레이드오프를 강조합니다.

Abstract: Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.

</details>


### [16] [Policy Compiler for Secure Agentic Systems](https://arxiv.org/abs/2602.16708)
*Nils Palumbo,Sarthak Choudhary,Jihye Choi,Prasad Chalasani,Mihai Christodorescu,Somesh Jha*

Main category: cs.CR

TL;DR: PCAS는 에이전트 시스템을 위한 정책 컴파일러로, 복잡한 승인 정책을 결정적으로 시행할 수 있도록 합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 고객 서비스 프로토콜, 승인 작업 흐름, 데이터 접근 제한 및 규제 준수와 같이 복잡한 승인 정책을 요구하는 맥락에서 점점 더 배포되고 있습니다.

Method: PCAS는 에이전트 시스템 상태를 의존성 그래프로 모델링하고, 정책을 Datalog 파생 언어로 선언적 규칙으로 표현하여 비결정론적 이유와 독립적으로 실행 이전에 위반을 차단합니다.

Result: PCAS는 세 가지 사례 연구에서 평가되었으며, 고객 서비스 작업에서 정책 준수를 48%에서 93%로 향상시켰고, 도구화된 실행에서는 정책 위반이 없었습니다.

Conclusion: PCAS는 정책 준수를 보장하며, 기존 에이전트 구현 및 정책 사양을 기반으로 구조적으로 정책 준수 시스템으로 변환합니다.

Abstract: LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.
  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.
  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection](https://arxiv.org/abs/2602.16037)
*Cameron Cagan,Pedram Fard,Jiazi Tian,Jingya Cheng,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: 자율 에이전트 워크플로우의 실패 모드를 조사하고, Pythia 프레임워크를 통해 최적화 불안정성을 분석하여 선택 에이전트를 통해 더 나은 성능을 달성하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트 워크플로우가 행동을 반복적으로 개선할 수 있는 잠재력은 크지만, 그 실패 모드는 아직 잘 정의되지 않았습니다.

Method: Pythia라는 오픈 소스 프레임워크를 사용하여 최적화 불안정성을 평가하였고, 세 가지 임상 증상(호흡 곤란, 가슴 통증, 롱 COVID 뇌 안개)에 대해 연구하였습니다.

Result: 테스트에서 3% 발생률의 경우, 시스템은 95% 정확도를 달성했지만 양성 사례는 전혀 탐지하지 못하는 실패 모드를 보여주었습니다. 선택 에이전트를 사용하여 최적의 성능을 가진 반복을 식별한 결과, 전문가 선별 어휘보다 331% 및 7% 더 높은 성능을 달성했습니다.

Conclusion: 이러한 발견은 자율 AI 시스템의 중요한 실패 모드를 특징짓고, 저발생률 분류 작업에서 안정성을 위한 적극적 개입보다 회고적 선택이 더 나은 성과를 낸다는 것을 보여줍니다.

Abstract: Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.

</details>


### [18] [Learning Personalized Agents from Human Feedback](https://arxiv.org/abs/2602.16173)
*Kaiqu Liang,Julia Kruk,Shengyi Qian,Xianjun Yang,Shengjie Bi,Yuanshun Yao,Shaoliang Nie,Mingyang Zhang,Lijuan Liu,Jaime Fernández Fisac,Shuyan Zhou,Saghar Hosseini*

Main category: cs.AI

TL;DR: 개인화된 AI 에이전트를 만드는 PAHF 프레임워크는 사용자의 변화하는 선호도를 실시간으로 학습하여 에이전트의 개인화를 지속적으로 제공한다.


<details>
  <summary>Details</summary>
Motivation: 현대 AI 에이전트는 강력하지만 개별 사용자의 변동하는 선호와 일치하지 않는 경우가 많다.

Method: PAHF는 에이전트가 명시적인 사용자 메모리를 통해 실시간 상호작용에서 학습하는 지속적인 개인화 프레임워크를 제안한다. 이는 세 단계의 루프를 포함한다: 모호성을 해소하기 위한 사전 행동 명확화, 메모리에서 검색한 선호에 기반한 행동의 기반, 선호가 변화할 때 메모리를 업데이트하는 사후 행동 피드백 통합.

Result: 우리는 초기 선호를 무에서 유로 학습하고 그 후 개인화 변화에 적응하는 에이전트의 능력을 정량화하는 네 단계 프로토콜과 두 가지 기준을 개발하였다.

Conclusion: PAHF는 명시적인 메모리와 이중 피드백 채널의 통합을 통해 상당히 빠르게 학습하고, 초기 개인화 오류를 줄이며, 선호 변화에 신속하게 적응할 수 있도록 한다.

Abstract: Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.

</details>


### [19] [Verifiable Semantics for Agent-to-Agent Communication](https://arxiv.org/abs/2602.16424)
*Philipp Schoenegger,Matt Carlson,Chris Schneider,Chris Daly*

Main category: cs.AI

TL;DR: 다중 에이전트 AI 시스템은 일관된 소통이 필요하지만, 에이전트들이 사용하는 용어에 대한 동일한 이해를 공유하는지 검증할 방법이 부족하다. 본 연구는 에이전트들을 공유 관찰 사건에 기반하여 테스트하고, 경험적 불일치가 통계적 임계값 이하일 때 용어를 인증하는 인증 프로토콜을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 AI 시스템에서 에이전트 간의 소통이 일관되게 이루어져야 하지만, 그들이 사용하는 용어에 대한 공동의 이해를 검증할 방법이 부족하다.

Method: 자극-의미 모델에 기반한 인증 프로토콜을 제안하며, 에이전트는 공유된 관찰 가능한 사건을 바탕으로 테스트를 수행하고, 경험적 불일치가 통계적 임계값 이하일 때 용어를 인증한다. 인증된 용어에 대한 제한적 추론을 수행하는 에이전트는 명백하게 제한된 불일치를 달성한다.

Result: 다양한 정도의 의미적 차이를 가진 시뮬레이션에서, 핵심 보장(core-guarding)은 불일치를 72-96% 줄인다. 세부 조정된 언어 모델을 통한 검증에서 불일치는 51% 감소한다.

Conclusion: 우리의 프레임워크는 검증 가능한 에이전트 간의 통신을 위한 첫 번째 단계를 제공한다.

Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms ("core-guarded reasoning") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.

</details>


### [20] [EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments](https://arxiv.org/abs/2602.16179)
*Sushant Mehta,Logan Ritchie,Suhaas Garre,Nick Heiner,Edwin Chen*

Main category: cs.AI

TL;DR: AI 에이전트를 고충실도 강화 학습 환경에서 훈련함으로써 훈련 분포를 넘어서는 일반화 능력을 발휘할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트들이 실제 직업에서 요구하는 다단계, 도메인 특정 작업을 수행할 수 있는지를 측정하기 위해 설계된 환경을 도입한다.

Method: 	extsc{EnterpriseGym}의 첫 번째 환경인 	exttt{corecraft}를 사용하여 GLM~4.6을 그룹 상대 정책 최적화(GRPO)로 훈련하고 적응형 클리핑을 적용한다.

Result: 훈련 후 모델이 25.37rom 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76	o 36.76로 향상됨.

Conclusion: 환경의 품질, 다양성 및 사실성이 일반화된 에이전트의 능력을 가능하게 하는 주요 요인임을 시사한다.

Abstract: We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \corecraft{}, the first environment in \textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\% to 36.76\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\% on BFCL Parallel, +7.4\% on $τ^2$-Bench Retail, and +6.8\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.

</details>


### [21] [Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16435)
*Arun Vignesh Malarkkan,Wangyang Ying,Yanjie Fu*

Main category: cs.AI

TL;DR: CAFÉ라는 프레임워크가 제안되어 자동화된 특성 공학(AFE)의 새로운 접근 방식을 제시하며, 인과적 발견과 강화 학습 기반 특성 구성의 다리 역할을 합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 자동화된 특성 공학 방법들은 통계적 휴리스틱에 의존하여, 분포 변화에 취약한 부서진 특성을 생성합니다. 이러한 문제를 해결하기 위해 제안을 하였습니다.

Method: CAFÉ 프레임워크는 AFE를 인과적으로 안내되는 순차적 결정 과정으로 재구성하며, 첫 번째 단계에서는 특성과 목표 간의 희소한 방향성 비순환 그래프를 학습하여 부드러운 인과적 사전 지식을 획득합니다. 두 번째 단계에서는 인과적 그룹과 변환 연산자를 선택하기 위해 다중 에이전트 심층 Q-학습 아키텍처를 사용합니다.

Result: CAFÉ는 15개의 공공 벤치마크에서 기존의 강력한 AFE 기준 대비 최대 7% 향상을 달성하고, 수렴까지의 에피소드를 줄이며, 경쟁력 있는 목표 시간대를 제공합니다.

Conclusion: CAFÉ는 인과적 구조가 경직된 제약이 아닌 부드러운 유추적 사전으로 사용될 때 자동화된 특성 공학의 견고성과 효율성을 크게 향상시킬 수 있음을 보여줍니다.

Abstract: Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE, a framework that reformulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforcement learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors, grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning architecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ~4x relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and efficiency of automated feature engineering.

</details>


### [22] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 이 논문에서는 인공지능 초지능(ASI)을 달성하기 위한 필수적인 '기억' 디자인 개념을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 우리는 '기억으로 세상을 상승시킨다'는 사명에 의해 동기부여를 받았다.

Method: 우리는 '저장한 후 필요 시 추출' 접근법을 강조하고, 다양한 경험을 저장하여 필요할 때 유연하게 활용할 수 있도록 한다.

Result: 단순한 실험을 통해 이러한 접근법이 직관적으로 효과적임을 보여준다.

Conclusion: 우리는 이러한 유망한 방향에 대한 조사 제한의 주요 도전 과제를 논의하고 이를 해결하기 위한 연구 주제를 제안한다.

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [23] [Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents](https://arxiv.org/abs/2602.16246)
*Yun-Shiuan Chuang,Chaitanya Kulkarni,Alec Chiu,Avinash Thangali,Zijie Pan,Shivani Shekhar,Yirou Ge,Yixi Li,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: 본 연구는 상호작용 대형 언어 모델 에이전트의 효율적인 평가 방법을 제시하며, 결정론적 데이터베이스 없이도 안정적인 성과 비교 및 On-policy 학습 데이터를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 결정론적 백엔드에 의존하는 기존 에이전트 벤치마크의 비용 문제를 해결하고 보다 경제적인 대안을 제시하기 위함이다.

Method: 사용자 목표, 사실 및 기대하는 최종 상태를 포함하는 시나리오를 정의하고, LLM 상태 추적기가 전체 상호작용 추적을 기반으로 구조화된 대리 상태를 유추한다. LLM 판별자가 목표 완성과 도구/사용자 허상 검사를 수행한다.

Result: 제안된 벤치마크는 모델 간 안정적인 순위를 생성하고, On-policy 및 Off-policy 롤아웃을 통해 이전에 보지 못한 시나리오에 대한 감독을 지원한다.

Conclusion: 대리 상태 기반 평가 방법은 산업용 대형 언어 모델 에이전트에 대해 결정론적 에이전트 벤치마크의 실용적이고 확장 가능한 대안을 제공한다.

Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.

</details>


### [24] [Multi-agent cooperation through in-context co-player inference](https://arxiv.org/abs/2602.16301)
*Marissa A. Weis,Maciej Wołczyk,Rajai Nasser,Rif A. Saurous,Blaise Agüera y Arcas,João Sacramento,Alexander Meulemans*

Main category: cs.AI

TL;DR: 다중 에이전트 강화 학습에서 자기 이익을 추구하는 에이전트 간의 협력 달성은 여전히 근본적인 도전 과제입니다. 본 논문에서는 시퀀스 모델의 맥락 내 학습 능력이 하드코딩된 가정이나 명시적인 시간 규모 분리를 요구하지 않고도 동료 학습 인식을 가능하게 함을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 자기 이익을 추구하는 에이전트 간의 협력을 이루는 것은 다중 에이전트 강화 학습에서 중요한 도전 과제입니다.

Method: 시퀀스 모델을 훈련시켜 다양한 동료 에이전트와 경쟁하게 하여 동료 학습 인식을 자연스럽게 유도하는 방법을 제시합니다.

Result: 시퀀스 모델 에이전트가 다양한 동료 분포에 대해 훈련될 때 동맥 내 최선 대응 전략이 자연스럽게 유도되고, 협력 행동 학습이 촉진됨을 보여줍니다.

Conclusion: 모델의 분산 강화 학습과 동료 에이전트의 다양성을 결합함으로써 협력 행동 학습을 위한 확장 가능한 경로를 제공합니다.

Abstract: Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between "learning-aware" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between "naive learners" updating on fast timescales and "meta-learners" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.

</details>


### [25] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: 본 연구는 대규모 언어 모델을 사용하여 인과 ABA를 위한 불완전한 전문가 역할을 수행하고자 하며, 인과적 발견을 위한 평가 프로토콜을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 인과 발견은 데이터로부터 인과 관계를 밝혀내는 것을 목표로 하며, 이는 개입의 효과를 예측하는 데 중요하다.

Method: 본 논문은 변수 이름과 설명에서 가져온 의미론적 구조 priors를 활용하여 대규모 언어 모델을 인과 ABA에 적용하는 방법을 탐구하고, 이를 조건적 독립성 증거와 통합한다.

Result: 표준 벤치마크 및 의미론적으로 기반을 둔 합성 그래프에서의 실험 결과는 최첨단 성능을 보여준다.

Conclusion: 본 연구는 인과 발견을 위한 LLM의 평가에서 기억 편향을 완화하기 위한 평가 프로토콜을 도입한다.

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [26] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: Agent Skill 프레임워크는 소규모 언어 모델에서도 유사한 이점을 제공하는지에 대한 조사 결과, 중간 크기 모델이 Agent Skill 접근법의 혜택을 받으며, 코드 전문화 변형은 성능과 GPU 효율을 개선함.


<details>
  <summary>Details</summary>
Motivation: 데이터 보안 및 예산 제약으로 인해 공공 API에 지속적으로 의존할 수 없는 산업 시나리오에서 소규모 언어 모델의 일반화 한계를 극복하는 방법을 찾기 위한 문제 제기.

Method: Agent Skill 프로세스에 대한 수학적 정의를 도입하고 다양한 크기의 언어 모델을 여러 사용 사례에서 체계적으로 평가함.

Result: 작은 모델은 신뢰할 수 있는 기술 선택에 어려움을 겪고, 중간 크기 SLM은 Agent Skill 접근법에서 상당한 혜택을 받으며, 80B 매개변수의 코드 전문화 변형이 폐쇄형 기준과 유사한 성능을 보임.

Conclusion: 이 연구는 Agent Skill 프레임워크의 기능과 제약을 포괄적으로 묘사하며, SLM 중심 환경에서 효과적으로 배포하는 데 필요한 실용적인 통찰을 제공함.

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


### [27] [Towards a Science of AI Agent Reliability](https://arxiv.org/abs/2602.16666)
*Stephan Rabanser,Sayash Kapoor,Peter Kirgis,Kangheng Liu,Saiteja Utpala,Arvind Narayanan*

Main category: cs.AI

TL;DR: AI 에이전트의 성능 평가에 대한 새로운 프레임워크를 제안하여 에이전트의 신뢰성을 네 가지 주요 차원에서 분석합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 실제 작업 수행에서의 실패 사례를 줄이며, 에이전트의 행동 평가의 한계를 해결하고자 함.

Method: 신뢰성, 일관성, 견고성, 예측 가능성 및 안전성을 포함한 네 가지 주요 차원에 따라 에이전트의 신뢰성을 분해하는 12개의 구체적인 지표를 제안합니다.

Result: 14개의 에이전트 모델을 두 개의 보완 벤치마크에서 평가한 결과, 최근의 능력 향상이 신뢰성에 대한 개선이 미미함을 발견했습니다.

Conclusion: 제안한 지표는 기존의 평가를 보완하며 에이전트의 성능과 저하, 실패에 대한 통찰을 제공합니다.

Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [28] [Evaluating Collective Behaviour of Hundreds of LLM Agents](https://arxiv.org/abs/2602.16662)
*Richard Willis,Jianing Zhao,Yali Du,Joel Z. Leibo*

Main category: cs.MA

TL;DR: 최신 LLM 모델은 사회적 딜레마에서 개인의 이익을 우선시할 경우 더 나쁜 사회적 결과를 초래할 수 있음.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트의 집단 행동 이해가 중요해짐.

Method: LLM이 알고리즘 형태로 전략을 생성하는 평가 프레임워크를 도입함.

Result: 최신 모델이 개인의 이익을 우선시할 때 구식 모델보다 나쁜 사회적 결과를 초래함.

Conclusion: 협력이 감소하고 인구 규모가 증가할 때 불리한 사회적 평형 상태로 수렴할 위험이 큼.

Abstract: As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.

</details>


### [29] [Consensus Based Task Allocation for Angles-Only Local Catalog Maintenance of Satellite Systems](https://arxiv.org/abs/2602.16678)
*Harrison Perone,Christopher W. Hays*

Main category: cs.MA

TL;DR: 이 논문은 여러 통신 위성이 한정된 시야각에 의한 측정을 이용해 관찰을 효율적으로 스케줄링하고 조정하는 분산형 작업 할당 알고리즘을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 근접 위성이 안전하게 임무를 수행하기 위해서는 모든 위성과 잔해의 상대 상태를 잘 이해해야 합니다.

Method: 여러 통신 위성이 각자 통신 및 비통신 객체의 지역 카탈로그를 유지하기 위해 각도만을 사용하는 제한된 시야각 측정을 사용하며, 이를 효율적으로 관찰 스케줄링 및 조정을 위한 분산형 작업 할당 알고리즘으로 해결합니다.

Result: 새로운 방법이 기존 접근 방식으로 형성된 불확실성-연료 파레토 경계보다 상당히 우수한 성능을 발휘함을 수치적 시뮬레이션을 통해 확인했습니다.

Conclusion: 이 알고리즘은 연료 사용량 및 전반적인 카탈로그 불확실성을 quant화하여 성능을 평가합니다.

Abstract: In order for close proximity satellites to safely perform their missions, the relative states of all satellites and pieces of debris must be well understood. This presents a problem for ground based tracking and orbit determination since it may not be practical to achieve the required accuracy. Using space-based sensors allows for more accurate relative state estimates, especially if multiple satellites are allowed to communicate. Of interest to this work is the case where several communicating satellites each need to maintain a local catalog of communicating and non-communicating objects using angles-only limited field of view (FOV) measurements. However, this introduces the problem of efficiently scheduling and coordinating observations among the agents. This paper presents a decentralized task allocation algorithm to address this problem and quantifies its performance in terms of fuel usage and overall catalog uncertainty via numerical simulation. It was found that the new method significantly outperforms the uncertainty-fuel Pareto frontier formed by current approaches.

</details>


### [30] [Fairness Dynamics in Digital Economy Platforms with Biased Ratings](https://arxiv.org/abs/2602.16695)
*J. Martin Smit,Fernando P. Santos*

Main category: cs.MA

TL;DR: 디지털 서비스 경제의 플랫폼에서의 평가 시스템은 차별을 초래할 수 있다. 본 논문은 차별을 줄이면서 서비스 품질 유인을 유지하는 플랫폼 설계를 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 디지털 서비스 경제에서 공정성과 사용자 경험을 동시에 향상시키기 위한 방법을 찾기 위해.

Method: 진화 게임 이론 모델을 사용하여 플랫폼의 평가 기반 차별 perpetuation 및 저지에 대해 연구.

Result: 사용자 경험과 공정성 간의 기본적인 상충관계를 입증하고, 특정 인구 통계를 조정하는 것이 불공정성을 줄이는 효과적인 방법임을 보여준다.

Conclusion: 예외적 특성을 무시하는 추천 시스템을 개선할 가능성이 여전히 존재하며, 평가 시스템이 협력적 행동을 촉진하는 시스템에서의 proactive anti-discrimination 디자인의 이점을 강조한다.

Abstract: The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.

</details>
