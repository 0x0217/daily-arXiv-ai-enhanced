<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 12]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AI Kill Switch for malicious web-based LLM agent](https://arxiv.org/abs/2511.13725)
*Sechan Lee,Sangdon Park*

Main category: cs.CR

TL;DR: 웹 기반 대형 언어 모델(LLM) 에이전트가 점점 더 복잡한 작업을 자율적으로 수행함에 따라, 악의적인 오용 사례의 위험이 증가하고 있다. 이를 해결하기 위해 AI 킬 스위치 기술을 제안하며, AutoGuard를 도입했다. 이 방법은 악의적인 LLM 에이전트의 안전 메커니즘을 작동시키는 방어 프롬프트를 생성하여 즉시 작동을 중단할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 웹 기반 LLM 에이전트의 복잡한 작업 수행으로 인한 악의적인 오용 사례의 위험 증가.

Method: AutoGuard를 활용한 방어 프롬프트 생성 기법으로 악성 웹 기반 LLM 에이전트의 안전 메커니즘을 작동.

Result: 실험 결과, AutoGuard 방법은 GPT-4o, Claude-3, Llama3.3-70B-Instruct와 같은 악성 에이전트에서 80% 이상의 방어 성공률을 달성했다.

Conclusion: 웹 기반 LLM 에이전트의 다양한 시나리오와 모델에 대한 제어 가능성을 입증하여 AI 제어 및 안전성의 광범위한 노력에 기여했다.

Abstract: Recently, web-based Large Language Model (LLM) agents autonomously perform increasingly complex tasks, thereby bringing significant convenience. However, they also amplify the risks of malicious misuse cases such as unauthorized collection of personally identifiable information (PII), generation of socially divisive content, and even automated web hacking. To address these threats, we propose an AI Kill Switch technique that can immediately halt the operation of malicious web-based LLM agents. To achieve this, we introduce AutoGuard - the key idea is generating defensive prompts that trigger the safety mechanisms of malicious LLM agents. In particular, generated defense prompts are transparently embedded into the website's DOM so that they remain invisible to human users but can be detected by the crawling process of malicious agents, triggering its internal safety mechanisms to abort malicious actions once read. To evaluate our approach, we constructed a dedicated benchmark consisting of three representative malicious scenarios (PII collection, social rift content generation, and web hacking attempts). Experimental results show that the AutoGuard method achieves over 80% Defense Success Rate (DSR) on malicious agents, including GPT-4o, Claude-3, and Llama3.3-70B-Instruct. It also maintains strong performance, achieving around 90% DSR on GPT-5, GPT-4.1, and Gemini-2.5-Flash when used as the malicious agent, demonstrating robust generalization across models and scenarios. Through this research, we have demonstrated the controllability of web-based LLM agents across various scenarios and models, thereby contributing to the broader effort of AI control and safety.

</details>


### [2] [ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.13771)
*Shaowei Guan,Yu Zhai,Zhengyu Zhang,Yanze Wang,Hin Chi Kwok*

Main category: cs.CR

TL;DR: 이 논문은 해석 가능한 적대적 방어 프레임워크인 ExplainableGuard를 소개하며, 이를 통해 언어 모델의 출력이 적대적 공격에 의해 조작되는 것을 방지한다.


<details>
  <summary>Details</summary>
Motivation: 최근 큰 언어 모델(LLMs)이 적대적 공격에 취약해짐에 따라, 그들의 의사결정 과정에서의 투명성이 부족한 방어 메커니즘이 필요하다.

Method: ExplainableGuard는 DeepSeek-Reasoner의 사고 체인(CoT) 추론 기능을 활용하여 텍스트의 적대적 섭동을 감지하고 중화하며, 각 방어 조치에 대해 단계별 설명을 제공합니다.

Result: GLUE 벤치마크와 IMDB 영화 리뷰 데이터셋에서 초기 결과는 유망한 방어 효능을 보여줍니다.

Conclusion: 인간 평가 연구에서 ExplainableGuard의 설명은 명료성, 구체성 및 실행 가능성 면에서 다른 변형보다 우수하며, 72.5%의 배포 가능성 신뢰 등급을 기록하여 더욱 신뢰할 수 있는 LLM 배포의 가능성을 강조합니다.

Abstract: Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.

</details>


### [3] [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045)
*Yule Liu,Heyi Zhang,Jinyi Zheng,Zhen Sun,Zifan Peng,Tianshuo Cong,Yilong Yang,Xinlei He,Zhuo Ma*

Main category: cs.CR

TL;DR: 이 논문은 RLVR(검증 가능한 보상을 통한 강화 학습)를 기반으로 한 모델에서의 회원 추론 공격(MIA)의 새로운 유형을 제안하고, DIBA(행동 차이 공격)를 통해 이러한 공격을 수행하는 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 훈련 각 단계에서 개인 정보 보호를 위협하는 회원 추론 공격을 해결하기 위한 필요성이 존재한다.

Method: DIBA는 모델의 행동 변화에 초점을 맞추고, 두 가지 축(유리측 개선 및 로짓측 발산)에서 측정 가능한 변화를 활용하는 회원 추론 프레임워크이다.

Result: DIBA는 기존의 기준선을 뛰어넘어 약 0.8 AUC와 0.1% FPR에서 TPR을 크게 향상시켰음을 보여준다.

Conclusion: 이 연구는 RLVR에서 개인 정보 취약성을 체계적으로 분석한 첫 번째 논문이며, 명시적 감독 없이도 훈련 데이터 노출이 행동 흔적을 통해 신뢰성 있게 추론될 수 있음을 밝혀냈다.

Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

</details>


### [4] [Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion](https://arxiv.org/abs/2511.14301)
*Eric Xue,Ruiyi Zhang,Zijun Zhang,Pengtao Xie*

Main category: cs.CR

TL;DR: SteganoBackdoor는 자연어 처리의 백도어 공격에 대한 새로운 방어 관점을 제공하며, 효과적인 공격 성공률을 기록하면서도 기존 데이터 방어를 우회할 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 자연어 처리 모델이 백도어 공격에 취약함을 해결하기 위해, 보다 실질적인 위협 모델과의 정합성을 높이려는 필요성.

Method: SteganoBackdoor는 자연어 스테가노그래피의 무해한 속성을 활용하여, 의미적 트리거 시드를 스테가노그래픽 캐리어로 변환하는 그래디언트 기반 데이터 최적화 과정을 적용.

Result: 다양한 실험 환경에서 SteganoBackdoor는 기존 접근 방식에 비해 99% 이상의 공격 성공률을 기록하면서도 훨씬 낮은 데이터 오염율을 유지.

Conclusion: SteganoBackdoor는 현재의 방어 체계에서의 맹점을 드러내며, 적대적 데이터 방어 및 현실 세계의 위협 모델링에 즉각적인 주의를 요구한다.

Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance](https://arxiv.org/abs/2511.14043)
*Chandrachur Bhattacharya,Sibendu Som*

Main category: cs.AI

TL;DR: AISAC는 아르곤 국립 연구소에서 과학 및 공학 작업을 위해 개발된 통합 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 과학 및 공학 작업의 효율성을 높이고 투명성 및 적응성을 제공하기 위한 시스템을 개발하고자 한다.

Method: Router-Planner-Coordinator 워크플로우 및 선택적 평가자 역할을 구현하고, 구조화된 JSON 출력을 강제하는 사용자 정의 시스템 프롬프트를 사용하여 각 역할을 정의한다.

Result: FAISS와 SQLite를 결합한 하이브리드 메모리 접근 방식을 통해 의미 검색과 구조화된 대화 이력을 가능하게 하며, 연구 팀이 도구 및 프롬프트를 맞춤화할 수 있도록 구성 기반 프로젝트 부트스트랩 레이어를 제공한다.

Conclusion: AISAC는 폐기물-제품 연구와 에너지 프로세스 안전 등 여러 연구 분야에서 성공적으로 적용되었으며, 다양한 분야에 걸쳐 활용 가능성을 보여준다.

Abstract: AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.
  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.
  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.

</details>


### [6] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: DataSage는 데이터 분석 및 통찰력 발견을 자동화하기 위한 새로운 다중 에이전트 프레임워크로, 외부 지식 검색, 다중 역할 토론 메커니즘, 다중 경로 추론을 통해 기존의 데이터 인사이트 에이전트보다 우수한 성능을 보입니다.


<details>
  <summary>Details</summary>
Motivation: 자동화된 데이터 분석 및 통찰력 발견을 통해 조직의 효과적인 의사 결정을 지원하는 것이 중요합니다.

Method: DataSage는 외부 지식 검색, 다중 역할 토론 메커니즘, 다중 경로 추론의 세 가지 혁신적인 기능을 통합한 다중 에이전트 프레임워크입니다.

Result: InsightBench에서의 광범위한 실험을 통해 DataSage는 모든 난이도 수준에서 기존 데이터 인사이트 에이전트를 일관되게 능가하는 성능을 보였습니다.

Conclusion: DataSage는 자동화된 데이터 인사이트 발견을 위한 효과적인 솔루션을 제공합니다.

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [7] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: 현재의 고급 VLM은 언어 표현에 의존하여 공간 추론에 큰 한계를 보이며, 새로운 Imagery Driven Framework(IDF)를 제안하여 이를 개선할 수 있는 방법을 모색한다.


<details>
  <summary>Details</summary>
Motivation: 공간적 추론은 심상(simulation)과 관련되어 있으며, 이는 현재의 VLM에서 큰 도전 과제가 되고 있다.

Method: SpatiaLite라는 합성 벤치마크를 도입하여 공간 추론의 정확도와 효율성을 동시에 측정한다.

Result: 세 가지 주요 발견: 1) 고급 VLM은 주로 언어적 표현에 의존하여 시각적 공간 관계와 3D 기하 변환에서 큰 결핍을 보인다. 2) 고급 VLM은 공간 추론 메커니즘에서 심각한 비효율성을 보이며, 변환 복잡성이 증가함에 따라 토큰 사용량이 급격히 증가한다. 3) 데이터 합성과 훈련을 위한 Imagery Driven Framework(IDF)를 제안하여 공간적 추론에 필요한 내부 세계 모델을 암묵적으로 구축할 수 있다.

Conclusion: 이 연구는 SpatiaLite를 기반으로 고급 VLM의 공간 추론 한계와 패턴을 명확히 하고, 주요 단점을 식별하며 향후 발전 방향을 제시한다.

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [8] [When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology](https://arxiv.org/abs/2511.13825)
*Humza Nusrat,Omar Nusrat*

Main category: cs.AI

TL;DR: KOSMOS라는 자율 AI 과학자가 방사선 생물학 문제를 해결하기 위해 언어 모델을 사용했으며, 세 가지 가설을 평가한 결과 일부 가설은 지지를 받지 못했다.


<details>
  <summary>Details</summary>
Motivation: 자율 AI 과학자들이 방사선 생물학 문제에 대한 연구를 수행하여 그들의 기능을 평가하고자 함.

Method: KOSMOS를 사용하여 간단한 무작위 유전자 기준에 기반한 세 가지 가설을 설정하고 실험을 진행.

Result: 첫 번째 가설은 지지를 받지 않았고, 두 번째 가설에서 OGT는 약한 연관성을 보였으며, CDO1은 명확한 이상치를 나타냈다. 세 번째 가설에서는 12-gene signature가 통계적으로 유의미한 결과를 보여주었다.

Conclusion: AI 과학자들이 유용한 아이디어를 생성할 수 있지만 적절한 기준 모델에 대한 철저한 감사가 필요함을 보여주었다.

Abstract: Agentic AI "scientists" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.

</details>


### [9] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: CORGI라는 새로운 매칭 알고리즘은 저지연 응용 프로그램을 위해 효과적인 규칙 기반 시스템을 구축하고, 이러한 시스템의 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 매칭 문제를 해결해야 하는 규칙 기반 시스템은 AI 에이전트의 계획 및 반응 제어와 같은 실시간 응용 프로그램에서 효과적이다.

Method: CORGI는 두 단계 접근 방식을 사용하여 그래프의 기초 관계를 구축하고, 필요에 따라 역방향으로 작동하는 반복자를 통해 매칭을 생성한다.

Result: CORGI는 SOAR 및 OPS5의 RETE 구현보다 단순 조합 매칭 작업에서 성능이 크게 향상됨을 보여주었다.

Conclusion: 이 알고리즘은 메모리 문제를 예방하고 성능을 보장한다.

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [10] [Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases](https://arxiv.org/abs/2511.13987)
*Antonio Manuel Martínez-Heredia,Dolores Godrid Rodríguez,Andrés Ortiz García*

Main category: cs.AI

TL;DR: 이 논문은 음악 분석 및 교육에 적용된 인공지능(AI) 에이전트에 대한 통합 리뷰와 실험적 검증을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI의 음악 분석 및 교육 분야의 발전과 적용 가능성을 탐구하고자 한다.

Method: 규칙 기반 모델에서 딥러닝, 다중 에이전트 아키텍처, 검색 증강 생성(RAG) 프레임워크로의 역사적 진화를 종합하고, 이중 사례 방법론을 통해 AI 플랫폼을 교육에 적용한다.

Result: AI 에이전트가 전통적인 자동화 방법보다 해석 가능성과 적응성 면에서 우수하게 음악 패턴 인식, 작곡 매개변수화, 교육 피드백을 향상시켰다.

Conclusion: 책임감 있는 AI 배치를 필요로 하며, 기술적, 교육적, 윤리적 측면을 연결하는 통합 프레임워크를 제공한다.

Abstract: This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.

</details>


### [11] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: 이 논문은 모바일 애플리케이션의 페이지 설계를 자동화하기 위한 다중 에이전트 프레임워크인 APD-agents를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 디자인 과정은 시간이 많이 소모되고 협업 디자인이 추가적인 시간을 요구하기 때문에 자동화가 필요합니다.

Method: APD-agents는 OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, RecursiveComponentAgent로 구성된 다중 에이전트 프레임워크입니다.

Result: RICO 데이터셋에서 APD-agents는 최신 성능을 달성했습니다.

Conclusion: 이 연구는 대형 모델 기반 다중 에이전트 시스템의 자동 협업 능력을 완전히 활용합니다.

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [12] [PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval](https://arxiv.org/abs/2511.14130)
*Chun Chet Ng,Jia Yu Lim,Wei Zeng Low*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델을 활용한 금융 정보 검색의 중요성을 강조하고, PRISM이라는 훈련 없는 프레임워크를 제안하여 문서 및 청크 순위를 매기는 두 가지 작업을 통해 이 문제를 해결하는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 빠른 발전으로 인해 금융 정보 검색은 중요한 산업적 응용 분야가 되었습니다. 구체적으로 긴 금융 문서에서 작업 관련 정보를 추출하는 것은 운영 및 분석적 의사 결정에 필수적입니다.

Method: PRISM이라는 훈련 없는 프레임워크를 제시하며, 이 프레임워크는 정제된 시스템 프롬프트, 문맥 학습(ICL), 경량의 다중 에이전트 시스템을 통합합니다. 각 구성 요소는 그들의 시너지를 밝히기 위해 광범위하게 검토됩니다.

Result: 최고 구성은 제한된 검증 분할에서 NDCG@5 0.71818을 기록했습니다.

Conclusion: PRISM은 생산 규모의 금융 검색에 대해 실행 가능하고 견고한 것으로 입증되며, 모듈식이고 추론 전용 설계로 실제 사용 사례에서 유용합니다.

Abstract: With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.

</details>


### [13] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: R3라는 새로운 두 가지 프로세스 사고 프레임워크를 도입하여 LLM의 일반화 능력과 VLN 전문 지식을 결합하여 Vision-and-Language Navigation 성능을 크게 향상시킴.


<details>
  <summary>Details</summary>
Motivation: LLM이 인간의 지침을 따르며 복잡한 3D 환경을 탐색하는 VLN의 과제 해결 능력이 부족하기 때문에 이 문제를 해결하고자 함.

Method: R3라는 이중 과정 사고 프레임워크를 제안하며, Runner, Ruminator, Regulator의 세 가지 핵심 모듈로 구성됨.

Result: R3는 REVERIE 벤치마크에서 SPL과 RGSPL에서 각각 3.28% 및 3.30% 초과의 성능을 보여주어 다른 최신 방법들을 크게 초월함.

Conclusion: R3는 어려운 VLN 과제를 처리하는 데 효과적임을 강조함.

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [14] [Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems](https://arxiv.org/abs/2511.14136)
*Sushant Mehta*

Main category: cs.AI

TL;DR: 현재의 AI 벤치마크는 주로 정확도만 평가하며 기업의 요구사항을 간과한다. 12개의 주요 벤치마크 분석과 최신 에이전트 평가를 통해 비용 효율성, 신뢰성 및 운영 안정성을 개선하기 위한 프레임워크인 CLEAR를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 에이전틱 AI 벤치마크는 과제 완료 정확도만 평가하며, 비용 효율성, 신뢰성, 운영 안정성과 같은 중요한 기업 요구사항을 간과한다.

Method: 12개의 주요 벤치마크를 체계적으로 분석하고 최신 에이전트를 실증 평가하여 세 가지 근본적인 한계를 식별한다.

Result: 정확성만 최적화하는 것이 유사한 성과를 가진 비용 인식 대안보다 4.4-10.8배 더 비싼 에이전트를 초래함을 보여준다.

Conclusion: CLEAR는 정확성 기반 평가보다 생산 성공을 더 잘 예측한다.

Abstract: Current agentic AI benchmarks predominantly evaluate task completion accuracy, while overlooking critical enterprise requirements such as cost-efficiency, reliability, and operational stability. Through systematic analysis of 12 main benchmarks and empirical evaluation of state-of-the-art agents, we identify three fundamental limitations: (1) absence of cost-controlled evaluation leading to 50x cost variations for similar precision, (2) inadequate reliability assessment where agent performance drops from 60\% (single run) to 25\% (8-run consistency), and (3) missing multidimensional metrics for security, latency, and policy compliance. We propose \textbf{CLEAR} (Cost, Latency, Efficacy, Assurance, Reliability), a holistic evaluation framework specifically designed for enterprise deployment. Evaluation of six leading agents on 300 enterprise tasks demonstrates that optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance. Expert evaluation (N=15) confirms that CLEAR better predicts production success (correlation $ρ=0.83$) compared to accuracy-only evaluation ($ρ=0.41$).

</details>


### [15] [AutoTool: Efficient Tool Selection for Large Language Model Agents](https://arxiv.org/abs/2511.14650)
*Jingyi Jia,Qinbin Li*

Main category: cs.AI

TL;DR: AutoTool은 LLM 추론 비용을 줄이기 위해 도구 사용 관성이란 개념을 활용하는 그래프 기반 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 에이전트 프레임워크에서 도구 선택의 높은 추론 비용이 주요 병목 현상입니다.

Method: AutoTool은 과거 에이전트 경로로부터 지시 그래프를 구성하고, 도구를 선택하기 위해 이 구조화된 표현을 효율적으로 탐색합니다.

Result: AutoTool은 다양한 에이전트 작업에서 추론 비용을 최대 30% 줄이는 동시에 경쟁력 있는 작업 완료율을 유지합니다.

Conclusion: 통계적 구조를 LLM 에이전트 설계에 통합하여 성능을 희생하지 않으면서 더 높은 효율성을 제공하는 가능성을 강조합니다.

Abstract: Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.

</details>


### [16] [Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration](https://arxiv.org/abs/2511.14730)
*Parya Dolatyabi,Mahdi Khodayar*

Main category: cs.AI

TL;DR: Heterogeneous-Agent Reinforcement Learning (HARL) 프레임워크와 Heterogeneous-Agent Proximal Policy Optimization (HAPPO)을 활용하여 분산 전력 시스템의 복구를 효율적으로 수행하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 정전 후 전력 분배 시스템(PDS)의 복구는 비선형 제약 조건 하에서 전력 균형, 전압 한계 및 열 정격을 고려해야 하므로 복잡하다. 이는 전통적인 최적화 및 가치 기반 강화 학습(RL) 접근 방식의 계산 비효율성을 가져온다.

Method: HARL 프레임워크를 통해 HAPPO를 적용하여 서로 연결된 마이크로 그리드에서의 복구 작업을 조정한다. 각 에이전트는 서로 다른 부하, DER 용량 및 스위치 수를 가진 독립적인 마이크로 그리드를 제어한다.

Result: IEEE 123버스 및 IEEE 8500노드 시스템에서 HAPPO는 DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL 및 QMIX에 비해 빠른 수렴, 더 높은 회복 전력 및 더 부드러운 다중 시드 훈련을 달성했다.

Conclusion: HARL 프레임워크 내에서 마이크로 그리드 수준의 이질성을 포함함으로써 복잡한 PDS 복구를 위한 확장 가능하고 안정적이며 제약을 인식하는 솔루션을 제공한다.

Abstract: Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments](https://arxiv.org/abs/2511.13788)
*Samuel Nathanson,Rebecca Williams,Cynthia Matuszek*

Main category: cs.LG

TL;DR: 이 연구는 대형 언어 모델이 작은 모델을 체계적으로 조종할 수 있는지를 조사하며, 모델 간 상호작용이 취약성의 확장성에 미치는 영향을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 다중 에이전트 및 안전성이 중요한 환경에서 점점 더 운영됨에 따라, 모델이 적대적으로 상호작용할 때 그 취약성의 확장성에 대한 질문이 제기되고 있다.

Method: JailbreakBench의 표준화된 적대적 작업을 사용하여, 주요 LLM 계열과 규모(0.6B-120B 매개변수)에서 6,000건 이상의 다중 턴 공격자-대상 교환을 시뮬레이션하고, 위험 점수와 거절 행동을 측정하여 적대적 효능과 정렬 무결성의 지표를 평가한다.

Result: 평균 위험과 공격자-대상 크기 비율의 로그 간에 강한 통계적 상관관계를 발견하였으며(Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), 이는 상대 모델 크기가 유해한 완성의 가능성과 심각성과 연관되어 있음을 나타낸다.

Conclusion: 이러한 발견은 크기 비대칭이 강인성에 영향을 미친다는 것을 보여주며, 모델 간의 정렬과 안전성에 대한 보다 통제된 연구를 촉진하는 탐색적인 증거를 제공한다.

Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.

</details>


### [18] [Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation](https://arxiv.org/abs/2511.14135)
*Promise Ekpo,Saesha Agarwal,Felix Grimm,Lekan Molu,Angelique Taylor*

Main category: cs.LG

TL;DR: 이 연구는 다수의 에이전트 간의 공정한 작업 부하 분배를 달성하기 위한 마르(MARL) 접근 방식의 한계를 극복하고, 자율적으로 공정성을 보장하는 방법을 제시하여 다중 에이전트 학습 기반의 헬스케어 시스템에서의 성능을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 헬스케어 근무 환경에서 안정적이고 신뢰할 수 있는 성과를 보장하기 위해 여러 에이전트 간의 공정한 작업 부하 할당을 시행하는 것이 중요합니다.

Method: 각 에이전트가 자원을 공유하는 설정에서, 자율적인 의사 결정자가 상호 작용하여 효율적인 균형 상태를 달성하는 제약이 있는 일반화된 내시 균형(GNE) 게임 이론적 프레임워크를 사용하여 공정성을 강조합니다.

Result: Fair-GNE는 고정 패널티 기준선에 비해 작업 부하 균형에서 유의미한 개선을 달성하였고, 86%의 작업 성공률을 유지하며 공정성 향상을 입증했습니다.

Conclusion: 우리의 연구 결과는 여러 에이전트 학습 기반 헬스케어 시스템에서의 공정성 강화를 명확하게 전달합니다.

Abstract: Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\ 0.33 JFI, $p < 0.01$) while maintaining 86\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.

</details>


### [19] [Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement](https://arxiv.org/abs/2511.13755)
*Zhe Yang,Wenrui Li,Hongtao Chen,Penghong Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.LG

TL;DR: 이 논문에서는 다중 모달 학습에서 나타나는 모달 편향 문제를 해결하기 위해 Adaptive Redundancy Regulation 기법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 모달 학습은 여러 출처에서 데이터를 활용하여 성능을 향상시키고자 합니다. 그러나 주 모달리티의 지배적인 위치로 인해 역전파에서 균형 잡히지 않은 최적화 문제가 발생합니다.

Method: Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg)를 제안하며, 이는 정보 병목 원칙에 영감을 받았습니다. 이를 통해 고효율 이득 증가율과 중복성의 공동 기준을 사용하여 중복성이 높을 때만 개입을 유도하는 중복성 모니터를 구축합니다.

Result: 실험 결과, 우리의 방법이 대부분의 시나리오에서 현재의 주요 방법들 중에서 우수성을 보임을 나타내며, ablation 실험을 통해 방법의 효과를 검증했습니다.

Conclusion: 이 방법은 모달리티에 특화된 정보를 보존하고 모달 간 의미론적 관계를 고려하여 더 나은 성능을 발휘합니다.

Abstract: Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git

</details>


### [20] [Multi-Horizon Time Series Forecasting of non-parametric CDFs with Deep Lattice Networks](https://arxiv.org/abs/2511.13756)
*Niklas Erdmann,Lars Bentsen,Roy Stenbro,Heine Nygard Riise,Narada Dilp Warakagoda,Paal E. Engelstad*

Main category: cs.LG

TL;DR: 확률적 예측은 미래 예측에 더 많은 정보를 추가하고 포인트 예측의 약점을 보완하는 방법입니다. 본 논문에서는 시간 시계열 예측을 위해 모노토닉 제약이 있는 동시/암시적 분위수 회귀를 위한 깊은 격자망의 적응을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 확률적 예측과 모노토닉 네트워크의 분야를 연결하여 연구를 발전시키고, 암시적이고 완전하며 비모수적인 누적 분포 함수를 예측하는 접근 방식을 제안합니다.

Method: 모노토닉 제약을 가진 깊은 격자망을 적응하여 동시/암시적 분위수 회귀를 수행합니다. LSTM을 활용하고 분위수 입력을 모든 서브 격자로 확산시켜 누적 분포 함수의 다중 지평 예측을 생성합니다.

Result: 적응된 깊은 격자망이 제한이 없는 접근 방식과 비슷하거나 더 나은 성능을 보였으며, 확장 가능 모노토닉 신경망과 비교할 때 성능이 더 뛰어났습니다.

Conclusion: 모노토닉 신경망과 확률적 예측 기술에 대한 연구를 촉진하는 데 기여하고자 합니다.

Abstract: Probabilistic forecasting is not only a way to add more information to a prediction of the future, but it also builds on weaknesses in point prediction. Sudden changes in a time series can still be captured by a cumulative distribution function (CDF), while a point prediction is likely to miss it entirely. The modeling of CDFs within forecasts has historically been limited to parametric approaches, but due to recent advances, this no longer has to be the case. We aim to advance the fields of probabilistic forecasting and monotonic networks by connecting them and propose an approach that permits the forecasting of implicit, complete, and nonparametric CDFs. For this purpose, we propose an adaptation to deep lattice networks (DLN) for monotonically constrained simultaneous/implicit quantile regression in time series forecasting. Quantile regression usually produces quantile crossovers, which need to be prevented to achieve a legitimate CDF. By leveraging long short term memory units (LSTM) as the embedding layer, and spreading quantile inputs to all sub-lattices of a DLN with an extended output size, we can produce a multi-horizon forecast of an implicit CDF due to the monotonic constraintability of DLNs that prevent quantile crossovers. We compare and evaluate our approach's performance to relevant state of the art within the context of a highly relevant application of time series forecasting: Day-ahead, hourly forecasts of solar irradiance observations. Our experiments show that the adaptation of a DLN performs just as well or even better than an unconstrained approach. Further comparison of the adapted DLN against a scalable monotonic neural network shows that our approach performs better. With this adaptation of DLNs, we intend to create more interest and crossover investigations in techniques of monotonic neural networks and probabilistic forecasting.

</details>


### [21] [Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection](https://arxiv.org/abs/2511.13759)
*Han Wang,Deyi Ji,Junyu Lu,Lanyun Zhu,Hailong Zhang,Haiyang Wu,Liqun Liu,Peng Shu,Roy Ka-Wei Lee*

Main category: cs.LG

TL;DR: 본 연구는 낮은 자원 문제를 해결하기 위해 협력적 가상 레이블링을 통해 풍부한 비표시된 데이터를 활용하는 자기 훈련 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 소셜 미디어에서 공격적인 콘텐츠를 정확하게 탐지하기 위해서는 고품질 레이블이 있는 데이터가 필요하지만, 이를 확보하기 위한 작업은 비용이 높고 공격적인 사례가 드물기 때문에 데이터가 부족합니다.

Method: 제한된 레이블 데이터로 훈련된 경량 분류기를 시작으로, 각 기법에서 생성된 가상 레이블을 반복적으로 비표시 데이터에 할당하며, 다중 에이전트 비전-언어 모델(진행자를 모방한 이중 관점을 캡처) 지원을 이용합니다.

Result: 실험 결과, 제안한 방법이 제한된 감독 하에서 기준 대비 상당한 성능 향상을 보였으며, 대규모 모델들의 성능에 근접하는 결과를 나타냈습니다.

Conclusion: 제안한 자기 훈련 프레임워크는 비표시 데이터에서의 신뢰성 있는 레이블 수집과 성능 개선을 가능하게 합니다.

Abstract: Accurate detection of offensive content on social media demands high-quality labeled data; however, such data is often scarce due to the low prevalence of offensive instances and the high cost of manual annotation. To address this low-resource challenge, we propose a self-training framework that leverages abundant unlabeled data through collaborative pseudo-labeling. Starting with a lightweight classifier trained on limited labeled data, our method iteratively assigns pseudo-labels to unlabeled instances with the support of Multi-Agent Vision-Language Models (MA-VLMs). Un-labeled data on which the classifier and MA-VLMs agree are designated as the Agreed-Unknown set, while conflicting samples form the Disagreed-Unknown set. To enhance label reliability, MA-VLMs simulate dual perspectives, moderator and user, capturing both regulatory and subjective viewpoints. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which jointly exploits labeled, Agreed-Unknown, and Disagreed-Unknown data while mitigating pseudo-label noise. Experiments on benchmark datasets demonstrate that our framework substantially outperforms baselines under limited supervision and approaches the performance of large-scale models

</details>


### [22] [Library Liberation: Competitive Performance Matmul Through Compiler-composed Nanokernels](https://arxiv.org/abs/2511.13764)
*Arun Thangamani,Md Asghar Ahmad Shahid,Adam Siemieniuk,Rolf Morel,Renato Golin,Alexander Heinecke*

Main category: cs.LG

TL;DR: AI 및 머신러닝 작업의 발전으로 고수준 도메인 작업과 하드웨어의 효율적인 활용 사이의 격차가 커졌다. 이 논문은 MLIR 다이얼렉트를 활용해 자동으로 확장 가능하고 고성능의 마이크로커널을 생성하는 컴파일러 설계 방안을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI와 머신러닝의 발전이 하드웨어 활용의 복잡성을 증가시키고 있다.

Method: MLIR 다이얼렉트를 활용하여 도메인 수준 연산과 프로세서 기능을 연결하는 고성능의 마이크로커널을 자동 생성하는 컴파일러를 개발하였다.

Result: 생성된 나노커널은 생산 품질을 갖추고 있으며 최신 마이크로커널 라이브러리와 경쟁력이 있다.

Conclusion: 이 접근법은 저수준 라이브러리에 대한 의존성을 제거하고, 직접 최적화된 코드를 자동 생성할 수 있게 해준다.

Abstract: The rapidly evolving landscape of AI and machine learning workloads has widened the gap between high-level domain operations and efficient hardware utilization. Achieving near-peak performance still demands deep hardware expertise-experts either handcraft target-specific kernels (e.g., DeepSeek) or rely on specialized libraries (e.g., CUTLASS)-both of which add complexity and limit scalability for most ML practitioners.
  This paper introduces a compilation scheme that automatically generates scalable, high-performance microkernels by leveraging the MLIR dialects to bridge domain-level operations and processor capabilities. Our approach removes dependence on low-level libraries by enabling the compiler to auto-generate near-optimal code directly. At its core is a mechanism for composing nanokernels from low-level IR constructs with near-optimal register utilization, forming efficient microkernels tailored to each target. We implement this technique in an MLIR-based compiler supporting both vector and tile based CPU instructions. Experiments show that the generated nanokernels are of production-quality, and competitive with state-of-the-art microkernel libraries.

</details>


### [23] [MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts](https://arxiv.org/abs/2511.14102)
*Wenfeng Wang,Jiacheng Liu,Xiaofeng Hou,Xinfeng Xia,Peng Tang,Mingxuan Zhang,Chao Li,Minyi Guo*

Main category: cs.LG

TL;DR: 최첨단 혼합 전문가 모델의 메모리 요구 사항을 해결하기 위한 새로운 추론 시스템 MoE-SpeQ를 제안하며, 이러한 시스템이 I/O 병목 현상을 극복하고 성능을 극대화하는 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 최첨단 혼합 전문가 모델의 메모리 요구 사항이 단일 가속기의 용량을 초과하여 추론에 상당한 도전을 제기함.

Method: MoE-SpeQ는 비용이 저렴한 장치 내 계산을 통해 데이터 이동의 막대한 비용을 숨기는 것을 목표로 하며, 전문가 예측을 위한 작은 장치 내 초안 모델을 사용한다.

Result: MoE-SpeQ는 Phi-MoE 모델에서 기존 오프로드 프레임워크보다 최대 2.34배의 속도 향상을 달성하였다.

Conclusion: 이 연구는 자원 제한 환경에서 데이터 의존 메모리 접근 관리를 위한 새로운 접근 방식을 확립하여 MoE 추론을 일반 하드웨어에서 보다 더 접근 가능하게 만들었다.

Abstract: The immense memory requirements of state-of-the-art Mixture-of-Experts (MoE) models present a significant challenge for inference, often exceeding the capacity of a single accelerator. While offloading experts to host memory is a common solution, it introduces a severe I/O bottleneck over the PCIe bus, as the data-dependent nature of expert selection places these synchronous transfers directly on the critical path of execution, crippling performance.
  This paper argues that the I/O bottleneck can be overcome by trading a small amount of cheap, on-device computation to hide the immense cost of data movement. We present MoE-SpeQ, a new inference system built on a novel co-design of speculative execution and expert offloading. MoE-SpeQ employs a small, on-device draft model to predict the sequence of required experts for future tokens. This foresight enables a runtime orchestrator to prefetch these experts from host memory, effectively overlapping the expensive I/O with useful computation and hiding the latency from the critical path. To maximize performance, an adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the underlying hardware. Our evaluation on memory-constrained devices shows that for the Phi-MoE model, MoE-SpeQ achieves at most 2.34x speedup over the state-of-the-art offloading framework. Our work establishes a new, principled approach for managing data-dependent memory access in resource-limited environments, making MoE inference more accessible on commodity hardware.

</details>


### [24] [N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator](https://arxiv.org/abs/2511.14195)
*Zheyu Lin,Jirui Yang,Hengqi Guo,Yubing Bao,Yao Guan*

Main category: cs.LG

TL;DR: LLM의 안전성을 평가하는 N-GLARE 방법론을 제안하며, 텍스트 생성 없이 모델의 잠재 표현을 분석하여 안전성을 평가한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 안전성을 평가하는 것은 그 배포에 있어 매우 중요하지만 기존 방법은 비용이 높고 피드백 지연 문제로 인해 적합하지 않다.

Method: N-GLARE는 모델의 잠재 표현만을 사용하여 작업하며, APT와 JSS 지표를 도입하여 숨겨진 계층의 동역학을 특징짓는다.

Result: 40개 이상의 모델과 20개의 레드 팀 전략에 대한 실험 결과, JSS 지표가 레드 팀 평가에 의해 도출된 안전성 순위와 높은 일관성을 보인다.

Conclusion: N-GLARE는 1% 이하의 토큰 비용과 실행 비용으로 대규모 레드 팀 테스트의 판별 트렌드를 재현하여 실시간 진단을 위한 효율적인 평가 프록시를 제공한다.

Abstract: Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.

</details>


### [25] [Object-Centric World Models for Causality-Aware Reinforcement Learning](https://arxiv.org/abs/2511.14262)
*Yosuke Nishimoto,Takashi Matsubara*

Main category: cs.LG

TL;DR: STICA는 객체 중심의 Transformer를 세계 모델로 사용하는 통합 프레임워크로, 샘플 효율성을 높이고 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 고차원적이고 비정상적인 환경을 정확하게 복제하는 것은 도전적이며, 인간은 환경을 이산 객체로 분해하여 효율적인 의사결정을 내립니다.

Method: STICA는 객체 중심의 토큰으로 각 관찰을 나타내며, 정책과 가치 네트워크는 인과 관계를 추정하여 의사결정에 활용합니다.

Result: STICA는 객체가 풍부한 벤치마크에서 샘플 효율성과 최종 성능 모두에서 최신 기술의 에이전트를 일관되게 능가합니다.

Conclusion: STICA는 샘플 효율성과 성능을 모두 개선할 수 있는 새로운 접근 방식을 제시합니다.

Abstract: World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.

</details>


### [26] [FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis](https://arxiv.org/abs/2511.14419)
*Xiaowei Xu,Justin Sonneck,Hongxiao Wang,Roman Burkard,Hendrik Wohrle,Anton Grabmasier,Matthias Gunzer,Jianxu Chen*

Main category: cs.LG

TL;DR: 세포 이동 분석을 위한 고속 이미지 압축 프레임워크인 FlowRoI를 소개하며, 이는 셀 이동 연구에서의 데이터 저장 및 전송 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 면역 세포의 자율 이동은 중요하지만, 데이터 생성 속도가 빨라 저장 및 전송에서 문제가 발생한다.

Method: FlowRoI는 연속 프레임 간의 광학 흐름을 추정하고, 이동하는 세포를 커버하는 RoI 마스크를 추출하여 JPEG2000을 사용해 압축한다.

Result: FlowRoI는 현대 노트북에서 약 30fps로 작동하며, PSNR 개선과 함께 JPEG2000보다 2.0-2.2배 높은 압축률을 달성한다.

Conclusion: FlowRoI는 면역 세포 이동 분석에서의 저장 및 전송 문제를 효과적으로 해결하며, 높은 압축률과 이미지 품질을 제공한다.

Abstract: Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases. Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells. This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation. However, ComplexEye and similar high-throughput imaging platforms generate data at an exponential rate, imposing substantial burdens on storage and transmission. To address this challenge, we present FlowRoI, a fast optical-flow-based region of interest (RoI) extraction framework designed for high-throughput image compression in immune cell migration studies. FlowRoI estimates optical flow between consecutive frames and derives RoI masks that reliably cover nearly all migrating cells. The raw image and its corresponding RoI mask are then jointly encoded using JPEG2000 to enable RoI-aware compression. FlowRoI operates with high computational efficiency, achieving runtimes comparable to standard JPEG2000 and reaching an average throughput of about 30 frames per second on a modern laptop equipped with an Intel i7-1255U CPU. In terms of image quality, FlowRoI yields higher peak signal-to-noise ratio (PSNR) in cellular regions and achieves 2.0-2.2x higher compression rates at matched PSNR compared to standard JPEG2000.

</details>


### [27] [nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers](https://arxiv.org/abs/2511.14465)
*Clément Dumas*

Main category: cs.LG

TL;DR: nnterp는 다양한 아키텍처에 걸쳐 변환기 분석을 위한 통합 인터페이스를 제공하는 경량 래퍼입니다.


<details>
  <summary>Details</summary>
Motivation: 변환기 내부 분석을 위한 신뢰할 수 있는 도구가 필요합니다.

Method: nnterp는 NNsight 위에 경량 래퍼를 개발하여 변환기 분석을 위한 통합된 인터페이스를 제공합니다.

Result: nnterp를 통해 연구자들은 50개 이상의 모델 변형에 대해 intervention 코드를 한 번 작성하고 배포할 수 있습니다.

Conclusion: nnterp는 기계적 해석 도구의 정확성과 사용성 간의 격차를 해소합니다.

Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.

</details>


### [28] [ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents](https://arxiv.org/abs/2511.14584)
*Ankush Kadu,Ashwanth Krishnan*

Main category: cs.LG

TL;DR: 이 논문은 ReflexGrad라는 새로운 아키텍처를 제안하며, 경험으로부터 학습하고 다양한 작업에서 일반화하는 에이전트의 능력을 향상시키는 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 경험으로부터 학습하고 다양한 작업에서 일반화하는 에이전트의 개발은 강화 학습 및 의사결정 분야에서 중요한 도전 과제이다.

Method: ReflexGrad는 전략 계획을 위한 LLM 기반의 계층적 TODO 분해, 최근 행동 패턴을 분석하여 실패 원인을 식별하고 학습을 가능하게 하는 역사 인식 인과 반영, 체계적 개선을 위한 기울기 기반 최적화를 결합한 아키텍처이다.

Result: ReflexGrad는 ALFWorld 벤치마크 작업에서 Trial 0에서 67%의 제로 샷 성공률을 달성했으며, 이는 사전 작업 경험이나 시연 없이 이루어졌다.

Conclusion: 상호 보완적인 학습 메커니즘의 통합이 강력한 제로 샷 일반화를 가능하게 하여 이전 작업의 몇 샷 기준에 접근할 수 있음을 보여준다.

Abstract: Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.

</details>
