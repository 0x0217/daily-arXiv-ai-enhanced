<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Indispensable Role of User Simulation in the Pursuit of AGI](https://arxiv.org/abs/2509.19456)
*Krisztian Balog,ChengXiang Zhai*

Main category: cs.AI

TL;DR: AGI 개발의 병목 현상을 극복하기 위해 사용자 시뮬레이션의 중요성을 강조하고, 현실적인 시뮬레이터의 필요성을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: AGI로의 진전은 복잡한 인터랙티브 시스템의 평가와 적응형 에이전트 훈련에 필요한 방대한 상호작용 데이터 확보에서 중요한 병목 현장을 맞고 있다.

Method: 본 논문은 사용자 시뮬레이션이 AGI 개발을 가속화하는 데 중요한 촉매 역할을 한다고 주장한다.

Result: 현실적인 시뮬레이터는 대규모 평가, 인터랙티브 학습을 위한 데이터 생성, AGI의 중심인 적응 능력 육성을 위한 필수 환경을 제공한다.

Conclusion: 사용자 시뮬레이션 기술과 지능형 작업 에이전트에 대한 연구는 상호 보완적이며 함께 발전해야 한다.

Abstract: Progress toward Artificial General Intelligence (AGI) faces significant
bottlenecks, particularly in rigorously evaluating complex interactive systems
and acquiring the vast interaction data needed for training adaptive agents.
This paper posits that user simulation -- creating computational agents that
mimic human interaction with AI systems -- is not merely a useful tool, but is
a critical catalyst required to overcome these bottlenecks and accelerate AGI
development. We argue that realistic simulators provide the necessary
environments for scalable evaluation, data generation for interactive learning,
and fostering the adaptive capabilities central to AGI. Therefore, research
into user simulation technology and intelligent task agents are deeply
synergistic and must advance hand-in-hand. This article elaborates on the
critical role of user simulation for AGI, explores the interdisciplinary nature
of building realistic simulators, identifies key challenges including those
posed by large language models, and proposes a future research agenda.

</details>


### [2] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: 대규모 언어 모델(LLMs)의 성능과 동적 정보 환경에서의 취약성 간의 격차를 해소하기 위해, 본 연구는 계산적 인지 부하 이론을 제안하며, 이를 통해 성능 저하의 주요 메커니즘인 과도한 정보와 과제 전환의 간섭을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 성능이 정적 벤치마크에서는 뛰어나지만 동적 환경에서는 취약하다는 점을 이해하고, 인지 부하가 이들 모델의 추론 능력에 미치는 영향을 규명하고자 한다.

Method: Interleaved Cognitive Evaluation (ICE)라는 벤치마크를 설계하여 복잡한 다단계 추론 과제에서 인지 부하 요소를 체계적으로 조작하였다.

Result: 200개의 질문에 대해 10회의 반복 실험을 통해 다섯 개의 지침 조정 모델에서 성능 차이를 발견하였으며, 소형 오픈 소스 아키텍처는 모든 조건에서 0% 정확도를 나타냈다.

Conclusion: 동적이고 인지 인식을 고려한 스트레스 테스트는 고급 AI 시스템의 진정한 회복력과 안전성을 평가하는 데 필수적이다.

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [3] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: 이 논문에서는 100억 미만의 소형 언어 모델을 유전자 질문 응답에 적용하여 환각 문제와 계산 비용 과제를 해결하기 위한 에이전트 프레임워크를 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 작은 언어 모델을 이용하여 유전자 데이터 관련 질문에 대한 답변의 정확성과 비용 효율성을 높이기 위함입니다.

Method: Nano Bio-Agent (NBA) 프레임워크를 구현하여 작업 분해, 도구 조정 및 NCBI와 AlphaGenome과 같은 기존 시스템에 대한 API 접근을 통합했습니다.

Result: 우리의 최상의 모델-에이전트 조합은 GeneTuring 벤치마크에서 98%의 정확도를 달성했으며, 소형 모델은 85-97%의 정확도를 일관되게 달성했습니다.

Conclusion: 소형 언어 모델은 효율성 향상, 비용 절감, ML 기반 유전자 도구의 민주화를 위한 유망한 가능성을 보여줍니다.

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [4] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL은 사용자 중심 능력을 훈련하고 평가하기 위한 통합 프레임워크로, 다양한 상호작용에서의 학습을 개선하는 방법론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 사용자 상호작용의 다양성과 동적 특성이 에이전트의 유용성에 도전 과제를 제기함에 따라, 사용자 보조에 중점을 둔 에이전트를 개발할 필요가 있다.

Method: 표준화된 체육관 환경과 시뮬레이션 된 사용자를 결합하여 턴-레벨 보상 할당 및 경로-레벨 점수 계산을 체계적으로 변화시킨다.

Result: Qwen3 모델에 대한 실험 결과, 초기 상호작용 능력을 열기 위해 SFT 콜드 스타트가 중요하며, 의도적인 경로 점수가 효율적인 다중 턴 상호작용을 낳고, 강력한 시뮬레이터 사용자(예: GPT-4o)가 훈련을 촉진하지만, 오픈 소스 시뮬레이터는 비용 효율적이고 전이 가능한 옵션임을 강조한다.

Conclusion: 보상 형성 및 사용자 시뮬레이션 선택의 신중한 설계가 모델 규模만큼 중요하다는 것을 강조하며, UserRL을 사용자 중심 에이전트 모델 개발을 위한 실용적인 경로로 설정한다.

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [5] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 이 논문은 효율적인 추론 작업을 위해 최적화된 워크플로를 소개하여 소형 오픈 소스 모델이 더 큰 모델을 능가할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 현대 LLM의 추론은 내부 모델 훈련과 외부 에이전트 조정에 의해 추진되는 광범위한 테스트 시간 계산에 의존하지만, 효율성 부족으로 인해 계산 자원이 낭비되는 문제가 있다.

Method: 이 논문에서는 최적화된 추론 워크플로(epo)를 도입하여 소형 오픈 소스 모델이 그 크기보다 몇 배 큰 모델보다 우수한 성능을 발휘하도록 한다.

Result: 최적화된 워크플로를 통해 모델은 성능을 높이고 효율성을 개선한다.

Conclusion: 이 연구는 기본 모델 기능과 함께 조정 프레임워크를 공동 설계하여 소형 및 중형 모델에서 강력한 추론을 가능하게 하는 명확한 경로를 제시한다.

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [6] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 자율 에이전트의 비결정론적 특성이 신뢰성 문제를 야기한다. 이 보고서는 에이전트를 모니터링하는 메타인지 계층을 통합해 이러한 문제를 해결하는 새로운 아키텍처 패턴을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트의 신뢰성을 높이고 사용자 불만을 줄이기 위해.

Method: 에이전트의 작업 실패를 예측하는 메타인지 계층을 통합하고, 예측 시 인간 담당자에게 프로세스 요약 및 실패 이유를 설명한다.

Result: 프로토타입 시스템의 실증 분석을 통해 전체 작업 성공률이 유의미하게 증가함을 보여준다.

Conclusion: 메타인지 계층이 시스템의 복원력, 사용자 경험 및 신뢰성을 강화하는 핵심 설계 요소임을 시사하며, 관련 연구 방향을 논의한다.

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [7] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 본 논문은 Embodied AI의 발전을 탐구하고 LLM과 WM의 역할을 강조하며, 복잡한 작업을 가능하게 하는 공동 구조의 필요성을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: Embodied AI는 인공지능의 진화를 이끄는 중요한 패러다임으로, AGI를 달성하기 위한 기초를 구축하고 있습니다.

Method: 이 논문은 Embodied AI의 기본부터 고급 개념까지 문헌을 포괄적으로 탐구하고, LLM과 WM 기반의 연구를 다룹니다.

Result: LLM과 WM이 Embodied AI에 기여하는 바를 세밀하게 설명하고, 실질적인 응용 사례를 제시합니다.

Conclusion: Embodied AI는 미래 연구 방향을 제시하며, 실제 세계에서의 광범위한 적용 가능성을 보여줍니다.

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>


### [8] [MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM](https://arxiv.org/abs/2509.20067)
*Wenliang Li,Rui Yan,Xu Zhang,Li Chen,Hongji Zhu,Jing Zhao,Junjun Li,Mengru Li,Wei Cao,Zihang Jiang,Wei Wei,Kun Zhang,Shaohua Kevin Zhou*

Main category: cs.AI

TL;DR: 새로운 다중 에이전트 임상 진단 프레임워크는 대규모 언어 모델이 스스로 임상 지식을 학습하게 하여 진단 정확성을 크게 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 프롬프트 방법으로는 복잡한 임상 진단을 효과적으로 처리하는 데 한계가 있으며, 재사용 가능한 임상 경험의 축적이 간과되고 있다.

Method: MACD 프레임워크를 제안하여 다중 에이전트 파이프라인을 통해 진단 통찰을 요약, 정제 및 적용하며, 반복 상담을 지원하는 평가자 에이전트와 인간의 감독을 포함하는 MACD-인간 협업 워크플로우를 확장하였다.

Result: MACD는 4,390개의 실제 환자 사례에서 기존 임상 가이드라인보다 최대 22.3% 개선된 진단 정확성을 보였다. 의사와의 진단 성능도 비슷하거나 초과달성하였다.

Conclusion: 이 연구는 LLM 지원 진단을 위한 확장 가능한 자가 학습 패러다임을 제시하고 있으며, LLM의 내재적 지식과 실제 임상 실무 간의 간극을 줄이고 있다.

Abstract: Large language models (LLMs) have demonstrated notable potential in medical
applications, yet they face substantial challenges in handling complex
real-world clinical diagnoses using conventional prompting methods. Current
prompt engineering and multi-agent approaches typically optimize isolated
inferences, neglecting the accumulation of reusable clinical experience. To
address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)
framework, which allows LLMs to self-learn clinical knowledge via a multi-agent
pipeline that summarizes, refines, and applies diagnostic insights. It mirrors
how physicians develop expertise through experience, enabling more focused and
accurate diagnosis on key disease-specific cues. We further extend it to a
MACD-human collaborative workflow, where multiple LLM-based diagnostician
agents engage in iterative consultations, supported by an evaluator agent and
human oversight for cases where agreement is not reached. Evaluated on 4,390
real-world patient cases across seven diseases using diverse open-source LLMs
(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves
primary diagnostic accuracy, outperforming established clinical guidelines with
gains up to 22.3% (MACD). On the subset of the data, it achieves performance on
par with or exceeding that of human physicians (up to 16% improvement over
physicians-only diagnosis). Additionally, on the MACD-human workflow, it
achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,
self-learned knowledge exhibits strong cross-model stability, transferability,
and model-specific personalization, while the system can generate traceable
rationales, enhancing explainability. Consequently, this work presents a
scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap
between the intrinsic knowledge of LLMs and real-world clinical practice.

</details>


### [9] [From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms](https://arxiv.org/abs/2509.20095)
*Aymeric Vellinger,Nemanja Antonic,Elio Tuci*

Main category: cs.AI

TL;DR: 이 연구는 페로몬 매개 집합과 강화 학습 간의 이론적 동등성을 확립하고, 스웜 인텔리전스의 원리를 설명하며, 집단적 문제 해결 능력을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 스웜 인텔리전스는 단순한 에이전트 간의 분산된 상호작용에서 발생하며, 집단적 문제 해결을 가능하게 한다.

Method: 엔지니어링된 선충 스웜을 모델링하여 채집 작업을 수행하면서, 페로몬 동역학이 기본적인 RL 알고리즘인 교차 학습 업데이트를 수학적으로 반영함을 보여준다.

Result: 실험적 검증을 통해 모델이 정적 조건에서 celeg의 채집 패턴을 정확하게 재현함을 확인하였다.

Conclusion: 이 연구는 스티그머지 시스템이 자연스럽게 분산된 RL 프로세스를 인코딩하며, 환경 신호가 집단적 신용 부여를 위한 외부 메모리 역할을 함을 보여준다.

Abstract: Swarm intelligence emerges from decentralised interactions among simple
agents, enabling collective problem-solving. This study establishes a
theoretical equivalence between pheromone-mediated aggregation in \celeg\ and
reinforcement learning (RL), demonstrating how stigmergic signals function as
distributed reward mechanisms. We model engineered nematode swarms performing
foraging tasks, showing that pheromone dynamics mathematically mirror
cross-learning updates, a fundamental RL algorithm. Experimental validation
with data from literature confirms that our model accurately replicates
empirical \celeg\ foraging patterns under static conditions. In dynamic
environments, persistent pheromone trails create positive feedback loops that
hinder adaptation by locking swarms into obsolete choices. Through
computational experiments in multi-armed bandit scenarios, we reveal that
introducing a minority of exploratory agents insensitive to pheromones restores
collective plasticity, enabling rapid task switching. This behavioural
heterogeneity balances exploration-exploitation trade-offs, implementing
swarm-level extinction of outdated strategies. Our results demonstrate that
stigmergic systems inherently encode distributed RL processes, where
environmental signals act as external memory for collective credit assignment.
By bridging synthetic biology with swarm robotics, this work advances
programmable living systems capable of resilient decision-making in volatile
environments.

</details>


### [10] [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
*Lorenzo Giusti,Ole Anton Werner,Riccardo Taiello,Matilde Carvalho Costa,Emre Tosun,Andrea Protani,Marc Molina,Rodrigo Lopes de Almeida,Paolo Cacace,Diogo Reis Santos,Luigi Serio*

Main category: cs.AI

TL;DR: FoA는 정적 다중 에이전트 조정을 동적 협업으로 전환하는 분산 오케스트레이션 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 정적 다중 에이전트 시스템의 한계를 극복하고, 에이전트들이 자신의 능력에 따라 동적으로 협력할 수 있는 방법을 제시하기 위해.

Method: FoA는 버전 관리된 능력 벡터(VCVs)를 활용하여 에이전트의 능력을 기계 판독 가능한 프로필로 정의하고, 의미적 라우팅 및 동적 작업 분해를 통한 협업을 지원한다.

Result: HealthBench에서의 평가 결과, FoA는 단일 모델 기준에 비해 13배의 성능 향상을 보였으며, 복잡한 추론 작업에 특히 효과적이다.

Conclusion: FoA는 비슷한 하위 작업을 수행하는 에이전트들을 클러스터링하여 협업 채널을 형성하고, 이를 통해 AI 에이전트 집단의 집단 지능을 극대화할 수 있음을 보여준다.

Abstract: We present Federation of Agents (FoA), a distributed orchestration framework
that transforms static multi-agent coordination into dynamic, capability-driven
collaboration. FoA introduces Versioned Capability Vectors (VCVs):
machine-readable profiles that make agent capabilities searchable through
semantic embeddings, enabling agents to advertise their capabilities, cost, and
limitations. Our aarchitecturecombines three key innovations: (1) semantic
routing that matches tasks to agents over sharded HNSW indices while enforcing
operational constraints through cost-biased optimization, (2) dynamic task
decomposition where compatible agents collaboratively break down complex tasks
into DAGs of subtasks through consensus-based merging, and (3) smart clustering
that groups agents working on similar subtasks into collaborative channels for
k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe
semantics for scalable message passing, FoA achieves sub-linear complexity
through hierarchical capability matching and efficient index maintenance.
Evaluation on HealthBench shows 13x improvements over single-model baselines,
with clustering-enhanced laboration particularly effective for complex
reasoning tasks requiring multiple perspectives. The system scales horizontally
while maintaining consistent performance, demonstrating that semantic
orchestration with structured collaboration can unlock the collective
intelligence of heterogeneous federations of AI agents.

</details>


### [11] [Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent](https://arxiv.org/abs/2509.20270)
*Xingjian Kang,Linda Vorberg,Andreas Maier,Alexander Katzmann,Oliver Taubmann*

Main category: cs.AI

TL;DR: 본 연구에서는 CT 스캔 프로토콜 관리의 비효율성을 해결하기 위해 LLM 기반 에이전트 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: CT 스캔 프로토콜 관리에는 많은 시간과 전문성이 요구되며, 기술자의 인력이 부족한 상황입니다.

Method: 자연어 또는 구조화된 형식으로 주어진 프로토콜 구성 요청을 해석하고 실행하기 위해 LLM 기반 에이전트를 개발하였습니다.

Result: 실험 결과, 에이전트는 프로토콜 구성 요소를 효과적으로 검색하고, 장치에 호환되는 프로토콜 정의 파일을 생성하며, 사용자 요청을 충실히 실행할 수 있음을 보여주었습니다.

Conclusion: LLM 기반 에이전트가 CT 이미징에서 스캔 프로토콜 관리를 지원할 수 있는 가능성을 제시합니다.

Abstract: Managing scan protocols in Computed Tomography (CT), which includes adjusting
acquisition parameters or configuring reconstructions, as well as selecting
postprocessing tools in a patient-specific manner, is time-consuming and
requires clinical as well as technical expertise. At the same time, we observe
an increasing shortage of skilled workforce in radiology. To address this
issue, a Large Language Model (LLM)-based agent framework is proposed to assist
with the interpretation and execution of protocol configuration requests given
in natural language or a structured, device-independent format, aiming to
improve the workflow efficiency and reduce technologists' workload. The agent
combines in-context-learning, instruction-following, and structured toolcalling
abilities to identify relevant protocol elements and apply accurate
modifications. In a systematic evaluation, experimental results indicate that
the agent can effectively retrieve protocol components, generate device
compatible protocol definition files, and faithfully implement user requests.
Despite demonstrating feasibility in principle, the approach faces limitations
regarding syntactic and semantic validity due to lack of a unified device API,
and challenges with ambiguous or complex requests. In summary, the findings
show a clear path towards LLM-based agents for supporting scan protocol
management in CT imaging.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [12] [Identifying and Addressing User-level Security Concerns in Smart Homes Using "Smaller" LLMs](https://arxiv.org/abs/2509.19485)
*Hafijul Hoque Chowdhury,Riad Ahmed Anonto,Sourov Jajodia,Suryadipta Majumdar,Md. Shohrab Hossain*

Main category: cs.CR

TL;DR: 스마트 홈 IoT 장치의 보안 문제를 해결하기 위해 사용자 수준에서의 주요 보안 우려를 식별하고 이를 해결하는 QA 시스템을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 스마트 홈 IoT 장치의 확산으로 사용자들이 안정성 문제에 노출되고 있으며, 그들의 요구는 복잡한 정보 검색 과정을 요구한다.

Method: 대중 포럼에서 Q&A 데이터셋을 수집하고, Latent Dirichlet Allocation (LDA)를 활용하여 주요 보안 문제를 추출하며, T5와 Flan-T5 모델을 미세 조정하여 QA 시스템을 구축하였다.

Result: 제작된 QA 시스템은 스마트 홈 IoT 장치에 대한 정확하고 관련성 있는 답변을 제공하며, 실제 사용자 문제에 대한 성능 향상이 확인되었다.

Conclusion: 이 연구는 자원 제한 환경에서도 효율적으로 배포할 수 있는 QA 시스템이 스마트 홈의 보안 우려를 해결하는 데 기여할 수 있음을 보여준다.

Abstract: With the rapid growth of smart home IoT devices, users are increasingly
exposed to various security risks, as evident from recent studies. While
seeking answers to know more on those security concerns, users are mostly left
with their own discretion while going through various sources, such as online
blogs and technical manuals, which may render higher complexity to regular
users trying to extract the necessary information. This requirement does not go
along with the common mindsets of smart home users and hence threatens the
security of smart homes furthermore. In this paper, we aim to identify and
address the major user-level security concerns in smart homes. Specifically, we
develop a novel dataset of Q&A from public forums, capturing practical security
challenges faced by smart home users. We extract major security concerns in
smart homes from our dataset by leveraging the Latent Dirichlet Allocation
(LDA). We fine-tune relatively "smaller" transformer models, such as T5 and
Flan-T5, on this dataset to build a QA system tailored for smart home security.
Unlike larger models like GPT and Gemini, which are powerful but often resource
hungry and require data sharing, smaller models are more feasible for
deployment in resource-constrained or privacy-sensitive environments like smart
homes. The dataset is manually curated and supplemented with synthetic data to
explore its potential impact on model performance. This approach significantly
improves the system's ability to deliver accurate and relevant answers, helping
users address common security concerns with smart home IoT devices. Our
experiments on real-world user concerns show that our work improves the
performance of the base models.

</details>


### [13] [CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning](https://arxiv.org/abs/2509.20166)
*Lauren Deason,Adam Bali,Ciprian Bejean,Diana Bolocan,James Crnkovich,Ioana Croitoru,Krishna Durai,Chase Midler,Calin Miron,David Molnar,Brad Moon,Bruno Ostarcevic,Alberto Peltea,Matt Rosenberg,Catalin Sandu,Arthur Saputkin,Sagar Shah,Daniel Stan,Ernest Szocs,Shengye Wan,Spencer Whitman,Sven Krasser,Joshua Saxe*

Main category: cs.CR

TL;DR: AI 시스템이 사이버 방어 작업을 향상시킬 필요성이 증가하고 있으며, CyberSOCEval이라는 새로운 오픈 소스 벤치마크 툴을 통해 LLM의 성능을 평가하고 개선할 수 있는 기회를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 방어자는 보안 경고와 위협 정보 신호의 홍수에 압도되어 있으며, 운영 보안을 강화하기 위한 AI 시스템의 긴급한 필요성이 있습니다.

Method: CyberSOCEval은 Malware Analysis와 Threat Intelligence Reasoning 두 가지 작업에서 LLM을 평가하기 위한 맞춤형 벤치마크로 구성됩니다.

Result: 더 크고 현대적인 LLM이 더 나은 성능을 발휘하며, 테스트 시간 스케일링을 활용한 추론 모델은 사이버 보안 분석에 대한 훈련이 부족함을 보여줍니다.

Conclusion: CyberSOCEval은 AI 개발자에게 사이버 방어 능력을 개선할 수 있는 중요한 도전 과제를 제시합니다.

Abstract: Today's cyber defenders are overwhelmed by a deluge of security alerts,
threat intelligence signals, and shifting business context, creating an urgent
need for AI systems to enhance operational security work. While Large Language
Models (LLMs) have the potential to automate and scale Security Operations
Center (SOC) operations, existing evaluations do not fully assess the scenarios
most relevant to real-world defenders. This lack of informed evaluation impacts
both AI developers and those applying LLMs to SOC automation. Without clear
insight into LLM performance in real-world security scenarios, developers lack
a north star for development, and users cannot reliably select the most
effective models. Meanwhile, malicious actors are using AI to scale cyber
attacks, highlighting the need for open source benchmarks to drive adoption and
community-driven improvement among defenders and model developers. To address
this, we introduce CyberSOCEval, a new suite of open source benchmarks within
CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in
two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive
domains with inadequate coverage in current benchmarks. Our evaluations show
that larger, more modern LLMs tend to perform better, confirming the training
scaling laws paradigm. We also find that reasoning models leveraging test time
scaling do not achieve the same boost as in coding and math, suggesting these
models have not been trained to reason about cybersecurity analysis, and
pointing to a key opportunity for improvement. Finally, current LLMs are far
from saturating our evaluations, showing that CyberSOCEval presents a
significant challenge for AI developers to improve cyber defense capabilities.

</details>


### [14] [STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation](https://arxiv.org/abs/2509.20190)
*Tanmay Khule,Stefan Marksteiner,Jose Alguindigue,Hannes Fuchs,Sebastian Fischmeister,Apurva Narayan*

Main category: cs.CR

TL;DR: 이 논문은 STAF(Security Test Automation Framework)를 도입하여 자동차 보안 테스트 케이스 생성을 자동화하는 새로운 접근 방식을 제시하며, 이는 공격 트리에서 실행 가능한 테스트 케이스를 생성하는 과정을 포함합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 자동차 개발에서는 보안 테스트가 증가하는 위협으로부터 시스템을 보호하는 데 필수적이다.

Method: 대형 언어 모델(LLMs)과 4단계 자기 교정 검색 증강 생성(RAG) 프레임워크를 활용하여 STAF는 공격 트리에서 실행 가능한 보안 테스트 케이스의 생성을 자동화한다.

Result: 효율성, 정확성, 확장성 및 모든 작업 흐름과의 용이한 통합에서 상당한 개선을 보여준다.

Conclusion: 자동차 보안 테스트 방법론의 자동화에서 중요한 진전을 이룬 결과이다.

Abstract: In modern automotive development, security testing is critical for
safeguarding systems against increasingly advanced threats. Attack trees are
widely used to systematically represent potential attack vectors, but
generating comprehensive test cases from these trees remains a labor-intensive,
error-prone task that has seen limited automation in the context of testing
vehicular systems. This paper introduces STAF (Security Test Automation
Framework), a novel approach to automating security test case generation.
Leveraging Large Language Models (LLMs) and a four-step self-corrective
Retrieval-Augmented Generation (RAG) framework, STAF automates the generation
of executable security test cases from attack trees, providing an end-to-end
solution that encompasses the entire attack surface. We particularly show the
elements and processes needed to provide an LLM to actually produce sensible
and executable automotive security test suites, along with the integration with
an automated testing framework. We further compare our tailored approach with
general purpose (vanilla) LLMs and the performance of different LLMs (namely
GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our
operation step-by-step in a concrete case study. Our results show significant
improvements in efficiency, accuracy, scalability, and easy integration in any
workflow, marking a substantial advancement in automating automotive security
testing methodologies. Using TARAs as an input for verfication tests, we create
synergies by connecting two vital elements of a secure automotive development
process.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [15] [The Heterogeneous Multi-Agent Challenge](https://arxiv.org/abs/2509.19512)
*Charles Dansereau,Junior-Samuel Lopez-Yepez,Karthik Soma,Antoine Fagette*

Main category: cs.MA

TL;DR: Heterogeneous Multi-Agent Reinforcement Learning (HeMARL)은 다양한 센서와 자원을 가진 에이전트들이 지역 정보를 바탕으로 협력해야 하는 문제로, 많은 실world 사례가 있지만 표준화된 평가 기준이 부족하여 연구가 미비하다.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous Multi-Agent Reinforcement Learning (HeMARL)의 필요성과 연구 부족을 강조함.

Method: 현재 MARL 연구는 주로 동질적인 에이전트에 집중되었으며, HeMARL을 위한 표준화된 평가 환경이 부족하다.

Result: 많은 알고리즘이 단순한 환경에서 근본적으로 최적 성능에 가까운 결과를 보이고 있으며, 약한 동질성을 가진 MARL 환경이 사용되고 있다.

Conclusion: HeMARL 분야에서 표준화된 테스트베드의 개발이 필요하다.

Abstract: Multi-Agent Reinforcement Learning (MARL) is a growing research area which
gained significant traction in recent years, extending Deep RL applications to
a much wider range of problems. A particularly challenging class of problems in
this domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where
agents with different sensors, resources, or capabilities must cooperate based
on local information. The large number of real-world situations involving
heterogeneous agents makes it an attractive research area, yet underexplored,
as most MARL research focuses on homogeneous agents (e.g., a swarm of identical
robots). In MARL and single-agent RL, standardized environments such as ALE and
SMAC have allowed to establish recognized benchmarks to measure progress.
However, there is a clear lack of such standardized testbed for cooperative
HeMARL. As a result, new research in this field often uses simple environments,
where most algorithms perform near optimally, or uses weakly heterogeneous MARL
environments.

</details>


### [16] [Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems](https://arxiv.org/abs/2509.19599)
*Danilo Trombino,Vincenzo Pecorella,Alessandro de Giulii,Davide Tresoldi*

Main category: cs.MA

TL;DR: KBA Orchestration은 정적 에이전트 설명을 동적 지식 기반 신호로 보완하여 에이전트의 자율성과 데이터 비밀성을 유지하며 보다 정확한 작업 라우팅을 달성하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡하고 지식 집약적인 문제를 해결하는 데 있어 효과적인 에이전트 조정이 중요하지만 기존의 정적 설명 방법은 종종 비효율적이다.

Method: 정적 설명이 충분하지 않을 때, 조정자는 서브 에이전트에게 병렬로 작업의 관련성을 평가하게 하고 에이전트는 자신의 개인 지식 기반에 대해 경량의 ACK 신호를 반환한다.

Result: KBA 조정은 정적 설명 기반 방법보다 라우팅 정밀도와 시스템 효율성이 뛰어난 성능을 보여준다.

Conclusion: 이 메커니즘은 큰 규모의 시스템에서도 높은 정확도를 요구하는 작업에 적합하다.

Abstract: Multi-agent systems (MAS) are increasingly tasked with solving complex,
knowledge-intensive problems where effective agent orchestration is critical.
Conventional orchestration methods rely on static agent descriptions, which
often become outdated or incomplete. This limitation leads to inefficient task
routing, particularly in dynamic environments where agent capabilities
continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a
novel approach that augments static descriptions with dynamic,
privacy-preserving relevance signals derived from each agent's internal
knowledge base (KB). In the proposed framework, when static descriptions are
insufficient for a clear routing decision, the orchestrator prompts the
subagents in parallel. Each agent then assesses the task's relevance against
its private KB, returning a lightweight ACK signal without exposing the
underlying data. These collected signals populate a shared semantic cache,
providing dynamic indicators of agent suitability for future queries. By
combining this novel mechanism with static descriptions, our method achieves
more accurate and adaptive task routing preserving agent autonomy and data
confidentiality. Benchmarks show that our KBA Orchestration significantly
outperforms static description-driven methods in routing precision and overall
system efficiency, making it suitable for large-scale systems that require
higher accuracy than standard description-driven routing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System](https://arxiv.org/abs/2509.19363)
*Zhuqi Wang,Qinghe Zhang,Zhuopei Cheng*

Main category: cs.LG

TL;DR: 이 논문에서는 신용 카드 사기를 해결하기 위한 새로운 혼합 분석 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 신용 카드 사기가 미국 가정의 재정적 위치에 주요 위협으로 자리 잡고 있으며, 가정의 경제적 행동에 예측 불가능한 변화를 초래하고 있습니다.

Method: Enhanced ANFIS를 사용한 새로운 혼합 분석 방법을 제안하며, 여기에는 다중 해상도 웨이브렛 분해 모듈과 시간적 주의 메커니즘이 포함됩니다.

Result: 모델은 역사적 거래 데이터 및 거시경제 지표에 대해 이산 웨이브렛 변환을 수행하여 지역화된 경제 충격 신호를 생성합니다.

Conclusion: 제안된 방법은 고정된 입력-출력 관계를 가진 고전적인 ANFIS와는 달리 웨이브렛 기초 선택 및 시간 상관 계수를 통합하여 효과를 높입니다.

Abstract: Credit card fraud is assuming growing proportions as a major threat to the
financial position of American household, leading to unpredictable changes in
household economic behavior. To solve this problem, in this paper, a new hybrid
analysis method is presented by using the Enhanced ANFIS. The model proposes
several advances of the conventional ANFIS framework and employs a
multi-resolution wavelet decomposition module and a temporal attention
mechanism. The model performs discrete wavelet transformations on historical
transaction data and macroeconomic indicators to generate localized economic
shock signals. The transformed features are then fed into a deep fuzzy rule
library which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian
membership functions. The model proposes a temporal attention encoder that
adaptively assigns weights to multi-scale economic behavior patterns,
increasing the effectiveness of relevance assessment in the fuzzy inference
stage and enhancing the capture of long-term temporal dependencies and
anomalies caused by fraudulent activities. The proposed method differs from
classical ANFIS which has fixed input-output relations since it integrates
fuzzy rule activation with the wavelet basis selection and the temporal
correlation weights via a modular training procedure. Experimental results show
that the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and
conventional LSTM models.

</details>


### [18] [Learning from Observation: A Survey of Recent Advances](https://arxiv.org/abs/2509.19379)
*Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 이 연구는 전문가의 행동을 모방하는 대신 상태 방문 정보만 사용하는 관찰 학습의 기존 방법들을 조사하고 분류하는 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 전문가의 행동 정보를 요구하지 않는 학습 방법의 필요성이 제기됩니다.

Method: 관찰 학습(LfO) 프레임워크를 제시하고, 기존 LfO 방법들을 경로 구성, 가정 및 알고리즘 디자인 측면에서 분류합니다.

Result: LfO 방법들의 분류 결과를 통해 기존의 다양한 관련 분야와 연결성을 지적합니다.

Conclusion: 개방된 문제를 파악하고 향후 연구 방향을 제안합니다.

Abstract: Imitation Learning (IL) algorithms offer an efficient way to train an agent
by mimicking an expert's behavior without requiring a reward function. IL
algorithms often necessitate access to state and action information from expert
demonstrations. Although expert actions can provide detailed guidance,
requiring such action information may prove impractical for real-world
applications where expert actions are difficult to obtain. To address this
limitation, the concept of learning from observation (LfO) or state-only
imitation learning (SOIL) has recently gained attention, wherein the imitator
only has access to expert state visitation information. In this paper, we
present a framework for LfO and use it to survey and classify existing LfO
methods in terms of their trajectory construction, assumptions and algorithm's
design choices. This survey also draws connections between several related
fields like offline RL, model-based RL and hierarchical RL. Finally, we use our
framework to identify open problems and suggest future research directions.

</details>


### [19] [Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2509.19417)
*Andreas Lebedev,Abhinav Das,Sven Pappert,Stephan Schlüter*

Main category: cs.LG

TL;DR: 이 연구는 독일 시장에서 전기 가격 예측을 위한 최신 통계 및 딥러닝 확률 모델의 불확실성 정량화를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 정확한 확률 예측은 에너지 리스크 관리에 필수적이며, 이를 위한 다양한 통계 및 머신러닝 모델이 존재한다.

Method: 딥 배포 신경망(DDNN)을 사용하고, 앙상블 접근법, 몬테카를로 드롭아웃, 규범적 예측으로 모델 불확실성을 고려한다.

Result: LEAR 기반 모델이 확률 예측에서 좋은 성능을 보이며, DDNN은 데이터와 모델 불확실성을 통합할 경우 성능이 향상된다.

Conclusion: 모든 모델의 상대적 성능은 포인트 및 확률 예측을 위한 지표 선택에 따라 달라진다.

Abstract: Precise probabilistic forecasts are fundamental for energy risk management,
and there is a wide range of both statistical and machine learning models for
this purpose. Inherent to these probabilistic models is some form of
uncertainty quantification. However, most models do not capture the full extent
of uncertainty, which arises not only from the data itself but also from model
and distributional choices. In this study, we examine uncertainty
quantification in state-of-the-art statistical and deep learning probabilistic
forecasting models for electricity price forecasting in the German market. In
particular, we consider deep distributional neural networks (DDNNs) and augment
them with an ensemble approach, Monte Carlo (MC) dropout, and conformal
prediction to account for model uncertainty. Additionally, we consider the
LASSO-estimated autoregressive (LEAR) approach combined with quantile
regression averaging (QRA), generalized autoregressive conditional
heteroskedasticity (GARCH), and conformal prediction. Across a range of
performance metrics, we find that the LEAR-based models perform well in terms
of probabilistic forecasting, irrespective of the uncertainty quantification
method. Furthermore, we find that DDNNs benefit from incorporating both data
and model uncertainty, improving both point and probabilistic forecasting.
Uncertainty itself appears to be best captured by the models using conformal
prediction. Overall, our extensive study shows that all models under
consideration perform competitively. However, their relative performance
depends on the choice of metrics for point and probabilistic forecasting.

</details>


### [20] [On the Fragility of Contribution Score Computation in Federated Learning](https://arxiv.org/abs/2509.19921)
*Balazs Pejo,Marcell Frank,Krisztian Varga,Peter Veliczky*

Main category: cs.LG

TL;DR: 이 논문은 연합 학습에서 기여 평가의 취약성을 조사하며, 기여 점수가 아키텍처 민감도와 의도적인 조작이라는 두 가지 근본적인 관점에서 심각한 왜곡에 취약하다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 공정성을 보장하고 참여를 유도하는 중요한 메커니즘인 기여 평가의 신뢰성을 높이는 필요성.

Method: 모델 집합 방법이 기여 점수에 미치는 영향을 다각도로 분석하고, 해로운 참가자들이 기여 점수를 조작하기 위한 공격 방식도 탐구한다.

Result: 다양한 데이터셋 및 모델 아키텍처를 통해 집합 방법의 선택과 공격자의 존재가 기여 점수를 왜곡하는 강력한 요인이 됨을 입증하였다.

Conclusion: 기여 점수 왜곡을 방지하기 위해 보다 견고한 평가 방식의 필요성을 강조한다.

Abstract: This paper investigates the fragility of contribution evaluation in federated
learning, a critical mechanism for ensuring fairness and incentivizing
participation. We argue that contribution scores are susceptible to significant
distortions from two fundamental perspectives: architectural sensitivity and
intentional manipulation. First, we explore how different model aggregation
methods impact these scores. While most research assumes a basic averaging
approach, we demonstrate that advanced techniques, including those designed to
handle unreliable or diverse clients, can unintentionally yet significantly
alter the final scores. Second, we explore vulnerabilities posed by poisoning
attacks, where malicious participants strategically manipulate their model
updates to inflate their own contribution scores or reduce the importance of
other participants. Through extensive experiments across diverse datasets and
model architectures, implemented within the Flower framework, we rigorously
show that both the choice of aggregation method and the presence of attackers
are potent vectors for distorting contribution scores, highlighting a critical
need for more robust evaluation schemes.

</details>


### [21] [Frictional Q-Learning](https://arxiv.org/abs/2509.19771)
*Hyunwoo Kim,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: 본 논문에서는 고전 역학의 정적 마찰과 오프 정책 강화 학습의 외삽 오류 사이의 유사성을 활용하여 정책이 지원되지 않는 행동으로 떠나지 않도록 제한을 설정합니다. 이는 Frictional Q-learning이라는 지속적인 제어를 위한 심층 강화 학습 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 정책이 지원되지 않은 행동으로 떠나는 것을 방지하기 위해 정적 마찰의 개념을 도입합니다.

Method: Frictional Q-learning 알고리즘은 에이전트의 행동 공간을 제한하여 리플레이 버퍼의 행동과 유사한 행동을 장려하며, 직교 정규 행동 공간의 다양성과 거리를 유지합니다.

Result: 우리의 알고리즘은 표준 지속적인 제어 벤치마크에서 경쟁력 있는 성능을 달성함을 입증합니다.

Conclusion: Frictional Q-learning은 배치 제약의 단순성을 유지하면서 외삽 오류에 대한 직관적인 물리적 해석을 제공합니다.

Abstract: We draw an analogy between static friction in classical mechanics and
extrapolation error in off-policy RL, and use it to formulate a constraint that
prevents the policy from drifting toward unsupported actions. In this study, we
present Frictional Q-learning, a deep reinforcement learning algorithm for
continuous control, which extends batch-constrained reinforcement learning. Our
algorithm constrains the agent's action space to encourage behavior similar to
that in the replay buffer, while maintaining a distance from the manifold of
the orthonormal action space. The constraint preserves the simplicity of
batch-constrained, and provides an intuitive physical interpretation of
extrapolation error. Empirically, we further demonstrate that our algorithm is
robustly trained and achieves competitive performance across standard
continuous control benchmarks.

</details>


### [22] [RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving](https://arxiv.org/abs/2509.19789)
*Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim*

Main category: cs.LG

TL;DR: RDAR는 자율주행 시스템에서 에이전트의 영향을 학습하는 효율적인 방법으로, 기존의 방식보다 적은 수의 에이전트를 처리하면서도 비슷한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 인간 운전자는 제한된 수의 에이전트에만 집중하는 반면, 자율주행 시스템은 많은 수의 에이전트를 동시에 처리해야 하므로 효율적인 에이전트 선택이 필요하다.

Method: RDAR는 에이전트의 중요도를 학습하며, 에이전트 선택을 나타내는 이진 마스크를 사용하는 마르코프 결정 프로세스로 마스킹 절차를 공식화한다.

Result: 대규모 주행 데이터셋에서 RDAR를 평가한 결과, 기존 모델에 비해 더 적은 수의 에이전트를 처리하면서도 비슷한 운전 성능을 달성하였다.

Conclusion: RDAR는 자율주행 시스템의 에이전트 선택을 개선하여 효율성을 높이는 데 기여할 수 있다.

Abstract: Human drivers focus only on a handful of agents at any one time. On the other
hand, autonomous driving systems process complex scenes with numerous agents,
regardless of whether they are pedestrians on a crosswalk or vehicles parked on
the side of the road. While attention mechanisms offer an implicit way to
reduce the input to the elements that affect decisions, existing attention
mechanisms for capturing agent interactions are quadratic, and generally
computationally expensive. We propose RDAR, a strategy to learn per-agent
relevance -- how much each agent influences the behavior of the controlled
vehicle -- by identifying which agents can be excluded from the input to a
pre-trained behavior model. We formulate the masking procedure as a Markov
Decision Process where the action consists of a binary mask indicating agent
selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate
its ability to learn an accurate numerical measure of relevance by achieving
comparable driving performance, in terms of overall progress, safety and
performance, while processing significantly fewer agents compared to a state of
the art behavior model.

</details>


### [23] [Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches](https://arxiv.org/abs/2509.19924)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: 강화 학습에서 탐색은 여전히 도전적이다. 우리는 VLM이 높은 수준의 목표를 추론할 수 있지만, 정밀한 저수준 제어에서 지속적으로 실패하는 모습을 발견하였다.


<details>
  <summary>Details</summary>
Motivation: 현재의 강화 학습 방법에서는 희소 보상 환경에서의 탐색이 여전히 어렵기 때문이다.

Method: LLM과 VLM을 다중 무기 대회, Gridworld, 희소 보상을 주는 Atari에서 제로샷 탐색을 테스트하기 위해 벤치마킹하였다.

Result: VLM이 시각적 입력으로부터 고수준 목표를 추론할 수 있지만, 저수준 제어에서의 실패를 지속적으로 보였다.

Conclusion: 기본 모델을 탐색 유도에 사용하는 것의 잠재력과 한계를 명확하게 분석하였다.

Abstract: Exploration in reinforcement learning (RL) remains challenging, particularly
in sparse-reward settings. While foundation models possess strong semantic
priors, their capabilities as zero-shot exploration agents in classic RL
benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed
bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our
investigation reveals a key limitation: while VLMs can infer high-level
objectives from visual input, they consistently fail at precise low-level
control: the "knowing-doing gap". To analyze a potential bridge for this gap,
we investigate a simple on-policy hybrid framework in a controlled, best-case
scenario. Our results in this idealized setting show that VLM guidance can
significantly improve early-stage sample efficiency, providing a clear analysis
of the potential and constraints of using foundation models to guide
exploration rather than for end-to-end control.

</details>


### [24] [Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment](https://arxiv.org/abs/2509.20214)
*Deokjae Lee,Hyun Oh Song*

Main category: cs.LG

TL;DR: 이 연구에서는 재학습 없이 대규모 언어 모델(LLM)의 가중치만을 양자화하는_WEIGHT-ONLY POST-TRAINING QUANTIZATION(PTQ)에 대해 다룬다. 이는 메모리 요구 사항과 지연 시간을 줄이는 데 필수적이다.


<details>
  <summary>Details</summary>
Motivation: LLM 추론의 메모리 풋프린트와 지연 시간을 줄이기 위한 노력.

Method: Q-Palette라는 다양한 분수 비트 양자화기 모음을 소개하고 기초 요소로 활용하는 혼합 스킴 양자화 프레임워크를 제안한다.

Result: 미세한 분수 비트 양자화기를 통해 최적의 양자화 성능에 도달할 수 있음을 발견하였다.

Conclusion: Q-Palette는 다양한 비트 폭에서 효율적으로 구현되며 빠른 추론을 위해 최적화된 벡터 및 스칼라 양자화기를 포함한다.

Abstract: We study weight-only post-training quantization (PTQ), which quantizes the
weights of a large language model (LLM) without retraining, using little or no
calibration data. Weight-only PTQ is crucial for reducing the memory footprint
and latency of LLM inference, especially in memory-bound, small-batch inference
scenarios, such as personalized inference on edge devices. Despite its
importance, irregular weight distributions with heavy-tailed outliers in LLMs
complicate quantization, recently motivating rotation-based methods that
transform weights into near-Gaussian distributions, which are more regular with
fewer outliers, thereby reducing quantization error. In this work, we first
derive the information-theoretically optimal bit allocation for Gaussianized
weights under given bit budgets, revealing that fine-grained fractional-bit
quantizers approaching the Gaussian distortion-rate bound are essential to
achieve near-optimal quantization performance. To bridge this theoretical
insight and practical implementation, we introduce Q-Palette, a versatile
collection of fractional-bit quantizers that range from trellis-coded
quantizers offering near-optimal distortion to simpler vector and scalar
quantizers optimized for faster inference, all efficiently implemented with
optimized CUDA kernels across various bitwidths. Furthermore, leveraging
Q-Palette as a foundational component, we propose a novel mixed-scheme
quantization framework, jointly optimizing quantizer choices and layer fusion
decisions given resource constraints. The code is available at
https://github.com/snu-mllab/Q-Palette.

</details>


### [25] [Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute](https://arxiv.org/abs/2509.20241)
*Felipe Oviedo,Fiodar Kazhamiaka,Esha Choukse,Allen Kim,Amy Luers,Melanie Nakagawa,Ricardo Bianchini,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: AI 추론이 수십억 개 질의로 확장됨에 따라 쿼리별 에너지 사용에 대한 신뢰할 수 있는 추정이 점점 더 중요해지고 있다. 본 연구에서는 대규모 LLM 시스템의 쿼리당 에너지를 추정하는 새로운 방법론을 제시하며, 실제 테스트 환경에서 모델의 에너지 사용을 측정한 결과를 보고한다.


<details>
  <summary>Details</summary>
Motivation: AI 서비스의 규모와 복잡성이 증가하면서, 쿼리당 에너지 사용에 대한 신뢰할 수 있는 추정이 필요하다.

Method: H100 노드에서 실제 워크로드 및 GPU 활용도, PUE 제약을 바탕으로 대규모 LLM 시스템의 쿼리당 에너지를 추정하는 하향식 방법론을 개발하였다.

Result: 최전선 모델의 쿼리당 에너지를 0.34 Wh로 추정하였고, 이는 생산 규모의 구성에서 측정한 결과와 일치하며 비생산 추정이 에너지 사용을 4-20배 과대 평가할 수 있음을 보여준다.

Conclusion: 효율성을 목표로 하는 개입을 통해 대규모 운영 시 에너지를 효과적으로 절감할 수 있으며, 이는 웹 검색과 유사한 에너지 발자국을 약속한다.

Abstract: As AI inference scales to billions of queries and emerging reasoning and
agentic workflows increase token demand, reliable estimates of per-query energy
use are increasingly important for capacity planning, emissions accounting, and
efficiency prioritization. Many public estimates are inconsistent and overstate
energy use, because they extrapolate from limited benchmarks and fail to
reflect efficiency gains achievable at scale. In this perspective, we introduce
a bottom-up methodology to estimate the per-query energy of large-scale LLM
systems based on token throughput. For models running on an H100 node under
realistic workloads, GPU utilization and PUE constraints, we estimate a median
energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200
billion parameters). These results are consistent with measurements using
production-scale configurations and show that non-production estimates and
assumptions can overstate energy use by 4-20x. Extending to test-time scaling
scenarios with 15x more tokens per typical query, the median energy rises 13x
to 4.32 Wh, indicating that targeting efficiency in this regime will deliver
the largest fleet-wide savings. We quantify achievable efficiency gains at the
model, serving platform, and hardware levels, finding individual median
reductions of 1.5-3.5x in energy per query, while combined advances can
plausibly deliver 8-20x reductions. To illustrate the system-level impact, we
estimate the baseline daily energy use of a deployment serving 1 billion
queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8
GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,
similar to the energy footprint of web search at that scale. This echoes how
data centers historically tempered energy growth through efficiency gains
during the internet and cloud build-up.

</details>


### [26] [Dynamic Lagging for Time-Series Forecasting in E-Commerce Finance: Mitigating Information Loss with A Hybrid ML Architecture](https://arxiv.org/abs/2509.20244)
*Abhishek Sharma,Anat Parush,Sumit Wadhwa,Amihai Savir,Anne Guinard,Prateek Srivastava*

Main category: cs.LG

TL;DR: 전자 상거래 금융 분야에서의 정확한 예측은 비정기적인 인보이스 일정, 지불 연기 및 사용자별 행동 변동성으로 인해 특히 도전적입니다. 본 연구에서는 이 문제를 해결하기 위해 하이브리드 예측 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전자 상거래 금융의 예측 정확성을 높이는 것은 비정기적인 인보이스, 지불 지연, 그리고 사용자 행동의 변동성으로 인해 어렵습니다.

Method: 본 논문에서는 동적 지연 특징 엔지니어링 및 적응형 롤링 윈도우 표현을 고전 통계 모델 및 앙상블 학습자와 통합하는 하이브리드 예측 프레임워크를 제안합니다.

Result: 실험 결과, 기준 모델과 비교할 때 MAPE가 약 5% 감소하며, 이는 상당한 재정적 절감을 의미합니다.

Conclusion: 구조화된 지연, 인보이스 수준의 종료 모델링 및 행동 통찰을 결합함으로써 희소한 금융 시계열 예측에서 예측 정확성을 향상시킬 수 있습니다.

Abstract: Accurate forecasting in the e-commerce finance domain is particularly
challenging due to irregular invoice schedules, payment deferrals, and
user-specific behavioral variability. These factors, combined with sparse
datasets and short historical windows, limit the effectiveness of conventional
time-series methods. While deep learning and Transformer-based models have
shown promise in other domains, their performance deteriorates under partial
observability and limited historical data. To address these challenges, we
propose a hybrid forecasting framework that integrates dynamic lagged feature
engineering and adaptive rolling-window representations with classical
statistical models and ensemble learners. Our approach explicitly incorporates
invoice-level behavioral modeling, structured lag of support data, and custom
stability-aware loss functions, enabling robust forecasts in sparse and
irregular financial settings. Empirical results demonstrate an approximate 5%
reduction in MAPE compared to baseline models, translating into substantial
financial savings. Furthermore, the framework enhances forecast stability over
quarterly horizons and strengthens feature target correlation by capturing both
short- and long-term patterns, leveraging user profile attributes, and
simulating upcoming invoice behaviors. These findings underscore the value of
combining structured lagging, invoice-level closure modeling, and behavioral
insights to advance predictive accuracy in sparse financial time-series
forecasting.

</details>


### [27] [PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction](https://arxiv.org/abs/2509.20290)
*Dayu Tan,Jing Chen,Xiaoping Zhou,Yansen Su,Chunhou Zheng*

Main category: cs.LG

TL;DR: 이 연구는 감염병과 올리고펩타이드 간의 연관성을 예측하기 위한 새로운 계산적 접근법인 PGCLODA를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 감염병은 여전히 공공 건강에 심각한 위협을 가하고 있으며, 새로운 항감염제 스크리닝을 위한 효과적인 계산적 접근법의 필요성이 강조됩니다.

Method: PGCLODA는 올리고펩타이드, 미생물 및 질병을 노드로 하는 삼부 구성 그래프를 사용하여 구조적 및 의미적 정보를 통합하며, 대조 학습 중 중요한 영역을 보존하기 위한 프롬프트 유도 그래프 증강 전략을 채택합니다.

Result: PGCLODA는 벤치마크 데이터셋에서 AUROC, AUPRC 및 정확도에서 최고의 모델을 일관되게 초과 성능을 보였습니다.

Conclusion: 이 연구는 메커니즘 기반 발견 및 올리고펩타이드 기반 약물 개발에 대한 귀중한 통찰력을 제공합니다.

Abstract: Infectious diseases continue to pose a serious threat to public health,
underscoring the urgent need for effective computational approaches to screen
novel anti-infective agents. Oligopeptides have emerged as promising candidates
in antimicrobial research due to their structural simplicity, high
bioavailability, and low susceptibility to resistance. Despite their potential,
computational models specifically designed to predict associations between
oligopeptides and infectious diseases remain scarce. This study introduces a
prompt-guided graph-based contrastive learning framework (PGCLODA) to uncover
potential associations. A tripartite graph is constructed with oligopeptides,
microbes, and diseases as nodes, incorporating both structural and semantic
information. To preserve critical regions during contrastive learning, a
prompt-guided graph augmentation strategy is employed to generate meaningful
paired views. A dual encoder architecture, integrating Graph Convolutional
Network (GCN) and Transformer, is used to jointly capture local and global
features. The fused embeddings are subsequently input into a multilayer
perceptron (MLP) classifier for final prediction. Experimental results on a
benchmark dataset indicate that PGCLODA consistently outperforms
state-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and
hyperparameter studies confirm the contribution of each module. Case studies
further validate the generalization ability of PGCLODA and its potential to
uncover novel, biologically relevant associations. These findings offer
valuable insights for mechanism-driven discovery and oligopeptide-based drug
development. The source code of PGCLODA is available online at
https://github.com/jjnlcode/PGCLODA.

</details>
