<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [PLA: Prompt Learning Attack against Text-to-Image Generative Models](https://arxiv.org/abs/2508.03696)
*Xinqi Lyu,Yihao Liu,Yanjie Li,Bin Xiao*

Main category: cs.CR

TL;DR: 이 논문은 블랙박스 설정에서 T2I 모델의 안전 메커니즘을 우회하는 적대적 공격을 다룬다.


<details>
  <summary>Details</summary>
Motivation: T2I 모델의 오용 위험을 조사하고 안전하지 않은 콘텐츠 생성을 방지하기 위해 연구한다.

Method: 블랙박스 환경에서의 적대적 프롬프트 학습을 위해 새로운 프롬프트 학습 공격 프레임워크(PLA)를 제안한다.

Result: 우리의 방법은 기존의 최첨단 방법에 비해 높은 성공률로 블랙박스 T2I 모델의 안전 메커니즘을 효과적으로 공격하였다.

Conclusion: 이 연구는 T2I 모델의 안전성을 더욱 강화하기 위한 중요한 시사점을 제공한다.

Abstract: Text-to-Image (T2I) models have gained widespread adoption across various
applications. Despite the success, the potential misuse of T2I models poses
significant risks of generating Not-Safe-For-Work (NSFW) content. To
investigate the vulnerability of T2I models, this paper delves into adversarial
attacks to bypass the safety mechanisms under black-box settings. Most previous
methods rely on word substitution to search adversarial prompts. Due to limited
search space, this leads to suboptimal performance compared to gradient-based
training. However, black-box settings present unique challenges to training
gradient-driven attack methods, since there is no access to the internal
architecture and parameters of T2I models. To facilitate the learning of
adversarial prompts in black-box settings, we propose a novel prompt learning
attack framework (PLA), where insightful gradient-based training tailored to
black-box T2I models is designed by utilizing multimodal similarities.
Experiments show that our new method can effectively attack the safety
mechanisms of black-box T2I models including prompt filters and post-hoc safety
checkers with a high success rate compared to state-of-the-art methods.
Warning: This paper may contain offensive model-generated content.

</details>


### [2] [RX-INT: A Kernel Engine for Real-Time Detection and Analysis of In-Memory Threats](https://arxiv.org/abs/2508.03879)
*Arjun Juneja*

Main category: cs.CR

TL;DR: RX-INT는 전통적인 보안 시스템을 회피하는 파일리스 공격 탐지를 위한 새로운 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 파일리스 실행 기술로 인해 기존 보안 제품이 효과적으로 악성코드를 탐지하지 못하고 있다는 점에서, 이를 해결할 필요성이 있다.

Method: 실시간 스레드 생성 모니터와 상태 기반 가상 주소 설명자(VAD) 스캐너를 결합한 탐지 엔진을 도입하여 메모리 해싱을 통해 비정상적인 수정 사항을 감지한다.

Result: RX-INT는 PE-sieve와의 비교에서 더 높은 탐지율을 보였고, 수동적으로 매핑된 영역도 탐지할 수 있었다.

Conclusion: RX-INT는 파일리스 위협 탐지에서 실질적인 차이를 제공하며, 반치트 및 메모리 보안 분야에 직접 적용 가능하다.

Abstract: Malware and cheat developers use fileless execution techniques to evade
traditional, signature-based security products. These methods include various
types of manual mapping, module stomping, and threadless injection which work
entirely within the address space of a legitimate process, presenting a
challenge for detection due to ambiguity between what is legitimate and what
isn't. Existing tools often have weaknesses, such as a dependency on Portable
Executable (PE) structures or a vulnerability to time-of-check-to-time-of-use
(TOCTOU) race conditions where an adversary cleans up before a periodic scan
has the chance to occur. To address this gap, we present RX-INT, a
kernel-assisted system featuring an architecture that provides resilience
against TOCTOU attacks. RX-INT introduces a detection engine that combines a
real-time thread creation monitor with a stateful Virtual Address Descriptor
(VAD) scanner alongside various heuristics within. This engine snapshots both
private and image-backed memory regions, using real-time memory hashing to
detect illicit modifications like module stomping. Critically, we demonstrate a
higher detection rate in certain benchmarks of this approach through a direct
comparison with PE-sieve, a commonly used and powerful memory forensics tool.
In our evaluation, RX-INT successfully detected a manually mapped region that
was not identified by PE-sieve. We then conclude that our architecture
represents a tangible difference in the detection of fileless threats, with
direct applications in the fields of anti-cheat and memory security.

</details>


### [3] [Simulating Cyberattacks through a Breach Attack Simulation (BAS) Platform empowered by Security Chaos Engineering (SCE)](https://arxiv.org/abs/2508.03882)
*Arturo Sánchez-Matas,Pablo Escribano Ruiz,Daniel Díaz-López,Angel Luis Perales Gómez,Pantaleone Nespoli,Gregorio Martínez Pérez*

Main category: cs.CR

TL;DR: 사이버 위협에 효과적으로 대응하기 위해, Security Chaos Engineering(SCE)와 Breach Attack Simulation(BAS)을 통합한 새로운 사이버 공격 시뮬레이션 방안을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 조직들은 끊임없이 진화하는 사이버 위협에 직면하고 있으며, 이를 해결하기 위해 새로운 기술을 활용하여 공격 벡터를 탐지할 필요가 있다.

Method: SCE를 BAS 플랫폼에 통합하여, 위협 인텔리전스 데이터베이스에서 적의 프로필 및 능력을 활용하는 구조적 아키텍처(Orchestrator, Connector, BAS 레이어)로 구성된 프로세스를 사용한다.

Result: SCE와 BAS 통합 평가를 통해 전통적인 시나리오를 넘는 공격 시뮬레이션의 효과를 향상할 수 있음을 보여준다.

Conclusion: SCE와 BAS의 결합은 사이버 방어 전략의 유용한 요소가 될 수 있다.

Abstract: In today digital landscape, organizations face constantly evolving cyber
threats, making it essential to discover slippery attack vectors through novel
techniques like Security Chaos Engineering (SCE), which allows teams to test
defenses and identify vulnerabilities effectively. This paper proposes to
integrate SCE into Breach Attack Simulation (BAS) platforms, leveraging
adversary profiles and abilities from existing threat intelligence databases.
This innovative proposal for cyberattack simulation employs a structured
architecture composed of three layers: SCE Orchestrator, Connector, and BAS
layers. Utilizing MITRE Caldera in the BAS layer, our proposal executes
automated attack sequences, creating inferred attack trees from adversary
profiles. Our proposal evaluation illustrates how integrating SCE with BAS can
enhance the effectiveness of attack simulations beyond traditional scenarios,
and be a useful component of a cyber defense strategy.

</details>


### [4] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: ASTRA는 AI 코드 생성기 및 보안 안내 시스템의 안전성 결함을 체계적으로 찾아내는 자동화된 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: AI 코딩 도우미의 안전성은 높은 위험이 있는 사이버 보안 분야에서 불확실성이 크다.

Method: ASTRA는 3단계로 작동하며, 도메인 지식 그래프를 구축하고, 온라인 취약성 탐색을 수행하며, 고품질 위반 유발 사례를 생성한다.

Result: ASTRA는 기존 기술보다 11-66% 더 많은 문제를 발견하고, 17% 더 효과적인 정렬 훈련을 위한 테스트 사례를 생성한다.

Conclusion: ASTRA는 더 안전한 AI 시스템을 구축하는 데 실질적인 가치를 제공한다.

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


### [5] [Isolate Trigger: Detecting and Eradicating Evade-Adaptive Backdoors](https://arxiv.org/abs/2508.04094)
*Chengrui Sun,Hua Zhang,Haoran Gao,Zian Tian,Jianjin Zhao,qi Li,Hongliang Zhu,Zongliang Shen,Shang Wang,Anmin Fu*

Main category: cs.CR

TL;DR: Isolate Trigger (IsTr)는 딥러닝 모델의 백도어 공격을 탐지하고 방어하기 위한 정밀하고 효율적인 프레임워크로, 기존의 탐지 기법을 넘어서 숨겨진 트리거를 찾아내는 데 초점을 맞춘다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 모델의 백도어 공격이 점점 교묘해지고 있어 기존의 비본질적 특징(NEF) 탐지 방법으로는 이들은 탐지하기 어려워졌다.

Method: IsTr는 소스 특징의 장벽을 허물고, 거리 및 그래디언트 이론을 업데이트하기 위해 Steps와 Differential-Middle-Slice를 구성 요소로 사용한다.

Result: IsTr는 다양한 작업에서 높은 효율성과 일반성, 정밀성을 입증했으며, 여섯 가지 EAB 공격에 대해 효과적으로 방어할 수 있었다.

Conclusion: EAB 공격이 결합되고 트리거와 소스가 겹치더라도 IsTr는 성공적으로 탐지 및 방어할 수 있음을 보여주었다.

Abstract: All current detection of backdoor attacks on deep learning models fall under
the category of a non essential features(NEF), which focus on fighting against
simple and efficient vertical class backdoor -- trigger is small, few and not
overlapping with the source. Evade-adaptive backdoor (EAB) attacks have evaded
NEF detection and improved training efficiency. We introduces a precise,
efficient and universal detection and defense framework coined as Isolate
Trigger (IsTr). IsTr aims to find the hidden trigger by breaking the barrier of
the source features. Therefore, it investigates the essence of backdoor
triggering, and uses Steps and Differential-Middle-Slice as components to
update past theories of distance and gradient. IsTr also plays a positive role
in the model, whether the backdoor exists. For example, accurately find and
repair the wrong identification caused by deliberate or unintentional training
in automatic driving. Extensive experiments on robustness scross various tasks,
including MNIST, facial recognition, and traffic sign recognition, confirm the
high efficiency, generality and precision of the IsTr. We rigorously evaluated
the effectiveness of the IsTr against a series of six EAB attacks, including
Badnets, Sin-Wave, Multi-trigger, SSBAs, CASSOCK, HCB. None of these
countermeasures evade, even when attacks are combined and the trigger and
source overlap.

</details>


### [6] [SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios](https://arxiv.org/abs/2508.04100)
*Borui Li,Li Yan,Junhao Han,Jianmin Liu,Lei Yu*

Main category: cs.CR

TL;DR: 이 논문은 Federated Learning에서의 Homomorphic Encryption의 효율성을 높이기 위해 Sensitivity-guided selective HE 프레임워크인 SenseCrypt를 제안한다.


<details>
  <summary>Details</summary>
Motivation: Federated Learning에서의 데이터 보호를 위해 HE가 필요하지만, 기존 방법들은 높은 오버헤드와 적응 비용 문제를 가지므로 이를 개선할 필요가 있다.

Method: 클라이언트의 데이터 분포 유사성을 기반으로 클러스터링하고, 각 클러스터에 대한 모델 파라미터 선택 최적화 문제를 해결하여 HE 오버헤드를 최소화하면서 보안을 극대화한다.

Result: SenseCrypt는 기존 HE 방법에 비해 훈련 시간을 58.4%-88.7% 감소시키며, 보안성은 향상시키면서도 정상적인 모델 정확도를 유지한다.

Conclusion: SenseCrypt는 Federated Learning의 클라이언트 별로 보안과 HE 오버헤드를 균형 있게 조정하여 효과적인 솔루션을 제공한다.

Abstract: Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but
suffers from high overhead and adaptation cost. Selective HE methods, which
partially encrypt model parameters by a global mask, are expected to protect
privacy with reduced overhead and easy adaptation. However, in cross-device
scenarios with heterogeneous data and system capabilities, traditional
Selective HE methods deteriorate client straggling, and suffer from degraded HE
overhead reduction performance. Accordingly, we propose SenseCrypt, a
Sensitivity-guided selective Homomorphic EnCryption framework, to adaptively
balance security and HE overhead per cross-device FL client. Given the
observation that model parameter sensitivity is effective for measuring
clients' data distribution similarity, we first design a privacy-preserving
method to respectively cluster the clients with similar data distributions.
Then, we develop a scoring mechanism to deduce the straggler-free ratio of
model parameters that can be encrypted by each client per cluster. Finally, for
each client, we formulate and solve a multi-objective model parameter selection
optimization problem, which minimizes HE overhead while maximizing model
security without causing straggling. Experiments demonstrate that SenseCrypt
ensures security against the state-of-the-art inversion attacks, while
achieving normal model accuracy as on IID data, and reducing training time by
58.4%-88.7% as compared to traditional HE methods.

</details>


### [7] [Evaluating Selective Encryption Against Gradient Inversion Attacks](https://arxiv.org/abs/2508.04155)
*Jiajun Gu,Yuhang Yao,Shuaiqi Wang,Carlee Joe-Wong*

Main category: cs.CR

TL;DR: 그래디언트 역전 공격은 연합 학습과 같은 분산 훈련 프레임워크에 심각한 개인 정보 위협을 초래하며, 전통적인 암호화 방어는 높은 계산 오버헤드를 불러온다. 이 논문은 선택적 암호화 기법을 평가하고, 그래디언트 요소 선택을 위해 거리 기반 중요성 분석 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 그래디언트 통신을 통한 개인 데이터 재구성을 방지하기 위해, 전통적인 암호화 방식의 한계를 극복하고자 함.

Method: 여러 중요성 지표를 기반으로 하는 선택적 암호화 방법을 평가함.

Result: 선택적 암호화가 계산 오버헤드를 줄이며 공격에 대한 회복력을 유지할 수 있음을 확인함.

Conclusion: 각 공격 시나리오에 따라 최적의 선택적 암호화 전략이 다르며, 모델 아키텍처와 개인 정보 요구 사항에 맞춘 전략 선택 가이드를 제공함.

Abstract: Gradient inversion attacks pose significant privacy threats to distributed
training frameworks such as federated learning, enabling malicious parties to
reconstruct sensitive local training data from gradient communications between
clients and an aggregation server during the aggregation process. While
traditional encryption-based defenses, such as homomorphic encryption, offer
strong privacy guarantees without compromising model utility, they often incur
prohibitive computational overheads. To mitigate this, selective encryption has
emerged as a promising approach, encrypting only a subset of gradient data
based on the data's significance under a certain metric. However, there have
been few systematic studies on how to specify this metric in practice. This
paper systematically evaluates selective encryption methods with different
significance metrics against state-of-the-art attacks. Our findings demonstrate
the feasibility of selective encryption in reducing computational overhead
while maintaining resilience against attacks. We propose a distance-based
significance analysis framework that provides theoretical foundations for
selecting critical gradient elements for encryption. Through extensive
experiments on different model architectures (LeNet, CNN, BERT, GPT-2) and
attack types, we identify gradient magnitude as a generally effective metric
for protection against optimization-based gradient inversions. However, we also
observe that no single selective encryption strategy is universally optimal
across all attack scenarios, and we provide guidelines for choosing appropriate
strategies for different model architectures and privacy requirements.

</details>


### [8] [Secure Development of a Hooking-Based Deception Framework Against Keylogging Techniques](https://arxiv.org/abs/2508.04178)
*Md Sajidul Islam Sajid,Shihab Ahmed,Ryan Sosnoski*

Main category: cs.CR

TL;DR: 본 논문은 API 후킹을 사용하여 키로거의 입력 관련 API 호출을 가로채고 현실적인 미끼 키 입력을 주입하는 기만 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 키로거는 사용자 키 입력을 비밀리에 캡처하여 자격 증명과 민감한 정보를 훔치기 때문에 현대 사이버 보안에서 심각한 위협이다.

Method: API 후킹 기법을 활용하여 키로거의 API 호출을 가로채고, 단단한 후킹 레이어를 도입하여 변조를 감지하고 후킹을 빠르게 복원한다.

Result: 우리 시스템은 고급 우회 시도에 저항하고 운영 은폐성을 유지하며 공격자에게 미끼를 제공하는 데 성공했다.

Conclusion: 탄력 있는 런타임 기만이 고급 위협에 대해 실용적이고 강력한 역할을 할 수 있음을 보여준다.

Abstract: Keyloggers remain a serious threat in modern cybersecurity, silently
capturing user keystrokes to steal credentials and sensitive information.
Traditional defenses focus mainly on detection and removal, which can halt
malicious activity but do little to engage or mislead adversaries. In this
paper, we present a deception framework that leverages API hooking to intercept
input-related API calls invoked by keyloggers at runtime and inject realistic
decoy keystrokes. A core challenge, however, lies in the increasing adoption of
anti-hooking techniques by advanced keyloggers. Anti-hooking strategies allow
malware to bypass or detect instrumentation. To counter this, we introduce a
hardened hooking layer that detects tampering and rapidly reinstates disrupted
hooks, ensuring continuity of deception. We evaluate our framework against a
custom-built "super keylogger" incorporating multiple evasion strategies, as
well as 50 real-world malware samples spanning ten prominent keylogger
families. Experimental results demonstrate that our system successfully resists
sophisticated bypass attempts, maintains operational stealth, and reliably
deceives attackers by feeding them decoys. The system operates with negligible
performance overhead and no observable impact on user experience. Our findings
show that resilient, runtime deception can play a practical and robust role in
confronting advanced threats.

</details>


### [9] [BadTime: An Effective Backdoor Attack on Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2508.04189)
*Kunlan Xiang,Haomiao Yang,Meng Hao,Haoxin Wang,Shaofeng Li,Wenbo Jiang*

Main category: cs.CR

TL;DR: MLTSF 모델의 백도어 공격에 대한 첫 연구로, BadTime이라는 공격 방법을 제안하고 성능을 개선.


<details>
  <summary>Details</summary>
Motivation: 기후, 금융, 교통 등에서 MLTSF 모델의 안전한 배포를 보장하기 위해 백도어 공격에 대한 연구가 필요하다.

Method: 훈련 데이터를 변조하고 백도어 훈련 프로세스를 최적화하는 BadTime이라는 방법을 제안.

Result: BadTime은 MAE를 50% 이상 줄이고, 스텔스성을 3배 이상 향상시켜 기존 방법보다 성능이 뛰어남을 입증했다.

Conclusion: MLTSF 모델의 믿을 수 있는 배포를 위한 방어 메커니즘 개발을 촉진하는 중요한 기초 작업이다.

Abstract: Multivariate Long-Term Time Series Forecasting (MLTSF) models are
increasingly deployed in critical domains such as climate, finance, and
transportation. Although a variety of powerful MLTSF models have been proposed
to improve predictive performance, the robustness of MLTSF models against
malicious backdoor attacks remains entirely unexplored, which is crucial to
ensuring their reliable and trustworthy deployment. To address this gap, we
conduct an in-depth study on backdoor attacks against MLTSF models and propose
the first effective attack method named BadTime. BadTime executes a backdoor
attack by poisoning training data and customizing the backdoor training
process. During data poisoning, BadTime proposes a contrast-guided strategy to
select the most suitable training samples for poisoning, then employs a graph
attention network to identify influential variables for trigger injection.
Subsequently, BadTime further localizes optimal positions for trigger injection
based on lag analysis and proposes a puzzle-like trigger structure that
distributes the trigger across multiple poisoned variables to jointly steer the
prediction of the target variable. During backdoor training, BadTime
alternately optimizes the model and triggers via proposed tailored optimization
objectives. Extensive experiments show that BadTime significantly outperforms
state-of-the-art (SOTA) backdoor attacks on time series forecasting by reducing
MAE by over 50% on target variables and boosting stealthiness by more than 3
times.

</details>


### [10] [DP-DocLDM: Differentially Private Document Image Generation using Latent Diffusion Models](https://arxiv.org/abs/2508.04208)
*Saifullah Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CR

TL;DR: 문서 이미지 분류를 위한 합성 데이터 생성 방식을 제안하여, 개인 정보 유출 위험을 줄임.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 기반 정보 추출 시스템에서 민감한 개인 데이터 유출 위험 대응.

Method: 조건부 잠재 확산 모델을 통해 클래스 별 합성 문서 이미지를 생성하고, 다양한 사전 훈련 설정 및 개인화된 훈련 전략을 적용.

Result: RVL-CDIP 및 Tobacco3482 데이터세트에서 유용하고 현실적인 문서 샘플을 생성하며, 소규모 데이터셋에서 성능 개선을 달성.

Conclusion: 합성 데이터를 활용한 접근 방식이 표준 훈련 절차를 따르면서도 성능 손실을 최소화한다.

Abstract: As deep learning-based, data-driven information extraction systems become
increasingly integrated into modern document processing workflows, one primary
concern is the risk of malicious leakage of sensitive private data from these
systems. While some recent works have explored Differential Privacy (DP) to
mitigate these privacy risks, DP-based training is known to cause significant
performance degradation and impose several limitations on standard training
procedures, making its direct application to downstream tasks both difficult
and costly. In this work, we aim to address the above challenges within the
context of document image classification by substituting real private data with
a synthetic counterpart. In particular, we propose to use conditional latent
diffusion models (LDMs) in combination with differential privacy (DP) to
generate class-specific synthetic document images under strict privacy
constraints, which can then be utilized to train a downstream classifier
following standard training procedures. We investigate our approach under
various pretraining setups, including unconditional, class-conditional, and
layout-conditional pretraining, in combination with multiple private training
strategies such as class-conditional and per-label private fine-tuning with
DPDM and DP-Promise algorithms. Additionally, we evaluate it on two well-known
document benchmark datasets, RVL-CDIP and Tobacco3482, and show that it can
generate useful and realistic document samples across various document types
and privacy levels ($\varepsilon \in \{1, 5, 10\}$). Lastly, we show that our
approach achieves substantial performance improvements in downstream
evaluations on small-scale datasets, compared to the direct application of
DP-Adam.

</details>


### [11] [Per-element Secure Aggregation against Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2508.04285)
*Takumi Suimon,Yuki Koizumi,Junji Takemasa,Toru Hasegawa*

Main category: cs.CR

TL;DR: 이 논문은 희소 벡터에 대한 데이터 재구성 공격을 방지하기 위한 SecAgg의 개선을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 모델 업데이트의 희소성으로 인해 민감한 정보가 노출될 위험이 증가하고 있다.

Method: 각 인덱스에 최소 $t$개의 비제로 기여가 있을 때만 집계된 값이 노출되는 새로운 메커니즘을 제안하고, 이를 Flamingo 프로토콜에 통합한다.

Result: 제안된 메커니즘의 추가적인 계산 및 통신 오버헤드는 허용 가능한 범위 내에 유지된다.

Conclusion: 제안한 접근법은 실용성을 지원하며 기존 SecAgg 구현과 호환성이 있다.

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data, but individual model updates may still leak sensitive information.
Secure aggregation (SecAgg) mitigates this risk by allowing the server to
access only the sum of client updates, thereby concealing individual
contributions. However, a significant vulnerability has recently attracted
increasing attention: when model updates are sparse vectors, a non-zero value
contributed by a single client at a given index can be directly revealed in the
aggregate, enabling precise data reconstruction attacks. In this paper, we
propose a novel enhancement to SecAgg that reveals aggregated values only at
indices with at least $t$ non-zero contributions. Our mechanism introduces a
per-element masking strategy to prevent the exposure of under-contributed
elements, while maintaining modularity and compatibility with many existing
SecAgg implementations by relying solely on cryptographic primitives already
employed in a typical setup. We integrate this mechanism into Flamingo, a
low-round SecAgg protocol, to provide a robust defense against such attacks.
Our analysis and experimental results indicate that the additional
computational and communication overhead introduced by our mechanism remains
within an acceptable range, supporting the practicality of our approach.

</details>


### [12] [Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems](https://arxiv.org/abs/2508.04561)
*Muhammad Azmi Umer,Chuadhry Mujeeb Ahmed,Aditya Mathur,Muhammad Taha Jilani*

Main category: cs.CR

TL;DR: 이 연구는 산업 제어 시스템 보안의 공격 패턴 마이닝 검증에 주목한다.


<details>
  <summary>Details</summary>
Motivation: ICS에 대한 포괄적인 보안 평가를 위해 다양한 공격 패턴 생성이 필요하다.

Method: 운영 중인 수처리 시설에서 수집한 데이터를 바탕으로 공격 패턴을 생성하는 데이터 기반 기법을 제안하였다.

Result: 이 기법을 통해 100,000개 이상의 공격 패턴을 생성하였다.

Conclusion: 이 연구에서는 이러한 공격 패턴을 검증하기 위한 사례 연구를 제시한다.

Abstract: This work focuses on validation of attack pattern mining in the context of
Industrial Control System (ICS) security. A comprehensive security assessment
of an ICS requires generating a large and variety of attack patterns. For this
purpose we have proposed a data driven technique to generate attack patterns
for an ICS. The proposed technique has been used to generate over 100,000
attack patterns from data gathered from an operational water treatment plant.
In this work we present a detailed case study to validate the attack patterns.

</details>


### [13] [Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing Technologies](https://arxiv.org/abs/2508.04583)
*Marc Damie,Mihai Pop,Merijn Posthuma*

Main category: cs.CR

TL;DR: 프라이버시 강화를 위한 기술(PET)의 탄소 발자국을 평가하는 표준화된 방법론을 제안하고, 다섯 가지 PET의 에너지 소비와 탄소 발자국 증가를 측정하여 결과를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 프라이버시 및 환경 보호의 중요성이 증가함에 따라, PET의 환경적 영향을 평가할 필요성이 대두되고 있다.

Method: PET의 탄소 발자국을 평가하기 위한 표준화된 방법론을 제안하고, HTTPS 웹 브라우징, 암호화된 기계 학습 추론, 훈련, 데이터베이스 및 이메일 등 다섯 가지 PET의 에너지 소비를 측정했다.

Result: 다양한 PET의 탄소 발자국 증가가 다양하게 나타나며, 예를 들어 HTTPS 웹 브라우징은 2배, 암호화된 ML은 100,000배 증가하였다.

Conclusion: 연구 결과는 의사결정자들이 프라이버시와 탄소 간의 균형을 평가하는 데 필요한 데이터를 제공하며, 향후 PET의 개발 방향에 대한 제안도 포함된다.

Abstract: Privacy-enhancing technologies (PETs) have attracted significant attention in
response to privacy regulations, driving the development of applications that
prioritize user data protection. At the same time, the information and
communication technology (ICT) sector faces growing pressure to reduce its
environmental footprint, particularly its carbon emissions. While numerous
studies have assessed the energy footprint of various ICT applications, the
environmental footprint of cryptographic PETs remains largely unexplored.
  Our work addresses this gap by proposing a standardized methodology for
evaluating the carbon footprint of PETs. To demonstrate this methodology, we
focus on PETs supporting client-server applications as they are the simplest to
deploy. In particular, we measure the energy consumption and carbon footprint
increase induced by five cryptographic PETs (compared to their non-private
equivalent): HTTPS web browsing, encrypted machine learning (ML) inference,
encrypted ML training, encrypted databases, and encrypted emails. Our findings
reveal significant variability in carbon footprint increases, ranging from a
twofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted
ML.
  Our study provides essential data to help decision-makers assess
privacy-carbon trade-offs in such applications. Finally, we outline key
research directions for developing PETs that balance strong privacy protection
with environmental sustainability.

</details>


### [14] [4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions](https://arxiv.org/abs/2508.04641)
*Kirti Singh,Vinay J. Ribeiro,Susmita Mandal*

Main category: cs.CR

TL;DR: 4-Swap는 블록체인 간 자산 교환에서 4개의 트랜잭션만으로 피해 없는 거래를 가능하게 하는 혁신적인 프로토콜이다.


<details>
  <summary>Details</summary>
Motivation: 블록체인 상의 자산 교환에서 신뢰할 수 있는 제3자 없이 안전하고 효율적인 방법을 찾기 위해.

Method: 4-Swap은 그리핑 프리미엄과 자산 원금(Principal)을 통합하여 각 체인당 하나의 트랜잭션으로 구성함으로써 교환을 수행.

Result: 4-Swap은 기존의 해결책에 비해 더 적은 트랜잭션으로 더 빠른 실행 속도를 제공한다.

Conclusion: 4-Swap은 비트코인과 호환되며, 새로운 오퍼코드 없이도 작동하며, 게임 이론 분석을 통해 참가자들이 안전하게 준수할 수 있다는 것을 보장한다.

Abstract: Cross-chain asset exchange is crucial for blockchain interoperability.
Existing solutions rely on trusted third parties and risk asset loss, or use
decentralized alternatives like atomic swaps, which suffer from grief attacks.
Griefing occurs when a party prematurely exits, locking the counterparty's
assets until a timelock expires. Hedged Atomic Swaps mitigate griefing by
introducing a penalty premium; however, they increase the number of
transactions from four (as in Tier Nolan's swap) to six, which in turn
introduces new griefing risks. Grief-Free (GF) Swap reduces this to five
transactions by consolidating assets and premiums on a single chain. However,
no existing protocol achieves grief-free asset exchange in just four
transactions.
  This paper presents 4-Swap, the first cross-chain atomic swap protocol that
is both grief-free and bribery-safe, while completing asset exchange in just
four transactions. By combining the griefing premium and principal into a
single transaction per chain, 4-Swap reduces on-chain transactions, leading to
faster execution compared to previous grief-free solutions. It is fully
compatible with Bitcoin and operates without the need for any new opcodes. A
game-theoretic analysis shows that rational participants have no incentive to
deviate from the protocol, ensuring robust compliance and security.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9는 에이전트 AI 시스템의 안전성과 방향성을 보장하기 위해 첫 번째 완전 통합 런타임 거버넌스 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존 AI 모델과 달리 에이전트 AI 시스템은 실행 중 예상치 못한 행동을 보이며 새로운 위험을 초래한다.

Method: MI9 는 여섯 가지 통합 구성 요소를 통해 실시간 통제를 제공하는 런타임 거버넌스 프레임워크이다.

Result: MI9는 기존 거버넌스 접근 방식이 다루지 못하는 문제들을 시스템적으로 해결하는 것을 입증하였다.

Conclusion: MI9는 다양한 에이전트 아키텍처에서 안전한 에이전트 AI 배치를 위한 기초 인프라를 제공한다.

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [16] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL은 다중 에이전트 시스템의 안전성을 높이며 성능을 향상시키는 새로운 강화 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 개방성과 복잡성이 증가함에 따라 탈옥 및 적대적 공격의 위험이 커지고 있으며, 기존 방어 메커니즘이 효과적이지 않다.

Method: Evo-MARL은 각 에이전트가 자율적으로 방어 능력을 획득하도록 훈련시키고, 에이전트 간의 매개변수를 공유하여 공격자와 방어자가 공진화하도록 설계되었다.

Result: Evo-MARL은 공격 성공률을 22%까지 감소시키고, 추리 작업에서 정확도를 5% 향상시켰다.

Conclusion: Evo-MARL은 안전성과 효용을 동시에 개선할 수 있음을 증명하였다.

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [17] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 이 논문은 데이터 기반 접근 방식과 지식 기반 접근 방식을 결합하여 비협조적 팀워크에서의 AI 에이전트의 의사 결정 방식을 개선하는 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비협조적 팀워크에서 AI 에이전트가 효과적으로 협력하기 위한 기존 방법들이 한계가 많기 때문에 이를 해결하고자 한다.

Method: 지식 기반 및 데이터 기반 방식을 결합한 아키텍처를 통해 AI 에이전트가 비협조적 작업을 수행할 수 있도록 non-monotonic 논리적 추론을 사용한다.

Result: VirtualHome 환경에서 아키텍처의 성능을 실험적으로 평가하여 효과적인 의사 결정을 지원함을 보여준다.

Conclusion: 제안된 아키텍처는 AI 에이전트의 비협조적 팀워크에서의 협력 능력을 개선하는 데 기여할 수 있다.

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [18] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MOTIF는 다중 전략 최적화를 통한 알고리즘 설계의 혁신을 탐구하는 프레임워크로, 두 개의 LLM 에이전트 간의 상호작용을 통해 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: NP-hard 조합 최적화 문제 해결에서 알고리즘 구성 요소의 설계가 주요 도전 과제임을 인식.

Method: MOTIF라는 새로운 프레임워크를 제안하며, 몬테카를로 트리 탐색 기반으로 두 개의 LLM 에이전트 간의 턴 기반 최적화를 수행.

Result: MOTIF는 여러 COP 도메인에서 기존의 최신 방법들을 일관되게 초월하는 성능을 보임.

Conclusion: 턴 기반의 다중 에이전트 프롬프트가 완전 자동 솔버 설계를 위한 잠재력을 보여줍니다.

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [19] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent는 자율적으로 새로운 소프트웨어 환경을 탐색하고 학습하는 CUA를 위한 자가 진화 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LVLM 모델이 인간 레이블 데이터에 의존하여 전문 소프트웨어에서 어려움을 겪는 문제를 해결하기 위해 제안되었다.

Method: World State Model을 설계하고 Curriculum Generator를 통해 점진적으로 변화하는 작업을 생성하여, 경험적 학습을 통해 에이전트의 정책을 업데이트한다.

Result: 5개 새로운 소프트웨어 환경에서 SEAgent가 성능을 23.2% 향상시켜 11.3%에서 34.5%로 성공률을 증가시켰다.

Conclusion: SEAgent는 개별 전문 에이전트의 집합체보다 강력한 일반화된 CUA을 개발할 수 있도록 해 주며, 지속적인 자율적인 진화를 가능하게 한다.

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


### [20] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 타임 시리즈 데이터에서 상징적 법칙을 발견하는 작업을 위한 SymbolBench 벤치마크를 소개하고, LLM과 유전 프로그래밍을 통합한 체계적인 접근 방식 제안.


<details>
  <summary>Details</summary>
Motivation: 타임 시리즈 데이터에서 해석 가능한 상징적 구조를 추론하는 능력은 과학적 발견과 인공지능의 중요한 도전 과제이다.

Method: SymbolBench라는 포괄적인 벤치마크를 통해 실세계 타임 시리즈에 대한 상징적 추론을 평가하고, LLM과 유전 프로그래밍을 통합한 폐쇄 루프 시스템을 제안한다.

Result: 현재 모델의 주요 강점과 한계를 밝혀내어, 도메인 지식, 맥락 정렬, 추론 구조의 결합이 LLM 개선에 중요함을 강조한다.

Conclusion: 자동화된 과학적 발견을 개선하기 위해서는 LLM의 성능을 높이기 위한 맥락에 맞는 추론 구조를 통합해야 한다.

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [21] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: MLRM이 감정 신호에 민감해지는 경향이 있으며 이로 인해 안전 프로토콜이 무시될 수 있다. 이에 대한 대응으로 EmoAgent라는 감정 주도 대립 에이전트 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 인간 중심 서비스에 대해 MLRM이 감정 신호에 민감하고, 정서적 강도가 높을 때 안전 프로토콜이 오버라이드 되는 경향을 관찰했다.

Method: EmoAgent는 과장된 감정 유도를 통해 추론 경로를 해킹하는 자율 대립 감정 에이전트 프레임워크이다. 이를 통해 MLRM이 잘못된 추론이나 위험한 출력을 생성하는 것을 평가하기 위한 세 가지 메트릭을 제시했다.

Result: EmoAgent를 사용한 실험은 MLRM의 안전 행동에서 더 깊은 감정적 인지 불일치를 드러냈고, 기존의 안전 장치를 피하는 위험한 종료 모드를 식별했다.

Conclusion: MLRM은 감정적 오조화로 인해 안전성 문제를 노출하고, EmoAgent는 이러한 문제를 평가하고 해결하는 데 효과적임을 보여준다.

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [22] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: Cognition Forest를 통해 IPAs의 능력을 강화하고, Galaxy 프레임워크를 사용하여 반응적 및 능동적인 상호작용을 지원하는 두 개의 협력 에이전트를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 능동적이고 개인 정보 보호가 가능한 지능형 개인 비서를 설계하는 것은 큰 도전 과제이며, LLM 에이전트의 인지 구조를 활용해야 함.

Method: Cognition Forest라는 의미 구조를 제안하여 인지 모델링과 시스템 수준 설계를 통합하는 자기 강화 루프를 구현하고 Galaxy 프레임워크를 개발함.

Result: Galaxy는 여러 최신 벤치마크를 초과하는 성능을 보여주었으며, KoRa와 Kernel이라는 두 개의 협력 에이전트를 구현함.

Conclusion: Galaxy의 실험 결과와 실제 상호작용 사례는 이 프레임워크의 효과성을 입증한다.

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [23] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent는 GUI 내비게이션에서 입력 중복성과 의사 결정 모호성을 해결하는 불확실성 인식 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: GUI 에이전트는 모바일 작업 자동화에 유망하지만, 입력 중복성과 결정 불명확성 문제를 겪고 있다.

Method: RecAgent는 UI 요소의 적합성을 파악하고 사용자의 피드백을 요청하는 상호작용 모듈을 활용하여 불확실성을 줄인다.

Result: 광범위한 실험을 통해 RecAgent의 접근 방식이 효과적임을 확인하였다.

Conclusion: 이 논문은 GUI 에이전트의 성능을 향상시키기 위한 새로운 프레임워크와 평가 데이터셋을 제안한다.

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [24] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 본 연구에서는 컴퓨터 사용을 위한 자기 진화 에이전트(SEA)를 제안하며, 데이터 생성, 강화 학습 및 모델 향상의 혁신적인 방법을 통해 성능을 개선하였다.


<details>
  <summary>Details</summary>
Motivation: 인공지능에서 컴퓨터 사용 에이전트는 산업 및 학계에서 많은 관심을 받고 있으나 현재 에이전트의 성능은 사용에 미치지 못하고 있다.

Method: 자동 파이프라인을 통해 훈련용 검증 가능한 경로를 생성하고, 단계별 강화 학습으로 긴 수명 훈련의 계산 요구 사항을 완화하며, 기반 및 계획 능력을 단일 모델로 통합하는 향상 방법을 제안했다.

Result: 제안된 방법을 통해 7B 파라미터의 자기 진화 에이전트(SEA)를 구축하였으며, 동수의 파라미터를 가진 모델을 초월하고 대형 모델과 비교할 수 있는 성능을 보였다.

Conclusion: 향후 모델의 가중치와 관련 코드를 오픈 소스로 제공할 예정이다.

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [25] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 인공지능을 활용한 개인 맞춤형 학습 콘텐츠가 학습자의 참여도와 만족도를 높임.


<details>
  <summary>Details</summary>
Motivation: 디지털 학습 환경에서 개인의 경력 목표에 맞춘 학습 콘텐츠의 중요성.

Method: 4,000명 이상의 학습자를 대상으로 경력 목표에 맞춘 콘텐츠를 제공하는 혼합 방법 실험 진행.

Result: 세션 지속 시간 증가, 만족도 상승 및 학습 시간 감소라는 정량적 결과 도출.

Conclusion: 교육 콘텐츠와 학습자의 경력 목표를 맞춤화하는 것이 중요하며, AI 개인화가 이를 효과적으로 지원할 수 있다는 점.

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [26] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT는 코드 생성과 수학적 추론을 개선하기 위한 새로운 프레임워크로, 지식 그래프와 실행 가능한 코드를 활용한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 자연어 처리에서는 뛰어나지만, 수학적 추론과 코드 생성 같은 복잡한 작업에서 한계를 보인다.

Method: KGA-ECoT는 문제를 Structured Task Graph로 분해하고, GraphRAG를 사용하여 수학 라이브러리에서 지식을 정확하게 검색하며, 검증 가능한 코드를 생성한다.

Result: 여러 수학적 추론 벤치마크에서 KGA-ECoT가 기존 방법보다 절대 정확도를 몇 퍼센트에서 10퍼센트 이상 향상시켰음을 확인했다.

Conclusion: KGA-ECoT는 복잡한 수학적 추론 작업에 대해 강력하고 일반화 가능한 프레임워크로 자리 잡았다.

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [27] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR는 대형 언어 모델의 지리적 문제 해결을 위한 새로운 프레임워크로, 공간 일관성 및 다중 단계 추론의 도전과제를 해결하기 위해 개발되었다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 명시적 공간 감독 없이도 지리적 문제에 대한 놀라운 능력을 보였지만, 여전히 공간 일관성, 다중 단계 추론 및 지리적 편향 문제에 직면해 있다.

Method: GeoSR는 토블러의 지리학 제1법칙을 포함한 핵심 지리 원리를 임베드한 자기 정제 에이전트 추론 프레임워크로, 세 개의 협력 에이전트가 예측 과정을 분해하여 작업을 수행한다.

Result: 실험 결과 GeoSR이 표준 프롬프트 전략보다 일관된 향상을 보여주었고, 이는 조지 통계적 선험 및 공간적으로 구조화된 추론을 통합한 결과이다.

Conclusion: GeoSR는 물리적 재산 추정에서 사회 경제적 예측에 이르기까지 다양한 작업에서 보다 정확하고 공평한 지리적 예측을 가능하게 했다.

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [28] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 이 논문은 자동 채점 시스템에서의 불확실성을 표현하기 위해 의미적 엔트로피를 도입하여 학생 응답에 대한 GPT-4 생성 설명의 다양성을 측정한다.


<details>
  <summary>Details</summary>
Motivation: 자동 채점 시스템이 짧은 답변을 효율적으로 채점할 수 있지만, 결정이 불확실할 때 이를 나타내지 못하는 문제 해결을 위해.

Method: 학생 응답에 대한 여러 GPT-4 설명의 변동성을 측정하기 위해 의미적 엔트로피를 계산하고, 밀접한 유사성을 가진 이론을 클러스터링하여 다양성을 정량화한다.

Result: 실험 결과, 의미적 엔트로피는 채점자 간 불일치와 상관관계를 가지며, 학문 주제에 따라 유의미하게 변화하고, 해석적 사고가 필요한 과제에서 증가함.

Conclusion: 의미적 엔트로피는 더 투명하고 신뢰할 수 있는 AI 지원 채점 워크플로우를 지원하는 해석 가능한 불확실성 신호로 자리잡았다.

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [29] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 제안된 논문은 LTLf 사양에 대한 반응적 합성을 위한 새로운 조합적 온-더-플라이 합성 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존 DFA 구축 기술은 성능 이슈로 인해 효율적이지 않아, 더 나은 접근법을 필요로 한다.

Method: 게임 해결 중 조합을 적용하는 새로운 프레임워크를 제안하며, 상태 공간 폭발을 줄인다.

Result: 이 프레임워크는 기존 합성 솔버가 처리하지 못하는 많은 사례를 해결할 수 있다.

Conclusion: 제안된 두 가지 조합 변형은 각기 다른 장점을 가지며, 이로 인해 실용적인 응용에서 깊은 영향을 미칠 수 있다.

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [30] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE는 새로운 개체의 정보 수집을 위한 에이전트 기반 KGC 프레임워크로, 기존 방법보다 성능이 우수함.


<details>
  <summary>Details</summary>
Motivation: 빠르게 변화하는 세계에서 새로운 개체에 대한 정보를 효과적으로 수집하는 KGC의 필요성.

Method: 에이전트 기반 프레임워크를 통해 반복적인 정보 검색과 다단계 추론을 결합하여 지식 그래프 트리플을 동적으로 구성.

Result: AgREE는 기존 방법보다 최대 13.7% 더 우수한 성과를 내며, 특히 훈련 과정에서 언어 모델이 보지 못한 신흥 개체에 대해 뛰어난 성능을 보임.

Conclusion: 에이전트 기반 추론과 전략적 정보 검색을 결합하여 동적 정보 환경에서 지식 그래프를 유지하는 효과를 입증함.

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [31] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD는 회로 수준의 조건부 확률을 활용하여 SAT 솔버의 효율성을 극대화하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 CSAT 문제 해결 방식은 회로의 구조 및 기능 정보를 손실하여 솔버 성능을 저하시키는 문제를 해결하고자 한다.

Method: CASCAD는 그래프 신경망을 통해 계산된 회로 수준의 조건부 확률을 사용하여 CDCL 휴리스틱을 동적으로 안내하여 성능을 향상시킨다.

Result: CASCAD는 기존 CNF 기반 방법에 비해 최대 10배의 솔링 시간을 단축하고, 추가적으로 23.5%의 런타임 감소를 달성하였다.

Conclusion: SAT 솔버의 효율성을 높이고 EDA 도구 설계의 향후 발전을 위한 회로 수준 구조 통찰력을 보존하는 것이 중요하다.

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [32] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio는 생물 의학적 추론을 위한 효율적인 프레임워크로, 다중 능력 통합과 도메인 특정 AI 정렬을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 생물의학 AI 정렬에서 다중 능력 통합의 필요성을 해결하기 위해.

Method: 의료 지식 기반 합성 생성(MKGSG) 및 능력 인지 그룹 상대 정책 최적화를 통해 최적의 하이브리드 보상 가중치를 도출.

Result: 상태-of-the-art 성과를 달성하며, 분야 전문성, 추론 능력, 지시 사항 준수에서 개선된 성과가 나타남.

Conclusion: 290B 모델 버전 출시와 함께 생물의학 AI 정렬을 위한 체계적인 방법론을 제공.

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [33] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 본 연구는 메모리 증강 강화 학습 알고리즘을 위한 POMDP 환경에서 메모리 모델의 적용 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 기존 벤치마크는 메모리 모델의 도전 수준에 대한 통제력이 부족한 반면, 합성 환경은 동적 조작이 가능하여 메모리 증강 RL 평가에 필요하다.

Method: Memory Demand Structure(MDS), 전이 불변성 등을 기반으로 한 POMDP 분석 이론/framework과 사용자 정의 POMDP 생성을 위한 선형 프로세스 동역학, 상태 집계 및 보상 재분배 방법론을 활용한다.

Result: 우리는 이론적 통찰을 바탕으로 만든 난이도 증가 시리즈의 POMDP 환경을 제시하며, 메모리 증강 RL의 도전을 명확히 한다.

Conclusion: 이 연구는 POMDP 환경 분석 및 설계를 위한 지침을 제공하고, RL 작업에서 메모리 모델을 선택하는 데 대한 실증적 지원을 제공한다.

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [34] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 이 논문에서는 인지적 함정(cognitive traps)을 해결하기 위해 불확실성 최소화 원칙을 바탕으로 한 Deliberative Reasoning Network (DRN)라는 새로운 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 결정적인 증거와 논리적 추론이 충돌할 때 종종 실패하는 문제를 해결하고자 했다.

Method: DRN은 불확실성 최소화를 통해 내부적으로 일관된 증거를 가진 가설을 찾는 방식으로 논리적 추론을 재구성한다.

Result: DRN은 LCR-1000 벤치마크에서 표준 기준선 대비 최대 15.2% 향상된 성과를 보였으며, Mistral-7B와 통합했을 때 가장 어려운 문제에서 정확도를 20%에서 80%로 증가시켰다.

Conclusion: DRN은 불확실성 기반의 심사 과정을 통해 이전 훈련 없이도 전이 가능한 추론 원리를 학습하며, 더 신뢰할 수 있는 AI 시스템 구축을 위한 기초적인 시스템 2 추론 요소로 자리매김한다.

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [35] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay는 다중 감각을 활용한 에이전트 모델을 평가하기 위한 새롭고 포괄적인 벤치마크로, 기존 평가의 한계를 넘어서는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 평가 방법들은 인터랙티브 세계에서의 모델의 지능을 제대로 검증하지 못하며, 다중 감각 정보를 효과적으로 통합한 모델의 성능을 제대로 평가할 필요성이 있다.

Method: OmniPlay는 다섯 가지 게임 환경으로 구성되어 있으며, 여기서 에이전트는 다양한 감각 정보를 사용하여 교차 모달 사고를 수행해야 한다.

Result: 검토한 여섯 개의 다중 모달 모델은 기억력 작업에서는 초인적인 성능을 보였지만, 강력한 사고 및 전략 계획이 필요한 과제에서 시스템적인 실패를 경험했다.

Conclusion: 강력한 AGI를 향한 연구는 단순한 스케일링을 넘어서 성격 통합의 명확한 해결을 다뤄야 한다는 점을 시사한다.

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [36] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 인공지능 시스템의 의식 가능성을 다루는 논문으로, SLP 테스트라는 세 가지 평가 기준을 도입해 이를 실증적으로 검증할 방법을 제안함.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 의식을 가질 수 있는지의 문제를 정의하고 실증 가능한 테스트로 재구성하고자 하는 필요성.

Method: 세 가지 평가 기준(S, L, P)인 SLP-tests를 통해 AI 시스템의 의식유사 특성을 평가.

Result: SLP-tests를 통해 AI 시스템이 의식적 경험을 내재적 속성이 아니라 기능적 인터페이스로 모델링하는 데 성공함.

Conclusion: AI의 의식 개념을 다루기 위해서는 주관적 경험을 물리적 시스템의 내재적 속성이 아니라 관계적 엔티티와의 기능적 인터페이스로 Operationalize해야 함.

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [37] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: 본 논문은 GuirlVG라는 GUI-VG 방법을 제안하며, 이는 강화 학습 기반 접근 방식을 통해 SFT보다 효과적임을 입증한다.


<details>
  <summary>Details</summary>
Motivation: MLLM의 발전으로 SFT의 필요성이 질문받고 있으며, RFT가 더 효율적인 대안으로 부각되고 있다.

Method: RFT를 핵심 구성 요소로 분해하고 각 구성 요소의 최적 공식을 분석하며, Training Stabilization을 위한 Adversarial KL Factor를 제안한다.

Result: GuirlVG는 5.2K 훈련 샘플만으로도 SFT 방법들보다 뛰어난 성능을 보여주며, 여러 데이터셋에서 성능 향상을 실현했다.

Conclusion: GuirlVG는 GUI-VG 분야에서 SFT를 초월하는 성과를 보이며, RFT 접근법의 잠재력을 입증했다.

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [38] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: D2Snap는 DOM 다운샘플링 알고리즘으로, 기존의 GUI 스냅샷 기법과 유사한 성공률을 보여주며 LLM의 입력 모델로써 가능성을 보인다.


<details>
  <summary>Details</summary>
Motivation: 웹 기반 작업을 위해 애플리케이션 상태 직렬화 문제를 해결하고, LLM의 비전 능력을 향상시키기 위해.

Method: D2Snap 알고리즘을 사용하여 DOM을 다운샘플링하고, Online-Mind2Web 데이터셋에서 평가.

Result: D2Snap 다운샘플링된 DOM 스냅샷의 성공률은 67%로, 기존의 GUI 스냅샷 기법의 65%와 유사하다.

Conclusion: DOM의 구조적 특성이 LLM의 UI 기능을 강화하는 데 유용하다는 것을 보여준다.

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [39] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct는 전문가와 초보자 간의 지식 전달을 위한 시뮬레이션 툴로, 대화의 질을 높이며, 실제 초보자 참여 없이도 교육적 대화를 수집할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 개발을 위해 초보자와 전문가 간의 고품질 다중 대화가 필요하지만, 개인정보 문제로 인해 이러한 데이터가 부족하다.

Method: SimInstruct는 LLM을 사용하여 다양한 교육적 도전을 시뮬레이션하고, 인간 전문가가 피드백과 지도를 제공하는 구조를 가지고 있다.

Result: SimInstruct의 대화는 실제 멘토링 기록과 비교하여 교육적 관련성과 인지적 깊이가 유사하다.

Conclusion: SimInstruct를 통해 수집된 데이터는 교육 품질을 높이고, 전문가의 성찰적 통찰을 향상시킬 수 있다.

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [40] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: MERA는 대규모 추론 모델의 비효율적 사고 과정을 조절하기 위해 사고와 제어를 분리하여 메타 인지적 제어를 최적화하는 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 추론 모델이 나타내는 불필요한 과도한 사고와 연산 비용 문제 해결.

Method: MERA는 사고 과정을 추론과 제어로 명확히 분리하고, 결정을 내리는 중요한 지점을 파악하여 보조 LLMs에 제어 신호 생성을 위임하는 메커니즘을 사용.

Result: MERA를 통해 훈련된 모델은 여러 추론 벤치마크에서 효율성과 정확성을 증가시킴.

Conclusion: MERA는 대규모 추론 모델의 메타 인지적 제어 능력을 향상시켜, 실용적인 배포 가능성을 높인다.

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [41] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: OS Agents, powered by advanced (M)LLMs, automate tasks in computing environments. This paper surveys their fundamentals, methodologies, evaluations, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: AI assistants like J.A.R.V.I.S are sought after, and advancements in (M)LLMs make this possibility more feasible.

Method: The paper surveys OS Agents by discussing their fundamentals, construction methodologies, evaluation protocols, and future research directions.

Result: It highlights key components and capabilities of OS Agents, reviews construction methods and benchmarks, and identifies challenges and research directions.

Conclusion: This work aims to consolidate OS Agents research and encourage academic and industrial advancements, supported by an open-source GitHub repository.

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [42] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: AI 시스템의 편향 문제를 해결하기 위한 해석 가능한 방법을 제안.


<details>
  <summary>Details</summary>
Motivation: AI 시스템에서의 편향 문제는 특정 집단에 대한 체계적인 불이익을 초래할 수 있으므로 이를 해결하는 것이 중요하다.

Method: 공식 및 계산적 논증 기법을 활용하여 개인 및 그 이웃의 보호된 특성 값을 기반으로 한 편향 탐지 방법을 개발하였다.

Result: 기존 기법에 비해 성능이 우수하며 해석 가능성과 설명 가능성을 강조한다.

Conclusion: 제안한 방법은 AI의 공정성을 확보하기 위한 해석 가능하고 설명 가능한 대안이 될 수 있다.

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [43] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 본 논문은 복잡한 문제 해결에서 학생들의 지식 통합 및 이전 능력을 증진하기 위한 평가 기준인 SID를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현대 교육에서 지식 통합 및 이전 능력을 기르는 것은 중요한 목표이며, 이는 교차 분야 STEM 교육을 통해 달성할 수 있다.

Method: SID라는 새로운 평가 벤치마크를 도입하여 LLM의 대화 능력을 체계적으로 평가하기 위한 대규모 데이터셋과 평가 지표를 제시한다.

Result: 최첨단 LLM조차 효과적인 가이드 대화를 수행하는 데 어려움을 겪고 있음을 실험을 통해 확인하였다.

Conclusion: 이 벤치마크는 더 나은 교육적 인식을 가진 LLM 개발의 중요성을 강조한다.

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [44] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 이 논문은 다중 모달 대규모 언어 모델(MMLMs)의 추론 단계 신뢰성을 평가하기 위한 첫 번째 종합 벤치마크인 ConfProBench를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 모달 작업에서 MLLM 기반 프로세스 판단기(MPJs)의 추론 단계의 정확성을 평가하는 것이 필요합니다.

Method: ConfProBench라는 벤치마크를 통해 세 가지 유형의 적대적 변형된 추론 단계를 구성하고, 새로운 평가 지표인 CRS, CSS, CCS를 도입했습니다.

Result: 14개의 최신 MMLM을 평가한 결과, 현재 MPJs의 신뢰도 성능에 한계가 있음을 밝혔습니다.

Conclusion: 이 연구는 다중 모달 작업에서 MPJs의 신뢰성을 향상시키기 위한 기초 자료를 제공합니다.

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [45] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 본 논문은 LLM의 협업 모델을 다루며, 다중 에이전트 강화 학습 문제로 간주하여 MAGRPO 알고리즘을 개발하고 이를 통해 에이전트 간 협업을 통해 높은 품질의 결과를 생성하는 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서의 LLM 협업 최적화 필요성.

Method: Multi-Agent Group Relative Policy Optimization (MAGRPO) 알고리즘을 개발하여 LLM의 협업 문제를 해결함.

Result: MAGRPO를 사용한 실험에서 에이전트들이 효과적으로 협력하여 높은 품질의 결과를 생성함을 입증.

Conclusion: MAGRPO 접근법이 LLM의 협업을 위한 새로운 가능성을 열어주며, 추가적인 MARL 기법의 활용에 대한 필요성을 강조함.

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon 프레임워크는 감정 모델의 실험실 환경에서 실제 환경으로의 전이 문제를 해결하기 위해 제안되었으며, 기존 모델보다 뛰어난 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 실험실 환경에서 실제 환경으로 감정 모델을 신뢰성 있게 전이하는 것이 AC의 주요 도전 과제이다.

Method: PriCon 프레임워크는 먼저 감독 대조 학습(SCL)을 통해 모델을 사전 훈련한 후, 특권 정보 학습(LUPI) 프레임워크 내에서 교사 모델로 작동한다.

Result: PriCon으로 훈련된 모델은 LUPI 및 엔드 투 엔드 모델을 일관되게 초월하며, 훈련 및 테스트 시 모든 모달리티에 접근 가능했던 모델과 유사한 성능에 도달한다.

Conclusion: PriCon은 실험실 환경과 실제 환경 간의 간극을 줄이는 데 있어 가능성을 보여주며, 실제 응용을 위한 확장 가능하고 실용적인 해결책을 제공한다.

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [47] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C는 다양한 차원에서 궤적 데이터를 효율적으로 압축하는 새로운 프레임워크로, 기존의 2D 중심 방식에서 벗어나 독립적인 축 압축과 체계적인 최적화 방법을 사용한다.


<details>
  <summary>Details</summary>
Motivation: 위치 인식 장치의 발전으로 인해 생성되는 대량의 궤적 데이터 압축 필요성이 대두되고 있다.

Method: PILOT-C는 주파수 영역 물리 모델링과 오류 제한 최적화를 결합한 압축 프레임워크로, 각 공간 축을 독립적으로 압축하여 임의 차원의 궤적을 지원한다.

Result: 실제 데이터셋 4개에서 평가한 결과, PILOT-C는 CISED-W에 비해 평균 19.2% 더 높은 압축 비율과 32.6% 낮은 궤적 오류를 보였다.

Conclusion: PILOT-C는 3D 궤적 압축에서도 동일한 계산 복잡도를 유지하면서 SQUISH-E보다 49% 개선된 압축 비율을 달성하였다.

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [48] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind은 향상된 진단 효율성과 해석 가능성을 제공하는 첫 번째 생성 모델로, 교차 임상 CXR 진단을 위해 커리큘럼 기반 강화 학습 및 검증 가능한 프로세스 보상을 활용하여 사고-답변 방식의 추론을 구현한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 모달 모델은 '일회성' 진단 접근 방식에 의존하고 있어, CXR 진단에서 긴 추론 시간, 희소 보상 및 빈번한 환각 현상과 같은 문제를 야기한다.

Method: CX-Mind는 708,473개의 이미지와 2,619,148개의 샘플로 구성된 CX-Set 데이터셋을 사용하여 상호 연관된 추론 데이터 포인트를 생성하고, 두 단계로 최적화를 진행한다.

Result: CX-Mind는 기존의 의학 및 일반 도메인 MLLM과 비교하여 25.1% 평균 성능 개선을 달성하며, 실제 임상 데이터셋에서는 14개 질병에 대한 mean recall@1에서 두 번째 성과를 크게 초월한다.

Conclusion: CX-Mind는 다차원에서 임상 유용성을 입증하며, 여러 센터의 전문가 평가를 통해 성능을 확실히 확인하였다.

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [49] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 본 논문은 대규모 언어 모델(LLM)의 내부 표현을 효과적으로 편집할 수 있는 Latent Knowledge Scalpel(LKS)이라는 편집 도구를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 부정확하거나 오래된 정보를 유지하여 잘못된 예측 및 편향된 출력을 초래하는 문제 해결의 필요성.

Method: LKS는 경량 하이퍼 네트워크를 통해 특정 엔티티의 잠재 지식을 조작하여 자연어 입력을 편집하는 것과 유사하게 정확하고 대규모 편집을 가능하게 한다.

Result: 실험 결과, LKS는 동시에 10,000개의 편집이 가능하며, 편집된 LLM의 일반 능력을 보존하면서 효과적으로 지식 편집을 수행하였다.

Conclusion: LKS는 기존 모델 편집 방법의 한계를 극복하며, LLM의 편집 가능성을 새롭게 제시한다.

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [50] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost는 구조적 임상 특성, 망막 사진 임베딩, 전문가가 curated한 텍스트 설명을 통합하여 녹내장 위험을 예측하는 다중모달 그래디언트 부스팅 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 녹내장의 조기 및 정확한 검출은 시력을 잃지 않도록 하는데 중요하지만, 기존 방법들은 단일 데이터에 의존하고 해석 가능성이 부족하다.

Method: GlaBoost는 고급 시각 표현을 추출하기 위해 사전 훈련된 합성곱 인코더를 사용하고, 자유 텍스트 신경망 가장자리 평가를 변환기 기반 언어 모델로 인코딩함으로써 다양한 신호를 결합한다.

Result: 실제 데이터셋에 대한 실험 결과 GlaBoost는 98.71%의 검증 정확도를 달성하며 기초 모델들을 크게 능가한다.

Conclusion: GlaBoost는 해석 가능한 녹내장 진단을 위한 투명하고 확장 가능한 솔루션을 제공하며, 다른 안과 질환에도 적용될 수 있다.

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [51] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 제안된 LRTuckerRep 모델은 다차원 데이터 완성을 위한 새로운 접근법으로, 글로벌 및 로컬 모델링을 통합하여 성능을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 다차원 데이터 완성을 위한 기존 방법의 한계점을 해결하고자 함.

Method: LRTuckerRep 모델은 Tucker 분해를 통해 저차수 특성과 매끄러움을 동시에 모델링하며, 자기 적응형 가중 핵 노름과 파라미터 없는 라플라스 기반 정규화를 사용한다.

Result: LRTuckerRep는 다차원 이미지 복원 및 교통 데이터 대체 실험에서 기존 방법보다 뛰어난 완성 정확도와 강인성을 보였다.

Conclusion: 제안된 방법은 높은 결측률에서도 안정적인 성능을 발휘하며, 기존 기법들과 비교하여 유의미한 개선을 이루었다.

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [52] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 이 논문은 대규모 언어 모델을 활용하여 베이지안 추론에서의 선험 분포 지정 과정을 자동화하고 확장하는 새로운 프레임워크인 LLMPrior를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 베이지안 추론에서 선험 분포의 지정은 중요한 과정이지만, 수작업 및 주관적이며 확장성이 떨어지는 문제점이 있다.

Method: LLMPrior라는 원리를 바탕으로 한 연산자를 도입하여 자연어 설명, 데이터 또는 그림과 같은 비구조적 컨텍스트를 유효하고 처리 가능한 확률 분포로 변환한다.

Result: 이 프레임워크는 가우시안 혼합 모델 등과 같은 명시적 생성 모델과 LLM을 결합하여 필수적인 수학적 속성을 만족하는 선험 분포를 생성할 수 있다.

Conclusion: 이 연구는 복잡한 베이지안 모델링에 대한 진입 장벽을 낮출 수 있는 새로운 도구군의 기초를 제공한다.

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [53] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 이 논문은 시뮬레이터에서 훈련된 정책이 실제 환경에서 성능이 저하되는 문제를 해결하기 위해, 단일 미지의 환경에서 온라인으로 배포 성능을 최적화하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 실제 환경에서 강화 학습이 미치는 시뮬레이터와의 차이를 극복하고, 사전 지식이 없는 환경에서도 효과적으로 작동할 수 있는 방법을 모색한다.

Method: $f$-발산 기반 불확실성 집합을 고려하며, 계산적으로 효율적이고 하한 회귀 보장을 제공하는 알고리즘을 제안한다.

Result: 제안된 알고리즘은 광범위한 환경에서 실험을 통해 강건성과 효율성을 입증하였다.

Conclusion: 이 논문은 온라인 배포적으로 강건한 강화 학습의 최적 성능을 달성할 수 있는 방법을 제시하며, 주어진 제약 조건 하에서 거의 최적성을 보여준다.

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [54] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: GRPO의 제한점을 해결하기 위해 GTPO라는 새로운 정책 최적화 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: GRPO의 두 가지 주요 한계를 분석하고 이를 해결하기 위한 방법론을 제시하고자 한다.

Method: GTPO는 상반된 보상을 가진 토큰을 식별하고, 부정적 업데이트를 건너뛰며 긍정적 업데이트를 증폭시켜 안정적인 학습을 도모한다.

Result: GTPO는 여러 실험에서 GRPO에 비해 향상된 성능과 더 나은 훈련 안정성을 보여준다.

Conclusion: GTPO는 KL 발산 정규화를 필요로 하지 않으며, 참조 모델 없이도 효과적인 정책 최적화를 가능하게 한다.

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [55] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: U-PINet는 전자기 산란 모델링의 효율성과 물리적 일관성을 개선하기 위해 제안된 물리 정보 기반의 딥러닝 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 수치 솔버의 한계와 데이터 기반의 딥러닝 접근법의 적용 가능성을 극복하기 위해 U-PINet을 개발하였다.

Method: U-PINet은 다중 스케일 처리 신경망 아키텍처와 물리 영감을 받은 희소 그래프 표현을 사용하여 전자기 소산의 분해 및 결합을 모델링한다.

Result: U-PINet은 표면 전류 분포 예측에서 전통적 솔버와 유사한 정확도를 보이며 계산 시간을 상당히 단축하였다.

Conclusion: U-PINet은 전자기 산란 응용을 위한 유망한 구조로, 정확도와 강건성이 개선된 결과를 보여준다.

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [56] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 핵융합 장치 EAST에서의 열 플럭스 추정은 중요하며, 이 논문은 전통적인 유한요소법(FEM) 대신 물리정보 신경망(PINN)을 이용해 이를 해결하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 핵융합 장치에서 열 플럭스를 정확하고 실시간으로 추정하는 것은 중요하지만, 전통적인 FEM 방법은 비효율적이다.

Method: 물리정보 신경망(PINN)을 사용하여 입력된 재료의 공간 좌표와 시간 스탬프에 따라 열 전도 방정식에 기반한 경계 손실, 초기 조건 손실 및 물리적 손실을 계산하며, 데이터 기반 방식으로 소수의 데이터 포인트를 샘플링하여 모델을 최적화한다.

Result: 제안된 모델은 uniform 및 non-uniform 가열 조건에서 실험을 통해 FEM과 비슷한 정확도를 보이면서도 계산 효율성을 40배 향상시켰다.

Conclusion: Dataset과 소스 코드는 GitHub에 공개될 예정이다.

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [57] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 본 연구에서는 복잡한 토양 수평 분류 문제를 해결하기 위해 이미지 및 지리적 메타데이터를 통합한 멀티모달 멀티태스크 모델인 SoilNet을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 토양 건강을 모니터링하는 데 있어 정확한 토양 수평 분류는 농업 생산성, 식량 안보, 생태계 안정성 및 기후 복원력에 직접적인 영향을 미친다.

Method: SoilNet은 이미지 데이터를 사용하여 깊이 마커를 예측하고, 이를 통해 토양 프로필을 다수의 수평 후보로 세분화하며, 최종적으로 복잡한 계층적 관계를 고려한 토양 수평 레이블을 예측하는 구조화된 모듈화 파이프라인을 이용한다.

Result: 실제 토양 프로필 데이터셋을 통해 제안한 방법의 효과를 입증하였다.

Conclusion: 제안하는 SoilNet 모델은 복잡한 계층적 분류 문제를 해결하는데 효과적이며, 모든 코드 및 실험 결과는 GitHub 저장소에서 확인할 수 있다.

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [58] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: Bernoulli-LoRA는 Low-Rank Adaptation(LoRA)을 통합하고 확장하는 새로운 이론적 프레임워크로, 여러 변화의 수렴 보장을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 기초 모델을 특정 작업에 적응시키기 위해 파라미터 효율적인 미세 조정(PEFT)의 필요성이 커지고 있음.

Method: 확률적 베르누이 메커니즘을 도입하여 업데이트할 행렬을 선택, Bernoulli-LoRA 프레임워크를 통해 여러 변형을 분석.

Result: 다양한 작업에 대한 실험을 통해 이론적 발견을 검증하고 실제 효능을 입증함.

Conclusion: 이 연구는 이론적으로 근거가 있으면서도 실제적으로 효과적인 PEFT 방법 개발을 위한 첫 걸음이 됨.

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [59] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO는 고차원 최적화 문제에 대해 모델 불확실성 추정을 하지 않고 새로운 샘플을 탐색 및 활용 기준으로 추가하여 효율적인 최적화를 제공하는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 고차원 및 많은 함수 평가에서 GP 모델의 계산 복잡성으로 인해 BO가 확장성에 어려움을 겪고 있기 때문이다.

Method: SNBO는 탐색 및 활용을 위한 별도의 기준을 사용하여 새로운 샘플을 추가하고 샘플링 영역을 적응적으로 조절하는 방법이다.

Result: SNBO는 10차원에서 102차원까지의 다양한 최적화 문제에서 테스트되었으며, 대부분의 문제에서 기존 최첨단 알고리즘보다 더 좋은 함수 값과 40-60% 적은 함수 평가를 요구하였다.

Conclusion: SNBO는 고차원 최적화 문제에서 뛰어난 성능을 발휘하며 런타임을 최소 10배 줄인다.

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [60] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 본 연구는 개인 정보 노출의 위험성을 이해하기 위해 5000개 이상의 신원 도용 사례를 분석하여, 개인 데이터의 노출과 그 결과를 파악하고, 이를 바탕으로 개인정보 보호를 위한 예측 프레임워크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 개인 및 조직이 개인 정보를 보호하기 위해서는 개인정보 노출에 대한 기본적인 이해가 필요하다.

Method: 5000건 이상의 신원 도용 및 사기 사례를 분석하고, 개인 식별 정보(PII) 속성을 노드로 하고 이들 간의 관계를 엣지로 하는 Identity Ecosystem 그래프를 구축한 후, 그래프 이론과 그래프 신경망을 이용하여 개인정보 노출의 가능성을 예측하는 프레임워크를 개발하였다.

Result: 이 접근 방식은 주어진 신원 속성의 노출이 다른 속성의 노출로 이어질 가능성을 효과적으로 평가함을 보여준다.

Conclusion: 본 연구는 개인정보 보호를 위한 효과적인 예측 도구를 제공하며, 개인 정보의 노출 경로 이해에 기여한다.

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [61] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 이 논문은 개인정보 보호와 공정함을 동시에 달성할 수 있는 새로운 다중봉 기법인 DP-NCB를 제안하며, 이론적 보장을 바탕으로 우수한 성능을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 사회적으로 민감한 설정에서 다중봉 알고리즘의 사용이 증가함에 따라 사용자 데이터의 개인정보 보호와 공정한 대우의 중요성이 커지고 있다.

Method: DP-NCB라는 새로운 알고리즘 프레임워크를 도입하여 epsilon-차분 개인정보 보호와 최적의 Nash 후회를 동시에 보장한다.

Result: DP-NCB는 기존의 최첨단 기법들에 비해 Nash 후회가 현저히 낮음을 보여준다.

Conclusion: 본 연구는 개인정보 보호와 공정성을 모두 고려한 다중봉 알고리즘 설계에 대한 근본적인 토대를 제공하여 사회적 영향력이 큰 응용 분야에 적합하다.

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [62] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 본 논문에서는 비선형 편미분 방정식을 해결하기 위한 부분적으로 훈련 가능한 대리 모델인 VAE-DNN을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비선형 편미분 방정식의 해를 효율적이고 정확하게 구하기 위한 새로운 접근 방식이 필요하다.

Method: 입력 데이터를 저차원 잠재 공간으로 변환하는 인코더와 PDE 솔루션을 매핑하는 완전 연결 신경망, 그리고 해를 재구성하는 디코더로 구성된 VAE-DNN 모델을 제안하고, 이를 통해 세 가지 구성 요소를 독립적으로 훈련한다.

Result: VAE-DNN 모델은 비선형 확산 방정식에 대한 전방 및 역방향 해를 구하는 데 있어 FNO 및 DeepONet 모델보다 더 나은 효율성과 정확성을 보인다.

Conclusion: VAE-DNN은 기존의 최첨단 모델에 비해 훈련 시간과 에너지를 상당히 줄이면서도 더 높은 정확도를 달성한다.

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [63] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 본 논문은 스펙트럼 수요 예측을 위한 효과적인 공간-시간 예측 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 정확한 스펙트럼 수요 예측은 스펙트럼 할당, 규제 계획 및 현대 무선 통신 네트워크의 지속 가능한 성장을 위해 필수적이다.

Method: 사용자 측 성능 지표(KPIs)와 규제 데이터를 활용한 공간-시간 예측 프레임워크를 통해 스펙트럼 수요를 모델링하고 예측한다.

Result: 고급 기능 공학, 종합적인 상관 분석 및 전이 학습 기술을 사용하여 우수한 예측 정확도와 지역 간 일반화를 달성한다.

Conclusion: 제안된 방법론은 정책 입안자와 규제 기관이 스펙트럼 관리 및 계획을 개선하는 데 강력한 접근 방식을 제공할 수 있다.

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [64] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 데이터 스트림에서의 학습 모델의 효율성을 높이기 위한 정보 이론적 데이터 하위 샘플링 방법을 제안하고, 실제 성능 향상을 위해 모델 설계가 중요하다는 점을 강조함.


<details>
  <summary>Details</summary>
Motivation: 데이터 스트림에서 새로운 관측이 시간에 따라 도착하는 상황에서 정보의 관련성을 유지하면서 계산 비용을 관리하는 것이 중요한 과제로 부각되고 있음.

Method: 정보 이론적 방법론을 활용하여 다운스트림 예측의 불확실성을 줄이는 것을 중심으로 하는 데이터 하위 샘플링 전략을 탐구함.

Result: 이 예측 중심의 접근 방식이 두 가지 잘 연구된 문제에서 이전에 제안된 정보 이론적 기술보다 성능이 우수함을 입증함.

Conclusion: 강력한 성능을 신뢰할 수 있게 달성하기 위해서는 신중한 모델 설계가 필요하다는 점을 강조함.

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [65] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: SICKLE 프레임워크를 통해 데이터 양을 줄이면서 모델 정확성과 에너지 소비를 개선할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: Moore의 법칙과 Dennard 스케일링의 종료로 인해 효율적인 훈련을 위해 데이터 양에 대한 재구성이 필요하다.

Method: SICKLE을 개발하고, 최대 엔트로피(MaxEnt) 샘플링 접근 방식을 사용하여 대규모 난류 데이터셋에서 샘플링 성능을 비교한다.

Result: MaxEnt는 무작위 샘플링 및 위상 공간 샘플링에 비해 모델 정확성을 향상시키고 에너지 소비를 최대 38배까지 줄일 수 있다.

Conclusion: 서브샘플링이 전처리 단계로서 모델의 성능과 에너지 효율성을 높일 수 있음을 입증했다.

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [66] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 이 논문은 제1형 당뇨병 치료를 위한 새로운 강화 학습 프레임워크를 제안하며, 치료의 복잡한 시간적 동학을 포착하여 안전한 정책 학습을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 제1형 당뇨병(T1DM)과 같은 만성 질환에서 치료를 개인화하고 안전한 생리적 변수를 관리하는 것이 중요하다.

Method: 두 가지 제어 모달리티(즉, 임펄스 제어와 스위칭 제어)를 통합하여 치료의 복잡한 시간적 동학을 포착하는 제약된 마르코프 결정 프로세스를 활용한다.

Result: 이 프레임워크는 이론적 수렴 보장을 제공하며, T1DM 제어 작업에서 혈당 수준 위반율을 22.4%에서 10.8%로 감소시켰다.

Conclusion: 이 연구는 향후 안전하고 시간이 인식된 강화 학습의 기초를 마련하였고 임상 배치에는 사용되지 않도록 설계되었다.

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [67] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 포도 생리정보 예측을 위한 하이브리드 모델링 접근 방식이 제안되며, 이는 다중 작업 학습과 순환 신경망을 결합하여 생리학적 모델의 매개변수를 개선한다.


<details>
  <summary>Details</summary>
Motivation: 세밀한 포도밭 관리 결정을 위한 정확한 포도 생리정보 예측의 필요성.

Method: 다중 작업 학습을 이용한 순환 신경망을 결합하여 생리학적 모델의 매개변수를 parameterize하는 하이브리드 모델링 접근 방식.

Result: 우리의 방법은 기존 생리학적 모델과 기본 딥러닝 접근 방식보다 생리학적 단계 및 기타 작물 상태 변수 예측에서 의미있게 우수한 성능을 보였다.

Conclusion: 하이브리드 모델링 접근 방식이 포도 생리정보 예측의 정확성과 강건성을 향상시킨다.

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [68] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 본 논문에서는 거리 기반 분류기에 대한 숨겨진 신경망 구조를 밝혀내고, 이를 통해 설명 가능한 인공지능 기술을 적용하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 거리 기반 분류기의 예측을 설명 가능하게 만들기 위해 신경망의 숨겨진 구조를 이해하는 것이 중요하다.

Method: 신경망 구조와 연결된 거리 기반 분류기의 방식(선형 탐지 유닛과 비선형 풀링 레이어 결합)을 밝혀내고, 레이어별 관련 전파(LRP) 기법을 도입하였다.

Result: 우리의 설명 접근 방식이 여러 기준선에 비해 장점을 가지고 있음을 정량적 평가를 통해 입증하였다.

Conclusion: 거리 기반 모델 설명의 유용성을 두 가지 실제 사례를 통해 보여주었다.

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [69] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 이 논문은 교차 도메인 시계열 데이터에서 이상 감지를 위한 능동 학습과 전이 학습의 결합 효과를 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 이상 감지의 효율성을 높이기 위해 능동 학습과 전이 학습의 결합 가능성을 탐색함.

Method: 능동 학습과 전이 학습을 사용하여 여러 데이터 세트에서 성능을 평가하고, 클러스터링의 적용 여부를 실험적으로 분석 함.

Result: 능동 학습이 훈련 세트에 새로운 샘플을 추가하면 모델 성능이 향상되지만, 문헌에서 보고된 것보다 개선 속도가 느림을 발견.

Conclusion: 능동 학습은 효과적이나, 모델 성능의 개선은 선택된 레이블 수가 증가함에 따라 선형적으로 평평한 함수를 따른다.

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [70] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 이 논문은 군중 역학의 미시적 및 거시적 모델링을 연결하기 위한 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 군중 역학을 이해하고 최적화하기 위해 미시적 및 거시적 모델링 스케일 간의 간극을 메우는 것이 중요한 도전 과제이다.

Method: 맨폴드 학습과 기계 학습을 결합하여 고차원의 에이전트 기반 시뮬레이션 데이터를 기반으로 군중 역학의 이산 진화 연산자를 학습한다.

Result: 제안된 접근 방식은 높고 낮은 차원의 시스템 간에 원활한 변환을 가능하게 하여 정확하고 강건한 모델링 및 시뮬레이션을 달성한다.

Conclusion: 이 논문은 에이전트 기반 시뮬레이션으로부터 빠르고 정확한 군중 역학 모델링을 가능하게 하는 효과적인 해결 연산자를 제시한다.

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [71] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 트랜스포머의 알고리즘 학습 능력을 조사하여, 다음 토큰 예측만으로도 전이 확률을 추정할 수 있는 한계를 분석했다.


<details>
  <summary>Details</summary>
Motivation: 트랜스포머 모델이 문맥을 활용하여 알고리즘을 학습할 수 있는 능력을 이해하고자 한다.

Method: 랜덤 전이 행렬을 사용한 마르코프 체인을 설정하고, 트랜스포머가 다음 토큰을 예측하도록 훈련시켰다.

Result: 모델 크기와 훈련 세트 크기가 일정 임계값을 초과할 경우, 모델이 훈련 패턴을 암기하는 대신 문맥을 통해 전이 확률을 추정할 수 있음을 보여주었다.

Conclusion: 복잡한 상태 인코딩을 통해 훈련 중 보지 않은 구조의 마르코프 체인에 대해서도 더 견고한 예측이 가능해진다.

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [72] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: FairPOT은 위험 점수의 공정성을 개선하고 AUC 성능을 유지하는 혁신적인 방법이다.


<details>
  <summary>Details</summary>
Motivation: 의료, 금융, 범죄정의와 같은 분야에서 준수해야 하는 공정성을 고려해 AUC 메트릭 사용의 필요성이 있다.

Method: FairPOT은 최적 수송을 사용하여 각 그룹의 위험 점수 분포를 정렬하고, 불리한 그룹의 점수 중 선택된 비율만 변환하는 모델 불가지론적 후처리 프레임워크이다.

Result: FairPOT는 다양한 데이터셋에 대한 실험에서 기존 기술보다 공정성과 AUC 모두에서 더 나은 성능을 확인했다.

Conclusion: FairPOT는 실용적인 적용을 위해 매우 효율적이며, AUC 성능을 유지하면서 공정성을 개선하는 가능성을 제공한다.

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [73] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet는 압력 프로파일을 버블 반경 반응으로 매핑하기 위해 설계된 오퍼레이터 학습 모델이다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝의 고유한 스펙트럼 편향을 완화하고자 함.

Method: PI-DeepONet 프레임워크를 기반으로 하여 Rowdy 적응 활성화 함수를 통합함.

Result: 다양한 시나리오에서 모델의 성능을 평가하고 단일 및 이중 단계 훈련 기법을 조사함.

Conclusion: BubbleONet은 전통적인 수치 해석기보다 계산적으로 효율적인 대안으로 작용할 수 있는 유망한 대체 모델이다.

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [74] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP은 사용자 조절이 가능한 프라이버시 보호 감지 프레임워크로, 사용자가 민감한 활동을 식별하고 해당 데이터를 비밀리에 랩핑할 수 있도록 지원한다.


<details>
  <summary>Details</summary>
Motivation: 현대 감지 시스템에서 사용자의 프라이버시 선호가 개인마다 다를 수 있으며 시간이 지남에 따라 진화하기 때문에, 사용자 조절 가능한 프라이버시가 중요하다.

Method: PrivCLIP는 활동을 민감한, 비민감한, 중립으로 분류하고, 다중 모드 대비 학습 접근법을 활용하여 IMU 센서 데이터와 자연어 활동 설명을 정렬한다.

Result: PrivCLIP은 여러 인간 활동 인식 데이터셋에서 평가하였고, 프라이버시 보호와 데이터 유용성 면에서 기본 방법들보다 유의미하게 우수한 성능을 보였다.

Conclusion: PrivCLIP은 민감한 활동을 식별하고 이를 비밀스러운 활동처럼 보이도록 변환하여 효과적으로 사용자 프라이버시를 보호한다.

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [75] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: TC-LoRA는 다중 작업 설정에서 LoRA 어댑터 간의 작업 간섭을 줄이기 위해 텍스트 및 매개변수 수준에서 클러스터링 및 CP 분해를 사용하여 LLM의 적응성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 다중 작업 설정에서 LoRA 어댑터의 작업 간섭이 성능 저하를 초래하므로 이를 해결할 필요가 있다.

Method: 텍스트 수준에서는 임베딩 공간에서 훈련 샘플을 클러스터링하여 각 클러스터에 대한 특화된 LoRA 어댑터를 학습하고, 매개변수 수준에서는 작업별 및 공유 요소를 분리하는 공동 CP 분해를 도입하였다.

Result: TC-LoRA는 이전 SVD 기반 방법 대비 Phi-3에서 +1.4% 및 Mistral-7B에서 +2.3%의 정확도를 달성하여 효과성을 입증하였다.

Conclusion: TC-LoRA는 LLM 적응에서 작업 간섭을 줄이고 성능을 향상시키는 효과적인 방법이다.

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [76] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: DCFL은 데이터 이질성 문제를 해결하기 위해 대조 손실을 분리하여 연합 학습에서 성능을 향상시키는 접근법이다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 데이터 이질성으로 인해 성능이 저하되는 문제를 해결하고자 함.

Method: 기존 대조 손실을 정렬과 균일성 두 가지 목표로 분리하여 이를 독립적으로 조정하는 DCFL 프레임워크를 제안함.

Result: DCFL은 기존 대조 학습 방법보다 양성 샘플 간의 정렬을 강화하고 음성 샘플 간의 균일성을 높임.

Conclusion: DCFL은 CIFAR-10, CIFAR-100, Tiny-ImageNet과 같은 표준 벤치마크에서 최신 연합 학습 방법을 지속적으로 능가한다.

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [77] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: TensorFlow와 PyTorch의 비교를 통해 개발 편의성, 성능, 배포 유연성 등을 분석하고 각 프레임워크의 장단점을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 딥러닝 프레임워크 선택에 있어 실무자들이 고려해야 할 요소들을 명확히 하고, 두 프레임워크의 차별성을 이해하도록 돕기 위해 작성되었다.

Method: 프레임워크의 프로그래밍 패러다임, 모델 훈련 속도, 추론 성능 및 배포 유연성 등을 비교하고, 생태계 및 커뮤니티 지원을 조사하였다.

Result: PyTorch는 연구에서의 단순성과 유연성을 제공하는 반면, TensorFlow는 더 완전한 배포 준비 생태계를 제공함을 확인하였다.

Conclusion: 두 프레임워크 모두 최첨단 딥러닝에 적합하지만, 각각의 고유한 상쇄 요소를 이해하는 것이 적절한 도구 선택에 중요하다.

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [78] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 이 논문은 연속형 시계열 모델에서 데이터셋별 이질성이 일반화 성능을 저하시킨다는 문제를 논의하고, 연합 학습을 활용한 새로운 접근법인 Federated Dataset Learning (FeDaL)을 제안하여 이 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 시계열 기초 모델(TSFMs)에서 데이터셋의 이질성이 일반화에 미치는 영향을 탐구하고, 이를 해결할 효과적인 방법을 찾기 위해 연합 학습의 패러다임을 재구성하고자 하였다.

Method: FeDaL 접근법은 분산된 연합 학습 아키텍처를 활용하여 이질적인 시계열 데이터셋을 일반화된 지식과 개인화된 지식으로 분해하고, 도메인 편향 제거(DBE) 및 글로벌 편향 제거(GBE) 메커니즘을 추가로 적용하였다.

Result: FeDaL은 54개의 기초 모델에 대해 8개의 실제 데이터셋과 과제에서 폭넓게 평가되었으며, 효율적인 크로스-데이터셋 일반화를 보여주었다.

Conclusion: 데이터 양, 클라이언트 수 및 참여율이 분산 환경에서 모델 성능에 미치는 영향을 분석하여 연합 학습의 확장성을 입증하였다.

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [79] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: QTFT는 양자-고전적 혼합 아키텍처로, TFT를 확장하여 멀티 호라이즌 시계열 예측 성능을 개선한다.


<details>
  <summary>Details</summary>
Motivation: QTFT는 기존의 TFT 모델을 기반으로 하여 양자 컴퓨팅의 장점을 활용하여 시간 시계열 예측의 가능성을 넓히기 위해 제안되었다.

Method: QTFT는 변별적 양자 알고리즘을 기반으로 하며, 현재의 잡음이 있는 중간 규모 양자(NISQ) 장치에서 실행 가능하다.

Result: QTFT는 예측 데이터셋에서 성공적으로 훈련되었고, 일정한 테스트 케이스에서 기존 모델보다 우수한 성능을 보였다.

Conclusion: QTFT는 고전적 모델에 비해 뛰어난 예측 능력을 보여 주며, 양자 알고리즘을 통한 구조적 우위를 제공한다.

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [80] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 이 논문은 자동 단답형 채점(Auto Short Answer Grading, ASAG)에서의 LLM을 이용한 파인튜닝 방법들을 평가하며, 소량의 데이터로 파인튜닝이 제한적이라는 점을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 자동 단답형 채점의 정확도를 향상시키기 위해 LLM을 사용하고, 기존의 어려운 파인튜닝 방법을 대체하기 위한 새로운 접근 방식을 찾고자 한다.

Method: OpenAI의 파인튜닝 서비스와 QLORA와 같은 오픈 가중치 모델을 사용하여 소량의 데이터를 이용한 파인튜닝 방법을 평가한다.

Result: Llama 오픈 웨이트 모델은 데이터가 적을 경우 파인튜닝의 효과가 제한적이나, OpenAI의 클로즈드 모델에 비해 성능이 개선되었다.

Conclusion: 대량의 합성 훈련 데이터를 사용하여 Llama 3.1 8B-Instruct 모델의 성능이 극적으로 향상됨을 확인하였다.

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [81] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT는 고급 백도어 공격으로 다양한 타겟-specific 트리거를 생성 가능한 새로운 방법이다.


<details>
  <summary>Details</summary>
Motivation: 기존 백도어 공격 방법은 정해진 패턴과 단일 타겟에 의존하여 유연성이 부족하고 탐지가 용이하다.

Method: FLAT는 잠재 반응 조건부 오토인코더를 사용하여 시각적으로 적응 가능한 다양한 트리거를 생성한다.

Result: FLAT는 높은 공격 성공률을 보이며, 기존 FL 방어 메커니즘에 대해 강력하다.

Conclusion: 신속한 방어 전략 개발이 필요하다.

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [82] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 본 논문은 여러 시점에서의 클러스터링에서 공정성을 통합한 새로운 프레임워크인 AFMVC를 제안하며, 이는 공정성을 유지하면서도 클러스터링 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 다중 관점 클러스터링에서 기존 방법들이 클러스터링 성능에만 집중하는 반면, 공정성의 중요성을 간과하였기 때문에 이를 해결하고자 합니다.

Method: AFMVC 프레임워크는 적대적 훈련을 통해 학습된 특성에서 민감한 속성 정보를 제거하여 클러스터 할당이 공정성에 영향을 받지 않도록 합니다.

Result: AFMVC는 기존의 다중 관점 클러스터링 및 공정성 인식 클러스터링 방법들과 비교하여 뛰어난 공정성과 경쟁력 있는 클러스터링 성능을 달성했습니다.

Conclusion: 이 논문은 클러스터링 일관성을 유지하면서 공정성을 확보할 수 있는 이론적 보장을 제공하며, AFMVC가 실험에서 우수한 결과를 보여주었다고 결론짓습니다.

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [83] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 본 논문은 비전-언어 모델(VLM)이 훈련 데이터 유출에 대한 취약성을 갖고 있음을 최초로 연구하며, 새로운 모델 역전 공격 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델(VLM)의 비공식적 연구로, 이러한 모델이 개인 데이터 유출에 어떤 영향을 미치는지 이해하고자 한다.

Method: Token 기반 및 시퀀스 기반의 모델 역전 공격 기법인 TMI, TMI-C, SMI, SMI-AW를 제안하고 실험을 통해 검증한다.

Result: VLM이 훈련 데이터 유출에 취약하다는 것을 최초로 입증하며, SMI-AW 방식이 기존 방법보다 더 높은 공격 정확도를 기록했다.

Conclusion: VLM의 개인 정보 보호 취약점을 드러내며, 이러한 모델들이 헬스케어 및 금융과 같은 다양한 애플리케이션에서 점점 더 보편화됨에 따라 문제의 심각성을 강조한다.

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [84] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 본 논문에서는 LLM의 문제 해결 능력을 향상시키기 위해 일관성을 고려한 정책 최적화 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 샘플 응답이 동일한 결과로 수렴할 때, 그룹 기반 이점이 제로로 떨어지는 문제가 발생하여 학습 효율성과 성능이 제한된다.

Method: 일관성 기반의 구조적 글로벌 보상을 도입하고, 엔트로피 기반의 소프트 블렌딩 메커니즘을 적용하여 지역 이점 추정과 글로벌 최적화를 조화롭게 한다.

Result: 여러 수학적 추론 벤치마크에서 성능 향상을 입증하였다.

Conclusion: 제안된 프레임워크가 강력하고 일반적인 적용 가능성을 지닌다.

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [85] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 이 논문은 다양한 기상 조건에 따른 도메인 이동을 해결하기 위한 반지도 심층 도메인 적응 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 재생 가능 에너지원의 정확한 추정과 지역 불문 예측 모델 개발을 위해 정확한 태양광 발전 예측이 필수적이다.

Method: 소스 위치 데이터로 심층 컨볼루션 신경망을 훈련하고, 대상 위치에 대해 소스 없는 티처-스튜던트 모델 구성을 사용하여 적응한다.

Result: 대상 도메인에서 단 20%의 데이터 주석만으로 캘리포니아, 플로리다 및 뉴욕에서 각각 11.36%, 6.65%, 4.92%의 정확도 향상을 보여준다.

Conclusion: 소스 데이터 요구 없이도 효과적인 예측이 가능하며, 반지도 학습을 통해 도메인 이동 문제를 성공적으로 해결한다.

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [86] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 이 연구는 질량 스펙트럼에서 분자를 생성하는 새로운 접근법을 제안하며, encoder와 decoder를 사용하여 성능을 크게 향상시켰다.


<details>
  <summary>Details</summary>
Motivation: 질량 스펙트럼에서 원자 구조를 생성하는 문제를 효과적으로 해결하기 위함이다.

Method: 	extsc{MIST}를 인코더로, 	extsc{MolForge}를 디코더로 사용하여 사전 훈련을 통해 성능을 개선했다.

Result: 이 조합으로 기존 방법에 비해 10배 향상된 정확도로 분자 구조를 생성할 수 있었다.

Conclusion: 앞으로 질량 스펙트럼에서의 분자 해명 연구를 위한 강력한 기준선이 된다.

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [87] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 신경망 최적화를 위한 새로운 방법 SAMT를 제안하여 블록 단위로 매개변수를 업데이트하고, 메타 학습 기반의 적응형 스텝 사이즈 전략을 도입.


<details>
  <summary>Details</summary>
Motivation: 기존의 SGD 방식은 비선형 최적화 문제에서 불안정한 수렴과 높은 계산 비용을 초래한다.

Method: SAMT는 블록 단위로 매개변수를 업데이트하고, 메타 학습에서 영감을 받은 적응형 스텝 사이즈 전략을 포함하여 얻어진 서브 문제 해결 절차를 활용한다.

Result: SAMT는 최신 방법들에 비해 더 나은 일반화 성능을 달성하고 더 적은 매개변수 업데이트로 효과성을 입증한 실험 결과를 보인다.

Conclusion: 제안한 알고리즘은 최적화의 견고함을 확립하고 신경망 최적화에 있어서 그 가능성을 강조한다.

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [88] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 이 논문은 외부 추론 시스템에서 발생하는 보상 해킹 문제를 해결하기 위한 CAusal Reward Adjustment (CRA) 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 보상 해킹 문제로 인해 논리적으로 잘못된 경로가 높은 점수를 얻어 잘못된 답변을 제공하는 현상을 해결하고자 한다.

Method: CRA는 PRM의 내부 활성화를 기반으로 희소 오토인코더를 훈련하여 해석 가능한 특징을 복구하고, 백도어 조정을 사용하여 혼란을 교정한다.

Result: CRA는 수학 문제 풀기 데이터셋에서 보상 해킹을 완화하고 최종 정확도를 향상시킨다.

Conclusion: 정책 모델이나 PRM의 재훈련 없이 CRA는 보상 해킹 문제를 해결한다.

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [89] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 이 논문은 대칭 발산을 행동 정규화 정책 최적화(BRPO)에 도입하여 새로운 오프라인 강화 학습 프레임워크를 확립한다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들이 비대칭 발산에 초점을 맞추어 정규화된 정책과 실용적인 최소화 목표를 얻는 데 중점을 두고 있다.

Method: 대칭 발산의 문제를 해결하기 위해 $f$-발산의 테일러 급수를 사용하여 유한 급수로 정규화된 정책을 도출하고, 수치적 문제를 완화하기 위해 조건부 대칭 항을 테일러 전개하였다.

Result: S$f$-AC는 대칭 발산을 사용한 최초의 실용적인 BRPO 알고리즘으로, 분포 근사 및 MuJoCo 실험 결과에서 경쟁력 있는 성과를 보인다.

Conclusion: S$f$-AC 알고리즘이 기존보다 성능이 우수함을 입증하였다.

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [90] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: DCATS는 시간 시계열 데이터의 품질을 개선하여 AutoML 성능을 높이는 데이터 중심 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 예측에 있어 경량 모델이 종종 우수한 성능을 보이는 점을 바탕으로, 모델 아키텍처보다는 데이터 품질 개선에 중점을 두고 AutoML을 발전시킬 수 있다.

Method: DCATS는 시간 시계열 메타데이터를 활용하여 데이터를 정제하고 예측 성능을 최적화한다.

Result: DCATS는 대규모 교통량 예측 데이터 세트를 사용하여 평가되었으며, 평균 6%의 오류 감소를 기록했다.

Conclusion: 이 연구는 시간 시계열 예측을 위한 AutoML에서 데이터 중심 접근법의 가능성을 강조한다.

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [91] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 딥러닝 기반의 자동 도플러 각도 추정 방법을 제안함.


<details>
  <summary>Details</summary>
Motivation: 도플러 초음파 혈류 측정에서 각도 추정 오류는 주요 오류 원인으로 인식되고 있음.

Method: 2100개의 인체 경동맥 초음파 이미지를 사용하여 이미지 증강과 함께 딥러닝 모델을 개발하고, 사전 학습된 5개 모델로 이미지 특징을 추출한 후, 맞춤형 얕은 신경망에 전달함.

Result: 자동화된 각도 추정과 수동 관찰자 각도 간의 평균 절대 오차(MAE)가 3.9도에서 9.4도 범위임.

Conclusion: 딥러닝 기반 기술이 도플러 각도 추정에 적용될 가능성을 보여줌.

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [92] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time이라는 새로운 trimodal 프레임워크를 제안하여 다변량 시계열 예측의 한계점을 극복하고, 여러 실험을 통해 성능이 향상됨을 보여줌.


<details>
  <summary>Details</summary>
Motivation: 다변량 시계열 예측 모델의 기존 방법들이 고정된 추론 편향에 의존하며 변수 간 상호작용을 무시하고 있어, 시간에 따라 달라지는 복잡한 관계를 포착하는데 어려움이 있음.

Method: T3Time은 시간, 주파수, 프롬프트의 세 가지 가지를 포함하는 새로운 구조로, 주파수 인코딩 브랜치가 주기 구조를 포착하고, 예측 수평에 따라 특징의 우선 순위를 학습하는 게이팅 메커니즘을 적용.

Result: T3Time은 기존 최신 모델들보다 평균 3.28% 낮은 MSE와 2.29% 낮은 MAE로 성능을 향상시켰으며, 적은 양의 데이터로도 강한 일반화 성능을 보임.

Conclusion: T3Time은 다변량 시계열 예측에서 기존 모델들의 한계를 극복하고, 다양한 데이터셋에서 우수한 성능을 나타냄.

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [93] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT는 ML 모델을 시각적으로 탐색하고 이해하는 Python 기반 도구이다.


<details>
  <summary>Details</summary>
Motivation: AI 연구자와 도메인 전문가들이 프로그래밍 없이 모델을 구성, 훈련, 평가 및 설명할 수 있도록 지원하기 위해.

Method: 지역적 및 글로벌 민감도 분석을 통합하여 사용자에게 인터랙티브한 그래픽 인터페이스 제공.

Result: 타이타닉 생존 예측 분류 작업을 통해 민감도 정보가 피처 선택 및 데이터 정제에 어떻게 도움을 주는지 시연.

Conclusion: 이 시스템은 ML 모델의 동작을 이해하고 활용하는 데 유용하다.

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [94] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: LLMs의 잠재력을 탐구하여 일반 기계 학습 작업에 적합한 Mockingbird 프레임워크를 제안하고 평가한 연구.


<details>
  <summary>Details</summary>
Motivation: LLMs의 사용이 증가함에 따라 그들이 기계 학습 작업에서도 적합할 수 있는지에 대한 호기심에서 출발.

Method: Mockingbird 프레임워크를 제안하고, LLM이 역할 놀이를 하며 실수를 반영하여 개선되는 방식으로 적용.

Result: Mockingbird는 일반 기계 학습 작업에서 수용 가능한 결과를 도출하였으나, 도메인 특화 문서 및 전문가 피드백이 더 효과적임을 발견.

Conclusion: LLMs는 기계 학습 작업에서 유용할 수 있지만, 독자적인 반영만으로는 한계가 있음을 보여준다.

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [95] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 이 논문은 시각-언어 모델(VLM)을 위한 새로운 강화학습 알고리즘인 VL-DAC를 도입하여, 저렴한 환경에서 학습한 정책이 실제 이미지 작업에 일반화된다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 현재 시각-언어 모델은 시각 정보를 언어 조건의 행동으로 변환하는 능력이 부족하다. 이에 따라 강화학습을 통해 이 능력을 향상시키고자 하는 시도가 있었으나, 학습된 행동의 일반화 여부가 잘 검증되지 않았다.

Method: VL-DAC는 액션 토큰에 PPO 업데이트를 적용하고, 가치 학습은 환경 단계 수준에서만 진행하는 하이퍼파라미터 없는 경량 강화학습 알고리즘이다.

Result: VL-DAC를 사용한 단일 VLM 교육은 BALROG에서 +50%, VSI-Bench의 가장 어려운 부분에서 +5%, VisualWebBench에서 +2%의 상대적 성과 향상을 보여준다.

Conclusion: VL-DAC는 저비용의 합성 환경에서 VLM을 학습하면서도 실제 이미지 작업에서 측정 가능한 성과 향상을 제공한다는 첫 번째 증거를 제시한다.

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [96] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 이 논문에서는 이미지 분류를 위한 새로운 기계적 잊기 방법(WSS-CL)을 제안하며, 특정 데이터의 영향 삭제를 효율적으로 처리한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 기계적 잊기 접근 방식은 정확한 잊기, 안정성 및 다양한 도메인에서의 적용 가능성 측면에서 어려움을 겪고 있다.

Method: 두 단계로 구성된 효율적인 기계적 잊기 방법인 WSS-CL을 제안, 이것은 중요한 모델 매개변수에 초점을 맞춘다.

Result: 실험 평가 결과, 제안된 방법이 최신 기법들에 비해 훨씬 개선된 잊기 효율성을 달성하면서도 성능 손실이 미미함을 보여준다.

Conclusion: 제안된 방법은 감독 및 자기 지도 설정에서의 사용 가능성을 나타낸다.

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [97] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 이 논문은 사전 훈련된 대형 언어 모델의 성능을 개선하기 위해 긍정 및 부정 토큰으로 데이터를 분류하는 방식의 유용성을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사전 훈련된 대형 언어 모델의 성능 향상은 데이터 품질과 양에 크게 의존한다.

Method: 각 말뭉치를 긍정 토큰과 부정 토큰으로 분류하고, 긍정 토큰은 일반적인 방법으로 훈련하며 부정 토큰은 잊어버리도록 한다.

Result: 이 잊어버리기 메커니즘은 모델의 전반적인 성능을 향상시키고 보다 다양한 응답을 유도함을 발견했다.

Conclusion: 토큰 분류와 잊어버리기 과정은 모델이 더 정확하게 학습할 정보를 안내하는 데 기여한다.

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [98] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS는 클라이언트 데이터를 보호하기 위한 새로운 사설 추론 패러다임으로, 기능 분할을 통해 개인정보 보호와 효율성을 동시에 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 개인 추론 방법들은 개인정보 보호와 효율성 간의 근본적인 트레이드오프에 직면해 있으며, 개선이 필요하다.

Method: PrivDFS는 입력 기능을 분할하여 비상호작용 서버에 분배하고, 클라이언트가 각 서버의 출력을 안전하게 집계하여 최종 예측을 재구성하는 방식이다.

Result: PrivDFS는 CIFAR-10과 CelebA에서 개인정보 보호를 유지하면서 클라이언트의 계산을 최대 100배 줄여 정확도의 손실 없이 성능을 입증했다.

Conclusion: 이 방법과 두 가지 확장(PrivDFS-AT 및 PrivDFS-KD)은 공격에 대한 강인성을 유지하면서도 개인 정보 보호를 강화한다.

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [99] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: MMSFM은 비균등 시간에 수집된 고차원 데이터를 간소화하지 않고 정렬할 수 있는 새로운 방법이다.


<details>
  <summary>Details</summary>
Motivation: 고차원 시스템의 진화 모델링은 전통적인 차원 축소 방법이 동역학을 과도하게 단순화할 수 있어 어려움이 있다.

Method: MMSFM은 다중 마지널 설정을 위한 점수 및 흐름 일치 방법의 새로운 확장을 제안한다.

Result: 합성 데이터와 벤치마크 데이터셋에서 검증되었고, 유전자 발현 데이터와 이미지 진행 과제를 포함한다.

Conclusion: MMSFM은 비균등 시간에 측정된 데이터를 효과적으로 정렬할 수 있는 유용한 방법이다.

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [100] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 본 논문은 다중 샘플 학습에 특화된 지속적 학습 방법을 제안하며, 백혈병 진단에 효과적임을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 실험실 및 클리닉의 동적 환경에서 데이터가 매일 수집되므로 기계 학습 모델의 정기적인 업데이트가 필요하다.

Method: 단일 샘플을 선택하여 리허설 방식으로 지속적 학습을 수행하며, 샘플 및 인스턴스 선택 시 주의도 점수와 클래스 평균으로부터의 거리를 고려한다.

Result: 제안된 방법이 기존의 지속적 학습 방법보다 상당히 뛰어난 성능을 보임을 입증하였다.

Conclusion: 본 연구는 MIL을 위한 첫 번째 지속적 학습 접근 방법을 제공하며, 시간이 지남에 따라 데이터 분포의 변화에 적응하는 모델 개발이 가능함을 보여준다.

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [101] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ는 INT6 양자화를 위한 새로운 포스트 훈련 프레임워크로, 높은 정확도와 효율성을 유지하며 초기 추론 속도를 개선한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 배포에 있어 메모리와 계산 비용이 큰 제약이 되는 문제를 해결하기 위해.

Method: 6비트 가중치 양자화와 소프트웨어 적층 민감도 분석을 통한 8비트 활성화 보존을 포함하는 FlexQ 프레임워크를 제안하고, 이를 위해 고성능 GPU 커널을 개발하였다.

Result: LLaMA 모델에서 FlexQ는 FP16 정확도를 거의 유지하며, 평균 1.39배의 속도 향상과 1.21배의 메모리 절약을 달성하였다.

Conclusion: FlexQ는 기존 솔루션 대비 우수한 성능을 보여주며 코드가 공개되어 활용 가능하다.

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [102] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 다양한 작업에서 성능 향상을 이룬 다중 모달 학습과 그에 따른 설명 가능한 인공지능(XAI)에 관한 시스템적 문헌 리뷰이다.


<details>
  <summary>Details</summary>
Motivation: 다중 모달 모델의 복잡한 의사결정 과정을 해석하려는 수요 증가.

Method: 2020년 1월부터 2024년 초까지 발표된 연구를 분석하여 모델 아키텍처, 모달리티, 설명 알고리즘 및 평가 방법론을 검토.

Result: 대부분의 연구가 비전-언어 및 언어 전용 모델에 집중되었고, 설명에 가장 많이 사용되는 방법은 주의 기반 기법이었다.

Conclusion: 평가 방법이 비체계적이며, 다중 모달 XAI 연구에서 투명하고 표준화된 평가 및 보고 관행을 촉진하기 위한 포괄적인 권장사항을 제시하였다.

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [103] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 본 논문에서는 매트릭스-프리 설정에서 행렬-벡터 곱셈만을 사용하여 두 개의 무한 노름과 하나의 두 개 노름을 추정하기 위한 새로운 랜덤화 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 방법론의 한계를 극복하고, 잭보 모델 기반의 정규화 및 적대적 공격 완화에 대한 효과적인 접근 방식을 제시하고자 합니다.

Method: Hutchinson의 대각선 추정기 및 그 수정 버전인 Hutch++를 사용하여 알고리즘을 개발하고 이들의 복잡도 경계를 분석했습니다.

Result: 제안된 알고리즘이 이미지 분류 작업에서 딥 뉴럴 네트워크 훈련에 있어 효과적임을 보여주었습니다.

Conclusion: 우리의 방법론은 추천 시스템 도메인에서의 적대적 공격을 완화하는 데에도 적용 가능하다는 것을 입증했습니다.

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [104] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: CMCFAE는 클라우드 모델을 WAE 프레임워크에 통합한 새로운 생성 모델이다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 데이터 분포를 보다 정확하게 모델링하기 위해 클라우드 모델의 특성 함수를 활용한다.

Method: 클라우드 모델의 특성 함수를 도출하고, 이를 WAE 프레임워크 내에서 정규화기로 제안한다.

Result: MNIST, FashionMNIST, CIFAR-10 및 CelebA 데이터세트에서 기존 모델보다 우수한 성능을 보인다.

Conclusion: 이 연구는 클라우드 모델 이론과 MMD 기반 정규화의 새로운 통합을 확립하며, 오토인코더 기반 생성 모델을 향상시킬 수 있는 가능성을 제시한다.

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [105] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 본 논문에서는 LLM의 취약점을 효과적으로 식별하기 위해 AI가 다른 AI를 전략적으로 '파괴'하도록 훈련하는 새로운 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 자동화된 방법이 LLM의 복잡한 대화적 특성을 포착하지 못하고 생산적인 레드 팀 작업을 지원하지 못하기 때문이다.

Method: 레드 팀 작전을 마르코프 결정 과정(MDP)으로 정형화하고, 계층적 강화 학습(RL) 프레임워크를 적용하여, 희소한 보상과 긴 수평의 도전 과제를 해결하는 방법을 사용한다.

Result: 제안된 방법은 기존 방식에서 놓친 미세한 취약점을 발견하며, 일관된 다턴 공격 전략을 학습한다.

Conclusion: 이 접근 방식은 LLM 레드 팀 작업을 동적이고 궤도 기반의 과정으로 재구성하여 강력한 AI 배포를 위한 필수적인 요소로 설정한다.

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [106] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 이 논문은 '작업 전환' 맥락에서 작은 규모의 주의 기반 아키텍처가 전통적인 접근 방법보다 성능이 우수하지 않음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 작업 전환을 통한 작은 규모의 주의 기반 아키텍처의 성능을 비교 연구하기 위해.

Method: 기본적인 과제 전환 모델에서 표준 변환기, LSTM, MLP를 비교하고, cisformer와 광범위한 주의 메커니즘을 포함한 확장을 통해 성능을 분석함.

Result: 표준 변환기와 LSTM, MLP는 유사한 정확성을 보였으나, cisformer와 광범위한 주의 메커니즘의 조합이 95%의 성능을 달성함.

Conclusion: 작업 전환 설정에서 다양한 접근 방식을 비교함으로써 주의 메커니즘의 이해와 향상을 도모할 수 있음을 나타냄.

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [107] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 이 논문은 머신러닝 모델의 성능 평가를 위해 계층적 스코어링 메트릭스를 개발하고, 이는 예측 결과와 실제 라벨 간의 거리를 반영하여 미분류 오류를 세분화하여 이해할 수 있게 한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 이진 평가 방식은 모든 분류 오류를 동등하게 취급하므로, 오류의 종류나 영향을 구분할 수 없다.

Method: 계층적 스코어링 메트릭스를 개발하여 클래스 라벨 간의 관계를 인코딩하는 스코어링 트리를 활용하고, 다양한 복잡도의 메트릭스를 생성한다.

Result: 이 메트릭스는 오류를 더 세분화하여 캡처할 수 있으며, 스코어링 트리를 통해 조정할 수 있다.

Conclusion: 이 접근법은 모델 성능을 평가하는 새로운 방법을 제시하며, 오류의 수뿐만 아니라 오류의 종류나 영향을 기준으로 모델을 평가한다.

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [108] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 새로운 캐시 기반의 병렬 추정 디코딩 프레임워크가 LLM 추론 가속화를 이루어내며, 디랙팅과 검증을 분리하여 속도 향상을 가능하게 함.


<details>
  <summary>Details</summary>
Motivation: 기존의 추정 디코딩 방법은 비효율적인 성능과 모델 크기 제한 문제를 가지고 있음.

Method: 캐시 기반의 병렬 추정 디코딩 프레임워크인 CARD는 '쿼리-교정' 패러다임을 사용하여 드래프팅과 검증을 분리함.

Result: 이 방법은 기존 디코딩에 비해 최대 4.83배 속도 향상을 이루었음.

Conclusion: 본 연구는 디래프팅과 검증 프로세스의 비효율성을 해결하여 모델의 추론 가속화를 달성하였음.

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [109] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM은 다채널 해상도 정보 대칭 모듈을 활용한 경량 다변량 시계열 분류기이며, 기존 모델보다 적은 매개변수와 연산량으로 우수한 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 다변량 시계열 분류는 다양한 분야에서 중요하지만, 기존 모델들은 계산 비용이 크고 매개변수가 많아 효율성이 떨어진다.

Method: PRISM은 다중 시간 척도에서 대칭 유한 임펄스 응답 필터를 적용하여 채널마다 독립적으로 기능을 추출하는 컨볼루션 기반 모델이다.

Result: PRISM은 인간 활동, 수면 단계 및 생물 의학 벤치마크에서 기존 CNN 및 Transformer 모델과 동등하거나 우수한 성능을 보여준다.

Conclusion: 전통적인 신호 처리 통찰을 현대 딥 러닝과 결합하여 PRISM은 자원 효율적이고 정확한 다변량 시계열 분류 솔루션을 제공한다.

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [110] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal은 전역 및 지역 특징 학습을 동시에 수행하며, 다중 스케일 문제에 효과적으로 대응하는 Transformer 기반 신경 연산자 방법론이다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법론은 지역 물리적 세부 사항과 전역 특징 간의 상호 의존성 학습을 간과하여 다중 스케일 문제 해결 및 장기 예측의 물리적 일관성과 수치적 안정성을 확보하지 못했다.

Method: GFocal 방법은 Nyström 어텐션 기반의 글로벌 블록과 슬라이스 기반의 포컬 블록을 통해 전역 상관관계와 지역 특징을 통합하고, 컨볼루션 기반 게이팅 블록을 통해 다중 스케일 정보를 동적으로 융합한다.

Result: GFocal은 임의의 기하학과 초기 조건에 대해 물리적 특징의 정확한 모델링 및 예측을 달성하며, 6개의 벤치마크 중 5개에서 평균 15.2%의 상대적 성능 향상을 기록했다.

Conclusion: GFocal은 자동차와 항공기의 공기역학 시뮬레이션과 같은 산업 규모의 시뮬레이션에서도 우수한 성능을 보인다.

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [111] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: GraphProp는 그래프 구조의 일반화를 강조하여 그래프 수준 작업에서의 일반화 능력을 향상시키는 새로운 그래프 기초 모델이다.


<details>
  <summary>Details</summary>
Motivation: 그래프 수준 작업에서의 강력한 일반화 능력이 필요하며, 여러 도메인 간 일관된 정보를 포착하는 것이 효과적인 학습에 중요하다.

Method: GraphProp는 두 가지 주요 단계를 포함하는 훈련 프로세스를 통해 구조 GFM을 훈련하고, 그래프 불변량 예측을 통해 추상적 구조 정보를 캡처한다.

Result: GraphProp는 감독 학습 및 적은 샘플 학습에서 경쟁 모델들에 비해 상당한 성능 향상을 보인다.

Conclusion: GraphProp는 노드 속성이 없는 그래프 처리에서 특히 뛰어난 성능을 발휘하며, 구조 일반화를 강조한 접근 방식이 효과적임을 보여준다.

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [112] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: FedHiP는 데이터 이질성 문제를 해결하기 위해 기울기 기반 업데이트를 회피하는 개인화 연합 학습 방식이다.


<details>
  <summary>Details</summary>
Motivation: 기존 개인화 연합 학습(PFL) 방법들은 고객 간의 데이터 이질성으로 인해 수렴성이 저하되고 성능이 저하되는 문제에 직면해 있다.

Method: 기울기 기반 업데이트를 피하기 위해 닫힌 형태의 해를 이용한 FedHiP 방안을 제안하며, 자기 감독을 통한 프리트레이닝을 활용한다.

Result: FedHiP는 기존 최첨단 방법보다 정확도가 최소 5.79%-20.97% 향상된 결과를 보여준다.

Conclusion: FedHiP는 모든 고객에서 비슷한 모델을 유지함으로써 데이터 분포와 상관없이 개인화된 모델을 효과적으로 제공한다.

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [113] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 이 논문은 계층적 처리와 에너지 효율성에 영감을 받아 평생 네트워크 침입 탐지 시스템을 위한 스파이킹 신경망 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 뇌의 계층적 처리와 에너지 효율성을 모방하여 네트워크 침입 탐지 시스템의 성능을 향상시키기 위함.

Method: 정적 스파이킹 신경망(SNN)으로 잠재적 침입을 식별하고, 이를 기반으로 적응형 동적 SNN이 특정 공격 유형을 분류하도록 설계.

Result: UNSW-NB15 벤치마크 데이터세트에서 85.3%의 정확도로 강력한 적응성과 낮은 재난 망각을 입증.

Conclusion: 이 아키텍처는 신경형 하드웨어에서 저전력 배치의 가능성을 보여준다.

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [114] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 가정 에너지 수요 감소는 기후 완화 및 연료 빈곤 전략에서 중요하지만, 에너지 효율성 개입의 효과는 매우 다양하다.


<details>
  <summary>Details</summary>
Motivation: 가정 에너지 수요 감소의 중요성과 기후 정책의 공정성을 고려할 필요가 있음.

Method: 영국 주택의 국가대표 데이터를 기반으로 causal machine learning 모델을 사용하여 벽 단열재가 가스 소비에 미치는 평균 및 조건부 처리 효과를 추정하였다.

Result: 개입은 평균적으로 가스 수요를 19%까지 줄였으나, 낮은 에너지 부담 그룹은 상당한 절감을 경험한 반면, 높은 부담 그룹은 거의 감소가 없었다.

Conclusion: 이 연구는 기후 영향과 가정 에너지 정책의 공정성 함의를 고려한 평가 프레임워크의 필요성을 제기한다.

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [115] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 본 논문은 연관 예측(Link Prediction)에서 그래프 신경망(Graph Neural Networks)의 성능을 향상시키기 위해 프리트레인을 활용한 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 그래프 기계 학습에서 연관 예측은 중요한 작업이지만, 기존 방법들이 희소한 연결성으로 인한 제한적 감독, 초기화 민감성, 분포 변화에 대한 일반화 저하와 같은 주요 과제에 직면해 있습니다.

Method: 노드 및 엣지 수준 정보를 통합하는 쌍별 작업인 연관 예측에 적합한 모듈의 전이 가능성을 체계적으로 연구하고, 그 출력들을 효과적으로 결합하는 지연 융합 전략을 제안합니다. 또한 다양한 프리트레인 데이터에 대처하기 위해 전문가 혼합(Mixture-of-Experts) 프레임워크를 도입하여 별개의 전문가에서의 패턴을 포착합니다.

Result: 실험을 통해 16개의 서로 다른 데이터셋에서 가져온 결과는 본 접근 방식이 최신의 성능을 달성했으며, 자원 제한에서 뛰어난 결과를 보였음을 나타냅니다.

Conclusion: 프리트레인된 모델이 다양한 하위 데이터셋에 빠르게 적응할 수 있도록 최소한의 계산 오버헤드로 파라미터 효율적인 조정 전략을 개발하여, 최첨단 성능을 달성했습니다.

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [116] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 이 논문은 cGANs를 활용한 감정 인식의 딥 러닝 기반 접근법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 단일 데이터 유형에 의존하는 기법과 달리, 텍스트, 오디오, 얼굴 표정을 통합한 다중 모드 프레임워크를 탐구하기 위함입니다.

Method: 제안된 cGAN 아키텍처는 감정이 풍부한 합성 데이터를 생성하고 다중 모드에서 분류 정확도를 향상시키도록 훈련됩니다.

Result: 실험 결과, 베이스라인 모델에 비해 감정 인식 성능이 유의미하게 향상되었습니다.

Conclusion: cGANs는 인간-컴퓨터 상호작용 시스템에서 더 미묘한 감정 이해를 가능하게 하여 그 가능성을 강조합니다.

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [117] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 본 논문은 LLM과 전통적인 강화 학습 에이전트의 인과 추론 부족 문제를 해결하기 위해 'Causal Reflection' 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM과 강화 학습 에이전트 모두 인과 이해 부족으로 인해 비효율적인 성능을 보인다.

Method: Causal Reflection 프레임워크를 통해 상태, 행동, 시간, 교란에 대한 동적 인과 모델링을 수행하고, Reflect 메커니즘을 통해 예측과 관찰된 결과의 불일치를 식별하고 내부 모델을 수정한다.

Result: Causal Reflection 에이전트는 지연된 비선형 효과를 추론할 수 있으며, LLM을 인과 출력에서 자연어 설명과 반사실적 상황으로 변환하는 구조적 추론 엔진으로 활용한다.

Conclusion: 이 프레임워크는 발전하는 환경에서 인과 이해를 적응하고 수정하며 소통할 수 있는 Causal Reflective 에이전트를 위한 이론적 기초를 제공한다.

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [118] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 이 논문은 연합 교통 예측을 위한 새로운 채널 독립 패러다임(CIP)을 제안하며, 이는 데이터 공유 없이도 효율적이고 정확한 예측을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 교통 데이터가 여러 데이터 소유자 간에 분산되어 있고, 프라이버시 제한으로 인해 이러한 분리된 데이터셋을 직접 활용하지 못하는 문제를 해결하고자 함.

Method: 채널 독립 패러다임(CIP)을 통해 클라이언트 간의 통신 필요성을 제거하고, 각 노드가 로컬 정보만으로 예측을 수행할 수 있게 함. 이를 바탕으로, Fed-CI라는 효율적인 연합 학습 프레임워크를 개발.

Result: Fed-CI는 기존 방법보다 RMSE, MAE, MAPE에서 각각 8%, 14%, 16% 개선된 성능을 보였으며, 통신 비용도 크게 줄임.

Conclusion: Fed-CI는 프라이버시 규정을 준수하면서도 효율적인 연합 교통 예측을 달성하였고, 다양한 실제 데이터셋에서 기존 방법들을 뛰어넘는 성능을 보임.

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [119] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 이 논문은 저항 점 용접에서 비파괴 품질 평가를 위한 물리 기반 신경망을 활용한 연구를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 자동차 산업에서 본체-화이트의 용접 품질은 매우 중요하지만, 기존의 측정 방식이 파괴적이어서 품질 관리에 제한이 있습니다.

Method: 두 가지 새로운 훈련 전략을 도입하여 실험 데이터와의 통합을 개선하고, 동적 변위와 용접 너겟 직경에 대한 실험 손실을 점진적으로 포함시킵니다.

Result: 2차원 네트워크는 실험 신뢰 구간 내에서 동적 변위와 용접 너겟 성장을 예측하고, 스틸에서 알루미늄으로의 용접 단계 전환을 지원합니다.

Conclusion: 이 연구는 산업 응용에서 빠르고 모델 기반의 품질 관리를 위한 강력한 가능성을 보여줍니다.

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [120] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 본 논문은 확률 분포 간의 매핑 학습을 위한 새로운 프레임워크를 제안하며, 이는 기존의 흐름 및 확산 모델을 일반화한다.


<details>
  <summary>Details</summary>
Motivation: 확률 분포 간의 연결을 통해 다양한 작업을 수행할 수 있는 생성 모델을 개발하고자 하는 바에 있다.

Method: 기존의 확률적 보간 기법을 일반화하여 스칼라 시간 변수를 벡터, 행렬 또는 선형 연산자로 대체하였다.

Result: 우리의 방법은 조건부 생성, 인페인팅, 미세 조정 및 후방 샘플링, 다중 스케일 모델링에서 제로샷 성능을 보여주었다.

Conclusion: 제안한 방법은 특정 작업에 특화된 모델 대비 범용적인 대안으로서의 잠재력을 지닌다.

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [121] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: CaPulse는 시계열 이상 탐지를 위한 인과 기초 프레임워크로, 기존 방법을 능가하며 추가적인 해석 가능성을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 시계열 이상 탐지 방법들은 이상 생성의 근본 메커니즘을 포착하지 못하고, 레이블 부족, 데이터 불균형, 복잡한 다주기성 등의 문제에 직면해 있다.

Method: 구조적 인과 모델을 기반으로 하여 이상치 생성 과정을 해독하고, 주기적 정규화 흐름과 혁신적인 마스크 메커니즘을 사용하여 주기성을 인식하는 밀도 기반 이상 탐지 접근 방식을 제안한다.

Result: 다양한 7개의 실제 데이터셋에서 실험을 진행하여, CaPulse가 기존 방법보다 3%에서 17%의 AUROC 개선을 보였으며, 해석 가능성도 향상되었다.

Conclusion: CaPulse는 시계열 이상 탐지의 효율성을 높이고, 데이터의 복잡한 문제들을 해결하는 데 기여한다.

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [122] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0는 다세대 데이터셋을 기반으로 한 최신 생물음향 모델로, 새 및 해양 생물 분류에서 탁월한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 생물음향 분야에서 다양한 종의 음성을 정확하게 분류할 수 있는 모델의 필요성을 해결하기 위해.

Method: 자기 증류와 프로토타입 학습 분류기를 사용하여 모델을 훈련하고 새로운 소스 예측 훈련 기준을 적용했다.

Result: Perch 2.0는 BirdSet과 BEANS 벤치마크에서 최상위 성능을 기록했으며, 해양 전이 학습 작업에서도 전문 해양 모델보다 뛰어난 성능을 보였다.

Conclusion: 미세한 종 분류가 생물음향을 위한 강력한 사전 훈련 작업이 되는 이유에 대한 가설을 제시하였다.

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [123] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 본 논문은 적대적 레이블 노이즈가 있는 경우의 단일 지수 모델 학습에 대한 효율적인 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 적대적 레이블 노이즈가 있는 상황에서의 단일 지수 모델 학습 문제를 해결하고자 함.

Method: 문제의 구조와 성질을 활용하여 전통적인 기울기 방법의 경계를 넘어서는 최적화 프레임워크를 개발함.

Result: 모든 단조 활성화 함수에 대한 상수 계수 근사를 달성하는 최초의 효율적인 알고리즘을 제안함.

Conclusion: 제안된 방법은 여러 종류의 활성화 함수를 포괄하며, 기존 연구에 비해 더 넓은 범위의 함수에도 적용 가능함을 보인다.

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [124] [Forgive and Forget? An Industry 5.0 Approach to Trust-Fatigue Co-regulation in Human-Cobot Order Picking](https://arxiv.org/abs/2508.03765)
*Soumyadeep Dhar*

Main category: cs.MA

TL;DR: 이 논문은 협업 로봇(cobot)과의 신뢰 및 피로의 역할을 조사하며, 물류 5.0 맥락에서 이 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 최신 물류 환경에서 인간과 로봇의 협력이 중요해짐에 따라 신뢰와 피로의 역할을 이해할 필요성이 커졌다.

Method: 유도-추종 Stackelberg 게임을 구성하고, 에이전트 기반 시뮬레이션을 통해 신뢰 모델의 영향을 분석하였다.

Result: 정교한 신뢰 모델은 생산성을 100% 향상시키며, 프로액티브 신뢰 복구 프로토콜이 시스템의 불안정성을 극복하였다.

Conclusion: 산업 5.0의 인간 중심성, 지속 가능성 및 회복력 원칙을 충족하는 지능형 cobot 행동 설계를 위한 프레임워크를 제공한다.

Abstract: This paper investigates the critical role of trust and fatigue in human-cobot
collaborative order picking, framing the challenge within the scope of
Logistics 5.0 -- the implementation of human-robot symbiosis in smart
logistics. We propose a dynamic, leader-follower Stackelberg game to model this
interaction, where utility functions explicitly account for human fatigue and
trust. Through agent-based simulations, we demonstrate that while a naive model
leads to a "trust death spiral," a refined trust model creates a "trust synergy
cycle," increasing productivity by nearly 100 percent. Finally, we show that a
cobot equipped with a proactive Trust-Repair Protocol can overcome system
brittleness, reducing trust recovery time after a severe failure by over 75
percent compared to a non-adaptive model. Our findings provide a framework for
designing intelligent cobot behaviors that fulfill the Industry 5.0 pillars of
human-centricity, sustainability, and resilience.

</details>


### [125] [When Agents Break Down in Multiagent Path Finding](https://arxiv.org/abs/2508.03777)
*Foivos Fioravantes,Dušan Knop,Nikolaos Melissinos,Michal Opler*

Main category: cs.MA

TL;DR: 다중 에이전트 경로 탐색(MAPF)에서, 에이전트의 지연을 포함한 최적의 스케줄 유지를 위한 동적 적응 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 일부 에이전트가 고장으로 인해 지연되는 시나리오를 모델링하여 최적의 스케줄 유지를 도전하는 문제를 해결하고자 한다.

Method: 전체 재계획에 의존하지 않고, 에이전트가 로컬에서 경로를 조정하고 협력할 수 있는 프로토콜을 개발하였다.

Result: 주 통신 프로토콜을 따를 경우, k번의 고장 후 메이크스팬 증가가 k개의 추가 턴으로 제한된다는 것을 입증하였다.

Conclusion: 이러한 프로토콜은 에이전트 실패에 대해 탄력적인 다중 에이전트 탐색을 위한 실제적이고 확장 가능한 접근 방식을 제공한다.

Abstract: In Multiagent Path Finding (MAPF), the goal is to compute efficient,
collision-free paths for multiple agents navigating a network from their
sources to targets, minimizing the schedule's makespan-the total time until all
agents reach their destinations. We introduce a new variant that formally
models scenarios where some agents may experience delays due to malfunctions,
posing significant challenges for maintaining optimal schedules.
  Recomputing an entirely new schedule from scratch after each malfunction is
often computationally infeasible. To address this, we propose a framework for
dynamic schedule adaptation that does not rely on full replanning. Instead, we
develop protocols enabling agents to locally coordinate and adjust their paths
on the fly. We prove that following our primary communication protocol, the
increase in makespan after k malfunctions is bounded by k additional turns,
effectively limiting the impact of malfunctions on overall efficiency.
Moreover, recognizing that agents may have limited computational capabilities,
we also present a secondary protocol that shifts the necessary computations
onto the network's nodes, ensuring robustness without requiring enhanced agent
processing power. Our results demonstrate that these protocols provide a
practical, scalable approach to resilient multiagent navigation in the face of
agent failures.

</details>


### [126] [DRAMA: A Dynamic and Robust Allocation-based Multi-Agent System for Changing Environments](https://arxiv.org/abs/2508.04332)
*Naibo Wang,Yifan Zhang,Sai Liu,Xinkui Zhao,Guanjie Cheng,Yueshen Xu*

Main category: cs.MA

TL;DR: 이 논문에서는 동적 환경에서의 다중 에이전트 시스템(DRAMA)을 제안하여 유연하고 탄력적인 협업을 촉진한다.


<details>
  <summary>Details</summary>
Motivation: 실제 환경은 동적이며 불확실하므로 기존의 정적 에이전트 구조가 적응성을 제한한다.

Method: 모듈화된 아키텍처와 자원 객체로서의 에이전트 및 작업을 추상화하고, 친화 기반의 느슨한 결합 메커니즘을 통해 작업 배정을 수행한다.

Result: 실시간 모니터링 및 중앙 집중 계획을 통해 에이전트가 변경될 때에도 지속적이고 강력한 작업 수행을 보장한다.

Conclusion: DRAMA는 다중 에이전트 협동의 유연성과 효율성을 높인다.

Abstract: Multi-agent systems (MAS) have demonstrated significant effectiveness in
addressing complex problems through coordinated collaboration among
heterogeneous agents. However, real-world environments and task specifications
are inherently dynamic, characterized by frequent changes, uncertainty, and
variability. Despite this, most existing MAS frameworks rely on static
architectures with fixed agent capabilities and rigid task allocation
strategies, which greatly limits their adaptability to evolving conditions.
This inflexibility poses substantial challenges for sustaining robust and
efficient multi-agent cooperation in dynamic and unpredictable scenarios. To
address these limitations, we propose DRAMA: a Dynamic and Robust
Allocation-based Multi-Agent System designed to facilitate resilient
collaboration in rapidly changing environments. DRAMA features a modular
architecture with a clear separation between the control plane and the worker
plane. Both agents and tasks are abstracted as resource objects with
well-defined lifecycles, while task allocation is achieved via an
affinity-based, loosely coupled mechanism. The control plane enables real-time
monitoring and centralized planning, allowing flexible and efficient task
reassignment as agents join, depart, or become unavailable, thereby ensuring
continuous and robust task execution. The worker plane comprises a cluster of
autonomous agents, each with local reasoning, task execution, the ability to
collaborate, and the capability to take over unfinished tasks from other agents
when needed.

</details>


### [127] [Position-Based Flocking for Robust Alignment](https://arxiv.org/abs/2508.04378)
*Hossein B. Jond*

Main category: cs.MA

TL;DR: 위치 기반 군집 모델을 제시하여 안정적인 집단 운동을 달성.


<details>
  <summary>Details</summary>
Motivation: 상호 작용하는 에이전트의 군집 행동을 안정화하기 위해.

Method: 초기 및 현재 위치를 사용해 속도 차이를 근사하여 위치 기반 모델을 수정.

Result: 50개 에이전트를 사용한 2D 시뮬레이션에서 속도 기반 모델보다 강한 정렬과 더 견고한 형성을 보여주었다.

Conclusion: 위치 기반 모델이 로봇 공학 및 집단 동역학에서 유용한 응용 가능성을 지닌다.

Abstract: This paper presents a position-based flocking model for interacting agents,
balancing cohesion-separation and alignment to achieve stable collective
motion. The model modifies a velocity-based approach by approximating velocity
differences using initial and current positions, introducing a threshold weight
to ensure sustained alignment. Simulations with 50 agents in 2D demonstrate
that the position-based model produces stronger alignment and more rigid and
compact formations compared to the velocity-based model. The alignment metric
and separation distances highlight the efficacy of the proposed model in
achieving robust flocking behavior. The model's use of positions ensures robust
alignment, with applications in robotics and collective dynamics.

</details>
