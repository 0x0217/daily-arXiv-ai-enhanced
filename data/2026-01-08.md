<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 9]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks](https://arxiv.org/abs/2601.02439)
*Hao Bai,Alexey Taymanov,Tong Zhang,Aviral Kumar,Spencer Whitehead*

Main category: cs.LG

TL;DR: WebGym은 현실적인 시각 웹 에이전트를 훈련하기 위한 최대 규모의 오픈 소스 환경으로, 300,000개의 다양한 작업과 실제 웹사이트에서의 평가를 포함하고 있다.


<details>
  <summary>Details</summary>
Motivation: 실제 웹사이트는 비정상적이고 다양하여, 인공지능이나 소규모 작업 집합으로는 강력한 정책 학습에 부족하다.

Method: 에이전트의 상호작용 기록을 바탕으로 학습하는 간단한 강화 학습 레시피를 사용하여 에이전트를 훈련시킨다. 높은 처리량의 비동기 롤아웃 시스템을 개발하여 샘플링 속도를 높였다.

Result: WebGym에서 강력한 기초 비전-언어 모델인 Qwen-3-VL-8B-Instruct를 미세 조정하여 성공률을 26.2%에서 42.9%로 향상시켰다.

Conclusion: 이는 훈련 중에 본 적이 없는 웹사이트에서만 작업으로 구성된 테스트 세트를 사용하여 시각 웹 에이전트를 훈련한 기존 연구들에 비해 상당한 개선이다.

Abstract: We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.

</details>


### [2] [LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2601.02511)
*Bahareh Golchin,Banafsheh Rekabdar,Danielle Justo*

Main category: cs.LG

TL;DR: 이 연구는 시간 연속 데이터의 이상 탐지 문제를 해결하기 위한 통합 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 재무, 헬스케어, 센서 네트워크 및 산업 모니터링 응용 프로그램에서 시간 연속 데이터의 이상 탐지를 수행하는 것이 중요하다.

Method: 대형 언어 모델 기반의 잠재적 기능을 보상 형태로 통합하고, 강화 학습(RL)과 변분 오토인코더(VAE)로 보상을 동적으로 조절하며, 적극적 학습을 통해 레이블 전파를 수행하는 통합 프레임워크를 제안한다.

Result: 이 방법은 Yahoo-A1 및 SMD 벤치마크에서 제한된 레이블 예산으로 첨단 탐지 정확도를 달성하며 데이터 제약 환경에서도 효과적으로 작동한다.

Conclusion: 이 연구는 실제 응용 프로그램에서 강력하고 확장 가능한 이상 탐지를 위해 LLM과 RL, 고급 비지도 기술을 결합하는 것의 가능성을 강조한다.

Abstract: Detecting anomalies in time series data is crucial for finance, healthcare, sensor networks, and industrial monitoring applications. However, time series anomaly detection often suffers from sparse labels, complex temporal patterns, and costly expert annotation. We propose a unified framework that integrates Large Language Model (LLM)-based potential functions for reward shaping with Reinforcement Learning (RL), Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation. An LSTM-based RL agent leverages LLM-derived semantic rewards to guide exploration, while VAE reconstruction errors add unsupervised anomaly signals. Active learning selects the most uncertain samples, and label propagation efficiently expands labeled data. Evaluations on Yahoo-A1 and SMD benchmarks demonstrate that our method achieves state-of-the-art detection accuracy under limited labeling budgets and operates effectively in data-constrained settings. This study highlights the promise of combining LLMs with RL and advanced unsupervised techniques for robust, scalable anomaly detection in real-world applications.

</details>


### [3] [Normalized Conditional Mutual Information Surrogate Loss for Deep Neural Classifiers](https://arxiv.org/abs/2601.02543)
*Linfeng Ye,Zhixiang Chi,Konstantinos N. Plataniotis,En-hui Yang*

Main category: cs.LG

TL;DR: 이 논문에서는 딥 뉴럴 네트워크 기반 분류기 훈련을 위한 대안으로 정규화 조건부 상호 정보(NCMI)라는 새로운 정보 이론적 대체 손실을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 모델의 NCMI가 정확도와 반비례함을 관찰했습니다.

Method: NCMI를 효율적으로 최소화하기 위한 교대 알고리즘을 도입했습니다.

Result: NCMI로 훈련된 모델이 국가별 최첨단 손실을 상당히 초과하였고, 계산 비용은 CE와 비슷했습니다.

Conclusion: NCMI는 CE에 대한 실용적이고 경쟁력 있는 대안입니다.

Abstract: In this paper, we propose a novel information theoretic surrogate loss; normalized conditional mutual information (NCMI); as a drop in alternative to the de facto cross-entropy (CE) for training deep neural network (DNN) based classifiers. We first observe that the model's NCMI is inversely proportional to its accuracy. Building on this insight, we introduce an alternating algorithm to efficiently minimize the NCMI. Across image recognition and whole-slide imaging (WSI) subtyping benchmarks, NCMI-trained models surpass state of the art losses by substantial margins at a computational cost comparable to that of CE. Notably, on ImageNet, NCMI yields a 2.77% top-1 accuracy improvement with ResNet-50 comparing to the CE; on CAMELYON-17, replacing CE with NCMI improves the macro-F1 by 8.6% over the strongest baseline. Gains are consistent across various architectures and batch sizes, suggesting that NCMI is a practical and competitive alternative to CE.

</details>


### [4] [CutisAI: Deep Learning Framework for Automated Dermatology and Cancer Screening](https://arxiv.org/abs/2601.02562)
*Rohit Kaushik,Eva Kaushik*

Main category: cs.LG

TL;DR: 본 논문에서는 피부과 이미징과 모바일 진단 도구의 발전을 반영한 시스템인 Conformal Bayesian Dermatological Classifier (CBDC)를 제안하며, 이는 피부과 변동성을 반영하는 분포 의존성 일반화 경계를 제공하고, 안정성을 보장하는 정리와 신뢰할 수 있는 불확실성 정량화를 위한 유한한 보장을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 피부과 이미징의 빠른 성장에 따라 실증적 성능뿐만 아니라 강력한 이론적 보장이 필요한 시스템의 개발이 요구되고 있다.

Method: 우리는 통계학적 학습 이론, 토폴로지 데이터 분석(TDA) 및 베이지안 적합 추론을 결합한 Conformal Bayesian Dermatological Classifier (CBDC)라는 프레임워크를 제안한다.

Result: CBDC는 피부과 변동성을 반영하는 분포 의존적 일반화 경계를 제공하고, 합성곱 신경망 임베딩이 광학적 및 형태적 섭동에 대해 불변임을 보장하는 토폴로지 안정성 정리를 증명하며, 신뢰할 수 있는 불확실성 정량화를 위한 유한한 보장을 제공한다.

Conclusion: CBDC는 HAM10000, PH2, ISIC 2020 데이터셋에 대한 광범위한 실험을 통해 임상적으로 해석 가능한 보정된 예측을 생성하면서도 분류 정확도를 달성함을 보여준다.

Abstract: The rapid growth of dermatological imaging and mobile diagnostic tools calls for systems that not only demonstrate empirical performance but also provide strong theoretical guarantees. Deep learning models have shown high predictive accuracy; however, they are often criticized for lacking well, calibrated uncertainty estimates without which these models are hardly deployable in a clinical setting. To this end, we present the Conformal Bayesian Dermatological Classifier (CBDC), a well, founded framework that combines Statistical Learning Theory, Topological Data Analysis (TDA), and Bayesian Conformal Inference. CBDC offers distribution, dependent generalization bounds that reflect dermatological variability, proves a topological stability theorem that guarantees the invariance of convolutional neural network embeddings under photometric and morphological perturbations and provides finite conformal coverage guarantees for trustworthy uncertainty quantification.
  Through exhaustive experiments on the HAM10000, PH2, and ISIC 2020 datasets, we show that CBDC not only attains classification accuracy but also generates calibrated predictions that are interpretable from a clinical perspective. This research constitutes a theoretical and practical leap for deep dermatological diagnostics, thereby opening the machine learning theory clinical applicability interface.

</details>


### [5] [LendNova: Towards Automated Credit Risk Assessment with Language Models](https://arxiv.org/abs/2601.02573)
*Kiarash Shamsi,Danijel Novokmet,Joshua Peters,Mao Lin Liu,Paul K Edwards,Vahab Khoshdel*

Main category: cs.LG

TL;DR: LendNova는 신용 위험 평가를 위한 최초의 자동화된 엔드 투 엔드 파이프라인으로, 복잡한 수작업 없이 원시 신용 기록의 모든 정보를 활용합니다.


<details>
  <summary>Details</summary>
Motivation: 신용 위험 평가는 금융 분야에서 필수적이지만, 기존의 방법들은 고비용의 특징 기반 모델에 의존해왔다.

Method: LendNova는 고급 NLP 기술과 언어 모델을 활용하여 원시 신용 기록의 모든 정보를 사용합니다.

Result: 실제 데이터에 대한 평가에서 정확하고 효율적인 위험 평가의 강력한 잠재력을 보여줍니다.

Conclusion: 미래의 연구를 위한 기초를 마련하고, 더 정확하고 유연하며 자동화된 금융 의사 결정을 가능하게 하는 기초 시스템을 위한 토대를 제공합니다.

Abstract: Credit risk assessment is essential in the financial sector, but has traditionally depended on costly feature-based models that often fail to utilize all available information in raw credit records. This paper introduces LendNova, the first practical automated end-to-end pipeline for credit risk assessment, designed to utilize all available information in raw credit records by leveraging advanced NLP techniques and language models. LendNova transforms risk modeling by operating directly on raw, jargon-heavy credit bureau text using a language model that learns task-relevant representations without manual feature engineering. By automatically capturing patterns and risk signals embedded in the text, it replaces manual preprocessing steps, reducing costs and improving scalability. Evaluation on real-world data further demonstrates its strong potential in accurate and efficient risk assessment. LendNova establishes a baseline for intelligent credit risk agents, demonstrating the feasibility of language models in this domain. It lays the groundwork for future research toward foundation systems that enable more accurate, adaptable, and automated financial decision-making.

</details>


### [6] [When Prompting Meets Spiking: Graph Sparse Prompting via Spiking Graph Prompt Learning](https://arxiv.org/abs/2601.02662)
*Bo Jiang,Weijun Zhao,Beibei Wang,Jin Tang*

Main category: cs.LG

TL;DR: 이 논문은 희소 그래프 프롬프트(feature) 학습을 제안하여, 신경 세포 메커니즘을 활용해 예측 노드의 특징을 선택적으로 프로모팅하여 성능을 향상시키는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 스파이크 뉴런이 저렴한 정보 처리를 수행하고 희소 출력을 생성한다는 관찰에서 영감을 받았다.

Method: 스파이크 뉴런 아키텍처를 활용하여 각 노드에 대해 희소 프롬프트 벡터를 학습하고, 희소 표현 이론에 근거한 새로운 프롬프트 표현 학습 모델을 도입하였다.

Result: 여러 벤치마크에 대한 광범위한 실험을 통해 SpikingGPF의 효율성과 강건성을 입증하였다.

Conclusion: SpikingGPF는 컴팩트하고 경량화된 프로모팅 설계를 제공하며, 노드 노이즈에 대한 견고성을 향상시켰다.

Abstract: Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node's all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.

</details>


### [7] [Topology-Independent Robustness of the Weighted Mean under Label Poisoning Attacks in Heterogeneous Decentralized Learning](https://arxiv.org/abs/2601.02682)
*Jie Peng,Weiyu Li,Stefan Vlaski,Qing Ling*

Main category: cs.LG

TL;DR: 본 논문은 라벨 포이즈닝 공격에 대한 분산 경량화의 로버스트성을 분석하며, 전통적인 평균 가중 집계기가 하이퍼파라미터 변화에 따라 더 나은 성능을 발휘할 수 있는 경우를 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 분산 신호 처리 및 기계 학습 시스템에서 악의적인 공격에 대한 로버스트성은 매우 중요합니다.

Method: 이 논문은 라벨 포이즈닝 공격 하에서 분산 경량화의 로버스트성을 분석하며, 견고한 집계기와 가중 평균 집계기를 모두 고려합니다.

Result: 이론적 결과는 강력한 집계기의 학습 오류가 네트워크 토폴로지에 의존하는 반면, 가중 평균 집계기의 성능은 토폴로지와 무관하다는 것을 드러냅니다.

Conclusion: 특히, 글로발 오염률이 지역 오염률보다 작거나, 정규 에이전트 네트워크가 연결이 끊어져 있거나, 정규 에이전트 네트워크가 희소하고 지역 오염률이 높은 경우에 가중 평균 집계기가 견고한 집계기보다 더 나은 성능을 발휘할 수 있다는 것을 보여주었습니다.

Abstract: Robustness to malicious attacks is crucial for practical decentralized signal processing and machine learning systems. A typical example of such attacks is label poisoning, meaning that some agents possess corrupted local labels and share models trained on these poisoned data. To defend against malicious attacks, existing works often focus on designing robust aggregators; meanwhile, the weighted mean aggregator is typically considered a simple, vulnerable baseline. This paper analyzes the robustness of decentralized gradient descent under label poisoning attacks, considering both robust and weighted mean aggregators. Theoretical results reveal that the learning errors of robust aggregators depend on the network topology, whereas the performance of weighted mean aggregator is topology-independent. Remarkably, the weighted mean aggregator, although often considered vulnerable, can outperform robust aggregators under sufficient heterogeneity, particularly when: (i) the global contamination rate (i.e., the fraction of poisoned agents for the entire network) is smaller than the local contamination rate (i.e., the maximal fraction of poisoned neighbors for the regular agents); (ii) the network of regular agents is disconnected; or (iii) the network of regular agents is sparse and the local contamination rate is high. Empirical results support our theoretical findings, highlighting the important role of network topology in the robustness to label poisoning attacks.

</details>


### [8] [ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis](https://arxiv.org/abs/2601.02915)
*Kenan Li,Yijian Zhang,Jin Wang,Haipeng Gan,Zeying Sun,Xiaoguang Lei,Hao Dong*

Main category: cs.LG

TL;DR: ChemBART는 화학 반응을 기반으로 한 SMILES 기반의 대형 언어 모델로, 여러 화학 작업을 위한 통합 모델을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 발전을 통해 다양한 화학 문제에 대한 접근 방식을 개선하기 위함입니다.

Method: ChemBART는 화학 반응에 대해 사전 훈련된 SMILES 기반의 성능을 활용하며 여러 다운스트림 화학 작업을 수행할 수 있습니다.

Result: ChemBART는 전구체/시약 생성, 온도-수확량 회귀, 분자 특성 분류 등 다양한 화학 문제를 성공적으로 해결했습니다.

Conclusion: ChemBART는 화학 합성 계획 사이클을 발전시키는 데 넓은 유틸리티를 보여주고 있으며, 실험실 검증을 통해 ~30%의 수확량 향상을 확인했습니다.

Abstract: Recent advances in large language models (LLMs) have demonstrated transformative potential across diverse fields. While LLMs have been applied to molecular simplified molecular input line entry system (SMILES) in computer-aided synthesis planning (CASP), existing methodologies typically address single tasks, such as precursor prediction. We introduce ChemBART, a SMILES-based LLM pre-trained on chemical reactions, which enables a unified model for multiple downstream chemical tasks--achieving the paradigm of "one model, one pre-training, multiple tasks." By leveraging outputs from a mask-filling pre-training task on reaction expressions, ChemBART effectively solves a variety of chemical problems, including precursor/reagent generation, temperature-yield regression, molecular property classification, and optimizing the policy and value functions within a reinforcement learning framework, integrated with Monte Carlo tree search for multi-step synthesis route design. Unlike single-molecule pre-trained LLMs constrained to specific applications, ChemBART addresses broader chemical challenges and integrates them for comprehensive synthesis planning. Crucially, ChemBART-designed multi-step synthesis routes and reaction conditions directly inspired wet-lab validation, which confirmed shorter pathways with ~30% yield improvement over literature benchmarks. Our work validates the power of reaction-focused pre-training and showcases the broad utility of ChemBART in advancing the complete synthesis planning cycle.

</details>


### [9] [When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability](https://arxiv.org/abs/2601.03047)
*Raphael Ronge,Markus Maier,Frederick Eberhardt*

Main category: cs.LG

TL;DR: 이 연구는 기계적 해석 가능성에 대한 Anthropic의 최근 작업을 stress-test 하였고, 주요 결과의 일반화 가능성에 대한 경고를 제기합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 AI 안전성을 위해 인간의 감독을 가능하게 할 수 있는 기계적 해석 가능성을 연구함.

Method: 대체로 Llama 3.1에서 오픈 소스 SAE를 사용하여 주요 결과를 복제함으로써 기계적 해석 가능성 주장에 대한 초기 stress-test를 수행함.

Result: 기본적인 특성 추출 및 조정 기능을 성공적으로 복제했으나, 일반화 가능성에 대한 주요 경고가 제기됨.

Conclusion: SAE 기반 해석 가능성이 일부 사례에서 매력적인 시연을 제공하지만, 현재 방법은 안전 중요 응용에 필요한 체계적인 신뢰성이 부족함을 나타냄. 해석 가능성보다는 모델 출력의 신뢰할 수 있는 예측 및 통제에 중점을 두어야 할 필요성이 제기됨.

Abstract: Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature extraction and steering capabilities, our investigation suggests that major caution is warranted regarding the generalizability of these claims. We find that feature steering exhibits substantial fragility, with sensitivity to layer selection, steering magnitude, and context. We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another. While SAE-based interpretability produces compelling demonstrations in selected cases, current methods often fall short of the systematic reliability required for safety-critical applications. This suggests a necessary shift in focus from prioritizing interpretability of internal representations toward reliable prediction and control of model output. Our work contributes to a more nuanced understanding of what mechanistic interpretability has achieved and highlights fundamental challenges for AI safety that remain unresolved.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [10] [Stigmergic Swarming Agents for Fast Subgraph Isomorphism](https://arxiv.org/abs/2601.02449)
*H. Van Dyke Parunak*

Main category: cs.MA

TL;DR: 이 논문에서는 최대 부분 서브그래프 동형성 문제를 해결하기 위한 ASSIST 알고리즘을 제안하며, 이는 기존의 휴리스틱 방식보다 효율적이다.


<details>
  <summary>Details</summary>
Motivation: 최대 부분 서브그래프 동형성 문제는 두 그래프 간의 가장 큰 공통 서브그래프를 찾는 문제로, NP-완전 문제이며, 이를 해결하기 위한 나쁜 지식이 있는 경우에 비효율적이다.

Method: 본 논문에서는 개미 군집 최적화 접근 방식을 바탕으로 한 ASSIST 알고리즘을 제안하며, 쿼리와 데이터 그래프에서 일치하는 개별 노드를 식별한 후 선형 시간으로 반복적인 서브그래프 검색을 수행한다.

Result: ASSIST는 쿼리 크기와 상관없이 선형 시간 내에 복잡한 조합 문제를 해결할 수 있으며, 데이터 크기에 대해서는 일정한 시간을 요구한다.

Conclusion: 또한 ASSIST는 기존의 휴리스틱들이 해결하는 데 어려움을 겪는 다양한 매칭 문제를 지원하도록 확장할 수 있다.

Abstract: Maximum partial subgraph isomorphism compares two graphs (nodes joined by edges) to find a largest common subgraph. A common use case, for graphs with labeled nodes, seeks to find instances of a \textit{query} graph with $q$ nodes in a (typically larger) \textit{data} graph with $d$ nodes. The problem is NP-complete, and naïve solutions are exponential in $q + d$. The fastest current heuristic has complexity $O(d^2)$. This paper outlines ASSIST (Approximate Swarming Subgraph Isomorphism through Stigmergy), inspired by the ant colony optimization approach to the traveling salesperson. After peering (identifying matching individual nodes in query and data) in time $O(q\cdot log(d))$, the time required for ASSIST's iterative subgraph search, the combinatorially complex part of the problem, is linear in query size and constant in data size. ASSIST can be extended to support matching problems (such as temporally ordered edges, inexact matches, and missing nodes or edges in the data graph) that frustrate other heuristics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [SastBench: A Benchmark for Testing Agentic SAST Triage](https://arxiv.org/abs/2601.02941)
*Jake Feiglin,Guy Dar*

Main category: cs.CR

TL;DR: SAST 도구의 성능을 평가하기 위한 새로운 벤치마크인 SastBench를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: SAST 도구는 소프트웨어의 취약점을 식별하는 데 널리 사용되지만, 많은 거짓 긍정 사례로 인해 수작업 필터링이 필요합니다.

Method: SastBench는 실제 CVE와 필터링된 SAST 도구 발견 결과를 결합하여 SAST 트리아지 에이전트를 평가하는 벤치마크입니다.

Result: 다양한 에이전트를 벤치마크에서 평가하고 그 성능의 비교 분석을 제공합니다.

Conclusion: 이 연구는 향후 개발에 대한 시사점을 논의합니다.

Abstract: SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 이 논문은 자율 에이전트의 행동을 이해하기 위한 새로운 설명 가능한 강화 학습(XRL) 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트가 인간의 기대에 맞춰 행동하는 것을 보장하기 위해 강화 학습 정책을 이해하는 것이 중요하다.

Method: 텍스트 설명을 생성하고 이를 투명한 규칙 집합으로 변환하며, 그 품질을 개선하고 평가하는 XRL 프레임워크를 제안한다. 전문가의 지식 및 자동 술어 생성기를 포함한다.

Result: 제안된 프레임워크는 기존 방법의 한계를 극복하고 특정 작업에서 만족스러운 성능을 달성할 수 있는 투명한 규칙을 생성한다.

Conclusion: 이 프레임워크는 텍스트 설명의 체계적이고 정량적인 평가를 가능하게 하여 XRL 분야에 귀중한 통찰을 제공한다.

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [13] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: 이 논문에서는 복잡한 환경에서 신뢰할 수 있는 장기 상호작용을 지원하기 위한 메모리 시스템인 SimpleMem을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 환경에서의 신뢰할 수 있는 장기 상호작용을 위해 LLM 에이전트가 효율적으로 역사적 경험을 관리할 수 있는 메모리 시스템이 필요합니다.

Method: 세 가지 단계의 파이프라인으로 구성된 메모리 프레임워크를 제안합니다: (1) 	extit{Semantic Structured Compression}는 비구조적 상호작용을 압축된 메모리 유닛으로 변환합니다; (2) 	extit{Recursive Memory Consolidation}은 연관된 유닛을 통합하여 중복성을 줄입니다; (3) 	extit{Adaptive Query-Aware Retrieval}는 쿼리 복잡도에 따라 검색 범위를 조정하여 정확한 맥락을 효율적으로 구성합니다.

Result: 벤치마크 데이터셋에서 우리의 방법이 정확성, 검색 효율성, 추론 비용에서 기준 방법을 지속적으로 초과하며 평균 F1 개선이 26.4%에 달하고, 추론 시 토큰 소비를 최대 30배 감소시킨다는 결과를 보여줍니다.

Conclusion: 성능과 효율성 간의 우수한 균형을 입증합니다.

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [14] [InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents](https://arxiv.org/abs/2601.03204)
*Chenglin Yu,Yuchen Wang,Songmiao Wang,Hongxia Yang,Ming Li*

Main category: cs.AI

TL;DR: InfiAgent는 에이전트의 추론 맥락을 작업 기간과 관계없이 엄격하게 제한하여 안정적인 장기 작업 수행을 가능하게 하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 도구를 사용하고 추론할 수 있지만, 장기 작업에서는 맥락 증대와 누적된 오류로 인해 자주 실패한다.

Method: InfiAgent는 지속 상태를 파일 중심 상태 추상화로 외부화하여 에이전트의 추론 맥락을 엄격히 제한한다. 각 단계에서 에이전트는 작업 공간 상태 스냅샷과 최근 행동의 고정 창을 사용해 맥락을 재구성한다.

Result: DeepResearch 및 80개 논문 문헌 검토 작업에 대한 실험 결과는, 작업 특정 미세 조정 없이도 20B 오픈 소스 모델을 사용한 InfiAgent가 더 큰 독점 시스템과 경쟁력을 가지며, 맥락 중심 기준선보다 상당히 높은 장기 범위를 유지함을 보여준다.

Conclusion: 이 결과는 안정적인 장기 에이전트를 위한 실용적인 기초로서 명시적 상태 외부화를 뒷받침한다.

Abstract: LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent

</details>


### [15] [Orchestral AI: A Framework for Agent Orchestration](https://arxiv.org/abs/2601.02577)
*Alexander Roman,Jacob Roman*

Main category: cs.AI

TL;DR: Orchestral은 LLM 에이전트를 위한 경량 Python 프레임워크로, 주요 제공업체 간의 통합 및 단순성 유지를 지원한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트 프레임워크의 급속한 확산으로 인해 개발자들은 공급업체 특정 SDK를 통한 잠금 또는 복잡한 다중 패키지 생태계 사이에서 선택해야 했다.

Method: Orchestral은 주요 제공업체를 아우르는 통합된 타입 안전 인터페이스와 단일 보편적 표현 방식을 제공하여 API의 단편화, 메시지 형식의 불일치 및 도구 호출 동작의 불일치를 해결한다.

Result: Python 타입 힌트로부터 자동 도구 스키마 생성을 통해 수동 설명자의 필요가 제거되며, 동기 실행 모델과 스트리밍 지원으로 결정론적 행동과 실시간 상호작용이 가능하다.

Conclusion: Orchestral는 모듈형 아키텍처로 전체 시스템의 확장성을 보장하면서도 복잡성을 줄인다.

Abstract: The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.

</details>


### [16] [AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents](https://arxiv.org/abs/2601.02643)
*Mehmet Kurmaz*

Main category: cs.AI

TL;DR: 대화형 에이전트가 구조화된 데이터베이스를 쿼리할 때, 두 가지 문제인 과소 명세와 불가능성을 다루는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대화형 에이전트가 쿼리를 수행할 때 필요한 제약 조건이 누락되거나, 모든 제약 조건을 만족하는 항목이 없는 경우 사용자의 의도를 위반하지 않으면서 이를 해결할 필요가 있다.

Method: 우리는 쿼리가 만족되지 않는 경우, 사용자에게 중요성이 낮은 제약을 완화하도록 에이전트에 요구하는 선호 인식 쿼리 수정 문제로 정의한다. 이를 위해 세 가지 LLM 기반 방법을 제안한다: (1) 지역 가중치, (2) 전역 일회성 가중치, (3) 쌍 비교 순위.

Result: 실험 결과 지역 가중치가 가장 우수한 선호 일치를 달성한 반면, 전역 가중치는 정확한 제약 완화에서 가장 좋은 성능을 보였다.

Conclusion: AWARE-US라는 벤치마크를 도입하여, 에이전트가 대화를 통해 요청을 분명히 하고 사용자의 선호에 일치하도록 불가능성을 해결하도록 요구한다.

Abstract: Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with "no results" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.

</details>


### [17] [Learning User Preferences Through Interaction for Long-Term Collaboration](https://arxiv.org/abs/2601.02702)
*Shuhaib Mehri,Priyanka Kargupta,Tal August,Dilek Hakkani-Tür*

Main category: cs.AI

TL;DR: MultiSessionCollab은 에이전트가 사용자 선호를 학습하고 이를 통해 협업 품질을 개선하는 능력을 평가하는 벤치마크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자와의 협력 경험이 축적됨에 따라 사용자 선호에 적응하는 것은 장기적인 관계를 형성하고 협력 품질을 향상하는 데 필수적입니다.

Method: 기억을 갖춘 장기적 협업 에이전트를 개발하고, MultiSessionCollab의 사용자 시뮬레이터 행동에서 학습 신호를 도출하여 에이전트를 훈련합니다.

Result: 기억을 장착한 에이전트는 장기 협업을 개선하여 작업 성공률, 상호작용 효율성, 사용자 노력을 줄입니다.

Conclusion: 기억은 실제 환경에서 사용자 경험을 향상하는 데 기여합니다.

Abstract: As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.

</details>


### [18] [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714)
*Zhi Liu,Guangzhi Wang*

Main category: cs.AI

TL;DR: 이 논문은 인공지능 패러다임의 수렴과 복잡한 문제 해결을 위한 시간 확장(Time-Scaling)의 필요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 인간의 인지 자원이 제한된 상황에서 복잡한 문제를 해결하는 방법을 이해하고, 인공지능의 깊은 추론 능력을 향상시킬 필요성이 있다.

Method: 시간 확장(Time-Scaling)이라는 아키텍처 디자인을 활용하여 시간에 따라 추론을 펼치는 능력을 체계적으로 확장하고 최적화한다.

Result: DeepSeek-R1과 같은 최근 모델들이 명시적 추론 궤적을 통해 성능을 향상시키지만, 검색 완전성과 효율성에서 제한이 있다.

Conclusion: 인공지능 에이전트의 깊은 추론 및 문제 해결을 향상시키려면 시간 확장 원리를 최우선으로 두어야 한다.

Abstract: Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for "Time-Scaling"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.

</details>


### [19] [The Path Ahead for Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2601.02749)
*Nadia Sibai,Yara Ahmed,Serry Sibaee,Sawsan AlHalawani,Adel Ammar,Wadii Boulila*

Main category: cs.AI

TL;DR: 대규모 언어 모델의 진화가 인공지능의 패러다임 전환을 나타내며, 에이전트 AI 시스템의 출현과 핵심 구성요소를 분석함.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 발전은 자율적이고 목표 지향적인 시스템으로의 전환을 필요로 하며, 이는 복잡한 환경에서 자율적으로 작동할 수 있는 능력을 요구한다.

Method: LLM의 기능이 추론-행동-반성 루프를 통해 에이전시로 확장되는 방법을 종합하고, LLM과 자율 행동을 연결하는 핵심 구성 요소를 설명하는 통합 프레임워크를 제시한다.

Result: LLM의 구조적 발전을 추적하면서 장기적인 추론, 맥락 인식 및 적응적 의사결정 능력을 확인하였고, 에이전트 행동을 가능하게 하는 기능을 도출하였다.

Conclusion: 책임 있는 발전을 위해서는 기술적 강건성, 해석 가능성 및 윤리적 보호 장치에서 동시 발전이 필요하다.

Abstract: The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.

</details>


### [20] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: ChangeGPT는 다양한 실세계 쿼리를 처리할 수 있는 일반 에이전트 프레임워크로, LLM과 비전 기초 모델이 통합되어 있다.


<details>
  <summary>Details</summary>
Motivation: 기존 변경 감지 방법은 다양한 실세계 쿼리를 처리하는 데 있어 유연성이 부족하고, 포괄적인 분석을 위한 지능도 결여되어 있다.

Method: 이 논문은 ChangeGPT로 명명된 일반 에이전트 프레임워크를 제시하며, LLM과 비전 기초 모델을 통합하고, 환각을 완화하기 위해 계층 구조를 사용한다.

Result: ChangeGPT는 GPT-4-turbo 백엔드를 사용하여 90.71 %의 일치율을 달성하며 뛰어난 성능을 나타냈다.

Conclusion: ChangeGPT는 지능, 적응성 및 다중 유형의 변경 분석을 제공하여 원격 탐지 응용 프로그램에서 의사 결정을 위한 강력한 솔루션을 제시한다.

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [21] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: AI 기반의 시스템이 문헌 검토를 향상시켜, 환자 치료에 중요한 정확성을 확보함.


<details>
  <summary>Details</summary>
Motivation: 1.5백만 개 이상의 연간 출판물을 수작업으로 검토하는 것은 불가능하며, 현재 AI 접근법은 체계적 검토 업무에서 환각 문제가 있다.

Method: 명시적 인과 추론과 이중 수준 지식 그래프를 통합한 인과 그래프 향상 검색 보강 생성 시스템을 제안한다.

Result: CausalAgent는 95% 정확도, 100% 검색 성공률, 0% 환각을 달성했다.

Conclusion: 신뢰할 수 있는 의료 AI를 위한 전이 가능한 원칙을 보여준다.

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [22] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: Multi-Agent Debate(MAD)에서의 평가를 표준화하기 위한 M3MAD-Bench가 제안된다.


<details>
  <summary>Details</summary>
Motivation: MAD 방법의 평가가 단편적이고 일관되지 않은 설정에서 이루어져 공정한 비교를 방해하고 있으며, 단일 모달리티 시나리오에 제한되어 있는 문제를 해결하고자 한다.

Method: M3MAD-Bench는 다중 도메인 작업, 다중 모달 입력 및 다차원 메트릭스에 대한 MAD 방법 평가를 위한 통합되고 확장 가능한 벤치마크로, 다섯 가지 핵심 작업 도메인에 대한 표준화된 프로토콜을 수립한다.

Result: 다양한 아키텍처, 규모 및 모달리티 기능을 가진 아홉 개의 기본 모델에 대해 MAD 방법을 평가하며, 정확성 외에도 효율성을 고려한 메트릭을 포함한다.

Conclusion: M3MAD-Bench는 텍스트 전용 및 다중 모달 시나리오에서 MAD의 효과성, 강건성 및 효율성에 대한 체계적인 통찰을 제공하며, 표준화된 MAD 평가를 위한 신뢰할 수 있는 기반을 제공한다.

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [23] [SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection](https://arxiv.org/abs/2601.02871)
*Zhiyong Cao,Dunqiang Liu,Qi Dai,Haojun Xu,Huaiyan Xu,Huan He,Yafei Liu,Siyuan Liu,XiaoLin Lin,Ke Ma,Ruqian Shi,Sijia Yao,Hao Wang,Sicheng Zhou*

Main category: cs.AI

TL;DR: SimRPD는 채용 대화 에이전트를 훈련하기 위한 3단계 프레임워크로, 고충실도 사용자 시뮬레이터와 다차원 평가 프레임워크를 사용하여 대화 데이터를 생성하고 선별하여 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 고품질의 목표 지향적 도메인 전문 학습 데이터의 부족으로 인해 채용 대화 에이전트의 성능이 제한되므로, 이 문제를 해결할 방법을 모색한다.

Method: SimRPD라는 3단계 프레임워크를 제안하며, 사용자 시뮬레이터를 개발하여 대화 데이터를 생성하고, Chain-of-Intention 기반의 평가 프레임워크로 데이터를 선택한다.

Result: 실제 채용 시나리오에서 SimRPD가 기존 시뮬레이터 기반 데이터 선택 전략보다 우수한 성능을 보임을 실험을 통해 입증하였다.

Conclusion: SimRPD는 산업적 배치에 대한 실용적 가치를 지니며, 다른 비즈니스 지향적 대화 시나리오에도 적용 가능성이 있다.

Abstract: Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.

</details>


### [24] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: Batch-of-Thought (BoT)는 관련된 쿼리를 함께 처리하여 크로스 인스턴스 학습을 가능하게 하는 훈련 없는 방법으로, 정확도와 신뢰도 보정을 향상시키고 추론 비용을 최대 61%까지 줄이는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 대형 언어 모델 추론 시스템은 쿼리를 독립적으로 처리하여 공유된 추론 패턴과 일관성 제약과 같은 귀중한 크로스 인스턴스 신호를 버리고 있다.

Method: Batch-of-Thought (BoT) 방법을 제안하며, 관련 쿼리를 함께 처리하여 크로스 인스턴스 학습을 가능하게 한다. BoT는 배치 간 비교 분석을 통해 고품질 추론 템플릿을 식별하고 일관성 검사를 통해 오류를 탐지하며 계산 비용을 절감한다.

Result: BoT를 다중 에이전트 반영 아키텍처(BoT-R) 내에 구현하며, Reflector가 공동 평가를 수행하여 독립 처리에서 사용할 수 없는 상호 정보 이득을 실현한다. 세 가지 모델 계열과 여섯 가지 벤치마크를 통한 실험 결과, BoT-R은 일관되게 정확도와 신뢰도 보정을 향상시키면서 추론 비용을 최대 61%까지 줄인다.

Conclusion: 우리의 이론적 및 실험적 분석을 통해 배치 인식 추론이 LLM 시스템에 대해 언제, 왜 이점을 제공하는지를 밝혀낸다.

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [25] [A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace](https://arxiv.org/abs/2601.03120)
*Adam Keane,Nick Pepper,Chris Burr,Amy Hodgkin,Dewi Gould,John Korna,Marc Thomas*

Main category: cs.AI

TL;DR: 본 논문은 AI 항공교통 관리를 위한 확률적 디지털 트윈 개발과 이를 위한 보증 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 항공 산업에서 디지털 트윈 기술의 잠재적 이점을 최대화하고, AI 항공교통 제어 에이전트의 교육 및 테스트 환경을 제공하기 위해.

Method: 신뢰성과 윤리적 보증(TEA) 방법론을 활용하여 디지털 트윈이 물리적 대응물을 정확하게 나타내고 있는지를 입증하는 보증 케이스를 개발하였다.

Result: 제안된 프레임워크는 디지털 트윈의 강점과 한계를 평가하고 문서화하는 구조적인 접근 방식을 제공한다.

Conclusion: 이 프레임워크는 이해관계자 및 규제 당국과의 소통의 기초를 이루며, 규제 필요성에 대한 논의를 지원하고 디지털 트윈의 구체적인 사례를 통해 새로운 가이드를 기여한다.

Abstract: Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.
  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.
  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.

</details>


### [26] [MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents](https://arxiv.org/abs/2601.03236)
*Dongming Jiang,Yi Li,Guanpeng Li,Bingzhe Li*

Main category: cs.AI

TL;DR: MAGMA는 메모리 항목을 서로 독립적인 그래프에서 표현하여 장기적 추론에서 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존의 메모리 증강 생성 기법은 단일 메모리 저장소에 의존하여 해석 가능성과 증거 간의 정렬을 제한한다.

Method: MAGMA는 기저 그래프를 통해 메모리에 접근하는 정책 기반 탐색을 통해 메모리의 구조화된 맥락을 구성한다.

Result: MAGMA는 LoCoMo와 LongMemEval 실험에서 최신 에이전트 메모리 시스템을 지속적으로 초월하는 성능을 보였다.

Conclusion: MAGMA는 메모리 표현과 검색 논리를 분리하여 투명한 추론 경로와 세부적인 검색 제어를 제공한다.

Abstract: Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.

</details>
