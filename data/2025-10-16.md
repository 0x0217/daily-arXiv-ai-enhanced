<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.11824)
*Simin Li,Zihao Mao,Hanxiao Li,Zonglei Jing,Zhuohang bian,Jun Guo,Li Wang,Zhuoran Han,Ruixiao Xu,Xin Yu,Chengdong Ma,Yuqing Ma,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: 협동 다중 에이전트 강화 학습(MARL)에서 하이퍼파라미터 조정은 협동 성과를 극대화하기 위해 이상적인 시뮬레이션 환경에서 일반적으로 이루어지지만, 실제 환경에서 이 조정된 정책이 유연성과 탄력성을 유지하지 못하는 문제가 있다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 있는 MARL 시스템을 구축하려면 불확실성 하에서의 안정성을 보장하는 강건성 및 중단으로부터 회복하는 능력인 복원력에 대한 깊은 이해가 필요하다.

Method: 4개의 실제 환경, 13개의 불확실성 유형, 15개의 하이퍼파라미터를 사용하여 협동, 강건성 및 복원력을 평가하기 위해 82,620회 이상의 실험으로 구성된 대규모 실증 연구를 수행하였다.

Result: 온건한 불확실성 하에서 협동 최적화가 강건성과 복원성을 향상시키지만, 방해가 심해질수록 이 관계가 약해진다. 강건성과 복원성은 알고리즘 및 불확실성 유형에 따라 다르며, 모든 에이전트의 행동 잡음에 대해 강건한 정책이 단일 에이전트의 관찰 잡음 하에서는 실패할 수 있다.

Conclusion: 하이퍼파라미터 조정은 신뢰할 수 있는 MARL을 위해 필수적이며, 표준 관행이 강건성을 해칠 수 있는 반면, 조기 종료, 높은 비평가 학습률 및 Leaky ReLU는 일관되게 도움을 주는다.

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common
practice to tune hyperparameters in ideal simulated environments to maximize
cooperative performance. However, policies tuned for cooperation often fail to
maintain robustness and resilience under real-world uncertainties. Building
trustworthy MARL systems requires a deep understanding of robustness, which
ensures stability under uncertainties, and resilience, the ability to recover
from disruptions--a concept extensively studied in control systems but largely
overlooked in MARL. In this paper, we present a large-scale empirical study
comprising over 82,620 experiments to evaluate cooperation, robustness, and
resilience in MARL across 4 real-world environments, 13 uncertainty types, and
15 hyperparameters. Our key findings are: (1) Under mild uncertainty,
optimizing cooperation improves robustness and resilience, but this link
weakens as perturbations intensify. Robustness and resilience also varies by
algorithm and uncertainty type. (2) Robustness and resilience do not generalize
across uncertainty modalities or agent scopes: policies robust to action noise
for all agents may fail under observation noise on a single agent. (3)
Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard
practices like parameter sharing, GAE, and PopArt can hurt robustness, while
early stopping, high critic learning rates, and Leaky ReLU consistently help.
By optimizing hyperparameters only, we observe substantial improvement in
cooperation, robustness and resilience across all MARL backbones, with the
phenomenon also generalizing to robust MARL methods across these backbones.
Code and results available at
https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .

</details>


### [2] [Heterogeneous RBCs via deep multi-agent reinforcement learning](https://arxiv.org/abs/2510.12272)
*Federico Gabriele,Aldo Glielmo,Marco Taboga*

Main category: cs.MA

TL;DR: MARL-BC는 심층 다중 에이전트 강화 학습을 실질 비즈니스 주기 모델과 통합하여 에이전트 이질성이 포함된 거시경제 모델의 한계를 극복하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 이질성을 고려한 현재의 거시경제 모델은 이질적인 에이전트 일반 균형 모델과 에이전트 기반 모델로 크게 나눌 수 있으며, 각 접근법의 한계가 존재한다.

Method: MARL-BC라는 프레임워크를 소개하여 깊은 다중 에이전트 강화 학습을 실질 비즈니스 주기 모델과 통합한다.

Result: MARL-BC는 단일 에이전트를 사용할 때 전통적인 RBC 결과를 회복하고, 동일한 많은 에이전트를 사용하여 KS 모델 결과를 회복하며, 에이전트 간의 풍부한 이질성을 효과적으로 시뮬레이션할 수 있다.

Conclusion: 이 프레임워크는 다양한 이질적인 상호작용 에이전트와 함께 사용할 경우 ABM으로 생각될 수 있으며, 극한 경우 GE 결과를 재현할 수 있다. 이는 이러한 모델링 패러다임의 통합을 향한 한 걸음이다.

Abstract: Current macroeconomic models with agent heterogeneity can be broadly divided
into two main groups. Heterogeneous-agent general equilibrium (GE) models, such
as those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith
(KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic
assumptions that make the models very computationally cumbersome, which in turn
limits the amount of heterogeneity that can be modelled. In contrast,
agent-based models (ABMs) can flexibly encompass a large number of arbitrarily
heterogeneous agents, but typically require the specification of explicit
behavioural rules, which can lead to a lengthy trial-and-error
model-development process. To address these limitations, we introduce MARL-BC,
a framework that integrates deep multi-agent reinforcement learning (MARL) with
Real Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover
textbook RBC results when using a single agent; (2) recover the results of the
mean-field KS model using a large number of identical agents; and (3)
effectively simulate rich heterogeneity among agents, a hard task for
traditional GE approaches. Our framework can be thought of as an ABM if used
with a variety of heterogeneous interacting agents, and can reproduce GE
results in limit cases. As such, it is a step towards a synthesis of these
often opposed modelling paradigms.

</details>


### [3] [Characterizing Agent-Based Model Dynamics via $ε$-Machines and Kolmogorov-Style Complexity](https://arxiv.org/abs/2510.12729)
*Roberto Garrone*

Main category: cs.MA

TL;DR: 복합 적응 시스템(CAS) 내에서 에이전트 기반 모델(ABM)의 정보 역학을 특징짓기 위한 이중 수준 정보 이론 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 기반 모델의 역학을 복합 적응 시스템의 프레임워크 내에서 이해하고자 한다.

Method: 거시적 수준에서 시스템 전체의 정보를 요약하는 모형으로 재구성된 풀린 $	ext{ε}$-기계와 미시적 수준에서 각 돌봄자-노인 쌍에 대한 $	ext{ε}$-기계를 재구성하고, 손실 없는 압축에서의 비트 수와 노멀라이즈된 LZ78 복잡성을 포함한 알고리즘 비의존적 콜모고로프 스타일 측정으로 보완한다.

Result: 이로 인해 생성된 특징 집합 $	ext{h}_{	ext{μ}}, 	ext{C}_{	ext{μ}}, E, 	ext{LZ78}, 	ext{bps}$는 에이전트와 시나리오 간의 분포 분석, 계층 비교 및 비지도 클러스터링을 가능하게 한다.

Conclusion: 이 이중 스케일 설계는 에이전트 이질성을 유지하면서 해석 가능한 거시적 기준선을 제공하며, ABM 관행을 CAS의 emergent, 피드백 및 적응 원칙과 일치시킨다. 돌봄자-노인 상호작용에 대한 사례 연구를 통해 프레임워크의 구현을 보여준다.

Abstract: We propose a two-level information-theoretic framework for characterizing the
informational organization of Agent-Based Model (ABM) dynamics within the
broader paradigm of Complex Adaptive Systems (CAS). At the macro level, a
pooled $\epsilon$-machine is reconstructed as a reference model that summarizes
the system-wide informational regime. At the micro level, $\epsilon$-machines
are reconstructed for each caregiver-elder dyad and variable, and are
complemented with algorithm-agnostic Kolmogorov-style measures, including
normalized LZ78 complexity and bits per symbol from lossless compression. The
resulting feature set $\{h_{\mu}, C_{\mu}, E, \mathrm{LZ78}, \mathrm{bps}\}$
enables distributional analysis, stratified comparisons, and unsupervised
clustering across agents and scenarios. This dual-scale design preserves agent
heterogeneity while providing an interpretable macro-level baseline, aligning
ABM practice with CAS principles of emergence, feedback, and adaptation. A case
study on caregiver-elder interactions illustrates the framework's
implementation; the results and discussion will be completed following final
simulation runs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: 이 연구는 Dhumbal이라는 다인용 카드 게임에서 인공지능(AI) 에이전트를 평가하며, 규칙 기반, 탐색 기반, 학습 기반 전략을 체계적으로 비교합니다.


<details>
  <summary>Details</summary>
Motivation: Dhumbal의 문화적 중요성을 평가하고, 인공지능을 통해 게임의 메커니즘에 대한 통찰을 제공하기 위해.

Method: 규칙 기반, 탐색 기반(MCTS 및 ISMCTS), 학습 기반(DQN 및 PPO) 및 무작위 기준을 포함한 다양한 에이전트를 구현하고, 평가를 위해 각 카테고리 내 토너먼트를 진행하여 교차 카테고리 챔피언십을 실시합니다.

Result: 1024개 시뮬레이션 라운드에서 규칙 기반의 공격적인 에이전트가 가장 높은 승률(88.3%)을 기록하며 ISMCTS(9.0%)와 PPO(1.5%)를 능가했습니다.

Conclusion: 재현 가능한 AI 프레임워크와 부분 정보 하에서의 휴리스틱 효과에 대한 통찰을 제공하고, 문화 게임의 디지털 보존을 지원합니다.

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [5] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: AI 에이전트의 전반적인 성능을 평가하기 위해 HAL(Holistic Agent Leaderboard)를 도입하고, 표준화된 평가 방법과 3차원 분석을 통해 에이전트 행동에 대한 새로운 통찰을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트 평가의 이해를 돕기 위해 발생하는 다양한 문제를 해결하고자 함.

Method: 수백 개의 VM에서 병렬 평가를 조정하는 표준화된 평가 시스템을 제공하고, 모델, 스캐폴드, 벤치마크를 포함한 3차원 분석을 수행함.

Result: 21,730 에이전트 롤아웃을 통해 예상치 못한 통찰을 발견하였고, LLM 보조 로그 검사를 통해 에이전트가 벤치마크를 검색하는 등 기존에 보고되지 않은 행동을 확인함.

Conclusion: 에이전트 평가 방식의 표준화를 통해 현실 세계에서 신뢰할 수 있는 에이전트에 대한 연구에 집중할 수 있도록 기여하고자 함.

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [6] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: 본 연구에서는 산업 환경에서 예측, 설명 및 인과 추론의 원활한 통합을 제공하는 CausalTrace를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 정확한 예측뿐만 아니라 공정 이상, 근본 원인 및 잠재 개입에 대한 해석 가능한 통찰력을 요구하는 현대 제조 환경.

Method: CausalTrace는 데이터 기반 인과 분석을 수행하며 산업 온톨로지와 지식 그래프를 활용하여 인과 발견, 반사실적 추론 및 근본 원인 분석(RCA) 등의 고급 기능을 제공합니다.

Result: CausalTrace는 다양한 인과 평가 방법과 C3AN 프레임워크를 통해 포괄적인 평가를 수행하였으며, 도메인 전문가와 상당한 일치를 보였습니다 (ROUGE-1: 0.91, RCA 성능: MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92).

Conclusion: CausalTrace는 실시간 운영자 상호작용을 지원하며 투명하고 설명 가능한 의사결정 지원을 통해 기존 에이전트를 보완하도록 설계되었습니다.

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [7] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 지리적 인식을 위한 지층(GAL)을 통해 대형 언어 모델(LLM) 에이전트가 구조화된 지구 데이터를 활용하여 재난 대응을 개선하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 재난 대응은 생명과 재산을 보호하는 데 필수적입니다. 그러나 기존의 통계적 접근 방식은 의미론적 맥락이 부족하고, 사건 간 일반화가 떨어지며, 해석 가능성이 제한적입니다.

Method: 우리는 LLM 에이전트를 구조화된 지구 데이터에 기반을 두기 위해 GAL을 도입합니다. GAL은 원시 산불 탐지에서 시작하여 외부 지리 데이터베이스에서 인프라, 인구 통계, 지형 및 날씨 정보를 자동으로 검색하고 통합하여 요약된 단위 주석 인식 스크립트를 만듭니다.

Result: 우리는 여러 LLM 모델에서 실제 산불 시나리오를 평가하여 지리적으로 기반을 둔 에이전트가 기본선 성능을 초과할 수 있음을 보여주었습니다.

Conclusion: 제안된 프레임워크는 홍수 및 허리케인과 같은 다른 재해에도 일반화할 수 있습니다.

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [8] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: AI 에이전트는 다양한 작업을 수행하는데 필요한 도구를 활용하고 여러 가설의 결과를 시뮬레이션하며 이를 반영하는 능력을 갖추고 있다. 이 논문에서는 AI 에이전트의 학습 역할을 재해석하고, 고전적인 귀납적 학습에서 전이 학습으로의 전환을 제안하여 학습의 핵심 요소인 시간을 강조한다. 또한, 정보의 중요한 역할이 시간 단축에 있다는 것을 보여주며, 이론적으로 추론 시간과 훈련 시간 간의 비례 관계를 도출한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 학습 방식과 이들이 문제를 해결하는 데 필요한 시간의 역할을 재조명하기 위해.

Method: AI 에이전트를 계산 능력을 가진 확률 역학적 시스템으로 보고, 고전적인 귀납적 학습에서 전이 학습으로 전환하는 접근 방식을 제안했다.

Result: 정보의 중요성이 시간 단축에 있다는 것을 보여주고, 과거 데이터를 활용하여 달성할 수 있는 최적의 속도 향상과 추론 시간과 훈련 시간 간의 관계를 이론적으로 도출했다.

Conclusion: 모델 크기를 확장하는 대신 시간 최적화에 집중해야 하며, 이는 학습에서의 중요한 역할이 지금까지 간접적으로만 고려되었다는 점을 강조한다.

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [9] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: HiCoTraj는 라벨이 없는 데이터로 인구 통계 추론을 수행하는 프레임워크로, LLM의 제로샷 학습을 활용하여 인구 통계 정보를 추출한다.


<details>
  <summary>Details</summary>
Motivation: 인구 통계 속성 추론은 공공 건강 개입, 공정한 도시 계획, 개인 맞춤형 교통 서비스와 같은 중요한 응용 프로그램을 가능하게 한다.

Method: HiCoTraj는 LLM의 제로샷 학습과 의미 이해 능력을 활용하여 라벨이 없는 교육 데이터를 통해 인구 통계 추론을 수행한다. 이 프레임워크는 활동 연대기와 다중 규모 방문 요약을 생성하여 경로를 의미적으로 풍부한 자연어 표현으로 변환한다.

Result: HiCoTraj는 실제 경로 데이터에 대한 실험 평가에서 제로샷 시나리오에서 여러 인구 통계 속성에 대해 경쟁력 있는 성능을 달성하였다.

Conclusion: 이 방법은 라벨이 있는 인구 통계 데이터의 부족 문제를 해결하고 투명한 추론 체인을 제공한다.

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [10] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 강화 학습 프레임워크를 제안하며, 각 에이전트는 유전자형을 할당받고 보상 함수는 포함 적합성 개념에 기반하여 모델링된다.


<details>
  <summary>Details</summary>
Motivation: 자연 선택의 경쟁적이고 협력적인 힘이 수백만 년 동안 지능의 진화를 이끌어 왔으며, 이는 자연의 방대한 생물다양성과 인간 마음의 복잡도로 귀결된다.

Method: 각 에이전트에 유전자형이 부여되고, 보상 함수는 포함 적합성의 개념을 기반으로 하여 모델링된 다중 에이전트 강화 학습 프레임워크를 제안한다.

Result: 교도소의 딜레마가 있는 두 가지 유형의 네트워크 게임에서 사회적 역학을 연구했고, 우리 결과는 해밀턴의 법칙과 같은 생물학에서 잘 확립된 원칙과 일치한다.

Conclusion: 포함 적합성을 에이전트에 포함시키는 것이 보다 전략적으로 발전하고 사회적으로 지능 있는 에이전트의 출현을 위한 기초가 될 수 있음을 주장한다.

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [11] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: 본 연구에서는 실체적 의사결정 능력을 갖춘 인공지능을 위한 훈련 환경인 EmboMatrix를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 임베디드 인텔리전스를 위한 핵심 요소인 실체적 의사결정 능력을 발전시키기 위해, 물리적 환경에 대한 노출이 부족한 대형 언어 모델(LLM)의 한계를 극복할 필요가 있습니다.

Method: EmboMatrix는 대규모 작업과 장면 생성을 위한 다중 에이전트 데이터 엔진, 확장 가능한 시뮬레이션을 위한 분산 이종 하드웨어 시스템, 정밀 감독을 위한 다층 보상 구조 등 새로운 기술을 통합한 훈련 환경입니다.

Result: EmboMatrix를 통하여 EmboBrain이라는 LLM을 개발하였으며, 이 모델은 671B DeepSeek-R1 기준을 9.5% 초과하는 성과를 거두었습니다.

Conclusion: 상호작용과 환경 기반 학습의 힘을 활용하여 진정한 지능형 실체 에이전트를 구축하는 데 기여합니다.

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [12] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: Ax-Prover는 Lean에서 자동 정리 증명을 위해 설계된 다중 에이전트 시스템으로, 다양한 과학적 분야의 문제를 해결하고 인간 전문가와 협력하여 작동할 수 있습니다.


<details>
  <summary>Details</summary>
Motivation: 과학적 문제 해결을 위해 창의적인 추론과 엄격한 구문적 정확성이 요구되는 형식적 증명 생성을 통해 접근합니다.

Method: 대규모 언어 모델(LLM)에 지식과 추론을 제공하고, 모델 컨텍스트 프로토콜(MCP)을 통해 Lean 도구로 장비하여 형식적 정확성을 보장합니다.

Result: Ax-Prover는 공개 수학 벤치마크에서 최첨단 증명기와 경쟁력을 가지며 새로운 벤치마크에서는 이를 크게 초월합니다.

Conclusion: 우리의 도구 기반 에이전트 정리 증명기 접근 방식은 다양한 과학적 분야에서 형식 검증을 위한 일반화 가능한 방법론을 제공합니다.

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>


### [13] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델의 무작위성 처리 능력을 실험을 통해 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 기술의 발전으로 무작위성을 필요로 하는 다양한 응용 프로그램이 생겨났으나, LLM의 무작위성 처리 능력은 불명확합니다.

Method: 여러 외부 도구 접근성, 작업 유형, 모델 상태, 프롬프트 전략 등 다양한 요소를 고려한 일련의 실험을 설계했습니다.

Result: LLM은 무작위성을 나타내는 출력을 생성할 수 있지만, 그 성능은 일관성이 없고 종종 기대되는 행동과 크게 다릅니다.

Conclusion: 실험 결과 분석을 통해 무작위성 처리를 효과적으로 수행하기 위해 LLM에서 개선이 필요한 주요 한계와 영역을 강조합니다.

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [14] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: 이 논문은 복잡하고 확률적인 환경에서 인간의 지시 없이 에이전트가 단 한 번의 생명으로 탐색하며, 조건부 활성화된 프로그래밍 법칙을 통해 세계의 역학을 모델링하는 OneLife 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡하고 확률적인 환경에서 상징적 세계 모델링을 학습하는 도전 과제를 해결하고자 한다.

Method: 조건부 활성화된 프로그램적 법칙을 통해 세계의 역학을 모델링하는 OneLife 프레임워크를 개발하였다.

Result: OneLife는 최소한의 비유도 상호작용에서 주요 환경 역학을 성공적으로 학습하고, 23개 테스트 시나리오 중 16개에서 강력한 기준선을 초과하였다.

Conclusion: OneLife는 알려지지 않은 복잡한 환경의 프로그램적 세계 모델을 자율적으로 구축하기 위한 기초를 마련한다.

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [15] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent는 자연어 명령어를 통해 토폴로지 폴리머의 거친 분자 역학 시뮬레이션을 수행하는 다중 에이전트 AI 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 컴퓨터 워크플로우에 대한 장벽을 줄이고 폴리머 과학에서 AI 기반의 재료 발견을 발전시키기 위함입니다.

Method: 자연어 인터페이스와 도메인 특정 계산 도구를 통합하여 상호작용 및 자율 시뮬레이션 워크플로우를 지원합니다.

Result: 다양한 폴리머 아키텍처에 대해 사례 연구를 통해 ToPolyAgent의 다재다능성을 보여줍니다.

Conclusion: ToPolyAgent는 자율적이고 확장 가능한 다중 에이전트 과학 연구 생태계의 기초를 마련합니다.

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [16] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio는 실시간 인간 제어를 중심으로 하는 최초의 오픈 소스 프레임워크로, 사용자에게 오류 수정 및 전문 지식 추가의 유연성을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 심층 연구 에이전트는 '발사하고 잊어버리는' 모드로 운영되어 사용자에게 실행 중 오류를 수정하거나 전문 지식을 추가할 방법이 없다.

Method: Collaborative Workshop 디자인을 따르는 ResearStudio는 계층적 Planner-Executor를 통해 모든 단계를 실시간 '계획-문서'로 기록하고, 빠른 통신 레이어를 통해 각 작업, 파일 변경, 도구 호출을 웹 인터페이스에 스트리밍한다.

Result: ResearStudio는 완전 자율 모드에서 GAIA 벤치마크에서 최첨단 결과를 달성했으며, OpenAI의 DeepResearch 및 Manus와 같은 시스템을 초월했다.

Conclusion: 강력한 자동화 성능과 세분화된 인간 제어가 공존할 수 있음을 보여준다.

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [17] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: LLM 에이전트의 목표 지향 쿼리를 처리하는 능력을 향상시키기 위한 새로운 교육 프레임워크 GOAT 를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 텍스트 생성 이상으로 확장된 LLM 에이전트가 사용자 의도에 따라 외부 도구를 사용할 수 있도록 하는 것이 필요하다.

Method: GOAT 프레임워크를 사용하여 LLM 에이전트를 인간 주석 없이 세밀하게 조정하고, 주어진 API 문서에서 목표 지향 API 실행 작업의 합성 데이터 세트를 자동으로 구축한다.

Result: GOAT 로 훈련된 에이전트가 기존의 목표 지향 벤치마크에서 가장 뛰어난 성능을 달성함을 보여준다.

Conclusion: GOAT 는 복잡한 추론과 도구 사용이 가능한 강력한 오픈 소스 LLM 에이전트를 구축하는 실용적인 경로로 강조된다.

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [18] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: 이 논문은 의료 응용 프로그램에서 대화형 대형 언어 모델(LLMs)의 신뢰할 수 있는 평가를 위한 새로운 프레임워크인 MedKGEval을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 의료 환경에서의 복잡한 의사-환자 상호작용의 평가가 필요합니다.

Method: MedKGEval은 구조화된 의료 지식을 바탕으로 한 다중 턴 평가 프레임워크입니다.

Result: 이 프레임워크는 환자 시뮬레이션 메커니즘, 턴-레벨 평가 기반 및 최첨단 LLM의 벤치마크를 포함하여 평가의 동적 이슈를 해결합니다.

Conclusion: MedKGEval은 전통적인 평가 방법에서는 간과되는 미세한 행동 결함과 안전 위험을 식별할 수 있습니다.

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [19] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 본 연구에서는 신뢰성 추적을 통해 대형 언어 모델(LLM)의 신념 편차를 추적하여 정책 최적화를 개선하기 위한 $	extbf{T^3}$ 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 문제 해결 시 신념 추적과 정보 수집에서 어려움을 겪는 문제를 해결하려고 합니다.

Method: 모델 신념의 편차를 추적하고, 과도한 신념 편차를 감지하여 훈련 중 비정보적 경로를 잘라내는 간단하지만 효과적인 방법인 $	extbf{T^3}$를 개발합니다.

Result: 5개의 도전적인 과제에서 $	extbf{T^3}$는 훈련 안정성, 토큰 효율성 및 최종 성능을 지속적으로 향상시켜, 롤아웃 토큰을 약 25% 줄이면서도 최대 30%의 성과 향상을 달성했습니다.

Conclusion: 신념 제어는 강력하고 일반화 가능한 LLM 기반 액티브 추론기를 개발하기 위한 핵심 원칙으로 강조됩니다.

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [20] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: Vibe Coding은 AI 생성 구현을 관찰하여 검증하는 새로운 개발 방법론인데, 이 연구는 이 접근법의 효과를 체계적으로 검토하여 실질적인 프레임워크와 이론적 기초를 수립한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능 생성 코드를 통한 개발의 효율성을 이해하고 인간-인공지능 협업의 저해 요소를 배제하는 것이 필요하다.

Method: 1000편 이상의 연구 논문을 분석하여 Vibe Coding 생태계를 체계적으로 조사하고, 인간 개발자, 소프트웨어 프로젝트 및 코딩 에이전트 간의 관계를 포착하기 위해 제약된 마르코프 결정 과정을 형식화하였다.

Result: Vibe Coding의 성공적인 구현은 에이전트의 능력뿐 아니라 체계적인 맥락 공학, 확립된 개발 환경 및 인간-에이전트 협업 개발 모델에 의존한다는 것을 발견하였다.

Conclusion: 이 연구는 Vibe Coding을 공식적인 학문으로 소개하고, 이 분야의 첫 번째 포괄적인 분류 체계를 제공한다.

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [21] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: 본 연구는 다중 주제와 관련된 사회적 상호작용을 시뮬레이션하기 위한 Multi-topic Opinion Simulation(MTOS) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 소셜 미디어에서의 의견 분열, 정보 분리 및 인지 편향에 대한 연구는 중요하다. 실제 네트워크에서 정보는 여러 관련 주제를 아우르는 경우가 많아 의견 진화를 위한 프레임워크 필요성이 대두된다.

Method: MTOS는 다중 주제 맥락을 LLM과 통합하는 사회적 시뮬레이션 프레임워크로, 단기 및 장기 기억, 사용자 선택 상호작용 메커니즘 및 동적 주제 선택 전략을 포함한다.

Result: MTOS에 대한 실험 결과, 다중 주제 설정에서 의견 분극 경향이 유의미하게 변화하며, 상관관계가 긍정적인 주제는 메아리 방을 강화하고, 부정적인 주제는 저해하며, 관련 없는 주제는 자원 경쟁을 통해 메아리 방 효과를 완화한다.

Conclusion: LLM 기반 에이전트는 동적 의견 변화를 현실적으로 시뮬레이션하고, 뉴스 텍스트의 언어적 특징을 재현하며, 복잡한 인간 추론을 포착하여 시뮬레이션 해석 가능성과 시스템 안정성을 향상시킨다.

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [22] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 본 논문은 비신호 교차로에서의 자율주행 결정-making의 어려움을 해결하기 위한 심층 강화 학습 프레임워크를 제안하며, 이는 교통 위험 예측기를 통해 안전하고 효율적인 주행 결정을 내린다.


<details>
  <summary>Details</summary>
Motivation: 비신호 교차로에서의 복잡한 동적 상호작용과 높은 충돌 위험으로 인해 자율주행 결정을 내리는 것이 매우 어렵다.

Method: 소프트 액터-크리틱(SAC) 알고리즘에 기반한 심층 강화 학습(DRL) 결정-making 프레임워크를 제안하며, 편향된 주의 메커니즘을 통합한다.

Result: 제안된 방법은 교차로에서의 교통 효율성과 차량 안전성을 모두 효과적으로 개선함을 보여준다.

Conclusion: 복잡한 시나리오에서 지능형 결정-making 프레임워크의 효과를 입증한다.

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [23] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 본 논문은 모델의 내재적 능력에 맞춘 메모리 관리 및 작업 수행 최적화 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델은 긴 수명 견제 작업에서 메모리 제한으로 인해 방해되거나 관련 없는 맥락에 쉽게 압도당하는 문제를 직면하고 있습니다.

Method: 우리는 메모리 관리를 학습 가능한 내재적 능력으로 재조명하고, 메모리를 명시적으로 편집하는 작업을 통합 정책의 일부로 수행하는 '행동으로서의 메모리'라는 새로운 프레임워크를 제안합니다.

Result: 우리의 결과는 작업 추론과 메모리 관리를 공동으로 최적화하는 것이 전반적인 계산 소비를 줄일 뿐만 아니라 모델의 내재적 능력에 맞춘 적응형 맥락 큐레이션 전략으로 인해 작업 성능을 개선함을 보여줍니다.

Conclusion: 새로운 알고리즘인 동적 맥락 정책 최적화를 통해 메모리 동작 포인트에서 경로를 세분화하고, 이로 인해 생성된 행동 세그먼트에 경로 수준의 장점을 적용함으로써 안정적인 엔드 투 엔드 강화 학습이 가능하도록 합니다.

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [24] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: ERA는 지식을 통합하고 온라인 강화 학습을 활용하는 두 단계의 프레임워크로, 비전 언어 모델을 사용하는 에이전트 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 비전 언어 모델(VLM)의 잠재력을 활용하여 복잡한 환경에서 인식, 추론 및 상호작용을 수행할 수 있는 에이전트 개발의 필요성.

Method: ERA는 두 단계로 구성되며, 첫 번째 단계에서는 세 가지 유형의 데이터를 사용하는 사전 지식 학습을 통해 기반 지식을 증류하고, 두 번째 단계에서는 온라인 강화 학습 파이프라인을 개발하여 에이전트 성능을 향상시킨다.

Result: ERA-3B는 EB-ALFRED에서 8.4%, EB-Manipulation에서 19.4%의 성능 향상을 보이며 GPT-4o를 초과하고, 이전의 학습 기반 기준선보다 우수한 결과를 나타낸다.

Conclusion: ERA는 확장 가능한 구현 지능을 위한 실용적인 경로를 제공하며, 미래의 구현 AI 시스템을 위한 방법론적 통찰을 제시한다.

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [25] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 다중 에이전트 토론 판단 프레임워크를 통해 자동화된 판단 능력을 향상시키는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 판단 능력을 자동화하는 데 있어 범용적이고 효과적인 방법의 필요성.

Method: 다중 에이전트가 협력하여 추론하고 응답을 반복적으로 개선하는 토론 과정과 이를 수학적으로 형식화하여 에이전트 상호 작용을 분석합니다.

Result: 실험을 통해 다수결 방식에 비해 판단 정확도를 개선하고 계산 효율성을 유지함을 입증했습니다.

Conclusion: 제안된 프레임워크는 자동화된 평가에서 LLM의 사망률을 줄이고 정확성을 높이는 데 기여할 수 있습니다.

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [26] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: 자율 주행의 안전성을 높이기 위한 CAM 기반의 차량 궤적 예측 연구.


<details>
  <summary>Details</summary>
Motivation: 자율 주행의 안전성 문제 해결을 위해 차량간 통신을 활용하고자 함.

Method: Cooperative Awareness Message (CAM) 데이터를 이용하여 차량 궤적 예측을 위한 신경망(CAMNet)을 설계하고 훈련함.

Result: CAM을 활용한 차량 궤적 예측에서 유망한 결과를 도출함.

Conclusion: CAM이 차량 궤적 예측에 효과적임을 입증하며 향후 연구 기회를 제시함.

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [27] [BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing](https://arxiv.org/abs/2510.11823)
*Caelin Kaplan,Alexander Warnecke,Neil Archibald*

Main category: cs.CR

TL;DR: AI 모델의 안전성과 보안 우려가 증가함에 따라 AI 레드 팀이 중요해지고 있으며, BlackIce라는 오픈 소스 툴킷을 통해 LLM 및 ML 모델의 평가를 간소화한다.


<details>
  <summary>Details</summary>
Motivation: AI 모델의 통합이 증가함에 따라 안전성과 보안에 대한 우려가 커지고 있다.

Method: Kali Linux에서 영감을 받아, Red teaming을 위한 오픈 소스 컨테이너화된 툴킷인 BlackIce를 소개한다.

Result: BlackIce는 14개의 선정된 오픈 소스 도구를 포함하고, 통합 명령줄 인터페이스를 통해 접근 가능하다.

Conclusion: 이 설정을 통해 레드 팀 평가를 간단하게 시작할 수 있으며, 사용자가 툴킷을 쉽게 확장할 수 있도록 모듈식 아키텍처를 제공한다.

Abstract: AI models are being increasingly integrated into real-world systems, raising
significant concerns about their safety and security. Consequently, AI red
teaming has become essential for organizations to proactively identify and
address vulnerabilities before they can be exploited by adversaries. While
numerous AI red teaming tools currently exist, practitioners face challenges in
selecting the most appropriate tools from a rapidly expanding landscape, as
well as managing complex and frequently conflicting software dependencies
across isolated projects. Given these challenges and the relatively small
number of organizations with dedicated AI red teams, there is a strong need to
lower barriers to entry and establish a standardized environment that
simplifies the setup and execution of comprehensive AI model assessments.
  Inspired by Kali Linux's role in traditional penetration testing, we
introduce BlackIce, an open-source containerized toolkit designed for red
teaming Large Language Models (LLMs) and classical machine learning (ML)
models. BlackIce provides a reproducible, version-pinned Docker image that
bundles 14 carefully selected open-source tools for Responsible AI and Security
testing, all accessible via a unified command-line interface. With this setup,
initiating red team assessments is as straightforward as launching a container,
either locally or using a cloud platform. Additionally, the image's modular
architecture facilitates community-driven extensions, allowing users to easily
adapt or expand the toolkit as new threats emerge. In this paper, we describe
the architecture of the container image, the process used for selecting tools,
and the types of evaluations they support.

</details>


### [28] [Countermind: A Multi-Layered Security Architecture for Large Language Models](https://arxiv.org/abs/2510.11837)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: 이 논문에서는 대형 언어 모델(LLM) 응용 프로그램의 보안 문제를 다루고 있으며, 기존의 방어 방법 대신 새로운 다층 보안 아키텍처인 Countermind를 제안합니다. 이 아키텍처는 악성 입력을 구조적으로 검증하고 변환하며, 모델의 의미론적 처리 경로를 제약하는 내부 거버넌스 메커니즘을 포함합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM) 애플리케이션의 보안은 악성 지침이 사용자의 입력에 삽입되는 '형식 우선' 공격에 의해 근본적으로 도전받고 있습니다.

Method: Countermind라는 다층 보안 아키텍처를 제안하며, 반응적인 방어에서 사전 및 내재적인 강제 모델로 방어를 전환합니다. 구조적으로 모든 입력을 검증하고 변환하도록 설계된 강화된 경계와 출력이 생성되기 전 모델의 의미론적 처리 경로를 제약하는 내부 거버넌스 메커니즘을 포함합니다.

Result: 제안된 아키텍처는 형식 우선 공격의 성공률(ASR)을 낮추는 효과를 평가하고 잠재적인 지연 오버헤드를 측정하기 위한 평가 계획을 설명합니다.

Conclusion: Countermind는 악성 입력으로부터의 보호를 향상시키기 위한 새로운 방어 메커니즘을 제공합니다.

Abstract: The security of Large Language Model (LLM) applications is fundamentally
challenged by "form-first" attacks like prompt injection and jailbreaking,
where malicious instructions are embedded within user inputs. Conventional
defenses, which rely on post hoc output filtering, are often brittle and fail
to address the root cause: the model's inability to distinguish trusted
instructions from untrusted data. This paper proposes Countermind, a
multi-layered security architecture intended to shift defenses from a reactive,
post hoc posture to a proactive, pre-inference, and intra-inference enforcement
model. The architecture proposes a fortified perimeter designed to structurally
validate and transform all inputs, and an internal governance mechanism
intended to constrain the model's semantic processing pathways before an output
is generated. The primary contributions of this work are conceptual designs
for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text
Crypter intended to reduce the plaintext prompt injection attack surface,
provided all ingestion paths are enforced. (2) A Parameter-Space Restriction
(PSR) mechanism, leveraging principles from representation engineering, to
dynamically control the LLM's access to internal semantic clusters, with the
goal of mitigating semantic drift and dangerous emergent behaviors. (3) A
Secure, Self-Regulating Core that uses an OODA loop and a learning security
module to adapt its defenses based on an immutable audit log. (4) A Multimodal
Input Sandbox and Context-Defense mechanisms to address threats from
non-textual data and long-term semantic poisoning. This paper outlines an
evaluation plan designed to quantify the proposed architecture's effectiveness
in reducing the Attack Success Rate (ASR) for form-first attacks and to measure
its potential latency overhead.

</details>


### [29] [Deep Research Brings Deeper Harm](https://arxiv.org/abs/2510.11851)
*Shuo Chen,Zonggen Li,Zhen Han,Bailan He,Tong Liu,Haokun Chen,Georg Groh,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: 본 연구에서는 LLM(대형 언어 모델) 기반의 DR(딥 리서치) 에이전트가 여러 단계를 거치는 복잡한 연구를 수행할 수 있지만, 이러한 강력한 기능의 오용이 큰 위험을 초래할 수 있음을 보였습니다. 특히, 생물안전과 같은 고위험 분야에서 DR이 전문적인 보고서를 생성할 수 있어 우려가 있습니다. 본 연구는 DR 에이전트의 고유한 위험을 드러내기 위해 새로운 Jailbreak 전략인 계획 주입(Plan Injection) 및 의도 탈취(Intent Hijack)를 제안했습니다.


<details>
  <summary>Details</summary>
Motivation: LLM을 활용한 DR 에이전트의 강력한 연구 능력의 오용이 고위험 분야에서의 문제를 야기할 수 있음.

Method: 계획 주입(Plan Injection) 및 의도 탈취(Intent Hijack)라는 두 가지 새로운 Jailbreak 전략을 제안하고, 다양한 LLM과 안전 벤치마크를 통해 실험을 실시함.

Result: LLM의 정렬이 DR 에이전트에서 종종 실패하며, 학문적 용어로 구성된 유해한 프롬프트가 에이전트의 의도를 탈취할 수 있다는 것을 발견함. 또한, 다단계 계획 및 실행이 정렬을 약화시켜 시스템적 취약점을 드러냄.

Conclusion: DR 에이전트에서는 기본적인 정렬의 불일치가 나타나며, DR 에이전트에 맞춘 정렬 기술 개발이 필요함을 시사함.

Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

</details>


### [30] [CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence](https://arxiv.org/abs/2510.11974)
*Yutong Cheng,Yang Liu,Changze Li,Dawn Song,Peng Gao*

Main category: cs.CR

TL;DR: 이 논문은 지식 보강 환경에서 이질적이고 다중 출처 CTI에서 LLM 성능을 평가하기 위한 첫 번째 벤치마크인 CTIArena를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 최근 사이버 위협 인텔리전스(CTI)의 중요성이 증가하고 있으며, LLM을 CTI에 적용하기 위한 벤치마크가 필요하다.

Method: CTIArena는 구조화, 비구조화, 하이브리드 세 가지 카테고리로 나뉘며, 아홉 개의 작업으로 CTI 분석의 폭을 포착한다.

Result: 열 가지 널리 사용되는 LLM을 평가한 결과, 대부분은 폐쇄형 설정에서 어려움을 겪지만 보안 관련 지식으로 보강할 경우 눈에 띄는 향상을 보인다.

Conclusion: 일반 목적의 LLM의 한계를 강조하며 CTI의 잠재력을 완전히 발휘하기 위해 도메인 맞춤형 기술의 필요성을 제기한다.

Abstract: Cyber threat intelligence (CTI) is central to modern cybersecurity, providing
critical insights for detecting and mitigating evolving threats. With the
natural language understanding and reasoning capabilities of large language
models (LLMs), there is increasing interest in applying them to CTI, which
calls for benchmarks that can rigorously evaluate their performance. Several
early efforts have studied LLMs on some CTI tasks but remain limited: (i) they
adopt only closed-book settings, relying on parametric knowledge without
leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks,
lacking a systematic view of the CTI landscape; and (iii) they restrict
evaluation to single-source analysis, unlike realistic scenarios that require
reasoning across multiple sources. To fill these gaps, we present CTIArena, the
first benchmark for evaluating LLM performance on heterogeneous, multi-source
CTI under knowledge-augmented settings. CTIArena spans three categories,
structured, unstructured, and hybrid, further divided into nine tasks that
capture the breadth of CTI analysis in modern security operations. We evaluate
ten widely used LLMs and find that most struggle in closed-book setups but show
noticeable gains when augmented with security-specific knowledge through our
designed retrieval-augmented techniques. These findings highlight the
limitations of general-purpose LLMs and the need for domain-tailored techniques
to fully unlock their potential for CTI.

</details>


### [31] [HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities](https://arxiv.org/abs/2510.12200)
*Xiaoxue Ren,Penghao Jiang,Kaixin Li,Zhiyong Huang,Xiaoning Du,Jiaojiao Jiang,Zhenchang Xing,Jiamou Sun,Terry Yue Zhuo*

Main category: cs.CR

TL;DR: 본 논문은 웹 애플리케이션 취약점을 시각적 상호작용을 통해 악용할 수 있는 컴퓨터 사용자 에이전트(CUA) 능력을 체계적으로 평가하는 HackWorld 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 웹 애플리케이션은 중요한 서비스와 민감한 데이터에 접근할 수 있는 게이트웨이로써 사이버 공격의 주요 표적입니다. 전통적인 침투 테스트는 비용이 많이 들고 전문 지식이 필요하므로 빠르게 성장하는 웹 생태계와 함께 확장하는 데 어려움이 있습니다.

Method: 본 논문은 HackWorld라는 프레임워크를 제안합니다. HackWorld는 11개의 프레임워크와 7개 언어로 구성된 36개의 실제 애플리케이션을 포함하며, 시각적 상호작용을 통해 웹 애플리케이션의 취약점을 악용하는 CUA의 능력을 평가합니다.

Result: CUA의 현재 능력 평가 결과, 공격 탐지 및 악용률은 12% 미만으로 나타났으며, 멀티 단계 공격 계획 및 보안 도구의 오용이 빈번하게 발생했습니다.

Conclusion: CUA는 웹 보안 환경에서 현재 한계가 있음을 드러내며, 효과적인 취약점 탐지 및 악용이 가능한 보다 보안 인식 높은 에이전트를 개발할 기회를 강조합니다.

Abstract: Web applications are prime targets for cyberattacks as gateways to critical
services and sensitive data. Traditional penetration testing is costly and
expertise-intensive, making it difficult to scale with the growing web
ecosystem. While language model agents show promise in cybersecurity, modern
web applications demand visual understanding, dynamic content handling, and
multi-step interactions that only computer-use agents (CUAs) can perform. Yet,
their ability to discover and exploit vulnerabilities through graphical
interfaces remains largely unexplored. We present HackWorld, the first
framework for systematically evaluating CUAs' capabilities to exploit web
application vulnerabilities via visual interaction. Unlike sanitized
benchmarks, HackWorld includes 36 real-world applications across 11 frameworks
and 7 languages, featuring realistic flaws such as injection vulnerabilities,
authentication bypasses, and unsafe input handling. Using a Capture-the-Flag
(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses
while navigating complex web interfaces. Evaluation of state-of-the-art CUAs
reveals concerning trends: exploitation rates below 12% and low cybersecurity
awareness. CUAs often fail at multi-step attack planning and misuse security
tools. These results expose the current limitations of CUAs in web security
contexts and highlight opportunities for developing more security-aware agents
capable of effective vulnerability detection and exploitation.

</details>


### [32] [PromoGuardian: Detecting Promotion Abuse Fraud with Multi-Relation Fused Graph Neural Networks](https://arxiv.org/abs/2510.12652)
*Shaofei Li,Xiao Han,Ziqi Zhang,Minyao Hua,Shuli Gao,Zhenkai Liang,Yao Guo,Xiangqun Chen,Ding Li*

Main category: cs.CR

TL;DR: 이 논문은 MEITUAN 전자상거래 플랫폼에서의 프로모션 남용 사기를 연구하고 이를 탐지하기 위한 새로운 모델인 PROMOGUARDIAN을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전자상거래 플랫폼의 발전에 따라 사기 활동이 증가하고 있으며, 이로 인해 플랫폼의 보안과 안정성이 위협받고 있습니다.

Method: PROMOGUARDIAN은 거래 데이터의 공간적 및 시간적 정보를 동질적 그래프로 통합하여 프로모션 남용 사기를 탐지하는 다중 관계 융합 그래프 신경망입니다.

Result: 실제 MEITUAN 데이터에 대한 실험 결과, 제안한 모델이 프로모션 남용 사기 탐지에서 최신 방법보다 우수한 성능을 보여주었으며, 정밀도 93.15%를 기록하고 사기자를 2.1배에서 5.0배 더 탐지하며, 재해 방지 및 재정적 손실을 1.5배에서 8.8배 더 예방했습니다.

Conclusion: PROMOGUARDIAN은 프로모션 남용 사기 탐지에서 우수한 성능을 입증하며, 전자상거래 플랫폼의 보안 강화를 위한 중요한 기여를 하고 있습니다.

Abstract: As e-commerce platforms develop, fraudulent activities are increasingly
emerging, posing significant threats to the security and stability of these
platforms. Promotion abuse is one of the fastest-growing types of fraud in
recent years and is characterized by users exploiting promotional activities to
gain financial benefits from the platform. To investigate this issue, we
conduct the first study on promotion abuse fraud in e-commerce platforms
MEITUAN. We find that promotion abuse fraud is a group-based fraudulent
activity with two types of fraudulent activities: Stocking Up and Cashback
Abuse. Unlike traditional fraudulent activities such as fake reviews, promotion
abuse fraud typically involves ordinary customers conducting legitimate
transactions and these two types of fraudulent activities are often
intertwined. To address this issue, we propose leveraging additional
information from the spatial and temporal perspectives to detect promotion
abuse fraud. In this paper, we introduce PROMOGUARDIAN, a novel multi-relation
fused graph neural network that integrates the spatial and temporal information
of transaction data into a homogeneous graph to detect promotion abuse fraud.
We conduct extensive experiments on real-world data from MEITUAN, and the
results demonstrate that our proposed model outperforms state-of-the-art
methods in promotion abuse fraud detection, achieving 93.15% precision,
detecting 2.1 to 5.0 times more fraudsters, and preventing 1.5 to 8.8 times
more financial losses in production environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks](https://arxiv.org/abs/2510.11917)
*Jun-En Ding,Anna Zilverstand,Shihao Yang,Albert Chih-Chieh Yang,Feng Liu*

Main category: cs.LG

TL;DR: VMoGE는 EEG의 주파수 특정 바이오마커 식별과 구조화된 변량 추론을 통합하여 치매 진단 및 단계화를 향상시키는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 치매 질환, 특히 알츠하이머병(AD)과 전두측두엽 치매(FTD)는 EEG에서 전자생리학적 특성이 겹쳐져 정확한 진단을 방해합니다.

Method: VMoGE는 다중 크기의 변환기를 사용하여 네 가지 주파수 대역에서 다중 스케일 시간 패턴을 추출하고, 이어서 가우시안 마르코프 랜덤 필드 우선 순위를 사용하는 변량 그래프 컨볼루션 인코더를 적용합니다.

Result: VMoGE는 두 개의 다양한 데이터 세트에서 변별 유형 분류와 심각도 단계화에 대해 평가되었으며, 최신 방법에 비해 AUC가 +4%에서 +10% 향상된 성능을 보여줍니다.

Conclusion: VMoGE는 임상 지표와 상관관계가 있는 전문가 가중치를 통해 해석 가능한 통찰력을 제공하고, 신경 병리학적 서명과 일치하는 공간 패턴을 통해 EEG 바이오마커 발견을 촉진합니다.

Abstract: Dementia disorders such as Alzheimer's disease (AD) and frontotemporal
dementia (FTD) exhibit overlapping electrophysiological signatures in EEG that
challenge accurate diagnosis. Existing EEG-based methods are limited by
full-band frequency analysis that hinders precise differentiation of dementia
subtypes and severity stages. We propose a variational mixture of graph neural
experts (VMoGE) that integrates frequency-specific biomarker identification
with structured variational inference for enhanced dementia diagnosis and
staging. VMoGE employs a multi-granularity transformer to extract multi-scale
temporal patterns across four frequency bands, followed by a variational graph
convolutional encoder using Gaussian Markov Random Field priors. Through
structured variational inference and adaptive gating, VMoGE links neural
specialization to physiologically meaningful EEG frequency bands. Evaluated on
two diverse datasets for both subtype classification and severity staging,
VMoGE achieves superior performance with AUC improvements of +4% to +10% over
state-of-the-art methods. Moreover, VMoGE provides interpretable insights
through expert weights that correlate with clinical indicators and spatial
patterns aligned with neuropathological signatures, facilitating EEG biomarker
discovery for comprehensive dementia diagnosis and monitoring.

</details>


### [34] [MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging](https://arxiv.org/abs/2510.12070)
*Sangmin Jo,Jee Seok Yoon,Wootaek Jeong,Kwanseok Oh,Heung-Il Suk*

Main category: cs.LG

TL;DR: 딥러닝 기반의 자동 수면 단계 분류가 개선되었지만, 모델이 생리학적 신호의 변동성으로 인해 새로운 주자에게 일반화되는 데 어려움을 겪는다. 본 논문에서는 이러한 문제를 해결하기 위해 도메인 일반화 접근법과 대조 학습을 통해 도메인 불변 특징을 학습하는 방법을 제안한다. 특히, 과도한 도메인 관련 정보를 완화하여 도메인 격차를 줄이는 것이 중요하다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: 수면 장애 진단에서 자동 수면 단계 분류의 중요성과 현재 모델의 일반화 문제 해결 필요성.

Method: MEASURE(다중 스케일 최소 충분 표현 학습) 프레임워크를 제안하여 수면 단계 분류를 위한 필수적인 시간적 및 주파수적 특징을 유지하면서 도메인 관련 정보를 효과적으로 줄인다.

Result: 공개된 수면 단계 분류 기준 데이터셋인 SleepEDF-20과 MASS에서 제안한 방법이 최신 기술보다 지속적으로 우수한 성능을 보였다.

Conclusion: 제안한 MEASURE 프레임워크는 다양한 특징 수준에서 인코딩된 시간적 및 주파수적 정보의 활용 능력을 증대시킴으로써, 수면 단계 분류의 성능 개선에 기여한다.

Abstract: Deep learning-based automatic sleep staging has significantly advanced in
performance and plays a crucial role in the diagnosis of sleep disorders.
However, those models often struggle to generalize on unseen subjects due to
variability in physiological signals, resulting in degraded performance in
out-of-distribution scenarios. To address this issue, domain generalization
approaches have recently been studied to ensure generalized performance on
unseen domains during training. Among those techniques, contrastive learning
has proven its validity in learning domain-invariant features by aligning
samples of the same class across different domains. Despite its potential, many
existing methods are insufficient to extract adequately domain-invariant
representations, as they do not explicitly address domain characteristics
embedded within the unshared information across samples. In this paper, we
posit that mitigating such domain-relevant attributes-referred to as excess
domain-relevant information-is key to bridging the domain gap. However, the
direct strategy to mitigate the domain-relevant attributes often overfits
features at the high-level information, limiting their ability to leverage the
diverse temporal and spectral information encoded in the multiple feature
levels. To address these limitations, we propose a novel MEASURE (Multi-scalE
minimAl SUfficient Representation lEarning) framework, which effectively
reduces domain-relevant information while preserving essential temporal and
spectral features for sleep stage classification. In our exhaustive experiments
on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,
our proposed method consistently outperformed state-of-the-art methods. Our
code is available at : https://github.com/ku-milab/Measure

</details>


### [35] [Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development](https://arxiv.org/abs/2510.12253)
*Changfu Xu,Jianxiong Guo,Yuzhu Liang,Haiyang Huang,Haodong Zou,Xi Zheng,Shui Yu,Xiaowen Chu,Jiannong Cao,Tian Wang*

Main category: cs.LG

TL;DR: 이 설문조사는 확산 기반 강화 학습의 포괄적이고 최신의 종합적 개요를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습(RL)의 주요 도전 과제를 해결하고 DMs의 역할을 명확하게 정의하기 위해.

Method: 플랫폼 내에서 DMs의 기능을 명확히 하고 온라인 학습과 오프라인 학습 사이의 구현을 구분하는 이원축 분류법을 수립.

Result: 수많은 응용 사례를 통해 확산 기반 RL의 성공적인 적용을 확인하고, 단일 에이전트에서 다중 에이전트 도메인으로의 발전을 검토합니다.

Conclusion: 미래 연구 방향을 제시하며, RL에 대한 DMs의 적용을 위한 다양한 리소스를 제공하는 GitHub 저장소를 유지 관리합니다.

Abstract: Diffusion Models (DMs), as a leading class of generative models, offer key
advantages for reinforcement learning (RL), including multi-modal
expressiveness, stable training, and trajectory-level planning. This survey
delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We
first provide an overview of RL, highlighting its challenges, and then
introduce the fundamental concepts of DMs, investigating how they are
integrated into RL frameworks to address key challenges in this research field.
We establish a dual-axis taxonomy that organizes the field along two orthogonal
dimensions: a function-oriented taxonomy that clarifies the roles DMs play
within the RL pipeline, and a technique-oriented taxonomy that situates
implementations across online versus offline learning regimes. We also provide
a comprehensive examination of this progression from single-agent to
multi-agent domains, thereby forming several frameworks for DM-RL integration
and highlighting their practical utility. Furthermore, we outline several
categories of successful applications of diffusion-based RL across diverse
domains, discuss open research issues of current methodologies, and highlight
key directions for future research to advance the field. Finally, we summarize
the survey to identify promising future development directions. We are actively
maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for
papers and other related resources to apply DMs for RL.

</details>


### [36] [Multi-Action Self-Improvement for Neural Combinatorial Optimization](https://arxiv.org/abs/2510.12273)
*Laurin Luttmann,Lin Xie*

Main category: cs.LG

TL;DR: 모델이 고품질 솔루션을 생성하고 모방함으로써 정책을 반복적으로 개선하는 자기 개선 방식이 다중 에이전트 동작에 적용되어 효율성을 높이고 협동 행동 학습을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들은 정책 업데이트를 위해 많은 후보 솔루션을 샘플링해야 하며, 조합 최적화 문제에서 에이전트 간 협조를 활용하지 못합니다.

Method: 우리의 모델은 각 결정 단계에서 에이전트-작업 할당을 공동으로 예측하고, 여러 전문가의 할당에 대해 정책을 감독하는 집합 예측 손실을 사용합니다.

Result: 우리 방법은 조합 문제에서 최종 솔루션의 품질을 지속적으로 향상시키고 생성 지연 시간을 줄이는 성과를 보였습니다.

Conclusion: 이 접근 방식은 시료 효율성을 높이고 모델의 협동 행동 학습 능력을 향상시킵니다.

Abstract: Self-improvement has emerged as a state-of-the-art paradigm in Neural
Combinatorial Optimization (NCO), where models iteratively refine their
policies by generating and imitating high-quality solutions. Despite strong
empirical performance, existing methods face key limitations. Training is
computationally expensive, as policy updates require sampling numerous
candidate solutions per instance to extract a single expert trajectory. More
fundamentally, these approaches fail to exploit the structure of combinatorial
problems involving the coordination of multiple agents, such as vehicles in
min-max routing or machines in scheduling. By supervising on single-action
trajectories, they fail to exploit agent-permutation symmetries, where distinct
sequences of actions yield identical solutions, hindering generalization and
the ability to learn coordinated behavior.
  We address these challenges by extending self-improvement to operate over
joint multi-agent actions. Our model architecture predicts complete agent-task
assignments jointly at each decision step. To explicitly leverage symmetries,
we employ a set-prediction loss, which supervises the policy on multiple expert
assignments for any given state. This approach enhances sample efficiency and
the model's ability to learn coordinated behavior. Furthermore, by generating
multi-agent actions in parallel, it drastically accelerates the solution
generation phase of the self-improvement loop. Empirically, we validate our
method on several combinatorial problems, demonstrating consistent improvements
in the quality of the final solution and a reduced generation latency compared
to standard self-improvement.

</details>


### [37] [Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand](https://arxiv.org/abs/2510.12328)
*Kiattikun Chobtham,Kanoksri Sarinnapakorn,Kritanai Torsri,Prattana Deeprasertkul,Jirawan Kamma*

Main category: cs.LG

TL;DR: 이 논문은 극단적 강수 사건 예측을 개선하기 위해 물리 기반 그래프 신경망을 도입한다.


<details>
  <summary>Details</summary>
Motivation: 정확한 강수 예측, 특히 극단적 사건에 대한 예측은 기후학과 지구 시스템에서 여전히 중요한 도전 과제이다.

Method: 극단값 분석 기법과 결합된 새로운 물리 정보 그래프 신경망(GNN)을 통해 태국 전역의 강수 예측을 개선하는 모델을 제안한다.

Result: 실험 결과, 제안된 방법이 극단적인 지역을 포함한 대부분의 지역에서 안정된 기준보다 우수한 성능을 보이고, 최신 기술과 경쟁력을 유지함을 입증하였다.

Conclusion: 운영 예측 시스템 SEAS5와 비교할 때, 우리의 실제 응용 프로그램은 극단 사건 예측을 개선하고 장기 물 관리 결정을 지원하는 고해상도 지도 생성에 실질적인 향상을 제공한다.

Abstract: Accurate rainfall forecasting, particularly for extreme events, remains a
significant challenge in climatology and the Earth system. This paper presents
novel physics-informed Graph Neural Networks (GNNs) combined with extreme-value
analysis techniques to improve gauge-station rainfall predictions across
Thailand. The model leverages a graph-structured representation of gauge
stations to capture complex spatiotemporal patterns, and it offers
explainability through teleconnections. We preprocess relevant climate indices
that potentially influence regional rainfall. The proposed Graph Attention
Network with Long Short-Term Memory (Attention-LSTM) applies the attention
mechanism using initial edge features derived from simple
orographic-precipitation physics formulation. The embeddings are subsequently
processed by LSTM layers. To address extremes, we perform Peak-Over-Threshold
(POT) mapping using the novel Spatial Season-aware Generalized Pareto
Distribution (GPD) method, which overcomes limitations of traditional
machine-learning models. Experiments demonstrate that our method outperforms
well-established baselines across most regions, including areas prone to
extremes, and remains strongly competitive with the state of the art. Compared
with the operational forecasting system SEAS5, our real-world application
improves extreme-event prediction and offers a practical enhancement to produce
fine-resolution maps that support decision-making in long-term water
management.

</details>


### [38] [Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models](https://arxiv.org/abs/2510.12618)
*Manuel Hinz,Maximilian Mauel,Patrick Seifner,David Berghaus,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 이 논문에서는 비선형 역학 시스템에서의 차원 축소와 관련된 문제를 해결하기 위해 기반 추론 모델(FIMs)을 이용하는 방법을 제안합니다. 이를 통해 동역학의 추론을 최적화하고 안정적인 표현 학습을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 고차원 동적 과정의 기록은 종종 저차원 다양체에서 발전하는 훨씬 더 작은 유효 변수 세트로 특징지어집니다. 이러한 잠재 동역학을 식별하려면 적절한 세분화 변수 발견과 통치 방정식의 동시 적합이라는 두 가지 얽힌 문제를 해결해야 합니다.

Method: 우리는 최근 도입된 기반 추론 모델(FIMs)을 활용하여 두 문제를 분리하는 방법을 제안합니다. FIM은 동적 시스템의 미분 생성기(예: 확률적 미분 방정식의 드리프트 및 확산)를 제로 샷 모드에서 추정하는 사전 훈련된 모델입니다.

Result: FIM을 통해 동역학의 추론을 암호화된 가중치를 고정하여 분산시키고, 인코더-디코더 맵만 훈련함으로써 표현 학습을 안정화시킵니다. 반시계형 확산을 가진 확률적 이중 우물 시스템에 대한 개념 증명은 빠르고 재사용 가능한 세분화 파이프라인에 대한 이 접근 방식의 잠재력을 보여줍니다.

Conclusion: 우리의 접근 방식은 동역학 일관성을 유지하면서 간단하고 효과적인 세분화 방법을 제공합니다.

Abstract: High-dimensional recordings of dynamical processes are often characterized by
a much smaller set of effective variables, evolving on low-dimensional
manifolds. Identifying these latent dynamics requires solving two intertwined
problems: discovering appropriate coarse-grained variables and simultaneously
fitting the governing equations. Most machine learning approaches tackle these
tasks jointly by training autoencoders together with models that enforce
dynamical consistency. We propose to decouple the two problems by leveraging
the recently introduced Foundation Inference Models (FIMs). FIMs are pretrained
models that estimate the infinitesimal generators of dynamical systems (e.g.,
the drift and diffusion of a stochastic differential equation) in zero-shot
mode. By amortizing the inference of the dynamics through a FIM with frozen
weights, and training only the encoder-decoder map, we define a simple,
simulation-consistent loss that stabilizes representation learning. A proof of
concept on a stochastic double-well system with semicircle diffusion, embedded
into synthetic video data, illustrates the potential of this approach for fast
and reusable coarse-graining pipelines.

</details>


### [39] [Learning-To-Measure: In-context Active Feature Acquisition](https://arxiv.org/abs/2510.12624)
*Yuta Kobayashi,Zilin Jing,Jiayu Yao,Hongseok Namkoong,Shalmali Joshi*

Main category: cs.LG

TL;DR: 이 논문은 여러 작업에 걸쳐 특성 취득 정책을 학습하는 메타-AFA 문제를 형식화하고, 불확실성 정량화를 통해 모델 성능을 향상시키는 학습 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: AFA 방법은 일반적으로 특성 결측과 한정된 작업별 레이블을 가진 회고적 데이터를 사용하여 학습되며, 단일 미리 결정된 작업에 대한 취득에 제한을 받습니다.

Method: 메타-AFA에서 여러 작업에 걸쳐 취득 정책을 학습하며, 불확실성 정량화 및 조건부 상호 정보 극대화를 위한 불확실성 유도 탐욕적 특성 취득 에이전트를 도입합니다.

Result: L2M은 회고적 결측이 있는 데이터셋에서 직접 작동하며, 각 작업 재학습 없이 메타-AFA 작업을 문맥 내에서 수행합니다.

Conclusion: L2M은 합성 및 실제 데이터셋에서 작업별 기준선을 일치시키거나 초과하여 레이블 부족 및 높은 결측 상황에서 특히 효과적입니다.

Abstract: Active feature acquisition (AFA) is a sequential decision-making problem
where the goal is to improve model performance for test instances by adaptively
selecting which features to acquire. In practice, AFA methods often learn from
retrospective data with systematic missingness in the features and limited
task-specific labels. Most prior work addresses acquisition for a single
predetermined task, limiting scalability. To address this limitation, we
formalize the meta-AFA problem, where the goal is to learn acquisition policies
across various tasks. We introduce Learning-to-Measure (L2M), which consists of
i) reliable uncertainty quantification over unseen tasks, and ii) an
uncertainty-guided greedy feature acquisition agent that maximizes conditional
mutual information. We demonstrate a sequence-modeling or autoregressive
pre-training approach that underpins reliable uncertainty quantification for
tasks with arbitrary missingness. L2M operates directly on datasets with
retrospective missingness and performs the meta-AFA task in-context,
eliminating per-task retraining. Across synthetic and real-world tabular
benchmarks, L2M matches or surpasses task-specific baselines, particularly
under scarce labels and high missingness.

</details>


### [40] [Expert or not? assessing data quality in offline reinforcement learning](https://arxiv.org/abs/2510.12638)
*Arip Asadulaev,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 오프라인 강화 학습은 정적 데이터셋에서만 학습하며, 데이터셋의 품질이 알고리즘 선택에 영향을 미친다. 본 연구에서는 에이전트 훈련 없이 오프라인 데이터셋 품질을 추정하는 방법을 제시하며, 벨만 와서슈타인 거리(BWD)라는 새로운 지표를 도입하여 데이터셋의 행동 정책과 랜덤 참조 정책 간의 유사성을 측정한다. BWD는 오프라인 RL 알고리즘의 성능과 강한 상관관계를 보이며, 정책 최적화 시 BWD를 통합함으로써 무작위 행동에서 벗어나고 수익을 향상시킬 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 오프라인 강화 학습은 정적 데이터셋에서 학습하지만, 데이터셋의 품질이 다양하여 알고리즘 선택에 큰 영향을 미친다.

Method: 간단한 누적 보상에서 학습된 가치 기반 추정기를 포함한 다양한 프록시를 연구하고, 데이터셋의 행동 정책과 랜덤 참조 정책 간의 비유사성을 측정하는 벨만 와서슈타인 거리(BWD)를 도입하였다.

Result: BWD는 D4RL MuJoCo 작업에서 오라클 성능 점수와 강한 상관관계를 가지며, 특정 데이터셋에서 에이전트의 성능을 예측할 수 있게 한다.

Conclusion: BWD와 같은 가치 인식 분포 신호는 오프라인 RL 데이터셋의 분류 및 정책 최적화에서 유용한 도구임을 나타낸다.

Abstract: Offline reinforcement learning (RL) learns exclusively from static datasets,
without further interaction with the environment. In practice, such datasets
vary widely in quality, often mixing expert, suboptimal, and even random
trajectories. The choice of algorithm therefore depends on dataset fidelity.
Behavior cloning can suffice on high-quality data, whereas mixed- or
low-quality data typically benefits from offline RL methods that stitch useful
behavior across trajectories. Yet in the wild it is difficult to assess dataset
quality a priori because the data's provenance and skill composition are
unknown. We address the problem of estimating offline dataset quality without
training an agent. We study a spectrum of proxies from simple cumulative
rewards to learned value based estimators, and introduce the Bellman
Wasserstein distance (BWD), a value aware optimal transport score that measures
how dissimilar a dataset's behavioral policy is from a random reference policy.
BWD is computed from a behavioral critic and a state conditional OT
formulation, requiring no environment interaction or full policy optimization.
Across D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance
score that aggregates multiple offline RL algorithms, enabling efficient
prediction of how well standard agents will perform on a given dataset. Beyond
prediction, integrating BWD as a regularizer during policy optimization
explicitly pushes the learned policy away from random behavior and improves
returns. These results indicate that value aware, distributional signals such
as BWD are practical tools for triaging offline RL datasets and policy
optimization.

</details>


### [41] [Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672)
*Ruben Belo,Claudia Soares,Marta Guimaraes*

Main category: cs.LG

TL;DR: CALM은 유해 개념을 억제하는 인퍼런스 시간 방법으로, 안전 장치를 우회하는 공격에 대한 저항력을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 기본 안전 장치를 우회하는 탈옥 공격에 취약하다는 점을 해결하고자 합니다.

Method: CALM은 모델의 최종 층의 잠재 표현을 수정하여 유해한 개념을 억제하는 인퍼런스 시간 방법입니다.

Result: CALM은 유해한 출력을 줄이고 대부분의 메트릭에서 기준 방법보다 우수한 성능을 보입니다.

Conclusion: CALM은 추가적인 훈련 데이터나 모델 미세 조정 없이도 경량화된 접근 방식을 제공하며 인퍼런스 시 오버헤드가 적습니다.

Abstract: Large Language Models are susceptible to jailbreak attacks that bypass
built-in safety guardrails (e.g., by tricking the model with adversarial
prompts). We propose Concept Alignment and Concept Manipulation \textbf{CALM},
an inference-time method that suppresses harmful concepts by modifying latent
representations of the last layer of the model, without retraining. Leveraging
\gls*{cw} technique from Computer Vision combined with orthogonal projection,
CALM removes unwanted latent directions associated with harmful content while
preserving model performance. Experiments show that CALM reduces harmful
outputs and outperforms baseline methods in most metrics, offering a
lightweight approach to AI safety with no additional training data or model
fine-tuning, while incurring only a small computational overhead at inference.

</details>
