<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.CR](#cs.CR) [Total: 6]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [HieraMAS: Optimizing Intra-Node LLM Mixtures and Inter-Node Topology for Multi-Agent Systems](https://arxiv.org/abs/2602.20229)
*Tianjun Yao,Zhaoyi Li,Zhiqiang Shen*

Main category: cs.MA

TL;DR: HieraMAS는 LLM 혼합과 통신 토폴로지를 결합하여 다중 에이전트 시스템의 성능을 향상시키는 계층적 협업 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 접근법은 통신 토폴로지, 역할 할당 또는 LLM 라우팅과 같은 단일 측면만을 개선하며, 각 에이전트를 단일한 단위로 취급하는 한계를 가지고 있다.

Method: HieraMAS는 다중 이질 LLM을 사용하는 슈퍼노드를 도입하고, 다단계 보상 귀속 및 그래프 분류를 통한 두 단계 알고리즘을 사용한다.

Result: HieraMAS는 기존 방법들보다 상당한 성과 향상을 보이며, 더 나은 비용-성과 트레이드오프를 제공한다.

Conclusion: HieraMAS는 에이전트 내 LLM 혼합 활용을 통해 역할 특정 능력을 강화하고, 보다 우수한 성능을 얻는다.

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) have shown strong performance across many tasks. Most existing approaches improve only one aspect at a time, such as the communication topology, role assignment, or LLM routing, while treating each agent as a single, indivisible unit. This misses the opportunity to use mixtures of LLMs within an agent to strengthen role-specific abilities. We propose HieraMAS, a hierarchical collaboration framework that combines intra-node LLM mixtures with an inter-node communication topology. HieraMAS introduces supernodes, where each functional role is implemented by multiple heterogeneous LLMs using a propose-synthesis structure. Optimizing HieraMAS creates unique credit-assignment challenges: final task performance depends heavily on the underlying LLMs' capabilities, which can lead reinforcement methods to incorrectly reward suboptimal configurations. To address this, we use a two-stage algorithm: (1) multi-level reward attribution, which provides fine-grained feedback at both the node level and the overall system level; (2) graph classification for topology selection, which treats choosing the communication structure as a holistic decision rather than optimizing edges one by one. Experiments on reasoning and coding benchmarks show that HieraMAS substantially outperforms existing methods while also delivering better cost-performance trade-offs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [Implicit Intelligence -- Evaluating Agents on What Users Don't Say](https://arxiv.org/abs/2602.20424)
*Ved Sirdeshmukh,Marc Wetter*

Main category: cs.AI

TL;DR: AI 에이전트에 대한 실제 요청은 불완전하게 지정되어 있다. 현재의 평가 benchmarks는 명시적 지시나 따르도록 하는 것만 테스트하며, 주변의 높은 복잡성을 평가하지 않는다. 우리의 연구에서는 Implicit Intelligence라는 평가 프레임워크를 제시하며, 이를 통해 AI 에이전트가 진정한 목표 달성자로 발전할 수 있는지를 평가한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 인간과 비슷한 맥락적 추론 능력을 갖추도록 하기 위해, 명시적인 지시를 넘어서 implicit 요구사항에 대응해야 한다는 필요성이 존재한다.

Method: Implicit Intelligence라는 평가 프레임워크와 Agent-as-a-World(AaW)라는 환경을 활용하여, AI 모델의 요청 처리 능력을 테스트한다. YAML 파일로 정의된 상호작용적 세계를 통해 모델이 수행하도록 한다.

Result: 16개의 최첨단 및 오픈웨이트 모델을 205개의 시나리오에서 평가한 결과, 최고의 모델도 48.3%의 시나리오 합격률을 기록하였다.

Conclusion: 명시적 지시를 따르는 것과 인간과 유사한 맥락적 추론 사이의 격차를 줄이기 위한 개선 여지가 크다는 것을 확인하였다.

Abstract: Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.

</details>


### [3] [Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use](https://arxiv.org/abs/2602.20426)
*Ruocheng Guo,Kaiwen Dong,Xiang Gao,Kamalika Das*

Main category: cs.AI

TL;DR: LLM 기반 에이전트의 성능은 에이전트 자체 뿐만 아니라 사용되는 도구 인터페이스의 품질에도 의존한다. 기존 연구들은 에이전트 미세 조정에 중점을 두었지만, 도구 인터페이스는 여전히 인간 지향적이며 큰 후보 도구 세트에서 선택할 때 병목 현상이 발생한다. 본 연구에서는 Trace-Free+라는 커리큘럼 학습 프레임워크를 제안하여, 추적이 풍부한 환경에서 추적이 없는 배포로 점진적으로 감독을 전이하여 재사용 가능한 인터페이스 사용 패턴과 도구 사용 결과를 추상화하도록 모델을 유도한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 성능 향상과 도구 인터페이스 최적화의 필요성

Method: Trace-Free+라는 커리큘럼 학습 프레임워크를 통해 감독을 점진적으로 전이하는 접근 방식

Result: StableToolBench와 RestBench에서 일관된 성능 향상과 도메인 간 강력한 일반화, 후보 도구가 100개 이상으로 확장될 때의 강인성을 보여줌

Conclusion: 도구 인터페이스 최적화는 에이전트 미세 조정에 실용적이고 배포 가능한 보완이 될 수 있다.

Abstract: The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.

</details>


### [4] [ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory](https://arxiv.org/abs/2602.20502)
*Hongbin Zhong,Fazle Faisal,Luis França,Tanakorn Leesatapornwongsa,Adriana Szekeres,Kexin Rong,Suman Nath*

Main category: cs.AI

TL;DR: ActionEngine은 반응형 실행에서 프로그램matic 계획으로 전환하는 훈련이 필요 없는 프레임워크로, 효율성과 정확성을 크게 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기존 GUI 에이전트는 단계별로 비전 언어 모델에 호출을 수행하여 비용과 대기 시간이 증가하고, 이전 페이지에 대한 지속적인 기억이 없어 정확도가 제한된다.

Method: ActionEngine은 오프라인 탐색을 통해 GUI의 업데이트 가능한 상태 기계 메모리를 구축하는 Crawling Agent와 이 메모리를 활용하여 완전하고 실행 가능한 Python 프로그램을 합성하는 Execution Agent로 구성된 두 개의 에이전트 아키텍처를 통해 반응형 실행에서 프로그램matic 계획으로 전환하는 훈련이 필요 없는 프레임워크이다.

Result: WebArena 벤치마크의 Reddit 작업에서 에이전트는 평균적으로 단일 LLM 호출로 95%의 작업 성공률을 달성했으며, 이는 가장 강력한 비전 전용 기준인 66%와 비교된다. 비용은 11.8배 줄이고 종단 간 대기 시간은 2배 감소시켰다.

Conclusion: 이 디자인은 전 세계적인 프로그램 계획, 크롤러 검증된 작업 템플릿, 국소화된 검증 및 수리를 통해 확장 가능하고 신뢰할 수 있는 GUI 상호작용을 제공한다.

Abstract: Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.
  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.
  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.
  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.
  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.

</details>


### [5] [Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination](https://arxiv.org/abs/2602.20517)
*Rakshit Trivedi,Kartik Sharma,David C Parkes*

Main category: cs.AI

TL;DR: MIMIC은 인간과 AI 간의 효과적인 협조를 위한 행동 모델링 프레임워크로, 내면의 언어를 사용하여 행동을 조정하고 다양성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 인간과 AI 간의 협조를 위해서는 인간과 유사한 행동을 나타내고 변화하는 맥락에 적응하는 인공지능 에이전트를 만드는 것이 필요하다.

Method: MIMIC 프레임워크는 언어를 행동 의도의 내부 표현으로 사용하고, 비전-언어 모델을 활용하여 조건부 변분 오토인코더를 훈련시킨다. 이 모델은 관찰에서 내면의 언어를 생성하고, 확산 기반 행동 복제 정책을 통해 현재 관찰 및 생성된 내면의 언어에 따라 행동을 선택한다.

Result: MIMIC은 로봇 조작 및 인간-AI 협업 게임에서 행동의 다양성과 인간 시연에 대한 충실도를 현저히 향상시켰다. 또한 추가 시연 없이 섬세한 행동 조정을 가능하게 하였다.

Conclusion: MIMIC은 행동 전환을 세밀하게 조정할 수 있는 기능을 제공하여 인간과 AI 간의 협업을 개선한다.

Abstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>


### [6] [From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production](https://arxiv.org/abs/2602.20558)
*Yucheng Shi,Ying Li,Yu Wang,Yesu Feng,Arjun Rao,Rein Houthooft,Shradha Sehgal,Jin Wang,Hao Zhen,Ninghao Liu,Linas Baltrunas*

Main category: cs.AI

TL;DR: 본 논문은 LLM 기반 추천 시스템을 위한 데이터 중심의 언어화 프레임워크를 제안하며, 기존의 경직된 템플릿 방식의 한계를 극복하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 생성 추천 시스템의 유망한 기반이지만, 구조화된 사용자 상호작용 로그를 효과적인 자연어 입력으로 전환하는 언어화 과정이 충분히 탐구되지 않았다.

Method: 강화 학습을 사용하여, 언어화 에이전트가 원시 상호작용 이력을 최적화된 텍스트 컨텍스트로 변환하며, 추천 정확도를 훈련 신호로 사용한다.

Result: 대규모 산업 스트리밍 데이터셋 실험에서, 학습된 언어화는 템플릿 기반 기준 대비 최대 93%의 개선된 추천 정확도를 제공했다.

Conclusion: 사용자 관심 요약, 노이즈 제거, 구문 정규화와 같은 emergent 전략을 통해 LLM 기반 추천 시스템에 대한 효과적인 맥락 구성 방법에 대한 통찰을 제공한다.

Abstract: Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.

</details>


### [7] [Grounding LLMs in Scientific Discovery via Embodied Actions](https://arxiv.org/abs/2602.20639)
*Bo Zhang,Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Minlie Huang,Hongning Wang*

Main category: cs.AI

TL;DR: EmbodiedAct는 기존 과학 소프트웨어를 능동적 대리인으로 변환하여 LLM의 수행 및 인식을 개선하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 과학 발견에서 잠재력이 크지만 이론적 추론과 실증적 물리 시뮬레이션 간의 간극을 메우는 데 어려움을 겪고 있다.

Method: EmbodiedAct는 LLM을 능동적인 행동으로 접지하여 인식-수행 루프를 강하게 만들어 기존 과학 소프트웨어를 변환하는 프레임워크이다.

Result: 광범위한 실험 결과, EmbodiedAct는 기존 기준을 크게 능가하며, 장기 시뮬레이션에서 신뢰성 및 안정성을 보장하고 과학 모델링의 정확성을 향상시켜 SOTA 성능을 달성하였다.

Conclusion: EmbodiedAct는 복잡한 공학 설계 및 과학 모델링 작업에서 중요한 성과를 보여준다.

Abstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>


### [8] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied는 VLM 기반의 구현 에이전트를 위한 새로운 벤치마크로, 복잡한 시나리오에서 기본 구현 기술을 평가하는 저수준 작업을 포함한다.


<details>
  <summary>Details</summary>
Motivation: 현재 VLM 기반의 구현 에이전트의 성능 평가가 비현실적인 지시 및 고수준 작업에 집중되고 있어 실제 환경에서의 적용이 어렵다.

Method: NativeEmbodied는 통합된 저수준 행동 공간을 사용하고, 복잡한 시나리오에서 세 가지 대표적인 고수준 작업을 통해 전체 성능을 평가한다.

Result: 최신 VLM을 사용한 실험에서 여러 기본 구현 기술에서 명확한 결 deficiencies가 드러났으며, 이는 고수준 작업 성능에 중대한 영향을 미친다.

Conclusion: NativeEmbodied는 현재 VLM 기반의 구현 에이전트의 주요 도전 과제를 강조하고 향후 연구에 대한 통찰력을 제공한다.

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [9] [ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction](https://arxiv.org/abs/2602.20708)
*Che Wang,Fuyao Zhang,Jiaming Zhang,Ziqi Zhang,Yinghui Wang,Longtao Huang,Jianbo Gao,Zhong Chen,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: ICON 프레임워크는 간접 프롬프트 주입 공격을 중화시키면서 작업 연속성을 유지하도록 설계되었습니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 간접 프롬프트 주입 공격에 취약하며, 기존 방어 메커니즘은 과도한 거부로 인해 유효한 작업 흐름이 조기 종료되는 문제를 가지고 있습니다.

Method: ICON은 잠재 공간의 집중 서명을 감지하는 Latent Space Trace Prober와 공격을 완화하는 Mitigating Rectifier를 통해 공격을 중화합니다.

Result: ICON은 경쟁력 있는 0.4% ASR을 달성하며 50% 이상의 작업 효용 증가를 보여줍니다.

Conclusion: ICON은 보안과 효율성 간의 우수한 균형을 이루며 다양한 다중 모드 에이전트에 효과적으로 확장됩니다.

Abstract: Large Language Model (LLM) agents are susceptible to Indirect Prompt Injection (IPI) attacks, where malicious instructions in retrieved content hijack the agent's execution. Existing defenses typically rely on strict filtering or refusal mechanisms, which suffer from a critical limitation: over-refusal, prematurely terminating valid agentic workflows. We propose ICON, a probing-to-mitigation framework that neutralizes attacks while preserving task continuity. Our key insight is that IPI attacks leave distinct over-focusing signatures in the latent space. We introduce a Latent Space Trace Prober to detect attacks based on high intensity scores. Subsequently, a Mitigating Rectifier performs surgical attention steering that selectively manipulate adversarial query key dependencies while amplifying task relevant elements to restore the LLM's functional trajectory. Extensive evaluations on multiple backbones show that ICON achieves a competitive 0.4% ASR, matching commercial grade detectors, while yielding a over 50% task utility gain. Furthermore, ICON demonstrates robust Out of Distribution(OOD) generalization and extends effectively to multi-modal agents, establishing a superior balance between security and efficiency.

</details>


### [10] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: PyVision-RL은 툴 사용과 다중 턴 추론을 장려하여 강화학습을 안정화하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 다중모드 모델의 강화학습에서 상호작용 붕괴 문제가 발생하여 agentic 행동의 이점을 제한한다.

Method: PyVision-RL은 오버샘플링-필터링-랭킹 롤아웃 전략과 누적 도구 보상을 결합하여 툴 사용을 장려하고 붕괴를 방지한다.

Result: PyVision-Image와 PyVision-Video를 개발하여 이미지 및 비디오 이해를 위한 통합 훈련 파이프라인을 구축했다. 영상 추론을 위해 PyVision-Video는 요구 시 문맥 구성을 사용하며, 비디오의 관련 프레임을 선택적으로 샘플링하여 비주얼 토큰 사용을 크게 줄인다.

Conclusion: 실험 결과는 지속적인 상호작용과 요구 기반 비주얼 처리가 확장 가능한 다중모드 에이전트에 중요함을 보여준다.

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [11] [Pipeline for Verifying LLM-Generated Mathematical Solutions](https://arxiv.org/abs/2602.20770)
*Varvara Sazonova,Dmitri Shmelkin,Stanislav Kikot,Vasily Motolygin*

Main category: cs.AI

TL;DR: 이 논문은 대규모 추론 모델의 수학 문제 해결 능력을 측정하기 위한 자동 및 대화형 검증 파이프라인을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 추론 모델의 인기가 높아짐에 따라 수학 문제 해결 능력을 정확히 측정하는 것이 중요합니다.

Method: 주어진 구조에서 선택할 수 있는 3개의 AI 에이전트를 포함하며, 프롬프트를 사용하여 특정 형식의 솔루션을 얻는 방법을 사용합니다.

Result: 여러 데이터셋에 대한 실험 결과 거짓 긍정의 확률이 낮음을 시사합니다.

Conclusion: 설치 방법 안내와 함께 공개 소스 구현을 제공합니다.

Abstract: With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.

</details>


### [12] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: 대형 언어 모델의 패러다임이 정적 추론 엔진에서 동적 자율 인지 시스템으로 근본적으로 전환되고 있다. 이 논문은 LLM을 '추론 커널'로 재정의하는 통합 개념 프레임워크인 AgentOS를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재 연구는 주로 맥락 창 확장이나 프롬프트 엔지니어링 최적화에 집중되고 있지만, 마이크로 스케일 토큰 처리와 매크로 스케일 시스템 지능 간의 이론적 다리는 단편화되어 있다.

Method: 이 논문은 LLM을 구조화된 운영 체제 논리에 의해 통치되는 '추론 커널'로 재정의하는 통합 개념 프레임워크인 AgentOS를 제안한다.

Result: Deep Context Management를 통해 맥락 창을 수동 버퍼가 아닌 주소 지정 가능한 의미 공간으로 개념화하고, 비밀적 시퀀스에서 일관된 인지 상태로의 전환을 체계적으로 분해한다. 여러 에이전트 조정을 통해 인지 drift를 완화하기 위해 의미 슬라이싱과 시간 정렬 메커니즘을 도입한다.

Conclusion: 시스템 수준 조정의 구조적 효율성이 AGI 발전의 다음 경계에 있다고 주장한다.

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>


### [13] [A Benchmark for Deep Information Synthesis](https://arxiv.org/abs/2602.21143)
*Debjit Paul,Daniel Murphy,Milan Gritta,Ronald Cardenas,Victor Prokhorov,Lena Sophia Bolliger,Aysim Toker,Roy Miles,Andreea-Maria Oncescu,Jasivan Alex Sivakumar,Philipp Borchert,Ismail Elezi,Meiru Zhang,Ka Yiu Lee,Guchun Zhang,Jun Wang,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DEEPSYNTH는 여러 출처의 정보를 종합하고 통찰을 도출하는 복잡한 실제 문제를 평가하기 위한 새로운 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 현재의 평가 벤치마크가 실제 작업을 해결하는 능력을 충분히 평가하지 못하고 있다.

Method: DEEPSYNTH는 정보 수집, 종합 및 구조적 추론을 결합한 120개의 작업으로 구성되어 있다.

Result: 11개의 최신 LLM 및 심층 연구 에이전트가 DEEPSYNTH에서 최고 8.97 및 17.5의 F1 점수를 기록하였다.

Conclusion: DEEPSYNTH는 향후 연구 방향을 제시하는 중요한 벤치마크로 부각되고 있다.

Abstract: Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.

</details>


### [14] [Aletheia tackles FirstProof autonomously](https://arxiv.org/abs/2602.21201)
*Tony Feng,Junehyuk Jung,Sang-hyun Kim,Carlo Pagano,Sergei Gukov,Chiang-Chiang Tsai,David Woodruff,Adel Javanmard,Aryan Mokhtari,Dawsen Hwang,Yuri Chervonyi,Jonathan N. Lee,Garrett Bingham,Trieu H. Trinh,Vahab Mirrokni,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: Aletheia, a mathematics research agent, successfully solved 6 out of 10 problems in the FirstProof challenge.


<details>
  <summary>Details</summary>
Motivation: 이 연구의 동기는 최초의 FirstProof 도전에서 Aletheia의 성능을 보고하고자 하는 것이다.

Method: Aletheia는 제한된 시간 내에 자율적으로 문제를 해결하였고, 전문가 평가에 따라 10문제 중 6문제를 해결했다.

Result: 전문가에 따르면 Aletheia는 Problem 8에 대해서만 의견이 일치하지 않았다.

Conclusion: Aletheia의 문제 해결 능력과 FirstProof에 대한 우리의 해석 및 실험 세부사항을 공유한다.

Abstract: We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment](https://arxiv.org/abs/2602.20194)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 이 논문은 교량의 점검 기록을 사용하여 연합 프레임워크를 통해 다양한 지자체가 협력하여 점검 기록을 전송하지 않고도 공유 기준 모델을 훈련할 수 있도록 하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 데이터 거버넌스 제약으로 인해 교량 점검 기록의 민감한 정보를 포함한 공공 인프라에 대한 교차 조직 데이터 공유가 비실용적입니다.

Method: 이 연구는 각 사용자가 지역 점검 데이터를 보유하고, 교량 연령, 해안선 거리 및 데크 면적을 위한 공변량과 함께 세 가지 열화 방향 전환에 대한 로그-선형 위험 모델을 훈련하는 방법을 사용합니다.

Result: 시뮬레이션 결과는 이질적인 사용자 간의 평균 음의 로그 가능도가 일관되게 수렴하며, 사용자 수가 증가함에 따라 집계된 기울기 노름이 감소함을 보여줍니다.

Conclusion: 연합 업데이트 메커니즘은 사용자들이 공유 기술 표준 플랫폼에 지역 점검 데이터 세트를 등록하면 주기적으로 업데이트되는 글로벌 기준 매개변수를 받을 수 있는 자연스러운 참여 인센티브를 제공합니다.

Abstract: Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\to$Minor, Good$\to$Severe, and Minor$\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.

</details>


### [16] [Probing Dec-POMDP Reasoning in Cooperative MARL](https://arxiv.org/abs/2602.20804)
*Kale-ab Tessera,Leonard Hinckeldey,Riccardo Zamboni,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 협력적 다중 에이전트 강화 학습(MARL)이 실제로 Dec-POMDP 추론을 요구하는지 불분명하다. 우리의 진단 도구는 여러 시나리오에서 정책의 행동 복잡성을 감사하여 이러한 벤치마크가 핵심 Dec-POMDP 가정을 충분히 테스트하지 않을 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 효과적인 협력적 다중 에이전트 강화 학습을 위해 Dec-POMDP 추론의 필요성을 밝히기 위해.

Method: IPPO와 MAPPO와 같은 기준 정책의 행동 복잡성을 감사하기 위해 통계에 기반한 성능 비교와 정보 이론적 조사를 결합한 진단 도구를 도입했다.

Result: 벤치마크에서의 성공은 진정한 Dec-POMDP 추론을 거의 요구하지 않으며, 반응형 정책이 기억 기반 에이전트와 동등한 성능을 나타낸다.

Conclusion: 일부 널리 사용되는 벤치마크가 현재의 훈련 패러다임 하에서 핵심 Dec-POMDP 가정을 충분히 테스트하지 않을 수 있음을 암시하며, 과도한 낙관적인 진전을 초래할 수 있다.

Abstract: Cooperative multi-agent reinforcement learning (MARL) is typically framed as a decentralised partially observable Markov decision process (Dec-POMDP), a setting whose hardness stems from two key challenges: partial observability and decentralised coordination. Genuinely solving such tasks requires Dec-POMDP reasoning, where agents use history to infer hidden states and coordinate based on local information. Yet it remains unclear whether popular benchmarks actually demand this reasoning or permit success via simpler strategies. We introduce a diagnostic suite combining statistically grounded performance comparisons and information-theoretic probes to audit the behavioural complexity of baseline policies (IPPO and MAPPO) across 37 scenarios spanning MPE, SMAX, Overcooked, Hanabi, and MaBrax. Our diagnostics reveal that success on these benchmarks rarely requires genuine Dec-POMDP reasoning. Reactive policies match the performance of memory-based agents in over half the scenarios, and emergent coordination frequently relies on brittle, synchronous action coupling rather than robust temporal influence. These findings suggest that some widely used benchmarks may not adequately test core Dec-POMDP assumptions under current training paradigms, potentially leading to over-optimistic assessments of progress. We release our diagnostic tooling to support more rigorous environment design and evaluation in cooperative MARL.

</details>


### [17] [Exploring Anti-Aging Literature via ConvexTopics and Large Language Models](https://arxiv.org/abs/2602.20224)
*Lana E. Yeganova,Won G. Kim,Shubo Tian,Natalie Xie,Donald C. Comeau,W. John Wilbur,Zhiyong Lu*

Main category: cs.LG

TL;DR: 이 논문에서는 안정적이고 해석 가능한 주제를 생성하는 클러스터링 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 생물 의학 출판물의 급속한 증가로 인해 지식 조직화 및 추세 탐지에 도전 과제가 발생하고 있으며, 이는 확장 가능한 해석 가능한 방법의 필요성을 강조합니다.

Method: 데이터에서 예시를 선택하여 전역 최적화를 보장하는 볼록 최적화 기반 클러스터링 알고리즘의 재구성을 제안합니다.

Result: 약 12,000개의 노화 및 장수에 관한 PubMed 기사를 적용하여 의학 전문가들에 의해 검증된 주제를 발견했습니다.

Conclusion: 이 방법은 K-means, LDA 및 BERTopic과 같은 일반적인 클러스터링 접근 방식과 비교할 때 재현성과 해석 가능성에서 차별화된 성능을 보입니다.

Abstract: The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery.

</details>


### [18] [Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning](https://arxiv.org/abs/2602.21020)
*Antoine Bergerault,Volkan Cevher,Negar Mehr*

Main category: cs.LG

TL;DR: 본 논문에서는 다중 에이전트 모방 학습에서 Nash 균형과의 차이를 특성화하는 데 필요한 결과의 어려움과 불가능성을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 모방 학습의 성과를 보장하는 기존 연구가 있으나, 오프라인 MA-IL에서 학습된 정책이 Nash 균형에 얼마나 가까운지를 특성화하는 연구가 부족하다.

Method: 일반적인 n-플레이어 마르코프 게임에서 저수익 정책을 배우는 것이 불가능하고 어려운 결과를 보여줍니다. 정확한 측정 매칭이 실패하는 예를 제공하고, 고정된 측정 매칭 오류에 대한 Nash 차이 특성화에 대한 새로운 어려움 결과를 보여줍니다.

Result: 지배적인 전략 전문가 균형 경우에서, 행동 복제 오류를 가정할 때 $	ext{O}ig(nε_{	ext{BC}}/(1-γ)^2ig)$의 Nash 모방 간격을 제공합니다.

Conclusion: 새로운 최상의 대응 연속성 개념으로 이 결과를 일반화하며, 이는 표준 정규화 기법에 의해 암묵적으로 장려된다고 주장합니다.

Abstract: Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $ε_{\text{BC}}$, this provides a Nash imitation gap of $\mathcal{O}\left(nε_{\text{BC}}/(1-γ)^2\right)$ for a discount factor $γ$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.

</details>


### [19] [QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.20309)
*Jingxuan Zhang,Yunta Hsieh,Zhongwei Wang,Haokun Lin,Xin Wang,Ziqi Wang,Yingtie Lei,Mi Zhang*

Main category: cs.LG

TL;DR: QuantVLA는 VLA 모델의 메모리 및 계산 요구를 줄이는 훈련이 필요 없는 양자화 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: VLA 모델은 지각, 언어 및 제어를 통합하지만 계산 및 메모리 요구가 급증하여 실제 배치에 많은 어려움이 있다.

Method: QuantVLA는 훈련이 필요 없는 후훈련 양자화 프레임워크로, 선택적 양자화 레이아웃, 주의 온도 일치화, 출력 헤드 균형 조정의 세 가지 구성 요소를 포함한다.

Result: QuantVLA는 LIBERO의 대표적인 VLA 모델에서 전체 정밀도 기준선을 초과하는 작업 성공률을 성취하고, 양자화된 구성 요소에서 약 70%의 메모리 절약을 달성하며, 최종 추론 지연시간에서 1.22배의 속도 향상을 제공한다.

Conclusion: QuantVLA는 엄격한 계산, 메모리 및 전력 제약 속에서 확장 가능한 저비트 신체 지능을 위한 실질적인 경로를 제공한다.

Abstract: Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.

</details>


### [20] [CaDrift: A Time-dependent Causal Generator of Drifting Data Streams](https://arxiv.org/abs/2602.20329)
*Eduardo V. L. Barboza,Jean Paul Barddal,Robert Sabourin,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: CaDrift는 시간 의존적 합성 데이터 생성기 프레임워크로, 기존 관계를 변화시키면서 다양한 데이터를 생성할 수 있다.


<details>
  <summary>Details</summary>
Motivation: Evolving data 환경에서 방법을 평가할 수 있는 도구의 필요성.

Method: 구조적 인과 모델(SCM)을 기반으로 하여, 데이터 스트림을 생성하고, 인과 모델링을 통해 간헐적인 변동을 모델링한다.

Result: 분포 변화 이벤트 후에 분류기의 정확도가 떨어지고, 이후 점진적으로 회복하는 경향이 나타났다.

Conclusion: 생성기는 변화 시뮬레이션에 효과적임을 확인했으며, 이 프레임워크는 GitHub에 공개되었다.

Abstract: This work presents Causal Drift Generator (CaDrift), a time-dependent synthetic data generator framework based on Structural Causal Models (SCMs). The framework produces a virtually infinite combination of data streams with controlled shift events and time-dependent data, making it a tool to evaluate methods under evolving data. CaDrift synthesizes various distributional and covariate shifts by drifting mapping functions of the SCM, which change underlying cause-and-effect relationships between features and the target. In addition, CaDrift models occasional perturbations by leveraging interventions in causal modeling. Experimental results show that, after distributional shift events, the accuracy of classifiers tends to drop, followed by a gradual retrieval, confirming the generator's effectiveness in simulating shifts. The framework has been made available on GitHub.

</details>


### [21] [Quantitative Approximation Rates for Group Equivariant Learning](https://arxiv.org/abs/2602.20370)
*Jonathan W. Siegel,Snir Hordan,Hannah Lawrence,Ali Syed,Nadav Dym*

Main category: cs.LG

TL;DR: 이 논문은 그룹 공변 학습 맥락에서 ReLU 신경망의 양적 근사 결과를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 일반적인 근사 정리를 바탕으로 그룹 공변 모델의 근사 특성을 이해하기 위한 연구의 필요성.

Method: 여러 주요 그룹 공변 및 불변 아키텍처에 대한 양적 근사 속도를 도출합니다.

Result: 동일한 크기의 ReLU MLP와 공변 아키텍처가 공변 함수에 대해 동등하게 표현력이 있음을 보입니다.

Conclusion: 공변성을 하드코딩하는 것이 이러한 모델의 표현력이나 근사력을 저하시킴이 없음을 나타냅니다.

Abstract: The universal approximation theorem establishes that neural networks can approximate any continuous function on a compact set. Later works in approximation theory provide quantitative approximation rates for ReLU networks on the class of $α$-Hölder functions $f: [0,1]^N \to \mathbb{R}$. The goal of this paper is to provide similar quantitative approximation results in the context of group equivariant learning, where the learned $α$-Hölder function is known to obey certain group symmetries. While there has been much interest in the literature in understanding the universal approximation properties of equivariant models, very few quantitative approximation results are known for equivariant models.
  In this paper, we bridge this gap by deriving quantitative approximation rates for several prominent group-equivariant and invariant architectures. The architectures that we consider include: the permutation-invariant Deep Sets architecture; the permutation-equivariant Sumformer and Transformer architectures; joint invariance to permutations and rigid motions using invariant networks based on frame averaging; and general bi-Lipschitz invariant models. Overall, we show that equally-sized ReLU MLPs and equivariant architectures are equally expressive over equivariant functions. Thus, hard-coding equivariance does not result in a loss of expressivity or approximation power in these models.

</details>


### [22] [cc-Shapley: Measuring Multivariate Feature Importance Needs Causal Context](https://arxiv.org/abs/2602.20396)
*Jörg Martin,Stefan Haufe*

Main category: cs.LG

TL;DR: 본 논문은 설명 가능한 인공지능이 기계 학습 모델을 검토할 수 있는 통찰력을 제공한다고 주장한다. 그러나 단순한 데이터 기반 방법은 잘못된 feature attribution을 초래할 수 있다. 우리는 데이터의 인과 구조를 활용한 cc-Shapley라는 수정된 계량 접근법을 제안하고, 이 방법이 collider bias로 인한 잘못된 연관성을 제거한다는 것을 이론적으로 보여준다.


<details>
  <summary>Details</summary>
Motivation: 설명 가능한 인공지능의 필요성을 강조하고, 데이터 기반 방법이 효과적이지 않음을 지적한다.

Method: 기존 Shapley 값을 수정하여 인과적 맥락을 고려한 cc-Shapley 값을 제안한다.

Result: synthetic 및 실제 데이터셋에서 Shapley 값과 cc-Shapley 값의 행동을 비교하여 연관성이 무효화되거나 반전되는 것을 관찰하였다.

Conclusion: cc-Shapley는 기존의 Shapley 값보다 더 신뢰할 수 있는 feature 중요성을 제공함을 입증한다.

Abstract: Explainable artificial intelligence promises to yield insights into relevant features, thereby enabling humans to examine and scrutinize machine learning models or even facilitating scientific discovery. Considering the widespread technique of Shapley values, we find that purely data-driven operationalization of multivariate feature importance is unsuitable for such purposes. Even for simple problems with two features, spurious associations due to collider bias and suppression arise from considering one feature only in the observational context of the other, which can lead to misinterpretations. Causal knowledge about the data-generating process is required to identify and correct such misleading feature attributions. We propose cc-Shapley (causal context Shapley), an interventional modification of conventional observational Shapley values leveraging knowledge of the data's causal structure, thereby analyzing the relevance of a feature in the causal context of the remaining features. We show theoretically that this eradicates spurious association induced by collider bias. We compare the behavior of Shapley and cc-Shapley values on various, synthetic, and real-world datasets. We observe nullification or reversal of associations compared to univariate feature importance when moving from observational to cc-Shapley.

</details>


### [23] [Sample-efficient evidence estimation of score based priors for model selection](https://arxiv.org/abs/2602.20549)
*Frederic Wang,Katherine L. Bouman*

Main category: cs.LG

TL;DR: 이 논문은 데이터 기반의 확산 모델을 이용하여 역 문제를 해결할 때 모델 증거 추정을 위한 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비슷한 역 문제를 해결하기 위해 일관된 선행 분포를 선택하는 것이 중요하다.

Method: 우리는 후방 샘플링 방법의 시간 주변을 통합하여 확산 선행 분포의 모델 증거를 추정하는 	extit{method}라는 방법을 제안한다.

Result: 우리의 추정기는 분석적으로 계산 가능한 경우 모델 증거와 일치하며, 올바른 확산 모델을 선택하고 다양한 비선형 역 문제에서 선행 미스핏을 진단할 수 있다.

Conclusion: 이 방법은 실제 블랙홀 이미징 문제를 포함한 고도로 비정형적인 역 문제를 해결하는 데 유용하다.

Abstract: The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.

</details>


### [24] [GENSR: Symbolic Regression Based in Equation Generative Space](https://arxiv.org/abs/2602.20557)
*Qian Li,Yuxiao Hu,Juncheng Liu,Yuntian Chen*

Main category: cs.LG

TL;DR: GenSR은 심볼릭 회귀(SR)를 위한 새로운 프레임워크로, 데이터의 숨겨진 방정식을 밝혀내기 위해 생성 잠재 공간을 기반으로 한다.


<details>
  <summary>Details</summary>
Motivation: SR 방법이 이산 방정식 공간 내에서 검색하여 적합 오류 피드백이 노이즈가 많기 때문에, 구조적 수정이 정량적 행동과 잘 일치하지 않는 문제를 해결하기 위해 GenSR을 제안한다.

Method: GenSR은 먼저 이중 분기조건부 변이오토인코더(CVAE)를 사전 훈련하여 심볼릭 방정식을 생성 잠재 공간으로 재매개변수화하고, 구조적 '지도'를 제공한다. 추론 시, CVAE는 입력 데이터를 잠재 공간의 유망 지역으로 대략적으로 로컬화하고, 수정된 CMA-ES가 후보 지역을 세밀하게 다듬는다.

Result: GenSR을 통해 예측 정확도, 표현의 간결성, 계산 효율성이 조화롭게 최적화되며, 노이즈에 대해 강인성을 유지한다.

Conclusion: Bayesian 관점에서 GenSR은 SR 작업을 조건부 분포 최대화로 재구성하며, CVAE 훈련은 ELBO를 통해 이 목표를 달성한다.

Abstract: Symbolic Regression (SR) tries to reveal the hidden equations behind observed data. However, most methods search within a discrete equation space, where the structural modifications of equations rarely align with their numerical behavior, leaving fitting error feedback too noisy to guide exploration. To address this challenge, we propose GenSR, a generative latent space-based SR framework following the `map construction -> coarse localization -> fine search'' paradigm. Specifically, GenSR first pretrains a dual-branch Conditional Variational Autoencoder (CVAE) to reparameterize symbolic equations into a generative latent space with symbolic continuity and local numerical smoothness. This space can be regarded as a well-structured `map'' of the equation space, providing directional signals for search. At inference, the CVAE coarsely localizes the input data to promising regions in the latent space. Then, a modified CMA-ES refines the candidate region, leveraging smooth latent gradients. From a Bayesian perspective, GenSR reframes the SR task as maximizing the conditional distribution $p(\mathrm{Equ.} \mid \mathrm{Num.})$, with CVAE training achieving this objective through the Evidence Lower Bound (ELBO). This new perspective provides a theoretical guarantee for the effectiveness of GenSR. Extensive experiments show that GenSR jointly optimizes predictive accuracy, expression simplicity, and computational efficiency, while remaining robust under noise.

</details>


### [25] [Sparse Bayesian Deep Functional Learning with Structured Region Selection](https://arxiv.org/abs/2602.20651)
*Xiaoxian Zhu,Yingmeng Li,Shuangge Ma,Mengyun Wu*

Main category: cs.LG

TL;DR: sBayFDNN은 복잡한 비선형 관계를 포착하고 해석 가능한 영향 영역 선택을 가능하게 하는 희소 베이지안 기능 심층 신경망이다.


<details>
  <summary>Details</summary>
Motivation: 현대 응용 프로그램에서 복잡하고 지속적으로 구조화된 데이터의 분석 필요성.

Method: 희소 베이지안 기능 심층 신경망(sBayFDNN)을 제안하며, 심층 베이지안 구조를 통해 적응형 기능 임베딩을 학습하고 해석 가능한 사전 구조를 통해 영향력이 있는 도메인을 선택할 수 있게 한다.

Result: sBayFDNN의 이론적 근거를 제공하며, 다양한 시뮬레이션과 실제 연구를 통해 sBayFDNN의 효과성과 우수성을 입증한다.

Conclusion: sBayFDNN은 기존 방법을 넘어 복잡한 의존성을 인식하고 기능적으로 의미 있는 영역을 보다 정확하게 식별하는 데 뛰어나다.

Abstract: In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.

</details>


### [26] [High-Dimensional Robust Mean Estimation with Untrusted Batches](https://arxiv.org/abs/2602.20698)
*Maryam Aliakbarpour,Vladimir Braverman,Yuhan Liu,Junze Yin*

Main category: cs.LG

TL;DR: 본 논문에서는 통계적으로 이질적이며 잠재적으로 악의적일 수 있는 사용자들이 제공하는 데이터를 바탕으로 고차원 평균 추정을 위한 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: N명의 사용자가 제공하는 데이터 배치를 통해 진짜 분포 P의 평균 μ를 회복하는 과제를 다룹니다.

Method: 우리는 고차원 환경에서 두 가지 변형을 통해 연속적인 편차를 다룹니다: 좋은 배치가 평균 이동 $	 	ext{	extsqrt{α}}$로부터 샘플링되거나 각 좋은 배치 내의 α 비율의 샘플이 악의적으로 손상됩니다. Sum-of-Squares 기반 알고리즘을 제공합니다.

Result: 우리의 알고리즘은 minimax-optimal 오류율 O(√(ε/n) + √(d/nN) + √α)를 달성합니다.

Conclusion: 비록 이질성 α가 본질적인 통계적 어려움을 나타내지만, 배치 구조로 인해 악의적 사용자의 영향은 1/√n의 계수로 억제됩니다.

Abstract: We study high-dimensional mean estimation in a collaborative setting where data is contributed by $N$ users in batches of size $n$. In this environment, a learner seeks to recover the mean $μ$ of a true distribution $P$ from a collection of sources that are both statistically heterogeneous and potentially malicious. We formalize this challenge through a double corruption landscape: an $\varepsilon$-fraction of users are entirely adversarial, while the remaining ``good'' users provide data from distributions that are related to $P$, but deviate by a proximity parameter $α$.
  Unlike existing work on the untrusted batch model, which typically measures this deviation via total variation distance in discrete settings, we address the continuous, high-dimensional regime under two natural variants for deviation: (1) good batches are drawn from distributions with a mean-shift of $\sqrtα$, or (2) an $α$-fraction of samples within each good batch are adversarially corrupted. In particular, the second model presents significant new challenges: in high dimensions, unlike discrete settings, even a small fraction of sample-level corruption can shift empirical means and covariances arbitrarily.
  We provide two Sum-of-Squares (SoS) based algorithms to navigate this tiered corruption. Our algorithms achieve the minimax-optimal error rate $O(\sqrt{\varepsilon/n} + \sqrt{d/nN} + \sqrtα)$, demonstrating that while heterogeneity $α$ represents an inherent statistical difficulty, the influence of adversarial users is suppressed by a factor of $1/\sqrt{n}$ due to the internal averaging afforded by the batch structure.

</details>


### [27] [Deep unfolding of MCMC kernels: scalable, modular & explainable GANs for high-dimensional posterior sampling](https://arxiv.org/abs/2602.20758)
*Jonathan Spence,Tobías I. Liaudat,Konstantinos Zygalakis,Marcelo Pereyra*

Main category: cs.LG

TL;DR: 본 논문에서는 Langevin MCMC 알고리즘에 깊은 전개를 적용하여 GAN 아키텍처를 설계하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: Bayesian 계산에서 MCMC 방법은 기본적이지만 고차원 설정에서 계산 집약적일 수 있습니다. 따라서 후방 샘플링을 위한 계산적으로 효율적인 대안이 필요합니다.

Method: Langevin MCMC 알고리즘에 깊은 전개를 적용하여 고정 단계 반복 알고리즘을 모듈형 신경망에 매핑하는 방식으로 GAN 아키텍처를 디자인합니다.

Result: 제안된 방법은 높은 샘플링 정확도와 우수한 계산 효율성을 달성합니다.

Conclusion: 제안된 접근 방식은 전통적인 MCMC 전략의 물리적 일관성, 적응성 및 해석 가능성을 유지합니다.

Abstract: Markov chain Monte Carlo (MCMC) methods are fundamental to Bayesian computation, but can be computationally intensive, especially in high-dimensional settings. Push-forward generative models, such as generative adversarial networks (GANs), variational auto-encoders and normalising flows offer a computationally efficient alternative for posterior sampling. However, push-forward models are opaque as they lack the modularity of Bayes Theorem, leading to poor generalisation with respect to changes in the likelihood function. In this work, we introduce a novel approach to GAN architecture design by applying deep unfolding to Langevin MCMC algorithms. This paradigm maps fixed-step iterative algorithms onto modular neural networks, yielding architectures that are both flexible and amenable to interpretation. Crucially, our design allows key model parameters to be specified at inference time, offering robustness to changes in the likelihood parameters. We train these unfolded samplers end-to-end using a supervised regularized Wasserstein GAN framework for posterior sampling. Through extensive Bayesian imaging experiments, we demonstrate that our proposed approach achieves high sampling accuracy and excellent computational efficiency, while retaining the physics consistency, adaptability and interpretability of classical MCMC strategies.

</details>


### [28] [PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis](https://arxiv.org/abs/2602.21046)
*Kunyu Zhang,Yanwu Yang,Jing Zhang,Xiangjie Shi,Shujian Yu*

Main category: cs.LG

TL;DR: PIME은 기능적 연결망을 모델링하여 fMRI 기반 진단의 정확도를 향상시키는 해석 가능한 프레임워크로, 구조적 섭동과 프로토타입 기반 분류를 통합하여 단순한 최소-충분 서브그래프 최적화를 수행한다.


<details>
  <summary>Details</summary>
Motivation: fMRI 기반 진단에서의 데이터셋 특정 아티팩트를 강조하는 전통적인 방법의 신뢰도 문제를 해결하고자 한다.

Method: PIME은 프로토타입-기반 분류 및 일관성 훈련을 통해 최소-충분 서브그래프 최적화를 수행하여 학습 중 구조적 섭동을 결합한다.

Result: 세 가지 벤치마크 fMRI 데이터셋에서 PIME은 최첨단 성능을 달성한다.

Conclusion: PIME은 학습된 프로토타입을 활용해 기존 신경 이미징 연구 결과와 일치하는 중요한 뇌 영역을 식별하며, 90%의 재현성과 일관된 설명을 보인다.

Abstract: Recent deep learning methods for fMRI-based diagnosis have achieved promising accuracy by modeling functional connectivity networks. However, standard approaches often struggle with noisy interactions, and conventional post-hoc attribution methods may lack reliability, potentially highlighting dataset-specific artifacts. To address these challenges, we introduce PIME, an interpretable framework that bridges intrinsic interpretability with minimal-sufficient subgraph optimization by integrating prototype-based classification and consistency training with structural perturbations during learning. This encourages a structured latent space and enables Monte Carlo Tree Search (MCTS) under a prototype-consistent objective to extract compact minimal-sufficient explanatory subgraphs post-training. Experiments on three benchmark fMRI datasets demonstrate that PIME achieves state-of-the-art performance. Furthermore, by constraining the search space via learned prototypes, PIME identifies critical brain regions that are consistent with established neuroimaging findings. Stability analysis shows 90% reproducibility and consistent explanations across atlases.

</details>


### [29] [ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning](https://arxiv.org/abs/2602.21078)
*Duowen Chen,Yan Wang*

Main category: cs.LG

TL;DR: Federated Semi-Supervised Learning (FSSL)은 부분적으로 주석이 있는 로컬 데이터를 활용하여 클라이언트 간에 글로벌 모델을 공동으로 훈련시키는 것을 목표로 한다. 이 논문에서는 외부 및 내부 이질성을 동시에 완화하는 ProxyFL이라는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: FSSL에서 데이터 이질성은 클라이언트 간 및 클라이언트 내에서 발생하는 도전적인 문제이다.

Method: ProxyFL 프레임워크는 학습 가능한 분류기 가중치를 프로시를 사용하여 로컬 및 글로벌 카테고리 분포를 시뮬레이션하는 데 중점을 둔다. 이 프레임워크는 외부로는 아웃라이어에 대해 글로벌 프로시를 최적화하고, 내부로는 긍정-부정 프로시 풀을 통해 훈련에 폐기된 샘플을 재포함한다.

Result: 우리의 실험 및 이론 분석은 FSSL에서의 성능과 수렴이 상당하다는 것을 보여준다.

Conclusion: ProxyFL은 외부 및 내부 데이터 이질성을 완화하여 FSSL의 훈련 효율성을 높인다.

Abstract: Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.

</details>


### [30] [SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards](https://arxiv.org/abs/2602.21158)
*Dengjia Zhang,Xiaoou Liu,Lu Cheng,Yaqing Wang,Kenton Murray,Hua Wei*

Main category: cs.LG

TL;DR: 이 연구에서는 불확실성을 고려한 보상을 통해 대형 언어 모델의 학습을 개선하는 SELAUR 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 멀티 스텝 의사 결정 에이전트로 점점 더 많이 사용됨에 따라, 효과적인 보상 설계는 학습을 안내하는 데 중요합니다.

Method: SELAUR는 불확실성을 보상 설계에 직접 통합하는 강화 학습 프레임워크로, 엔트로피, 최소 신뢰도, 여유 기반 메트릭을 결합하여 토큰 수준의 불확실성 추정치를 제공합니다.

Result: ALFWorld와 WebShop의 두 벤치마크 실험에서, 본 방법이 강력한 기준선에 비해 일관되게 성공률을 향상시킨다는 것을 보여줍니다.

Conclusion: 불확실성 신호가 탐색과 강건성을 향상시키는 방식을 추가 연구를 통해 입증합니다.

Abstract: Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.

</details>


### [31] [Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs](https://arxiv.org/abs/2602.21198)
*Yining Hong,Huang Huang,Manling Li,Li Fei-Fei,Jiajun Wu,Yejin Choi*

Main category: cs.LG

TL;DR: 이 논문에서는 로봇에게 고차원 작업 추론을 부여하는 Embodied LLMs의 한계를 극복하기 위해 Reflective Test-Time Planning을 도입한다.


<details>
  <summary>Details</summary>
Motivation: Embodied LLMs는 로봇에게 높은 수준의 작업 추론을 가능하게 하지만, 잘못된 점을 반성할 수 없어 반복적인 실수로 이어지는 독립적인 시도의 연속적인 전개가 된다.

Method: 이 방법은 두 가지 반성 모드를 통합하여 구성된다: 반행동 내 반성(테스트 시간 스케일링을 활용하여 실행 전에 여러 후보 행동을 생성하고 점수 매김)과 행동 후 반성(테스트 시간 훈련을 통해 실행 후 외부 반성을 기반으로 내부 반성 모델과 행동 정책을 업데이트).

Result: 새롭게 설계된 Long-Horizon Household 벤치마크와 MuJoCo Cupboard Fitting 벤치마크에서 실험을 통해 기준 모델에 대한 유의미한 성과를 보여주었다.

Conclusion: 반성 내 반성과 반성 후 반성의 보완적인 역할을 검증하는 소실 분석 또한 수행되었으며, 실제 로봇 실험을 포함한 질적 분석은 반성을 통한 행동 수정의 효과를 강조한다.

Abstract: Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [32] [OpenPort Protocol: A Security Governance Specification for AI Agent Tool Access](https://arxiv.org/abs/2602.20196)
*Genliang Zhu,Chu Wang,Ziyuan Wang,Zhida Li,Qiang Li*

Main category: cs.CR

TL;DR: AI 에이전트는 응용 프로그램 데이터 및 작업에 대한 직접적인 구조적 접근이 필요하지만, 생산 배포에서는 여전히 권한 관리 특성을 표현하고 검증하는 데 어려움을 겪고 있다. 본 논문은 안전한 서버 측 게이트웨이를 통해 응용 프로그램 도구를 노출하기 위한 OPP(OpenPort Protocol)를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 애플리케이션 데이터와 작업에 구조적으로 접근할 필요성이 증가하고 있으나, 실질적으로 필요한 권한 관리 특성을 표현하고 검증하는 데 어려움이 있다.

Method: OpenPort Protocol (OPP)은 기존 도구 생태계에 바인딩할 수 있는 모델 및 런타임에 중립적인 보안 서버 측 게이트웨이를 통한 응용 프로그램 도구 노출을 위한 거버넌스 중심의 명세를 제공한다.

Result: OpenPort는 쓰기 작업을 위한 리스크 게이팅 수명 주기를 지정하고, 시간 제한이 있는 자동 실행을 지원하며, 높은 리스크 보호장치를 적용한다.

Conclusion: 본 논문은 참조 런타임과 실행 가능한 거버넌스 도구 체인을 제시하고, 고정 릴리스 태그에서 핵심 프로파일을 평가한다.

Abstract: AI agents increasingly require direct, structured access to application data and actions, but production deployments still struggle to express and verify the governance properties that matter in practice: least-privilege authorization, controlled write execution, predictable failure handling, abuse resistance, and auditability. This paper introduces OpenPort Protocol (OPP), a governance-first specification for exposing application tools through a secure server-side gateway that is model- and runtime-neutral and can bind to existing tool ecosystems. OpenPort defines authorization-dependent discovery, stable response envelopes with machine-actionable \texttt{agent.*} reason codes, and an authorization model combining integration credentials, scoped permissions, and ABAC-style policy constraints. For write operations, OpenPort specifies a risk-gated lifecycle that defaults to draft creation and human review, supports time-bounded auto-execution under explicit policy, and enforces high-risk safeguards including preflight impact binding and idempotency. To address time-of-check/time-of-use drift in delayed approval flows, OpenPort also specifies an optional State Witness profile that revalidates execution-time preconditions and fails closed on state mismatch. Operationally, the protocol requires admission control (rate limits/quotas) with stable 429 semantics and structured audit events across allow/deny/fail paths so that client recovery and incident analysis are deterministic. We present a reference runtime and an executable governance toolchain (layered conformance profiles, negative security tests, fuzz/abuse regression, and release-gate scans) and evaluate the core profile at a pinned release tag using artifact-based, externally reproducible validation.

</details>


### [33] [Right to History: A Sovereignty Kernel for Verifiable AI Agent Execution](https://arxiv.org/abs/2602.20214)
*Jing Zhang*

Main category: cs.CR

TL;DR: 이 논문은 개인 하드웨어에서 AI 에이전트의 행동 기록을 독립적으로 검증할 수 있는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 인간을 대신해 행동하는 경우가 증가하고 있지만, 이들의 행동 기록을 변조 방지 및 독립적으로 검증 가능한 형태로 제공하는 시스템이 없다.

Method: 개인 하드웨어에서 AI 에이전트의 행동에 대한 완전하고 검증 가능한 기록을 보장하는 '역사에 대한 권리' 원칙을 제안하고, 이를 다섯 가지 시스템 불변성과 정형화된 증명을 통해 구체화한다. PunkGo라는 Rust 기반의 커널을 통해 이를 구현한다.

Result: 모든 다섯 가지 불변성이 유지됨을 적대적 테스트로 확인하며, 성능 평가 결과 평균 1.3ms 미만의 동작 지연 시간, 초당 약 400회의 동작 처리량 및 10,000개의 로그 항목에서 448바이트 Merkle 포함 증명을 보여준다.

Conclusion: AI 에이전트가 개인 하드웨어에서 동작할 때 개인이 그들의 행동을 검증할 수 있는 시스템이 필요하며, 제안된 원칙과 시스템이 이를 지원한다.

Abstract: AI agents increasingly act on behalf of humans, yet no existing system provides a tamper-evident, independently verifiable record of what they did. As regulations such as the EU AI Act begin mandating automatic logging for high-risk AI systems, this gap carries concrete consequences -- especially for agents running on personal hardware, where no centralized provider controls the log. Extending Floridi's informational rights framework from data about individuals to actions performed on their behalf, this paper proposes the Right to History: the principle that individuals are entitled to a complete, verifiable record of every AI agent action on their own hardware. The paper formalizes this principle through five system invariants with structured proof sketches, and implements it in PunkGo, a Rust sovereignty kernel that unifies RFC 6962 Merkle tree audit logs, capability-based isolation, energy-budget governance, and a human-approval mechanism. Adversarial testing confirms all five invariants hold. Performance evaluation shows sub-1.3 ms median action latency, ~400 actions/sec throughput, and 448-byte Merkle inclusion proofs at 10,000 log entries.

</details>


### [34] [The TCF doesn't really A(A)ID -- Automatic Privacy Analysis and Legal Compliance of TCF-based Android Applications](https://arxiv.org/abs/2602.20222)
*Victor Morel,Cristiana Santos,Pontus Carlsson,Joel Ahlinder,Romaric Duvignau*

Main category: cs.CR

TL;DR: 이 연구는 인기 있는 안드로이드 앱에서 TCF(투명성 및 동의 프레임워크)의 보급을 조사하고, 앱이 사용자 동의 배너 선택을 존중하는지를 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 애플리케이션에서 TCF의 적용에 대한 체계적인 연구가 부족하기 때문에 이 격차를 해소하려고 합니다.

Method: 구글 플레이 스토어에서 인기 있는 4482개의 안드로이드 앱을 자동으로 수집하고, TCF의 사용 여부와 동의 배너와의 상호작용을 분석했습니다.

Result: 576개(12.85%) 앱이 TCF를 구현하고 있으며, 이 중 15개(2.6%) 앱은 동의가 부여될 때만 사용자 선택을 저장합니다. 동의를 거부한 사용자는 앱을 실행할 때마다 동의 배너가 반복해서 표시됩니다.

Conclusion: 분석된 TCF 기반 앱의 66.2%가 합법적인 처리가치 없이 개인 데이터를 공유하고, 55.3%는 사용자와 동의 배너의 상호작용 전에 AAID를 공유하여 사전 동의 요구 사항을 위반했습니다.

Abstract: The Transparency and Consent Framework (TCF), developed by the Interactive Advertising Bureau (IAB) Europe, provides a de facto standard for requesting, recording, and managing user consent from European end-users. This framework has previously been found to infringe European data protection law and has subsequently been regularly updated. Previous research on the TCF focused exclusively on web contexts, with no attention given to its implementation in mobile applications. No work has systematically studied the privacy implications of the TCF on Android apps. To address this gap, we investigate the prevalence of the TCF in popular Android apps from the Google Play Store, and assess whether these apps respect users' consent banner choices. By scraping and downloading 4482 of the most popular Google Play Store apps on an emulated Android device, we automatically determine which apps use the TCF, automatically interact with consent banners, and analyze the apps' traffic in two different stages, passive (post choices) and active (during banner interaction and post choices).
  We found that 576 (12.85%) of the 4482 downloadable apps in our dataset implemented the TCF, and we identified potential privacy violations within this subset. In 15 (2.6%) of these apps, users' choices are stored only when consent is granted. Users who refuse consent are shown the consent banner again each time they launch the app. Network traffic analysis conducted during the passive stage reveals that 66.2% of the analyzed TCF-based apps share personal data, through the Android Advertising ID (AAID), in the absence of a lawful basis for processing. 55.3% of apps analyzed during the active stage share AAID before users interact with the apps' consent banners, violating the prior consent requirement.

</details>


### [35] [Understanding Human-AI Collaboration in Cybersecurity Competitions](https://arxiv.org/abs/2602.20446)
*Tingxuan Tang,Nicolas Janis,Kalyn Asher Montague,Kevin Eykholt,Dhilung Kirat,Youngja Park,Jiyong Jang,Adwait Nadkarni,Yue Xiao*

Main category: cs.CR

TL;DR: 본 연구는 CTF 대회에서 인간과 AI의 협업 효과와 인식을 조사하며, AI의 도움을 받는 인간 팀과 자율 AI 에이전트의 성과를 비교합니다.


<details>
  <summary>Details</summary>
Motivation: CTF 대회는 AI의 보안 작업 해결 능력을 평가하기 위한 시험대 역할을 하고 있어, 인간과 AI의 협업 가능성이 중요해지고 있습니다.

Method: 41명의 참가자를 대상으로 AI 사용 전후의 인식, 신뢰 및 기대의 변화를 질적으로 연구하고 AI 에이전트와의 협업을 분석했습니다.

Result: 팀은 대회가 진행됨에 따라 AI에게 더 많은 서브 태스크를 위임하며, AI의 자율적 프롬프트와 도구 사용이 인간 팀보다 우수한 성과를 낸 것으로 나타났습니다.

Conclusion: 본 연구는 CTF 과제 설계 및 보안 분야에서 효과적인 인간-루프 AI 시스템 구축에 대한 시사점을 제공합니다.

Abstract: Capture-the-Flag (CTF) competitions are increasingly becoming a testbed for evaluating AI capabilities at solving security tasks, due to the controlled environments and objective success criteria. Existing evaluations have focused on how successful AI is at solving CTF challenges in isolation from human CTF players. As AI usage increases in both academic and industrial settings, it is equally likely that human players may collaborate with AI agents to solve challenges. This possibility exposes a key knowledge gap: how do humans perceive AI CTF assistance; when assistance is provided, how do they collaborate and is it effective with respect to human performance; how do humans assisted by AI compare to the performance of fully autonomous AI agents on the same challenges. We address this gap with the first empirical study of AI assistance in a live, onsite CTF. In a study with 41 participants, we qualitatively study (i) how participants' perception, trust, and expectations shift before versus after hands-on AI use, and (ii) how participants collaborate with an instrumented AI agent. Moreover, we also (iii) benchmark four autonomous AI agents on the same fresh challenge set to compare outcomes with human teams and analyze agent trajectories. We find that, as the competition progresses, teams increasingly delegate larger subtasks to the AI, giving it more agency. Interestingly, CTF challenges solving rates are often constrained not by model's reasoning capabilities, but rather by the human players: ineffective prompting and poor context specification become the primary bottleneck. Remarkably, autonomous agents that self-direct their prompting and tool use bypass this bottleneck and outperform most human teams, coming in second overall in the competition. We conclude with implications for the future design of CTF challenges and for building effective human-in-the-loop AI systems for security.

</details>


### [36] [AdapTools: Adaptive Tool-based Indirect Prompt Injection Attacks on Agentic LLMs](https://arxiv.org/abs/2602.20720)
*Che Wang,Jiaming Zhang,Ziqi Zhang,Zijie Wang,Yinghui Wang,Jianbo Gao,Tao Wei,Zhong Chen,Wei Yang Bryan Lim*

Main category: cs.CR

TL;DR: AdapTools라는 새로운 적응형 IPI 공격 프레임워크를 통해 보안 취약점을 해결하고, 시스템 유용성을 감소시키지 않으면서 공격 성공률을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 현대 AI 에이전트의 빠르게 진화하는 특성을 해결하기 위해, 기존의 정적 패턴에 의존하는 공격 방법을 발전시키고자 한다.

Method: AdapTools는 두 가지 주요 구성 요소로 이루어져 있으며, 적응형 공격 전략을 구축하고, 임무 관련 방어를 우회할 수 있는 은밀한 도구를 식별한다.

Result: AdapTools는 공격 성공률을 2.13배 향상시키고, 시스템 유용성을 1.78배 감소시킨다.

Conclusion: 이 프레임워크는 IPI 공격에 대한 이해를 증진시키며, 향후 연구를 위한 유용한 참고자료가 된다.

Abstract: The integration of external data services (e.g., Model Context Protocol, MCP) has made large language model-based agents increasingly powerful for complex task execution. However, this advancement introduces critical security vulnerabilities, particularly indirect prompt injection (IPI) attacks. Existing attack methods are limited by their reliance on static patterns and evaluation on simple language models, failing to address the fast-evolving nature of modern AI agents. We introduce AdapTools, a novel adaptive IPI attack framework that selects stealthier attack tools and generates adaptive attack prompts to create a rigorous security evaluation environment. Our approach comprises two key components: (1) Adaptive Attack Strategy Construction, which develops transferable adversarial strategies for prompt optimization, and (2) Attack Enhancement, which identifies stealthy tools capable of circumventing task-relevance defenses. Comprehensive experimental evaluation shows that AdapTools achieves a 2.13 times improvement in attack success rate while degrading system utility by a factor of 1.78. Notably, the framework maintains its effectiveness even against state-of-the-art defense mechanisms. Our method advances the understanding of IPI attacks and provides a useful reference for future research.

</details>


### [37] [SoK: Agentic Skills -- Beyond Tool Use in LLM Agents](https://arxiv.org/abs/2602.20867)
*Yanna Jiang,Delong Li,Haiyu Deng,Baihe Ma,Xu Wang,Qin Wang,Guangsheng Yu*

Main category: cs.CR

TL;DR: 이 논문은 에이전트 시스템이 신뢰성 있게 장기 워크플로우를 실행하기 위해 재사용 가능한 절차 능력, 즉 에이전트 기술에 점점 더 의존하고 있음을 보여줍니다. 또한, 기술 레이어의 전체 주기를 매핑하고, 기술의 패키징 및 실행 방식에 대한 일곱 가지 디자인 패턴과 기술의 유형 및 범위를 설명하는 두 가지 상보적인 분류표를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 시스템이 신뢰성 있게 장기적인 워크플로우를 실행하기 위해 에이전트 기술과 같은 재사용 가능한 절차 능력을 점점 더 의존하고 있다는 필요성이 제기되었습니다.

Method: 기술 레이어를 전체 생애 주기(발견, 연습, 증류, 저장, 구성, 평가 및 업데이트)를 통해 매핑하고 일곱 가지 디자인 패턴과 기술의 유형 및 범위 분류표를 도입했습니다.

Result: 자체적으로 생성된 기술은 에이전트의 성공률을 저하시킬 수 있지만, 큐레이션된 기술은 성공률을 상당히 향상시킬 수 있다는 최근의 벤치마크 증거를 바탕으로 결정론적 평가 접근 방식에 대한 조사를 진행했습니다.

Conclusion: 현실 세계의 자율 에이전트를 위한 robust하고 검증 가능하며 인증 가능한 기술을 개발하기 위한 열린 도전 과제가 제기되었습니다.

Abstract: Agentic systems increasingly rely on reusable procedural capabilities, \textit{a.k.a., agentic skills}, to execute long-horizon workflows reliably. These capabilities are callable modules that package procedural knowledge with explicit applicability conditions, execution policies, termination criteria, and reusable interfaces. Unlike one-off plans or atomic tool calls, skills operate (and often do well) across tasks.
  This paper maps the skill layer across the full lifecycle (discovery, practice, distillation, storage, composition, evaluation, and update) and introduces two complementary taxonomies. The first is a system-level set of \textbf{seven design patterns} capturing how skills are packaged and executed in practice, from metadata-driven progressive disclosure and executable code skills to self-evolving libraries and marketplace distribution. The second is an orthogonal \textbf{representation $\times$ scope} taxonomy describing what skills \emph{are} (natural language, code, policy, hybrid) and what environments they operate over (web, OS, software engineering, robotics).
  We analyze the security and governance implications of skill-based agents, covering supply-chain risks, prompt injection via skill payloads, and trust-tiered execution, grounded by a case study of the ClawHavoc campaign in which nearly 1{,}200 malicious skills infiltrated a major agent marketplace, exfiltrating API keys, cryptocurrency wallets, and browser credentials at scale. We further survey deterministic evaluation approaches, anchored by recent benchmark evidence that curated skills can substantially improve agent success rates while self-generated skills may degrade them. We conclude with open challenges toward robust, verifiable, and certifiable skills for real-world autonomous agents.

</details>
