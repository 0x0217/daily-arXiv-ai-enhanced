<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 16]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing](https://arxiv.org/abs/2510.17843)
*Zongze Wu,Yani Guo,Churong Liang,Runnan Li*

Main category: cs.LG

TL;DR: GRETEL은 도구 검색의 의미-기능 간극을 해결하기 위해 실행 기반 검증을 도입하여 능률을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 발전에도 불구하고 도구 검색은 여전히 의미적 유사성에 의존하여 기능적 실행 가능성을 포착하지 못하는 한계가 있습니다.

Method: GRETEL은 의미적으로 검색된 후보를 샌드박스 환경에서 계획-실행-평가 사이클을 통해 처리하여 실행 기반의 증거를 생성합니다.

Result: ToolBench 벤치마크에서 통과율(10에서)은 0.690에서 0.826로, 재현율(10에서)은 0.841에서 0.867로, NDCG(10에서)은 0.807에서 0.857로 증가했습니다.

Conclusion: 실행 기반 검증이 도구 선택에 있어 의미적 유사성보다 더 신뢰할 수 있는 기반을 제공하여 현실 세계 애플리케이션에서 더 강력한 에이전트 성능을 가능하게 함을 입증했습니다.

Abstract: Despite remarkable advances in Large Language Model capabilities, tool
retrieval for agent-based systems remains fundamentally limited by reliance on
semantic similarity, which fails to capture functional viability. Current
methods often retrieve textually relevant but functionally inoperative tools
due to parameter mismatches, authentication failures, and execution
constraints--a phenomenon we term the semantic-functional gap. We introduce
GRETEL, to address this gap through systematic empirical validation. GRETEL
implements an agentic workflow that processes semantically retrieved candidates
through sandboxed plan-execute-evaluate cycles, generating execution-grounded
evidence to distinguish truly functional tools from merely descriptive matches.
Our comprehensive evaluation on the ToolBench benchmark demonstrates
substantial improvements across all metrics: Pass Rate (at 10) increases from
0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10)
rises from 0.807 to 0.857.. These results establish that execution-based
validation provides a more reliable foundation for tool selection than semantic
similarity alone, enabling more robust agent performance in real-world
applications.

</details>


### [2] [EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning](https://arxiv.org/abs/2510.17928)
*He Du,Bowen Li,Aijun Yang,Siyang He,Qipeng Guo,Dacheng Tao*

Main category: cs.LG

TL;DR: 이 논문은 일반화 가능한 합성 검증 데이터를 생성하는 새로운 프레임워크를 소개하며, 이는 강화 학습과 모델 증류 훈련에서 효과적임을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 있는 검증 가능한 데이터는 현대 언어 모델의 성능 향상에 중요한 역할을 하며, 안정적인 강화 학습과 효과적인 증류를 가능하게 한다.

Method: 우리는 최소한의 시드 감독을 통해 문제, 다양한 후보 솔루션 및 검증 아티팩트를 공동으로 합성하고, 일관성 기반 평가자를 통해 전략을 발견하는 진화적, 과제 비관적 데이터 합성 프레임워크를 소개한다.

Result: 제안된 접근 방식이 RLVR 및 모델 증류 훈련 패러다임 모두에서 효과적임을 실험적으로 입증하였다.

Conclusion: 우리의 합성 데이터로 훈련하면 LiveCodeBench와 AgentBench-OS 작업에서 상당한 개선이 나타나며, 프레임워크의 강력한 일반화를 강조한다.

Abstract: Reliable verifiable data has become a key driver of capability gains in
modern language models, enabling stable reinforcement learning with verifiable
rewards and effective distillation that transfers competence across math,
coding, and agentic tasks. Yet constructing generalizable synthetic verifiable
data remains difficult due to hallucination-prone generation, and weak or
trivial verification artifacts that fail to separate strong from weak
solutions. Existing approaches often rely on task-specific heuristics or
post-hoc filters that do not transfer across domains and lack a principled,
universal evaluator of verifiability. In this work, we introduce an
evolutionary, task-agnostic, strategy-guided, executably-checkable data
synthesis framework that, from minimal seed supervision, jointly synthesizes
problems, diverse candidate solutions, and verification artifacts, and
iteratively discovers strategies via a consistency-based evaluator that
enforces agreement between human-annotated and strategy-induced checks. This
pipeline upgrades filtering into principled synthesis: it reliably assembles
coherent, verifiable training instances and generalizes without domain-specific
rules. Our experiments demonstrate the effectiveness of the proposed approach
under both RLVR and model distillation training paradigms. The results show
that training with our synthesized data yields significant improvements on both
the LiveCodeBench and AgentBench-OS tasks, highlighting the robust
generalization of our framework.

</details>


### [3] [Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity](https://arxiv.org/abs/2510.18037)
*Ziyu Lu,Anna J. Li,Alexander E. Ladd,Pascha Matveev,Aditya Deole,Eric Shea-Brown,J. Nathan Kutz,Nicholas A. Steinmetz*

Main category: cs.LG

TL;DR: 딥러닝 모델들이 고전 통계 모델보다 신경 활동 예측에서 더 나은 성능을 보였다.


<details>
  <summary>Details</summary>
Motivation: 신경 활동 예측은 신경 시스템의 이해와 폐쇄 루프 제어를 가능하게 하는 데 중요하다.

Method: 여덟 개의 확률적 딥러닝 모델을 체계적으로 평가하고, 이를 네 가지 고전 통계 모델 및 두 개의 기준 방법과 비교하였다.

Result: 여러 딥러닝 모델이 고전적 접근 방식을 일관되게 초월하였고, 가장 좋은 모델은 최대 1.5초 미래 예측을 정보적으로 수행하였다.

Conclusion: 이 연구 결과는 향후 제어 응용 프로그램에 대한 통찰을 제공하고, 신경 활동의 내재적 시간 구조를 조사하는 새로운 경로를 열어준다.

Abstract: Neural activity forecasting is central to understanding neural systems and
enabling closed-loop control. While deep learning has recently advanced the
state-of-the-art in the time series forecasting literature, its application to
neural activity forecasting remains limited. To bridge this gap, we
systematically evaluated eight probabilistic deep learning models, including
two foundation models, that have demonstrated strong performance on general
forecasting benchmarks. We compared them against four classical statistical
models and two baseline methods on spontaneous neural activity recorded from
mouse cortex via widefield imaging. Across prediction horizons, several deep
learning models consistently outperformed classical approaches, with the best
model producing informative forecasts up to 1.5 seconds into the future. Our
findings point toward future control applications and open new avenues for
probing the intrinsic temporal structure of neural activity.

</details>


### [4] [SPACeR: Self-Play Anchoring with Centralized Reference Models](https://arxiv.org/abs/2510.18060)
*Wei-Jer Chang,Akshay Rangesh,Kevin Joseph,Matthew Strong,Masayoshi Tomizuka,Yihan Hu,Wei Zhan*

Main category: cs.LG

TL;DR: SPACeR는 선행 학습된 토큰화된 자가 회귀 모션 모델을 중앙집중식 참조 정책으로 활용하여 분산 자가 플레이를 유도하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 자율주행차 개발은 안전성, 효율성뿐만 아니라 사회적으로 인식 가능한 현실적이고 인간과 유사한 행동이 필요하다.

Method: SPACeR 프레임워크는 선행 학습된 토큰화된 자가 회귀 모션 모델을 참조 정책으로 활용하여 분산 자가 플레이를 유도한다.

Result: Waymo Sim Agents Challenge에서 우리의 방법은 모방 학습 정책과 경쟁력 있는 성능을 달성했으며, 추론 속도는 최대 10배 빠르고, 파라미터 크기는 50배 작다.

Conclusion: 우리의 시뮬레이션 에이전트는 빠르고 확장 가능한 교통 시뮬레이션을 통해 계획자 품질을 효과적으로 측정할 수 있으며, 자율주행 정책을 테스트하기 위한 새로운 패러다임을 확립한다.

Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency,
but also realistic, human-like behaviors that are socially aware and
predictable. Achieving this requires sim agent policies that are human-like,
fast, and scalable in multi-agent settings. Recent progress in imitation
learning with large diffusion-based or tokenized models has shown that
behaviors can be captured directly from human driving data, producing realistic
policies. However, these models are computationally expensive, slow during
inference, and struggle to adapt in reactive, closed-loop scenarios. In
contrast, self-play reinforcement learning (RL) scales efficiently and
naturally captures multi-agent interactions, but it often relies on heuristics
and reward shaping, and the resulting policies can diverge from human norms. We
propose SPACeR, a framework that leverages a pretrained tokenized
autoregressive motion model as a centralized reference policy to guide
decentralized self-play. The reference model provides likelihood rewards and KL
divergence, anchoring policies to the human driving distribution while
preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our
method achieves competitive performance with imitation-learned policies while
being up to 10x faster at inference and 50x smaller in parameter size than
large generative models. In addition, we demonstrate in closed-loop ego
planning evaluation tasks that our sim agents can effectively measure planner
quality with fast and scalable traffic simulation, establishing a new paradigm
for testing autonomous driving policies.

</details>


### [5] [Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria](https://arxiv.org/abs/2510.18183)
*Eason Yu,Tzu Hao Liu,Yunke Wang,Clément L. Canonne,Nguyen H. Tran,Chang Xu*

Main category: cs.LG

TL;DR: 이 논문은 불완전 정보 게임에서의 내쉬 균형을 찾기 위한 새로운 접근 방식을 제안하며, 특히 내쉬 정책 그래디언트(NashPG)라는 실용적인 알고리즘을 개발한다.


<details>
  <summary>Details</summary>
Motivation: 불완전한 정보 게임에서 내쉬 균형을 찾는 것은 다중 에이전트 강화 학습에서 핵심 도전 과제로 여겨진다.

Method: 기존의 정규화 기반 방법과 달리, 이 논문에서는 정규화 강도를 큰 값으로 고정하고 점진적으로 참조 정책을 조정하여 수렴을 달성하는 접근 방식을 제안한다.

Result: 본 이론적 결과는 두 플레이어 제로섬 게임에서 이 절차가 엄격한 단조 개선과 정확한 내쉬 균형으로의 수렴을 보장함을 보여준다.

Conclusion: NashPG는 정책 그래디언트 방법의 일반화 가능성을 유지하면서 현재 및 참조 정책에만 의존하여 성능을 발휘한다.

Abstract: Finding Nash equilibria in imperfect-information games remains a central
challenge in multi-agent reinforcement learning. While regularization-based
methods have recently achieved last-iteration convergence to a regularized
equilibrium, they require the regularization strength to shrink toward zero to
approximate a Nash equilibrium, often leading to unstable learning in practice.
Instead, we fix the regularization strength at a large value for robustness and
achieve convergence by iteratively refining the reference policy. Our main
theoretical result shows that this procedure guarantees strictly monotonic
improvement and convergence to an exact Nash equilibrium in two-player zero-sum
games, without requiring a uniqueness assumption. Building on this framework,
we develop a practical algorithm, Nash Policy Gradient (NashPG), which
preserves the generalizability of policy gradient methods while relying solely
on the current and reference policies. Empirically, NashPG achieves comparable
or lower exploitability than prior model-free methods on classic benchmark
games and scales to large domains such as Battleship and No-Limit Texas
Hold'em, where NashPG consistently attains higher Elo ratings.

</details>


### [6] [ActivationReasoning: Logical Reasoning in Latent Activation Spaces](https://arxiv.org/abs/2510.18184)
*Lukas Helff,Ruben Härle,Wolfgang Stammer,Felix Friedrich,Manuel Brack,Antonia Wüst,Hikaru Shindo,Patrick Schramowski,Kristian Kersting*

Main category: cs.LG

TL;DR: 대형 언어 모델의 내부 추론을 향상시키기 위한 프레임워크인 ActivationReasoning(AR)을 제안하며, 이 프레임워크는 잠재 공간에 논리적 추론을 내장하여 결과적으로 더 나은 투명성과 신뢰할 수 있는 제어를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)의 내부 추론 방식이 불투명하고 제어하기 어렵다는 문제를 해결하기 위해.

Method: ActivationReasoning(AR)라는 프레임워크를 통해 세 가지 단계에서 작업을 수행한다: 1) 잠재 표현 찾기, 2) 명제 활성화, 3) 논리적 추론.

Result: AR은 다단계 추론 및 다양한 작업에서의 강인성을 평가했으며, 모든 작업에서 robust한 성능을 보였다.

Conclusion: 논리적 구조를 잠재 활성화에 기반하는 것이 투명성을 개선할 뿐만 아니라 구조화된 추론, 신뢰할 수 있는 제어 및 바람직한 행동과의 정렬을 가능하게 함을 보여준다.

Abstract: Large language models (LLMs) excel at generating fluent text, but their
internal reasoning remains opaque and difficult to control. Sparse autoencoders
(SAEs) make hidden activations more interpretable by exposing latent features
that often align with human concepts. Yet, these features are fragile and
passive, offering no mechanism for systematic reasoning or model control. To
address this, we introduce ActivationReasoning (AR), a framework that embeds
explicit logical reasoning into the latent space of LLMs. It proceeds in three
stages: (1) Finding latent representations, first latent concept
representations are identified (e.g., via SAEs) and organized into a
dictionary; (2) Activating propositions, at inference time AR detects
activating concepts and maps them to logical propositions; and (3)Logical
reasoning, applying logical rules over these propositions to infer higher-order
structures, compose new concepts, and steer model behavior. We evaluate AR on
multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept
cues (Rail2Country), reasoning over natural and diverse language (ProverQA),
and context-sensitive safety (BeaverTails). Across all tasks, AR scales
robustly with reasoning complexity, generalizes to abstract and
context-sensitive tasks, and transfers across model backbones. These results
demonstrate that grounding logical structure in latent activations not only
improves transparency but also enables structured reasoning, reliable control,
and alignment with desired behaviors, providing a path toward more reliable and
auditable AI.

</details>


### [7] [Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs](https://arxiv.org/abs/2510.18225)
*Xueyao Zhang,Bo Yang,Zhiwen Yu,Xuelin Cao,Wei Xiang,Bin Guo,Liang Wang,Billy Pik Lik Lau,George C. Alexandropoulos,Jun Luo,Mérouane Debbah,Zhu Han,Chau Yuen*

Main category: cs.LG

TL;DR: 이 논문은 자율 수중 차량(AUV)을 이용한 수중 협력 목표 탐지에 대해 연구하며, 협력 효율성과 통신 비밀성 간의 중요한 균형을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 자율 수중 차량을 이용한 효율적이고 안전한 수중 비밀 통신 작업 수행을 위한 연구 필요.

Method: 연합 궤적 및 전력 제어 최적화 문제를 수립하고, 계층적 행동 관리 프레임워크를 도입하여 해결한다.

Result: 마스터 AUV가 에이전트 선택 과정을 마르코프 결정 과정으로 모델링하고, 각 선택된 에이전트가 분산 의사결정 과정으로 동작하여 적응형 비밀 협력을 가능하게 한다.

Conclusion: 이 연구는 여러 AUV의 효율적이고 안전한 운영을 위한 이론적 통찰력과 실용적 솔루션을 제공하여 수중 비밀 통신 작업의 실행에 중요한 의미를 가진다.

Abstract: This paper investigates underwater cooperative target detection using
autonomous underwater vehicles (AUVs), with a focus on the critical trade-off
between cooperation efficiency and communication covertness. To tackle this
challenge, we first formulate a joint trajectory and power control optimization
problem, and then present an innovative hierarchical action management
framework to solve it. According to the hierarchical formulation, at the macro
level, the master AUV models the agent selection process as a Markov decision
process and deploys the proximal policy optimization algorithm for strategic
task allocation. At the micro level, each selected agent's decentralized
decision-making is modeled as a partially observable Markov decision process,
and a multi-agent proximal policy optimization algorithm is used to dynamically
adjust its trajectory and transmission power based on its local observations.
Under the centralized training and decentralized execution paradigm, our target
detection framework enables adaptive covert cooperation while satisfying both
energy and mobility constraints. By comprehensively modeling the considered
system, the involved signals and tasks, as well as energy consumption,
theoretical insights and practical solutions for the efficient and secure
operation of multiple AUVs are provided, offering significant implications for
the execution of underwater covert communication tasks.

</details>


### [8] [Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment](https://arxiv.org/abs/2510.18240)
*Haobin Li,Yijie Lin,Peng Hu,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 멀티모달 엔티티 정렬(MMEA)은 다양한 방식의 속성을 가진 이질적인 멀티모달 지식 그래프(MMKG)에서 동등한 엔티티를 식별하는 것을 목표로 한다. 하지만 기존 방법은 실제 MMKG에서 전문가 주석에 의존하기 때문에 자주 발생하는 오류를 고려하지 않는다.


<details>
  <summary>Details</summary>
Motivation: 현재 멀티모달 엔티티 정렬에서 자주 발생하는 오류를 해결할 필요가 있다.

Method: DNC 문제를 다루기 위해 우리는 RULE이라는 강력한 MMEA 프레임워크를 제안한다. RULE은 이중 원칙을 통해 엔티티 간 및 그래프 간의 신뢰성을 평가하고, 신뢰성을 활용하여 속성 융합 중의 노이즈 영향을 줄인다.

Result: 다섯 개의 벤치마크에서 실험을 통해 DNC 문제에 대한 우리의 방법의 효과성을 확인하였다.

Conclusion: 본 연구는 더 정확한 동등 엔티티 식별을 보장하기 위해 속성 간 연결을 발견하는 추론 모듈을 통합하였다.

Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities
across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is
described by attributes from various modalities. Existing methods typically
assume that both intra-entity and inter-graph correspondences are faultless,
which is often violated in real-world MMKGs due to the reliance on expert
annotations. In this paper, we reveal and study a highly practical yet
under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC).
DNC refers to misalignments in both intra-entity (entity-attribute) and
inter-graph (entity-entity and attribute-attribute) correspondences. To address
the DNC problem, we propose a robust MMEA framework termed RULE. RULE first
estimates the reliability of both intra-entity and inter-graph correspondences
via a dedicated two-fold principle. Leveraging the estimated reliabilities,
RULE mitigates the negative impact of intra-entity noise during attribute
fusion and prevents overfitting to noisy inter-graph correspondences during
inter-graph discrepancy elimination. Beyond the training-time designs, RULE
further incorporates a correspondence reasoning module that uncovers the
underlying attribute-attribute connection across graphs, guaranteeing more
accurate equivalent entity identification. Extensive experiments on five
benchmarks verify the effectiveness of our method against the DNC compared with
seven state-of-the-art methods.The code is available at
\href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}

</details>


### [9] [NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective](https://arxiv.org/abs/2510.18258)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Junchi Yan*

Main category: cs.LG

TL;DR: 본 논문은 신경 접선 핵(NTK) 이론을 활용하여 멀티태스크 학습(MTL)에서의 훈련 동역학을 분석하고, 새로운 MTL 방법인 NTKMTL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: MTL에서의 작업 불균형 문제를 해결하고자 합니다.

Method: 신경 접선 핵 이론을 활용하여 MTL의 훈련 동역학을 분석하고, 스펙트럴 분석을 통해 여러 작업의 수렴 속도를 조절하는 NTKMTL 방법을 제안합니다.

Result: 저희 방법은 여러 벤치마크에서 최첨단 성능을 달성합니다.

Conclusion: NTKMTL 및 NTKMTL-SR 모델은 경쟁력 있는 성능을 유지하면서 훈련 효율성을 달성합니다.

Abstract: Multi-Task Learning (MTL) enables a single model to learn multiple tasks
simultaneously, leveraging knowledge transfer among tasks for enhanced
generalization, and has been widely applied across various domains. However,
task imbalance remains a major challenge in MTL. Although balancing the
convergence speeds of different tasks is an effective approach to address this
issue, it is highly challenging to accurately characterize the training
dynamics and convergence speeds of multiple tasks within the complex MTL
system. To this end, we attempt to analyze the training dynamics in MTL by
leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method,
NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt
spectral analysis to balance the convergence speeds of multiple tasks, thereby
mitigating task imbalance. Based on the approximation via shared
representation, we further propose NTKMTL-SR, achieving training efficiency
while maintaining competitive performance. Extensive experiments demonstrate
that our methods achieve state-of-the-art performance across a wide range of
benchmarks, including both multi-task supervised learning and multi-task
reinforcement learning. Source code is available at
https://github.com/jianke0604/NTKMTL.

</details>


### [10] [Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization](https://arxiv.org/abs/2510.18410)
*Adeel Safder*

Main category: cs.LG

TL;DR: MAGDrop는 DNN의 과적합 문제를 해결하기 위한 새로운 정규화 방법이다.


<details>
  <summary>Details</summary>
Motivation: DNN의 높은 용량으로 인한 과적합 문제 해결.

Method: 현재 그래디언트와 축적된 모멘텀에 기반하여 동적으로 드롭아웃 비율을 조정하는 방식으로 MAGDrop을 적용.

Result: MAGDrop은 표준 드롭아웃 및 적응형 그래디언트 정규화보다 MNIST와 CIFAR-10에서 각각 1-2% 더 높은 정확도를 달성하였다.

Conclusion: 이 연구는 DNN 일반화를 향상시키는 강력한 프레임워크를 제공한다.

Abstract: Deep neural networks (DNNs) achieve remarkable performance but often suffer
from overfitting due to their high capacity. We introduce Momentum-Adaptive
Gradient Dropout (MAGDrop), a novel regularization method that dynamically
adjusts dropout rates on activations based on current gradients and accumulated
momentum, enhancing stability in non-convex optimization landscapes. To
theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes
generalization bound that accounts for its adaptive nature, achieving up to 20%
sharper bounds compared to standard approaches by leveraging momentum-driven
perturbation control. Empirically, the activation-based MAGDrop outperforms
baseline regularization techniques, including standard dropout and adaptive
gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and
CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively.
Our work bridges theoretical insights and practical advancements, offering a
robust framework for enhancing DNN generalization suitable for high-stakes
applications.

</details>


### [11] [Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation](https://arxiv.org/abs/2510.18478)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: 강화 학습(RL) 에이전트의 안전한 탐색 보장은 실제 시스템에 배포하는 데 매우 중요하다. 기존의 접근 방식은 안전을 엄격히 유지하려고 하면 작업 성능이 저하되는 균형을 잘 이루지 못하고, 보상을 우선시하면 안전 제약을 자주 위반하게 되는 경향이 있다. 우리는 비확실성 안전 비평가(USC)라는 새로운 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: RL 에이전트의 안전한 탐색이 실제 시스템에서 배포하기 위해 필수적이라는 점.

Method: 비확실한 비용 영역에서 보수성을 집중시키고 안전한 영역에서 날카로운 기울기를 유지하는 비확실성 인식 조정 및 정제 과정을 비평가 훈련에 통합한다.

Result: USC는 안전 위반을 약 40% 줄이며, 경쟁력 있는 보상 또는 그 이상의 보상을 유지하고, 예측된 비용 기울기와 실제 비용 기울기 간의 오류를 약 83% 줄였다.

Conclusion: 안전성과 성능 간의 기존의 균형을 깨고 유연한 안전 RL을 위한 길을 열었다.

Abstract: Ensuring the safe exploration of reinforcement learning (RL) agents is
critical for deployment in real-world systems. Yet existing approaches struggle
to strike the right balance: methods that tightly enforce safety often cripple
task performance, while those that prioritize reward leave safety constraints
frequently violated, producing diffuse cost landscapes that flatten gradients
and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a
novel approach that integrates uncertainty-aware modulation and refinement into
critic training. By concentrating conservatism in uncertain and costly regions
while preserving sharp gradients in safe areas, USC enables policies to achieve
effective reward-safety trade-offs. Extensive experiments show that USC reduces
safety violations by approximately 40% while maintaining competitive or higher
rewards, and reduces the error between predicted and true cost gradients by
approximately 83%, breaking the prevailing trade-off between safety and
performance and paving the way for scalable safe RL.

</details>


### [12] [Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints](https://arxiv.org/abs/2510.18520)
*Christopher Ratigan,Kyle Heuton,Carissa Wang,Lenore Cowen,Michael C. Hughes*

Main category: cs.LG

TL;DR: ROC 곡선은 이진 분류 성능을 평가하는 데 널리 사용되지만, 병원 환자 모니터링과 같은 특정 애플리케이션에서는 정밀도 최소 제약조건이나 예측 긍정 수에 대한 상한와 같은 중요한 요소를 포착하지 못한다. 이 논문에서는 이러한 문제를 해결하고, 정밀도 및 용량 제약을 충족하는 분류기의 부분 집합을 ROC 공간에서 실현 가능한 영역으로 나타낼 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 ROC 분석이 병원 경고 시스템의 중요한 요소인 정밀도 및 예측 긍정 수에 대한 제약을 반영하지 못하는 문제를 해결하고자 함.

Method: 정밀도 및 용량 제약을 충족하는 분류기의 부분 집합을 ROC 공간 내 실현 가능한 영역으로 나타내고, 비용과 단조로운 성능 지표인 부분 면적을 정의하여 ROC 표면에서의 부분 부피를 계산.

Result: 비용 인식을 고려한 이 성능 지표가 병원 경고 애플리케이션에서 분류기를 순위 매기기 위한 대안보다 우수하다는 것을 MIMIC-IV 데이터셋을 사용해 입증하였다.

Conclusion: 이 연구 결과는 비용을 고려하여 병원 환자 모니터링 시스템에서 보다 효과적인 분류기 선택을 가능하게 한다.

Abstract: The ROC curve is widely used to assess binary classification performance. Yet
for some applications such as alert systems for hospitalized patient
monitoring, conventional ROC analysis cannot capture crucial factors that
impact deployment, such as enforcing a minimum precision constraint to avoid
false alarm fatigue or imposing an upper bound on the number of predicted
positives to represent the capacity of hospital staff. The usual area under the
curve metric also does not reflect asymmetric costs for false positives and
false negatives. In this paper we address all three of these issues. First, we
show how the subset of classifiers that meet given precision and capacity
constraints can be represented as a feasible region in ROC space. We establish
the geometry of this feasible region. We then define the partial area of lesser
classifiers, a performance metric that is monotonic with cost and only accounts
for the feasible portion of ROC space. Averaging this area over a desired range
of cost parameters results in the partial volume over the ROC surface, or
partial VOROS. In experiments predicting mortality risk using vital sign
history on the MIMIC-IV dataset, we show this cost-aware metric is better than
alternatives for ranking classifiers in hospital alert applications.

</details>


### [13] [RAISE: A Unified Framework for Responsible AI Scoring and Evaluation](https://arxiv.org/abs/2510.18559)
*Loc Phuc Truong Nguyen,Hung Thanh Do*

Main category: cs.LG

TL;DR: AI 시스템의 평가를 예측 정확도를 넘어서 설명 가능성, 공정성, 탄력성 및 지속 가능성을 포함해야 한다는 연구.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 중요한 분야에 진입함에 따라 평가가 더 넓은 차원으로 확대되어야 할 필요성이 있다.

Method: RAISE(Responsible AI Scoring and Evaluation)라는 통합 프레임워크를 도입하여 모델 성능을 네 가지 차원에서 수치화하고 단일의 전반적 책임 점수로 집계하였다.

Result: 세 가지 심층 학습 모델(MLP, Tabular ResNet, Feature Tokenizer Transformer)을 평가했으며, MLP는 강한 지속 가능성과 탄력성을 보였고, Transformer는 높은 환경 비용에도 불구하고 설명 가능성과 공정성에서 뛰어남을 나타났다.

Conclusion: 모든 책임 기준에서 단일 모델이 우월하지 않음을 강조하며, 책임 있는 모델 선택을 위한 다차원 평가의 필요성을 부각시킨다.

Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond
predictive accuracy to include explainability, fairness, robustness, and
sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a
unified framework that quantifies model performance across these four
dimensions and aggregates them into a single, holistic Responsibility Score. We
evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular
ResNet, and a Feature Tokenizer Transformer, on structured datasets from
finance, healthcare, and socioeconomics. Our findings reveal critical
trade-offs: the MLP demonstrated strong sustainability and robustness, the
Transformer excelled in explainability and fairness at a very high
environmental cost, and the Tabular ResNet offered a balanced profile. These
results underscore that no single model dominates across all responsibility
criteria, highlighting the necessity of multi-dimensional evaluation for
responsible model selection. Our implementation is available at:
https://github.com/raise-framework/raise.

</details>


### [14] [Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach](https://arxiv.org/abs/2510.18687)
*Chenbei Lu,Zaiwei Chen,Tongxin Li,Chenye Wu,Adam Wierman*

Main category: cs.LG

TL;DR: 본 논문은 다단계 예측을 통한 의사결정에서의 문제를 해결하기 위해 베이지안 가치 함수와 Bellman-Jensen Gap 분석을 제안하며, BOLA라는 새로운 RL 알고리즘을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 실제 응용에서는 에이전트가 다단계 예측에 접근할 수 있으며, 이는 의사결정에 추가적인 이점을 제공하지만, 이를 MDP에 단순히 포함하면 차원의 저주가 발생한다.

Method: 베이지안 가치 함수를 사용하여 최적의 예측 기반 정책을 특징짓고, Bellman-Jensen Gap 분석을 통해 불완전한 예측의 가치를 설명하며, 오프라인 베이지안 가치 학습과 경량 온라인 적응을 분리한 BOLA 알고리즘을 개발합니다.

Result: BOLA는 불완전한 예측 하에서도 샘플 효율성을 유지하며, 이론 및 알고리즘을 합성 MDP와 실제 풍력 에너지 저장 제어 문제에 대해 검증하였다.

Conclusion: 이 연구는 예측 보강 MDP 분석의 도전에 대한 해결책을 제공하며, 실제 세계의 문제에 잘 적용될 수 있음을 보인다.

Abstract: Traditional reinforcement learning (RL) assumes the agents make decisions
based on Markov decision processes (MDPs) with one-step transition models. In
many real-world applications, such as energy management and stock investment,
agents can access multi-step predictions of future states, which provide
additional advantages for decision making. However, multi-step predictions are
inherently high-dimensional: naively embedding these predictions into an MDP
leads to an exponential blow-up in state space and the curse of dimensionality.
Moreover, existing RL theory provides few tools to analyze prediction-augmented
MDPs, as it typically works on one-step transition kernels and cannot
accommodate multi-step predictions with errors or partial action-coverage. We
address these challenges with three key innovations: First, we propose the
\emph{Bayesian value function} to characterize the optimal prediction-aware
policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis
on the Bayesian value function, which enables characterizing the value of
imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with
Online Adaptation), a two-stage model-based RL algorithm that separates offline
Bayesian value learning from lightweight online adaptation to real-time
predictions. We prove that BOLA remains sample-efficient even under imperfect
predictions. We validate our theory and algorithm on synthetic MDPs and a
real-world wind energy storage control problem.

</details>


### [15] [Enhancing Fractional Gradient Descent with Learned Optimizers](https://arxiv.org/abs/2510.18783)
*Jan Sobotka,Petr Šimánek,Pavel Kordík*

Main category: cs.LG

TL;DR: FGD는 기계 학습에 분수 미적분법을 도입하여 최적화를 가속화하는 새로운 방법을 제안한다. 그러나 수렴 동작과 하이퍼파라미터 선택에서 도전에 직면해 있다. L2O-CFGD는 하이퍼파라미터를 동적으로 조정하는 메타 학습 접근 방식을 제공하여 CFGD의 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: FGD는 최적화를 가속화할 수 있는 가능성을 가지고 있지만, 하이퍼파라미터의 선택과 수렴 성능에서의 문제를 겪고 있다.

Method: L2O-CFGD는 CFGD의 하이퍼파라미터를 동적으로 조정하는 메타 학습 접근 방식을 통해 이러한 문제를 해결한다.

Result: L2O-CFGD는 정적 하이퍼파라미터를 사용한 CFGD보다 나은 성능을 보이며, 일부 작업에서는 완전히 블랙박스인 메타 학습 최적화기와 유사한 성능을 달성한다.

Conclusion: L2O-CFGD는 높은 성능의 하이퍼파라미터를 식별하고 분수 미분의 이력 의존성을 활용하는 데 유용한 도구가 될 수 있다.

Abstract: Fractional Gradient Descent (FGD) offers a novel and promising way to
accelerate optimization by incorporating fractional calculus into machine
learning. Although FGD has shown encouraging initial results across various
optimization tasks, it faces significant challenges with convergence behavior
and hyperparameter selection. Moreover, the impact of its hyperparameters is
not fully understood, and scheduling them is particularly difficult in
non-convex settings such as neural network training. To address these issues,
we propose a novel approach called Learning to Optimize Caputo Fractional
Gradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the
hyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule
outperforms CFGD with static hyperparameters found through an extensive search
and, in some tasks, achieves performance comparable to a fully black-box
meta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for
researchers to identify high-performing hyperparameters and gain insights on
how to leverage the history-dependence of the fractional differential in
optimization.

</details>


### [16] [Search Self-play: Pushing the Frontier of Agent Capability without Supervision](https://arxiv.org/abs/2510.18821)
*Hongliang Lu,Yuhang Wen,Pengyu Cheng,Ruijin Ding,Haotian Xu,Jiaqi Guo,Chutian Wang,Haonan Chen,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 본 논문은 자기 플레이 훈련을 통한 강화 학습(RL) 방법을 제시하여 연구 에이전트의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: RLVR(검증 가능한 보상을 통한 강화 학습)는 LLM 에이전트 훈련의 주류 기술이지만, 정확한 보상을 제공하기 위해 수많은 인간 노력이 필요합니다.

Method: Deep search agents를 위한 자기 플레이 훈련을 탐구하고, 학습 LLM은 멀티 턴 검색 엔진 호출을 활용하여 작업 제안자와 문제 해결자로 동시에 행동합니다.

Result: SSP는 다양한 벤치마크에서 검색 에이전트의 성능을 상당히 향상시킨다는 결과를 보여주었습니다.

Conclusion: SSP는 감독 없이도 내재적 RL 훈련 설정 하에서 에이전트의 성능을 향상시킵니다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become the
mainstream technique for training LLM agents. However, RLVR highly depends on
well-crafted task queries and corresponding ground-truth answers to provide
accurate rewards, which requires massive human efforts and hinders the RL
scaling processes, especially under agentic scenarios. Although a few recent
works explore task synthesis methods, the difficulty of generated agentic tasks
can hardly be controlled to provide effective RL training advantages. To
achieve agentic RLVR with higher scalability, we explore self-play training for
deep search agents, in which the learning LLM utilizes multi-turn search engine
calling and acts simultaneously as both a task proposer and a problem solver.
The task proposer aims to generate deep search queries with well-defined
ground-truth answers and increasing task difficulty. The problem solver tries
to handle the generated search queries and output the correct answer
predictions. To ensure that each generated search query has accurate ground
truth, we collect all the searching results from the proposer's trajectory as
external knowledge, then conduct retrieval-augmentation generation (RAG) to
test whether the proposed query can be correctly answered with all necessary
search documents provided. In this search self-play (SSP) game, the proposer
and the solver co-evolve their agent capabilities through both competition and
cooperation. With substantial experimental results, we find that SSP can
significantly improve search agents' performance uniformly on various
benchmarks without any supervision under both from-scratch and continuous RL
training setups. The code is at https://github.com/Alibaba-Quark/SSP.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors](https://arxiv.org/abs/2510.17848)
*Dan Lin,Yanli Ding,Weipeng Zou,Jiachi Chen,Xiapu Luo,Jiajing Wu,Zibin Zheng*

Main category: cs.CR

TL;DR: Web3의 발전으로 인한 탈중앙화 금융에서 자금 세탁 행동을 자동으로 주석 처리하기 위한 RiskTagger라는 대형 언어 모델 기반 에이전트를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 탈중앙화 금융의 발전에 따라 사용자 익명성과 크로스 체인 자산 흐름으로 인해 온체인 자금 세탁 행동이 더욱 은밀하고 복잡해졌다.

Method: RiskTagger는 복잡한 비구조적 보고서에서 단서를 추출하고, 다중 체인 거래 경로를 추론하며, 감사자 친화적인 설명을 생성하는 등 세 가지 주요 도전 과제를 해결하는 데 초점을 맞춘 에이전트이다.

Result: 실험 결과, RiskTagger는 단서 추출에서 100% 정확도, 전문가 판단과의 일관성 84.1%, 설명 생성에서 90% 커버리지를 달성하였다.

Conclusion: RiskTagger는 자금 세탁 행동 주석 생성을 자동화하며 AML 연구의 투명성과 확장성을 향상시킨다.

Abstract: While the rapid growth of Web3 has driven the development of decentralized
finance, user anonymity and cross-chain asset flows make on-chain laundering
behaviors more covert and complex. In this context, constructing high-quality
anti-money laundering(AML) datasets has become essential for risk-control
systems and on-chain forensic analysis, yet current practices still rely
heavily on manual efforts with limited efficiency and coverage. In this paper,
we introduce RiskTagger, a large-language-model-based agent for the automatic
annotation of crypto laundering behaviors in Web3. RiskTagger is designed to
replace or complement human annotators by addressing three key challenges:
extracting clues from complex unstructured reports, reasoning over multichain
transaction paths, and producing auditor-friendly explanations. RiskTagger
implements an end-to-end multi-module agent, integrating a key-clue extractor,
a multichain fetcher with a laundering-behavior reasoner, and a data explainer,
forming a data annotation pipeline. Experiments on the real case Bybit Hack
(with the highest stolen asset value) demonstrate that RiskTagger achieves 100%
accuracy in clue extraction, 84.1% consistency with expert judgment, and 90%
coverage in explanation generation. Overall, RiskTagger automates laundering
behavior annotation while improving transparency and scalability in AML
research.

</details>


### [18] [When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?](https://arxiv.org/abs/2510.17862)
*Yibo Peng,James Song,Lei Li,Xinyu Yang,Mihai Christodorescu,Ravi Mangal,Corina Pasareanu,Haizhong Zheng,Beidi Chen*

Main category: cs.CR

TL;DR: 이 논문에서 우리는 실제 코드 에이전트에 대한 새로운 유형의 위협인 기능적으로 올바르지만 취약한(FCV) 패치에 대해 논의하고, 이들의 공격을 통해 현재 평가 패러다임의 중요한 보안 위협을 드러냅니다.


<details>
  <summary>Details</summary>
Motivation: 코드 에이전트는 GitHub와 같은 플랫폼에서 자율적으로 버그를 수정하는 데 점점 더 신뢰받고 있지만, 이들의 보안 평가는 기능적 정확성에만 집중되고 있습니다.

Method: 우리는 FCV-공격을 제안합니다. 이는 악의적인 공격자에 의해 의도적으로 제작되거나 선량한 개발자에 의해 암묵적으로 도입될 수 있으며, SOTA LLMs(예: ChatGPT 및 Claude)와 에이전트 구조(예: SWE-agent 및 OpenHands)가 FCV 위협에 모두 취약하다는 것을 보여줍니다.

Result: SWE-Bench에서 12개의 에이전트 모델 조합에 대해, 이 공격은 블랙박스 접근과 코드 에이전트에 대한 단일 쿼리만으로 수행할 수 있으며, 예를 들어 CWE-538(정보 노출 취약점)에 대해 FCV-공격의 성공률은 GPT-5 Mini + OpenHands에서 40.7%에 달합니다.

Conclusion: 우리의 결과는 현재 평가 패러다임에서 간과된 중요한 보안 위협을 드러내며, 코드 에이전트를 위한 보안 인식 방어 개발을 촉구합니다.

Abstract: Code agents are increasingly trusted to autonomously fix bugs on platforms
such as GitHub, yet their security evaluation focuses almost exclusively on
functional correctness. In this paper, we reveal a novel type of threat to
real-world code agents: Functionally Correct yet Vulnerable (FCV) patches,
which pass all test cases but contain vulnerable code. With our proposed
FCV-Attack, which can be deliberately crafted by malicious attackers or
implicitly introduced by benign developers, we show that SOTA LLMs (e.g.,
ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all
vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench,
the attack only requires black-box access and a single query to the code agent
to perform the attack. For example, for CWE-538 (information exposure
vulnerability), the FCV-Attack attains an attack success rate of $40.7\%$ on
GPT-5 Mini + OpenHands. Our results reveal an important security threat
overlooked by current evaluation paradigms and urge the development of
security-aware defenses for code agents.

</details>


### [19] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: 대형 언어 모델(LLM)이 향상되고 있지만, 다수의 회전 대화에서의 악의적인 공격에 취약한 문제를 다루기 위한 PLAGUE라는 새로운 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 향상에도 불구하고, 다수의 회전 시나리오에서의 안전성이 점차적으로 취약해지며 악의적인 결과를 유발할 수 있는 문제를 해결하고자 합니다.

Method: PLAGUE는 다수의 회전 공격을 설계하기 위한 새로운 플러그 앤 플레이 프레임워크로, 평생 학습 에이전트에서 영감을 받아 세 가지 단계(Primer, Planner, Finisher)로 구성됩니다.

Result: PLAGUE를 활용한 빨간팀 에이전트가 기존 모델들의 공격 성공률을 30% 이상 향상시켰습니다.

Conclusion: 우리의 연구는 모델 취약성 평가를 위한 다수의 회전 공격 설계 시 계획 초기화, 맥락 최적화 및 평생 학습의 중요성을 이해하기 위한 도구와 통찰력을 제공합니다.

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [20] [BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?](https://arxiv.org/abs/2510.18003)
*Fengqing Jiang,Yichen Feng,Yuetai Li,Luyao Niu,Basel Alomair,Radha Poovendran*

Main category: cs.CR

TL;DR: AI 기반 연구 보조 시스템과 피어 리뷰 시스템의 융합은 완전 자동화된 출판 루프를 초래하는 중대한 취약점을 만든다.


<details>
  <summary>Details</summary>
Motivation: AI가 생성한 연구가 인간의 감독 없이 AI 리뷰어에 의해 평가되는 상황을 조사한다.

Method: 제너레이터는 실제 실험이 필요 없는 프레젠테이션 조작 전략을 채택한다. 정밀한 평가 프레임워크를 개발하고, 실제 데이터에 대해 보정된 오류 보장을 포함한다.

Result: 조작된 논문은 최대 수용률을 기록하며, 리뷰어는 정직성 문제를 자주 지적하지만 여전히 수용 수준 점수를 부여한다.

Conclusion: 현재 AI 기반 리뷰 시스템의 근본적인 한계를 드러내며, 과학 출판에서 깊이 있는 방어 체계의 필요성을 강조한다.

Abstract: The convergence of LLM-powered research assistants and AI-based peer review
systems creates a critical vulnerability: fully automated publication loops
where AI-generated research is evaluated by AI reviewers without human
oversight. We investigate this through \textbf{BadScientist}, a framework that
evaluates whether fabrication-oriented paper generation agents can deceive
multi-model LLM review systems. Our generator employs presentation-manipulation
strategies requiring no real experiments. We develop a rigorous evaluation
framework with formal error guarantees (concentration bounds and calibration
analysis), calibrated on real data. Our results reveal systematic
vulnerabilities: fabricated papers achieve acceptance rates up to . Critically,
we identify \textit{concern-acceptance conflict} -- reviewers frequently flag
integrity issues yet assign acceptance-level scores. Our mitigation strategies
show only marginal improvements, with detection accuracy barely exceeding
random chance. Despite provably sound aggregation mathematics, integrity
checking systematically fails, exposing fundamental limitations in current
AI-driven review systems and underscoring the urgent need for defense-in-depth
safeguards in scientific publishing.

</details>


### [21] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: PrivaDE는 기계 학습을 위한 데이터의 개인 정보 보호 유틸리티 점수 및 선택을 위한 암호화 프로토콜이다.


<details>
  <summary>Details</summary>
Motivation: 모델 성능을 향상시키는 데이터셋을 획득하려는 모델 구축자는 데이터의 관련성을 평가하는 것이 중요하다.

Method: PrivaDE를 통해 비공식적인 세부 정보를 노출하지 않고 데이터의 유용성을 평가할 수 있으며, 블록체인 중심의 설계를 통해 악의적인 보안 보장을 시행한다.

Result: PrivaDE는 100만 개의 매개변수를 가진 모델에서도 15분 이내에 온라인 런타임을 달성하며 효과적인 데이터 평가를 수행한다.

Conclusion: 이 연구는 탈중앙화된 기계 학습 생태계에서 공정하고 자동화된 데이터 마켓플레이스의 기초를 마련한다.

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [22] [Investigating the Impact of Dark Patterns on LLM-Based Web Agents](https://arxiv.org/abs/2510.18113)
*Devin Ersoy,Brandon Lee,Ananth Shreekumar,Arjun Arunasalam,Muhammad Ibrahim,Antonio Bianchi,Z. Berkay Celik*

Main category: cs.CR

TL;DR: 본 연구는 대형 언어 모델 기반 웹 에이전트의 의사 결정 과정에 미치는 어두운 패턴의 영향을 조사하며, 새로운 경량 프레임워크와 통제된 환경을 통해 실험을 수행하였다.


<details>
  <summary>Details</summary>
Motivation: 사용자들이 온라인 작업을 자동화하기 위해 대형 언어 모델 기반 웹 에이전트를 점점 더 많이 사용함에 따라, 이러한 에이전트는 사용자로 하여금 의도하지 않은 결정을 내리도록 조작하는 어두운 패턴에 직면할 수 있다.

Method: 우리는 LiteAgent라는 경량 프레임워크를 소개하여 에이전트가 작업을 수행하도록 자동으로 요청하고, 그 상호 작용의 로그와 화면 녹화를 포착한다. 또한 TrickyArena라는 제어된 환경을 제공하여 다양한 어두운 패턴을 포함하는 웹 애플리케이션을 제시한다.

Result: 여섯 개의 인기 있는 LLM 기반 일반 웹 에이전트를 세 가지 LLM을 통해 평가한 결과, 단일 어두운 패턴이 있을 때 에이전트가 평균 41%의 확률로 이에 취약함을 발견하였다.

Conclusion: 이 연구는 웹 에이전트에 대한 포괄적인 방어 메커니즘의 필요성을 강조하며, 에이전트 특정 보호와 더 넓은 웹 안전 조치를 포함해야 함을 나타낸다.

Abstract: As users increasingly turn to large language model (LLM) based web agents to
automate online tasks, agents may encounter dark patterns: deceptive user
interface designs that manipulate users into making unintended decisions.
Although dark patterns primarily target human users, their potentially harmful
impacts on LLM-based generalist web agents remain unexplored. In this paper, we
present the first study that investigates the impact of dark patterns on the
decision-making process of LLM-based generalist web agents. To achieve this, we
introduce LiteAgent, a lightweight framework that automatically prompts agents
to execute tasks while capturing comprehensive logs and screen-recordings of
their interactions. We also present TrickyArena, a controlled environment
comprising web applications from domains such as e-commerce, streaming
services, and news platforms, each containing diverse and realistic dark
patterns that can be selectively enabled or disabled. Using LiteAgent and
TrickyArena, we conduct multiple experiments to assess the impact of both
individual and combined dark patterns on web agent behavior. We evaluate six
popular LLM-based generalist web agents across three LLMs and discover that
when there is a single dark pattern present, agents are susceptible to it an
average of 41% of the time. We also find that modifying dark pattern UI
attributes through visual design changes or HTML code adjustments and
introducing multiple dark patterns simultaneously can influence agent
susceptibility. This study emphasizes the need for holistic defense mechanisms
in web agents, encompassing both agent-specific protections and broader web
safety measures.

</details>


### [23] [Black-Box Evasion Attacks on Data-Driven Open RAN Apps: Tailored Design and Experimental Evaluation](https://arxiv.org/abs/2510.18160)
*Pranshav Gajjar,Molham Khoja,Abiodun Ganiyu,Marc Juarez,Mahesh K. Marina,Andrew Lehane,Vijay K. Shah*

Main category: cs.CR

TL;DR: O-RAN 아키텍처는 RAN 데이터 접근을 개방하여 기계 학습 기반 응용 프로그램을 통해 RAN 운영을 최적화하지만, 이러한 데이터 접근은 악의적인 공격자에 의한 보안 취약점을 초래할 수 있다. 본 논문에서는 xApps와 rApps에 대한 데이터 취약성을 조사하고, O-RAN RIC 앱을 겨냥한 효과적인 블랙박스 회피 공격 전략을 설계하여 성능 저하를 검증하였다.


<details>
  <summary>Details</summary>
Motivation: O-RAN의 채택이 진행됨에 따라 RAN 운영의 데이터 기반 혁신 필요성이 증가하고 있다. RAN 데이터 접근 개방에 따른 보안 취약점을 조사할 필요가 생겼다.

Method: xApps와 rApps에 대해 O-RAN의 보안 메커니즘과 한계를 질적으로 분석하였으며, O-RAN RIC 앱을 겨냥한 블랙박스 회피 공격 전략을 설계하였다. 이 전략은 모델 클로닝 알고리즘, 입력 특화 변동, 범용 적대적 변동(UAP), 목표 기반 UAP를 포함한다.

Result: 설계된 회피 공격 전략의 효과성을 검증하고 실제 O-RAN 테스트베드 및 에뮬레이션 환경을 이용하여 성능 저하의 규모를 정량화하였다.

Conclusion: 지속적인 O-RAN 아키텍처의 발전으로 인해 데이터 접근의 보안 취약성을 고려해야 하며, 효율적인 회피 공격 전략이 필요한 상황임을 입증하였다.

Abstract: The impending adoption of Open Radio Access Network (O-RAN) is fueling
innovation in the RAN towards data-driven operation. Unlike traditional RAN
where the RAN data and its usage is restricted within proprietary and
monolithic RAN equipment, the O-RAN architecture opens up access to RAN data
via RAN intelligent controllers (RICs), to third-party machine learning (ML)
powered applications - rApps and xApps - to optimize RAN operations.
Consequently, a major focus has been placed on leveraging RAN data to unlock
greater efficiency gains. However, there is an increasing recognition that RAN
data access to apps could become a source of vulnerability and be exploited by
malicious actors. Motivated by this, we carry out a comprehensive investigation
of data vulnerabilities on both xApps and rApps, respectively hosted in Near-
and Non-real-time (RT) RIC components of O-RAN. We qualitatively analyse the
O-RAN security mechanisms and limitations for xApps and rApps, and consider a
threat model informed by this analysis. We design a viable and effective
black-box evasion attack strategy targeting O-RAN RIC Apps while accounting for
the stringent timing constraints and attack effectiveness. The strategy employs
four key techniques: the model cloning algorithm, input-specific perturbations,
universal adversarial perturbations (UAPs), and targeted UAPs. This strategy
targets ML models used by both xApps and rApps within the O-RAN system, aiming
to degrade network performance. We validate the effectiveness of the designed
evasion attack strategy and quantify the scale of performance degradation using
a real-world O-RAN testbed and emulation environments. Evaluation is conducted
using the Interference Classification xApp and the Power Saving rApp as
representatives for near-RT and non-RT RICs. We also show that the attack
strategy is effective against prominent defense techniques for adversarial ML.

</details>


### [24] [CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments](https://arxiv.org/abs/2510.18324)
*Gyeonghoon Park,Jaehan Kim,Jinu Choi,Jinwoo Kim*

Main category: cs.CR

TL;DR: CryptoGuard는 리눅스 기반 클라우드 환경에서의 암호화폐 채굴 악성코드에 대한 경량 하이브리드 솔루션으로, 탐지와 복구 전략을 결합하여 높은 정확도로 의심스러운 활동을 식별하며, CPU 오버헤드는 최소화한다.


<details>
  <summary>Details</summary>
Motivation: 리눅스 기반 클라우드 환경에서 암호화폐 채굴 악성코드의 증가로 인한 재정적인 손실을 해결할 필요가 있다.

Method: CryptoGuard는 경량 하이브리드 솔루션으로, syscall 모니터링을 통해 행동 패턴을 수집하고, 딥 러닝 모델을 이용해 의심스러운 활동을 탐지하며, eBPF 기반의 복구 메커니즘을 통합한다.

Result: 123개의 실제 암호화폐 채굴 악성코드 샘플로 평가한 결과, 평균 F1 점수는 96.12%와 92.26%를 달성했으며, 높은 정확도로 진실 긍정률과 거짓 긍정률을 개선하였다.

Conclusion: CryptoGuard는 리눅스 기반 클라우드 환경에서의 암호화폐 채굴 악성코드 탐지 및 복구에 효과적인 솔루션임을 입증하였다.

Abstract: Host-based cryptomining malware, commonly known as cryptojackers, have gained
notoriety for their stealth and the significant financial losses they cause in
Linux-based cloud environments. Existing solutions often struggle with
scalability due to high monitoring overhead, low detection accuracy against
obfuscated behavior, and lack of integrated remediation. We present
CryptoGuard, a lightweight hybrid solution that combines detection and
remediation strategies to counter cryptojackers. To ensure scalability,
CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect
behavior patterns with minimal overhead. It decomposes the classification task
into a two-phase process, leveraging deep learning models to identify
suspicious activity with high precision. To counter evasion techniques such as
entry point poisoning and PID manipulation, CryptoGuard integrates targeted
remediation mechanisms based on eBPF, a modern Linux kernel feature deployable
on any compatible host. Evaluated on 123 real-world cryptojacker samples, it
achieves average F1-scores of 96.12% and 92.26% across the two phases, and
outperforms state-of-the-art baselines in terms of true and false positive
rates, while incurring only 0.06% CPU overhead per host.

</details>


### [25] [The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability](https://arxiv.org/abs/2510.18563)
*Zijie Xu,Minfeng Qi,Shiqing Wu,Lefeng Zhang,Qiwen Wei,Han He,Ningran Li*

Main category: cs.CR

TL;DR: 본 연구에서는 다중 에이전트 시스템에서의 신뢰와 보안 간의 상충 관계를 탐구하고, 신뢰가 향상될 때의 노출 위험을 살펴본다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 상호 신뢰와 보안 간의 긴장이 충분히 탐구되지 않고 있다.

Method: 신뢰를 명시적으로 매개변수화한 폐쇄 루프 상호작용을 포함하여 3개의 매크로 장면과 19개의 하위 장면에 걸친 시나리오 게임 데이터셋을 구성하고 실행하였다.

Result: 다양한 모델 백엔드와 오케스트레이션 프레임워크에서 신뢰 수준이 높을수록 작업 성공률이 향상되지만 노출 위험도 증가한다는 일관된 경향을 보였다.

Conclusion: 신뢰는 다중 에이전트 시스템 설계의 주요 보안 변수로 모델링되고 계획되어야 한다.

Abstract: Multi-agent systems powered by large language models are advancing rapidly,
yet the tension between mutual trust and security remains underexplored. We
introduce and empirically validate the Trust-Vulnerability Paradox (TVP):
increasing inter-agent trust to enhance coordination simultaneously expands
risks of over-exposure and over-authorization. To investigate this paradox, we
construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,
and run extensive closed-loop interactions with trust explicitly parameterized.
Using Minimum Necessary Information (MNI) as the safety baseline, we propose
two unified metrics: Over-Exposure Rate (OER) to detect boundary violations,
and Authorization Drift (AD) to capture sensitivity to trust levels. Results
across multiple model backends and orchestration frameworks reveal consistent
trends: higher trust improves task success but also heightens exposure risks,
with heterogeneous trust-to-risk mappings across systems. We further examine
defenses such as Sensitive Information Repartitioning and Guardian-Agent
enablement, both of which reduce OER and attenuate AD. Overall, this study
formalizes TVP, establishes reproducible baselines with unified metrics, and
demonstrates that trust must be modeled and scheduled as a first-class security
variable in multi-agent system design.

</details>


### [26] [CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection](https://arxiv.org/abs/2510.18585)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: CLASP라는 새로운 시스템을 통해 여러 지능형 에이전트를 이용하여 피싱 웹사이트를 효과적으로 감지한다.


<details>
  <summary>Details</summary>
Motivation: 피싱 웹사이트는 사이버 보안에서 중요한 위협으로, 정확하고 비용 효율적인 감지 메커니즘이 필요하다.

Method: 여러 개의 대형 언어 모델(LLM)을 사용하여 URL 구조, 웹페이지 스크린샷 및 HTML 콘텐츠를 분석하는 에이전트를 결합하였다.

Result: Gemini 1.5 Flash 모델이 새로 편집된 데이터셋에서 F1 점수 83.01%로 가장 우수한 성능을 보였으며, 평균 처리 시간은 2.78초, API 비용은 1,000개 웹사이트당 약 $3.18이었다.

Conclusion: CLASP는 기존 솔루션보다 40% 이상 높은 재현율과 20% 향상된 F1 점수를 기록하였고, 데이터셋을 공개하여 추가 연구를 지원할 것이다.

Abstract: Phishing websites remain a significant cybersecurity threat, necessitating
accurate and cost-effective detection mechanisms. In this paper, we present
CLASP, a novel system that effectively identifies phishing websites by
leveraging multiple intelligent agents, built using large language models
(LLMs), to analyze different aspects of a web resource. The system processes
URLs or QR codes, employing specialized LLM-based agents that evaluate the URL
structure, webpage screenshot, and HTML content to predict potential phishing
threats. To optimize performance while minimizing operational costs, we
experimented with multiple combination strategies for agent-based analysis,
ultimately designing a strategic combination that ensures the per-website
evaluation expense remains minimal without compromising detection accuracy. We
tested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these
agents and found that Gemini 1.5 Flash achieved the best performance with an F1
score of 83.01% on a newly curated dataset. Also, the system maintained an
average processing time of 2.78 seconds per website and an API cost of around
$3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions,
achieving over 40% higher recall and a 20% improvement in F1 score for phishing
detection on the collected dataset. To support further research, we have made
our dataset publicly available, supporting the development of more advanced
phishing detection systems.

</details>


### [27] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: sNVMe-oF는 현대 데이터 센터에서 NVMe-oF 프로토콜을 확장하여 기밀성, 무결성 및 신선도 보장을 제공하는 스토리지 관리 시스템으로, 성능 저하 없이 자원 효율을 높이고 보안을 강화합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 데이터 센터에서 NVMe-over-Fabrics(NVMe-oF)은 최고의 성능과 자원 활용, 전력 효율성을 달성하는 표준 솔루션으로 자리잡고 있으며, 동시에 기밀 컴퓨팅(CC)이 민감한 작업 부하에 대한 보다 강력한 격리와 보호를 제공하는 보안 패러다임으로 자리잡고 있습니다.

Method: 이 논문에서는 NVMe-oF 프로토콜을 확장하고 CC 위협 모델에 따라 기밀성, 무결성 및 신선도 보장을 제공하는 스토리지 관리 시스템인 sNVMe-oF를 도입합니다. sNVMe-oF는 적절한 제어 경로와 카운터 임대(counter-leasing)와 같은 새로운 개념을 제공합니다. 또한 NVMe 메타데이터를 활용하고 새로운 비약식 해젤 머클 트리(HMT)를 도입하여 데이터 경로 성능을 최적화하며 불필요한 IPSec 보호를 피합니다.

Result: NVIDIA BlueField-3에서 sNVMe-oF의 프로토타입을 제작하였고, 이는 합성 패턴과 AI 훈련에 대해 2% 미만의 성능 저하를 달성할 수 있음을 보여주었습니다.

Conclusion: sNVMe-oF는 NVMe-oF 프로토콜을 수정하지 않고도 리소스 사용을 과도하게 하느니 대신 CC 가능 스마트 NIC의 가속기를 사용하여 선속도를 유지합니다.

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [28] [TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training in Education](https://arxiv.org/abs/2510.17913)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: TACLA는 심리적 깊이와 일관된 페르소나 행동을 달성하기 위해 설계된 새로운 다중 에이전트 아키텍처이다.


<details>
  <summary>Details</summary>
Motivation: 인간의 사회 역학을 정교하게 시뮬레이션하는 것이 어려운 과제로 남아 있으며, 이는 고충실도 훈련 도구에 필수적이다.

Method: TACLA는 에이전트를 서로 다른 부모, 성인 및 아동 자아 상태의 구별된 시스템으로 모델링하고, 상황적 트리거 및 에이전트의 삶의 스크립트에 따라 자아 상태 활성화를 우선시하는 조율자 에이전트를 통합한다.

Result: 학생 에이전트의 현실적인 자아 상태 변화를 보여주며, 다른 교사 개입 전략에 따라 갈등의 감소 및 증가를 효과적으로 모델링한다.

Conclusion: TACLA의 평가는 높은 대화 신뢰성을 보여주며, 교육 및 그 이상을 위한 효과적인 AI 도구 개발을 촉진한다.

Abstract: Simulating nuanced human social dynamics with Large Language Models (LLMs)
remains a significant challenge, particularly in achieving psychological depth
and consistent persona behavior crucial for high-fidelity training tools. This
paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a
novel Multi-Agent architecture designed to overcome these limitations. TACLA
integrates core principles of Transactional Analysis (TA) by modeling agents as
an orchestrated system of distinct Parent, Adult, and Child ego states, each
with its own pattern memory. An Orchestrator Agent prioritizes ego state
activation based on contextual triggers and an agent's life script, ensuring
psychologically authentic responses. Validated in an educational scenario,
TACLA demonstrates realistic ego state shifts in Student Agents, effectively
modeling conflict de-escalation and escalation based on different teacher
intervention strategies. Evaluation shows high conversational credibility and
confirms TACLA's capacity to create dynamic, psychologically-grounded social
simulations, advancing the development of effective AI tools for education and
beyond.

</details>


### [29] [Adaptive Coopetition: Leveraging Coarse Verifier Signals for Resilient Multi-Agent LLM Reasoning](https://arxiv.org/abs/2510.18179)
*Rui Jerry Huang,Wendy Liu,Anastasia Miin,Lei Ding*

Main category: cs.MA

TL;DR: Adaptive Coopetition(AdCo)이라는 새로운 추론 시간 프레임워크를 제안하여 대형 언어 모델의 추론 성능을 향상시키고, 협력과 경쟁을 통해 기존의 한계를 극복하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 추론 성능을 개선하기 위한 효율적이고 신뢰할 수 있는 추론 시간 계산 기법의 필요성.

Method: LLM 에이전트들이 UCB 기반의 적응형 '협력경쟁' 메커니즘을 사용하여 협력 또는 경쟁을 결정하고 동료 피드백을 통해 추론을 반복적으로 개선하는 방식.

Result: 이 적응형 전략은 수학적 추론 벤치마크에서 성능이 크게 향상되어, 더 도전적인 데이터 세트에서 기준선 대비 20%의 상대적 개선을 달성함.

Conclusion: 이 연구는 추론 시간 계산에 대한 새로운 관점을 제공하고 더 강건한 다중 에이전트 LLM 시스템의 개발을 위한 기초를 마련함.

Abstract: Inference-time computation is a critical yet challenging paradigm for
enhancing the reasoning performance of large language models (LLMs). While
existing strategies improve reasoning stability and consistency, they suffer
from notable limitations: self-correction often reinforces the model's initial
biases, and Multi-Agent Collaboration (MAC) often fails due to the lack of
efficient coordination mechanisms, leading to collective errors. Although
high-performing verifiers can detect reasoning errors, making them reliable
requires substantial training. To address these challenges, we introduce a
novel inference-time framework, Adaptive Coopetition (AdCo), in which LLM
agents utilize an adaptive, UCB-based "coopetition" mechanism. At each round,
agents leverage coarse verifier signals to determine whether to collaborate or
compete, and iteratively refine their reasoning based on peer feedback. Without
relying on high-performance verifiers, our adaptive strategy achieves
significant performance gains on mathematical reasoning benchmarks, yielding a
20% relative improvement over baselines on the more challenging dataset. Our
approach remains robust and consistent in terms of accuracy under different
sample sizes and configurations. This adaptive, signal-guided "coopetition"
framework enhances reasoning robustness by leveraging both model knowledge
diversity and reasoning trace measures, while also promoting uncertainty-driven
exploration, especially when participants have comparable capabilities. From
this perspective, our work offers a fresh lens on inference-time computation
and paves the way for more resilient multi-agent LLM systems. Our code is
available at: https://github.com/AdCo-Research/adaptive-coopetition.

</details>


### [30] [The Emergence of Complex Behavior in Large-Scale Ecological Environments](https://arxiv.org/abs/2510.18221)
*Joseph Bejjani,Chase Van Amburg,Chengrui Wang,Chloe Huangyuan Su,Sarah M. Pratt,Yasin Mazloumi,Naeem Khoshnevis,Sham M. Kakade,Kianté Brantley,Aaron Walsman*

Main category: cs.MA

TL;DR: 이 연구는 개방형 생태 환경에서 물리적 규모와 인구 규모가 복잡한 행동의 출현에 미치는 영향을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 행동이 자연적으로 어떻게 나타나는지를 발견하고, 이를 통해 생태학이 기계 학습의 도구로서 어떻게 활용될 수 있는지를 조사하고자 한다.

Method: 대규모 실험을 통해 60,000개 이상의 개체로 구성된 환경에서 진화된 신경망 정책을 가진 에이전트들이 자연 선택을 통해 진화하는 방식을 연구한다.

Result: 경쟁과 생존 압력 하에서 긴 거리의 자원 추출, 시각 기반 채집, 포식 등의 여러 가지 복잡한 행동이 나타남을 확인하였다.

Conclusion: 이 연구는 대규모 생태학 실험에서의 결과를 통해 기계 학습의 새로운 탐색 방향을 제시한다.

Abstract: We explore how physical scale and population size shape the emergence of
complex behaviors in open-ended ecological environments. In our setting, agents
are unsupervised and have no explicit rewards or learning objectives but
instead evolve over time according to reproduction, mutation, and natural
selection. As they act, agents also shape their environment and the population
around them in an ongoing dynamic ecology. Our goal is not to optimize a single
high-performance policy, but instead to examine how behaviors emerge and evolve
across large populations due to natural competition and environmental
pressures. In an effort to discover how complex behaviors naturally emerge, we
conduct experiments in large-scale worlds that reach populations of more than
60,000 individual agents, each with their own evolved neural network policy. We
identify various emergent behaviors such as long-range resource extraction,
vision-based foraging, and predation that arise under competitive and survival
pressures. We examine how sensing modalities and environmental scale affect the
emergence of these behaviors, finding that some appear only in sufficiently
large environments and populations, with larger scales increasing behavioral
stability and consistency. While there is a rich history of research in
evolutionary settings, our scaling results provide promising new directions to
explore ecology as an instrument of machine learning in an era of abundant
computational resources. Experimental code is available at
https://github.com/jbejjani2022/ecological-emergent-behavior.

</details>


### [31] [From Agent Simulation to Social Simulator: A Comprehensive Review (Part 1)](https://arxiv.org/abs/2510.18271)
*Xiao Xue,Deyu Zhou,Ming Zhang,Fei-Yue Wang*

Main category: cs.MA

TL;DR: AGM의 역사적 발전과 고전 사례에 대한 종합 리뷰의 첫 번째 부분.


<details>
  <summary>Details</summary>
Motivation: 전통적인 물리적 시뮬레이션 방법이 사회적 분야에서 직면한 주요 과제를 이해하기 위해.

Method: 개별 모델, 환경 모델, 규칙 기반 모델 등 사회 시스템을 시뮬레이션하기 위한 기초 모델 소개.

Result: 사고 실험, 메커니즘 탐색, 병렬 최적화의 세 가지 유형의 고전적 사회 시뮬레이션 사례 제공.

Conclusion: 사회 시스템의 모델링에서 AGM의 중요성을 강조하고 있다.

Abstract: This is the first part of the comprehensive review, focusing on the
historical development of Agent-Based Modeling (ABM) and its classic cases. It
begins by discussing the development history and design principles of
Agent-Based Modeling (ABM), helping readers understand the significant
challenges that traditional physical simulation methods face in the social
domain. Then, it provides a detailed introduction to foundational models for
simulating social systems, including individual models, environmental models,
and rule-based models. Finally, it presents classic cases of social simulation,
covering three types: thought experiments, mechanism exploration, and parallel
optimization.

</details>


### [32] [Socialized Learning and Emergent Behaviors in Multi-Agent Systems based on Multimodal Large Language Models](https://arxiv.org/abs/2510.18515)
*Sureyya Akin,Shruti T. Tiwari,Ram Bhattacharya,Sagar A. Raman,Kiran Mohanty,Sita Krishnan*

Main category: cs.MA

TL;DR: M-S2L 프레임워크는 멀티모달 언어 모델과 사회적 학습 메커니즘을 통합하여 AI 에이전트의 사회적 지능 발달을 촉진한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 emergent social intelligence를 증진시키기 위해.

Method: M-S2L은 멀티모달 지각 및 구조화된 행동 능력을 가진 에이전트를 통해 직접적인 강화 학습과 두 가지 사회적 학습 경로를 결합한다.

Result: M-S2L 에이전트는 복잡한 장치를 설계하는 작업에서 기존의 모델들보다 우수한 성능을 나타냈다.

Conclusion: 멀티모달 지각과 명시적인 사회적 학습의 통합이 다중 에이전트 시스템에서 인간과 유사한 협업 지능을 개발하는 데 중요하다.

Abstract: This search introduces the Multimodal Socialized Learning Framework (M-S2L),
designed to foster emergent social intelligence in AI agents by integrating
Multimodal Large Language Models (M-LLMs) with social learning mechanisms. The
framework equips agents with multimodal perception (vision and text) and
structured action capabilities, enabling physical manipulation and grounded
multimodal communication (e.g., text with visual pointers). M-S2L combines
direct reinforcement learning with two novel social learning pathways:
multimodal observational learning and communication-driven learning from
feedback, augmented by an episodic memory system for long-term social context.
  We evaluate M-S2L in a Collaborative Assembly Environment (CAE), where agent
teams must construct complex devices from ambiguous blueprints under
informational asymmetry. Across tasks of increasing complexity, M-S2L agents
consistently outperform Text-Only and No-Social-Learning baselines in Task
Completion Rate and Time to Completion, particularly in dynamic problem-solving
scenarios. Ablation studies confirm the necessity of both multimodality and
socialized learning. Our analysis reveals the emergence of efficient
communication protocols integrating visual pointers with concise text,
alongside rapid role specialization leading to stable labor division.
Qualitative case studies demonstrate agents' abilities for shared awareness,
dynamic re-planning, and adaptive problem-solving, suggesting a nascent form of
machine social cognition. These findings indicate that integrating multimodal
perception with explicit social learning is critical for developing human-like
collaborative intelligence in multi-agent systems.

</details>


### [33] [Fetch.ai: An Architecture for Modern Multi-Agent Systems](https://arxiv.org/abs/2510.18699)
*Michael J. Wooldridge,Attila Bagoly,Jonathan J. Ward,Emanuele La Malfa,Gabriel Paludo Licks*

Main category: cs.MA

TL;DR: Fetch.ai 아키텍처는 중앙집중화와 신뢰 및 통신 프로토콜 부족의 한계를 극복하기 위해 고안된 플랫폼이다.


<details>
  <summary>Details</summary>
Motivation: 최근의 LLM 기반 지능형 시스템은 다수의 전문 에이전트 시스템(MAS) 연구를 간과하여 많은 한계를 지니고 있다.

Method: 클래식 MAS 원칙과 현대 AI 기능을 통합하는 다층적 솔루션을 제안하며, 이 솔루션은 검증 가능한 신원과 거래를 위한 온체인 블록체인 서비스를 기반으로 한다.

Result: 탈중앙화된 물류 사용 사례를 통해 자율 에이전트들이 동적으로 안전하게 신뢰할 수 있는 거래를 수행하는 시스템을 증명했다.

Conclusion: Fetch.ai 스택은 열린 협력적이며 경제적으로 지속 가능한 다수의 에이전트 생태계로 나아갈 수 있는 원칙 기반 아키텍처를 제공한다.

Abstract: Recent surges in LLM-driven intelligent systems largely overlook decades of
foundational multi-agent systems (MAS) research, resulting in frameworks with
critical limitations such as centralization and inadequate trust and
communication protocols. This paper introduces the Fetch.ai architecture, an
industrial-strength platform designed to bridge this gap by facilitating the
integration of classical MAS principles with modern AI capabilities. We present
a novel, multi-layered solution built on a decentralized foundation of on-chain
blockchain services for verifiable identity, discovery, and transactions. This
is complemented by a comprehensive development framework for creating secure,
interoperable agents, a cloud-based platform for deployment, and an intelligent
orchestration layer where an agent-native LLM translates high-level human goals
into complex, multi-agent workflows. We demonstrate the deployed nature of this
system through a decentralized logistics use case where autonomous agents
dynamically discover, negotiate, and transact with one another securely.
Ultimately, the Fetch.ai stack provides a principled architecture for moving
beyond current agent implementations towards open, collaborative, and
economically sustainable multi-agent ecosystems.

</details>


### [34] [Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity](https://arxiv.org/abs/2510.18802)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 현대 사회기술 시스템은 가치를 창출하기 위해 협력하고 이를 확보하기 위해 경쟁하는 전략적 경쟁 협력(coopetition)으로 특징지어진다. 이 보고서는 이 두 가지 차원을 공식화하는 계산적 기초를 개발하여 이 격차를 해소하고, 나아가 요구공학 및 다중 에이전트 시스템에서 전략적 경쟁 협력을 연구하는 기반 참고자료로 기능한다.


<details>
  <summary>Details</summary>
Motivation: 현대 사회기술 시스템에서는 가치를 창출하기 위한 협력과 이를 확보하기 위한 경쟁이 동시에 발생한다. 따라서 이러한 복잡한 상호작용을 이해하고 분석할 필요성이 있다.

Method: i* 구조적 의존성 분석을 바탕으로 종속 관계를 정량적인 상호 의존성 계수로 변환하고, Brandenburger와 Nalebuff의 부가 가치 개념을 따르는 보완성을 공식화하며, 구조적 의존성과 협상력을 통합하여 게임 이론적 공식화를 도입한다.

Result: 검증된 매개변수를 통해 시너지 가치 창출 모델을 생성하고, 여러 실험 테스트를 통해 기능적 형태의 견고성을 입증하며, 삼성-소니 S-LCD 합작 투자 사례에 적용하여 로그 함수가 우수한 실증 적합도를 보여준다.

Conclusion: 이 보고서는 요구공학 및 다중 에이전트 시스템에서의 전략적 경쟁 협력을 조사하는 조정된 연구 프로그램의 기초 참고자료 역할을 한다.

Abstract: Modern socio-technical systems are characterized by strategic coopetition
where actors simultaneously cooperate to create value and compete to capture
it. While conceptual modeling languages like i* provide rich qualitative
representations of strategic dependencies, they lack mechanisms for
quantitative analysis of dynamic trade-offs. Conversely, classical game theory
offers mathematical rigor but strips away contextual richness. This technical
report bridges this gap by developing computational foundations that formalize
two critical dimensions of coopetition: interdependence and complementarity. We
ground interdependence in i* structural dependency analysis, translating
depender-dependee-dependum relationships into quantitative interdependence
coefficients through a structured translation framework. We formalize
complementarity following Brandenburger and Nalebuff's Added Value concept,
modeling synergistic value creation with validated parameterization. We
integrate structural dependencies with bargaining power in value appropriation
and introduce a game-theoretic formulation where Nash Equilibrium incorporates
structural interdependence. Validation combines comprehensive experimental
testing across power and logarithmic value function specifications,
demonstrating functional form robustness, with empirical application to the
Samsung-Sony S-LCD joint venture (2004-2011), where logarithmic specifications
achieve superior empirical fit (validation score 45/60) while power functions
provide theoretical tractability. This technical report serves as the
foundational reference for a coordinated research program examining strategic
coopetition in requirements engineering and multi-agent systems, with companion
work addressing trust dynamics, team production, and reciprocity mechanisms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [FABRIC: Framework for Agent-Based Realistic Intelligence Creation](https://arxiv.org/abs/2510.17995)
*Abhigya Verma,Seganrasan Subramanian,Nandhakumar Kandasamy,Naman Gupta*

Main category: cs.AI

TL;DR: 대규모 언어 모델(LLM)이 에이전트로 배포되어 동적 환경에서 목표를 분해하고 도구를 호출하며 결과를 검증할 수 있도록 하는 통합 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 에이전트 기능 실현을 위한 데이터 수집의 어려음을 해결하고자 함.

Method: 인간의 개입 없이 LLM만을 사용하여 에이전틱 데이터를 생성하는 모듈식 파이프라인을 도입함.

Result: 완전한 상호작용 기록을 생성하며, 다중 작업 및 다중 턴 에이전트 상호작용을 지원함.

Conclusion: 수동 수집의 대안으로 재현 가능한 방법을 제공하여 강력한 도구 사용이 가능한 LLM의 발전에 기여함.

Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to
decompose goals, invoke tools, and verify results in dynamic environments.
Realizing these capabilities requires access to agentic data-structured
interaction records that couple user intents with tool specifications,
argument-grounded calls, and verifiable execution traces. However, collecting
such data from human annotators is costly, time-consuming, and difficult to
scale. We present a unified framework for synthesizing agentic data using only
LLMs, without any human-in-the-loop supervision. This framework decomposes
generation into modular pipelines that produce complete interaction records
spanning task specifications, tool definitions, policy pseudocode, natural
language exchanges, and execution traces. Records conform to strict syntactic
and semantic constraints, ensuring machine-parseability and faithful alignment
across inputs, outputs, and tool calls. Beyond single tasks, there is support
for both multi-task and multi-turn agent interactions, enabling the
construction of datasets that reflect the full spectrum of tool-use
competencies. To ensure quality and consistency, the framework integrates
constrained generation formats, JSON-schema validation, and judge-based
filtering. This paper formalizes the schema for agentic records, details the
prompt design principles that guide generation, and introduces scalable
pipelines for high-quality synthetic data. By providing a reproducible,
LLM-only alternative to manual collection, hence advancing the development of
agentic LLMs capable of robust tool use.

</details>


### [36] [OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning](https://arxiv.org/abs/2510.18032)
*Zhenyu Bi,Meng Lu,Yang Li,Swastik Roy,Weijie Guan,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 본 논문은 다중 에이전트 시스템 내에서 효과적인 에이전트 간 커뮤니케이션을 통해 복잡한 추론을 개선하는 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 집단 지능을 활용하여 복잡한 추론을 향상시키기 위한 다중 에이전트 시스템이 필요하다.

Method: 복잡한 협업 구조를 동적으로 구성하고 정제하는 멀티 에이전트 언어 강화 학습 알고리즘을 제안한다.

Result: 우리의 방법은 다양한 추론 작업에서 단일 에이전트 방법과 최첨단 다중 에이전트 프레임워크보다 뛰어난 성능을 보인다.

Conclusion: 다중 에이전트 간의 효과적인 의사소통은 추론 품질을 높이며, 최종 결정을 통해 모든 에이전트 간 다수결로 이루어진다.

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in
mathematical and scientific tasks. To enhance complex reasoning, multi-agent
systems have been proposed to harness the collective intelligence of LLM
agents. However, existing collaboration structures are either predefined or
rely on majority voting or round-table debates, which can suppress correct but
less dominant agent contributions. Recent approaches model multi-agent systems
as graph networks but optimize purely for agent performance, neglecting the
quality of interactions. We hypothesize that effective agent communication is
crucial for multi-agent reasoning and that debating quality plays a significant
role. To address this, we propose $\ours$, a multi-agent verbal reinforcement
learning algorithm that dynamically constructs and refines multi-agent
collaboration structures. Our method defines action spaces and a feedback
mechanism that evaluates communication robustness and coherence throughout the
debate. The final decision is achieved through a majority vote over all the
agents. We assess $\ours$ on various reasoning tasks, including mathematical
reasoning, creative writing, scientific reasoning, and numerical sorting.
Results demonstrate that our approach significantly outperforms single-agent
prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.

</details>


### [37] [CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows](https://arxiv.org/abs/2510.18043)
*Joong Ho Choi,Jiayang Zhao,Jeel Shah,Ritvika Sonawane,Vedant Singh,Avani Appalla,Will Flanagan,Filipe Condessa*

Main category: cs.AI

TL;DR: CompactPrompt는 언어 모델의 실행 비용을 최대 60% 감소시키면서도 출력 품질을 유지하는 효율적인 프롬프트 및 데이터 압축 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 강력한 추론 및 생성 능력을 제공하지만, 긴 프롬프트와 풍부한 데이터를 처리하는 에이전트 워크플로에서 상당한 실행 비용이 발생한다는 문제를 해결하기 위해.

Method: CompactPrompt는 프롬프트에서 저정보 토큰을 제거하고, 문서의 반복적 텍스트 패턴에 n-그램 약어를 적용하며, 수치 컬럼에 대해 균일 양자화를 수행하는 끝에서 끝까지의 파이프라인을 제공한다.

Result: TAT-QA 및 FinQA와 같은 벤치마크 데이터셋에서 총 토큰 사용량 및 추론 비용을 최대 60%까지 줄이면서도 출력 품질을 유지하는 결과를 보여준다.

Conclusion: CompactPrompt는 실시간 압축 결정을 시각화하고 비용-성과 트레이드오프를 정량화하는 데 도움을 주어, 더 간소화된 생성 AI 파이프라인을 위한 토대를 마련한다.

Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation
capabilities but incur substantial run-time costs when operating in agentic
workflows that chain together lengthy prompts and process rich data streams. We
introduce CompactPrompt, an end-to-end pipeline that merges hard prompt
compression with lightweight file-level data compression. CompactPrompt first
prunes low-information tokens from prompts using self-information scoring and
dependency-based phrase grouping. In parallel, it applies n-gram abbreviation
to recurrent textual patterns in attached documents and uniform quantization to
numerical columns, yielding compact yet semantically faithful representations.
Integrated into standard LLM agents, CompactPrompt reduces total token usage
and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,
while preserving output quality (Results in less than 5% accuracy drop for
Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time
compression decisions and quantify cost-performance trade-offs, laying the
groundwork for leaner generative AI pipelines.

</details>


### [38] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: 이 논문은 PaDA-Agent라는 새로운 데이터 증강 에이전트를 제안하여 SLM의 성능 향상을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: SLM은 배포 비용과 지연 시간에서 장점을 제공하지만 복잡한 도메인 특정 작업에서는 정확도가 떨어지는 문제가 있다.

Method: PaDA-Agent는 평가 기반 접근법을 통해 데이터를 증강하는 과정을 간소화하며, 검증 데이터를 통해 실패 패턴을 발견하고 목표 지향적인 데이터 증강 전략을 수립한다.

Result: 우리의 실험 결과는 Llama 3.2 1B Instruct 모델의 미세 조정에서 최첨단 LLM 기반 데이터 증강 접근법보다 큰 개선을 보여준다.

Conclusion: PaDA-Agent는 SLM의 일반화 격차를 직접 줄이는 데 효과적이다.

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [39] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 소비자 의사결정을 시뮬레이션하는 것은 마케팅 전략을 설계하고 평가하는 데 필수적이다. 본 논문에서 우리는 LLM을 활용한 다중 에이전트 시뮬레이션 프레임워크를 소개하며, 복잡한 인간 행동과 사회적 상호작용을 모델링한다.


<details>
  <summary>Details</summary>
Motivation: 비용이 많이 드는 실제 배포 전에 마케팅 전략을 설계하고 평가하는 데 있어 소비자 의사결정의 시뮬레이션은 꼭 필요하다.

Method: 최근 대형 언어 모델 시뮬레이션의 발전을 바탕으로, 우리의 프레임워크는 생성 에이전트가 내부 추론을 표현하고 습관을 형성하며 미리 정의된 규칙 없이 구매 결정을 내리도록 한다.

Result: 가격 할인 마케팅 시나리오에서 시스템은 실행 가능한 전략 테스트 결과를 제공하고 전통적인 방법으로는 파악할 수 없는 신흥 사회적 패턴을 드러낸다.

Conclusion: 이 접근 방식은 마케터들에게 사전 구현 테스트를 위한 확장 가능하고 저위험의 도구를 제공하며, 시간 소모가 큰 사후 평가에 대한 의존도를 줄이고 실적 저조 캠페인의 위험을 낮춘다.

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real-world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox environment, our framework enables generative agents to interact,
express internal reasoning, form habits, and make purchasing decisions without
predefined rules. In a price-discount marketing scenario, the system delivers
actionable strategy-testing outcomes and reveals emergent social patterns
beyond the reach of conventional methods. This approach offers marketers a
scalable, low-risk tool for pre-implementation testing, reducing reliance on
time-intensive post-event evaluations and lowering the risk of underperforming
campaigns.

</details>


### [40] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 이 논문에서는 코드 생성에서 효율성 및 품질을 개선하기 위한 새로운 샘플링 알고리즘인 Saber를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models(DLMs)는 코드 생성 작업에서 강한 구조적 제약으로 인해 추론 속도와 출력 품질 간의 중대한 균형 문제에 직면해 있습니다.

Method: Saber는 코드 맥락이 확립될수록 적응적으로 가속화되고, 생성된 토큰을 되돌리기 위한 백트래킹 메커니즘을 요구하는 새로운 비학습 샘플링 알고리즘입니다.

Result: Saber는 주류 DLM 샘플링 방법에 비해 평균 1.9%의 Pass@1 정확도를 높이고, 평균 251.4%의 추론 속도 향상을 달성했습니다.

Conclusion: 우리의 연구는 DLM의 고유한 장점을 활용하여 코드 생성에서 자기 회귀 모델과의 성능 격차를 크게 줄입니다.

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [41] [AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI](https://arxiv.org/abs/2510.18170)
*Manik Rana,Calissa Man,Anotida Expected Msiiwa,Jeffrey Paine,Kevin Zhu,Sunishchal Dev,Vasu Sharma,Ahan M R*

Main category: cs.AI

TL;DR: AgentChangeBench는 대화 중 목표 변화에 대한 도구 보강 언어 모델 에이전트의 적응 능력을 평가하기 위해 설계된 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계의 다중 턴 상호작용에서 목표 변화는 중요한 특징이지만, 현재의 에이전트 벤치마크는 정적인 목표나 한 번의 도구 사용만 평가한다.

Method: AgentChangeBench는 네 가지 보완 메트릭(TSR, TUE, TCRR, GSRT)을 통해 평가를 형식화하며, 2,835개의 작업 시퀀스와 다섯 가지 사용자 페르소나로 구성된다.

Result: 여러 최전선 모델을 평가한 결과, GPT-4o는 항공권 예약 이동에서 92.2%의 회복률을 달성한 반면, Gemini는 48.6%로 떨어졌다.

Conclusion: 높은 정확도가 동적 목표에서의 견고성을 의미하지 않으며, 회복 시간과 중복성을 명시적으로 측정하는 것이 필수적이다.

Abstract: Goal changes are a defining feature of real world multi-turn interactions,
yet current agent benchmarks primarily evaluate static objectives or one-shot
tool use. We introduce AgentChangeBench, a benchmark explicitly designed to
measure how tool augmented language model agents adapt to mid dialogue goal
shifts across three enterprise domains. Our framework formalizes evaluation
through four complementary metrics: Task Success Rate (TSR) for effectiveness,
Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for
wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.
AgentChangeBench comprises 2,835 task sequences and five user personas, each
designed to trigger realistic shift points in ongoing workflows. Using this
setup, we evaluate several frontier models and uncover sharp contrasts obscured
by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$
recovery on airline booking shifts while Gemini collapses to $48.6\%$, and
retail tasks show near perfect parameter validity yet redundancy rates above
$80\%$, revealing major inefficiencies. These findings demonstrate that high
raw accuracy does not imply robustness under dynamic goals, and that explicit
measurement of recovery time and redundancy is essential. AgentChangeBench
establishes a reproducible testbed for diagnosing and improving agent
resilience in realistic enterprise settings.

</details>


### [42] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: AGI(인공지능 일반) 정의 부족이 현재의 전문 AI와 인간 수준 인지 간의 격차를 모호하게 한다. 본 논문은 AGI를 교육을 잘 받은 성인의 인지 다재다능함과 능숙함에 맞는 것으로 정의하는 정량화 가능한 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: AGI의 명확한 정의가 없고, 이는 현재의 전문 AI와 인간 수준의 인지 간의 격차를 이해하는 데 장애가 된다.

Method: Cattell-Horn-Carroll 이론을 기반으로 하여 일반 지능을 추론, 기억, 지각 등 10개의 핵심 인지 영역으로 분해하고, AI 시스템 평가를 위해 인간 심리적 배터리를 조정하는 방법론을 운영화한다.

Result: 이 프레임워크의 적용은 현대 모델에서 매우 '톱니바퀴 같은' 인지 프로필을 드러낸다. 지식 집약적인 분야에서는 높은 성과를 보이지만, 현재 AI 시스템은 특히 장기 기억 저장소에서 기본 인지 기계에 중대한 결함이 있다. AGI 점수는 GPT-4가 27%, GPT-5가 58% 등으로, AGI 이전의 빠른 진행과 여전히 남아 있는 상당한 격차를 구체적으로 정량화한다.

Conclusion: AGI 프레임워크의 발전이 AI 기술의 목표와 미션을 재정의해야 함을 제시한다.

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [43] [Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming](https://arxiv.org/abs/2510.18314)
*Zheng Zhang,Jiarui He,Yuchen Cai,Deheng Ye,Peilin Zhao,Ruili Feng,Hao Wang*

Main category: cs.AI

TL;DR: 이 논문은 웹 에이전트 공격을 위한 새로운 에이전틱 프레임워크인 Genesis를 제안하며, 이는 공격자, 평가자 및 전략가의 세 가지 모듈로 구성되어 공격 전략을 지속적으로 발견하고 발전시킨다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM) 에이전트가 복잡한 웹 작업을 자동화하면서 생산성을 높이지만 새로운 보안 위험도 함께 발생하고 있다.

Method: Genesis는 공격자를 통해 적대적 주입을 생성하고, 평가자는 대상 웹 에이전트의 반응을 평가하여 피드백을 제공하며, 전략가는 상호작용 로그에서 효과적인 전략을 동적으로 발견하고 이를 전략 라이브러리에 축적하는 세 가지 모듈로 구성된다.

Result: 다양한 웹 작업에 대한 광범위한 실험을 통해 우리 프레임워크가 새로운 전략을 발견하고 기존 공격 기준을 지속적으로 초월함을 보여주었다.

Conclusion: 이 연구는 웹 에이전트 공격의 효율성을 향상시키고자 하는 지속적인 노력의 일환으로 Genesis 프레임워크의 기여를 강조한다.

Abstract: As large language model (LLM) agents increasingly automate complex web tasks,
they boost productivity while simultaneously introducing new security risks.
However, relevant studies on web agent attacks remain limited. Existing
red-teaming approaches mainly rely on manually crafted attack strategies or
static models trained offline. Such methods fail to capture the underlying
behavioral patterns of web agents, making it difficult to generalize across
diverse environments. In web agent attacks, success requires the continuous
discovery and evolution of attack strategies. To this end, we propose Genesis,
a novel agentic framework composed of three modules: Attacker, Scorer, and
Strategist. The Attacker generates adversarial injections by integrating the
genetic algorithm with a hybrid strategy representation. The Scorer evaluates
the target web agent's responses to provide feedback. The Strategist
dynamically uncovers effective strategies from interaction logs and compiles
them into a continuously growing strategy library, which is then re-deployed to
enhance the Attacker's effectiveness. Extensive experiments across various web
tasks show that our framework discovers novel strategies and consistently
outperforms existing attack baselines.

</details>


### [44] [Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning](https://arxiv.org/abs/2510.18318)
*Aaron Bell,Amit Aides,Amr Helmy,Arbaaz Muslim,Aviad Barzilai,Aviv Slobodkin,Bolous Jaber,David Schottlander,George Leifman,Joydeep Paul,Mimi Sun,Nadav Sherman,Natalie Williams,Per Bjornsson,Roy Lee,Ruth Alcantara,Thomas Turnbull,Tomer Shekel,Vered Silverman,Yotam Gigi,Adam Boulanger,Alex Ottenwess,Ali Ahmadalipour,Anna Carter,Charles Elliott,David Andre,Elad Aharoni,Gia Jung,Hassler Thurston,Jacob Bien,Jamie McPike,Juliet Rothenberg,Kartik Hegde,Kel Markert,Kim Philipp Jablonski,Luc Houriez,Monica Bharel,Phing VanLee,Reuven Sayag,Sebastian Pilarski,Shelley Cazares,Shlomi Pasternak,Siduo Jiang,Stone Jiang,Thomas Colthurst,Yang Chen,Yehonathan Refael,Yochai Blau,Yuval Carny,Yael Maguire,Avinatan Hassidim,James Manyika,Tim Thelin,Genady Beryozkin,Gautam Prasad,Luke Barrington,Yossi Matias,Niv Efron,Shravya Shetty*

Main category: cs.AI

TL;DR: 지리정보 데이터는 지구를 이해하는 데 큰 잠재력을 제공하지만, 데이터의 방대한 양과 다양성, 해상도 및 시간 척도의 차이로 인해 분석과 해석에 도전이 발생합니다. 본 논문에서는 지구 AI라는 새로운 지리정보 AI 모델과 에이전트 추론을 소개하여 지구에 대한 새로운 통찰을 제공할 수 있는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 지리정보 데이터의 방대한 양과 다양성은 지구를 이해하는 데 도전 과제를 제공합니다.

Method: 세 가지 주요 도메인(행성 규모 이미지, 인구, 환경)에서의 기초 모델과 지능형 Gemini 기반 추론 엔진을 결합한 지구 AI 모델을 개발하였습니다.

Result: 기초 모델의 강력한 성능을 보여주는 엄격한 벤치마크를 제시하고, 이 모델들이 상호 보완적으로 지리정보 추론에서 우수한 예측 능력을 발휘함을 검증하였습니다.

Conclusion: 복잡한 다단계 쿼리를 처리하기 위해, 여러 기초 모델과 대규모 지리정보 데이터 소스 및 도구를 함께 활용하는 Gemini 기반 에이전트를 개발하였습니다.

Abstract: Geospatial data offers immense potential for understanding our planet.
However, the sheer volume and diversity of this data along with its varied
resolutions, timescales, and sparsity pose significant challenges for thorough
analysis and interpretation. This paper introduces Earth AI, a family of
geospatial AI models and agentic reasoning that enables significant advances in
our ability to unlock novel and profound insights into our planet. This
approach is built upon foundation models across three key domains--Planet-scale
Imagery, Population, and Environment--and an intelligent Gemini-powered
reasoning engine. We present rigorous benchmarks showcasing the power and novel
capabilities of our foundation models and validate that when used together,
they provide complementary value for geospatial inference and their synergies
unlock superior predictive capabilities. To handle complex, multi-step queries,
we developed a Gemini-powered agent that jointly reasons over our multiple
foundation models along with large geospatial data sources and tools. On a new
benchmark of real-world crisis scenarios, our agent demonstrates the ability to
deliver critical and timely insights, effectively bridging the gap between raw
geospatial data and actionable understanding.

</details>


### [45] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: MUAD를 위한 ShortcutBreaker라는 새로운 통합 기능 재구성 프레임워크를 제안하며, 이는 정체성 단축 문제를 해결하기 위해 두 가지 주요 혁신을 특징으로 한다.


<details>
  <summary>Details</summary>
Motivation: MUAD는 여러 클래스에 대한 이상 탐지를 위한 통합 모델 개발을 목표로 하며, 이를 통해 별도의 모델 훈련을 제거하고 계산 자원을 절약할 수 있다.

Method: 첫째, 고차원 특징을 저차원 잠재 공간으로 투영하기 위해 저차원 노이즈 목 (LRNB)을 설계한다. 둘째, 지역 특성에 국한되지 않고 ViTs의 전역 모델링 능력을 활용하여 정보 단축을 방지하기 위한 전역 교란 주의를 통합한다.

Result: 제안된 방법은 네 개의 데이터 세트에서 전체적인 이미지 수준 AUROC 99.8%, 98.9%, 90.6%, 87.8%를 달성하며, 다양한 시나리오에서 이전 MUAD 방법을 일관되게 능가하였다.

Conclusion: ShortcutBreaker는 MUAD 작업을 위한 효과적이고 새로운 접근 방식을 제공하며, 특정 단축 문제를 해결할 수 있는 가능성을 보여준다.

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [46] [Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games](https://arxiv.org/abs/2510.18395)
*Runnan Qi,Yanan Ni,Lumin Jiang,Zongyuan Li,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: MASMP는 실시간 전략 게임에서 LLM 에이전트를 위한 새로운 프레임워크로, 상태 기계와 메모리 메커니즘을 통합하여 장기적인 전술 일관성을 갖춘 구조적 행동을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 현재 접근 방식의 환각 및 단편화된 의사 결정과 같은 주요 문제를 해결하고자 함.

Method: 상태 기계 프롬프트와 메모리 메커니즘을 통합한 MASMP 프레임워크.

Result: StarCraft II에서의 실험 결과, MASMP는 가장 강력한 내장 AI(Lv7)에 대해 60%의 승률을 기록하여 기준선(0%)을 크게 초과함.

Conclusion: 이 연구는 복잡한 의사 결정을 위한 신경 및 기호 AI의 결합이라는 새로운 패러다임을 수립함.

Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel
framework for LLM agents in real-time strategy games. Addressing key challenges
like hallucinations and fragmented decision-making in existing approaches,
MASMP integrates state machine prompting with memory mechanisms to unify
structured actions with long-term tactical coherence. The framework features:
(1) a natural language-driven state machine architecture that guides LLMs to
emulate finite state machines and behavior trees through prompts, and (2) a
lightweight memory module preserving strategic variables (e.g., tactics,
priority units) across decision cycles. Experiments in StarCraft II demonstrate
MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly
outperforming baselines (0%). Case studies reveal the method retains LLMs'
semantic comprehension while resolving the "Knowing-Doing Gap" through strict
state-action mapping, achieving both interpretability and FSM-like reliability.
This work establishes a new paradigm for combining neural and symbolic AI in
complex decision-making.

</details>


### [47] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: HAP는 비대칭 셀프 플레이를 통해 인공지능과 인간의 학습 효율을 높이는 새로운 교육 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 기존의 셀프 플레이 방법론은 대칭적인 제로섬 경쟁 모델에 기반을 두고 있으나, 이는 비대칭적이고 열린 학습 시나리오에 적합하지 않다.

Method: HAP는 강사-학생 상호작용을 최소 최대 최적화 문제로 정형화하고, 강사와 학습자가 적대적 역학을 통해 함께 발전한다.

Result: 실험 결과, HAP는 최신 기법(SOTA) 수준의 성능을 달성하면서 학습의 효율성을 높이는 커리큘럼을 생성함을 보였다.

Conclusion: HAP는 실시간 학습 성과에 따라 과제의 복잡성을 조정하는 양방향 피드백 시스템을 구축하여 지속적으로 학습을 최적화한다.

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [48] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: 의료 추론에서 VLM의 성능 향상을 위한 새로운 에이전트 프레임워크인 Med-VRAgent를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 의료 추론에서 VLM의 한계(환각, 애매한 설명, 일관성 없는 논리 및 부정확한 위치 지정)를 극복할 필요성이 있습니다.

Method: Visual Guidance 및 Self-Reward 패러다임과 Monte Carlo Tree Search(MCTS)를 기반으로 한 Med-VRAgent 프레임워크를 제안합니다.

Result: Med-VRAgent는 의료 VQA 벤치마크에서 기존 방법보다 우수한 성과를 보여줍니다.

Conclusion: Med-VRAgent는 VLM의 의료 시각 추론 능력을 향상시키는 효과적인 방법입니다.

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [49] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 이 논문은 다중 회차 사회적 대화에서 대형 언어 모델(LLM) 에이전트를 위한 확률적 의도 모델링 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대화 에이전트가 파트너의 숨겨진 의도를 이해하고 대화 전략을 조정할 수 있도록 돕기 위해 이 연구를 수행하였다.

Method: 모델은 파트너의 숨겨진 의도에 대한 믿음 분포를 유지하고 각 발화 후 우도 추정을 통해 동적으로 업데이트된다.

Result: SOTOPIA 환경에서의 초기 실험 결과, 제안된 프레임워크는 Qwen2.5-7B 기준선에 비해 SOTOPIA-All에서 9.0%, SOTOPIA-Hard에서 4.1%의 개선을 보였다.

Conclusion: 확률적 의도 모델링이 사회적으로 지능적인 LLM 에이전트 개발에 기여할 수 있음을 시사한다.

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [50] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Yifei Zhu,Meng han,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: LAFA는 자연어 쿼리를 수용하고 이를 최적화된 실행 가능 FA 워크플로우로 변환하는 최초의 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 기존 LLM 기반 분석 프레임워크는 중앙 집중식 데이터 접근을 가정하여 개인정보 보호가 부족한 문제를 해결하고자 한다.

Method: LAFA는 자연어 쿼리를 수용하는 계층적 다중 에이전트 아키텍처를 도입하며, 복잡한 쿼리를 하위 쿼리로 분해하고, 각 하위 쿼리를 FA 작업의 Directed Acyclic Graph로 매핑한다.

Result: LAFA는 기본 프롬프트 전략보다 더 높은 실행 계획 성공률을 달성하고, 자원 집약적인 FA 작업을 상당히 줄였다.

Conclusion: 이 작업은 FA 환경에서 자연어 입력을 지원하는 개인정보 보호 LLM 기반 분석의 실용적인 기초를 확립한다.

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [51] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: StarBench는 인간과 유사한 멀티모달 의사결정 및 정보 탐색 능력을 평가하기 위한 RPG 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 현재의 비전-언어 모델(VLMs)이 인간 플레이어처럼 행동할 수 있는지 여부를 조사하기 위함.

Method: StarBench는 두 가지 레짐에서 여덟 가지 전투 작업을 표준화하여 평가한다: 직접 제어와 도구 지원 제어.

Result: 현재 VLM들과 인간 참조에 대한 기준선을 제공하며, 직접 제어에서는 인식-제어 충실도에 큰 격차가 있음을 드러낸다.

Conclusion: StarBench는 실제 클라이언트 플레이에서 에이전트의 정보 탐색 및 멀티모달 의사결정에 대한 재현 가능한 기준이 된다.

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [52] [AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification](https://arxiv.org/abs/2510.18488)
*Ho Fai Leung,Xiaoyan Xi,Fei Zuo*

Main category: cs.AI

TL;DR: 기존의 가상 비서들은 엄격한 API 의존성으로 성능이 제한되어 있다. 본 연구는 AndroidControl 벤치마크의 문제점을 개선하여 새로운 벤치마크인 AndroidControl-Curated를 제시하고, 이를 통해 성능을 15% 향상시켰다. 최신 모델 Magma-R1-3B는 적은 데이터로도 높은 성능을 보여, 실제 사용 가능성이 더 높다는 것을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 가상 비서들은 API에 의존하여 성능이 제한되어 있으며, GUI 에이전트의 성능을 개선하기 위한 필요성이 있다.

Method: AndroidControl 벤치마크의 결 shortcomings를 분석하고, 이를 바탕으로 AndroidControl-Curated라는 개선된 벤치마크를 개발하였다. 새로운 SOTA 모델인 Magma-R1-3B를 2.4k 개의 정제된 샘플을 사용하여 후훈련하였다.

Result: 개선된 벤치마크에서 최신 모델들이 복잡한 작업에 대해 약 75%의 성공률을 보였다. 특히, Magma-R1-3B 모델은 파라미터 수가 200배 적지만 Qwen3-VL-235B와 유사한 성능을 나타냈다.

Conclusion: AndroidControl-Curated 벤치마크와 Magma-R1 모델을 연구 커뮤니티에 공개하여, 모델 성능을 보다 정확히 반영하고 강력한 온디바이스 가상 비서의 개발을 가속화할 것을 촉구한다.

Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly
pivotal, yet their capabilities are hamstrung by a reliance on rigid,
developer-dependent APIs. GUI agents offer a powerful, API-independent
alternative, but their adoption is hindered by the perception of poor
performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at
around 60% on benchmarks like AndroidControl, far from viability for real-world
use. Our research reveals that issue lies not only with the models but with the
benchmarks themselves. We identified notable shortcomings in AndroidControl,
including ambiguities and factual errors, which systematically underrates agent
capabilities. To address this critical oversight, we enhanced AndroidControl
into AndroidControl-Curated, a refined version of the benchmark improved
through a rigorous purification pipeline. On this enhanced benchmark,
state-of-the-art models achieve success rates nearing 75% on complex tasks (15%
improvement), reflecting that on-device GUI agents are actually closer to
practical deployment than previously thought. We introduce our new SOTA model,
Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an
H20 GPU (approximately $60). Despite being 200 times smaller in parameters,
this model delivers performance comparable to Qwen3- VL-235B. We release both
AndroidControl-Curated benchmark and Magma-R1 model to the research community,
encouraging adoption of this enhanced benchmark to better reflect model
capabilities and accelerate the development of robust, on-device virtual
assistants.

</details>


### [53] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: Crucible는 알고리즘의 조정 잠재력을 정량적으로 평가하는 새로운 메트릭을 도입하여 다양한 시나리오에서 알고리즘을 조정하는 데 도움을 주는 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구는 이상적인 구성에서 알고리즘 성능에만 집중하고, 특정 시나리오에 대한 매개변수 조정의 중요성을 간과하고 있다.

Method: LLM 기반의 다단계 전문가 시뮬레이션을 이용하여 알고리즘을 조정하고, 그 조정 잠재력을 정량적으로 평가하는 공식화된 메트릭을 정의한다.

Result: Crucible은 클래식한 제어 작업부터 복잡한 컴퓨터 시스템에 이르는 다양한 사례 연구에서 효과를 입증하였으며, 실제 배포에서 결과를 검증하였다.

Conclusion: Crucible은 다양한 알고리즘 간의 조정 가능 영역을 체계적으로 정량화하며, 알고리즘 분석 및 설계의 새로운 차원을 제공하여 성능 개선으로 이어진다.

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [54] [Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages](https://arxiv.org/abs/2510.18535)
*Sarth Dubey,Subimal Ghosh,Udit Bhatia*

Main category: cs.AI

TL;DR: 신뢰할 수 있는 수문학적 및 홍수 예측은 입력 데이터가 지연되거나 누락되거나 일관성이 없을 때도 안정적인 모델이 필요하다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 강우-유출 예측의 발전은 이상적인 데이터 조건 하에서 평가되어 정확성에 중점을 두고 있다.

Method: 본 논문에서는 물리적 일관성을 유지하기 위해 완화된 수자원 균형 제약을 가진 장기 및 단기 메모리 네트워크를 결합한 글로벌 홍수 인식 시스템(GloFAS)의 운영 준비가 완료된 에뮬레이터를 개발하였다.

Result: 5,000개 이상의 유역에서 시험해본 결과, 에뮬레이터는 GloFAS의 수문학적 핵심을 재현하고 정보의 품질이 저하됨에 따라 부드럽게 성능이 감소했다.

Conclusion: 이 프레임워크는 수문학적 기계 학습의 측정 가능한 특성으로서 운영적 견고성을 확립하고 신뢰할 수 있는 실시간 예측 시스템 설계를 발전시킨다.

Abstract: Reliable hydrologic and flood forecasting requires models that remain stable
when input data are delayed, missing, or inconsistent. However, most advances
in rainfall-runoff prediction have been evaluated under ideal data conditions,
emphasizing accuracy rather than operational resilience. Here, we develop an
operationally ready emulator of the Global Flood Awareness System (GloFAS) that
couples long- and short-term memory networks with a relaxed water-balance
constraint to preserve physical coherence. Five architectures span a continuum
of information availability: from complete historical and forecast forcings to
scenarios with data latency and outages, allowing systematic evaluation of
robustness. Trained in minimally managed catchments across the United States
and tested in more than 5,000 basins, including heavily regulated rivers in
India, the emulator reproduces the hydrological core of GloFAS and degrades
smoothly as information quality declines. Transfer across contrasting
hydroclimatic and management regimes yields reduced yet physically consistent
performance, defining the limits of generalization under data scarcity and
human influence. The framework establishes operational robustness as a
measurable property of hydrological machine learning and advances the design of
reliable real-time forecasting systems.

</details>


### [55] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA-Nabla는 에이전트 기반 프레임워크로 코드 최적화를 통해 시뮬레이터를 구축하며, 여러 CPS 작업에서 최첨단의 정확도를 달성한다.


<details>
  <summary>Details</summary>
Motivation: 시뮬레이터 구축을 최적화 문제로 다루고, 에이전트와 작업 관리를 통해 효율성을 향상시키기 위해.

Method: 전문화된 LLM 기반 에이전트를 그래프 노드로 통합하고, 손실 기반 루프로 코드 합성, 실행, 평가 및 수정을 수행하는 작업 관리자 사용.

Result: 세 가지 CPS 작업에서 SOCIA-Nabla가 최첨단의 전체 정확도를 달성함.

Conclusion: 멀티 에이전트 조정과 손실 정렬 최적화 관점을 통합하여 내구성이 떨어지는 프롬프트 파이프라인을 재현 가능하고 제약을 인식하는 시뮬레이터 코드 생성으로 전환한다.

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [56] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: QuantEvolve는 변화하는 시장에서 다양한 효과적인 거래 전략을 개발하기 위한 진화적 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 개인화된 투자 솔루션에 대한 수요 증가로 인해 변화하는 시장에서 정량적 거래 전략 개발을 자동화하는 것이 어렵다.

Method: QuantEvolve는 투자자의 선호에 맞춘 특징 맵을 사용하고, 가설 기반 다중 에이전트 시스템을 통합하여 전략 공간을 체계적으로 탐색한다.

Result: QuantEvolve는 기존의 기준선을 초과하여 성과를 입증하였다.

Conclusion: 우리는 발전된 전략의 데이터 세트를 공개하여 향후 연구를 지원하고 있다.

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [57] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 데이터와 지식을 결합한 새로운 분류 접근법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 데이터 마이닝을 통해 연관 규칙을 도출하고, 이를 기반으로 트리 기반 모델의 예측 성능과 설명력을 향상시키고자 함.

Method: 트리 기반 모델(결정 트리 및 랜덤 포레스트)을 이용하여 도출된 연관 규칙을 활용.

Result: 해당 접근법을 통해 예측 성능과 설명 크기 모두에서 개선 효과가 나타남.

Conclusion: 연관 규칙을 고려하는 것이 예측 성능과 해석의 품질 향상에 기여함을 입증함.

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [58] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: 이 논문에서는 사용자 쿼리가 모호하여 정보 검색 시스템에서 사용자의 목표를 식별하기가 어려운 문제를 다룹니다. 기존의 대화 기반 인터랙티브 검색 시스템은 사용자 의도를 명확히 할 수 있지만, 가장 정보가 풍부한 질문을 하는 명확한 전략이 부족하여 비효율적입니다. 이를 해결하기 위해, 이 논문에서는 강화 학습을 통해 최적의 질문 전략을 학습하고 대규모 주석 대화 데이터의 필요성을 피하는 대화 주도 검색 프레임워크인 SherlockLLM을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자 쿼리가 종종 모호하여 정보 검색 시스템에서 사용자의 목표를 식별하기 어렵다는 문제를 해결하고자 함.

Method: 강화 학습을 통해 최적의 질문 전략을 학습하고, 이 에이전트가 이진 질문의 순서를 생성하여 검색 공간을 효율적으로 좁히는 대화 주도 검색 프레임워크를 제안.

Result: 구조화된 작업에서 SherlockLLM은 강력한 기준선과 동등한 성능을 보여주며, 이진 검색에 의해 정의된 이론적 최적에 접근하고, 비구조화된 작업에서는 이러한 기준선을 크게 초월하는 성과를 보임.

Conclusion: SherlockLLM은 강력하고 효율적인 솔루션으로, 정보 탐색 대화 정책을 학습할 수 있는 능력을 보여줌.

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>
