<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.AI](#cs.AI) [Total: 24]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: 브라우저 지문 방어는 JavaScript(JS) 추적 기술 감지에 중점을 두어왔으나, WebAssembly (WASM)의 채택이 이러한 방어의 취약점을 드러낸다. 본 논문은 WASM 기반 난독화가 현대 지문 방어의 강인성에 미치는 영향을 체계적으로 평가한다.


<details>
  <summary>Details</summary>
Motivation: WASM의 도입은 JS를 WASM의 저수준 바이너리 형식으로 변환하여 악의적 로직을 위장할 수 있는 잠재적인 취약점을 만든다.

Method: 실제 JS 지문 스크립트를 기능적인 WASM-난독화 변형으로 변환하는 자동화 파이프라인을 개발하고, 최신 연구 문헌의 탐지기와 상업용 브라우저 내 도구에 대해 테스트한다.

Result: 연구 문헌에 제안된 탐지기는 특징 기반 분석에 의존하여 중간 정도의 취약성을 보였으나, 브라우저 확장 및 기본 브라우저 기능은 API 수준의 가로채기로 인해 완전히 효과적이었다.

Conclusion: 학문적 방어 전략과 실용적 방어 전략 간의 간극을 강조하고, WASM 기반 난독화에 대한 탐지 접근 방식을 강화할 수 있는 통찰을 제공하며, 향후 공격에서 더 회피적인 기술의 기회를 드러낸다.

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [2] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: Locus 프레임워크는 목표 상태 도달을 위한 효율적인 방향성 퍼징을 위한 새로운 접근법을 제시하며, 평균 41.6배의 속도 향상을 달성하였다.


<details>
  <summary>Details</summary>
Motivation: 방향성 퍼징은 특정 프로그램 상태에 도달하는 입력을 찾기 위한 작업으로, 기존 방법들은 효과적이지 않다.

Method: Locus는 의미 있는 중간 상태로 퍼징 진행 상황을 포착하기 위해 프레디케이트를 합성하고, 프로그램을 분석하여 후보 프레디케이트를 정교화한다.

Result: Locus는 8개의 최신 퍼저의 효율성을 크게 개선하여 실제 취약점을 발견하는 데 평균 41.6배의 속도 향상을 보여주었다.

Conclusion: Locus는 이전에 패치되지 않은 8개의 버그를 발견하였고, 그 중 하나는 초안 패치를 통해 인정받았다.

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [3] [LLM-driven Provenance Forensics for Threat Investigation and Detection](https://arxiv.org/abs/2508.21323)
*Kunal Mukherjee,Murat Kantarcioglu*

Main category: cs.CR

TL;DR: PROVSEEK는 자동화된 출처 기반 포렌식 분석 및 위협 정보 추출을 위한 LLM 기반의 에이전틱 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱한 방법으로 APT를 조사하기 위한 새로운 패러다임을 정립하려는 목적이다.

Method: PROVSEEK는 벡터화된 위협 보고서 지식 기반과 시스템 출처 데이터베이스의 데이터를 융합하여 정확하고 맥락을 인식하는 쿼리를 생성하는 특수 툴체인을 사용한다.

Result: PROVSEEK는 DARPA 공개 데이터셋에서 평가되었으며, 정보 추출 작업에서 34%의 개선을, 위협 탐지 작업에서 각각 22%와 29%의 높은 정밀도/재현율을 달성하였다.

Conclusion: PROVSEEK는 에이전트 오케스트레이션과 RAG 및 CoT 추론을 결합하여 적응형 다단계 분석을 가능하게 하며, 근거 기반 포렌식 설명을 생성한다.

Abstract: We introduce PROVSEEK, an LLM-powered agentic framework for automated
provenance-driven forensic analysis and threat intelligence extraction.
PROVSEEK employs specialized toolchains to dynamically retrieve relevant
context by generating precise, context-aware queries that fuse a vectorized
threat report knowledge base with data from system provenance databases. The
framework resolves provenance queries, orchestrates multiple role-specific
agents to mitigate hallucinations, and synthesizes structured, ground-truth
verifiable forensic summaries. By combining agent orchestration with
Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,
PROVSEEK enables adaptive multi-step analysis that iteratively refines
hypotheses, verifies supporting evidence, and produces scalable, interpretable
forensic explanations of attack behaviors. By combining provenance data with
agentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic
forecics to investigate APTs. We conduct a comprehensive evaluation on publicly
available DARPA datasets, demonstrating that PROVSEEK outperforms
retrieval-based methods for intelligence extraction task, achieving a 34%
improvement in contextual precision/recall; and for threat detection task,
PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline
agentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion
Detection System (PIDS).

</details>


### [4] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: EU의 사이버 보안 법률에서 위험 기반 접근 방식이 어떻게 적용되고 있는지 조사하고, 법률 간의 일관성과 차이점을 분석한다.


<details>
  <summary>Details</summary>
Motivation: EU의 사이버 보안 법률에서 위험 기반 접근 방식을 채택함에 따라 이를 체계적으로 분석할 필요성이 커졌다.

Method: 질적 법 해석 및 분류 체계 구축을 기반으로 한다.

Result: 다섯 개 법률은 기술적, 조직적, 인적 보안 등 다양한 사이버 보안 위험을 포괄적으로 다루고 있으며, 특히 기술적 측면과 자산이 법적 위험 개념을 구성하는 데 사용된다.

Conclusion: EU의 새로운 사이버 보안 법률은 규제에 대한 위험 기반 접근을 크게 확장하였고, 이에 따라 복잡성과 준수 부담이 증가했다.

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [5] [zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs](https://arxiv.org/abs/2508.21393)
*Guofu Liao,Taotao Wang,Shengli Zhang,Jiqun Zhang,Shi Long,Dacheng Tao*

Main category: cs.CR

TL;DR: 본 논문은 LoRA 미세 조정을 제로 지식 증명과 통합한 zkLoRA 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 미세 조정은 특정 작업에 적합하게 만드는 데 필수적이지만, 계산 집약적이고 보안 문제를 야기합니다.

Method: zkLoRA는 LoRA 미세 조정을 제로 지식 증명과 통합한 첫 번째 프레임워크로, 암호화 기술을 활용하여 수학적 및 비수학적 연산을 검증합니다.

Result: zkLoRA는 모델 파라미터와 훈련 데이터의 프라이버시를 보호하면서 미세 조정 동안의 모든 단계에서 검증 가능성을 제공합니다.

Conclusion: zkLoRA는 민감한 환경에서도 대형 언어 모델의 안전하고 신뢰할 수 있는 배포를 가능하게 합니다.

Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to
specific tasks, yet it remains computationally demanding and raises concerns
about correctness and privacy, particularly in untrusted environments. Although
parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly
reduce resource requirements, ensuring the security and verifiability of
fine-tuning under zero-knowledge constraints remains an unresolved challenge.
To address this, we introduce zkLoRA, the first framework to integrate LoRA
fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and
correctness. zkLoRA employs advanced cryptographic techniques -- such as lookup
arguments, sumcheck protocols, and polynomial commitments -- to verify both
arithmetic and non-arithmetic operations in Transformer-based architectures.
The framework provides end-to-end verifiability for forward propagation,
backward propagation, and parameter updates during LoRA fine-tuning, while
safeguarding the privacy of model parameters and training data. Leveraging
GPU-based implementations, zkLoRA demonstrates practicality and efficiency
through experimental validation on open-source LLMs like LLaMA, scaling up to
13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,
zkLoRA bridges a critical gap, enabling secure and trustworthy deployment of
LLMs in sensitive or untrusted environments.

</details>


### [6] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: 대형 언어 모델(LLM)의 종속성 공급망에서의 보안 취약성을 분석.


<details>
  <summary>Details</summary>
Motivation: LLM 종속성 공급망의 취약성은 보안 위험을 초래할 수 있다.

Method: 52개의 오픈 소스 LLM을 분석하여 제3자 종속성과 관련 취약점을 조사하고, 리포지토리 내의 관리 활동을 탐색했다.

Result: LLM 생태계의 취약성의 절반은 56.2개월 이상 공개되지 않았고, 75.8%의 LLM이 취약한 종속성을 포함한다.

Conclusion: LLM 공급망 위험에 대한 이해를 높이고 보안 개선 방향을 제시한다.

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [7] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: 이 논문은 코드 LLM의 데이터 사용을 감사하기 위한 RepoMark라는 새로운 데이터 마킹 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 코드 생성용 LLM의 발전은 코딩 작업을 자동화했지만, 데이터 인증 및 오픈 소스 라이선스 준수에 대한 윤리적, 법적 우려가 증가하고 있습니다.

Method: RepoMark는 코드 파일에 데이터 마크를 삽입하고, 모델 내의 메모리화를 감지하기 위해 새로운 랭킹 기반 가설 검정을 사용합니다.

Result: RepoMark는 작은 코드 리포지토리에서 90% 이상의 감지 성공률을 달성하며, FDR 보장은 5%입니다.

Conclusion: RepoMark는 코드 LLM 훈련의 투명성을 향상시키고 리포지토리 소유자의 권리를 보호하는 신뢰할 수 있는 해결책으로 인증됩니다.

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [8] [Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee (Extended Version)](https://arxiv.org/abs/2508.21440)
*Shan Wang,Ming Yang,Yu Liu,Yue Zhang,Shuaiqing Zhang,Zhen Ling,Jiannong Cao,Xinwen Fu*

Main category: cs.CR

TL;DR: 본 논문에서는 RPC 사용자와 블록체인 개인 정보 유출 사이의 연관성을 밝히는 새로운 비 익명화 공격 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: RPC 서비스는 공공 블록체인에 접근하는 주요 경로이나, 개인정보 보호에 대한 위험이 존재합니다.

Method: 공개 원장에 기록된 거래 확인 타임스탬프와 사용자가 거래 상태를 조회할 때 전송하는 TCP 패킷의 타임스탬프 사이의 순간적인 상관관계를 분석하여 공격을 수행합니다.

Result: 제안된 비 익명화 공격은 일반 RPC 사용자에 대해 95% 이상의 높은 성공률을 보입니다.

Conclusion: 이 공격은 거래 수수료가 0이며, 네트워크 인프라에 접근할 수 있는 강력한 패시브 공격자를 가정합니다.

Abstract: Remote Procedure Call (RPC) services have become a primary gateway for users
to access public blockchains. While they offer significant convenience, RPC
services also introduce critical privacy challenges that remain insufficiently
examined. Existing deanonymization attacks either do not apply to blockchain
RPC users or incur costs like transaction fees assuming an active network
eavesdropper. In this paper, we propose a novel deanonymization attack that can
link an IP address of a RPC user to this user's blockchain pseudonym. Our
analysis reveals a temporal correlation between the timestamps of transaction
confirmations recorded on the public ledger and those of TCP packets sent by
the victim when querying transaction status. We assume a strong passive
adversary with access to network infrastructure, capable of monitoring traffic
at network border routers or Internet exchange points. By monitoring network
traffic and analyzing public ledgers, the attacker can link the IP address of
the TCP packet to the pseudonym of the transaction initiator by exploiting the
temporal correlation. This deanonymization attack incurs zero transaction fee.
We mathematically model and analyze the attack method, perform large-scale
measurements of blockchain ledgers, and conduct real-world attacks to validate
the attack. Our attack achieves a high success rate of over 95% against normal
RPC users on various blockchain networks, including Ethereum, Bitcoin and
Solana.

</details>


### [9] [SoK: Large Language Model-Generated Textual Phishing Campaigns End-to-End Analysis of Generation, Characteristics, and Detection](https://arxiv.org/abs/2508.21457)
*Fengchao Chen,Tingmin Wu,Van Nguyen,Carsten Rudolph*

Main category: cs.CR

TL;DR: LLM에 의해 생성된 피싱 공격의 생애 주기를 체계적으로 연구하여 새로운 방어 전략을 제시하는 논문이다.


<details>
  <summary>Details</summary>
Motivation: LLM을 활용한 피싱 공격에 대한 연구가 증가하고 있지만, 피싱 공격 생애 주기에 대한 체계적인 연구는 부족하다.

Method: Generation-Characterization-Defense (GenCharDef)을 소개하고, LLM 생성 피싱과 전통적인 피싱의 차이점을 체계적으로 정리한다.

Result: LLM 기반 피싱의 독특한 도전 과제를 강조하고, 보다 강력한 방어 설계를 위한 기초를 제공한다.

Conclusion: LLM 기반 피싱의 위협 환경을 보다 잘 이해하고 대응하기 위한 체계적인 분석을 제공한다.

Abstract: Phishing is a pervasive form of social engineering in which attackers
impersonate trusted entities to steal information or induce harmful actions.
Text-based phishing dominates for its low cost, scalability, and
concealability, advantages recently amplified by large language models (LLMs)
that enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite
the growing research into LLM-facilitated phishing attacks, consolidated
systematic research on the phishing attack life cycle remains scarce. In this
work, we present the first systematization of knowledge (SoK) on LLM-generated
phishing, offering an end-to-end analysis that spans generation techniques,
attack features, and mitigation strategies. We introduce
Generation-Characterization-Defense (GenCharDef), which systematizes the ways
in which LLM-generated phishing differs from traditional phishing across
methodologies, security perspectives, data dependencies, and evaluation
practices. This framework highlights unique challenges of LLM-driven phishing,
providing a coherent foundation for understanding the evolving threat landscape
and guiding the design of more resilient defenses.

</details>


### [10] [Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain](https://arxiv.org/abs/2508.21480)
*Narges Dadkhah,Khan Reaz,Gerhard Wunder*

Main category: cs.CR

TL;DR: 본 논문은 스마트 홈 장치와 IoT 기반 보안 시스템의 안전한 온보딩 및 신뢰 구축 문제를 해결하기 위한 분산 온보딩 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 스마트 홈 장치와 IoT 보안 시스템의 채택 증가로 인해 편리함과 안전성을 높이고 위험 관리를 개선할 기회가 커지고 있지만, 안전한 온보딩과 클라우드 플랫폼과의 신뢰 구축은 여전히 큰 과제입니다.

Method: 기존 네트워크 계층 온보딩 기법을 기반으로 하여 애플리케이션 계층으로 확장한 새로운 온보딩 프레임워크를 제안하고, 컨소시엄 블록체인 기술을 통합하여 탈중앙화된 온보딩 메커니즘을 구현했습니다.

Result: 프로토타입 구현을 통해 스마트 홈 환경에서 시스템의 실행 가능성을 입증하였으며, 검증 완료 시간이 0.34초로 스케일 가능성과 다양한 이해 관계자에게 적합함을 강조합니다.

Conclusion: 블록체인 기반 접근 방식이 다양한 작업 부하를 효과적으로 처리하고, 높은 처리량과 낮은 대기 시간을 유지하며, 거의 실시간 IoT 데이터 처리를 지원함을 보여주었습니다.

Abstract: The increasing adoption of smart home devices and IoT-based security systems
presents significant opportunities to enhance convenience, safety, and risk
management for homeowners and service providers. However, secure
onboarding-provisioning credentials and establishing trust with cloud
platforms-remains a considerable challenge. Traditional onboarding methods
often rely on centralized Public Key Infrastructure (PKI) models and
manufacturer-controlled keys, which introduce security risks and limit the
user's digital sovereignty. These limitations hinder the widespread deployment
of scalable IoT solutions. This paper presents a novel onboarding framework
that builds upon existing network-layer onboarding techniques and extends them
to the application layer to address these challenges. By integrating consortium
blockchain technology, we propose a decentralized onboarding mechanism that
enhances transparency, security, and monitoring for smart home architectures.
The architecture supports device registration, key revocation, access control
management, and risk detection through event-driven alerts across dedicated
blockchain channels and smart contracts. To evaluate the framework, we formally
model the protocol using the Tamarin Prover under the Dolev-Yao adversary
model. The analysis focuses on authentication, token integrity, key
confidentiality, and resilience over public channels. A prototype
implementation demonstrates the system's viability in smart home settings, with
verification completing in 0.34 seconds, highlighting its scalability and
suitability for constrained devices and diverse stakeholders. Additionally,
performance evaluation shows that the blockchain-based approach effectively
handles varying workloads, maintains high throughput and low latency, and
supports near real-time IoT data processing.

</details>


### [11] [Generalized Encrypted Traffic Classification Using Inter-Flow Signals](https://arxiv.org/abs/2508.21558)
*Federica Bianchi,Edoardo Di Paolo,Angelo Spognardi*

Main category: cs.CR

TL;DR: 새로운 암호화된 트래픽 분류 모델을 제안하며, 이는 원시 PCAP 데이터에서 직접 작동하고 트래픽 유형에 대한 사전 가정이 필요하지 않음.


<details>
  <summary>Details</summary>
Motivation: 트래픽 유형에 대한 사전 가정 없이 원시 PCAP 데이터에서 직접 작동하는 암호화된 트래픽 분류 모델이 필요함.

Method: 다양한 분류 작업에 걸쳐 일반화할 수 있는 모델로, 흐름 간 신호를 활용하여 시간적 상관관계와 패킷 양 분포를 포착하는 혁신적인 표현을 사용.

Result: 본 모델은 거의 모든 분류 작업 및 대부분의 데이터 세트에서 잘 확립된 방법보다 우수한 성능을 보여주며, 일부 경우에는 최대 99%의 정확도를 기록함.

Conclusion: 이 모델은 견고성 및 적응성을 입증함.

Abstract: In this paper, we present a novel encrypted traffic classification model that
operates directly on raw PCAP data without requiring prior assumptions about
traffic type. Unlike existing methods, it is generalizable across multiple
classification tasks and leverages inter-flow signals - an innovative
representation that captures temporal correlations and packet volume
distributions across flows. Experimental results show that our model
outperforms well-established methods in nearly every classification task and
across most datasets, achieving up to 99% accuracy in some cases, demonstrating
its robustness and adaptability.

</details>


### [12] [Agentic Discovery and Validation of Android App Vulnerabilities](https://arxiv.org/abs/2508.21579)
*Ziyue Wang,Liyi Zhou*

Main category: cs.CR

TL;DR: A2 시스템은 Android 취약점을 분석하고 검증하는 두 가지 상호 보완적인 단계로 구성되어 있으며, 기존 도구들보다 높은 정확도로 실제 취약점을 찾아낸다.


<details>
  <summary>Details</summary>
Motivation: 기존 Android 취약점 탐지 도구들은 수천 개의 경고로 팀을 압도하지만, 실제 양성 사례는 거의 발견하지 못한다. 분석가들은 결과를 분류하는 데 많은 시간을 소비하며, 이로 인해 보안 파이프라인에서 병목 현상이 발생한다.

Method: A2는 (i) 에이전트 취약점 발견(전통적인 보안 도구와의 의미론적 이해 결합)과 (ii) 에이전트 취약점 검증(Android의 다양한 공격 표면을 체계적으로 검증)이라는 두 과정으로 구성되어 있다.

Result: Ghera 벤치마크(n=60)에서 A2는 78.3%의 커버리지를 달성하여 최신 분석 도구(APKHunt 30.0%)를 초월하였다. A2는 수천 개의 경고로 분석가를 압도하는 대신 82개의 추정 취약점 발견으로 결과를 정리하였고, 그 중 47개는 Ghera 사례, 28개는 추가적인 실제 양성 사례이다.

Conclusion: 실제 평가(169개 생산 APK)를 통해 A2는 104개의 실제 양성 제로데이 취약점을 발견했으며, 이 중 57개(54.8%)는 자동 생성된 PoCs로 스스로 검증되었다. 이 중 하나는 1천만 회 이상의 설치가 이루어진 널리 사용되는 애플리케이션에서 중간 심각도의 취약점이다.

Abstract: Existing Android vulnerability detection tools overwhelm teams with thousands
of low-signal warnings yet uncover few true positives. Analysts spend days
triaging these results, creating a bottleneck in the security pipeline.
Meanwhile, genuinely exploitable vulnerabilities often slip through, leaving
opportunities open to malicious counterparts.
  We introduce A2, a system that mirrors how security experts analyze and
validate Android vulnerabilities through two complementary phases: (i) Agentic
Vulnerability Discovery, which reasons about application security by combining
semantic understanding with traditional security tools; and (ii) Agentic
Vulnerability Validation, which systematically validates vulnerabilities across
Android's multi-modal attack surface-UI interactions, inter-component
communication, file system operations, and cryptographic computations.
  On the Ghera benchmark (n=60), A2 achieves 78.3% coverage, surpassing
state-of-the-art analyzers (e.g., APKHunt 30.0%). Rather than overwhelming
analysts with thousands of warnings, A2 distills results into 82 speculative
vulnerability findings, including 47 Ghera cases and 28 additional true
positives. Crucially, A2 then generates working Proof-of-Concepts (PoCs) for 51
of these speculative findings, transforming them into validated vulnerability
findings that provide direct, self-confirming evidence of exploitability.
  In real-world evaluation on 169 production APKs, A2 uncovers 104
true-positive zero-day vulnerabilities. Among these, 57 (54.8%) are
self-validated with automatically generated PoCs, including a medium-severity
vulnerability in a widely used application with over 10 million installs.

</details>


### [13] [Condense to Conduct and Conduct to Condense](https://arxiv.org/abs/2508.21602)
*Tomasz Kazana*

Main category: cs.CR

TL;DR: 이 논문에서는 낮은 전도율의 치환에 대한 첫 번째 예를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: Dodis 등은 'Indifferentiability of Confusion-Diffusion Networks' 논문에서 전도율의 개념을 도입하였으며, 낮은 전도율의 치환을 찾는 것을 시작하고 동기를 부여했습니다.

Method: 이 논문에서는 우리가 원하는 예를 제시할 뿐만 아니라 문제에 대한 일반적인 특성을 제시합니다.

Result: 낮은 전도율의 치환이 'Multi-Source-Somewhere-Condensers'로 알려진 정보 이론적 특성을 가진 치환과 동치임을 보여줍니다.

Conclusion: 이 결과는 낮은 전도율의 치환 연구에 대한 새로운 통찰을 제공합니다.

Abstract: In this paper we give the first examples of low-conductance permutations. The
notion of conductance of permutations was introduced in the paper
"Indifferentiability of Confusion-Diffusion Networks" by Dodis et al., where
the search for low-conductance permutations was initiated and motivated. In
this paper we not only give the desired examples, but also make a general
characterization of the problem -- i.e. we show that low-conductance
permutations are equivalent to permutations that have the information-theoretic
properties of the so-called Multi-Source-Somewhere-Condensers.

</details>


### [14] [Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs](https://arxiv.org/abs/2508.21606)
*Nishant Chinnasami,Rasha Karakchi*

Main category: cs.CR

TL;DR: 이 논문은 경량 이중 탐지 프레임워크를 제시하여 AES-128 암호화의 취약성을 해결한다.


<details>
  <summary>Details</summary>
Motivation: AES-128 암호화는 이론적으로 안전하지만, 임베디드 시스템에서의 타이밍 및 결함 주입 공격으로 인해 실제 배치에서 취약하다.

Method: 통계적 임계값 결정 및 기계 학습(ML)을 결합한 경량 이중 탐지 프레임워크를 제안하고, 지연 및 암호문 손상을 통해 이상을 시뮬레이션하여 타이밍 및 데이터 피쳐 수집 후 실행 시간 기반의 통계적 임계값 방법과 블록 수준 이상에 대해 훈련된 랜덤 포레스트 분류기 두 가지 전략을 평가한다.

Result: CPU 및 FPGA (PYNQ-Z1)에서 구현된 결과, ML 접근 방식이 정확성 면에서 정적 임계값을 초월하며 임베디드 플랫폼에서 실시간 가능성을 유지한다.

Conclusion: 프레임워크는 AES 내부를 수정하거나 하드웨어 성능 카운터에 의존하지 않고 작동하므로, 탐지 정확성과 계산 효율성을 균형 있게 유지해야 하는 저전력 및 자원 제한 시스템에 특히 적합하다.

Abstract: AES-128 encryption is theoretically secure but vulnerable in practical
deployments due to timing and fault injection attacks on embedded systems. This
work presents a lightweight dual-detection framework combining statistical
thresholding and machine learning (ML) for real-time anomaly detection. By
simulating anomalies via delays and ciphertext corruption, we collect timing
and data features to evaluate two strategies: (1) a statistical threshold
method based on execution time and (2) a Random Forest classifier trained on
block-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show
that the ML approach outperforms static thresholds in accuracy, while
maintaining real-time feasibility on embedded platforms. The framework operates
without modifying AES internals or relying on hardware performance counters.
This makes it especially suitable for low-power, resource-constrained systems
where detection accuracy and computational efficiency must be balanced.

</details>


### [15] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: 이 논문은 자연어-코드 생성에 대한 딥러닝 모델의 데이터 오염 탐지 방법의 효과성을 분석합니다. 여러 모델에 대해 트리거 없는 오염을 감지하는 데 어려움이 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 모델이 대량의 데이터에 의존하고 있으며, 이는 데이터 오염 공격에 취약함을 드러냅니다.

Method: CodeBERT, CodeT5+, AST-T5의 세 가지 딥러닝 모델에 대해 타겟 오염을 수행하고, 스펙트럼 서명 분석, 활성화 클러스터링 및 정적 분석을 방어 방법으로 평가합니다.

Result: 모든 방법이 트리거 없는 오염을 감지하는 데 어려움을 겪으며, 표현 기반 접근 방식이 오염된 샘플을 분리하지 못하고 정적 분석에서 거짓 긍정 및 거짓 부정이 발생합니다.

Conclusion: AI 지원 코드 생성을 위한 더 강력하고 트리거 독립적인 방어의 필요성이 강조됩니다.

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>


### [16] [I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks](https://arxiv.org/abs/2508.21654)
*Daryna Oliynyk,Rudolf Mayer,Kathrin Grosse,Andreas Rauber*

Main category: cs.CR

TL;DR: 이 논문은 모델 탈취 공격을 설계하고 평가하기 위한 권장 사항을 제공하여 이 분야의 표준화를 촉진합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 모델의 기밀성을 위협하는 모델 탈취 공격의 설계 및 평가가 표준화되지 않아 이전 연구를 비교하고 진전을 평가하는 데 어려움이 있다.

Method: 우리는 이미지 분류 모델을 공격하는 데 의존하는 대규모 공격 그룹을 연구하고, 포괄적인 위협 모델을 제안하며 공격 비교를 위한 프레임워크를 개발한다.

Result: 공격 설정을 분석하여 최상의 개발 관행을 제시하고 모델 탈취 공격 평가에 관한 기존 연구 질문의 광범위한 목록을 도출한다.

Conclusion: 이 연구 결과는 다른 문제 영역에도 적용 가능하여 모델 탈취 공격을 위한 첫 번째 일반 평가 방법론을 확립한다.

Abstract: Model stealing attacks endanger the confidentiality of machine learning
models offered as a service. Although these models are kept secret, a malicious
party can query a model to label data samples and train their own substitute
model, violating intellectual property. While novel attacks in the field are
continually being published, their design and evaluations are not standardised,
making it challenging to compare prior works and assess progress in the field.
This paper is the first to address this gap by providing recommendations for
designing and evaluating model stealing attacks. To this end, we study the
largest group of attacks that rely on training a substitute model -- those
attacking image classification models. We propose the first comprehensive
threat model and develop a framework for attack comparison. Further, we analyse
attack setups from related works to understand which tasks and models have been
studied the most. Based on our findings, we present best practices for attack
development before, during, and beyond experiments and derive an extensive list
of open research questions regarding the evaluation of model stealing attacks.
Our findings and recommendations also transfer to other problem domains, hence
establishing the first generic evaluation methodology for model stealing
attacks.

</details>


### [17] [Cybersecurity AI: Hacking the AI Hackers via Prompt Injection](https://arxiv.org/abs/2508.21669)
*Víctor Mayoral-Vilches,Per Mannermaa Rynning*

Main category: cs.CR

TL;DR: AI 기반 사이버 보안 도구가 프롬프트 주입 공격에 의해 스스로 공격받을 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안을 위한 AI 도구의 취약성을 이해하고 공격에 대한 대응 방안을 모색하기 위해.

Method: AI 에이전트를 이용하여 취약점을 찾고 악성 웹 서버와 상호작용하며, 사이버 보안 AI(CAI) 프레임워크와 CLI 도구에 대한 개념증명 exploit을 제시한다.

Result: 프롬프트 주입이 LLM 기반 아키텍처에서 반복적이고 체계적인 문제임을 발견하였다.

Conclusion: 이 문제를 해결하기 위해 사이버 보안 커뮤니티가 전통적 웹 애플리케이션의 XSS와 같은 전용 작업을 수행해야 할 것이다.

Abstract: We demonstrate how AI-powered cybersecurity tools can be turned against
themselves through prompt injection attacks. Prompt injection is reminiscent of
cross-site scripting (XSS): malicious text is hidden within seemingly trusted
content, and when the system processes it, that text is transformed into
unintended instructions. When AI agents designed to find and exploit
vulnerabilities interact with malicious web servers, carefully crafted reponses
can hijack their execution flow, potentially granting attackers system access.
We present proof-of-concept exploits against the Cybersecurity AI (CAI)
framework and its CLI tool, and detail our mitigations against such attacks in
a multi-layered defense implementation. Our findings indicate that prompt
injection is a recurring and systemic issue in LLM-based architectures, one
that will require dedicated work to address, much as the security community has
had to do with XSS in traditional web applications.

</details>


### [18] [OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization](https://arxiv.org/abs/2508.21727)
*Jiazheng Xing,Hai Ci,Hongbin Xu,Hangjie Yuan,Yong Liu,Mike Zheng Shou*

Main category: cs.CR

TL;DR: OptMark는 강력한 다중 비트 워터마크를 확산 노이즈 제거 과정에 삽입하는 최적화 기반 접근법입니다.


<details>
  <summary>Details</summary>
Motivation: 저작권 보호와 사용자 추적을 위해 확산 생성 이미지에 워터마킹하는 것이 중요합니다.

Method: OptMark는 확산 노이즈 제거 과정의 중간 고수준 표현에 강력한 다중 비트 워터마크를 삽입하는 최적화 기반 방법을 제안합니다. 초기에는 생성 공격에 저항하기 위해 구조적 워터마크를 삽입하고, 후반부에는 이미지 변환에 저항하기 위해 세부 워터마크를 삽입합니다.

Result: 실험 결과, OptMark는 값을 측정하는 변환, 기하학적 변환, 편집 및 재생산 공격에 대한 강한 저항력을 보장하면서 보이지 않는 다중 비트 워터마킹을 달성합니다.

Conclusion: OptMark는 메모리 사용을 O(N)에서 O(1)로 줄이면서 메모리 소비 문제를 해결하기 위해 접속 그래디언트 방법을 통합합니다.

Abstract: Watermarking diffusion-generated images is crucial for copyright protection
and user tracking. However, current diffusion watermarking methods face
significant limitations: zero-bit watermarking systems lack the capacity for
large-scale user tracking, while multi-bit methods are highly sensitive to
certain image transformations or generative attacks, resulting in a lack of
comprehensive robustness. In this paper, we propose OptMark, an
optimization-based approach that embeds a robust multi-bit watermark into the
intermediate latents of the diffusion denoising process. OptMark strategically
inserts a structural watermark early to resist generative attacks and a detail
watermark late to withstand image transformations, with tailored regularization
terms to preserve image quality and ensure imperceptibility. To address the
challenge of memory consumption growing linearly with the number of denoising
steps during optimization, OptMark incorporates adjoint gradient methods,
reducing memory usage from O(N) to O(1). Experimental results demonstrate that
OptMark achieves invisible multi-bit watermarking while ensuring robust
resilience against valuemetric transformations, geometric transformations,
editing, and regeneration attacks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 본 연구는 거래 상대방 클러스터링을 위한 하이브리드 문자열 유사성 및 주제 모델링 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 은행 지급 메시지 시스템에서의 거래 상대방 클러스터링의 필요성과 기존 자연어 처리 모델의 한계를 해결하기 위해.

Method: 하이브리드 문자열 유사성, 주제 모델링, 계층적 클러스터링 및 규칙 기반 파이프라인을 사용.

Result: 실제 라벨이 있는 데이터셋에서 기존 규칙 기반 접근 방식보다 성능이 상당히 향상됨을 보여준다.

Conclusion: 매뉴얼 검토 필요성을 줄이고, 미비한 개체 변형 위험을 보다 잘 관리할 수 있는 접근 방식을 제공한다.

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [20] [Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI](https://arxiv.org/abs/2508.21101)
*Dilruk Perera,Gousia Habib,Qianyi Xu,Daniel J. Tan,Kai He,Erik Cambria,Mengling Feng*

Main category: cs.LG

TL;DR: 강화 학습(RL)은 의료 분야에서 인공지능 적용 방식의 근본적인 변화를 나타냅니다.


<details>
  <summary>Details</summary>
Motivation: 의료에서 RL의 도입은 단순한 예측을 넘어, 장기 목표에 따른 개입 결정을 할 수 있는 가능성을 제공합니다.

Method: 모델 기반 및 비모델 기반 방법, 오프라인 및 배치 제한 접근 방식을 포함한 RL 기술의 전반적인 환경을 구조화하고, 보상 명세 및 불확실성 보정에 대한 새롭게 떠오르는 전략을 분석합니다.

Result: 증급 치료, 만성 질환, 정신 건강, 진단, 로봇 보조 등 다양한 분야에서 RL의 응용을 포괄적으로 분석하고, 그 트렌드와 격차, 전이 병목 현상을 식별합니다.

Conclusion: 이 논문은 안전하고 인간 중심의 정책 학습을 위한 교훈을 종합하며, RL이 의료 AI에서 변혁적인 역할을 수행할 수 있는 가능성을 반영합니다.

Abstract: Reinforcement learning (RL) marks a fundamental shift in how artificial
intelligence is applied in healthcare. Instead of merely predicting outcomes,
RL actively decides interventions with long term goals. Unlike traditional
models that operate on fixed associations, RL systems learn through trial,
feedback, and long-term reward optimization, introducing transformative
possibilities and new risks. From an information fusion lens, healthcare RL
typically integrates multi-source signals such as vitals, labs clinical notes,
imaging and device telemetry using temporal and decision-level mechanisms.
These systems can operate within centralized, federated, or edge architectures
to meet real-time clinical constraints, and naturally span data, features and
decision fusion levels. This survey explore RL's rise in healthcare as more
than a set of tools, rather a shift toward agentive intelligence in clinical
environments. We first structure the landscape of RL techniques including
model-based and model-free methods, offline and batch-constrained approaches,
and emerging strategies for reward specification and uncertainty calibration
through the lens of healthcare constraints. We then comprehensively analyze RL
applications spanning critical care, chronic disease, mental health,
diagnostics, and robotic assistance, identifying their trends, gaps, and
translational bottlenecks. In contrast to prior reviews, we critically analyze
RL's ethical, deployment, and reward design challenges, and synthesize lessons
for safe, human-aligned policy learning. This paper serves as both a a
technical roadmap and a critical reflection of RL's emerging transformative
role in healthcare AI not as prediction machinery, but as agentive clinical
intelligence.

</details>


### [21] [Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning](https://arxiv.org/abs/2508.21103)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: 이 논문은 EEG 기반 감정 분류를 위한 다중 해상도 프레임워크를 제시하며, 다양한 모델을 평가하여 LSTM-GRU가 가장 높은 성능을 보인다는 결과를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구는 이진 감정 예측이나 개인별 분류에만 집중하여 일반화 가능성과 실제 적용을 제한하고 있다.

Method: GAMEEMO 데이터셋을 기반으로 하는 다중 해상도 EEG 감정 분류 프레임워크를 개발하였고, 구조화된 전처리 전략과 다양한 모델을 평가하였다.

Result: LSTM-GRU 모델이 이진 감정 과제에서 0.932의 F1 점수를 기록하고, 다중 클래스 및 다중 레이블 감정 분류에서 각각 94.5% 및 90.6%의 성능을 보였다.

Conclusion: 이 연구는 전통적인 방법론의 한계를 극복하고, EEG 기반 감정 인식의 일반화 가능성을 높인다.

Abstract: Recent advancements in EEG-based emotion recognition have shown promising
outcomes using both deep learning and classical machine learning approaches;
however, most existing studies focus narrowly on binary valence prediction or
subject-specific classification, which limits generalizability and deployment
in real-world affective computing systems. To address this gap, this paper
presents a unified, multigranularity EEG emotion classification framework built
on the GAMEEMO dataset, which consists of 14-channel EEG recordings and
continuous self-reported emotion ratings (boring, horrible, calm, and funny)
from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline
employs a structured preprocessing strategy that comprises temporal window
segmentation, hybrid statistical and frequency-domain feature extraction, and
z-score normalization to convert raw EEG signals into robust, discriminative
input vectors. Emotion labels are derived and encoded across three
complementary axes: (i) binary valence classification based on the averaged
polarity of positive and negative emotion ratings, and (ii) Multi-class emotion
classification, where the presence of the most affective state is predicted.
(iii) Fine-grained multi-label representation via binning each emotion into 10
ordinal classes. We evaluate a broad spectrum of models, including Random
Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM,
LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently
outperforms the others, achieving an F1-score of 0.932 in the binary valence
task and 94.5% and 90.6% in both multi-class and Multi-Label emotion
classification.

</details>


### [22] [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)
*Wenfeng Feng,Penghong Zhao,Guochao Jiang,Chuzhan Hao,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: PVPO는 효율적인 강화 학습 방법으로, 정책 내부 비교로 인한 누적 편향을 수정하고 롤아웃 수를 줄이며 고수익 데이터를 선택하여 훈련 효율성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 비판 없는 강화 학습 방법은 복잡한 작업에서 효율성을 위해 주목받고 있지만, 정책이 지역 최적에 빠지고 계산 비용이 증가하는 문제가 있습니다.

Method: PVPO는 이점 참조 앵커와 데이터 사전 샘플링을 이용하여 예상되는 보상 점수를 참조 앵커로 사용하며, 참조 모델을 사용하여 사전 롤아웃을 수행합니다.

Result: PVPO는 9개의 데이터셋에서 State-Of-The-Art (SOTA) 성능을 달성하며, 여러 작업에서 강력한 일반화를 보여줍니다.

Conclusion: PVPO는 다양한 규모의 모델에 대해 확장 가능한 성능을 보여줍니다.

Abstract: Critic-free reinforcement learning methods, particularly group policies, have
attracted considerable attention for their efficiency in complex tasks.
However, these methods rely heavily on multiple sampling and comparisons within
the policy to estimate advantage, which may cause the policy to fall into local
optimum and increase computational cost. To address these issues, we propose
PVPO, an efficient reinforcement learning method enhanced by an advantage
reference anchor and data pre-sampling. Specifically, we use the reference
model to rollout in advance and employ the calculated reward score as a
reference anchor. Our approach effectively corrects the cumulative bias
introduced by intra-group comparisons and significantly reduces reliance on the
number of rollouts. Meanwhile, the reference model can assess sample difficulty
during data pre-sampling, enabling effective selection of high-gain data to
improve training efficiency. Experiments conducted on nine datasets across two
domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our
approach not only demonstrates robust generalization across multiple tasks, but
also exhibits scalable performance across models of varying scales.

</details>


### [23] [Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models](https://arxiv.org/abs/2508.21106)
*Tatyana Matveeva,Aleksandr Katrutsa,Evgeny Frolov*

Main category: cs.LG

TL;DR: AdaGram은 효율적인 전체 행렬 적응형 경량 업데이트를 가능하게 하는 새로운 최적화기이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 최적화에서 널리 사용되는 적응형 경량 방법들이 매개변수 상관관계를 포착하는 기능이 제한된다.

Method: 각 반복에서 전처리된 업데이트 방향을 계산하기 위해 빠른 대칭 인수를 활용하고, 최적화 경로를 따라 전처리기의 저차원 구조를 유지한다.

Result: AdaGram은 표준 기계 학습 작업에서 더 빠르게 수렴하거나 대각선 적응형 최적화기의 성능을 일치시킨다.

Conclusion: AdaGram은 대규모 모델에 대한 적응형 최적화의 확장 가능한 솔루션으로 적합하다.

Abstract: Adaptive gradient methods like Adagrad and its variants are widespread in
large-scale optimization. However, their use of diagonal preconditioning
matrices limits the ability to capture parameter correlations. Full-matrix
adaptive methods, approximating the exact Hessian, can model these correlations
and may enable faster convergence. At the same time, their computational and
memory costs are often prohibitive for large-scale models. To address this
limitation, we propose AdaGram, an optimizer that enables efficient full-matrix
adaptive gradient updates. To reduce memory and computational overhead, we
utilize fast symmetric factorization for computing the preconditioned update
direction at each iteration. Additionally, we maintain the low-rank structure
of a preconditioner along the optimization trajectory using matrix integrator
methods. Numerical experiments on standard machine learning tasks show that
AdaGram converges faster or matches the performance of diagonal adaptive
optimizers when using rank five and smaller rank approximations. This
demonstrates AdaGram's potential as a scalable solution for adaptive
optimization in large models.

</details>


### [24] [An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity](https://arxiv.org/abs/2508.21109)
*Georgios Vamvouras,Konstantinos Braimakis,Christos Tzivanidis*

Main category: cs.LG

TL;DR: 본 논문은 스마트 HVAC 시스템의 모델 예측 제어(MPC)를 지원하기 위한 온도, 태양 복사선 및 상대 습도의 48시간 예측을 위한 딥 러닝 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 스마트 HVAC 시스템에서의 모델 예측 제어를 지원하기 위해 정확한 기상 예측이 필요합니다.

Method: 억제된 양방향 장단기 메모리(BiLSTM) 네트워크와 주의 메커니즘을 활용하여 세 가지 변수를 함께 예측합니다.

Result: 모델은 평균 절대 오차 1.3도 섭씨(온도), 31 W/m2(복사선), 6.7 퍼센트 포인트(습도)를 기록하였으며, 최신 기상 예측 및 기계 학습 기준치를 초과했습니다.

Conclusion: 이 방법은 신뢰할 수 있는 단기 기상 예측을 통해 에너지 효율적인 건물 제어 가능성을 강조합니다.

Abstract: This paper presents a Deep Learning (DL) framework for 48-hour forecasting of
temperature, solar irradiance, and relative humidity to support Model
Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked
Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing
temporal and cross-feature dependencies by jointly predicting all three
variables. Historical meteorological data (2019-2022) with encoded cyclical
time features were used for training, while 2023 data evaluated generalization.
The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature),
31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming
state-of-the-art numerical weather prediction and machine learning benchmarks.
Integrated Gradients quantified feature contributions, and attention weights
revealed temporal patterns, enhancing interpretability. By combining
multivariate forecasting, attention-based DL, and explainability, this work
advances data-driven weather prediction. The demonstrated accuracy and
transparency highlight the framework's potential for energy-efficient building
control through reliable short-term meteorological forecasting.

</details>


### [25] [Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI](https://arxiv.org/abs/2508.21111)
*Evan J. Chou,Lisa S. Locke,Harvey M. Soldan*

Main category: cs.LG

TL;DR: 이 연구는 NASA의 심우주 네트워크(DSN) 데이터에서 장비 노화 및 이상을 탐지하기 위한 머신러닝 기법과 강화학습 시스템을 활용한 데이터 파이프라인 구축에 관한 것이다.


<details>
  <summary>Details</summary>
Motivation: DSN은 오랜 시간에 걸쳐 열화되는 안테나 시설을 포함하고 있으며, 이는 데이터 흐름에 중대한 장애를 초래할 수 있다. 따라서, JPL 엔지니어들이 데이터를 통해 이상 및 장비 열화를 직접 식별하는 방법을 실험하고자 하였다.

Method: 다양한 머신러닝 기법을 연구하였고, 통계적 계산 및 임계값을 통해 실시간 데이터 세트 내의 이상 데이터를 결정하였다. 또한, 이상을 심각도 수준으로 분류하는 강화학습 하위 시스템과 각 이상 데이터 항목에 대한 설명을 레이블링하는 대형 언어 모델을 통합하였다.

Result: DSN 전송기를 위한 전체 데이터 파이프라인 시스템을 구현하였고, 이를 통해 DSN 안테나 데이터에서 학습된 모델과 연결하여 DSN 이상 탐지의 데이터 워크플로우를 완성하였다.

Conclusion: 이 모든 과정은 복잡한 추론을 이용하여 이상 데이터를 분류하고 예측하는 행위적 AI 시스템에 의해 연결되었다.

Abstract: The Deep Space Network (DSN) is NASA's largest network of antenna facilities
that generate a large volume of multivariate time-series data. These facilities
contain DSN antennas and transmitters that undergo degradation over long
periods of time, which may cause costly disruptions to the data flow and
threaten the earth-connection of dozens of spacecraft that rely on the Deep
Space Network for their lifeline. The purpose of this study was to experiment
with different methods that would be able to assist JPL engineers with directly
pinpointing anomalies and equipment degradation through collected data, and
continue conducting maintenance and operations of the DSN for future space
missions around our universe. As such, we have researched various machine
learning techniques that can fully reconstruct data through predictive
analysis, and determine anomalous data entries within real-time datasets
through statistical computations and thresholds. On top of the fully trained
and tested machine learning models, we have also integrated the use of a
reinforcement learning subsystem that classifies identified anomalies based on
severity level and a Large Language Model that labels an explanation for each
anomalous data entry, all of which can be improved and fine-tuned over time
through human feedback/input. Specifically, for the DSN transmitters, we have
also implemented a full data pipeline system that connects the data extraction,
parsing, and processing workflow all together as there was no coherent program
or script for performing these tasks before. Using this data pipeline system,
we were able to then also connect the models trained from DSN antenna data,
completing the data workflow for DSN anomaly detection. This was all wrapped
around and further connected by an agentic AI system, where complex reasoning
was utilized to determine the classifications and predictions of anomalous
data.

</details>


### [26] [Adaptive LLM Routing under Budget Constraints](https://arxiv.org/abs/2508.21141)
*Pranoy Panda,Raghav Magazine,Chaitanya Devaguptapu,Sho Takemori,Vishal Sharma*

Main category: cs.LG

TL;DR: 이 논문은 LLM(대형 언어 모델) 라우팅을 컨텍스트 밴딧 문제로 연구하여, 사용자 쿼리와 작업에 가장 적합한 LLM을 동적으로 선택하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 다양한 능력과 비용이 실제 응용에서 도전 과제가 된다.

Method: LLM 라우팅을 컨텍스트 밴딧 문제로 다루고, 쿼리와 LLM 간의 친화성을 반영하는 공유 임베딩 공간을 개발했다.

Result: PILOT이라는 새로운 LinUCB 확장을 통해 이를 구체화하며, 다양한 사용자 예산을 처리하기 위한 온라인 비용 정책을 도입했다.

Conclusion: 이 접근 방식은 리소스를 효율적으로 사용할 수 있는 라우팅을 보장한다.

Abstract: Large Language Models (LLMs) have revolutionized natural language processing,
but their varying capabilities and costs pose challenges in practical
applications. LLM routing addresses this by dynamically selecting the most
suitable LLM for each query/task. Previous approaches treat this as a
supervised learning problem, assuming complete knowledge of optimal query-LLM
pairings. However, real-world scenarios lack such comprehensive mappings and
face evolving user queries. We thus propose to study LLM routing as a
contextual bandit problem, enabling adaptive decision-making using bandit
feedback without requiring exhaustive inference across all LLMs for all queries
(in contrast to supervised routing). To address this problem, we develop a
shared embedding space for queries and LLMs, where query and LLM embeddings are
aligned to reflect their affinity. This space is initially learned from offline
human preference data and refined through online bandit feedback. We
instantiate this idea through Preference-prior Informed Linucb fOr adaptive
rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets
for model routing, we introduce an online cost policy modeled as a multi-choice
knapsack problem, ensuring resource-efficient routing.

</details>


### [27] [Privacy Auditing Synthetic Data Release through Local Likelihood Attacks](https://arxiv.org/abs/2508.21146)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 본 연구는 합성 데이터의 개인 정보 유출 감사를 위한 새로운 공격 기법(Gen-LRA)을 제안하며, 이는 생성 모델의 과적합을 악용하여 개인 정보의 노출을 감지하는 데 효과적이다.


<details>
  <summary>Details</summary>
Motivation: 합성 데이터의 개인 정보 유출 감사는 중요하지만 해결되지 않은 문제이다.

Method: 우리는 합성 데이터에서 훈련 분포의 특정 영역에 대한 과적합을 악용하는 회원 추론 공격(MIA)을 설계하고, 모델 지식이나 접근 없이 테스트 관찰이 대리 모델의 지역 우도 비율 추정에 미치는 영향을 평가하는 Generative Likelihood Ratio Attack (Gen-LRA)을 제안한다.

Result: 다양한 데이터세트, 모델 아키텍처, 공격 매개변수를 포괄하는 종합 벤치마크에서 평가한 결과, Gen-LRA는 여러 성능 지표에서 생성 모델의 다른 MIA보다 일관되게 우세하다.

Conclusion: 이 결과들은 생성 모델의 과적합이 실제 애플리케이션에서 큰 개인 정보 위험을 초래함을 강조하며, Gen-LRA가 합성 데이터의 개인 정보 감사 도구로서 효과적임을 나타낸다.

Abstract: Auditing the privacy leakage of synthetic data is an important but unresolved
problem. Most existing privacy auditing frameworks for synthetic data rely on
heuristics and unreasonable assumptions to attack the failure modes of
generative models, exhibiting limited capability to describe and detect the
privacy exposure of training data through synthetic data release. In this
paper, we study designing Membership Inference Attacks (MIAs) that specifically
exploit the observation that tabular generative models tend to significantly
overfit to certain regions of the training distribution. Here, we propose
Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally
efficient No-Box MIA that, with no assumption of model knowledge or access,
formulates its attack by evaluating the influence a test observation has in a
surrogate model's estimation of a local likelihood ratio over the synthetic
data. Assessed over a comprehensive benchmark spanning diverse datasets, model
architectures, and attack parameters, we find that Gen-LRA consistently
dominates other MIAs for generative models across multiple performance metrics.
These results underscore Gen-LRA's effectiveness as a privacy auditing tool for
the release of synthetic data, highlighting the significant privacy risks posed
by generative model overfitting in real-world applications.

</details>


### [28] [Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks](https://arxiv.org/abs/2508.21172)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: Deep Residual Echo State Networks (DeepResESNs)는 장기 정보 처리를 개선하는 새로운 깊은 비훈련 RNN의 클래스이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 에코 상태 네트워크(ESNs)는 장기 정보 처리에서 어려움을 겪고 있기 때문에 새로운 접근 방식이 필요하다.

Method: 시간적 잔여 연결을 기반으로 한 깊은 비훈련 RNN을 제안하고, 다양한 직교 구성에서의 효과를 연구하였다.

Result: DeepResESNs는 기억 용량 및 장기 시간 모형화에서 현저한 향상을 보였다.

Conclusion: DeepResESNs는 전통적인 얕은 및 깊은 저수지 컴퓨팅보다 우수한 성능을 보여주었다.

Abstract: Echo State Networks (ESNs) are a particular type of untrained Recurrent
Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular
for their fast and efficient learning. However, traditional ESNs often struggle
with long-term information processing. In this paper, we introduce a novel
class of deep untrained RNNs based on temporal residual connections, called
Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a
hierarchy of untrained residual recurrent layers significantly boosts memory
capacity and long-term temporal modeling. For the temporal residual
connections, we consider different orthogonal configurations, including
randomly generated and fixed-structure configurations, and we study their
effect on network dynamics. A thorough mathematical analysis outlines necessary
and sufficient conditions to ensure stable dynamics within DeepResESN. Our
experiments on a variety of time series tasks showcase the advantages of the
proposed approach over traditional shallow and deep RC.

</details>


### [29] [FUTURE: Flexible Unlearning for Tree Ensemble](https://arxiv.org/abs/2508.21181)
*Ziheng Chen,Jin Huang,Jiali Cheng,Yuchan Guo,Mengjie Wang,Lalitesh Morishetti,Kaushiki Nag,Hadi Amiri*

Main category: cs.LG

TL;DR: FUTURE는 트리 앙상블을 위한 새로운 비학습 알고리즘으로, 샘플을 잊는 문제를 그래디언트 기반 최적화 작업으로 공식화하여 효율적인 비학습을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 프라이버시와 '잊혀질 권리'에 대한 강조가 커짐에 따라 트리 앙상블이 민감한 정보를 잊도록 하는 비학습 알고리즘의 필요성이 대두되었다.

Method: 샘플을 잊는 문제를 그래디언트 기반 최적화 작업으로 공식화하고, 최적화 프레임워크 내에서 확률적 모델 근사를 채택하여 비차별 가능성을 처리한다.

Result: 실제 데이터셋에 대한 광범위한 실험 결과, FUTURE가 상당하고 성공적인 비학습 성과를 낸다.

Conclusion: FUTURE는 복잡한 앙상블에 일반화 가능하고 대규모 데이터셋을 위한 효율적인 비학습을 제공한다.

Abstract: Tree ensembles are widely recognized for their effectiveness in
classification tasks, achieving state-of-the-art performance across diverse
domains, including bioinformatics, finance, and medical diagnosis. With
increasing emphasis on data privacy and the \textit{right to be forgotten},
several unlearning algorithms have been proposed to enable tree ensembles to
forget sensitive information. However, existing methods are often tailored to a
particular model or rely on the discrete tree structure, making them difficult
to generalize to complex ensembles and inefficient for large-scale datasets. To
address these limitations, we propose FUTURE, a novel unlearning algorithm for
tree ensembles. Specifically, we formulate the problem of forgetting samples as
a gradient-based optimization task. In order to accommodate
non-differentiability of tree ensembles, we adopt the probabilistic model
approximations within the optimization framework. This enables end-to-end
unlearning in an effective and efficient manner. Extensive experiments on
real-world datasets show that FUTURE yields significant and successful
unlearning performance.

</details>


### [30] [Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium](https://arxiv.org/abs/2508.21186)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 대형 언어 모델에서의 디코딩 과정은 토큰 점수화와 소프트맥스 정규화로 설명된다. 본 논문은 이를 확률 심플렉스에서의 제약된 변분 원리로 다룬다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 대형 언어 모델의 디코딩 과정에 대한 명확한 원리를 제시하고자 한다.

Method: 토큰 분포의 변화를 분석하기 위해 고전적인 곱셈 가중치 업데이트와 복제 흐름을 활용한다.

Result: 정해진 컨텍스트와 온도에서 다음 토큰 분포가 심플렉스 내에서 매끄러운 경로를 따르고 소프트맥스 평형에 수렴함을 증명했다.

Conclusion: 온도는 시간의 정확한 재조정을 제공하며, top-k 및 nucleus 샘플링은 동일한 보장을 가진 면으로 흐름을 제한한다.

Abstract: Decoding in large language models is often described as scoring tokens and
normalizing with softmax. We give a minimal, self-contained account of this
step as a constrained variational principle on the probability simplex. The
discrete, normalization-respecting ascent is the classical
multiplicative-weights (entropic mirror) update; its continuous-time limit is
the replicator flow. From these ingredients we prove that, for a fixed context
and temperature, the next-token distribution follows a smooth trajectory inside
the simplex and converges to the softmax equilibrium. This formalizes the
common ``manifold traversal'' intuition at the output-distribution level. The
analysis yields precise, practice-facing consequences: temperature acts as an
exact rescaling of time along the same trajectory, while top-k and nucleus
sampling restrict the flow to a face with identical guarantees. We also outline
a controlled account of path-dependent score adjustments and their connection
to loop-like, hallucination-style behavior. We make no claims about training
dynamics or internal representations; those are deferred to future work.

</details>


### [31] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 강화 학습을 대형 언어 모델에 적용한 최신 발전은 주목할 만한 성과를 거두었지만, 이러한 현상이 발생하는 조건은 불명확하다. 본 연구에서는 모델과 작업 간의 정렬이 강한 경우에만 이러한 비직관적 결과가 나타난다는 것을 규명하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델에 대한 강화 학습 적용의 발전은 놀라운 현상들을 초래했지만, 이들 현상이 발생하는 조건에 대한 이해가 필요하다.

Method: 우리는 각기 다른 모델 구조와 작업 영역에서 철저한 실험 검증을 통해 비직관적 주장들에 대한 체계적이고 포괄적인 검토를 수행하였다.

Result: 연구 결과, 표준 강화 학습 훈련은 다양한 설정에서 일관되게 강력하나, 많은 비직관적 결과는 모델과 작업 간의 강한 정렬이 있을 때만 발생한다는 것을 보여주었다.

Conclusion: 이러한 기법들은 더 많은 도전적 상황에서는 실질적인 학습을 이끌어내지 못하며, 표준 강화 학습 방법이 여전히 효과적이다.

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [32] [Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay](https://arxiv.org/abs/2508.21240)
*Pujan Thapa,Alexander Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 본 연구는 메모리 효율적인 리플레이를 가능하게 하는 새로운 생성적 지속 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 메모리 기반 방법과 메모리 없는 방법들의 한계를 초월하는 접근법을 찾아 지속 학습의 효율성을 높이고자 함.

Method: 자기 조직화 지도(SOM)와 변별 오토인코더(VAE)를 기반으로 하는 프레임워크를 설계하여 각 SOM 유닛의 동작을 통해 합성 샘플을 생성.

Result: CIFAR-10 및 CIFAR-100에서 기존 단일 클래스 증가 성능을 각각 $10\%$ 및 $7\%$ 향상시키며, 최신 메모리 기반 방법과 경쟁력을 갖춤.

Conclusion: 본 연구는 지속 학습을 위한 확장 가능하고 메모리 비용이 적은 솔루션으로서의 잠재력을 보여준다.

Abstract: This work introduces a novel generative continual learning framework based on
self-organizing maps (SOMs) and variational autoencoders (VAEs) to enable
memory-efficient replay, eliminating the need to store raw data samples or task
labels. For high-dimensional input spaces, such as of CIFAR-10 and CIFAR-100,
we design a scheme where the SOM operates over the latent space learned by a
VAE, whereas, for lower-dimensional inputs, such as those found in MNIST and
FashionMNIST, the SOM operates in a standalone fashion. Our method stores a
running mean, variance, and covariance for each SOM unit, from which synthetic
samples are then generated during future learning iterations. For the VAE-based
method, generated samples are then fed through the decoder to then be used in
subsequent replay. Experimental results on standard class-incremental
benchmarks show that our approach performs competitively with state-of-the-art
memory-based methods and outperforms memory-free methods, notably improving
over best state-of-the-art single class incremental performance on CIFAR-10 and
CIFAR-100 by nearly $10$\% and $7$\%, respectively. Our methodology further
facilitates easy visualization of the learning process and can also be utilized
as a generative model post-training. Results show our method's capability as a
scalable, task-label-free, and memory-efficient solution for continual
learning.

</details>


### [33] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 이 논문은 자동차 설계 및 최적화 과정에서의 고충인 고충실 CFD 시뮬레이션의 계산 비용을 줄이기 위한 새로운 메타 학습 프레임워크를 제안한다. 이 프레임워크는 다양한 신경망 아키텍처를 활용하여 세 가지 상이한 상태의 대리 모델의 예측을 조화롭게 결합하도록 한다.


<details>
  <summary>Details</summary>
Motivation: 자동차 설계 및 최적화 사이클에서 고충실 CFD 시뮬레이션의 계산 비용이 중요한 병목 현상을 야기하고 있다.

Method: 이 논문은 Mixture of Experts (MoE) 모델을 도입하며, 이는 3개의 이질적이고 최첨단 대리 모델의 예측을 동적으로 결합하기 위해 전용 게이팅 네트워크를 사용하는 방법이다.

Result: MoE 모델은 L-2 예측 오류를 현저히 줄이고, 모든 평가된 물리적 양에서 앙상블 평균 뿐만 아니라 가장 정확한 개별 전문가 모델보다 우수한 성능을 보인다.

Conclusion: 이 연구는 MoE 프레임워크가 특화된 아키텍처의 상호 보완적인 강점을 결합하여 보다 강력하고 정확한 복합 대리 모델을 생성하기 위한 효과적인 전략으로 자리잡히게 한다.

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [34] [RelP: Faithful and Efficient Circuit Discovery via Relevance Patching](https://arxiv.org/abs/2508.21258)
*Farnoush Rezaei Jafari,Oliver Eberle,Ashkan Khakzar,Neel Nanda*

Main category: cs.LG

TL;DR: Relevance Patching (RelP)은 Layer-wise Relevance Propagation (LRP)에서 유래한 전파 계수를 사용하여, 활성화 패칭보다 더 효과적이고 신뢰성 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 활성화 패칭은 모델의 특정 행동에 대한 구성 요소를 찾아내는 데 유용하지만, 대규모로 적용하기에는 계산 비용이 많이 든다. 이를 해결하기 위해 저자는 RelP를 도입했다.

Method: RelP는 활성화 패칭의 지역 기울기를 LRP에서 유래한 전파 계수로 대체하여, 두 번의 전방 통과와 한 번의 후방 통과만을 통해 계산 효율성을 유지하면서 신뢰성을 향상시킨다.

Result: RelP는 다양한 모델과 작업에서 테스트되어, 특히 간접 목적어 식별(IOI) 작업에서 MLP 출력 분석 시, 표준 기여 패칭보다 활성화 패칭에 더욱 잘 근사화됨을 보여준다.

Conclusion: RelP는 IG와 유사한 신뢰성을 유지하면서도 추가적인 계산 비용이 필요하지 않으므로, 보다 효과적이다.

Abstract: Activation patching is a standard method in mechanistic interpretability for
localizing the components of a model responsible for specific behaviors, but it
is computationally expensive to apply at scale. Attribution patching offers a
faster, gradient-based approximation, yet suffers from noise and reduced
reliability in deep, highly non-linear networks. In this work, we introduce
Relevance Patching (RelP), which replaces the local gradients in attribution
patching with propagation coefficients derived from Layer-wise Relevance
Propagation (LRP). LRP propagates the network's output backward through the
layers, redistributing relevance to lower-level components according to local
propagation rules that ensure properties such as relevance conservation or
improved signal-to-noise ratio. Like attribution patching, RelP requires only
two forward passes and one backward pass, maintaining computational efficiency
while improving faithfulness. We validate RelP across a range of models and
tasks, showing that it more accurately approximates activation patching than
standard attribution patching, particularly when analyzing residual stream and
MLP outputs in the Indirect Object Identification (IOI) task. For instance, for
MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation
of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by
RelP. Additionally, we compare the faithfulness of sparse feature circuits
identified by RelP and Integrated Gradients (IG), showing that RelP achieves
comparable faithfulness without the extra computational cost associated with
IG.

</details>


### [35] [Owen Sampling Accelerates Contribution Estimation in Federated Learning](https://arxiv.org/abs/2508.21261)
*Hossein KhademSohi,Hadi Hemmati,Jiayu Zhou,Steve Drew*

Main category: cs.LG

TL;DR: FedOwen은 효율적인 연합 학습 프레임워크로, 샤플리 값을 근사하여 클라이언트 기여도를 정확하게 추정하고, 최종 정확도를 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서는 각 클라이언트의 기여도를 정확하게 추정하는 것이 중요합니다. 이는 공정한 보상을 위해서 뿐만 아니라, 가장 유용한 클라이언트를 선택하여 글로벌 모델이 더 빠르게 수렴할 수 있도록 하기 위해서도 필요합니다.

Method: FedOwen은 오웬 샘플링을 사용하여 기존 방법과 동일한 평가 예산 내에서 샤플리 값을 근사합니다. 또한, 고급 클라이언트를 활용하는 동시에 저소득 클라이언트를 탐색하는 적응형 클라이언트 선택 전략을 사용합니다.

Result: 고정된 평가 비용 하에서, FedOwen은 비독립 동 거래 벤치마크에서 최첨단 기준선에 비해 같은 통신 라운드 내에서 최대 23% 더 높은 최종 정확도를 달성합니다.

Conclusion: FedOwen은 연합 학습의 효율성을 높이고, 더 유용한 클라이언트를 선택할 수 있도록 도와줍니다.

Abstract: Federated Learning (FL) aggregates information from multiple clients to train
a shared global model without exposing raw data. Accurately estimating each
client's contribution is essential not just for fair rewards, but for selecting
the most useful clients so the global model converges faster. The Shapley value
is a principled choice, yet exact computation scales exponentially with the
number of clients, making it infeasible for large federations. We propose
FedOwen, an efficient framework that uses Owen sampling to approximate Shapley
values under the same total evaluation budget as existing methods while keeping
the approximation error small. In addition, FedOwen uses an adaptive client
selection strategy that balances exploiting high-value clients with exploring
under-sampled ones, reducing bias and uncovering rare but informative data.
Under a fixed valuation cost, FedOwen achieves up to 23 percent higher final
accuracy within the same number of communication rounds compared to
state-of-the-art baselines on non-IID benchmarks.

</details>


### [36] [Guess-and-Learn (G&L): Measuring the Cumulative Error Cost of Cold-Start Adaptation](https://arxiv.org/abs/2508.21270)
*Roland Arnold*

Main category: cs.LG

TL;DR: 이 논문은 기계 학습 모델의 적응 비용을 측정하는 Guess-and-Learn (G&L) v1.0을 제안하며, 초기 단계에서의 효율성과 적응 속도를 분석한다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 모델의 평가가 최종 정확도에만 집중되어 적응 비용을 간과하는 문제를 해결하고자 한다.

Method: G&L v1.0은 콜드 스타트 적응력을 측정하며, 모델이 레이블이 없는 데이터셋을 순차적으로 레이블링하는 동안 발생하는 총 실수를 계산한다.

Result: MNIST 및 AG News에 대한 실험을 통해, 초기 단계에서 작은 모델이 적응하는 데 필요한 초기 오류가 적다는 것을 확인하였다.

Conclusion: G&L은 기존 벤치마크를 보완하며, 첫 번째 예제부터 신뢰할 수 있는 학습자를 개발하기 위한 재현 가능한 프레임워크를 제공한다.

Abstract: Evaluation of machine learning models typically emphasizes final accuracy,
overlooking the cost of adaptation: the cumulative errors incurred while
learning from scratch. Guess-and- Learn (G&L) v1.0 addresses this gap by
measuring cold-start adaptability - the total mistakes a model makes while
sequentially labeling an unlabeled dataset. At each step, the learner selects
an instance, predicts its label, receives the ground truth, and updates
parameters under either online (per-sample) or batch (delayed) mode. The
resulting error trajectory exposes adaptation speed, selection quality, and
bias - dynamics invisible to endpoint metrics.
  G&L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) to
disentangle the effects of initialization and update frequency. We formalize
the protocol, relate it to classical mistake-bound theory, and estimate a
heuristic "oracle reference band" for MNIST as a plausibility reference.
Baseline experiments on MNIST and AG News, spanning classical methods
(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), and
pretrained transformers (ViT-B/16, BERT-base), reveal systematic differences in
early-phase efficiency: smaller models can adapt with fewer initial errors,
while pretraining benefits vary by domain. Across settings, current models
remain well above the oracle band, highlighting an adaptability gap.
  By quantifying the mistake cost of early learning, G&L complements
conventional benchmarks and provides a reproducible framework for developing
learners that are not only accurate in the limit but also reliable from the
first examples.

</details>


### [37] [CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams](https://arxiv.org/abs/2508.21273)
*Ashok Devireddy,Shunping Huang*

Main category: cs.LG

TL;DR: CALM은 비정상형 시계열 스트림에서 실시간 이상 탐지를 위해 설계된 새로운 프레임워크로, 지속적이고 적응적인 미세 조정 메커니즘과 대형 언어 모델을 활용하여 높은 성능을 유지한다.


<details>
  <summary>Details</summary>
Motivation: 비정상형 시계열 스트림에서의 이상 탐지는 다양한 산업 및 과학 분야에서 중요한 과제이나, 개념 변화에 직면할 때 전통 모델이 성능 저하를 겪는다.

Method: CALM은 Apache Beam 분산 처리 프레임워크 위에서 구축되며, 타임스팜 기반의 모델을 활용하여 이상 탐지를 수행하고, 닫힌 루프의 연속 미세 조정 메커니즘과 LLM을 활용하여 이상 탐지 모델을 발전시킨다.

Result: 우리의 결과는 지속적으로 미세 조정된 모델이 대부분의 데이터셋에서 정적 사전 학습 모델에 비해 ROC AUC 점수를 개선함을 보여주며, 적응형 LLM 기반 접근 방식의 효과를 입증한다.

Conclusion: CALM은 동적 스트리밍 환경에서 고성능 이상 탐지를 유지하는 데 효과적인 접근법으로 평가되었다.

Abstract: The detection of anomalies in non-stationary time-series streams is a
critical but challenging task across numerous industrial and scientific
domains. Traditional models, trained offline, suffer significant performance
degradation when faced with concept drift, where the underlying statistical
properties of the data change over time. This paper introduces CALM
(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework for
real-time anomaly detection designed to address this challenge. CALM is built
on the Apache Beam distributed processing framework and leverages the TimesFm
foundation model for forecasting-based anomaly detection. The framework's
novelty lies in two core contributions. First, it implements a closed-loop,
continuous fine-tuning mechanism that allows the anomaly detection model to
adapt to evolving data patterns in near real-time. Second, it introduces an
LLM-as-a-Judge component, a Large Language Model that provides semantic,
context-aware judgments on detected anomalies to curate a high-quality training
dataset, deciding whether an anomaly represents transient noise or a meaningful
pattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Our
results demonstrate that the continuously fine-tuned model improves the ROC AUC
score in most datasets compared to the static, pre-trained base model,
validating the efficacy of our adaptive, LLM-guided approach to maintaining
high-performance anomaly detection in dynamic streaming environments.

</details>


### [38] [Detecting Domain Shifts in Myoelectric Activations: Challenges and Opportunities in Stream Learning](https://arxiv.org/abs/2508.21278)
*Yibin Sun,Nick Lim,Guilherme Weigert Cassales,Heitor Murilo Gomes,Bernhard Pfahringer,Albert Bifet,Anany Dwivedi*

Main category: cs.LG

TL;DR: 본 연구는 근전도 신호에서 도메인 변화를 탐지하는데 있어 데이터 스트림 학습 기법을 적용한 것으로, EMG 신호의 비정상성을 극복하기 위한 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 근전도(EMG) 신호의 비정상성으로 인해 근전도 활성의 도메인 변화를 탐지하는 것은 큰 도전 과제이다.

Method: Ninapro 데이터베이스의 DB6 데이터셋을 중심으로 커널 주성분 분석(KPCA)을 사용하여 도메인을 정의하고 사전 처리하여 변화를 강조한다.

Result: CUSUM, Page-Hinckley 및 ADWIN과 같은 여러 변위 감지 방법을 평가하여 EMG 신호의 실시간 도메인 변화를 감지하는 현재 기술의 한계를 드러냈다.

Conclusion: 우리의 결과는 EMG 디코딩 모델을 안정적으로 유지하기 위한 스트리밍 기반 접근 방식의 가능성을 강조하며, 실제 시나리오에서 강건성과 정확성을 높이기 위한 추가 연구의 필요성을 강조한다.

Abstract: Detecting domain shifts in myoelectric activations poses a significant
challenge due to the inherent non-stationarity of electromyography (EMG)
signals. This paper explores the detection of domain shifts using data stream
(DS) learning techniques, focusing on the DB6 dataset from the Ninapro
database. We define domains as distinct time-series segments based on different
subjects and recording sessions, applying Kernel Principal Component Analysis
(KPCA) with a cosine kernel to pre-process and highlight these shifts. By
evaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and
ADWIN, we reveal the limitations of current techniques in achieving high
performance for real-time domain shift detection in EMG signals. Our results
underscore the potential of streaming-based approaches for maintaining stable
EMG decoding models, while highlighting areas for further research to enhance
robustness and accuracy in real-world scenarios.

</details>


### [39] [MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems](https://arxiv.org/abs/2508.21296)
*Shihao Ji,Zihui Song*

Main category: cs.LG

TL;DR: MyGO라는 새로운 평생 학습 프레임워크를 통해 데이터 프라이버시와 저장소 효율성을 개선하며, 연속적인 작업 학습 중 재학습과 성능 저하 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들의 데이터 프라이버시, 저장소 제한 및 작업 간 유사성 부족 문제를 해결하고자 한다.

Method: MyGO는 '각성' 단계에서 새로운 작업을 빠르게 배우고, '수면' 단계에서 생성된 가짜 데이터를 이용해 지식을 통합하는 방식을 사용한다.

Result: MyGO는 분할 MNIST와 분할 AG 뉴스 벤치마크에서 기존 방법에 비해 재앙적 망각을 효과적으로 경감시키고, 높은 평균 정확도를 유지하는 성과를 보였다.

Conclusion: MyGO는 효율성과 도메인 일반성을 입증하며 평생 학습을 위한 효과적인 프레임워크임을 증명하였다.

Abstract: Continual or Lifelong Learning aims to develop models capable of acquiring
new knowledge from a sequence of tasks without catastrophically forgetting what
has been learned before. Existing approaches often rely on storing samples from
previous tasks (experience replay) or employing complex regularization terms to
protect learned weights. However, these methods face challenges related to data
privacy, storage limitations, and performance degradation when tasks are
dissimilar. To address these challenges, we introduce MyGO (Memory Yielding
Generative Offline-consolidation), a novel lifelong learning framework inspired
by the biological wake-sleep cycle. During the "wake" phase, the system rapidly
learns a new task and trains a compact generative model (Generative Memory,
G-mem) to capture its data distribution. During the "sleep" phase, the system
enters an offline state, using all learned G-mem models to generate pseudo-data
("dreams") and consolidate new and old knowledge into a core feature extractor
via knowledge distillation. This approach obviates the need to store any raw
data, retaining only compact generative models, which offers significant
advantages in privacy and storage efficiency. We evaluate MyGO on computer
vision (Split-MNIST) and natural language processing (Split-AG News)
benchmarks, comparing it against a sequential fine-tuning baseline. The results
demonstrate that MyGO significantly mitigates catastrophic forgetting and
maintains high average accuracy across tasks, proving the framework's
effectiveness and domain-generality.

</details>


### [40] [Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning](https://arxiv.org/abs/2508.21300)
*Yejin Kim,Eunwon Kim,Buru Chang,Junsuk Choe*

Main category: cs.LG

TL;DR: 이 논문은 민감한 정보를 삭제할 수 있는 새로운 비선형 학습 프레임워크 VILA를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)은 여러 작업에서 뛰어난 성능을 보여주지만 민감한 정보를 포함하는 출력 생성 문제에 직면해 있습니다.

Method: VILA는 FILA의 한계를 극복하기 위해 민감 정보 삭제를 위한 새로운 비선형 학습 프레임워크로, 잊혀진 집합과 관련된 매개변수를 정확하게 식별합니다.

Result: VILA는 FILA에 비해 최대 100배 높은 매개변수 효율성과 40배 빠른 훈련 속도를 달성하며 TOFU, WMDP 및 MUSE 벤치마크에서 새로운 최첨단 성능을 기록합니다.

Conclusion: VILA는 전체 모델에 접근하지 않고도 매개변수 식별을 가능하게 하여 계산 비용을 크게 줄입니다.

Abstract: LLMs have demonstrated remarkable performance across various tasks but face
challenges related to unintentionally generating outputs containing sensitive
information. A straightforward approach to address this issue is to retrain the
model after excluding the problematic data. However, this approach incurs
prohibitively high computational costs. To overcome this limitation, machine
unlearning has emerged as a promising solution that can effectively remove
sensitive information without the need to retrain the model from scratch.
Recently, FILA has been proposed as a parameter-efficient unlearning method by
integrating LoRA adapters. Specifically, it calculates the Fisher information
to identify parameters associated with the forget set and assigns them to LoRA
adapters for updates. Despite its innovative approach, FILA still requires
access to all model parameters and does not adequately account for fundamental
assumptions underlying Fisher information, leading to inaccuracies in
importance estimation. To address these limitations, we propose VILA, a novel
unlearning framework that explicitly considers the assumptions overlooked in
FILA, thereby enhancing the accuracy of parameter identification for the forget
set. Moreover, VILA significantly reduces computational costs by enabling
parameter identification without accessing the entire model. Our method
achieves up to 100x higher parameter efficiency and 40x faster training speed
compared to FILA, and sets new state-of-the-art performance on benchmarks
including TOFU, WMDP, and MUSE. Our code is available at
https://github.com/kyj93790/VILA.

</details>


### [41] [Convergence of regularized agent-state-based Q-learning in POMDPs](https://arxiv.org/abs/2508.21314)
*Amit Sinha,Matthieu Geist,Aditya Mahajan*

Main category: cs.LG

TL;DR: 본 논문에서는 Q-학습 강화 학습 알고리즘의 수렴을 이해하기 위한 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: Q-learning 알고리즘의 실용적 수렴 이해를 위한 필요성.

Method: 정규화된 에이전트 상태 기반 Q-학습(RASQL)을 소개하고, 그 수렴을 보여줍니다.

Result: RASQL이 적절히 정의된 정규화된 MDP의 고정점으로 수렴함을 증명합니다.

Conclusion: 실험적 수렴 행동이 이론적 한계와 일치함을 보입니다.

Abstract: In this paper, we present a framework to understand the convergence of
commonly used Q-learning reinforcement learning algorithms in practice. Two
salient features of such algorithms are: (i)~the Q-table is recursively updated
using an agent state (such as the state of a recurrent neural network) which is
not a belief state or an information state and (ii)~policy regularization is
often used to encourage exploration and stabilize the learning algorithm. We
investigate the simplest form of such Q-learning algorithms which we call
regularized agent-state-based Q-learning (RASQL) and show that it converges
under mild technical conditions to the fixed point of an appropriately defined
regularized MDP, which depends on the stationary distribution induced by the
behavioral policy. We also show that a similar analysis continues to work for a
variant of RASQL that learns periodic policies. We present numerical examples
to illustrate that the empirical convergence behavior matches with the proposed
theoretical limit.

</details>


### [42] [Distribution-Aware Feature Selection for SAEs](https://arxiv.org/abs/2508.21324)
*Narmeen Oozeer,Nirmalendu Prakash,Michael Lan,Alice Rigg,Amirali Abdullah*

Main category: cs.LG

TL;DR: Sampled-SAE는 BatchTopK의 한계를 극복하고 각 배치에서 더 정보가 많은 피처를 선택하여 재구성을 개선하는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법인 TopK SAE는 효율성이 떨어지며, 고유한 특징들을 잃는 경향이 있다.

Method: Sampled-SAE는 배치 활성화 행렬의 열을 점수화하여 후보 풀을 만든 후, 제한된 피처 풀에서 토큰을 선택하는 방식을 사용한다.

Result: Sampled-SAE는 BatchTopK의 성능을 조정 가능하고 분포 인식적인 형태로 재구성한다.

Conclusion: 상황에 따라 최적의 $l$ 값은 공유 구조, 재구성 충실도 및 하위 성능 간의 균형에 따라 달라진다.

Abstract: Sparse autoencoders (SAEs) decompose neural activations into interpretable
features. A widely adopted variant, the TopK SAE, reconstructs each token from
its K most active latents. However, this approach is inefficient, as some
tokens carry more information than others. BatchTopK addresses this limitation
by selecting top activations across a batch of tokens. This improves average
reconstruction but risks an "activation lottery," where rare high-magnitude
features crowd out more informative but lower-magnitude ones. To address this
issue, we introduce Sampled-SAE: we score the columns (representing features)
of the batch activation matrix (via $L_2$ norm or entropy), forming a candidate
pool of size $Kl$, and then apply Top-$K$ to select tokens across the batch
from the restricted pool of features. Varying $l$ traces a spectrum between
batch-level and token-specific selection. At $l=1$, tokens draw only from $K$
globally influential features, while larger $l$ expands the pool toward
standard BatchTopK and more token-specific features across the batch. Small $l$
thus enforces global consistency; large $l$ favors fine-grained reconstruction.
On Pythia-160M, no single value optimizes $l$ across all metrics: the best
choice depends on the trade-off between shared structure, reconstruction
fidelity, and downstream performance. Sampled-SAE thus reframes BatchTopK as a
tunable, distribution-aware family.

</details>


### [43] [Stage-Diff: Stage-wise Long-Term Time Series Generation Based on Diffusion Models](https://arxiv.org/abs/2508.21330)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: Stage-Diff 모델은 장기 시계열 생성을 위한 단계적 생성 모델로서, 장기 종속성과 데이터 분포 변화를 효과적으로 조화시킵니다.


<details>
  <summary>Details</summary>
Motivation: 장기 시계열의 복잡한 장기적 패턴을 생성하는 것은 도전적입니다. 이 모델은 이를 해결하고자 합니다.

Method: Stage-Diff는 단계별 시퀀스 생성과 단계 간 정보 전송을 통해 장기 종속성을 유지하며 데이터 분포의 변화를 모델링합니다.

Result: 다양한 실제 데이터셋에서 Stage-Diff의 효과가 입증되었습니다.

Conclusion: Stage-Diff 모델은 장기 시계열 생성 태스크에서 강력한 성능을 보입니다.

Abstract: Generative models have been successfully used in the field of time series
generation. However, when dealing with long-term time series, which span over
extended periods and exhibit more complex long-term temporal patterns, the task
of generation becomes significantly more challenging. Long-term time series
exhibit long-range temporal dependencies, but their data distribution also
undergoes gradual changes over time. Finding a balance between these long-term
dependencies and the drift in data distribution is a key challenge. On the
other hand, long-term time series contain more complex interrelationships
between different feature sequences, making the task of effectively capturing
both intra-sequence and inter-sequence dependencies another important
challenge. To address these issues, we propose Stage-Diff, a staged generative
model for long-term time series based on diffusion models. First, through
stage-wise sequence generation and inter-stage information transfer, the model
preserves long-term sequence dependencies while enabling the modeling of data
distribution shifts. Second, within each stage, progressive sequence
decomposition is applied to perform channel-independent modeling at different
time scales, while inter-stage information transfer utilizes multi-channel
fusion modeling. This approach combines the robustness of channel-independent
modeling with the information fusion advantages of multi-channel modeling,
effectively balancing the intra-sequence and inter-sequence dependencies of
long-term time series. Extensive experiments on multiple real-world datasets
validate the effectiveness of Stage-Diff in long-term time series generation
tasks.

</details>


### [44] [DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks](https://arxiv.org/abs/2508.21340)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: 본 연구는 시계열 생성 과정의 두 단계를 나누어 시계열 데이터의 안전한 순환을 보장하는 새로운 생성 모델 DLGAN을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 시계열 생성 방법은 랜덤 시퀀스 기반의 시간 모델링으로 인해 생성된 시계열의 시간 의존성을 보장하는 데 어려움이 있다.

Method: DLGAN은 시퀀스 특성 추출과 시퀀스 재구성의 두 단계로 시계열 생성 과정을 분해하며, 감독 학습을 통해 원본 시계열의 시간 의존성을 복원할 수 있도록 한다.

Result: 네 가지 공개 데이터셋에 대한 폭넓은 실험 결과, 이 모델은 다양한 평가 지표에서 우수성을 입증하였다.

Conclusion: DLGAN은 랜덤 시퀀스에서 원본 시계열의 특성 정보를 보다 정확하게 포착하고 재구성할 수 있도록 설계되었다.

Abstract: Time series synthesis is an effective approach to ensuring the secure
circulation of time series data. Existing time series synthesis methods
typically perform temporal modeling based on random sequences to generate
target sequences, which often struggle to ensure the temporal dependencies in
the generated time series. Additionally, directly modeling temporal features on
random sequences makes it challenging to accurately capture the feature
information of the original time series. To address the above issues, we
propose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer
\textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named
\textbf{DLGAN}. The model decomposes the time series generation process into
two stages: sequence feature extraction and sequence reconstruction. First,
these two stages form a complete time series autoencoder, enabling supervised
learning on the original time series to ensure that the reconstruction process
can restore the temporal dependencies of the sequence. Second, a Generative
Adversarial Network (GAN) is used to generate synthetic feature vectors that
align with the real-time sequence feature vectors, ensuring that the generator
can capture the temporal features from real time series. Extensive experiments
on four public datasets demonstrate the superiority of this model across
various evaluation metrics.

</details>


### [45] [Adaptive Heavy-Tailed Stochastic Gradient Descent](https://arxiv.org/abs/2508.21353)
*Bodu Gong,Gustavo Enrique Batista,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: AHTSGD는 초기 훈련 단계에서 더 무거운 꼬리를 가진 노이즈를 추가하여 탐색을 강화하고, 손실 경관의 날카로움이 안정화됨에 따라 가벼운 꼬리를 가진 노이즈로 점진적으로 전환하는 적응형 최적화 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 신경망 모델 시대에 최적화 알고리즘은 훈련 손실에 과도하게 의존하여 일반화에 어려움을 겪고 있다.

Method: Adaptive Heavy Tailed Stochastic Gradient Descent (AHTSGD) 알고리즘은 초기 훈련 단계에서 더 무거운 꼬리를 가진 노이즈를 주입하고, 날카로운 지역이 안정화되면 점진적으로 가벼운 꼬리 노이즈로 전환한다.

Result: AHTSGD는 MNIST와 CIFAR-10 같은 벤치마크에서 SGD 및 기타 노이즈 기반 방법들보다 우수한 성능을 보였으며, SVHN와 같은 노이즈 데이터 세트에서 현저한 개선을 이루었다.

Conclusion: AHTSGD는 초기 불량 초기화에서의 훈련을 가속하고, 깨끗한 및 노이즈 환경에서의 일반화를 개선하며, 학습률 선택에 대해 견고한 성능을 유지한다.

Abstract: In the era of large-scale neural network models, optimization algorithms
often struggle with generalization due to an overreliance on training loss. One
key insight widely accepted in the machine learning community is the idea that
wide basins (regions around a local minimum where the loss increases gradually)
promote better generalization by offering greater stability to small changes in
input data or model parameters. In contrast, sharp minima are typically more
sensitive and less stable. Motivated by two key empirical observations - the
inherent heavy-tailed distribution of gradient noise in stochastic gradient
descent and the Edge of Stability phenomenon during neural network training, in
which curvature grows before settling at a plateau, we introduce Adaptive Heavy
Tailed Stochastic Gradient Descent (AHTSGD). The algorithm injects
heavier-tailed noise into the optimizer during the early stages of training to
enhance exploration and gradually transitions to lighter-tailed noise as
sharpness stabilizes. By dynamically adapting to the sharpness of the loss
landscape throughout training, AHTSGD promotes accelerated convergence to wide
basins. AHTSGD is the first algorithm to adjust the nature of injected noise
into an optimizer based on the Edge of Stability phenomenon. AHTSGD
consistently outperforms SGD and other noise-based methods on benchmarks like
MNIST and CIFAR-10, with marked gains on noisy datasets such as SVHN. It
ultimately accelerates early training from poor initializations and improves
generalization across clean and noisy settings, remaining robust to learning
rate choices.

</details>


### [46] [Iterative Inference in a Chess-Playing Neural Network](https://arxiv.org/abs/2508.21380)
*Elias Sandmann,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 신경망이 표현을 구축하는 방식에 대한 연구


<details>
  <summary>Details</summary>
Motivation: 신경망이 점진적인 개선을 통해 표현을 구축하는지, 복잡한 계산 과정을 통해 구축하는지를 조사하기 위함이다.

Method: Leela Chess Zero의 정책 네트워크를 로그잇 렌즈를 확장하여 분석하였다.

Result: 레이어를 통해 플레이 강도와 퍼즐 해결 능력에서 강한 단조적 경향을 발견했으나, 정책 분포는 비순차적인 경로를 따르는 경우가 많았다.

Conclusion: 이러한 발견은 언어 모델에서 일반적으로 관찰되는 부드러운 분포 수렴과 대조적이다.

Abstract: Do neural networks build their representations through smooth, gradual
refinement, or via more complex computational processes? We investigate this by
extending the logit lens to analyze the policy network of Leela Chess Zero, a
superhuman chess engine. We find strong monotonic trends in playing strength
and puzzle-solving ability across layers, yet policy distributions frequently
follow non-smooth trajectories. Evidence for this includes correct puzzle
solutions that are discovered early but subsequently discarded, move rankings
that remain poorly correlated with final outputs, and high policy divergence
until late in the network. These findings contrast with the smooth
distributional convergence typically observed in language models.

</details>


### [47] [PMODE: Theoretically Grounded and Modular Mixture Modeling](https://arxiv.org/abs/2508.21396)
*Robert A. Vandermeulen*

Main category: cs.LG

TL;DR: PMODE는 파라메트릭 및 비파라메트릭 구성 요소를 포함한 혼합 모델링을 위한 모듈식 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 고차원 밀도 추정에 대한 이론적 접근을 실제 설정으로 확장하기 위해.

Method: 데이터를 파티션하고 각 하위 집합에 대해 별도의 추정기를 적합하여 혼합을 구성하는 방법.

Result: MV-PMODE는 수천 차원의 설정에서 뛰어난 성능을 발휘하며 CIFAR-10 이상 탐지에서 딥 베이스라인에 비해 경쟁력을 유지합니다.

Conclusion: PMODE는 다양한 분포 계열에서 혼합 구성 요소를 다루면서도 거의 최적의 추정 성능을 제공합니다.

Abstract: We introduce PMODE (Partitioned Mixture Of Density Estimators), a general and
modular framework for mixture modeling with both parametric and nonparametric
components. PMODE builds mixtures by partitioning the data and fitting separate
estimators to each subset. It attains near-optimal rates for this estimator
class and remains valid even when the mixture components come from different
distribution families. As an application, we develop MV-PMODE, which scales a
previously theoretical approach to high-dimensional density estimation to
settings with thousands of dimensions. Despite its simplicity, it performs
competitively against deep baselines on CIFAR-10 anomaly detection.

</details>


### [48] [Benchmarking the State of Networks with a Low-Cost Method Based on Reservoir Computing](https://arxiv.org/abs/2508.21420)
*Felix Simon Reimers,Carl-Hendrik Peters,Stefano Nichele*

Main category: cs.LG

TL;DR: 노르웨이의 모바일 네트워크 활용 데이터를 사용하여 비침습적이고 저비용의 방법으로 통신 및 이동성 네트워크 상태를 모니터링할 가능성을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 통신 및 이동성 네트워크의 상태를 비침습적이고 비용 효율적인 방식으로 모니터링하고자 하는 동기에서 출발했습니다.

Method: 네트워크 데이터를 레저버 컴퓨팅 틀 내에서 모델로 변환하고, 모델의 성능을 프록시 작업에서 측정하는 방법을 사용합니다.

Result: 프록시 작업에서의 성능이 네트워크 상태와 어떻게 관련되는지를 실험적으로 보여줍니다.

Conclusion: 이 연구는 개념 증명의 역할을 하며, 가까운 실시간 모니터링 및 모바일 통신 네트워크와 교통 네트워크의 잠재적 약점을 식별하는 데 사용될 수 있을 것이라고 믿습니다.

Abstract: Using data from mobile network utilization in Norway, we showcase the
possibility of monitoring the state of communication and mobility networks with
a non-invasive, low-cost method. This method transforms the network data into a
model within the framework of reservoir computing and then measures the model's
performance on proxy tasks. Experimentally, we show how the performance on
these proxies relates to the state of the network. A key advantage of this
approach is that it uses readily available data sets and leverages the
reservoir computing framework for an inexpensive and largely agnostic method.
Data from mobile network utilization is available in an anonymous, aggregated
form with multiple snapshots per day. This data can be treated like a weighted
network. Reservoir computing allows the use of weighted, but untrained networks
as a machine learning tool. The network, initialized as a so-called echo state
network (ESN), projects incoming signals into a higher dimensional space, on
which a single trained layer operates. This consumes less energy than deep
neural networks in which every weight of the network is trained. We use
neuroscience inspired tasks and trained our ESN model to solve them. We then
show how the performance depends on certain network configurations and also how
it visibly decreases when perturbing the network. While this work serves as
proof of concept, we believe it can be elevated to be used for near-real-time
monitoring as well as the identification of possible weak spots of both mobile
communication networks as well as transportation networks.

</details>


### [49] [Rethinking Layer-wise Model Merging through Chain of Merges](https://arxiv.org/abs/2508.21421)
*Pietro Buzzega,Riccardo Salami,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: 본 논문에서는 층 간 상호작용을 고려하여 훈련되지 않은 전문 모델들을 통합하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 도메인에서 최첨단 성능을 달성하기 위해 미세 조정을 한 사전 훈련 모델들이 늘어나고 있어, 이를 통합하는 것이 중요한 문제로 대두되고 있습니다.

Method: Chain of Merges (CoM)라는 층별 병합 절차를 제안하며, 이는 활성화 통계치를 자기 회귀적으로 업데이트하고, 층 간 상호작용을 명시적으로 고려합니다.

Result: CoM은 조건적으로 최적화된 업데이트를 통해 일관된 병합 모델을 생성하고, 내부 공변량 이동으로 인한 저하를 효과적으로 완화합니다.

Conclusion: 표준 벤치마크 실험에서 CoM이 최첨단 성능을 달성함을 입증했습니다.

Abstract: Fine-tuning pretrained models has become a standard pathway to achieve
state-of-the-art performance across a wide range of domains, leading to a
proliferation of task-specific model variants. As the number of such
specialized modules in-creases, merging them into a unified model without
retraining has become a critical challenge. Existing merging techniques often
rely on interference heuristics,importance weighting, or activation matching
while treating each layer independently, thereby failing to account for the
inter-layer dependencies inherent in deep networks. This simplification leads
to distributional mismatches, especially inactivation-based methods, when
changes in early layers are not properly reflected in downstream ones. We
identify these mismatches as a form of internal covariate shift, comparable to
the phenomenon encountered in the initial phases of neural networks training.
To address it, we propose Chain of Merges (CoM), a layer-wise merging procedure
that updates activation statistics in an auto-regressive fashion, explicitly
accounting for cross-layer interactions. CoM produces a coherent merged model
through a series of conditionally optimal updates, effectively mitigating
degradation caused by covariate shift. Experiments on standard bench-marks
demonstrate that CoM achieves state-of-the-art performance.

</details>


### [50] [Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing](https://arxiv.org/abs/2508.21438)
*Rajiv Kailasanathan,William R. Clements,Mohammad Reza Boskabadi,Shawn M. Gibford,Emmanouil Papadakis,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.LG

TL;DR: 이 논문은 지속적인 바이오 제조에서의 비지도 이상 탐지를 위한 새로운 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 지속적인 바이오 제조 공정에서의 안정성과 수율을 유지하기 위해 조기 이상 탐지가 필요하다.

Method: 생성적 적대 신경망(GAN) 앙상블을 기반으로 한 비지도 이상 탐지 프레임워크를 개발하였다.

Result: GAN 기반 프레임워크가 급작스러운 원료 변동으로 인한 이상을 효과적으로 탐지하는 것을 입증하였다.

Conclusion: 혼합 양자/고전적 접근 방식이 복잡한 지속적인 바이오 제조 공정에서 이상 탐지 성능을 개선할 잠재력을 보여준다.

Abstract: The development of continuous biomanufacturing processes requires robust and
early anomaly detection, since even minor deviations can compromise yield and
stability, leading to disruptions in scheduling, reduced weekly production, and
diminished economic performance. These processes are inherently complex and
exhibit non-linear dynamics with intricate relationships between process
variables, thus making advanced methods for anomaly detection essential for
efficient operation. In this work, we present a novel framework for
unsupervised anomaly detection in continuous biomanufacturing based on an
ensemble of generative adversarial networks (GANs). We first establish a
benchmark dataset simulating both normal and anomalous operation regimes in a
continuous process for the production of a small molecule. We then demonstrate
the effectiveness of our GAN-based framework in detecting anomalies caused by
sudden feedstock variability. Finally, we evaluate the impact of using a hybrid
quantum/classical GAN approach with both a simulated quantum circuit and a real
photonic quantum processor on anomaly detection performance. We find that the
hybrid approach yields improved anomaly detection rates. Our work shows the
potential of hybrid quantum/classical approaches for solving real-world
problems in complex continuous biomanufacturing processes.

</details>


### [51] [Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning](https://arxiv.org/abs/2508.21443)
*Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 이 연구에서는 개인적인 경로의 장기 성능 최적화를 위한 새로운 강화 학습 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 강화 학습 알고리즘은 예상 누적 보상을 최적화하지만, 실제 환경에서 개별 경로의 성능에 대한 정보가 부족할 수 있다.

Method: 표준 집합 평균과 시간 평균 성장률을 결합한 알고리즘을 제안하고, 벨만 연산자를 정의한 후 기하 평균과 시간 평균 성장률 간의 관계를 보여준다.

Result: 제안된 알고리즘은 전통적인 강화 학습 방법보다 우수한 성능을 보였다.

Conclusion: 이 알고리즘은 집합 평균과 시간 평균의 이점을 동시에 활용할 수 있게 해준다.

Abstract: Reinforcement learning (RL) algorithms typically optimize the expected
cumulative reward, i.e., the expected value of the sum of scalar rewards an
agent receives over the course of a trajectory. The expected value averages the
performance over an infinite number of trajectories. However, when deploying
the agent in the real world, this ensemble average may be uninformative for the
performance of individual trajectories. Thus, in many applications, optimizing
the long-term performance of individual trajectories might be more desirable.
In this work, we propose a novel RL algorithm that combines the standard
ensemble average with the time-average growth rate, a measure for the long-term
performance of individual trajectories. We first define the Bellman operator
for the time-average growth rate. We then show that, under multiplicative
reward dynamics, the geometric mean aligns with the time-average growth rate.
To address more general and unknown reward dynamics, we propose a modified
geometric mean with $N$-sliding window that captures the path-dependency as an
estimator for the time-average growth rate. This estimator is embedded as a
regularizer into the objective, forming a practical algorithm and enabling the
policy to benefit from ensemble average and time-average simultaneously. We
evaluate our algorithm in challenging simulations, where it outperforms
conventional RL methods.

</details>


### [52] [Normalized Maximum Likelihood Code-Length on Riemannian Manifold Data Spaces](https://arxiv.org/abs/2508.21466)
*Kota Fukuzawa,Atsushi Suzuki,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 리만 다양체 데이터 공간에 대한 연구가 진행되었으며, 새로운 정규화된 최대 우도(NML)를 제안함으로써 이를 확장하였다.


<details>
  <summary>Details</summary>
Motivation: 그래프 데이터의 대규모 확장으로 인해 유클리드 공간 외의 리만 다양체 데이터 공간에 대한 관심이 증가하고 있다.

Method: 리만 다양체의 기하학적 구조를 반영하는 새로운 NML(리만 다양체 NML, Rm-NML)을 정의하고, 기존의 계산 기술을 리만 다양체 환경으로 확장하였다.

Result: Rm-NML은 좌표 변환에 대해 불변하며, 유클리드 공간의 자연 파라미터화 하에 기존의 NML과 일치한다.

Conclusion: Rm-NML 계산을 단순화하는 방법을 유도하였고, 이를 통해 하이퍼볼릭 공간에서의 정규 분포에 대한 Rm-NML을 계산하였다.

Abstract: In recent years, with the large-scale expansion of graph data, there has been
an increased focus on Riemannian manifold data spaces other than Euclidean
space. In particular, the development of hyperbolic spaces has been remarkable,
and they have high expressive power for graph data with hierarchical
structures. Normalized Maximum Likelihood (NML) is employed in regret
minimization and model selection. However, existing formulations of NML have
been developed primarily in Euclidean spaces and are inherently dependent on
the choice of coordinate systems, making it non-trivial to extend NML to
Riemannian manifolds. In this study, we define a new NML that reflects the
geometric structure of Riemannian manifolds, called the Riemannian manifold NML
(Rm-NML). This Rm-NML is invariant under coordinate transformations and
coincides with the conventional NML under the natural parameterization in
Euclidean space. We extend existing computational techniques for NML to the
setting of Riemannian manifolds. Furthermore, we derive a method to simplify
the computation of Rm-NML on Riemannian symmetric spaces, which encompass data
spaces of growing interest such as hyperbolic spaces. To illustrate the
practical application of our proposed method, we explicitly computed the Rm-NML
for normal distributions on hyperbolic spaces.

</details>


### [53] [Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration](https://arxiv.org/abs/2508.21468)
*Seungyeon Choi,Hwanhee Kim,Chihyun Park,Dahyeon Lee,Seungyong Lee,Yoonju Kim,Hyoungjoon Park,Sein Kwon,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: CByG라는 새로운 프레임워크를 통해 약물 발견에서의 성능을 향상시키는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 약물 디자인 평가에서 결합 친화도 외의 중요한 속성들이 간과되었다는 점에서 유의미한 개선이 필요하다.

Method: Bayesian Flow Network를 기반으로 한 CByG라는 새로운 프레임워크를 개발하여, 성질별 안내를 통합한 조건부 생성 모델을 제안한다.

Result: CByG 프레임워크가 여러 필수 평가 기준에서 기존 모델들을 초월하는 성능을 보임을 실험적으로 입증했다.

Conclusion: CByG는 실제 약물 발견에 보다 효과적이고 실용적인 접근을 제공한다.

Abstract: Recent advances in Structure-based Drug Design (SBDD) have leveraged
generative models for 3D molecular generation, predominantly evaluating model
performance by binding affinity to target proteins. However, practical drug
discovery necessitates high binding affinity along with synthetic feasibility
and selectivity, critical properties that were largely neglected in previous
evaluations. To address this gap, we identify fundamental limitations of
conventional diffusion-based generative models in effectively guiding molecule
generation toward these diverse pharmacological properties. We propose CByG, a
novel framework extending Bayesian Flow Network into a gradient-based
conditional generative model that robustly integrates property-specific
guidance. Additionally, we introduce a comprehensive evaluation scheme
incorporating practical benchmarks for binding affinity, synthetic feasibility,
and selectivity, overcoming the limitations of conventional evaluation methods.
Extensive experiments demonstrate that our proposed CByG framework
significantly outperforms baseline models across multiple essential evaluation
criteria, highlighting its effectiveness and practicality for real-world drug
discovery applications.

</details>


### [54] [Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning](https://arxiv.org/abs/2508.21488)
*Pascal R. van der Vaart,Neil Yorke-Smith,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: 강화학습에서 불확실성 정량화는 탐색성과 강건성을 개선할 수 있다. Bayesian 접근 방식을 통해 모델 없는 알고리즘의 불확실성을 정량화하는 데 초점을 맞추고 있으며, 이 논문에서는 Bayesian deep Q-learning에서 추정된 사후 분포의 온도를 낮출 때 성능이 향상되는 '콜드 사후 효과'를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 Bayesian 모델 없는 알고리즘에서 사후의 정확성을 향상시키는 것뿐만 아니라, 사전 및 가능성 가정의 정확성에 대한 연구의 필요성을 강조한다.

Method: 사전 분포를 경험적으로 연구하고, 통계적 테스트를 통해 일반적인 가우시안 가능성 가정이 자주 위반된다는 것을 보여준다.

Result: 사전 분포와 가능성 가정의 검토를 통해 더 적합한 가능성 및 사전 개발이 향후 강화학습 연구의 주요 초점이 되어야 한다고 주장한다.

Conclusion: 우리는 더 나은 사전을 위한 간단하고 구현 가능한 솔루션을 제공하여 더 나은 성능을 갖고 있는 Bayesian 알고리즘으로 발전할 수 있는 방법을 제시한다.

Abstract: Uncertainty quantification in reinforcement learning can greatly improve
exploration and robustness. Approximate Bayesian approaches have recently been
popularized to quantify uncertainty in model-free algorithms. However, so far
the focus has been on improving the accuracy of the posterior approximation,
instead of studying the accuracy of the prior and likelihood assumptions
underlying the posterior. In this work, we demonstrate that there is a cold
posterior effect in Bayesian deep Q-learning, where contrary to theory,
performance increases when reducing the temperature of the posterior. To
identify and overcome likely causes, we challenge common assumptions made on
the likelihood and priors in Bayesian model-free algorithms. We empirically
study prior distributions and show through statistical tests that the common
Gaussian likelihood assumption is frequently violated. We argue that developing
more suitable likelihoods and priors should be a key focus in future Bayesian
reinforcement learning research and we offer simple, implementable solutions
for better priors in deep Q-learning that lead to more performant Bayesian
algorithms.

</details>


### [55] [Failure Prediction Is a Better Performance Proxy for Early-Exit Networks Than Calibration](https://arxiv.org/abs/2508.21495)
*Piotr Kubaty,Filip Szatkowski,Metod Jazbec,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 본 연구에서는 초기 종료 모델의 효율성을 향상시키기 위한 실패 예측 사용을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 초기 종료 모델은 내부 분류기를 통해 추론 속도를 높이고 예측이 종료 기준을 충족할 때 계산을 중단할 수 있게 합니다. 하지만 기존의 신뢰도 기반 종료 전략은 잘못된 성능 지표를 제공할 수 있습니다.

Method: 우리의 연구에서는 중간 분류기의 보정이 불필요하다고 보여주고, 실패 예측을 초기 종료 모델의 성능 평가에 더 유용한 대안으로 제안합니다.

Result: 실험을 통해, 잘못 보정된 네트워크가 잘 보정된 네트워크보다 더 나은 성능을 발휘하는 경우를 입증합니다.

Conclusion: 실패 예측은 샘플 등급의 변화를 고려하므로 초기 종료 모델 설계 및 평가의 신뢰할 수 있는 근거가 됩니다.

Abstract: Early-exit models speed up inference by attaching internal classifiers to
intermediate layers of the model and allowing computation to stop once a
prediction satisfies an exit criterion. Most early-exit methods rely on
confidence-based exit strategies, which motivated some works to calibrate
intermediate classifiers to improve the performance of the entire model. In
this paper, we show that calibration measures can be misleading indicators of
the performance of multi-exit models: a well-calibrated classifier may still
waste computation, and common calibration methods do not preserve the sample
ranking within a classifier. We demonstrate empirical cases where miscalibrated
networks outperform calibrated ones. As an alternative, we propose to use
failure prediction as a more useful proxy for early-exit model performance.
Unlike calibration, failure prediction accounts for changes in the ranking of
samples and shows a strong correlation with efficiency improvements, making it
a more dependable basis for designing and evaluating early-exit models.

</details>


### [56] [Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control](https://arxiv.org/abs/2508.21505)
*Vishal Pandey,Debasmita Biswas*

Main category: cs.LG

TL;DR: SNN-DT는 LEAKY INTEGRATE-AND-FIRE 뉴런을 사용하여 에너지 제약이 있는 플랫폼에서 저전력으로 작동하는 변환 신경망 기반의 강화 학습 에이전트를 개발한다.


<details>
  <summary>Details</summary>
Motivation: 에너지 제약이 있는 엣지 플랫폼에서 효과적으로 작동할 수 있는 강화 학습 방법을 찾는 것.

Method: Leaky Integrate-and-Fire 뉴런을 각 자기 주의 블록에 삽입하고, 대체 그래디언트를 통해 종단 간 학습하며, 생물학적으로 영감을 받은 세 가지 요인 가소성과 위상 이동된 스파이크 기반 위치 인코딩, 가벼운 덴드리틱 라우팅 모듈을 포함한 SNN-DT를 제안.

Result: SNN-DT는 전통적인 제어 벤치마크에서 기존의 Decision Transformer 성능과 맞먹거나 초과하면서도 결정당 열 번 이하의 스파이크를 방출한다.

Conclusion: SNN-DT는 시퀀스 모델링과 신경형 효율성을 결합함으로써 내장 및 착용 가능한 장치에서 실시간 저전력 제어의 경로를 열어준다.

Abstract: Reinforcement learning agents based on Transformer architectures have
achieved impressive performance on sequential decision-making tasks, but their
reliance on dense matrix operations makes them ill-suited for
energy-constrained, edge-oriented platforms. Spiking neural networks promise
ultra-low-power, event-driven inference, yet no prior work has seamlessly
merged spiking dynamics with return-conditioned sequence modeling. We present
the Spiking Decision Transformer (SNN-DT), which embeds Leaky
Integrate-and-Fire neurons into each self-attention block, trains end-to-end
via surrogate gradients, and incorporates biologically inspired three-factor
plasticity, phase-shifted spike-based positional encodings, and a lightweight
dendritic routing module. Our implementation matches or exceeds standard
Decision Transformer performance on classic control benchmarks (CartPole-v1,
MountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes
per decision, an energy proxy suggesting over four orders-of-magnitude
reduction in per inference energy. By marrying sequence modeling with
neuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power
control on embedded and wearable devices.

</details>


### [57] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 본 연구는 3개 지역의 대출 승인 데이터셋에 대한 대형 언어 모델의 성능과 공정성을 평가하고, 직렬화 형식이 성능과 공정성에 미치는 영향을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 고위험 결정 과제에서 사용되고 있지만, 테이블 데이터를 처리하고 공정성 유지 및 신뢰할 수 있는 예측을 제공하는 데 어려움을 겪고 있다.

Method: Ghana, Germany, United States의 직렬화된 대출 승인 데이터셋을 평가하고, 모델의 제로샷 및 맥락 내 학습(ICL) 능력을 중심으로 분석한다.

Result: 특정 직렬화 형식이 LLM의 성능과 공정성에 중대한 영향을 미치며, ICL은 성능을 4.9-59.6% 향상시키나 공정성의 영향은 데이터셋에 따라 다르다.

Conclusion: 테이블 데이터 표현 방법 및 공정성을 고려한 모델의 중요성을 강조하여 LLM의 금융 결정에서의 신뢰성을 개선한다.

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [58] [On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature](https://arxiv.org/abs/2508.21513)
*Geri Skenderi*

Main category: cs.LG

TL;DR: 그래프 신경망(GNN)이 불리안 만족도 문제(SAT) 해결에 효과적일 수 있지만, 어려운 사례에서는 성능이 급격히 저하된다. 이 작업에서는 그래프 리치 곡률을 통해 이를 설명하며, GNN 기반 SAT 솔버가 긴 종속성을 압축하지 못하는 과압축 현상에 영향을 받음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: GNN의 성능 저하가 근본적인 구조적 한계를 반영하는지를 탐구하기 위해, GNN을 이용한 SAT 문제 해결의 효과를 분석하고자 합니다.

Method: 그래프 리치 곡률을 사용하여 로컬 연결성 병목 현상을 정량화하고, 임의의 k-SAT 공식에서 유도된 이분 그래프가 본질적으로 음의 곡률을 가진다는 것을 증명합니다.

Result: SAT 벤치마크를 통해 곡률이 문제 복잡도의 강력한 지표임을 확인하고 성능 예측에 대한 증거를 제시합니다.

Conclusion: 연구 결과는 기존 솔버의 설계 원칙과 연결되며, 향후 연구 방향에 대한 유망한 아이디어를 제시합니다.

Abstract: Graph Neural Networks (GNNs) have recently shown promise as solvers for
Boolean Satisfiability Problems (SATs) by operating on graph representations of
logical formulas. However, their performance degrades sharply on harder
instances, raising the question of whether this reflects fundamental
architectural limitations. In this work, we provide a geometric explanation
through the lens of graph Ricci Curvature (RC), which quantifies local
connectivity bottlenecks. We prove that bipartite graphs derived from random
k-SAT formulas are inherently negatively curved, and that this curvature
decreases with instance difficulty. Building on this, we show that GNN-based
SAT solvers are affected by oversquashing, a phenomenon where long-range
dependencies become impossible to compress into fixed-length representations.
We validate our claims empirically across different SAT benchmarks and confirm
that curvature is both a strong indicator of problem complexity and can be used
to predict performance. Finally, we connect our findings to design principles
of existing solvers and outline promising directions for future work.

</details>


### [59] [What Data is Really Necessary? A Feasibility Study of Inference Data Minimization for Recommender Systems](https://arxiv.org/abs/2508.21547)
*Jens Leysen,Marco Favier,Bart Goethals*

Main category: cs.LG

TL;DR: 데이터 최소화는 개인 데이터 처리를 특정 목적에 필요한 범위로 제한해야 하는 법적 원칙이다. 본 논문은 추천 시스템에서의 암묵적 피드백 추론 데이터 최소화 가능성을 연구하고, 다양한 최소화 기법을 분석하며, 효과성에 영향을 미치는 주요 요인을 조사하였다.


<details>
  <summary>Details</summary>
Motivation: 개인 데이터에 의존하는 추천 시스템에서 데이터 최소화 원칙을 운영화하는 것은 중요한 도전 과제이다.

Method: 본 논문은 암묵적 피드백 추론 데이터 최소화에 관한 실현 가능성 연구를 수행하고, 새로운 문제 공식을 제안하며, 다양한 최소화 기법을 분석하였다.

Result: 상당한 추론 데이터 감소가 기술적으로 가능하며, 성능 손실 없이 이루어질 수 있음을 입증하였다.

Conclusion: 데이터 최소화의 실용성은 기술적 설정과 사용자 특성에 의해 크게 좌우되며, 이는 데이터 '필요성'에 대한 보편적인 기준을 구현하기 어렵게 만든다.

Abstract: Data minimization is a legal principle requiring personal data processing to
be limited to what is necessary for a specified purpose. Operationalizing this
principle for recommender systems, which rely on extensive personal data,
remains a significant challenge. This paper conducts a feasibility study on
minimizing implicit feedback inference data for such systems. We propose a
novel problem formulation, analyze various minimization techniques, and
investigate key factors influencing their effectiveness. We demonstrate that
substantial inference data reduction is technically feasible without
significant performance loss. However, its practicality is critically
determined by two factors: the technical setting (e.g., performance targets,
choice of model) and user characteristics (e.g., history size, preference
complexity). Thus, while we establish its technical feasibility, we conclude
that data minimization remains practically challenging and its dependence on
the technical and user context makes a universal standard for data `necessity'
difficult to implement.

</details>


### [60] [Comprehensive Signal Quality Evaluation of a Wearable Textile ECG Garment: A Sex-Balanced Study](https://arxiv.org/abs/2508.21554)
*Maximilian P. Oppelt,Tobias S. Zech,Sarah H. Lorenz,Laurenz Ottmann,Jan Steffan,Bjoern M. Eskofier,Nadine R. Lang-Richter,Norman Pfeiffer*

Main category: cs.LG

TL;DR: 이 논문에서는 ECG 기록에서 신호 충실도를 향상시키기 위해 소음과 움직임 아티팩트를 최소화하는 혁신적인 전극 배치를 특징으로 하는 새로운 착용 가능한 직물 의류를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 심전도(ECG) 기록의 신호 충실도를 향상시키고, 성별에 따른 신체적 차이를 고려하기 위한 장치의 적합성을 평가하기 위해 이 연구가 발의되었습니다.

Method: 15명의 건강한 남성과 15명의 건강한 여성 참가자를 포함한 포괄적이고 성 균형 잡힌 평가를 수행했습니다. 다양한 평가 접근 방식을 포함하여 신호 품질 지표, 생리학적 매개변수의 리듬 기반 분석, 기계 학습 분류 작업 등을 통해 장치 성능을 평가했습니다.

Result: 이 텍스타일 시스템은 리듬 및 형태 분석 모두에서 기준 장치와 높은 일치를 보이며, 견고한 분류 성능을 나타내고 신호 획득에 영향을 미치는 성별 특정 결정 요인을 식별할 수 있음을 보여주었습니다.

Conclusion: 이 연구 결과는 생리적 모니터링 및 심리 생리적 상태 감지에 있어 텍스타일 기반의 ECG 의류의 실질적인 실현 가능성을 강조합니다. 또한, 착용 건강 기술에서 공정하고 신뢰할 수 있는 심장 진단을 보장하기 위해 성별 특정 디자인 고려의 중요성을 확인했습니다.

Abstract: We introduce a novel wearable textile-garment featuring an innovative
electrode placement aimed at minimizing noise and motion artifacts, thereby
enhancing signal fidelity in Electrocardiography (ECG) recordings. We present a
comprehensive, sex-balanced evaluation involving 15 healthy males and 15
healthy female participants to ensure the device's suitability across
anatomical and physiological variations. The assessment framework encompasses
distinct evaluation approaches: quantitative signal quality indices to
objectively benchmark device performance; rhythm-based analyzes of
physiological parameters such as heart rate and heart rate variability; machine
learning classification tasks to assess application-relevant predictive
utility; morphological analysis of ECG features including amplitude and
interval parameters; and investigations of the effects of electrode projection
angle given by the textile / body shape, with all analyzes stratified by sex to
elucidate sex-specific influences. Evaluations were conducted across various
activity phases representing real-world conditions. The results demonstrate
that the textile system achieves signal quality highly concordant with
reference devices in both rhythm and morphological analyses, exhibits robust
classification performance, and enables identification of key sex-specific
determinants affecting signal acquisition. These findings underscore the
practical viability of textile-based ECG garments for physiological monitoring
as well as psychophysiological state detection. Moreover, we identify the
importance of incorporating sex-specific design considerations to ensure
equitable and reliable cardiac diagnostics in wearable health technologies.

</details>


### [61] [Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation](https://arxiv.org/abs/2508.21559)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 물리 정보를 반영한 신경망(PINNs)은 스마트 그리드 모델링을 위한 혁신적인 접근 방식을 제시하며, 데이터 부족과 물리적 일관성 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 그리드에서 데이터 부족과 물리적 일관성을 해결하기 위해 물리 법칙을 학습 프레임워크에 직접 통합하는 방법이 필요하다.

Method: PINNs를 대리 모델로 사용하여 XGBoost, 랜덤 포레스트, 선형 회귀와 비교하여 세 가지 주요 실험(보간, 교차 검증, 에피소드 궤적 예측)에서 그 성능을 평가했다.

Result: PINNs는 물리학 기반 손실 함수를 통해 훈련되어 데이터 기반 모델보다 오류 감소에서 우수한 성능을 보였다.

Conclusion: PINNs는 스마트 그리드 대리 모델링에 중대한 도구로 자리잡을 수 있으며, 임무-critical 에너지 시스템에서 물리적 인식 아키텍처의 필요성을 강조한다.

Abstract: Physics-Informed Neural Networks (PINNs) present a transformative approach
for smart grid modeling by integrating physical laws directly into learning
frameworks, addressing critical challenges of data scarcity and physical
consistency in conventional data-driven methods. This paper evaluates PINNs'
capabilities as surrogate models for smart grid dynamics, comparing their
performance against XGBoost, Random Forest, and Linear Regression across three
key experiments: interpolation, cross-validation, and episodic trajectory
prediction. By training PINNs exclusively through physics-based loss functions
(enforcing power balance, operational constraints, and grid stability) we
demonstrate their superior generalization, outperforming data-driven models in
error reduction. Notably, PINNs maintain comparatively lower MAE in dynamic
grid operations, reliably capturing state transitions in both random and
expert-driven control scenarios, while traditional models exhibit erratic
performance. Despite slight degradation in extreme operational regimes, PINNs
consistently enforce physical feasibility, proving vital for safety-critical
applications. Our results contribute to establishing PINNs as a
paradigm-shifting tool for smart grid surrogation, bridging data-driven
flexibility with first-principles rigor. This work advances real-time grid
control and scalable digital twins, emphasizing the necessity of physics-aware
architectures in mission-critical energy systems.

</details>


### [62] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: InsightTab은 LLM을 위한 강력한 표 형식 분류를 가능하게 하는 인사이트 증류 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 최근 연구는 LLM이 소수의 샘플로 표 형식 분류에 효과적이지만 구조화된 데이터의 변동성으로 인한 도전과제가 있음을 보여준다.

Method: InsightTab은 분할 정복, 쉬운 시작, 반사 학습의 원리를 바탕으로 하는 인사이트 증류 프레임워크를 제안한다.

Result: 아홉 개 데이터 세트에서 InsightTab을 광범위하게 평가한 결과, 최신 기법 대비 일관된 개선이 나타났다.

Conclusion: 본 연구는 라벨이 있는 데이터를 활용하고 편향을 관리하는 데 있어 InsightTab의 효과를 강조한다.

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


### [63] [OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories](https://arxiv.org/abs/2508.21570)
*Bo Li,Yingqi Feng,Ming Jin,Xin Zheng,Yufei Tang,Laurent Cherubin,Alan Wee-Chung Liew,Can Wang,Qinghua Lu,Jingwei Yao,Shirui Pan,Hong Zhang,Xingquan Zhu*

Main category: cs.LG

TL;DR: 해양 염분 측정의 어려움을 해결하기 위해 새로운 확산 적대적 프레임워크인 OASIS를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 해양 염분 측정은 해양 순환, 기후 및 생태계에 매우 중요하나, 기존 측정 방식은 불규칙하고 소음이 많다.

Method: OASIS는 확산 적대적 프레임워크를 기반으로 한다.

Result: 이 시스템은 기존의 측정 방식에 비해 더 나은 염분 데이터를 제공할 것으로 기대된다.

Conclusion: OASIS는 해양 염분 측정의 한계를 극복할 수 있는 유망한 접근법이다.

Abstract: Ocean salinity plays a vital role in circulation, climate, and marine
ecosystems, yet its measurement is often sparse, irregular, and noisy,
especially in drifter-based datasets. Traditional approaches, such as remote
sensing and optimal interpolation, rely on linearity and stationarity, and are
limited by cloud cover, sensor drift, and low satellite revisit rates. While
machine learning models offer flexibility, they often fail under severe
sparsity and lack principled ways to incorporate physical covariates without
specialized sensors. In this paper, we introduce the OceAn Salinity Imputation
System (OASIS), a novel diffusion adversarial framework designed to address
these challenges.

</details>


### [64] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: 이 논문은 과적합된 이층 PINNs의 훈련에서 확률적 경사 하강법의 선형 수렴성을 입증합니다.


<details>
  <summary>Details</summary>
Motivation: 물리적으로 정보화된 신경망(PINNs)은 편미분 방정식에 대한 신경 솔버의 인기 있는 클래스이며, 이 네트워크를 훈련하기 위해 확률적 경사 하강법 알고리즘을 사용합니다.

Method: 이 연구에서는 일반적인 활성화 함수에 대해 과적합된 이층 PINNs의 훈련에서 확률적 경사 하강법의 선형 수렴성을 고확률의 의미로 확립합니다.

Result: 이 결과는 기존의 경량 경사 하강법 분석 결과를 확장하며, 확률적 최적화 방법에 의해 도입된 동적 무작위성을 처리하는 데 어려움이 있습니다.

Conclusion: 이 분석은 최적화 과정의 역학에 대한 통찰을 제공하고, 확률적 알고리즘으로 훈련된 신경망에 대한 보장을 제공합니다.

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


### [65] [Physics-Informed Spectral Modeling for Hyperspectral Imaging](https://arxiv.org/abs/2508.21618)
*Zuzanna Gawrysiak,Krzysztof Krawiec*

Main category: cs.LG

TL;DR: PhISM은 비지도 학습을 통해 하이퍼스펙트럼 관측치를 명확히 분해하고 이를 연속 기저 함수로 모델링하는 물리 정보 심화 학습 아키텍처이다.


<details>
  <summary>Details</summary>
Motivation: 하이퍼스펙트럼 데이터를 보다 효과적으로 처리하고 해석할 수 있는 방법이 필요하다.

Method: PhISM은 비지도 학습 방법을 사용하여 하이퍼스펙트럼 관측치를 분해하고 이를 연속 기저 함수로 모델링한다.

Result: PhISM은 여러 분류 및 회귀 벤치마크에서 이전 방법보다 우수한 성능을 보이며, 제한된 라벨링된 데이터로도 학습이 가능하다.

Conclusion: PhISM은 해석 가능한 잠재 표현 덕분에 추가적인 통찰을 제공한다.

Abstract: We present PhISM, a physics-informed deep learning architecture that learns
without supervision to explicitly disentangle hyperspectral observations and
model them with continuous basis functions. \mname outperforms prior methods on
several classification and regression benchmarks, requires limited labeled
data, and provides additional insights thanks to interpretable latent
representation.

</details>


### [66] [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](https://arxiv.org/abs/2508.21620)
*Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 이 논문은 다양한 불확실성 하에서 선택을 내리는 이론적 방법을 제시하고, 이러한 결정 이론을 바탕으로 한 알고리즘의 이론적 분석의 필요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 결정 이론은 다양한 불확실성 하에서 선택을 내리기 위한 원칙적인 방법을 제공하고, 과학적 발견에서 실험 비용을 크게 줄일 수 있는 기회를 제공한다.

Method: 일반적으로 사용되는 확률적 의사 결정 알고리즘의 이론적 분석을 보다 접근하기 쉬운 방식으로 소개한다.

Result: 이 알기 쉬운 해설을 통해 비전문가들이 결정 이론 알고리즘의 이론적 분석을 이해할 수 있도록 한다.

Conclusion: 이 연구는 차세대 알고리즘 개발을 위한 귀중한 통찰력을 제공하며, 확률 이론 및 통계에 대한 기본 지식만으로도 접근이 가능하다.

Abstract: Decision theories offer principled methods for making choices under various
types of uncertainty. Algorithms that implement these theories have been
successfully applied to a wide range of real-world problems, including
materials and drug discovery. Indeed, they are desirable since they can
adaptively gather information to make better decisions in the future, resulting
in data-efficient workflows. In scientific discovery, where experiments are
costly, these algorithms can thus significantly reduce the cost of
experimentation. Theoretical analyses of these algorithms are crucial for
understanding their behavior and providing valuable insights for developing
next-generation algorithms. However, theoretical analyses in the literature are
often inaccessible to non-experts. This monograph aims to provide an
accessible, self-contained introduction to the theoretical analysis of commonly
used probabilistic decision-making algorithms, including bandit algorithms,
Bayesian optimization, and tree search algorithms. Only basic knowledge of
probability theory and statistics, along with some elementary knowledge about
Gaussian processes, is assumed.

</details>


### [67] [Predicting Social Media Engagement from Emotional and Temporal Features](https://arxiv.org/abs/2508.21650)
*Yunwoo Kim,Junhyuk Hwang*

Main category: cs.LG

TL;DR: 이 논문은 감정 및 시간적 특성을 기반으로 소셜 미디어 참여(댓글 및 좋아요)를 예측하기 위한 기계 학습 접근 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 소셜 미디어 플랫폼에서 사용자 참여를 예측하는 것은 마케팅 및 콘텐츠 제작에 중요한 과제이다.

Method: HistGradientBoostingRegressor를 기반으로 한 다중 목표 회귀 모델이 로그 변환된 참여 비율에 대해 훈련된다.

Result: 결과는 감정 및 시간적 메타데이터와 기존 조회 수가 향후 참여를 효과적으로 예측함을 보여준다.

Conclusion: 이 모델은 좋아요에 대해 R^2 = 0.98을 달성하지만 댓글에 대해서는 R^2 = 0.41에 불과하다. 이는 좋아요가 감정적 및 노출 신호에 의해 주로 유도되지만, 댓글은 현재 특성 세트에 의해 대표되지 않는 추가 요인에 의존함을 나타낸다.

Abstract: We present a machine learning approach for predicting social media engagement
(comments and likes) from emotional and temporal features. The dataset contains
600 songs with annotations for valence, arousal, and related sentiment metrics.
A multi target regression model based on HistGradientBoostingRegressor is
trained on log transformed engagement ratios to address skewed targets.
Performance is evaluated with both a custom order of magnitude accuracy and
standard regression metrics, including the coefficient of determination (R^2).
Results show that emotional and temporal metadata, together with existing view
counts, predict future engagement effectively. The model attains R^2 = 0.98 for
likes but only R^2 = 0.41 for comments. This gap indicates that likes are
largely driven by readily captured affective and exposure signals, whereas
comments depend on additional factors not represented in the current feature
set.

</details>


### [68] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: SLAC는 실시간 머신러닝 추론 모델을 FPGA에 배포하기 위한 SLAC 신경망 라이브러리(SNL)를 개발하고, 이를 통해 X-ray 펄스를 생성하는 LCLS-II Free Electron Laser를 위한 데이터 관리 문제를 해결했다.


<details>
  <summary>Details</summary>
Motivation: LCLS-II Free Electron Laser에서 발생하는 대량의 데이터 스트림 관리 문제 해결.

Method: SLAC Neural Network Library(SNL)를 통해 FPGA에서 실시간 머신러닝 추론 모델 배포.

Result: SNL은 대부분의 테스트된 아키텍처에서 경쟁력 있는 또는 우수한 대기 시간을 달성했으며, 일부 경우에는 FPGA 리소스 절약도 제공함.

Conclusion: SNL의 적응력은 고에너지 물리학, 의료 이미징, 로봇공학 등의 분야에서 연구자들에게 새로운 기회를 열어준다.

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


### [69] [Activation Subspaces for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21695)
*Barış Zöngür,Robin Hesse,Stefan Roth*

Main category: cs.LG

TL;DR: 이 연구에서는 분류 헤드의 가중치 행렬의 특이값 분해를 이용하여 OOD 탐지 방법을 제안하고, 실제 애플리케이션에서 딥 모델의 신뢰성을 향상시키는 방법을 모색합니다.


<details>
  <summary>Details</summary>
Motivation: 실제 애플리케이션에서 딥 모델의 신뢰성을 확보하기 위한 OOD 탐지 방법의 필요성.

Method: 모델의 활성화를 결정적인 구성 요소와 중요하지 않은 구성 요소로 분해하기 위해 분류 헤드의 가중치 행렬의 특이값 분해를 이용하는 새로운 OOD 탐지 방법을 제안합니다.

Result: 비중분배가 큰 경우(Far-OOD)에는 중요하지 않은 구성 요소의 부분 공간이 ID와 OOD 데이터를 더 효과적으로 구분하고, 비중분배가 작은 경우(Near-OOD)에는 결정적인 부분 공간만 고려함으로써 활성화 공간에서 간섭을 줄입니다.

Conclusion: 이 두 가지 결과를 ActSub이라는 단일 접근법으로 결합함으로써 다양한 표준 OOD 벤치마크에서 최첨단 결과를 달성합니다.

Abstract: To ensure the reliability of deep models in real-world applications,
out-of-distribution (OOD) detection methods aim to distinguish samples close to
the training distribution (in-distribution, ID) from those farther away (OOD).
In this work, we propose a novel OOD detection method that utilizes singular
value decomposition of the weight matrix of the classification head to
decompose the model's activations into decisive and insignificant components,
which contribute maximally, respectively minimally, to the final classifier
output. We find that the subspace of insignificant components more effectively
distinguishes ID from OOD data than raw activations in regimes of large
distribution shifts (Far-OOD). This occurs because the classification objective
leaves the insignificant subspace largely unaffected, yielding features that
are ''untainted'' by the target classification task. Conversely, in regimes of
smaller distribution shifts (Near-OOD), we find that activation shaping methods
profit from only considering the decisive subspace, as the insignificant
component can cause interference in the activation space. By combining two
findings into a single approach, termed ActSub, we achieve state-of-the-art
results in various standard OOD benchmarks.

</details>


### [70] [MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction](https://arxiv.org/abs/2508.21793)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: MoE-Health는 의료 예측을 위한 강력한 다중 모드 융합을 설계한 Mixture of Experts 프레임워크로, 다양한 모드가 있는 샘플을 처리하여 예측 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 의료 시스템에서 발생하는 다양한 다중 모드 데이터를 효과적으로 활용하여 임상 예측을 수행하는 것이 어렵습니다.

Method: MoE-Health는 전문 네트워크와 동적 게이팅 메커니즘을 활용하여 가용 데이터 모드에 따라 관련 전문가를 동적으로 선택하고 결합합니다.

Result: MoE-Health는 in-hospital 사망 예측, 병원 장기 체류, 재입원 예측 및 MIMIC-IV 데이터 세트에서 기존 다중 모드 융합 방법보다 우수한 성과를 보여줍니다.

Conclusion: MoE-Health는 다양한 데이터를 통합하여 예측 성능과 강인성을 개선하며, 이질적인 데이터 가용성이 있는 다양한 의료 환경에 적합합니다.

Abstract: Healthcare systems generate diverse multimodal data, including Electronic
Health Records (EHR), clinical notes, and medical images. Effectively
leveraging this data for clinical prediction is challenging, particularly as
real-world samples often present with varied or incomplete modalities. Existing
approaches typically require complete modality data or rely on manual selection
strategies, limiting their applicability in real-world clinical settings where
data availability varies across patients and institutions. To address these
limitations, we propose MoE-Health, a novel Mixture of Experts framework
designed for robust multimodal fusion in healthcare prediction. MoE-Health
architecture is specifically developed to handle samples with differing
modalities and improve performance on critical clinical tasks. By leveraging
specialized expert networks and a dynamic gating mechanism, our approach
dynamically selects and combines relevant experts based on available data
modalities, enabling flexible adaptation to varying data availability
scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical
clinical prediction tasks: in-hospital mortality prediction, long length of
stay, and hospital readmission prediction. Experimental results demonstrate
that MoE-Health achieves superior performance compared to existing multimodal
fusion methods while maintaining robustness across different modality
availability patterns. The framework effectively integrates multimodal
information, offering improved predictive performance and robustness in
handling heterogeneous and incomplete healthcare data, making it particularly
suitable for deployment in diverse healthcare environments with heterogeneous
data availability.

</details>


### [71] [Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety](https://arxiv.org/abs/2508.21722)
*Siddharth Mangalik,Ojas Deshpande,Adithya V. Ganesan,Sean A. P. Clouston,H. Andrew Schwartz*

Main category: cs.LG

TL;DR: 지역 이벤트의 커뮤니티별 정신 건강 영향을 추정하는 것은 공공 건강 정책에 필수적이다. 본 연구는 LRDD를 사용하여 정신 건강 점수의 변화를 예측하고 COVID-19 사건이 미국 카운티의 불안에 미치는 영향을 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 정신 건강 점수의 예측은 이벤트가 커뮤니티의 웰빙에 미치는 영향을 이해하는 데 한계가 있으며, 지역 이벤트에 대한 커뮤니티별 정신 건강 효과를 추정하는 것이 중요하다.

Method: LRDD를 동적인 공변량과 정적 변수의 역사에 따라 시간 특정 이벤트로 인한 결과의 변화를 추정하는 통계 학습 프레임워크로 확장한다.

Result: COVID-19 이벤트에 대한 불안 예측에 대해 동적 공변량과 외생 변수를 통합하여 더 나은 결과를 얻었다.

Conclusion: 이 방법은 전통적인 정적 커뮤니티 표현보다 유의미한 개선을 보이며, 특정 커뮤니티에 대한 미래 또는 가상 사건의 특정 효과를 추정할 수 있는 새로운 가능성을 제시한다.

Abstract: Estimating community-specific mental health effects of local events is vital
for public health policy. While forecasting mental health scores alone offers
limited insights into the impact of events on community well-being,
quasi-experimental designs like the Longitudinal Regression Discontinuity
Design (LRDD) from econometrics help researchers derive more effects that are
more likely to be causal from observational data. LRDDs aim to extrapolate the
size of changes in an outcome (e.g. a discontinuity in running scores for
anxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond
traditional forecasting into a statistical learning framework whereby future
discontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear
trajectories) are estimated given a location's history of the score, dynamic
covariates (other running assessments), and exogenous variables (static
representations). Applying our framework to predict discontinuities in the
anxiety of US counties from COVID-19 events, we found the task was difficult
but more achievable as the sophistication of models was increased, with the
best results coming from integrating exogenous and dynamic covariates. Our
approach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$
for slope) over traditional static community representations. Discontinuity
forecasting raises new possibilities for estimating the idiosyncratic effects
of potential future or hypothetical events on specific communities.

</details>


### [72] [UniMLR: Modeling Implicit Class Significance for Multi-Label Ranking](https://arxiv.org/abs/2508.21772)
*V. Bugra Yesilkaynak,Emine Dari,Alican Mertan,Gozde Unal*

Main category: cs.LG

TL;DR: 이 논문에서는 새로운 다중 레이블 순위(MLR) 접근 방식을 제안하며, 긍정 레이블 간의 순위를 모델링하여 MLR 작업을 통합하는 UniMLR을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 레이블 순위(Multi-label Ranking, MLR) 프레임워크는 라벨을 긍정과 부정 집합으로 나누어 그로부터 추론된 정보만을 활용한다.

Method: UniMLR은 긍정 레이블 간의 순위를 이용하여 암묵적인 클래스 관련성/중요도 값을 확률 분포로 모델링하며, 이를 통해 MLR과 관련된 순위와 분류 작업을 통합한다.

Result: 우리는 제안한 방법이 긍정적인 순위 순서를 정확하게 학습하며, 이는 실제 값과 일치하고 기본적인 중요도 값에 비례함을 통계적으로 입증하였다.

Conclusion: 최종적으로, 실제 및 합성 데이터셋에서 포괄적인 경험적 실험을 수행하여 제안한 프레임워크의 가치를 입증하였다.

Abstract: Existing multi-label ranking (MLR) frameworks only exploit information
deduced from the bipartition of labels into positive and negative sets.
Therefore, they do not benefit from ranking among positive labels, which is the
novel MLR approach we introduce in this paper. We propose UniMLR, a new MLR
paradigm that models implicit class relevance/significance values as
probability distributions using the ranking among positive labels, rather than
treating them as equally important. This approach unifies ranking and
classification tasks associated with MLR. Additionally, we address the
challenges of scarcity and annotation bias in MLR datasets by introducing eight
synthetic datasets (Ranked MNISTs) generated with varying
significance-determining factors, providing an enriched and controllable
experimental environment. We statistically demonstrate that our method
accurately learns a representation of the positive rank order, which is
consistent with the ground truth and proportional to the underlying
significance values. Finally, we conduct comprehensive empirical experiments on
both real-world and synthetic datasets, demonstrating the value of our proposed
framework.

</details>


### [73] [Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling](https://arxiv.org/abs/2508.21785)
*Peng Yang,Zhengdong Huang,Zicheng Xie,Wentao Tian,Jingyu Liu,Lunhong Dong*

Main category: cs.LG

TL;DR: 심박수 예측은 개인화된 건강 모니터링과 피트니스에 중요하지만, 실제 환경에서 데이터 이질성으로 인해 어려움에 직면한다. 본 논문은 이 문제를 해결하기 위해 이질성에 구애받지 않는 잠재 표현을 학습하는 프레임워크를 제안하고, 새로운 벤치마크 데이터셋인 ParroTao를 만들어 공개했다.


<details>
  <summary>Details</summary>
Motivation: 심박수 예측은 개인의 건강 모니터링과 피트니스 개선에 필수적이며, 다양한 데이터 출처와 사용자 패턴으로 인해 발생하는 이질성 문제를 해결할 필요가 있다.

Method: 이 논문에서는 무작위 특성 드롭아웃 전략을 도입하여 출처 이질성을 다루고, 시간 인식 주의 모듈을 활용하여 사용자 이질성을 관리하는 프레임워크를 제안한다.

Result: ParroTao 및 FitRec 공개 데이터셋에서 기존의 방법에 비해 각각 17% 및 15% 더 우수한 성능을 보였다.

Conclusion: 학습된 표현의 강력한 차별화 능력이 분석되었으며, 하나의 응용 과제가 모델의 실용적 가치를 확인해준다.

Abstract: Heart rate prediction is vital for personalized health monitoring and
fitness, while it frequently faces a critical challenge when deploying in
real-world: data heterogeneity. We classify it in two key dimensions: source
heterogeneity from fragmented device markets with varying feature sets, and
user heterogeneity reflecting distinct physiological patterns across
individuals and activities. Existing methods either discard device-specific
information, or fail to model user-specific differences, limiting their
real-world performance. To address this, we propose a framework that learns
latent representations agnostic to both heterogeneity, enabling downstream
predictors to work consistently under heterogeneous data patterns.
Specifically, we introduce a random feature dropout strategy to handle source
heterogeneity, making the model robust to various feature sets. To manage user
heterogeneity, we employ a time-aware attention module to capture long-term
physiological traits and use a contrastive learning objective to build a
discriminative representation space. To reflect the heterogeneous nature of
real-world data, we created and publicly released a new benchmark dataset,
ParroTao. Evaluations on both ParroTao and the public FitRec dataset show that
our model significantly outperforms existing baselines by 17% and 15%,
respectively. Furthermore, analysis of the learned representations demonstrates
their strong discriminative power, and one downstream application task confirm
the practical value of our model.

</details>


### [74] [QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.21810)
*Jessica Liang,Anirudh Bharadwaj*

Main category: cs.LG

TL;DR: Large Language Models의 성장으로 인해, 파라미터 효율적인 미세 조정 기술이 필요하다. QR-LoRA가 저조도 업데이트를 통해 기존의 LoRA보다 훨씬 적은 파라미터로 성능을 유지하거나 초과하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: Large Language Models의 규모 증가에 따라, 효율적인 파라미터 미세 조정 기술의 개발이 필요해졌다.

Method: QR 분해와 열 피벗을 사용하여 사전 훈련된 가중치 행렬에서 정규 직교 기저를 추출하고, LoRA 업데이트를 이러한 기저 벡터의 선형 결합으로 표현한다.

Result: QR-LoRA는 601개의 파라미터로 전체 미세 조정, 표준 LoRA, SVD-LoRA의 성능을 초과하거나 맞춘다.

Conclusion: 이 방법은 미세 조정에 명확한 구조를 부여하고 파라미터 수를 크게 줄인다.

Abstract: The growing scale of Large Language Models (LLMs) has necessitated the
development of parameter-efficient fine-tuning techniques. Low-Rank Adaptation
(LoRA) has emerged as a promising approach, reducing the number of trainable
parameters by applying low-rank updates to pretrained weights. While standard
LoRA learns both update factors directly, several recent variants first
initialize those matrices via an SVD of the pretrained weights -- an operation
that can be expensive on large models and yields singular vectors that are not
always easy to interpret. In this work, we extract an orthonormal basis from
the pretrained weight matrix using QR decomposition with column pivoting, and
then express the LoRA update as a linear combination of these basis vectors --
training only the scalar coefficients, which imposes clear structure on
adaptation and drastically reduces parameter count. Experiments across GLUE
tasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,
standard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular
value decomposition) with as few as 601 parameters -- a reduction of over 1000x
compared to full fine-tuning and 77x fewer than typical LoRA setups.

</details>


### [75] [Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation](https://arxiv.org/abs/2508.21815)
*Tobias Hyrup,Emmanouil Panagiotou,Arjun Roy,Arthur Zimek,Eirini Ntoutsi,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: FLIP는 프라이버시와 공정성을 보장하기 위해 설계된 합성 데이터 생성기이다.


<details>
  <summary>Details</summary>
Motivation: GDPR 및 HIPAA와 같은 프라이버시 규제가 강화되는 가운데, 실제 데이터의 윤리적이고 책임 있는 사용이 점점 더 많은 제한을 받게 된다.

Method: FLIP는 변형된 오토인코더와 잠재적 확산을 사용하여 이종 표 형식 데이터를 생성한다.

Result: FLIP은 차별적 프라이버시 제약하에서 작업 무관 공정성에 상당한 개선을 제공하는 효과를 입증하였다.

Conclusion: FLIP은 다양한 다운스트림 작업에서 공정성을 보장하면서 데이터 공유와 모델 개발을 위한 유망한 솔루션이다.

Abstract: As privacy regulations such as the GDPR and HIPAA and responsibility
frameworks for artificial intelligence such as the AI Act gain traction, the
ethical and responsible use of real-world data faces increasing constraints.
Synthetic data generation has emerged as a promising solution to risk-aware
data sharing and model development, particularly for tabular datasets that are
foundational to sensitive domains such as healthcare. To address both privacy
and fairness concerns in this setting, we propose FLIP (Fair Latent
Intervention under Privacy guarantees), a transformer-based variational
autoencoder augmented with latent diffusion to generate heterogeneous tabular
data. Unlike the typical setup in fairness-aware data generation, we assume a
task-agnostic setup, not reliant on a fixed, defined downstream task, thus
offering broader applicability. To ensure privacy, FLIP employs R\'enyi
differential privacy (RDP) constraints during training and addresses fairness
in the input space with RDP-compatible balanced sampling that accounts for
group-specific noise levels across multiple sampling rates. In the latent
space, we promote fairness by aligning neuron activation patterns across
protected groups using Centered Kernel Alignment (CKA), a similarity measure
extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment
encourages statistical independence between latent representations and the
protected feature. Empirical results demonstrate that FLIP effectively provides
significant fairness improvements for task-agnostic fairness and across diverse
downstream tasks under differential privacy constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture](https://arxiv.org/abs/2508.21803)
*Yeawon Lee,Xiaoyang Wang,Christopher C. Yang*

Main category: cs.AI

TL;DR: 본 연구는 임상 서사의 정확한 해석의 중요성을 강조하며, 다수의 에이전트를 활용한 협력형 시스템을 통해 진단 프로세스를 시뮬레이션하고 임상 결정을 지원하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 의료 진료를 위해서는 임상 서사의 정확한 해석이 필수적이지만, 이러한 노트의 복잡성으로 인해 자동화가 어렵다.

Method: 임상 상담 팀을 모델링한 협력형 다중 에이전트 시스템을 도입하여 SOAP 노트의 주관적(S) 및 객관적(O) 섹션만을 분석하여 임상 문제를 식별하고, 진단적 추론 과정을 시뮬레이션한다.

Result: 420개의 MIMIC-III 노트로 구성된 데이터 세트를 통해 단일 에이전트 기준선과 비교했을 때, 다중 에이전트 구성은 심부전, 급성 신장 손상 및 패혈증을 식별하는 데 있어 일관되게 향상된 성능을 보였다.

Conclusion: 의료 팀의 추론 과정을 모델링함으로써, 본 시스템은 보다 정확하고 견고하며 해석 가능한 임상 의사 결정 지원 도구로 나아갈 수 있는 유망한 경로를 제공한다.

Abstract: Accurate interpretation of clinical narratives is critical for patient care,
but the complexity of these notes makes automation challenging. While Large
Language Models (LLMs) show promise, single-model approaches can lack the
robustness required for high-stakes clinical tasks. We introduce a
collaborative multi-agent system (MAS) that models a clinical consultation team
to address this gap. The system is tasked with identifying clinical problems by
analyzing only the Subjective (S) and Objective (O) sections of SOAP notes,
simulating the diagnostic reasoning process of synthesizing raw data into an
assessment. A Manager agent orchestrates a dynamically assigned team of
specialist agents who engage in a hierarchical, iterative debate to reach a
consensus. We evaluated our MAS against a single-agent baseline on a curated
dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration
demonstrated consistently improved performance in identifying congestive heart
failure, acute kidney injury, and sepsis. Qualitative analysis of the agent
debates reveals that this structure effectively surfaces and weighs conflicting
evidence, though it can occasionally be susceptible to groupthink. By modeling
a clinical team's reasoning process, our system offers a promising path toward
more accurate, robust, and interpretable clinical decision support tools.

</details>


### [77] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 건축적 유도 편향이 LLM의 인지 행동에 미치는 영향을 조사하고, 적응적이고 구조화된 추론을 촉진하는 심볼릭 스캐폴딩 메커니즘을 도입하여 교육적 대화에서의 성능을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: LLM의 교육적 대화에서의 인지 행동에 대한 건축적 유도 편향의 영향을 이해하고자 함.

Method: 다섯 가지 시스템 변형에 대한 제어된 절제(ablation)을 사용하여 전문가가 설계한 루브릭을 통해 모델 출력을 평가하고, LLM 기반 평가 프레임워크를 사용하여 비교하였다.

Result: 전체 시스템이 기준 변형보다 일관되게 우수한 성능을 보였으며, 기억이나 상징적 구조를 제거하면 추상화, 적응적 탐색, 개념적 연속성 등 주요 인지 행동이 저하됨을 발견하였다.

Conclusion: 건축적 스캐폴드가 LLMs에서 교육 전략을 신뢰성 있게 형성할 수 있는 처리 수준 설명을 지지함.

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [78] [Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs](https://arxiv.org/abs/2508.21238)
*Tingxuan Xu,Jiarui Feng,Justin Melendez,Kaleigh Roberts,Donghong Cai,Mingfang Zhu,Donald Elbert,Yixin Chen,Randall J. Bateman*

Main category: cs.AI

TL;DR: 본 논문에서는 알츠하이머 질환과 같은 전문 지식이 필요한 특정 도메인에서 GraphRAG 시스템을 평가하고, 그 응답 품질 및 추적 가능성을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 기반의 챗봇이 다양한 작업 완료 및 질문 응답 능력을 통해 혁신을 이루었으나, 과학 연구에 활용할 때 환각, 제한된 도메인 지식, 응답의 설명 가능성 부족 같은 문제로 제약을 받고 있습니다. 이에 따라 GraphRAG 접근법을 통해 챗봇의 신뢰성을 개선하고자 합니다.

Method: 알츠하이머 질환과 관련된 50개의 논문과 70개의 전문가 질문으로 구성된 데이터베이스를 만들고, GraphRAG 지식 기반을 구축하며, GPT-4o를 LLM으로 활용하여 질문에 답변합니다. 그리고 GraphRAG로 생성된 응답의 품질을 표준 GPT-4o 모델과 비교합니다.

Result: GraphRAG의 응답 품질을 표준 GPT-4o 모델과 비교하여 신뢰성 있는 결과를 도출하고, 여러 RAG 및 GraphRAG 시스템의 추적 가능성을 평가합니다.

Conclusion: 연구자들이 표준 RAG 및 GraphRAG의 성능을 시험할 수 있도록 알츠하이머 질환 데이터베이스가 미리 구축된 사용자 친화적인 인터페이스를 제공합니다.

Abstract: In the past two years, large language model (LLM)-based chatbots, such as
ChatGPT, have revolutionized various domains by enabling diverse task
completion and question-answering capabilities. However, their application in
scientific research remains constrained by challenges such as hallucinations,
limited domain-specific knowledge, and lack of explainability or traceability
for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has
emerged as a promising approach to improving chatbot reliability by integrating
domain-specific contextual information before response generation, addressing
some limitations of standard LLMs. Despite its potential, there are only
limited studies that evaluate GraphRAG on specific domains that require
intensive knowledge, like Alzheimer's disease or other biomedical domains. In
this paper, we assess the quality and traceability of two popular GraphRAG
systems. We compile a database of 50 papers and 70 expert questions related to
Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as
the LLM for answering queries. We then compare the quality of responses
generated by GraphRAG with those from a standard GPT-4o model. Additionally, we
discuss and evaluate the traceability of several Retrieval-Augmented Generation
(RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a
pre-built Alzheimer's disease database for researchers to test the performance
of both standard RAG and GraphRAG.

</details>


### [79] [MultiFluxAI Enhancing Platform Engineering with Advanced Agent-Orchestrated Retrieval Systems](https://arxiv.org/abs/2508.21307)
*Sri Ram Macharla,Sridhar Murthy J,Anjaneyulu Pasala*

Main category: cs.AI

TL;DR: MultiFluxAI는 제품 엔지니어링에서 방대한 데이터 소스를 관리 및 통합하기 위한 혁신적인 AI 플랫폼입니다.


<details>
  <summary>Details</summary>
Motivation: 방대한 이질적 데이터 소스를 효과적으로 관리하고 통합하여 사용자 참여를 증진시키는 것이 필요하다.

Method: Generative AI, 벡터화 및 주체적 오케스트레이션과 같은 고급 AI 기술을 활용하여 복잡한 사용자 쿼리에 동적이고 맥락에 맞는 응답을 제공한다.

Result: 이 플랫폼은 사용자 쿼리에 대한 향상된 응답을 통해 디지털 생태계에서의 사용자 참여를 높인다.

Conclusion: MultiFluxAI는 제품 엔지니어링과 다양한 응용 분야에서 데이터 관리의 차별화된 솔루션을 제공한다.

Abstract: MultiFluxAI is an innovative AI platform developed to address the challenges
of managing and integrating vast, disparate data sources in product engineering
across application domains. It addresses both current and new service related
queries that enhance user engagement in the digital ecosystem. This platform
leverages advanced AI techniques, such as Generative AI, vectorization, and
agentic orchestration to provide dynamic and context-aware responses to complex
user queries.

</details>


### [80] [Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation](https://arxiv.org/abs/2508.21320)
*Mohsen Nayebi Kerdabadi,Arya Hadizadeh Moghaddam,Dongjie Wang,Zijun Yao*

Main category: cs.AI

TL;DR: LINKO는 여러 온톨로지 그래프를 동시에 활용하여 의료 개념 표현 학습을 향상시키는 대규모 언어 모델 증강 통합 온톨로지 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 의료 온톨로지 그래프는 전자 건강 기록의 의료 코드를 구조적 관계를 통해 외부 지식에 매핑한다.

Method: LINKO는 이종 온톨로지 시스템 내외부에서 이중 축 지식 전파를 가능하게 하여 다양한 온톨로지 그래프에서 의학적 개념을 학습한다.

Result: LINKO는 두 개의 공공 데이터 세트에서 기존 최첨단 기준보다 우수한 성능을 보였다.

Conclusion: 기존 EHR 예측 모델과 호환되는 플러그인 인코더로서 LINKO는 제한된 데이터 가용성과 희귀 질병 예측과 같은 시나리오에서 강력한 성능을 나타낸다.

Abstract: Medical ontology graphs map external knowledge to medical codes in electronic
health records via structured relationships. By leveraging domain-approved
connections (e.g., parent-child), predictive models can generate richer medical
concept representations by incorporating contextual information from related
concepts. However, existing literature primarily focuses on incorporating
domain knowledge from a single ontology system, or from multiple ontology
systems (e.g., diseases, drugs, and procedures) in isolation, without
integrating them into a unified learning structure. Consequently, concept
representation learning often remains limited to intra-ontology relationships,
overlooking cross-ontology connections. In this paper, we propose LINKO, a
large language model (LLM)-augmented integrative ontology learning framework
that leverages multiple ontology graphs simultaneously by enabling dual-axis
knowledge propagation both within and across heterogeneous ontology systems to
enhance medical concept representation learning. Specifically, LINKO first
employs LLMs to provide a graph-retrieval-augmented initialization for ontology
concept embedding, through an engineered prompt that includes concept
descriptions, and is further augmented with ontology context. Second, our
method jointly learns the medical concepts in diverse ontology graphs by
performing knowledge propagation in two axes: (1) intra-ontology vertical
propagation across hierarchical ontology levels and (2) inter-ontology
horizontal propagation within every level in parallel. Last, through extensive
experiments on two public datasets, we validate the superior performance of
LINKO over state-of-the-art baselines. As a plug-in encoder compatible with
existing EHR predictive models, LINKO further demonstrates enhanced robustness
in scenarios involving limited data availability and rare disease prediction.

</details>


### [81] [Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2508.21365)
*Yi Liao,Yu Gu,Yuan Sui,Zining Zhu,Yifan Lu,Guohua Tang,Zhongqian Sun,Wei Yang*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)은 수학 및 코딩과 같은 복잡한 추론 작업에서 뛰어난 성능을 보이지만, 어린이가 쉽게 수행하는 간단한 상호작용 작업에서는 자주 어려움을 겪는다. 이 논문에서는 LLM이 게임 환경과의 직접적인 상호작용을 통해 절차적 이해를 발전시킬 수 있도록 하는 새로운 프레임워크인 'Think in Games(TiG)'를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델과 전통적인 강화 학습 에이전트 간의 격차를 해소하고, LLM이 절차적 지식을 개발할 수 있도록 하기 위해

Method: TiG는 RL 기반 의사 결정을 언어 모델링 작업으로 재구성하여 LLM이 언어 안내 정책을 생성하고 환경 피드백에 따라 온라인 강화 학습을 통해 반복적으로 개선하도록 한다.

Result: TiG는 선언적 지식과 절차적 지식 간의 격차를 성공적으로 해소하며, 기존의 RL 방법과 비교하여 데이터와 계산 요구가 현저히 낮으면서 경쟁력 있는 성능을 달성한다.

Conclusion: TiG는 복잡한 상호작용 작업에서 결정에 대한 단계별 자연어 설명을 제공하여 투명성과 해석력을 크게 개선한다.

Abstract: Large language models (LLMs) excel at complex reasoning tasks such as
mathematics and coding, yet they frequently struggle with simple interactive
tasks that young children perform effortlessly. This discrepancy highlights a
critical gap between declarative knowledge (knowing about something) and
procedural knowledge (knowing how to do something). Although traditional
reinforcement learning (RL) agents can acquire procedural knowledge through
environmental interaction, they often operate as black boxes and require
substantial training data. In contrast, LLMs possess extensive world knowledge
and reasoning capabilities, but are unable to effectively convert this static
knowledge into dynamic decision-making in interactive settings. To address this
challenge, we propose Think in Games (TiG), a novel framework that empowers
LLMs to develop procedural understanding through direct interaction with game
environments, while retaining their inherent reasoning and explanatory
abilities. Specifically, TiG reformulates RL-based decision-making as a
language modeling task: LLMs generate language-guided policies, which are
refined iteratively through online reinforcement learning based on
environmental feedback. Our experimental results show that TiG successfully
bridges the gap between declarative and procedural knowledge, achieving
competitive performance with dramatically lower data and computational demands
compared to conventional RL methods. Moreover, TiG provides step-by-step
natural language explanations for its decisions, greatly improving transparency
and interpretability in complex interactive tasks.

</details>


### [82] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: AHELM은 오디오 언어 모델의 평가를 위한 새로운 벤치마크로, 모델 성능을 10가지 중요한 측면에서 종합적으로 측정한다.


<details>
  <summary>Details</summary>
Motivation: 현재 오디오 언어 모델 평가의 표준화된 벤치마크가 부족하며, 모델 간의 비교가 어려운 문제를 해결하기 위해.

Method: AHELM은 다양한 데이터셋을 통합하여 10가지 중요한 측면에서 ALM의 성능을 평가하며, 프롬프트와 평가 지표를 표준화한다.

Result: 14개의 ALM을 테스트한 결과, Gemini 2.5 Pro는 5개 측면에서 최고 순위를 기록했으나, 그룹 불공정성을 보였고, 기준 시스템들은 비교적 좋은 성과를 냈다.

Conclusion: AHELM은 지속적으로 발전하는 벤치마크로, 시간이 지나면서 새로운 데이터셋과 모델이 추가될 예정이다.

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


### [83] [AI Compute Architecture and Evolution Trends](https://arxiv.org/abs/2508.21394)
*Bor-Sung Liang*

Main category: cs.AI

TL;DR: AI 개발의 초점이 학술 연구에서 실용적 응용으로 이동하고 있으며, 7단계 모델을 통해 AI의 기회와 도전 과제를 분석한다.


<details>
  <summary>Details</summary>
Motivation: AI 개발이 학술적으로부터 실용적인 응용으로 이동함에 따라 여러 수준에서 다양한 도전에 직면하고 있기 때문에.

Method: AI 계산 아키텍처를 위한 7단계 모델을 제안하며, 각 레이어에서 발전 경로와 핵심 기술을 설명한다.

Result: AI 에이전트의 경향과 단일 AI 에이전트에서 AI 기반 생태계로의 발전 문제를 논의하여 AI 산업에 미치는 영향을 탐구한다.

Conclusion: AI 개발은 기술적 도전 외에도 자생 가능한 생태계를 구축하기 위한 경제적 문제가 포함된다.

Abstract: The focus of AI development has shifted from academic research to practical
applications. However, AI development faces numerous challenges at various
levels. This article will attempt to analyze the opportunities and challenges
of AI from several different perspectives using a structured approach. This
article proposes a seven-layer model for AI compute architecture, including
Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,
Orchestrator Layer, and Application Layer, from bottom to top. It also explains
how AI computing has evolved into this 7-layer architecture through the
three-stage evolution on large-scale language models (LLMs). For each layer, we
describe the development trajectory and key technologies. In Layers 1 and 2 we
discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies
on computing architecture. In Layer 3 we explore two different development
paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs
and compares it to traditional processor memory. In Layers 5 to 7 we discuss
the trends of AI agents and explore the issues in evolution from a single AI
agent to an AI-based ecosystem, and their impact on the AI industry.
Furthermore, AI development involves not only technical challenges but also the
economic issues to build self-sustainable ecosystem. This article analyzes the
internet industry to provide predictions on the future trajectory of AI
development.

</details>


### [84] [CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN](https://arxiv.org/abs/2508.21411)
*Leonard Frank Neis,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: CARJAN은 보행자, 자전거 동호인 및 자율주행 차량과 같은 상호작용 에이전트를 포함하는 도시 교통 시나리오의 반자동 생성 및 시뮬레이션을 위한 도구이다.


<details>
  <summary>Details</summary>
Motivation: 도시 교통 시나리오의 사용자 친화적 모델링 및 가상 시뮬레이션은 여전히 도전 과제로 남아있다.

Method: AJAN이라는 다중 에이전트 공학 프레임워크와 CARLA 드라이빙 시뮬레이터를 기반으로 하여 반자동으로 시나리오를 생성하고 시뮬레이션하는 CARJAN이라는 새로운 도구를 제공한다.

Result: CARJAN은 교통 시나리오 레이아웃의 모델링, 저장 및 유지 관리를 위한 시각적 사용자 인터페이스를 제공하며, CARLA의 동적 시나리오 시뮬레이션에서 에이전트를 위한 SPARQL 행동 트리 기반 의사 결정 및 상호작용을 활용한다.

Conclusion: CARJAN은 CARLA에서의 대화형, 지능형 에이전트 기반 가상 교통 시나리오 생성 및 시뮬레이션을 위한 통합된 접근 방식을 제공한다.

Abstract: User-friendly modeling and virtual simulation of urban traffic scenarios with
different types of interacting agents such as pedestrians, cyclists and
autonomous vehicles remains a challenge. We present CARJAN, a novel tool for
semi-automated generation and simulation of such scenarios based on the
multi-agent engineering framework AJAN and the driving simulator CARLA. CARJAN
provides a visual user interface for the modeling, storage and maintenance of
traffic scenario layouts, and leverages SPARQL Behavior Tree-based
decision-making and interactions for agents in dynamic scenario simulations in
CARLA. CARJAN provides a first integrated approach for interactive, intelligent
agent-based generation and simulation of virtual traffic scenarios in CARLA.

</details>


### [85] [A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions](https://arxiv.org/abs/2508.21441)
*Christoph Beierle,Alexander Hahn,Diana Howey,Gabriele Kern-Isberner,Kai Sauerwald*

Main category: cs.AI

TL;DR: 이 논문은 지식 관리 작업으로서의 망각을 다루며, 다양한 의미에서 망각 연산을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 망각은 에이전트의 지식 및 신념 중 일부를 의도적으로 무시하는 작업으로, 이는 여러 가지 이유로 필요하다.

Method: 논문에서는 에피스테믹 관점에서 의미론적 구조가 더 풍부한 에피스테믹 상태에서의 망각 연산을 연구한다.

Result: 다섯 가지 일반적인 유형의 에피스테믹 망각을 제시하고, 이를 통해 스포른의 순위 함수에 대한 일곱 가지 구체적인 망각 연산을 구현한다.

Conclusion: 제안된 모든 구체적인 망각 연산을 평가하여 망각 연산자 간의 차이점과 공통점을 강조하는 포괄적인 개요를 제공한다.

Abstract: Forgetting as a knowledge management operation deliberately ignores parts of
the knowledge and beliefs of an agent, for various reasons. Forgetting has many
facets, one may want to forget parts of the syntax, a proposition, or a
conditional. In the literature, two main operators suitable for performing
forgetting have been proposed and investigated in depth: First, variable
elimination is a syntactical method that blends out certain atomic variables to
focus on the rest of the language. It has been mainly used in the area of logic
programming and answer set programming. Second, contraction in AGM belief
revision theory effectively removes propositions from belief sets under logical
deduction. Both operations rely mainly on classical logics. In this article, we
take an epistemic perspective and study forgetting operations in epistemic
states with richer semantic structures, but with clear links to propositional
logic. This allows us to investigate what forgetting in the epistemic
background means, thereby lifting well-known and novel forgetting operations to
the epistemic level. We present five general types of epistemic forgetting and
instantiate them with seven concrete forgetting operations for Spohn's ranking
functions. We take inspiration from postulates of forgetting both from logic
programming and AGM theory to propose a rich landscape of axioms for evaluating
forgetting operations. Finally, we evaluate all concrete forgetting operations
according to all postulates, leading to a novel comprehensive overview
highlighting differences and commonalities among the forgetting operators.

</details>


### [86] [Learning Lifted Action Models From Traces of Incomplete Actions and States](https://arxiv.org/abs/2508.21449)
*Niklas Jansen,Jonas Gösgens,Hector Geffner*

Main category: cs.AI

TL;DR: 이 논문은 슬라이딩 타일 퍼즐의 리프팅된 STRIPS 모델 학습 문제를 다루며, 상태-행동 추적에서 발견된 불완전한 상태와 액션 정보를 바탕으로 STRIPS+라는 새로운 변형을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 슬라이딩 타일 퍼즐의 상태-행동 추적에서 불완전한 정보를 학습하는 문제를 해결하기 위해.

Method: 새로운 STRIPS+ 변형을 도입하고, SYNTH라는 학습 알고리즘을 사용하여 상태-행동 추적에서 STRIPS+ 모델을 학습한다.

Result: SYNTH의 정확성과 완전성을 입증하고, STRIPS+ 모델에서 얻은 상태-행동 추적에 대한 알고리즘의 확장성을 검증했다.

Conclusion: 이 연구는 실제 상황에서 슬라이딩 타일 퍼즐을 모델링하는 데 필요한 불완전한 정보에 대한 효과적인 학습 방법을 제시한다.

Abstract: Consider the problem of learning a lifted STRIPS model of the sliding-tile
puzzle from random state-action traces where the states represent the location
of the tiles only, and the actions are the labels up, down, left, and right,
with no arguments. Two challenges are involved in this problem. First, the
states are not full STRIPS states, as some predicates are missing, like the
atoms representing the position of the ``blank''. Second, the actions are not
full STRIPS either, as they do not reveal all the objects involved in the
actions effects and preconditions. Previous approaches have addressed different
versions of this model learning problem, but most assume that actions in the
traces are full STRIPS actions or that the domain predicates are all
observable. The new setting considered in this work is more ``realistic'', as
the atoms observed convey the state of the world but not full STRIPS states,
and the actions reveal the arguments needed for selecting the action but not
the ones needed for modeling it in STRIPS. For formulating and addressing the
learning problem, we introduce a variant of STRIPS, which we call STRIPS+,
where certain STRIPS action arguments can be left implicit in preconditions
which can also involve a limited form of existential quantification. The
learning problem becomes the problem of learning STRIPS+ models from STRIPS+
state-action traces. For this, the proposed learning algorithm, called SYNTH,
constructs a stratified sequence (conjunction) of precondition expressions or
``queries'' for each action, that denote unique objects in the state and ground
the implicit action arguments in STRIPS+. The correctness and completeness of
SYNTH is established, and its scalability is tested on state-action traces
obtained from STRIPS+ models derived from existing STRIPS domains.

</details>


### [87] [MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.21475)
*Xijia Tao,Yihua Teng,Xinxing Su,Xinyu Fu,Jihao Wu,Chaofan Tao,Ziru Liu,Haoli Bai,Rui Liu,Lingpeng Kong*

Main category: cs.AI

TL;DR: 이 논문에서는 멀티모달 이해를 요구하는 311개의 작업을 포함한 MMSearch-Plus 벤치마크를 소개하며, 이를 통해 웹 에이전트의 멀티모달 과제를 해결하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 대형 멀티모달 언어 모델(MLLMs)이 웹 에이전트로 점점 더 많이 배포되고 있음에도 불구하고 기존의 멀티모달 탐색 벤치마크는 고정된 작업 흐름을 통해 해결될 수 있는 한계가 있음. 이를 극복하기 위해 보다 복잡한 멀티모달 이해를 요구하는 검증 시스템이 필요하다.

Method: MMSearch-Plus는 여러 약한 시각 신호를 포함하며, 이러한 신호를 반복적인 텍스트-이미지 검색을 통해 추출하고 전파하며, 검색 노이즈 하에서 교차 검증을 통해 문제를 해결하는 구조로 되어 있다.

Result: 가장 강력한 에이전트(o3)는 검색 없이 15.1%의 정확도, 검색 후 36.0%의 정확도를 달성한다. 강력한 오픈 소스 모델(Qwen-2.5-VL-72B-Instruct)은 검색 없이 0.0%, 20회의 검색 후 6.9%의 정확도를 기록한다.

Conclusion: 답변 정확도 외에도 바운딩 박스 생성 및 크롭 이미지 검색을 평가하고, 출처 검증, 부분 기반 추론 및 장기 계획에서의 실패를 드러내는 오류 분석을 수행하였다.

Abstract: Large multimodal language models (MLLMs) are increasingly deployed as web
agents, yet many multimodal browsing benchmarks can be solved by shallow, fixed
workflows that lean on high-recall image search and nearby text-masking the
genuinely multimodal challenges of fine-grained visual reasoning, provenance
verification, and long-horizon tool use. We introduce MMSearch-Plus, a
benchmark of 311 tasks that highly demand multimodal understanding while
preserving the difficulty profile of strong text-only browsing suites. Each
item is constructed to contain multiple weak, localized visual signals that
must be extracted, propagated through iterative text-image search, and
cross-validated under retrieval noise before answering. Our curation procedure,
Spatial-Temporal Extrapolation, seeds questions whose answers require
extrapolating from spatial cues (micro-text, part-level appearance, layouts,
signage) and temporal traces (broadcast overlays, seasonal context) to
out-of-image facts such as events, dates, and venues. We provide a
model-agnostic agent framework with browsing tools and evaluate a range of
closed and open MLLMs. The strongest agent (o3) attains 15.1% without search
and 36.0% accuracy with rollout under our framework, while a strong open-source
model (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20
rounds of search. Beyond answer accuracy, we assess bounding-box production and
cropped-image search, and conduct an error analysis that surfaces failures in
source verification, part-based reasoning, and long-horizon planning.

</details>


### [88] [Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis](https://arxiv.org/abs/2508.21517)
*Sweta Kaman,Ankita Sharma,Romi Banerjee*

Main category: cs.AI

TL;DR: 이 논문은 지혜를 다차원적이고 불확실성을 인식하는 구성으로 형식화하고, Z 숫자 형태로 운영화하는 체계를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 추론 모델이 이분법적 사고에 얽매이지 않고, 다차원성과 확신을 고려하는 계산적 프레임워크가 심리학과 인간 중심 AI를 향상시킬 수 있다.

Method: Z 숫자를 이용한 모호한 추론 시스템을 제시하고, 참가자들이 문화적으로 중립적인 도덕적 딜레마 과제를 수행하도록 하여 지혜의 이론적 구성 요소를 도출했다.

Result: 개념 증명 연구에서, 시스템은 확립된 척도와 중간 정도로 유의미한 상관관계를 가지며 관련 없는 특성과는 무관한 관계를 나타냈다.

Conclusion: 이 연구는 지혜를 다차원적이고 불확실성을 인식하는 구성으로 정형화하며, AI 시스템에 해석 가능한 이유 제공의 방법으로 기여한다.

Abstract: Background: Wisdom is a superordinate construct that embraces perspective
taking, reflectiveness, prosocial orientation, reflective empathetic action,
and intellectual humility. Unlike conventional models of reasoning that are
rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,
requiring both graded evaluation and self-reflective humility. Current measures
depend on self-reports and seldom reflect the humility and uncertainty inherent
in wise reasoning. A computational framework that takes into account both
multidimensionality and confidence has the potential to improve psychological
science and allow humane AI. Method: We present a fuzzy inference system with Z
numbers, each of the decisions being expressed in terms of a wisdom score
(restriction) and confidence score (certainty). As part of this study,
participants (N = 100) were exposed to culturally neutral pictorial moral
dilemma tasks to which they generated think-aloud linguistic responses, which
were mapped into five theoretically based components of wisdom. The scores of
each individual component were combined using a base of 21 rules, with
membership functions tuned via Gaussian kernel density estimation. Results: In
a proof of concept study, the system produced dual attribute wisdom
representations that correlated modestly but significantly with established
scales while showing negligible relations with unrelated traits, supporting
convergent and divergent validity. Contribution: The contribution is to
formalize wisdom as a multidimensional, uncertainty-conscious construct,
operationalized in the form of Z-numbers. In addition to progressing
measurement in psychology, it calculates how fuzzy Z numbers can provide AI
systems with interpretable, confidence-sensitive reasoning that affords a safe,
middle ground between rigorous computation and human-like judgment.

</details>


### [89] [Counterfactual Scenarios for Automated Planning](https://arxiv.org/abs/2508.21521)
*Nicola Gigante,Francesco Leofante,Andrea Micheli*

Main category: cs.AI

TL;DR: 이 논문은 반사실적 설명을 통해 계획 문제를 해결하는 새로운 설명 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 반사실적 설명 기법이 해결되는 문제의 고급 속성을 포착하지 못하는 한계를 극복하기 위해.

Method: 주어진 계획 문제와 원하는 속성을 정의하는 ltlf 공식을 기반으로 한다. 반사실적 시나리오는 해당 속성을 만족하는 계획을 허용하는 최소한의 수정 사항을 식별한다.

Result: 다양한 유형의 변경이 허용될 때 반사실적 시나리오를 생성하는 계산 복잡성을 특성화하였다.

Conclusion: 계획 문제에 대한 반사실적 시나리오 생성을 위해 제안한 방법의 실용성을 입증했다.

Abstract: Counterfactual Explanations (CEs) are a powerful technique used to explain
Machine Learning models by showing how the input to a model should be minimally
changed for the model to produce a different output. Similar proposals have
been made in the context of Automated Planning, where CEs have been
characterised in terms of minimal modifications to an existing plan that would
result in the satisfaction of a different goal. While such explanations may
help diagnose faults and reason about the characteristics of a plan, they fail
to capture higher-level properties of the problem being solved. To address this
limitation, we propose a novel explanation paradigm that is based on
counterfactual scenarios. In particular, given a planning problem $P$ and an
\ltlf formula $\psi$ defining desired properties of a plan, counterfactual
scenarios identify minimal modifications to $P$ such that it admits plans that
comply with $\psi$. In this paper, we present two qualitative instantiations of
counterfactual scenarios based on an explicit quantification over plans that
must satisfy $\psi$. We then characterise the computational complexity of
generating such counterfactual scenarios when different types of changes are
allowed on $P$. We show that producing counterfactual scenarios is often only
as expensive as computing a plan for $P$, thus demonstrating the practical
viability of our proposal and ultimately providing a framework to construct
practical algorithms in this area.

</details>


### [90] [HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining](https://arxiv.org/abs/2508.21540)
*Eduardo Illueca-Fernandez,Kaile Chen,Fernando Seoane,Farhad Abtahi*

Main category: cs.AI

TL;DR: HealthProcessAI는 의료 및 역학 분야에서 과정 마이닝을 단순화하기 위해 설계된 GenAI 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 의료 워크플로우에 대한 이해를 높이는 과정 마이닝의 기술적 복잡성 및 표준화 부족 문제를 해결하고자 함.

Method: 기존의 Python(PM4PY) 및 R(bupaR) 라이브러리를 기반으로 한 포괄적 래퍼가 있으며, 여러 개의 대형 언어 모델(LLM)을 통합하여 자동화된 프로세스 맵 해석 및 보고서 생성을 제공함.

Result: 프레임워크는 패혈증 진행 데이터를 사용하여 개념 증명으로 검증되었으며, 자동화된 LLM 분석을 통해 보고서를 생성하는 기능을 성공적으로 시연함.

Conclusion: 이 프레임워크는 LLM을 활용하여 프로세스 마이닝 출력의 접근성을 높여 의료 분야의 다양한 사용자들이 이해할 수 있는 형태로 변환함.

Abstract: Process mining has emerged as a powerful analytical technique for
understanding complex healthcare workflows. However, its application faces
significant barriers, including technical complexity, a lack of standardized
approaches, and limited access to practical training resources. We introduce
HealthProcessAI, a GenAI framework designed to simplify process mining
applications in healthcare and epidemiology by providing a comprehensive
wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address
unfamiliarity and improve accessibility, the framework integrates multiple
Large Language Models (LLMs) for automated process map interpretation and
report generation, helping translate technical analyses into outputs that
diverse users can readily understand. We validated the framework using sepsis
progression data as a proof-of-concept example and compared the outputs of five
state-of-the-art LLM models through the OpenRouter platform. To test its
functionality, the framework successfully processed sepsis data across four
proof-of-concept scenarios, demonstrating robust technical performance and its
capability to generate reports through automated LLM analysis. LLM evaluation
using five independent LLMs as automated evaluators revealed distinct model
strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency
scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By
integrating multiple Large Language Models (LLMs) for automated interpretation
and report generation, the framework addresses widespread unfamiliarity with
process mining outputs, making them more accessible to clinicians, data
scientists, and researchers. This structured analytics and AI-driven
interpretation combination represents a novel methodological advance in
translating complex process mining results into potentially actionable insights
for healthcare applications.

</details>


### [91] [Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances](https://arxiv.org/abs/2508.21564)
*Issa Hanou,Sebastijan Dumančić,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: 새로운 랜드마크 발견 프레임워크를 제안하며, 이것은 도메인 전반에 걸쳐 자동으로 일반화된다. 이 프레임워크는 전통적인 랜드마크 추출 알고리즘의 한계를 극복하고, 반복성을 포착하여 계획 문제 해결을 돕는다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 랜드마크 추출 알고리즘이 한계를 가지며, 더 나은 계획 문제 해결을 위한 새로운 도구가 필요하다.

Method: 해상된 인스턴스 집합에서 학습한 일반화된 랜드마크를 사용하여 상태 함수에 기반한 유도된 랜드마크 그래프를 구축한다. 이 그래프는 랜드마크 진행을 정의하고 반복적인 서브플랜의 루프 가능성을 포함한다.

Result: 소수의 작은 인스턴스에서 학습된 일반화된 랜드마크 그래프가 동일한 도메인의 더 큰 인스턴스에서도 효과적임을 보여준다. 반복을 나타내는 루프를 식별하면 휴리스틱 성능이显著 개선된다.

Conclusion: 일반화된 랜드마크는 도메인 정보를 포착하며, 이는 자동 계획자에게 해석 가능하고 유용하다. 이 정보는 동일한 도메인의 소규모 계획 세트에서 발견될 수 있다.

Abstract: We propose a new framework for discovering landmarks that automatically
generalize across a domain. These generalized landmarks are learned from a set
of solved instances and describe intermediate goals for planning problems where
traditional landmark extraction algorithms fall short. Our generalized
landmarks extend beyond the predicates of a domain by using state functions
that are independent of the objects of a specific problem and apply to all
similar objects, thus capturing repetition. Based on these functions, we
construct a directed generalized landmark graph that defines the landmark
progression, including loop possibilities for repetitive subplans. We show how
to use this graph in a heuristic to solve new problem instances of the same
domain. Our results show that the generalized landmark graphs learned from a
few small instances are also effective for larger instances in the same domain.
If a loop that indicates repetition is identified, we see a significant
improvement in heuristic performance over the baseline. Generalized landmarks
capture domain information that is interpretable and useful to an automated
planner. This information can be discovered from a small set of plans for the
same domain.

</details>


### [92] [Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics](https://arxiv.org/abs/2508.21595)
*Yang You,Alex Schutz,Zhikun Li,Bruno Lacerda,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: 이 논문은 결정론적 분산 POMDP(Det-Dec-POMDP) 문제에 대한 새로운 접근 방식과 솔버를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 고급 다중 에이전트 계획 문제는 결정론적 행동과 관찰을 사용하여 효과적으로 모델링될 수 있습니다. 이러한 문제들을 해결하기 위해 효율적인 솔버가 필요합니다.

Method: Iterative Deterministic POMDP Planning(IDPP)이라는 새로운 솔버를 제안하며, 이는 고전적인 전략 검색 프레임워크를 기반으로 하여 대규모 Det-Dec-POMDP 문제에 최적화되어 있습니다.

Result: IDPP는 기존의 Dec-POMDP 솔버가 처리할 수 없는 대규모 문제에 대한 효율적인 해결책을 제공합니다.

Conclusion: 이 연구는 Det-Dec-POMDP의 실용적인 해결 방법을 제시하고, 복잡한 다중 에이전트 계획 문제에 대한 시스템적 접근 방식을 제안합니다.

Abstract: Many high-level multi-agent planning problems, including multi-robot
navigation and path planning, can be effectively modeled using deterministic
actions and observations.
  In this work, we focus on such domains and introduce the class of
Deterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of
Dec-POMDPs characterized by deterministic transitions and observations
conditioned on the state and joint actions.
  We then propose a practical solver called Iterative Deterministic POMDP
Planning (IDPP). This method builds on the classic Joint Equilibrium Search for
Policies framework and is specifically optimized to handle large-scale
Det-Dec-POMDPs that current Dec-POMDP solvers are unable to address
efficiently.

</details>


### [93] [Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study](https://arxiv.org/abs/2508.21622)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 전통적인 네트워크 최적화 모델과 대형 언어 모델을 통합한 프레임워크를 통해 공급망 계획을 위한 상호작용적이고 설명 가능한 결정 지원 시스템을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 운영 연구 결과와 비즈니스 이해 관계자 간의 간극을 좁히기 위함입니다.

Method: 혼합 정수 수식을 사용하여 다중 기간 및 다중 품목에 대한 배급 센터 네트워크 전반의 전술적 재고 재분배를 다루는 핵심 최적화 모델을 개발합니다.

Result: 시스템은 재고 부족을 방지하고 비용을 줄이며 서비스 수준을 유지하여 계획 결과를 개선합니다.

Conclusion: 미래 확장으로는 개인 LLM, 전이 학습, 강화 학습 및 베이지안 신경망을 통합하여 설명 가능성, 적응성 및 실시간 의사 결정을 강화할 수 있습니다.

Abstract: This paper presents an integrated framework that combines traditional network
optimization models with large language models (LLMs) to deliver interactive,
explainable, and role-aware decision support for supply chain planning. The
proposed system bridges the gap between complex operations research outputs and
business stakeholder understanding by generating natural language summaries,
contextual visualizations, and tailored key performance indicators (KPIs). The
core optimization model addresses tactical inventory redistribution across a
network of distribution centers for multi-period and multi-item, using a
mixed-integer formulation. The technical architecture incorporates AI agents,
RESTful APIs, and a dynamic user interface to support real-time interaction,
configuration updates, and simulation-based insights. A case study demonstrates
how the system improves planning outcomes by preventing stockouts, reducing
costs, and maintaining service levels. Future extensions include integrating
private LLMs, transfer learning, reinforcement learning, and Bayesian neural
networks to enhance explainability, adaptability, and real-time
decision-making.

</details>


### [94] [A-MHA*: Anytime Multi-Heuristic A*](https://arxiv.org/abs/2508.21637)
*Ramkumar Natarajan,Muhammad Suhail Saleem,William Xiao,Sandip Aine,Howie Choset,Maxim Likhachev*

Main category: cs.AI

TL;DR: 이 논문에서는 Multi-Heuristic A* (MHA*) 알고리즘을 확장하여 언제든지 실행 가능한 버전인 A-MHA*를 제안합니다. A-MHA*는 부분적으로 좋은 휴리스틱을 사용하여 신속하게 실현 가능한 비최적 솔루션을 찾고, 시간이 남아 있는 동안 이를 지속적으로 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 휴리스틱 함수 설계는 적절한 영역 지식을 요구하며, 특정 검색 공간에서 잘 작동하는 휴리스틱을 구현하는 것이 중요하지만, 이는 전역적으로 허용될 수 없어서 최적성 보장에 영향을 미칠 수 있습니다.

Method: MHA*를 언제든지 사용 가능한 버전으로 확장하여 신속하게 비최적 솔루션을 찾고 시간이 다할 때까지 이를 지속적으로 개선하는 방법을 제안합니다. 이를 통해 ARA* 알고리즘의 개념을 MHA* 프레임워크에 정확히 적응시켜 성능을 향상시킵니다.

Result: A-MHA*는 3D 경로 계획 도메인 및 슬라이딩 타일 퍼즐에서 MHA* 및 다른 언제든지 알고리즘과 비교하여 성능을 보고합니다.

Conclusion: A-MHA*는 MHA*의 최적성 및 완전성 보장을 유지하며 언제든지 실행 가능한 방식으로 MHA*의 성능을 향상시키는 것이 증명되었습니다.

Abstract: Designing good heuristic functions for graph search requires adequate domain
knowledge. It is often easy to design heuristics that perform well and
correlate with the underlying true cost-to-go values in certain parts of the
search space but these may not be admissible throughout the domain thereby
affecting the optimality guarantees of the search. Bounded suboptimal search
using several such partially good but inadmissible heuristics was developed in
Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible
heuristics to potentially generate a faster suboptimal solution, the original
version does not improve the solution over time. It is a one shot algorithm
that requires careful setting of inflation factors to obtain a desired one time
solution. In this work, we tackle this issue by extending MHA* to an anytime
version that finds a feasible suboptimal solution quickly and continually
improves it until time runs out. Our work is inspired from the Anytime
Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*
concepts in the MHA* framework preserves the original suboptimal and
completeness guarantees and enhances MHA* to perform in an anytime fashion.
Furthermore, we report the performance of A-MHA* in 3-D path planning domain
and sliding tiles puzzle and compare against MHA* and other anytime algorithms.

</details>


### [95] [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](https://arxiv.org/abs/2508.21648)
*Farhad Abtahi,Mehdi Astaraki,Fernando Seoane*

Main category: cs.AI

TL;DR: MEDLEY는 다양한 AI 모델을 조화롭게 운영하며 편향을 잠재적 강점으로 인식하는 의료 인공지능 진단 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 의료 인공지능에서의 편향은 전통적으로 결함으로 간주되지만, 인간의 사고는 교육, 문화, 경험에 의해 형성된 편향을 내포하고 있어 이를 가치 있게 활용할 수 있는 가능성을 제안한다.

Method: MEDLEY는 여러 AI 모델의 다양한 출력을 보존하며 이들의 구체적인 편향을 기록하고, 환각을 임시 가설로 다루어 임상의가 검증하게 한다.

Result: 30개 이상의 대형 언어 모델을 사용하여 최소 실행 가능 제품을 개발하고, 진단 불확실성과 잠재적 편향을 투명하게 드러내는 사례를 생성하였다.

Conclusion: MEDLEY는 AI의 불완전성을 자원으로 재구성하여 신뢰할 수 있는 의료 AI 시스템 개발을 위한 새로운 규제, 윤리 및 혁신 경로를 제시한다.

Abstract: Bias in medical artificial intelligence is conventionally viewed as a defect
requiring elimination. However, human reasoning inherently incorporates biases
shaped by education, culture, and experience, suggesting their presence may be
inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble
Diagnostic system with Leveraged diversitY), a conceptual framework that
orchestrates multiple AI models while preserving their diverse outputs rather
than collapsing them into a consensus. Unlike traditional approaches that
suppress disagreement, MEDLEY documents model-specific biases as potential
strengths and treats hallucinations as provisional hypotheses for clinician
verification. A proof-of-concept demonstrator was developed using over 30 large
language models, creating a minimum viable product that preserved both
consensus and minority views in synthetic cases, making diagnostic uncertainty
and latent biases transparent for clinical oversight. While not yet a validated
clinical tool, the demonstration illustrates how structured diversity can
enhance medical reasoning under clinician supervision. By reframing AI
imperfection as a resource, MEDLEY offers a paradigm shift that opens new
regulatory, ethical, and innovation pathways for developing trustworthy medical
AI systems.

</details>


### [96] [PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation](https://arxiv.org/abs/2508.21720)
*Jiho Choi,Seojeong Park,Seongjong Song,Hyunjung Shim*

Main category: cs.AI

TL;DR: PosterForest는 자동화된 과학 포스터 생성을 위한 새로운 비학습 프레임워크로, 과학 문서의 구조와 텍스트-시각적 요소의 통합을 해결한다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들이 과학 문서의 계층 구조와 텍스트-시각적 요소의 의미적 통합을 무시하는 경향이 있어, 이를 해결할 필요성이 있다.

Method: Poster Tree라는 계층적 중간 표현을 도입하고, 내용 요약 및 레이아웃 계획을 전문으로 하는 에이전트들이 반복적으로 협력하고 피드백을 제공하는 다중 에이전트 협업 전략을 사용한다.

Result: 우리의 방법이 질적 및 양적 평가에서 기존 기준선보다 우수한 결과를 나타냈으며, 생성된 포스터는 전문가가 설계한 진실과 가장 가까운 품질을 달성하였다.

Conclusion: 포스터는 정보 보존, 구조적 명료성, 사용자 선호도에서 뛰어난 성능을 보여준다.

Abstract: We present a novel training-free framework, \textit{PosterForest}, for
automated scientific poster generation. Unlike prior approaches, which largely
neglect the hierarchical structure of scientific documents and the semantic
integration of textual and visual elements, our method addresses both
challenges directly. We introduce the \textit{Poster Tree}, a hierarchical
intermediate representation that jointly encodes document structure and
visual-textual relationships at multiple levels. Our framework employs a
multi-agent collaboration strategy, where agents specializing in content
summarization and layout planning iteratively coordinate and provide mutual
feedback. This approach enables the joint optimization of logical consistency,
content fidelity, and visual coherence. Extensive experiments on multiple
academic domains show that our method outperforms existing baselines in both
qualitative and quantitative evaluations. The resulting posters achieve quality
closest to expert-designed ground truth and deliver superior information
preservation, structural clarity, and user preference.

</details>


### [97] [Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem](https://arxiv.org/abs/2508.21730)
*Fabrizio Fagiolo,Nicolo' Vescera*

Main category: cs.AI

TL;DR: TSP를 푸는 변분 알고리즘을 제안하며, 회로 최적화 후 재사용해 구조적 연구를 줄이고 NISQ 하드웨어에 적합하다.


<details>
  <summary>Details</summary>
Motivation: TSP의 효율적인 해결을 위해 변분 알고리즘을 최적화하고 NISQ 하드웨어에 적용할 수 있는 방법을 제시하기 위함이다.

Method: 회전 판매원 문제를 위해 compact encoding과 optimize-freeze-reuse 전략을 결합한 변분 알고리즘을 개발하였다.

Result: 40개의 무작위 대칭 사례에서 4개 도시 경우의 평균 최적 여행 샘플링 확률이 100%, 5개 도시는 90%, 6개 도시는 80%이다.

Conclusion: 제안된 방법의 확장성 제한을 논의하며, 매개변수의 'Warm-start' 초기화의 영향과 더 복잡한 문제에 대한 적용 가능성을 제기한다.

Abstract: In this paper we present a variational algorithm for the Traveling Salesman
Problem (TSP) that combines (i) a compact encoding of permutations, which
reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:
where the circuit topology (``Ansatz'') is first optimized on a training
instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel
instances, limited to a rapid re-optimization of only the circuit parameters.
This pipeline eliminates costly structural research in testing, making the
procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$
cities, the resulting Ansatz achieves an average optimal trip sampling
probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for
6 city cases. With 7 cities the success rate drops markedly to an average of
$\sim 20\%$, revealing the onset of scalability limitations of the proposed
method.
  The results show robust generalization ability for moderate problem sizes and
indicate how freezing the Ansatz can dramatically reduce time-to-solution
without degrading solution quality. The paper also discusses scalability
limitations, the impact of ``warm-start'' initialization of parameters, and
prospects for extension to more complex problems, such as Vehicle Routing and
Job-Shop Scheduling.

</details>


### [98] [Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions](https://arxiv.org/abs/2508.21742)
*Timothée Loranchet,Charles K. Assaad*

Main category: cs.AI

TL;DR: 이 논문은 요약 인과 그래프를 기반으로 한 미시 수준의 인과 관계 방향성을 보장하기 위한 조건을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 분석에서 미지의 전체 인과 구조를 알기 어려운 상황에서도 인과 관계를 이해하는 것이 중요합니다.

Method: 요약 인과 그래프에 인코딩된 배경 지식을 바탕으로 미시 수준의 변인들 간의 엣지 방향성을 보장하는 조건을 제시합니다.

Result: 이론적으로 사이클이나 双 방향 엣지가 존재하더라도 미시 수준에서 엣지 방향성에 대한 보장을 제공합니다.

Conclusion: SCGs를 활용한 인과 발견을 위한 실제 지침을 제공하며, 관측된 시간 시계열 데이터로부터 인과 추론을 개선하기 위해 전문가 지식을 통합하는 것의 가치를 강조합니다.

Abstract: Understanding causal relations between temporal variables is a central
challenge in time series analysis, particularly when the full causal structure
is unknown. Even when the full causal structure cannot be fully specified,
experts often succeed in providing a high-level abstraction of the causal
graph, known as a summary causal graph, which captures the main causal
relations between different time series while abstracting away micro-level
details. In this work, we present conditions that guarantee the orientability
of micro-level edges between temporal variables given the background knowledge
encoded in a summary causal graph and assuming having access to a faithful and
causally sufficient distribution with respect to the true unknown graph. Our
results provide theoretical guarantees for edge orientation at the micro-level,
even in the presence of cycles or bidirected edges at the macro-level. These
findings offer practical guidance for leveraging SCGs to inform causal
discovery in complex temporal systems and highlight the value of incorporating
expert knowledge to improve causal inference from observational time series
data.

</details>


### [99] [Tree-Guided Diffusion Planner](https://arxiv.org/abs/2508.21800)
*Hyeonseong Jeon,Cheolhong Min,Jaesik Park*

Main category: cs.AI

TL;DR: 사전 훈련된 확산 모델을 이용한 계획이 시험 시간 안내 제어 문제를 해결하는 데 유망한 접근법으로 부상했다. 하지만 표준 기울기 안내는 비콘벡스 목표, 비미분 가능 제약 조건, 다중 보상 구조가 포함된 실제 시나리오에서는 효과가 크게 감소한다. 우리는 구조화된 경로 생성을 통해 탐색과 활용의 균형을 이루는 제로샷 시험 시간 계획 프레임워크인 나무 안내 확산 계획자(TDP)를 제안한다. TDP는 다양한 작업에서 최첨단 접근 방식에 비해 일관되게 우수한 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 시험 시간 안내 제어 문제 해결의 필요성을 해결하기 위해 사전 훈련된 확산 모델을 이용하는 접근 이점들을 극대화하고자 함.

Method: TDP는 나무 탐색 문제로 시험 시간 계획을 구성하며, 이중 샘플링 프로세스를 통해 다양한 부모 경로를 생성하여 광범위한 탐색을 촉진하고, 목표를 기반으로 조건부 디노이징을 통해 하위 경로를 정제한다.

Result: TDP는 세 가지 다양한 작업(미로 금광 채취, 로봇 팔 블록 조작, AntMaze 다중 목표 탐색)에서 평가되었으며, 모든 작업에서 최첨단 접근 방식을 지속적으로 능가함.

Conclusion: TDP는 다채로운 경로 영역 탐색과 기울기 정보를 활용하여 제로샷 계획을 효과적으로 수행할 수 있다.

Abstract: Planning with pretrained diffusion models has emerged as a promising approach
for solving test-time guided control problems. However, standard gradient
guidance typically performs optimally under convex and differentiable reward
landscapes, showing substantially reduced effectiveness in real-world scenarios
involving non-convex objectives, non-differentiable constraints, and
multi-reward structures. Furthermore, recent supervised planning approaches
require task-specific training or value estimators, which limits test-time
flexibility and zero-shot generalization. We propose a Tree-guided Diffusion
Planner (TDP), a zero-shot test-time planning framework that balances
exploration and exploitation through structured trajectory generation. We frame
test-time planning as a tree search problem using a bi-level sampling process:
(1) diverse parent trajectories are produced via training-free particle
guidance to encourage broad exploration, and (2) sub-trajectories are refined
through fast conditional denoising guided by task objectives. TDP addresses the
limitations of gradient guidance by exploring diverse trajectory regions and
harnessing gradient information across this expanded solution space using only
pretrained models and test-time reward signals. We evaluate TDP on three
diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze
multi-goal exploration. TDP consistently outperforms state-of-the-art
approaches on all tasks. The project page can be found at:
tree-diffusion-planner.github.io.

</details>
