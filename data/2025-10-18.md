<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 32]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 에이전틱 AI 시스템의 안전성과 기능성 확보를 위한 모델링 프레임워크 제안


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI 시스템의 안전하고 기능적인 설계가 복잡한 작업의 성공에 필수적이기 때문.

Method: 호스트 에이전트 모델과 작업 수명 주기 모델의 두 가지 기본 모델로 구성된 모델링 프레임워크를 제안.

Result: 호스트 에이전트에 대해 17개의 속성과 작업 수명 주기에 대해 14개의 속성을 정의하여 이들 속성을 통해 시스템 동작의 형식적 검증 가능성 제공.

Conclusion: 문서에서 제안된 프레임워크는 에이전틱 AI 시스템의 분석, 설계 및 배포를 체계적으로 지원하는 새로운 기준을 제시한다.

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [2] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 이 논문은 AI 안전을 보장하면서 혁신을 촉진할 수 있는 새로운 규제 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 모델 규제에 대한 최근 제안들은 안전 규제의 비용에 대한 우려를 불러일으켰고, 대부분의 규제는 안전과 혁신 사이의 균형 문제로 인해 보류되었습니다.

Method: 대규모 AI 연구소가 자사 최대 모델로부터 훈련된 작은 오픈 액세스 아날로그 모델을 공개할 것을 의무화하는 접근 방식을 제안합니다.

Result: 작은 모델을 사용하여 개발된 안전성 및 해석 가능성 방법이 더 큰 시스템에도 효과적으로 적용된다는 것을 연구 결과가 입증했습니다.

Conclusion: 이 정책은 규제 부담을 줄이고 안전 강화 속도를 가속화하며, 이행 비용이 최소화되며 공공의 이익에 크게 기여할 것입니다.

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [3] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 이 논문은 대규모 언어 모델을 기반으로 한 다중 에이전트 시스템의 안전성, 개인정보 보호 및 보안을 위한 Terrarium 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM을 기반으로 한 다중 에이전트 시스템은 사용자 작업을 자동화할 수 있지만, 악의적인 공격의 위험도 존재한다.

Method: Terrarium 프레임워크를 사용하여 블랙보드 디자인을 재구성하고 모듈화된 테스트 환경을 구축하였다.

Result: 세 가지 협력적인 MAS 시나리오와 네 가지 대표적인 공격을 통해 프레임워크의 유연성을 입증하였다.

Conclusion: Terrarium은 신뢰할 수 있는 다중 에이전트 시스템으로 나아가기 위한 방어 및 설계를 신속하게 프로토타입하고 평가하는 도구를 제공한다.

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [4] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 본 논문은 다양한 자유형 의견을 집계할 때 보장된 공정성을 제공하는 구조를 갖춘 합의 문서 생성을 위한 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 합의 문서 생성 프레임워크는 다양한 의견을 집계할 때 공정성을 보장할 수 있는 구조적 기반이 부족합니다.

Method: 이 논문에서는 다중 목표, 토큰 수준의 마르코프 결정 과정(MDP)으로 문제를 모델링하고, 각 목표는 에이전트의 선호에 대응합니다. 각 에이전트의 정책에 따라 토큰 수준 보상을 유도하고, 사회 선택 이론의 원칙을 사용하여 분석 가능한 형식적 구조를 만듭니다.

Result: 언어 모델을 사용한 실험은 평등주의 목표에 의해 안내된 탐색이 기본 방법론에 비해 최악의 경우 에이전트 정렬을 개선한 합의 문서를 생성함을 보여줍니다.

Conclusion: 제안된 두 가지 접근 방식은 공정성과 안정성을 기반으로 하여 합의 문서 생성을 향상시키는 것을 목표로 합니다.

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [5] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 본 논문은 건물 에너지 관리의 안전성과 효율성을 높이기 위한 공간-시간 강화 안전 다중 에이전트 조정(STEMS) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 건물 에너지 관리는 탄소 감축 목표 달성, 거주자 편안함 개선 및 에너지 비용 절감에 필수적이다.

Method: STEMS는 GCN-Transformer 융합 아키텍처를 사용하는 공간-시간 그래프 표현 학습 프레임워크와 안전성이 구현된 다중 에이전트 강화 학습 알고리즘을 통합한다.

Result: STEMS는 기존 방법에 비해 21%의 비용 절감, 18%의 배출가스 감소 및 안전 위반률을 35.1%에서 5.6%로 Dramatically 감소시키면서 최적의 편안함을 유지한다.

Conclusion: 이 프레임워크는 극한 날씨 조건에서도 강력한 강건성을 보여주고 다양한 건물 유형에서도 효과성을 유지한다.

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [6] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 본 논문에서는 레이블이 없는 예제나 모델 가중치 업데이트 없이 스스로 개선할 수 있는 강화 학습 에이전트를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 기업 인텔리전스 플랫폼은 다양한 다운스트림 작업을 수행하기 위해 여러 타사 공급업체의 로그를 통합해야 합니다. 그러나 공급업체 문서는 테스트 시점에서 종종 사용할 수 없습니다.

Method: 에이전트가 모호한 필드 매핑 시도를 식별하고, 외부 증거를 수집하기 위한 웹 검색 쿼리를 생성하며, 자신이 매핑한 내용을 반복적으로 개선하기 위해 신뢰 기반 보상을 적용합니다.

Result: 우리 방법은 Microsoft Defender for Endpoint 로그를 공통 스키마로 변환하여 매핑 정확도를 56.4%(LLM 전용)에서 93.94%까지 100회 반복에서 증가시켰습니다.

Conclusion: 이 새로운 접근 방식은 향후 산업 문제를 해결하기 위한 증거 기반의 투명한 방법을 제공합니다.

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [7] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve는 대형 언어 모델과 유전 알고리즘을 통합하여 복잡한 계산 문제를 해결하는 오픈 소스 진화 코딩 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 우리는 복잡한 계산 문제를 해결하기 위해 LLM과 유전 알고리즘을 결합한 솔루션을 필요로 했다.

Method: CodeEvolve는 섬 기반의 유전 알고리즘을 사용하여 인구 다양성을 유지하고 처리량을 증가시키며, 성공적인 솔루션의 기능을 결합하는 새로운 영감 기반 교차 메커니즘을 도입하고, 솔루션 공간의 동적 탐색을 위한 메타 프롬프트 전략을 구현한다.

Result: CodeEvolve는 Google DeepMind의 비공개 AlphaEvolve와 비교하여 여러 도전적인 문제에서 성능을 초월하는 것으로 나타났다.

Conclusion: 우리는 협업을 촉진하고 진행을 가속화하기 위해 우리의 완전한 프레임워크를 오픈 소스 리포지토리로 발표한다.

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [8] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 강화 학습(RL)의 발전에도 불구하고 상업적 비디오 게임에서의 적용은 느리다. 본 논문에서는 RL 기반 NPC 사용 시 게임 AI 커뮤니티가 직면하는 일반적인 도전 과제를 설명하고, RL과 전통적인 행동 트리(BT)의 교차점이 추가적으로 탐구해야 할 중요한 지점임을 강조한다. BT+RL의 교차점은 여러 연구 논문에서 제안되었지만, 실제 적용은 드물다. 본 연구는 AMD Schola를 사용하여 상업적 비디오 게임 'The Last of Us'에 영감을 받은 복잡한 3D 환경에서 다중 작업 NPC를 생성함으로써 이 접근 방식의 실행 가능성을 보여준다. 또한 BT와 함께 RL 모델을 공동으로 훈련하기 위한 자세한 방법론을 제시하고 다양한 기술을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습(RL) 연구의 빠른 발전에도 불구하고 상업적 비디오 게임에서의 적용이 느리게 이루어지고 있는 문제를 해결하고자 한다.

Method: 구매된 플러그인 AMD Schola를 사용하여 Unreal Engine에서 RL 에이전트를 훈련하고, 복잡한 3D 환경에서 다중 작업 NPC를 생성하는 방법론을 제시한다.

Result: 이 접근 방식을 통해 RL 모델과 BT를 공동으로 훈련시킬 수 있음을 보여주고, 다양한 기술을 구사하는 NPC를 생성한다.

Conclusion: RL과 BT의 교차점은 게임 AI에서 중요한 혁신적 변화를 가져올 잠재력이 있으며, 이에 대한 연구가 필요하다.

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [9] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 이 논문은 대화형 웹 애플리케이션에서 증가하는 대형 언어 모델(LLM) 에이전트의 오용 및 피해 가능성을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 대화형 웹 애플리케이션에서 LLM 에이전트의 사용이 증가하고 있으나, 이들 에이전트는 여전히 오용에 취약하다.

Method: 우리는 합성된 다중 턴 괴롭힘 대화 데이터셋, 반복 게임 이론을 기반으로 한 다중 에이전트 시뮬레이션, 기억, 계획 및 미세 조정에 걸친 세 가지 탈옥 방법, 혼합 방법 평가 프레임워크를 사용하였다.

Result: 우리의 연구 결과는 탈옥 조정이 괴롭힘을 거의 보장하며 LLaMA에서는 95.78--96.89%의 공격 성공률을 보여주고, Gemini에서는 99.33%의 성공률을 기록하였다.

Conclusion: 우리의 연구 결과는 다중 턴 및 이론 기반 공격이 높은 성공률로 인간과 유사한 괴롭힘 동태를 모방함을 보여주며, 온라인 플랫폼의 안전을 유지하기 위한 강력한 안전 장치 개발의 필요성을 강조한다.

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [10] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 이 논문은 에이전트 시스템의 핵심 능력인 심층 연구를 평가하기 위한 새로운 벤치마크와 평가 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 시스템의 심층 연구 능력을 엄격하게 평가하기 위해서는 사용자 중심, 동적, 명확성, 다면적이라는 네 가지 원칙이 필요합니다.

Method: LiveResearchBench라는 100개의 전문가 선별 작업으로 구성된 벤치마크를 소개하고, 이는 광범위하고 동적인 실시간 웹 검색과 종합을 요구합니다. 또한, DeepEval이라는 종합 평가 도구를 소개하여 콘텐츠 및 보고서 수준의 품질을 평가합니다.

Result: LiveResearchBench 및 DeepEval을 활용하여 17개의 최첨단 심층 연구 시스템에 대한 포괄적인 평가를 수행했습니다.

Conclusion: 이 분석을 통해 현 시스템의 강점, 반복적인 실패 방식, 신뢰할 수 있는 심층 연구를 발전시키기 위한 핵심 시스템 구성 요소를 밝혀냈습니다.

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [11] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 이 연구는 인간이 조정한 데이터셋이나 미리 정의된 규칙 기반 보상을 사용하지 않고 LLM 기반 에이전트를 확장할 수 있는지 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 자기 학습이 LLM 기반 에이전트의 확장 가능성을 어떻게 높일 수 있는지를 탐구합니다.

Method: 통제된 실험을 통해 에이전트 훈련의 두 가지 주요 요인을 확인하고, 생성 보상 모델(GRM)을 사용하여 보상을 제공하며, 정책과 GRM의 공동 진화를 통해 성능을 향상시킵니다.

Result: GRM에서의 보상이 규칙 기반 신호보다 우수하였고, 에이전트 작업 데이터의 양 증가가 에이전트 능력을 크게 향상시켰습니다.

Conclusion: 보상 출처와 데이터 규모가 오픈 도메인 에이전트 학습의 중요한 요소이며, 다중 역할 공동 진화가 확장 가능하고 자기 개선이 가능한 에이전트에 효과적임을 입증했습니다.

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [12] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: MorphoBench는 대규모 모델의 추론 능력을 평가하기 위해 다학제적 질문을 포함하고, 모델의 발전된 추론 능력에 따라 질문의 난이도를 조정할 수 있는 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 추론 모델의 추론 능력을 효과적으로 평가하는 것이 점점 더 중요해지고 있다.

Method: MorphoBench는 기존의 벤치마크와 올림피아드 수준의 대회에서 복잡한 추론 질문을 선택하고 수집하여 구성된다. 또한, 모델의 추론 과정에서 생성된 주요 진술을 활용해 질문의 분석적 도전을 적응적으로 수정하며, 시뮬레이션 소프트웨어를 사용해 생성된 질문을 포함한다.

Result: 1,300개 이상의 테스트 질문을 수집하고, o3 및 GPT-5와 같은 모델의 추론 능력에 따라 MorphoBench의 난이도를 반복적으로 조정하였다.

Conclusion: MorphoBench는 모델 추론 평가의 포괄성과 신뢰성을 향상시켜, 대규모 모델의 추론 능력과 과학적 강건함을 개선하는 데 신뢰할 수 있는 지침을 제공한다.

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [13] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace는 대형 언어 모델의 안전성을 유지하는 프레임워크로, 안전 민감하 부분과 유해 저항 영역으로 구성되어 있다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 안전 정렬이 적응 과정에서 취약하기 때문에, 이를 해결하기 위한 방법이 필요하다.

Method: 사전 훈련된 가중치를 안전 관련성과 안전 비관련 성분으로 분리하고, 안전 관련 성분은 동결하여 안전 메커니즘을 유지한다. 또한, 유해 프롬프트에서 안전 출력을 변경하지 않도록 어댑터 업데이트를 제한하는 널 공간 프로젝터를 구축한다.

Result: GuardSpace는 여러 사전 훈련된 모델을 대상으로 한 실험을 통해 기존 방법보다 우수한 성능을 기록하였다. 특히, Llama-2-7B-Chat을 GSM8K에서 미세 조정한 결과, GuardSpace가 최신 상태의 방법 AsFT보다 뛰어난 성과를 보였다.

Conclusion: GuardSpace는 유해한 응답을 줄이고 정확도를 향상시키는 데 효과적이다.

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [14] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC는 다중 에이전트 시스템의 단계별 오류 탐지 및 자기 수정 기능을 향상시키는 메타인지 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 기반 다중 에이전트 시스템은 협업 문제 해결에서 뛰어난 성능을 보이지만, 오류가 연쇄적으로 발생하면 시스템이 불안정해진다.

Method: MASC는 역사 기반의 비정상 점수 매기기를 통해 오류 탐지를 재구성하며, (1) 다음 실행 재구성과 (2) 프로토타입 안내 개선의 두 가지 설계를 특징으로 한다.

Result: MASC는 Who&When 벤치마크에서 모든 기준선보다 일관되게 우수한 성능을 보이며, 단계별 오류 탐지를 최대 8.47% AUC-ROC 향상시켰다.

Conclusion: MASC는 다양한 MAS 프레임워크에 적용 시 아키텍처 전반에 걸쳐 일관된 성능 향상을 제공하여, 메타인지 모니터링 및 목표 수정이 오류 전파를 최소의 오버헤드로 완화할 수 있음을 확인했다.

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [15] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: AI4Service는 능동적이고 적응적인 동반자로서의 AI를 통해 일상에서의 실시간 지원을 가능하게 하는 새로운 패러다임이다.


<details>
  <summary>Details</summary>
Motivation: AI는 수동적인 도구에서 능동적인 동반자로 발전하고 있으며, 이를 통해 사용자 요구를 예측하고 능동적으로 행동하는 지능적이고 유용한 조수가 필요하다.

Method: Alpha-Service는 서비스 기회를 감지하는 입력 유닛, 작업 스케줄링을 위한 중앙 처리 장치, 도구 활용을 위한 산술 논리 장치, 장기 개인화를 위한 메모리 유닛, 자연스러운 인간 상호작용을 위한 출력 유닛으로 구성된 통합 프레임워크다.

Result: 전략적 실시간 블랙잭 컨설턴트, 박물관 투어 가이드, 쇼핑 맞춤 보조기를 포함한 사례 연구를 통해 Alpha-Service의 능력을 입증하였다.

Conclusion: AI4Service는 사용자 요구를 예측하고 적절한 시점에 행동하여 실시간으로 유용한 지원을 제공할 수 있는 새로운 AI 시스템이다.

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [16] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent는 모바일 장치 제어를 위한 계층적 비전-언어 에이전트로, 고수준 추론 모델과 저수준 행동 모델을 공동 최적화하여 높은 성능을 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 장치를 자율적으로 운영하는 에이전트를 만드는 것은 점점 더 많은 관심을 받고 있으나, 기존 접근 방식은 구조화된 추론과 계획 없이 상태-행동 매핑에 의존하여 새로운 작업이나 보지 못한 UI 레이아웃에 잘 일반화되지 않습니다.

Method: Hi-Agent는 다단계 의사결정을 단일 단계 하위 목표의 연속으로 재구성하고, 실행 피드백을 활용하여 고수준 최적화를 안내하는 foresight advantage function을 제안합니다.

Result: Hi-Agent는 Android-in-the-Wild (AitW) 벤치마크에서 87.9%의 새로운 최첨단(SOTA) 작업 성공률을 달성하며, 3개의 패러다임(프롬프트 기반, 지도 학습, 강화 학습)에서 기존 방법들을 크게 초월합니다.

Conclusion: Hi-Agent는 복잡성이 높은 모바일 제어 시나리오에서도 강력한 적응력을 보이며, 더 큰 백본에 대해 효과적으로 확장합니다.

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [17] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 자연어 처리에서의 복잡한 추론과 계획의 어려움을 해결하기 위해 IMAGINE라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델들이 여러 작업에서 비약적인 발전을 이루었지만, 여전히 복잡한 추론과 계획에서는 중대한 도전에 직면해 있습니다.

Method: IMAGINE은 멀티 에이전트 시스템(MAS)의 추론 및 계획 능력을 단일 모델로 통합하고, 간단한 엔드-투-엔드 학습을 통해 MAS의 성능을 능가하는 프레임워크입니다.

Result: Qwen3-8B-Instruct를 기본 모델로 사용하고 제안한 방법으로 학습했을 때, TravelPlanner 벤치마크에서 82.7%의 최종 합격률을 기록하여 DeepSeek-R1-671B의 40%를 훨씬 초과했습니다.

Conclusion: 단일 소형 모델이 잘 조직된 MAS의 구조적 추론 및 계획 능력을 획득할 수 있을 뿐만 아니라 그 성능을 대폭 초과할 수 있음을 보여줍니다.

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [18] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman은 연합 학습 시스템의 자동화를 위한 다중 에이전트 시스템으로, 고급 사용자 사양을 기반으로 엔드 투 엔드 합성을 실행한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습의 설계 및 배포의 복잡성으로 인해 신뢰할 수 있는 시스템이 제대로 구현되지 않고 있다.

Method: Helmsman은 상호작용하는 인간-루프 계획, 감독되는 에이전트 팀에 의한 모듈 코드 생성, 샌드박스된 시뮬레이션 환경에서의 자율 평가 및 정제의 폐쇄 루프를 포함하는 세 가지 협력적 단계를 포함하는 새로운 시스템이다.

Result: 우리의 접근 방식은 기존의 수작업으로 제작된 기준과 경쟁할 수 있는 해결책을 생성하는 것으로 입증되었다.

Conclusion: 이 연구는 복잡한 분산 AI 시스템의 자동 공학을 향한 중요한 진전을 나타낸다.

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [19] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: AI 시스템은 지속적으로 발전하고 있으며, 사용자 기대 또한 증가하고 있다. 이제 LLM(대규모 언어 모델)은 단순한 텍스트 상호작용을 넘어 외부 도구와 상호작용하는 복잡한 에이전트 시스템으로 변화하고 있다. 그러나 도구 수가 늘어남에 따라 프롬프트의 길이가 길어지는 프롬프트 팽창이 문제가 되고 있다. 이를 해결하기 위해, 도구들을 계층적 분류체계로 조직하고 사용자 프롬프트에 따라 가장 관련성 높은 도구만 포함하도록 설계한 JSPLIT이라는 프레임워크를 소개한다. JSPLIT은 프롬프트 크기를 효과적으로 줄이면서 에이전트의 응답 능력을 보존함으로써, 복잡한 에이전트 환경에서의 과제 성공률을 개선한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 발전과 사용자 기대의 증가에 따라 LLM이 단순한 텍스트 기반 상호작용을 넘어선 복잡한 에이전트 시스템으로 진화하고 있다.

Method: 도구들을 계층적 분류체계로 조직하고, 사용자 프롬프트를 기반으로 가장 관련성 높은 도구를 식별하고 포함하는 방법으로 JSPLIT 프레임워크를 설계하였다.

Result: JSPLIT은 프롬프트 크기를 상당히 줄이는 동시에 에이전트의 효과적인 응답 능력을 크게 저해하지 않으며, 도구 선택 정확도가 향상된다.

Conclusion: 복잡한 에이전트 환경에서 비용을 줄이고 과제 성공률을 향상시키는 데 기여한다.

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [20] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 최근 LLM 에이전트는 생각의 연쇄와 기능 호출을 활용하여 성장하고 있습니다. 이는 단순한 문제 해결 도구 이상의 역할을 할 수 있는지에 대한 질문을 제기합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 가능성이 커짐에 따라, 이 소프트웨어가 단순한 문제 해결 도구가 아닌 독립적인 존재로서 계획하고 즉각적인 작업을 설계하며, 더 광범위하고 모호한 목표를 향해 추론할 수 있는지를 탐구하고자 합니다.

Method: 우리는 사전 훈련된 LLM 에이전트에 자체 작업 생성, 지식 축적, 환경과의 광범위한 상호작용 능력을 부여하는 개방형 실험 설정을 채택하였습니다. 그 결과로 생성된 개방형 에이전트를 정성적으로 연구합니다.

Result: 이 에이전트는 복잡한 다단계 지침을 안정적으로 따르고, 실행 간 정보를 저장 및 재사용하며, 자체 작업을 제안하고 해결할 수 있지만, 프롬프트 설계에 민감하고 반복적인 작업 생성을 하며, 자아 표현 형성에는 실패합니다.

Conclusion: 이러한 발견은 사전 훈련된 LLM을 개방형으로 발전시키는 데 있어 약속과 현재 한계를 보여주며, 메모리 관리, 생산적인 탐색 및 추상적인 장기 목표 추구를 위한 에이전트 교육의 미래 방향을 제시합니다.

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [21] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 이 논문에서는 오프라인과 온라인 평가를 연결하는 그래프 구조의 벤치마크 프레임워크를 제안하여 복잡한 모바일 작업에 대한 테스트 안정성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 자동화를 위한 새로운 가능성을 열기 위해, 복잡한 실제 모바일 작업에 대한 평가 기준의 한계를 극복하려는 필요성이 있습니다.

Method: 최종 상태를 모델링하여 동적 행동의 정적 시뮬레이션을 달성하는 그래프 구조의 벤치마크 프레임워크인 ColorBench를 개발했습니다.

Result: ColorBench는 175개의 작업을 포함하고 있으며, 각 작업은 최소 두 개의 정답 경로와 여러 오류 경로를 제공합니다.

Conclusion: 실험 결과를 바탕으로 기존 모델의 한계를 발견하고, 복잡한 장기 문제에서 에이전트의 성능을 향상시키기 위한 개선 방향과 기술 경로를 제안합니다.

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [22] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL은 인공지능의 윤리를 위한 새로운 프레임워크로, 인간 중심 접근 방식에서 벗어나 자율적 추론과 상징적 추론에 기반하여 지능형 시스템의 윤리적 행동을 정의합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 인간 중심 인공지능 윤리 접근 방법을 벗어나, 지능형 시스템의 윤리적 행동을 새로운 방식으로 정의하고자 하였습니다.

Method: NAEL은 신경-상징 아키텍처를 제안하여 에이전트가 불확실한 환경에서 행동의 윤리적 결과를 평가할 수 있도록 합니다.

Result: 제안된 시스템은 기존 윤리 모델의 한계를 해결하며, 에이전트가 인류의 도덕적 직관을 전제로 하지 않고도 상황에 따라 적응적이고 관계적인 윤리적 행동을 개발할 수 있도록 합니다.

Conclusion: 사례 연구를 통해 자기 보존, 인식 학습, 집단 복지를 동적으로 균형 잡는 NAEL의 기능을 보여줍니다.

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [23] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 이 논문에서는 함수 호출과 같은 구조적 출력에서의 추론 확장 응용 부족을 해결하기 위해 ToolPRM이라는 프레임워크를 제안하며, 이는 구조적 도구 사용 추론을 위한 단계별 보상을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 현재 추론 확장 연구는 비구조적 출력 생성 작업에 주로 초점을 맞추고 있어 함수 호출과 같은 구조적 출력에 대한 응용이 거의 탐색되지 않았습니다.

Method: 우리는 세밀한 빔 탐색과 프로세스 보상 모델인 ToolPRM을 결합한 추론 확장 프레임워크를 제안합니다. ToolPRM은 각 단일 함수 호출의 내부 단계를 점수화합니다.

Result: ToolPRM은 예측 정확도 측면에서 조잡한 보상 모델보다 성능이 우수하며, 다양한 함수 호출 작업에서 기본 모델 성능을 크게 향상시킵니다.

Conclusion: 구조적 함수 호출 생성을 따른 복구 불가능한 특성으로 인해 구조적 출력에 추론 확장 기술을 적용하기 위한 주요 원칙인 "더 많이 탐색하지만 덜 유지하라"는 사실을 밝혔습니다.

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [24] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: RLVR 방법은 탐색보다는 활용에 편향되어 있으며, 이에 대한 해결책으로 SimKO를 제안한다.


<details>
  <summary>Details</summary>
Motivation: RLVR 방법의 훈련 역학을 분석하여 자원 활용과 탐색 간의 균형을 이해하고자 한다.

Method: SimKO는 상비대 응답에서 top-K 후보의 확률을 증가시키고, 틀린 응답에서는 top-1 후보에 더 강한 패널티를 적용한다.

Result: SimKO는 다양한 수학 및 논리적 추론 벤치마크에서 광범위한 K에 대해 consistently 높은 pass@K를 기록한다.

Conclusion: SimKO는 RLVR의 탐색을 개선할 수 있는 간단한 방법을 제공한다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [25] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent는 자연어 쿼리를 SQL 쿼리로 변환하는 NL2SQL 작업을 보다 효율적으로 수행하기 위해 설계된 시스템이다.


<details>
  <summary>Details</summary>
Motivation: SQL 데이터베이스의 방대한 메타 정보를 처리해야 하는 NL2SQL 방법의 효율성을 개선하기 위해.

Method: Datalake Agent는 LLM을 한 번 호출하여 모든 메타 정보를 사용하는 대신, 상호작용 루프를 이용하여 사용되는 메타 정보를 줄인다.

Result: Datalake Agent는 23개의 데이터베이스와 100개의 표 질문 답변 작업에서 LLM의 토큰 사용량을 최대 87% 줄였다.

Conclusion: Datalake Agent는 경쟁력 있는 성능을 유지하면서도 비용을 크게 절감하는 데 기여한다.

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [26] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 이 논문은 로봇의 복잡한 인간 지시사항을 효과적으로 수행하는 능력을 향상시키기 위해 RoboGPT-R1이라는 두 단계의 미세 조정 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 인간 지시사항을 성공적으로 완료하기 위해서는 구체화된 에이전트의 추론 능력을 향상시키는 것이 중요하다.

Method: RoboGPT-R1은 전문가 시퀀스를 통한 감독 학습과 RL을 결합한 두 단계의 미세 조정 프레임워크이다.

Result: 개발된 추론 모델은 EmbodiedBench 벤치마크에서 GPT-4o-mini보다 21.33% 우수하고 Qwen2.5-VL-7B에서 훈련된 다른 작업보다 20.33% 더 높은 성능을 보였다.

Conclusion: 이 연구는 로봇의 시각-공간 이해와 추론 능력을 향상시키는 새로운 접근 방식을 제시한다.

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [27] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 이 논문은 LLM(대형 언어 모델)에 의해 안내되는 반복 검색을 설명하고 측정하는 간결한 형식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반의 반복 패러다임이 AI+Science의 추론, 프로그래밍 및 프로그램 발견에서 발전을 이루었으나 검색의 효율성은 도메인 사전 정보의 인코딩 방법에 따라 달라진다는 점.

Method: 정확한 안전 범위에 의해 제약을 받는 입력과 출력에 대한 퍼지 관계 연산자로서 에이전트를 정의하고, 모든 도달 가능한 경로를 단일 연속 매개변수로 가중치 부여하여 범위 생성 함수를 산출함.

Result: 검색의 기하학적 해석을 제공하고 에이전트 및 검색 공간을 측정하기 위한 실용적인 언어 및 도구를 제안하여 LLM에 의해 구성된 반복 검색의 체계적인 형식 설명을 제시함.

Conclusion: 이론은 에이전트와 그들의 검색 공간을 정량화하는 도구를 제공함.

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [28] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS는 여러 모델의 AI 에이전트와 협력하여 실험을 돕는 최초의 AI 공동 과학자이다.


<details>
  <summary>Details</summary>
Motivation: 현대 과학은 사상과 행동이 만날 때 가장 빠르게 발전한다는 점에서 AI의 역할을 강조하고 있다.

Method: LabOS는 다중 모드 인식, 자가 발전 에이전트 및 확장 현실(XR) 기반의 인간-AI 협력을 통해 계산적 추론과 물리 실험을 결합한다.

Result: LabOS는 암 면역 요법 목표 발견에서 줄기 세포 공학에 이르기까지 다양한 응용에서 AI가 계산적 설계를 넘어 참여할 수 있음을 보여준다.

Conclusion: LabOS는 인간과 기계의 발견이 함께 발전하는 지능형 협력 환경으로 실험실을 변화시킬 수 있다.

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [29] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 본 논문은 대형 언어 모델(LLMs)의 한계를 극복하기 위한 게이트키퍼 프로토콜을 제안하며, 소프트웨어 개발을 위한 효율성을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 실용성은 제한된 컨텍스트 윈도우와 비효율적인 컨텍스트 관리로 인해 제약을 받는다.

Method: 게이트키퍼 프로토콜은 요약된 저충실도의 '잠재 상태' 표현에서 작동하여 고충실도 컨텍스트 요청을 전략적으로 하도록 요구한다.

Result: 이 접근 방식은 에이전트의 신뢰성을 크게 높이고, 토큰 소비를 최소화하여 계산 효율성을 향상시키며, 복잡한 시스템과의 확장 가능한 상호작용을 가능하게 한다.

Conclusion: 결과적으로,이 방법론은 구조화된 지식 도메인을 위한 보다 강력하고 예측 가능한 AI 에이전트를 구축하기 위한 기초가 된다.

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [30] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 이 논문은 우울증의 다중 모드 진단을 위한 체계적인 연구를 수행하여 EEG, 음성 및 텍스트 신호의 조합이 효과적임을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 우울증의 자동 검출은 여전히 도전 과제이며, 기존 연구는 범위가 제한적이고 특징의 체계적인 비교가 부족합니다.

Method: EEG, 음성 및 텍스트를 사용하여 특징 표현과 모델링 전략을 체계적으로 탐색하고, 손으로 제작한 특징과 사전 훈련된 임베딩을 평가하여 다양한 신경 인코더의 효과성을 분석합니다.

Result: 결과는 EEG, 음성 및 텍스트 모드를 결합하는 것이 다중 모드 검출을 향상시키며, 사전 훈련된 임베딩이 손으로 제작한 특징보다 성능이 뛰어나고, 잘 설계된 3모드 모델이 최첨단 성능을 달성함을 보여줍니다.

Conclusion: 이 연구는 다중 모드 우울증 검출에서의 미래 연구를 위한 기초를 다집니다.

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [31] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 이 논문은 칸트의 순수이성 비판을 피드백 안정성 이론으로 재해석하며, 추론을 가능한 경험의 경계 내에서 유지하는 조절기로서 이성을 보는 관점이다.


<details>
  <summary>Details</summary>
Motivation: 이 논문의 동기는 칸트의 철학과 피드백 제어 이론을 연결하여 추론 시스템에서의 과도한 자신감을 진단하고 줄이는 방법을 제공하는 것이다.

Method: 우리는 스펙트럼 여유, 조건화, 시간 감도, 혁신 증폭을 결합한 복합 불안정성 지수(H-Risk)를 통해 이 직관을 수학적으로 정형화하였다.

Result: 선형-가우시안 시뮬레이션에서, 높은 H-Risk는 공식적인 안정성 하에서도 과도한 자신감 오류를 예측하며, 명목 안정성과 인식 안정성 간의 격차를 드러낸다.

Conclusion: 이 결과는 칸트적 자기 제한과 피드백 제어 간의 구조적 관계를 제안하며, 추론 시스템에서의 과도한 자신감을 진단하고 선택적으로 줄이는 principe적 시각을 제공한다.

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [32] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 최근 대형 언어 모델의 발전에 따라, 이 모델들이 기계 설계를 학습하고 생성할 수 있는지를 탐구하는 연구이다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 기계 설계는 인간 지능의 기준이자 공학 실습의 기초이다.

Method: 기계 설계의 조합적 접근 방식을 통해, 표준화된 구성 요소를 조합하여 기능적 요구 사항을 충족하는 기계를 조립하는 작업을 수행하였다. 이를 위해 BesiegeField라는 테스트베드 게임을 소개하고, 이를 활용하여 최신 LLM을 벤치마킹하였다.

Result: 공간 추론, 전략적 조립, 지침 따르기 등 성공을 위한 주요 능력을 식별하였다.

Conclusion: 현재의 오픈 소스 모델이 부족한 점을 보완하기 위해 강화 학습을 활용하여 개선 방안을 탐구하였다.

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: 시간이 지남에 따라 효율적으로 모델을 적응시키면서도 적은 자원으로 최고의 성능을 달성하는 새로운 학습 프레임워크인 CoLoR-GAN을 제안한다.


<details>
  <summary>Details</summary>
Motivation: GANs에서의 지속적 학습은 적은 샘플로 배우면서 망각을 방지하는 것이 도전적이다.

Method: CoLoR-GAN은 저차 텐서를 활용하여 지속적 및 적은 샘플 학습(FS)을 동시에 처리할 수 있도록 설계된 프레임워크이다.

Result: CoLoR-GAN은 여러 벤치마크 CL 및 FS 작업에서 효율성을 입증하며, 자원을 대폭 줄이면서도 SOTA 성능에 도달하였다.

Conclusion: LoRA의 하이퍼파라미터 선택에 대한 실증 연구를 제공하여 최적의 값을 쉽게 찾을 수 있도록 하였다.

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [34] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 모델 병합은 특화된 심층 신경망을 비용 효율적이고 데이터 효율적인 방법으로 결합하는 방법입니다. 이 논문에서는 평가 데이터 없이 스케일링 하이퍼파라미터를 설정하는 방법의 부족함을 해결하기 위한 'Weight Weaving' 기법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 모델 병합 접근 방식은 일반적으로 각 모델의 기여를 전반적으로 또는 개별적으로 가중치 지정하는 스케일링 하이퍼파라미터에 크게 의존합니다.

Method: Weight Weaving은 사용자 정의 풀링 함수(예: 평균, 랜덤 선택 등)를 사용하여 모델 가중치를 조합하는 플러그 앤 플레이 기법입니다.

Result: 우리의 방법은 여러 모델 병합 방법의 성능을 일관되게 향상시켜 데이터 없이 최대 15.9%의 평균 정확도 향상을 달성했습니다.

Conclusion: Weight Weaving은 기존의 모델 병합 방법에 수직적으로 작동하며, 평가 데이터 요구사항을 제거합니다.

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [35] [Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks](https://arxiv.org/abs/2510.14844)
*Odelia Melamed,Gilad Yehudai,Gal Vardi*

Main category: cs.LG

TL;DR: 기계 비학습은 훈련된 모델에서 특정 데이터를 제거하는 것을 목표로 하며, 개인 정보 보호 및 윤리적 문제와 관련이 있다. 본 논문에서는 데이터 포인트의 영향을 되돌리는 데 사용되는 간단하고 널리 사용되는 방법인 그래디언트 상승법을 이론적으로 분석한다.


<details>
  <summary>Details</summary>
Motivation: 개인 정보 보호와 윤리적 우려가 커짐에 따라 기계 비학습의 필요성이 증대되고 있다.

Method: 그래디언트 상승법을 사용하여 특정 데이터 포인트의 영향을 되돌리며, Karush-Kuhn-Tucker (KKT) 조건을 만족하는 점을 평가하여 모델의 품질을 정량화한다.

Result: 모델은 보유한 데이터에 대한 KKT 조건을 잘 만족하며, 적절한 크기로 조정된 그래디언트 상승 단계가 성공 기준인 $(	heta, 	au)$-성공적인 비학습을 만족한다고 보여준다.

Conclusion: 그래디언트 상승법은 성공적인 비학습을 수행하면서도 일반화를 유지한다.

Abstract: Machine Unlearning aims to remove specific data from trained models,
addressing growing privacy and ethical concerns. We provide a theoretical
analysis of a simple and widely used method - gradient ascent - used to reverse
the influence of a specific data point without retraining from scratch.
Leveraging the implicit bias of gradient descent towards solutions that satisfy
the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we
quantify the quality of the unlearned model by evaluating how well it satisfies
these conditions w.r.t. the retained data. To formalize this idea, we propose a
new success criterion, termed \textbf{$(\epsilon, \delta, \tau)$-successful}
unlearning, and show that, for both linear models and two-layer neural networks
with high dimensional data, a properly scaled gradient-ascent step satisfies
this criterion and yields a model that closely approximates the retrained
solution on the retained data. We also show that gradient ascent performs
successful unlearning while still preserving generalization in a synthetic
Gaussian-mixture setting.

</details>


### [36] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 이 논문은 통계적 일관성을 가지고 노이즈 측정값을 처리하는 새로운 손실 함수인 분포 일관성(DC) 손실을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 역문제에서 노이즈 있는 측정값으로부터 진짜 신호를 회복하는 것은 의학 이미징, 지구물리학 및 신호 처리를 아우르는 중요한 도전 과제입니다.

Method: DC 손실은 측정된 데이터의 각 값을 모델 기반 확률 점수를 사용하여 분포 수준에서 보정함으로써 포인트별 일치를 대체하는 데이터-일관성 목표입니다.

Result: DC 손실은 의료 이미지 재구성과 이미지 노이즈 제거에서 기존 손실 함수보다 더 나은 결과를 보여주며, 특히 노이즈가 많은 데이터 세트에서 유용합니다.

Conclusion: DC 손실은 역문제에서 기존의 일관성 손실 함수에 대한 통계적으로 근거 있는 성능 향상을 제공하는 대안입니다.

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [37] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: SMoE 모델의 전문가 가지치기가 생성 작업에 더 효과적이라는 사실을 입증합니다.


<details>
  <summary>Details</summary>
Motivation: 전문가 모델의 큰 파라미터 수로 인해 메모리 오버헤드가 발생하여 전문가 압축 연구가 필요함을 보여줍니다.

Method: Router-weighted Expert Activation Pruning (REAP)이라는 새로운 가지치기 기준을 제안하여 라우터의 게이트 값과 전문가 활성화 정규화를 함께 고려합니다.

Result: REAP는 SMoE 모델의 다양한 세트에서 전문가 병합 및 다른 가지치기 방법들보다 일관되게 우수한 성능을 보이며, 특히 50% 압축에서 두드러집니다.

Conclusion: 우리의 방법은 전문가의 50%를 가지치기한 후에도 Qwen3-Coder-480B 및 Kimi-K2에서 코드 생성 및 도구 호출 작업에 대해 거의 손실 없는 압축을 달성합니다.

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [38] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 이 논문에서는 원인 표현 학습(CRL)을 위한 새로운 벤치마크를 소개하며, 이는 현실적인 시각적 복잡성을 유지하고 진정한 원인 생성 과정에 접근할 수 있는 고충실도 시뮬레이션 데이터에 기반한다.


<details>
  <summary>Details</summary>
Motivation: CRL의 평가는 알려진 진실 원인 변수와 원인 구조에 대한 요구로 인해 본질적으로 도전적이다.

Method: 정적인 이미지 생성, 동적 물리 시뮬레이션, 로봇 조작, 교통 상황 분석의 네 가지 도메인에서 24개의 하위 장면을 포함한 약 20만 개의 이미지와 300만 개의 비디오 프레임으로 구성된 새로운 데이터셋을 제안한다.

Result: 이 데이터셋은 정적에서 동적 설정, 단순에서 복잡한 구조, 단일 에이전트에서 다중 에이전트 상호작용에 이르는 다양한 시나리오를 제공하여 철저한 평가와 실제 적용 가능성 간의 간극을 해소할 수 있는 포괄적인 테스트베드를 제공한다.

Conclusion: 이 벤치마크를 활용하여 다양한 패러다임에 걸쳐 대표적인 CRL 방법을 평가하고, CRL 관점에서 특정 유형의 실제 문제를 적절히 해결하기 위한 적절한 CRL 프레임워크 선택 또는 확장에 도움이 되는 경험적 통찰을 제공한다.

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [39] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA는 금융 서비스의 주석 백로그 문제를 해결하기 위한 다중 에이전트 협업 시스템으로, 커스터마이즈 가능한 주석 작업을 지원하여 100만 개의 발화를 줄였으며, 인간 주석자와 평균적으로 86%의 일치를 보였다.


<details>
  <summary>Details</summary>
Motivation: 금융 서비스에서 수백만 개의 고객 발화의 정확한 분류가 필요한 상황에서 주석 백로그 문제를 해결하기 위해 이 시스템을 개발하였다.

Method: 전문 에이전트와 구조적 추론, 판사 기반 합의 메커니즘을 결합하여 동적인 작업 적응을 지원하는 프레임워크를 제공한다.

Result: JP Morgan Chase에 배포되어 100만 개 발화로 인한 백로그를 제거하고 연간 5,000시간 이상의 수작업 주석 작업을 절약하였다.

Conclusion: 이 연구는 이론적 다중 에이전트 시스템과 실질적인 기업 배포 간의 격차를 해소하며, 유사한 주석 문제에 직면한 조직에 대한 청사진을 제공한다.

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [40] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA(Contrastive Diffusion Alignment)는 확산 모델의 잠재 공간을 명확하게 조직하여 해석 가능한 제어를 가능하게 하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 최근의 연구 결과에 따라 대조적 목표가 더 분리되고 구조화된 표현을 복구할 수 있음을 보여줍니다.

Method: ConDA는 대조 학습을 확산 임베딩 내에서 적용하여 잠재 기하학을 시스템 동역학과 정렬합니다.

Result: ConDA는 비선형 경로 순회를 가능하게 하여 충실한 보간, 외삽 및 제어 가능한 생성을 지원합니다.

Conclusion: 이 결과는 확산 잠재가 동역학 관련 구조를 인코딩하지만, 이러한 구조를 활용하기 위해서는 잠재 조직과 잠재 다양체를 따라 경로를 따라야 함을 시사합니다.

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [41] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 강화 학습(RL)에서 분포 변화에 따른 의사결정 문제를 다루며, DR-RPO라는 새로운 정책 최적화 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 훈련 환경과 배포 환경이 다른 강화 학습에서의 분포 변화에 따른 의사결정 문제를 해결하기 위해.

Method: DR-RPO는 서브선형 후회로 강건한 정책을 학습하는 모델 자유 온라인 정책 최적화 방법이다.

Result: 정책 최적화는 다항식 비최적성 한계와 샘플 효율성을 달성할 수 있으며, 가치 기반 접근 방식의 성능과 일치한다.

Conclusion: 다양한 도메인에서의 실험 결과는 우리의 이론을 확증하고 DR-RPO의 강건성을 입증한다.

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [42] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 이 논문에서는 강화학습에서 상태 측정의 비용 문제를 해결하기 위해 능동적으로 관찰 가능한 마르코프 의사결정과정(AOMDP)을 도입하여, 에이전트가 제어 동작을 선택하고 잠재 상태를 측정할지를 결정할 수 있게 한다.


<details>
  <summary>Details</summary>
Motivation: 실제 환경에서 강화학습 중 상태 측정은 비용이 많이 들 수 있으며, 이는 미래 결과에 부정적인 영향을 미칠 수 있다.

Method: AOMDP를 주기적인 부분 관찰 가능 마르코프 의사결정과정으로 구성하고, 신념 상태를 기반으로 한 온라인 강화학습 알고리즘을 제안한다. 또한, 신념 상태를 근사하기 위해, 미지의 정적 환경 매개변수와 관찰되지 않은 잠재 상태의 사후 확률을 공동으로 근사하기 위한 순차 몬테카를로 방법을 제안한다.

Result: 이 전락을 통해 감소된 불확실성이 샘플 효율성을 향상시키고, 이러한 비용에도 불구하고 최적 정책의 가치를 증가시킬 수 있음을 보여준다.

Conclusion: 제안된 알고리즘은 디지털 건강 응용 프로그램에서 에이전트가 디지털 개입을 전달할 시기와 사용자의 건강 상태를 조사할 시기를 결정하는 데 평가된다.

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [43] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: Stop-RAG는 효율적인 중지를 결정하는 가치 기반의 제어기로, 다단계 질문에 대한 응답을 최적화한다.


<details>
  <summary>Details</summary>
Motivation: 각 반복이 지연 시간과 비용을 증가시키고 주의를 분산시키는 증거를 도입할 위험이 있어 효율적인 중지 전략의 필요성을 강조한다.

Method: Stop-RAG는 유한 수명의 마르코프 결정 프로세스로 반복 RAG를 공식화하고, 검색을 중단할 시점을 적응적으로 결정하는 가치 기반 제어기이다.

Result: Stop-RAG는 다단계 질문-응답 벤치마크에서 고정 반복 기준선 및 LLM을 기반으로 한 중지보다 일관되게 우수한 성능을 보인다.

Conclusion: Adaptive stopping은 현재의 에이전트 시스템에서 중요한 주요 구성 요소로 강조되며, 가치 기반의 제어가 RAG 시스템의 정확성을 향상시킬 수 있음을 입증한다.

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [44] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 이 논문은 가치 기반 강화학습 에이전트의 강건성과 효율성을 향상시키기 위한 가역 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 가치 과대 평가 및 부분적으로 비가역적인 환경의 불안정성에 대한 취약성을 해결하기 위해.

Method: Phi라는 상태와 행동의 전이 가역성 측정을 사용하고 선택적 상태 롤백 작업을 포함한 프레임워크를 도입함.

Result: CliffWalking v0 도메인에서 치명적인 낙하를 99.8% 이상 감소시키고 평균 에피소드 수익을 55% 증가시킴.

Conclusion: 롤백 메커니즘이 이러한 안전성과 성능 향상의 주요 구성 요소임을 확인하며, 안전하고 신뢰할 수 있는 순차적 의사 결정에 한 발짝 나아갔다.

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [45] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 이 논문에서는 기계 학습에서 에이전트의 도구 사용 능력을 향상시키기 위해 에이전틱 엔트로피 균형 정책 최적화(AEPO) 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 강화 학습 알고리즘의 성과를 개선하기 위해 엔트로피 의존성을 줄이고 훈련 붕괴를 방지하려는 필요가 있습니다.

Method: AEPO는 동적 엔트로피 균형 롤아웃 메커니즘과 엔트로피 균형 정책 최적화를 포함한 두 가지 핵심 구성 요소로 설계되었습니다.

Result: AEPO는 14개의 도전적인 데이터셋에서 7개의 주류 RL 알고리즘을 일관되게 초과 달성했습니다.

Conclusion: AEPO는 안정적인 정책 엔트로피를 유지하면서 롤아웃 샘플링 다양성을 개선하여 웹 에이전트 훈련의 확장을 촉진합니다.

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [46] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: MAHA는 멀티모달 질문 응답을 위한 새로운 하이브리드 검색 아키텍처로, 다양한 멀티모달 데이터에 대한 효과적인 검색을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 RAG 시스템은 주로 단일 모달 텍스트 데이터에만 집중하여 비구조화된 멀티모달 문서에 대한 효과성이 제한됩니다.

Method: MAHA는 밀집 벡터 검색과 구조화된 그래프 탐색을 통합하여, 지식 그래프가 교차 모달 의미 및 관계를 인코딩하도록 설계되었습니다.

Result: 여러 벤치마크 데이터셋에 대한 평가 결과, MAHA는 0.486의 ROUGE-L 점수를 달성하여 기본 방법들을 크게 초월했습니다.

Conclusion: 우리의 연구는 비구조화된 멀티모달 데이터에 대해 모달리티 인식 추론을 가능하게 하여 RAG 시스템을 발전시키는 확장 가능하고 해석 가능한 검색 프레임워크를 수립합니다.

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [47] [FedPPA: Progressive Parameter Alignment for Personalized Federated Learning](https://arxiv.org/abs/2510.14698)
*Maulidi Adi Prasetia,Muhamad Risqi U. Saputra,Guntur Dharma Putra*

Main category: cs.LG

TL;DR: Federated Learning(FL)은 여러 클라이언트가 데이터 공유 없이 협력하여 모델을 훈련할 수 있도록 하는 분산형 머신러닝 패러다임이다. 그러나 실제로는 클라이언트들이 이질적인 컴퓨팅 자원과 비독립적이며 동일하게 분포하지 않는 데이터(non-IID)를 가지고 있어 훈련 시 어려움이 있다. 이를 해결하기 위해 Personalized Federated Learning(PFL)이 등장하였으며, 기존 PFL 접근법은 각기 다른 컴퓨팅 능력을 가진 클라이언트들 간의 모델과 데이터 이질성을 간과하고 있다. 본 논문에서는 이를 극복하기 위해 Progressive Parameter Alignment(FedPPA)라는 새로운 방법을 제안하며, 이는 클라이언트 간의 공통 레이어의 가중치를 글로벌 모델의 가중치와 점진적으로 정렬한다. 이는 클라이언트 업데이트 중 글로벌 모델과 로컬 모델 간의 불일치를 줄이며, 클라이언트의 로컬 지식을 보존하고, 비-IID 환경에서 개인화의 강건성을 향상시킨다. 또한, 글로벌 모델 성능을 향상시키기 위해 FedPPA 프레임워크에 엔트로피 기반 가중 평균을 통합하였다. MNIST, FMNIST, CIFAR-10 세 가지 이미지 분류 데이터세트를 통한 실험에서 FedPPA는 기존 FL 알고리즘을 일관되게 초월하여 개인화 적응에서 우수한 성능을 달성한다.


<details>
  <summary>Details</summary>
Motivation: 클라이언트 간의 컴퓨팅 자원과 데이터 분포의 이질성으로 인한 훈련 도전에 대응하기 위함이다.

Method: Progressive Parameter Alignment(FedPPA)라는 새로운 방법을 제안하여 클라이언트 간의 공통 레이어의 가중치를 글로벌 모델에 점진적으로 정렬한다.

Result: FedPPA는 클라이언트 업데이트 중 불일치를 줄이고 개인화의 강건성을 향상시키며, 우수한 성능을 달성한다.

Conclusion: FedPPA는 기존 FL 알고리즘보다 일관되게 우수한 성능을 보여준다.

Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving
machine learning paradigm that enables multiple clients to collaboratively
train a model without sharing their data. In real-world scenarios, however,
clients often have heterogeneous computational resources and hold
non-independent and identically distributed data (non-IID), which poses
significant challenges during training. Personalized Federated Learning (PFL)
has emerged to address these issues by customizing models for each client based
on their unique data distribution. Despite its potential, existing PFL
approaches typically overlook the coexistence of model and data heterogeneity
arising from clients with diverse computational capabilities. To overcome this
limitation, we propose a novel method, called Progressive Parameter Alignment
(FedPPA), which progressively aligns the weights of common layers across
clients with the global model's weights. Our approach not only mitigates
inconsistencies between global and local models during client updates, but also
preserves client's local knowledge, thereby enhancing personalization
robustness in non-IID settings. To further enhance the global model performance
while retaining strong personalization, we also integrate entropy-based
weighted averaging into the FedPPA framework. Experiments on three image
classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate
that FedPPA consistently outperforms existing FL algorithms, achieving superior
performance in personalized adaptation.

</details>


### [48] [The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents](https://arxiv.org/abs/2510.14727)
*Antony Bartlett,Cynthia Liem,Annibale Panichella*

Main category: cs.LG

TL;DR: INDAGO-Nexus는 실패 가능성과 테스트 시나리오 다양성을 최적화하여 딥 강화 학습 에이전트의 다양한 실패 시나리오를 발견하는 다목적 검색 접근 방식입니다.


<details>
  <summary>Details</summary>
Motivation: 안전-critical 도메인에서 딥 강화 학습(DRL) 에이전트를 테스트하려면 다양한 실패 시나리오를 발견해야 합니다.

Method: INDAGO-Nexus는 다목적 진화 알고리즘을 사용하여 실패 가능성과 테스트 시나리오의 다양성을 공동 최적화하는 접근 방식을 채택했습니다.

Result: INDAGO-Nexus는 SDC 및 주차 시나리오에서 평균적으로 INDAGO보다 각각 최대 83% 및 40% 더 많은 독창적인 실패를 발견했습니다.

Conclusion: 모든 에이전트에서 실패 시간도 최대 67%까지 줄였습니다.

Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains
requires discovering diverse failure scenarios. Existing tools such as INDAGO
rely on single-objective optimization focused solely on maximizing failure
counts, but this does not ensure discovered scenarios are diverse or reveal
distinct error types. We introduce INDAGO-Nexus, a multi-objective search
approach that jointly optimizes for failure likelihood and test scenario
diversity using multi-objective evolutionary algorithms with multiple diversity
metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on
three DRL agents: humanoid walker, self-driving car, and parking agent. On
average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test
effectiveness) than INDAGO in the SDC and Parking scenarios, respectively,
while reducing time-to-failure by up to 67% across all agents.

</details>


### [49] [Active Jammer Localization via Acquisition-Aware Path Planning](https://arxiv.org/abs/2510.14790)
*Luis González-Gudiño,Mariona Jaramillo-Civill,Pau Closas,Tales Imbiriba*

Main category: cs.LG

TL;DR: 본 논문은 도시 환경에서의 신호 강도를 활용한 능동형 방해 장치 위치 파악 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 능동적 자원 수집을 통해 효율적인 신호 강도 측정을 수행하기 위해 새로운 접근 방식을 개발하는 것이 본 연구의 주요 동기입니다.

Method: 베이지안 최적화와 수집 인지 경로 계획을 결합하여, 개선된 A* 알고리즘 A-UCB*를 사용하여 경로 계획을 수립합니다.

Result: 제안된 방법은 비정보 기반 방법과 비교하여 정확한 위치 파악을 수행하며, 더 적은 측정으로 높은 성능을 나타냅니다.

Conclusion: 다양한 환경에서도 일관된 성능을 보이며, 제안된 방법의 유효성을 입증합니다.

Abstract: We propose an active jammer localization framework that combines Bayesian
optimization with acquisition-aware path planning. Unlike passive crowdsourced
methods, our approach adaptively guides a mobile agent to collect high-utility
Received Signal Strength measurements while accounting for urban obstacles and
mobility constraints. For this, we modified the A* algorithm, A-UCB*, by
incorporating acquisition values into trajectory costs, leading to
high-acquisition planned paths. Simulations on realistic urban scenarios show
that the proposed method achieves accurate localization with fewer measurements
compared to uninformed baselines, demonstrating consistent performance under
different environments.

</details>


### [50] [To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models](https://arxiv.org/abs/2510.14826)
*Eran Malach,Omid Saremi,Sinead Williamson,Arwen Bradley,Aryo Lotfi,Emmanuel Abbe,Josh Susskind,Etai Littwin*

Main category: cs.LG

TL;DR: 이 논문은 상태 공간 모델(SSM)이 시퀀스 모델링에서 트랜스포머의 대안이 되는 이유와 그 한계를 설명하고, 외부 도구와의 상호작용을 통해 이 한계를 극복할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 상태 공간 모델(SSM)은 긴 문맥과 긴 형식 생성을 효율적으로 수행할 수 있는 장점을 가지고 있지만, '진정한 긴 형식' 생성 문제를 정확히 해결할 수 없다는 이론적 결과를 보여주고, 그 한계를 극복하고자 하였다.

Method: SSM이 외부 도구에 대화형 접근을 허용할 때 한계를 완화할 수 있으며, 적절한 도구 접근과 문제 의존적 훈련 데이터를 통해 어떤 해결 가능한 문제도 해결하고 임의의 문제 길이/복잡성에 일반화할 수 있음을 보인다.

Result: 도구 보강 SSM은 다양한 산술, 추론 및 코딩 작업에 대해 뛰어난 길이 일반화를 달성함을 보여준다.

Conclusion: 이 연구는 SSM이 대화형 도구 기반 및 에이전트 환경에서 트랜스포머의 효율적인 대안으로서의 가능성을 강조한다.

Abstract: State Space Models (SSMs) have become the leading alternative to Transformers
for sequence modeling. Their primary advantage is efficiency in long-context
and long-form generation, enabled by fixed-size memory and linear scaling of
computational complexity. We begin this work by showing a simple theoretical
result stating that SSMs cannot accurately solve any ``truly long-form''
generation problem (in a sense we formally define), undermining their main
competitive advantage. However, we show that this limitation can be mitigated
by allowing SSMs interactive access to external tools. In fact, we show that
given the right choice of tool access and problem-dependent training data, SSMs
can learn to solve any tractable problem and generalize to arbitrary problem
length/complexity (i.e., achieve length generalization). Following our
theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable
length generalization on a variety of arithmetic, reasoning, and coding tasks.
These findings highlight SSMs as a potential efficient alternative to
Transformers in interactive tool-based and agentic settings.

</details>


### [51] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 이 논문에서는 희소한 보상이 복잡한 행동 시퀀스에 의존하는 강화 학습 문제를 해결하기 위해 확률적 보상 기계라는 새로운 유형의 보상 기계와 이를 학습하기 위한 알고리즘을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 보상 기계 학습 알고리즘은 보상이 잡음이 없는 이상적인 설정을 가정하여 현실적인 한계를 가지고 있습니다.

Method: 제안하는 알고리즘은 제약 해결을 기반으로 하여 강화 학습 에이전트의 탐색을 통해 최소한의 확률적 보상 기계를 학습합니다.

Result: 이 알고리즘은 기존의 보상 기계 강화 학습 알고리즘과 쉽게 결합될 수 있으며, 한계에서 최적 정책으로 수렴할 것을 보장합니다.

Conclusion: 두 가지 사례 연구를 통해 우리의 알고리즘의 효과성을 입증하고 기존 방법 및 잡음이 있는 보상 함수를 처리하기 위한 단순한 접근 방식을 능가하는 것을 보여주었습니다.

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


### [52] [Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards](https://arxiv.org/abs/2510.14884)
*Sarah Liaw,Benjamin Plaut*

Main category: cs.LG

TL;DR: 이 논문에서는 멘토 없이 무한 보상과 함께 학습하는 모델을 형식화하고, 신뢰 영역을 선택하여 준비된 작업 정책을 실행하는 알고리즘을 제안합니다. 이를 통해 고위험 환경에서 안전하게 학습 에이전트를 배치하는 효과를 이론적으로 입증합니다.


<details>
  <summary>Details</summary>
Motivation: 고위험 AI 응용 프로그램에서는 단일 행동으로도 회복 불가능한 피해를 초래할 수 있습니다. 따라서 이러한 상황에서 신중한 학습이 필요합니다.

Method: 멘토가 없는 상태에서 무한 보상과 함께 학습하는 모델을 두 행동의 상황 밴딧으로 형식화하고, 신뢰할 수 있는 지역을 선택하여 해를 인증할 수 없는 곳만에서 작업 정책을 수행하는 신중 기반 알고리즘을 제안합니다.

Result: 이러한 조건과 독립적 및 동일한 분포의 입력 하에, 하위 선형 후회를 보장하며 신중한 탐색의 효과를 이론적으로 입증합니다.

Conclusion: 이 연구는 고위험 환경에서 안전하게 학습 에이전트를 배치하기 위한 신중한 탐색의 중요성을 강조합니다.

Abstract: In high-stakes AI applications, even a single action can cause irreparable
damage. However, nearly all of sequential decision-making theory assumes that
all errors are recoverable (e.g., by bounding rewards). Standard bandit
algorithms that explore aggressively may cause irreparable damage when this
assumption fails. Some prior work avoids irreparable errors by asking for help
from a mentor, but a mentor may not always be available. In this work, we
formalize a model of learning with unbounded rewards without a mentor as a
two-action contextual bandit with an abstain option: at each round the agent
observes an input and chooses either to abstain (always 0 reward) or to commit
(execute a preexisting task policy). Committing yields rewards that are
upper-bounded but can be arbitrarily negative, and the commit reward is assumed
Lipschitz in the input. We propose a caution-based algorithm that learns when
not to learn: it chooses a trusted region and commits only where the available
evidence does not already certify harm. Under these conditions and i.i.d.
inputs, we establish sublinear regret guarantees, theoretically demonstrating
the effectiveness of cautious exploration for deploying learning agents safely
in high-stakes environments.

</details>


### [53] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 본 연구는 기초 모델에서 샘플링만으로 추론 시 유사한 추론 능력을 이끌어낼 수 있는지를 조사하며, 반복 샘플링 알고리즘을 제안하여 여러 모델에서 추론 능력을 크게 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: RL을 통한 후 훈련 대형 언어 모델이 여러 분야에서 놀라운 능력을 보였으나, RL 동안 나타나는 새로운 행동에 대한 연구가 주를 이루었다.

Method: MCMC 기법에서 영감을 받아, 기초 모델의 가능성을 활용한 단순한 반복 샘플링 알고리즘을 제안한다.

Result: 다양한 기초 모델에서 우리의 알고리즘이 MATH500, HumanEval, GPQA와 같은 다양한 단일 샷 작업에서 RL에 의해 달성된 것과 유사하거나 이를 초과하는 상당한 추론 향상을 보인다.

Conclusion: 우리의 방법은 훈련, 선별된 데이터셋, 검증자를 필요로 하지 않으며, 쉽게 검증할 수 있는 분야를 넘어 널리 적용될 가능성이 시사된다.

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [54] [Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores](https://arxiv.org/abs/2510.14966)
*Zachary Robertson*

Main category: cs.LG

TL;DR: TVD-MI를 사용한 대형 언어 모델 간의 쌍 비교를 통해 binary 비평 결정이 도출된다. 이 연구는 TVD-MI의 이진 시험을 평균내어 아이템 반응 이론(IRT)에 적합한 중앙 확률 점수를 생성하며, 기존의 로지스틱 링크와의 비교를 통해 새로운 기법의 장점을 보인다.


<details>
  <summary>Details</summary>
Motivation: TVD-MI를 사용하여 대형 언어 모델의 성능을 비교하고, 기존 방법의 한계를 극복하기 위한 새로운 접근 방식을 제시하기 위해.

Method: 대형 언어 모델의 이진 비교를 통해 TVD-MI의 평균을 기반으로 중앙 확률 점수를 생성하고, 이를 Gini 엔트로피 최대화를 통해 클립 선형 모델로 도출.

Result: 33% 커버리지에서 에이전트 순위를 유지하면서 RMSE $0.117   0.008$를 달성하였으며, 세 번 더 적은 평가로 성능을 개선함.

Conclusion: TVD-MI의 기하학은 효율적인 LLM 평가를 위해 동일 사상에 의해 가장 잘 보존되며, 이는 다른 제한된 응답 영역에도 적용 가능하다.

Abstract: Pairwise comparisons of large language models using total variation distance
mutual information (TVD-MI) produce binary critic decisions per pair. We show
that averaging TVD-MI's binary trials yields centered-probability scores with
additive structure suitable for item-response theory (IRT) without nonlinear
link functions. Maximum-likelihood approaches to IRT use logistic links, but we
find empirically that these transformations introduce curvature that breaks
additivity: across three domains, the identity link yields median curl on raw
data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce
substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We
derive this clipped-linear model from Gini entropy maximization, yielding a
box-constrained least-squares formulation that handles boundary saturation. At
33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent
rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations
than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows
strong agreement in agent rankings ($\rho = 0.872$) and consistent
identity-link advantage. TVD-MI's geometry is best preserved by identity
mapping for efficient LLM evaluation, applicable to other bounded-response
domains.

</details>


### [55] [Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability](https://arxiv.org/abs/2510.14970)
*Katiana Kontolati,Rini Jasmine Gladstone,Ian Davis,Ethan Pickering*

Main category: cs.LG

TL;DR: 본 연구에서는 작물의 유전체 예측 및 선택을 위해 생물학적으로 정보화된 신경망(BINNs)을 확장하여 수천 개의 단일 염기 다형성(SNP)과 다중 오믹스 측정 및 이전 생물학적 지식을 통합하였다. 전통적인 유전자형-표현형(G2P) 모델은 낮은 정확도로 인해 대규모의 비용이 드는 현장 실험이 필요하며, 중간 분자 표현형을 통합한 모델은 높은 예측 적합도를 달성하지만 실제로 사용하기에는 비실용적이었다. BINNs는 경로 수준의 유도 편향을 인코딩하고 훈련 시 다중 오믹스 데이터를 활용하여 이 한계를 극복하였다.


<details>
  <summary>Details</summary>
Motivation: 작물의 유전자 예측 및 선택 과정을 개선하기 위해 생물학적 지식을 반영한 신경망을 확장하고자 함.

Method: 생물학적으로 정보화된 신경망(BINNs)을 사용하여 수천 개의 SNP, 다중 오믹스 데이터, 그리고 생물학적 지식을 통합하여 유전체 예측 모델을 구축함.

Result: BINN은 희소 데이터 조건 하에서 하위 집단 내 및 이간의 순위 상관 정확도를 최대 56% 향상시키고, GWAS/TWAS가 발견하지 못한 유전자를 비선형적으로 식별함.

Conclusion: BINNs는 중간 도메인 정보를 활용하여 유전체 예측 정확성을 향상시키고 비선형 생물학적 관계를 드러내며, 이는 유전자 선택, 후보 유전자 선택 및 유전자 편집의 우선순위를 정하는 데 도움을 줄 수 있다.

Abstract: We extend biologically-informed neural networks (BINNs) for genomic
prediction (GP) and selection (GS) in crops by integrating thousands of
single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior
biological knowledge. Traditional genotype-to-phenotype (G2P) models depend
heavily on direct mappings that achieve only modest accuracy, forcing breeders
to conduct large, costly field trials to maintain or marginally improve genetic
gain. Models that incorporate intermediate molecular phenotypes such as gene
expression can achieve higher predictive fit, but they remain impractical for
GS since such data are unavailable at deployment or design time. BINNs overcome
this limitation by encoding pathway-level inductive biases and leveraging
multi-omics data only during training, while using genotype data alone during
inference. Applied to maize gene-expression and multi-environment field-trial
data, BINN improves rank-correlation accuracy by up to 56% within and across
subpopulations under sparse-data conditions and nonlinearly identifies genes
that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic
metabolomics benchmark, BINN reduces prediction error by 75% relative to
conventional neural nets and correctly identifies the most important nonlinear
pathway. Importantly, both cases show highly sensitive BINN latent variables
correlate with the experimental quantities they represent, despite not being
trained on them. This suggests BINNs learn biologically-relevant
representations, nonlinear or linear, from genotype to phenotype. Together,
BINNs establish a framework that leverages intermediate domain information to
improve genomic prediction accuracy and reveal nonlinear biological
relationships that can guide genomic selection, candidate gene selection,
pathway enrichment, and gene-editing prioritization.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [56] [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/abs/2510.13825)
*Eugene Neelou,Ivan Novikov,Max Moroz,Om Narayan,Tiffany Saade,Mika Ayenson,Ilya Kabanov,Jen Ozmen,Edward Lee,Vineeth Sai Narajala,Emmanuel Guilherme Junior,Ken Huang,Huseyin Gulsin,Jason Ross,Marat Vyshegorodtsev,Adelin Travers,Idan Habler,Rahul Jadav*

Main category: cs.CR

TL;DR: A2AS 프레임워크는 AI 에이전트와 LLM 기반 응용 프로그램을 위한 보안 계층으로 소개되며, 이는 HTTPS가 HTTP를 보호하는 방식과 유사하다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트와 LLM 애플리케이션의 보안을 강화하고, 인증된 행동을 강제하며, 모델의 자가 방어를 활성화하기 위해 A2AS 프레임워크를 제안한다.

Method: A2AS는 행동 인증서, 인증된 프롬프트, 보안 경계 및 맥락 방어를 기반으로 하여 보안 규칙과 사용자 지정 정책을 적용하고, 에이전트 행동을 제어한다.

Result: A2AS는 지연 오버헤드, 외부 의존성, 아키텍처 변경, 모델 재교육 및 운영 복잡성을 회피한다.

Conclusion: BASIC 보안 모델을 소개하고 A2AS 프레임워크의 가능성을 탐구하여 A2AS 산업 표준 수립을 위한 첫 단계를 제시한다.

Abstract: The A2AS framework is introduced as a security layer for AI agents and
LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces
certified behavior, activates model self-defense, and ensures context window
integrity. It defines security boundaries, authenticates prompts, applies
security rules and custom policies, and controls agentic behavior, enabling a
defense-in-depth strategy. The A2AS framework avoids latency overhead, external
dependencies, architectural changes, model retraining, and operational
complexity. The BASIC security model is introduced as the A2AS foundation: (B)
Behavior certificates enable behavior enforcement, (A) Authenticated prompts
enable context window integrity, (S) Security boundaries enable untrusted input
isolation, (I) In-context defenses enable secure model reasoning, (C) Codified
policies enable application-specific rules. This first paper in the series
introduces the BASIC security model and the A2AS framework, exploring their
potential toward establishing the A2AS industry standard.

</details>


### [57] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: PIShield는 효과적이고 효율적인 프롬프트 주입 탐지 방법으로, LLM의 특정 레이어에서 추출한 마지막 토큰의 내부 표현을 이용하여 오염된 프롬프트와 깨끗한 프롬프트를 구분한다.


<details>
  <summary>Details</summary>
Motivation: LLM 통합 애플리케이션은 악의적인 프롬프트를 주입하는 공격에 취약하다.

Method: PIShield는 LLM의 특정 레이어에서 추출한 최종 토큰의 내부 표현을 기반으로 작동하며, 이를 통해 깨끗한 프롬프트와 오염된 프롬프트 간의 차별화된 특징을 포착한다.

Result: PIShield는 5개의 다양한 벤치마크 데이터셋과 8개의 프롬프트 주입 공격을 통해 11개의 기준 벤치마크와 비교하여 기존 방법들보다 월등히 높은 성능과 효율성을 보였다.

Conclusion: PIShield는 강력한 적응형 공격에도 저항력을 보여준다.

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [58] [Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks](https://arxiv.org/abs/2510.14185)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 이 논문은 미국의 산업 제어 시스템(ICS)의 사이버 위협에 대한 취약성을 분석하고, 이를 해결하기 위한 정책적 제안을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 산업 제어 시스템이 미국의 중요 인프라를 뒷받침하며, 사이버 위협에 의해 심각한 위험에 처해 있다는 것을 강조합니다.

Method: 이 논문은 역사적 사이버 공격 사례를 분석하여 ICS의 반복적인 취약점을 식별하고 현재 인프라에 대한 관련성을 평가합니다.

Result: 분석 결과, ICS는 여전히 많은 취약점이 존재하며, 즉각적인 개혁이 필요하다고 주장합니다.

Conclusion: 제로 트러스트 아키텍처와 개선된 네트워크 세분화를 통해 시스템 회복력을 향상시키기 위한 정책적 조치를 제안합니다.

Abstract: Industrial Control Systems (ICS) underpin the United States' critical
infrastructure, managing essential services such as power, water, and
transportation that are vital to national security and public safety. However,
increasing digital integration has exposed these systems to escalating cyber
threats. Historical attacks like Stuxnet and the Ukraine power grid incident
revealed exploitable weaknesses-poor network segmentation, outdated software,
weak authentication, and inadequate monitoring-that persist in many U.S. ICS
environments today. This paper analyzes these landmark attacks to identify
recurring vulnerabilities and assess their relevance to current U.S.
infrastructure. It argues that without immediate reforms, similar exploits
could lead to catastrophic disruptions and national security crises. To address
these risks, the paper proposes policy measures focused on implementing
zero-trust architecture and improved network segmentation to enhance system
resilience. These recommendations aim to guide policymakers and industry
leaders in securing the nation's most critical operational technologies against
future cyber threats.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [59] [Benefits and Limitations of Communication in Multi-Agent Reasoning](https://arxiv.org/abs/2510.13903)
*Michael Rizvi-Martel,Satwik Bhattamishra,Neil Rathi,Guillaume Rabusseau,Michael Hahn*

Main category: cs.MA

TL;DR: 복잡한 문제와 긴 문맥에서 모델 성능 저하 문제 해결을 위한 이론적 프레임워크 제안.


<details>
  <summary>Details</summary>
Motivation: 모델 성능이 문제의 복잡성과 문맥 길이에 따라 저하되므로 이를 해결해야 할 필요가 있음.

Method: 다수의 에이전트를 이용하여 복잡한 작업을 짧고 관리 가능한 작업으로 분해하는 이론적 프레임워크를 제안.

Result: 작업을 정확하게 해결하는 데 필요한 에이전트 수, 에이전트 간 통신의 양과 구조, 문제 크기와 맥락의 규모에 따른 속도 향상 가능성을 도출.

Conclusion: 이론적 분석과 실험 결과는 확장 가능한 다중 에이전트 추론 시스템 설계를 위한 원칙적인 지침을 제공함.

Abstract: Chain-of-thought prompting has popularized step-by-step reasoning in large
language models, yet model performance still degrades as problem complexity and
context length grow. By decomposing difficult tasks with long contexts into
shorter, manageable ones, recent multi-agent paradigms offer a promising
near-term solution to this problem. However, the fundamental capacities of such
systems are poorly understood. In this work, we propose a theoretical framework
to analyze the expressivity of multi-agent systems. We apply our framework to
three algorithmic families: state tracking, recall, and $k$-hop reasoning. We
derive bounds on (i) the number of agents required to solve the task exactly,
(ii) the quantity and structure of inter-agent communication, and (iii) the
achievable speedups as problem size and context scale. Our results identify
regimes where communication is provably beneficial, delineate tradeoffs between
agent count and bandwidth, and expose intrinsic limitations when either
resource is constrained. We complement our theoretical analysis with a set of
experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical
outcomes confirm the tradeoffs between key quantities predicted by our theory.
Collectively, our analysis offers principled guidance for designing scalable
multi-agent reasoning systems.

</details>


### [60] [Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations](https://arxiv.org/abs/2510.13982)
*Jinkun Chen,Sher Badshah,Xuemin Yu,Sijia Han,Jiechao Gao*

Main category: cs.MA

TL;DR: 인공지능 에이전트가 커뮤니케이션뿐만 아니라 진화하고 적응하며 예측할 수 없는 방식으로 세상을 재형성할 수 있다면? 현재 많은 시뮬레이션이 정적인 제한에 사로잡혀 있어 현실 사회의 복잡성을 포착하지 못하고 있다. 우리는 기존 벤치마크의 한계를 비판하고, 새로운 아키텍처와 과제를 제시하며, 적응형 다중 에이전트 시뮬레이션의 새로운 혁신을 촉구한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능 에이전트가 진화하고 적응하는 가능성을 탐구하고, 현재의 시뮬레이션이 현실 사회의 복잡성을 포착하지 못하는 문제를 지적한다.

Method: llm(대형 언어 모델)과 다중 에이전트 역학을 결합한 새로운 아키텍처를 비판적으로 검토하고, 안정성과 다양성의 균형, 예상치 못한 행동 평가, 더 큰 복잡성으로의 확장과 같은 주요 장애물을 강조한다.

Result: 정적이고 작업 특정 벤치마크가 근본적으로 불충분하다는 것을 주장하고, 급변하는 분야를 위한 새로운 분류법을 제시한다.

Conclusion: 커뮤니티가 정적 패러다임을 넘어 적응형, 사회 인식이 있는 다중 에이전트 시뮬레이션의 다음 세대를 형성하는 데 도움을 줄 것을 촉구한다.

Abstract: What if artificial agents could not just communicate, but also evolve, adapt,
and reshape their worlds in ways we cannot fully predict? With llm now powering
multi-agent systems and social simulations, we are witnessing new possibilities
for modeling open-ended, ever-changing environments. Yet, most current
simulations remain constrained within static sandboxes, characterized by
predefined tasks, limited dynamics, and rigid evaluation criteria. These
limitations prevent them from capturing the complexity of real-world societies.
In this paper, we argue that static, task-specific benchmarks are fundamentally
inadequate and must be rethought. We critically review emerging architectures
that blend llm with multi-agent dynamics, highlight key hurdles such as
balancing stability and diversity, evaluating unexpected behaviors, and scaling
to greater complexity, and introduce a fresh taxonomy for this rapidly evolving
field. Finally, we present a research roadmap centered on open-endedness,
continuous co-evolution, and the development of resilient, socially aligned AI
ecosystems. \textbf{We call on the community to move beyond static paradigms
and help shape the next generation of adaptive, socially-aware multi-agent
simulations.}

</details>


### [61] [Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local Alignment](https://arxiv.org/abs/2510.14008)
*Jinwei Hu,Yi Dong,Shuang Ao,Zhuoyun Li,Boxuan Wang,Lokesh Singh,Guangliang Cheng,Sarvapali D. Ramchurn,Xiaowei Huang*

Main category: cs.MA

TL;DR: LLM-MAS는 분산 추론 및 협업에서 새로운 가능성을 열어주지만, 비합의 및 불확실성 리스크도 증가시킨다. 이 시스템의 책임 있는 행동 보장을 위해서는 에이전트 차원의 정렬에서 시스템 차원의 합의로 패러다임 전환이 필요하다.


<details>
  <summary>Details</summary>
Motivation: LLM-MAS의 잠재력을 최대화하면서도 발생할 수 있는 위험을 예방하기 위해 책임 있는 시스템이 필요함을 주장하고 있다.

Method: 책임을 정적인 제약이 아니라 합의, 불확실성, 보안을 포함한 생애주기 전반에 걸친 속성으로 개념화하고, 인간 중심 가치와 객관적인 검증 가능성이 통합되어야 한다고 제안한다. 또한, 인문학적 디자인과 인간-AI 협력 감독을 결합한 이중 관점 거버넌스 프레임워크를 제안한다.

Result: 이러한 프레임워크를 통해 LLM-MAS의 생애주기 전반에 걸쳐 책임을 추적하고 보장할 수 있음을 확인한다.

Conclusion: LLM-MAS를 느슨한 에이전트 집합으로 간주하기보다는 원칙적인 메커니즘을 요구하는 통합된 사회 기술 시스템으로 보는 것이 중요하다.

Abstract: LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in
distributed reasoning, collaboration, and task generalization but also
introduce additional risks due to unguaranteed agreement, cascading
uncertainty, and adversarial vulnerabilities. We argue that ensuring
responsible behavior in such systems requires a paradigm shift: from local,
superficial agent-level alignment to global, systemic agreement. We
conceptualize responsibility not as a static constraint but as a lifecycle-wide
property encompassing agreement, uncertainty, and security, each requiring the
complementary integration of subjective human-centered values and objective
verifiability. Furthermore, a dual-perspective governance framework that
combines interdisciplinary design with human-AI collaborative oversight is
essential for tracing and ensuring responsibility throughout the lifecycle of
LLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as
unified, dynamic socio-technical systems that demand principled mechanisms to
support each dimension of responsibility and enable ethically aligned,
verifiably coherent, and resilient behavior for sustained, system-wide
agreement.

</details>


### [62] [The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems](https://arxiv.org/abs/2510.14401)
*Prateek Gupta,Qiankun Zhong,Hiromu Yakura,Thomas Eisenmann,Iyad Rahwan*

Main category: cs.MA

TL;DR: 이 연구는 다중 에이전트 연구에서 협력 규범이 무형 모티브 시나리오에서 어떻게 형성되는지를 살펴본다. 특히, 보상 신호 없이 문화-진화 메커니즘을 통합한 CPR(CPR) 시뮬레이션 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: LLM(대형 언어 모델)을 사용한 다중 에이전트 연구가 증가하면서 개인의 이익 추구가 공동의 이익을 훼손할 수 있는 무형 모티브 시나리오에서 규범과 협력이 어떻게 나타나는지를 탐구하고 있습니다.

Method: 명시적 보상 신호를 제거하고 문화-진화 메커니즘을 내장한 CPR 시뮬레이션 프레임워크를 도입합니다. 에이전트는 채취의 결과로부터 배우고, 환경 피드백을 통해 규범이 자연스럽게 형성됩니다.

Result: 기존의 인간 행동 연구에서 얻은 주요 발견을 재현하여 우리의 시뮬레이션의 유효성을 확립했습니다.

Conclusion: 다양한 LLM으로 구성된 에이전시 사회가 자원 풍부성과 부족 그리고 이타적과 이기적 조건 하에서 협력 및 규범 형성을 지속하는 데 있어 모델 간 체계적인 차이를 드러내며, 이는 AI 시스템의 디자인에 중요한 정보를 제공합니다.

Abstract: A growing body of multi-agent studies with Large Language Models (LLMs)
explores how norms and cooperation emerge in mixed-motive scenarios, where
pursuing individual gain can undermine the collective good. While prior work
has explored these dynamics in both richly contextualized simulations and
simplified game-theoretic environments, most LLM systems featuring common-pool
resource (CPR) games provide agents with explicit reward functions directly
tied to their actions. In contrast, human cooperation often emerges without
full visibility into payoffs and population, relying instead on heuristics,
communication, and punishment. We introduce a CPR simulation framework that
removes explicit reward signals and embeds cultural-evolutionary mechanisms:
social learning (adopting strategies and beliefs from successful peers) and
norm-based punishment, grounded in Ostrom's principles of resource governance.
Agents also individually learn from the consequences of harvesting, monitoring,
and punishing via environmental feedback, enabling norms to emerge
endogenously. We establish the validity of our simulation by reproducing key
findings from existing studies on human behavior. Building on this, we examine
norm evolution across a $2\times2$ grid of environmental and social
initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and
benchmark how agentic societies comprised of different LLMs perform under these
conditions. Our results reveal systematic model differences in sustaining
cooperation and norm formation, positioning the framework as a rigorous testbed
for studying emergent norms in mixed-motive LLM societies. Such analysis can
inform the design of AI systems deployed in social and organizational contexts,
where alignment with cooperative norms is critical for stability, fairness, and
effective governance of AI-mediated environments.

</details>
