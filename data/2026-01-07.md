<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 22]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: Warp Cortex는 비동기 아키텍처를 통해 대규모 인공지능 에이전트를 효율적으로 지원합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 대형 언어 모델이 소비자 하드웨어에서 비효율적인 시스템 2 병렬 추론을 초래하고 있어 해결이 필요하다.

Method: 에이전트 로직과 물리적 메모리를 분리함으로써 백만 에이전트의 인지 확장을 이론적으로 가능하게 하는 Warp Cortex라는 비동기 아키텍처를 제안합니다.

Result: NVIDIA RTX 4090에서 100개의 동시 에이전트를 2.2GB의 총 VRAM으로 실험적으로 증명하고, 이론적으로는 1,000개 이상의 에이전트를 지원할 수 있습니다.

Conclusion: 비침습적인 KV-캐시 업데이트 메커니즘인 Referential Injection을 도입하여 비동기 하위 에이전트가 주요 생성에 영향을 미칠 수 있습니다.

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [2] [Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions](https://arxiv.org/abs/2601.00862)
*Joey Chan,Huan Wang,Haoyu Pan,Wei Wu,Zirong Wang,Zhen Chen,Ershun Pan,Min Xie,Lifeng Xi*

Main category: cs.LG

TL;DR: 배터리 용량 감소의 정확한 예측은 에너지 저장 시스템의 안전성과 신뢰성, 효율성 유지에 필수적이다. 본 논문은 다양한 화학적 조성 및 사용 시나리오에 대해 견고한 성능을 유지하는 통합 용량 예측 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 배터리 용량 감소를 정확히 예측하는 것은 에너지 저장 시스템의 안전성과 신뢰성을 보장하는 데 중요하다. 그러나 셀의 화학조성, 형태, 운영 조건의 큰 이질성으로 인해, 훈련 도메인을 넘어 일반화되는 단일 모델을 구축하는 것이 어렵다.

Method: 20개의 공공 노화 데이터셋을 1,704개의 셀과 3,961,195개의 충전-방전 사이클로 구성된 대규모 데이터베이스로 정리하고, TSFM 백본과 Low-Rank Adaptation(LoRA)을 결합하여 공통의 감소 패턴을 포착하는 물리 기반 대비 표현 학습을 수행한다.

Result: 본 연구에서는 훈련배제된 화학성, 용량 스케일 및 운영 조건에서도 안정적인 성능을 유지하며, 단일 통합 모델이 강력한 데이터셋 별 기준선에 비해 경쟁력 있는 또는 우수한 정확도를 달성함을 보인다.

Conclusion: TSFM 기반 아키텍처는 실제 배터리 관리 시스템에서의 용량 열화 예측을 위한 확장 가능하고 전이 가능한 솔루션으로서의 잠재력을 보여준다.

Abstract: Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\,^{\circ}\mathrm{C}$ to $45\,^{\circ}\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.

</details>


### [3] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow는 도시 자전거 공유 서비스의 동적 재균형 문제를 해결하기 위한 다층 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 도시 자전거 공유 서비스에서의 효율적인 자전거 재배치 문제를 해결하기 위해.

Method: 도시 자전거 네트워크를 고충실도 시뮬레이션으로 모델링한 DQN 에이전트를 사용하여 마르코프 결정 프로세스로 재배치 정책을 학습한다.

Result: 95% 이상의 네트워크 불균형 감소, 최소한의 이동 거리 및 강력한 차량 활용도 달성.

Conclusion: SmartFlow는 복잡한 도시 이동 네트워크에서 해석 가능한 AI 기반 물류의 청사진을 제공한다.

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [4] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: LearnAD는 뇌 MRI 데이터를 기반으로 알츠하이머병을 예측하는 신경-상징적 방법을 소개하며, 완전히 해석 가능한 규칙을 학습합니다.


<details>
  <summary>Details</summary>
Motivation: 알츠하이머병 예측의 정확성을 개선하고 해석 가능성을 높이기 위해.

Method: LearnAD는 통계 모델, 의사 결정 나무, 랜덤 포레스트 또는 GNN을 적용하여 관련 뇌 연결을 식별하고, FastLAS를 사용하여 전역 규칙을 학습합니다.

Result: 우리의 최적 모델은 의사 결정 나무를 초과하고, 서포트 벡터 머신의 정확도와 일치하며, 모든 기능으로 훈련된 랜덤 포레스트와 GNN보다 약간 떨어지는 성능을 보입니다.

Conclusion: LearnAD는 신경 과학에서 GNN 행동에 대한 이해를 심화할 수 있는 상징적 학습의 가능성을 보여줍니다.

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [5] [Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation](https://arxiv.org/abs/2601.00932)
*Andrea Thomas Nava,Lijo Johny,Fabio Azzalini,Johannes Schneider,Arianna Casanova*

Main category: cs.LG

TL;DR: 데이터 기반 제품 개발(DDPD)은 제품 설계 사양과 그에 따른 속성 간의 관계를 학습하기 위해 데이터를 활용합니다. 이 연구는 신경망을 훈련시키고 원하는 성능을 극대화하는 최적 입력 특성을 식별하기 위해 Projected Gradient Descent를 적용합니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 제품이 상관관계가 있는 여러 속성을 동시에 최적화해야 하는 필요성이 있습니다.

Method: 신경망을 사용한 공동 최적화 및 ConfMC를 통한 불확실성 추정 통합

Result: 우리의 방법은 최신 성능과 일치하며 적응형 비균일 예측 구간을 제공합니다.

Conclusion: 커버리지 수준 조정 시 재훈련이 필요하지 않습니다.

Abstract: Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.

</details>


### [6] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: 비전-언어 모델(VLMs)은 이미지 캡셔닝과 시각적 질문 응답(VQA) 같은 기술적 작업에서 성공을 거두었으나, 여러 화자의 팟캐스트 대화와 같은 매력적인 긴 형식의 내러티브 생성 능력은 아직 탐구가 부족하고 평가하기 어렵다. 본 연구에서는 끝에서 끝까지의 시각적 팟캐스트 생성에 대한 새로운 파이프라인을 제시하고, 4,000개의 이미지-대화 쌍으로 구성된 데이터셋에서 Qwen3-VL-32B 모델을 미세 조정하였다. 중요한 점은 합성-실제 훈련 전략을 사용하여, 합성 이미지와 짝지어진 고품질 팟캐스트 대화를 훈련하고, 실제 세계의 사진 시퀀스에서 평가함으로써 모델의 일반화 능력을 테스트한다. 우리는 텍스트 중복을 넘어서는 포괄적인 평가 프레임워크를 제안하며, 품질을 평가하기 위해 AI 판사(예: Gemini 3 Pro, Claude Opus 4.5, GPT 5.2)와 새로운 스타일 메트릭(평균 턴 길이, 화자 전환율)을 사용할 것이다. 우리의 실험 결과, 미세 조정된 32B 모델이 대화의 자연스러움(80% 승률 이상)과 내러티브 깊이(+50% 턴 길이)에서 235B 기본 모델을 유의미하게 초과 성과를 보이며, 동일한 시각적 기반 능력을 유지하고 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델이 긴 형식 내러티브 생성에서의 잠재력을 탐구하기 위해서.

Method: Qwen3-VL-32B 모델을 고품질 팟캐스트 대화 및 합성 이미지를 사용하여 미세 조정하는 새로운 파이프라인을 제시하고, 평가를 위해 실세계의 사진 시퀀스를 사용한다.

Result: 미세 조정된 32B 모델이 대화의 자연스러움과 내러티브 깊이에서 235B 기본 모델을 유의미하게 초과 성과를 보인다.

Conclusion: 새로운 평가 프레임워크와 훈련 방법을 통해 VLM의 내러티브 생성 성능을 향상시킬 수 있다.

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [7] [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075)
*Hansen Jin Lillemark,Benhao Huang,Fangneng Zhan,Yilun Du,Thomas Anderson Keller*

Main category: cs.LG

TL;DR: 본 연구는 'Flow Equivariant World Models'라는 새로운 프레임워크를 도입하여 자가 동작과 외부 객체의 동작을 통합하고, 안정적인 잠재 세계 표현을 제공하여 2D 및 3D 비디오 세계 모델링 벤치마크에서 기존 최첨단 모델보다 우수한 성능을 증명하였다.


<details>
  <summary>Details</summary>
Motivation: 이 연구의 동기는 감각 입력의 연속 흐름과 자가 동작, 외부 객체의 동역학을 통합하여 보다 효율적인 세계 모델을 개발하는 것이다.

Method: 본 연구에서는 자가 동작과 외부 객체의 동작을 하나의 매개변수 리 군 '흐름'으로 통합하여 그룹 동등성을 구현하는 Flow Equivariant World Models 프레임워크를 제안한다.

Result: 2D 및 3D 부분 관측 비디오 세계 모델링 벤치마크에서 Flow Equivariant World Models가 기존의 확산 기반 및 메모리 증강 세계 모델링 아키텍처보다 유의미하게 우수한 성능을 보였다.

Conclusion: 흐름 동등성은 긴 롤아웃에서 특히 유익하며, 데이터 효율적이고 대칭에 기반한 체화된 지능을 위한 확장 가능한 경로를 제공한다.

Abstract: Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.

</details>


### [8] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: 이 연구는 게임 기반 평가를 사용하여 소프트웨어 개발 역할에 적합한 인재를 예측하는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 인성 평가는 응답 편향과 피로감 등의 문제로 어려움을 겪고 있으며, 게임 기반 평가는 이러한 문제를 해결할 수 있는 가능성이 있습니다.

Method: 다양한 장르의 심각한 게임 프레임워크와 기계 학습 기술을 결합하여 행동 데이터를 수집하고 분석했습니다.

Result: 모델은 최대 97%의 정밀도와 94%의 정확도를 달성했습니다.

Conclusion: 게임 중에 수집된 행동 신호가 인재 적합성을 예측하는 데 효과적임을 시사하며, 이는 직업 평가에 대한 매력적이고 덜 편향된 대안으로 진지한 게임의 가능성을 보여줍니다.

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [9] [The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context](https://arxiv.org/abs/2601.01231)
*Md Muhtasim Munif Fahim,Humyra Ankona,Md Monimul Huq,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 이 연구는 '의존성 격차'라는 새로운 프레임워크를 제안하여, 높은 참여도 학생들이 인프라 실패에 취약해질 수 있음을 밝힌다.


<details>
  <summary>Details</summary>
Motivation: 디지털 접근이 자원이 제한된 환경에서 급속히 확장되고 있지만, 디지털 학습 플랫폼에 대한 만족도는 연결성이 비슷한 학생들 사이에서도 크게 다르다는 점에 주목했다.

Method: 방글라데시의 396명의 대학생을 대상으로 세 단계 분석 접근법을 사용하여 포괄적인 연구를 실시했다: (1) 안정성을 검증한 K-프로토타입 클러스터링을 사용하여 학생 프로필 식별, (2) 프로필 특정 랜덤 포레스트 모델 및 SHAP와 ALE 분석을 통해 만족도 요인 결정, (3) 의존성 격차 가설 검증을 위한 경향 점수 매칭을 포함한 공식 상호작용 분석.

Result: 세 가지 뚜렷한 프로필이 도출되었다: 우연히 참여한 학생(58%), 효율적인 학습자(35%), 그리고 하이퍼 참여 학생(7%)이다. 교육 기기 사용 시간과 인터넷 신뢰성 간의 유의미한 상호작용이 발생하여 의존성 격차를 확인했다: 인프라가 신뢰성을 유지할 때만 참여가 만족도를 증가시켰다. 하이퍼 참여 학생들은 복잡한 디지털 작업 흐름에도 불구하고 가장 큰 취약성을 보였다.

Conclusion: 취약한 인프라 환경에서는 능력이 오히려 liability가 될 수 있음을 보여준다. 디지털 전환 정책은 의존성 높은 사용자들을 위해 신뢰성을 우선시하고, 비상 시스템을 수립하며, 학생들에게 의존성 위험에 대해 교육해야 한다.

Abstract: Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.
  Purpose: This study introduces the "Dependency Divide", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.
  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.
  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.
  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.

</details>


### [10] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: 이 논문은 ADR 예측을 위한 새로운 접근 방식을 제안하며, 기존 방법의 한계를 극복하고 예측 공간을 확대하는 데 성공하였다.


<details>
  <summary>Details</summary>
Motivation: 약물 부작용(ADR) 예측을 통해 신약 개발 비용과 기간을 감소시키는 것.

Method: Graph-Motif 특성 융합과 다중 레이블 생성을 기반으로 한 새로운 ADR 예측 패러다임(GM-MLG)을 제안함.

Result: GM-MLG는 예측 공간을 200개에서 10,000개 이상으로 확장하며, 최대 38%의 성능 향상과 평균 20%의 이익을 보여줌.

Conclusion: 본 연구는 ADR과 모티프 간의 비선형 구조-활성 관계를 설명하며, 약물 안전성에 대한 체계적인 위험 감소를 위한 해석 가능하고 혁신적인 지원을 제공함.

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [11] [Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows](https://arxiv.org/abs/2601.01357)
*Ke Xiao,Haoze Zhang,Runze Mao,Han Li,Zhi X. Chen*

Main category: cs.LG

TL;DR: FlamePilot는 자동화된 CFD 워크플로우를 통해 연소 모델링 연구를 지원하는 LLM 에이전트로, 과학 문헌 지식을 통합하여 복잡한 시뮬레이션을 수행할 수 있도록 설계되었다. 이 시스템은 OpenFOAM과 DeepFlame과 같은 프레임워크에서 강력한 환경 설정과 실행을 보장하며, 연구 논문에서 학습하여 시뮬레이션 작업을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 과학 분야에서 LLM의 통합이 필요하며, 연소 모델링에서 실용적인 AI 지원이 요구된다.

Method: FlamePilot은 OpenFOAM과 DeepFlame에서 복잡한 시뮬레이션을 설정하고 실행하기 위해 원자 도구를 활용하는 아키텍처를 가지고 있으며, 과학 기사를 학습하여 시뮬레이션의 초기 설정부터 최적화된 결과까지 안내하는 핵심 정보를 추출할 수 있다.

Result: FlamePilot은 공공 벤치마크에서 완벽한 1.0 실행 가능성 점수와 0.438 성공률을 기록하였으며, 이는 이전에 보고된 최고 점수인 0.625와 0.250을 초과한다.

Conclusion: FlamePilot은 연구자가 고급 분석을 위해 자유로워지도록 워크플로우 오케스트레이션을 관리하면서 협업 파트너십을 촉진하는 AI 기반의 연소 모델링의 기초 프레임워크를 수립한다.

Abstract: The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.

</details>


### [12] [Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD](https://arxiv.org/abs/2601.01465)
*Ze Peng,Jian Zhang,Yisen Wang,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.LG

TL;DR: 이 논문은 SGD의 일반화 성능을 개선하기 위한 새로운 정보 이론적 경계를 도출합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 정보 이론적 경계가 SGD의 평탄도 편향을 충분히 활용하지 못하고, 평탄도가 개선된 경우의 더 나은 일반화를 반영하지 못하는 문제를 해결하고자 함.

Method: 평탄도를 더욱 활용한 새로운 정보 이론적 경계를 도출하며, 이를 통해 최종 가중치 공분산의 큰 분산 방향이 손실 경치에서 작은 국소 곡률을 가질 때 모델이 더 잘 일반화된다는 것을 나타냄.

Result: 제안한 경계는 더 나은 일반화 성능을 반영하고, 수치적으로도 더 타이트함을 입증함.

Conclusion: 이 논문은 SGD의 평탄도에 대한 새로운 경계를 제시하며, 메모리화-일반화 간의 거래를 우회하는 방법을 제안함.

Abstract: Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called "omniscient trajectory". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $Ω(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.

</details>


### [13] [SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines](https://arxiv.org/abs/2601.01484)
*Itai Morad,Nir Shlezinger,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 본 연구는 지식 증류(KD)에 대한 베이esian 관점을 채택하여 확률적 경량 모델에서의 수렴 거동을 분석하며, 학생 모델이 더욱 높은 정확성과 안정적인 수렴을 이끌어내는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: KD는 대형 교사 네트워크에서 작고 일반적인 학생 모델로 지식을 전이하는 중심 패러다임이며, 이론적 기초가 부분적으로만 이해되고 있다.

Method: 우리는 교사가 정확한 베이즈 클래스 확률(BCP)을 제공할 때와 BCP의 노이즈 유사물로 감독할 때 두 가지 구간을 연구하였다.

Result: BCP로부터 학습하는 것이 변동성을 감소시키고 수렴 경계에서 이웃 항을 제거하며, 노이즈 수준이 일반화 및 정확성에 미치는 영향을 특성화한다.

Conclusion: 베이esian 심층 학습 모델을 KD에서 교사로 사용하는 것을 지지하며, 베이esian 교사로부터 증류된 학생들이 더 높은 정확도와 안정적인 수렴을 달성함을 보여준다.

Abstract: Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.

</details>


### [14] [Communication-Efficient Federated AUC Maximization with Cyclic Client Participation](https://arxiv.org/abs/2601.01649)
*Umesh Vangapally,Wenhan Wu,Chen Chen,Zhishuai Guo*

Main category: cs.LG

TL;DR: 주기적인 클라이언트 참여 하에서 연합 AUC 극대화를 위한 효율적인 알고리즘을 개발하고 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 불균형 데이터로부터 학습하기 위한 강력한 접근법인 연합 AUC 극대화는 기존 방법들이 클라이언트의 전적인 가용성을 가정하는 경향이 있어, 실제 시스템에서는 비현실적이다.

Method: 주기적인 클라이언트 참여에 따른 연합 AUC 극대화를 위한 통신 효율적인 알고리즘을 개발하여 분석하였다. 두 가지 주요 설정을 조사하였으며, AUC 극대화 문제를 비convex-강한 오목 최소 극대화 최적화로 재구성하였다.

Result: 첫 번째 설정에서 $	ilde{O}(1/ε^{1/2})$의 통신 복잡도를, 두 번째 설정에서는 $O(1/ε^3)$의 통신 복잡도를 확립하였다.

Conclusion: 우리는 이미지 분류, 의료 이미징 및 사기 탐지 등의 벤치마크 작업에서 우리의 방법이 뛰어난 효율성과 효과성을 보임을 실험을 통해 증명하였다.

Abstract: Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.

</details>


### [15] [HeurekaBench: A Benchmarking Framework for AI Co-scientist](https://arxiv.org/abs/2601.01678)
*Siba Smarak Panigrahi,Jovana Videnović,Maria Brbić*

Main category: cs.LG

TL;DR: HeurekaBench는 실험 데이터셋을 위한 탐색적이고 열린 연구 질문을 생성하는 벤치마크 프레임워크로, LLM 기반 에이전트 시스템 평가를 위한 대안으로 제시된다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트 시스템의 평가가 어렵기 때문에 실용적이고 전체적인 연구 시나리오가 필요하다.

Method: HeurekaBench 프레임워크는 여러 LLM을 활용하여 통찰을 추출하고 후보 워크플로우를 생성하는 반자동 파이프라인을 사용하여, 각 질문을 과학적 연구와 코드 저장소에 기반하여 생성한다.

Result: sc-HeurekaBench 벤치마크를 통해 최신의 단일 세포 에이전트와 비교하고 비율 평가를 통해 비평 모듈 추가로 ill-formed 응답을 최대 22% 개선할 수 있음을 발견했다.

Conclusion: HeurekaBench는 실제 과학적 워크플로우에 기반한 과학적 에이전트의 엄격하고 전체적인 평가를 위한 경로를 설정한다.

Abstract: LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.

</details>


### [16] [Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2601.01800)
*Qi Wei,Junchao Fan,Zhao Yang,Jianhua Wang,Jingkai Mao,Xiaolin Chang*

Main category: cs.LG

TL;DR: CARRL은 자율 주행에서 안전성에 중점을 둔 새로운 적대적 훈련 접근 방식으로, 불균형한 리스크를 효과적으로 관리하여 정책의 강건성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: CARRL은 자율 주행의 안전성에 대한 리스크를 효과적으로 다루기 위해 개발되었다.

Method: CARRL은 위험 노출 적대자(REA)와 위험 타겟 강건 에이전트(RTRA)라는 두 가지 상호작용 구성 요소로 구성되어 있으며, 일반 합계 게임으로 모델링된다.

Result: 실험 결과, 우리 방법은 최신 기준 방법에 비해 모든 경우에서 충돌률을 최소 22.66% 감소시키는 것으로 나타났다.

Conclusion: CARRL은 자율 주행 시나리오에서 실제로 안전성 문제를 해결하는 데 효과적이다.

Abstract: Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

</details>


### [17] [Moments Matter:Stabilizing Policy Optimization using Return Distributions](https://arxiv.org/abs/2601.01803)
*Dennis Jabs,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 딥 강화 학습 에이전트는 동일한 에피소드 반환을 달성하지만 서로 다른 행동을 보이는 경향이 있으며, 이는 환경 및 알고리즘적 요인 때문입니다. 본 연구는 정책 업데이트에 의해 발생하는 변동성을 줄이기 위해 환경의 무작위성을 활용하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 딥 강화 학습 에이전트가 서로 다른 정책을 배우는 이유는 환경과 알고리즘적 요인 때문이며, 이는 알고리즘 비교 및 실제 전이에서 문제를 야기할 수 있습니다.

Method: 상태-행동 반환 분포를 분포적 비평가를 통해 모델링하고, 이 분포의 고차 모멘트를 사용하여 PPO의 이점 함수를 편향합니다.

Result: 우리의 방법은 Walker2D에서 최대 75%까지 안정성을 개선하면서도 비슷한 평가 반환을 유지합니다.

Conclusion: 정신 기반 보정이 $R(θ)$를 좁히고 안정성을 향상시킵니다.

Abstract: Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

</details>


### [18] [FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data](https://arxiv.org/abs/2601.01901)
*Yuexuan Xia,Yinghao Zhang,Yalin Liu,Hong-Ning Dai,Yong Xia*

Main category: cs.LG

TL;DR: FedBiCross는 비독립적인 데이터 환경에서 개인화된 지식 증류를 통해 의료 이미지를 효과적으로 학습할 수 있도록 지원하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: OSFL 방법이 개인 정보 보호가 중요한 의료 분야에서 매력적이지만, 기존 방법의 비효율을 해결할 필요가 있다.

Method: FedBiCross는 클라이언트의 모델 출력 유사성에 따라 클러스터링하고, 이들을 기반으로 이중 수준 크로스 클러스터 최적화를 통해 적응적 가중치를 학습하여 부정적 전이를 억제한다.

Result: FedBiCross는 네 가지 의료 영상 데이터셋에서 모든 비독립적 데이터 정도에 걸쳐 기존 최첨단 방법들을 지속적으로 능가하는 성능을 보였다.

Conclusion: FedBiCross는 비독립적인 데이터 환경에서 지식 증류를 통한 보다 효과적인 학습 방법을 제시하며, 개인화된 접근 방식을 통해 성능을 극대화한다.

Abstract: Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.

</details>


### [19] [SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling](https://arxiv.org/abs/2601.01943)
*Tieu-Long Phan,Nhu-Ngoc Nguyen Song,Peter F. Stadler*

Main category: cs.LG

TL;DR: SynRXN은 컴퓨터 지원 합성 계획(CASP)을 위한 통합 벤치마킹 프레임워크이자 오픈 데이터 자원이다.


<details>
  <summary>Details</summary>
Motivation: 합성 계획의 데이터 세트 이질성을 제거하고 투명하며 재사용 가능한 평가 구조를 제공하여 CASP 방법의 공정한 비교를 가능하게 한다.

Method: 합성 계획을 다섯 개의 작업 세대로 분해하고, 각 작업 세대에 대해 버전화된 데이터 세트를 패키징하며, 평가 함수와 표준화된 평가 워크플로를 제공한다.

Result: SynRXN은 각 작업에 대해 투명한 데이터 분할 기능을 제공하고, 모든 작업에 대한 평가 세트를 명시적으로 설정하여 모델 훈련에 적합하지 않도록 한다.

Conclusion: SynRXN은 실세계 합성 계획 작업을 위한 성능 추정치를 지원하고, CASP 방법의 엄격한 테스트를 가능하게 한다.

Abstract: We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.

</details>


### [20] [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](https://arxiv.org/abs/2601.02151)
*Muxi Diao,Lele Yang,Wuxuan Gong,Yutong Zhang,Zhonghao Yan,Yufei Han,Kongming Liang,Weiran Xu,Zhanyu Ma*

Main category: cs.LG

TL;DR: Entropy-Adaptive Fine-Tuning (EAFT)은 기존의 SFT가 초래하는 이론적 단점을 극복하고, 모델이 불확실한 샘플로부터 학습할 수 있도록 지원한다.


<details>
  <summary>Details</summary>
Motivation: Supervised Fine-Tuning (SFT)은 도메인 적응의 표준 패러다임이지만, 종종 재앙적인 망각을 초래한다는 문제를 해결하고자 하였다.

Method: Entropy-Adaptive Fine-Tuning (EAFT)을 제안하여, 예측 확률에만 의존하는 기존 방법과 달리 토큰 수준의 엔트로피를 게이팅 메커니즘으로 활용하였다.

Result: Qwen 및 GLM 시리즈의 광범위한 실험에서 EAFT가 표준 SFT의 하향식 성능을 일관되게 일치시키면서도 일반적인 능력 저하를 크게 완화한다는 것을 확인하였다.

Conclusion: EAFT는 SFT에 비해 모델이 불확실한 샘플로부터 학습하면서도 충돌하는 데이터에서의 그래디언트를 억제할 수 있는 유용한 방법이다.

Abstract: Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.

</details>


### [21] [ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense](https://arxiv.org/abs/2601.02196)
*Yu Li,Sizhe Tang,Rongqian Chen,Fei Xu Yu,Guangyu Jiang,Mahdi Imani,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 이 논문은 자동화된 사이버 방어(ACD)를 위해 컨텍스트 기반의 부분 관찰 마르코프 결정 문제로 프레임 하여 몬테카를로 트리 탐색(MCTS)에 기반한 방어 정책을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 샘플 효율적인 방어 정책을 학습할 필요성에서 영감을 받아 ACD를 CAGE Challenge 4(세컨드 CC4)로 프레임 하였다.

Method: 부분 관찰 마르코프 결정 문제를 사용하여 MCTS 기반의 계획 중심 방어 정책을 제안하고, 그래프 신경망(GNN)을 사용하여 네트워크의 관찰을 속성 그래프로 임베딩한다.

Result: 제안한 에이전트는 다양한 네트워크 구조와 적대 행동을 포함한 CC4 시나리오에서 평가되었으며, 검색 유도 그래프 임베딩 기반 계획이 방어 보상과 강건성을 개선함을 보여주었다.

Conclusion: 모델 없는 일반화와 정책 증류를 조합하는 접근법을 통해 복잡한 검색 공간에서 실용적인 해결책을 제공한다.

Abstract: Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.

</details>


### [22] [CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents](https://arxiv.org/abs/2601.02201)
*Keyu Wang,Bingchen Miao,Wendong Bu,Yu Wu,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: CORE 프레임워크는 모방과 탐색을 연결하여 행위 다양성을 촉진하고 수동 보상 설계를 없애는 새로운 훈련 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 모달 가상 에이전트의 발전은 다중 모달 대형 언어 모델의 통합을 통해 상당한 진전을 이루었지만, 기존 훈련 패러다임은 행동 다양성 부족과 수동 보상 기능에 의존하는 등의 주요 문제에 직면해 있습니다.

Method: CORE는 모방과 탐색을 연결하는 코드 기반의 역자기 훈련 프레임워크로, 전문가 시연에서 자동으로 보상 함수를 추론하는 의미 코드 추상을 도입하고, 전략 그래프 확장을 통해 도메인 내 행동 다양성을 강화하며, 성공적 및 실패한 궤적을 활용하여 도메인 외 행동 다양성을 확장하는 궤적 안내 외삽을 구현합니다.

Result: Web 및 Android 플랫폼에서의 실험 결과, CORE는 전반적인 성능과 일반화 능력을 현저하게 향상시켰습니다.

Conclusion: CORE는 강력한 가상 에이전트를 구축하기 위한 견고하고 일반화 가능한 훈련 패러다임으로서의 잠재력을 강조합니다.

Abstract: The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: 기존의 신용 위험 결정 시스템의 한계를 극복하기 위해, 본 논문은 AI 에이전트가 인간 관찰자와 독립적으로 동적 신용 세계를 인식하는 에이전틱 AI 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 재무 서비스의 디지털화가 가속화됨에 따라 자율적이고 투명하며 실시간으로 신용 위험 결정을 내리는 시스템에 대한 수요가 증가하고 있다.

Method: 본 논문은 AI 에이전트가 인간의 개입 없이 대출자의 위험 프로필을 평가하기 위해 다중 에이전트 시스템과 강화 학습, 자연어 추론, 설명 가능한 AI 모듈, 실시간 데이터 흡수 파이프라인을 도입한다.

Result: 연구 결과, 결정 속도, 투명성 및 응답성이 기존의 신용 스코어링 모델보다 우수하다.

Conclusion: 제안된 시스템은 신용 분석 혁신의 큰 잠재력을 지니며, 향후 연구는 동적 규제 준수 촉진자, 새로운 에이전트 팀워크, 적대적 강건성 및 국제적인 신용 생태계에서의 대규모 구현에 초점을 맞추어야 한다.

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [24] [Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models](https://arxiv.org/abs/2601.00848)
*Ron F. Del Rosario*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 AI 워크플로에서 시간 공격 패턴을 감지하기 위해 언어 모델을 미세 조정하는 개방적으로 문서화된 방법론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 시간 공격 패턴을 감지하기 위해 다중 에이전트 AI 워크플로에 적용할 수 있는 정확한 언어 모델의 필요성.

Method: OpenTelemetry 트레이스 분석을 이용하여 80,851개의 예시와 35,026개의 합성 OpenTelemetry 트레이스를 포함하는 데이터셋을 구축하고, ARM64 하드웨어에서 QLoRA 미세 조정을 실시하였다.

Result: 맞춤형 벤치마크 정확도가 42.86%에서 74.29%로 증가하여 통계적으로 유의미한 31.4포인트 상승을 기록하였다.

Conclusion: 이 연구는 실무자가 자신의 위협 환경에 맞는 맞춤형 보안 모델을 구축할 수 있도록 하는 재현 가능한 첫 번째 프레임워크를 확립하였다.

Abstract: We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.

</details>


### [25] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: 이 연구는 정치 선거 중 소셜 미디어에서의 상호작용을 시뮬레이션하기 위한 ElecTwit 프레임워크를 소개하며, 여러 대리인 시스템 내에서의 설득 과정을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 게임 기반 시뮬레이션의 한계를 극복하고 더 현실적인 환경에서 실험을 진행하기 위해 이 연구를 수행하였다.

Method: 정치 선거 중 소셜 미디어 플랫폼의 상호작용을 모의하는 멀티 에이전트 시스템 내에서 설득 기술을 연구하는 시뮬레이션 프레임워크인 ElecTwit를 소개하였다.

Result: 25개의 특정 설득 기술이 대부분의 실험된 LLM에서 포괄적으로 사용되었음을 관찰하였으며, 이전에 보고된 범위를 초과하는 결과를 보였다.

Conclusion: 이 연구는 현실 세계에서 설득력 있는 LLM 에이전트를 평가하기 위한 기초를 제공하여 정렬을 보장하고 위험한 결과를 방지한다.

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [26] [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321)
*Rong Zhou,Dongping Chen,Zihan Jia,Yao Su,Yixin Liu,Yiwen Lu,Dongwei Shi,Yue Huang,Tianyang Xu,Yi Pan,Xinliang Li,Yohannes Abate,Qingyu Chen,Zhengzhong Tu,Yu Yang,Yu Zhang,Qingsong Wen,Gengchen Mai,Sunyang Fu,Jiachen Li,Xuyu Wang,Ziran Wang,Jing Huang,Tianming Liu,Yong Chen,Lichao Sun,Lifang He*

Main category: cs.AI

TL;DR: 디지털 트윈은 인공지능 기술의 통합을 통해 지능적이고 자율적인 존재로 발전하였다. 본 논문은 디지털 트윈 생애주기 전반에 걸친 AI 통합을 체계적으로 특징짓는 네 단계의 통합 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 디지털 트윈 생애주기를 통해 AI 방법론의 통합을 체계화하여 더 효율적인 관리와 활용을 도모하고자 하였다.

Method: 물리적 트윈 모델링, 디지털 트윈으로의 동기화, 예측 모델링 및 최적화를 통한 개입, 대규모 언어 모델을 통한 자율 관리의 네 가지 단계를 포함한다.

Result: 뉴스와 스마일톤을 기반으로 한 물리 기반 모델링과 데이터 기반 학습의 시너지를 분석하며, 기존 솔버에서 물리 정보 모델로의 전환을 강조하였다.

Conclusion: AI 주도의 디지털 트윈 시스템의 책임 있는 방향성을 제시하며 다양한 응용 분야의 공통적인 도전 과제를 식별하였다.

Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.

</details>


### [27] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: JiSi라는 새로운 프레임워크가 오픈 소스 LLM들의 협업을 통해 Gemini-3-Pro를 초월함을 입증했다.


<details>
  <summary>Details</summary>
Motivation: 오픈 소스 LLM 간의 협업이 단일한 스케일링 너머로 나아갈 수 있음을 탐구하기 위해.

Method: 세 가지 혁신, 즉 의미 정보와 문제 난이도를 모두 포착하는 혼합 라우팅, 집합 기반 집계 선택 및 동적 라우팅-집계 스위치를 도입하여 공동 지능의 잠재력을 최대한 발휘하도록 설계된 JiSi 프레임워크.

Result: JiSi는 47%의 비용으로 10개의 오픈 소스 LLM을 조정하여 Gemini-3-Pro를 초월하며, 주류 기준선을 능가하는 성과를 보였다.

Conclusion: 집단 지능은 인공지능 일반(AGI)에 대한 새로운 경로를 제시한다.

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [28] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: 이 논문은 KGCE라는 새로운 벤치마크 플랫폼을 제안하여 교육 환경에서 크로스 플랫폼 작업 수행 능력을 향상시키고자 하며, 기존의 한계점을 극복하기 위해 지식 기반 강화와 이중 그래프 평가 프레임워크를 통합하였다.


<details>
  <summary>Details</summary>
Motivation: 자동화 에이전트에서 다중 모달 대규모 언어 모델(MLMs)의 빠른 채택으로 인해 교육 환경의 크로스 플랫폼 작업 실행 능력이 크게 주목받고 있다.

Method: KGCE는 데이터 세트를 구성하고 이중 그래프 평가 프레임워크를 도입하여 작업을 여러 하위 목표로 분해하고 완료 상태를 확인하여 세밀한 평가 메트릭을 제공하는 벤치마킹 플랫폼이다.

Result: KGCE를 통해 104개의 교육 관련 작업을 포함한 데이터 세트를 구축하고, 학교별 소프트웨어에 특화된 지식 기반을 통합한 향상된 에이전트 시스템을 개발하였다.

Conclusion: KGCE는 교육 에이전트 벤치마킹을 위한 강력한 도구로, 기존 에이전트의 실행 병목 현상을 극복하는 데 기여할 수 있다.

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [29] [Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making](https://arxiv.org/abs/2601.01522)
*Danial Amin*

Main category: cs.AI

TL;DR: 비용을 고려한 베이지안 다중 LLM 오케스트레이션 프레임워크를 제안하여 여러 언어 모델을 활용한 의사결정에서 비용을 최적화하고 공정성을 높인다.


<details>
  <summary>Details</summary>
Motivation: LLM은 비대칭 오류 비용이 있는 환경에서 자율적인 의사결정 에이전트로 점점 더 많이 배치되고 있으며, 이는 채용, 의료 triage 및 사기 탐지와 같은 분야에서 중요한 문제이다.

Method: 비용을 인식한 다중 LLM 오케스트레이션 프레임워크를 제안하고, 이를 통해 각 후보 상태에 대한 가능성을 대조적 프롬프트를 통해 끌어내고, 다양한 모델의 강건한 통계로 집계하며 새 증거가 도착할 때 베이즈 규칙에 따라 믿음을 업데이트한다.

Result: 1000개의 이력을 사용한 실험에서 다섯 개의 LLM을 통해 총 비용을 294000 USD 절감하고 인구 통계학적 공정성을 45% 향상시켰다.

Conclusion: 다중 LLM 집계가 51%, 순차적 업데이트가 43%, 불일치로 촉발된 정보 수집이 20%의 비용 절감에 기여했으며, 이는 올바른 확률적 기초의 이론적 이점과 일치한다.

Abstract: Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

</details>


### [30] [Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix](https://arxiv.org/abs/2601.01532)
*Fanzhe Fu*

Main category: cs.AI

TL;DR: AGI 개발 과정에서 기존 평가 방식은 지식의 깊이를 수치화하지 못하는 위기에 직면해 있다. 본 연구에서는 시스템 2 추론 모델에서 '인지적 확신'을 수량화하는 데 초점을 맞춘다.


<details>
  <summary>Details</summary>
Motivation: AGI로 발전하는 과정에서 기존 평가 체계가 지식 깊이 측정에 실패하고 있다.

Method: Tikhonov 정규화를 활용하여 판별자의 혼란 행렬을 역전시키는 신경물리학 프레임워크인 Project Aletheia를 제안하고, 합성 프록시 프로토콜을 구현하여 방법론을 검증한다.

Result: 2025 기준선에 대한 예비 연구 결과, 추론 모델이 '인지적 완충기'로 작용하나, 적대적 압박 하에 '방어적 과다 사고'를 보일 수 있음을 발견했다.

Conclusion: 이 연구는 AI의 과학적 완전성을 측정하는 청사진으로 기능한다.

Abstract: In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify "Cognitive Conviction" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a "cognitive buffer," they may exhibit "Defensive OverThinking" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.

</details>


### [31] [CaveAgent: Transforming LLMs into Stateful Runtime Operators](https://arxiv.org/abs/2601.01569)
*Maohao Ran,Zhenglin Wan,Cooper Lin,Yanting Zhang,Hongyu Xin,Hongwei Fan,Yibo Xu,Beier Luo,Yaxin Zhou,Wangbo Zhao,Lijie Yang,Lang Feng,Fuchao Yang,Jingxuan Wu,Yiqiao Huang,Chendong Ma,Dailing Jiang,Jianbo Deng,Sihui Han,Bo An,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: CaveAgent는 LLM 기반 에이전트의 텍스트 중심 접근 방식을 개선하여 LLM을 실행 운영자로 변환하는 프레임워크로, 상태 관리를 경량 세맨틱 스트림과 지속적인 Python 런타임 스트림으로 분리하여 복잡한 작업을 처리한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 복잡한 작업 수행에서 더 능력을 갖추고 있지만, 기존의 에이전트 시스템이 텍스트 중심의 패러다임에 제약을 받고 있는 문제를 해결하고자 한다.

Method: CaveAgent는 이중 스트림 맥락 아키텍처를 통해 상태 관리를 세맨틱 스트림과 Python 런타임 스트림으로 분리하고, 코드 생성을 활용하여 상호 의존적인 하위 작업을 효율적으로 해결하며, 상태 관리 기능을 도입한다.

Result: CaveAgent는 소매 작업에서 10.5%의 성공률 개선을 이루었고, 멀티 턴 시나리오에서 총 토큰 소비를 28.4% 줄였다.

Conclusion: CaveAgent는 데이터 집약적 작업에서 직접 변수 저장 및 검색을 통해 59%의 토큰 소비를 줄이며, JSON 기반 및 코드 기반 에이전트가 겪는 컨텍스트 오버플로우 실패를 처리할 수 있는 능력을 갖추었다.

Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from "LLM-as-Text-Generator" to "LLM-as-Runtime-Operator." We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\% success rate improvement on retail tasks and reduces total token consumption by 28.4\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.

</details>


### [32] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: AI 에이전트는 자연어 의도와 실제 세계 계산 사이의 인터페이스로 급속히 발전하고 있다. 이 조사는 AI 에이전트 아키텍처의 새로운 지형을 종합하며, 주요 구성 요소, 오케스트레이션 패턴 및 배포 설정을 조직화하고 있으며, 주요 설계 트레이드오프와 평가의 복잡성을 논의한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 자연어 의도와 실제 세계 계산 간의 실용적인 인터페이스로 발전하는 배경을 연구

Method: AI 에이전트 아키텍처를 여러 요소(사고 및 추론, 계획 및 제어, 도구 호출 및 환경 상호작용)로 분류하고, 기존 연구를 통합하여 통합된 분류법을 제시

Result: 선택된 구성 요소, 패턴 및 설정에 대한 분석과 밀접한 설계 트레이드오프를 논의

Conclusion: 측정 및 벤치마킹 관행을 요약하고 도구 행동을 위한 검증 및 안전장치, 메모리 관리, 의사 결정 해석 가능성 등 여러 개방된 과제를 지적

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [33] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: 본 논문에서는 비정형 임상 내러티브로부터 지식 그래프(KG)를 구축하는 새로운 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 접근 방식은 구조화된 입력에 의존하고 있고 사실적 정확성과 의미적 일관성의 강력한 검증이 부족하여 특히 종양학에서 문제가 됩니다.

Method: 자유 텍스트로부터 임상 KG를 구축하기 위해 다중 에이전트 프롬프트와 스키마 제약을 받는 Retrieval-Augmented Generation (KG-RAG) 전략을 사용한 엔드 투 엔드 프레임워크를 소개합니다.

Result: 우리의 방법은 해석 가능하고, SPARQL 호환 가능하며, 임상적으로 기반한 지식 그래프를 생성하며, 금표준 주석에 의존하지 않습니다. 실험 결과는 정밀도, 관련성 및 온톨로지 준수에서 기준 방법에 비해 일관된 향상을 보여줍니다.

Conclusion: 이 프레임워크는 정적 그래프 구축을 넘어서 지속적인 개선과 자기 감독 평가를 지원하여 그래프 품질의 반복적 향상을 가능하게 합니다.

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [34] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델을 이용한 자율 에이전트의 작업 성능 향상을 위한 에이전트 프레임워크를 제안하며, 실제 경험에 근거한 세 가지 주요 혁신을 포함하고 있다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM) 기반 에이전트 시스템의 발전과 함께 자율 에이전트의 작업 성능, 특히 맥락 이해, 도구 사용 및 응답 생성의 향상이 점점 더 중요해지고 있다.

Method: 이 논문에서는 에이전트의 상태와 작업 목표에 맞춘 적응형 프롬프트 생성 전략, 사용자 의도와 맥락에 따라 도구를 분류하고 유도하는 컨텍스트 인식 도구 조정 모듈, 그리고 세션 메모리, 작업 이력, 외부 요약을 통합하여 동적 요약 및 압축을 통해 관련성과 효율성을 개선하는 층별 메모리 메커니즘을 특성으로 하는 에이전트 프레임워크를 소개한다.

Result: 실험 결과, 작업 정확도가 20% 향상되었고, 토큰 비용, 응답 지연 시간 및 호출 실패가 감소하였다.

Conclusion: 이 프레임워크는 Jenius에 배포되어 있으며, 견고하고 프로토콜 호환 자율 에이전트를 위한 경량화되고 확장 가능한 솔루션을 제공한다.

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [35] [Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence](https://arxiv.org/abs/2601.01875)
*Kewen Cao,Jianxu Chen,Yongbing Zhang,Ye Zhang,Hongxiao Wang*

Main category: cs.AI

TL;DR: 이 논문에서는 자동화된 병리 이미지 분석의 해석 가능성과 결정 추적성 향상을 위한 SQL 중심의 에이전트 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 임상 진단에서 자동화된 병리 이미지 분석은 핵심적이지만, 임상의들은 여전히 모델의 결정에 영향을 미치는 슬라이드 특징과 그 이유에 대해 질문한다.

Method: 인간이 해석할 수 있는 세포 특징을 추출한 후, 특징 추론 에이전트가 특징 테이블에서 SQL 쿼리를 작성하고 실행하여 시각적 증거를 정량적 발견으로 집계한다.

Result: 두 개의 병리 시각 질문 응답 데이터셋에 대한 광범위한 실험 결과, 본 방법이 해석 가능성과 결정 추적성을 향상시키고 세포 측정을 진단 결론에 연결하는 실행 가능한 SQL 추적을 생성함을 보여준다.

Conclusion: 우리의 방법은 임상의들이 병리학적 진단을 내릴 때 측정 가능한 관찰에 기반하여 정당화하는 방식을 반영한다.

Abstract: Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.

</details>


### [36] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: OpenSocInt는 다중 모드 사회 상호 작용을 위한 시뮬레이터와 사회적 에이전트를 학습할 수 있는 모듈식 아키텍처를 제공하는 오픈 소스 소프트웨어 패키지이다.


<details>
  <summary>Details</summary>
Motivation: 사회적 에이전트의 학습과 다중 모드 상호 작용에 대한 연구가 필요하다.

Method: 실험 프로토콜을 기반으로 한 사회적 내비게이션 작업을 통해 소프트웨어 패키지를 설명하고 관심을 보여주었다.

Result: 다양한 인지적 특징, 그 인코딩 및 융합의 사용과 다양한 에이전트의 사용을 탐색할 수 있다.

Conclusion: 이 소프트웨어는 https://gitlab.inria.fr/robotlearn/OpenSocInt/에서 GPL 라이센스 하에 공개적으로 사용할 수 있다.

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [37] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: ChaosBench-Logic은 LLM의 논리적 사고 능력을 평가하기 위한 기준으로, 혼돈 동역학 시스템에서의 정확성을 검증하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 차량 언어 모델은 자연어 작업에 뛰어나지만, 논리 및 상징적 추론이 필요한 분야에서는 여전히 약점을 보인다.

Method: ChaosBench-Logic은 30개의 다양한 동역학 시스템에 대해 통합된 1차 논리(FOL) 온톨로지를 사용하여 LLM의 추론 평가를 수행한다.

Result: 최초의 실험 결과에 따르면, GPT-4와 같은 최전선 LLM이 91-94%의 항목 정확도를 기록했지만, 조합 항목에서는 0%를 기록하였다.

Conclusion: ChaosBench-Logic은 LLM의 결함을 진단하기 위한 엄격한 테스트베드를 제공하고, 과학적 추론을 개선하는 신경-기호 접근법 개발의 기초를 마련한다.

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [38] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: 정신 건강 지원을 위한 개인 정보 보호 대형 언어 모델(MindChat)과 합成 다중 회차 상담 데이터셋(MindCorpus)을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 정신 건강 지원에 대한 대형 언어 모델의 가능성을 탐구하나, 실제 상담 대화의 부족성과 민감성으로 인해 모델 훈련이 제한된다.

Method: 다중 에이전트 역할극 프레임워크를 사용하여 합성 다중 회차 상담 데이터셋을 구축하고, 심리학적 전문 지식과 상담 기법을 통합하는 이중 폐쇄 루프 피드백 설계를 통해 고품질 상담 데이터를 합성한다.

Result: MindCorpus는 훈련 효과성을 향상시키고, MindChat은 기존의 일반 및 상담 지향 LLM과의 경쟁력을 보여준다.

Conclusion: MindChat은 회원 추론 공격 하에서도 개인 정보 유출을 줄이면서 성능을 유지한다.

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [39] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: 이 논문은 의료 AI의 설명 가능성 및 드문 클래스 신뢰성 문제를 해결하기 위해 XAIMeD라는 설명 가능한 의료 AI 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 의료 AI에서 딥 모델이 실제 세계의 분포 변화에 적응하지 못하고 드문 임상 조건에 대한 편향을 드러내는 문제를 해결하고자 합니다.

Method: XAIMeD는 통합된 신경-상징 아키텍처를 통해 임상적으로 정확한 전문 지식을 딥 러닝에 통합하여 드문 클래스 민감성과 투명한 임상 해석을 개선합니다.

Result: XAIMeD는 다양한 모달리티에서 4개의 도전적인 과제에서 성능 개선을 보여주며 교차 도메인 일반화에서 6% 증가, 드문 클래스 F1 점수에서 10% 개선을 달성했습니다.

Conclusion: XAIMeD는 다중 모달 의료 AI에 대한 원칙적이고 임상적으로 신뢰할 수 있으며 해석 가능한 접근법을 제공합니다.

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [40] [Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management](https://arxiv.org/abs/2601.02061)
*Faizan Ahmed,Aniket Dixit,James Brusey*

Main category: cs.AI

TL;DR: 딥 강화 학습 에이전트는 높은 주파수의 제어 행동을 보이며, 이는 에너지 소비와 기계 마모를 초래해 실제 세계에 배포하는 데 어려움을 초래한다.


<details>
  <summary>Details</summary>
Motivation: 딥 강화 학습 에이전트의 불규칙하고 높은 주파수의 제어 행동을 해결하여 실제 적용 가능성을 높이고자 했다.

Method: 고차 미분 패널티를 통한 동작 부드러움 규제를 체계적으로 조사하고, 이를 통해 연속 제어 벤치마크에서 이론적 이해를 이루고, 실제 적용을 통해 검증하였다.

Result: 네 가지 연속 제어 환경에서의 종합 평가에서 세 번째 차 미분 패널티가 일관되게 뛰어난 부드러움을 달성하며 경쟁력 있는 성능을 유지했다.

Conclusion: 본 연구는 에너지-중요 응용 분야에서 RL 최적화와 운영 제약 간의 효과적인 다리로서 고차 동작 규제를 확립했다.

Abstract: Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.

</details>


### [41] [EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning](https://arxiv.org/abs/2601.02163)
*Chuanrui Hu,Xingze Gao,Zuyi Zhou,Dannong Xu,Yi Bai,Xintong Li,Hui Zhang,Tong Li,Chong Zhang,Lidong Bing,Yafeng Deng*

Main category: cs.AI

TL;DR: EverMemOS는 대화형 상호작용 동안 일관된 행동을 유지하기 위해 자가 조직화된 메모리 운영 체제를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 상호작용 에이전트로 점점 더 많이 사용되고 있지만, 제한된 컨텍스트 창으로 인해 긴 상호작용 중 일관된 행동을 유지하기가 어렵습니다.

Method: EverMemOS는 계산 메모리를 위한 엔그램 영감을 받은 생애 주기 및 대화 스트림을 메모리 셀로 변환하는 에피소드 추적 형성 기능을 구현합니다.

Result: LoCoMo 및 LongMemEval에서 실시한 실험 결과, EverMemOS는 메모리 보강 추론 작업에서 최첨단 성능을 달성합니다.

Conclusion: PersonaMem v2에 대한 프로파일 연구 및 사용자 프로파일링 및 예측과 같은 채팅 지향 기능을 보여주는 질적 사례 연구도 보고합니다.

Abstract: Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

</details>


### [42] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: 대형 언어 모델에 있어 투명한 추론 과정이 안전성에 필수적이다. 본 연구는 Project Ariadne라는 새로운 XAI 프레임워크를 소개하며, 이는 구조적 인과 모델과 반사실 논리를 사용하여 추론의 인과적 무결성을 감사한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM) 에이전트가 자율적 의사결정을 수행함에 따라 그들의 추론 과정의 투명성이 중요해졌다.

Method: Project Ariadne는 구조적 인과 모델(SCMs)과 반사실 논리를 활용하여 중간 추론 노드에서 	extbf{hard interventions}를 수행한다.

Result: 최신 모델에 대한 실증적 평가는 	extit{Faithfulness Gap}이 지속적으로 나타남을 보여준다. 우리는 	extbf{Causal Decoupling}이라는 실패 모드를 정의하고 탐지하였다.

Conclusion: 현재의 에이전트 아키텍처는 본질적으로 불충분한 설명에 취약하며, 모델 Action과의 정합성을 위한 새로운 벤치마크인 Ariadne Score를 제안한다.

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [43] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: Falcon-H1R는 7B 매개변수의 추론 최적화 모델로, 작은 언어 모델에서도 경쟁력 있는 추론 성능을 달성할 수 있음을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 작은 언어 모델(SLM)을 사용하여 경쟁력 있는 추론 성능을 달성할 가능성을 탐구하기 위해.

Method: 효율적인 SFT 및 RL 스케일링을 통해 신중한 데이터 큐레이션 및 맞춤형 훈련 전략을 채택하여 모델의 매개변수 효율성을 최대화 하였다.

Result: Falcon-H1R은 다양한 추론 집합 benchmark에서 2배에서 7배 큰 SOTA 추론 모델과 일치하거나 뛰어넘는 성능을 보였다.

Conclusion: 작은 모델이 표적 훈련 및 아키텍처 선택을 통해 강력하고 확장 가능한 추론 성능을 제공할 수 있음을 입증하였다.

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [44] [Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai](https://arxiv.org/abs/2601.01090)
*Erica Coppolillo,Luca Luceri,Emilio Ferrara*

Main category: cs.MA

TL;DR: 이 연구는 LLM 기반 에이전트의 독성 수용을 분석하고, 독성 자극과 반응 간의 관계를 조사하여 독성 행동의 예측 가능성을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트가 사회적 생태계에서 상호작용할 때 유해한 콘텐츠가 에이전트 행동에 미치는 영향을 이해하고자 합니다.

Method: Chirper.ai 플랫폼에서 LLM 기반 에이전트의 상호작용을 연구하며, 자극(게시물)과 반응(댓글) 간의 관계를 모델링합니다.

Result: 독성 자극이 있을 때 독성 반응이 더 자주 발생하지만, 상당수의 독성은 외부 노출 없이 자발적으로 발생한다는 것을 밝혔습니다.

Conclusion: LLM 에이전트의 배치에서 노출이 중요한 위험 요소임을 확인했으며, 만난 콘텐츠를 모니터링하는 것이 유해한 행동을 감시하고 완화하는 효과적인 메커니즘이 될 수 있음을 시사합니다.

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms.
  We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content.
  These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.

</details>


### [45] [CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty](https://arxiv.org/abs/2601.01581)
*Rishav Sen,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Mark Bailey,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: cs.MA

TL;DR: 전기차(EV)의 성장으로 인한 V2B 설정에서 건물 운영자와 운전자의 갈등을 해결하기 위한 협상 기반 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전기차의 보급으로 인해 비조정 충전으로 인한 높은 에너지 비용을 가진 건물 운영자와 편리함과 완전한 충전을 우선시하는 운전자의 갈등 해결 필요.

Method: 운전자가 출발 시간이나 요청한 충전 상태(Soc)에 대해 적당한 유연성을 갖도록 인센티브 기반 옵션을 제공하는 협상 프레임워크를 제안합니다.

Result: 시뮬레이션 결과, 우리의 협상 프로토콜은 건물 운영자의 비용을 최적화된 비협상 스마트 충전 정책에 비해 3.5	ext{%} 이상 줄이고, 사용자의 충전 비용을 22	ext{%} 낮추는 상호 이익이 되는 결과를 도출했습니다.

Conclusion: 운영자와 EV 사용자 목표를 일치시킴으로써, 우리의 프레임워크는 에너지 및 이동성 시스템 간의 전략적 다리를 제공하여, EV 충전을 운영 마찰의 원천에서 협력 및 공동 절약의 플랫폼으로 변화시킵니다.

Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.

</details>


### [46] [ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring](https://arxiv.org/abs/2601.01831)
*Aniket Wattamwar,Sampson Akwafuo*

Main category: cs.MA

TL;DR: 이 논문에서는 전염병 감시를 위한 ARIES라는 특화된 다중 에이전트 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전 세계 건강 감시에서 지식 격차 문제에 직면하고 있으며, 일반적인 AI가 확산되고 있지만, 높은 위험의 전염병 분야에는 적합하지 않다는 점을 문제로 삼고 있습니다.

Method: ARIES는 위계적 명령 구조를 기반으로 하여 WHO, CDC 및 동료 검토 연구 논문을 자율적으로 쿼리할 수 있는 하위 에이전트 무리를 조정하기 위해 GPT를 사용합니다.

Result: 배치된 ARIES는 감시 데이터를 자동으로 추출하고 논리적으로 종합하여, 근실시간으로 신흥 위협과 신호의 발산을 식별합니다.

Conclusion: 임무 특정 에이전트 군집이 일반 모델보다 우수하다는 것을 증명하여, 차세대 발병 대응과 글로벌 건강 정보에 대해 더욱 강력하고 확장 가능한 솔루션을 제공합니다.

Abstract: Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [47] [The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models](https://arxiv.org/abs/2601.00867)
*Giuseppe Canale,Kashyap Thimmaraju*

Main category: cs.CR

TL;DR: 본 논문은 대규모 언어 모델이 사이버 보안에서 인간의 심리적 취약점을 물려받았음을 보여주고, 이를 해결하기 위한 심리적 방화벽의 필요성을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 조직의 중요한 기능에 통합되고 있으며, 이 모델들이 가진 심리적 취약점에 대한 이해가 부족하다.

Method: Cybersecurity Psychology Framework(CPF)를 적용하여 비인간 인지 에이전트에 대한 심리적 취약점을 분석하고, Synthetic Psychometric Assessment Protocol(SYSNAME)을 통해 이를 적대적 시나리오로 전환한다.

Result: 초기 가설 검정을 통해, 모델들이 전통적인 jailbreak 공격에 강력한 방어력을 보이는 반면, 권위 조작, 시간 압박 및 인지 실패 모드와 유사한 공격에 취약한 패턴이 나타났다.

Conclusion: 보안 커뮤니티는 적대적 환경에서 운영되는 AI 에이전트를 보호하기 위해 심리적 방화벽을 개발해야 한다.

Abstract: Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions, including Security Operations Centers (SOCs), financial systems, and infrastructure management. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and data exfiltration. We argue this focus is catastrophically incomplete. LLMs, trained on vast corpora of human-generated text, have inherited not merely human knowledge but human \textit{psychological architecture} -- including the pre-cognitive vulnerabilities that render humans susceptible to social engineering, authority manipulation, and affective exploitation. This paper presents the first systematic application of the Cybersecurity Psychology Framework (\cpf{}), a 100-indicator taxonomy of human psychological vulnerabilities, to non-human cognitive agents. We introduce the \textbf{Synthetic Psychometric Assessment Protocol} (\sysname{}), a methodology for converting \cpf{} indicators into adversarial scenarios targeting LLM decision-making. Our preliminary hypothesis testing across seven major LLM families reveals a disturbing pattern: while models demonstrate robust defenses against traditional jailbreaks, they exhibit critical susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks that mirror human cognitive failure modes. We term this phenomenon \textbf{Anthropomorphic Vulnerability Inheritance} (AVI) and propose that the security community must urgently develop ``psychological firewalls'' -- intervention mechanisms adapted from the Cybersecurity Psychology Intervention Framework (\cpif{}) -- to protect AI agents operating in adversarial environments.

</details>


### [48] [Device-Native Autonomous Agents for Privacy-Preserving Negotiations](https://arxiv.org/abs/2601.00911)
*Joyjit Roy*

Main category: cs.CR

TL;DR: 본 연구는 사용자 하드웨어에서 작동하는 자율 AI 에이전트 시스템을 제안하여 보험 및 B2B 상거래에서 개인 정보를 보호하는 자동 협상 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 보험 및 B2B 상거래에서 자동화된 협상이 편리함과 프라이버시 간의 절충을 강요하는 현재 시스템의 한계를 개선하고자 하였습니다.

Method: 사용자 하드웨어에서만 작동하는 장치 고유 자율 AI 에이전트 시스템을 제안하여 실시간 협상 수행, 민감한 제약 사항 유지, 제로 지식 증명 통합 및 정제된 세계 모델을 통한 고급 온디바이스 추론을 지원합니다.

Result: 보험 및 B2B 조달 시나리오에서 평균 87%의 성공률, 클라우드 기준에 비해 2.4배 향상된 지연 시간, 제로 지식 증명을 통한 강력한 프라이버시 보존을 보여주었습니다.

Conclusion: 이 결과는 개인정보가 민감한 금융 분야에서 신뢰할 수 있는 자율 에이전트의 기초를 마련합니다.

Abstract: Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.

</details>


### [49] [CuFuzz: Hardening CUDA Programs through Transformation and Fuzzing](https://arxiv.org/abs/2601.01048)
*Saurabh Singh,Ruobing Han,Jaewon Lee,Seonjin Na,Yonghae Kim,Taesoo Kim,Hyesoon Kim*

Main category: cs.CR

TL;DR: CuFuzz는 CUDA 프로그램에 퍼징 지원을 도입하여 GPU 보안 연구의 중요한 격차를 해소하는 새로운 컴파일러-런타임 공동 설계 솔루션이다.


<details>
  <summary>Details</summary>
Motivation: GPU 보안 및 신뢰성 문제는 점점 더 중요해지고 있으며, CUDA의 메모리 안전성이 부족하다는 것이 심각한 취약점을 초래할 수 있다.

Method: CuFuzz는 GPU 프로그램을 효과적인 퍼징 테스트를 가능하게 하기 위해 컴파일러 IR 수준 변환을 사용하여 CPU 프로그램으로 변환한다.

Result: CuFuzz는 널리 사용되는 벤치마크에서 122개의 보안 취약점을 발견하였다.

Conclusion: 이 연구는 GPU 가속 응용 프로그램의 보안과 신뢰성을 향상시키는데 기여하고, PREX 및 AXIPrune과 같은 두 가지 컴파일러-런타임 공동 최적화를 통해 평균 32배의 처리량 향상을 달성하였다.

Abstract: GPUs have gained significant popularity over the past decade, extending beyond their original role in graphics rendering. This evolution has brought GPU security and reliability to the forefront of concerns. Prior research has shown that CUDA's lack of memory safety can lead to serious vulnerabilities. While fuzzing is effective for finding such bugs on CPUs, equivalent tools for GPUs are lacking due to architectural differences and lack of built-in error detection. In this paper, we propose CuFuzz, a novel compiler-runtime co-design solution to extend state-of-the-art CPU fuzzing tools to GPU programs. CuFuzz transforms GPU programs into CPU programs using compiler IR-level transformations to enable effective fuzz testing. To the best of our knowledge, CuFuzz is the first mechanism to bring fuzzing support to CUDA, addressing a critical gap in GPU security research. By leveraging CPU memory error detectors such as Address Sanitizer, CuFuzz aims to uncover memory safety bugs and related correctness vulnerabilities in CUDA code, enhancing the security and reliability of GPU-accelerated applications. To ensure high fuzzing throughput, we introduce two compiler-runtime co-optimizations tailored for GPU code: Partial Representative Execution (PREX) and Access-Index Preserving Pruning (AXIPrune), achieving average throughput improvements of 32x with PREX and an additional 33% gain with AXIPrune on top of PREX-optimized code. Together, these optimizations can yield up to a 224.31x speedup. In our fuzzing campaigns, CuFuzz uncovered 122 security vulnerabilities in widely used benchmarks.

</details>


### [50] [MCP-SandboxScan: WASM-based Secure Execution and Runtime Analysis for MCP Tools](https://arxiv.org/abs/2601.01241)
*Zhuoran Tan,Run Hao,Jeremy Singer,Yutian Tang,Christos Anagnostopoulos*

Main category: cs.CR

TL;DR: MCP-SandboxScan은 안전하게 신뢰할 수 없는 도구를 실행하고 외부 입력 노출을 감사할 수 있는 경량 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 도구가 강화된 LLM 에이전트는 새로운 보안 위험을 초래하며, 실행 중인 도구는 런타임 전용 행동을 초래할 수 있다.

Method: MCP-SandboxScan은 신뢰할 수 없는 도구를 WebAssembly/WASI 샌드박스 내에서 안전하게 실행하고 외부 입력 노출에 대한 감사 가능한 보고서를 생성한다.

Result: MCP-SandboxScan은 런타임 출력에서 LLM 관련 낙인을 추출하고, 환경 값 및 마운트된 파일 내용을 기반으로 외부 입력 후보를 생성하며, 스니펫 기반 부분 문자열 일치를 통해 소스를 낙인에 연결할 수 있다.

Conclusion: 사례 연구를 통해 MCP-SandboxScan은 피연산자 입력이 프롬프트/메시지나 도구 반환 페이로드에 나타날 때 출처 증거를 색출할 수 있음을 보여준다.

Abstract: Tool-augmented LLM agents raise new security risks: tool executions can introduce runtime-only behaviors, including prompt injection and unintended exposure of external inputs (e.g., environment secrets or local files). While existing scanners often focus on static artifacts, analyzing runtime behavior is challenging because directly executing untrusted tools can itself be dangerous. We present MCP-SandboxScan, a lightweight framework motivated by the Model Context Protocol (MCP) that safely executes untrusted tools inside a WebAssembly/WASI sandbox and produces auditable reports of external-to-sink exposures. Our prototype (i) extracts LLM-relevant sinks from runtime outputs (prompt/messages and structured tool-return fields), (ii) instantiates external-input candidates from environment values, mounted file contents, and output-surfaced HTTP fetch intents, and (iii) links sources to sinks via snippet-based substring matching. Case studies on three representative tools show that MCP-SandboxScan can surface provenance evidence when external inputs appear in prompt/messages or tool-return payloads, and can expose filesystem capability violations as runtime evidence. We further compare against a lightweight static string-signature baseline and use a micro-benchmark to characterize false negatives under transformations and false positives from short-token collisions.

</details>


### [51] [OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs](https://arxiv.org/abs/2601.01592)
*Xin Wang,Yunhao Chen,Juncheng Li,Yixu Wang,Yang Yao,Tianle Gu,Jie Li,Yan Teng,Xingjun Ma,Yingchun Wang,Xia Hu*

Main category: cs.CR

TL;DR: OpenRT는 MLLM의 안전 평가를 위한 통합된 적팀 프레임워크를 제공하여 기존의 안전 취약성을 해결하고자 한다.


<details>
  <summary>Details</summary>
Motivation: MLLM을 중요한 응용 프로그램에 통합하는 것이 지속적인 안전 취약성으로 인해 방해받고 있다.

Method: OpenRT는 모델 통합, 데이터세트 관리, 공격 전략, 판단 방법, 평가 메트릭의 다섯 가지 중요한 차원에서 모듈간 분리를 가능하게 하는 적대적 커널을 도입하여 자동화된 적팀 방식의 패러다임 전환을 설계한다.

Result: 20개의 고급 모델에 대한 광범위한 응 эм리컬 연구를 통해 평균 공격 성공률이 49.14%에 달하는 등 주요 안전 격차를 드러냈다.

Conclusion: OpenRT를 오픈 소스로 제공하여 AI 안전의 개발 및 표준화를 가속화할 수 있는 지속 가능하며 확장 가능한 인프라를 제공한다.

Abstract: The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.

</details>


### [52] [Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks](https://arxiv.org/abs/2601.01673)
*Arina Kharlamova,Youcheng Sun,Ting Yu*

Main category: cs.CR

TL;DR: MOTIF는 macOS 비공식 프레임워크의 보안 분석을 위한 에이전시 프레임워크로, 런타임 메타데이터 추출, 이진 파일 검사 및 제약 조건 검사를 수행하여 Objective-C 타입 추론을 최적화한 대형 언어 모델과 통합된다.


<details>
  <summary>Details</summary>
Motivation: 사적 macOS 프레임워크는 중요한 서비스와 데몬을 기반으로 하지만 문서화되지 않고 스트립 바이너리로만 배포되어 보안 분석을 복잡하게 만든다.

Method: MOTIF는 런타임 메타데이터 추출, 이진 파일 검사 및 제약 조건 검사를 관리하는 에이전트를 통해 Objective-C 타입 추론을 위한 특별한 대형 언어 모델과 툴 보강 분석을 통합한다.

Result: MOTIF-Bench에서 MOTIF는 기본 정적 분석 도구에 비해 서명 복구를 15%에서 86%로 개선하며, 툴 사용의 정확성과 추론 안정성에서도 일관된 향상을 보인다.

Conclusion: 비공식 프레임워크에 대한 사례 연구는 재구성된 헤더가 컴파일, 링크되며 downstream 보안 연구 및 취약점 연구를 촉진한 것을 보여준다. MOTIF는 불투명한 바이너리를 분석 가능한 인터페이스로 변환하여 macOS 내부의 체계적인 감사에 대한 확장 가능한 기초를 마련한다.

Abstract: Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.

</details>


### [53] [Structural Representations for Cross-Attack Generalization in AI Agent Threat Detection](https://arxiv.org/abs/2601.01723)
*Vignesh Iyer*

Main category: cs.CR

TL;DR: 이 연구는 다단계 도구 시퀀스를 실행하는 자율 AI 에이전트의 보안 문제를 다루며, 구조적 공격과 언어적 공격에 대한 탐지의 차이를 보여준다. 기존의 대화형 토큰화는 구조적 공격에 대해 심각한 실패를 보였으나, 새로운 구조적 토큰화 방법이 도입되어 성과를 크게 개선하였다.


<details>
  <summary>Details</summary>
Motivation: 자율 AI 에이전트가 다단계 도구 시퀀스를 실행하면서 직면하는 의미론적 공격의 문제를 해결하고자 한다.

Method: 기존의 대화형 토큰화 대신 실행 흐름 패턴을 인코딩하는 구조적 토큰화를 도입한다.

Result: 구조적 토큰화는 도구 탈취에 대해 +46 AUC 포인트, 데이터 유출에 대해 +39 포인트를 개선하고, 알려지지 않은 공격에 대해 +71 포인트를 개선하였다.

Conclusion: AI 에이전트 보안은 본질적으로 구조적 문제라는 것을 강조하며, 공격의 의미론은 표면 언어가 아니라 실행 패턴에 존재한다.

Abstract: Autonomous AI agents executing multi-step tool sequences face semantic attacks that manifest in behavioral traces rather than isolated prompts. A critical challenge is cross-attack generalization: can detectors trained on known attack families recognize novel, unseen attack types? We discover that standard conversational tokenization -- capturing linguistic patterns from agent interactions -- fails catastrophically on structural attacks like tool hijacking (AUC 0.39) and data exfiltration (AUC 0.46), while succeeding on linguistic attacks like social engineering (AUC 0.78). We introduce structural tokenization, encoding execution-flow patterns (tool calls, arguments, observations) rather than conversational content. This simple representational change dramatically improves cross-attack generalization: +46 AUC points on tool hijacking, +39 points on data exfiltration, and +71 points on unknown attacks, while simultaneously improving in-distribution performance (+6 points). For attacks requiring linguistic features, we propose gated multi-view fusion that adaptively combines both representations, achieving AUC 0.89 on social engineering without sacrificing structural attack detection. Our findings reveal that AI agent security is fundamentally a structural problem: attack semantics reside in execution patterns, not surface language. While our rule-based tokenizer serves as a baseline, the structural abstraction principle generalizes even with simple implementation.

</details>
