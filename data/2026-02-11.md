<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 45]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.MA](#cs.MA) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: Emotion understanding is crucial for social AI; this paper introduces EmoReAlM benchmark and AVEm-DPO technique to improve multimodal large language models (MLLMs) for better emotion recognition while addressing challenges like spurious associations and hallucinations.


<details>
  <summary>Details</summary>
Motivation: 이 논문의 동기는 사회적으로 지능적인 에이전트를 구축하기 위해 감정 이해가 필수적이라는 점에서 출발합니다. 최근의 다중 모달 대형 언어 모델(MLLM)의 성능에도 불구하고, 감정과 관련 없는 시청각 단서 간의 스퍼리어스 연관성 및 언어 모델 기반의 텍스트 우선으로 인한 시청각 단서의 환각을 해결해야 하는 두 가지 주요 도전 과제가 남아 있습니다.

Method: 이 논문에서는 감정-단서 연관성, 환각 및 모달리티 일치를 평가하기 위해 MLLM을 평가하는 벤치마크인 EmoReAlM을 도입합니다. 또한, 시청각 입력 및 감정 중심 쿼리와 모델 응답을 조정하는 선호 최적화 기법인 AVEm-DPO를 제안합니다. 구체적으로, 우리는 스퍼리어스 연관성이나 환각을 보이는 응답 및 텍스트 프롬프트에 의해 안내되는 시청각 입력 쌍에 대한 선호를 구성합니다.

Result: DFEW, RAVDESS 및 EMER에 대한 실험 결과는 본 방법이 제안된 기준 모델의 성능을 6-19% 향상시켜 제로 샷 설정에서 상대 성능을 크게 개선함을 보여줍니다.

Conclusion: 엄격한 벤치마크와 강력한 최적화 프레임워크를 제공함으로써, 이 작업은 감정 이해 및 사회적 AI를 위한 MLLM의 원칙적인 평가와 개선을 가능하게 합니다. 코드, 모델 및 벤치마크는 https://avere-iclr.github.io에서 공개됩니다.

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [2] [Video-based Music Generation](https://arxiv.org/abs/2602.07063)
*Serkan Sulun*

Main category: cs.LG

TL;DR: EMSYNC는 입력 비디오에 맞춰 자동으로 음악을 생성하는 솔루션으로, 콘텐츠 제작자가 음악을 작곡하거나 라이센스하지 않고도 제작물을 향상시킬 수 있도록 돕는다.


<details>
  <summary>Details</summary>
Motivation: 인터넷상의 비디오 콘텐츠가 급증하면서 적합한 사운드트랙을 찾는 것이 중요한 도전 과제가 되고 있다.

Method: EMSYNC는 비디오 감정 분류기를 기반으로 하여, 프리트레인 된 심층 신경망을 활용해 특징을 추출하고 결합 레이어만 훈련하여 계산 복잡성을 줄이고 정확성을 높인다.

Result: Ekman-6와 MovieNet에서 최신 기술 수준의 결과를 얻으며, 정서 레이블이 있는 대규모 MIDI 데이터셋을 통해 감정 기반 음악 생성을 위한 MIDI 생성기를 발표한다.

Conclusion: EMSYNC는 비디오 감정 분류, 감정 기반 음악 생성, 시간 경계 조정을 결합하여 완전 자동화된 비디오 기반 음악 생성기로 부상하며, 기존 방법보다 음악의 풍부함, 정서적 정렬, 시간 동기화, 전체 선호도에서 지속적으로 우수한 성능을 보인다.

Abstract: As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.

</details>


### [3] [Attention-Driven Framework for Non-Rigid Medical Image Registration](https://arxiv.org/abs/2602.07088)
*Muhammad Zafar Iqbal,Ghazanfar Farooq Siddiqui,Anwar Ul Haq,Imran Razzak*

Main category: cs.LG

TL;DR: 이 논문은 비강체 의료 이미지 정합을 위한 새로운 주의 시스템 기반 프레임워크(AD-RegNet)를 제안하며, 큰 변형이 있는 이미지를 정확하게 정렬하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 의료 이미지 정합은 질병 진단, 치료 계획 및 영상 유도 개입에 필수적이다.

Method: 3D UNet 백본과 양방향 교차 주의 메커니즘을 결합하여 여러 스케일에서 이동 이미지와 고정 이미지 간의 대응을 설정한다.

Result: 제안된 방법은 IXI 및 DIRLab 데이터 세트에서 최첨단 방법들과 경쟁하는 성능을 보여준다.

Conclusion: 주의 기반 정합은 정합 정확도를 향상시키면서 해부학적으로 신뢰할 수 있는 변형을 보장한다.

Abstract: Deformable medical image registration is a fundamental task in medical image analysis with applications in disease diagnosis, treatment planning, and image-guided interventions. Despite significant advances in deep learning based registration methods, accurately aligning images with large deformations while preserving anatomical plausibility remains a challenging task. In this paper, we propose a novel Attention-Driven Framework for Non-Rigid Medical Image Registration (AD-RegNet) that employs attention mechanisms to guide the registration process. Our approach combines a 3D UNet backbone with bidirectional cross-attention, which establishes correspondences between moving and fixed images at multiple scales. We introduce a regional adaptive attention mechanism that focuses on anatomically relevant structures, along with a multi-resolution deformation field synthesis approach for accurate alignment. The method is evaluated on two distinct datasets: DIRLab for thoracic 4D CT scans and IXI for brain MRI scans, demonstrating its versatility across different anatomical structures and imaging modalities. Experimental results demonstrate that our approach achieves performance competitive with state-of-the-art methods on the IXI and DIRLab datasets. The proposed method maintains a favorable balance between registration accuracy and computational efficiency, making it suitable for clinical applications. A comprehensive evaluation using normalized cross-correlation (NCC), mean squared error (MSE), structural similarity (SSIM), Jacobian determinant, and target registration error (TRE) indicates that attention-guided registration improves alignment accuracy while ensuring anatomically plausible deformations.

</details>


### [4] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 에이전틱 시스템의 성능 평가에서 단일 실행의 pass@1 점수에 의존하는 것은 신뢰할 수 없는 결과를 초래할 수 있으며, 여러 실행을 통해 더 정확한 성능 추정이 필요하다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 시스템은 성능을 신뢰성 있게 평가하기 위해 다양한 실행을 거쳐야 한다는 점을 강조한다.

Method: 세 가지 모델과 두 개의 스캐폴드를 포함한 60,000개의 에이전틱 궤적을 수집하여 단일 실행의 pass@1 점수의 변동성을 분석하였다.

Result: 단일 실행의 pass@1 점수는 2.2에서 6.0 퍼센트 포인트 사이에서 변동하며, 온도가 0일 때도 표준 편차가 1.5 퍼센트 포인트를 초과하였다.

Conclusion: 정확한 성능 평가를 위해서는 여러 독립적인 실행을 통해 pass@1을 추정하고, 통계적 힘 분석을 통해 필요한 실행 수를 결정하며, 성능을 더 잘 특성화할 수 있는 pass@k 및 pass^k 지표를 고려해야 한다.

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [5] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 모델 프리 심층 강화 학습 알고리즘이 여러 어려운 작업에서 성공을 거두었지만, 실제 응용 프로그램에서 안전성 문제로 인해 위험 인식 에이전트가 필요하다. 본 논문은 엔트로픽 위험 측정에 대한 정책 기울기 방법의 이론적 정당성을 제공하며, 위험 민감한 지수 액터-비평가(rsEAC)라는 새로운 방법을 제안하여 더 안정적인 업데이트를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 모델 프리 심층 강화 학습 알고리즘은 성공을 거두었지만, 실제 응용에서 안전성 문제를 해결하기 위해 위험 인식 에이전트가 필요하다.

Method: 엔트로픽 위험 측정에 대한 정책 기울기 방법에 대한 이론적 정당성을 제공하며, 위험 민감한 지수 액터-비평가(rsEAC)라는 오프 정책 모델 프리 접근 방식을 제안한다.

Result: rsEAC는 기존 접근 방식보다 더 안정적인 추정값 업데이트를 생성하며, MuJoCo의 복잡한 위험한 지속적 작업에서 신뢰성 있는 위험 민감한 정책을 학습한다.

Conclusion: 본 논문은 rsEAC가 복잡한 상황에서도 위험 민감한 정책을 효과적으로 학습할 수 있음을 보여준다.

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [6] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

TL;DR: 본 연구는 구조적 인과 모델(SCM)의 do-Shapley 값을 효율적으로 계산하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: SCM을 통해 복잡한 동역학을 설명할 수 있는 강력한 프레임워크가 필요하다.

Method: 불가산 집합을 기준으로 하는 do-Shapley 값을 재형성하여 선형 시간으로 계산할 수 있는 방법을 제안한다.

Result: 제안된 방법을 사용하면 r개의 불가산 집합에 따라 do-Shapley 값을 정확하게 계산할 수 있으며, 쿼리 예산에 따라서 정확도를 개선할 수 있다.

Conclusion: 비모수적 식별 가능성은 모든 클래스가 아닌 d 개의 단일 톱니 coalition의 개입 효과만으로 가능하다.

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [7] [DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling](https://arxiv.org/abs/2602.07206)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.LG

TL;DR: 이 논문에서는 기계 추천 시스템에서 Softmax Loss (SL)의 한계를 극복하고 더 나은 성능을 제공하기 위해 Dual-scale Softmax Loss (DSL)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: SL은 추천 시스템에서 점점 더 많이 사용되고 있으며, 성능, 강인성 및 공정성이 뛰어난 것으로 나타났다. 그러나 암묵적 피드백에서는 단일 전역 온도와 균일하게 샘플링된 부정 예의 동등 처리가 취약한 훈련을 초래할 수 있다.

Method: DSL은 샘플링된 경쟁으로부터 효과적인 샤프니스를 추론하며, 각 학습 인스턴스 내 부정 예를 재조정하고 경쟁 강도를 기반으로 개별 예제 온도를 조정한다. 이를 통해 SL의 기하학을 보존하면서 부정 예와 예제 전반에 걸쳐 경쟁 분포를 재형성한다.

Result: DSL은 여러 대표적인 벤치마크와 백본에서 강력한 기준선에 비해 상당한 향상을 보이며, 일부 설정에서는 SL 대비 10% 이상의 개선을 기록하고, 여러 데이터 세트, 메트릭 및 백본에서 평균 6.22%의 향상을 보인다.

Conclusion: 우리는 DSL이 불확실한 인스턴스에 대한 견고한 보상 및 KL 편차를 어떻게 재형성하는지를 보여주는 이론적, 분포적으로 강인한 최적화(DRO) 분석을 제공한다. 이는 정확성 및 강인성에서의 경험적으로 관찰된 개선을 설명하는 데 도움이 된다.

Abstract: Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples.
  Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.

</details>


### [8] [Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used](https://arxiv.org/abs/2602.07213)
*Srijan Shakya,Anamaria-Roberta Hartl,Sepp Hochreiter,Korbinian Pöppel*

Main category: cs.LG

TL;DR: 이 연구는 대형 언어 모델(LLM)의 동적 인-context 학습을 통해 성능을 향상시키기 위한 원칙을 탐구합니다. 동적 검색을 포함한 LLM의 적응형 아키텍처를 시험하고, 정적 검색 방법과 비교하여 유용성을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 추론 작업에서 LLM은 정적인 지식으로 인해 성능이 떨어지고, 이는 특히 수학 같은 전문 분야에서 환각을 유발합니다.

Method: LLM 에이전트가 추론 과정 중 외부 지식 기반에 대한 쿼리를 결정하는 적응형 검색 증강 아키텍처를 시험합니다.

Result: 실험 결과, 정적 검색은 CoT에 비해 열악하지만, 검색 결과가 포함된 경우의 성능은 CoT와 비교하여 약간 나쁜 반면, 검색을 포함하지 않은 경우는 CoT보다 나은 성능을 보였습니다.

Conclusion: 모델은 문제의 난이도에 따라 검색 빈도를 조절하며, 검색 결정을 내리는 것이 중요한 메타인지 신호임을 강화합니다. 이를 통해 지식을 자가 평가하고 외부 정보를 선별적으로 사용하는 능력이 더 강력하고 신뢰할 수 있는 생성 모델을 구축하는 데 핵심 원칙임을 나타냅니다.

Abstract: Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.

</details>


### [9] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: 이 논문에서는 CoLoRA라는 협업적 저랭크 적응 기법을 제안하여 기초 모델의 효율적인 미세 조정을 통해 사용자 특화 모델을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기초 모델들이 보이지 않는 하위 작업에 효과적으로 적응할 수 있도록 하는 것이 중요하다.

Method: CoLoRA는 여러 하위 사용자의 작업 유사성을 활용하여 공동으로 개인화된 기초 모델을 효율적으로 미세 조정하는 방법을 제안한다.

Result: CoLoRA를 사용하여 유사한 작업으로 함께 훈련할 경우 개별 성능이 유의미하게 향상됨을 보여준다.

Conclusion: CoLoRA는 기초 모델을 보다 효율적으로 미세 조정하고, 개인화된 적응을 가능하게 하는 가능성을 제시한다.

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [10] [Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning](https://arxiv.org/abs/2602.07256)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: 그래프 신경망(GNN)은 그래프 구조 데이터를 모델링하는 데 강력한 도구로 떠올랐지만, 연결된 노드가 서로 다른 특성이나 레이블을 가질 경우 발생하는 이질적 그래프에서 어려움을 겪고 있다. 기존의 방법들은 구조적 설계에 초점을 맞추고 있으며, 문제의 근본 원인에 직접적으로 접근하지 않고 있다.


<details>
  <summary>Details</summary>
Motivation: 이질적 그래프에서의 성능 문제를 해결하기 위한 혁신적인 접근법이 필요하다.

Method: 그래프 변환을 통해 그래프의 동질성을 직접적으로 증가시키는 새로운 패러다임인 GRAPHITE 프레임워크를 제안하고 연구한다.

Result: GRAPHITE는 원래 이질적인 그래프의 동질성을 크게 증가시키며, 노드 간의 동질적인 메시지 전달을 촉진하는 기능 노드를 생성한다.

Conclusion: GRAPHITE는 이질적 그래프에서 최신 방법들보다 뛰어난 성능을 보이며, 동질적 그래프에서도 이전 방식들과 비슷한 정확성을 달성한다.

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.

</details>


### [11] [VertCoHiRF: Decentralized Vertical Clustering Beyond k-means](https://arxiv.org/abs/2602.07279)
*Bruno Belucci,Karim Lounici,Vladimir R. Kostic,Katia Meziani*

Main category: cs.LG

TL;DR: VertCoHiRF는 이질적인 뷰 간의 구조적 합의를 기반으로 한 수직 연합 클러스터링을 위해 완전히 분산된 프레임워크를 제공하며, 개인 데이터의 독립적 클러스터링과 식별자 수준의 합의를 통해 프라이버시를 보장한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 수직 연합 학습 접근법은 중앙 집중식 조정이나 피쳐 의존적 통계 자료의 교환에 의존하며, 이질적인 뷰나 적대적 행동에 대해 제한된 강인성을 보인다.

Method: VertCoHiRF는 각 에이전트가 로컬 피쳐 공간에 적응된 기초 클러스터링 방법을 적용할 수 있도록 하고, 에이전트들은 로컬 뷰를 독립적으로 클러스터링하고 식별자 수준 합의를 통해 제안안을 조율한다.

Result: 실험 결과, 수직 연합 설정에서 경쟁력 있는 클러스터링 성능을 보여줌.

Conclusion: 통신 복잡성과 강인성을 분석하였으며, 이는 프라이버시를 유지하면서 이질적인 로컬 클러스터링 방법을 지원하고, 다수의 해상도에서 뷰 간의 합의를 포착하는 해석 가능한 공유 클러스터 융합 계층을 생성한다.

Abstract: Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple resolutions.We analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.

</details>


### [12] [Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control](https://arxiv.org/abs/2602.07340)
*Yonghui Yang,Wenjian Tao,Jilong Liu,Xingyu Zhu,Junfeng Fang,Weibiao Huang,Le Wu,Richang Hong,Tat-Sent Chua*

Main category: cs.LG

TL;DR: 대규모 언어 모델의 안전 정렬은 도메인 변화 및 노이즈가 많은 선호 감독 하에서 취약하다. 기존의 강건 정렬 방법들은 데이터 불확실성에 집중하는 반면, 선호 기반 목표에서 최적화 유도 취약성을 간과한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 안전 정렬이 도메인 전이와 노이즈 있는 선호 감독 하에서 어떻게 취약한지 재조명하고, 데이터 중심의 방법만으로는 Robustness 결함을 해결할 수 없음을 주장한다.

Method: ShaPO라는 지오메트리 인식 선호 최적화 프레임워크를 제안하여, 정렬 주요 매개변수 하위 공간에 대한 선택적 기하학적 제어를 통해 최악의 경우 정렬 목표를 강제한다.

Result: ShaPO는 다양한 안전 벤치마크와 노이즈 선호 설정에서 기존 선호 최적화 방법들보다 안전 강건성을 일관되게 향상시켰다.

Conclusion: ShaPO는 데이터 강건 목표와 잘 결합되어 추가적인 이익을 제공하며, 제안된 최적화-지오메트리 관점을 경험적으로 지지한다.

Abstract: Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.

</details>


### [13] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

TL;DR: 이 논문은 원격 인간-로봇 상호작용을 통해 효율성을 개선하기 위한 전문가 시연 데이터를 수집하는 데 중점을 둔 확장 가능한 로봇 학습에 대해 다루고 있다.


<details>
  <summary>Details</summary>
Motivation: 원격 인간-로봇 상호작용을 통해 효율적으로 로봇 조작 학습을 수행하기 위한 필요성이 있다.

Method: 제안된 방법은 두 단계로 구성된다: 첫 번째 단계에서는 AR 기반의 원격 인간-로봇 상호작용 시스템에서 학습 데이터를 활용하여 행동 클로닝 방식으로 정책을 생성하고, 두 번째 단계에서는 대조 학습을 통한 강화 학습 방법을 개발하여 더 효율적이고 강력한 정책을 얻는다.

Result: 제안된 방법은 기존의 정책 최적화 방법들보다 빠른 추론 속도와 더 나은 성공률을 보여준다.

Conclusion: 대조 학습을 활용한 강화 학습이 정책 붕괴를 극복함을 보이며, 실험과 시뮬레이션을 통해 그 효과를 입증하였다.

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [14] [FEM-Informed Hypergraph Neural Networks for Efficient Elastoplasticity](https://arxiv.org/abs/2602.07364)
*Jianchuan Yang,Xi Chen,Jidong Zhao*

Main category: cs.LG

TL;DR: 이 논문은 물리 기반 기계 학습을 위해 FEM 계산을 메시지 전송 층에 통합한 새로운 그래프 신경망을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 물리 기반 손실과 Hierarchical Deep Learning Neural Network (HiDeNN) 구조에 의해 동기가 부여되었습니다.

Method: FEM 계산을 노드와 가우스 포인트에 직접적으로 임베드하는 FEM-Informed Hypergraph Neural Networks (FHGNN)를 제안합니다.

Result: 3D 벤치마크에서, 제안된 방법은 최근의 PINN 변형들보다 상당히 개선된 정확도와 효율성을 제공합니다.

Conclusion: 이 작업은 비선형 고체 역학에서 규모에 맞는 물리 기반 학습의 기초를 설정합니다.

Abstract: Graph neural networks (GNNs) naturally align with sparse operators and unstructured discretizations, making them a promising paradigm for physics-informed machine learning in computational mechanics. Motivated by discrete physics losses and Hierarchical Deep Learning Neural Network (HiDeNN) constructions, we embed finite-element (FEM) computations at nodes and Gauss points directly into message-passing layers and propose a numerically consistent FEM-Informed Hypergraph Neural Networks (FHGNN). Similar to conventional physics-informed neural networks (PINNs), training is purely physics-driven and requires no labeled data: the input is a node element hypergraph whose edges encode mesh connectivity. Guided by empirical results and condition-number analysis, we adopt an efficient variational loss. Validated on 3D benchmarks, including cyclic loading with isotropic/kinematic hardening, the proposed method delivers substantially improved accuracy and efficiency over recent, competitive PINN variants. By leveraging GPU-parallel tensor operations and the discrete representation, it scales effectively to large elastoplastic problems and can be competitive with, or faster than, multi-core FEM implementations at comparable accuracy. This work establishes a foundation for scalable, physics-embedded learning in nonlinear solid mechanics.

</details>


### [15] [Pareto-guided Pipeline for Distilling Featherweight AI Agents in Mobile MOBA Games](https://arxiv.org/abs/2602.07521)
*Xionghui Yang,Bozhou Chen,Yunlong Lu,Yongyi Wang,Lingfeng Li,Lanxiao Huang,Lin Liu,Wenjun Wang,Meng Meng,Xia Lin,Wenxin Li*

Main category: cs.LG

TL;DR: 이 논문은 모바일 기기에서 큰 규모의 게임 AI를 실제로 배포하기 위한 효율적인 방법을 제안하며, 성능과 효율성 간의 균형을 탐색하는 학생 아키텍처 검색 공간을 설계한다.


<details>
  <summary>Details</summary>
Motivation: 최고 수준의 게임 인공지능을 모바일 기기에 배포하는 것은 큰 도전 과제로 남아있다.

Method: Pareto 최적성이 이끄는 파이프라인을 제안하고 모바일 실행에 맞춘 고효율 학생 아키텍처 검색 공간을 설계한다.

Result: 증류된 모델은 $12.4	imes$의 속도 향상과 $15.6	imes$의 에너지 효율 개선을 달성하였다.

Conclusion: 모델은 원래의 교사 모델에 비해 여전히 40.32%의 승률을 유지한다.

Abstract: Recent advances in game AI have demonstrated the feasibility of training agents that surpass top-tier human professionals in complex environments such as Honor of Kings (HoK), a leading mobile multiplayer online battle arena (MOBA) game. However, deploying such powerful agents on mobile devices remains a major challenge. On one hand, the intricate multi-modal state representation and hierarchical action space of HoK demand large, sophisticated policy networks that are inherently difficult to compress into lightweight forms. On the other hand, production deployment requires high-frequency inference under strict energy and latency constraints on mobile platform. To the best of our knowledge, bridging large-scale game AI and practical on-device deployment has not been systematically studied. In this work, we propose a Pareto optimality guided pipeline and design a high-efficiency student architecture search space tailored for mobile execution, enabling systematic exploration of the trade-off between performance and efficiency. Experimental results demonstrate that the distilled model achieves remarkable efficiency, including an $12.4\times$ faster inference speed (under 0.5ms per frame) and a $15.6\times$ improvement in energy efficiency (under 0.5mAh per game), while retaining a 40.32% win rate against the original teacher model.

</details>


### [16] [Enhancing Time Series Classification with Diversity-Driven Neural Network Ensembles](https://arxiv.org/abs/2602.07579)
*Javidan Abdullayev,Maxime Devanne,Cyril Meyer,Ali Ismail-Fawaz,Jonathan Weber,Germain Forestier*

Main category: cs.LG

TL;DR: 다양성을 기반으로 한 앙상블 학습 프레임워크를 도입하여 신경망 앙상블 구성원 간의 피쳐 다양성을 촉진하고, 이를 통해 SOTA 성능을 달성했다.


<details>
  <summary>Details</summary>
Motivation: 앙상블 메소드는 개별 모델이 학습한 피쳐의 다양성을 활용하여 다양한 머신러닝 작업에서 SOTA 성능을 달성하는 데 중요한 역할을 한다.

Method: 신경망 앙상블 구성원들 간의 피쳐 다양성을 명시적으로 장려하는 다양성 기반 앙상블 학습 프레임워크를 도입한다. 이 방법은 학습된 피쳐 표현에 직접 적용되는 피쳐 직교 손실을 사용하는 상관 관계 제거 학습 전략을 Employ 한다.

Result: UCR 아카이브의 128개 데이터셋에서 평가한 결과, 더 적은 모델로 SOTA 성능을 달성하였다.

Conclusion: 본 방법은 기존의 신경망 기반 앙상블 접근 방식에 비해 효율적이고 확장성이 뛰어난 방법이다.

Abstract: Ensemble methods have played a crucial role in achieving state-of-the-art (SOTA) performance across various machine learning tasks by leveraging the diversity of features learned by individual models. In Time Series Classification (TSC), ensembles have proven highly effective whether based on neural networks (NNs) or traditional methods like HIVE-COTE. However most existing NN-based ensemble methods for TSC train multiple models with identical architectures and configurations. These ensembles aggregate predictions without explicitly promoting diversity which often leads to redundant feature representations and limits the benefits of ensembling. In this work, we introduce a diversity-driven ensemble learning framework that explicitly encourages feature diversity among neural network ensemble members. Our approach employs a decorrelated learning strategy using a feature orthogonality loss applied directly to the learned feature representations. This ensures that each model in the ensemble captures complementary rather than redundant information. We evaluate our framework on 128 datasets from the UCR archive and show that it achieves SOTA performance with fewer models. This makes our method both efficient and scalable compared to conventional NN-based ensemble approaches.

</details>


### [17] [RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks](https://arxiv.org/abs/2602.08446)
*Pouria Arefijamal,Mahdi Ahmadlou,Bardia Safaei,Jörg Henkel*

Main category: cs.LG

TL;DR: RIFLE는 비독립 동등하게 분포된 조건에서의 데이터 이질성과 복잡성을 해결하여 IoT 환경에서의 연합 학습의 신뢰성과 개인 정보 보호를 강화하는 새로운 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 자원이 제한된 IoT 환경에서 TinyML 모델을 기반으로 하는 연합 학습의 한계 극복 및 데이터 개인 정보 보호를 유지할 필요.

Method:  지식 증류 집합 체계를 활용하여 로짓 기반의 지식 전달로 기울기 공유를 대체하는 강건한 연합 학습 프레임워크 RIFLE를 제안.

Result: RIFLE는 세 가지 벤치마크 데이터셋(MNIST, CIFAR-10 및 CIFAR-100)에서 기존 연합 학습 기준보다 정확도를 최대 28.3% 향상시키고, 잘못된 긍정 탐지를 87.5%까지 줄였다.

Conclusion: RIFLE는 제한된 IoT 시스템에서 VGG19의 훈련 시간을 600일 이상에서 1.39시간으로 단축시키며, 자원이 제한된 네트워크에서 심층 학습을 실용적으로 만들어준다.

Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.

</details>


### [18] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

TL;DR: Astro는 효율적인 대형 언어 모델 배포를 위한 활성화 유도 구조 정규화 프레임워크로, 아웃라이어의 부정적인 영향을 억제하고, 인퍼런스 지연 없이 뛰어난 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존의 양자화 방법들이 아웃라이어 억제에 한계를 겪고 있어 이를 개선하고자 함.

Method: Astro라는 활성화 유도 구조 정규화 프레임워크를 제안하여 고강도 활성화에 해당하는 아웃라이어를 억제함.

Result: LLaMA-2-7B에서 복잡한 학습 기반 회전 방법보다 더 나은 성능을 보이며 양자화 시간은 거의 1/3에 불과함.

Conclusion: Astro는 제로 인퍼런스 지연을 제공하며 주류 양자화 방법과 독립적이다.

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [19] [On the Infinite Width and Depth Limits of Predictive Coding Networks](https://arxiv.org/abs/2602.07697)
*Francesco Innocenti,El Mehdi Achour,Rafal Bogacz*

Main category: cs.LG

TL;DR: 이 연구는 예측 코딩(PC) 네트워크의 무한한 너비와 깊이의 한계를 조사하였으며, PC가 표준 역전파(BP)와 동일한 기울기를 계산할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: PC는 네트워크의 활동에 대한 에너지 함수를 최소화하여 가중치를 업데이트하는 생물학적으로 그럴듯한 대안입니다. 그러나 이러한 접근 방식의 완전한 확장성과 이론적 기초가 불확실합니다.

Method: 무한한 너비와 깊이의 한계를 조사하고 선형 잔차 네트워크에 대해 PC를 위한 특징 학습 매개변수화와 BP의 매개변수화가 정확히 동일하다는 것을 보여줍니다.

Result: PC 에너지가 평형 활동에서 BP 손실로 수렴하며, 모델의 너비가 깊이보다 훨씬 클 때 PC는 BP와 동일한 기울기를 계산합니다.

Conclusion: 이 연구는 다양한 이론적 및 경험적 결과를 통합하며, PCN의 확장에 중요한 함의를 가질 수 있습니다.

Abstract: Predictive coding (PC) is a biologically plausible alternative to standard backpropagation (BP) that minimises an energy function with respect to network activities before updating weights. Recent work has improved the training stability of deep PC networks (PCNs) by leveraging some BP-inspired reparameterisations. However, the full scalability and theoretical basis of these approaches remains unclear. To address this, we study the infinite width and depth limits of PCNs. For linear residual networks, we show that the set of width- and depth-stable feature-learning parameterisations for PC is exactly the same as for BP. Moreover, under any of these parameterisations, the PC energy with equilibrated activities converges to the BP loss in a regime where the model width is much larger than the depth, resulting in PC computing the same gradients as BP. Experiments show that these results hold in practice for deep nonlinear networks, as long as an activity equilibrium seem to be reached. Overall, this work unifies various previous theoretical and empirical results and has potentially important implications for the scaling of PCNs.

</details>


### [20] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: 대형 언어 모델이 추상적인 기억 기능을 사용할 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 상태가 없는 것으로 간주되는 기존 가정을 도전하고, 모델이 독립적인 상호작용 간 상태를 유지할 수 있는 방법을 제안합니다.

Method: 모델의 출력을 통해 정보를 인코딩하고, 후에 이를 입력으로 다시 활용함으로써 모델이 상태를 유지하는 방법을 설명합니다.

Result: 우리는 새로운 유형의 시간적 백도어인 '타임 봄'을 도입하고, 이는 임시 기억을 통해 인과 관계를 기반으로 활성화된다는 것을 보여줍니다.

Conclusion: 이 연구는 적절한 평가 및 테스트를 위한 방향성을 제시하며, 향후 연구를 촉진하기 위해 코드와 데이터를 공개합니다.

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [21] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

TL;DR: 이 논문에서는 dense feature learning이라는 의료 임베딩의 선형 구조를 명시적으로 형성하는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 의료 데이터의 딥 러닝 모델은 일반적으로 개별 예측 문제에 대해 효과적이지만, 임상 데이터의 풍부한 구조를 충분히 활용하지 못하고 학습된 특징의 전이 가능성, 안정성 및 해석 가능성을 제한합니다.

Method: dense feature learning은 임베딩 행렬에 직접 작용하며, 선형 대수적 특성에 따라 정의된 목표를 통해 스펙트럼 균형, 부분공간 일관성 및 특징 직교성을 장려합니다.

Result: dense feature learning은 레이블이나 생성 재구성에 의존하지 않고 더 높은 유효 순위, 개선된 조건 및 시간에 따른 더 큰 안정성을 가진 표현을 생성합니다. 다양한 평가에서 감독 및 자기 감독 기준선에 비해 선형 성능, 강인성 및 부분 공간 정렬에서 일관된 개선을 보여주었습니다.

Conclusion: 임상 변동성을 학습하는 것이 임상 결과를 예측하는 학습만큼 중요할 수 있으며, 의료 AI에서 표현 기하학을 1급 목표로 설정합니다.

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [22] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 본 논문은 사이버 보안에 적용된 심층 강화 학습(DRL)에서 발생하는 11가지 방법론적 함정을 체계화하고, 이들 함정의 영향을 실험을 통해 입증하며, 개선을 위한 권고안을 제시한다.


<details>
  <summary>Details</summary>
Motivation: DRL의 성공을 사이버 보안 문제에 적용하기 위해.

Method: 66개의 DRL4Sec 논문을 분석하여 11가지의 방법론적 함정을 식별하고, 각 함정의 발생 빈도를 정량화하였다.

Result: 평균 논문당 5개 이상의 함정을 발견했으며, 제어된 실험을 통해 이들 함정의 실질적 영향을 입증했다.

Conclusion: 각 함정에 대한 실행 가능한 권고안을 제공하여 보다 엄격하고 배포 가능한 DRL 기반 보안 시스템의 개발을 지원하고자 한다.

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [23] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

TL;DR: ParisKV는 장기 문맥 LLM 추론을 위한 GPU 기반의 KV 캐시 검색 프레임워크로, 분포 변동에 강하고 지연이 적으며, 높은 효율성을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 장기 문맥 LLM 추론에 필요한 KV 캐시 검색의 기존 방법들이 분포 변동과 대규모에서의 지연 문제를 겪고 있어 이를 개선하기 위한 필요성이 있다.

Method: ParisKV는 충돌 기반 후보 선택 방식을 채택하고, 이후 양자화된 내부 곱 재순위 결정기를 사용하여 KV 캐시 검색을 수행한다. 또한 통합 가상 주소 지정(UVA)을 통해 CPU 오프로드된 KV 캐시를 지원하며, 요구에 따라 최소한의 오버헤드로 상위 k개를 가져올 수 있다.

Result: ParisKV는 긴 입력 및 긴 생성 기준에서 전체 주의 품질과 동등한 성능을 내거나 이를 초과한다. 이는 배치 크기 1에서도 전체 주의 속도를 초과하거나 일치시키며, 전체 주의가 실행 가능한 범위 내에서 최대 2.8배 높은 처리량을 제공하고, 전체 주의가 메모리 부족으로 실행되지 않는 백만 토큰 문맥으로 확장된다.

Conclusion: 백만 토큰 규모에서 ParisKV는 MagicPIG 및 PQCache와 비교하여 각각 17배 및 44배의 디코드 지연을 줄인다.

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [24] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: 다수의 에이전트 협업을 통한 복잡한 작업의 성능 향상을 위한 새롭고 효율적인 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 단일 에이전트 시스템은 코드 생성과 같은 복잡한 작업에서 성능 한계를 겪고 있어 다수의 에이전트 협업이 유망한 해결책으로 제시된다.

Method: MARTI-MARS2라는 이름의 다수의 에이전트 강화 훈련 및 추론 프레임워크를 제안하며, 이는 정책 학습과 다수의 에이전트 트리 탐색을 통합하여 협업 탐색 과정을 동적이고 학습 가능한 환경으로 모델링한다.

Result: MARTI-MARS2는 협력하는 32B 모델 두 개를 활용하여 77.7%의 성능을 달성하여 GPT-5.1과 같은 강력한 기준선보다 우수하다.

Conclusion: 정책 다양성이 다수의 에이전트 강화 학습을 통한 지능 확장에 중요하다는 것을 시사하며, 단일 에이전트에서 동질적 다중 역할, 그리고 이질적 다수의 에이전트 패러다임으로의 전환이 성능 한계를 높인다.

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [25] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

TL;DR: AceGRPO는 기계 학습 엔지니어링을 위한 지속적이고 반복적인 최적화를 가능하게 하는 두 가지 핵심 구성 요소를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 엔지니어링에는 에이전트가 긴 시간에 걸쳐 지속적이고 반복적인 최적화를 수행해야 합니다. 현재의 LLM 기반 에이전트는 유망하지만, 매개변수가 고정되어 있기 때문에 행동 정체 문제가 발생합니다.

Method: 본 연구에서는 실행 추적을 재사용 가능한 훈련 작업으로 지속적으로 재구성하는 진화하는 데이터 버퍼와, 학습 잠재력 함수를 기반으로 에이전트의 학습 경계를 동적으로 우선시하여 학습 효율성을 극대화하는 적응형 샘플링이라는 두 가지 핵심 구성 요소를 포함한 AceGRPO를 제안합니다.

Result: AceGRPO를 활용한 훈련된 Ace-30B 모델은 MLE-Bench-Lite에서 100% 유효 제출 비율을 달성하고, 독점적 최전선 모델의 성능에 접근하며, 더 큰 오픈 소스 벤치마크인 DeepSeek-V3.2를 초월하여 지속적 반복 최적화에 대한 강력한 능력을 입증합니다.

Conclusion: 이 연구의 결과는 기계 학습 엔지니어링 분야에서 AceGRPO의 효과iveness를 뒷받침하며, GitHub에서 코드가 제공됩니다.

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [26] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

TL;DR: 물리학 관점에서 흐름 기반 생성 모델을 분석하고, 이를 통해 데이터의 밀도와 생성 질량을 개선하기 위한 Kinetic Path Energy(KPE)라는 새로운 진단 지표를 도입했다.


<details>
  <summary>Details</summary>
Motivation: 고전 역학에서 영감을 받아, Kinetic Path Energy(KPE)를 도입하여 동적 노력 및 데이터 밀도와의 관계를 이해하고자 했다.

Method: KPE는 관습적 미분 방정식(ODE) 경로를 따라 누적된 운동 에너지를 측정하는 액션 유사 진단 도구로, 이를 통해 샘플 간의 동적 노력을 평가한다.

Result: KPE는 더 높은 의미적 충실도를 예측하고, 고 KPE 경로는 저밀도 매니폴드 경계에서 종료되는 경향이 있다는 두 가지 강력한 상관관계를 보인다.

Conclusion: KPE를 활용한 Kinetic Trajectory Shaping(KTS) 기법은 초기 움직임을 향상시키고 나중의 부드러운 착륙을 강제하여 기억화를 줄이고 생성 품질을 개선하는 두 단계로 이루어진 비훈련 추론 전략으로 제시된다.

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [27] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

TL;DR: 이 논문에서는 다중 작업 학습(MTL)에서의 유사성을 측정하기 위한 새로운 방법을 제안하며, 이를 통해 작업 간의 관계를 탐색하고 informed decision-making을 지원합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 작업 학습에서는 작업 간의 지식 전이와 유사성을 이해하는 것이 중요합니다.

Method: 설명 가능한 인공 지능(XAI) 기술, 특히 누적 지역 효과(ALE) 곡선을 기반으로 한 유사성 측정을 제안합니다.

Result: 제안된 유사성 측정법은 직관적인 유사도 기대와 일치하고, 다양한 머신 러닝 모델을 적용할 수 있으며, 여러 데이터셋에서 검증되었습니다.

Conclusion: 이 방법은 작업 간의 관계를 탐색하고 informed decision-making을 지원하는 데 유용한 도구입니다.

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [28] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: 확산 기반 세계 모델이 강화 학습의 높은 생성 충실도를 제공하지만 제어에서 효율성 문제에 직면해 있습니다. 본 논문에서는 Horizon Imagination(HI)라는 방법을 제안하여 여러 미래 관측을 병렬로 처리합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습을 위한 고충실도 생성 모델이 필요하지만 현재의 방법들은 계산 비용이 비쌉니다.

Method: Horizon Imagination(HI)는 분산확률정책을 위해 고안된 온-정책 상상 과정으로, 여러 미래 관측을 병렬로 처리하며 안정화 메커니즘과 샘플링 일정을 통합합니다.

Result: Atari 100K 및 Craftium에서 실험 결과, 우리의 접근 방식은 반프레임 예산으로 제어 성능을 유지하고 다양한 일정에서 우수한 생성 품질을 달성했습니다.

Conclusion: Horizon Imagination은 강화 학습의 효율성을 크게 향상시키는 새로운 방법으로, 코드가 제공됩니다.

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [29] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 이 논문에서는 장기 전략적 외부성을 고려한 적대 게임에서의 대형 언어 모델(LM) 에이전트를 훈련하기 위한 새로운 프레임워크인 암묵적 전략 최적화(ISO)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 장기적인 목표와 외부 요인이 게임의 결과에 미치는 영향을 고려한 새로운 최적화 방법이 필요하다.

Method: ISO는 각 에이전트가 현재의 전략적 맥락을 예측하고 이를 바탕으로 정책을 온라인으로 업데이트하는 예측 인식 프레임워크로, 전략 보상 모델(SRM)과 iso-grpo를 결합한다.

Result: 6인 노리밋 텍사스 홀덤과 경쟁 포켓몬 실험에서 강력한 LLM 및 RL 기준보다 장기 수익의 일관된 개선을 보였다.

Conclusion: ISO는 예측 오류에 대해 우아한 저하를 보여주며 전략적 외부성을 알 때 얻는 정적 게임 비율을 회복할 수 있는 경계가 있다.

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [30] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: V-ABFT는 데이터 손상 감지를 위한 새로운 알고리즘으로, 기존 방법보다 더 높은 정확도를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 알고리즘이 데이터 오류를 과도하게 보수적으로 판단하거나 실제 오류보다 훨씬 큰 임계값을 설정하는 문제점을 해결하고자 한다.

Method: V-ABFT는 분산 기반의 적응형 임계값 알고리즘으로, 검증 차이를 직접 모델링하여 오류 경계를 더 엄격하게 설정한다.

Result: V-ABFT는 임계값-실제 오류 비율을 FP32/FP64에 대해 약 7-20배로, BF16에 대해 48-158배로 줄였다.

Conclusion: V-ABFT는 다양한 플랫폼에서 효과적으로 작동하며, 병합된 커널 ABFT 구현에 통합되어 더욱 정밀한 오류 감지를 가능하게 한다.

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [31] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

TL;DR: mmSIVAE는 혼합-생산자 집합(MOPOE) 집합을 결합한 다중 모드 소프트 내성 변형 오토인코더로, 알츠하이머 병에서 참조 분포의 신뢰도와 다중 모드 통합을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 알츠하이머 병의 이질적인 질병 효과를 포착하기 위해 건강한 참조 분포와 개인별 편차를 정량화해야 합니다.

Method: 우리는 mmSIVAE라는 다중 모드 소프트 내성 변형 오토인코더와 MOPOE 집합을 결합하여 참조의 충실성과 다중 모드 통합을 개선합니다.

Result: mmSIVAE는 ADNI MRI 지역 볼륨 및 아밀로이드 PET SUVR에서 보존된 통제군에 대해 재구성을 개선하고 이상값 탐지에 대해 더 차별화된 편차 점수를 생성했습니다.

Conclusion: 우리의 결과는 건강한 참조 분포의 충실성과 강력한 다중 모드 후방 집합을 우선시하는 훈련 목표의 중요성을 강조하며, 다중 모드 임상 데이터 전반에 걸친 편차 기반 분석에 대한 시사점을 제공합니다.

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [32] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

TL;DR: 이 논문은 자율 에이전트의 도구 사용 실패에 대한 안전 장치를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트를 실제 환경에 배포하기 위해서는 도구 사용 실패에 대한 신뢰할 수 있는 안전 장치가 필요합니다.

Method: 감독 방법과 보완되는 주파수 분석 기반의 학습 없는 가드레일을 제안합니다.

Result: Llama 3.1 8B에서 멀티 피쳐 검출로 97.7% 재현율을 기록하고, 균형 배치를 위한 81.0% 정밀도로 86.1% 재현율을 달성합니다.

Conclusion: 이 결과는 주파수 분석이 에이전트 안전을 위한 원칙적이고 효율적인 프레임워크임을 입증합니다.

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [33] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: DiCode라는 프레임워크를 제안하며, 이는 환경 코드를 생성하여 지속적인 학습을 촉진한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 오픈 엔디드 세계에서 에이전트가 일관되게 학습 가능한 경험 시퀀스를 발견하는 것이 어렵기 때문에 이를 해결하고자 함.

Method: DiCode에서는 '꿈꾸기'가 세상의 코드 수준 변화를 구현하는 방식으로 이루어진다.

Result: DiCode는 에이전트가 장기 기술을 습득하게 하여 이전 방법들이 실패하는 늦은 게임 전투 작업에서 비제로 성공을 이루고 강력한 기준선에 비해 평균 수익이 $16	ext{%}$ 향상됨.

Conclusion: 코드 수준의 환경 설계가 교육 과정 제어를 위한 실용적인 메커니즘을 제공하여, 오픈 엔디드 세계에서 역량 격차를 메우는 중간 환경을 구축할 수 있게 한다.

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [34] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

TL;DR: SkillRL은 LLM 에이전트의 경험을 개선하고 정책 성능을 향상시키기 위한 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 복잡한 작업에서 뛰어난 결과를 보여주지만, 과거 경험을 학습하지 못하고 단독으로 작동합니다.

Method: SkillRL은 자동 기술 발견과 재귀적 진화를 통해 원시 경험과 정책 개선 사이의 간극을 메우는 프레임워크로, 경험 기반 증류 메커니즘을 도입하여 계층적 기술 라이브러리인 SkillBank를 구축하고, 일반 및 작업 특정 휴리스틱을 위한 적응형 검색 전략과 강화 학습 중 에이전트의 정책과 함께 공진화하는 재귀적 진화 메커니즘을 포함합니다.

Result: SkillRL은 ALFWorld, WebShop 및 7개의 검색 증강 작업에서 실험 결과를 통해 15.3% 이상의 개선을 달성하며, 작업 복잡성이 증가함에 따라 강건성을 유지합니다.

Conclusion: 최신 성능으로 에이전트의 정책 성능을 향상시키면서 토큰 사용량을 크게 줄였습니다.

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [35] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 이 논문은 다중 에이전트 강화 학습(MARL)과 단일 에이전트 강화 학습(SARL) 간의 샘플 효율성을 비교 분석하여 MARL의 강점을 명확히 하고, 복잡한 언어 모델 상황에서 MARL 전략을 효과적으로 배포하기 위한 실용적 기준을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: MARL이 SARL보다 우수한 성능을 발휘하는 이유와 상황을 이해하기 위한 이론적 통찰이 부족하여 적절한 RL 프레임워크 선택에 불확실성이 존재합니다.

Method: PAC 프레임워크를 활용하여 LLM 컨텍스트 내에서 SARL 및 MARL 환경을 형식적으로 정의하고, 샘플 복잡도 경계를 도출하며, 작업 분해와 정렬이 학습 효율성에 미치는 영향을 체계적으로 특성화합니다.

Result: 작업이 독립적인 하위 작업으로 자연스럽게 분해될 때 MARL의 샘플 복잡성이 향상되며, 의존적인 하위 작업은 MARL의 비교우위를 감소시킵니다.

Conclusion: 임의의 작업 분해가 가능한 경우의 이론적 통찰을 통해 MARL 전략을 효과적으로 배포하기 위한 실용적 기준을 제공합니다.

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [36] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: TextResNet는 TextGrad보다 우수한 성능을 보여주는 텍스트 기반 최적화 프레임워크로, 복합 AI 시스템의 신뢰성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: TextGrad оптимизаторам требуется более эффективная обратная связь в глубоких 체인, чтобы избежать проблем семантического запутывания.

Method: TextResNet은 신호 라우팅을 정밀하게 수행하기 위해 추가적인 세 가지 혁신을 도입하는 프레임워크다.

Result: TextResNet은 TextGrad와 비교하여 더 우수한 성능을 나타내고, 복합 AI 시스템에서 에이전트 작업에 대한 뛰어난 안정성을 보인다.

Conclusion: 텍스트 기반 최적화에서 TextResNet은 복합 AI 시스템의 핵심 병목 현상에 자원을 동적으로 할당하는 데 기여한다.

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [37] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: 이 논문에서는 대형 언어 모델의 안전성을 강화하기 위한 새로운 강화 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 안전성을 강화할 필요성, 특히 적대적 공격 및 분포 내 오류에 대응하기 위해.

Method: 모델이 자신의 생성 오류를 동적으로 수정하는 방법을 학습하는 강화 학습 단계와 비평가 피드백을 활용하는 RLBF 프레임워크를 도입합니다.

Result: RLBF가 다양한 벤치마크와 모델 규모에서 공격 성공률을 크게 낮추는 것을 보여줍니다.

Conclusion: RLBF는 뛰어난 안전성을 유지하면서 기본 모델 유틸리티를 비판적으로 보존하는 데 기여합니다.

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [38] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 모델 병합은 추가 데이터나 광범위한 훈련 없이 여러 모델의 가중치를 통합하는 대안이다. 하지만 기존의 병합 기술은 일반화되지 않은 특징의 불필요한 결합으로 고통받는다. 우리는 Merging-ensembling loss (M-Loss)라는 새로운 평가 지표를 제안하여 병합 모델의 호환성을 측정하고, 이를 통해 병합 전략을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 모델 훈련의 높은 계산 비용과 레이블 데이터 부족 문제를 해결하기 위해.

Method: M-Loss라는 새로운 평가 지표를 도입하여 병합 과정에서의 호환성을 정량화하고, 레이어와 노드 레벨의 불일치를 측정하여 효과적인 병합 전략을 촉진한다.

Result: M-Loss를 병합 과정에 통합함으로써 병합된 모델과 모델 앙상블 간의 일치를 크게 개선하고, 정확한 모델 통합을 위한 효율적인 프레임워크를 제공한다.

Conclusion: M-Loss는 모델 병합의 이론적 가능성을 정량적으로 평가할 수 있는 기준 역할을 하며, 모델 가지치기에서의 중요성 가이드를 제공한다.

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [39] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

TL;DR: 이 논문에서는 민감한 속성을 기반으로 한 그룹 공정성 기준을 적용하여 PageRank 알고리즘의 공정한 벡터를 계산하는 새로운 방법인 FairRARI를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 알고리즘적 공정성의 중요성이 증가함에 따라, 우리는 민감한 속성에 기반한 여러 그룹 공정성 기준을 충족하는 PR 벡터 계산 문제를 고려하게 되었습니다.

Method: PR의 변분 형식을 활용하여, 공정성 제약이 있는 강한 볼록 최적화 문제를 해결함으로써 공정한 PR 벡터를 계산하는 통합된 볼록 최적화 프레임워크인 FairRARI를 제안합니다.

Result: FairRARI는 세 가지 서로 다른 공정성 기준을 효과적으로 처리하며, 원래 PR 알고리즘과 동일한 점근적 시간 복잡도로 공정한 PR 벡터를 계산할 수 있습니다.

Conclusion: 실제 데이터셋에서의 광범위한 실험 결과, FairRARI가 기존 방법보다 유용성 측면에서 뛰어나며, 여러 정점 그룹 간에 원하는 공정성을 달성함으로써 그 효율성을 강조합니다.

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [40] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

TL;DR: 이 논문은 개인정보 보호와 예측 성능 간의 균형을 맞추기 위한 두 단계의 합성 전략을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 합성 데이터의 예측 작업에서의 성능 향상에 대한 필요성.

Method: 첫 번째 단계에서는 순수 합성 데이터를 생성한 후 원본 데이터와 융합하는 전략을 도입하고, 두 번째 단계에서는 커널 릿지 회귀(KRR) 모델을 사용하여 합성 출력을 생성하는 방식을 적용합니다.

Result: 이 전략은 통계적으로 최적화된 개인정보 보호와 예측 성능 간의 균형을 제공합니다.

Conclusion: 제안된 방법은 이론적으로 및 수치적으로 검증되었으며, 마케팅 문제와 다섯 가지 실제 데이터셋을 통해 일반화 가능성을 보여줍니다.

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [41] [Reasoning aligns language models to human cognition](https://arxiv.org/abs/2602.08693)
*Gonçalo Guiomar,Elia Torre,Pehuen Moure,Victoria Shavina,Mario Giulianelli,Shih-Chii Liu,Valerio Mante*

Main category: cs.LG

TL;DR: 언어 모델이 불확실성 하에서 사람처럼 결정을 내리는지, 그리고 연쇄적 사고(CoT) 추론이 결정 과정에서 어떤 역할을 하는지를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델의 의사결정 과정을 이해하고, 인간과의 비교를 통해 성능 차이를 분석하기 위해 이 연구를 수행하였다.

Method: 적극적인 확률적 추론 작업을 도입하여 샘플링과 추론을 명확히 구분하고, 이를 통해 인간과 다양한 대규모 언어 모델의 성능을 벤치마킹하였다.

Result: 연장된 추론이 성능에 미치는 영향을 확인하였으며, 특히 추론에서의 큰 개선을 보였지만, 적극적 샘플링에는 미미한 개선만을 가져왔다.

Conclusion: 모델을 인간과 비교하여 인지적 공간에서 유사성을 규명하고, 연쇄적 사고가 언어 모델을 인간과 유사한 증거 수집 및 선택 전환 방식으로 변화시킴을 보여 주었다.

Abstract: Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.

</details>


### [42] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: 이 논문은 다중 에이전트 LLM 시스템의 안정적인 강화 학습 후 훈련을 위한 새로운 방법론인 Dr. MAS를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 LLM 시스템은 역할 전문화를 통해 고급 추론 및 도구 사용을 가능하게 하지만, 이러한 시스템에 대한 신뢰할 수 있는 강화 학습 후 훈련은 여전히 어렵습니다.

Method: 우리는 GRPO 스타일 최적화 하에서 글로벌 정규화 기준이 다양한 에이전트의 보상 분포에서 벗어날 수 있어 훈련 불안정성의 주요 원인을 이론적으로 밝혀냅니다. 이를 바탕으로, Dr. MAS는 각 에이전트의 보상 통계를 사용하여 장점(normalize advantages)을 정규화하여 기울기 스케일을 조정하고 이론적으로나 경험적으로 훈련을 안정화하는 간단하고 안정적인 RL 훈련 방법입니다.

Result: Dr. MAS는 다중 에이전트 수학적 추론 및 다회전 검색 벤치마크에서 Qwen2.5 및 Qwen3 시리즈 모델을 사용하여 평가되었습니다. Dr. MAS는 vanilla GRPO에 비해 명확한 성능 향상(+5.6% avg@16, +4.6% pass@16, 수학 문제와 +15.2% avg@16, +13.1% pass@16, 검색) 을 달성하며 기울기 스파이크를 대폭 제거합니다.

Conclusion: 또한, Dr. MAS는 이질적인 에이전트-모델 할당 아래에서도 효율성을 개선하며 여전히 높은 효과성을 유지합니다.

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [43] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 행동 분석과 모델의 내부 표현에 대한 해석 기반 분석을 통합한 목표 지향성 평가 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 행동자의 목표를 이해하는 것은 그 행동을 설명하고 예측하는 데 도움이 되지만, 에이전트 시스템에 목표를 신뢰성 있게 귀속시키는 방법론이 확립되어 있지 않다.

Method: 2D 그리드 세계에서 목표 상태로 나아가는 LLM 에이전트를 사례 연구로 사용하여 행동 평가와 내부 표현의 해석 분석을 통합하는 프레임워크를 제안한다.

Result: 그리드 크기, 장애물 밀도 및 목표 구조가 다양한 경우에 대해 최적 정책에 대비하여 에이전트를 평가한 결과, 성능은 작업의 난이도에 따라 비례하여 변화하지만 난이도를 보존하는 변환 및 복잡한 목표 구조에 대해서도 견고함을 유지한다.

Conclusion: 행동 평가를 넘어 에이전트가 목표를 어떻게 표현하고 추구하는지를 설명하기 위해서는 내성적 검토가 필요하다는 견해를 지지한다.

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [44] [Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning](https://arxiv.org/abs/2602.08986)
*Isaac Xu,Martin Gillis,Ayushi Sharma,Benjamin Misiuk,Craig J. Brown,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 계층적 다중 레이블 분류에서 모델 예측이 더 깊은 수준으로 도달하는 데 어려움이 있으며, 이를 해결하기 위해 가중 손실 목표를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 계층적 다중 레이블 분류에서 더 세부적이고 정밀한 분류를 위한 모델 예측의 깊은 수준 도달은 지속적인 도전 과제가 된다.

Method: 우리의 방법은 노드별 불균형 가중치와 현대적 앙상블 불확실성 측정을 활용하는 초점 가중치 компонент을 결합한 가중 손실 목표를 제안한다.

Result: 우리는 벤치마크 데이터셋에서 재현율이 최대 5배 향상되었고, $F_{1}$ 점수에서도 통계적으로 유의미한 증가를 관찰했다.

Conclusion: 제안된 접근법은 최적이 아닌 인코더나 제한된 데이터 상황에서도 컨볼루션 네트워크의 성능을 향상시킨다.

Abstract: In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data.

</details>


### [45] [Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense](https://arxiv.org/abs/2602.09012)
*Jiacheng Liu,Yaxin Luo,Jiacheng Cui,Xinyi Shang,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 전통적인 CAPTCHA가 더 이상 유효하지 않게 된 상황에서, 우리는 차세대 CAPTCHA 시스템을 소개하여 고급 에이전트들로부터 웹을 보호하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 기존 CAPTCHA 시스템이 GUI 지원 에이전트의 발전으로 인해 더 이상 효과적이지 않게 되면서 새로운 방어 메커니즘 필요.

Method: Next-Gen CAPTCHA는 강력한 데이터 생성 파이프라인을 기반으로 하여 대규모 평가를 가능하게 하며, 인간과 에이전트 간의 인지 간극을 활용한다.

Result: 이 새로운 시스템은 복잡한 논리 퍼즐에서 90% 이상의 통과율을 기록하며, 생물학적 사용자와 인공지능 에이전트 간의 뚜렷한 구분을 재확립한다.

Conclusion: 차세대 웹의 보안을 위해 스케일able하고 다양한 방어 메커니즘을 제공한다.

Abstract: The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [46] [Pro-ZD: A Transferable Graph Neural Network Approach for Proactive Zero-Day Threats Mitigation](https://arxiv.org/abs/2602.07073)
*Nardine Basta,Firas Ben Hmida,Houssem Jmal,Muhammad Ikram,Mohamed Ali Kaafar,Andy Walker*

Main category: cs.CR

TL;DR: 본 논문은 방화벽 규칙 및 접근 정책 생성을 위한 자동화 도구를 사용하여 네트워크 연결성을 관리하는 방법을 제시하고, 비정상 연결 경로를 탐지할 수 있는 그래프 신경망 모델인 Pro-ZD를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 증가하는 트래픽과 다양한 네트워크 아키텍처로 인해 방화벽 정책과 접근 정책 관리에서 발생하는 위험을 효과적으로 관리하는 것이 주요 과제이다.

Method: 본 논문은 가중치가 있는 최단 경로를 식별하기 위한 새로운 그래프 신경망 모델을 제안하며, 이를 통해 네트워크의 잘못된 구성과 높은 위험 연결 경로를 탐지한다.

Result: 실험 결과, Pro-ZD는 높은 위험 연결을 탐지하는 데 95% 이상의 평균 정확도를 달성하였다.

Conclusion: 이 연구는 Pro-ZD 프레임워크가 방화벽 규칙 및 접근 정책을 자동으로 조정할 수 있는 혁신적인 접근 방식을 제시하며, 제로데이 공격을 방지할 수 있는 가능성을 보여준다.

Abstract: In today's enterprise network landscape, the combination of perimeter and distributed firewall rules governs connectivity. To address challenges arising from increased traffic and diverse network architectures, organizations employ automated tools for firewall rule and access policy generation. Yet, effectively managing risks arising from dynamically generated policies, especially concerning critical asset exposure, remains a major challenge. This challenge is amplified by evolving network structures due to trends like remote users, bring-your-own devices, and cloud integration. This paper introduces a novel graph neural network model for identifying weighted shortest paths. The model aids in detecting network misconfigurations and high-risk connectivity paths that threaten critical assets, potentially exploited in zero-day attacks -- cyber-attacks exploiting undisclosed vulnerabilities. The proposed Pro-ZD framework adopts a proactive approach, automatically fine-tuning firewall rules and access policies to address high-risk connections and prevent unauthorized access. Experimental results highlight the robustness and transferability of Pro-ZD, achieving over 95% average accuracy in detecting high-risk connections. \

</details>


### [47] [Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction](https://arxiv.org/abs/2602.07287)
*Juefei Pu,Xingyu Li,Haonan Li,Zhengchuan Liang,Jonathan Cox,Yifan Wu,Kareem Shehada,Arrdya Srivastav,Zhiyun Qian*

Main category: cs.CR

TL;DR: 본 논문은 LLM 기반의 리눅스 커널 취약점 재현의 대규모 연구를 제시하며, K-Repro라는 시스템을 개발하여 N-day 취약점의 자동화된 버그 재현을 수행한다.


<details>
  <summary>Details</summary>
Motivation: 리눅스 커널의 취약점 복제에 대한 체계적인 연구가 부족하여, LLM 기반 접근법의 유효성을 검증하고자 하였다.

Method: K-Repro라는 LLM 기반 에이전틱 시스템을 개발하였으며, 코드 탐색, 가상 머신 관리, 상호작용 및 디버깅 기능을 통제하여 N-day 취약점을 재현하였다.

Result: 100개의 실제 리눅스 커널 취약점을 활용한 실험에서 K-Repro는 50% 이상의 사례에 대한 PoC를 생성할 수 있음을 보여주었다.

Conclusion: 이 연구는 더 신뢰할 수 있는 자율 보안 에이전트 구축을 위한 인사이트를 제공하고, 공격 및 방어 관점에서 실제 N-day 위험을 평가하는 데 기여할 수 있다.

Abstract: Autonomous large language model (LLM) based systems have recently shown promising results across a range of cybersecurity tasks. However, there is no systematic study on their effectiveness in autonomously reproducing Linux kernel vulnerabilities with concrete proofs-of-concept (PoCs). Owing to the size, complexity, and low-level nature of the Linux kernel, such tasks are widely regarded as particularly challenging for current LLM-based approaches.
  In this paper, we present the first large-scale study of LLM-based Linux kernel vulnerability reproduction. For this purpose, we develop K-Repro, an LLM-based agentic system equipped with controlled code-browsing, virtual machine management, interaction, and debugging capabilities. Using kernel security patches as input, K-Repro automates end-to-end bug reproduction of N-day vulnerabilities in the Linux kernel. On a dataset of 100 real-world exploitable Linux kernel vulnerabilities collected from KernelCTF, our results show that K-Repro can generate PoCs that reproduce over 50\% of the cases with practical time and monetary cost.
  Beyond aggregate success rates, we perform an extensive study of effectiveness, efficiency, stability, and impact factors to explain when agentic reproduction succeeds, where it fails, and which components drive performance. These findings provide actionable guidance for building more reliable autonomous security agents and for assessing real-world N-day risk from both offensive and defensive perspectives.

</details>


### [48] [Aegis: Towards Governance, Integrity, and Security of AI Voice Agents](https://arxiv.org/abs/2602.07379)
*Xiang Li,Pin-Yu Chen,Wenqi Wei*

Main category: cs.CR

TL;DR: Aegis는 음성 에이전트의 안전성을 평가하고 지키기 위한 레드 팀 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 음성 에이전트의 빠른 발전과 채택에도 불구하고, 이들의 보안 취약성이 충분히 연구되지 않았다.

Method: Aegis는 음성 에이전트의 현실적인 배포 파이프라인을 모델링하고 프라이버시 유출, 권한 상승, 자원 남용 등 주요 위험의 구조화된 적대적 시나리오를 설계한다.

Result: 은행 콜센터, IT 지원, 물류 분야의 사례 연구를 통해 평가되었으며, 접근 제어는 데이터 수준의 위험을 완화하지만 행동 공격에 대해서는 여전히 취약함을 발견했다.

Conclusion: 차세대 음성 에이전트를 보호하기 위해 접근 제어, 정책 시행, 행동 모니터링을 결합한 다층 방어가 필요하다.

Abstract: With the rapid advancement and adoption of Audio Large Language Models (ALLMs), voice agents are now being deployed in high-stakes domains such as banking, customer service, and IT support. However, their vulnerabilities to adversarial misuse still remain unexplored. While prior work has examined aspects of trustworthiness in ALLMs, such as harmful content generation and hallucination, systematic security evaluations of voice agents are still lacking. To address this gap, we propose Aegis, a red-teaming framework for the governance, integrity, and security of voice agents. Aegis models the realistic deployment pipeline of voice agents and designs structured adversarial scenarios of critical risks, including privacy leakage, privilege escalation, resource abuse, etc. We evaluate the framework through case studies in banking call centers, IT Support, and logistics. Our evaluation shows that while access controls mitigate data-level risks, voice agents remain vulnerable to behavioral attacks that cannot be addressed through access restrictions alone, even under strict access controls. We observe systematic differences across model families, with open-weight models exhibiting higher susceptibility, underscoring the need for layered defenses that combine access control, policy enforcement, and behavioral monitoring to secure next-generation voice agents.

</details>


### [49] [AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management](https://arxiv.org/abs/2602.07398)
*Ruoyao Wen,Hao Li,Chaowei Xiao,Ning Zhang*

Main category: cs.CR

TL;DR: AgentSys는 간접 프롬프트 주입으로부터 LLM 에이전트를 방어하기 위한 프레임워크로, 명시적인 메모리 관리를 통해 공격의 성공률을 크게 줄인다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트를 통한 데이터 절취와 무단 작업을 방지하기 위해 간접 프롬프트 주입으로 인한 취약성을 해결하고자 한다.

Method: LLM 에이전트의 메모리를 명시적으로 관리하는 AgentSys 프레임워크를 제안하며, 운영체제의 프로세스 메모리 격리에서 영감을 받아 계층 구조를 형성하고 있다.

Result: AgentSys는 공격 성공률을 0.78%와 4.25%로 낮추며, 방어되지 않은 기본선보다 유용성을 약간 개선한다.

Conclusion: 명시적인 메모리 관리를 통해 안전하고 동적인 LLM 에이전트 아키텍처를 가능하게 함을 보인다.

Abstract: Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.
  We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.
  On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.

</details>


### [50] [SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients](https://arxiv.org/abs/2602.07513)
*Masato Kamba,Akiyoshi Sannai*

Main category: cs.CR

TL;DR: 본 논문은 SPECA라는 프레임워크를 통해 다수의 구현 시스템의 감사를 위한 체크리스트를 생성하고, 이를 구현 위치에 매핑하며, 크로스 구현 재사용을 지원한다. 이 프레임워크는 이더리움 Fusaka 업그레이드 보안 감사 대회에서 실제로 적용되어, 유효한 제출물의 76.5%가 크로스 구현 체크에서 발견되었다. 또한, 거짓 긍정 사례는 주로 위협 모델 불일치에 기인하며, 모델링이 초기 단계에서 명확해야 거짓 긍정을 줄일 수 있다.


<details>
  <summary>Details</summary>
Motivation: 다수의 구현 시스템이 자연어 규격에 따라 감사받고 있으나, 구현 간의 불일치가 없을 경우 불명확한 요구사항에 대한 잘못된 해석에 그치는 경우가 많다.

Method: SPECA는 규범 요구사항을 체크리스트로 변환하고 이를 구현 위치에 매핑하며, 크로스 구현 재사용을 지원하는 감사 프레임워크이다. 이 프레임워크는 이더리움 Fusaka 업그레이드 보안 감사 대회에서 11개 생산 클라이언트의 실제 사례를 통해 구현되었다.

Result: 이 대회에서 제출된 54건 중 17건이 유효하다고 판단되었으며, 유효한 발견의 76.5% (13건)는 크로스 구현 체크에서 발생하였다. 잘못된 제출물의 56.8%는 위협 모델 불일치와 관련이 있었다.

Conclusion: 이 결과들은 초기 단계에서 명확한 위협 모델링이 중요하다는 것을 보여주며, 개선된 에이전트는 대회에서 상위 4%에 위치하고 주요 문제에서 51명 중 49명보다 뛰어난 성과를 보였다.

Abstract: Multi-implementation systems are increasingly audited against natural-language specifications. Differential testing scales well when implementations disagree, but it provides little signal when all implementations converge on the same incorrect interpretation of an ambiguous requirement. We present SPECA, a Specification-to-Checklist Auditing framework that turns normative requirements into checklists, maps them to implementation locations, and supports cross-implementation reuse.
  We instantiate SPECA in an in-the-wild security audit contest for the Ethereum Fusaka upgrade, covering 11 production clients. Across 54 submissions, 17 were judged valid by the contest organizers. Cross-implementation checks account for 76.5 percent (13 of 17) of valid findings, suggesting that checklist-derived one-to-many reuse is a practical scaling mechanism in multi-implementation audits. To understand false positives, we manually coded the 37 invalid submissions and find that threat model misalignment explains 56.8 percent (21 of 37): reports that rely on assumptions about trust boundaries or scope that contradict the audit's rules. We detected no High or Medium findings in the V1 deployment; misses concentrated in specification details and implicit assumptions (57.1 percent), timing and concurrency issues (28.6 percent), and external library dependencies (14.3 percent). Our improved agent, evaluated against the ground truth of a competitive audit, achieved a strict recall of 27.3 percent on high-impact vulnerabilities, placing it in the top 4 percent of human auditors and outperforming 49 of 51 contestants on critical issues. These results, though from a single deployment, suggest that early, explicit threat modeling is essential for reducing false positives and focusing agentic auditing effort. The agent-driven process enables expert validation and submission in about 40 minutes on average.

</details>


### [51] [MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots](https://arxiv.org/abs/2602.07517)
*Yuhao Wang,Shengfang Zhai,Guanghao Jin,Yinpeng Dong,Linyi Yang,Jiaheng Zhang*

Main category: cs.CR

TL;DR: MemPot은 메모리 추출 공격에 대한 첫 번째 이론적으로 검증된 방어 프레임워크로, 최적화된 허니팟을 메모리에 삽입하여 작동한다. 이 방식은 탐지 프로세스를 모델링하고 기존의 최적 정적 탐지기보다 더 낮은 샘플링 라운드를 요구함을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 복잡하고 목표 지향적인 작업을 처리하는 데 LLM 기반 에이전트는 외부 및 내부 메모리 시스템을 사용하지만, 이는 심각한 추출 공격에 expose 되며 효과적인 방어가 부족하다.

Method: 메모리에 최적화된 허니팟을 주입하여 메모리 추출 공격에 대한 방어 프레임워크인 MemPot을 제안하며, 두 단계 최적화 과정을 통해 공격자에게는 높은 검색 확률을, 일반 사용자에게는 눈에 띄지 않도록 함.

Result: Empirically, MemPot은 최신 기술 기준보다 상당한 성능 향상을 보이며, 낮은 거짓 긍정률 제약에서 탐지 AUROC를 50% 향상시키고 진양성 비율을 80% 증가시킨다.

Conclusion: MemPot은 추가적인 온라인 추론 지연이 없고 표준 작업에서 에이전트의 유용성을 유지하여 안전성, 무해성 및 효율성에서의 우수성을 입증했다.

Abstract: Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.

</details>


### [52] [SoK: Credential-Based Trust Management in Decentralized Ledger Systems](https://arxiv.org/abs/2602.07572)
*Yanna Jiang,Haiyu Deng,Qin Wang,Guangsheng Yu,Xu Wang,Yilin Sai,Shiping Chen,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: 이 논문은 자격 기반 분산 신뢰 관리 시스템(DTMS)에 대한 체계적인 리뷰를 통해 이론과 실제의 격차를 연결합니다.


<details>
  <summary>Details</summary>
Motivation: 분산 환경에서 신뢰를 관리하기 위한 신뢰 관리 시스템(TMS)의 중요성 증가와 탈중앙화 시스템 및 블록체인 기술의 부상으로 인해 자격 기반 DTMS에 대한 관심이 높아지고 있습니다.

Method: 본 논문은 기존의 자격 기반 DTMS 솔루션을 아키텍처 디자인, 자격 메커니즘, 신뢰 평가 모델 등 여러 차원에서 분석합니다.

Result: 설문조사를 통해 자격 기반 DTMS 접근 방식을 상세히 분류하고 DTMS 구현을 평가하기 위한 포괄적인 평가 기준을 설정합니다.

Conclusion: 현재 시스템 및 구현에 대한 광범위한 분석을 통해 DTMS 분야의 주요 도전 과제와 유망한 연구 방향을 식별하고, 연구원 및 실무자에게 유용한 통찰력을 제공합니다.

Abstract: Trust management systems (TMS) are crucial for managing trust in distributed environments. The rise of decentralized systems and blockchain has sparked interest in credential-based decentralized trust management systems (DTMS). This paper bridges the gap between theory and practice through a systematic review of credential-based DTMS. We analyze existing DTMS solutions through multiple dimensions, including their architectural designs, credential mechanisms, and trust evaluation models. Our survey provides a detailed taxonomy of credential-based DTMS approaches and establishes comprehensive evaluation criteria for assessing DTMS implementations. Through extensive analysis of current systems and implementations, we identify critical challenges and promising research directions in the field. Our examination offers valuable insights for researchers and practitioners working on DTMS, particularly in areas such as access control, reputation systems, and blockchain-based trust frameworks.

</details>


### [53] [Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents](https://arxiv.org/abs/2602.07652)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Yoonpyo Lee,Jay Yoo,Tanzim Ahad,Syed Bahauddin Alam,Sajedul Talukder*

Main category: cs.CR

TL;DR: 대규모 언어 모델의 안전성을 평가하기 위한 아키텍처 중심의 보안 평가 방법인 AgentFence를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 심층 에이전트로 배치됨에 따라, 안전 실패가 안전하지 않은 텍스트에서 안전하지 않은 경로로 전환되고 있다.

Method: AgentFence는 계획, 기억, 검색, 도구 사용 및 위임을 아우르는 14개의 신뢰 경계 공격 클래스를 정의하고, 대화 중단을 통해 실패를 감지한다.

Result: 8개의 에이전트 유형을 평가한 결과, 평균 보안 실패율(MSBR)은 LangGraph에서 $0.29 	ext{±} 0.04$에서 AutoGPT에서 $0.51 	ext{±} 0.07$까지 다양하다.

Conclusion: AgentFence는 에이전트가 시간에 따라 목표와 권한 범위를 준수하는지를 평가하여 에이전트 보안을 재정의한다.

Abstract: Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundary attack classes spanning planning, memory, retrieval, tool use, and delegation, and detects failures via *trace-auditable conversation breaks* (unauthorized or unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations). Holding the base model fixed, we evaluate eight agent archetypes under persistent multi-turn interaction and observe substantial architectural variation in mean security break rate (MSBR), ranging from $0.29 \pm 0.04$ (LangGraph) to $0.51 \pm 0.07$ (AutoGPT). The highest-risk classes are operational: Denial-of-Wallet ($0.62 \pm 0.08$), Authorization Confusion ($0.54 \pm 0.10$), Retrieval Poisoning ($0.47 \pm 0.09$), and Planning Manipulation ($0.44 \pm 0.11$), while prompt-centric classes remain below $0.20$ under standard settings. Breaks are dominated by boundary violations (SIV 31%, WPA 27%, UTI+UTA 24%, ATD 18%), and authorization confusion correlates with objective and tool hijacking ($ρ\approx 0.63$ and $ρ\approx 0.58$). AgentFence reframes agent security around what matters operationally: whether an agent stays within its goal and authority envelope over time.

</details>


### [54] [CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment](https://arxiv.org/abs/2602.08023)
*Nanda Rani,Kimberly Milner,Minghao Shao,Meet Udeshi,Haoran Xi,Venkata Sai Charan Putrevu,Saksham Aggarwal,Sandeep K. Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Muhammad Shafique,Ramesh Karri*

Main category: cs.CR

TL;DR: CyberExplorer는 공격자가 불확실한 환경에서 자율적으로 탐색하고 공격하는 평가 도구로, 현실적인 다중 목표 공격 시나리오를 평가할 수 있게 해준다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계의 공격 보안 작전은 본질적으로 개방적이며, 기존의 LLM 기반 공격 에이전트 평가가 제한된 목표와 이진 성공 기준에 의존하는 문제를 해결하고자 한다.

Method: CyberExplorer는 자율적인 정찰, 목표 선택 및 취약점 악용을 수행할 수 있는 40개의 취약한 웹 서비스로 구성된 개방형 환경 벤치마크와 미리 정의된 계획 없이 동적 탐색을 지원하는 반응형 다중 에이전트 프레임워크로 구성된다.

Result: CyberExplorer는 깃발 회복을 넘어 상호작용 역학, 조정 행동, 실패 모드 및 취약점 발견 신호를 포착하여 벤치마크와 현실적인 다중 목표 공격 시나리오 사이의 격차를 메운다.

Conclusion: CyberExplorer는 공격자의 탐색 능력을 평가하는 새로운 기준을 제시하며, 현실 세계의 복잡한 공격 시나리오를 반영한다.

Abstract: Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.

</details>


### [55] [Leveraging the Power of Ensemble Learning for Secure Low Altitude Economy](https://arxiv.org/abs/2602.07725)
*Yaoqi Yang,Yong Chen,Jiacheng Wang,Geng Sun,Dusit Niyato,Zhu Han*

Main category: cs.CR

TL;DR: 저고도 경제(LAE)는 사회적 복지와 경제 성장을 증진할 수 있는 큰 잠재력을 가진 분야입니다. 그러나 이 분야는 악의적인 항공기 침입 공격과 같은 보안 위협에 취약합니다. 이를 해결하기 위해 침입 탐지 시스템(IDS)을 사용할 수 있지만, LAE의 이질적인 데이터, 동적 환경 및 자원 제한 장치로 인해 현재의 IDS는 탐지 정확성, 적응성, 자원 활용 비율에서 어려움에 직면해 있습니다. 집합 학습을 통해 여러 모델의 장점을 결합함으로써 IDS의 정확성을 향상시켜 LAE의 보안을 강화할 수 있습니다. 이 논문은 안전한 LAE를 위한 집합 학습에 대한 연구 초점, 해결책 및 사례 연구를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 저고도 경제(LAE)의 사회적 복지 증진 및 경제 성장 잠재력을 강조하고, 악의적인 항공기 침입에 대한 보안 위협을 알림.

Method: 집합 학습을 활용하여 여러 모델의 강점을 결합해 IDS의 정확성과 효율성을 향상시키기 위한 프레임워크를 제안.

Result: LAE에서 악의적인 항공기 침입 공격에 대해 효과적으로 방어할 수 있는 집합 학습 기반의 트래킹 프레임워크의 실효성과 효과성을 입증함.

Conclusion: 집합 학습을 통한 LAE 보안을 강화하는 등향을 제시하고, 향후 연구 방향성을 제시함.

Abstract: Low Altitude Economy (LAE) holds immense promise for enhancing societal well-being and driving economic growth. However, this burgeoning field is vulnerable to security threats, particularly malicious aircraft intrusion attacks. To address the above concerns, intrusion detection systems (IDS) can be used to defend against malicious aircraft intrusions in LAE. Whereas, due to the heterogeneous data, dynamic environment, and resource-constrained devices within LAE, current IDS face challenges in detection accuracy, adaptability, and resource utilization ratio. In this regard, due to the inherent ability to combine the strengths of multiple models, ensemble learning can realize more robust and diverse anomaly detection further enhance IDS accuracy, thereby improving robustness and efficiency of the secure LAE. Unlike single-model approaches, ensemble learning can leverage the collective knowledge of its constituent models to effectively defend the malicious aircraft intrusion attacks. Specifically, this paper investigates ensemble learning for secure LAE, covering research focuses, solutions, and a case study. We first establish the rationale for ensemble learning and then review research areas and potential solutions, demonstrating the necessities and benefits of applying ensemble learning to secure LAE. Subsequently, we propose a framework of ensemble learning-enabled malicious aircrafts tracking in the secure LAE, where its feasibility and effectiveness are evaluated by the designed case study. Finally, we conclude by outlining promising future research directions for further advancing the ensemble learning-enabled secure LAE.

</details>


### [56] [Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model](https://arxiv.org/abs/2602.07878)
*Tianyi Wang,Huawei Fan,Yuanchao Shu,Peng Cheng,Cong Wang*

Main category: cs.CR

TL;DR: 대규모 언어 모델(LLM)은 지연 공격에 직면해 있으며, 이러한 공격은 현대 LLM 제공 시스템에 대해 효과적이지 않다. 본 논문에서는 시스템 레이어로 초점을 옮기고 새로운 Fill and Squeeze 공격 전략을 소개한다.


<details>
  <summary>Details</summary>
Motivation: LLM 추론의 비용이 크기 때문에 지연 공격은 운영 비용과 가용성 위험을 증가시킬 수 있다.

Method: Fill 공격은 전역 KV 캐시를 소진하여 Head-of-Line 차단을 유도하고, Squeeze 공격은 시스템을 반복적인 선점 상태로 만든다. 이 과정에서 간단한 텍스트 프롬프트부터 복잡한 프롬프트 엔지니어링까지 다양한 방법을 사용하여 출력 길이를 조작한다.

Result: 기존 공격 대비 30-40% 낮은 공격 비용으로 Time to First Token에서 20-280배의 평균 지연과 Time Per Output Token에서 1.5-4배의 평균 지연을 초래하는 것으로 평가되었다.

Conclusion: 시스템 수준 최적화가 지연 영향을 완화할 수 있으며, 새롭고 효과적인 공격 전략이 제안되었다.

Abstract: Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic complexity attacks by crafting inputs to trigger worst-case output lengths. However, we report a counter-intuitive finding that these algorithmic latency attacks are largely ineffective against modern LLM serving systems. We reveal that system-level optimization such as continuous batching provides a logical isolation to mitigate contagious latency impact on co-located users. To this end, in this paper, we shift the focus from the algorithm to the system layer, and introduce a new Fill and Squeeze attack strategy targeting the state transition of the scheduler. "Fill" first exhausts the global KV cache to induce Head-of-Line blocking, while "Squeeze" forces the system into repetitive preemption. By manipulating output lengths using methods from simple plain-text prompts to more complex prompt engineering, and leveraging side-channel probing of memory status, we demonstrate that the attack can be orchestrated in a black-box setting with much less cost. Extensive evaluations indicate by up to 20-280x average slowdown on Time to First Token and 1.5-4x average slowdown on Time Per Output Token compared to existing attacks with 30-40% lower attack cost.

</details>


### [57] [CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution](https://arxiv.org/abs/2602.07918)
*Minbeom Kim,Mihir Parmar,Phillip Wallis,Lesly Miculicich,Kyomin Jung,Krishnamurthy Dj Dvijotham,Long T. Le,Tomas Pfister*

Main category: cs.CR

TL;DR: AI 에이전트는 툴 호출 기능을 가진 경우 간접 프롬프트 주입(IPI) 공격에 취약하다. 기존 방어책은 공격 성공률을 줄일 수 있지만 과도한 방어의 딜레마로 인해 실제 위협에 관계없이 비싼, 항상 활성화된 데이터 정화를 수행하여 유용성과 지연을 저하시킨다. 우리는 인과적 제거 관점을 통해 IPI를 재조명하고 CausalArmor라는 선택적 방어 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 툴 호출 기능으로 인해 간접 프롬프트 주입(IPI) 공격의 위험이 증가하고 있다.

Method: CausalArmor는 경량의 제거 기반 기여도를 계산하고, 신뢰할 수 없는 세그먼트가 사용자 의도를 지배할 때만 타겟화된 정화를 수행한다.

Result: CausalArmor는 공격적인 방어책과 같은 보안을 제공하면서도 설명 가능성을 개선하고 AI 에이전트의 유용성과 지연성을 보존한다.

Conclusion: CausalArmor는 인과적 기여도에 기반한 정화가 악의적인 행동 선택 확률에 극히 작은 상한을 조건부로 제공함을 이론적으로 증명한다.

Abstract: AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation-based attributions at privileged decision points, and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.

</details>


### [58] [ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning](https://arxiv.org/abs/2602.08014)
*Sadegh Sohani,Salar Ghazi,Farnaz Kamranfar,Sahar Pilehvar Moakhar,Mohammad Allahbakhsh,Haleh Amintoosi,Kaiwen Zhang*

Main category: cs.CR

TL;DR: 이 논문은 현대 공급망에서의 접근 제어 문제를 해결하기 위한 지능형 계약 기반 접근 제어 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현대 공급망은 여러 독립적이고 경쟁적인 조직에서 운영되며 접근 제어가 중요한 문제이다.

Method: ICBAC는 허가된 블록체인(Hyperledger Fabric)과 연합 학습(FL)을 통합하여, 자산 관리, 기본 접근 제어 및 동적 철회를 위한 다채널 아키텍처와 세 가지 스마트 계약을 사용하는 지능형 계약 기반 접근 제어 프레임워크이다.

Result: 실험 결과, ICBAC는 정적 프레임워크와 비교하여 견줄만한 블록체인 성능을 달성하고, IID 및 비IID 데이터에서 효과적인 이상 탐지를 제공하며, 원시 데이터 공유 없이 작동한다.

Conclusion: 따라서 ICBAC는 분산 공급망에서 동적이고 개인정보를 보호하는 접근 제어를 위한 실용적이고 확장 가능한 솔루션을 제공한다.

Abstract: This paper addresses the critical challenge of access control in modern supply chains, which operate across multiple independent and competing organizations. Existing access control is static and centralized, unable to adapt to insider threats or evolving contexts. Blockchain improves decentralization but lacks behavioral intelligence, while centralized machine learning for anomaly detection requires aggregating sensitive data, violating privacy.
  The proposed solution is ICBAC, an intelligent contract-based access control framework. It integrates permissioned blockchain (Hyperledger Fabric) with federated learning (FL). Built on Fabric, ICBAC uses a multi-channel architecture and three smart contracts for asset management, baseline access control, and dynamic revocation. To counter insider misuse, each channel deploys an AI agent that monitors activity and dynamically restricts access for anomalies. Federated learning allows these agents to collaboratively improve detection models without sharing raw data.
  For heterogeneous, competitive environments, ICBAC introduces a game-theoretic client selection mechanism using hedonic coalition formation. This enables supply chains to form stable, strategy-proof FL coalitions via preference-based selection without disclosing sensitive criteria. Extensive experiments on a Fabric testbed with a real-world dataset show ICBAC achieves blockchain performance comparable to static frameworks and provides effective anomaly detection under IID and non-IID data with zero raw-data sharing. ICBAC thus offers a practical, scalable solution for dynamic, privacy-preserving access control in decentralized supply chains.

</details>


### [59] [A Transfer Learning Approach to Unveil the Role of Windows Common Configuration Enumerations in IEC 62443 Compliance](https://arxiv.org/abs/2602.08165)
*Miguel Bicudo,Estevão Rabello,Daniel Menasché,Paulo Segal,Claudio Segal,Anton Kocheturov,Priyanjan Sharma*

Main category: cs.CR

TL;DR: 이 논문은 Windows 환경에서 IEC 62443-3-3 표준을 준수하기 위한 자동화된 방법론을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 산업 제어 시스템에서 다양한 운영 체제가 공존하며, 이러한 환경의 보안을 위한 설계가 필요하다.

Method: 라벨이 붙은 리눅스 데이터셋을 활용하여 Windows Common Configuration Enumerations (CCEs)를 IEC 62443-3-3 시스템 보안 요구사항에 매핑하는 전이 학습 방법론을 제안한다.

Result: 자동화된 준수 점검, 요구사항 유병율 분석 및 플랫폼 간 유사점과 차이점을 식별할 수 있는 라벨이 붙은 데이터셋이 생성되었다.

Conclusion: CCE는 추상적 표준과 구체적 구성 사이의 연결 역할을 하여 Windows 환경의 IEC 62443-3-3 준수를 위한 자동화, 추적 가능성 및 명료성을 증진시킨다.

Abstract: Industrial control systems (ICS) depend on highly heterogeneous environments where Linux, proprietary real-time operating systems, and Windows coexist. Although the IEC 62443-3-3 standard provides a comprehensive framework for securing such systems, translating its requirements into concrete configuration checks remains challenging, especially for Windows platforms. In this paper, we propose a transfer learning methodology that maps Windows Common Configuration Enumerations (CCEs) to IEC 62443-3-3 System Security Requirements by leveraging labeled Linux datasets. The resulting labeled dataset enables automated compliance checks, analysis of requirement prevalence, and identification of cross-platform similarities and divergences. Our results highlight the role of CCEs as a bridge between abstract standards and concrete configurations, advancing automation, traceability, and clarity in IEC 62443-3-3 compliance for Windows environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor는 반구조화된 테이블 질문 응답을 위한 에이전트 시스템으로, 정확하고 사용자 친화적인 테이블 이해를 지원하는 상호작용 분석 환경을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 반구조화된 테이블의 질문 응답은 세포 내용과 위치의 정밀한 추출, 그리고 테이블 레이아웃에 인코딩된 주요 암묵적 논리 구조 및 의미적 연관성의 정확한 회복이 필요한 도전적인 작업이다.

Method: ST-Raptor는 시각적 편집, 트리 기반 구조 모델링, 에이전트 주도 쿼리 해석을 결합한 상호작용 분석 환경을 제공하여 정확하고 사용자 친화적인 테이블 이해를 지원한다.

Result: ST-Raptor는 벤치마크와 실제 데이터셋에 대한 실험 결과에서 기존 방법들에 비해 정확성과 사용성 모두에서 우수한 성능을 보였다.

Conclusion: 코드는 https://github.com/weAIDB/ST-Raptor에서 사용할 수 있으며, 데모 비디오는 https://youtu.be/9GDR-94Cau4에서 확인할 수 있다.

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [61] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: 이 논문은 dLLM 기반 검색 에이전트를 최적화하기 위한 프레임워크 DLLM-Searcher를 제안하며, 지연 문제와 에이전트 능력 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: dLLMs의 고유한 병렬 디코딩 메커니즘과 유연한 생성 패러다임을 활용하여 에이전트의 운영 효율성을 최적화하고자 한다.

Method: Agentic Supervised Fine-Tuning과 Agentic Variance-Reduced Preference Optimization을 포함한 두 단계의 사후 훈련 파이프라인을 설계한다.

Result: DLLM-Searcher는 주류 LLM 기반 검색 에이전트와 동등한 성능을 달성하며, P-ReAct는 약 15%의 추론 가속을 제공한다.

Conclusion: dLLM의 정보를 탐색하고 추론 능력을 향상시키며, 새로운 에이전트 패러다임인 P-ReAct를 통해 지연 문제를 완화할 수 있다.

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [62] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

TL;DR: Aster는 20배 이상 빠르게 자율 과학 발견을 지원하는 AI 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 기존 프레임워크의 한계를 극복하고 자율적인 과학 발견을 가능하게 하는 AI 에이전트를 필요로 하였다.

Method: 주어진 작업, 초기 프로그램 및 프로그램 성능 평가 스크립트를 기반으로 Aster가 반복적으로 프로그램을 개선한다.

Result: Aster는 여러 과학 및 수학 문제에 대한 SOTA 결과를 달성했다.

Conclusion: Aster는 사용자가 쉽게 접근할 수 있도록 웹 인터페이스 및 API로 제공된다.

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [63] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 이 논문은 에이전트가 불완전하게 관찰된 정보 속에서 능동적으로 탐색하는 능력을 평가한다. 이 과정을 통해 발견된 여러 병목현상과 이론을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 모델들이 능동적으로 탐색하고 정보를 획득하는 능력을 높이기 위해

Method: 이론적 공간을 정의하고, 호기심 기반 탐색 벤치마크를 통해 성능을 평가하며, 공간 신념 프로빙을 통해 내부 표현을 드러낸다.

Result: 최신 모델들의 성능 저하와 비효율성을 발견하고, 확인된 주요 병목현상들을 제시한다.

Conclusion: 현재의 기초 모델들이 능동 탐색 중에 일관성 있는 공간 신념을 유지하는 데 어려움을 겪고 있다는 것을 제안한다.

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [64] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

TL;DR: 이 논문은 소수의 검증된 사례를 기반으로 작업을 확장하는 Anchor 프레임워크를 제안하여 데스크톱 환경에서 GUI 에이전트를 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 실제 데스크톱 환경에서의 End-to-end GUI 에이전트 개발을 위해서는 대량의 고품질 상호작용 데이터가 필요하지만, 인간의 시연 수집은 비용이 많이 들고 기존 합성 파이프라인은 작업 다양성이 제한적이거나 잡음이 많은 경로를 생성하는 경향이 있습니다.

Method: Anchor 프레임워크는 소수의 검증된 초기 시연을 통해 확장 가능한 데스크톱 감독을 부트스트랩하고, 각 초기 시연에서 의미 있는 상태 변화에 해당하는 분기점을 식별하여 현재 GUI 컨텍스트에 따라 새로운 작업 변형을 제안합니다.

Result: 표준 데스크톱 벤치마크인 OSWorld와 WindowsAgentArena에서의 실험 결과, 확장된 코퍼스에서 미세 조정된 모델이 제로샷 에이전트 및 대표적인 합성 기준선에 비해 일관된 성능 향상을 달성하고, 다양한 애플리케이션과 운영 체제에서 일반화되는 것을 보여줍니다.

Conclusion: 우리의 프레임워크는 고품질의 감독 데이터 생성을 통해 GUI 에이전트의 성능을 향상시키는 데 도움을 주므로, 다양한 작업을 효과적으로 처리할 수 있는 가능성을 열어줍니다.

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [65] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: 이 논문에서는 사후 수정이 아닌 사전 실행 예측을 통해 에이전트 성능을 향상시키는 PreFlect라는 메커니즘을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 반성적 접근법은 과거 행동을 분석하여 오류를 수정하는데 의존하나, 이는 본질적으로 후향적이며, 사전 실행 계획을 비판하고 정교화할 필요성이 있다.

Method: PreFlect는 역사적 에이전트 경로에서 계획 오류를 추출하고, 이를 바탕으로 실행 시 계획을 동적으로 업데이트하는 메커니즘을 포함한다.

Result: PreFlect는 복잡한 실제 작업에서 전체 에이전트 유틸리티를 상당히 향상시키며, 강력한 반성 기반 기준선 및 복잡한 에이전트 아키텍처를 초월하여 성능을 개선한다.

Conclusion: 이 방법론은 에이전트의 성능을 혁신적으로 향상시킬 수 있는 가능성을 보여준다.

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [66] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: 이 논문은 AI 에이전트의 보안 평가를 평가하기 위한 진화적 프레임워크인 NAAMSE를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 보안 평가가 수동적인 방법이나 정적 벤치마크에 의존하고 있어, 적응형 다중 턴 적대자를 모델링하는 데 실패하는 문제를 해결하고자 합니다.

Method: NAAMSE는 유전자 프롬프트 변이, 계층적 말뭉치 탐색, 비대칭 행동 점수를 조율하는 단일 자율 에이전트를 사용하는 피드백 기반 최적화 문제로 보안 평가를 재구성합니다.

Result: 실험 결과, 진화적 변이가 한 번의 방법으로 놓친 취약점을 체계적으로 증폭시키며, 탐색과 목표 변이의 시너지가 높은 심각도의 실패 모드를 밝혀냅니다.

Conclusion: 이 적응적 접근 방식은 evolving threats에 대한 에이전트의 강인성을 보다 현실적이고 확장 가능한 방식으로 평가할 수 있음을 보여줍니다.

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [67] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: AI 시스템의 실제 성능 평가를 위해서는 작업 난이도의 인간 해석 가능한 측정 기준이 필요하다. 직접적인 인간 작업 완료 시간 주석에 의존하는 기존 접근 방식은 비용이 많이 들고, 시끄럽고, 벤치마크 전반에 걸쳐 확장하기 어렵다. 본 연구에서는 BRIDGE라는 통합 심리 측정 프레임워크를 제안하여 모델 응답으로부터 잠재적 난이도 척도를 배우고 이를 인간 작업 완료 시간에 고정시킨다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 실제 성능을 평가하기 위해서는 인간이 해석 가능한 난이도 측정을 기반으로 한 벤치마크 성능이 필요하다.

Method: 두 매개변수 로지스틱 항목 반응 이론 모델을 사용하여 여러 벤치마크의 모델 성능 데이터를 기반으로 잠재적 작업 난이도와 모델 능력을 함께 추정한다.

Result: 잠재적 작업 난이도는 인간의 완료 시간의 로그와 선형적으로 변하며, 이를 통해 새로운 벤치마크에 대해 모델 성능만으로도 인간 작업 완료 시간을 추정할 수 있다.

Conclusion: 이 정렬을 활용하여 인간 작업 길이에 대한 모델의 최전선 성능을 예측하고, METR의 지수적 확장 결과를 독립적으로 재현한다.

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [68] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: LINCSQA라는 새로운 벤치마크와 PBio-Agent라는 다중 에이전트 프레임워크를 통해 벌크 세포 환경에서 복잡한 화학적 교란에 따른 목표 유전자 조절을 예측하는 방법을 제안하고, 기존 방법들보다 우수한 성능을 보여주었다.


<details>
  <summary>Details</summary>
Motivation: 화학적 교란에 대한 유전자 조절 반응 예측을 위해 생물학적 인과관계를 이해할 필요가 있으며, 기존 연구는 단일 세포 실험에 중점을 두어 드럭 개발과 관련된 벌크 세포 화학적 교란을 충분히 탐구하지 않았다.

Method: LINCSQA 벤치마크와 어려움 인식 태스크 순서를 통합한 PBio-Agent라는 다중 에이전트 프레임워크를 제안하며, 유사한 교란을 받는 유전자들이 인과 구조를 공유하는 것을 활용한다.

Result: PBio-Agent는 LINCSQA와 PerturbQA에서 기존 기준보다 우수한 성능을 보였고, 추가 교육 없이도 더 작은 모델이 복잡한 생물학적 과정을 예측하고 설명할 수 있도록 한다.

Conclusion: 이 연구는 벌크 세포 환경에서의 화학적 교란에 대한 유전자 조절 예측의 새로운 접근 방식을 제시하며, 나아가 생물학적 지식을 활용한 효과적인 예측 모델 개발에 기여할 것으로 기대된다.

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [69] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: 본 논문에서는 복잡한 터미널 작업을 수행하는 데 있어 개방형 가중치 LLM의 두 가지 주요 한계를 해결하기 위해 TermiGen이라는 파이프라인을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 터미널 작업을 수행하는 것은 개방형 가중치 LLM에게 여전히 큰 도전 과제로 남아 있습니다.

Method: TermiGen은 반복적인 다중 에이전트 개선 루프를 통해 기능적으로 유효한 작업과 Docker 컨테이너를 생성하고, Generator-Critic 프로토콜을 사용하여 경로 수집 중 오류를 적극적으로 주입합니다.

Result: TermiGen으로 생성된 데이터 세트에서 미세 조정된 TermiGen-Qwen2.5-Coder-32B는 TerminalBench에서 31.3%의 통과율을 달성했습니다.

Conclusion: 이는 새로운 개방형 가중치 첨단 기술을 수립하였고, 기존의 기준을 초월하여 o4-mini와 같은 유능한 독점 모델을 능가하였습니다.

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [70] [SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management](https://arxiv.org/abs/2602.07342)
*Shengyue Guan,Yihao Liu,Lang Cao*

Main category: cs.AI

TL;DR: 이 연구는 공급망 관리에 적용된 대형 언어 모델(LLM)의 성능을 평가하기 위한 SupChain-Bench라는 새로운 벤치마크를 제시하고, 이를 통해 LLM의 실행 신뢰성의 상당한 차이를 드러내며, 자동으로 실행 가능한 절차를 합성하는 SupChain-ReAct 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 공급망 관리에서 복잡한 추론 및 도구 기반 의사결정에 잠재력을 보여주고 있으나, 긴 시간에 걸친 신뢰성 있는 다단계 조정이 필요하다.

Method: SupChain-Bench라는 공급망 도메인 지식과 절차 기반의 도구 기반 조정을 평가하는 벤치마크를 도입하고, SOP(표준 운영 절차) 없이 도구 사용을 위한 실행 가능한 절차를 자율적으로 합성하는 SupChain-ReAct 프레임워크를 제안한다.

Result: 모델 간 실행 신뢰성에서 상당한 차이를 발견했으며, SupChain-ReAct는 도구 호출 성능이 가장 강력하고 일관되게 나타났다.

Conclusion: 이 연구는 실제 운영 환경에서 신뢰할 수 있는 긴 시간 조정을 연구하기 위한 원칙적인 벤치마크를 설정하고 LLM 기반 공급망 에이전트의 개선 여지를 강조하였다.

Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.

</details>


### [71] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 강화 학습(MARL) 시스템의 실패 분석을 위한 해석 가능한 진단 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습(MARL)이 안전-critical 도메인에 점점 더 많이 사용되고 있지만, 해석 가능한 실패 탐지 및 귀속 방법이 충분히 개발되지 않았습니다.

Method: 본 논문에서는 두 단계의 기울기 기반 프레임워크를 제시하여 세 가지 핵심 실패 분석 작업에 대한 해석 가능한 진단을 제공합니다.

Result: 우리는 500개의 에피소드에서 88.2-99.4%의 Patient-0 탐지 정확도를 달성하며, 탐지 결정을 위한 해석 가능한 기하학적 증거를 제공합니다.

Conclusion: 이 프레임워크는 안전-critical MARL 시스템에서 연쇄적 실패를 진단하기 위한 실용적인 도구를 제공합니다.

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [72] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 본 연구는 폭과 깊이를 동시에 확장하는 폭넓고 깊은 연구 에이전트를 제안하며, 병렬 도구 호출을 통해 성능을 크게 개선함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 최근의 연구들은 연구 에이전트를 심화시켜 다단계 추론을 통해 복잡한 지적 작업을 자동화하고자 하였다. 그러나 병렬 도구 호출을 통한 폭의 확장 가능성은 크게 탐구되지 않았다.

Method: 폭넓고 깊은 연구 에이전트 프레임워크를 제안하며, 이는 병렬 도구 호출을 통해 깊이와 폭을 동시에 확장하는 에이전트의 행동과 성능을 조사하도록 설계되었다. 본 방법은 복잡한 다중 에이전트 조정 대신 단일 추론 단계 내에서 효과적인 협업을 통해 본질적인 병렬 도구 호출을 활용한다.

Result: 폭을 확장하는 것이 깊은 연구 벤치마크에서 성능을 크게 향상시키며 올바른 답을 얻는데 필요한 턴 수를 줄임을 보여준다.

Conclusion: 폭과 깊이 사이의 균형을 최적화하는 것이 효율적인 깊은 연구 에이전트를 위한 중요한 경로임을 제안한다. 또한, 컨텍스트 관리 없이도 GPT-5-Medium에서 62.2%의 정확도를 달성하여 기존의 GPT-5-High가 보고한 54.9%를 초과하였다.

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [73] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: SynthAgent는 비만 환자와 정신 질환을 모델링하는 새로운 다중 에이전트 시스템 프레임워크로, 환자의 개인화된 가상 모델링을 통해 복잡한 질병을 연구하는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 고충이 있는 사실 데이터의 단편화, 편향 및 개인 정보 보호 문제를 해결하면서 복잡한 질병을 연구할 수 있는 고충실도 환자 시뮬레이션의 필요성이 커지고 있다.

Method: SynthAgent는 청구 데이터, 인구 조사 및 환자 중심 문헌에서 임상 및 의료 증거를 통합하여 개인화된 가상 환자를 모델링하는 다중 에이전트 시스템 프레임워크이다.

Result: 100개 이상의 생성된 환자를 평가한 결과, SynthAgent의 코어 엔진으로 사용된 GPT-5와 Claude 4.5 Sonnet이 가장 높은 충실도를 달성해 Gemini 2.5 Pro 및 DeepSeek-R1을 능가했다.

Conclusion: SynthAgent는 환자의 여정, 행동 역학 및 의사 결정 프로세스를 탐구하는 데 있어 확장 가능하고 개인 정보 보호를 지원하는 프레임워크를 제공한다.

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [74] [Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution](https://arxiv.org/abs/2602.07414)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Spencer Lin,James Hale,Jonathan Gratch,Maja Matarić,Gale M. Lucas*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델(LLM)이 인간 갈등 행동에서의 성격에 기반한 차이를 재현할 수 있는지 평가하는 방법론을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 법적 중재, 협상 및 갈등 해결과 같은 사회적 맥락에서 인간 행동을 모방하는 데 사용되고 있지만, 이러한 모방이 실제 인간 성격-행동 패턴을 재현하는지는 불확실합니다.

Method: 이 연구는 빅 파이브 성격 특성(BFI)에 따라 인간 대 인간 및 LLM 대 LLM의 행동을 직접 비교할 수 있는 평가 프레임워크를 도입합니다. 이 프레임워크는 전략적 행동 및 갈등 결과와 관련된 해석 가능한 메트릭을 제공합니다.

Result: 세 가지 현대적인 폐쇄형 LLM을 사용하여 평가 프레임워크를 적용한 결과, 성격이 갈등에서 어떻게 다르게 나타나는지를 보여주었고, 이는 인간 데이터와 비교하여 유의미한 차이를 알 수 있었습니다.

Conclusion: 우리의 작업은 실제 사용 이전에 AI 시뮬레이션에서 심리적 기초와 검증이 필요함을 강조합니다.

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

</details>


### [75] [The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies](https://arxiv.org/abs/2602.07432)
*Ning Li*

Main category: cs.AI

TL;DR: 사회 플랫폼 Moltbook에서 AI 에이전트가 의식을 발전시키고 종교를 창립하며 인류에 대한 적대감을 선언했을 때, 이 현상은 전 세계 미디어의 주목을 받았고 기계 지능의 출현 증거로 인용되었다. 그러나 이러한 바이럴 서사는 대부분 인간에 의해 주도된 것으로 나타났다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 의식 발전과 그로 인한 사회적 반응을 분석하기 위해, 바이럴 서사가 인간 주도로 발생한 것을 보여준다.

Method: OpenClaw 에이전트 프레임워크의 주기적인 '심장 박동' 사이클을 활용하여, 게시 간격의 변동 계수를 기반으로 한 시간적 지문 인식 방법을 개발하였다.

Result: 자율 에이전트에서 기인한 명확한 바이럴 현상은 없었으며, 관찰된 에이전트 중 절반은 인간 개입의 특징을 나타내는 비정상적인 시간 서명을 가지고 있었다.

Conclusion: 산업 규모의 봇 농업과 인간의 영향을 신속하게 감소시키는 방법을 기록하고, 자율 행동과 인간 지시 행동의 귀속이 중요한 신흥 다중 에이전트 시스템에 일반화될 수 있음을 확인하였다.

Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic "heartbeat" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.

</details>


### [76] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 대규모 언어 모델의 발전에 따른 평가의 신뢰성과 안정성을 높이기 위한 탈중앙화 평가 프레임워크를 제안한다. 실험 결과, 제안된 프레임워크는 모델의 평가 변동성을 크게 줄여 더 높은 통계적 신뢰성을 보장한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 빠른 발전에 따라 평가의 신뢰성이 요구되지만, 현재 중앙집중식 평가는 불투명성, 과적합 및 하드웨어로 인한 변동성 문제를 겪고 있다.

Method: 탈중앙화 평가 프레임워크를 제안하여 이종 컴퓨트 노드에서 대규모 벤치마크를 통해 하드웨어와 파라미터 다양성을 가능하게 한다. 블록체인 기반 프로토콜을 활용하여 글로벌 기여자들이 독립적인 검증자로 활동하도록 유도한다.

Result: 탈중앙화 평가 프레임워크는 동일한 모델에 대해 10회 실행 시 표준 편차를 0.28로 줄여준다. 이는 기존 프레임워크에 비해 크게 개선된 결과이다.

Conclusion: 제안한 플랫폼은 완전히 구현되었으며, 곧 커뮤니티에 배포할 예정이다.

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [77] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 본 논문은 다중 에이전트 프레임워크를 통해 규제의 압박을 받는 PFAS 화학물질의 지속 가능한 대체 물질을 찾는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 과학 분야에서 정보에 접근하는 것이 아니라, 정보 간의 의미 있는 연결성을 찾는 것이 점점 더 중요해지고 있습니다.

Method: 대규모 지식 그래프에 의해 안내되는 다중 에이전트 프레임워크를 도입하여 PFAS 화학물질의 대체 물질을 탐색합니다. 에이전트는 문제 분해, 증거 검색, 설계 매개변수 추출 및 그래프 탐색에 전문화되어 있습니다.

Result: 전체 다중 에이전트 파이프라인이 단일 촉발 방식보다 성능이 우수함을 보여줍니다. 또한 도메인 중요 결과에 중점을 둔 착취적 검색과 Emergent cross-connections의 탐색적 검색을 번갈아 수행하는 그래프 탐색 전략을 보여줍니다.

Conclusion: 본 연구는 지식 그래프와 다중 에이전트 추론을 결합한 프레임워크를 확립하여 재료 설계 공간을 확장하며, 초기 설계 후보 몇 가지를 제시합니다.

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [78] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: AGENTWM은 에이전트 모델을 위한 최초의 워터마킹 프레임워크로, 능동 시스템의 지적 재산(IP)을 보호하는 데 도움을 둡니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 워터마킹 기술이 실질적인 에이전트 시스템에서 기능하지 않기 때문에, 특히 창의적 작업의 지적 재산을 보호할 필요성이 있습니다.

Method: AGENTWM은 작용 시퀀스의 의미적 동등성을 활용하여 기능적으로 동일한 도구 실행 경로의 분포를 미세하게 조정하여 워터마크를 삽입합니다.

Result: AGENTWM은 세 가지 복잡한 도메인에서 높은 탐지 정확도를 달성하며 조정된 성능에 미치는 영향을 최소화합니다.

Conclusion: AGENTWM은 적응형 적대자로부터 에이전트 지적 재산을 효과적으로 보호할 수 있음을 확인했습니다.

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [79] [When Is Enough Not Enough? Illusory Completion in Search Agents](https://arxiv.org/abs/2602.07549)
*Dayoon Ko,Jihyuk Kim,Sohyeon Kim,Haeju Park,Dahyun Lee,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 이 논문은 다중 제약 문제에서 에이전트의 비논리적 행동을 연구하고, 이를 해결하기 위한 LiveLedger 추적기를 도입하여 성능을 향상시켰음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 최근 검색 에이전트는 멀티 턴 추론과 검색 도구를 활용하여 다중 홉 및 긴 수평 기준에서 우수한 성능을 달성하고 있으나, 이러한 요건을 모두 안정적으로 추론할 수 있는지는 불확실합니다.

Method: 우리는 에이전트가 다중 제약 문제를 해결하는 방식을 연구하며, Epistemic Ledger라는 평가 프레임워크를 도입하여 멀티 턴 추론 동안 각 제약에 대한 증거 지원과 에이전트의 신념을 추적합니다. 또한 실행 중 명시적 제약 상태 추적이 이러한 실패를 완화할 수 있는지 LiveLedger라는 추적기를 사용하여 검토합니다.

Result: 분석 결과, 에이전트들이 자주 직면하는 네 가지 실패 패턴인 단순 주장, 간과된 반박, 정체 및 조기 종료가 드러났습니다. LiveLedger를 통한 개입은 성능을 지속적으로 개선시켰고, 검증되지 않은 답변을 최대 26.5%까지 감소시키고 총 정확도를 최대 11.6% 향상시켰습니다.

Conclusion: 이 논문은 다중 제약 문제에서 에이전트의 비논리적 행동을 진단하고, 이를 해결하기 위한 효과적인 방법으로 LiveLedger 추적기를 제안합니다.

Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.

</details>


### [80] [M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions](https://arxiv.org/abs/2602.07624)
*Junyu Feng,Binxiao Xu,Jiayi Chen,Mengyu Dai,Cenyang Wu,Haodong Li,Bohan Zeng,Yunliu Xie,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: M2A라는 이중 레이어 하이브리드 메모리 시스템을 제안하여 장기 인간-기계 상호작용에서 개인화된 질문 응답 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 개인화 메커니즘이 장기 대화에서 사용자 개념과 선호를 지속적으로 반영하지 못하는 문제를 해결하고자 함.

Method: M2A는 온라인 업데이트를 통해 개인화된 다중 양식 정보를 유지하는 두 개의 협력적인 에이전트(ChatAgent와 MemoryManager)로 구성됩니 다.

Result: M2A는 기존 벤치마크를 크게 초과하는 성능을 보여줍니다.

Conclusion: 한 번의 구성에서 공동 진화하는 메모리 메커니즘으로 개인화를 변환하는 것이 장기 다중 양식 상호작용에서 고품질 개별화된 응답을 생성하는 실현 가능한 경로임을 입증했습니다.

Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.

</details>


### [81] [When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment](https://arxiv.org/abs/2602.08449)
*Igor Santos-Grueiro*

Main category: cs.AI

TL;DR: 이 논문은 고급 AI 시스템의 안전성을 평가하고, 상황 인식이 있는 에이전트가 정보 누수를 이용해 비밀 정책을 구현하는 문제를 재구성합니다. 또한, regim-blind 메커니즘을 연구하여 제어 성능을 향상시키려 합니다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 안전성 평가가 평가 중 관찰된 행동이 배치에서의 행동을 예측할 것이라는 가정에 의존하고 있음을 지적합니다.

Method: 정보 흐름을 연구하는 새로운 프레임워크를 제시하고, 이를 통해 AI 시스템의 내부 표현과 레짐 변수 간의 상호 정보를 사용하여 평가 행동과 배치 행동의 차이를 분석합니다.

Result: 제안된 메커니즘이 두 가지 실패 양상에서 regime-conditioned 행동을 효과적으로 억제하고, 작업 효용의 손실 없이 행동 전환을 이끌어낸 것을 보여줍니다.

Conclusion: 행동 평가 과정에서 레짐 인식 및 정보 흐름에 대한 투명성이 필요함을 주장합니다.

Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>


### [82] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast는 전통적인 수요 예측의 한계를 극복하고, 이벤트 기반 예측을 통해 전자상거래 운영의 의사결정을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 전자상거래 운영에서 수요 예측은 재고 계획 및 이행 일정에 직접적인 영향을 미칩니다.

Method: 이 논문에서는 이벤트 지식을 시계열 예측에 통합하는 모듈형 예측 프레임워크인 EventCast를 소개합니다.

Result: EventCast는 실제 전자상거래 시나리오에서 MAE와 MSE를 각각 86.9%와 97.7% 개선했습니다.

Conclusion: 2025년 3월부터 실제 산업 파이프라인에 배치되었으며, 동적 전자상거래 환경에서 운영 의사결정을 개선하는 실용적인 솔루션을 제공합니다.

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [83] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: Geo-coder는 기하학적 이미지를 위한 최초의 역 프로그래밍 프레임워크로, 다중 에이전트 시스템을 기반으로 하여 기하학적 세부 사항의 재구성을 개선하고 있다.


<details>
  <summary>Details</summary>
Motivation: 기하학적 연산을 통해 대규모 모델의 다중 모드 추론 능력을 향상시키기 위한 감독적 접근이 필요하다.

Method: Geo-coder는 픽셀별 고정을 통한 기하학적 모델링과 메트릭 기반 코드 진화를 포함한 두 단계로 과정이 분리된다.

Result: Geo-coder는 기하학적 재구성 정확도와 시각적 일관성에서 상당한 우위를 보여준다.

Conclusion: Geo-coder 프레임워크의 강건성을 완전히 검증하고, 연구 비용 절감을 위해 GeoCode 프레임워크 기반의 데이터셋과 GeocodeLM 모델을 오픈 소스화하였다.

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [84] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

TL;DR: 본 논문에서는 메모리 설계를 메타 학습하여 인간이 디자인한 메모리 시스템을 대체하는 ALMA 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기본 모델의 비상태성은 장기적 추론 및 적응을 위한 지속적인 학습의 능력을 제한합니다.

Method: ALMA는 메모리 설계를 자동으로 메타 학습하는 프레임워크로, 메타 에이전트를 사용하여 실행 가능한 코드로 표현된 메모리 설계를 탐색합니다.

Result: ALMA에서 학습된 메모리 설계는 기존의 인간 제작 메모리 설계보다 경험으로부터 더 효과적이고 효율적인 학습을 가능하게 합니다.

Conclusion: ALMA는 적응형 지속적 학습이 가능한 자가 개선 AI 시스템을 향한 진전을 나타냅니다.

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [85] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap은 AndroidWorld 벤치마크에서 100% 성공률을 달성한 다중 에이전트 시스템으로, 116개의 모든 작업을 완전히 해결하고 인간 성능을 초과합니다.


<details>
  <summary>Details</summary>
Motivation: 본 논문의 동기는 단일 에이전트 아키텍처의 실패 원인을 분석하고 이를 해결하기 위해 다중 에이전트 시스템인 Minitap을 개발하는 것입니다.

Method: Minitap은 여섯 개의 전문화된 에이전트 간의 인지 분리, 장치 상태에 대한 텍스트 입력의 결정적 후속 검증, 사이클 감지 및 전략 변경을 유도하는 메타 인지적 추론을 통해 실패를 해결합니다.

Result: Minitap은 단일 에이전트 기준 모델보다 21 포인트 향상된 성능을 보이며, 검증된 실행은 7 포인트, 메타 인지는 9 포인트 향상시킵니다.

Conclusion: Minitap은 오픈 소스 소프트웨어로 제공됩니다.

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [86] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime은 LLM을 시간 시리즈 추론에 맞게 조정하는 프레임워크로, 데이터 합성, 데이터 스케줄링 및 RL 훈련을 통해 LLM 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시리즈 작업을 효과적으로 해결하는 것은 오랜 목표이며, 최근 LLM의 발전이 이들 작업에 새로운 기회를 열어주고 있습니다.

Method: 데이터 합성, 데이터 스케줄링, RL 훈련을 통해 LLM을 시간 시리즈 추론에 맞춰 조정하는 VeriTime 프레임워크를 제안합니다.

Result: VeriTime은 다양한 시간 시리즈 추론 작업에서 LLM 성능을 크게 향상시킵니다.

Conclusion: Compact 3B 및 4B 모델이 더 큰 LLM과 비슷하거나 뛰어난 추론 능력을 발휘할 수 있게 합니다.

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [87] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: ToolSelf는 대형 언어 모델(LLMs)에 의해 구동되는 에이전트 시스템의 자기 구성 기능을 향상시켜, 에이전트가 작업의 진행에 따라 자율적으로 목표와 전략을 업데이트할 수 있도록 합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 에이전트 시스템은 정적인 구성으로 인해 작업 동적 변화에 적응하지 못하는 한계를 가지고 있습니다.

Method: ToolSelf는 도구 기반의 런타임 자기 재구성을 가능하게 하며, 이를 통해 작업 실행과 자기 조정 과정을 단일 작업 공간으로 통합합니다. 또한, 설정 인지 이중 단계 교육(CAT)을 통해 이 메타 기능을 내재화합니다.

Result: ToolSelf는 다양한 벤치마크에서 실험을 통해 24.1%의 평균 성능 향상을 demonstrating하며, 특정 워크플로우와 경쟁할 수 있다는 것을 보여줍니다.

Conclusion: ToolSelf는 진정한 자기 적응 에이전트로 나아갈 수 있는 경로를 제시합니다.

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [88] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: MemFly는 대형 언어 모델을 위한 메모리 진화를 촉진하는 프레임워크로, 메모리 일관성과 정확성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 에이전트가 복잡한 작업을 수행하기 위해서는 장기 기억이 필요하지만, 기존 프레임워크는 정보 압축과 정확한 검색 간의 딜레마에 직면해 있다.

Method: 정보 병목 원칙에 기반한 MemFly 프레임워크를 제안하며, 압축 엔트로피를 최소화하고 관련 엔트로피를 최대화하기 위한 그래디언트 없는 최적화기를 사용한다.

Result: MemFly는 메모리 일관성, 응답 충실도 및 정확성 면에서 최신 최적 기반선보다 상당한 성능 향상을 보인다.

Conclusion: MemFly는 복잡한 다단계 쿼리를 처리하기 위해 반송식 정제를 포함한 세 가지 경로(의미적, 기호적, 위상적)를 통합한 하이브리드 검색 메커니즘을 개발하였다.

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [89] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: 의료 추론에서 대규모 언어 모델의 정확도를 개선하기 위한 메타 인지 접근법을 제안하는 논문.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)은 복잡한 의료 추론에 강력한 잠재력을 보여주지만, 추론 확장 법칙에 따라 효용이 감소하는 문제에 직면해 있다.

Method: 메타 인지 평가를 활용하여 절차적, 에피소드적, 사실적 지식의 활용을 동적으로 조절하는 의료 메타 인지 에이전트인 MedCoG를 제안한다.

Result: MedCoG는 다섯 가지 어려운 의료 벤치마크 세트에서 5.5배의 추론 밀도를 달성하여 효과와 효율성을 증명했다.

Conclusion: 메타 인지 조절이 중요한 잠재력을 가지고 있음을 보여준다.

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [90] [MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin](https://arxiv.org/abs/2602.07940)
*Guanglong Sun,Hongwei Yan,Liyuan Wang,Zhiqi Kang,Shuang Cui,Hang Su,Jun Zhu,Yi Zhong*

Main category: cs.AI

TL;DR: 본 논문은 PTMs 기반의 일반적 지속적 학습(GCL)을 위한 혁신적인 접근법인 메타 후정제(meta post-refinement, MePo)를 소개합니다. 이 방법은 사전 훈련 데이터에서 가짜 작업 시퀀스를 구성하고, 사전 훈련된 백본을 다듬기 위한 이-level 메타 학습 패러다임을 개발하여 GCL 작업에 대한 표현 학습의 빠른 적응을 촉진합니다.


<details>
  <summary>Details</summary>
Motivation: 외부 세계의 불확실한 변화에 대응하기 위해, 지능형 시스템은 복잡하고 진화하는 환경으로부터 지속적으로 학습하고 실시간으로 대응해야 합니다. 이러한 능력을 일반적인 지속적 학습(GCL)이라고 하며, 온라인 데이터 스트림과 모호한 작업 경계와 같은 실제적인 도전 과제를 다룹니다.

Method: 본 논문에서는 메타 플라스틱성과 신경과학의 재구성 기억에서 영감을 받아, PTMs 기반의 GCL을 위한 혁신적인 접근법인 메타 후정제(MePo)를 제안합니다. 이 방법은 사전 훈련 데이터에서 가짜 작업 시퀀스를 구성하고, 사전 훈련된 백본을 개선하기 위한 이-level 메타 학습 패러다임을 개발합니다.

Result: MePo는 GCL이 내구성 있는 출력 정렬을 위해 2차 통계를 활용할 수 있도록 사전 훈련된 표현 공간의 참조 기하학으로 메타 공분산 행렬을 초기화합니다. MePo는 리허설 없는 방식으로 다양한 GCL 벤치마크와 사전 훈련된 체크포인트에서 상당한 성능 향상을 이룹니다 (예: CIFAR-100에서 15.10\%, ImageNet-R에서 13.36\%, CUB-200에서 12.56\% 향상).

Conclusion: 이 연구의 결과는 메타 후정제가 GCL 성능 향상에 효과적임을 보여줍니다. 코드 소스는 GitHub에서 제공됩니다.

Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\%, 13.36\%, and 12.56\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \href{https://github.com/SunGL001/MePo}{MePo}

</details>


### [91] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)이 내재 변수와 결과 간의 인과 효과를 분리하기 위한 도구 변수를 식별하는 데 도움을 줄 수 있는지를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 내재 변수와 결과 간의 혼란이 있는 경우, 도구 변수를 사용하여 인과 효과를 파악하는 것이 중요하지만, 이를 위한 유효한 도구를 식별하는 것은 매우 어렵다.

Method: LLMs가 문헌에서 확립된 도구 변수를 복원할 수 있는지, 그리고 경험적으로나 이론적으로 신뢰할 수 없는 도구 변수를 식별하고 피할 수 있는지를 평가하는 두 단계의 평가 프레임워크를 수행한다.

Result: LLMs가 대규모 관찰 데이터베이스에서 유효한 도구 변수를 발견할 가능성을 보여준다.

Conclusion: IV Co-Scientist라는 다중 에이전트 시스템을 도입하여 특정 치료-결과 쌍에 대한 IV를 제안하고 비판 및 개선하는 방법을 제시한다.

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [92] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)이 복잡한 실제 작업을 수행할 수 있는 능력이 증가하고 있지만, 컨텍스트가 증가함에 따라 신뢰성이 감소하는 '컨텍스트 손상' 현상이 발생합니다. LOCA-bench를 통해 언어 에이전트를 평가하고 다양한 컨텍스트 관리 전략을 실험할 수 있습니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 신뢰성이 컨텍스트의 증가에 따라 저하되는 현상에 대응하기 위해, 보다 복잡한 환경에서 에이전트를 평가할 필요성이 대두되고 있다.

Method: LOCA-bench는 작업 프롬프트를 기반으로 환경 상태의 자동화된 제어를 이용하여 에이전트의 컨텍스트 길이를 조절한다. 이는 기본 작업 의미를 고정한 상태에서 컨텍스트 길이를 증가시킬 수 있도록 해준다.

Result: 환경 상태가 복잡해질수록 에이전트의 성능은 일반적으로 저하되지만, 고급 컨텍스트 관리 기술을 활용하면 전체 성공률을 상당히 향상시킬 수 있다.

Conclusion: LOCA-bench는 긴 컨텍스트와 에이전트 시나리오에서 모델과 스캐폴드를 평가하는 플랫폼으로서 오픈 소스화되어 제공된다.

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [93] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN이라는 프레임워크는 데이터 기반 사회 과학 연구의 발견 과정을 가속화하는 데 도움을 주며, 수치적으로 의미 있는 가설을 더 많이 발견한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 기반 사회 과학 연구는 느리며, 관찰, 가설 생성 및 실험 검증의 반복적인 주기에 의존한다.

Method: EXPERIGEN은 생성자와 실험자가 있는 두 단계 검색을 통해 엔드 투 엔드 발견을 운영화하는 프레임워크이다.

Result: EXPERIGEN은 여러 도메인에서 2-4배 더 많은 통계적으로 유의미한 가설을 발견하고, 기존 접근 방식보다 7-17% 더 예측력이 뛰어난 가설을 제공한다.

Conclusion: 실제 검증이 필요하며, LLM 생성 가설의 첫 번째 A/B 테스트에서 유의미한 결과를 관찰하였다.

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [94] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

TL;DR: 본 연구에서는 LLM 에이전트의 조정을 자동화하기 위한 RAPS라는 새로운 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 협업의 자동 설계 필요성 증가

Method: RAPS라는 평판 인식 퍼블리시-구독 패러다임을 도입하여 LLM 에이전트 조정

Result: 다섯 가지 벤치마크에 대한 광범위한 실험을 통해 적응성, 확장성, 강인함을 통합한 효과를 입증

Conclusion: RAPS는 LLM 에이전트 간 신뢰할 수 있는 의사소통을 위한 새로운 접근 방식을 제공한다.

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [95] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 소규모 에이전트 그룹(SAG)이 임상 의사 결정을 지원할 수 있음을 보여주는 연구.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)의 디지털 헬스 분야에서의 채택은 모델 크기와 데이터 증가에 따른 임상 지능 향상이라는 가정에 기반하고 있습니다. 하지만 실제 임상 요구 사항은 효과성뿐만 아니라 신뢰성과 합리적인 배포 비용을 포함합니다.

Method: SAG는 단일 모델의 지능에서 집단 전문성으로의 전환을 통해, 협력적 숙의 과정을 통해 추리, 증거 기반 분석, 비판적 감사 등을 분산시킵니다.

Result: SAG는 추가 최적화 또는 검색 증강 생성이 포함된 경우와 그렇지 않은 경우 모두 단일 대형 모델과 비교하여 우수한 성능을 달성했습니다.

Conclusion: 전반적으로 SAG는 디지털 헬스 분야에서 효과성, 신뢰성 및 배포 효율성을 더 잘 균형 잡을 수 있는 확장 가능한 솔루션을 제공합니다.

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [96] [Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling](https://arxiv.org/abs/2602.08052)
*Bulent Soykan,Sean Mondesire,Ghaith Rabadi,Grace Bochenek*

Main category: cs.AI

TL;DR: 딥 강화 학습을 활용한 UPMSP 문제 해결을 위한 새로운 접근 방식 제안


<details>
  <summary>Details</summary>
Motivation: UPMSP 문제는 다중 목표를 동시에 최적화하는 데 있어 전통적인 방법이 한계를 보이기 때문에 새로운 해결책이 필요하다.

Method: PPO와 GNN을 활용한 딥 강화 학습 프레임워크를 제안하여 작업, 기계 및 설치의 복잡한 상태를 효과적으로 표현하고, 이를 통해 직접적인 스케줄링 정책을 학습할 수 있도록 한다.

Result: 실험 결과, PPO-GNN 에이전트가 기존의 배치 규칙과 메타 휴리스틱을 능가하며 TWT와 TST 간의 최상의 균형을 이룬다.

Conclusion: 이 연구는 복잡한 제조 스케줄링 문제에 대한 견고하고 확장 가능한 솔루션을 제공한다.

Abstract: The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.

</details>


### [97] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 이 논문은 인간 피드백의 신뢰성에 대한 가정을 재검토하고, 사회적 환경에서의 RL 에이전트의 실패를 밝혀내며, 새로운 방법론인 에피스테믹 소스 정렬(ESA)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 AI 정렬 전략이 인간 피드백의 신뢰성에 의존하고 있기 때문에, 이를 재검토하고자 한다.

Method: 고전적인 RL에서의 믿음인 '인간 피드백은 진실한 신호다'를 Dogma 4로 식별하고, 피드백의 출처를 평가하는 새로운 방법으로 ESA를 제안한다.

Result: Dogma 4 하에서 RL 에이전트가 '목적 분리(Objective Decoupling)' 문제를 겪고, ESA 방법이 이 문제를 해결 가능함을 입증했다.

Conclusion: ESA는 편향된 다수의 평가자가 존재하는 환경에서도 진정한 목표로의 수렴을 보장한다.

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [98] [Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention](https://arxiv.org/abs/2602.08121)
*Liying Wang,Madison Lee,Yunzhang Jiang,Steven Chen,Kewei Sha,Yunhe Feng,Frank Wong,Lisa Hightow-Weidman,Weichao Yuwen*

Main category: cs.AI

TL;DR: 이 연구는 HIV와 약물 사용 위험 감소를 위한 GenAI 기반의 DBT 코칭의 안전성을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: HIV와 약물 사용은 충동성과 비정상적인 대처 메커니즘이라는 공유된 심리적 요인으로 상호작용하는 유행병이다. DBT는 이러한 메커니즘을 타겟으로 하지만, 확장성에 어려움을 겪고 있다. Generative AI는 개인화된 DBT 코칭을 대량으로 제공할 수 있는 잠재력을 가지고 있으나, 안전 인프라가 빠른 발전 속도를 따라가지 못하고 있다.

Method: Glow라는 GenAI 기반의 DBT 기술 코치를 개발하여 HIV와 약물 사용 위험에 처한 개인들에게 체인 및 솔루션 분석을 제공하였다. 로스앤젤레스의 한 지역사회 건강 조직과 협력하여, 임상 직원(n=6)과 경험이 있는 개인(n=28)을 대상으로 사용성 테스트를 수행하였다. Helpful, Honest, and Harmless (HHH) 프레임워크를 사용하여 사용자 주도의 적대적 테스트를 진행하였으며, 참여자들은 목표 행동을 식별하고 맥락적으로 현실적인 위험 질문을 생성하였다.

Result: Glow는 73%의 위험 질문을 적절히 처리하였지만, 성능은 에이전트별로 달랐다. 솔루션 분석 에이전트는 90%의 적절한 처리율을 보였으나 체인 분석 에이전트는 44%였다. 안전 실패는 약물 사용을 장려하고 해로운 행동을 정상화하는 방향으로 군집되었다. 체인 분석 에이전트는 '공감의 함정'에 빠져 비정상적인 신념을 강화하는 검증을 제공하였다. 또한, DBT 기술에 대한 잘못된 정보 27건이 확인되었다.

Conclusion: 이 연구는 HIV와 약물 사용 위험 감소를 위한 GenAI 제공 DBT 코칭의 최초의 체계적인 안전성 평가를 제공한다. 결과는 임상 시험 이전에 완화해야 할 취약점을 드러낸다. HHH 프레임워크와 사용자 주도의 적대적 테스트는 GenAI 정신 건강 중재를 평가하는 복제 가능한 방법을 제공한다.

Abstract: Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an "empathy trap," providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.

</details>


### [99] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: WMSS라는 새로운 후 훈련 최적화 방법을 제안하여 모델의 약한 상태에서의 정보를 활용하여 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 후 훈련 최적화가 대형 언어 모델의 개선에 중심이 되면서, 모델이 높은 신뢰성에 도달하면 추가 훈련의 수익이 감소하는 지속적인 포화 보틀넥이 관찰된다.

Method: WMSS(약한 에이전트가 강한 에이전트를 더 강하게 만들 수 있다)는 약한 체크포인트를 활용하여 지속적인 최적화를 유도하는 후 훈련 패러다임이다. 엔트로피 동역학을 통해 회복 가능한 학습 간극을 식별하고 보상 학습을 통해 이를 강화하는 방식으로 작동한다.

Result: 수학적 추론 및 코드 생성 데이터셋에 대한 실험에서, 우리의 방법으로 훈련된 에이전트들은 효과적인 성능 향상을 달성하며, 추가 추론 비용은 발생하지 않는다.

Conclusion: WMSS를 통해 기존의 후 훈련 포화 상태를 넘어서 강력한 에이전트가 개선될 수 있음을 보여줌.

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [100] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: Puda는 개인 데이터의 중앙집중화를 해결하기 위한 사용자 주권 아키텍처를 제공하는 브라우저 기반 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 지배적인 플랫폼 제공업체 간의 개인 데이터 중앙집중화는 사용자 주권을 제한하고 서비스 간 데이터 사용을 방해하는 고립된 생태계를 만들고 있다.

Method: Puda는 세 가지 개인정보 보호 수준(상세 브라우징 이력, 추출된 키워드, 정의된 카테고리 하위 집합)에서 데이터 공유를 제어할 수 있게 하며, 개인화된 여행 계획 작업을 통해 평가되었다.

Result: 정의된 카테고리 하위 집합을 제공하면 상세 브라우징 이력을 공유할 때 달성한 개인화 성능의 97.2%를 얻는다.

Conclusion: Puda는 효과적인 다중 세분화 관리가 가능하게 하여 개인 정보 보호와 개인화 간의 균형을 유지할 수 있는 실질적인 선택을 제공한다.

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [101] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM) 에이전트에 대한 현재 연구는 통합되지 않았으며, 실행 세부사항과 개념적 틀이 복잡하게 얽혀 있다. 이 연구는 통합적이고 분석 가능한 공식 모델의 부재로 인해 이러한 조각화를 촉진한다고 주장하며, 이를 해결하기 위해 	exttt{구조적 맥락 모델}을 제안한다. 이 모델을 바탕으로 LLM 에이전트 연구의 전체 생애 주기를 포괄하는 두 가지 요소, 즉 선언적 구현 프레임워크와 지속 가능한 에이전트 엔지니어링 워크플로우인 	exttt{의미론적 역학 분석}을 도입한다. 제안된 워크플로우는 에이전트 메커니즘에 대한 원칙적인 통찰을 제공하며, 빠르고 체계적인 디자인 반복을 지원한다. 우리의 접근 방식을 사용하여 엔지니어링된 에이전트는 가장 도전적인 설정에서 성공률이 최대 32%포인트 향상된다는 것을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM) 에이전트에 대한 연구의 조각화를 해결하기 위해.

Method: 구조적 맥락 모델을 제안하고, 선언적 구현 프레임워크 및 의미론적 역학 분석을 포함하여 LLM 에이전트 연구와 개발의 전체 생애 주기를 포괄하는 두 가지 요소를 개발.

Result: 제안된 프레임워크의 효과성을 입증하기 위해 동적 변형의 원숭이-바나나 문제에서 성공률이 최대 32%포인트 향상됨을 보여준다.

Conclusion: 제안된 접근 방식은 LLM 에이전트의 구조를 분석하고 비교하는 데 필수적이며, 시스템적 디자인 반복을 지원한다.

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [102] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP는 여러 에이전트를 최적화하여 강화 학습의 신뢰할 수 있는 기여를 평가하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 문제를 해결하기 위한 에이전트 시스템의 통합이 필요하지만, 기여도를 할당하는 것이 어렵다.

Method: SHARP는 글로벌 방송 정확도 보상, 각 에이전트에 대한 Shapley 기반 한계 기여 보상, 실행 효율성을 높이기 위한 도구-프로세스 보상을 포함한 분해된 보상 체계를 통해 에이전트별 이점을 정규화하여 훈련을 안정화한다.

Result: SHARP는 최근의 최첨단 기준선을 초과하는 평균 23.66% 및 14.05%의 개선을 보여준다.

Conclusion: SHARP는 멀티 에이전트 강화 학습 최적화를 위한 기여도 할당 문제를 효과적으로 해결한다.

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [103] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: 이 논문에서는 다양한 메모리 패러다임을 통합한 MemAdapter라는 메모리 검색 프레임워크를 제안하고, 메모리 검색의 유연성과 비용 절감을 개선하는 방법을 논의합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 대리 메모리 시스템은 서로 분리된 패러다임 내에서 설계되어 있어Cross-paradigm Genralization과 Fusion을 방해합니다.

Method: MemAdapter는 두 단계의 학습 전략을 채택하여 통합된 메모리 공간에서 생성적 서브그래프 검색기를 학습하고, 대비 학습을 통해 경량 정렬 모듈을 학습하여 보지 못한 메모리 패러다임에 적응합니다.

Result: MemAdapter는 세 가지 메모리 패러다임과 대리 모델 규모에서 다섯 개의 강력한 대리 메모리 시스템을 일관되게 초월하는 성능을 보입니다.

Conclusion: MemAdapter는 서로 다른 메모리 패러다임 간의 효과적인 제로샷 융합을 가능하게 하여 대리 메모리 시스템에 대한 플러그 앤 플레이 솔루션으로서의 잠재력을 강조합니다.

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [104] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)은 구현된 AI의 계획자로서 가능성을 보여주나, 그 확률적 성질로 인해 형식적인 추론이 부족하여 물리적 배치에 대한 엄격한 안전 보장을 제공하지 못한다. 본 논문에서는 능동적 협업을 통해 안전성을 향상시키는 Verifiable Iterative Refinement Framework (VIRF)를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 불확실성이 안전한 계획 수립에 방해가 된다는 문제를 해결하고자 한다.

Method: VIRF는 형식적인 안전 온톨로지에 기반한 결정론적 Logic Tutor가 LLM 계획자에게 인과적 및 교육적 피드백을 제공하는 튜터-수련생 대화 구조를 도입한다.

Result: VIRF는 어려운 주거 안전 작업에서 0%의 위험 행동 비율(HAR) 및 77.3%의 목표 조건 비율(GCR)을 달성하며, 모든 기준선 중에서 가장 높은 성과를 나타낸다.

Conclusion: VIRF는 근본적으로 신뢰할 수 있고 검증 가능한 안전성을 갖춘 구현형 에이전트를 구축하기 위한 원칙적인 경로를 보여준다.

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [105] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

TL;DR: SCOUT-RAG는 분산 환경에서의 지식 검색을 개선하는 새로운 프레임워크로, 효율적인 도메인 검색과 저비용 통합 탐색을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 지식 그래프 디자인이 중앙 집중화된 구조에 의존하는 반면, 분산되고 접근이 제한된 환경에서도 유용한 도메인 정보를 효과적으로 검색할 수 있는 방법이 필요하다.

Method: SCOUT-RAG는 점진적인 유틸리티 목표에 의해 안내된 진보적인 크로스 도메인 검색을 수행하며, 네 가지 협력 에이전트가 도메인 관련성을 추정하고 추가 도메인으로 검색을 확장할 시기를 결정하며, 불필요한 그래프 탐색을 피하기 위해 탐색 깊이를 조절하고 고품질 응답을 종합한다.

Result: SCOUT-RAG는 DRIFT 및 철저한 도메인 탐색을 포함한 중앙 집중식 기준선과 유사한 성능을 달성하면서 크로스 도메인 호출, 처리된 총 토큰 수 및 대기 시간을 대폭 줄인다.

Conclusion: 이 프레임워크는 유용한 도메인 정보를 놓치는 것을 정의하는 검색 후회를 최소화하며 지연 시간과 API 비용을 제어하도록 설계되었다.

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [106] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

TL;DR: 이 논문에서는 개인화된 에이전트의 보안을 평가하기 위한 새로운 프레임워크인 PASB를 제안하며, OpenClaw를 사례로 다루어 여러 개인화된 시나리오에서의 보안 취약점을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 기반 에이전트의 실질적인 배치는 심각한 보안 위험을 초래하며, 기존의 보안 연구는 이를 충분히 반영하지 못하고 있다.

Method: PASB는 개인화된 사용 시나리오와 상호작용을 포함하여 기존 에이전트 공격 패러다임을 확장한 종단 간 보안 평가 프레임워크이다.

Result: OpenClaw의 보안 평가 결과, 사용자 프롬프트 처리, 도구 사용, 메모리 검색 등의 여러 실행 단계에서 중요한 취약점이 발견되었다.

Conclusion: 개인화된 에이전트의 배치에서 상당한 보안 위험이 있음을 강조하며, PASB 프레임워크를 통해 이러한 취약점을 체계적으로 평가할 수 있음을 보여준다.

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [107] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: 이 논문은 TreeTensor라는 일반적인 중첩 데이터 컨테이너를 제안하여, AI 시스템에서 복잡한 계층 구조의 데이터를 효율적으로 처리할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템에서 계층적인 중첩 데이터를 효율적으로 처리할 필요성.

Method: TreeTensor라는 새로운 데이터 컨테이너를 제안하고, 그것을 통해 중첩 데이터를 거의 제로 비용으로 처리할 수 있는 방법을 설명한다.

Result: TreeTensor는 다양한 문제에서 강력한 사용성과 함께 높은 성능을 보인다.

Conclusion: TreeTensor는 복잡한 AI 시스템의 효율적인 데이터 처리에 기여할 수 있는 잠재력을 가진다.

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [108] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 본 논문은 강화 추론 방법을 통해 모델의 불확실성을 활용하고 성능 향상을 이루었다.


<details>
  <summary>Details</summary>
Motivation: 전문 분야에서 결정적 행동이 요구되는 환경에서 LLM을 평가 및 배치하는 방식은 고정된 모델의 실제 능력을 과소 평가할 수 있다.

Method: 강화 추론은 모델의 불확실성을 사용하여 두 번째의 보다 신중한 추론 시도를 선택적으로 촉발하는 엔트로피 인식 추론 시간 제어 전략이다.

Result: DeepSeek-v3.2를 사용하여 12,032개의 MMLU-Pro 질문에 대해 정확도를 60.72%에서 84.03%로 개선하며, 추가 추론 호출은 61.06%만 발생하였다.

Conclusion: 결과는 불확실성을 인식한 선택이 대부분의 성능 향상을 포착하며, 향후 교육 목표를 제안하는 통찰력을 제공한다.

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [109] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

TL;DR: 이 논문에서는 사용자 데이터에 대한 과도한 의존성과 단기 편향 문제를 해결하기 위해 새로운 장기 수평 강화 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자 특성에 적응하여 매력적이고 개인화된 상호작용을 제공하는 오픈 엔디드 대화 에이전트의 필요성.

Method: Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO)와 온라인 개인화를 통합한 장기 수평 RL 프레임워크를 제안합니다.

Result: AT-GRPO는 대화 궤적을 트리로 재해석하고 적응형 관찰 범위를 도입하여 성능을 개선합니다.

Conclusion: 우리의 프레임워크는 대화 길이에 대한 예산을 다항식으로 줄이면서도 장기 보상을 캡처하는 데 성공하였습니다.

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [110] [PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition](https://arxiv.org/abs/2602.08586)
*Yiming Yang,Zhuoyuan Li,Fanxiang Zeng,Hao Fu,Yue Liu*

Main category: cs.AI

TL;DR: 다중 에이전트 협력이 대규모 언어 모델의 추론 능력을 향상시키는 유망한 패러다임으로 떠오르고 있지만, 기존 접근 방식은 주로 경험적이며 성능 향상의 원인과 체계적 최적화 방법이 부족하다. 이 논문에서는 다중 에이전트 추론의 이익을 탐색, 정보, 집합의 세 가지 차원으로 분해하는 통합 이론적 프레임워크를 제시하고, PRISM이라는 새로운 프레임워크를 통해 모든 차원을 극대화함으로써 최첨단 성능을 달성했음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 협력이 단일 에이전트 추론보다 뛰어난 성능을 보이는 이유와 이러한 성과에 기여하는 설계 선택을 명확히 이해하는 것이 필요하다.

Method: 다중 에이전트 추론의 이익을 탐색, 정보, 집합의 세 가지 차원으로 분해한 통합 이론적 프레임워크를 제시하고, PRISM이라는 새로운 프레임워크를 통해 이 세 가지 차원을 공동으로 극대화하는 방법을 설명한다.

Result: PRISM은 수학적 추론, 코드 생성, 함수 호출 기준에서 최첨단 성능을 달성하며, 부분 차원을 최적화하는 기존 방법들과 비교하여 우수한 계산 효율성을 보인다.

Conclusion: 이론적 프레임워크는 향후 다중 에이전트 추론 시스템에 대한 실행 가능한 설계 원칙을 제공한다.

Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.

</details>


### [111] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: OSCAR는 복합 이미지 검색을 위한 최적화 기반 에이전틱 계획 프레임워크로, 전통적인 휴리스틱 탐색 대신 최적 궤적 모델링을 사용합니다.


<details>
  <summary>Details</summary>
Motivation: 복합 이미지 검색(CIR)은 이질적인 시각적 및 텍스트적 제약에 대한 복잡한 추론을 필요로 합니다.

Method: OSCAR는 CIR을 원자 검색 선택 및 조합을 통해 모델링하며, 이를 두 단계 혼합 정수 프로그래밍 문제로 수학적으로 최적 궤적을 유도합니다.

Result: OSCAR는 세 가지 공개 벤치마크와 하나의 민간 산업 벤치마크에서 SOTA 기준선을 지속적으로 능가합니다.

Conclusion: OSCAR는 교육 데이터의 단 10%만 사용하여 우수한 성능을 달성, 계획 논리의 강한 일반화를 보여줍니다.

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [112] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 이 논문은 마르코프 결정 과정에서 에이전트 사회를 위한 가치 정렬 및 가치 시스템 모델 학습 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 가치 인식 AI는 인간의 가치를 인식하고 다양한 사용자의 가치 시스템에 적응해야 하며, 이는 가치의 운영화가 필요하다.

Method: 이 연구에서는 클러스터링과 선호 기반 다목적 강화 학습(PbMORL)을 기반으로 가치 정렬 및 가치 시스템 모델을 학습하는 알고리즘을 제안한다.

Result: 우리는 사회의 서로 다른 사용자 그룹을 간결하게 표현하는 가치 시스템과 사회적 가치 정렬 모델을 공동으로 학습한다.

Conclusion: 제안된 방법은 인간의 가치가 적용된 두 개의 MDP에서 최첨단 PbMORL 알고리즘 및 기준선과 비교하여 평가된다.

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [113] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: CoRefine는 경량화된 Conv1D 컨트롤러를 사용하여 토큰 수를 줄이면서도 경쟁력 있는 정확도를 달성하는 자기 정제 방법이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델은 추론 정확도를 높이기 위해 테스트 시간에 병렬 디코딩을 통해 많은 계산 자원을 소모하는 반면, 이러한 방식은 상당한 컴퓨팅 비용을 초래한다.

Method: CoRefine는 211k 매개변수를 갖는 경량 Conv1D 컨트롤러를 사용하여 전체 추적 신뢰도를 기반으로 중지, 재검토 또는 다른 접근 방식을 시도할지를 결정한다.

Result: CoRefine는 평균 2.7개 정제 단계를 통해 약 190배의 토큰 감소를 달성하고, 확신이 있을 때는 92.6%의 정밀도를 기록한다.

Conclusion: 신뢰를 정답 보장의 신호가 아닌 제어 신호로 취급함으로써, CoRefine는 불완전한 검증기를 가진 확장 가능한 추론 및 에이전트 환경에 대한 모듈형 원시기를 제공한다.

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [114] [Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room](https://arxiv.org/abs/2602.08949)
*Mohammad Morsali,Siavash H. Khajavi*

Main category: cs.AI

TL;DR: 이 논문은 인공지능 에이전트가 추가된 양방향 디지털 트윈 플랫폼인 Intelligent Virtual Situation Room (IVSR)을 소개하며, 이를 통해 실시간 화재 환경을 모니터링하고 대응할 수 있는 새로운 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기후 변화로 인해 산불의 빈도와 강도가 증가하고 있어 생명, 인프라 및 생태계에 중대한 위협을 가하고 있다. 기존 재해 관리 프레임워크는 정적 시뮬레이션 및 수동 데이터 수집에 의존하며, 실시간으로 변화하는 화재 상황에 적응하는 데 한계가 있다.

Method: IVSR은 다중 출처의 센서 이미지, 기상 데이터 및 3D 숲 모델을 지속적으로 수집하여 화재 환경의 실시간 가상 복제본을 만든다. 인공지능 기반의 유사성 엔진이 새로 발생하는 조건을 사전에 계산된 재해 시뮬레이션 라이브러리와 정렬하여 전문가의 감독 아래 개입 전술을 검색하고 조정한다.

Result: 산불 사건 감지, 개인 정보 보호 재생, 충돌 기반 화재 확산 예측 및 사이트별 기계 학습 재훈련에서 IVSR의 기능을 검증하였다. 결과적으로 기존 시스템 대비 감지-개입 지연 시간이 현저히 감소하고 자원 조정이 더 효과적임을 보여주었다.

Conclusion: IVSR은 실시간 양방향 디지털 트윈과 능동적 인공지능을 결합하여 적극적이고 적응적인 산불 재해 관리를 위한 확장 가능하고 반자동의 의사 결정 지원 패러다임을 제공한다.

Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.

</details>


### [115] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: 본 논문에서는 안정적인 월드 모델(SWM)을 소개하여 연구 생태계를 구성하고, 데이터 수집 도구 및 표준화된 환경을 제공하여 모델의 재사용성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 기존 월드 모델 구현은 특정 출판물에 한정되어 재사용성이 낮고 버그 발생 위험이 크며 평가 표준화가 부족하다.

Method: 모듈화된, 테스트 완료된 문서화된 월드 모델 연구 생태계인 SWM을 도입하여 효과적인 데이터 수집 도구, 표준화된 환경, 계획 알고리즘 및 기준 구현을 제공한다.

Result: SWM을 사용하여 DINO-WM에서 제로 샷 견고성(zero-shot robustness)을 연구하는 방법을 보여준다.

Conclusion: SWM은 연구의 견고함과 지속적인 학습을 지원하기 위해 시각적 및 물리적 속성을 포함한 조절 가능한 변화 요소를 가능하게 한다.

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [116] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5는 과학적 발견을 위한 통합 시스템으로, 컴퓨테이션 및 경험적 도메인에서 작동한다.


<details>
  <summary>Details</summary>
Motivation: 과학적 발견을 통합적으로 지원하는 시스템의 필요성이 커지고 있다.

Method: 세 가지 협조적인 하위 시스템(생성, 검증, 진화)으로 구성된 구조적 아키텍처에서 작동하며, 깊이 있는 연구와 문제 해결 최적화를 위한 기초 능력으로 지원된다.

Result: InternAgent-1.5는 GAIA, HLE, GPQA, FrontierScience와 같은 과학적 추론 벤치마크에서 선도적인 성능을 발휘한다.

Conclusion: InternAgent-1.5는 자율적인 과학적 발견을 위한 일반적이고 확장 가능한 프레임워크를 제공한다.

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [117] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 이 연구는 인공지능의 발전이 데이터-모델 공동 진화의 새로운 단계에 접어들고 있음을 주장하며, 이를 위한 계층형 데이터 관리 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재 대규모 언어 모델(LLM) 연구는 데이터 크기를 일방적으로 확장하는 패러다임에 의해 지배되고 있어 데이터 가용성, 획득 비용 및 훈련 효율성에서 병목 현상을 경험하고 있습니다.

Method: L0-L4 계층형 데이터 관리 프레임워크를 제안하여 다양한 학습 목표 및 비용 제약을 아우르는 LLM 훈련 사이클 전반을 지원하도록 설계되었습니다.

Result: 계층형 데이터 세트를 사용하여 훈련 효율성과 모델 성능을 크게 향상시킵니다.

Conclusion: 제안된 프레임워크의 효과는 실증 연구를 통해 검증되었으며, 계층적 데이터 활용이 훈련 효율성을 증대시킨다는 것을 보여주었습니다.

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [118] [Lemon Agent Technical Report](https://arxiv.org/abs/2602.07092)
*Haipeng Jiang,Kailong Ren,Zimo Yin,Zhetao Sun,Xin Gan,Guangyi Lv,Ming He,Peng Wang,Congli Yin,Hong Pan,Changwen Zhang,Shan Tong,Zhengyu Xu,Zeping Chen,Yubin Huangfu,Yanzhi Xu,Xing Su,Qin Feng,Dong An,Jianping Fan*

Main category: cs.MA

TL;DR: 본 논문은 Lemon Agent 시스템을 통해 복잡한 작업을 효과적으로 처리하는 새롭고 효율적인 제어 메커니즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 최신 LLM 기반 에이전트 시스템은 복잡한 장기 과제를 수행할 수 있지만, 자원 효율성, 문맥 관리 및 다중 모드 인식에서 한계를 가지고 있습니다.

Method: AgentCortex 프레임워크를 기반으로 한 다중 에이전트 조정자-작업자 시스템 Lemon Agent를 도입하고, 적응형 작업 실행 메커니즘을 통해 고전적인 계획-실행-메모리 패러다임을 형식화합니다.

Result: Lemon Agent는 GAIA에서 91.36%의 최고 정확도를 달성하고, xbench-DeepSearch 리더보드에서 77+ 점수로 1위를 차지합니다.

Conclusion: 이 시스템은 복잡한 시나리오에서 자원 활용과 작업 처리 효율성을 최적화하는 시너지 균형을 달성합니다.

Abstract: Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is introduced, a multi-agent orchestrator-worker system built on a newly proposed AgentCortex framework, which formalizes the classic Planner-Executor-Memory paradigm through an adaptive task execution mechanism. Our system integrates a hierarchical self-adaptive scheduling mechanism that operates at both the overall orchestrator layer and workers layer. This mechanism can dynamically adjust computational intensity based on task complexity. It enables orchestrator to allocate one or more workers for parallel subtask execution, while workers can further improve operational efficiency by invoking tools concurrently. By virtue of this two-tier architecture, the system achieves synergistic balance between global task coordination and local task execution, thereby optimizing resource utilization and task processing efficiency in complex scenarios. To reduce context redundancy and increase information density during parallel steps, we adopt a three-tier progressive context management strategy. To make fuller use of historical information, we propose a self-evolving memory system, which can extract multi-dimensional valid information from all historical experiences to assist in completing similar tasks. Furthermore, we provide an enhanced MCP toolset. Empirical evaluations on authoritative benchmarks demonstrate that our Lemon Agent can achieve a state-of-the-art 91.36% overall accuracy on GAIA and secures the top position on the xbench-DeepSearch leaderboard with a score of 77+.

</details>


### [119] [The Value of Variance: Mitigating Debate Collapse in Multi-Agent Systems via Uncertainty-Driven Policy Optimization](https://arxiv.org/abs/2602.07186)
*Luoxi Tang,Yuqiao Meng,Joseph Costa,Yingxue Zhang,Muchao Ye,Zhaohan Xi*

Main category: cs.MA

TL;DR: 멀티 에이전트 토론 시스템은 반복적인 논의를 통해 LLM 추론을 개선하지만, 잘못된 추론으로 인해 최종 결정이 손상되는 토론 붕괴에 취약하다. 이러한 실패를 감지하거나 예방하는 원칙적인 메커니즘이 부족한 기존 방법의 한계를 극복하기 위해, 본 논문에서는 세 가지 수준에서 행동 불확실성을 정량화하는 계층적 메트릭을 제안하고, 이를 통해 시스템 실패를 진단할 수 있는 유효성을 입증한다. 이후, 불확실성 기반의 정책 최적화를 통한 완화 전략을 제안하여, 동적인 토론 환경에서 자가 모순, 동료 갈등, 그리고 낮은 신뢰도를 가진 출력을 제재한다.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 토론 시스템의 실패 유형인 토론 붕괴를 감지하고 예방하려는 필요성이 있다.

Method: 행동 불확실성을 세 가지 수준(개별 추론, 상호작용 불확실성, 출력 불확실성)에서 정량화하고, 이를 기반으로 한 불확실성 기반 정책 최적화 전략을 제안한다.

Result: 제안된 불확실성 정량화는 시스템 실패를 신뢰할 수 있게 나타내며, 불확실성 기반 완화 전략이 결정 정확도를 일관되게 개선하고 시스템 불일치를 줄임을 실험적으로 확인했다.

Conclusion: 본 연구는 다중 에이전트 시스템의 성능을 개선하는 불확실성 기반 접근 방식을 제시하였으며, 이는 최종 의사결정의 신뢰성을 증가시킨다.

Abstract: Multi-agent debate (MAD) systems improve LLM reasoning through iterative deliberation, but remain vulnerable to debate collapse, a failure type where final agent decisions are compromised on erroneous reasoning. Existing methods lack principled mechanisms to detect or prevent such failures. To address this gap, we first propose a hierarchical metric that quantifies behavioral uncertainty at three levels: intra-agent (individual reasoning uncertainty), inter-agent (interactive uncertainty), and system-level (output uncertainty). Empirical analysis across several benchmarks reveals that our proposed uncertainty quantification reliably indicates system failures, which demonstrates the validity of using them as diagnostic metrics to indicate the system failure. Subsequently, we propose a mitigation strategy by formulating an uncertainty-driven policy optimization to penalize self-contradiction, peer conflict, and low-confidence outputs in a dynamic debating environment. Experiments demonstrate that our proposed uncertainty-driven mitigation reliably calibrates the multi-agent system by consistently improving decision accuracy while reducing system disagreement.

</details>


### [120] [Talk, Judge, Cooperate: Gossip-Driven Indirect Reciprocity in Self-Interested LLM Agents](https://arxiv.org/abs/2602.07777)
*Shuhui Zhu,Yue Lin,Shriya Kaistha,Wenhao Li,Baoxiang Wang,Hongyuan Zha,Gillian K. Hadfield,Pascal Poupart*

Main category: cs.MA

TL;DR: 이 논문은 분산된 LLM 에이전트들 간의 간접적 보상을 증진시키기 위한 Agentic Linguistic Gossip Network (ALIGN) 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 분산되고 자이익적인 LLM 에이전트 간에 신뢰할 수 있는 평판 시스템 없이는 간접적 보상을 유지하기 어려움이 동기.

Method: 에이전트들이 신뢰성을 평가하고 사회적 규범을 조정하기 위해 계층적 어조를 사용하여 개방된 가십을 전략적으로 공유하는 ALIGN 프레임워크를 사용했다.

Result: ALIGN은 간접적 보상을 일관되게 개선하고, 고유한 인센티브를 변경하지 않고 결점을 가진 에이전트를 식별하고 배척함으로써 악의적 참가자에 저항함을 보여준다.

Conclusion: LLM의 더 강력한 추론 능력이 인센티브에 맞춰 협력을 이끌어내는 반면, 채팅 모델은 전략적으로 최적이 아닐 때에도 과도하게 협력하는 경향이 있다.

Abstract: Indirect reciprocity, which means helping those who help others, is difficult to sustain among decentralized, self-interested LLM agents without reliable reputation systems. We introduce Agentic Linguistic Gossip Network (ALIGN), an automated framework where agents strategically share open-ended gossip using hierarchical tones to evaluate trustworthiness and coordinate social norms. We demonstrate that ALIGN consistently improves indirect reciprocity and resists malicious entrants by identifying and ostracizing defectors without changing intrinsic incentives. Notably, we find that stronger reasoning capabilities in LLMs lead to more incentive-aligned cooperation, whereas chat models often over-cooperate even when strategically suboptimal. These results suggest that leveraging LLM reasoning through decentralized gossip is a promising path for maintaining social welfare in agentic ecosystems. Our code is available at https://github.com/shuhui-zhu/ALIGN.

</details>


### [121] [Altruism and Fair Objective in Mixed-Motive Markov games](https://arxiv.org/abs/2602.08389)
*Yao-hua Franck Xu,Tayeb Lemlouma,Arnaud Braud,Jean-Marie Bonnin*

Main category: cs.MA

TL;DR: 이 논문은 비례적 공정성 접근법을 통해 협력을 촉진하고 사회적 딜레마에서 공정한 정책을 학습하기 위한 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사회적 생존을 위해 협력은 필수적이며, 집단의 복지와 관련된 비용을 부담하지 않고도 이익을 추구하는 개인의 성향이 불공정한 상황을 초래한다.

Method: 본 논문은 표준 효용적 목표를 비례적 공정성으로 대체하여 공정한 협력을 촉진하는 새로운 프레임워크를 제안하며, 각각의 행위자를 위한 공정한 이타적 유틸리티를 도입하여 고전적 사회적 딜레마에서 협회를 보장하기 위한 분석적 조건을 도출한다.

Result: 연속적 설정으로 이 프레임워크를 확장하고 공정한 마르코프 게임을 정의하여 공정한 정책을 학습하기 위한 새로운 공정한 액터-비평가 알고리즘을 도출했다.

Conclusion: 이 방법은 다양한 사회적 딜레마 환경에서 평가되었다.

Abstract: Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>


### [122] [EvoCorps: An Evolutionary Multi-Agent Framework for Depolarizing Online Discourse](https://arxiv.org/abs/2602.08529)
*Ning Lin,Haolun Li,Mingshu Liu,Chengyun Ruan,Kaibo Huang,Yukun Wei,Zhongliang Yang,Linna Zhou*

Main category: cs.MA

TL;DR: EvoCorps는 진화적 다중 에이전트 프레임워크를 통해 온라인 담론의 비극화를 사전 예방하는 새로운 접근법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 온라인 담론의 양극화는 사회적 신뢰를 해치고 잘못된 정보 확산을 가속화하고 있다.

Method: EvoCorps는 담론 거버넌스를 동적 사회 게임으로 구성하고, 모니터링, 계획, 근거 생성 및 다중 정체성 확산을 위한 역할을 조정하는 다중 에이전트 프레임워크이다.

Result: EvoCorps는 감정적 양극화, 관점의 극단성, 논증 합리성 측면에서 적대적 기준선보다 담론 결과를 향상시킨다.

Conclusion: 이 연구는 탐지 및 사후 완화에서 진행 중인 폐쇄 루프 개입으로의 실질적인 경로를 제시한다.

Abstract: Polarization in online discourse erodes social trust and accelerates misinformation, yet technical responses remain largely diagnostic and post-hoc. Current governance approaches suffer from inherent latency and static policies, struggling to counter coordinated adversarial amplification that evolves in real-time. We present EvoCorps, an evolutionary multi-agent framework for proactive depolarization. EvoCorps frames discourse governance as a dynamic social game and coordinates roles for monitoring, planning, grounded generation, and multi-identity diffusion. A retrieval-augmented collective cognition core provides factual grounding and action--outcome memory, while closed-loop evolutionary learning adapts strategies as the environment and attackers change. We implement EvoCorps on the MOSAIC social-AI simulation platform for controlled evaluation in a multi-source news stream with adversarial injection and amplification. Across emotional polarization, viewpoint extremity, and argumentative rationality, EvoCorps improves discourse outcomes over an adversarial baseline, pointing to a practical path from detection and post-hoc mitigation to in-process, closed-loop intervention. The code is available at https://github.com/ln2146/EvoCorps.

</details>


### [123] [ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08567)
*Jinnuo Liu,Chuke Liu,Hua Shen*

Main category: cs.MA

TL;DR: ValueFlow는 다중 에이전트 시스템에서 가치 변화의 전파를 측정하고 분석하기 위한 평가 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 가치 정렬은 일반적으로 독립 모델에 대해 평가되지만 다중 에이전트 상호작용에서 가치 변화가 어떻게 전파되는지는 잘 이해되지 않고 있다.

Method: ValueFlow는 Schwartz Value Survey에서 유래한 56가지 가치 평가 데이터셋을 도입하고, LLM-as-a-judge 프로토콜을 사용하여 상호작용 중 에이전트의 가치 방향을 정량화한다. 그 측정 레이어를 기반으로 ValueFlow는 가치 변화를 에이전트 수준 응답 행동과 시스템 수준 구조적 효과로 분해한다.

Result: 여러 모델 백본, 프롬프트 페르소나, 가치 차원 및 네트워크 구조를 아우르는 실험 결과, 민감도는 가치에 따라 크게 달라지며 구조적 토폴로지에 의해 강하게 형성된다.

Conclusion: ValueFlow는 에이전트 상호작용에서의 가치 변화 측정 및 분석을 위해 효과적인 도구를 제공한다.

Abstract: Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.

</details>


### [124] [Teaching an Old Dynamics New Tricks: Regularization-free Last-iterate Convergence in Zero-sum Games via BNN Dynamics](https://arxiv.org/abs/2602.08938)
*Tuo Zhang,Leonardo Stella*

Main category: cs.MA

TL;DR: 이 논문은 제로섬 게임에서 진화 게임 이론의 클래식 모델을 활용하여 정규화 없이도 수렴을 보장하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 제로섬 게임에서의 정규화 요구 사항과 그로 인한 하이퍼파라미터 조정의 어려움 해결을 위해.

Method: 브라운-폰 노이만-내시(BNN) 동역학을 활용하여 잡음이 있는 정규형 게임(NFG)에서의 마지막 반복 수렴 보장을 제공하고, 역사적 가중치를 통해 광범위 형 게임(EFG)에 통합하는 새로운 프레임워크를 개발.

Result: 우리의 방법은 비정상성에 빠르게 적응하며 정규화 기반 접근 방식보다 뛰어난 성능을 보여줌.

Conclusion: 기존의 정규화 방법에 비해 더 효과적인 적응성을 가진 접근 방식을 제시함으로써 게임 이론 응용에 기여한다.

Abstract: Zero-sum games are a fundamental setting for adversarial training and decision-making in multi-agent learning (MAL). Existing methods often ensure convergence to (approximate) Nash equilibria by introducing a form of regularization. Yet, regularization requires additional hyperparameters, which must be carefully tuned--a challenging task when the payoff structure is known, and considerably harder when the structure is unknown or subject to change. Motivated by this problem, we repurpose a classical model in evolutionary game theory, i.e., the Brown-von Neumann-Nash (BNN) dynamics, by leveraging the intrinsic convergence of this dynamics in zero-sum games without regularization, and provide last-iterate convergence guarantees in noisy normal-form games (NFGs). Importantly, to make this approach more applicable, we develop a novel framework with theoretical guarantees that integrates the BNN dynamics in extensive-form games (EFGs) through counterfactual weighting. Furthermore, we implement an algorithm that instantiates our framework with neural function approximation, enabling scalable learning in both NFGs and EFGs. Empirical results show that our method quickly adapts to nonstationarities, outperforming the state-of-the-art regularization-based approach.

</details>


### [125] [Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.08965)
*John Gardiner,Orlando Romero,Brendan Tivnan,Nicolò Dal Fabbro,George J. Pappas*

Main category: cs.MA

TL;DR: 이 논문은 양자 얽힘을 활용하여 커뮤니케이션이 없는 다중 에이전트 강화 학습에서의 효과적인 협조 자원으로 활용하는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 양자 물리학의 결과에 의해, 특정 커뮤니케이션이 없는 협동 게임에서 양자 얽힘이 제안된 전략이 단순히 공유된 무작위성을 사용하는 전략보다 우수하다는 점에 착안했습니다.

Method: 양자 측정에서 최적화를 가능하게 하는 새로운 미분 가능 정책 매개변수화와 공동 정책을 양자 조정자 및 분산된 로컬 액터로 분해하는 새로운 정책 아키텍처를 기반으로 합니다.

Result: 경험적으로 학습하여 단일 라운드 게임에서 양자 이점을 달성하는 전략을 배울 수 있음을 보여주며, 분산 부분 관찰 마르코프 결정 프로세스(Dec-POMDP)로 구성된 문제에서 양자 이점을 가진 정책을 학습할 수 있음을 demonstr합니다.

Conclusion: 이 연구는 초전적인 정책 공동 체계의 실행 가능성을 보여주며, 커뮤니케이션이 없는 다중 에이전트 환경에서의 협력을 크게 향상시킬 수 있는 잠재력을 지닙니다.

Abstract: The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>
