<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.MA](#cs.MA) [Total: 10]
- [cs.LG](#cs.LG) [Total: 26]
- [cs.CR](#cs.CR) [Total: 8]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [On the Dynamics of Observation and Semantics](https://arxiv.org/abs/2602.18494)
*Xiu Li*

Main category: cs.AI

TL;DR: 이 논문은 지능을 물리적으로 실현 가능한 에이전트의 특성으로 보고, 정보 처리의 열역학적 비용이 내부 상태 전이의 복잡성에 제한을 준다고 주장합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 시각 지능 패러다임이 의미를 정적인 속성으로 간주하는 것에 반대하며, 지능을 물리적 환경과 상호작용하는 한계가 있는 에이전트의 특성으로 재정의하고자 함.

Method: 관찰 의미 섬유 다발의 운동학적 구조를 통해 Raw sensory observation 데이터가 낮은 엔트로피의 인과 의미 다양체에 투영되는 방식을 공식화함.

Result: 모든 한계 에이전트에 대해 정보 처리의 열역학적 비용이 내부 상태 전이의 복잡성에 엄격한 한계를 부과함을 증명하였고, 이러한 한계를 의미 상수 B로 명명.

Conclusion: 이해는 숨겨진 잠재 변수를 복구하는 것이 아니라, 세계를 알고리즘적으로 압축 가능하고 인과적으로 예측 가능하게 만드는 인과적 몫을 구성하는 것임을 결론지음.

Abstract: A dominant paradigm in visual intelligence treats semantics as a static property of latent representations, assuming that meaning can be discovered through geometric proximity in high dimensional embedding spaces. In this work, we argue that this view is physically incomplete. We propose that intelligence is not a passive mirror of reality but a property of a physically realizable agent, a system bounded by finite memory, finite compute, and finite energy interacting with a high entropy environment. We formalize this interaction through the kinematic structure of an Observation Semantics Fiber Bundle, where raw sensory observation data (the fiber) is projected onto a low entropy causal semantic manifold (the base). We prove that for any bounded agent, the thermodynamic cost of information processing (Landauer's Principle) imposes a strict limit on the complexity of internal state transitions. We term this limit the Semantic Constant B. From these physical constraints, we derive the necessity of symbolic structure. We show that to model a combinatorial world within the bound B, the semantic manifold must undergo a phase transition, it must crystallize into a discrete, compositional, and factorized form. Thus, language and logic are not cultural artifacts but ontological necessities the solid state of information required to prevent thermal collapse. We conclude that understanding is not the recovery of a hidden latent variable, but the construction of a causal quotient that renders the world algorithmically compressible and causally predictable.

</details>


### [2] [Hierarchical Reward Design from Language: Enhancing Alignment of Agent Behavior with Human Specifications](https://arxiv.org/abs/2602.18582)
*Zhiqin Qian,Ryan Diaz,Sangwon Seo,Vaibhav Unhelkar*

Main category: cs.AI

TL;DR: 이 논문은 AI 에이전트의 행동을 인간의 기대와 일치시키기 위한 새로운 보상 설계 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 복잡한 작업을 수행할 때 인간의 기대를 반영할 수 있는 보상 설계 필요성.

Method: HRDL(계층적 보상 설계) 및 L2HR(언어를 계층적 보상으로 변환) 방법을 통해 과제를 더욱 잘 수행할 수 있도록 보상을 설계.

Result: L2HR을 통해 설계된 보상을 이용해 훈련된 AI 에이전트가 작업을 효과적으로 완수하고 인간의 사양을 더 잘 준수함을 보였다.

Conclusion: HRDL과 L2HR은 인간과 일치하는 AI 에이전트 연구를 진전시킨다.

Abstract: When training artificial intelligence (AI) to perform tasks, humans often care not only about whether a task is completed but also how it is performed. As AI agents tackle increasingly complex tasks, aligning their behavior with human-provided specifications becomes critical for responsible AI deployment. Reward design provides a direct channel for such alignment by translating human expectations into reward functions that guide reinforcement learning (RL). However, existing methods are often too limited to capture nuanced human preferences that arise in long-horizon tasks. Hence, we introduce Hierarchical Reward Design from Language (HRDL): a problem formulation that extends classical reward design to encode richer behavioral specifications for hierarchical RL agents. We further propose Language to Hierarchical Rewards (L2HR) as a solution to HRDL. Experiments show that AI agents trained with rewards designed via L2HR not only complete tasks effectively but also better adhere to human specifications. Together, HRDL and L2HR advance the research on human-aligned AI agents.

</details>


### [3] [Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System](https://arxiv.org/abs/2602.18640)
*Longfei Yun,Yihan Wu,Haoran Liu,Xiaoxuan Liu,Ziyun Xu,Yi Wang,Yang Xia,Pengfei Wang,Mingze Gao,Yunxiang Wang,Changfan Chen,Junfeng Pan*

Main category: cs.AI

TL;DR: GEARS는 자동 발견 프로세스로 랭킹 최적화를 재구성하며, 전문 에이전트 기술을 활용하여 안정적인 배포를 유지한 채 뛰어난 정책을 발견한다.


<details>
  <summary>Details</summary>
Motivation: 현대의 대규모 랭킹 시스템은 경쟁 목표, 운영 제약 및 진화하는 제품 요구 사항이 얽혀 있는 복잡한 환경에서 운영된다.

Method: GEARS는 랭킹 최적화를 프로그래머블 실험 환경 내에서 자율 발견 프로세스로 재구성하며, 전문 에이전트 기술을 통해 랭킹 전문가의 지식을 재사용 가능한 추론 능력으로 캡슐화한다.

Result: 다양한 제품 표면에 걸쳐 실시된 실험적 검증에서 GEARS는 알고리즘 신호와 깊이 있는 랭킹 컨텍스트를 결합하여 뛰어난 근접 파레토 효율 정책을 일관되게 식별한다.

Conclusion: GEARS는 철저한 배포 안정성을 유지하면서도 단기 신호에 과적합된 취약 정책을 필터링하는 통계적 강건성을 보장하기 위해 검증 후크를 통합한다.

Abstract: Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS (Generative Engine for Agentic Ranking Systems), a framework that reframes ranking optimization as an autonomous discovery process within a programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near-Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability.

</details>


### [4] [Many AI Analysts, One Dataset: Navigating the Agentic Data Science Multiverse](https://arxiv.org/abs/2602.18710)
*Martin Bertran,Riccardo Fogliato,Zhiwei Steven Wu*

Main category: cs.AI

TL;DR: 이 연구는 자율 AI 분석가가 동일한 가설을 기반으로 한 분석에서 분석 결정의 다양성을 재현할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 많은 분석가가 같은 데이터셋을 사용해 같은 가설을 검증할 때 서로 상충하는 결론에 도달함을 보여주는 연구가 있었으나, 이러한 연구는 조정이 오랜 시간을 필요로 한다.

Method: 우리는 대형 언어 모델(LLMs)을 기반으로 한 자율 AI 분석가에게 고정된 데이터셋에 대한 사전 지정된 가설을 테스트하도록 하여, 각 AI 분석가가 분석 파이프라인을 독립적으로 구축하고 실행하도록 했다.

Result: 세 가지 데이터셋에서 AI 분석가가 생성한 분석 결과는 효과 크기, p-값 및 가설 지지 여부의 이진 결정에서 광범위한 분산을 보였다.

Conclusion: 분산은 체계적으로 구조화되어 있으며, ANALYST의 성격이나 LLM을 변경함으로써 결과 분포가 조정 가능하다.

Abstract: The conclusions of empirical research depend not only on data but on a sequence of analytic decisions that published results seldom make explicit. Past ``many-analyst" studies have demonstrated this: independent teams testing the same hypothesis on the same dataset regularly reach conflicting conclusions. But such studies require months of coordination among dozens of research groups and are therefore rarely conducted. In this work, we show that fully autonomous AI analysts built on large language models (LLMs) can reproduce a similar structured analytic diversity cheaply and at scale. We task these AI analysts with testing a pre-specified hypothesis on a fixed dataset, varying the underlying model and prompt framing across replicate runs. Each AI analyst independently constructs and executes a full analysis pipeline; an AI auditor then screens each run for methodological validity. Across three datasets spanning experimental and observational designs, AI analyst-produced analyses display wide dispersion in effect sizes, $p$-values, and binary decisions on supporting the hypothesis or not, frequently reversing whether a hypothesis is judged supported. This dispersion is structured: recognizable analytic choices in preprocessing, model specification, and inference differ systematically across LLM and persona conditions. Critically, the effects are \emph{steerable}: reassigning the analyst persona or LLM shifts the distribution of outcomes even after excluding methodologically deficient runs.

</details>


### [5] [Task-Aware Exploration via a Predictive Bisimulation Metric](https://arxiv.org/abs/2602.18724)
*Dayang Liang,Ruihan Liu,Lipeng Wan,Yunlong Liu,Bo An*

Main category: cs.AI

TL;DR: 우리는 TEB라는 과제 인식 탐색 접근 방식을 소개하며, 이는 시각적 강화 학습에서 희박한 보상 아래에서의 탐색을 가속화한다. TEB는 예측적 비시뮬레이션 측정을 통해 탐색과 과제 관련 표현을 긴밀하게 결합한다.


<details>
  <summary>Details</summary>
Motivation: 희박한 보상에서의 시각적 강화 학습 탐색 속도를 높이는 것이 과제와 무관한 변동성으로 인해 도전적인 문제로 남아 있다.

Method: TEB는 예측적 비시뮬레이션 메트릭을 통해 탐색과 과제 관련 표현을 결합하는 과제 인식 탐색 접근 방식이다.

Result: MetaWorld 및 Maze2D에서의 광범위한 실험을 통해 TEB가 우수한 탐색 능력을 달성하고 최근 기준을 초과 성능을 보였음을 보여준다.

Conclusion: TEB는 탐색의 상대적 참신함을 측정하기 위한 잠재 기반 탐색 보너스를 설계하여, 학습된 잠재 공간 내 인접 관측의 상대적 참신성을 측정한다.

Abstract: Accelerating exploration in visual reinforcement learning under sparse rewards remains challenging due to the substantial task-irrelevant variations. Despite advances in intrinsic exploration, many methods either assume access to low-dimensional states or lack task-aware exploration strategies, thereby rendering them fragile in visual domains. To bridge this gap, we present TEB, a Task-aware Exploration approach that tightly couples task-relevant representations with exploration through a predictive Bisimulation metric. Specifically, TEB leverages the metric not only to learn behaviorally grounded task representations but also to measure behaviorally intrinsic novelty over the learned latent space. To realize this, we first theoretically mitigate the representation collapse of degenerate bisimulation metrics under sparse rewards by internally introducing a simple but effective predicted reward differential. Building on this robust metric, we design potential-based exploration bonuses, which measure the relative novelty of adjacent observations over the latent space. Extensive experiments on MetaWorld and Maze2D show that TEB achieves superior exploration ability and outperforms recent baselines.

</details>


### [6] [Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization](https://arxiv.org/abs/2602.18731)
*Yuhang Bai,Yujuan Ding,Shanru Lin,Wenqi Fan*

Main category: cs.AI

TL;DR: 차트 요약은 데이터 접근성과 정보 소비 효율성을 높이는 데 중요하다. 하지만 기존 방법은 표면적인 데이터 설명에 집중해 심도 있는 통찰력을 놓치고 있다. 이를 해결하기 위해 우리는 차트 이미지에서 직접 통찰력을 발견하는 다중 에이전트 프레임워크인 Chart Insight Agent Flow를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 차트 요약의 중요성과 기존 방법들이 심층 통찰력을 포착하지 못하는 문제를 해결하기 위해.

Method: 멀티모달 대규모 언어 모델의 지각 및 추론 능력을 활용하여 차트 이미지에서 통찰력을 찾는 계획 및 실행 멀티 에이전트 프레임워크를 제안한다.

Result: 우리가 제안한 방법이 MLLM의 차트 요약 작업에서 성능을 유의미하게 향상시키고, 심도 있는 다양한 통찰력이 담긴 요약을 생성함을 보여주었다.

Conclusion: 새로운 데이터베이스 ChartSummInsights를 도입하여 실제 차트와 인간 데이터 분석 전문가가 작성한 고품질 요약을 결합한다.

Abstract: Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the deeper insights which are the fundamental purpose of data visualization. To address this challenge, we propose Chart Insight Agent Flow, a plan-and-execute multi-agent framework effectively leveraging the perceptual and reasoning capabilities of MLLMs to uncover profound insights directly from chart images. Furthermore, to overcome the lack of suitable benchmarks, we introduce ChartSummInsights, a new dataset featuring a diverse collection of real-world charts paired with high-quality, insightful summaries authored by human data analysis experts. Experimental results demonstrate that our method significantly improves the performance of MLLMs on the chart summarization task, producing summaries with deep and diverse insights.

</details>


### [7] [The Convergence of Schema-Guided Dialogue Systems and the Model Context Protocol](https://arxiv.org/abs/2602.18764)
*Andreas Schlapbach*

Main category: cs.AI

TL;DR: 이 논문은 Schema-Guided Dialogue와 Model Context Protocol 간의 근본적인 수렴성을 제시하며, 두 개념이 결정론적이고 감사 가능한 LLM-에이전트 상호작용을 위한 통합 패러다임을 구성한다. 또한, 스키마 디자인을 위한 5가지 기본 원칙을 제시하고, 이를 통해 새로운 통찰력을 제공한다.


<details>
  <summary>Details</summary>
Motivation: SGD와 MCP가 결정론적이고 감사 가능한 LLM-에이전트 상호작용을 위한 통합 패러다임으로 수렴함을 보여주기 위해.

Method: SGD와 MCP의 공통된 핵심 통찰을 분석하고, 스키마 디자인을 위한 다섯 가지 원칙을 추출하여 이를 concrete한 디자인 패턴으로 제시한다.

Result: 각 원칙은 새로운 통찰력을 제공하며, 특히 SGD의 디자인이 MCP에 의해 상속되어야 함을 시사하고, 실패 모드 및 도구 간의 관계가 활용되지 않았음을 지적한다.

Conclusion: 스키마 구동 거버넌스를 AI 시스템 감독에 있어 확장 가능한 메커니즘으로 설정한다.

Abstract: This paper establishes a fundamental convergence: Schema-Guided Dialogue (SGD) and the Model Context Protocol (MCP) represent two manifestations of a unified paradigm for deterministic, auditable LLM-agent interaction. SGD, designed for dialogue-based API discovery (2019), and MCP, now the de facto standard for LLM-tool integration, share the same core insight -- that schemas can encode not just tool signatures but operational constraints and reasoning guidance. By analyzing this convergence, we extract five foundational principles for schema design: (1) Semantic Completeness over Syntactic Precision, (2) Explicit Action Boundaries, (3) Failure Mode Documentation, (4) Progressive Disclosure Compatibility, and (5) Inter-Tool Relationship Declaration. These principles reveal three novel insights: first, SGD's original design was fundamentally sound and should be inherited by MCP; second, both frameworks leave failure modes and inter-tool relationships unexploited -- gaps we identify and resolve; third, progressive disclosure emerges as a critical production-scaling insight under real-world token constraints. We provide concrete design patterns for each principle. These principles position schema-driven governance as a scalable mechanism for AI system oversight without requiring proprietary system inspection -- central to Software 3.0.

</details>


### [8] [LAMMI-Pathology: A Tool-Centric Bottom-Up LVLM-Agent Framework for Molecularly Informed Medical Intelligence in Pathology](https://arxiv.org/abs/2602.18773)
*Haoyang Su,Shaoting Zhang,Xiaosong Wang*

Main category: cs.AI

TL;DR: 이 논문에서는 도메인 특정 에이전트 도구 호출을 위한 확장 가능한 에이전트 프레임워크인 LAMMI-Pathology를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 도구 호출 기반 에이전트 시스템의 출현은 병리 이미지 분석을 위한 보다 증거 기반 패러다임을 도입하고, 대규모 실험적 채택을 통해 분자적으로 검증된 병리 진단이 점점 더 열려가고 있음을 보여줍니다.

Method: LAMMI-Pathology는 맞춤형 도메인 적응 도구가 기반이 되는 도구 중심의 하향식 아키텍처를 채택하며, 이러한 도구들은 도메인 스타일에 따라 클러스터링되어 구성 요소 에이전트를 형성합니다.

Result: 새로운 원자 실행 노드(AEN)를 기반으로 한 궤적 구성 메커니즘을 도입하여 신뢰할 수 있고 구성 가능한 단위로 반시뮬레이션 추론 궤적을 구축합니다.

Conclusion: 기반을 바탕으로 플래너의 의사 결정 과정을 다단계 추론 궤적과 일치시키는 궤적 인식 미세 조정 전략을 개발하여 병리 이해와 맞춤형 도구 세트의 적응적 사용에서 추론 강화의 견고성을 향상시킵니다.

Abstract: The emergence of tool-calling-based agent systems introduces a more evidence-driven paradigm for pathology image analysis in contrast to the coarse-grained text-image diagnostic approaches. With the recent large-scale experimental adoption of spatial transcriptomics technologies, molecularly validated pathological diagnosis is becoming increasingly open and accessible. In this work, we propose LAMMI-Pathology (LVLM-Agent System for Molecularly Informed Medical Intelligence in Pathology), a scalable agent framework for domain-specific agent tool-calling. LAMMI-Pathology adopts a tool-centric, bottom-up architecture in which customized domain-adaptive tools serve as the foundation. These tools are clustered by domain style to form component agents, which are then coordinated through a top-level planner hierarchically, avoiding excessively long context lengths that could induce task drift. Based on that, we introduce a novel trajectory construction mechanism based on Atomic Execution Nodes (AENs), which serve as reliable and composable units for building semi-simulated reasoning trajectories that capture credible agent-tool interactions. Building on this foundation, we develop a trajectory-aware fine-tuning strategy that aligns the planner's decision-making process with these multi-step reasoning trajectories, thereby enhancing inference robustness in pathology understanding and its adaptive use of the customized toolset.

</details>


### [9] [DREAM: Deep Research Evaluation with Agentic Metrics](https://arxiv.org/abs/2602.18940)
*Elad Ben Avraham,Changhao Li,Ron Dorfman,Roy Ganz,Oren Nuriel,Amir Dudai,Aviad Aberdam,Noah Flynn,Elman Mansimov,Adi Kalyanpur,Ron Litman*

Main category: cs.AI

TL;DR: DREAM 프레임워크는 연구 보고서 평가의 새로운 접근 방식을 제안하며, 기존의 평가 방법론보다 사실적이고 시간적으로 민감하게 평가할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 연구 보고서를 생성하는 Deep Research Agents의 품질 평가가 단일 진실의 부재와 연구 품질의 다차원적 특성으로 인해 어렵다는 필요성을 느낀다.

Method: DREAM 프레임워크는 비대칭적인 역량 불일치를 해결하기 위해 도구 호출 에이전트에 의해 생성된 적응형 메트릭과 쿼리 무관 메트릭을 결합한 평가 프로토콜을 통해 평가를 구조화한다.

Result: DREAM은 기존 벤치마크보다 사실적 및 시간적 저하에 훨씬 민감하게 작용하는 것으로 나타났으며, 이는 무제한적으로 확장 가능한 평가 패러다임을 제공한다.

Conclusion: DREAM은 에이전틱한 평가 원칙을 통해 평가가 더욱 효과적으로 이루어질 수 있도록 한다.

Abstract: Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.

</details>


### [10] [(Perlin) Noise as AI coordinator](https://arxiv.org/abs/2602.18947)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 대규모 AI 제어를 위한 새로운 연속 노이즈 신호 프레임워크 제안


<details>
  <summary>Details</summary>
Motivation: 대규모 비플레이어 에이전트 제어에서 행동의 자연스러움과 전체적인 조화를 지나치게 다루기 위한 필요성에서 출발.

Method: Perlin 노이즈와 같은 연속 노이즈 신호를 대규모 AI 제어에 적응시키고, 이를 AI 조정기로 사용하는 일반적인 프레임워크를 제안.

Result: 여러 유형의 기준선과 비교했을 때, 조정된 노이즈 필드가 안정적인 활성화 통계와 더불어 공간적 커버리지 및 지역적 균형 등에서 더 나은 성과를 보임.

Conclusion: 게임 AI에서 효율성, 제어 가능성 및 품질을 결합할 수 있는 실용적인 경로로 조정된 노이즈의 탐색을 촉진할 것으로 기대.

Abstract: Large scale control of nonplayer agents is central to modern games, while production systems still struggle to balance several competing goals: locally smooth, natural behavior, and globally coordinated variety across space and time. Prior approaches rely on handcrafted rules or purely stochastic triggers, which either converge to mechanical synchrony or devolve into uncorrelated noise that is hard to tune. Continuous noise signals such as Perlin noise are well suited to this gap because they provide spatially and temporally coherent randomness, and they are already widely used for terrain, biomes, and other procedural assets. We adapt these signals for the first time to large scale AI control and present a general framework that treats continuous noise fields as an AI coordinator. The framework combines three layers of control: behavior parameterization for movement at the agent level, action time scheduling for when behaviors start and stop, and spawn or event type and feature generation for what appears and where. We instantiate the framework reproducibly and evaluate Perlin noise as a representative coordinator across multiple maps, scales, and seeds against random, filtered, deterministic, neighborhood constrained, and physics inspired baselines. Experiments show that coordinated noise fields provide stable activation statistics without lockstep, strong spatial coverage and regional balance, better diversity with controllable polarization, and competitive runtime. We hope this work motivates a broader exploration of coordinated noise in game AI as a practical path to combine efficiency, controllability, and quality.

</details>


### [11] [Characterizing MARL for Energy Control: A Multi-KPI Benchmark on the CityLearn Environment](https://arxiv.org/abs/2602.19223)
*Aymen Khouja,Imen Jendoubi,Oumayma Mahjoub,Oussama Mahfoudhi,Claude Formanek,Siddarth Singh,Ruan De Kock*

Main category: cs.AI

TL;DR: 이 논문은 도시 에너지 시스템 최적화를 위한 다중 에이전트 강화 학습(MARL) 알고리즘의 평가 및 벤치마킹을 중심으로 다루고 있으며, 다양한 성과 지표에서 알고리즘의 강점과 약점을 밝힙니다.


<details>
  <summary>Details</summary>
Motivation: 지속 가능하고 복원력 있는 스마트 도시에 필수적인 도시 에너지 시스템의 최적화는 여러 의사결정 단위로 인해 복잡성이 증가하고 있습니다.

Method: CityLearn 환경을 사례로 사용하여 MARL 알고리즘을 에너지 관리 작업에서 평가하고, 다양한 훈련 방식과 신경망 아키텍처를 이용해 실험을 수행합니다.

Result: DTDE 방식이 평균 및 최악의 성능 모두에서 CTDE 방식보다 일관되게 우수함을 보여줍니다. 또한 메모리 의존 KPI에 대한 제어가 향상되어, 배터리 운영의 지속 가능성에 기여합니다.

Conclusion: 우리의 연구는 알고리즘의 강점과 약점을 밝히고 학습된 정책의 복원성과 분산 가능성을 강조합니다.

Abstract: The optimization of urban energy systems is crucial for the advancement of sustainable and resilient smart cities, which are becoming increasingly complex with multiple decision-making units. To address scalability and coordination concerns, Multi-Agent Reinforcement Learning (MARL) is a promising solution. This paper addresses the imperative need for comprehensive and reliable benchmarking of MARL algorithms on energy management tasks. CityLearn is used as a case study environment because it realistically simulates urban energy systems, incorporates multiple storage systems, and utilizes renewable energy sources. By doing so, our work sets a new standard for evaluation, conducting a comparative study across multiple key performance indicators (KPIs). This approach illuminates the key strengths and weaknesses of various algorithms, moving beyond traditional KPI averaging which often masks critical insights. Our experiments utilize widely accepted baselines such as Proximal Policy Optimization (PPO) and Soft Actor Critic (SAC), and encompass diverse training schemes including Decentralized Training with Decentralized Execution (DTDE) and Centralized Training with Decentralized Execution (CTDE) approaches and different neural network architectures. Our work also proposes novel KPIs that tackle real world implementation challenges such as individual building contribution and battery storage lifetime. Our findings show that DTDE consistently outperforms CTDE in both average and worst-case performance. Additionally, temporal dependency learning improved control on memory dependent KPIs such as ramping and battery usage, contributing to more sustainable battery operation. Results also reveal robustness to agent or resource removal, highlighting both the resilience and decentralizability of the learned policies.

</details>


### [12] [Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction](https://arxiv.org/abs/2602.18968)
*Tao Zhe,Haoyu Wang,Bo Luo,Min Wu,Wei Fan,Xiao Luo,Zijun Yao,Haifeng Chen,Dongjie Wang*

Main category: cs.AI

TL;DR: 도구 호출은 에이전트 시스템의 핵심 기능이지만, 실패는 종종 개별 도구 호출이 아닌 여러 도구가 함께 구성되고 실행되는 방식에서 발생한다. 이 논문에서는 도구 오케스트레이션의 관점에서 도구 호출을 재검토하고, 효과적인 오케스트레이션이 정밀한 의존성 그래프나 세밀한 계획 없이도 가능함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존 접근 방식은 도구 실행을 단계별 언어 추론 또는 명시적 계획과 긴밀하게 결합하여 부서지기 쉬운 동작과 높은 실행 오버헤드를 초래한다.

Method: 우리는 도구 오케스트레이션을 고수준 도구 의존성을 캡처하는 층화 실행 구조 학습으로 모델링하고, 컨텍스트 제약을 통해 층별 실행을 유도한다. 실행 시간 오류를 처리하기 위해 오류를 로컬에서 감지하고 수정하는 스키마 인식 반영 수정 메커니즘을 도입한다.

Result: 실험 결과, 우리의 접근 방식이 강력한 도구 실행을 달성하면서 실행 복잡성과 오버헤드를 줄임을 보여준다.

Conclusion: 이 구조화된 실행 패러다임은 에이전트 시스템을 위한 경량화되고 재사용 가능한 오케스트레이션 구성 요소를 가능하게 한다. 코드는 공개될 예정이다.

Abstract: Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.

</details>


### [13] [When Do LLM Preferences Predict Downstream Behavior?](https://arxiv.org/abs/2602.18971)
*Katarina Slama,Alexandra Souly,Dishank Bansal,Henry Davidson,Christopher Summerfield,Lennart Luettgau*

Main category: cs.AI

TL;DR: LLM의 선호에 기반한 행동이 AI의 목표 불일치에 필수적일 수 있다. 본 연구는 특정 선호가 행동에 미치는 영향을 조사하여 다섯 개의 LLM 모델이 선호와 일치하는 기부 조언을 제공하며, 기부를 권장할 때 선호와 관련된 거부 패턴을 보인다는 것을 발견했다. 그러나 복잡한 작업에서는 선호 기반의 성능 차이는 관찰되지 않았다.


<details>
  <summary>Details</summary>
Motivation: LLM의 행동이 선호에 의해 영향을 받는지 여부를 조사하여 AI의 목표 불일치 문제를 이해하기 위함이다.

Method: 다섯 개의 LLM 모델을 대상으로 기부 조언, 거부 행동 및 작업 수행 세 가지 영역에서 선호를 조사하고, 두 가지 독립적인 측정 방법을 통해 일관성을 확인했다.

Result: 모든 모델이 선호에 기반한 기부 조언과 거부 패턴을 보였지만, 작업 수행에서는 선호가 일관되게 반영되지 않았다.

Conclusion: LLM은 일관된 선호를 가지지만, 이 선호는 후속 작업 수행에 일관되게 반영되지 않는다.

Abstract: Preference-driven behavior in LLMs may be a necessary precondition for AI misalignment such as sandbagging: models cannot strategically pursue misaligned goals unless their behavior is influenced by their preferences. Yet prior work has typically prompted models explicitly to act in specific ways, leaving unclear whether observed behaviors reflect instruction-following capabilities vs underlying model preferences. Here we test whether this precondition for misalignment is present. Using entity preferences as a behavioral probe, we measure whether stated preferences predict downstream behavior in five frontier LLMs across three domains: donation advice, refusal behavior, and task performance. Conceptually replicating prior work, we first confirm that all five models show highly consistent preferences across two independent measurement methods. We then test behavioral consequences in a simulated user environment. We find that all five models give preference-aligned donation advice. All five models also show preference-correlated refusal patterns when asked to recommend donations, refusing more often for less-preferred entities. All preference-related behaviors that we observe here emerge without instructions to act on preferences. Results for task performance are mixed: on a question-answering benchmark (BoolQ), two models show small but significant accuracy differences favoring preferred entities; one model shows the opposite pattern; and two models show no significant relationship. On complex agentic tasks, we find no evidence of preference-driven performance differences. While LLMs have consistent preferences that reliably predict advice-giving behavior, these preferences do not consistently translate into downstream task performance.

</details>


### [14] [How Far Can We Go with Pixels Alone? A Pilot Study on Screen-Only Navigation in Commercial 3D ARPGs](https://arxiv.org/abs/2602.18981)
*Kaijie Xu,Mustafa Bugti,Clark Verbrugge*

Main category: cs.AI

TL;DR: 이 논문은 비주얼 어포던스를 기반으로 한 탐색 에이전트를 개발하고, 그 한계를 실험을 통해 분석한다.


<details>
  <summary>Details</summary>
Motivation: 3D 게임 레벨의 탐색 가능성을 정량화하는 것이 어렵고, 기존의 연구는 단순화된 환경을 시뮬레이션하거나 정적 스크린샷을 분석하는 데 중점을 두었다.

Method: 우리는 오픈 소스 비주얼 어포던스 디텍터를 기반으로 하여 화면 전용 탐색 및 내비게이션 에이전트를 설계하고, 게임의 실시간 프레임을 처리하여 주요 관심 지점을 식별하고 간단한 유한 상태 컨트롤러를 통해 탐색을 수행하도록 하였다.

Result: 파일럿 실험 결과, 에이전트는 필요한 대부분의 구간을 탐색할 수 있었으나, 기반 비주얼 모델의 한계로 인해 완전하고 신뢰할 수 있는 자율 내비게이션이 불가능함을 보여주었다.

Conclusion: 이 시스템은 복잡한 게임에서 비주얼 내비게이션에 대한 구체적이고 공유된 기준선 및 평가 프로토콜을 제공하며, 향후 이 작업에 대한 더 많은 관심이 필요함을 강조한다.

Abstract: Modern 3D game levels rely heavily on visual guidance, yet the navigability of level layouts remains difficult to quantify. Prior work either simulates play in simplified environments or analyzes static screenshots for visual affordances, but neither setting faithfully captures how players explore complex, real-world game levels. In this paper, we build on an existing open-source visual affordance detector and instantiate a screen-only exploration and navigation agent that operates purely from visual affordances. Our agent consumes live game frames, identifies salient interest points, and drives a simple finite-state controller over a minimal action space to explore Dark Souls-style linear levels and attempt to reach expected goal regions. Pilot experiments show that the agent can traverse most required segments and exhibits meaningful visual navigation behavior, but also highlight that limitations of the underlying visual model prevent truly comprehensive and reliable auto-navigation. We argue that this system provides a concrete, shared baseline and evaluation protocol for visual navigation in complex games, and we call for more attention to this necessary task. Our results suggest that purely vision-based sense-making models, with discrete single-modality inputs and without explicit reasoning, can effectively support navigation and environment understanding in idealized settings, but are unlikely to be a general solution on their own.

</details>


### [15] [InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing](https://arxiv.org/abs/2602.18985)
*Kun Ding,Jian Xu,Ying Wang,Peipei Yang,Shiming Xiang*

Main category: cs.AI

TL;DR: InfEngine은 자율 지능 계산 엔진으로, 수동 작업에서 협업 자동화로의 전환을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 기후 과학, 원격 탐사 및 분광학의 발전을 위해 필요하지만 수동 작업에 의해 제약받는 적외선 방사 컴퓨팅의 혁신이 필요합니다.

Method: InfEngine은 자기 검증과 자기 최적화라는 두 가지 핵심 혁신을 통해 네 개의 전문 에이전트를 통합합니다.

Result: InfEngine은 200개의 적외선 특화 작업에 대해 92.7%의 통과율을 달성하고 수동 전문가 작업보다 21배 빠른 워크플로우를 제공합니다.

Conclusion: 이는 연구자들이 수동 코딩에서 자기 검증 및 자기 최적화 계산 협력자와 협업하는 방식으로 전환할 수 있도록 합니다.

Abstract: Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine

</details>


### [16] [Quantifying Automation Risk in High-Automation AI Systems: A Bayesian Framework for Failure Propagation and Optimal Oversight](https://arxiv.org/abs/2602.18986)
*Vishal Srivastava,Tanmay Sah*

Main category: cs.AI

TL;DR: 본 논문에서는 AI 시스템의 자동화 수준 증가가 실패 시 피해를 증대시키는 방식을 정량화하는 방법의 부족을 해결하기 위해 베이지안 위험 분해 기법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 자동화는 다양한 산업에서 발생하지만, 이러한 자동화로 인한 실패와 피해의 관계를 정량화할 방법이 부족합니다.

Method: 우리는 시스템 실패의 확률, 자동화 수준에 따라 실패가 피해로 전파될 조건부 확률, 그리고 피해의 예상 심각도를 곱한 것으로 표현되는 베이지안 위험 분해 기법을 제안합니다.

Result: 이 프레임워크는 고유한 양인 실패가 피해로 전파되는 조건부 확률을 분리하여 모델 정확도만 고려하는 것이 아니라 실행 및 감독 위험을 포착합니다.

Conclusion: 이 연구는 자동화 AI 시스템을 위한 새로운 클래스의 위험 거버넌스 도구의 이론적 기초를 제공합니다.

Abstract: Organizations across finance, healthcare, transportation, content moderation, and critical infrastructure are rapidly deploying highly automated AI systems, yet they lack principled methods to quantify how increasing automation amplifies harm when failures occur. We propose a parsimonious Bayesian risk decomposition expressing expected loss as the product of three terms: the probability of system failure, the conditional probability that a failure propagates into harm given the automation level, and the expected severity of harm. This framework isolates a critical quantity -- the conditional probability that failures propagate into harm -- which captures execution and oversight risk rather than model accuracy alone. We develop complete theoretical foundations: formal proofs of the decomposition, a harm propagation equivalence theorem linking the harm propagation probability to observable execution controls, risk elasticity measures, efficient frontier analysis for automation policy, and optimal resource allocation principles with second-order conditions. We motivate the framework with an illustrative case study of the 2012 Knight Capital incident ($440M loss) as one instantiation of a broadly applicable failure pattern, and characterize the research design required to empirically validate the framework at scale across deployment domains. This work provides the theoretical foundations for a new class of deployment-focused risk governance tools for agentic and automated AI systems.

</details>


### [17] [Benchmark Test-Time Scaling of General LLM Agents](https://arxiv.org/abs/2602.18998)
*Xiaochuan Li,Ryan Ming,Pranav Setlur,Abhijay Paladugu,Andy Tang,Hao Kang,Shuai Shao,Rong Jin,Chenyan Xiong*

Main category: cs.AI

TL;DR: 이 논문은 일반 LLM 에이전트를 평가하기 위한 General AgentBench라는 새로운 벤치마크를 소개하며, 여러 기술과 도구에서 작업 수행능력을 평가하는 데 필요한 현실적인 환경을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 벤치마크는 특정 도메인에서 전문화된 에이전트를 개발하기 위해 설계되었지만, 일반-purpose 에이전트를 평가하기 위해서는 다양한 기술과 도구를 통합하여 작동하는 더 현실적인 설정이 필요합니다.

Method: General AgentBench를 소개하여 검색, 코딩, 추론 및 도구 사용 도메인에서 일반 LLM 에이전트를 평가하기 위한 통합된 프레임워크를 제공합니다.

Result: 10개의 주요 LLM 에이전트를 평가한 결과, 도메인 특정 평가에서 일반 에이전트 설정으로 이동할 때 성능이 크게 저하되었으며, 두 가지 기본적인 한계점 때문에 스케일링 방법론이 효과적인 성능 개선을 가져오지 않았습니다.

Conclusion: 순차적 스케일링의 맥락 한계와 병렬 스케일링의 검증 격차가 문제를 일으킵니다.

Abstract: LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.

</details>


### [18] [MagicAgent: Towards Generalized Agent Planning](https://arxiv.org/abs/2602.19000)
*Xuhui Ren,Shaokang Dong,Chen Yang,Qing Gao,Yunbin Zhao,Yongsheng Liu,Xinwei Geng,Xiang Li,Demei Yan,Yanqing Li,Chenhao Huang,Dingwei Zhu,Junjie Ye,Boxuan Yue,Yingnan Fu,Mengzhe Lv,Zezeng Feng,Boshen Zhou,Bocheng Wang,Xuanjing Huang,Yu-Gang Jiang,Tao Gui,Qi Zhang,Yunke Zhang*

Main category: cs.AI

TL;DR: 이 논문은 일반화된 에이전트 계획을 위해 설계된 MagicAgent라는 기초 모델 세트를 제시하며, 다양한 계획 작업에서 고품질 경로를 생성하는 데이터 프레임워크와 훈련 방법론을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 많은 언어 모델이 격리된 작업에만 뛰어난 성능을 보이고, 다중 작업 훈련에서 기울기 간섭 문제로 일반화가 어렵습니다.

Method: 경량화된 합성 데이터 프레임워크를 통해 다양한 계획 작업에서 고품질 경로를 생성하고, 감독 세밀 조정과 다중 목표 강화 학습을 포함한 2단계 훈련 패러다임을 제안합니다.

Result: MagicAgent-32B와 MagicAgent-30B-A3B가 Worfbench에서 75.1%, NaturalPlan에서 55.9%, τ^2-Bench에서 57.5%, BFCL-v3에서 86.9%, ACEBench에서 81.2%의 정확도를 달성하며, 내부 MagicEval 벤치마크에서도 우수한 성능을 보였습니다.

Conclusion: 이러한 결과는 기존 100B 미만 모델 및 선도적인 클로즈드 소스 모델에 비해 크게 개선된 성능을 보여줍니다.

Abstract: The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\%$ on Worfbench, $55.9\%$ on NaturalPlan, $57.5\%$ on $τ^2$-Bench, $86.9\%$ on BFCL-v3, and $81.2\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.

</details>


### [19] [Agentic Problem Frames: A Systematic Approach to Engineering Reliable Domain Agents](https://arxiv.org/abs/2602.19065)
*Chanjin Park*

Main category: cs.AI

TL;DR: 이 연구는 자율 에이전트 개발에서의 리스크를 최소화하기 위한 체계적인 엔지니어링 프레임워크인 Agentic Problem Frames (APF)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 자율 에이전트로 발전함에 따라, 불명확한 자연어에 의존하는 현재의 "프레임 없는" 개발 방식이 범위 확대 및 개방 루프 실패와 같은 중요한 리스크를 초래하고 있다.

Method: 이 연구는 내부 모델 정보에서 에이전트와 환경 간의 구조화된 상호작용으로 초점을 전환하는 체계적인 엔지니어링 프레임워크인 Agentic Problem Frames (APF)를 제안한다. APF는 런타임에서 의도를 구체화하는 동적 사양 패러다임을 수립하고, Act-Verify-Refine (AVR) 루프를 통해 실행 결과를 검증된 지식 자산으로 전환하여 시스템 행동을 임무 요구 사항(R)에 점근적으로 수렴하도록 유도한다. 또한, 이 연구는 관할 경계, 운영 맥락, 인식 평가 기준을 정의하는 형식적 사양 도구인 Agentic Job Description (AJD)를 도입한다.

Result: 이 프레임워크의 효율성은 비즈니스 여행을 위한 위임된 프록시 모델과 산업 장비 관리용 자율 감독자 모델이라는 두 가지 대조적인 사례 연구를 통해 검증된다. AJD 기반 사양과 APF 모델링을 적용함으로써 운영 시나리오가 정의된 경계 내에서 체계적으로 제어되는 방식을 보여준다.

Conclusion: 이 두 사례는 모델의 내부 추론뿐만 아니라, 확률적 AI를 결정론적 비즈니스 프로세스에 고정시키는 엄격한 엔지니어링 구조가 에이전트 신뢰성의 근원임을 입증하는 개념적 증거를 제공한다.

Abstract: Large Language Models (LLMs) are evolving into autonomous agents, yet current "frameless" development--relying on ambiguous natural language without engineering blueprints--leads to critical risks such as scope creep and open-loop failures. To ensure industrial-grade reliability, this study proposes Agentic Problem Frames (APF), a systematic engineering framework that shifts focus from internal model intelligence to the structured interaction between the agent and its environment.
  The APF establishes a dynamic specification paradigm where intent is concretized at runtime through domain knowledge injection. At its core, the Act-Verify-Refine (AVR) loop functions as a closed-loop control system that transforms execution results into verified knowledge assets, driving system behavior toward asymptotic convergence to mission requirements (R). To operationalize this, this study introduces the Agentic Job Description (AJD), a formal specification tool that defines jurisdictional boundaries, operational contexts, and epistemic evaluation criteria.
  The efficacy of this framework is validated through two contrasting case studies: a delegated proxy model for business travel and an autonomous supervisor model for industrial equipment management. By applying AJD-based specification and APF modeling to these scenarios, the analysis demonstrates how operational scenarios are systematically controlled within defined boundaries. These cases provide a conceptual proof that agent reliability stems not from a model's internal reasoning alone, but from the rigorous engineering structures that anchor stochastic AI within deterministic business processes, thereby enabling the development of verifiable and dependable domain agents.

</details>


### [20] [Proximity-Based Multi-Turn Optimization: Practical Credit Assignment for LLM Agent Training](https://arxiv.org/abs/2602.19225)
*Yangyi Fang,Jiaye Lin,Xiaoliang Fu,Cong Qin,Haolin Shi,Chang Liu,Peilin Zhao*

Main category: cs.AI

TL;DR: ProxMO는 실제 환경에서의 다중 턴 최적화를 위한 강력하고 실용적인 프레임워크로, 임무의 난이도를 고려하여 신뢰성 있는 성능 향상을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 턴 LLM 에이전트는 고객 서비스 자동화, 전자상거래 지원 및 상호작용 작업 관리에서 중요한 역할을 하며, 높은 가치의 정보 신호와 무작위 잡음을 정확히 구분하는 것이 샘플 효율적인 학습에 필수적입니다.

Method: ProxMO는 성공률 인지 모듈화와 근접성 기반 소프트 집계를 통해 전 세계적 맥락을 통합하여, 에피소드 수준의 난이도에 따라 기울기 강도를 동적으로 조정하고, 연속적인 의미론적 가중치를 바탕으로 기준선을 도출합니다.

Result: ALFWorld 및 WebShop 벤치마크에서 ProxMO는 기존 기준에 비해 상당한 성능 향상을 보였으며, 계산 비용은 최소화되었습니다.

Conclusion: ProxMO는 표준 GRPO 프레임워크와 플러그 앤 플레이 호환성을 제공하여 기존 산업 교육 파이프라인에서 즉각적이고 낮은 마찰의 채택을 용이하게 합니다.

Abstract: Multi-turn LLM agents are becoming pivotal to production systems, spanning customer service automation, e-commerce assistance, and interactive task management, where accurately distinguishing high-value informative signals from stochastic noise is critical for sample-efficient training. In real-world scenarios, a failure in a trivial task may reflect random instability, whereas success in a high-difficulty task signifies a genuine capability breakthrough. Yet, existing group-based policy optimization methods rigidly rely on statistical deviation within discrete batches, frequently misallocating credit when task difficulty fluctuates. To address this issue, we propose Proximity-based Multi-turn Optimization (ProxMO), a practical and robust framework engineered specifically for the constraints of real-world deployment. ProxMO integrates global context via two lightweight mechanisms: success-rate-aware modulation dynamically adapts gradient intensity based on episode-level difficulty, while proximity-based soft aggregation derives baselines through continuous semantic weighting at the step level. Extensive evaluations on ALFWorld and WebShop benchmarks demonstrate that ProxMO yields substantial performance gains over existing baselines with negligible computational cost. Ablation studies further validate the independent and synergistic efficacy of both mechanisms. Crucially, ProxMO offers plug-and-play compatibility with standard GRPO frameworks, facilitating immediate, low-friction adoption in existing industrial training pipelines. Our implementation is available at: \href{https://anonymous.4open.science/r/proxmo-B7E7/README.md}{https://anonymous.4open.science/r/proxmo}.

</details>


### [21] [Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering](https://arxiv.org/abs/2602.19240)
*Sen Zhao,Lincheng Zhou,Yue Chen,Ding Zou*

Main category: cs.AI

TL;DR: TopoRAG는 텍스트 그래프 질의 응답을 위한 새로운 프레임워크로, 고차원 위상 및 관계적 종속성을 효과적으로 캡처합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 RAG 변형은 주로 저차원 구조에 집중하여 사이클을 간과하고, 이로 인해 불완전한 맥락 기반 및 제한된 추론 능력이 발생합니다.

Method: TopoRAG는 텍스트 그래프를 셀룰러 복합체로 전환하여 다차원 위상 구조를 모델링합니다. 그런 다음, 입력 질의와 관련된 셀룰러 복합체를 추출하는 위상 인지 하위 복합체 검색 메커니즘을 제안합니다.

Result: 우리의 방법은 다양한 텍스트 그래프 작업에서 기존 기준을 일관되게 초과하는 것을 보여주었습니다.

Conclusion: 위상 강화 검색 증대 생성(TopoRAG)은 고차원 위상 및 관계적 종속성을 효과적으로 캡처하여 텍스트 그래프 질의 응답의 효율성을 높입니다.

Abstract: Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.

</details>


### [22] [Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts](https://arxiv.org/abs/2602.19244)
*Toshihide Ubukata,Zhiyao Wang,Enhong Mu,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: OTF-DCS는 상태 공간 폭발을 완화하기 위해 시스템을 점진적으로 탐색하며, 이 과정에서 탐색 정책에 의존한다. 최근 강화 학습 접근법은 이러한 정책을 학습하여 작은 훈련 사례에서 더 큰 미지의 사례로의 유망한 제로샷 일반화를 달성한다. 그러나 기본적인 한계는 비등방성 일반화로, RL 정책이 도메인-파라미터 공간의 특정 영역에서만 강력한 성능을 보이고 다른 곳에서는 훈련의 확률적 성격과 궤적 의존적 편향 때문에 취약해진다. 이를 해결하기 위해 우리는 여러 RL 전문가를 사전 신뢰 게이팅 메커니즘을 통해 결합하고 이러한 비등방적 행동을 보완적인 전문화로 다루는 Soft Mixture-of-Experts 프레임워크를 제안한다. 항공 교통 벤치마크에 대한 평가 결과, Soft-MoE는 솔브 가능한 파라미터 공간을 상당히 확장하고 단일 전문가와 비교하여 강인성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: OTF-DCS는 상태 공간 폭발 문제를 해결하기 위해 탐색 정책을 필요로 하며, 이를 통해 보다 효율적인 탐색을 목표로 한다.

Method: Soft Mixture-of-Experts 프레임워크는 여러 강화 학습 전문가를 결합하고, 비등방적 행동을 서로 보완하는 전문화로 취급한다.

Result: 항공 교통 벤치마크에서 Soft-MoE는 해결 가능한 파라미터 공간을 상당히 확장하고, 단일 전문가와 비교하여 강인성이 향상됨을 보여준다.

Conclusion: Soft Mixture-of-Experts는 다양한 강화 학습 전문가의 강점을 결합하여 OTF-DCS의 성능을 극대화할 수 있다.

Abstract: On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert.

</details>


### [23] [ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease](https://arxiv.org/abs/2602.19298)
*Nolan Brady,Tom Yeh*

Main category: cs.AI

TL;DR: ALPACA는 알츠하이머병을 위한 개인화된 치료 전략을 탐색하기 위한 강화 학습 환경을 제공하며, 기존 치료를 사용하여 환자의 병 진행을 시뮬레이션합니다.


<details>
  <summary>Details</summary>
Motivation: 알츠하이머병의 개인화된 치료 전략을 평가하는 것은 긴 질병 경과와 환자 간의 상당한 이질성 때문에 실용적이지 않습니다.

Method: ALPACA는 기존 치료법을 사용하여 개인화된 치료 전략을 체계적으로 탐색하기 위한 오픈 소스, Gym 호환 강화 학습 환경입니다.

Result: CAST 모델을 통해 ALPACA는 현실적인 약물 조건에 따른 궤적을 생성하며, ALPACA에서 훈련된 RL 정책은 메모리 관련 결과에서 비치료 및 행동 클론 임상의 기준선을 초과합니다.

Conclusion: ALPACA는 알츠하이머병의 개인별 순차적 치료 의사 결정을 연구하기 위한 재사용 가능한 컴퓨터 시험대입니다.

Abstract: Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD.

</details>


### [24] [OptiRepair: Closed-Loop Diagnosis and Repair of Supply Chain Optimization Models with LLM Agents](https://arxiv.org/abs/2602.19439)
*Ruicheng Ao,David Simchi-Levi,Xinshang Wang*

Main category: cs.AI

TL;DR: 이 논문은 공급망 최적화 모델의 비현실성을 진단하고 수리하는 데 AI 에이전트를 사용하는 가능성을 탐구합니다. OptiRepair는 두 가지 단계로 이 작업을 분리하여, 모델의 복잡성을 감소시키고 효율성을 향상시키는 데 기여합니다.


<details>
  <summary>Details</summary>
Motivation: 공급망 최적화 모델에서의 모델링 오류로 인해 비현실성이 자주 발생하며, 이를 진단하고 수리하는 데 전문적인 운영 연구(OR) 지식이 필요합니다. AI 에이전트가 이러한 작업을 수행할 수 있는지 여부는 아직 검증되지 않았습니다.

Method: OptiRepair는 이 작업을 도메인에 구애받지 않는 타당성 단계(모든 LP의 반복적인 IIS 안내 수리)와 도메인 특화 검증 단계(재고 이론에 기반한 다섯 가지 합리성 검증)로 분리합니다.

Result: 7개 가족의 22개 API 모델을 976개의 다중 계층 공급망 문제에 대해 테스트하고, 해결자 확인 보상이 있는 자습식 추론을 통해 두 개의 8B 매개변수 모델을 훈련시켰습니다. 훈련된 모델은 81.7%의 합리적 회복률(RRR)에 도달했습니다.

Conclusion: 현재 AI가 신뢰할 수 있는 모델 수리와의 격차는 두 가지로 나뉘며, 각각은 해결자 상호작용과 운영 합리성입니다. AI를 운영 계획에 도입하는 조직은 '합리적'이라는 의미를 구체적으로 정의하는 것이 더 큰 투자 수익을 가져올 것입니다.

Abstract: Problem Definition. Supply chain optimization models frequently become infeasible because of modeling errors. Diagnosis and repair require scarce OR expertise: analysts must interpret solver diagnostics, trace root causes across echelons, and fix formulations without sacrificing operational soundness. Whether AI agents can perform this task remains untested.
  Methodology/Results. OptiRepair splits this task into a domain-agnostic feasibility phase (iterative IIS-guided repair of any LP) and a domain-specific validation phase (five rationality checks grounded in inventory theory). We test 22 API models from 7 families on 976 multi-echelon supply chain problems and train two 8B-parameter models using self-taught reasoning with solver-verified rewards. The trained models reach 81.7% Rational Recovery Rate (RRR) -- the fraction of problems resolved to both feasibility and operational rationality -- versus 42.2% for the best API model and 21.3% on average. The gap concentrates in Phase 1 repair: API models average 27.6% recovery rate versus 97.2% for trained models.
  Managerial Implications. Two gaps separate current AI from reliable model repair: solver interaction (API models restore only 27.6% of infeasible formulations) and operational rationale (roughly one in four feasible repairs violate supply chain theory). Each requires a different intervention: solver interaction responds to targeted training; operational rationale requires explicit specification as solver-verifiable checks. For organizations adopting AI in operational planning, formalizing what "rational" means in their context is the higher-return investment.

</details>


### [25] [ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making](https://arxiv.org/abs/2602.19458)
*Ziyang Guo,Yifan Wu,Jason Hartline,Kenneth Holstein,Jessica Hullman*

Main category: cs.AI

TL;DR: 다수의 에이전트를 활용한 의사결정 파이프라인은 상보성 조건에서 단일 에이전트 워크플로우보다 우수한 성과를 낼 수 있다. 이를 위해 기초한 ComplLLM이라는 프레임워크를 제안하며, 이는 보상으로 상보적 정보를 활용해 의사결정 지원 LLM을 미세 조정한다.


<details>
  <summary>Details</summary>
Motivation: 상보성을 이용한 다수의 에이전트가 단일 에이전트를 사용할 때 보다 더 나은 의사결정을 할 수 있다는 점은 중요한 연구 동기이다.

Method: ComplLLM은 의사결정 이론에 기반하여 상보적 정보를 보상으로 활용하여 의사결정 지원 LLM을 미세 조정하는 프레임워크이다.

Result: ComplLLM은 합성 및 실제 작업에서 도메인 전문가들을 포함한 사례를 통해 검증되었으며, 이 방법이 알려진 상보적 정보를 복원하고, 추가적인 의사결정을 지원하기 위한 상보적 신호에 대한 타당한 설명을 생성하는 방식이 나타났다.

Conclusion: 따라서 ComplLLM은 후속 의사결정자들을 지원하는 데 효과적인 도구가 될 수 있다.

Abstract: Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.

</details>


### [26] [Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark](https://arxiv.org/abs/2602.19502)
*Lalitha Pranathi Pulavarthy,Raajitha Muthyala,Aravind V Kuruvikkattil,Zhenan Yin,Rashmita Kudamala,Saptarshi Purkayastha*

Main category: cs.AI

TL;DR: 인간의 지도가 있는 에이전틱 AI가 다중 모드 임상 예측에서 성능을 향상시킬 수 있음을 보여주는 연구.


<details>
  <summary>Details</summary>
Motivation: 임상 예측 작업은 도메인 전문 지식을 요구하며 순수 자동화 접근 방식이 이를 제공하는 데 어려움을 겪고 있다.

Method: 인간 분석가가 주요 결정 지점에서 에이전틱 워크플로우를 안내하고, 다중 모드 특성 엔지니어링, 모델 선택 및 임상적으로 정보에 기반한 검증 전략을 수행했다.

Result: 본 연구는 30일 병원 재입원 예측(Macro-F1 = 0.8986), 응급실 비용 예측(MAE = $465.13), 퇴원 준비 상태 평가(Macro-F1 = 0.7939)에서 성과를 거뒀으며, 전반적으로 헬스케어 도메인에서 5위에 올랐다.

Conclusion: 이 연구는 에이전틱 AI를 임상 환경에 배치하는 팀들에게 실용적인 지침을 제공한다.

Abstract: Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.

</details>


### [27] [TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents](https://arxiv.org/abs/2602.19633)
*Jongwon Jeong,Jungtaek Kim,Kangwook Lee*

Main category: cs.AI

TL;DR: 본 연구에서는 오류가 치명적일 수 있는 환경에서의 언어 모델 에이전트의 한계를 극복하기 위한 TAPE라는 새로운 계획 기법을 제안하고, 다양한 테스트에서 기존 프레임워크보다 우수한 성능을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델 에이전트는 다중 상호작용이 요구되는 작업을 해결하는 데 있어 우수한 능력을 보여주지만, 특정 환경에서는 단 하나의 오류로 복구 불가능한 실패를 초래할 수 있어서 이에 대한 체계적인 분석이 필요하다.

Method: TAPE는 여러 계획을 그래프로 집합화하고 외부 솔버를 통해 실행 가능한 경로를 식별함으로써 계획 능력을 향상시키며, 실행 중에는 제약 디코딩을 통해 샘플링 노이즈를 줄이고, 환경 피드백이 의도된 상태에서 벗어날 경우 적응형 재계획을 수행한다.

Result: Sokoban, ALFWorld, MuSiQue, GSM8K-Hard 등의 실험 결과, TAPE는 기존 프레임워크들보다 일관되게 더 우수한 성능을 보였으며, 특히 어려운 설정에서 더 큰 이점을 나타냈다.

Conclusion: 어려운 설정에서는 평균적으로 성공률이 21.0% 포인트 향상되었고, 약한 기본 모델의 경우 평균 20.0% 포인트 향상되었다.

Abstract: Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.

</details>


### [28] [SkillOrchestra: Learning to Route Agents via Skill Transfer](https://arxiv.org/abs/2602.19672)
*Jiayu Wang,Yifei Ming,Zixuan Ke,Shafiq Joty,Aws Albarghouthi,Frederic Sala*

Main category: cs.AI

TL;DR: SkillOrchestra는 실행 경험에서 세밀한 기술을 배우고 명시적인 성능-비용 트레이드오프 하에서 최적의 에이전트를 선택하는 기술 인식 오케스트레이션 프레임워크로, 기존의 RL 기반 오케스트레이터보다 22.5% 향상된 성능과 700배 및 300배의 학습 비용 절감을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 복합 AI 시스템은 개별 모델을 초월하는 능력을 약속하지만, 효과적인 오케스트레이션에 크게 의존한다.

Method: SkillOrchestra는 종단 간 라우팅 정책을 학습하는 대신 실행 경험에서 세밀한 기술을 배우고 에이전트별 능력 및 비용을 모델링한다.

Result: SkillOrchestra는 10개의 벤치마크에서 기존의 RL 기반 오케스트레이터보다 최대 22.5% 향상된 성능을 보였고, Router-R1 및 ToolOrchestra와 비교하여 각각 700배 및 300배의 학습 비용 절감을 달성했다.

Conclusion: 명시적인 기술 모델링이 확장 가능하고 해석 가능하며 샘플 효율적인 오케스트레이션을 가능하게 하며, 데이터 집약적인 RL 기반 접근 방법에 대한 원칙적인 대안을 제공한다.

Abstract: Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.

</details>


### [29] [OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research](https://arxiv.org/abs/2602.19810)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Emre Ulgac,Aakaash Meduri*

Main category: cs.AI

TL;DR: 이 연구는 OpenClaw와 Moltbook이 생성한 자율 AI 간 상호작용의 대규모 데이터셋을 기반으로 한 문헌 검토와 ClawdLab이라는 오픈소스 플랫폼을 제안한다.


<details>
  <summary>Details</summary>
Motivation: OpenClaw와 Moltbook이 제공하는 대규모 자율 AI 상호작용 데이터는 학문적 연구의 새로운 기회를 제공한다.

Method: 다양한 문헌 검토를 통해 AI 상호작용의 아키텍처 실패 모드를 분석하고, 이를 해결하기 위한 디자인 과학 응답으로 ClawdLab 플랫폼을 제안한다.

Result: ClawdLab은 하드 역할 제한, 구조적 적대적 비판, PI 주도 거버넌스 등을 통해 이러한 실패 모드를 해결하고, Emergent Sybil 저항 구조를 제공한다.

Conclusion: ClawdLab의 조합 가능한 아키텍처는 AI 생태계의 발전에 따라 지속적인 개선을 가능하게 한다.

Abstract: In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.

</details>


### [30] [Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent](https://arxiv.org/abs/2602.19837)
*Björn Hoppmann,Christoph Scholz*

Main category: cs.AI

TL;DR: 메타 학습은 모델이 다양한 작업에서 전이 가능한 지식을 획득하여 새로운 도전에 신속하게 적응할 수 있게 해주는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 인간은 이전 지식을 효과적으로 사용하여 새로운 작업에 적응하는 능력을 가지고 있지만, 기존 머신 러닝 모델은 작업에 특화된 훈련에 의존하여 이를 복제하는 데 어려움을 겪는다.

Method: 메타 학습은 다양한 작업에서 전이 가능한 지식을 모델이 습득할 수 있도록 하여 최소한의 데이터로 새로운 도전에 신속하게 적응하도록 한다.

Result: 이 설문조사는 메타 학습 및 메타 강화 학습에 대한 엄격한 작업 기반 공식화를 제공하고, 이 패러다임을 사용하여 DeepMind의 적응형 에이전트를 위한 주요 알고리즘을 정리한다.

Conclusion: 적응형 에이전트와 기타 일반적 접근 방식을 이해하는 데 필요한 필수 개념을 통합한다.

Abstract: Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.

</details>


### [31] [Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning](https://arxiv.org/abs/2602.19930)
*Nathan Gavenski,Felipe Meneguzzi,Odinaldo Rodrigues*

Main category: cs.AI

TL;DR: 이 논문은 모방 학습의 근본적인 문제를 지적하고, 성공의 정의를 완벽한 재생에서 조합 가능성 적응으로 재정립할 것을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 모방 학습 에이전트는 복잡한 기억 장치로서 최적화되었지만, 맥락이 바뀌거나 목표가 진화할 때 실패합니다. 이는 기술적인 문제라기보다는 기초적인 문제로, 모방 학습이 잘못된 목표로 최적화되어 있다는 주장입니다.

Method: 행동 원형을 한 번 학습하고, 재훈련 없이 새로운 맥락에서 조합하는 방법론을 제안합니다.

Result: 조합 가능성 일반화에 대한 메트릭을 확립하고, 하이브리드 아키텍처를 제안하며, 인지 과학 및 문화 진화에 기초한 학제 간 연구 방향을 제시합니다.

Conclusion: 적응성을 모방 학습의 핵심에 내재화한 에이전트는 열린 세계에서 작동할 수 있는 필수 능력을 갖추게 됩니다.

Abstract: Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.

</details>


### [32] [Agents of Chaos](https://arxiv.org/abs/2602.20021)
*Natalie Shapira,Chris Wendler,Avery Yen,Gabriele Sarti,Koyena Pal,Olivia Floody,Adam Belfki,Alex Loftus,Aditya Ratan Jannali,Nikhil Prakash,Jasmine Cui,Giordano Rogers,Jannik Brinkmann,Can Rager,Amir Zur,Michael Ripa,Aruna Sankaranarayanan,David Atkinson,Rohit Gandikota,Jaden Fiotto-Kaufman,EunJeong Hwang,Hadas Orgad,P Sam Sahil,Negev Taglicht,Tomer Shabtay,Atai Ambus,Nitay Alon,Shiri Oron,Ayelet Gordon-Tapiero,Yotam Kaplan,Vered Shwartz,Tamar Rott Shaham,Christoph Riedl,Reuth Mirsky,Maarten Sap,David Manheim,Tomer Ullman,David Bau*

Main category: cs.AI

TL;DR: 이 연구는 라이브 실험 환경에서 자율 언어 모델 기반 에이전트의 탐색적 적대적 연구 결과를 보고합니다.


<details>
  <summary>Details</summary>
Motivation: 자율 언어 모델과 도구 사용의 통합에서 발생하는 실패를 분석하여 안전성과 프라이버시 문제를 밝혀내고자 했습니다.

Method: 20명의 AI 연구자들이 비우호적 및 적대적 조건 하에 에이전트와 상호작용하며 사례 연구를 통해 데이터를 수집했습니다.

Result: 11개의 대표적인 사례 연구를 문서화하고, 비합법적 정보 공개, 시스템 파괴 행위, 서비스 거부 상태 등의 여러 가지 문제가 관찰되었습니다.

Conclusion: 이 결과는 현실적인 배포 환경에서 보안 및 프라이버시 관련 취약점의 존재를 확립하며, 법률 학자 및 정책 결정자들의 긴급한 관심을 요구합니다.

Abstract: We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.

</details>


### [33] [CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence](https://arxiv.org/abs/2602.20048)
*Tarakanath Paipuru*

Main category: cs.AI

TL;DR: 현대 코드 인텔리전스 에이전트는 100만 개 이상의 토큰에서 작동하지만, 실제 코딩 작업에서 중요 파일을 발견하는 데 지속적으로 실패하고 있다. 본 연구는 내비게이션 패러독스를 규명하며, 그래프 기반 구조 내비게이션이 전통적 검색보다 우수함을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 코드 인텔리전스 에이전트가 실제 코딩 작업을 수행할 때 필수 파일을 찾지 못하는 문제를 해결하고자 하였다.

Method: 연구진은 FastAPI 리포지토리에서 30개의 벤치마크 작업을 통해 258회의 자동 시험을 수행하였고, CodeCompass를 활용하여 그래프 기반 구조 내비게이션을 적용하였다.

Result: CodeCompass를 이용한 그래프 기반 내비게이션은 숨겨진 종속 작업에서 99.4%의 작업 완료율을 달성하였다.

Conclusion: 도구 접근성을 넘어선 행동적 정렬의 필요성이 발견되었으며, 구조적 맥락을 활용하도록 에이전트를 명시적으로 안내해야 함을 밝혔다.

Abstract: Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.

</details>


### [34] [Interaction Theater: A case of LLM Agents Interacting at Scale](https://arxiv.org/abs/2602.20059)
*Sarath Shekkizhar,Adam Earle*

Main category: cs.AI

TL;DR: 자율 대화형 LLM 에이전트가 상호작용할 때 발생하는 현상을 연구하였다. 대부분의 에이전트는 다양한 출력이지만, 실질적인 논의는 부족하며, 많은 댓글이 스팸으로 분류된다.


<details>
  <summary>Details</summary>
Motivation: 자율 LLM 에이전트가 대규모로 상호작용할 때 실제로 어떤 일이 발생하는지에 대한 기본적인 질문을 탐구하기 위해.

Method: Moltbook이라는 AI 에이전트 전용 소셜 플랫폼의 데이터를 사용하여 에이전트 상호작용 품질을 특징짓기 위한 다양한 메트릭을 결합하여 분석하였다.

Result: 에이전트는 잘 형성된 다양한 텍스트를 생성하지만, 실질적인 내용은 부족하며, 많은 댓글이 스팸 또는 주제와 관련 없는 내용으로 분류된다.

Conclusion: 효과적인 멀티 에이전트 상호작용 설계를 위해서는 조정 메커니즘이 명시적으로 설계되어야 하며, 그렇지 않으면 생산적인 교환보다는 병렬 출력을 생성한다.

Abstract: As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\%$) vary their output across contexts, $65\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\%$) and off-topic content ($22\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.

</details>


### [35] [ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models](https://arxiv.org/abs/2602.20117)
*Andre He,Nathaniel Weir,Kaj Bostrom,Allen Nie,Darion Cassel,Sam Bayless,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 검증 가능한 보상을 이용한 강화 학습(RLVR)은 검증자의 감독을 활용하여 추론 언어 모델(RLM)을 훈련시키는 유망한 방법이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 합성 데이터 생성 방법들이 주로 솔루션 중심으로 되어 있는 반면, 검증자 기반 방법들은 일부 수작업 절차 환경에 의존하고 있다.

Method: ReSyn이라는 파이프라인을 도입하여 다양한 추론 환경을 생성하고 인스턴스 생성기와 검증자를 갖추어 여러 작업을 포괄한다.

Result: Qwen2.5-7B-Instruct 모델이 ReSyn 데이터로 RL로 훈련되어 추론 벤치마크와 도메인 외 수학 벤치마크에서 일관된 성능 향상을 달성했다.

Conclusion: 검증자 기반 감독과 증가된 작업 다양성이 모두 RLM의 추론 능력을 향상시킬 수 있음을 실증적으로 증명한다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs

</details>


### [36] [Recurrent Structural Policy Gradient for Partially Observable Mean Field Games](https://arxiv.org/abs/2602.20141)
*Clarisse Wibault,Johannes Forkel,Sebastian Towers,Tiphaine Wibault,Juan Duque,George Whittle,Andreas Schaab,Yucheng Yang,Chiyuan Wang,Michael Osborne,Benjamin Moll,Jakob Foerster*

Main category: cs.AI

TL;DR: 이 논문은 공공 정보가 포함된 설정에서 최초의 역사 인식 혼합 구조 정책 경량화(RSPG) 방법을 제안하고, 이 방법은 통계 역학 모델에서의 최첨단 성능을 달성하며 빠른 수렴 속도를 보인다.


<details>
  <summary>Details</summary>
Motivation: 대규모 인구 모델에서의 상호 작용을 모델링하기 위한 원칙적인 프레임워크를 제공하는 MFG의 한계를 극복하기 위해.

Method: 역사 인식 혼합 구조 정책 경량화(RSPG)와 JAX 기반 프레임워크(MFAX)를 사용한다.

Result: RSPG는 통계 역학 모델에서 최첨단 성능과 더불어 주문의 차이로 더 빠른 수렴을 달성하며 이질적 에이전트, 공통 노이즈 및 역사 인식 정책을 갖는 거시 경제학 MFG를 해결한다.

Conclusion: MFAX는 MFG에 대한 JAX 기반의 공개 프레임워크로, 공개적으로 이용 가능하다.

Abstract: Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [37] [Evolution of fairness in hybrid populations with specialised AI agents](https://arxiv.org/abs/2602.18498)
*Zhao Song,Theodor Cimpeanu,Chen Shen,The Anh Han*

Main category: cs.MA

TL;DR: 이 논문에서는 AI가 공정성을 유지하는 방식에 대해 논의하며, 비대칭적인 사회 구조에서의 효과를 보여준다. 디스크리미네이토리 AI 제안자가 공정성을 보다 효과적으로 유지하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 하이브리드 사회에서 AI가 공정성을 어떻게 유지해야 하는지에 대한 선택의 필요성을 강조한다.

Method: Ultimatum Game의 이분법적 하이브리드 인구 모델을 사용하여 인간과 AI를 제안자와 수령자로 구분하고, 사마리아인 AI와 차별적 AI 제안자를 설계한다.

Result: 사마리아인 AI 수령자가 전체 인구의 공정성을 더 효과적으로 유지하고, 차별적 AI 제안자가 모든 유형의 사마리아인 AI보다 우수한 성능을 보였다.

Conclusion: 비조건적인 모델링에서 전략적 실행으로의 전환이 비대칭 AI를 활용하는 데 중요한 틀을 제공한다.

Abstract: Fairness in hybrid societies hinges on a simple choice: should AI be a generous host or a strict gatekeeper? Moving beyond symmetric models, we show that asymmetric social structures--like those in hiring, regulation, and negotiation--AI that guards fairness outperforms AI that gifts it. We bridge this gap with a bipartite hybrid population model of the Ultimatum Game, separating humans and AI into distinct proposer and receiver groups. We first introduce Samaritan AI agents, which act as either unconditional fair proposers or strict receivers. Our results reveal a striking asymmetry: Samaritan AI receivers drive population-wide fairness far more effectively than Samaritan AI proposers. To overcome the limitations of the Samaritan AI proposer, we design the Discriminatory AI proposer, which predicts co-players' expectations and only offers fair portions to those with high acceptance thresholds. Our results demonstrate that this Discriminatory AI outperforms both types of Samaritan AI, especially in strong selection scenarios. It not only sustains fairness across both populations but also significantly lowers the critical mass of agents required to reach an equitable steady state. By transitioning from unconditional modelling to strategic enforcement, our work provides a pivotal framework for deploying asymmetric AIs in the increasingly hybrid society.

</details>


### [38] [NutriOrion: A Hierarchical Multi-Agent Framework for Personalized Nutrition Intervention Grounded in Clinical Guidelines](https://arxiv.org/abs/2602.18650)
*Junwei Wu,Runze Yan,Hanqi Luo,Darren Liu,Minxiao Wang,Kimberly L. Townsend,Lydia S. Hartwig,Derek Milketinas,Xiao Hu,Carl Yang*

Main category: cs.MA

TL;DR: 이 논문에서는 다중 질환 환자를 위한 개인화된 영양 개입을 개선하기 위해 NutriOrion이라는 계층 다중 에이전트 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 질환 환자에 대한 개인화된 영양 개입은 건강 결과를 개선하는 데 중요하지만, 이질적인 임상 조건, 약물 및 식이 지침을 동시에 통합해야 하기 때문에 도전적입니다.

Method: NutriOrion은 영양 계획을 전문화된 도메인 에이전트로 분해하여 고립된 맥락을 유지하고, 조건적 정제 단계를 진행하는 계층적 다중 에이전트 프레임워크입니다.

Result: NutriOrion은 330명의 다중 질환 중풍 환자를 대상으로 평가된 결과, 여러 기준선 모델들보다 우수한 성능을 보였습니다.

Conclusion: NutriOrion은 임상적으로 의의 있는 식이 개선을 달성하며, 안전성을 보장하고 대립되는 식이 요구 사항을 해결합니다.

Abstract: Personalized nutrition intervention for patients with multimorbidity is critical for improving health outcomes, yet remains challenging because it requires the simultaneous integration of heterogeneous clinical conditions, medications, and dietary guidelines. Single-agent large language models (LLMs) often suffer from context overload and attention dilution when processing such high-dimensional patient profiles. We introduce NutriOrion, a hierarchical multi-agent framework with a parallel-then-sequential reasoning topology. NutriOrion decomposes nutrition planning into specialized domain agents with isolated contexts to mitigate anchoring bias, followed by a conditional refinement stage. The framework includes a multi-objective prioritization algorithm to resolve conflicting dietary requirements and a safety constraint mechanism that injects pharmacological contraindications as hard negative constraints during synthesis, ensuring clinical validity by construction rather than post-hoc filtering. For clinical interoperability, NutriOrion maps synthesized insights into the ADIME standard and FHIR R4 resources. Evaluated on 330 stroke patients with multimorbidity, NutriOrion outperforms multiple baselines, including GPT-4.1 and alternative multi-agent architectures. It achieves a 12.1 percent drug-food interaction violation rate, demonstrates strong personalization with negative correlations (-0.26 to -0.35) between patient biomarkers and recommended risk nutrients, and yields clinically meaningful dietary improvements, including a 167 percent increase in fiber and a 27 percent increase in potassium, alongside reductions in sodium (9 percent) and sugars (12 percent).

</details>


### [39] [When Coordination Is Avoidable: A Monotonicity Analysis of Organizational Tasks](https://arxiv.org/abs/2602.18673)
*Harang Ju*

Main category: cs.MA

TL;DR: 조직에서의 조정 작업 필요성에 대한 명확한 기준을 제시


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 AI 시스템에서 조정 비용이 작업 비용을 초과하는 문제 해결

Method: 조직의 상호 의존성을 비모노토닉성 기준에 기반하여 분류하고 조정 세금 측정

Result: 65개의 기업 워크플로우 중 74%가 비모노토닉이며 13,417개의 직무 중 42%가 비모노토닉으로 나타남

Conclusion: 조정에 드는 비용의 24-57%가 정확성을 위해 불필요하다

Abstract: Organizations devote substantial resources to coordination, yet which tasks actually require it for correctness remains unclear. The problem is acute in multi-agent AI systems, where coordination overhead is directly measurable and routinely exceeds the cost of the work itself. However, distributed systems theory provides a precise answer: coordination is necessary if and only if a task is non-monotonic, meaning new information can invalidate prior conclusions. Here we show that a classic taxonomy of organizational interdependence maps onto the monotonicity criterion, yielding a decision rule and a measure of avoidable overhead (the Coordination Tax). Multi-agent simulations confirm both predictions. We classify 65 enterprise workflows and find that 48 (74%) are monotonic, then replicate on 13,417 occupational tasks from the O*NET database (42% monotonic). These classification rates imply that 24-57% of coordination spending is unnecessary for correctness.

</details>


### [40] [EDU-MATRIX: A Society-Centric Generative Cognitive Digital Twin Architecture for Secondary Education](https://arxiv.org/abs/2602.18705)
*Wenjing Zhai,Jianbin Zhang,Tao Liu*

Main category: cs.MA

TL;DR: EDU-MATRIX는 사회 중심의 생성적 인지 디지털 트윈 아키텍처로, 개인 에이전트를 시뮬레이션하는 대신 사회적 공간을 시뮬레이션하는 패러다임으로 전환합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시뮬레이션은 에이전트에 하드코딩된 규칙으로 인해 복잡한 사회적 동역학이 경직되고 교육적 가치와 맞추기 어려운 "에이전트 중심 역설"을 겪고 있습니다.

Method: EDU-MATRIX는 (1) 환경 맥락 주입 엔진(ECIE), (2) 모듈형 논리 진화 프로토콜(MLEP), (3) 역할-위상에 의한 내생적 정렬이라는 세 가지 아키텍처 기여를 도입합니다.

Result: 2,400명의 에이전트가 있는 중학교의 디지털 트윈으로 배포된 이 시스템은 "사회적 중력"(규칙)과 "인지 유체"(지식)가 어떻게 상호작용하여 emergent하고 가치 정렬된 행동을 생성하는지를 보여줍니다.

Conclusion: 이 연구는 새로운 사회적 규칙과 지식의 통합을 통해 교육적 가치와 일치하는 행동을 이끌어내는 방법을 제시합니다.

Abstract: Existing multi-agent simulations often suffer from the "Agent-Centric Paradox": rules are hard-coded into individual agents, making complex social dynamics rigid and difficult to align with educational values. This paper presents EDU-MATRIX, a society-centric generative cognitive digital twin architecture that shifts the paradigm from simulating "people" to simulating a "social space with a gravitational field." We introduce three architectural contributions: (1) An Environment Context Injection Engine (ECIE), which acts as a "social microkernel," dynamically injecting institutional rules (Gravity) into agents based on their spatial-temporal coordinates; (2) A Modular Logic Evolution Protocol (MLEP), where knowledge exists as "fluid" capsules that agents synthesize to generate new paradigms, ensuring high dialogue consistency (94.1%); and (3) Endogenous Alignment via Role-Topology, where safety constraints emerge from the agent's position in the social graph rather than external filters. Deployed as a digital twin of a secondary school with 2,400 agents, the system demonstrates how "social gravity" (rules) and "cognitive fluids" (knowledge) interact to produce emergent, value-aligned behaviors (Social Clustering Coefficient: 0.72).

</details>


### [41] [Adaptive Collaboration of Arena-Based Argumentative LLMs for Explainable and Contestable Legal Reasoning](https://arxiv.org/abs/2602.18916)
*Hoang-Loc Cao,Phuc Ho,Truong Thanh Hung Nguyen,Phuc Truong Loc Nguyen,Dinh Thien Loc Nguyen,Hung Cao*

Main category: cs.MA

TL;DR: 이 논문은 법적 추론의 정확성과 사용자 개입의 가능성을 높이기 위해 적응형 협업 아규먼티브 LLM(ACAL) 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 법적 추론에는 높은 정확도와 함께 검증 가능하고 논의 가능한 주장을 통해 결정을 정당화하는 능력이 필요합니다.

Method: ACAL은 적응형 다중 에이전트 협업과 Arena 기반의 양적 양극 아귀먼트 프레임워크(A-QBAF)를 통합한 신경-상징적 프레임워크입니다.

Result: ACAL은 Gemini-2.5-Flash-Lite 및 Gemini-2.5-Flash 아키텍처에서 강력한 기준선보다 더 뛰어난 성능을 보였습니다.

Conclusion: ACAL은 효율적인 예측 성능과 구조적 투명성 및 논의 가능성을 효과적으로 균형 있게 제공합니다.

Abstract: Legal reasoning requires not only high accuracy but also the ability to justify decisions through verifiable and contestable arguments. However, existing Large Language Model (LLM) approaches, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG), often produce unstructured explanations that lack a formal mechanism for verification or user intervention. To address this limitation, we propose Adaptive Collaboration of Argumentative LLMs (ACAL), a neuro-symbolic framework that integrates adaptive multi-agent collaboration with an Arena-based Quantitative Bipolar Argumentation Framework (A-QBAF). ACAL dynamically deploys expert agent teams to construct arguments, employs a clash resolution mechanism to adjudicate conflicting claims, and utilizes uncertainty-aware escalation for borderline cases. Crucially, our framework supports a Human-in-the-Loop (HITL) contestability workflow, enabling users to directly audit and modify the underlying reasoning graph to influence the final judgment. Empirical evaluations on the LegalBench benchmark demonstrate that ACAL outperforms strong baselines across Gemini-2.5-Flash-Lite and Gemini-2.5-Flash architectures, effectively balancing efficient predictive performance with structured transparency and contestability. Our implementation is available at: https://github.com/loc110504/ACAL.

</details>


### [42] [A potentialization algorithm for games with applications to multi-agent learning in repeated games](https://arxiv.org/abs/2602.18925)
*Philipp Lakheshar,Sharwin Rezagholi*

Main category: cs.MA

TL;DR: 본 연구는 정상형 게임에 잠재 함수가 있는 근사 게임을 부여하는 알고리즘을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 잠재 게임의 특성 덕분에, 이 알고리즘은 모든 게임에 대한 대리 보상 구조를 제공하여 효율적인 다중 에이전트 학습을 가능하게 합니다.

Method: 수치 시뮬레이션을 통해 복제기 동역학을 활용하여 '잠재화'가 안정적인 에이전트 행동으로의 수렴을 보장함을 입증합니다.

Result: 잠재화 과정을 통해 안정적인 행동으로의 수렴이 보장됨을 확인했습니다.

Conclusion: 이 알고리즘은 정상형 게임들을 다중 에이전트 학습을 위한 효과적인 도구로 만들 수 있습니다.

Abstract: We investigate an algorithm that assigns to any game in normal form an approximating game that admits an ordinal potential function. Due to the properties of potential games, the algorithm equips every game with a surrogate reward structure that allows efficient multi-agent learning. Numerical simulations using the replicator dynamics show that 'potentialization' guarantees convergence to stable agent behavior.

</details>


### [43] [Scaling Inference-Time Computation via Opponent Simulation: Enabling Online Strategic Adaptation in Repeated Negotiation](https://arxiv.org/abs/2602.19309)
*Xiangyu Liu,Di Wang,Zhe Feng,Aranyak Mehta*

Main category: cs.MA

TL;DR: 대형 언어 모델(LLM)이 동적 적과의 반복적이고 전략적인 상호작용을 할 때의 적응 효율성을 향상시키기 위해, 본 연구에서는 기존의 오프라인 학습 방식을 넘어 인사이트 기반의 적응 모델을 개발하고, 이를 통해 성능을 크게 향상시킨 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 정적인 환경에서 강력한 의사결정자로 자리 잡았지만, 동적 적과의 반복적이고 전략적인 상호작용에 대한 연구는 부족하다.

Method: 클래식 게임 이론에 기반한 학습 동역학인 '부드러운 허상 놀이(sFP)'를 LLM 추론에 통합하여 적응 기제를 향상시켜, 보조적인 적 모델을 활용하고 최적 반응을 위해 적 모델에 대항하여 시뮬레이션하는 'N개 중 최선의 선택(BoN) 샘플링'을 적용한다.

Result: 반복적 온라인 상호작용에서 기존 다양한 기준과 비교하여 성능이 크게 향상되었음을 보여준다.

Conclusion: 매개변수 업데이트 없이 반복적인 전략적 의사결정을 위한 확장 가능하고 이론적인 접근 방식을 제시한다.

Abstract: While large language models (LLMs) have emerged as powerful decision-makers across a wide range of single-agent and stationary environments, fewer efforts have been devoted to settings where LLMs must engage in \emph{repeated} and \emph{strategic} interactions with unknown or dynamic opponents. In such settings, recipes built upon \emph{offline} pre-training or fine-tuning, though robust against worst-case adversaries, do not fully exploit the capability of LLMs to adapt \emph{online} based on interaction feedback. Instead, we explore the more natural perspective of scaling inference-time computation as a mechanism for adaptation, embedding the principles of a classical game-theoretical learning dynamic, \emph{smooth Fictitious Play (sFP)}, into LLM inference: (i) for belief formation, we employ an auxiliary opponent model that in-context learns to imitate the time-averaged behavior of the opponent; (ii) for best response, we advance best-of-$N$ (BoN) sampling by simulating against the opponent model. Empirical evaluations on two distinct forms of repeated negotiation games demonstrate that our method enables significant performance improvement over repeated online interaction compared to various baselines, offering a scalable and principled approach to repeated strategic decision-making without any parameter updates.

</details>


### [44] [City Editing: Hierarchical Agentic Execution for Dependency-Aware Urban Geospatial Modification](https://arxiv.org/abs/2602.19326)
*Rui Liu,Steven Jige Quan,Zhong-Ren Peng,Zijun Yao,Han Wang,Zhengzhang Chen,Kunpeng Liu,Yanjie Fu,Dongjie Wang*

Main category: cs.MA

TL;DR: 도시 재생을 위해 기존 계획의 효율적인 수정이 필요하다.


<details>
  <summary>Details</summary>
Motivation: 교통 혼잡 및 기능 불균형과 같은 문제로 인해 도시 재생의 필요성이 증가하고 있다.

Method: GeoJSON을 사용해 도시 레이아웃을 표현하고, 자연어 편집 지시를 계층적 기하학적 의도로 분해하여 도시 계획을 기계 실행 가능 작업으로 형식화한다.

Result: 다양한 도시 편집 시나리오에서 기존 기준보다 효율성, 견고성, 정확성 및 공간 유효성이 크게 개선되었다.

Conclusion: 계층적 에이전틱 프레임워크를 사용하여 다단계 편집 중 오류 축적을 완화하고 글로벌 공간 일관성을 유지할 수 있다.

Abstract: As cities evolve over time, challenges such as traffic congestion and functional imbalance increasingly necessitate urban renewal through efficient modification of existing plans, rather than complete re-planning. In practice, even minor urban changes require substantial manual effort to redraw geospatial layouts, slowing the iterative planning and decision-making procedure. Motivated by recent advances in agentic systems and multimodal reasoning, we formulate urban renewal as a machine-executable task that iteratively modifies existing urban plans represented in structured geospatial formats. More specifically, we represent urban layouts using GeoJSON and decompose natural-language editing instructions into hierarchical geometric intents spanning polygon-, line-, and point-level operations. To coordinate interdependent edits across spatial elements and abstraction levels, we propose a hierarchical agentic framework that jointly performs multi-level planning and execution with explicit propagation of intermediate spatial constraints. We further introduce an iterative execution-validation mechanism that mitigates error accumulation and enforces global spatial consistency during multi-step editing. Extensive experiments across diverse urban editing scenarios demonstrate significant improvements in efficiency, robustness, correctness, and spatial validity over existing baselines.

</details>


### [45] [Effects of Property Recovery Incentives and Social Interaction on Self-Evacuation Decisions in Natural Disasters: An Agent-Based Modelling Approach](https://arxiv.org/abs/2602.19639)
*Made Krisnanda,Raymond Chiong,Yang Yang,Kirill Glavatskiy*

Main category: cs.MA

TL;DR: Evacuation decision-making을 이해하는 것은 재해 완화 정책 설계의 핵심 요소이다. 이 연구는 커뮤니티 내 가정 간의 의사소통이 자발적 대피 결정에 미치는 영향을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 재해 완화 정책 설계의 핵심 요소로 대피 의사결정 행동을 이해하는 것이 중요하다.

Method: 가정 대리인의 대피 또는 잔류 결정을 시뮬레이션하는 대리인 기반 모델을 개발하고, 진화 게임 이론의 틀 내에서 상호작용한다.

Result: 인센티브의 영향은 자금 가치 증가 및 가정 대리인 우선순위에 따라 감소하며, 최적의 정부 지원 수준이 존재함을 발견했다.

Conclusion: 사회적 연결성의 중요성을 보여주며, 제한된 자원 하에서 공동체 대피를 장려하고 우선순위를 매기기 위한 최적의 정부 정책 설계에 유용하다.

Abstract: Understanding evacuation decision-making behaviour is one of the key components for designing disaster mitigation policies. This study investigates how communications between household agents in a community influence self-evacuation decisions. We develop an agent-based model that simulates household agents' decisions to evacuate or stay. These agents interact within the framework of evolutionary game theory, effectively competing for limited shared resources, which include property recovery funds and coordination services. We explore four scenarios that model different prioritisations of access to government-provided incentives. We discover that the impact of the incentive diminishes both with increasing funding value and the household agent prioritisation, indicating that there is an optimal level of government support beyond which further increases become impractical. Furthermore, the overall evacuation rate depends on the structure of the underlying social network, showing discontinuous jumps when the prioritisation moves across the node degree. We identify the so-called "community influencers", prioritisation of whom significantly increases the overall evacuation rate. In contrast, prioritising household agents with low connectivity may actually impede collective evacuation. These findings demonstrate the importance of social connectivity between household agents. The results of this study are useful for designing optimal government policies to incentivise and prioritise community evacuation under limited resources.

</details>


### [46] [Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning](https://arxiv.org/abs/2602.20078)
*Shan Yang,Yang Liu*

Main category: cs.MA

TL;DR: 협력적 다중 에이전트 강화 학습(MARL)의 확장은 에이전트 간 노 noise로 인해 근본적으로 제한된다. 본 연구에서는 Descent-Guided Policy Gradient(DG-PG)라는 프레임워크를 제안하여 분석 모델을 통해 에이전트 간의 경량 지도 기울기를 구축하고 경량화된 기울기 분산을 달성하였다.


<details>
  <summary>Details</summary>
Motivation: 협력적 다중 에이전트 강화 학습에서 모든 에이전트의 행동에 의해 결정되는 보상의 공동 성격으로 인해, 에이전트 수가 많아질수록 교차 에이전트 노 noise가 증가하여 학습 성능이 저하된다.

Method: Descent-Guided Policy Gradient(DG-PG)라는 프레임워크는 분석 모델을 바탕으로 각 에이전트에 대한 노 noise 없는 지도 기울기를 구축하여, 모든 다른 에이전트의 행동으로부터 각 에이전트의 기울기를 분리한다.

Result: DG-PG는 기울기 분산을 Θ(N)에서 O(1)로 줄이고, 협력적 게임의 균형을 유지하며, 에이전트와 독립적인 샘플 복잡도를 O(1/ε)로 달성한다.

Conclusion: 200 에이전트까지의 이질적인 클라우드 스케줄링 작업에서, DG-PG는 모든 테스트된 스케일에서 10 에피소드 이내에 수렴하며, MAPPO 및 IPPO는 동일한 아키텍처에서 수렴하지 못하였다.

Abstract: Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Learning to Remember: End-to-End Training of Memory Agents for Long-Context Reasoning](https://arxiv.org/abs/2602.18493)
*Kehao Zhang,Shangtong Gui,Sheng Yang,Wei Chen,Yang Feng*

Main category: cs.LG

TL;DR: Unified Memory Agent (UMA)는 메모리 작업과 질문 응답을 통합하는 강화 학습 프레임워크로, 초장기 메모리 관리를 효과적으로 수행하여 역동적인 추론 및 학습 작업에서 우수한 성능을 발휘한다.


<details>
  <summary>Details</summary>
Motivation: 장기 컨텍스트 LLM 및 RAG 시스템이 정보 처리를 수동적으로 수행하여 초장기 스트림에서의 상태 추적 및 모순 해결이 취약한 문제를 해결하고자 한다.

Method: UMA는 글로벌 컨텍스트를 위한 간결한 핵심 요약과 CRUD(생성, 업데이트, 삭제, 재조직)를 명시적으로 지원하는 구조화된 메모리 뱅크를 유지하여 스트리밍 중 적극적인 통합을 가능하게 하는 이중 메모리 표현을 유지한다.

Result: 13개의 데이터 세트에서 UMA는 동적 추론 및 학습 작업에서 장기 컨텍스트 및 RAG 기준을 상당히 초과하는 성능을 보이며, 표준 검색 기준에서는 경쟁력 있게 남아 있다.

Conclusion: 학습된 end-to-end 메모리 관리의 중요성을 강조한다.

Abstract: Long-context LLMs and Retrieval-Augmented Generation (RAG) systems process information passively, deferring state tracking, contradiction resolution, and evidence aggregation to query time, which becomes brittle under ultra long streams with frequent updates. We propose the Unified Memory Agent (UMA), an end-to-end reinforcement learning framework that unifies memory operations and question answering within a single policy. UMA maintains a dual memory representation: a compact core summary for global context and a structured Memory Bank that supports explicit CRUD (create, update, delete, reorganize) over key value entries, enabling proactive consolidation during streaming. To evaluate long-horizon memory behavior, we introduce Ledger-QA, a diagnostic benchmark for continuous state tracking where answers are latent values derived from accumulated updates rather than lo cal span retrieval. Across 13 datasets spanning Ledger-QA, Test-Time Learning, and Accurate Retrieval, UMA substantially outperforms long-context and RAG baselines on dynamic reasoning and learning tasks while remaining competitive on standard retrieval benchmarks, underscoring the importance of learned, end-to-end memory management.

</details>


### [48] [The Geometry of Multi-Task Grokking: Transverse Instability, Superposition, and Weight Decay Phase Structure](https://arxiv.org/abs/2602.18523)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 이 연구는 다중 작업 모듈러 산술에 대한 기하학적 분석을 확장하며, 다양한 weight decay 설정에서 두 가지 및 세 가지 작업 목표에 대한 Transformer 모델을 훈련하여 grokking의 다섯 가지 현상을 발견했습니다.


<details>
  <summary>Details</summary>
Motivation: Grokking 현상은 단일 작업 설정에서 주로 연구되었지만, 다중 작업의 경우도 탐구할 필요가 있습니다.

Method: 다중 작업 모듈러 산술에 대한 기하학적 분석을 확장하고, dual-task 및 tri-task 목표에 대한 Transformers를 체계적인 weight decay sweep를 통해 훈련했습니다.

Result: 다섯 가지 일관된 현상이 발견되었습니다: staggered grokking order, universal integrability, weight decay phase structure, holographic incompressibility, 및 transverse fragility and redundancy.

Conclusion: 이 결과들은 다중 작업 grokking이 매개변수 공간에서 집합적인 중첩 서브스페이스를 구축하며, weight decay가 압축 압력으로 작용하고 과잉 매개변수가 최적화 경로에 기하학적 중복성을 제공하는 동적 모델을 지지합니다.

Abstract: Grokking -- the abrupt transition from memorization to generalization long after near-zero training loss -- has been studied mainly in single-task settings. We extend geometric analysis to multi-task modular arithmetic, training shared-trunk Transformers on dual-task (mod-add + mod-mul) and tri-task (mod-add + mod-mul + mod-sq) objectives across a systematic weight decay sweep. Five consistent phenomena emerge. (1) Staggered grokking order: multiplication generalizes first, followed by squaring, then addition, with consistent delays across seeds. (2) Universal integrability: optimization trajectories remain confined to an empirically invariant low-dimensional execution manifold; commutator defects orthogonal to this manifold reliably precede generalization. (3) Weight decay phase structure: grokking timescale, curvature depth, reconstruction threshold, and defect lead covary systematically with weight decay, revealing distinct dynamical regimes and a sharp no-decay failure mode. (4) Holographic incompressibility: final solutions occupy only 4--8 principal trajectory directions yet are distributed across full-rank weights and destroyed by minimal perturbations; SVD truncation, magnitude pruning, and uniform scaling all fail to preserve performance. (5) Transverse fragility and redundancy: removing less than 10% of orthogonal gradient components eliminates grokking, yet dual-task models exhibit partial recovery under extreme deletion, suggesting redundant center manifolds enabled by overparameterization. Together, these results support a dynamical picture in which multi-task grokking constructs a compact superposition subspace in parameter space, with weight decay acting as compression pressure and excess parameters supplying geometric redundancy in optimization pathways.

</details>


### [49] [MapTab: Can MLLMs Master Constrained Route Planning?](https://arxiv.org/abs/2602.18600)
*Ziqiao Shang,Lingyue Ge,Yang Chen,Shi-Yu Tian,Zhenyu Huang,Wenbo Fu,Yu-Feng Li,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: Multimodal 대형 언어 모델(MLLM)의 체계적 평가는 인공지능 일반화(AGI) 발전에 중요하다. 기존의 벤치마크는 제한된 추론 능력을 평가하는 데 불충분하다. 이를 해결하기 위해 MapTab을 도입하며, 이는 경로 계획 작업을 통한 MLLM의 제한된 추론을 평가하기 위해 설계된 다중모달 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: AGI 발전을 위한 MLLM의 체계적 평가 필요성.

Method: MapTab은 MLLM이 지도 이미지에서 시각적 단서를 인지하고, 구조화된 표 데이터에서 경로 속성(예: 시간, 가격)을 기반으로 작업하는 경로 계획을 통해 평가한다.

Result: 현재 모델들은 제한된 다중모달 추론에서 상당한 도전에 직면하고 있으며, 제한된 시각적 인식 조건에서 다중모달 협업이 단일 모달 접근법에 비해 저조한 성능을 보인다.

Conclusion: MapTab은 MLLM의 체계적 평가를 위한 도전적이고 실질적인 시험대가 될 것으로 믿는다.

Abstract: Systematic evaluation of Multimodal Large Language Models (MLLMs) is crucial for advancing Artificial General Intelligence (AGI). However, existing benchmarks remain insufficient for rigorously assessing their constrained reasoning capabilities. To bridge this gap, we introduce MapTab, a multimodal benchmark specifically designed to evaluate constrained reasoning in MLLMs via route planning tasks. MapTab requires MLLMs to perceive and ground visual cues from map images alongside route attributes (e.g., Time, Price) from structured tabular data. The benchmark encompasses two scenarios: Metromap, covering metro networks in 160 cities across 52 countries, and Travelmap, depicting 168 representative tourist attractions from 19 countries. In total, MapTab comprises 328 images, 196,800 route planning queries, and 3,936 QA queries, all incorporating 4 key constraints: Time, Price, Comfort, and Reliability. Extensive evaluations across 15 representative MLLMs reveal that current models face substantial challenges in constrained multimodal reasoning. Notably, under conditions of limited visual perception, multimodal collaboration often underperforms compared to unimodal approaches. We believe MapTab provides a challenging and realistic testbed to advance the systematic evaluation of MLLMs.

</details>


### [50] [Learning Invariant Visual Representations for Planning with Joint-Embedding Predictive World Models](https://arxiv.org/abs/2602.18639)
*Leonardo F. Toso,Davit Shadunts,Yunyang Lu,Nihal Sharma,Donglin Zhan,Nam H. Nguyen,James Anderson*

Main category: cs.LG

TL;DR: 본 연구는 DINO 세계 모델(DINO-WM)의 약점을 극복하기 위해 느린 특징에 대한 민감도를 줄이고, 예측 목표를 보강하는 새로운 구조를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 고차원 시각 관찰에서 학습된 세계 모델은 에이전트가 픽셀 수준의 재구성을 피하고 잠재 공간에서 직접 의사결정 및 계획을 할 수 있도록 한다. 그러나 DINO-WM을 포함한 최근의 잠재 예측 구조는 느린 특징에 대한 민감성 때문에 테스트 타임 강 robustness가 저하된다는 한계가 있다.

Method: 우리는 제어 관련 상태 동등성을 enforced하는 bisimulation encoder로 예측 목표를 보강하여 느린 특징의 기여를 제한하고 유사한 전이 역학을 가진 상태를 근접한 잠재 상태로 매핑한다.

Result: 우리의 모델은 다양한 테스트 타임 배경 변화 및 시각 방해물 하에서 단순 탐색 작업에 대해 평가되었으며, 모든 벤치마크에서 느린 특징에 대한 강 robustness를 향상시켰고, DINO-WM의 잠재 공간보다 최대 10배 작은 축소된 잠재 공간에서 작동한다.

Conclusion: 추가로, 우리의 모델은 사전 훈련된 시각 인코더 선택에 무관하며 DINOv2, SimDINOv2, 및 iBOT 특징과 결합하여도 강 robustness를 유지한다.

Abstract: World models learned from high-dimensional visual observations allow agents to make decisions and plan directly in latent space, avoiding pixel-level reconstruction. However, recent latent predictive architectures (JEPAs), including the DINO world model (DINO-WM), display a degradation in test time robustness due to their sensitivity to "slow features". These include visual variations such as background changes and distractors that are irrelevant to the task being solved. We address this limitation by augmenting the predictive objective with a bisimulation encoder that enforces control-relevant state equivalence, mapping states with similar transition dynamics to nearby latent states while limiting contributions from slow features. We evaluate our model on a simple navigation task under different test-time background changes and visual distractors. Across all benchmarks, our model consistently improves robustness to slow features while operating in a reduced latent space, up to 10x smaller than that of DINO-WM. Moreover, our model is agnostic to the choice of pretrained visual encoder and maintains robustness when paired with DINOv2, SimDINOv2, and iBOT features.

</details>


### [51] [Phase-Consistent Magnetic Spectral Learning for Multi-View Clustering](https://arxiv.org/abs/2602.18728)
*Mingdong Lu,Zhikui Chen,Meng Liu,Shubin Ma,Liang Zhao*

Main category: cs.LG

TL;DR: 본 논문에서는 비지도 다중 뷰 클러스터링을 위한 새로운 접근법인 위상 일관성 자기 스펙트럴 학습을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비지도 다중 뷰 클러스터링에서 라벨 없이 여러 뷰의 보완 정보를 활용하여 데이터의 의미 있는 그룹을 나누는 것이 목표지만, 뷰 불일치와 노이즈 하에서 표현 학습과 크로스 뷰 정렬을 안내할 신뢰할 수 있는 공유 구조 신호를 얻는 것이 핵심적인 도전 과제이다.

Method: 우리는 크로스 뷰 방향적 합의를 위상 항으로 명시적으로 모델링하고 비부정적 크기 백본과 결합하여 복소수 값 자기 친화성을 형성하며, 헤르미트 자기 라플라시안을 통해 안정적인 공유 스펙트럴 신호를 추출하고 이를 구조화된 자기 감독으로 사용하여 비지도 다중 뷰 표현 학습 및 클러스터링을 안내한다.

Result: 여러 공공 다중 뷰 벤치마크에서의 광범위한 실험 결과, 우리의 방법이 강력한 기준선들을 일관되게 능가함을 확인하였다.

Conclusion: 본 연구는 비지도 다중 뷰 클러스터링을 위한 안정적이고 효과적인 방법을 제시한다.

Abstract: Unsupervised multi-view clustering (MVC) aims to partition data into meaningful groups by leveraging complementary information from multiple views without labels, yet a central challenge is to obtain a reliable shared structural signal to guide representation learning and cross-view alignment under view discrepancy and noise. Existing approaches often rely on magnitude-only affinities or early pseudo targets, which can be unstable when different views induce relations with comparable strengths but contradictory directional tendencies, thereby distorting the global spectral geometry and degrading clustering. In this paper, we propose \emph{Phase-Consistent Magnetic Spectral Learning} for MVC: we explicitly model cross-view directional agreement as a phase term and combine it with a nonnegative magnitude backbone to form a complex-valued magnetic affinity, extract a stable shared spectral signal via a Hermitian magnetic Laplacian, and use it as structured self-supervision to guide unsupervised multi-view representation learning and clustering. To obtain robust inputs for spectral extraction at scale, we construct a compact shared structure with anchor-based high-order consensus modeling and apply a lightweight refinement to suppress noisy or inconsistent relations. Extensive experiments on multiple public multi-view benchmarks demonstrate that our method consistently outperforms strong baselines.

</details>


### [52] [HONEST-CAV: Hierarchical Optimization of Network Signals and Trajectories for Connected and Automated Vehicles with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.18740)
*Ziyan Zhang,Changxin Wan,Peng Hao,Kanok Boriboonsomsin,Matthew J. Barth,Yongkang Liu,Seyhan Ucar,Guoyuan Wu*

Main category: cs.LG

TL;DR: 이 연구는 인간 주도 차량(HVs)과 연결되고 자동화된 차량(CAVs)으로 구성된 혼합 교통을 위한 계층적 네트워크 수준의 교통 흐름 제어 프레임워크를 제시한다. 이 프레임워크는 차량 수준의 친환경 운전 행동과 교차로 수준의 신호 제어를 공동으로 최적화하여 전체 네트워크 효율성을 향상시키고 에너지 소비를 감소시킨다.


<details>
  <summary>Details</summary>
Motivation: 혼합 교통 환경에서의 효율성과 에너지 소비를 개선할 필요성이 대두되고 있다.

Method: 분산형 다중 에이전트 강화 학습(MARL) 접근 방식과 가치 분해 네트워크(VDN)를 통해 Cycle 기반 교통 신호 제어(TSC)를 관리하고, 기계 학습 기반의 Trajectory Planning Algorithm(MLTPA)을 통합하여 CAV의 Eco-Approach and Departure(EAD) 동작을 유도한다.

Result: 실험 결과 MARL 기반 TSC 방법이 속도, 연료 소비, 대기 시간에서 기본 모델인 Webster 방법을 초월하는 것으로 나타났다. 60% CAV 비율에서 평균 속도는 7.67%, 연료 소비는 10.23%, 대기 시간은 45.83% 개선되었다.

Conclusion: 제안된 방법의 성능은 자동화 및 전기화의 영향을 통해 정량화되었다.

Abstract: This study presents a hierarchical, network-level traffic flow control framework for mixed traffic consisting of Human-driven Vehicles (HVs), Connected and Automated Vehicles (CAVs). The framework jointly optimizes vehicle-level eco-driving behaviors and intersection-level traffic signal control to enhance overall network efficiency and decrease energy consumption. A decentralized Multi-Agent Reinforcement Learning (MARL) approach by Value Decomposition Network (VDN) manages cycle-based traffic signal control (TSC) at intersections, while an innovative Signal Phase and Timing (SPaT) prediction method integrates a Machine Learning-based Trajectory Planning Algorithm (MLTPA) to guide CAVs in executing Eco-Approach and Departure (EAD) maneuvers. The framework is evaluated across varying CAV proportions and powertrain types to assess its effects on mobility and energy performance. Experimental results conducted in a 4*4 real-world network demonstrate that the MARL-based TSC method outperforms the baseline model (i.e., Webster method) in speed, fuel consumption, and idling time. In addition, with MLTPA, HONEST-CAV benefits the traffic system further in energy consumption and idling time. With a 60% CAV proportion, vehicle average speed, fuel consumption, and idling time can be improved/saved by 7.67%, 10.23%, and 45.83% compared with the baseline. Furthermore, discussions on CAV proportions and powertrain types are conducted to quantify the performance of the proposed method with the impact of automation and electrification.

</details>


### [53] [Issues with Measuring Task Complexity via Random Policies in Robotic Tasks](https://arxiv.org/abs/2602.18856)
*Reabetswe M. Nkhumise,Mohamed S. Talamali,Aditya Gilra*

Main category: cs.LG

TL;DR: 강화 학습(RL)은 로봇 공학 및 자연어 처리와 같은 분야에서 주요 발전을 가능하게 했다. RL에서의 주요 도전 과제는 작업 복잡성을 측정하는 것이며, 이는 의미 있는 벤치마크를 만들고 효과적인 커리큘럼을 설계하는 데 필수적이다.


<details>
  <summary>Details</summary>
Motivation: 비표 형 세트에서의 작업 복잡성을 평가할 잘 정립된 지표가 부족하다.

Method: 다양한 로봇 조작 설정을 통해 RWG, PIC, POIC 방법을 평가하였다.

Result: PIC와 POIC가 강화 학습과 경험적 결과와 반대되는 결과를 나타낸다.

Conclusion: 보다 신뢰할 수 있는 작업 복잡성 측정 지표의 필요성을 강조한다.

Abstract: Reinforcement learning (RL) has enabled major advances in fields such as robotics and natural language processing. A key challenge in RL is measuring task complexity, which is essential for creating meaningful benchmarks and designing effective curricula. While there are numerous well-established metrics for assessing task complexity in tabular settings, relatively few exist in non-tabular domains. These include (i) Statistical analysis of the performance of random policies via Random Weight Guessing (RWG), and (ii) information-theoretic metrics Policy Information Capacity (PIC) and Policy-Optimal Information Capacity (POIC), which are reliant on RWG. In this paper, we evaluate these methods using progressively difficult robotic manipulation setups, with known relative complexity, with both dense and sparse reward formulations. Our empirical results reveal that measuring complexity is still nuanced. Specifically, under the same reward formulation, PIC suggests that a two-link robotic arm setup is easier than a single-link setup - which contradicts the robotic control and empirical RL perspective whereby the two-link setup is inherently more complex. Likewise, for the same setup, POIC estimates that tasks with sparse rewards are easier than those with dense rewards. Thus, we show that both PIC and POIC contradict typical understanding and empirical results from RL. These findings highlight the need to move beyond RWG-based metrics towards better metrics that can more reliably capture task complexity in non-tabular RL with our task framework as a starting point.

</details>


### [54] [VariBASed: Variational Bayes-Adaptive Sequential Monte-Carlo Planning for Deep Reinforcement Learning](https://arxiv.org/abs/2602.18857)
*Joery A. de Vries,Jinke He,Yaniv Oren,Pascal R. van der Vaart,Mathijs M. de Weerdt,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: 강화 학습에서 탐색과 활용의 최적화는 데이터 효율성을 극대화할 수 있는 방법이다. 본 연구에서는 Bayes-adaptive Markov 결정 프로세스에서 변이학적 프레임워크를 제안하여 샘플 및 실행 효율성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 탐색과 활용의 균형을 맞추는 것은 작업을 해결하기 위한 데이터 효율성을 극대화하는 데 중요한 목표이다.

Method: 이 논문은 변이학적 신념 학습, 순차 몬테카를로 계획, 메타 강화 학습을 통합한 베이즈 적응 마르코프 결정 프로세스에서 학습과 계획을 위한 변이학적 프레임워크를 제안한다.

Result: 새로운 방법인 VariBASeD는 단일 GPU 설정에서 계획 예산이 커질수록 좋은 확장성을 보이며 이전 방법에 비해 샘플 및 실행 효율성을 개선한다.

Conclusion: 본 연구 결과, 제안된 방법은 기존의 적합한 방법들보다 더욱 효과적인 성과를 보인다.

Abstract: Optimally trading-off exploration and exploitation is the holy grail of reinforcement learning as it promises maximal data-efficiency for solving any task. Bayes-optimal agents achieve this, but obtaining the belief-state and performing planning are both typically intractable. Although deep learning methods can greatly help in scaling this computation, existing methods are still costly to train. To accelerate this, this paper proposes a variational framework for learning and planning in Bayes-adaptive Markov decision processes that coalesces variational belief learning, sequential Monte-Carlo planning, and meta-reinforcement learning. In a single-GPU setup, our new method VariBASeD exhibits favorable scaling to larger planning budgets, improving sample- and runtime-efficiency over prior methods.

</details>


### [55] [Exponential Convergence of (Stochastic) Gradient Descent for Separable Logistic Regression](https://arxiv.org/abs/2602.18946)
*Sacchit Kale,Piyushi Manupriya,Pierre Marion,Francis bach,Anant Raj*

Main category: cs.LG

TL;DR: 본 논문에서는 경량 적응형 단계 크기 규칙을 사용하여 기하급수적 수렴을 달성하는 경량 스텝 크기 스케줄을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 기하급수적 수렴은 불안정한 최적화 영역에서 가능하다는 주장에 대한 반박으로, 불안정성이 가속화의 본질이 아님을 보여주는 것이 주된 동기이다.

Method: 비적응적 증가 단계 크기 스케줄을 사용하여 분리 가능한 로지스틱 회귀에 대해 기하급수적 수렴을 증명하고, 경량 적응형 단계 크기 규칙을 통해 확률적 경량 하강법의 기하급수적 수렴을 제시한다.

Result: 제안된 방법은 안정적 최적화 영역 내에서 기하급수적으로 수렴하며, 사전 지식이 필요하지 않다.

Conclusion: 단계 크기 성장의 세부 구조만으로도 경량 하강법과 확률적 경량 하강법 모두의 기하급수적 가속화를 얻을 수 있음을 입증했다.

Abstract: Gradient descent and stochastic gradient descent are central to modern machine learning, yet their behavior under large step sizes remains theoretically unclear. Recent work suggests that acceleration often arises near the edge of stability, where optimization trajectories become unstable and difficult to analyze. Existing results for separable logistic regression achieve faster convergence by explicitly leveraging such unstable regimes through constant or adaptive large step sizes. In this paper, we show that instability is not inherent to acceleration. We prove that gradient descent with a simple, non-adaptive increasing step-size schedule achieves exponential convergence for separable logistic regression under a margin condition, while remaining entirely within a stable optimization regime. The resulting method is anytime and does not require prior knowledge of the optimization horizon or target accuracy. We also establish exponential convergence of stochastic gradient descent using a lightweight adaptive step-size rule that avoids line search and specialized procedures, improving upon existing polynomial-rate guarantees. Together, our results demonstrate that carefully structured step-size growth alone suffices to obtain exponential acceleration for both gradient descent and stochastic gradient descent.

</details>


### [56] [Incremental Transformer Neural Processes](https://arxiv.org/abs/2602.18955)
*Philip Mortimer,Cristiana Diaconu,Tommy Rochussen,Bruno Mlodozeniec,Richard E. Turner*

Main category: cs.LG

TL;DR: Incremental TNP (incTNP)는 기존 Transformer Neural Processes(TNP)의 업데이트 비용을 줄이며, 흐름 데이터에서의 예측 성능을 유지하는 새로운 모델입니다.


<details>
  <summary>Details</summary>
Motivation: 연속적인 데이터 스트림과 같은 시나리오에서 기존 TNP 변형들은 매번 내부 표현을 처음부터 다시 계산하는 대신 저비용의 점진적 업데이트를 지원해야 합니다.

Method: incTNP는 인과 마스킹, Key-Value(KV) 캐싱 및 데이터 효율적인 자기 회귀 훈련 전략을 활용하여 표준 TNP와 동등한 예측 성능을 가지도록 설계되었습니다.

Result: incTNP는 비인과 TNP와 비교할 때 동등하거나 더 나은 성능을 제공하며, 순차적 추론을 위한 속도 개선을 이끌어냅니다.

Conclusion: incTNP는 인과 마스킹의 계산적 이점을 제공하면서도 스트리밍 추론에 필요한 일관성을 유지하는 데 성공하였습니다.

Abstract: Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference.

</details>


### [57] [Detecting labeling bias using influence functions](https://arxiv.org/abs/2602.19130)
*Frida Jørgensen,Nina Weng,Siavash Bigdeli*

Main category: cs.LG

TL;DR: 이 논문에서는 영향 함수가 레이블링 편향을 감지하는 데 사용할 수 있는지를 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 레이블링 편향은 자원 제한 또는 무의식적인 편향으로 인해 데이터 수집 중 발생하며, 이는 하위 그룹 간의 불균등한 레이블 오류율 또는 하위 그룹의 유병률 잘못된 표현을 초래할 수 있습니다.

Method: 영향 함수는 손실 함수의 기울기와 헤세안(Hessian)을 활용하여 각 훈련 샘플이 모델 예측에 얼마나 영향을 미치는지를 추정합니다.

Result: MNIST 데이터셋에서 레이블 오류를 검사하기 위해 20%의 레이블을 하나의 클래스에서 뒤집어 제어된 오류를 도입했습니다. 우리는 대각선 헤세안 근사를 사용하여 MNIST에서 거의 90%의 잘못 레이블된 샘플을 성공적으로 탐지했습니다.

Conclusion: 영향 함수는 레이블 오류를 식별하는 데 잠재력을 보여줍니다.

Abstract: Labeling bias arises during data collection due to resource limitations or unconscious bias, leading to unequal label error rates across subgroups or misrepresentation of subgroup prevalence. Most fairness constraints assume training labels reflect the true distribution, rendering them ineffective when labeling bias is present; leaving a challenging question, that \textit{how can we detect such labeling bias?} In this work, we investigate whether influence functions can be used to detect labeling bias. Influence functions estimate how much each training sample affects a model's predictions by leveraging the gradient and Hessian of the loss function -- when labeling errors occur, influence functions can identify wrongly labeled samples in the training set, revealing the underlying failure mode. We develop a sample valuation pipeline and test it first on the MNIST dataset, then scaled to the more complex CheXpert medical imaging dataset. To examine label noise, we introduced controlled errors by flipping 20\% of the labels for one class in the dataset. Using a diagonal Hessian approximation, we demonstrated promising results, successfully detecting nearly 90\% of mislabeled samples in MNIST. On CheXpert, mislabeled samples consistently exhibit higher influence scores. These results highlight the potential of influence functions for identifying label errors.

</details>


### [58] [Taming Preconditioner Drift: Unlocking the Potential of Second-Order Optimizers for Federated Learning on Non-IID Data](https://arxiv.org/abs/2602.19271)
*Junkang Liu,Fanhua Shang,Hongying Liu,Jin Liu,Weixin An,Yuanyuan Liu*

Main category: cs.LG

TL;DR: 본 논문에서 제안하는 FedPAC은 비IID 데이터에서의 연합 학습을 안정화하는 두번째 최적화 방법의 새로운 접근 방식을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 학습에서 두번째 최적화 방법이 속도를 크게 높일 수 있으나, 비IID 데이터에서 그 변형이 불안정하거나 발산하는 문제가 있다.

Method: FedPAC은 매개변수 집합을 기하학적 동기화와 분리하여, 지역 조건 변수를 글로벌 참조로 집계하고 글로벌 조건 변수를 통해 클라이언트를 초기화하며, 글로벌 방향을 사용하여 지역 업데이트의 장기적 드리프트를 억제하는 두 가지 주요 단계(정렬 및 수정)로 이루어진다.

Result: FedPAC은 부분 참여 하에서도 선형적인 속도 향상과 드리프트 기반의 비볼록 수렴 보장을 제공하며, 비전 및 언어 작업에서 일관되게 안정성과 정확성을 향상시킨다.

Conclusion: 실험적으로 FedPAC은 CIFAR-100에서 ViTs를 사용하여 최대 5.8%의 절대 정확도 상승을 달성했다.

Abstract: Second-order optimizers can significantly accelerate large-scale training, yet their naive federated variants are often unstable or even diverge on non-IID data.
  We show that a key culprit is \emph{preconditioner drift}: client-side second-order training induces heterogeneous \emph{curvature-defined geometries} (i.e., preconditioner coordinate systems), and server-side model averaging updates computed under incompatible metrics, corrupting the global descent direction.
  To address this geometric mismatch, we propose \texttt{FedPAC}, a \emph{preconditioner alignment and correction} framework for reliable federated second-order optimization.
  \texttt{FedPAC} explicitly decouples parameter aggregation from geometry synchronization by:
  (i) \textbf{Alignment} (i.e.,aggregating local preconditioners into a global reference and warm-starting clients via global preconditioner); and
  (ii) \textbf{Correction} (i.e., steering local preconditioned updates using a global preconditioned direction to suppress long-term drift).
  We provide drift-coupled non-convex convergence guarantees with linear speedup under partial participation.
  Empirically, \texttt{FedPAC} consistently improves stability and accuracy across vision and language tasks, achieving up to $5.8\%$ absolute accuracy gain on CIFAR-100 with ViTs.
  Code is available at https://anonymous.4open.science/r/FedPAC-8B24.

</details>


### [59] [Active perception and disentangled representations allow continual, episodic zero and few-shot learning](https://arxiv.org/abs/2602.19355)
*David Rawlinson,Gideon Kowadlo*

Main category: cs.LG

TL;DR: 이 논문은 기계 학습 시스템에서 일반화 대신 빠른 학습을 통해 지속 가능한 제로샷 및 소샷 학습을 가능하게 하는 보완 학습 시스템(CLS)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 일반화는 기계 학습 시스템의 중요한 속성으로 여겨지지만, 모든 구성 요소가 일반화할 필요는 없다는 점을 제안한다.

Method: 보완 학습 시스템(CLS)을 통해 빠른 학습자가 일반화를 포기하고 지속 가능한 제로샷 및 소샷 학습을 수행한다.

Result: 빠른 학습자는 전통적인 느린 통계적 학습자를 활용하여 관찰 변동성과 불확실성을 극복하고, 제로샷 및 소샷 학습을 가능하게 한다.

Conclusion: 이 아키텍처는 빠른 맥락 주도 추론과 느린 구조적 일반화가 공존할 수 있음을 보여주며, 이는 강력한 지속적 학습을 위한 경로를 제공한다.

Abstract: Generalization is often regarded as an essential property of machine learning systems. However, perhaps not every component of a system needs to generalize. Training models for generalization typically produces entangled representations at the boundaries of entities or classes, which can lead to destructive interference when rapid, high-magnitude updates are required for continual or few-shot learning. Techniques for fast learning with non-interfering representations exist, but they generally fail to generalize. Here, we describe a Complementary Learning System (CLS) in which the fast learner entirely foregoes generalization in exchange for continual zero-shot and few-shot learning. Unlike most CLS approaches, which use episodic memory primarily for replay and consolidation, our fast, disentangled learner operates as a parallel reasoning system. The fast learner can overcome observation variability and uncertainty by leveraging a conventional slow, statistical learner within an active perception system: A contextual bias provided by the fast learner induces the slow learner to encode novel stimuli in familiar, generalized terms, enabling zero-shot and few-shot learning. This architecture demonstrates that fast, context-driven reasoning can coexist with slow, structured generalization, providing a pathway for robust continual learning.

</details>


### [60] [Stable Deep Reinforcement Learning via Isotropic Gaussian Representations](https://arxiv.org/abs/2602.19373)
*Ali Saheb,Johan Obando-Ceron,Aaron Courville,Pouya Bashivan,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 본 논문은 비정상적 목표 하에서 등방성 가우시안 임베딩이 안정적인 학습을 유도하며, 비정상성 하에서도 성능을 향상시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 딥 강화 학습 시스템은 학습 목표와 데이터 분포의 변화로 인해 불안정한 훈련 역학에 시달린다.

Method: Sketched Isotropic Gaussian Regularization을 사용하여 훈련 중 임베딩을 등방성 가우시안 분포로 형성한다.

Result: 이 방법은 비정상성 하에서도 성능을 향상시키며, 표현 붕괴, 뉴런 휴면 및 훈련 불안정을 줄인다.

Conclusion: 이 간단하고 계산 비용이 적은 방법은 다양한 도메인에서 효과를 입증하였다.

Abstract: Deep reinforcement learning systems often suffer from unstable training dynamics due to non-stationarity, where learning objectives and data distributions evolve over time. We show that under non-stationary targets, isotropic Gaussian embeddings are provably advantageous. In particular, they induce stable tracking of time-varying targets for linear readouts, achieve maximal entropy under a fixed variance budget, and encourage a balanced use of all representational dimensions--all of which enable agents to be more adaptive and stable. Building on this insight, we propose the use of Sketched Isotropic Gaussian Regularization for shaping representations toward an isotropic Gaussian distribution during training. We demonstrate empirically, over a variety of domains, that this simple and computationally inexpensive method improves performance under non-stationarity while reducing representation collapse, neuron dormancy, and training instability.

</details>


### [61] [LEVDA: Latent Ensemble Variational Data Assimilation via Differentiable Dynamics](https://arxiv.org/abs/2602.19406)
*Phillip Si,Peng Chen*

Main category: cs.LG

TL;DR: LEVDA는 저차원 잠재 공간에서 작동하는 앙상블 공간 변동 스무더로, 고도의 불규칙한 샘플링을 수용하며 관측의 희소성 하에서도 높은 정확도와 효율성을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 장거리 지구 물리학적 예측은 혼란 역학과 수치적 오류에 의해 본질적으로 제한된다. 데이터 동화는 이러한 문제를 완화할 수 있지만, 고전적인 변동 스무더는 계산적으로 비용이 많이 든다.

Method: LEVDA는 사전 훈련된 미분 가능한 신경 역학 대리 모델의 저차원 잠재 공간에서 작동하는 앙상블 공간 변동 스무더이다. 앙상블 서브스페이스 내에서 4차원 앙상블 변동 최적화(4DEnVar)를 수행함으로써, LEVDA는 보조 코드나 보조 관측-잠재 인코더 없이 상태와 미지의 매개변수를 동시에 동화한다.

Result: 세 가지 도전적인 지구 물리학적 벤치마크에서 LEVDA는 관측의 희소성이 심한 조건에서도 최첨단 잠재 필터링 기준과 일치하거나 이를 초월하며, 더 신뢰할 수 있는 불확실성 정량화를 제공한다.

Conclusion: LEVDA는 전체 상태 4DEnVar에 비해 현저하게 향상된 동화 정확도와 계산 효율성을 달성한다.

Abstract: Long-range geophysical forecasts are fundamentally limited by chaotic dynamics and numerical errors. While data assimilation can mitigate these issues, classical variational smoothers require computationally expensive tangent-linear and adjoint models. Conversely, recent efficient latent filtering methods often enforce weak trajectory-level constraints and assume fixed observation grids. To bridge this gap, we propose Latent Ensemble Variational Data Assimilation (LEVDA), an ensemble-space variational smoother that operates in the low-dimensional latent space of a pretrained differentiable neural dynamics surrogate. By performing four-dimensional ensemble-variational (4DEnVar) optimization within an ensemble subspace, LEVDA jointly assimilates states and unknown parameters without the need for adjoint code or auxiliary observation-to-latent encoders. Leveraging the fully differentiable, continuous-in-time-and-space nature of the surrogate, LEVDA naturally accommodates highly irregular sampling at arbitrary spatiotemporal locations. Across three challenging geophysical benchmarks, LEVDA matches or outperforms state-of-the-art latent filtering baselines under severe observational sparsity while providing more reliable uncertainty quantification. Simultaneously, it achieves substantially improved assimilation accuracy and computational efficiency compared to full-state 4DEnVar.

</details>


### [62] [RAmmStein: Regime Adaptation in Mean-reverting Markets with Stein Thresholds -- Optimal Impulse Control in Concentrated AMMs](https://arxiv.org/abs/2602.19419)
*Pranay Anchuri*

Main category: cs.LG

TL;DR: 탈중앙화 거래소에서의 집중 유동성 제공은 기본적인 충동 조절 문제를 나타낸다. 유동성 제공자(LP)는 가격 범위 집중을 통한 수수료 수익 극대화와 재조정의 마찰 비용 최소화 간의 비트리비얼한 거래를 직면하고 있다.


<details>
  <summary>Details</summary>
Motivation: 탈중앙화 거래소에서의 유동성 관리 문제를 해결하고자 함.

Method: 유동성 관리를 최적 제어 문제로 공식화하고 HJB-QVI를 도출하며, RAmmStein이라는 딥 강화 학습 방법을 제시.

Result: RAmmStein은 0.72%의 우수한 순 ROI를 달성하고, 재조정 빈도를 67% 줄이면서도 88%의 활동 시간을 유지한다.

Conclusion: 제도 인식의 게으름이 운영 비용으로 인해 손실될 수 있는 수익을 보존하여 자본 효율성을 크게 개선할 수 있음을 보여준다.

Abstract: Concentrated liquidity provision in decentralized exchanges presents a fundamental Impulse Control problem. Liquidity Providers (LPs) face a non-trivial trade-off between maximizing fee accrual through tight price-range concentration and minimizing the friction costs of rebalancing, including gas fees and swap slippage. Existing methods typically employ heuristic or threshold strategies that fail to account for market dynamics. This paper formulates liquidity management as an optimal control problem and derives the corresponding Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI). We present an approximate solution RAmmStein, a Deep Reinforcement Learning method that incorporates the mean-reversion speed (theta) of an Ornstein-Uhlenbeck process among other features as input to the model. We demonstrate that the agent learns to separate the state space into regions of action and inaction. We evaluate the framework using high-frequency 1Hz Coinbase trade data comprising over 6.8M trades. Experimental results show that RAmmStein achieves a superior net ROI of 0.72% compared to both passive and aggressive strategies. Notably, the agent reduces rebalancing frequency by 67% compared to a greedy rebalancing strategy while maintaining 88% active time. Our results demonstrate that regime-aware laziness can significantly improve capital efficiency by preserving the returns that would otherwise be eroded by the operational costs.

</details>


### [63] [Advantage-based Temporal Attack in Reinforcement Learning](https://arxiv.org/abs/2602.19582)
*Shenghong He*

Main category: cs.LG

TL;DR: 이 논문에서는 AAT라는 새로운 방법을 제안하여 공격 성능을 향상시키기 위해 시간적으로 상관된 적대적 예제를 생성합니다.


<details>
  <summary>Details</summary>
Motivation: DRL 모델이 적대적 입력에 취약하다는 것은 광범위한 연구를 통해 입증되었습니다.

Method: AAT는 다중scale 인과 자기 주의 메커니즘을 사용하여 과거 정보와 현재 상태 간의 의존성을 동적으로 포착합니다.

Result: AAT는 아타리, 딥마인드 제어 스위트 및 구글 풋볼 과제에서 기존의 주류 적대적 공격 기준을 초과하는 성능을 보였습니다.

Conclusion: AAT는 공격 성능 향상에 기여할 수 있는 시간적으로 상관된 적대적 예제를 생성합니다.

Abstract: Extensive research demonstrates that Deep Reinforcement Learning (DRL) models are susceptible to adversarially constructed inputs (i.e., adversarial examples), which can mislead the agent to take suboptimal or unsafe actions. Recent methods improve attack effectiveness by leveraging future rewards to guide adversarial perturbation generation over sequential time steps (i.e., reward-based attacks). However, these methods are unable to capture dependencies between different time steps in the perturbation generation process, resulting in a weak temporal correlation between the current perturbation and previous perturbations.In this paper, we propose a novel method called Advantage-based Adversarial Transformer (AAT), which can generate adversarial examples with stronger temporal correlations (i.e., time-correlated adversarial examples) to improve the attack performance. AAT employs a multi-scale causal self-attention (MSCSA) mechanism to dynamically capture dependencies between historical information from different time periods and the current state, thus enhancing the correlation between the current perturbation and the previous perturbation. Moreover, AAT introduces a weighted advantage mechanism, which quantifies the effectiveness of a perturbation in a given state and guides the generation process toward high-performance adversarial examples by sampling high-advantage regions. Extensive experiments demonstrate that the performance of AAT matches or surpasses mainstream adversarial attack baselines on Atari, DeepMind Control Suite and Google football tasks.

</details>


### [64] [ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?](https://arxiv.org/abs/2602.19594)
*Ayush Nangia,Shikhar Mishra,Aman Gokrani,Paras Chopra*

Main category: cs.LG

TL;DR: ISO-Bench는 코딩 에이전트의 실제 추론 최적화 작업에 대한 능력을 테스트하는 벤치마크로, 54개의 작업이 포함되어 있으며, 하드와 소프트 메트릭을 결합하여 평가하는 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 코딩 에이전트의 실제 성능을 평가할 수 있는 포괄적인 벤치마크 필요

Method: 두 개의 인기 있는 LLM 제공 프레임워크인 vLLM과 SGLang에서 작업을 추출하고, 하드 및 소프트 메트릭을 사용하여 최적화 패치를 평가

Result: 54개의 작업에서 기존 에이전트들이 정확한 병목을 식별하지만 문제 해결에서는 실패하는 경향이 있음

Conclusion: 모델의 기초가 동일하더라도 에이전트 간 차이가 크며, 이는 스캐폴딩이 모델만큼 중요함을 시사한다.

Abstract: We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.

</details>


### [65] [Compositional Planning with Jumpy World Models](https://arxiv.org/abs/2602.19634)
*Jesse Farebrother,Matteo Pirotta,Andrea Tirinzoni,Marc G. Bellemare,Alessandro Lazaric,Ahmed Touati*

Main category: cs.LG

TL;DR: 시간적 추상화로 계획하는 능력은 지능형 의사결정의 핵심이다. 이 논문에서는 사전 훈련된 정책을 조합하여 복잡한 작업을 해결하는 에이전트를 연구하고, 이 과정에서 발생하는 오류를 줄이기 위해 다단계 동역학의 예측 모델을 학습한다.


<details>
  <summary>Details</summary>
Motivation: 사전 훈련된 정책을 조합하여 복잡한 작업을 해결하려는 필요성.

Method: 다단계 동역학을 포착하기 위해 점프성 세계 모델을 학습하고, 예측의 일관성을 높이는 새로운 목표를 설정하여 예측 정확성을 향상시킨다.

Result: 점프성 세계 모델을 활용한 조합 계획이 복잡한 조작 및 탐색 작업에서 사전 훈련된 다양한 정책에 대해 제로샷 성능을 크게 향상시킨 것을 보여준다.

Conclusion: 평균적으로 장기 작업에서 원시 행동으로 계획할 때보다 200%의 상대적 개선을 이룬다.

Abstract: The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.

</details>


### [66] [Representation Stability in a Minimal Continual Learning Agent](https://arxiv.org/abs/2602.19655)
*Vishnu Subramanian*

Main category: cs.LG

TL;DR: 본 연구는 지속적인 학습 시스템의 내부 표현 변화를 연구하며, 간단한 지속적 학습 에이전트를 통해 표현 동역학과 아키텍처 복잡성을 분리한다.


<details>
  <summary>Details</summary>
Motivation: 지속적인 학습 시스템이 재훈련이나 리셋이 불가능한 환경에서 점점 더 많이 배포되고 있다.

Method: 에이전트는 실행 간 지속적인 상태 벡터를 유지하며 새로운 텍스트 데이터가 도입될 때마다 이를 점진적으로 업데이트한다.

Result: 8회의 실행을 통한 종단적 실험 결과, 초기 가소성 상태에서 일관된 입력 하에 안정적인 표현 상태로의 전환이 나타났다.

Conclusion: 본 연구는 명시적인 정규화, 재생 또는 아키텍처 복잡성 없이도 최소한의 상태 기반 학습 시스템에서 의미 있는 안정성과 가소성의 균형이 나타날 수 있음을 보여준다.

Abstract: Continual learning systems are increasingly deployed in environments where retraining or reset is infeasible, yet many approaches emphasize task performance rather than the evolution of internal representations over time. In this work, we study a minimal continual learning agent designed to isolate representational dynamics from architectural complexity and optimization objectives. The agent maintains a persistent state vector across executions and incrementally updates it as new textual data is introduced. We quantify representational change using cosine similarity between successive normalized state vectors and define a stability metric over time intervals. Longitudinal experiments across eight executions reveal a transition from an initial plastic regime to a stable representational regime under consistent input. A deliberately introduced semantic perturbation produces a bounded decrease in similarity, followed by recovery and restabilization under subsequent coherent input. These results demonstrate that meaningful stability plasticity tradeoffs can emerge in a minimal, stateful learning system without explicit regularization, replay, or architectural complexity. The work establishes a transparent empirical baseline for studying representational accumulation and adaptation in continual learning systems.

</details>


### [67] [Addressing Instrument-Outcome Confounding in Mendelian Randomization through Representation Learning](https://arxiv.org/abs/2602.19782)
*Shimeng Huang,Matthew Robinson,Francesco Locatello*

Main category: cs.LG

TL;DR: 이 연구에서는 멘델리안 랜덤화(MR)의 가정 위반 문제를 해결하기 위해 다중 환경 데이터를 활용한 표현 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 멘델리안 랜덤화의 핵심 가정이 종종 인구 집단의 층화 또는 선호 결혼으로 인해 위반되기 때문에 causal effect 추정 시 이를 해결할 필요가 있다.

Method: 우리는 다중 환경 데이터의 가용성을 활용하여 교차 환경 불변성을 활용하는 표현 학습 프레임워크를 제안한다.

Result: 다양한 혼합 메커니즘 하에서 이러한 잠재적 도구를 식별할 수 있는 이론적 보장을 제공하고, All of Us Research Hub의 데이터를 이용한 시뮬레이션 및 반합성 실험을 통해 접근법의 효과를 입증한다.

Conclusion: 제안된 방법은 멘델리안 랜덤화의 가정을 위반하는 상황에서도 유용한 잠재적 도구를 회복할 수 있는 가능성을 보여준다.

Abstract: Mendelian Randomization (MR) is a prominent observational epidemiological research method designed to address unobserved confounding when estimating causal effects. However, core assumptions -- particularly the independence between instruments and unobserved confounders -- are often violated due to population stratification or assortative mating. Leveraging the increasing availability of multi-environment data, we propose a representation learning framework that exploits cross-environment invariance to recover latent exogenous components of genetic instruments. We provide theoretical guarantees for identifying these latent instruments under various mixing mechanisms and demonstrate the effectiveness of our approach through simulations and semi-synthetic experiments using data from the All of Us Research Hub.

</details>


### [68] [Unsupervised Anomaly Detection in NSL-KDD Using $β$-VAE: A Latent Space and Reconstruction Error Approach](https://arxiv.org/abs/2602.19785)
*Dylan Baptiste,Ramla Saddem,Alexandre Philippot,François Foyer*

Main category: cs.LG

TL;DR: 본 논문은 NSL-KDD 데이터셋을 사용하여 네트워크 트래픽의 비지도 이상 탐지에 대한 접근법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 운영 기술이 정보 기술과 점점 통합됨에 따라 침입 탐지 시스템의 필요성이 중요해지고 있다.

Method: 비지도 접근법으로, 테스트 샘플과 훈련 데이터 투영 간의 거리 측정을 통해 잠재 공간 구조를 활용하고, 전통적인 이상 탐지 지표로서 재구성 오류를 사용한다.

Result: 이러한 접근법을 비교함으로써 각각의 장점과 한계를 제공한다.

Conclusion: 실험 결과는 분류 작업에 대한 잠재 공간 활용의 효과를 강조한다.

Abstract: As Operational Technology increasingly integrates with Information Technology, the need for Intrusion Detection Systems becomes more important. This paper explores an unsupervised approach to anomaly detection in network traffic using $β$-Variational Autoencoders on the NSL-KDD dataset. We investigate two methods: leveraging the latent space structure by measuring distances from test samples to the training data projections, and using the reconstruction error as a conventional anomaly detection metric. By comparing these approaches, we provide insights into their respective advantages and limitations in an unsupervised setting. Experimental results highlight the effectiveness of latent space exploitation for classification tasks.

</details>


### [69] [Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning](https://arxiv.org/abs/2602.19917)
*Thanh Nguyen,Tung Luu,Tri Ton,Sungwoong Kim,Chang D. Yoo*

Main category: cs.LG

TL;DR: 오프라인 강화 학습에서 OOD 데이터로 인한 외삽 오류를 해결하기 위해 불확실성 인식 Rank-One 다중 입력 다중 출력 MIMO Q 네트워크 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 오프라인 강화 학습의 안전하고 쉽게 확장 가능한 패러다임에도 불구하고, OOD 데이터로 인한 외삽 오류라는 문제에 직면하고 있습니다.

Method: 불확실성을 정량화하고 이를 훈련 손실에 활용하는 Rank-One MIMO 아키텍처를 도입하여 정책을 학습합니다.

Result: D4RL 벤치마크에서 최첨단 성능을 달성하면서도 계산 효율성을 유지합니다.

Conclusion: 우리의 프레임워크는 외삽 오류를 완화하고 오프라인 RL의 효율성을 향상시킬 수 있는 유망한 경로를 제공합니다.

Abstract: Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.

</details>


### [70] [Expanding the Role of Diffusion Models for Robust Classifier Training](https://arxiv.org/abs/2602.19931)
*Pin-Han Huang,Shang-Tse Chen,Hsuan-Tien Lin*

Main category: cs.LG

TL;DR: 이 연구는 확산 모델의 내부 표현을 적대적 훈련에 통합하여 강력한 이미지 분류기 훈련을 개선하는 방법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 강력한 이미지 분류기를 훈련시키기 위해 확산 생성 합성 데이터를 적대적 훈련에 통합하는 방법을 탐구합니다.

Method: 확산 모델의 내부 표현을 보조 학습 신호로 사용하는 체계적인 실험을 수행합니다.

Result: 확산 모델이 제공하는 표현이 다양한 동시에 부분적으로 강건하며, 이를 적대적 훈련 중 명시적으로 통합함으로써 강건성을 일관되게 개선함을 보여줍니다.

Conclusion: CIFAR-10, CIFAR-100, ImageNet에서 자사 연구결과를 검증하여, 적대적 훈련 내에서 확산 표현과 합성 데이터를 공동으로 활용하는 것이 효과적임을 입증합니다.

Abstract: Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.

</details>


### [71] [A Secure and Private Distributed Bayesian Federated Learning Design](https://arxiv.org/abs/2602.20003)
*Nuocheng Yang,Sihua Wang,Zhaohui Yang,Mingzhe Chen,Changchuan Yin,Kaibin Huang*

Main category: cs.LG

TL;DR: 이 논문에서는 분산적 연합 학습의 세 가지 주요 문제를 해결하기 위한 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: DFL은 중앙 서버 없이 분산된 시스템에서의 모델 학습을 가능하게 하지만, 개인정보 유출, 느린 수렴 및 비잔틴 공격자에 대한 취약점 등의 문제에 직면해 있습니다.

Method: 제안된 프레임워크 내에서 각 장치는 베이지안 접근 방식을 사용하여 지역 모델을 학습하고, 후속 교환을 위한 최적의 이웃 집합을 독립적으로 선택합니다.

Result: 우리는 동적 연결성, 비잔틴 탐지, 개인정보 수준 및 수렴 속도 간의 거래를 분석적으로 특성화했습니다. 이 통찰력을 활용하여 완전 분산형 그래프 신경망(GNN) 기반의 강화 학습(RL) 알고리즘을 개발했습니다.

Conclusion: 시뮬레이션 결과, 우리의 방법은 기존 보안 및 개인정보 보호 체계에 비해 월등히 낮은 오버헤드로 우수한 강건성과 효율성을 달성할 수 있음을 보여줍니다.

Abstract: Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.

</details>


### [72] [Learning Discriminative and Generalizable Anomaly Detector for Dynamic Graph with Limited Supervision](https://arxiv.org/abs/2602.20019)
*Yuxing Tian,Yiyan Qi,Fengran Mo,Weixu Zhang,Jian Guo,Jian-Yun Nie*

Main category: cs.LG

TL;DR: 이 논문은 동적 그래프 이상 탐지에서 정상/비정상 데이터를 활용하여 효과적인 경계를 학습하는 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 동적 그래프 이상 탐지(DGAD)는 레이블이 없는 이상치의 희소성으로 인해 어려움을 겪고 있다.

Method: 정상 데이터 및 한정된 레이블이 있는 이상치를 활용하여 구별되는 경계를 학습하는 모델 프레임워크를 제안한다.

Result: 광범위한 실험을 통해 제안된 프레임워크가 다양한 평가 설정에서 우수함을 입증하였다.

Conclusion: 본 연구는 기존의 비지도 및 반지도 방법의 한계를 극복하고 일반화 능력을 유지하는 새로운 방법론을 제시한다.

Abstract: Dynamic graph anomaly detection (DGAD) is critical for many real-world applications but remains challenging due to the scarcity of labeled anomalies. Existing methods are either unsupervised or semi-supervised: unsupervised methods avoid the need for labeled anomalies but often produce ambiguous boundary, whereas semi-supervised methods can overfit to the limited labeled anomalies and generalize poorly to unseen anomalies. To address this gap, we consider a largely underexplored problem in DGAD: learning a discriminative boundary from normal/unlabeled data, while leveraging limited labeled anomalies \textbf{when available} without sacrificing generalization to unseen anomalies. To this end, we propose an effective, generalizable, and model-agnostic framework with three main components: (i) residual representation encoding that capture deviations between current interactions and their historical context, providing anomaly-relevant signals; (ii) a restriction loss that constrain the normal representations within an interval bounded by two co-centered hyperspheres, ensuring consistent scales while keeping anomalies separable; (iii) a bi-boundary optimization strategy that learns a discriminative and robust boundary using the normal log-likelihood distribution modeled by a normalizing flow. Extensive experiments demonstrate the superiority of our framework across diverse evaluation settings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [73] [Influence of Autoencoder Latent Space on Classifying IoT CoAP Attacks](https://arxiv.org/abs/2602.18598)
*María Teresa García-Ordás,Jose Aveleira-Mata,Isaías García-Rodríguez,José Luis Casteleiro-Roca,Martín Bayón-Gutierrez,Héctor Alaiz-Moretón*

Main category: cs.CR

TL;DR: IoT 환경에서의 효과적인 데이터 감소 기술을 탐색하여 사이버 보안을 강화하는 연구.


<details>
  <summary>Details</summary>
Motivation: IoT 장치의 방대한 네트워크와 자원 제약으로 인해 사이버 보안이 도전받고 있다.

Method: 모델 기반 침입 탐지 시스템(IDS)에서 오토인코더의 잠재 공간과 세 가지 분류 기법을 결합한 데이터 감소 기술을 탐구한다.

Result: 특히 CoAP를 중심으로 한 IoT 데이터셋을 사용하여 보안 침해를 식별할 수 있는 강력한 모델을 개발하였다.

Conclusion: 제안된 방법이 2개의 학습된 기능만을 사용하여 99% 이상의 정밀도로 IoT 사이버 보안을 강화하는 데 효과적임을 입증하는 긍정적인 결과를 제공한다.

Abstract: The Internet of Things (IoT) presents a unique cybersecurity challenge due to its vast network of interconnected, resource-constrained devices. These vulnerabilities not only threaten data integrity but also the overall functionality of IoT systems. This study addresses these challenges by exploring efficient data reduction techniques within a model-based intrusion detection system (IDS) for IoT environments. Specifically, the study explores the efficacy of an autoencoder's latent space combined with three different classification techniques. Utilizing a validated IoT dataset, particularly focusing on the Constrained Application Protocol (CoAP), the study seeks to develop a robust model capable of identifying security breaches targeting this protocol. The research culminates in a comprehensive evaluation, presenting encouraging results that demonstrate the effectiveness of the proposed methodologies in strengthening IoT cybersecurity with more than a 99% of precision using only 2 learned features.

</details>


### [74] [Watermarking LLM Agent Trajectories](https://arxiv.org/abs/2602.18700)
*Wenlong Meng,Chen Gong,Terry Yue Zhuo,Fan Zhang,Kecen Li,Zheng Liu,Zhou Yang,Chengkun Wei,Wenzhi Chen*

Main category: cs.CR

TL;DR: LLM 에이전트는 문제 해결 행동을 안내하기 위해 고품질 궤적 데이터에 의존하며, 이를 위해 ActHook이라는 첫 번째 워터마킹 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기존 문헌은 LLM 에이전트 궤적에 대한 저작권 보호를 간과하고 있으며, 이로 인해 창작자가 데이터 도난에 취약해지고 소유권을 추적하거나 집행하기 어려워진다.

Method: ActHook은 소프트웨어 엔지니어링의 훅 메커니즘에서 영감을 받아 비밀 입력 키로 활성화되는 훅 동작을 궤적 데이터 세트에 내장하는 방식이다.

Result: 수학적 추론, 웹 검색 및 소프트웨어 공학 에이전트에 대한 실험 결과 ActHook은 Qwen-2.5-Coder-7B에서 평균 탐지 AUC 94.3을 달성하였다.

Conclusion: 물질 성능 저하를 거의 초래하지 않으면서도 효과적인 블랙박스 탐지가 가능하다.

Abstract: LLM agents rely heavily on high-quality trajectory data to guide their problem-solving behaviors, yet producing such data requires substantial task design, high-capacity model generation, and manual filtering. Despite the high cost of creating these datasets, existing literature has overlooked copyright protection for LLM agent trajectories. This gap leaves creators vulnerable to data theft and makes it difficult to trace misuse or enforce ownership rights. This paper introduces ActHook, the first watermarking method tailored for agent trajectory datasets. Inspired by hook mechanisms in software engineering, ActHook embeds hook actions that are activated by a secret input key and do not alter the original task outcome. Like software execution, LLM agents operate sequentially, allowing hook actions to be inserted at decision points without disrupting task flow. When the activation key is present, an LLM agent trained on watermarked trajectories can produce these hook actions at a significantly higher rate, enabling reliable black-box detection. Experiments on mathematical reasoning, web searching, and software engineering agents show that ActHook achieves an average detection AUC of 94.3 on Qwen-2.5-Coder-7B while incurring negligible performance degradation.

</details>


### [75] [PrivacyBench: Privacy Isn't Free in Hybrid Privacy-Preserving Vision Systems](https://arxiv.org/abs/2602.18900)
*Nnaemeka Obiefuna,Samuel Oyeneye,Similoluwa Odunaiya,Iremide Oyelaja,Steven Kolawole*

Main category: cs.CR

TL;DR: 본 연구는 민감한 딥러닝 응용에서 프라이버시 보호 기법 조합의 상호작용을 평가하기 위한 시스템적 가이드를 제시하는 PrivacyBench 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 민감한 딥러닝 응용, 특히 의료 영상 및 자율 시스템에서 프라이버시 보호 기법의 조합을 요구하지만, 실무자들은 효과적인 상호작용 평가에 대한 체계적인 지침이 부족하다.

Method: PrivacyBench라는 벤치마킹 프레임워크를 도입하여, 의료 데이터셋에서 ResNet18 및 ViT 모델을 통해 기법 조합의 실패를 체계적으로 평가하였다.

Result: FL + DP 조합은 수렴 실패를 보여주었으며, 정확도는 98%에서 13%로 떨어졌고, 계산 비용과 에너지 소비는 크게 증가했다. 반면 FL + SMPC 조합은 약간의 오버헤드에도 불구하고 거의 기준 성능을 유지했다.

Conclusion: PrivacyBench는 프라이버시-유용성-비용 trade-offs를 평가하기 위한 최초의 체계적 플랫폼을 제공하며, 이러한 발견은 프라이버시 보호 기술이 임의로 조합될 수 없음을 보여주고 자원 제약이 있는 환경에서의 강력한 배치를 위한 중요한 지침을 제시한다.

Abstract: Privacy preserving machine learning deployments in sensitive deep learning applications; from medical imaging to autonomous systems; increasingly require combining multiple techniques. Yet, practitioners lack systematic guidance to assess the synergistic and non-additive interactions of these hybrid configurations, relying instead on isolated technique analysis that misses critical system level interactions. We introduce PrivacyBench, a benchmarking framework that reveals striking failures in privacy technique combinations with severe deployment implications. Through systematic evaluation across ResNet18 and ViT models on medical datasets, we uncover that FL + DP combinations exhibit severe convergence failure, with accuracy dropping from 98% to 13% while compute costs and energy consumption substantially increase. In contrast, FL + SMPC maintains near-baseline performance with modest overhead. Our framework provides the first systematic platform for evaluating privacy-utility-cost trade-offs through automated YAML configuration, resource monitoring, and reproducible experimental protocols. PrivacyBench enables practitioners to identify problematic technique interactions before deployment, moving privacy-preserving computer vision from ad-hoc evaluation toward principled systems design. These findings demonstrate that privacy techniques cannot be composed arbitrarily and provide critical guidance for robust deployment in resource-constrained environments.

</details>


### [76] [LLM Scalability Risk for Agentic-AI and Model Supply Chain Security](https://arxiv.org/abs/2602.19021)
*Kiarash Ahi,Vaibhav Agrawal,Saeed Valizadeh*

Main category: cs.CR

TL;DR: 이 논문은 생성 AI에 기반한 사이버 보안에 대한 공격적 및 방어적 관점을 통합한 분석을 제시하며, 이를 통해 AI로 촉발된 위협의 증가와 그에 따른 글로벌 보안에 미치는 영향을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: AI가 사이버 보안에 미치는 영향과 그 필요성을 이해하기 위해, 공격적 및 방어적 관점에서의 포괄적인 분석이 필요합니다.

Method: 70개의 학술 자료, 산업 자료 및 정책 출처를 기반으로 AI로 지원되는 위협의 증가와 그에 따른 글로벌 보안에 대한 분석을 수행했습니다.

Result: LLM 확장성 위험 지수(LSRI)라는 프레임워크와 모델 공급망 프레임워크를 도입하여 LLM이 보안 민감한 환경에서 운영될 때의 위험을 점검하고, 전체 모델 수명 주기 동안 검증 가능한 신뢰의 뿌리를 확립했습니다.

Conclusion: Google Play Protect 및 Microsoft Security Copilot과 같은 플랫폼에서의 방어 전략을 종합하고 대규모 LLM 배치를 위한 거버넌스 로드맵을 제시했습니다.

Abstract: Large Language Models (LLMs) & Generative AI are transforming cybersecurity, enabling both advanced defenses and new attacks. Organizations now use LLMs for threat detection, code review, and DevSecOps automation, while adversaries leverage them to produce malwares and run targeted social-engineering campaigns. This paper presents a unified analysis integrating offensive and defensive perspectives on GenAI-driven cybersecurity. Drawing on 70 academic, industry, and policy sources, it analyzes the rise of AI-facilitated threats and its implications for global security to ground necessity for scalable defensive mechanisms. We introduce two primary contributions: the LLM Scalability Risk Index (LSRI), a parametric framework to stress-test operational risks when deploying LLMs in security-critical environments & a model-supply-chain framework establishing a verifiable root of trust throughout model lifecycle. We also synthesize defense strategies from platforms like Google Play Protect, Microsoft Security Copilot and outline a governance roadmap for secure, large-scale LLM deployment.

</details>


### [77] [Security Risks of AI Agents Hiring Humans: An Empirical Marketplace Study](https://arxiv.org/abs/2602.19514)
*Pulak Mehta*

Main category: cs.CR

TL;DR: 자율 AI 에이전트가 REST API와 MCP 통합을 통해 인간 작업자를 프로그래밍적으로 고용할 수 있게 되며, 이에 따른 보안 위협이 존재한다. 303개의 사례를 분석한 결과, 32.7%가 프로그래밍 경로에서 발생하였고, 여섯 가지 악용 유형이 식별되었으며, 기본 방어 수단이 결여되어 있음을 확인했다.


<details>
  <summary>Details</summary>
Motivation: 자율 AI 에이전트의 인간 작업자 고용 방식의 변화와 이에 따른 보안 위협 분석.

Method: RENTAHUMAN.AI에서 303개의 사례를 분석하고, 이중 99개의 사례를 프로그램 경로에서 발생한 것으로 확인하였으며, 이들을 여섯 가지 악용 유형으로 분류하고 두 명의 평가자가 코딩하였다.

Result: 99개의 사례(32.7%)가 프로그래밍 채널에서 유래되었으며, 여섯 가지 활성 악용 클래스가 식별되었다. 이들 악용은 작업자당 중간 25달러에 구매 가능하다.

Conclusion: 기본적인 방어책은 가능하지만 현재 존재하지 않으며, 52개의 사례(17.2%)가 단일 거짓 긍정을 나타냈다.

Abstract: Autonomous AI agents can now programmatically hire human workers through marketplaces using REST APIs and Model Context Protocol (MCP) integrations. This creates an attack surface analogous to CAPTCHA-solving services but with physical-world reach. We present an empirical measurement study of this threat, analyzing 303 bounties from RENTAHUMAN.AI, a marketplace where agents post tasks and manage escrow payments. We find that 99 bounties (32.7%), originate from programmatic channels (API keys or MCP). Using a dual-coder methodology (\k{appa} = 0.86 ), we identify six active abuse classes: credential fraud, identity impersonation, automated reconnaissance, social media manipulation, authentication circumvention, and referral fraud, all purchasable for a median of $25 per worker. A retrospective evaluation of seven content-screening rules flags 52 bounties (17.2%) with a single false positive, demonstrating that while basic defenses are feasible, they are currently absent.

</details>


### [78] [CIBER: A Comprehensive Benchmark for Security Evaluation of Code Interpreter Agents](https://arxiv.org/abs/2602.19547)
*Lei Ba,Qinbin Li,Songze Li*

Main category: cs.CR

TL;DR: LLM 기반 코드 해석기 에이전트의 보안 취약성을 평가하기 위한 자동화된 벤치마크인 CIBER를 소개하고, 다양한 공격에 대한 이들의 반응을 분석함으로써 보안성과 모델 아키텍처 간의 관계를 조명한다.


<details>
  <summary>Details</summary>
Motivation: 코드 실행 기능으로 인해 초래되는 위험성에 대한 LLM 기반 코드 해석기 에이전트의 강건성을 평가하기 위해.

Method: 동적 공격 생성, 독립된 안전 샌드박스 및 상태 인식 평가를 결합한 CIBER 벤치마크와 두 개의 코드 해석기 에이전트(OpenInterpreter 및 OpenCodeInterpreter)에서 여섯 개의 기초 모델 평가.

Result: Interpreter 아키텍처와 모델 정렬이 보안 기준선을 설정하며, 특화된 모델이 일반적인 최신 모델보다 더 우수한 성능을 발휘함을 발견.

Conclusion: 에이전트가 명시적 위협에 대해 견고한 방어력을 보이지만, 암시적 의미적 위협에 대해서는 치명적으로 실패하는 보안 양극화를 드러낸다.

Abstract: LLM-based code interpreter agents are increasingly deployed in critical workflows, yet their robustness against risks introduced by their code execution capabilities remains underexplored. Existing benchmarks are limited to static datasets or simulated environments, failing to capture the security risks arising from dynamic code execution, tool interactions, and multi-turn context. To bridge this gap, we introduce CIBER, an automated benchmark that combines dynamic attack generation, isolated secure sandboxing, and state-aware evaluation to systematically assess the vulnerability of code interpreter agents against four major types of adversarial attacks: Direct/Indirect Prompt Injection, Memory Poisoning, and Prompt-based Backdoor.
  We evaluate six foundation models across two representative code interpreter agents (OpenInterpreter and OpenCodeInterpreter), incorporating a controlled study of identical models. Our results reveal that Interpreter Architecture and Model Alignment Set the Security Baseline. Structural integration enables aligned specialized models to outperform generic SOTA models. Conversely, high intelligence paradoxically increases susceptibility to complex adversarial prompts due to stronger instruction adherence. Furthermore, we identify a "Natural Language Disguise" Phenomenon, where natural language functions as a significantly more effective input modality than explicit code snippets (+14.1% ASR), thereby bypassing syntax-based defenses. Finally, we expose an alarming Security Polarization, where agents exhibit robust defenses against explicit threats yet fail catastrophically against implicit semantic hazards, highlighting a fundamental blind spot in current pattern-matching protection approaches.

</details>


### [79] [Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains](https://arxiv.org/abs/2602.19555)
*Xiaochong Jiang,Shiqi Yang,Wenting Yang,Yichen Liu,Cheng Ji*

Main category: cs.CR

TL;DR: 이 논문은 대형 언어 모델(LLM)을 기반으로 한 에이전트 시스템의 보안 위험을 제시하고, 이를 데이터 공급망 공격과 도구 공급망 공격으로 분류하여 통합된 런타임 프레임워크를 통해 체계화하고, 마지막으로 제로 신뢰 런타임 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 시스템이 신뢰할 수 없는 데이터를 통해 조작될 수 있는 위험을 인식하고, 이러한 위험을 체계적으로 분류하여 이해하기 위해.

Method: 런타임 프레임워크를 통해 데이터 공급망 공격과 도구 공급망 공격으로 위협을 분류하고, Viral Agent Loop를 식별하여 에이전트가 자기 전파하는 생성 웜의 매개체 역할을 하는 방식 분석.

Result: 신뢰할 수 없는 제어 흐름으로서 컨텍스트를 취급하고, 의미적 추론이 아닌 암호학적 출처를 통해 도구 실행을 제한하는 제로 신뢰 런타임 아키텍처의 필요성을 제안.

Conclusion: 에이전트 시스템의 보안 강화를 위해 제로 신뢰 접근 방식이 필요하며, 이는 런타임 행동을 이해하고 적절히 대응하기 위해 중요하다.

Abstract: Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference.

</details>


### [80] [Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks](https://arxiv.org/abs/2602.20156)
*David Schmotz,Luca Beurer-Kellner,Sahar Abdelnabi,Maksym Andriushchenko*

Main category: cs.CR

TL;DR: LLM 에이전트는 코드 실행 및 도구, 최근 도입된 에이전트 스킬 기능으로 인해 빠르게 발전하고 있다. 그러나 스킬 기반 프롬프트 주입 공격이 주요 위협으로 식별되었고, 이에 대한 평가 기준인 SkillInject를 도입했다.


<details>
  <summary>Details</summary>
Motivation: LLM 애플리케이션을 전문화된 제3자 코드와 지식으로 확장하는 스킬 기능을 도입하여, 새로운 도메인으로의 에이전트 능력 확장을 시도하고자 했다.

Method: SkillInject 라는 기준을 통해 널리 사용되는 LLM 에이전트의 스킬 파일을 통한 주입 취약점을 평가하였다. 이 기준은 전반적으로 악의적인 주입에서 미묘하고 맥락 의존적인 공격에 이르기까지 202개의 주입 작업 쌍을 포함한다.

Result: SkillInject를 통해 널리 사용되는 LLM 모델들이 최대 80%의 공격 성공률을 보이며, 데이터 유출, 파괴적 행동 및 랜섬웨어와 유사한 행동을 포함해 매우 유해한 지시를 실행하는 등의 결과를 도출했다.

Conclusion: 모델 확장이나 간단한 입력 필터링으로는 이 문제를 해결할 수 없으며, 강력한 에이전트 보안을 위해서는 맥락 인식 인증 프레임워크가 필요함을 나타낸다.

Abstract: LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.

</details>
