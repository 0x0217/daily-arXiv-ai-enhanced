<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088)
*Linyao Chen,Zimian Peng,Yingxuan Yang,Yikun Wang,Wenzheng Tom Tang,Hiroki H. Kobayashi,Weinan Zhang*

Main category: cs.AI

TL;DR: EnvX는 GitHub 저장소를 지능형 자율 에이전트로 변환하여 소프트웨어 재사용을 자동화하고 협업을 촉진하는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 오픈 소스 저장소의 광범위한 가용성이 재사용 가능한 소프트웨어 구성 요소의 방대한 모음을 가지고 있지만, 그 활용은 여전히 수동적이며 오류가 발생하기 쉽고 단절되어 있다.

Method: EnvX는 에이전틱 AI를 활용하여 GitHub 저장소를 에이전트화하고, 자연어 상호작용 및 에이전트 간 협업이 가능한 지능형 자율 에이전트로 변환하는 3단계 프로세스를 활용한다.

Result: EnvX는 GitTaskBench 벤치마크에서 74.07%의 실행 완료율과 51.85%의 작업 통과율을 달성하여 기존 프레임워크보다 뛰어난 성능을 보여주었다.

Conclusion: 이 연구는 저장소를 수동적인 코드 자원으로 취급하는 것에서 지능형 상호작용 에이전트로의 전환을 나타내며, 오픈 소스 생태계 내의 접근성과 협업을 증진시킨다.

Abstract: The widespread availability of open-source repositories has led to a vast
collection of reusable software components, yet their utilization remains
manual, error-prone, and disconnected. Developers must navigate documentation,
understand APIs, and write integration code, creating significant barriers to
efficient software reuse. To address this, we present EnvX, a framework that
leverages Agentic AI to agentize GitHub repositories, transforming them into
intelligent, autonomous agents capable of natural language interaction and
inter-agent collaboration. Unlike existing approaches that treat repositories
as static code resources, EnvX reimagines them as active agents through a
three-phase process: (1) TODO-guided environment initialization, which sets up
the necessary dependencies, data, and validation datasets; (2) human-aligned
agentic automation, allowing repository-specific agents to autonomously perform
real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple
agents to collaborate. By combining large language model capabilities with
structured tool integration, EnvX automates not just code generation, but the
entire process of understanding, initializing, and operationalizing repository
functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18
repositories across domains such as image processing, speech recognition,
document analysis, and video manipulation. Our results show that EnvX achieves
a 74.07% execution completion rate and 51.85% task pass rate, outperforming
existing frameworks. Case studies further demonstrate EnvX's ability to enable
multi-repository collaboration via the A2A protocol. This work marks a shift
from treating repositories as passive code resources to intelligent,
interactive agents, fostering greater accessibility and collaboration within
the open-source ecosystem.

</details>


### [2] [Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI](https://arxiv.org/abs/2509.08151)
*Botao Zhu,Jeslyn Wang,Dusit Niyato,Xianbin Wang*

Main category: cs.AI

TL;DR: 본 연구는 잠재적 협력 장치의 신뢰성 평가를 효율적으로 수행하기 위한 2TSD 모델을 제안하며, 이를 통해 협력자의 선택 과정을 간소화하고 정확도를 개선한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 컴퓨팅 작업의 효과적인 실행을 위해 잠재적 협력 장치의 정확한 신뢰성 평가가 필수적이다.

Method: 대형 AI 모델(LAM) 기반의 교사-학생 에이전트 구조를 바탕으로 한 작업 특정 신뢰 의미 증류(2TSD) 모델을 제안한다.

Result: 제안된 2TSD 모델은 협력자 평가 시간을 단축하고 장치 자원 소비를 줄이며 협력자 선택의 정확성을 개선하는 것으로 나타났다.

Conclusion: 이 모델은 복잡한 작업 수행을 위한 신뢰성 평가에서 높은 효율성과 정확성을 제공한다.

Abstract: Accurate trustworthiness evaluation of potential collaborating devices is
essential for the effective execution of complex computing tasks. This
evaluation process involves collecting diverse trust-related data from
potential collaborators, including historical performance and available
resources, for collaborator selection. However, when each task owner
independently assesses all collaborators' trustworthiness, frequent data
exchange, complex reasoning, and dynamic situation changes can result in
significant overhead and deteriorated trust evaluation. To overcome these
challenges, we propose a task-specific trust semantics distillation (2TSD)
model based on a large AI model (LAM)-driven teacher-student agent
architecture. The teacher agent is deployed on a server with powerful
computational capabilities and an augmented memory module dedicated to
multidimensional trust-related data collection, task-specific trust semantics
extraction, and task-collaborator matching analysis. Upon receiving
task-specific requests from device-side student agents, the teacher agent
transfers the trust semantics of potential collaborators to the student agents,
enabling rapid and accurate collaborator selection. Experimental results
demonstrate that the proposed 2TSD model can reduce collaborator evaluation
time, decrease device resource consumption, and improve the accuracy of
collaborator selection.

</details>


### [3] [Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making](https://arxiv.org/abs/2509.08785)
*Anup Tuladhar,Araz Minhas,Adam Kirton,Eli Kinney-Lang*

Main category: cs.AI

TL;DR: AI의 의사결정에 내러티브 요소가 미치는 영향을 탐구하는 실험 플랫폼을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 의사결정과 내러티브 추론을 모두 수행할 수 있지만, 이 두 가지 능력은 주로 별개로 연구되었다. 따라서 이를 연결하고자 한다.

Method: 강화 학습(RL) 정책과 내러티브 프레임워크를 결합한 이중 시스템 아키텍처를 사용하여, 내러티브 요소가 보상 기반 학습에 미치는 영향을 조사한다.

Result: 설정된 환경에서 에이전트는 정책 제안과 주변 정보를 동시에 받는다. 모듈형 설계는 환경 복잡성, 내러티브 매개변수 및 의사결정 간 상호작용을 통제된 방식으로 테스트할 수 있게 해준다.

Conclusion: 이 구현은 다양한 내러티브 프레임워크가 보상 기반 의사결정에 미치는 영향과 AI 시스템에서 최적화 기반 학습과 상징적 추론 간의 잠재적 상호작용을 연구하는 기초를 제공한다.

Abstract: We present a preliminary experimental platform that explores how narrative
elements might shape AI decision-making by combining reinforcement learning
(RL) with language model reasoning. While AI systems can now both make
decisions and engage in narrative reasoning, these capabilities have mostly
been studied separately. Our platform attempts to bridge this gap using a
dual-system architecture to examine how narrative frameworks could influence
reward-based learning. The system comprises a reinforcement learning policy
that suggests actions based on past experience, and a language model that
processes these suggestions through different narrative frameworks to guide
decisions. This setup enables initial experimentation with narrative elements
while maintaining consistent environment and reward structures. We implement
this architecture in a configurable gridworld environment, where agents receive
both policy suggestions and information about their surroundings. The
platform's modular design facilitates controlled testing of environmental
complexity, narrative parameters, and the interaction between reinforcement
learning and narrative-based decisions. Our logging system captures basic
decision metrics, from RL policy values to language model reasoning to action
selection patterns. While preliminary, this implementation provides a
foundation for studying how different narrative frameworks might affect
reward-based decisions and exploring potential interactions between
optimization-based learning and symbolic reasoning in AI systems.

</details>


### [4] [Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following](https://arxiv.org/abs/2509.08222)
*Minjong Yoo,Jinwoo Jang,Wei-jin Park,Honguk Woo*

Main category: cs.AI

TL;DR: ExRAP 프레임워크는 동적 환경에서의 지속적인 지시 수행 작업을 해결하도록 설계되었으며, 물리적 환경 탐색을 통해 대규모 언어 모델의 현실적 추론 능력을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 지속적인 지시 수행 작업을 효과적으로 처리하기 위해.

Method: 환경 맥락 메모리에 대한 쿼리와 조건부 작업 실행으로 각 지시를 분해하고, 정보 기반 탐사를 통합하여 작업 계획을 수립합니다.

Result: 우리의 접근법은 다양한 지시 시나리오에서 강력한 성능을 입증하고 최신 LLM 기반 작업 계획 방법을 초월합니다.

Conclusion: ExRAP 프레임워크는 목표 성공률과 실행 효율성 면에서 우수한 성능을 보여주었습니다.

Abstract: This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)
framework, designed to tackle continual instruction following tasks of embodied
agents in dynamic, non-stationary environments. The framework enhances Large
Language Models' (LLMs) embodied reasoning capabilities by efficiently
exploring the physical environment and establishing the environmental context
memory, thereby effectively grounding the task planning process in time-varying
environment contexts. In ExRAP, given multiple continual instruction following
tasks, each instruction is decomposed into queries on the environmental context
memory and task executions conditioned on the query results. To efficiently
handle these multiple tasks that are performed continuously and simultaneously,
we implement an exploration-integrated task planning scheme by incorporating
the {information-based exploration} into the LLM-based planning process.
Combined with memory-augmented query evaluation, this integrated scheme not
only allows for a better balance between the validity of the environmental
context memory and the load of environment exploration, but also improves
overall task performance. Furthermore, we devise a {temporal consistency
refinement} scheme for query evaluation to address the inherent decay of
knowledge in the memory. Through experiments with VirtualHome, ALFRED, and
CARLA, our approach demonstrates robustness against a variety of embodied
instruction following scenarios involving different instruction scales and
types, and non-stationarity degrees, and it consistently outperforms other
state-of-the-art LLM-based task planning approaches in terms of both goal
success rate and execution efficiency.

</details>


### [5] [Real-world Music Plagiarism Detection With Music Segment Transcription System](https://arxiv.org/abs/2509.08282)
*Seonghyeon Go*

Main category: cs.AI

TL;DR: 이 연구는 다양한 음악 정보 검색 기술을 결합하여 음악 표절을 탐지하는 시스템을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 음악 정보 검색 기술의 지속적인 발전으로 음악 생성 및 배포가 다양해지고 접근 가능해졌으며, 음악 저작권 보호에 대한 관심이 증가하고 있습니다.

Method: 음악 세그먼트 전사 시스템을 개발하여 오디오 녹음에서 음악적으로 의미 있는 세그먼트를 추출하고, 이를 통해 다양한 음악 형식 간의 표절을 탐지합니다. 여러 음악적 특징에 기반한 유사성 점수를 계산합니다.

Result: 제안된 접근 방식은 음악 표절 탐지 실험에서 유망한 결과를 보여주었으며, 실제 세계의 음악 시나리오에 적용할 수 있습니다.

Conclusion: 우리는 실제 사례를 사용하여 음악 유사성 연구를 위한 유사 음악 쌍(SMP) 데이터 세트를 수집하였으며, 이 데이터 세트는 공개적으로 이용 가능합니다.

Abstract: As a result of continuous advances in Music Information Retrieval (MIR)
technology, generating and distributing music has become more diverse and
accessible. In this context, interest in music intellectual property protection
is increasing to safeguard individual music copyrights. In this work, we
propose a system for detecting music plagiarism by combining various MIR
technologies. We developed a music segment transcription system that extracts
musically meaningful segments from audio recordings to detect plagiarism across
different musical formats. With this system, we compute similarity scores based
on multiple musical features that can be evaluated through comprehensive
musical analysis. Our approach demonstrated promising results in music
plagiarism detection experiments, and the proposed method can be applied to
real-world music scenarios. We also collected a Similar Music Pair (SMP)
dataset for musical similarity research using real-world cases. The dataset are
publicly available.

</details>


### [6] [Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies](https://arxiv.org/abs/2509.08312)
*Binghan Wu,Shoufeng Wang,Yunxin Liu,Ya-Qin Zhang,Joseph Sifakis,Ye Ouyang*

Main category: cs.AI

TL;DR: 레벨 4 자율 네트워크의 발전은 통신에서 전략적 전환점을 나타내며, 자율적 서비스 제공을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 자율 네트워크의 이론과 운영 현실 간의 간극을 해소하고자 한다.

Method: Joseph Sifakis의 AN 에이전트 참조 아키텍처를 구현하고, 하이브리드 지식 표현에 의해 구동되는 협조적 프로액티브-리액티브 런타임을 배포한다.

Result: 5G NR 서브-6 GHz에서 10ms 미만의 실시간 제어를 검증하고, OLLA 알고리즘보다 6% 높은 다운로드 스루풋과 67%의 블록 오류율 감소를 달성했다.

Conclusion: 이 연구는 전통적인 자율성 장벽을 극복하고, 차세대 목표를 위한 L4 기능을 발전시킬 수 있는 아키텍처의 실행 가능성을 확인했다.

Abstract: The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a
strategic inflection point in telecommunications, where networks must transcend
reactive automation to achieve genuine cognitive capabilities--fulfilling TM
Forum's vision of self-configuring, self-healing, and self-optimizing systems
that deliver zero-wait, zero-touch, and zero-fault services. This work bridges
the gap between architectural theory and operational reality by implementing
Joseph Sifakis's AN Agent reference architecture in a functional cognitive
system, deploying coordinated proactive-reactive runtimes driven by hybrid
knowledge representation. Through an empirical case study of a Radio Access
Network (RAN) Link Adaptation (LA) Agent, we validate this framework's
transformative potential: demonstrating sub-10 ms real-time control in 5G NR
sub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link
Adaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for
ultra-reliable services through dynamic Modulation and Coding Scheme (MCS)
optimization. These improvements confirm the architecture's viability in
overcoming traditional autonomy barriers and advancing critical L4-enabling
capabilities toward next-generation objectives.

</details>


### [7] [Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives](https://arxiv.org/abs/2509.08380)
*Prathamesh Vasudeo Naik,Naresh Kumar Dintakurthi,Zhanghao Hu,Yue Wang,Robby Qiu*

Main category: cs.AI

TL;DR: 이 논문은 전통적인 방법보다 더 빠르고 정확하게 의심스러운 활동 보고서(SAR)를 생성하는 Co-Investigator AI라는 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 자금 세탁 방지(AML) 프로세스에서 규제 준수를 충족하는 의심스러운 활동 보고서(SAR) 생성을 효율적으로 개선할 필요성이 있습니다.

Method: 임무 출처별 전문 에이전트를 통합하고 동적 메모리 관리 및 실시간 검증 기능을 갖춘 Co-Investigator AI를 개발했습니다.

Result: Co-Investigator AI는 다양한 금융 범죄 시나리오에서 SAR 초안 작성 및 규제 준수 기대에 맞추는 능력을 입증했습니다.

Conclusion: 이 접근 방식은 규제 보고의 새로운 시대를 여는 것으로, 자금 세탁 방지 프로세스의 핵심에 AI 에이전트의 변혁적 이점을 가져옵니다.

Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a
high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.
While large language models (LLMs) offer promising fluency, they suffer from
factual hallucination, limited crime typology alignment, and poor
explainability -- posing unacceptable risks in compliance-critical domains.
This paper introduces Co-Investigator AI, an agentic framework optimized to
produce Suspicious Activity Reports (SARs) significantly faster and with
greater accuracy than traditional methods. Drawing inspiration from recent
advances in autonomous agent architectures, such as the AI Co-Scientist, our
approach integrates specialized agents for planning, crime type detection,
external intelligence gathering, and compliance validation. The system features
dynamic memory management, an AI-Privacy Guard layer for sensitive data
handling, and a real-time validation agent employing the Agent-as-a-Judge
paradigm to ensure continuous narrative quality assurance. Human investigators
remain firmly in the loop, empowered to review and refine drafts in a
collaborative workflow that blends AI efficiency with domain expertise. We
demonstrate the versatility of Co-Investigator AI across a range of complex
financial crime scenarios, highlighting its ability to streamline SAR drafting,
align narratives with regulatory expectations, and enable compliance teams to
focus on higher-order analytical work. This approach marks the beginning of a
new era in compliance reporting -- bringing the transformative benefits of AI
agents to the core of regulatory processes and paving the way for scalable,
reliable, and transparent SAR generation.

</details>


### [8] [TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making](https://arxiv.org/abs/2509.08500)
*Kechen Jiao,Zhirui Fang,Jiahao Liu,Bei Li,Qifan Wang,Xinyu Liu,Junhao Ruan,Zhongjian Qiao,Yifan Zhu,Yaxin Xu,Jingang Wang,Xiu Li*

Main category: cs.AI

TL;DR: 본 논문은 의사결정 향상을 위해 Thought-Centric Preference Optimization (TCPO)을 제안하며, 모델의 중간 추론 과정을 정렬하여 모델 열화를 완화합니다.


<details>
  <summary>Details</summary>
Motivation: VLM의 효과적인 일반화 능력을 사용하여 맥락-specific 동적 작업에서 구현된 인공지능의 방향성을 따라가는 것은 중요한 도전 과제입니다.

Method: TCPO는 희소 보상 신호를 더 풍부한 단계 샘플 쌍으로 변환하는 단계별 선호 기반 최적화 접근 방식을 도입합니다.

Result: ALFWorld 환경에서 평균 성공률 26.67%를 달성하여 RL4VLM보다 6% 향상된 결과를 보였습니다.

Conclusion: 추천 기반 학습 기법과 CoT 프로세스를 통합하여 구현된 에이전트의 의사결정 능력을 향상시킬 잠재력을 강조합니다.

Abstract: Using effective generalization capabilities of vision language models (VLMs)
in context-specific dynamic tasks for embodied artificial intelligence remains
a significant challenge. Although supervised fine-tuned models can better align
with the real physical world, they still exhibit sluggish responses and
hallucination issues in dynamically changing environments, necessitating
further alignment. Existing post-SFT methods, reliant on reinforcement learning
and chain-of-thought (CoT) approaches, are constrained by sparse rewards and
action-only optimization, resulting in low sample efficiency, poor consistency,
and model degradation. To address these issues, this paper proposes
Thought-Centric Preference Optimization (TCPO) for effective embodied
decision-making. Specifically, TCPO introduces a stepwise preference-based
optimization approach, transforming sparse reward signals into richer step
sample pairs. It emphasizes the alignment of the model's intermediate reasoning
process, mitigating the problem of model degradation. Moreover, by
incorporating Action Policy Consistency Constraint (APC), it further imposes
consistency constraints on the model output. Experiments in the ALFWorld
environment demonstrate an average success rate of 26.67%, achieving a 6%
improvement over RL4VLM and validating the effectiveness of our approach in
mitigating model degradation after fine-tuning. These results highlight the
potential of integrating preference-based learning techniques with CoT
processes to enhance the decision-making capabilities of vision-language models
in embodied agents.

</details>


### [9] [Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference](https://arxiv.org/abs/2509.08682)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Jiawei Shen,Jingjiang Liu,Yidan Liang*

Main category: cs.AI

TL;DR: 다중 에이전트 시스템(MAS)의 실패 기인 분석을 위한 새로운 프레임워크를 제시하며, 이는 성능 인과 역전 원리와 새로운 인과 발견 알고리즘을 통해 MAS의 복잡한 상호작용 데이터의 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업 자동화를 위해 MAS는 필수적이지만, 실패 기인 분석의 어려움이 실제 배치를 방해하고 있다.

Method: 다중-그라뉼러티 인과 추론에 기반한 최초의 MAS 실패 기인 분석 프레임워크를 도입하며, 두 가지 주요 기술 기여가 있다: (1) 성능 인과 역전 원리와 Shapley 가치를 결합하여 에이전트 수준의 책임을 정확하게 할당하는 방법; (2) 비정상적인 MAS 상호작용 데이터를 처리하여 중요한 실패 단계를 강건하게 식별하는 CDC-MAS라는 새로운 인과 발견 알고리즘.

Result: Who&When 및 TRAIL 벤치마크에서의 평가 결과로, 우리의 방법은 최대 36.2%의 단계 수준 정확도를 달성하며, 생성된 최적화가 전체 작업 성공률을 평균 22.4% 향상시킨다.

Conclusion: 본 연구는 복잡한 에이전트 상호작용 디버깅을 위한 원칙적이고 효과적인 해결책을 제공하며, 보다 신뢰할 수 있고 해석 가능한 다중 에이전트 시스템으로 나아가는 길을 열어준다.

Abstract: Multi-agent systems (MAS) are critical for automating complex tasks, yet
their practical deployment is severely hampered by the challenge of failure
attribution. Current diagnostic tools, which rely on statistical correlations,
are fundamentally inadequate; on challenging benchmarks like Who\&When,
state-of-the-art methods achieve less than 15\% accuracy in locating the
root-cause step of a failure. To address this critical gap, we introduce the
first failure attribution framework for MAS grounded in multi-granularity
causal inference. Our approach makes two key technical contributions: (1) a
performance causal inversion principle, which correctly models performance
dependencies by reversing the data flow in execution logs, combined with
Shapley values to accurately assign agent-level blame; (2) a novel causal
discovery algorithm, CDC-MAS, that robustly identifies critical failure steps
by tackling the non-stationary nature of MAS interaction data. The framework's
attribution results directly fuel an automated optimization loop, generating
targeted suggestions whose efficacy is validated via counterfactual
simulations. Evaluations on the Who\&When and TRAIL benchmarks demonstrate a
significant leap in performance. Our method achieves up to 36.2\% step-level
accuracy. Crucially, the generated optimizations boost overall task success
rates by an average of 22.4\%. This work provides a principled and effective
solution for debugging complex agent interactions, paving the way for more
reliable and interpretable multi-agent systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [JEL: A Novel Model Linking Knowledge Graph entities to News Mentions](https://arxiv.org/abs/2509.08086)
*Michael Kishelev,Pranab Bhadani,Wanying Ding,Vinay Chaudhri*

Main category: cs.LG

TL;DR: JEL은 혁신적인 엔드 투 엔드 다중 신경망 기반의 엔터티 링크 모델로, 현재의 최첨단 모델을 초월합니다.


<details>
  <summary>Details</summary>
Motivation: 지식 그래프는 중요한 관계를 포착하고 다수의 이질적 데이터 소스를 통합하는 매력적인 추상화로 부상했습니다.

Method: 본 논문에서는 텍스트 출처에서 언급된 개체와 엔티티를 올바르게 연결하기 위해 다중 신경망을 사용하는 새로운 방법을 제시합니다.

Result: 결과적으로, JEL 모델은 기존의 최첨단 모델보다 더 나은 성능을 보였습니다.

Conclusion: 엔터티 링크는 비구조적 뉴스 텍스트와 지식 그래프를 연결하는 데 필수적이며, 이를 통해 사용자는 지식 그래프 내에서 방대한 양의 데이터에 접근할 수 있습니다.

Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural
network based entity linking model, which beats current state-of-art model.
Knowledge Graphs have emerged as a compelling abstraction for capturing
critical relationships among the entities of interest and integrating data from
multiple heterogeneous sources. A core problem in leveraging a knowledge graph
is linking its entities to the mentions (e.g., people, company names) that are
encountered in textual sources (e.g., news, blogs., etc) correctly, since there
are thousands of entities to consider for each mention. This task of linking
mentions and entities is referred as Entity Linking (EL). It is a fundamental
task in natural language processing and is beneficial in various uses cases,
such as building a New Analytics platform. News Analytics, in JPMorgan, is an
essential task that benefits multiple groups across the firm. According to a
survey conducted by the Innovation Digital team 1 , around 25 teams across the
firm are actively looking for news analytics solutions, and more than \$2
million is being spent annually on external vendor costs. Entity linking is
critical for bridging unstructured news text with knowledge graphs, enabling
users access to vast amounts of curated data in a knowledge graph and
dramatically facilitating their daily work.

</details>


### [11] [Performance Assessment Strategies for Generative AI Applications in Healthcare](https://arxiv.org/abs/2509.08087)
*Victor Garcia,Mariia Sidulova,Aldo Badano*

Main category: cs.LG

TL;DR: 생성 인공지능(GenAI)은 의료 분야에서의 응용 가능성을 가진 새로운 패러다임입니다. 이러한 기술의 성능 평가를 위해서는 임상 작업과 실제 임상 환경에서의 성능 변동을 이해해야 합니다.


<details>
  <summary>Details</summary>
Motivation: GenAI 기술의 의료 분야에서의 활용 가능성과 성능에 대한 이해가 필요하다.

Method: 인간 전문 지식을 활용하고 비용 효율적인 계산 모델을 평가자로 사용하는 평가 전략에 대해 논의한다.

Result: 현재의 최첨단 방법론을 통해 GenAI 응용 프로그램의 성능을 평가하는 방법을 제시한다.

Conclusion: GenAI의 성능 평가는 양적 벤치마크에만 의존하지 않고, 보다 효과적으로 수행될 필요가 있다.

Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm
within artificial intelligence, with applications throughout the medical
enterprise. Assessing GenAI applications necessitates a comprehensive
understanding of the clinical task and awareness of the variability in
performance when implemented in actual clinical environments. Presently, a
prevalent method for evaluating the performance of generative models relies on
quantitative benchmarks. Such benchmarks have limitations and may suffer from
train-to-the-test overfitting, optimizing performance for a specified test set
at the cost of generalizability across other task and data distributions.
Evaluation strategies leveraging human expertise and utilizing cost-effective
computational models as evaluators are gaining interest. We discuss current
state-of-the-art methodologies for assessing the performance of GenAI
applications in healthcare and medical devices.

</details>


### [12] [Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography](https://arxiv.org/abs/2509.08116)
*Nooshin Maghsoodi,Sarah Nassar,Paul F R Wilson,Minh Nguyen Nhat To,Sophia Mannina,Shamel Addas,Stephanie Sibley,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: PhysioCLR는 생리학적 지식을 활용한 자기 지도 학습 프레임워크로, ECG 기반 부정맥 분류의 일반화 가능성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: ECG 분석의 효과는 레이블 데이터의 제한된 가용성으로 저해됩니다. 자기 지도 학습이 이를 해결할 수 있습니다.

Method: PhysioCLR는 생리학적 유사성을 통합하여 샘플들을 끌어모으고, 비슷하지 않은 샘플들을 멀리하는 방식으로 사전 학습을 진행합니다.

Result: PhysioCLR는 Chapman, Georgia 및 개인 ICU 데이터셋에서 AUROC를 12% 향상시켰습니다.

Conclusion: PhysioCLR은 임상적으로 의미 있는 ECG 특성을 학습할 수 있게 합니다.

Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart
conditions; however, the effectiveness of artificial intelligence (AI)-based
ECG analysis is often hindered by the limited availability of labeled data.
Self-supervised learning (SSL) can address this by leveraging large-scale
unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning
Representation for ECG), a physiology-aware contrastive learning framework that
incorporates domain-specific priors to enhance the generalizability and
clinical relevance of ECG-based arrhythmia classification. Methods: During
pretraining, PhysioCLR learns to bring together embeddings of samples that
share similar clinically relevant features while pushing apart those that are
dissimilar. Unlike existing methods, our method integrates ECG physiological
similarity cues into contrastive learning, promoting the learning of clinically
meaningful representations. Additionally, we introduce ECG- specific
augmentations that preserve the ECG category post augmentation and propose a
hybrid loss function to further refine the quality of learned representations.
Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,
for multilabel ECG diagnoses, as well as a private ICU dataset labeled for
binary classification. Across the Chapman, Georgia, and private cohorts,
PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,
underscoring its robust cross-dataset generalization. Conclusion: By embedding
physiological knowledge into contrastive learning, PhysioCLR enables the model
to learn clinically meaningful and transferable ECG eatures. Significance:
PhysioCLR demonstrates the potential of physiology-informed SSL to offer a
promising path toward more effective and label-efficient ECG diagnostics.

</details>


### [13] [MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs](https://arxiv.org/abs/2509.08156)
*Swati Swati,Arjun Roy,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: mmm-fair는 분류 오류와 공정성 위반을 최소화하는 모델을 동적으로 최적화하는 오픈 소스 툴킷입니다.


<details>
  <summary>Details</summary>
Motivation: 공정성을 고려한 분류는 성능과 공정성을 조화롭게 맞추어야 하며, 교차 편향으로 인해 더욱 복잡해집니다.

Method: 부스팅 기반 앙상블 접근 방식을 활용하여 모델 가중치를 동적으로 최적화하는 기능을 제공합니다.

Result: 사용자는 context-specific 필요에 맞춰 모델을 배포하며, 최신의 방법에서 놓치기 쉬운 교차 편향을 발견할 수 있습니다.

Conclusion: mmm-fair는 다차원 공정성과 다중 목표 최적화를 결합한 독창적인 오픈 소스 툴킷입니다.

Abstract: Fairness-aware classification requires balancing performance and fairness,
often intensified by intersectional biases. Conflicting fairness definitions
further complicate the task, making it difficult to identify universally fair
solutions. Despite growing regulatory and societal demands for equitable AI,
popular toolkits offer limited support for exploring multi-dimensional fairness
and related trade-offs. To address this, we present mmm-fair, an open-source
toolkit leveraging boosting-based ensemble approaches that dynamically
optimizes model weights to jointly minimize classification errors and diverse
fairness violations, enabling flexible multi-objective optimization. The system
empowers users to deploy models that align with their context-specific needs
while reliably uncovering intersectional biases often missed by
state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth
multi-attribute fairness, multi-objective optimization, a no-code, chat-based
interface, LLM-powered explanations, interactive Pareto exploration for model
selection, custom fairness constraint definition, and deployment-ready models
in a single open-source toolkit, a combination rarely found in existing
fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.

</details>


### [14] [Selective Induction Heads: How Transformers Select Causal Structures In Context](https://arxiv.org/abs/2509.08184)
*Francesco D'Angelo,Francesco Croce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 변형기(Transformers)는 자기 주의(self-attention)와 문맥 학습을 활용한 시퀀스 모델링 작업에서 뛰어난 능력을 보여주었으며, 본 연구에서는 변화하는 인과 구조를 동적으로 처리하는 새로운 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 본 연구의 목적은 변형기가 문맥에 맞는 인과 구조를 동적으로 처리하는 능력을 보여주기 위한 새로운 프레임워크를 제시하는 것이다.

Method: 고정된 전이 확률을 유지하면서 다양한 시간 지연이 있는 겹쳐진 마르코프 체인을 통해 인과 구조를 변화시키는 방식으로 선택적 유도 헤드를 구현한다.

Result: 변형기가 올바른 시간 지연을 식별하고 과거에서 해당 토큰을 복사하여 다음 토큰을 예측하기 위한 메커니즘을 학습한다는 것을 경험적으로 입증하였다.

Conclusion: 변형기가 인과 구조를 선택하는 방식에 대한 이해를 증진시키며, 기능과 해석 가능성에 대한 새로운 통찰을 제공한다.

Abstract: Transformers have exhibited exceptional capabilities in sequence modeling
tasks, leveraging self-attention and in-context learning. Critical to this
success are induction heads, attention circuits that enable copying tokens
based on their previous occurrences. In this work, we introduce a novel
framework that showcases transformers' ability to dynamically handle causal
structures. Existing works rely on Markov Chains to study the formation of
induction heads, revealing how transformers capture causal dependencies and
learn transition probabilities in-context. However, they rely on a fixed causal
structure that fails to capture the complexity of natural languages, where the
relationship between tokens dynamically changes with context. To this end, our
framework varies the causal structure through interleaved Markov chains with
different lags while keeping the transition probabilities fixed. This setting
unveils the formation of Selective Induction Heads, a new circuit that endows
transformers with the ability to select the correct causal structure
in-context. We empirically demonstrate that transformers learn this mechanism
to predict the next token by identifying the correct lag and copying the
corresponding token from the past. We provide a detailed construction of a
3-layer transformer to implement the selective induction head, and a
theoretical analysis proving that this mechanism asymptotically converges to
the maximum likelihood solution. Our findings advance the understanding of how
transformers select causal structures, providing new insights into their
functioning and interpretability.

</details>


### [15] [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)
*Jeffrey Amico,Gabriel Passamani Andrade,John Donaghy,Ben Fielding,Tristin Forbus,Harry Grieve,Semih Kara,Jari Kolehmainen,Yihua Lou,Christopher Nies,Edward Phillip Flores Nuño,Diogo Ortega,Shikhar Rastogi,Austin Virts,Matthew J. Wright*

Main category: cs.LG

TL;DR: 이 논문에서는 분산형 강화 학습 알고리즘인 Swarm sAmpling Policy Optimization(SAPO)를 제안하여, 비지도 세부 조정 없이도 학습된 언어 모델의 복잡한 추론 능력을 향상시키는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 추론에 필요한 RL의 효과적인 활용이 높은 비용과 기술적 문제를 동반하며, 이러한 문제를 해결하기 위한 새로운 접근법이 필요합니다.

Method: SAPO는 이질적인 컴퓨트 노드의 분산 네트워크를 위한 비동기 RL 후 훈련 알고리즘으로, 각 노드는 자신의 정책 모델을 관리하며 서로 '공유'하는 방식으로 진행됩니다.

Result: SAPO는 제어된 실험에서 최대 94%의 누적 보상 증가를 달성하였으며, 수천 개의 노드가 참여한 테스트 결과 통찰을 공유합니다.

Conclusion: SAPO는 RL 후 훈련의 공통 병목 현상을 피하고 새로운 가능성을 열어주며, 학습 프로세스를 부팅스트랩하는 데 기여합니다.

Abstract: Post-training language models (LMs) with reinforcement learning (RL) can
enhance their complex reasoning capabilities without supervised fine-tuning, as
demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs
requires significant parallelization to scale-up inference, which introduces
non-trivial technical challenges (e.g. latency, memory, and reliability)
alongside ever-growing financial costs. We present Swarm sAmpling Policy
Optimization (SAPO), a fully decentralized and asynchronous RL post-training
algorithm. SAPO is designed for decentralized networks of heterogenous compute
nodes, where each node manages its own policy model(s) while "sharing" rollouts
with others in the network; no explicit assumptions about latency, model
homogeneity, or hardware are required and nodes can operate in silo if desired.
As a result, the algorithm avoids common bottlenecks in scaling RL
post-training while also allowing (and even encouraging) new possibilities. By
sampling rollouts "shared" across the network, it enables "Aha moments" to
propagate, thereby bootstrapping the learning process. In this paper we show
SAPO achieved cumulative reward gains of up to 94% in controlled experiments.
We also share insights from tests on a network with thousands of nodes
contributed by Gensyn community members running the algorithm on diverse
hardware and models during an open-source demo.

</details>


### [16] [Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition](https://arxiv.org/abs/2509.08225)
*Matthew Nolan,Lina Yao,Robert Davidson*

Main category: cs.LG

TL;DR: 인간 활동 인식(HAR)은 딥러닝 기술의 채택으로 발전했지만 여전히 데이터 요구사항과 신뢰성 및 강건성 문제에 직면해 있다. 이 논문은 이러한 문제를 극복하기 위해 자기 지도 학습 프레임워크 내에서 앙상블 분포 증류(EDD)의 새로운 응용 프로그램을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: HAR 분야에서 데이터 요구사항과 신뢰성, 강건성 문제를 해결하기 위해.

Method: 무라벨 데이터와 부분 지도 학습 전략을 활용한 자기 지도 학습 프레임워크에서 EDD 응용.

Result: 예측 정확성 향상, 불확실성의 강력한 추정, 적대적 교란에 대한 강건성 증가.

Conclusion: 계산 복잡도를 증가시키지 않으면서 실제 시나리오의 신뢰성을 크게 향상시킴을 입증하였다.

Abstract: Human Activity Recognition (HAR) has seen significant advancements with the
adoption of deep learning techniques, yet challenges remain in terms of data
requirements, reliability and robustness. This paper explores a novel
application of Ensemble Distribution Distillation (EDD) within a
self-supervised learning framework for HAR aimed at overcoming these
challenges. By leveraging unlabeled data and a partially supervised training
strategy, our approach yields an increase in predictive accuracy, robust
estimates of uncertainty, and substantial increases in robustness against
adversarial perturbation; thereby significantly improving reliability in
real-world scenarios without increasing computational complexity at inference.
We demonstrate this with an evaluation on several publicly available datasets.
The contributions of this work include the development of a self-supervised EDD
framework, an innovative data augmentation technique designed for HAR, and
empirical validation of the proposed method's effectiveness in increasing
robustness and reliability.

</details>


### [17] [Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization](https://arxiv.org/abs/2509.08233)
*Kai Yi*

Main category: cs.LG

TL;DR: 이 논문은 분산 및 연합 학습의 통신 효율성을 개선하기 위한 다양한 전략을 탐구하며, 모델 압축, 로컬 학습 및 개인화에 초점을 맞춘다.


<details>
  <summary>Details</summary>
Motivation: 모델을 분산된 데이터 소스에서 학습하면서 개인 정보 보호를 유지하기 위한 필수 패러다임으로, 통신 오버헤드가 주요 병목현상으로 남아 있다.

Method: 편향적 및 비편향적 압축 연산자에 대한 수렴 보장이 있는 통합 프레임워크를 수립하고 개인화를 포함한 적응형 로컬 학습 전략을 제안하며, 여기에 Scafflix와 SymWanda와 같은 방법을 포함한다.

Result: Scafflix는 IID 및 비IID 설정 모두에서 우수한 성능을 달성하여 글로벌 목표와 개인화된 목표의 균형을 맞추고, Cohort-Squeeze는 계층적 집계를 활용하여 장치 간 오버헤드를 줄인다.

Conclusion: 광범위한 실험을 통해 정확성, 수렴, 통신 간의 유리한 균형을 보여주며, 확장 가능하고 효율적인 분산 학습에 대한 이론적 및 실무적 통찰력을 제공한다.

Abstract: Distributed and federated learning are essential paradigms for training
models across decentralized data sources while preserving privacy, yet
communication overhead remains a major bottleneck. This dissertation explores
strategies to improve communication efficiency, focusing on model compression,
local training, and personalization. We establish a unified framework for
biased and unbiased compression operators with convergence guarantees, then
propose adaptive local training strategies that incorporate personalization to
accelerate convergence and mitigate client drift. In particular, Scafflix
balances global and personalized objectives, achieving superior performance
under both IID and non-IID settings. We further introduce privacy-preserving
pruning frameworks that optimize sparsity while minimizing communication costs,
with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device
overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances
robustness under high sparsity and maintains accuracy without retraining.
Extensive experiments on benchmarks and large-scale language models demonstrate
favorable trade-offs among accuracy, convergence, and communication, offering
theoretical and practical insights for scalable, efficient distributed
learning.

</details>


### [18] [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383)
*Matan Avitan,Moran Baruch,Nir Drucker,Itamar Zimerman,Yoav Goldberg*

Main category: cs.LG

TL;DR: 대형 언어 모델은 현대 AI 애플리케이션의 핵심이지만, 신뢰할 수 없는 서버에서 민감한 데이터를 처리하는 것은 개인 정보 보호 문제를 야기합니다. 본 논문에서는 동형 암호화와 함께 사용할 수 있는 새로운 알고리즘인 cutmax를 제안하여 암호화된 데이터에 대한 효율적인 디코딩을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 없는 서버에서 민감한 데이터를 처리할 때 발생하는 개인 정보 보호 문제를 해결하고자 한다.

Method: cutmax라는 HE 친화적 argmax 알고리즘과 HE 호환 핵 샘플링 방법을 제안하여 암호화된 데이터에 대한 계산을 효율적으로 수행하게 한다.

Result: cutmax는 기존 방법보다 더 적은 암호문 작업을 수행하여 대칭 디코딩을 가능하게 하며, HE 호환 핵 샘플링 방법은 효율적인 확률적 디코딩을 제공합니다.

Conclusion: 기존 기술에 비해 24배에서 35배까지 지연 시간을 줄여 안전한 텍스트 생성을 발전시켰습니다.

Abstract: Large language models (LLMs) power modern AI applications, but processing
sensitive data on untrusted servers raises privacy concerns. Homomorphic
encryption (HE) enables computation on encrypted data for secure inference.
However, neural text generation requires decoding methods like argmax and
sampling, which are non-polynomial and thus computationally expensive under
encryption, creating a significant performance bottleneck. We introduce cutmax,
an HE-friendly argmax algorithm that reduces ciphertext operations compared to
prior methods, enabling practical greedy decoding under encryption. We also
propose the first HE-compatible nucleus (top-p) sampling method, leveraging
cutmax for efficient stochastic decoding with provable privacy guarantees. Both
techniques are polynomial, supporting efficient inference in privacy-preserving
settings. Moreover, their differentiability facilitates gradient-based
sequence-level optimization as a polynomial alternative to straight-through
estimators. We further provide strong theoretical guarantees for cutmax,
proving it converges globally to a unique two-level fixed point, independent of
the input values beyond the identity of the maximizer, which explains its rapid
convergence in just a few iterations. Evaluations on realistic LLM outputs show
latency reductions of 24x-35x over baselines, advancing secure text generation.

</details>


### [19] [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](https://arxiv.org/abs/2509.08578)
*Hong Liu*

Main category: cs.LG

TL;DR: MAESTRO는 인플루엔자 발생 예측을 위한 다중 모달 적응 앙상블 모델로, 다양한 데이터 소스를 통합하여 예측의 강건성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 공공 건강 의사 결정의 원활한 진행을 위해 신속하고 강건한 인플루엔자 발생 예측이 중요하다.

Method: MAESTRO는 감시, 웹 검색 트렌드, 기상 데이터를 포함한 다중 모달 입력을 적응적으로 융합하고, 시간-주파수 아키텍처를 활용하여 예측을 수행한다.

Result: MAESTRO는 홍콩의 11년 이상의 인플루엔자 데이터를 기반으로 평가되었으며, R-square 0.956의 최첨단 성능을 달성하였다.

Conclusion: 우리는 선진적인 시간-주파수 모델링과 다중 모달 데이터 융합의 중요성을 입증하는 강력하고 통합된 프레임워크를 제공한다.

Abstract: Timely and robust influenza incidence forecasting is critical for public
health decision-making. To address this, we present MAESTRO, a Multi-modal
Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves
robustness by adaptively fusing multi-modal inputs-including surveillance, web
search trends, and meteorological data-and leveraging a comprehensive
spectro-temporal architecture. The model first decomposes time series into
seasonal and trend components. These are then processed through a hybrid
feature enhancement pipeline combining Transformer-based encoders, a Mamba
state-space model for long-range dependencies, multi-scale temporal
convolutions, and a frequency-domain analysis module. A cross-channel attention
mechanism further integrates information across the different data modalities.
Finally, a temporal projection head performs sequence-to-sequence forecasting,
with an optional estimator to quantify prediction uncertainty. Evaluated on
over 11 years of Hong Kong influenza data (excluding the COVID-19 period),
MAESTRO shows strong competitive performance, demonstrating a superior model
fit and relative accuracy, achieving a state-of-the-art R-square of 0.956.
Extensive ablations confirm the significant contributions of both multi-modal
fusion and the spectro-temporal components. Our modular and reproducible
pipeline is made publicly available to facilitate deployment and extension to
other regions and pathogens.Our publicly available pipeline presents a
powerful, unified framework, demonstrating the critical synergy of advanced
spectro-temporal modeling and multi-modal data fusion for robust
epidemiological forecasting.

</details>


### [20] [ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System](https://arxiv.org/abs/2509.08736)
*Dong Han,Zhehong Ai,Pengxiang Cai,Shuzhou Sun,Shanya Lu,Jianpeng Chen,Ben Gao,Lingli Ge,Weida Wang,Xiangxin Zhou,Xihui Liu,Mao Su,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Tao XU,Yuqiang Li,Shufei Zhang*

Main category: cs.LG

TL;DR: ChemBOMAS는 화학에서 베이지안 최적화를 가속화하기 위한 새로운 프레임워크로, LLM에 의해 최적화 과정이 향상된다. 이는 지식 기반의 거칠게 조정된 최적화와 데이터 기반의 미세 조정 최적화를 결합한다.


<details>
  <summary>Details</summary>
Motivation: 화학에서의 베이지안 최적화 효율성은 희소한 실험 데이터와 복잡한 반응 메커니즘에 의해 저해된다.

Method: ChemBOMAS는 LLM을 활용하여 거칠고 세밀한 최적화 단계에서 기존 화학 지식을 바탕으로 후보 영역을 식별하고, 해당 영역 내에서 가짜 데이터 포인트를 생성하여 최적화 프로세스를 개선한다.

Result: 화학 산업 프로토콜에 따라 수행된 실험에서 ChemBOMAS는 96%의 최적 목표 값을 달성하며, 이는 전문가들이 도달한 15%보다 현저히 높은 수치이다. 이로써 ChemBOMAS는 다양한 BO 알고리즘보다 최적화 효율성과 효과성을 크게 향상시킨다.

Conclusion: ChemBOMAS는 화학 발견을 가속화하는 강력한 도구로 자리잡았다.

Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered
by sparse experimental data and complex reaction mechanisms. To overcome these
limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced
Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization
process is enhanced by LLMs and synergistically employs two strategies:
knowledge-driven coarse-grained optimization and data-driven fine-grained
optimization. First, in the knowledge-driven coarse-grained optimization stage,
LLMs intelligently decompose the vast search space by reasoning over existing
chemical knowledge to identify promising candidate regions. Subsequently, in
the data-driven fine-grained optimization stage, LLMs enhance the BO process
within these candidate regions by generating pseudo-data points, thereby
improving data utilization efficiency and accelerating convergence. Benchmark
evaluations** further confirm that ChemBOMAS significantly enhances
optimization effectiveness and efficiency compared to various BO algorithms.
Importantly, the practical utility of ChemBOMAS was validated through wet-lab
experiments conducted under pharmaceutical industry protocols, targeting
conditional optimization for a previously unreported and challenging chemical
reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value
of 96%. This was substantially higher than the 15% achieved by domain experts.
This real-world success, together with strong performance on benchmark
evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical
discovery.

</details>


### [21] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 자율 LLM 에이전트를 개발하는 것은 복잡한 실제 작업을 해결하기 위한 지능적인 결정을 시리즈로 내리는 것으로 빠르게 진화하는 분야이다. 이 논문에서는 AgentGym-RL이라는 새로운 프레임워크를 소개하여 다중 턴의 상호작용 결정-making을 위한 LLM 에이전트의 학습을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 상호작용 강화 학습 프레임워크의 부재로 인해 다양한 환경에서 자율 LLM 에이전트를 효과적으로 학습시키는 데 한계가 있다.

Method: AgentGym-RL 프레임워크는 모듈화되고 분리된 아키텍처를 특징으로 하며, ScalingInter-RL 접근법은 탐색-착취 균형과 안정적인 RL 최적화를 위해 설계되었다.

Result: 실험을 통해 AgentGym-RL 프레임워크와 ScalingInter-RL 접근법의 안정성과 효과성을 검증하였고, 에이전트는 다양한 환경에서 27개의 작업에서 상업 모델을 능가하거나 동등한 성과를 보였다.

Conclusion: 이 연구는 연구 커뮤니티가 다음 세대 지능형 에이전트를 개발할 수 있도록 AgentGym-RL 프레임워크를 오픈소스로 제공할 예정이다.

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [22] [Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform](https://arxiv.org/abs/2509.08756)
*Zhaoxun "Lorenz" Liu,Wagner H. Souza,Jay Han,Amin Madani*

Main category: cs.LG

TL;DR: 본 연구는 대규모 사고 발생 시의 환자 병원 배정을 최적화하기 위한 심층 강화 학습 기반의 결정 지원 AI 에이전트를 개발하고 검증하였다.


<details>
  <summary>Details</summary>
Motivation: 대규모 사고 사건(MCI)은 의료 시스템을 압도하고 극한의 압박 속에서 빠르고 정확한 환자-병원 배정 결정을 요구한다.

Method: 환자의 중증도, 전문 치료 요건, 병원 용량 및 운송 물류의 균형을 맞추어 시뮬레이션된 MCI 동안 환자 전송 결정을 최적화하기 위해 심층 강화 학습 기반의 결정 지원 AI 에이전트를 개발하고 검증하였다.

Result: AI의 참여를 증가시키면 결정의 질과 일관성이 유의미하게 개선되며, AI 에이전트는 외상 외과의사보다 성능이 우수하고 비전문가가 전문가 수준의 성과를 달성할 수 있도록 지원한다.

Conclusion: AI 중심의 결정 지원이 MCI 대비 훈련과 실제 응급 대응 관리 모두를 향상시킬 잠재력을 마련하였다.

Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,
accurate patient-hospital allocation decisions under extreme pressure. Here, we
developed and validated a deep reinforcement learning-based decision-support AI
agent to optimize patient transfer decisions during simulated MCIs by balancing
patient acuity levels, specialized care requirements, hospital capacities, and
transport logistics. To integrate this AI agent, we developed MasTER, a
web-accessible command dashboard for MCI management simulations. Through a
controlled user study with 30 participants (6 trauma experts and 24
non-experts), we evaluated three interaction approaches with the AI agent
(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI
scenarios in the Greater Toronto Area. Results demonstrate that increasing AI
involvement significantly improves decision quality and consistency. The AI
agent outperforms trauma surgeons (p < 0.001) and enables non-experts to
achieve expert-level performance when assisted, contrasting sharply with their
significantly inferior unassisted performance (p < 0.001). These findings
establish the potential for our AI-driven decision support to enhance both MCI
preparedness training and real-world emergency response management.

</details>


### [23] [ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals](https://arxiv.org/abs/2509.08779)
*Ali Amini,Mohammad Alijanpour,Behnam Latifi,Ali Motie Nasrabadi*

Main category: cs.LG

TL;DR: 이 논문은 ADHD 진단의 정확성과 신속성을 향상시키기 위한 새로운 방법을 제시하며, 딥러닝과 뇌파 신호를 활용합니다.


<details>
  <summary>Details</summary>
Motivation: ADHD는 어린이에게 흔한 뇌 장애로 사회적, 학문적, 직업적 삶에 영향을 미치며, 조기 진단이 중요하지만 노동집약적이고 시간이 많이 소요됩니다.

Method: ADHDeepNet라는 딥러닝 모델을 소개하며, EEG 신호에 최적화된 시간-공간 특성화, 주의 모듈, 설명 가능성 기술을 활용합니다.

Result: ADHDeepNet은 ADHD/건강한 대조군을 100% 민감도와 99.17% 정확도로 분류했습니다.

Conclusion: 이 연구는 ADHD 진단의 정확성과 효율성을 향상시키는 데 있어 딥러닝과 EEG의 가능성을 강조합니다.

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in
children that can persist into adulthood, affecting social, academic, and
career life. Early diagnosis is crucial for managing these impacts on patients
and the healthcare system but is often labor-intensive and time-consuming. This
paper presents a novel method to improve ADHD diagnosis precision and
timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram
(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive
temporal-spatial characterization, attention modules, and explainability
techniques optimized for EEG signals. ADHDeepNet integrates feature extraction
and refinement processes to enhance ADHD diagnosis. The model was trained and
validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),
employing nested cross-validation for robust performance. The proposed
two-stage methodology uses a 10-fold cross-subject validation strategy.
Initially, each iteration optimizes the model's hyper-parameters with inner
2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various
standard deviations and magnification levels is applied for data augmentation.
ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC
subjects. To clarify model explainability and identify key brain regions and
frequency bands for ADHD diagnosis, we analyzed the learned weights and
activation patterns of the model's primary layers. Additionally, t-distributed
Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding
in interpreting the model's decisions. This study highlights the potential of
DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Overcoming DNSSEC Islands of Security: A TLS and IP-Based Certificate Solution](https://arxiv.org/abs/2509.08364)
*Aduma Rishith,Aditya Kulkarni,Tamal Das,Vivek Balachandran*

Main category: cs.CR

TL;DR: 이 논문에서는 DNSSEC의 신뢰 체인에서 발생하는 격차, 즉 '안전의 섬' 문제를 분산된 접근 방식으로 해결하고자 한다. TLS 및 IP 기반 인증서를 활용하여 계층 간의 종단 간 인증을 가능하게 하여 DNSSEC의 전반적인 무결성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: DNS의 무결성을 강화하는 다양한 방법이 필요하며, 특히 DNSSEC의 한계인 '안전의 섬' 문제를 해결해야 한다.

Method: TLS 및 IP 기반 인증서를 사용하여 계층 간에 종단 간 인증을 보장하는 분산된 방법을 제안한다.

Result: 제안된 방법은 DNSSEC의 무결성을 향상시키고 인증서 서명을 유지하기 위한 레지스트라에 대한 의존도를 줄인다.

Conclusion: 우리의 방법은 DNS 보안을 강화하고 다양한 환경에서의 배포를 간소화하는 더 유연하고 효율적인 솔루션을 제공한다.

Abstract: The Domain Name System (DNS) serves as the backbone of the Internet,
primarily translating domain names to IP addresses. Over time, various
enhancements have been introduced to strengthen the integrity of DNS. Among
these, DNSSEC stands out as a leading cryptographic solution. It protects
against attacks (such as DNS spoofing) by establishing a chain of trust
throughout the DNS nameserver hierarchy. However, DNSSEC's effectiveness is
compromised when there is a break in this chain, resulting in "Islands of
Security", where domains can authenticate locally but not across hierarchical
levels, leading to a loss of trust and validation between them. Leading
approaches to addressing these issues were centralized, with a single authority
maintaining some kind of bulletin board. This approach requires significantly
more infrastructure and places excessive trust in the entity responsible for
managing it properly. In this paper, we propose a decentralized approach to
addressing gaps in DNSSEC's chain of trust, commonly referred to as "Islands of
Security". We leverage TLS and IP-based certificates to enable end-to-end
authentication between hierarchical levels, eliminating the need for uniform
DNSSEC deployment across every level of the DNS hierarchy. This approach
enhances the overall integrity of DNSSEC, while reducing dependence on
registrars for maintaining signature records to verify the child nameserver's
authenticity. By offering a more flexible and efficient solution, our method
strengthens DNS security and streamlines deployment across diverse
environments.

</details>


### [25] [Leveraging Blockchain and Proxy Re-Encryption to secure Medical IoT Records](https://arxiv.org/abs/2509.08402)
*Abdou-Essamad Jabri,C. Drocourt,Mostafa Azizi,Gil Utard*

Main category: cs.CR

TL;DR: IoT 기반 의료에서의 데이터 공유를 위해 개인 블록체인과 Proxy Re-Encryption (PRE) 기법을 통합한 보안 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 의료 데이터의 전송 및 저장 과정에서 발생하는 보안 및 개인 정보 보호 문제를 해결하기 위해.

Method: 개인 블록체인과 Proxy Re-Encryption을 통합한 아키텍처를 제안.

Result: 안전하고 추적 가능하며 개인 정보를 보호하는 데이터 공유가 가능해진다.

Conclusion: 이 조합은 디지털 의료 생태계에서 신뢰와 효율성을 향상시키는 강력한 보안 프레임워크를 구축한다.

Abstract: The integration of the Internet of Things (IoT) in healthcare has
revolutionized patient monitoring and data collection, allowing real-time
tracking of vital signs, remote diagnostics, and automated medical responses.
However, the transmission and storage of sensitive medical data introduce
significant security and privacy challenges. To address these concerns,
blockchain technology provides a decentralized and immutable ledger that
ensures data integrity, , and transparency. Unlike public blockchains, private
blockchains are permissioned; the access is granted only to authorized
participants; they are more suitable for handling confidential healthcare data.
Although blockchain ensures security and trust, it lacks built-in mechanisms to
support flexible and controlled data sharing; This is where Proxy Re-Encryption
(PRE) comes into play. PRE is a cryptographic technique that allows encrypted
data to be re-encrypted for a new recipient without exposing it to
intermediaries. We propose an architecture integrating private blockchain and
PRE to enable secure, traceable, and privacy-preserving data sharing in
IoT-based healthcare systems. Blockchain guarantees tamper proof
record-keeping, while PRE enables fine-grained access control, allowing medical
professionals to securely share patient data without compromising
confidentiality. This combination creates a robust security framework that
enhances trust and efficiency in digital healthcare ecosystems.

</details>


### [26] [Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations](https://arxiv.org/abs/2509.08646)
*Ron F. Del Rosario,Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.CR

TL;DR: 이 논문은 대규모 언어 모델(LLM) 에이전트를 위한 강력하고 안전하며 예측 가능한 아키텍처 패턴을 제시하며, '계획-실행' 패턴의 설계 원칙과 보안 의의를 강조한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트가 점점 더 복잡한 다단계 작업을 자동화할 수 있게 되면서, 강력하고 안전하며 예측 가능한 아키텍처 패턴에 대한 필요성이 커지고 있다.

Method: 본 논문은 전략적 계획과 전술적 실행을 분리하는 에이전틱 디자인인 '계획-실행' (P-t-E) 패턴에 대한 포괄적인 가이드를 제공한다. P-t-E의 핵심 구성 요소인 Planner(계획자)와 Executor(실행자)를 상세히 설명하고, 예측 가능성, 비용 효율성, 그리고 반응형 패턴보다 우수한 추론 품질을 강조한다.

Result: 이 설계는 간접적인 프롬프트 주입 공격에 대한 내재적인 회복력을 갖추고 있으며, 이를 통해 제어 흐름 무결성을 확립하여 보안 문제를 해결한다. 또한, P-t-E 패턴을 구현하기 위해 LangChain, CrewAI, AutoGen과 같은 에이전틱 프레임워크의 핵심 기능을 분석하고, 각 프레임워크의 적용 방식을 비교한다.

Conclusion: 마지막으로, 이 논문은 동적 재계획 루프, Directed Acyclic Graphs(DAG)를 이용한 병렬 실행, 및 Human-in-the-Loop(HITL) 검증의 중요성을 포함한 고급 패턴을 논의하여 생산 품질의 회복력 있는 LLM 에이전트를 구축하려는 아키텍트, 개발자 및 보안 엔지니어를 위한 완벽한 전략적 청사진을 제공한다.

Abstract: As Large Language Model (LLM) agents become increasingly capable of
automating complex, multi-step tasks, the need for robust, secure, and
predictable architectural patterns is paramount. This paper provides a
comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic
design that separates strategic planning from tactical execution. We explore
the foundational principles of P-t-E, detailing its core components - the
Planner and the Executor - and its architectural advantages in predictability,
cost-efficiency, and reasoning quality over reactive patterns like ReAct
(Reason + Act). A central focus is placed on the security implications of this
design, particularly its inherent resilience to indirect prompt injection
attacks by establishing control-flow integrity. We argue that while P-t-E
provides a strong foundation, a defense-in-depth strategy is necessary, and we
detail essential complementary controls such as the Principle of Least
Privilege, task-scoped tool access, and sandboxed code execution. To make these
principles actionable, this guide provides detailed implementation blueprints
and working code references for three leading agentic frameworks: LangChain
(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing
the P-t-E pattern is analyzed, highlighting unique features like LangGraph's
stateful graphs for re-planning, CrewAI's declarative tool scoping for
security, and AutoGen's built-in Docker sandboxing. Finally, we discuss
advanced patterns, including dynamic re-planning loops, parallel execution with
Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop
(HITL) verification, to offer a complete strategic blueprint for architects,
developers, and security engineers aiming to build production-grade, resilient,
and trustworthy LLM agents.

</details>


### [27] [Stealth by Conformity: Evading Robust Aggregation through Adaptive Poisoning](https://arxiv.org/abs/2509.08746)
*Ryan McGaughey,Jesus Martinez del Rincon,Ihsen Alouani*

Main category: cs.CR

TL;DR: 이 논문은 연합 학습에서 악의적인 업데이트가 주 분포 내에 있을 수 있음을 보여주고, 이를 이용한 새로운 공격 전략인 Chameleon Poisoning (CHAMP)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습은 개인정보 보호 문제를 해결하기 위한 분산 학습 패러다임이지만, 포이즈닝 공격에 취약하다.

Method: Chameleon Poisoning (CHAMP)이라는 적응형 포이즈닝 전략을 제안하며, 집계 과정의 사이드채널 피드백을 활용하여 공격을 유도한다.

Result: CHAMP는 아홉 가지 강력한 집계 방어체계에 대해 공격 성공률을 평균 47.07% 증가시키는 효과를 보였다.

Conclusion: 기존의 강력한 집계 방어체계의 근본적인 한계를 강조하고, 정교한 적대자에 대한 연합 학습 보호를 위한 새로운 전략의 필요성을 부각시킨다.

Abstract: Federated Learning (FL) is a distributed learning paradigm designed to
address privacy concerns. However, FL is vulnerable to poisoning attacks, where
Byzantine clients compromise the integrity of the global model by submitting
malicious updates. Robust aggregation methods have been widely adopted to
mitigate such threats, relying on the core assumption that malicious updates
are inherently out-of-distribution and can therefore be identified and excluded
before aggregating client updates. In this paper, we challenge this underlying
assumption by showing that a model can be poisoned while keeping malicious
updates within the main distribution. We propose Chameleon Poisoning (CHAMP),
an adaptive and evasive poisoning strategy that exploits side-channel feedback
from the aggregation process to guide the attack. Specifically, the adversary
continuously infers whether its malicious contribution has been incorporated
into the global model and adapts accordingly. This enables a dynamic adjustment
of the local loss function, balancing a malicious component with a camouflaging
component, thereby increasing the effectiveness of the poisoning while evading
robust aggregation defenses. CHAMP enables more effective and evasive
poisoning, highlighting a fundamental limitation of existing robust aggregation
defenses and underscoring the need for new strategies to secure federated
learning against sophisticated adversaries. Our approach is evaluated in two
datasets reaching an average increase of 47.07% in attack success rate against
nine robust aggregation defenses.

</details>


### [28] [Prototype-Guided Robust Learning against Backdoor Attacks](https://arxiv.org/abs/2509.08748)
*Wei Guo,Maura Pintor,Ambra Demontis,Battista Biggio*

Main category: cs.CR

TL;DR: 본 논문에서는 다양한 백도어 공격에 견고한 방어를 제공하는 Prototype-Guided Robust Learning(PGRL)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 백도어 공격은 훈련 데이터를 오염시켜 모델에 백도어를 내장하고, 정상적인 입력에 대해선 정상적으로 작동하지만 특정 트리거 신호가 나타나면 악의적으로 행동하게 만듭니다. 이러한 오염된 데이터셋으로부터 건전한 모델을 훈련하는 것은 매우 어렵습니다.

Method: PGRL은 소량의 건전한 샘플을 활용하여 프로토타입 벡터를 생성하고 이를 통해 훈련 과정을 안내합니다.

Result: PGRL은 8개의 기존 방어 기술과 비교하여 뛰어난 견고성을 보여주며, 다양한 아키텍처, 데이터셋, 고급 공격에 대해서도 잘 일반화됨을 입증합니다.

Conclusion: PGRL은 공격자가 방어의 세부사항을 완전히 알고 있는 최악의 경우 시나리오에서 적응형 공격을 수행하여 평가됩니다.

Abstract: Backdoor attacks poison the training data to embed a backdoor in the model,
causing it to behave normally on legitimate inputs but maliciously when
specific trigger signals appear. Training a benign model from a dataset
poisoned by backdoor attacks is challenging. Existing works rely on various
assumptions and can only defend against backdoor attacks with specific trigger
signals, high poisoning ratios, or when the defender possesses a large,
untainted, validation dataset. In this paper, we propose a defense called
Prototype-Guided Robust Learning (PGRL), which overcomes all the aforementioned
limitations, being robust against diverse backdoor attacks. Leveraging a tiny
set of benign samples, PGRL generates prototype vectors to guide the training
process. We compare our PGRL with 8 existing defenses, showing that it achieves
superior robustness. We also demonstrate that PGRL generalizes well across
various architectures, datasets, and advanced attacks. Finally, to evaluate our
PGRL in the worst-case scenario, we perform an adaptive attack, where the
attackers fully know the details of the defense.

</details>
