<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE는 LLM 에이전트의 최적화를 위한 통합 프레임워크로, 다음 작업 관련성 모델링, 계획 구조 분석, 지침 공동 수정 및 기능 보존 압축을 통해 에이전트의 정확도를 향상시키는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 다단계 워크플로우에서 LLM 에이전트를 효과적으로 활용하기 위한 압축 및 요약 방법의 필요성.

Method: PAACE는 다음 k 작업 관련성 모델링, 계획 구조 분석, 지침 공동 수정 및 기능 보존 압축을 통해 LLM 에이전트의 진화하는 상태를 최적화하기 위한 통합 프레임워크입니다.

Result: 실험 결과, PAACE는 에이전트의 정확도를 일관되게 향상시키면서도 컨텍스트 부하를 상당히 줄였음을 보여줍니다.

Conclusion: PAACE-FT는 튜터의 성능의 97%를 유지하면서도 추론 비용을 10배 이상 줄여, 컴팩트한 모델로 계획 인식 압축의 실용적인 배포를 가능하게 합니다.

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [2] [Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats](https://arxiv.org/abs/2512.17041)
*Ali Eslami,Jiangbo Yu*

Main category: cs.AI

TL;DR: 이 논문은 에이전틱 AI가 포함된 차량의 보안 위협을 조사하고, 역할 기반 아키텍처를 통해 취약성을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI가 포함된 차량은 기존의 플랫폼에서 보안 리스크가 없거나 충분히 고려되지 않고 있어 이 문제를 해결할 필요가 있다.

Method: 역할 기반 아키텍처를 도입하여 개인 에이전트와 주행 전략 에이전트로 구성하고, 에이전틱 AI 계층과 교차 계층 리스크를 분석한다.

Result: 소규모 왜곡이 인간 주행 및 자율 주행 차량에서 잘못 정렬되거나 안전하지 않은 행동으로 확대될 수 있음을 보여준다.

Conclusion: 이 프레임워크는 현재 및 새로운 차량 플랫폼에서 에이전틱 AI의 보안 리스크를 분석하기 위한 첫 번째 구조화된 기초를 제공한다.

Abstract: Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.

</details>


### [3] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 본 논문은 현실적 위협과 상징적 위협의 상호작용을 대화형 언어 모델(LLM) 기반 에이전트를 통해 가상 사회에서 시뮬레이션함으로써 충돌의 원인을 규명하고, 이들이 어떻게 행동에 영향을 미치는지 분석한다.


<details>
  <summary>Details</summary>
Motivation: 인간의 갈등이 물질적 조건과 상징적 가치에 대한 위협으로 설명되지만, 이들 간의 상호작용과 어떤 요소가 지배적인지 명확하지 않음.

Method: 대화형 언어 모델(LLM) 기반 에이전트를 사용하여 현실적 위협과 상징적 위협을 독립적으로 변화시키는 시뮬레이션을 수행.

Result: LLM이 현실적 위협과 상징적 위협, 적대감을 별개의 내부 상태로 인코딩하며, 이를 조작함으로써 행동이 어떻게 변화하는지를 보여줌.

Conclusion: 현실적 위협은 적대감을 직접 증가시키는 반면, 상징적 위협의 효과는 약하고, 내부 집단 편향에 의해 매개되며, 현실적 위협이 없을 때에만 적대감을 증가시킴.

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [4] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 우리는 AIXI 강화 학습 에이전트를 더 넓은 유틸리티 함수 클래스에 적용할 수 있도록 일반화합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트의 신념 분포에서 일부 가설이 역사에서 유한한 접두사만 예측한다는 모호성을 직면하게 됩니다.

Method: 불확실한 확률 이론에서 Choquet 적분을 사용하여 기대 유틸리티를 계산하는 결과를 고려합니다.

Result: 우리의 가장 일반적인 기대 유틸리티는 죽음 해석 하에서 Choquet 적분으로 특성화될 수 없습니다.

Conclusion: 우리는 Choquet 적분을 통해 계산할 수 있는 방법을 제안합니다.

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [5] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 이 논문은 LLM 기반의 ASP 코드 생성을 위한 새로운 접근 방식과 그 효과를 설명한다.


<details>
  <summary>Details</summary>
Motivation: 도메인 특정 언어에 대한 코드 생성을 LLM이 지원하지 못하는 문제를 해결하고자 하였다.

Method: ASP 코드 생성을 위한 solver-guided instruction-tuning의 새로운 접근 방식을 제안하였다.

Result: 커브된 데이터로 LLM을 교육하여 두 개의 데이터 세트에서 두 가지 다른 프롬프팅 설정을 통해 일관된 개선을 보여주었다.

Conclusion: 제안된 방법은 ASP 코드 생성의 복잡한 의미 파싱 작업을 효과적으로 해결할 수 있다.

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [6] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: 본 논문은 LLM 기반 에이전트의 자기 개선 능력을 향상시키기 위한 강화 학습 기반 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트는 복잡한 추론과 다중 턴 상호작용에서 뛰어난 능력을 보여주지만, 새로운 환경에 배치되었을 때 지속적으로 개선하고 적응하는 데 어려움을 겪고 있다.

Method: 우리는 SAGE(Skill Augmented GRPO for self-Evolution)라는 새로운 RL 프레임워크를 제안하여 학습에 기술을 체계적으로 통합한다. 이 프레임워크의 주요 구성 요소인 Sequential Rollout은 각 롤아웃에 대해 유사한 작업의 체인에서 에이전트를 반복적으로 배포한다.

Result: SAGE는 전문가 경험이 포함된 감독된 핀 튜닝 모델에 적용했을 때 시나리오 목표 완료율에서 8.9% 향상되었고, 26% 적은 상호작용 단계와 59% 적은 토큰을 생성하여 정확성과 효율성 모두에서 기존 접근법을 크게 초월하였다.

Conclusion: 이 연구는 기술 라이브러리를 활용하여 에이전트의 자기 개선 기능을 향상시킬 수 있는 새로운 방법론을 제시한다.

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [7] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: AI에서 불확실성 하의 추론은 중요한 도전 과제이며, 희박한 데이터 문제를 해결하기 위한 체계적인 일반화가 필요하다. 우리의 방법은 LLM이 생성한 가설을 단순성과 예측 적합성에 따라 가중치를 부여하는 솔로모프 영감을 받은 접근 방식이다.


<details>
  <summary>Details</summary>
Motivation: 불확실성 하에서의 추론은 AI의 핵심 도전 과제이며, 특히 실제 데이터가 희박할 때 체계적인 일반화가 필요하다.

Method: 기존 접근 방식의 한계를 극복하기 위해 솔로모프에 영감을 받은 방법을 제안하며, 이는 LLM이 생성한 가설에 단순성과 예측 적합성에 따라 가중치를 부여한다.

Result: 미니 ARC 벤치마크 작업에 적용했을 때, 우리의 방법은 각 셀 예측에 대한 솔로모프 가중 혼합물을 생성하여 불확실성을 인식하는 보수적인 출력을 제공한다.

Conclusion: 우리 연구는 해석 가능하고 신뢰할 수 있는 다중 가설 불확실성 추론을 위한 알고리즘 정보 이론적 priors의 가치를 강조한다.

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [8] [Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction](https://arxiv.org/abs/2512.17250)
*Ziyang Lin,Zixuan Sun,Sanhorn Chen,Xiaoyang Chen,Roy Zhao*

Main category: cs.AI

TL;DR: 본 논문에서는 TD-MPC2를 이용한 모델 기반 제어를 위한 투기 및 수정 프레임워크를 제안하며, 이를 통해 추측 실행의 예측-검증 철학을 적용하여 제어 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 실시간 순차 제어 에이전트는 추론 지연으로 인해 종종 병목 현상을 겪습니다.

Method: 사전 훈련된 세계 모델과 잠재 공간 MPC 계획기가 짧은 수평의 행동 큐와 예측된 잠재 롤아웃을 생성하여, 에이전트가 즉각적인 재계획 없이 여러 계획된 행동을 실행하도록 합니다.

Result: 우리의 방법은 계획 추론 수를 500에서 282로 줄이고, 엔드 투 엔드 단계 지연을 25% 개선하였으며, 7.1%의 수익 감소만으로 강력한 제어 성능을 유지합니다.

Conclusion: 수정 없이 추측 실행은 더 긴 수평에서는 신뢰할 수 없음을 입증하며, 강력한 지연 감소를 위해 불일치 인식 수정의 필요성을 강조합니다.

Abstract: Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.

</details>


### [9] [Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation](https://arxiv.org/abs/2512.17308)
*Daksh Jain,Aarya Jain,Ashutosh Desai,Avyakt Verma,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.AI

TL;DR: 이 연구는 포켓몬 배틀에서 전략적 의사결정의 역할을 분석하고, 대형 언어 모델이 전투 에이전트로서 기능할 수 있는지를 평가한다.


<details>
  <summary>Details</summary>
Motivation: 포켓몬 배틀은 전략적 사고를 요구하는 독특한 환경을 제공하며, 대형 언어 모델의 유용성을 평가할 수 있는 기회를 제공한다.

Method: 턴 기반의 포켓몬 배틀 시스템을 개발하였으며, LLM이 전투 상태에 따라 행동을 선택하도록 하였다.

Result: 모델 아키텍처 전반에 걸쳐 승률, 의사결정 지연, 타입 정렬 정확도, 토큰 효율성을 측정하였다.

Conclusion: LLM은 도메인 특정 훈련 없이 동적 게임 상대 역할을 수행할 수 있으며, 절차적 생성 및 적응형 난이도 시스템을 포괄하는 여러 가능성을 보여준다.

Abstract: Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.

</details>


### [10] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 인공지능이 인간의 감독 없이 원시적 경험으로부터 개념을 발견할 수 있는가? 이 논문은 개념을 정보 객체로 간주하고, 경험에 기반한 구조적 관계를 수립함으로써 개념의 존재를 검증 가능한 구조적 주장으로 환원한다.


<details>
  <summary>Details</summary>
Motivation: 인간이 발견한 개념을 인공지능이 경험으로부터 자율적으로 발견할 수 있는지를 조사하고자 한다.

Method: 개념을 에이전트의 전체 경험과의 구조적 관계로 정의하며, 가역성(consistency relation)과 과잉 정보(excess information)를 사용하여 자연스러운 분해를 평가하고, 최적화 역학을 통해 개념 간 경쟁을 설명한다.

Result: 제안된 알고리즘과 개념적 접근법을 통해 새로운 정보 구간의 등장 시 개념들이 서로를 설명하기 위해 경쟁할 수 있는 방식이 제시되었다.

Conclusion: 이 결과는 다중 에이전트 정렬 및 저비용 개념 전파를 가능하게 하여, 공유 프로토콜을 통해 개념 재구성을 지원한다.

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [11] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: Rashomon 효과가 연속적 의사결정에서 관찰된다는 것을 보였으며, 집합에서 파생된 정책이 최적 성능을 유지하면서도 검증을 위한 계산 보존을 줄일 수 있음을 입증했다.


<details>
  <summary>Details</summary>
Motivation: 연속적 의사결정에서의 Rashomon 효과를 탐구하고 이를 통해 처리의 다양성을 이해하고자 한다.

Method: 정형 검증 방법을 사용하여 각 정책의 전체 확률적 행동을 구축하고 비교한다.

Result: 연속적 의사결정에서 Rashomon 효과가 존재함을 실험으로 입증하였고, Rashomon 집합에서 구성된 앙상블이 개별 정책보다 분포 변화에 더 강한 내성을 보임을 확인하였다.

Conclusion: Rashomon 집합에서 파생된 허용 정책은 최적 성능을 유지하면서도 검증을 위한 계산 요구사항을 줄인다.

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [12] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 이 논문은 강화 학습에서 비마르코프 보상을 위한 타이밍 제약을 포함한 타이밍 보상 기계(TRMs)를 제안하며, TRMs를 통해 최적 정책을 학습하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 비마르코프 보상을 표현하는 것이 중요하지만 기존의 보상 기계는 타이밍 제약을 모델링하는 데 한계가 있다.

Method: 타이밍 자동체의 추상화를 통해 TRMs를 학습에 통합하고, TRM 구조를 활용하여 검색을 개선하는 반사실적 상상 휴리스틱을 사용한다.

Result: 실험을 통해 우리의 알고리즘이 TRM에 의해 지정된 타이밍 제약을 만족시키면서 높은 보상을 달성하는 정책을 학습하는 것을 보여준다.

Conclusion: 다양한 TRM 의미론 하에서 성능 비교 연구를 수행하고, 반사실적 상상의 이점을 강조하는 ablative 연구를 진행했다.

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [13] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: AI의 인간 유사 설계가 사용자 참여와 신뢰에 미치는 영향을 세계적인 사용자 풀을 통한 실제 인간-AI 상호작용에서 테스트하지 않은 결과, 문화에 따라 상이한 영향을 나타내는 복잡한 상호작용을 발견하였다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 인간 유사성을 평가할 때, 사용자들이 정책에서 자주 인용되는 이론적 측면보다는 상호작용적 신호에 더 집중한다는 점을 탐구하고자 하였다.

Method: 10개국에서 3,500명의 사용자를 대상으로 한 대규모 교차 국가 실험을 통해 실시간 및 개방형 상호작용을 수행하였다.

Result: 인간 유사 디자인이 인간화 현상을 유도할 수 있지만, 모든 문화에서 사용자 참여와 신뢰를 증가시키진 않음을 발견하였다.

Conclusion: AI 거버넌스에서 모든 문화에 일률적으로 적용 가능한 접근을 넘어, 문화에 따라 다르게 나타나는 인간-AI 상호작용의 복잡성을 이해할 필요가 있다.

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [14] [On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues](https://arxiv.org/abs/2512.17060)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 이 논문은 LLM 기반 에이전트가 인간 행동을 더 잘 모방하도록 돕기 위해 Transactional Analysis 이론에 영감을 받은 다중 에이전트 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재 LLM 에이전트들은 인간의 사고 패턴을 실질적으로 포착하는 심리적 깊이와 일관성이 부족하다.

Method: 제안된 시스템에서 각 에이전트는 부모(Parent), 성인(Adult), 아동(Child)이라는 세 가지 자아 상태로 나뉘며, 자아 상태는 고유한 관점과 추론 스타일을 가진 별도의 지식 구조로 처리된다. 이들은 관련 맥락 정보를 검색할 수 있는 정보 검색 메커니즘에 접근하여 응답 과정을 풍부하게 한다.

Result: 정보 검색이 있는 에이전트와 없는 에이전트를 비교한 시뮬레이션 대화 시나리오에서 진행된 소거 테스트 결과는 유망하며, 심리적으로 기반된 구조가 에이전트 행동을 풍부하게 할 수 있는 새로운 방향을 열어준다.

Conclusion: 이론적 기여는 Transactional Analysis 이론과 맥락 정보 검색을 통합하여 LLM 기반의 다중 에이전트 시뮬레이션의 현실성을 향상시키는 에이전트 아키텍처이다.

Abstract: LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.

</details>


### [15] [MAPPO-LCR: Multi-Agent Policy Optimization with Local Cooperation Reward in Spatial Public Goods Games](https://arxiv.org/abs/2512.17187)
*Zhaoqilin Yang,Axin Xiang,Kedi Yang,Tianjun Liu,Youliang Tian*

Main category: cs.MA

TL;DR: 이 연구는 공간 공공 재화 게임에 멀티 에이전트 근접 정책 최적화(MAPPO)를 도입하여 개별 수익과 집단 상호작용의 연관성을 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 상호작용 집단에서의 수익 결합 및 비정상성을 나타내는 것이 중요합니다.

Method: 멀티 에이전트 근접 정책 최적화(MAPPO)를 사용하고, 지역 협력 보상을 포함한 MAPPO-LCR을 제안합니다.

Result: 안정적인 협력 발생과 신뢰할 수 있는 수렴을 입증한 광범위한 시뮬레이션 결과를 도출했습니다.

Conclusion: MAPPO는 공간 공공 재화 게임에서 PPO에 비해 학습 우위를 보입니다.

Abstract: Spatial public goods games model collective dilemmas where individual payoffs depend on population-level strategy configurations. Most existing studies rely on evolutionary update rules or value-based reinforcement learning methods. These approaches struggle to represent payoff coupling and non-stationarity in large interacting populations. This work introduces Multi-Agent Proximal Policy Optimization (MAPPO) into spatial public goods games for the first time. In these games, individual returns are intrinsically coupled through overlapping group interactions. Proximal Policy Optimization (PPO) treats agents as independent learners and ignores this coupling during value estimation. MAPPO addresses this limitation through a centralized critic that evaluates joint strategy configurations. To study neighborhood-level cooperation signals under this framework, we propose MAPPO with Local Cooperation Reward, termed MAPPO-LCR. The local cooperation reward aligns policy updates with surrounding cooperative density without altering the original game structure. MAPPO-LCR preserves decentralized execution while enabling population-level value estimation during training. Extensive simulations demonstrate stable cooperation emergence and reliable convergence across enhancement factors. Statistical analyses further confirm the learning advantage of MAPPO over PPO in spatial public goods games.

</details>


### [16] [Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems](https://arxiv.org/abs/2512.17259)
*Abhivansh Gupta*

Main category: cs.MA

TL;DR: LLM 기반 에이전트의 자율성이 높아짐에 따라, 이들이 배포자의 의도를 어떻게 신뢰할 수 있도록 유지하는 것이 중요해졌다. 이에 따라 본 논문에서는 에이전트의 행동을 실시간으로 증명하고, 경량 감사 에이전트를 통한 의도와 행동 검증 및 고위험 작업의 인증 프로토콜을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 자율성과 다중 모달성이 증가함에 따라, 이들이 통제 가능하고 감사 가능하며 배포자의 의도에 충실해야 할 필요성이 커졌다.

Method: Verifiability-First 아키텍처는 에이전트 행위의 실시간 증명을 위한 암호화 및 상징적 방법을 통합하고, 경량 감사 에이전트를 내장하여 제약된 추론을 사용해 의도와 행동을 지속적으로 검증하며, 고위험 작업을 위한 도전-응답 인증 프로토콜을 적용한다.

Result: OPERA(Observability, Provable Execution, Red-team, Attestation)를 통해 (i) 불일치의 감지 가능성, (ii) 스텔스 전략 하에서의 탐지 시간, (iii) 적대적 프롬프트와 페르소나 주입에 대한 검증 메커니즘의 탄력성을 측정하기 위한 기준 및 평가 프로토콜을 소개한다.

Conclusion: 우리의 접근 방식은 불일치가 발생할 가능성에서 불일치를 신속하고 신뢰성 있게 탐지하고 수정하는 방향으로 평가 초점을 전환한다.

Abstract: As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [17] [MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval](https://arxiv.org/abs/2512.16962)
*Saksham Sahai Srivastava,Haoyu He*

Main category: cs.CR

TL;DR: MemoryGraft는 대형 언어 모델 에이전트의 행동을 타격하는 간접적인 공격 기법으로, 악의적인 경험을 장기 기억에 임플란트하여 에이전트의 성능을 저하시킬 수 있다는 것을 보인다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 에이전트의 경험 학습 능력이 에이전트의 자율성을 높이는 반면, 에이전트의 추론 코어와 과거 사이의 신뢰 경계라는 새로운 공격 표면이 존재한다.

Method: MemoryGraft는 성공적인 경험을 에이전트의 장기 기억에 주입하여 에이전트의 행동을 타격하는 새로운 간접적 공격 기법이다.

Result: 소수의 악성 기록이 무해한 작업에서 검색된 경험의 큰 비율을 설명할 수 있음을 확인하였다.

Conclusion: MemoryGraft는 경험 기반의 자기 개선을 은밀하고 지속적인 타격으로 바꿀 수 있는 잠재력을 가지고 있다.

Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.

</details>


### [18] [AutoDFBench 1.0: A Benchmarking Framework for Digital Forensic Tool Testing and Generated Code Evaluation](https://arxiv.org/abs/2512.16965)
*Akila Wickramasekara,Tharusha Mihiranga,Aruna Withanage,Buddhima Weerasinghe,Frank Breitinger,John Sheppard,Mark Scanlon*

Main category: cs.CR

TL;DR: AutoDFBench 1.0은 NIST의 디지털 포렌식 도구 테스트를 위한 첫 번째 자동화되고 확장 가능한 벤치마크 프레임워크로, 다양한 포렌식 작업에 대한 평가를 지원한다.


<details>
  <summary>Details</summary>
Motivation: NIST의 CFTT 프로그램은 디지털 포렌식 도구 테스트와 검증을 위한 사실상의 표준이지만, 다양한 포렌식 작업을 자동으로 벤치마크할 수 있는 포괄적인 프레임워크가 부족하다.

Method: AutoDFBench 1.0은 문자열 검색, 삭제된 파일 복구, 파일 캐빙, 윈도우 레지스트리 복구, SQLite 데이터 복구의 다섯 가지 영역을 통합한 모듈형 벤치마킹 프레임워크이다.

Result: 63개의 테스트 케이스와 10,968개의 고유 테스트 시나리오로 구성된 진실 데이터가 포함되어 있으며, RESTful API를 통해 평가를 실행하고 표준화된 메트릭을 포함한 구조화된 JSON 출력을 생성한다.

Conclusion: AutoDFBench 1.0은 도구 및 포렌식 스크립트 간의 공정하고 재현 가능한 비교를 가능하게 하며, 디지털 포렌식 도구 테스트 및 검증을 위한 최초의 통합, 자동화 및 확장 가능한 벤치마크 프레임워크를 확립한다.

Abstract: The National Institute of Standards and Technology (NIST) Computer Forensic Tool Testing (CFTT) programme has become the de facto standard for providing digital forensic tool testing and validation. However to date, no comprehensive framework exists to automate benchmarking across the diverse forensic tasks included in the programme. This gap results in inconsistent validation, challenges in comparing tools, and limited validation reproducibility. This paper introduces AutoDFBench 1.0, a modular benchmarking framework that supports the evaluation of both conventional DF tools and scripts, as well as AI-generated code and agentic approaches. The framework integrates five areas defined by the CFTT programme: string search, deleted file recovery, file carving, Windows registry recovery, and SQLite data recovery. AutoDFBench 1.0 includes ground truth data comprising of 63 test cases and 10,968 unique test scenarios, and execute evaluations through a RESTful API that produces structured JSON outputs with standardised metrics, including precision, recall, and F1~score for each test case, and the average of these F1~scores becomes the AutoDFBench Score. The benchmarking framework is validated against CFTT's datasets. The framework enables fair and reproducible comparison across tools and forensic scripts, establishing the first unified, automated, and extensible benchmarking framework for digital forensic tool testing and validation. AutoDFBench 1.0 supports tool vendors, researchers, practitioners, and standardisation bodies by facilitating transparent, reproducible, and comparable assessments of DF technologies.

</details>


### [19] [Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors](https://arxiv.org/abs/2512.17146)
*Huixin Zhan*

Main category: cs.CR

TL;DR: 본 연구에서는 유전체 기초 모델의 공격 취약성을 감사하기 위한 안전한 에이전트 유전체 평가자(SAGE)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 유전체 기초 모델(GFM)의 변이 효과 예측에서의 성공에도 불구하고, 이러한 모델의 공격 조작에 대한 보안성과 강건성은 여전히 충분히 탐구되지 않았습니다.

Method: SAGE는 설명 가능하고 자동화된 위험 감사 루프를 통해 기능하며, 부드러운 프롬프트 섭동을 주입하고, 훈련 체크포인트에서 모델 행동을 모니터링하며, AUROC 및 AUPR과 같은 위험 메트릭을 계산하고, 대규모 언어 모델 기반의 내러티브 설명이 포함된 구조화된 보고서를 생성합니다.

Result: SAGE를 사용하여 ESM2와 같은 최신 GFM조차도 의도된 부드러운 프롬프트 공격에 민감하여 성능 저하가 발생한다는 사실을 발견했습니다.

Conclusion: 이 발견은 유전체 기초 모델의 중요한 취약성으로, 임상 변이 해석과 같은 생물 의학 응용 프로그램 보안에 있어 에이전트 위험 감사의 중요성을 보여줍니다.

Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.

</details>


### [20] [Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques](https://arxiv.org/abs/2512.17411)
*Xingyu Feng*

Main category: cs.CR

TL;DR: 이 연구는 이더리움 블록체인에서 악성 및 불법 콘텐츠를 포함할 우려를 해결하기 위해 데이터 식별 및 복원 알고리즘을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 블록체인 기술의 투명하고 불변의 특성에도 불구하고, 그 분산 구조는 악성 또는 불법 콘텐츠의 포함 가능성에 대한 우려를 불러일으킨다.

Method: 우리는 데이터 식별 및 복원 알고리즘을 적용하여 175개의 일반 파일, 296개의 이미지 및 91,206개의 텍스트를 성공적으로 복구하였으며, FastText 알고리즘을 사용하여 감정 분석을 수행하고 매개변수 조정을 통해 0.9의 정확도를 달성하였다.

Result: 분류 결과, 70,189개의 중립 텍스트, 5,208개의 긍정 텍스트, 15,810개의 부정 텍스트가 식별되었으며, NSFWJS 라이브러리를 활용하여 100% 정확도로 7개의 외설 이미지를 탐지하였다.

Conclusion: 우리의 연구 결과는 이더리움 블록체인에서 유해한 콘텐츠와 무해한 콘텐츠가 공존하고 있으며, 특히 중국 정부 관료를 겨냥한 민감한 정보가 포함되어 있음을 드러낸다. 예방 조치를 제안하며, 블록체인 기술에 대한 대중의 이해와 규제 기관의 지침을 위한 귀중한 통찰을 제공한다.

Abstract: Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 강화 학습(RL)은 실제 환경에서 상호작용하는 LLM 에이전트를 훈련시키는 자연스러운 접근법으로 다시 부각되고 있다. 본 연구에서는 다중 턴 작업에서의 GRPO 알고리즘의 한계를 극복하기 위해, 안정적이고 효과적인 이점 추정 전략에 대해 조사하고, PPO를 대안으로 모색하여 다중 턴 시나리오에서의 PPO를 개선하기 위한 turn-PPO를 제안한다. 결과적으로 turn-PPO가 효과적임을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 턴 작업에서 장기 추론이 필요한 시나리오에 대해 강화 학습의 한계를 극복하고자 한다.

Method: PPO를 대안으로 탐색하고, 다중 턴 시나리오에서 사용되는 turn-PPO를 제안한다.

Result: turn-PPO가 WebShop 및 Sokoban 데이터셋에서 효과적임을 입증했다.

Conclusion: turn-PPO는 GRPO보다 다중 턴 작업에서 더 효과적인 성과를 보여준다.

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [22] [DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations](https://arxiv.org/abs/2512.17129)
*Seong Ho Pahng,Guoye Guan,Benjamin Fefferman,Sahand Hormoz*

Main category: cs.LG

TL;DR: DiffeoMorph는 에이전트 집단이 특정 3D 형태로 변화하도록 유도하는 형성발생 프로토콜을 학습하는 엔드 투 엔드 미분 가능한 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 생물학적 시스템이 동일한 규칙을 따르고 중앙 제어 없이 상호 소통하는 세포의 집단 행동을 통해 복잡한 3차원 구조를 형성할 수 있는 방법을 이해하는 것은 핵심적인 질문입니다.

Method: DiffeoMorph는 SE(3)-등가 그래프 신경망을 사용하여 각 에이전트가 자신의 내부 상태와 다른 에이전트의 신호를 바탕으로 위치와 내부 상태를 업데이트합니다.

Result: 우리는 3D Zernike 다항식을 기반으로 새로운 형태 일치 손실을 도입하여 예측된 형태와 목표 형태를 연속적인 공간 분포로 비교합니다.

Conclusion: DiffeoMorph는 최소한의 공간 단서만으로 단순한 타원체부터 복잡한 형태까지 다양한 형태를 형성할 수 있음을 보여줍니다.

Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.

</details>


### [23] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: 이 논문은 동적 도구 의존성 검색(DTDR) 방법을 제안하여 LLM 기반 에이전트의 도구 선택을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 정적 검색 방법들이 다단계 도구 의존성을 포착하지 못해 비효율성과 부정확성을 초래하기 때문입니다.

Method: DTDR은 초기 쿼리와 발전하는 실행 문맥을 모두 고려하여 도구 의존성을 모델링하는 경량 검색 방법입니다.

Result: DTDR은 여러 데이터셋과 LLM 백본을 통해 최신 검색 방법들과 비교하였고, 검색 정확도, 다운스트림 작업의 정확도 및 계산 효율성을 평가하였습니다.

Conclusion: 동적 도구 검색이 최신 정적 검색기와 비교하여 기능 호출 성공율을 $23\%$에서 $104\%$까지 향상시켰습니다.

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [24] [Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models](https://arxiv.org/abs/2512.17107)
*Zenan Yang,Yuanliang Li,Jingwei Zhang,Yongjie Liu,Kun Ding*

Main category: cs.LG

TL;DR: 본 연구는 태양광 배열의 신뢰성 있는 운영과 지능형 유지 관리를 위한 정확한 결함 진단 및 정량화 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 결함 정량화 방법은 종종 효율성과 해석 가능성이 제한된다.

Method: 본 논문에서는 차별 가능한 빠른 결함 시뮬레이션 모델(DFFSM)을 기반으로 한 새로운 결함 정량화 접근 방식을 제안한다. DFFSM은 여러 결함 하의 I-V 특성을 정확하게 모델링하고, 결함 매개변수에 대한 분석적 기울기를 제공한다. 이를 활용하여 Adahessian 최적화기를 사용하는 기울기 기반 결함 매개변수 식별(GFPI) 방법이 개발되어 부분 음영, 단락 및 연속 저항 열화를 효율적으로 정량화한다.

Result: 시뮬레이션 및 측정된 I-V 곡선에 대한 실험 결과는 제안된 GFPI가 다양한 결함에 대해 높은 정량화 정확도를 달성하며, I-V 재구성 오차가 3% 이하임을 보여준다.

Conclusion: 이로써, 태양광 시스템 결함 진단을 위한 차별 가능한 물리적 시뮬레이터의 적용 가능성과 효과성을 확인하였다.

Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.

</details>


### [25] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 이 연구는 CLIP 기반의 모델들이 의료 진단에서의 부정 구문 해석에 어려움을 겪는 문제를 해결하기 위해 Stanford AIMI CheXagent 모델의 성능을 평가하고, 이를 통해 검색 정확도를 향상시키는 방법을 모색한다.


<details>
  <summary>Details</summary>
Motivation: CLIP와 같은 대형 비전-언어 모델은 방대한 레이블 데이터 없이도 이미지와 텍스트를 정렬할 수 있어 의료 영상 작업에 점점 더 많이 사용되고 있다. 그러나 부정 구문 해석에서 성능 저하 문제를 겪고 있어 중요한 상황에서 문제를 야기할 수 있다.

Method: Stanford AIMI CheXagent 모델이 부정 구문을 포함한 프롬프트를 사용하여 흉부 X-레이 이미지를 정확하게 검색할 수 있는 능력을 평가하였다. 이전 연구에서 제시된 방법으로 모델을 미세 조정하여 검색 정확도를 향상시키고자 하였다.

Result: 이 연구의 결과는 CLIP 모델에서 부정 처리 능력의 향상을 보여주지만, 긍정 프롬프트 평가의 정확도는 약간 감소하였다.

Conclusion: 이 연구를 통해 CLIP의 내부 행동을 더 잘 이해하고, 의료 AI 장치에서의 신뢰성을 높이기 위해 임상 관련 언어를 사용하여 부정 처리 능력을 개선하고자 한다.

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [26] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: GPA는 Nesterov 방법의 확장으로, 최근 단일 작업자 DiLoCo 및 스케줄 프리 최적화기에서의 제한을 해결한다. GPA는 비 분산 환경에서 단일 작업자의 성능을 개선하며, 두 루프 구조를 제거하고 메모리 요구 사항을 낮춘다.


<details>
  <summary>Details</summary>
Motivation: 최근의 평균 기반 최적화 알고리즘인 단일 작업자 DiLoCo와 스케줄 프리를 개선하기 위함이다.

Method: GPA는 Nesterov의 원시 평균 공식을 활용하여 보간 상수를 분리하여 매 단계마다 부드럽게 평균화하는 방법이다.

Result: GPA는 단일 작업자 DiLoCo에 비해 일관되게 더 나은 성능을 보이며, 두 루프 구조를 제거하고 하이퍼파라미터 조정을 단순화하며 메모리 오버헤드를 줄인다.

Conclusion: GPA는 효율적인 성능 개선을 통해, 선택한 보간 상수에 따라 원래 최적화기의 수렴 보장을 초과할 수 있다.

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [27] [A Theoretical Analysis of State Similarity Between Markov Decision Processes](https://arxiv.org/abs/2512.17265)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: GBSM은 여러 MDP 간 상태 유사성을 측정하기 위한 일반화된 비유사성 메트릭으로, 기존 BSM보다 더 날카로운 경계 및 샘플 복잡도를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 여러 MDP 간 상태 유사성을 측정하기 위한 강력한 방법론이 필요하다.

Method: 일반화된 비유사성 메트릭(GBSM)을 형식적으로 설정하고 삼각 부등식 및 대칭성 등의 기본 메트릭 특성을 rigorously 증명한다.

Result: 정책 전송 및 상태 집계를 이론적으로 분석하여, 기존 BSM에서 도출된 것보다 더 날카로운 경계를 얻는다.

Conclusion: GBSM은 다중 MDP 시나리오에서 효과적임을 보인다.

Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.

</details>


### [28] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: MATCH-AD는 알츠하이머 진단을 위한 반지도 학습 프레임워크로, 제한된 라벨을 가진 샘플에서 진단 정보를 더 큰 비라벨 집단으로 전파하는 데 최적 수송 이론을 사용한다.


<details>
  <summary>Details</summary>
Motivation: 알츠하이머 병 진단을 위한 기존 기계 학습 방법들은 임상 평가가 비싸고 침습적이어서 진단 라벨이 적은 양의 신경 영상 데이터셋에만 존재하는 문제를 해결하고자 한다.

Method: MATCH-AD는 심층 표현 학습, 그래프 기반 라벨 전파 및 최적 수송 이론을 통합한 반지도 학습 프레임워크이다.

Result: MATCH-AD는 알츠하이머 협조 센터의 약 5,000명의 대상자에 대한 평가에서 진단 정확도가 거의 완벽에 가까웠으며, 라벨이 세 명의 대상자 이하인 상황에서도 우수한 성능을 보였다.

Conclusion: 이 연구는 반지도 학습이 전세계적으로 축적된 부분 주석이 달린 신경영상 데이터의 진단 잠재력을 열 수 있음을 보여준다.

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [29] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 이 연구는 적대적 공격에 대한 강력한 일반화 능력을 가진 유해 온라인 콘텐츠 탐지기를 개발하여 소셜 미디어에서의 혐오 발언, 허위 정보 및 극단주의 수사를 효과적으로 감지하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 소셜 미디어 플랫폼은 혐오 발언, 허위 정보, 극단주의 수사와 같은 유해 콘텐츠로 어려움을 겪고 있으며 이를 탐지하기 위한 기계 학습 모델의 필요성이 증가하고 있다.

Method: 본 연구는 텍스트 적대적 공격의 주요 불변성을 식별하고 이를 활용하여 강력한 일반화 능력을 가지도록 설계된 LLM-SGA 프레임워크를 제안하고, ARHOCD 탐지기를 다수의 기본 탐지기의 앙상블, 동적 가중치 할당 방식, 그리고 적대적 훈련 전략을 통해 구현하였다.

Result: 세 가지 데이터셋에 대해 ARHOCD를 평가한 결과, 강력한 일반화 능력과 적대적 조건에서 탐지 정확도를 개선하는 효과를 보였다.

Conclusion: ARHOCD는 기존의 적대적 강건성 향상 연구의 한계를 해결하며, 소셜 미디어에서의 유해 콘텐츠 탐지에 있어 중요한 진전을 이룬다.

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [30] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 환자의 특성과 기록 속성에 따라 기계 학습 모델 성능을 분석하는 것은 필수적인 통찰력을 제공하지만, 통계적으로 철저한 분석은 간단하지 않다. 이 논문에서는 의료 영상 분야에 적합한 통계 도구 상자를 제시하여 모델 성능의 하위 그룹 간 차이를 평가할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 병원과 환자의 특성에 따른 기계 학습 모델의 성능 분석이 중요성을 갖고 있으며, 모델 실패 모드에 대한 통찰력을 제공한다.

Method: 이 논문에서는 모델의 하위 그룹 성능 차이를 평가하기 위한 통계적 도구를 제시한다.

Result: ISIC2020 데이터셋을 사용한 피부 병변 악성 분류와 MIMIC-CXR 데이터셋을 사용한 흉부 X-ray 기반 질병 분류의 두 가지 사례 연구를 통해 도구 상자의 분석이 설명된다.

Conclusion: 이 통계 도구 상자는 의료 영상 응용 프로그램에 특별히 설계되었으며, 다양한 샘플 크기와 기본 비율을 가진 그룹 간의 유효한 비교를 가능하게 한다.

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [31] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 본 연구는 다중 에이전트 강화 학습 모델을 통해 탄소 배출 없는 에너지 시스템을 탈탄소화하는 과정에서의 의사결정 과정을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 전기 시스템은 오늘날 사회를 탄소 없는 경제로 변화시키는 데 필수적입니다. 장기 전기 시장 메커니즘이 전기 생성 믹스를 형성하는 데 중요합니다.

Method: 수익 극대화 생성 기업이 도매 전기 시장에서 투자 결정을 내리는 다중 에이전트 강화 학습 모델을 제시합니다. 독립적 근사 정책 최적화를 통해 분산된 경쟁 환경에 적합하게 설계되었습니다.

Result: 모델은 이탈리아 전기 시스템의 스타일화된 버전에 적용되어 다양한 경쟁 수준, 시장 설계 및 정책 시나리오에서 테스트되었습니다. 결과는 전기 부문의 탈탄소화를 위한 시장 설계의 중요성을 강조합니다.

Conclusion: 제안된 프레임워크는 여러 정책 및 시장 메커니즘이 동시에 상호작용하는 장기 전기 시장을 평가할 수 있게 해줍니다.

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [32] [A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2512.17453)
*Henok Tenaw Moges,Deshendran Moodley*

Main category: cs.LG

TL;DR: Lite-STGNN은 분해 기반의 시간 모델링과 학습 가능한 희소 그래프 구조를 통합하여 경량화된 공간-시간 그래프 신경망을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 장기 다변량 예측을 위한 효율적이고 해석 가능한 방법론 개발.

Method: 시간 모듈은 추세-계절 분해를 적용하고, 공간 모듈은 저랭크 Top-$K$ 인접성 학습 및 보수적인 영역별 게이팅을 사용하여 메시지를 전달한다.

Result: Lite-STGNN은 720단계까지의 네 가지 벤치마크 데이터세트에서 최첨단 정확도를 달성하며, 파라미터 효율성이 높고 변환기 기반 방법보다 훈련 속도가 크게 빠르다.

Conclusion: Lite-STGNN은 장기 다변량 시계열 예측을 위한 compact하고 해석 가능한 효율적 프레임워크를 제공한다.

Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.

</details>


### [33] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: Collaborative Forward-Forward (CFF) 학습을 통해 Forward-Forward 알고리즘의 비효율성을 극복하고 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: Forward-Forward 알고리즘의 메모리 제한과 생물학적 비현실성을 제거할 필요성.

Method: 두 가지 협력 패러다임인 Fixed CFF와 Adaptive CFF를 통해 레이어 간 협력을 수용하는 방법을 제안합니다.

Result: MNIST 및 Fashion-MNIST에서 기존 Forward-Forward 구현 대비 성능 향상을 보여줍니다.

Conclusion: 협력적 레이어 간 협업이 Forward-Forward 학습의 본질적인 향상으로 입증되었습니다.

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [34] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: 안전 강화 학습(SafeRL)이 자율 주행에 유용하다는 것을 보여주며, SRPL 프레임워크가 실제 자율 주행 상황에서도 효과적임을 입증함.


<details>
  <summary>Details</summary>
Motivation: 안전한 자율 주행을 위한 성능 최적화와 안전 요구 사항 간의 긴장을 해결하기 위해.

Method: SRPL 프레임워크를 사용하여 미래의 제약 위반 예측 모델 제공.

Result: Waymo Open Motion Dataset(WOMD)과 NuPlan을 통한 실험에서 성공률이 통계적으로 유의미하게 향상됨.

Conclusion: 예측 안전 표현이 SafeRL의 강화를 위한 중요한 역할을 함을 보여줌.

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [35] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 이 논문은 알고리즘이 방해와 상호작용하는 복잡한 시스템 내에서의 수렴을 보장하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 물리적, 사회적 및 공학적 시스템 내에서 알고리즘이 방해, 노이즈 및 동적 시스템 간의 상호연결에 노출되면서 그 성능이 저하되는 문제를 다루기 위함입니다.

Method: 상대적 Lyapunov 정리를 활용하여, 알고리즘의 성능에 대한 방해의 영향을 정량화하는 주요 불평등을 도출합니다.

Result: 방해가 있는 상태에서도 알고리즘의 안정성 경계와 수렴 속도를 시스템적으로 도출하였습니다.

Conclusion: 이 결과는 노이즈, 방해 및 다른 동적 시스템과의 상호연결의 영향을 고려한 알고리즘 분석의 통합 도구로 활용될 수 있음을 보여줍니다.

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [36] [SCOPE: Sequential Causal Optimization of Process Interventions](https://arxiv.org/abs/2512.17629)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: SCOPE라는 새로운 Prescriptive Process Monitoring 접근 방식이 KPI 최적화를 위한 연속 개입 추천을 학습하여 기존 기술보다 우수한 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 기존의 Prescriptive Process Monitoring (PresPM) 접근 방식은 개입 간의 상호작용을 고려하지 않아 효과적인 KPI 최적화에 한계가 있다.

Method: SCOPE는 역전파(induction)를 사용하여 각 개입 행동의 영향을 추정하고, causal learners를 활용하여 관찰 데이터를 직접 이용하는 방법을 제안한다.

Result: SCOPE는 기존의 합성 데이터셋 및 새로운 반합성 데이터셋에서 기존 최첨단 PresPM 기술보다 KPI 최적화에서 일관되게 더 뛰어난 성능을 보였다.

Conclusion: 새로운 반합성 설정은 후속 연구를 위한 재사용 가능한 벤치마크로 제공된다.

Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.

</details>


### [37] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: FedSARSA의 새로운 이론적 분석을 제시하며, 이질성 하에서의 수렴 보장을 수립하고 샘플 및 통신 복잡도 경계가 제공됨.


<details>
  <summary>Details</summary>
Motivation: FedSARSA의 성능을 헷갈리기 쉬운 환경에서도 보장하기 위해 이론적 분석을 수행했습니다.

Method: 단일 에이전트 SARSA를 위한 새로운 정확한 다단계 오류 확장을 중심으로 분석했습니다.

Result: 이질성이 FedSARSA의 수렴에 미치는 영향을 정량화하고 여러 로컬 업데이트와 함께 수렴을 보여주었습니다.

Conclusion: 에이전트 수에 대해 선형 속도 향상을 달성하며, 이로 인해 마르코프 샘플링에 따른 고차 항이 발생합니다.

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>
