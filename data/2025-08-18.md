<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 59]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 이 논문은 다양한 정보를 고려한 결정 방법을 제안하여 머신러닝 모델의 효과를 개선하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 기술이 널리 사용되지만 오버피팅/언더피팅, 클래스 불균형 등 해결해야 할 여러 한계가 있다.

Method: 협력 게임을 통해 다중 기준 상황에서 다양한 정보를 고려하여 결정을 내리는 방법을 제안한다.

Result: 기존 앙상블 가중치 방법과 비교해 우수한 성능을 보였다.

Conclusion: 제안된 방법은 정보의 적절한 반영과 성능 개선에 기여한다.

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [2] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker는 15억 개의 매개변수를 가진 모델로, 중간 크기의 최첨단 모델들과 유사한 성능을 보이면서 메모리 사용량을 절반으로 줄입니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 메모리 및 계산 비용으로 인해 실용적인 기업 환경에서의 사용이 어려워, 이를 해결하기 위해 개발되었습니다.

Method: 4단계 학습 파이프라인으로 훈련되며, 1) 기본 모델 업스케일링, 2) 지속적인 사전 훈련, 3) 감독식 미세 조정(SFT), 4) GRPO를 이용한 강화 학습을 포함합니다.

Result: 다양한 벤치마크 평가에서 Apriel-Nemotron-15B-Thinker 모델은 32억 개 파라미터 모델들과 동일하거나 더 나은 성능을 보입니다.

Conclusion: 이 모델은 반도 되는 크기에도 불구하고 경쟁 모델들과의 성능을 유지합니다.

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [3] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 이 연구는 의료 데이터를 위한 프롬프트 기반 지속 학습(PCL) 접근 방식을 제안하며, 세 개의 당뇨병성 망막병증 데이터세트에서 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 의료 분야의 윤리적, 사회적 및 제도적 제약으로 인해 데이터 공유가 severely 제한되어 중앙 집중식 학습이 거의 불가능하므로 각 기관은 지역 데이터만 사용하여 모델을 점진적으로 업데이트해야 합니다.

Method: 프롬프트 기반 지속 학습(PCL) 접근 방식은 최소한의 확장 전략을 가진 통합 프롬프트 풀을 특징으로 하며, 프롬프트의 하위 집합을 확장하고 동결하여 계산 오버헤드를 줄이고, 새로운 규제 항이 보존과 적응의 균형을 유지합니다.

Result: 세 개의 당뇨병성 망막병증 데이터세트(Aptos2019, LI2019 및 Diabetic Retinopathy Detection)에서, 우리의 모델은 최종 분류 정확도를 최소 10% 개선하고, 상태-최고 접근 방식에 비해 F1 점수를 9 포인트 향상시키며, 추론 비용을 낮춥니다.

Conclusion: 이 연구는 지속 가능한 의료 AI 발전을 이끄는 데 기여할 것으로 예상되며, 분산 의료에서 실시간 진단, 환자 모니터링 및 원격 의료 애플리케이션을 가능하게 합니다.

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [4] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert는 화학 논리에 기반한 자연어 설명을 생성하는 해석 가능한 회귀 합성 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존 모델들은 정적 패턴 매칭에 의존하여 효과적인 논리적 의사결정 능력을 제한한다.

Method: Retro-Expert는 대형 언어 모델과 전문 모델의 보완적 추론 강점을 통합하여 협력적 추론을 수행하는 해석 가능한 회귀 합성 프레임워크이다.

Result: Retro-Expert는 다양한 지표에서 LLM 기반 모델과 전문 모델을 초월하며 전문가와 일치하는 설명을 제공한다.

Conclusion: AI 예측과 실행 가능한 화학 통찰력 사이의 간극을 메운다.

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [5] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb은 고품질 합성 데이터를 생성하여 사전 훈련 성능을 향상시키는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 사전 훈련에서 합성 데이터의 사용이 유망한 패러다임으로 떠오르고 있다.

Method: 합성 데이터 생성 프레임워크인 BeyondWeb을 도입하여, 기존 웹 규모 데이터셋의 성능을 크게 향상시냈다.

Result: BeyondWeb은 Cosmopedia 및 Nemotron-Synth보다 각각 최대 5.1pp 및 2.6pp 더 높은 성능을 보였으며, 14개 기준 평가에서 평균적으로 7.7배 더 빠른 훈련 속도를 제공했다.

Conclusion: 고품질 합성 사전 훈련 데이터를 생성하는 데는 정답이 없으며, 여러 요소를 함께 최적화해야 한다는 것을 보여주었다.

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [6] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 이 논문은 T2I 모델을 위한 최초의 모델 선택 프레임워크인 M&C를 제안하며, 사용자가 데이터셋에 최적화된 T2I 모델을 효율적으로 선택할 수 있도록 지원한다.


<details>
  <summary>Details</summary>
Motivation: 공개된 선훈련 T2I 모델이 모델 민주화를 촉진하는 동시에, 사용자가 최적의 모델을 선택하는 새로운 도전에 직면하게 되었다.

Method: M&C는 모델 선택을 위한 매칭 그래프를 핵심으로 하며, 모델과 데이터셋의 노드, 그리고 모델-데이터 쌍과 데이터-데이터 쌍의 엣지를 포함한다.

Result: M&C는 32개의 데이터셋에 대해 10개의 T2I 모델 중에서 모델을 선택하는 데 평가되었으며, 61.3%의 경우 최적의 모델을 성공적으로 예측했다.

Conclusion: M&C는 사용자가 T2I 모델에서 효율적으로 최적 모델을 선택할 수 있도록 돕고, 관련 작업에서의 성능을 개선하는 데 기여한다.

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [7] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: 이 논문은 CURE라는 새로운 프레임워크를 제안하여 RLVR에서의 탐색과 활용의 균형을 이루어 모델의 성능을 향상시키는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 기존의 RLVR 파이프라인에서는 정적 초기 상태 샘플링의 반복 사용이 모델 행동의 다양성을 저하시키고 성능 향상을 저해하였다.

Method: CURE는 두 단계로 구성된 프레임워크로, 첫 번째 단계에서는 고엔트로피의 중요한 토큰을 재생성하고 원래 경로와 분기 경로를 공동 최적화하여 모델을 새로운 맥락으로 유도한다.

Result: CURE는 수학 추론 작업에서 DAPO보다 더 나은 성능을 달성하고 탐색을 위한 높은 엔트로피 수준을 유지함으로써 6개의 수학 벤치마크에서 5%의 성능 향상을 보였다.

Conclusion: CURE는 엔트로피와 정확성 모두에서 최첨단 성능을 설정하며, 우리의 접근 방식의 효과성을 실험적으로 검증하였다.

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [8] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 양자화는 신경망의 효율성을 높이는 중요한 기술이지만 이에 대한 이론적 이해는 제한적이다.


<details>
  <summary>Details</summary>
Motivation: 양자화된 설정에서의 신경망의 효율성과 이론적 이해를 증진하기 위해.

Method: Borgs et al.의 숫자 분할 문제에 대한 기초 결과를 바탕으로 양자화된 환경에서 랜덤 부분 합 문제에 대한 새로운 이론적 결과를 도출하고, SLTH 프레임워크를 유한 정밀도 네트워크로 확장한다.

Result: 양자화된 설정에서 목표 이산 신경망의 유사한 클래스가 정확히 표현될 수 있음을 보여주고, 초기 네트워크의 과다 매개변수화에 대한 최적 경계를 증명한다.

Conclusion: 이 연구는 양자화된 신경망의 이론적 이해를 넓히고 SLTH 결과를 양자화된 설정에 적용하는 방법을 제공한다.

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [9] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: 새로운 zono-conformal 예측 접근 방식은 예측 세트를 통해 다차원 출력의 의존성을 더 잘 캡처할 수 있도록 설계되었습니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 적합성 예측 방법은 계산 비용이 높고 데이터 집약적이며, 불확실성 모델을 구축해야 한다는 한계가 있습니다.

Method: zono-conformal 예측은 이를 해결하기 위한 새로운 접근 방식을 제안하며, 이는 예측 존토프를 구축합니다.

Result: zono-conformal 예측기는 기존의 구간 예측 모델보다 덜 보수적이며 유사한 테스트 데이터에 대한 적합성을 달성합니다.

Conclusion: 우리는 이 방법이 다양한 예측기에서 적용 가능하다는 것을 보여주며, 분류 환경에서도 최적의 zono-conformal 예측기를 구축합니다.

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [10] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 이 논문은 신뢰의 개념을 정의하고 이를 학습 및 믿음 업데이트 과정에서 어떻게 활용하는지를 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 신뢰는 들어오는 정보에 대한 믿음 상태에 미치는 영향을 설명하는 중요 개념입니다.

Method: 신뢰를 연속적으로 측정하는 두 가지 방법을 제시하고, 추가 가정 하에 신뢰 기반 학습의 더 간결한 표현을 유도했습니다.

Result: 신뢰는 벡터 필드와 손실 함수의 관점에서 보다 간결한 표현으로 나타낼 수 있으며, 복합 '병렬' 관측의 확장된 언어를 유도합니다.

Conclusion: 베이즈 규칙은 손실 표현이 선형 기대값인 최적화 학습자의 특별한 경우로 특징지어집니다.

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [11] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 비정규 분포에 대한 조건부 독립 구조를 정밀도 행렬을 통해 추론할 수 있는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 일반 비가우시안 분포의 경우, 공분산 및 정밀도 행렬은 변수의 독립 구조를 인코딩하지 않습니다. 그래서 비가우시안 분포에서도 조건부 독립 구조를 추론할 수 있는 방법을 탐구하게 되었습니다.

Method: 가우시안의 대각 변환에서 파생된 비가우스 분포 클래스에 대해 정밀도 행렬로부터 조건부 독립 구조를 추론할 수 있는 이론을 개발하고, 이를 기반으로 한 간단하고 효율적인 알고리즘을 제공합니다.

Result: 제안된 알고리즘은 합성 실험 및 실제 데이터 응용을 통해 그 효과가 입증되었습니다.

Conclusion: 특정 조건을 충족하는 데이터에서 비가우시안 분포의 조건부 독립 구조를 정밀도 행렬로부터 복원할 수 있음을 보여줍니다.

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [12] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: LIME 및 SHAP와 같은 설명 방법의 내구성을 평가하고 개선 전략을 연구함으로써, 편향된 모델에 대한 취약성을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: LIME 및 SHAP 등 후속 해석 방법들은 블랙박스 분류기에 대한 해석 가능한 통찰력을 제공하여 모델의 편향과 일반화 가능성을 평가하는 데 사용됩니다. 그러나 이러한 방법은 적대적 조작에 취약해 유해한 편향이 숨겨질 수 있습니다.

Method: 우리는 먼저 원래 COMPAS 실험을 재현하여 이전 결과를 검증하고 기준선을 설정합니다. 그 다음, 다양한 성능의 분류기에 대한 증강 및 앙상블 설명 접근법의 체계적인 평가를 가능하게 하는 모듈형 테스트 프레임워크를 도입합니다. 이 프레임워크를 사용하여, 우리는 LIME/SHAP 앙상블 구성의 여러 가지를 평가하고 원래 방법들과 비교하여 편향 은폐에 대한 저항력을 분석합니다.

Result: 우리는 편향 탐지를 실질적으로 개선하는 구성을 식별하였고, 이는 고위험 머신러닝 시스템의 배포에서 투명성을 높이는 잠재력을 강조합니다.

Conclusion: LIME 및 SHAP 방법의 내구성을 평가함으로써, 모델의 편향 검출을 개선할 수 있는 구성들을 제안합니다.

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [13] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 미생물군집 샘플을 정량적으로 반영한 임베딩을 통해 LLMs에 입력하는 것이 중요하다.


<details>
  <summary>Details</summary>
Motivation: 미생물 샘플의 제안된 표현 방법은 표현 품질을 향상시켜 후속 작업에서의 성능을 개선하는 데 목적이 있다.

Method: 상대적 풍부함에 따라 시퀀스 임베딩을 가중치 조정하여 고정 크기 샘플 수준 임베딩을 구성하는 풍부함 인식 Set Transformer 변형을 제안한다.

Result: 우리의 방법은 실제 미생물군집 분류 작업에서 평균 풀링 및 가중치 없는 Set Transformer보다 뛰어난 성능을 보이며, 경우에 따라 완벽한 성능을 달성한다.

Conclusion: 우리의 결과는 생물학적으로 인지된 미생물 표현을 위한 풍부함 인식 집계의 유용성을 보여준다.

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [14] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: 이 논문은 비공식적인 특성과 작은 크기로 인해 즉각적인 메시지 데이터셋의 분류에서 예측 코딩의 도전 과제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 즉각적인 메시지 데이터의 비공식적인 특성과 크기 때문에 예측 코딩의 주요 과제를 해결하고자 한다.

Method: 데이터 관리 워크플로우를 활용하여 메시지를 일일 채팅으로 그룹화한 후, 기능 선택과 로지스틱 회귀 분류기를 사용하여 경제적으로 실행 가능한 예측 코딩 솔루션을 제시한다.

Result: 즉각적인 블룸버그 데이터를 통해 검증된 방법론이 양적 정보가 풍부한 메시지 데이터셋에서 성능을 개선한다.

Conclusion: 차원 축소를 통해 솔루션의 기본 모델 성능을 향상시키고, 비용 절감 예시도 제공한다.

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [15] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 이 논문은 비디오 추천 시스템에서 사용자 만족도를 나타내는 지표로 사용되는 시청 시간을 다루며, 이 시청 시간을 편향 없이 교정하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 시청 시간은 비디오 추천 플랫폼에서 사용자 만족도의 대표 지표로 통용되지만, 여러 요인들로 인해 편향된 추천 모델을 초래할 수 있습니다.

Method: 사용자와 항목 그룹에 기반한 경험적으로 도출된 참조 분포와 비교하여 시청 시간을 교정하는 새로운 상대적 장점 디바이싱 프레임워크를 제안합니다. 이 접근 방식은 분위수 기반의 선호 신호를 생성하고 선호 학습과 분포 추정을 명시적으로 분리하는 2단계 아키텍처를 도입합니다.

Result: 오프라인 및 온라인 실험 결과, 기존 베이스라인 방법들과 비교해 추천 정확도와 견고성이 상당히 향상됨을 보여줍니다.

Conclusion: 따라서, 제안한 프레임워크는 추천 시스템의 효과를 개선할 수 있는 강력한 도구임을 입증합니다.

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [16] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 본 논문에서는 신경망을 활용하여 압축 학습 방법의 인코딩 및 디코딩 단계를 메타 학습하는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 새로운 데이터셋의 급증으로 인해 빠르고 효율적인 매개변수 학습 기법에 대한 필요성이 커졌습니다.

Method: 무작위 비선형 기능을 사용하여 대규모 데이터베이스를 압축된 정보 보존 표현으로 프로젝션 하는 압축 학습 프레임워크를 제안합니다.

Result: 제안된 압축 메타 학습 프레임워크를 통해 신속하고 정확한 시스템을 제공하는 신경망을 활용한 여러 응용 프로그램을 탐색합니다.

Conclusion: 이 연구를 통해 기존의 최첨단 접근 방식보다 더 나은 성능을 발휘할 수 있는 압축 학습 기술을 제안합니다.

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [17] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 이 연구는 환자 입원 초기에 활용할 수 있는 다중 모드 시스템을 제안하며, 임상 노트와 전자 건강 기록의 표 형식 이벤트를 융합하여 예측 모델링의 한계를 극복한다.


<details>
  <summary>Details</summary>
Motivation: ICD 코드 배정 문제는 널리 연구되었으나, 대부분의 연구는 퇴원 후 문서 분류에 집중되었다. 조기 정보 예측 모델은 건강 위험 식별, 효과적인 치료법 제안, 자원 allocation 최적화에 사용될 수 있다.

Method: 우리는 전자 건강 기록에서 임상 노트와 표 형식 이벤트를 융합하는 다중 모드 시스템을 제안한다. 이 모델은 사전 훈련된 인코더, 특징 풀링, 교차 모드 주의를 통합하여 각 모드에서 최적의 표현을 학습하고 모든 시점에서 균형을 맞춘다. 또한, 각 시점에서 기여를 조정하는 가중치 시간을 손실을 제시한다.

Result: 실험 결과 이러한 전략이 조기 예측 모델을 향상시켜 현재의 최첨단 시스템을 능가함을 보여준다.

Conclusion: 제안된 모델은 환자 입원 초기에 활용 가능한 예측 모델을 개선하여 보다 나은 건강 관리 결과를 도모할 수 있다.

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [18] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: 패션 산업의 빠른 확장과 제품 다양성 증가로 인해 e커머스 플랫폼에서 사용자가 호환 가능한 아이템을 찾는 것이 도전 과제가 되었다. 본 연구는 HFGN 모델에서 영감을 받은 FGAT라는 새로운 프레임워크를 제안하며, 그래프 신경망과 그래프 주의 메커니즘을 활용하여 아웃핏 호환성과 개인화된 추천 문제를 동시에 해결한다. 실험 결과, FGAT는 여러 기준에서 기존 모델보다 우수한 성능을 보였다.


<details>
  <summary>Details</summary>
Motivation: 패션 산업의 성장과 제품 종류의 다양화로 사용자들이 e커머스 플랫폼에서 적합한 아이템을 찾기 어려워졌다. 효과적인 패션 추천 시스템은 이러한 문제를 해결하는 데 필수적이다.

Method: 본 연구는 HFGN 모델에서 영감을 받아 FGAT라는 새로운 프레임워크를 제안한다. 이 프레임워크는 사용자, 아웃핏, 아이템으로 구성된 3단계 계층적 그래프를 구축하여 시각적 및 텍스트적 특징을 통합하여 아웃핏 호환성과 사용자 선호도를 동시에 모델링한다. 그래프 주의 메커니즘은 표현 전파 중 노드의 중요성을 동적으로 가중치를 부여한다.

Result: POG 데이터셋에서 FGAT는 HFGN과 같은 기준 모델보다 뛰어난 성능을 보였으며, 정확도, HR, 리콜, NDCG 및 정밀도에서 개선된 결과를 달성하였다.

Conclusion: 다양한 시각적-텍스트적 특징과 계층적 그래프 구조 및 주의 메커니즘을 결합함으로써 개인화된 패션 추천 시스템의 정확도와 효율성을 크게 향상할 수 있음을 보여준다.

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [19] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: 이 연구는 쪼개진 아핀 정규화(PAR)가 감량화 문제를 해결하는 유용한 방법임을 이론적으로 입증한다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 이산 또는 양자화된 변수에 대한 최적화 문제의 도전성을 극복하기 위해 PAR을 연구한다.

Method: 우리는 과다파라미터화된 환경에서 PAR 정규화 손실 함수의 모든 중요한 점은 높은 정량화를 보여주고, 다양한 PAR에 대한 닫힌 형태의 근접 맵을 도출하며, 근접 경량법과 그 가속 변형을 사용하여 PAR 정규화 문제를 해결하는 방법을 제시한다.

Result: 우리는 통계적 보장을 조사하고, PAR을 사용하여 고전적인 $_1$, 제곱 $_2$, 비볼록 정규화의 근사치를 얻으며, 양자화된 솔루션으로 유사한 통계적 보장을 얻는다.

Conclusion: 이 연구는 supervised learning에서 PAR의 이론적 기초를 다루며 정량화 문제를 해결하기 위한 유용한 방법으로 PAR을 입증한다.

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [20] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 이 논문에서는 클러스터된 전이 잔여 학습(CLT) 방법을 제안하여 다양한 데이터 출처에서의 예측 정확성을 향상시키고 차별성을 유지하는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 머신 러닝 작업은 다양한 출처에서 수집된 대규모 데이터를 활용하며, 각 출처 내에서 신뢰할 수 있는 예측을 원한다.

Method: 클러스터된 전이 잔여 학습(CRTL)은 교차 도메인 잔여 학습과 적응형 풀링/클러스터링의 강점을 결합한 메타 학습 방법이다.

Result: CTRL은 5개의 대규모 데이터셋에서 최첨단 기준선과 비교하여 일관되게 더 나은 성능을 발휘한다.

Conclusion: CTRL은 데이터의 양과 질 간의 균형을 잘 탐색하며, 특히 스위스의 국가 망명 프로그램과 같은 실제 데이터셋에서 효과적으로 적용될 수 있다.

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [21] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 이 논문은 고차원 베이지안 네트워크 분류기의 설계를 위한 새로운 패러다임을 제안하며, 이를 통해 복잡한 데이터의 발생 확률을 효과적으로 추정할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 베이지안 네트워크 분류기가 낮은 차원의 특징 종속성 모델링에 제한되어 있어 복잡한 실제 데이터의 발생 확률 추정이 어려운 문제를 해결하고자 한다.

Method: 특징 값에 대한 분포 표현을 학습하여 고차원 베이지안 네트워크 분류기를 설계하고, K-종속성 베이지안 분류기를 신경망으로 확장한 NeuralKDB를 제안한다.

Result: NeuralKDB 모델은 고차원 특징 종속성을 잘 포착하며, 60개의 UCI 데이터셋에 대한 광범위한 분류 실험 결과 기존 베이지안 네트워크 분류기 및 경쟁 분류기들보다 우수한 성능을 보인다.

Conclusion: 제안된 NeuralKDB 분류기는 복잡한 데이터의 발생 확률을 정확하게 추정할 수 있는 효과적인 솔루션을 제공한다.

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [22] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: IoT의 다중 모드 데이터 처리를 위한 온라인 학습 프레임워크인 MMO-FL의 한계와 QQI 문제를 해결하기 위해 QQR 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 소스에서 발생하는 방대한 다중 모드 데이터를 효율적으로 처리하는 분산 학습 패러다임이 필요하다.

Method: MMO-FL 프레임워크 내에서 QQI의 영향을 체계적으로 조사하고, 모드 불균형을 처리하기 위한 QQR 알고리즘을 개발하였다.

Result: 제안된 QQR 알고리즘은 두 개의 실세계 다중 모드 데이터 세트에서 벤치마크 대비 우수한 성능을 보여주었다.

Conclusion: QQR 알고리즘은 모드 불균형 조건에서 promising한 학습 성능을 달성하였다.

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [23] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 이 논문은 레이블이 있는 샘플과 레이블이 없는 샘플을 통합한 반지도 생성 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 관점 학습이 실제 데이터 세트에 널리 사용되지만 누락된 관점 및 레이블 문제로 어려움을 겪고 있다.

Method: 제안한 방법은 레이블이 있는 데이터에서 IB와 공유되는 잠재 공간을 학습하기 위해 레이블이 없는 샘플의 가능성을 극대화하고, 잠재 공간에서의 교차 관점 상호 정보 최대화를 수행한다.

Result: 제안된 모델은 누락된 관점과 제한된 레이블 샘플을 가진 이미지 및 다중 오믹스 데이터에서 더 나은 예측 및 보간 성능을 달성한다.

Conclusion: 우리는 레이블이 있는 샘플과 레이블이 없는 샘플을 모두 활용하여 반지도 생성 모델을 통해 성능을 개선할 수 있음을 보여준다.

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [24] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 이 논문은 양자 볼츠만 기계-변분 오토인코더(QBM-VAE)를 소개하여, 복잡한 생물학적 데이터를 포함한 다양한 데이터의 비가우시안적 특성을 보다 잘 포착하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 생물학적 데이터를 포함한 복잡한 자연 데이터의 비가우시안적 특성을 정확하게 포착하지 못할 때 모델의 신뢰성이 저하됩니다. 이는 과학적 발견을 방해하는 요인이 됩니다.

Method: QBM-VAE는 양자 프로세서를 활용하여 볼츠만 분포에서 효율적으로 샘플링하여 깊은 생성 모델 내에서 강력한 사전으로 사용할 수 있도록 합니다.

Result: QBM-VAE는 여러 출처의 백만 개 규모의 단일 세포 데이터셋에 적용되어, 복잡한 생물학적 구조를 잘 보존하는 잠재 공간을 생성하고, VAE와 SCVI와 같은 기존의 가우시안 기반 딥러닝 모델을 여러 필수 작업에서 지속적으로 능가합니다.

Conclusion: 이 연구는 양자 컴퓨팅을 이용하여 딥러닝의 실용적 quantum advantage를 시연하며, 하이브리드 양자 AI 모델 개발을 위한 전이 가능한 블루프린트를 제시합니다.

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [25] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 구조 보존 방식을 이용한 동역학 모델링이 물리 시스템 모델링에 큰 잠재력을 보여준다. 그러나 고정된 시스템 구성에 대해 훈련되어야 하며, 각 새로운 매개변수 세트에 대해 재훈련이 필요하다.


<details>
  <summary>Details</summary>
Motivation: 다양한 쿼리나 매개변수 변동 시나리오에서의 주요 한계를 극복하기 위해 메타 학습이 해결책이 될 수 있다.

Method: 잠재적인 미지의 시스템 매개변수의 컴팩트한 잠재 표현에 구조 보존 모델을 직접 조건화하는 조절 기반 메타 학습 프레임워크를 소개한다.

Result: 파라메트릭 에너지 보존 및 소산 시스템에 대한 새로운 조절 전략을 적용하여 확장 가능하고 일반화 가능한 학습을 가능하게 한다.

Conclusion: 표준 벤치마크 문제에서 우리의 접근 방식은 동역학 안정성과 매개변수 공간 전반에 걸친 효과적인 일반화 성능을 위해 필요한 필수 물리적 제약을 저해하지 않으면서도 몇 샷 학습 환경에서 정확한 예측을 달성한다.

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [26] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: 이 연구는 조기 위험 평가의 예측 성능 향상에 중점을 두고 있으며, '미래에서 빌리기(BFF)'라는 다중 모달 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 임상에서 조기 신뢰할 수 있는 위험 평가를 수행하는 것이 바람직하지만, 후속 단계에서 더 높은 정확도를 달성합니다.

Method: BFF는 각 시간 창을 독립된 모달리티로 처리하는 대조적 다중 모달 프레임워크로, 최신 정보를 사용하여 위험 평가를 수행하면서 모든 가용 데이터에서 모델을 훈련합니다.

Result: BFF는 두 개의 실제 소아 결과 예측 작업에서 검증되어 조기 위험 평가에서 일관된 개선을 보여줍니다.

Conclusion: BFF 프레임워크는 이전 단계에서 후속 단계의 유용한 신호를 '빌려' 학습을 보다 효과적으로 지원합니다.

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [27] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 인지 행동에 대한 설명은 표현에 대한 계산을 다룬다. 시스템이 적합한 표현 수단을 통해 주어진 계산을 구현하기 위해 무엇이 필요한지를 논의한다. 인과성과 인과 추상화 이론이 이 주제를 탐구하는 데 유용하다고 주장한다. 깊은 학습과 인공 신경망에 대한 논의를 통해 계산과 인지의 철학적 주제가 현대 기계 학습에서 다시 나타난다고 설명한다. 인과 추상화에 기반한 계산 구현의 설명과 표현의 역할을 검토하며 일반화와 예측과의 관련성이 더욱 유익하다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: 인지 행동에 대한 설명에서 표현에 대한 계산을 다루는 것이 중요하다.

Method: 인과성의 언어와 인과 추상화 이론을 사용하여 계산 구현을 설명하고 현대 기계 학습과의 관계를 탐구한다.

Result: 인과 추상화에 기반한 계산 구현에 관한 설명과 그 과정에서 표현의 역할을 검토한다.

Conclusion: 이러한 주제를 일반화와 예측과 연결하여 탐구하는 것이 가장 유익하다.

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [28] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 이 논문은 CNN-LSTM 구조를 기반으로 한 PM2.5 공기질 지수 예측 모델을 제안하고, 이 모델이 전통적인 시계열 모델보다 정확성과 일반화에서 우수한 성능을 보인다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 전 세계 기후 변화의 심화로 인해 PM2.5 농도와 같은 공기질 지표의 정확한 예측이 환경 보호, 공공 건강 및 도시 관리 분야에서 점점 더 중요해지고 있다.

Method: 이 모델은 공간적 특징 추출을 위한 컨볼루션 신경망(CNN)과 시계열 데이터의 시간적 의존성을 모델링하기 위한 장기 단기 기억(Long Short-Term Memory, LSTM) 네트워크를 결합한 하이브리드 아키텍처이다.

Result: 베이징 산업 지역에서 2010년부터 2015년까지 수집된 다변량 데이터셋을 사용하여, 모델은 6시간 간격으로 평균 PM2.5 농도를 예측하며, RMSE 5.236을 기록하여 전통적인 시계열 모델보다 우수한 성능을 보인다.

Conclusion: 그러나 다변량 입력의 복잡성으로 인해 높은 계산 리소스가 필요하며, 다양한 대기 요소를 처리하는 능력은 여전히 최적화가 필요하다. 향후 작업은 확장성을 향상시키고 더 복잡한 다변량 날씨 예측 작업에 대한 지원을 확장하는 데 초점을 맞출 것이다.

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [29] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: GODNF라는 새로운 신경망 프레임워크를 제안하여 다양한 그래프 구조에 적합하도록 다룬다.


<details>
  <summary>Details</summary>
Motivation: GNNs의 메시지 전달 메커니즘과 물리적 확산 과정 간의 연결성에 기반하여 확산 기반 GNN 개발에 대한 관심이 증가하고 있다.

Method: GODNF는 여러 의견 역동성 모델을 통합하여 원칙적이고 훈련 가능한 확산 메커니즘을 제공한다.

Result: GODNF는 다양한 수렴 구성을 모델링할 수 있는 능력을 보여주며, 탁월한 성능을 입증하였다.

Conclusion: GODNF는 최첨단 GNNs보다 뛰어난 성능을 보여주었다.

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [30] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: 본 논문은 다양한 샘플링 속도로 발생하는 궤적을 효율적으로 처리하기 위해 개선된 인터랙티브 투표 기반 맵 매칭 알고리즘을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: GPS 궤적을 입력 데이터 품질에 독립적으로 높은 정확도로 재구성하는 것이 주요 목표입니다.

Method: 원래의 알고리즘을 기반으로 궤적 보간을 통합하여 기능을 확장하였으며, 거리 제한 인터랙티브 투표 전략을 구현하여 계산 복잡성을 줄이고 도로 네트워크의 결측 데이터를 처리하는 수정 사항을 포함합니다.

Result: OpenStreetMap의 특수 제작 자산을 통합하여 이 접근 방식이 OpenStreetMap 도로 네트워크로 커버되는 어느 지역에서도 원활하게 적용될 수 있도록 하였습니다.

Conclusion: 이러한 발전은 원래 알고리즘의 핵심 강점을 보존하면서 실제 다양한 시나리오에 대한 적용성을 크게 확장합니다.

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [31] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS는 암호화폐 거래 성능을 향상시키기 위해 시장 레짐 인식을 통합한 새로운 미분 가능 아키텍처 검색 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 동적 재무 환경에서 정적 딥 러닝 모델의 한계를 극복하려고 합니다.

Method: 이 프레임워크는 베이지안 탐색 공간, 동적으로 활성화된 신경 모듈, 다중 목표 손실 함수 등 세 가지 핵심 혁신을 포함하고 있습니다.

Result: RegimeNAS는 기존 최첨단 벤치마크보다 훨씬 나은 성능을 보여주며, 평균 절대 오차를 80.3% 줄이고 훈련 시간이 상당히 단축되는 결과를 보입니다.

Conclusion: 이 연구는 시장 레짐과 같은 도메인 특화 지식을 NAS 프로세스에 직접 통합해야 한다는 점의 중요성을 강조합니다.

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [32] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: DNN 모델의 모듈화를 통한 훈련 비용 절감과 효율성 향상을 위한 NeMo 방법론 제안


<details>
  <summary>Details</summary>
Motivation: 현대 소프트웨어 시스템에 DNN 모델을 통합하면서 건설 비용이 크게 증가하고 있다. 모델 재사용은 훈련 비용을 줄이는 데 널리 사용되지만 전체 모델을 무분별하게 재사용하면 추론 오버헤드가 발생할 수 있다.

Method: NeMo는 모든 DNN에 공통적인 신경 수준에서 작동하며, 효과적인 복합 손실 함수를 사용한 대비 학습 기반의 모듈 훈련 방법을 설계하여 대규모 모델에 대한 확장성을 가능하게 한다.

Result: NeMo는 두 개의 트랜스포머 기반 모델과 네 개의 CNN 모델에 대한 포괄적인 실험을 통해 기존 MwT 방법보다 우수성을 입증하였고, 평균 1.72% 모듈 분류 정확도 향상과 58.10% 모듈 크기 감소를 기록했다.

Conclusion: NeMo는 CNN 및 대규모 트랜스포머 기반 모델 모두에서 효과성을 입증하며, 실용적인 시나리오에서 스케일 가능하고 일반화 가능한 DNN 모듈화 접근 방식을 제공한다.

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [33] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 본 논문에서는 폐쇄형 가중치 LLM으로부터 공정 분류기를 도출하기 위한 프레임워크를 제안하며, 이를 통해 높은 정확도와 공정성의 균형을 유지할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: LLM의 간편한 사용과 계속되는 발전은 인구 통계 그룹 간의 불균형한 영향을 방지하는 집단 공정성을 요구하는 고위험 응용 프로그램에서도 LLM의 적용 가능성을 증가시킬 수 있다.

Method: LLM을 특징 추출기로 취급하고, 공정성 기준을 위해 전략적으로 설계된 프롬프트를 사용하여 확률 예측에서 특성을 유도한다. 이후 이 특성에 공정 알고리즘을 적용하여 가벼운 공정 분류기를 후처리 방식으로 훈련한다.

Result: 다섯 개의 데이터셋에 대한 실험은 프레임워크에서 도출된 분류기가 높은 정확도와 공정성을 제공함을 보여준다.

Conclusion: 우리의 프레임워크는 데이터 효율적이며 LLM 임베딩에 대해 훈련된 공정 분류기보다 성능이 우수하다.

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [34] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 이 논문은 시험 시간 강화 학습(TTRL)을 통해 대규모 언어 모델의 탐색과 활용 균형을 개선하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 복잡한 추론 작업에서 개선되었지만, 주석 데이터에 의존하고 비지도 시나리오에서 적응성이 제한적이라는 문제를 해결하기 위해.

Method: 두 가지 전략, 즉 엔트로피-포크 트리 다수 결선(ETMR)과 엔트로피 기반 이점 재형성(EAR)을 통해 TTRL의 탐색-활용 균형을 향상시키는 엔트로피 기반 메커니즘을 도입함.

Result: Baseline과 비교하여, 우리의 접근 방식은 Llama3.1-8B가 AIME 2024 벤치마크에서 Pass at 1 메트릭에서 68%의 상대적 개선을 달성하도록 하였고, 롤아웃 토큰 예산의 60%만 소비함.

Conclusion: 이 방법은 추론 효율성, 다양성, 추정 강건성 간의 균형을 효율적으로 최적화하는 능력을 강조하여, 오픈 도메인 추론 작업을 위한 비지도 강화 학습을 발전시킨다.

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [35] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: SNN의 적대적 강인성을 개선하기 위한 연구


<details>
  <summary>Details</summary>
Motivation: SNN의 에너지 효율적이고 뇌 영감을 받은 컴퓨팅을 활용하려는 동기.

Method: Temporal ensembling을 통해 SNN의 적대적 강인성을 재조명하고 Robust Temporal self-Ensemble (RTE) 훈련 프레임워크를 제안하여 각 서브 네트워크의 강인성을 개선하고 적대적 요인의 시간 전이를 줄임.

Result: 다양한 벤치마크에서 RTE가 기존의 훈련 방법보다 일관되게 우수한 성능을 보였음을 입증.

Conclusion: 적대적 학습에서 시간 구조의 중요성을 강조하며 강인한 스파이킹 모델을 구축할 수 있는 근본적인 기반을 제공.

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [36] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: 이 논문은 보이지 않는 피험자에서 신뢰할 수 있는 EEG 디코딩을 위해 PTSM이라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: EEG 디코딩 분야에서의 큰 도전 과제인 피험자 간 변동성과 피험자 불변 표현의 부족을 해결하기 위해.

Method: PTSM은 개인화된 스페이오-타임 패턴을 배우기 위한 이중 분기 마스킹 메커니즘을 활용하여 개인의 신경 특성을 유지하고 태스크와 관련된 공유 특징을 추출합니다.

Result: PTSM은 제로샷 일반화에서 뛰어난 성능을 보여주며, 상태-of-the-art 기준보다 우수한 성능을 기록했습니다.

Conclusion: 모델은 분리된 신경 표현으로 개인화 및 전이 가능한 디코딩을 달성하는 데 효과적임을 보여줍니다.

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [37] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 그래프 사전 훈련 및 프롬프트 튜닝이 효율적인 지식 이전을 가능하게 하지만, 기존 방법은 다양한 스펙트럼 분포를 처리하지 못한다. 따라서 HS-GPPT 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 실제 그래프에서 다양한 동질성과 비동질성을 처리하기 위한 효과적인 지식 전이가 필요하다.

Method: HS-GPPT 모델은 하이브리드 스펙트럴 필터 백본과 로컬-글로벌 대조 학습을 사용하여 스펙트럴 지식을 획득하고, 프롬프트 그래프를 설계하여 스펙트럼 분포와 선행 과제를 정렬한다.

Result: 광범위한 실험을 통해 전이 학습 및 유도 학습 환경 모두에서 기법의 효율성을 입증한다.

Conclusion: 제안한 방법은 스펙트럼 지식 이전을 용이하게 하여 제한된 감독 하에서도 효과적인 적응을 가능하게 한다.

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [38] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 이 논문에서는 대체 손실을 최소화하는 방법을 제안하여, 결정 품질을 극대화하는 머신 러닝 모델을 훈련하는 결정 중심 학습(DFL)의 한계를 극복함.


<details>
  <summary>Details</summary>
Motivation: 결정 중심 학습(DFL)은 머신 러닝 모델이 최적화 문제의 매개변수를 예측하도록 훈련하여 결정 후회(regret)를 최소화하고 결정 품질을 극대화하고자 합니다.

Method: 이전 방법들이 직관적으로는 대체 손실을 최소화하는 방법에 초점을 맞추었다고 보고, 이 작업을 수행하기 위해 다분화 가능한 최적화 레이어를 사용하였습니다. 우리는 DYS-Net이라는 최근 제안된 다분화 가능한 최적화 기술을 활용하여 손실을 최소화하였습니다.

Result: Experiments show that our approach enables achieving performance of regret comparable to or better than existing gradient-based DFL methods.

Conclusion: 대체 손실을 최소화하는 방법을 통해 최신 기술과 동등한 성능을 달성하면서 훈련 시간을 효율적으로 줄일 수 있습니다.

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [39] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: 본 논문에서는 Large Language Models의 성능을 개선하기 위해 Supervised Fine-Tuning(SFT)과 Reinforcement Learning(RL)의 통합된 접근 방식을 제안한다. 새로운 프레임워크인 CHORD를 통해 SFT를 동적 가중치를 가진 보조 목표로 재구성하고, 광범위한 실험을 통해 안정적이고 효율적인 학습을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: Large Language Models의 행동을 조정하고 능력을 정제하기 위해 SFT와 RL을 통합할 필요성.

Method: CHORD라는 프레임워크를 통해 SFT를 동적 가중치를 가진 보조 목표로 설정하고, 전이와 탐색을 분리하지 않도록 설계하였다.

Result: CHORD는 기존 모델과 비교하여 안정적이고 효율적인 학습 과정을 달성하였고, 성능이 유의미하게 향상되었다.

Conclusion: CHORD는 오프-폴리시 전문가 데이터를 온-폴리시 탐색과 효과적으로 조화시킴으로써 기존 방법들에 비해 상당한 향상을 보여준다.

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [40] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 본 논문에서는 긴 꼬리 레이블 분포에서의 불균형한 커버리지를 해결하기 위한 Tail-Aware Conformal Prediction (TACP) 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 CP 방법은 평균적으로 커버리지를 보장하지만, 긴 꼬리 레이블 분포 하에서 클래스 간 불균형한 커버리지 문제를 보여, 적은 클래스에 대한 신뢰성을 해친다.

Method: TACP 방법은 긴 꼬리 구조를 이용하고 헤드와 테일 클래스 간의 커버리지 격차를 좁힘으로써 테일 클래스의 언더 커버리지를 완화한다. 또한, reweighting 메커니즘을 통해 soft TACP (sTACP)로 이 방법을 확장한다.

Result: 이론 분석을 통해 TACP가 표준 방법보다 일관되게 더 작은 헤드-테일 커버리지 격차를 달성함을 보였다. 또한, 여러 긴 꼬리 벤치마크 데이터셋에서 실험을 통해 제안한 방법의 효과를 입증하였다.

Conclusion: 제안된 프레임워크는 다양한 비적합 점수와 결합할 수 있으며, 모든 클래스 간의 커버리지 균형을 개선하는 데 기여한다.

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [41] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 복잡한 머신러닝 모델의 행동에 대한 통찰을 제공하는 지역적인 설명 알고리즘에 대한 이론적 보장이 결여되어 있으며, 이 논문에서는 설명이 의사결정 함수에 대한 정보를 제공한다는 의미를 학습 이론에 기반하여 제시합니다. 많은 인기 있는 설명 알고리즘이 복잡한 의사결정 함수에 적용될 때 정보적이지 않으며, 이를 수정하여 정보적으로 만들 수 있는 방법을 논의합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 머신러닝 모델을 설명하기 위한 이론적 보장이 부족한 상황에서 설명 알고리즘이 얼마나 유용한지를 탐구하고자 함.

Method: 설명이 정보적일 때를 정의하고, 다양한 설명 알고리즘이 언제 정보를 제공하는지에 대한 조건을 도출함.

Result: 많은 인기 있는 설명 알고리즘이 복잡한 의사결정 함수에 대해 정보적이지 않음을 수학적으로 검증하고, 특정 조건 하에서 정보적이게 만드는 방법을 제시함.

Conclusion: 설명 알고리즘의 분석은 수학적이지만, AI의 감사, 규제 및 고위험 응용에서의 실제 적용 가능성에 대한 강한 함의를 가짐.

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [42] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 본 연구는 기후 변화 완화를 위한 조림 및 재조림 노력의 신뢰성을 검토하고, 이를 위해 1,289,068개의 식재 사이트 데이터셋을 제공하여 위치 정보의 무결성을 평가하는 지표인 LDIS를 도입하였다.


<details>
  <summary>Details</summary>
Motivation: 자발적 탄소 시장에 대한 감시가 증가함에 따라, 조림과 재조림 노력의 효과성에 대한 데이터의 신뢰성과 프로젝트 무결성에 대한 우려가 제기되고 있다.

Method: 이 연구에서는 33년에 걸쳐 45,628개의 프로젝트에서 수집한 1,289,068개의 식재 사이트의 데이터셋을 제공하며, 이를 통해 위치 정보의 평가를 위해 LDIS를 도입한다.

Result: 모니터링된 지리 참조 식재 사이트의 약 79%가 10개의 LDIS 지표 중 최소 1개에서 실패하고, 15%의 프로젝트는 기계 판독 가능한 지리 참조 데이터가 결여되어 있다.

Conclusion: 제시된 데이터셋은 자발적 탄소 시장의 책임성을 높이는 데 기여할 뿐만 아니라, 수백만 개의 관련 Sentinel-2 및 Planetscope 위성 이미지로 컴퓨터 비전 관련 작업을 위한 훈련 데이터로도 가치가 있다.

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [43] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: 그래프 신경망(GNN)의 해석 가능성을 향상시키는 것은 안전하고 공정한 배치를 보장하는 데 중요하다. GraphOracle은 GNN을 위한 클래스 수준의 해석을 생성하고 평가하도록 설계된 새로운 자가 설명 GNN 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: GNN의 안전하고 공정한 배치를 보장하기 위해 해석 가능성을 향상시키는 것이 필요하다.

Method: GraphOracle은 GNN 분류기와 각 클래스에 대해 구별되는 구조적이고 희소한 서브 그래프 집합을 공동으로 학습하는 새로운 통합 훈련 방식을 제안한다.

Result: GraphOracle은 다양한 그래프 분류 작업에서 우수한 충실도, 설명 가능성 및 확장성을 달성하며, 이전 방법들이 효과적인 클래스 수준 해석을 제공하지 못한다는 것을 보여준다.

Conclusion: GraphOracle은 GNN의 신뢰할 수 있는 클래스 수준 자가 설명 가능성에 대한 실용적이고 원칙적인 해결책으로 자리매김한다.

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [44] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: 본 연구는 불균형 데이터 스트림 학습을 위한 HGD 알고리즘을 제안하며, 이는 다양한 클래스 간의 그래디언트 노름을 평준화하여 마이너 클래스의 언더피팅을 완화하고 균형 잡힌 온라인 학습을 달성한다.


<details>
  <summary>Details</summary>
Motivation: 실제 세계의 데이터는 시간에 따라 순차적으로 수집되며 불균형한 클래스 분포를 나타내는 경우가 많다.

Method: 우리는 HGD 알고리즘을 소개하며, 이는 그래디언트 노름을 클래스 간에 균등하게 하여 마이너 클래스에 대한 언더피팅을 완화한다.

Result: HGD는 어떤 학습 모델에도 적용 가능하고, 데이터 버퍼, 추가 매개변수 또는 사전 지식이 필요 없다.

Conclusion: HGD는 불균형 데이터 스트림 학습에서 효율적이고 효과적인 결과를 나타낸다.

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [45] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: HXAI는 데이터 분석 워크플로우의 모든 단계에 설명을 통합하며 사용자 맞춤형 설명을 제공하는 사용자 중심의 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 인공지능은 과학과 산업을 재형성하고 있지만 많은 사용자는 여전히 이를 불투명한 '블랙박스'로 간주한다.

Method: HXAI는 데이터, 분석 설정, 학습 과정, 모델 출력, 모델 품질, 커뮤니케이션 채널의 6개 요소를 통합한 세분화된 분류 체계를 통해 사용자 요구에 맞춘 설명을 제공한다.

Result: 112개 항목의 질문 은행이 필요한 사항을 다루며, 기존 도구의 중요한 커버리지 격차를 강조하는 설문 조사를 진행하였다.

Conclusion: HXAI는 설명을 명확하고 실행 가능하며 인지적으로 관리 가능하게 만드는 특성을 식별하고, 기술적 결과물을 이해관계자 맞춤 이야기로 번역하여 AI 개발자와 도메인 전문가 간의 격차를 해소하는 방법을 제시한다.

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [46] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 이 논문에서는 개별 보상과 쌍 선호를 결합한 강화 학습 알고리즘인 Dual-Feedback Actor(DFA)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 보상 모델링 단계를 생략하고 직관적으로 선호 확률을 모델링하기 위해 새로운 알고리즘이 필요하다.

Method: DFA는 정책의 로그 확률을 사용하여 선호 확률을 모델링하며, 인간 주석자나 오프 정책 리플레이 버퍼에 저장된 Q 값에서 선호를 합성한다.

Result: DFA는 생성된 선호로 훈련받을 때 여섯 개의 제어 환경에서 SAC를 초과하거나 맞먹는 결과를 보이며, 더 안정적인 훈련 과정을 보여준다.

Conclusion: Bradley-Terry 모델 하에는 DFA의 선호 손실 최소화가 엔트로피 정규화된 SAC 정책을 회수하며, 일부 반합성 선호 데이터셋으로도 RLHF 벤치마크를 초과한다.

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [47] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 이 논문에서는 Forman-Ricci 곱률을 이용한 구조적 리프팅 전략을 제안하여 그래프 신경망(GNN) 모델을 적용하기 전에 데이터 표현을 더 표현력 있는 위상으로 변환하는 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 그래프 신경망은 관계형 데이터에서 학습하는 데 매우 효과적이지만, 많은 실제 시스템은 더 높은 차원 위상 구조로 자연스럽게 표현되는 복잡한 상호작용을 나타낸다.

Method: Forman-Ricci 곱률을 이용한 구조적 리프팅 전략을 제안하여 기본 그래프 형태에서 더 표현력이 풍부한 위상으로 데이터 표현을 변환한다.

Result: 우리의 접근법은 그래프에서 정보 왜곡 문제를 해결하고 연결 고리 사이의 정보 흐름을 모델링하기 위해 하이퍼엣지를 통해 네트워크의 주요 커뮤니티 간의 연결을 형성하는 방식으로 네트워크의 골격을 드러낸다.

Conclusion: 우리의 방법은 장거리 메시지 전달 및 그래프 병목 현상에서 발생하는 정보 왜곡 문제를 완화하는 데 도움을 준다.

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [48] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: 이 논문은 항체 서열과 구조를 공동 모델링하기 위한 LatEnt blAck-box Design(LEAD) 프레임워크를 제안하며, 기존 방법의 한계를 극복하고 우수한 최적화 성능을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 항체 서열 및 구조의 공동 모델링을 통해 개발 가능성을 개선하고자 함.

Method: LatEnt blAck-box Design(LEAD)라는 서열-구조 공동 설계 프레임워크를 제안하여 공유 잠재 공간 내에서 서열 및 구조를 최적화함.

Result: LEAD는 단일 및 다속성 목표에 대한 우수한 최적화 성능을 보여주었으며, 쿼리 소모를 절반으로 줄이며 기존 방법보다 우수함.

Conclusion: LEAD는 효과적인 최적화를 구현하며, 해당 코드는 GitHub에서 사용할 수 있음.

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [49] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 이 연구에서는 Convolutional Neural Ordinary Differential Equations (NODEs)의 강인성을 향상시키기 위해 수축 이론을 사용하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 신경망은 입력 잡음과 적대적 공격에 취약할 수 있습니다.

Method: 우리는 수축적인 동적 시스템에서 시작하여, 수축적 Convolutional NODEs를 적용하여 특징의 미세한 perturbation이 출력에 큰 변화를 초래하지 않도록 하는 방법 개발.

Result: 제안된 정규화 기법은 MNIST 및 FashionMNIST 데이터셋에서 다양한 형태의 잡음과 공격으로 손상된 이미지를 분류하는 벤치마크 테스트를 통해 성능이 입증되었습니다.

Conclusion: 수축성을 프로모션하기 위해 훈련 중 자코비안 관련 정규화 항 및 가중치 정규화 항을 사용하여 계산 부담을 줄이는 방법을 보여주었습니다.

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [50] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: mCOCO라는 새로운 프레임워크를 통해 주요 뇌 연결 패턴을 파악하고 개선된 해석 가능성과 정보 처리 능력을 제공하며, 기존 GNN 기반 방법보다 월등한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: CBT 학습을 위한 기존 방법의 한계를 극복하여 개인 간 고유 연결 패턴을 발견하고 인지 과제를 처리하는 뇌의 기능을 모델링하고자 함.

Method: mCOCO는 Reservoir Computing(RC)을 활용하여 BOLD 신호에서 집단 수준의 기능적 CBT를 학습하며, 다중 감각 입력을 통합하여 구조 및 정보 처리 방식을 함습.

Result: mCOCO 기반 템플릿은 GNN 기반 CBT에 비해 중심성, 식별성, 위상적 적합성, 다중 감각 기억 유지 측면에서 상당히 우수한 성능을 보임.

Conclusion: mCOCO는 다중 감각 데이터를 효과적으로 통합하여 뇌 연결 템플릿을 생성하며, 기존 연구와 비교하여 새로운 접근 방식을 제시함.

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [51] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: 본 연구에서는 여섯 가지 확률적 기계 학습 알고리즘의 불확실성 추정을 조사하였으며, 모든 알고리즘이 잘 보정된 결과를 보였지만, 딥러닝 기반 알고리즘은 분포 외 데이터 포인트에 대해 실험적 증거 부족을 일관되게 반영하지 못함을 발견하였다.


<details>
  <summary>Details</summary>
Motivation: 자연 과학에서의 과학적 발견의 유효성을 뒷받침하기 위해서는 매개변수 추정 및 동반된 불확실성을 포함한 엄밀한 통계 방법이 필요하다.

Method: 우리는 근사 베이지안 추론의 통합 프레임워크와 주의 깊게 생성된 합성 분류 데이터셋에 대한 실증적 테스트를 결합하여 여섯 가지 확률적 기계 학습 알고리즘의 질적 특성을 조사하였다.

Result: 모든 알고리즘이 잘 보정되었지만, 딥러닝 기반 알고리즘은 분포 외 데이터 포인트에 대해 실험적 증거 부족을 일관되게 반영하지 못함을 발견하였다.

Conclusion: 우리의 연구가 과학 데이터 기반 모델링을 위한 새로운 불확실성 추정 방법을 개발하는 연구자들에게 명확한 사례로 남길 수 있기를 바란다.

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [52] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 이 연구는 오하이오에서 6년간(2017-2022) 300만 명 이상이 관련된 교통사고의 데이터셋을 소개하며, 이를 통해 심각한 사고의 위험 요인을 식별하고 해석하는 투명하고 재현 가능한 방법론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 교통사고는 전 세계적으로 부상과 사망의 주요 원인으로, 사고의 심각성을 이해하고 완화하기 위한 데이터 중심 접근이 필요하다.

Method: 자동화된 머신러닝(AutoML)과 설명 가능한 인공지능(AI)을 결합하여 주요 위험 요인을 식별하고 해석하는 방법론을 사용하였다.

Result: 최종 Ridge 로지스틱 회귀 모델은 훈련 세트에서 AUC-ROC 85.6%, 테스트 세트에서 84.9%를 기록하였으며, 17개의 주요 특징이 가장 영향력 있는 예측자로 일관되게 확인되었다.

Conclusion: 이 연구는 예측 성능보다 방법론적 엄밀성과 해석 가능성을 강조하며, Vision Zero를 지원하기 위한 확장 가능하고 데이터 기반의 교통 안전 정책을 위한 프레임워크를 제공한다.

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [53] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 이 논문은 다이나믹 환경에서의 의사결정 에이전트의 안전 검증을 위해 이중 공간 가이드 테스트 프레임워크를 제안하며, 시나리오 다양성 및 중요성을 효과적으로 균형 있게 처리하는 방법을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 다이나믹 환경에서 의사결정 에이전트의 배치가 증가함에 따라 안전 검증의 필요성이 커지고 있습니다.

Method: 이중 공간 가이드 테스트 프레임워크를 제안하여 시나리오 파라미터 공간과 에이전트 행동 공간을 조정하며, 다양성과 중요성을 고려한 테스트 시나리오를 생성합니다.

Result: 우리의 프레임워크는 평균 56.23%의 임계 시나리오 생성을 개선하고, 다섯 개의 의사결정 에이전트에서 최신 기술 구현을 초월한 다양한 메트릭을 보여줍니다.

Conclusion: 이 프레임워크는 시나리오 특성과 탐색을 지속적으로 향상시키는 닫힌 피드백 루프를 형성합니다.

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [54] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 이 논문에서는 유한 너비의 신경망에서의 NTK 통계를 계산하기 위한 파인먼 도표를 소개하여, 훈련 역학 예측을 위한 필요한 대수적 조작을 단순화하고, 다양한 통계를 다룰 수 있는 계층별 재귀 관계를 계산할 수 있게 한다.


<details>
  <summary>Details</summary>
Motivation: 신경 접선 커널(NTK)은 비선형 신경망을 분석하는 강력한 도구로, 무한 너비의 한계에서 훈련 역학에 대한 완전한 해석적 분석이 가능하다.

Method: 파인먼 도표를 소개하여 유한 너비 효과를 포함한 NTK 통계의 수정을 계산하고, 이는 대수적 조작을 단순화하며, 계층별 재귀 관계를 계산하는 데 도움을 준다.

Result: 안정성 결과를 NTK로 확장해 유한 너비 수정이 존재하지 않음을 증명하고 실험을 통해 결과를 검증하였다.

Conclusion: 유한 너비 신경망에서의 NTK 통계의 효용성을 입증하고, 특정 비선형성에 대해 유한 너비 수정이 없음을 보여주었다.

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [55] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 물리정보 확산 모델에 기반한 비지도 이상 탐지 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다변량 시계열 데이터에서 물리 정보에 의존하는 모델을 활용하여 이상 탐지 성능을 향상시키기 위해.

Method: 가중 물리 정보 손실을 사용하여 확산 모델을 훈련시키고, 이를 통해 시계열 데이터의 물리적 분포를 학습한다.

Result: 합성 및 실제 데이터셋에서 F1 점수 향상, 데이터 다양성 증가 및 로그 우도 개선을 입증하였다.

Conclusion: 우리 모델은 기존 접근법을 초월하며, 합성 데이터셋과 하나의 실세계 데이터셋에서 경쟁력을 발휘한다.

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [56] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST는 분산된 그래프 학습을 위한 새로운 프레임워크로, 적응형 통신 메커니즘을 통해 클라이언트 간의 통신 구조를 최적화하여 model aggregation을 효율적으로 이끈다.


<details>
  <summary>Details</summary>
Motivation: 기존 DFL 최적화 전략이 지역 서브그래프의 고유한 위상 정보에 대한 문제를 해결하지 못하고 있기 때문에 이러한 문제를 해결하기 위해 연구가 필요하다.

Method: DFed-SST는 이중 위상 적응형 통신 메커니즘을 활용하여 클라이언트의 지역 서브그래프의 독특한 위상적 특성을 활용하고, 클라이언트 간 통신 위상을 동적으로 구성하고 최적화하는 프레임워크이다.

Result: 8개의 실제 데이터셋에 대한 광범위한 실험 결과, DFed-SST는 기존 기법보다 평균 정확도를 3.26% 향상시키는 우수함을 입증하였다.

Conclusion: DFed-SST는 클라이언트의 이질성에 직면했을 때 모델 집합을 효율적으로 안내할 수 있는 것이다.

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [57] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 이 논문은 고차원 동적 시스템의 스냅샷 데이터에서 물리정보 기반의 축소차원 모델(ROM)을 학습하기 위한 데이터 기반의 중첩 연산자 추론(OpInf) 접근 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 고차원 동적 시스템에서 효율적인 축소차원 모델을 학습하기 위해.

Method: 중첩 연산자 추론 방법을 사용하여 우선적인 지배 모드의 상호작용을 고려한 초기 추정을 반복적으로 구축한다.

Result: 중첩 OpInf가 표준 OpInf보다 네 배 작은 오류를 달성했다.

Conclusion: 이 방법은 그린란드 빙상의 대규모 매개변수화 모델에 적용되었으며, 평균 3% 오류와 19,000 이상의 계산 속도 향상을 이루었다.

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [58] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow는 산업 규모의 강화 학습을 위한 서버 기반 프레임워크로, 실행 흐름과 GPU 활용도를 최적화합니다.


<details>
  <summary>Details</summary>
Motivation: 산업 규모의 강화 학습에서 RL 훈련과 에이전트의 복잡한 실행 흐름을 분리하고, 대규모 배치에 필요한 안정성과 확장성을 유지하면서 GPU 활용도를 극대화하는 것이 필요합니다.

Method: SeamlessFlow는 RL 훈련기를 복잡한 에이전트 구현과 분리시키는 데이터 평면과, 하드웨어를 태깅된 자원으로 추상화하는 태그 기반 스케줄링 패러다임을 도입합니다.

Result: SeamlessFlow는 완전한 상호작용 기록을 유지하고 부분 롤아웃을 지원하여 안정성과 성능을 동시에 제공합니다.

Conclusion: 이러한 혁신을 통해 SeamlessFlow는 다중 에이전트, 긴 지평선 및 복잡한 RL 작업에 적합합니다.

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [59] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 이 논문은 탄소 포집 및 저장(CCS) 프로젝트에서 이해관계자 간의 협력 및 최적화 전략을 분석하기 위해 마르코프 게임을 기반으로 한 패러다임을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: CCS 프로젝트의 복잡성과 장기적인 특성을 고려할 때, 이해관계자들이 독립적으로 이익을 극대화할 수 있는지 협력적인 연합 계약이 필요한지에 대한 질문은 효과적인 프로젝트 계획 및 관리의 핵심 과제입니다.

Method: 여러 이해관계자와 사이트가 포함된 문제를 안전 제약이 있는 다중 에이전트 강화 학습 문제로 재구성하고, E2C 프레임워크 기반의 대체 모델을 사용하여 최적 전략을 학습합니다.

Result: 여러 목표를 가진 이해관계자들이 포함된 상황에서 CO2 저장 관리의 최적화를 위해 제안된 프레임워크의 효과성을 입증했습니다.

Conclusion: 우리의 연구는 CCS 프로젝트의 이해관계자 간 협력의 중요성과 고급 모델의 반복 시뮬레이션 비용 문제를 해결하는 방법을 제시합니다.

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: 이 논문은 ASPIC+를 위한 지능형 지반 절차를 제안하여 규칙 기반 논증의 일관성을 유지하면서 입력 이론의 크기를 관리 가능한 수준으로 유지하는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: ASPIC+는 AI를 위한 규칙 기반 논증의 주요 일반 프레임워크 중 하나로, 1차 논리 규칙을 사용하려는 필요성이 증가하고 있다.

Method: 이 논문에서는 1차 ASPIC+ 인스턴스를 Datalog 프로그램으로 변환하고, Datalog 엔진을 쿼리하여 규칙과 반대어의 지반을 수행하기 위한 대체물을 얻는 지능형 지반 절차를 제안한다.

Result: 제안된 방법은 규칙이 추론 과정에 미치는 영향을 고려하여 지반의 크기를 관리 가능하게 유지하는 동시에 추론 과정의 정확성을 보장한다.

Conclusion: 프로토타입 구현의 실증적 평가를 통해 제안된 방법의 확장성을 보여주었다.

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [61] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 상황에서 알고리즘적 구제를 위한 새로운 프레임워크를 제안하며, 이는 사회적 복지를 극대화하고 개인의 행동 가능성을 유지하는 방향으로 나아갑니다.


<details>
  <summary>Details</summary>
Motivation: 결정권자가 민감한 상황에서 머신러닝에 의존하고 있으며, 알고리즘적 구제가 필요하다는 점이 본 연구의 동기입니다.

Method: 여러 구제 요청제와 구제 제공자를 고려한 다중 에이전트 알고리즘적 구제 프레임워크를 도입하고, 이를 정량적으로 최적화하기 위한 세 가지 단계의 최적화 프레임워크를 설계했습니다.

Result: 우리 프레임워크는 실험적으로 다수 대 다수의 알고리즘적 구제가 거의 최적의 복지를 달성할 수 있음을 입증했습니다.

Conclusion: 이 연구는 알고리즘적 구제를 개인 추천에서 시스템 수준 설계로 확장하여 개인의 행동 가능성을 유지하면서 사회적 복지를 높이는 접근법을 제공합니다.

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [62] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 본 연구는 H&N 암을 위한 양성자 PBS 치료 계획에서의 다수의 상충되는 목표를 해결하기 위해 데이터 기반의 역최적화 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: H&N암의 프로톤 PBS 치료 계획에서는 다수의 상충되는 목표가 존재하여 인간 계획자가 여러 임상 목표를 균형 있게 만족시키기 위해 상당한 노력을 기울여야 한다.

Method: 데이터 기반의 역최적화 기법을 PPO 기반의 자동 치료 계획 프레임워크에 통합하여 임상적으로 인정된 계획 시간 내에 고품질의 계획을 자동으로 생성한다.

Result: 97명의 환자를 대상으로 한 연구에서, L-BFGSB와 비교했을 때 L2O 기반의 역최적화 기법은 효과성 22.97%, 효율성 36.41%를 향상시킨다.

Conclusion: PPO 기반의 학습된 가상 계획자와 함께 생성된 계획은 평균 2.55시간 내에 이루어지며, 인간 생성 계획에 비해 다양한 처방 용량, 목표 부피 수, 빔 각도 등에 대해 향상된 목표 민감성을 보인다.

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [63] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 본 연구에서는 가정 기반 논증의 맥락에서 허용성 개념을 확장하여 강한 허용성과 약한 허용성의 두 가지 주요 대안을 논의하고, 일반 ABA를 위한 선호, 완전 및 기초 의미론을 도입한다.


<details>
  <summary>Details</summary>
Motivation: 가정 기반 논증에서 허용성 개념의 확장을 통해 강한 허용성과 약한 허용성의 중요성을 탐구하고자 한다.

Method: 추상 쌍방향 집합 기반 논증 프레임워크(BSAFs)를 사용하여 일반 비평면 ABA를 위한 의미론을 개발한다.

Result: 강한 허용성을 ABA에 도입하고, 이의 바람직한 특성을 조사하며, 약한 허용성의 최근 조사 결과를 비평면 ABA에 확대 적용하였다.

Conclusion: 강한 허용성과 약한 허용성이 비평면 ABA에서 표준 허용 의미론의 단점을 공유함을 보여주고, 이를 해결할 방법을 논의한다.

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [64] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 이 논문에서는 대규모 추론 모델이 문제를 적극적으로 질문하는 능력 부족을 발견하고, 이에 대한 새로운 데이터셋을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 추론 모델(LRMs)은 수학 문제 해결 능력을 보여주었지만, 충분한 정보를 요청할 수 있는 능력은 부족합니다.

Method: 새로운 데이터셋을 제안하고 LRMs의 평가를 체계적으로 수행하여 정보를 요청하는 능력을 평가합니다.

Result: LRMs가 정보 요청에서의 비능력을 드러내고, 과도한 생각과 환각과 관련된 행동을 발견하였습니다.

Conclusion: 진정한 지능을 가진 LRM 개발을 위한 새로운 통찰을 제공하고자 합니다.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [65] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE는 지속적인 지식 그래프 임베딩을 위한 스케일 인식 점진적 발전 프레임워크로, KG의 동적 특성을 반영하며 최적의 성능을 발휘한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 지식 그래프 임베딩 방법은 정적 그래프에 초점을 맞추고 있어, 동적으로 발전하는 실제 KG의 요구에 부응하지 못한다.

Method: SAGE는 업데이트 스케일에 따라 임베딩 차원을 결정하고, 다이나믹 증류 메커니즘을 사용하여 새로운 사실의 통합과 학습된 지식의 보존을 균형 있게 유지한다.

Result: SAGE는 7개의 벤치마크에서 실험하였으며, 기존의 기준선보다 항상 우수한 성능을 나타냈고 MRR에서 1.38%, H@1에서 1.25%, H@10에서 1.6% 개선을 기록했다.

Conclusion: SAGE는 각 스냅샷에서 최적의 성능을 달성함으로써, CKGE에서 적응형 임베딩 차원의 중요성을 강조한다.

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [66] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: 이 논문은 CRAFT-GUI라는 커리큘럼 학습 프레임워크를 제안하며, GUI 작업에서의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트가 그래픽 사용자 인터페이스 환경을 이해하고 상호작용하는 능력이 향상됨에 따라 자동화된 작업 실행의 새로운 시대가 열리고 있습니다.

Method: Group Relative Policy Optimization (GRPO)를 기반으로 한 CRAFT-GUI라는 커리큘럼 학습 프레임워크를 제안합니다.

Result: 실험 결과, 우리의 방법이 이전의 최첨단 접근 방식보다 5.6%와 10.3%의 성능 향상을 달성했습니다.

Conclusion: 강화 학습과 커리큘럼 학습의 통합이 GUI 상호작용 작업에서 효과적임을 실증적으로 검증했습니다.

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [67] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM) 에이전트의 재고 결정 과정에서의 편향과 불확실한 상황에서의 성능을 평가하기 위한 AIM-Bench 벤치마크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 재고 결정 능력과 그 결정 편향에 관한 연구가 부족하다.

Method: AIM-Bench 벤치마크를 통해 다양한 재고 보충 실험을 수행하여 LLM 에이전트의 의사결정 행동을 평가하였다.

Result: 여러 LLM들이 인간에서 관찰되는 것과 유사한 결정 편향을 보였다.

Conclusion: LLM의 재고 결정 과정에서의 잠재적 편향을 신중하게 고려해야 하며, 공급망을 위한 인간 중심의 의사결정 지원 시스템 개발이 필요하다.

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [68] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: 본 논문은 Inclusion Arena라는 플랫폼을 제안하여 LLMs와 MLLMs의 성능을 실제 사용자 피드백을 기반으로 평가하고, 동적 환경에서 모델 비교를 통한 신뢰성 있는 순위를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 현재 벤치마크가 실제 세계의 애플리케이션에서의 성능을 반영하지 못하므로, 이러한 중요한 격차를 해소하기 위해 Inclusion Arena를 제안한다.

Method: Inclusion Arena는 AI 기반 애플리케이션에서 직접 수집된 인간 피드백에 기초하여 모델을 평가하며, Placement Matches와 Proximity Sampling이라는 두 가지 혁신적인 전략을 통해 모델의 초기 평가와 비교를 최적화한다.

Result: 광범위한 경험적 분석 및 시뮬레이션 결과, Inclusion Arena는 신뢰할 수 있고 안정적인 순위를 제공하며, 일반적인 크라우드 소스 데이터셋에 비해 데이터 전이성이 더 높고 악의적인 조작의 위험을 상당히 완화했다.

Conclusion: Inclusion Arena는 기초 모델들과 실제 애플리케이션 간의 열린 동맹을 촉진하여 실용적이고 사용자 중심의 배치를 최적화하는 LLMs 및 MLLMs의 개발을 가속화하는 것을 목표로 한다.

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [69] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 이 논문에서는 확률론적 랜드마크를 정형화하고 UCT 알고리즘을 조정하여 MDP를 분해하는 데 사용하였습니다. 실험 결과, 잘 선택된 랜드마크가 온라인 확률 계획에서 UCT의 성능을 상당히 향상시킬 수 있음을 보여주었습니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 계획에서 랜드마크는 주요 발전에 기여했지만, 확률적 영역에서는 거의 사용되지 않았다.

Method: 확률론적 랜드마크를 정형화하고 UCT 알고리즘을 조정하여 랜드마크를 서브 목표로 활용하였다.

Result: 벤치마크 도메인에서 잘 선택된 랜드마크는 온라인 확률 계획에서 UCT의 성능을 상당히 향상시켰으며, 탐욕적인 목표 달성과 장기 목표 달성 간의 최적의 균형은 문제에 따라 다르다.

Conclusion: 랜드마크는 MDP 문제를 해결하기 위한 anytime 알고리즘에 유용한 지침을 제공할 수 있다.

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [70] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 본 논문은 LLM을 활용하여 대규모 계획 문제를 해결하는 새로운 계획자 시스템을 제안하고, 문제 분해를 통해 검색 공간을 최적화한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 계획 문제 해결의 중요성이 증가함에 따라 효과적인 검색 공간 축소 방법이 필요하다.

Method: 문제 분해를 통해 대규모 계획 문제를 여러 간단한 하위 작업으로 나누고, LLM4Inspire와 LLM4Predict라는 두 가지 새로운 패러다임을 활용한다.

Result: 여러 도메인에서 계획자의 효과iveness를 검증하여 대규모 계획 문제 해결 시 검색 공간 분할 능력을 입증한다.

Conclusion: 도메인 특화 지식을 LLM에 통합할 경우 더 나은 가능한 솔루션 검색이 이루어진다는 것을 보여준다.

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications](https://arxiv.org/abs/2508.10991)
*Wenpeng Xing,Zhonghao Qi,Yupeng Qin,Yilin Li,Caini Chang,Jiahui Yu,Changting Lin,Zhenzhen Xie,Meng Han*

Main category: cs.CR

TL;DR: LLM과 외부 도구의 통합은 보안 취약점을 초래하며, 이를 해결하기 위해 MCP-Guard라는 방어 아키텍처를 제안함.


<details>
  <summary>Details</summary>
Motivation: LLM과 외부 도구 간의 통합은 프롬프트 주입, 데이터 유출 등 여러 보안 취약점을 일으킵니다.

Method: MCP-Guard는 경량 정적 스캐닝, 심층 신경 탐지기로 악성 공격 탐지, E5 기반 모델을 활용한 적대적 프롬프트 식별을 포함하는 세 단계의 탐지 파이프라인을 사용합니다.

Result: 우리의 E5 기반 모델은 적대적 프롬프트 식별에서 96.01의 정확도를 기록했습니다.

Conclusion: MCP-AttackBench를 통해 70,000개 이상의 샘플을 포함하는 포괄적인 벤치마크를 제공하며, 이는 LLM-도구 생태계를 보호하기 위한 미래 연구의 기초가 됩니다.

Abstract: The integration of Large Language Models (LLMs) with external tools via
protocols such as the Model Context Protocol (MCP) introduces critical security
vulnerabilities, including prompt injection, data exfiltration, and other
threats. To counter these challenges, we propose MCP-Guard, a robust, layered
defense architecture designed for LLM--tool interactions. MCP-Guard employs a
three-stage detection pipeline that balances efficiency with accuracy: it
progresses from lightweight static scanning for overt threats and a deep neural
detector for semantic attacks, to our fine-tuned E5-based model achieves
(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM
arbitrator synthesizes these signals to deliver the final decision while
minimizing false positives. To facilitate rigorous training and evaluation, we
also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000
samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench
simulates diverse, real-world attack vectors in the MCP format, providing a
foundation for future research into securing LLM-tool ecosystems.

</details>


### [72] [A Constant-Time Hardware Architecture for the CSIDH Key-Exchange Protocol](https://arxiv.org/abs/2508.11082)
*Sina Bagheri,Masoud Kaveh,Francisco Hernando-Gallego,Diego Martín,Nuria Serrano*

Main category: cs.CR

TL;DR: 본 논문은 CSIDH 알고리즘의 하드웨어 성능을 FPGA와 ASIC 플랫폼에서 살펴보고, 키 생성 지연 및 성능 기준을 제시한다.


<details>
  <summary>Details</summary>
Motivation: CSIDH 알고리즘은 양자 저항 키 교환 프로토콜로, 작은 키 크기를 자랑하지만 키 생성에 많은 계산 자원이 소모된다.

Method: FPGA와 ASIC 플랫폼에서 공통 아키텍처를 기반으로 CSIDH에 대한 하드웨어 성능 연구를 수행하였다.

Result: FPGA에서는 200 MHz의 클럭 주파수와 515 ms의 키 생성 지연을, 180nm 공정의 ASIC에서는 	extasciitilde 180 MHz의 주파수와 591 ms의 키 생성 지연을 달성하였다.

Conclusion: 하드웨어 성능 메트릭을 제공함으로써 CSIDH를 기반으로 한 향후 양자 저항 암호화 기술의 발전에 중요한 기준을 제시하였다.

Abstract: The commutative supersingular isogeny Diffie-Hellman (CSIDH) algorithm is a
promising post-quantum key exchange protocol, notable for its exceptionally
small key sizes, but hindered by computationally intensive key generation.
Furthermore, practical implementations must operate in constant time to
mitigate side-channel vulnerabilities, which presents an additional performance
challenge. This paper presents, to our knowledge, the first comprehensive
hardware study of CSIDH, establishing a performance baseline with a unified
architecture on both field-programmable gate array (FPGA) and
application-specific integrated circuit (ASIC) platforms. The architecture
features a top-level finite state machine (FSM) that orchestrates a deeply
pipelined arithmetic logic unit (ALU) to accelerate the underlying 512-bit
finite field operations. The ALU employs a parallelized schoolbook multiplier,
completing a 512$\times$512-bit multiplication in 22 clock cycles and enabling
a full Montgomery modular multiplication in 87 cycles. The constant-time
CSIDH-512 design requires $1.03\times10^{8}$ clock cycles per key generation.
When implemented on a Xilinx Zynq UltraScale+ FPGA, the architecture achieves a
200 MHz clock frequency, corresponding to a 515 ms latency. For ASIC
implementation in a 180nm process, the design requires $1.065\times10^{8}$
clock cycles and achieves a \textasciitilde 180 MHz frequency, resulting in a
key generation latency of 591 ms. By providing the first public hardware
performance metrics for CSIDH on both FPGA and ASIC platforms, this work
delivers a crucial benchmark for future isogeny-based post-quantum cryptography
(PQC) accelerators.

</details>


### [73] [HEIR: A Universal Compiler for Homomorphic Encryption](https://arxiv.org/abs/2508.11095)
*Asra Ali,Jaeho Choi,Bryant Gipson,Shruthi Gorantala,Jeremy Kun,Wouter Legiest,Lawrence Lim,Alexander Viand,Meron Zerihun Demissie,Hongren Zheng*

Main category: cs.CR

TL;DR: 이 논문은 동형 암호 컴파일러를 구축하기 위한 통합 접근 방식인 동형 암호 중간 표현(HEIR)을 제시한다.


<details>
  <summary>Details</summary>
Motivation: HEIR는 동형 암호의 모든 주요 기술을 지원하고, 주요 소프트웨어 라이브러리 및 하드웨어 가속기와 통합하며, 연구 및 벤치마킹을 위한 플랫폼을 제공하기 위해 개발되었다.

Method: HEIR는 MLIR 컴파일러 프레임워크 위에 구축되며, 기존 최적화 및 새로운 연구 아이디어를 쉽게 구현할 수 있는 HE 전용 추상화 계층을 도입한다.

Result: HEIR는 HE 최적화 공간을 효과적으로 탐색할 수 있는 수단을 제공하며, Python을 포함한 다양한 프론트엔드를 지원하여 전체 HE 스택을 다룬다.

Conclusion: 이 연구의 기여에는 HE 컴파일러를 구축하기 위한 프레임워크로서의 HEIR 도입, HEIR의 디자인 검증, HEIR가 학술 연구 및 산업 개발을 위한 사실상 동형 암호 컴파일러로 떠오르고 있다는 증거 제공이 포함된다.

Abstract: This work presents Homomorphic Encryption Intermediate Representation (HEIR),
a unified approach to building homomorphic encryption (HE) compilers. HEIR aims
to support all mainstream techniques in homomorphic encryption, integrate with
all major software libraries and hardware accelerators, and advance the field
by providing a platform for research and benchmarking. Built on the MLIR
compiler framework, HEIR introduces HE-specific abstraction layers at which
existing optimizations and new research ideas may be easily implemented.
Although many HE optimization techniques have been proposed, it remains
difficult to combine or compare them effectively. HEIR provides a means to
effectively explore the space of HE optimizations. HEIR addresses the entire HE
stack and includes support for various frontends, including Python. The
contribution of this work includes: (1) We introduce HEIR as a framework for
building HE compilers. (2) We validate HEIR's design by porting a large
fraction of the HE literature to HEIR, and we argue that HEIR can tackle more
complicated and diverse programs than prior literature. (3) We provide evidence
that HEIR is emerging as the de facto HE compiler for academic research and
industry development.

</details>


### [74] [Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks](https://arxiv.org/abs/2508.11325)
*Georgios Michail Makrakis,Jeroen Pijpker,Remco Hassing,Rob Loves,Stephen McCombie*

Main category: cs.CR

TL;DR: 해양 산업에 대한 사이버 위협이 증가함에 따라, 독창적인 사이버 보안 접근 방식이 필요하다. 본 논문에서는 함선의 VSAT 시스템을 시뮬레이션하기 위해 설계된 honeynet인 Salty Seagull을 제안하고, 사이버 공격 캠페인에 대한 통찰을 수집하는 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 해양 산업에 대한 사이버 위협 증가에 대응하기 위해 혁신적인 사이버 보안 접근 방식이 필요하다.

Method: Salty Seagull이라는 honeynet을 통해 함선의 VSAT 시스템을 시뮬레이트 하고, 웹 대시보드와 CLI 환경을 통해 사용자와의 상호작용을 가능하게 한다.

Result: 30일 동안 인터넷에 노출시킨 결과, 많은 일반적인 공격이 시도되었지만, 시스템의 특성과 취약성에 대해 아는 호기심 있는 해커 단 한 명만이 접근할 수 있었다.

Conclusion: 일반적인 공격 시도는 많았지만, 공격자는 시스템의 전체 잠재력을 탐색하지는 못했다.

Abstract: Cyber threats against the maritime industry have increased notably in recent
years, highlighting the need for innovative cybersecurity approaches. Ships, as
critical assets, possess highly specialized and interconnected network
infrastructures, where their legacy systems and operational constraints further
exacerbate their vulnerability to cyberattacks. To better understand this
evolving threat landscape, we propose the use of cyber-deception techniques and
in particular honeynets, as a means to gather valuable insights into ongoing
attack campaigns targeting the maritime sector.
  In this paper we present Salty Seagull, a honeynet conceived to simulate a
VSAT system for ships. This environment mimics the operations of a functional
VSAT system onboard and, at the same time, enables a user to interact with it
through a Web dashboard and a CLI environment. Furthermore, based on existing
vulnerabilities, we purposefully integrate them into our system to increase
attacker engagement. We exposed our honeynet for 30 days to the Internet to
assess its capability and measured the received interaction. Results show that
while numerous generic attacks have been attempted, only one curious attacker
with knowledge of the nature of the system and its vulnerabilities managed to
access it, without however exploring its full potential.

</details>


### [75] [RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning](https://arxiv.org/abs/2508.11472)
*Yang Wang,Yaxin Zhao,Xinyu Jiao,Sihan Xu,Xiangrui Cai,Ying Zhang,Xiaojie Yuan*

Main category: cs.CR

TL;DR: 이 연구에서는 행동 수준의 약한 레이블을 도입하여 내부 위협 감지의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 행동 수준의 세부 주석이 부족하여 사용자 행동 시퀀스 내에서 특정 행동 수준의 이상을 감지하기가 어렵습니다.

Method: Robust Multi-sphere Learning(RMSL)이라는 새로운 프레임워크를 제안합니다. RMSL은 여러 개의 하이퍼 구를 사용하여 정상 행동 패턴을 나타냅니다.

Result: RMSL은 내부 위협 감지의 행동 수준 성능을 크게 향상시킵니다.

Conclusion: 이 접근법은 모델이 정상 행동과 이상 행동을 구분하는 능력을 강화합니다.

Abstract: Insider threat detection aims to identify malicious user behavior by
analyzing logs that record user interactions. Due to the lack of fine-grained
behavior-level annotations, detecting specific behavior-level anomalies within
user behavior sequences is challenging. Unsupervised methods face high false
positive rates and miss rates due to the inherent ambiguity between normal and
anomalous behaviors. In this work, we instead introduce weak labels of behavior
sequences, which have lower annotation costs, i.e., the training labels
(anomalous or normal) are at sequence-level instead of behavior-level, to
enhance the detection capability for behavior-level anomalies by learning
discriminative features. To achieve this, we propose a novel framework called
Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to
represent the normal patterns of behaviors. Initially, a one-class classifier
is constructed as a good anomaly-supervision-free starting point. Building on
this, using multiple instance learning and adaptive behavior-level
self-training debiasing based on model prediction confidence, the framework
further refines hyper-spheres and feature representations using weak
sequence-level labels. This approach enhances the model's ability to
distinguish between normal and anomalous behaviors. Extensive experiments
demonstrate that RMSL significantly improves the performance of behavior-level
insider threat detection.

</details>


### [76] [KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value Estimation](https://arxiv.org/abs/2508.11495)
*Jingnan Xu,Leixia Wang,Xiaofeng Meng*

Main category: cs.CR

TL;DR: 이 논문은 LDP 기반 키-값 추정 메커니즘을 감사하기 위해 KV-Auditor라는 프레임워크를 제안하며, 이를 통해 사용자 데이터 보호를 확인한다.


<details>
  <summary>Details</summary>
Motivation: LDP의 이론적 경계는 개인정보 보호에 있어 엄격하지만, 실제 구현 시 실수가 있을 수 있으므로 감사가 필수적이다.

Method: KV-Auditor는 무제한 출력 분포를 분석하여 LDP의 경험적 개인 정보 보호 하한을 추정하는 프레임워크로, 대화형 및 비대화형 메커니즘에 맞춰 설계되었다.

Result: 다양한 실험을 통해 KV-Auditor의 효과성을 입증하며 LDP 기반 키-값 추정기 최적화에 대한 통찰을 제공한다.

Conclusion: KV-Auditor는 LDP 감사의 새로운 장을 열며, 연속 데이터를 지원하는 방식으로 개인 정보 보호를 평가한다.

Abstract: To protect privacy for data-collection-based services, local differential
privacy (LDP) is widely adopted due to its rigorous theoretical bound on
privacy loss. However, mistakes in complex theoretical analysis or subtle
implementation errors may undermine its practical guarantee. To address this,
auditing is crucial to confirm that LDP protocols truly protect user data.
However, existing auditing methods, though, mainly target machine learning and
federated learning tasks based on centralized differentially privacy (DP), with
limited attention to LDP. Moreover, the few studies on LDP auditing focus
solely on simple frequency estimation task for discrete data, leaving
correlated key-value data - which requires both discrete frequency estimation
for keys and continuous mean estimation for values - unexplored.
  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based
key-value estimation mechanisms by estimating their empirical privacy lower
bounds. Rather than traditional LDP auditing methods that relies on binary
output predictions, KV-Auditor estimates this lower bound by analyzing
unbounded output distributions, supporting continuous data. Specifically, we
classify state-of-the-art LDP key-value mechanisms into interactive and
non-interactive types. For non-interactive mechanisms, we propose horizontal
KV-Auditor for small domains with sufficient samples and vertical KV-Auditor
for large domains with limited samples. For interactive mechanisms, we design a
segmentation strategy to capture incremental privacy leakage across iterations.
Finally, we perform extensive experiments to validate the effectiveness of our
approach, offering insights for optimizing LDP-based key-value estimators.

</details>


### [77] [Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends](https://arxiv.org/abs/2508.11548)
*Zhenhua Xu,Xubin Yue,Zhebo Wang,Qichen Liu,Xixiang Zhao,Jingxuan Zhang,Wenjun Zeng,Wengpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.CR

TL;DR: 대형 언어 모델의 저작권 보호가 중요하며, 이에 대한 기존 연구는 주로 텍스트 워터마킹 기법에 초점을 맞췄지만, 모델 자체 보호를 위한 방법에 대한 체계적인 탐색이 부족하다. 본 연구는 모델 지문채집에 초점을 맞춘 LLM 저작권 보호 기술에 대한 포괄적인 조사를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 개발 비용과 독점적 가치, 오용 가능성을 고려할 때 저작권 보호가 중요합니다.

Method: 텍스트 워터마킹과 모델 워터마킹 및 지문채집 간의 개념적 연결을 명확히 하고, 다양한 텍스트 워터마킹 기법을 개관하며, 기존 모델 지문채집 접근 방식을 체계적으로 분류 및 비교하는 방식을 채택했습니다.

Result: 모델 지문채집에 대한 새로운 기술과 평가 지표를 제시하였으며, 향후 연구 방향에 대해 논의했습니다.

Conclusion: 이 연구는 연구자들이 LLM 시대의 텍스트 워터마킹 및 모델 지문채집 기술을 종합적으로 이해할 수 있도록 돕고 지적 재산 보호의 발전을 촉진합니다.

Abstract: Copyright protection for large language models is of critical importance,
given their substantial development costs, proprietary value, and potential for
misuse. Existing surveys have predominantly focused on techniques for tracing
LLM-generated content-namely, text watermarking-while a systematic exploration
of methods for protecting the models themselves (i.e., model watermarking and
model fingerprinting) remains absent. Moreover, the relationships and
distinctions among text watermarking, model watermarking, and model
fingerprinting have not been comprehensively clarified. This work presents a
comprehensive survey of the current state of LLM copyright protection
technologies, with a focus on model fingerprinting, covering the following
aspects: (1) clarifying the conceptual connection from text watermarking to
model watermarking and fingerprinting, and adopting a unified terminology that
incorporates model watermarking into the broader fingerprinting framework; (2)
providing an overview and comparison of diverse text watermarking techniques,
highlighting cases where such methods can function as model fingerprinting; (3)
systematically categorizing and comparing existing model fingerprinting
approaches for LLM copyright protection; (4) presenting, for the first time,
techniques for fingerprint transfer and fingerprint removal; (5) summarizing
evaluation metrics for model fingerprints, including effectiveness,
harmlessness, robustness, stealthiness, and reliability; and (6) discussing
open challenges and future research directions. This survey aims to offer
researchers a thorough understanding of both text watermarking and model
fingerprinting technologies in the era of LLMs, thereby fostering further
advances in protecting their intellectual property.

</details>


### [78] [Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks](https://arxiv.org/abs/2508.11563)
*Nathaniel Moyer,Charalampos Papamanthou,Evgenios Kornaropoulos*

Main category: cs.CR

TL;DR: 이 논문은 고차원 암호화된 데이터에 대한 유출 악용 공격에서 주파수 분석의 기법을 통해 액세스 패턴 유출로부터 복원할 수 있는 정보와 이를 해결하기 위한 LAMA 공격 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 검색 가능한 암호화(SE)는 암호화된 데이터에서 검색을 위한 가장 확장 가능한 암호학적 원시입니다. 그러나 기존 SE 구조는 액세스 패턴 유출을 허용하며, 이로 인해 서버의 응답에서 어떤 암호화된 레코드가 검색되는지가 드러납니다.

Method: 본 연구에서는 암호화된 범위 쿼리를 지원하는 스킴에 대한 유출 악용 공격의 맥락에서 주파수 분석의 암호 분석 기법에 집중합니다. 주파수 분석은 쿼리 분포의 지식을 바탕으로 특정 암호화된 레코드의 검색 확률과 대응되는 평문 값의 검색 빈도를 비교합니다.

Result: 우리는 주파수 분석의 한계를 끌어올리는 LAMA라는 일반적인 공격 프레임워크를 도입하여 모든 종류의 볼록 쿼리에 대해 우리의 결과가 유효하다는 것을 보여주었습니다. 또한 주파수 분석을 공격자에게 도전적으로 만드는 쿼리 분포를 식별했습니다.

Conclusion: 마지막으로, LAMA를 구현 및 벤치마킹하고, 최대 4차원에 걸친 암호화된 범위 쿼리에서 평문 데이터를 처음으로 복원했습니다.

Abstract: Searchable encryption (SE) is the most scalable cryptographic primitive for
searching on encrypted data. Typical SE constructions often allow
access-pattern leakage, revealing which encrypted records are retrieved in the
server's responses. All the known generic cryptanalyses assume either that the
queries are issued uniformly at random or that the attacker observes the
search-pattern leakage. It remains unclear what can be reconstructed when using
only the access-pattern leakage and knowledge of the query distribution. In
this work, we focus on the cryptanalytic technique of frequency analysis in the
context of leakage-abuse attacks on schemes that support encrypted range
queries. Frequency analysis matches the frequency of retrieval of an encrypted
record with a plaintext value based on its probability of retrieval that
follows from the knowledge of the query distribution. We generalize this
underexplored cryptanalytic technique and introduce a generic attack framework
called Leakage-Abuse via Matching (LAMA) that works even on high-dimensional
encrypted data. We identify a parameterization of LAMA that brings frequency
analysis to its limit -- that is, we prove that there is no additional
frequency matching that an attacker can perform to refine the result.
Furthermore, we show that our results hold for any class of convex queries, and
not just axis-aligned rectangles, which is the assumption in all other attacks
on range schemes. Using these results, we identify query distributions that
make frequency analysis challenging for the attacker and, thus, can act as a
mitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,
for the first time, plaintext data from encrypted range queries spanning up to
four dimensions.

</details>


### [79] [Activate Me!: Designing Efficient Activation Functions for Privacy-Preserving Machine Learning with Fully Homomorphic Encryption](https://arxiv.org/abs/2508.11575)
*Nges Brian Njungle,Michel A. Kinsy*

Main category: cs.CR

TL;DR: 본 연구는 완전 동형암호(FHE)를 기반으로 하는 머신러닝에서의 비선형 활성화 함수 구현의 어려움을 해결하기 위해 설계, 구현 및 평가한 활성화 함수를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 민감한 분야인 의료 및 방위에서 머신러닝의 확산이 개인 정보 보호 및 보안 문제를 유발하고 있습니다.

Method: Square 함수와 ReLU 활성화 함수를 LeNet-5 및 ResNet-20 아키텍처에 적용하고, FHE 제약 하에서 안전하게 ReLU를 평가하기 위한 두 가지 방법(전통적인 저차 다항식 근사 및 새로운 스킴 전환 기법)을 사용하여 평가합니다.

Result: Square 함수는 LeNet-5와 같은 얕은 네트워크에서 99.4% 정확도를 달성하였고, ResNet-20과 같은 깊은 모델에서는 ReLU가 보다 유리하며, 스킴 전환 방법은 89.8%의 정확도를 달성하지만 더 긴 추론 시간이 소요되었습니다.

Conclusion: FHE 기반 머신러닝에서 활성화 함수의 속도와 정확도 사이의 중요한 트레이드오프를 보여줍니다.

Abstract: The growing adoption of machine learning in sensitive areas such as
healthcare and defense introduces significant privacy and security challenges.
These domains demand robust data protection, as models depend on large volumes
of sensitive information for both training and inference. Fully Homomorphic
Encryption (FHE) presents a compelling solution by enabling computations
directly on encrypted data, maintaining confidentiality across the entire
machine learning workflow. However, FHE inherently supports only linear
operations, making it difficult to implement non-linear activation functions,
essential components of modern neural networks. This work focuses on designing,
implementing, and evaluating activation functions tailored for FHE-based
machine learning. We investigate two commonly used functions: the Square
function and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20
architectures with the CKKS scheme from the OpenFHE library. For ReLU, we
assess two methods: a conventional low-degree polynomial approximation and a
novel scheme-switching technique that securely evaluates ReLU under FHE
constraints. Our findings show that the Square function performs well in
shallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per
image. In contrast, deeper models like ResNet-20 benefit more from ReLU. The
polynomial approximation yields 83.8% accuracy with 1,145 seconds per image,
while our scheme-switching method improves accuracy to 89.8%, albeit with a
longer inference time of 1,697 seconds. These results underscore a critical
trade-off in FHE-based ML: faster activation functions often reduce accuracy,
whereas those preserving accuracy demand greater computational resources.

</details>


### [80] [CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection](https://arxiv.org/abs/2508.11599)
*Zhihao Li,Zimo Ji,Tao Zheng,Hao Ren,Xiao Lan*

Main category: cs.CR

TL;DR: CryptoScope는 자동화된 암호화 취약점 탐지를 위한 새로운 프레임워크로, 대형 언어 모델(LLM)을 기반으로 한다.


<details>
  <summary>Details</summary>
Motivation: 암호화 알고리즘 구현에서 로직 결함을 발견하기 어려운 문제를 해결하기 위해.

Method: Chain-of-Thought(CoT) 프롬프트와 Retrieval-Augmented Generation(RAG)을 결합하여 12,000개 이상의 항목을 포함하는 암호화 지식 기반에 안내받는다.

Result: CryptoScope는 LLM-CLVA 벤치마크에서 성능을 일관되게 향상시키며, 다양한 LLM에서 DeepSeek-V3를 11.62%, GPT-4o-mini를 20.28%, GLM-4-Flash를 28.69% 향상시킨다.

Conclusion: 또한, 널리 사용되는 오픈소스 암호화 프로젝트에서 9개의 이전에 공개되지 않은 결함을 식별한다.

Abstract: Cryptographic algorithms are fundamental to modern security, yet their
implementations frequently harbor subtle logic flaws that are hard to detect.
We introduce CryptoScope, a novel framework for automated cryptographic
vulnerability detection powered by Large Language Models (LLMs). CryptoScope
combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation
(RAG), guided by a curated cryptographic knowledge base containing over 12,000
entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily
derived from real-world CVE vulnerabilities, complemented by cryptographic
challenges from major Capture The Flag (CTF) competitions and synthetic
examples across 11 programming languages. CryptoScope consistently improves
performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,
GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9
previously undisclosed flaws in widely used open-source cryptographic projects.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [81] [Allen: Rethinking MAS Design through Step-Level Policy Autonomy](https://arxiv.org/abs/2508.11294)
*Qiangong Zhou,Zhiting Wang,Mingyou Yao,Zongyang Liu*

Main category: cs.MA

TL;DR: 새로운 다중 에이전트 시스템 Allen을 도입하여 에이전트의 정책 자율성을 향상시키고 복잡한 네트워크에서 협업 효율성과 작업 감독, 인간 감독 간의 균형을 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 다중 에이전트 시스템 설계에서 시스템의 정책 자율성을 개선하고 복잡한 네트워크에서 협업 효율성과 인간 감독 간의 균형을 이루는 것이 필요합니다.

Method: 기본 실행 단위를 재정의하고, 에이전트가 이 단위를 조합하여 자율적으로 다양한 패턴을 형성할 수 있도록 하였습니다. 네 가지 계층의 상태 아키텍처(작업, 단계, 에이전트, 단계)를 구성하여 시스템 동작을 제어합니다.

Result: Allen은 정책 자율성을 부여하면서 협업 구조의 제어 가능성을 위한 균형을 맞췄습니다.

Conclusion: 프로젝트 코드는 오픈 소스로 제공됩니다: https://github.com/motern88/Allen

Abstract: We introduce a new Multi-Agent System (MAS) - Allen, designed to address two
core challenges in current MAS design: (1) improve system's policy autonomy,
empowering agents to dynamically adapt their behavioral strategies, and (2)
achieving the trade-off between collaborative efficiency, task supervision, and
human oversight in complex network topologies.
  Our core insight is to redefine the basic execution unit in the MAS, allowing
agents to autonomously form different patterns by combining these units. We
have constructed a four-tier state architecture (Task, Stage, Agent, Step) to
constrain system behavior from both task-oriented and execution-oriented
perspectives. This achieves a unification of topological optimization and
controllable progress.
  Allen grants unprecedented Policy Autonomy, while making a trade-off for the
controllability of the collaborative structure. The project code has been open
source at: https://github.com/motern88/Allen

</details>


### [82] [Defending a City from Multi-Drone Attacks: A Sequential Stackelberg Security Games Approach](https://arxiv.org/abs/2508.11380)
*Dolev Mutzari,Tonmoay Deb,Cristian Molinaro,Andrea Pugliese,V. S. Subrahmanian,Sarit Kraus*

Main category: cs.MA

TL;DR: 도시의 다중 드론 공격에 대비하기 위해 방어자는 드론을 배치하고, 이를 통해 공격의 잠재적 피해를 줄이는 전략을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 다중 드론 공격으로부터 도시를 방어하기 위해 효율적인 방어 전략이 필요하다.

Method: Sequential Stackelberg Security Game 모델을 사용하고, 방어자가 혼합 연속 방어 전략을 우선 이행한 이후 공격자가 최적 반응을 보이는 방식이다.

Result: 80개의 실제 도시 데이터를 바탕으로 한 실험에서 S2D2 알고리즘이 방어자의 성과를 이전 연구의 탐욕적 휴리스틱보다 개선하였다.

Conclusion: 도시 구조에 대한 합리적인 가정 하에 S2D2는 편리한 구조의 근사 강력 스택켈버그 균형을 출력함을 증명하였다.

Abstract: To counter an imminent multi-drone attack on a city, defenders have deployed
drones across the city. These drones must intercept/eliminate the threat, thus
reducing potential damage from the attack. We model this as a Sequential
Stackelberg Security Game, where the defender first commits to a mixed
sequential defense strategy, and the attacker then best responds. We develop an
efficient algorithm called S2D2, which outputs a defense strategy. We
demonstrate the efficacy of S2D2 in extensive experiments on data from 80 real
cities, improving the performance of the defender in comparison to greedy
heuristics based on prior works. We prove that under some reasonable
assumptions about the city structure, S2D2 outputs an approximate Strong
Stackelberg Equilibrium (SSE) with a convenient structure.

</details>


### [83] [Tapas are free! Training-Free Adaptation of Programmatic Agents via LLM-Guided Program Synthesis in Dynamic Environments](https://arxiv.org/abs/2508.11425)
*Jinwei Hu,Yi Dong,Youcheng Sun,Xiaowei Huang*

Main category: cs.MA

TL;DR: TAPA는 안전-critical 응용 프로그램에서 성능과 신뢰성을 유지하면서 동적 조건에 계속 적응할 수 있는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 안전-critical 응용 프로그램에서 자율 에이전트가 동적 조건에 적응할 필요성을 제기한다.

Method: TAPA는 고수준 행동을 위한 모듈 프로그램을 합성하고 적응시키는 방법으로, 대규모 언어 모델을 상징적 행동 공간의 지능형 중재자로 활용한다.

Result: TAPA는 자율 DDoS 방어 시나리오에서 77.7%의 네트워크 가동 시간을 달성하고, swarminter Intelligence 분야에서는 합의 유지에 성공한다.

Conclusion: 이 연구는 정책 적응에서 동적 행동 적응으로 자율 시스템 디자인의 패러다임 전환을 촉진한다.

Abstract: Autonomous agents in safety-critical applications must continuously adapt to
dynamic conditions without compromising performance and reliability. This work
introduces TAPA (Training-free Adaptation of Programmatic Agents), a novel
framework that positions large language models (LLMs) as intelligent moderators
of the symbolic action space. Unlike prior programmatic agents that typically
generate a monolithic policy program or rely on fixed symbolic action sets,
TAPA synthesizes and adapts modular programs for individual high-level actions,
referred to as logical primitives. By decoupling strategic intent from
execution, TAPA enables meta-agents to operate over an abstract, interpretable
action space while the LLM dynamically generates, composes, and refines
symbolic programs tailored to each primitive. Extensive experiments across
cybersecurity and swarm intelligence domains validate TAPA's effectiveness. In
autonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while
maintaining near-perfect detection accuracy in unknown dynamic environments. In
swarm intelligence formation control under environmental and adversarial
disturbances, TAPA consistently preserves consensus at runtime where baseline
methods fail completely. This work promotes a paradigm shift for autonomous
system design in evolving environments, from policy adaptation to dynamic
action adaptation.

</details>
