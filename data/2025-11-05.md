<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 8]
- [cs.LG](#cs.LG) [Total: 36]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.00034)
*Aditya Akella*

Main category: cs.MA

TL;DR: 비분산형 보상 학습 방법이 멀티 에이전트 협력 설정에서 제한적인 성과를 보여주고 있다.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 환경에서 효과적인 보상 신호를 자동으로 발견하고자 하는 필요성이 있다.

Method: DMARL-RSA라는 완전히 비분산된 시스템을 제안하고, 각 에이전트가 개별 보상 형태 학습하게 한다.

Result: DMARL-RSA는 평균 보상 -24.20 +/- 0.09를 달성하며, MAPPO와 비교했을 때 26.12점 차이를 보인다.

Conclusion: 비분산 보상 학습의 한계가 드러나며, 효과적인 다중 에이전트 협력을 위해 중앙 집중적인 조정이 필요함을 강조한다.

Abstract: Recent advances in learnable reward shaping have shown promise in
single-agent reinforcement learning by automatically discovering effective
feedback signals. However, the effectiveness of decentralized learnable reward
shaping in cooperative multi-agent settings remains poorly understood. We
propose DMARL-RSA, a fully decentralized system where each agent learns
individual reward shaping, and evaluate it on cooperative navigation tasks in
the simple_spread_v3 environment. Despite sophisticated reward learning,
DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with
centralized training at 1.92 +/- 0.87 -- a 26.12-point gap. DMARL-RSA performs
similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating
that advanced reward shaping cannot overcome fundamental decentralized
coordination limitations. Interestingly, decentralized methods achieve higher
landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out
of 3 total) but worse overall performance than centralized MAPPO (0.273 +/-
0.008 landmark coverage) -- revealing a coordination paradox between local
optimization and global performance. Analysis identifies three critical
barriers: (1) non-stationarity from concurrent policy updates, (2) exponential
credit assignment complexity, and (3) misalignment between individual reward
optimization and global objectives. These results establish empirical limits
for decentralized reward learning and underscore the necessity of centralized
coordination for effective multi-agent cooperation.

</details>


### [2] [Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System](https://arxiv.org/abs/2511.00096)
*Shangyu Lou*

Main category: cs.MA

TL;DR: Urban-MAS는 인간 중심의 도시 예측을 위해 LLM 기반의 다중 에이전트 시스템 프레임워크를 도입하며, 복잡한 도시 시스템에서 이질적인 데이터를 처리하는 데 도움을 준다.


<details>
  <summary>Details</summary>
Motivation: 도시 AI가 인간 중심의 도시 과업을 발전시킬 수 있으며, 대규모 언어 모델이 도시 시스템의 복잡한 데이터에 통합할 수 있도록 해야 한다.

Method: Urban-MAS 프레임워크는 세 가지 유형의 에이전트를 포함하여 제로샷 환경에서 인간 중심의 도시 예측을 수행한다.

Result: Urban-MAS는 도쿄, 밀라노, 시애틀에서의 실행량 예측 및 도시 인식 실험에서 단일 LLM 기준 대비 오류를 상당히 줄였다.

Conclusion: Urban-MAS는 인간 중심 도시 AI 예측에 있어 확장 가능한 패러다임으로 활용될 수 있다.

Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban
tasks such as perception prediction and human dynamics. Large Language Models
(LLMs) can integrate multimodal inputs to address heterogeneous data in complex
urban systems but often underperform on domain-specific tasks. Urban-MAS, an
LLM-based Multi-Agent System (MAS) framework, is introduced for human-centered
urban prediction under zero-shot settings. It includes three agent types:
Predictive Factor Guidance Agents, which prioritize key predictive factors to
guide knowledge extraction and enhance the effectiveness of compressed urban
knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve
robustness by comparing multiple outputs, validating consistency, and
re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which
integrate extracted multi-source information across dimensions for prediction.
Experiments on running-amount prediction and urban perception across Tokyo,
Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors
compared to single-LLM baselines. Ablation studies indicate that Predictive
Factor Guidance Agents are most critical for enhancing predictive performance,
positioning Urban-MAS as a scalable paradigm for human-centered urban AI
prediction. Code is available on the project
website:https://github.com/THETUREHOOHA/UrbanMAS

</details>


### [3] [Sherlock: Reliable and Efficient Agentic Workflow Execution](https://arxiv.org/abs/2511.00330)
*Yeonju Ro,Haoran Qiu,Íñigo Goiri,Rodrigo Fonseca,Ricardo Bianchini,Aditya Akella,Zhangyang Wang,Mattan Erez,Esha Choukse*

Main category: cs.MA

TL;DR: 대규모 언어 모델의 채택이 증가함에 따라 에이전트 워크플로우가 전통적인 애플리케이션을 대체하고 있으나, 이러한 워크플로우는 오류가 발생하기 쉽다. 본 연구에서는 오류가 발생하기 쉬운 노드를 식별하고 필요할 경우에만 비용 최적화된 검증기를 선택하는 방법을 제안하며, 결과적으로 효율성과 신뢰성을 균형 있게 조화시킨다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델을 사용하는 에이전트 워크플로우의 오류 문제를 해결하고자 함.

Method: Counterfactual 분석을 사용하여 오류가 발생하기 쉬운 노드를 식별하고, 필요할 때 비용 최적화된 검증기를 선택하여 검증을 진행.

Result: 검증을 통해 워크플로우 실행 시간을 최대 48.7% 단축하고, 비용을 26.0% 절감하며, 정확도는 평균 18.3% 향상됨.

Conclusion: 원칙적이고 결함 인식 기반의 검증이 에이전트 워크플로우에서 효율성과 신뢰성을 효과적으로 조화시킬 수 있음을 보여준다.

Abstract: With the increasing adoption of large language models (LLM), agentic
workflows, which compose multiple LLM calls with tools, retrieval, and
reasoning steps, are increasingly replacing traditional applications. However,
such workflows are inherently error-prone: incorrect or partially correct
output at one step can propagate or even amplify through subsequent stages,
compounding the impact on the final output. Recent work proposes integrating
verifiers that validate LLM output or actions, such as self-reflection, debate,
or LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant
latency and cost overheads.
  In this work, we seek to answer three key questions: which nodes in a
workflow are most error-prone and thus deserve costly verification, how to
select the most appropriate verifier for each node, and how to use verification
with minimal impact to latency? Our solution, Sherlock, addresses these using
counterfactual analysis on agentic workflows to identify error-prone nodes and
selectively attaching cost-optimal verifiers only where necessary. At runtime,
Sherlock speculatively executes downstream tasks to reduce latency overhead,
while verification runs in the background. If verification fails, execution is
rolled back to the last verified output. Compared to the non-verifying
baseline, Sherlock delivers an 18.3% accuracy gain on average across
benchmarks. Sherlock reduces workflow execution time by up to 48.7% over
non-speculative execution and lowers verification cost by 26.0% compared to the
Monte Carlo search-based method, demonstrating that principled, fault-aware
verification effectively balances efficiency and reliability in agentic
workflows.

</details>


### [4] [AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems](https://arxiv.org/abs/2511.00628)
*Yang Li,Siqi Ping,Xiyu Chen,Xiaojian Qi,Zigan Wang,Ye Luo,Xiaowei Zhang*

Main category: cs.MA

TL;DR: AgentGit은 다중 에이전트 시스템의 신뢰성과 확장성을 향상시키기 위한 프레임워크로, Git과 유사한 롤백 및 브랜칭 기능을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)의 빠른 발전과 함께 LLM 기반의 다중 에이전트 시스템(MAS)이 증가하는 관심을 받고 있으나, 현재의 많은 MAS 프레임워크는 복잡한 작업에 대한 신뢰성과 확장성에 어려움을 겪고 있습니다.

Method: AgentGit는 LangGraph 위에 구축된 인프라 레이어로, 상태 커밋, 되돌리기 및 브랜칭을 지원하여 에이전트가 여러 경로를 효율적으로 탐색, 비교 및 탐험할 수 있게 합니다.

Result: 실험을 통해 AgentGit이 중복 계산을 크게 줄이고, 실행 시간과 토큰 사용량을 낮추며, 여러 브랜치에서의 병렬 탐색을 지원하여 MAS 개발의 신뢰성과 확장성을 향상시킨다는 결과를 보여주었습니다.

Conclusion: 이 연구는 더 강력한 MAS 설계를 위한 실용적인 경로를 제시하고, 협업 AI 시스템에서 오류 복구, 안전한 탐색, 반복적 디버깅 및 A/B 테스트를 가능하게 합니다.

Abstract: With the rapid progress of large language models (LLMs), LLM-powered
multi-agent systems (MAS) are drawing increasing interest across academia and
industry. However, many current MAS frameworks struggle with reliability and
scalability, especially on complex tasks. We present AgentGit, a framework that
brings Git-like rollback and branching to MAS workflows. Built as an
infrastructure layer on top of LangGraph, AgentGit supports state commit,
revert, and branching, allowing agents to traverse, compare, and explore
multiple trajectories efficiently. To evaluate AgentGit, we designed an
experiment that optimizes target agents by selecting better prompts. We ran a
multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --
on a real-world task: retrieving and analyzing paper abstracts. Results show
that AgentGit significantly reduces redundant computation, lowers runtime and
token usage, and supports parallel exploration across multiple branches,
enhancing both reliability and scalability in MAS development. This work offers
a practical path to more robust MAS design and enables error recovery, safe
exploration, iterative debugging, and A/B testing in collaborative AI systems.

</details>


### [5] [Predictive Auxiliary Learning for Belief-based Multi-Agent Systems](https://arxiv.org/abs/2511.01078)
*Qinwei Huang,Stefan Wang,Simon Khan,Garrett Katz,Qinru Qiu*

Main category: cs.MA

TL;DR: 이 연구는 다중 에이전트 강화 학습(MARL)이 부분 관측 환경에서 효과적으로 정보를 집계하여 학습 효율성과 안정성을 개선할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 부분적으로 관측 가능한 환경에서 MARL의 성능을 향상시키기 위해 보상만으로는 부족하다는 것을 인식했다.

Method: BEPAL(framework) 소개, 보조 학습 목표를 포함하여 정책 최적화를 지원하는 방식으로 각 에이전트가 보상 또는 이동 방향 예측 모델을 학습한다.

Result: BEPAL은 성능 지표에서 약 16% 평균 개선을 달성했으며, 기존 방법들보다 안정적인 수렴을 보였다.

Conclusion: BEPAL은 MARL 훈련의 안정성을 높이고 전반적인 성능을 개선하는 효과적인 방법이다.

Abstract: The performance of multi-agent reinforcement learning (MARL) in partially
observable environments depends on effectively aggregating information from
observations, communications, and reward signals. While most existing
multi-agent systems primarily rely on rewards as the only feedback for policy
training, our research shows that introducing auxiliary predictive tasks can
significantly enhance learning efficiency and stability. We propose
Belief-based Predictive Auxiliary Learning (BEPAL), a framework that
incorporates auxiliary training objectives to support policy optimization.
BEPAL follows the centralized training with decentralized execution paradigm.
Each agent learns a belief model that predicts unobservable state information,
such as other agents' rewards or motion directions, alongside its policy model.
By enriching hidden state representations with information that does not
directly contribute to immediate reward maximization, this auxiliary learning
process stabilizes MARL training and improves overall performance. We evaluate
BEPAL in the predator-prey environment and Google Research Football, where it
achieves an average improvement of about 16 percent in performance metrics and
demonstrates more stable convergence compared to baseline methods.

</details>


### [6] [From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models](https://arxiv.org/abs/2511.01310)
*Sureyya Akin,Kavita Srivastava,Prateek B. Kapoor,Pradeep G. Sethi,Sunita Q. Patel,Rahu Srivastava*

Main category: cs.MA

TL;DR: 협력형 다중 에이전트 정책을 고차원 다중모달 감각 입력으로부터 직접 학습하는 것은 샘플 효율성이 낮으며, 우리는 새로운 다중모달 월드 모델 프레임워크를 제안하여 이를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 고차원 다중모달 감각 입력으로부터의 협력형 다중 에이전트 정책 학습의 비효율성을 해결하기 위해서입니다.

Method: 공유된 생성적 다중모달 월드 모델을 기반으로 한 새로운 프레임워크를 제안하며, 스케일러블한 주의 메커니즘을 사용해 모든 에이전트의 감각 정보를 융합하여 환경의 동적 피치를 압축된 잠재 표현으로 학습합니다.

Result: 제안한 프레임워크는 최첨단 비모델 기반 다중 에이전트 강화 학습 기준과 비교하여 수치적으로 훨씬 높은 샘플 효율성을 달성하였습니다.

Conclusion: 성능 성공은 감각 비대칭 환경에서 다중모달 융합의 중요성을 보여주며, 우리의 아키텍처는 센서 드롭아웃에 대한 우수한 강건성을 가진 것을 입증하였습니다.

Abstract: Learning cooperative multi-agent policies directly from high-dimensional,
multimodal sensory inputs like pixels and audio (from pixels) is notoriously
sample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)
algorithms struggle with the joint challenge of representation learning,
partial observability, and credit assignment. To address this, we propose a
novel framework based on a shared, generative Multimodal World Model (MWM). Our
MWM is trained to learn a compressed latent representation of the environment's
dynamics by fusing distributed, multimodal observations from all agents using a
scalable attention-based mechanism. Subsequently, we leverage this learned MWM
as a fast, "imagined" simulator to train cooperative MARL policies (e.g.,
MAPPO) entirely within its latent space, decoupling representation learning
from policy learning. We introduce a new set of challenging multimodal,
multi-agent benchmarks built on a 3D physics simulator. Our experiments
demonstrate that our MWM-MARL framework achieves orders-of-magnitude greater
sample efficiency compared to state-of-the-art model-free MARL baselines. We
further show that our proposed multimodal fusion is essential for task success
in environments with sensory asymmetry and that our architecture provides
superior robustness to sensor-dropout, a critical feature for real-world
deployment.

</details>


### [7] [An Explanation-oriented Inquiry Dialogue Game for Expert Collaborative Recommendations](https://arxiv.org/abs/2511.01489)
*Qurat-ul-ain Shaheen,Katarzyna Budzynska,Carles Sierra*

Main category: cs.MA

TL;DR: 이 연구는 의료 전문가들 간의 협업 대화 요구사항 분석과 설명 가능성을 포함한 다중 에이전트 시스템 설계를 위한 탐색 대화 게임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 의료 전문가 간의 협업 대화의 필요성을 인식하고, 이를 통해 설명 가능성을 설계에 포함시키고자 한다.

Method: 다양한 지식 기반을 가진 전문가들이 협력하여 추천을 하고, 탐색 대화에서 설명 기반의 발화 힘을 결합하여 추론 과정의 풍부한 흔적을 생성하는 대화 게임을 설계하였다.

Result: 형성적 사용자 연구를 통해 대화 게임이 의료 전문가 간의 협업 요구를 충족하는지를 평가하였다.

Conclusion: 이 대화 게임은 의료 커뮤니티에 대한 대화 기반 커뮤니케이션 도구의 실제 가치를 제공함을 보여준다.

Abstract: This work presents a requirement analysis for collaborative dialogues among
medical experts and an inquiry dialogue game based on this analysis for
incorporating explainability into multiagent system design. The game allows
experts with different knowledge bases to collaboratively make recommendations
while generating rich traces of the reasoning process through combining
explanation-based illocutionary forces in an inquiry dialogue. The dialogue
game was implemented as a prototype web-application and evaluated against the
specification through a formative user study. The user study confirms that the
dialogue game meets the needs for collaboration among medical experts. It also
provides insights on the real-life value of dialogue-based communication tools
for the medical community.

</details>


### [8] [Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning](https://arxiv.org/abs/2511.01554)
*Aditya Kapoor,Yash Bhisikar,Benjamin Freed,Jan Peters,Mingfei Sun*

Main category: cs.MA

TL;DR: 이 논문에서는 다중 에이전트 강화 학습에서의 효과적인 통신을 개선하기 위한 새로운 방법을 제안한다. 이는 디지털 메시지의 정밀성을 최적화하는 기술로, DDCL을 통해 구현된다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습(MARL)에서의 효과적인 통신은 성공에 필수적이며, 대역폭에 의해 제한된다.

Method: 우리는 Differentiable Discrete Communication Learning (DDCL) 프레임워크를 일반화하여 무제한 신호를 지원하는 방식으로 확장한다.

Result: 질적 분석을 통해 에이전트가 작업의 정보적 필요에 따라 메시지 정밀성을 동적으로 조절하는 방법을 보여주고, DDCL을 네 가지 최첨단 MARL 알고리즘에 통합하여 대역폭을 현저히 감소시킨다.

Conclusion: 간단한 Transformer 기반 정책이 복잡한 맞춤형 아키텍처와 동등한 성능을 내므로, 맞춤형 통신 디자인의 필요성을 의문하게 만든다.

Abstract: Effective communication in multi-agent reinforcement learning (MARL) is
critical for success but constrained by bandwidth, yet past approaches have
been limited to complex gating mechanisms that only decide \textit{whether} to
communicate, not \textit{how precisely}. Learning to optimize message precision
at the bit-level is fundamentally harder, as the required discretization step
breaks gradient flow. We address this by generalizing Differentiable Discrete
Communication Learning (DDCL), a framework for end-to-end optimization of
discrete messages. Our primary contribution is an extension of DDCL to support
unbounded signals, transforming it into a universal, plug-and-play layer for
any MARL architecture. We verify our approach with three key results. First,
through a qualitative analysis in a controlled environment, we demonstrate
\textit{how} agents learn to dynamically modulate message precision according
to the informational needs of the task. Second, we integrate our variant of
DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth
by over an order of magnitude while matching or exceeding task performance.
Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL
communication: a simple Transformer-based policy leveraging DDCL matches the
performance of complex, specialized architectures, questioning the necessity of
bespoke communication designs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout는 VR 환경에서 인간과 유사하게 자율적으로 탐색하고 상호작용할 수 있는 심층 학습 기반 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: VR 콘텐츠의 품질, 안전성 및 적절성을 보장하는 것은 여전히 중요한 도전 과제이다.

Method: VRScout는 행동 청킹 변환기를 사용하여 인간 시연으로부터 학습하며 다단계 행동 시퀀스를 예측한다.

Result: VRScout는 상업용 VR 타이틀에서 전문가 수준의 성능을 달성하며, 소비자 하드웨어에서 60 FPS의 실시간 추론을 유지한다.

Conclusion: VRScout는 자동 VR 게임 테스트를 위한 실용적이고 확장 가능한 프레임워크로 자리 잡는다.

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [10] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 반편향 선호 최적화(SSPO) 방법을 통해 적은 라벨 데이터와 대량의 비라벨 데이터로부터 인간의 선호를 효과적으로 학습하고 비용을 절감하는 연구.


<details>
  <summary>Details</summary>
Motivation: 언어 모델을 인간의 선호와 일치시키기 위한 방법에서 많은 자원이 소모되고 있는 문제를 해결하고자 함.

Method: 소수의 쌍방 선호 라벨과 대량의 비쌍 샘플을 동시에 학습하는 반편향 선호 최적화(SSPO) 접근법을 사용함.

Result: SSPO는 대규모 비쌍 데이터로부터 잠재적 선호를 효과적으로 추출하며, 아울러 데이터 효율성이 높음을 실험적으로 입증함.

Conclusion: SSPO는 1%의 UltraFeedback으로 Llama3-8B-Instruct로 학습된 경우 10%의 UltraFeedback으로 학습된 강력한 기준을 지속적으로 초과함.

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [11] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 이 논문은 Feature-Guided Analysis(FGA)의 산업적 적용 가능성을 MNIST와 LSC 데이터셋의 벤치마크를 통해 평가하고, 신경망의 동작을 설명하는 규칙을 계산하는 데 있어 FGA의 효과성을 실증적으로 검증하였다.


<details>
  <summary>Details</summary>
Motivation: 신경망의 결정 과정을 이해하는 것은 안전에 중요한 응용 분야에서 매우 중요하다.

Method: 이 논문은 MNIST 및 LSC 데이터셋으로 만든 벤치마크에서 FGA의 적용 가능성을 평가하였다.

Result: 우리의 결과는 FGA가 문헌에서 보고된 결과보다 벤치마크에서 더 높은 정밀도를 가지고 있음을 보여준다.

Conclusion: 신경망 아키텍처, 훈련 및 특성 선택이 FGA의 효과성에 significant한 영향을 미치고, 정밀도에는 미미한 영향을 미친다.

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [12] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 이 논문은 UAV 기반의 열상 이미지를 이용한 분산 학습인 연합 학습(FL)의 실제 적용 및 효과를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 중앙 집중 머신러닝의 한계를 극복하기 위해 연합 학습의 구현과 효과성을 탐구한다.

Method: FL 알고리즘을 실제 배포 시나리오에서 평가하고 여러 FL 접근법을 중앙 집중 학습 기준과 비교한다.

Result: 모델 정확도, 훈련 시간, 통신 오버헤드 및 에너지 사용과 같은 주요 성능 메트릭에서 비교한다.

Conclusion: UAV 기반 이미징의 세분화 작업에서 FL 방법의 실용적 응용 및 한계를 이해하는 데 귀중한 참고자료가 된다.

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [13] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 이 논문은 소리를 넘어 언어 이해의 뇌 속 처리를 탐구하는 연구이다.


<details>
  <summary>Details</summary>
Motivation: 사람이 '집'이라는 단어를 들었을 때, 우리는 단순히 소리를 처리하는 것이 아니라 벽, 문, 기억을 상상한다. 뇌는 원시 음향에서 풍부한 다중 모달 연상으로 이동하면서 의미를 구축한다.

Method: EEG 신호와 평균화된 wav2vec2 음성 임베딩을 정렬하는 최근 연구를 기반으로 하여, 사전 훈련된 모델의 어떤 층이 뇌에서의 이러한 다층 처리를 가장 잘 반영하는지 비교한다.

Result: 우리는 wav2vec2와 CLIP 모델의 임베딩을 비교하고, 자연 언어 지각 중 기록된 EEG와의 정렬을 ridge 회귀와 대조 디코딩을 통해 평가한다. 개별 층, 점진적 연결, 점진적 합산의 세 가지 전략을 테스트한다.

Conclusion: 발견된 결과는 다중 모달, 층 인식 표현을 결합하는 것이 뇌가 언어를 이해하는 방식, 즉 소리뿐만 아니라 경험으로서의 언어 이해에 더 가까워질 수 있음을 시사한다.

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [14] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: TR-GRPO는 토큰 기여도를 조절하여 LLM의 추론 능력을 향상시키는 강화 학습 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 저확률 토큰이 그래디언트 업데이트에서 과도한 영향을 미치는 문제를 해결하고자 한다.

Method: 모델의 예측 확률에 긍정적으로 상관된 토큰 수준의 가중치를 부여하여 저확률 토큰을 낮추고 고확률 토큰을 강조하는 TR-GRPO를 도입하였다.

Result: TR-GRPO는 여러 RLVR 작업에서 GRPO보다 일관되게 우수한 성능을 보였다.

Conclusion: 토큰 기여도를 조절하는 것이 RL 학습에서 중요하며, TR-GRPO는 LLM의 추론을增强하는 강력한 프레임워크로 자리잡았다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [15] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: Test-Time Scaling (TTS)은 대형 언어 모델의 추론 중 추가 계산을 할당하여 모델을 개선하는 방법입니다. 이 연구는 고정 예산 하에서 최적의 모델 조합과 아키텍처를 찾는 새로운 문제를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: TTS에서 최적의 아키텍처와 모델 조합이 작업에 따라 달라질 수 있음을 인지하고, 이를 탐색하는 문제에 대한 필요성을 느꼈습니다.

Method: 이 문제를 다중 LLM 협업 그래프 형태로 구체화하고, 확률적 그래프 최적화로 재정의하였습니다. 또한, Agent-REINFORCE라는 프레임워크를 제안하였습니다.

Result: Agent-REINFORCE는 샘플 효율성과 검색 성능 면에서 기존의 방법들과 비교하여 우수한 결과를 보였습니다.

Conclusion: Agent-REINFORCE는 정확도와 추론 지연 시간의 공동 목표 하에서 최적 그래프를 효과적으로 식별합니다.

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [16] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: MeixnerNet은 Meixner 다항식을 사용하는 새로운 스펙트럴 그래프 신경망으로, 그래프의 특정 스펙트럴 특성에 적응하는 필터를 통해 개선된 성능과 안정성을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 전통적으로 스펙트럴 GNN은 연속적인 직교 다항식을 기반으로 그래프 구조에 적용되는데, 이로 인해 이론적인 불일치가 발생하며 이는 성능 저하와 하이퍼파라미터 설정의 민감성을 초래할 수 있다.

Method: MeixnerNet은 이산 직교 다항식인 Meixner 다항식을 사용하며, 필터의 두 개 주요 형태 매개변수를 학습 가능하게 만들어 특정 그래프의 스펙트럴 특성에 적응할 수 있도록 한다.

Result: MeixnerNet은 최적의 K = 2 설정에서 ChebyNet보다 경쟁력 있는 성능을 달성하며, 3개의 기준에서 2개에서 승리하였다.

Conclusion: MeixnerNet은 ChebyNet이 성능 저하를 보이는 K 다항식 차수 변동에 대해 매우 강력한 안정성을 보인다.

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [17] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: 고밀도 데이터 센터에서의 액체 냉각은 열 관리에 중요하며, 기계 학습 기반 제어기가 에너지 효율성과 신뢰성을 높이는 데 필수적이다.


<details>
  <summary>Details</summary>
Motivation: AI 작업량 증가로 인해 고밀도 데이터 센터에서 열 관리를 위한 액체 냉각의 중요성이 커지고 있다.

Method: LC-Opt는 고성능 컴퓨팅 시스템의 에너지 효율적 액체 냉각을 위한 강화 학습 제어 전략의 벤치마크 환경이다.

Result: RL 에이전트는 IT 캐비닛 수준에서 액체 공급 온도, 유량, 밸브 작동 등을 최적화하며, Gymnasium 인터페이스를 통해 작업량의 동적 변화에 따라 냉각 타워의 세트를 조정한다.

Conclusion: LC-Opt는 사용자 접근성을 높이고 지속 가능한 데이터 센터 액체 냉각 제어 솔루션을 개발할 수 있는 기회를 제공한다.

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [18] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt는 전세계의 데이터 센터에서 AI 워크로드 관리를 최적화하기 위한 오픈 소스 시뮬레이션 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 AI의 증가하는 에너지 수요 및 탄소 발자국을 관리하기 위한 지능적인 작업 부하 관리 필요.

Method: DCcluster-Opt는 실제 데이터셋과 물리학 기반 모델을 결합하여 지속 가능한 작업 스케줄링을 지원.

Result: 복잡한 스케줄링 문제를 해결하고 다양한 목표를 최적화할 수 있는 모듈화된 보상 시스템을 제공한다.

Conclusion: DCcluster-Opt는 다음 세대의 지속 가능한 컴퓨팅 솔루션 개발을 가속화한다.

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [19] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight는 교통 신호 제어를 위한 이중 LLM 아키텍처로, 평균 이동 시간을 20.03% 단축시키고 대기열 길이를 10.74% 줄이는 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 기존 강화 학습 방법들의 한계를 극복하고, 신호 제어의 최적화 효율성을 높이기 위해 대형 언어 모델을 활용하고자 한다.

Method: Herald guided prompts를 통해 맥락 정보를 추출하고, 실시간 조건에 기반하여 대기열 길이를 예측하는 Herald 모듈을 도입한 이중 LLM 구조를 제안한다.

Result: HeraldLight는 CityFlow 시뮬레이션 실험을 통해 평균 이동 시간을 20.03%, 대기열 길이를 10.74% 줄이는 성과를 냈다.

Conclusion: 제안된 방법은 기존 최첨단 기준을 초월한 성능을 보이며, GitHub에서 소스 코드를 제공한다.

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [20] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 본 논문은 자원 집약적인 적대적 프롬프트 최적화 문제를 효율적이며 균등하게 추론 작업으로 변환하는 새로운 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 프롬프트 최적화의 자원 소모 문제를 해결하고자 한다.

Method: 사전훈련된 비자기회귀 생성 LLM을 활용하여 프롬프트-응답 쌍의 공동 분포를 모델링하고, 이를 통해 프롬프트를 직접 조건부 생성하는 접근법을 제안한다.

Result: 생성된 프롬프트는 낮은 혼란도와 다양한 특성을 가지며, 여러 블랙박스 모델에 강력한 전이 특성을 보인다.

Conclusion: 이 프레임워크는 적대적 프롬프트를 넘어 새로운 레드 팀 및 자동 프롬프트 최적화 방향을 열어준다.

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [21] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 이 논문에서는 카오틱 대류 흐름의 RL 기반 제어를 향상시키기 위해 도메인 정보를 활용한 에이전트를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 카오틱 대류 흐름의 안정화는 매우 중요하지만 도전적인 과제입니다.

Method: 다양한 초기 조건과 흐름 체제를 사용하여 Proximal Policy Optimization으로 훈련된 도메인 정보 RL 에이전트를 도입합니다.

Result: 도메인 정보 에이전트는 대류 열 전달을 최대 33%까지 감소시키고 카오틱 흐름에서는 10% 감소시킵니다.

Conclusion: 우리는 도메인 정보를 사용하여 RL 기반 제어의 강건성을 크게 향상시킬 수 있음을 입증했습니다.

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [22] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender는 다중 모달 발견 프레임워크로서, 화학 언어 표현과 분자 그래프 기반 표현을 통합하여 후보 폴리머를 효율적으로 검색하고 다양한 목표 속성에 따라 신뢰할 수 있는 순위를 매깁니다.


<details>
  <summary>Details</summary>
Motivation: 다음 세대 폴리머의 발견을 위한 AI 기반 디자인을 발전시키는 일반화 가능한 다중 모달 패러다임을 수립하기 위해.

Method: PolyBERT에서 화학 언어 표현과 그래프 인코더에서 분자 그래프 기반 표현을 통합하는 다중 모달 발견 프레임워크를 도입.

Result: 언어 기반 유사성을 사용하여 후보 폴리머를 검색하고, 멀티모달 임베딩을 융합하여 여러 목표 속성에 따라 순위를 매김.

Conclusion: PolyRecommender는 관련 폴리머 속성에 걸쳐 효율적인 검색과 강력한 순위를 가능하게 합니다.

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [23] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 이 연구는 생성형 임베딩을 탐구하여 다중 모드 임베딩 태스크를 일반화하는 UME-R1 프레임워크를 제안하며, 기존의 판별 임베딩 모델보다 월등한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 모드 대형 언어 모델(MLLM)의 성공은 다중 모드 임베딩의 발전을 이끌었지만, 기존 모델은 본질적으로 판별적이어서 추론 기반 생성 패러다임의 이점을 제한한다.

Method: UME-R1은 두 단계의 훈련 전략으로 구성된 범용 다중 모드 임베딩 프레임워크를 제안한다. 첫 번째 단계인 콜드 스타트 감독 세부 조정은 모델이 추론 기능을 갖추도록 하고 판별 및 생성 임베딩을 모두 생성할 수 있게 한다; 두 번째 단계인 강화 학습은 추론을 향상시키고 생성 임베딩 품질을 추가로 최적화한다.

Result: UME-R1은 MMEB-V2 벤치마크에서 비디오, 이미지 및 시각적 문서에 걸쳐 78개의 작업을 평가한 결과, 기존의 판별 임베딩 모델을 상당히 초월하며 보다 해석 가능하고 추론 기반의 생성형 다중 모드 임베딩의 기초를 제공한다.

Conclusion: 이 선도적인 연구는 생성형 임베딩이 기존 판별 임베딩에 비해 실질적인 성능 향상을 가능하게 함을 보여준다. 또한, 생성형 및 판별 임베딩의 상호 보완성을 강조하며, 반복 샘플링이 추론 시간의 스케일러블한 가능성을 효과적으로 증가시킨다.

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [24] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: Tree Training은 에이전트 LLM 시나리오에서 반복적인 접두사 계산을 줄이고, 큰 규모의 에이전틱 훈련의 계산 효율성을 개선하기 위한 방법론이다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 LLM에서의 상호작용 과정은 종종 분기 행동을 보이며, 이는 현재의 훈련 파이프라인에서 비효율성을 초래한다.

Method: Tree Training paradigm은 각 공유 접두사를 한 번만 계산하고 관련된 분기에서 그 중간 결과를 재사용하여 계산 효율성을 크게 개선한다.

Result: Tree Packing 및 Gradient Restoration 기법을 통해 최대 3.9배의 훈련 시간 단축을 보여준다.

Conclusion: 이러한 접근 방식은 에이전틱 LLM의 SFT 및 RL 훈련을 더 효율적으로 만든다.

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [25] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 이 논문은 휴식 상태 fMRI를 위한 기초 모델에서 지역 인식 재구성 전략을 탐구하고, 무작위 지역 마스킹에 의존하는 기존 접근 방식을 넘어서는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기초 모델의 출현은 대규모의 이질적인 뇌 이미징 데이터 세트의 가용성 증가에 의해 촉진되고 있습니다.

Method: 우리는 AAL3 자동 해부학적 라벨링 아틀라스를 사용하여 전체 4D fMRI 볼륨에 직접 적용되는 ROI 유도 마스킹 전략을 도입합니다.

Result: 우리의 방법은 ADHD-200 데이터 세트를 사용하여 건강한 대조군과 ADHD로 진단된 개인을 구분하는 분류 정확도에서 4.23% 개선을 달성했습니다.

Conclusion: 지역 인식 재구성의 마스킹이 모델의 해석 가능성을 향상시키고 더 강력하고 차별화된 표현을 제공합니다.

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [26] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 이 연구는 중앙 집중식 의사 결정 패러다임을 통해 다중 에이전트 시스템의 복잡성을 피하는 새로운 단일 에이전트 강화 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 교차로 대기열에 의해 주도되는 교통 혼잡이 도시 생활 수준, 안전, 환경 질 및 경제 효율성에 미치는 영향을 줄이기 위해.

Method: 본 연구는 단일 에이전트 강화 학습 프레임워크를 제안하고, 이 모델은 도로 네트워크의 인접 행렬을 사용하여 실시간 대기 상태와 현재 신호 시간을 통합하여 제어 정책을 학습합니다.

Result: SUMO에서 수행된 시뮬레이션 실험에서 이 모델은 다중 수준의 기원-목적지 수요 변동(10%, 20%, 30%) 하에서 강력한 반변동 능력을 보여주었습니다.

Conclusion: probe vehicle 기술과 호환되는 지능형 교통 제어를 위한 새로운 패러다임을 확립하였으며, 향후 연구는 실용성을 높이기 위한 방향을 모색할 것입니다.

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [27] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 이 연구는 LLM을 중심으로 하는 베이지안 네트워크 구조 발견을 위한 통합 프레임워크를 제안하며, 데이터가 없거나 존재할 때 모두를 지원한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 시스템 분석을 위해 변수 간 확률적 관계 이해가 중요하다.

Method: PromptBN을 통해 메타데이터로 LLM을 조회하고 유효한 확률적 관계를 밝혀내며, ReActBN을 통해 관측 데이터를 이용한 비迭代적 정제를 수행한다.

Result: 우리의 방법은 기존 LLM 기반 접근법 및 전통적인 데이터 기반 알고리즘보다 우수한 성능을 보였다.

Conclusion: LLM을 적극적으로 활용한 발견 프로세스를 유지하며, 코드가 공개되어 있다.

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [28] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 본 연구는 프로 하키에서 공격 모멘텀과 득점 가능성을 정량화하고 향상시키기 위한 통합 데이터 기반 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 프로 하키의 공격 성과를 개선하기 위한 효과적인 방법론 필요.

Method: 로지스틱 회귀를 통한 마이크로 이벤트의 해석 가능한 모멘텀 가중치 부여, 그라디언트 부스팅 결정 트리를 사용한 비선형 xG 추정, LSTM 네트워크를 이용한 시간 순서 모델링, PCA에 의한 공간 구조 발견 및 K-Means 클러스터링, X-Learner 인과 추정기를 통한 평균 처리 효과(AET) 추정.

Result: 최적의 이벤트 시퀀스와 형성을 채택했을 때 평균 처리 효과(AET)가 0.12(95% CI: 0.05-0.17, p < 1e-50)로 나타났으며, 이는 득점 잠재력의 15% 상대적 증가에 해당한다.

Conclusion: 전략적으로 구성된 시퀀스와 밀집 형상이 공격 성과를 원인적으로 향상시키는 것을 보여준다.

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [29] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: 이 논문은 공공 지식을 활용하여 개인적인 시간 시계열 데이터를 생성하는 새로운 프레임워크 Pub2Priv를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 금융, 의료 및 에너지 소비와 같은 도메인에서 민감한 시간 시계열 데이터를 공유하는 것이 사생활 문제로 자주 제한됩니다.

Method: Pub2Priv는 공공 데이터를 시간적 및 특성 인코딩으로 변환하는 자체 주의 메커니즘을 사용하여 합성 개인 시퀀스를 생성하는 확산 모델을 위한 조건 입력으로 활용합니다.

Result: 실험 결과에 따르면, Pub2Priv는 금융, 에너지 및 상품 거래 도메인에서 사생활과 유용성의 균형을 개선하는 데 있어 최첨단 벤치마크보다 항상 우수한 성능을 보입니다.

Conclusion: 이 연구는 공개 및 비공식 지식을 활용하여 생성된 개인 데이터의 유용성과 개인 정보 보호를 향상시킬 수 있는 방법을 제안합니다.

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [30] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 본 연구는 강화 학습을 개선하기 위해 데이터 효율성을 높이는 접근법인 PREPO를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현 강화 학습 알고리즘은 많은 연산을 필요로 하지만 최적화에 기여하지 않는 롤아웃이 많아 비용이 많이 든다.

Method: 모델 적응성을 학습하는 지표로 프롬프트 당혹감을 사용하고, 롤아웃 간 상대 엔트로피의 차이를 통해 탐색을 우선시하는 두 가지 상호 보완적인 구성 요소를 갖춘 PREPO를 제안한다.

Result: PREPO는 Qwen 및 Llama 모델에서 수학적 추론 벤치마크에서 기준선보다 최대 3배 적은 롤아웃으로 효과적인 결과를 달성한다.

Conclusion: 경험적 향상 외에도, 우리는 RLVR의 데이터 효율성을 개선하기 위한 방법론의 기초 rationale를 설명하는 이론적이고 심층적인 분석을 제공한다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [31] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache는 KPI 헤드의 시간적 안정성을 활용하여 GPU 메모리 사용량과 계산 오버헤드를 줄이고, 모델 정확도를 유지하는 계층적 KV-캐시 관리 시스템이다.


<details>
  <summary>Details</summary>
Motivation: LLM 서비스는 키-값(KV) 캐시 크기 증가로 제약을 받으며, 기존 시스템은 효율적으로 이를 활용하지 못한다.

Method: FlexiCache는 KV 헤드를 안정적 또는 불안정한 것으로 분류하고, 안정적인 헤드는 상위 K 페이지만 GPU에 유지하며, 나머지는 호스트 메모리로 오프로드한다.

Result: FlexiCache는 긴 컨텍스트 요청에 대해 GPU 메모리 사용량을 최대 70%까지 줄이고, 오프라인 서비스 처리량을 1.38-1.55배 향상시키며, 온라인 토큰 대기 시간을 1.6-2.1배 줄인다.

Conclusion: FlexiCache는 긴 컨텍스트와 긴 생성 시나리오에서 정확도를 유지하면서 성능을 향상시킨다.

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [32] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO는 안전 강화 학습 알고리즘으로, 안전성을 고려한 그래디언트 조작과 K-FAC 기반의 이차 정책 최적화를 결합하여 성능과 안전성을 모두 향상시키는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 정책 최적화 방법이 안전성 제약을 충분히 고려하지 않는 문제를 해결하기 위해 이 방법을 제안한다.

Method: KFCPO는 K-FAC을 이용하여 피셔 정보 행렬을 레이어별로 근사하며, 안전성 경계에 대한 접근성을 고려하여 보상 및 비용 그래디언트의 영향을 조정하는 마진 인식 그래디언트 조작 메커니즘을 도입한다.

Result: Safety Gymnasium에서 OmniSafe를 이용한 실험을 통해 KFCPO는 안전 제약을 준수하는 최상의 기준선에 비해 10.3%에서 50.2% 더 높은 평균 수익을 달성했다.

Conclusion: KFCPO는 안전성과 성능 간의 균형을 우수하게 유지하며, 정책 변화의 불안정을 방지하기 위해 미니배치 수준의 KL 롤백 전략을 채택한다.

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [33] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AI 에이전트의 데이터 처리 능력을 강조하고, 이를 기반으로 한 네 가지 주요 기능을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 인간과의 상호작용에서 데이터 처리를 어떻게 하는지가 중요하다.

Method: 네 가지 주요 기능을 제안한다: 1) 능동적인 데이터 수집, 2) 정교한 데이터 처리, 3) 상호작용하는 테스트 데이터 합성, 4) 지속적인 적응.

Result: 에이전트의 데이터 처리 능력이 AI의 다음 발전 방향으로 자리 잡을 수 있음을 증명하고자 한다.

Conclusion: 데이터에 능숙한 에이전트의 필요성을 인식하고, AI 시스템의 설계에서 우선 순위를 두어야 한다.

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [34] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: 본 연구에서는 비결정적 사고(Teacher)와 실행(Student)을 분리한 ATLAS라는 적응형 교육 및 학습 시스템을 도입하여 모델 파라미터가 아닌 시스템 수준의 조정으로 적응을 구현하는 지속 학습 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 지속적 학습 방법은 실제 상황에서 실시간으로 적응해야 하는 배치된 에이전트에게 맞지 않는 경량 기반 재훈련을 통해 재앙적 망각을 완화하는 데 초점을 맞추었다.

Method: ATLAS는 경험에서 축약된 가이드를 저장하는 지속적인 학습 메모리를 포함하여 사고(Teacher)와 실행(Student)을 분리한 이중 에이전트 아키텍처이다.

Result: ATLAS는 Microsoft의 ExCyTIn-Bench에서 GPT-5-mini를 학생으로 사용할 때 성공률 54.1%를 달성하였으며, 이는 더 큰 GPT-5(High)보다 13% 높은 성과이면서도 비용은 86% 감소하였다.

Conclusion: 이 결과들은 그라디언트 없는 지속적 학습이 적응 가능하고 배포 가능한 AI 시스템을 향한 유망한 경로로 자리 잡고, 명시적인 세계 모델을 훈련하는 데 유용한 인과적으로 주석이 달린 흔적을 제공함을 보여준다.

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [35] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 온라인 이층 최적화는 시간이 지남에 따라 외부 및 내부 목적이 변하는 기계 학습 문제에 대한 강력한 프레임워크입니다. 본 논문에서는 새로운 탐색 방향을 도입하고, 이 방향을 활용한 첫 번째 및 제로 차수 오차의 확률적 OBO 알고리즘이 창문 매끄러움 없이 서브선형 이층 오차를 달성함을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 온라인 이층 최적화 접근법은 함수가 빠르게 변화할 때 시스템 성능을 정확하게 반영하지 못하는 결정론적 창문 매끄러움에 의존합니다.

Method: 새로운 탐색 방향을 도입하고, 이를 활용하여 첫 번째 및 제로 차수의 확률적 온라인 이층 최적화 알고리즘을 제안합니다.

Result: 우리의 방법론은 서브선형 확률적 이층 오차를 달성하며, oracle 의존성을 줄이고, 내부 및 외부 변수를 선형 시스템 솔루션과 함께 업데이트하며, Hessians, Jacobians 및 기울기의 ZO 기반 추정을 사용합니다.

Conclusion: 온라인 매개변수 손실 조정 및 블랙박스 적대적 공격에 대한 실험을 통해 우리의 접근법을 검증했습니다.

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [36] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 이 논문은 전기차 충전소의 최적 배치를 위한 새로운 딥 강화 학습 프레임워크를 제안하며, 실제 상황에서의 효율성을 높입니다.


<details>
  <summary>Details</summary>
Motivation: 전기차(EV)의 급속한 성장으로 인해 충전소의 전략적 배치가 필요합니다.

Method: 딥 강화 학습과 에이전트 기반 시뮬레이션을 통합하여 EV의 이동을 모델링하고 실시간으로 충전 수요를 추정하는 프레임워크를 제안합니다.

Result: 하노이, 베트남에서 사례 연구를 통해 이 방법이 처음 상태에 비해 평균 대기 시간을 53.28% 줄이는 것으로 나타났습니다.

Conclusion: 이 확장 가능하고 적응 가능한 솔루션은 EV 인프라 계획을 향상시키고, 실제 복잡성을 효과적으로 해결하며 사용자 경험을 향상시킵니다.

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [37] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 이 논문은 머신러닝을 사용하여 동적인 환경에서 드론 비행 네트워크(FANET)의 커넥티비티를 향상시키는 방법에 대해 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 무선 환경에서의 머신러닝 기술 적용은 제한적이지만, 드론 네트워크와 같은 동적인 환경에서의 필요성이 증가하고 있기 때문입니다.

Method: 데이터 기반의 쿱만 접근법을 사용하여 UAV 트레젝토리 역학을 모델링합니다.

Result: 제안된 두 가지 접근법을 통해 UAV 간 신뢰할 수 있는 통신을 보장하기 위한 정확한 SINR 예측을 달성했습니다.

Conclusion: 이 접근법이 UAV의 전송 스케줄링에 도움을 줄 수 있음을 보여주었습니다.

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [38] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 본 논문에서는 진정한 제자리 계산을 달성하지 못하는 기존 FFT 구현의 문제를 해결하기 위해 최초의 전실수, 완전 제자리 FFT 프레임워크(rdFFT)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 FFT 및 rFFT 구현이 메모리 및 계산 비용을 줄이는데 한계를 가지고 있으며, rFFT는 입력 크기와 출력 크기 간의 차이로 인한 추가 메모리 할당이 필요하기 때문에 이 문제를 해결하고자 합니다.

Method: 주파수 도메인에서 버터플라이 연산의 대칭성과 공액 성질을 활용하여 중간 캐시 사용을 완전히 제거하는 암묵적 복소수 인코딩 방식을 설계했습니다.

Result: 다수의 자연어 이해 작업에서 이 방법의 효과를 입증하여 교육 메모리 비용을 줄이는 데 효과적임을 보여주었습니다.

Conclusion: 주파수 도메인에서 경량 적응을 위한 유망한 방향을 제공하는 방법입니다.

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [39] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 본 논문은 아프리카 사하라 이남 지역의 클러스터 수준의 부의 지수를 예측하기 위한 그래프 기반 접근 방식을 제안하며, 기존 설문조사의 한계를 개선하는 방법을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 정확하고 세분화된 빈곤 지도는 글로벌 남부의 많은 지역에서 부족하며, DHS 데이터는 좋지만 공간적 범위가 제한적이다.

Method: 저차원 AlphaEarth 위성 임베딩을 활용하고, 조사된 위치와 레이블이 없는 위치 간의 공간 관계를 모델링하고, 좌표 변위를 고려하기 위해 확률적 '퍼지 레이블' 손실을 도입한다.

Result: 37개의 DHS 데이터 세트에서 실험을 통해 그래프 구조를 포함함으로써 '이미지 전용' 기준선에 비해 정확성이 약간 향상됨을 보여준다.

Conclusion: 이 연구는 대규모 사회경제적 매핑을 위한 compact EO 임베딩의 잠재력을 입증한다.

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [40] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 이 논문은 Hadamard 컨볼루션 변환에 기반한 특징 추출 접근법을 제안하여 시간 시계열 분류의 효율성을 증가시키고, 기존 방법보다 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 분류는 통신, 정보 대책, 금융 및 의료 분야에서 광범위한 응용 가치를 지니고 있음에도 불구하고 기존의 최첨단 방법들이 높은 계산 복잡성과 긴 파라미터 조정 및 훈련 주기를 요구합니다.

Method: Hadamard 행렬의 열 또는 행 벡터를 컨볼루션 커널로 사용하는 Hadamard 컨볼루션 변환 기반의 특징 추출 접근법을 제안합니다. 이 접근법은 다양한 크기의 연장된 길이를 가진 커널을 사용합니다.

Result: UCR 시간 시계열 데이터 세트에서의 포괄적인 실험에서 SOTA 성능을 보여주며, F1 점수가 ROCKET 대비 최소 5% 향상되었고, 동일한 하이퍼파라미터에서 miniROCKET보다 50% 짧은 훈련 시간을 기록했습니다.

Conclusion: 제안된 방법은 모든 기존 방법과 완벽하게 호환되면서도 커널 직교성을 활용하여 계산 효율성, 강인성 및 적응성을 높입니다. 모든 코드는 GitHub에서 제공됩니다.

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [41] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM은 멀티 에이전트 Graph-CoT 시스템으로, 추론을 전문화된 에이전트로 분해하여 정확도를 높이고 토큰 소비를 줄인다.


<details>
  <summary>Details</summary>
Motivation: 기존의 Graph-CoT 시스템은 낮은 정확도와 높은 대기 시간 등으로 인해 효율성이 떨어진다.

Method: GLM은 다른 전문 에이전트들(분류, 추론, 행동 생성, 그래프 검색 등)로 추론을 분해하고, 그래프 특화 KV-캐시 관리를 통해 효율적인 서빙을 구현한다.

Result: GLM은 답변 정확도를 최대 38% 높이고, 토큰 비용을 최대 95.7% 줄이며, 대기 시간을 90.3% 낮추어 15.1배 높은 성능을 보인다.

Conclusion: GLM은 복잡한 현실 세계의 추론을 더 효율적으로 적용할 수 있도록 하는 혁신적인 접근법이다.

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [42] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 이 논문은 모바일 엣지 컴퓨팅(MEC)에서 효율적인 병렬 추측 디코딩을 지원하기 위해 사용자 연결 및 자원 할당을 공동 최적화하는 통합 프레임워크를 제안하며, 다중 에이전트 심층 강화 학습 알고리즘을 사용해 UARA 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 기기에서 대규모 언어 모델(LLM) 추론에 대한 증가하는 수요는 자원이 제한된 환경에서 효율적인 모바일 엣지 컴퓨팅(MEC) 솔루션의 필요성을 강조한다.

Method: 다중 에이전트 심층 강화 학습 알고리즘을 사용하여 사용자 연결 및 자원 할당(UARA) 문제를 해결한다.

Result: 우리 방법은 최대 28.0% 및 평균 23.7%의 종단 간 대기 시간을 줄이면서 추론 정확도를 손상시키지 않는 결과를 보여준다.

Conclusion: MEC 시스템에서 확장 가능하고 낮은 대기 시간을 가진 LLM 서비스를 가능하게 한다.

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [43] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: Fractional Diffusion Bridge Models (FDBM)은 비마르코프적 특성을 가진 프렉셔널 브라운 운동(fBM)을 근사하여 구동되는 새로운 생성적 확산 다리 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 실제 확률 과정은 메모리 효과, 장거리 의존성, 거칠기 및 비정상적 확산 현상을 나타내지만, 표준 확산이나 다리 모델링에서는 브라운 운동(BM)의 사용으로 인해 포착되지 않는다.

Method: MA-fBM의 최근 마르코프 근사를 활용하여 FDBM을 구성하여 비마르코프적 특성을 유지하면서도 수월한 추론을 가능하게 한다.

Result: FDBM은 정렬된 데이터에서 미래 단백질 구조의 예측과 비배치 이미지 번역 작업에서 우수한 성능을 보여주며, C$_\alpha$ 원자 위치에 대한 RMSD 감소 및 비배치 이미지 번역에서 Fréchet Inception Distance (FID) 감소를 달성한다.

Conclusion: 앞으로의 예측을 위해 쌍을 이루는 훈련 데이터를 사용하고, 슈뢰딩거 다리 문제로 범위를 확장하여 비배치 데이터 번역을 학습하기 위한 원칙적인 손실 함수를 도출하였다.

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [44] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 이 논문에서는 노이즈가 많은 초음파 데이터로부터 혈류를 추정하기 위해 물리 정보를 활용한 뉴럴 필드 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 혈류 분석은 심장 기능 진단에 중요하지만, 초음파는 심도에 따라 감쇠되어 이미지 품질이 제한됩니다.

Method: 물리 정보 기반의 뉴럴 필드 모델을 사용하여 다중 스케일 푸리에 특징 인코딩으로 희소하고 노이즈가 있는 초음파 데이터에서 혈류를 추정합니다.

Result: 이 모델은 합성 및 실제 데이터셋에서 노이즈 제거 및 인페인트링에서 일관되게 낮은 평균 제곱 오차를 달성했습니다.

Conclusion: 이 방법은 초음파 기반의 흐름 재구성을 위한 특정 도전 과제를 해결하기 위해 다른 이미징 모달리티에서 효과적인 방법을 적응했습니다.

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: 대형 언어 모델이 보안 문제와 가치 시스템 조정의 도전에 직면해 있으며, 제안된 Magic Image 프레임워크가 이를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 유해한 콘텐츠 생성과 안전 메커니즘으로 인해 필요한 쿼리에 대한 과도한 거부 반응 문제에 직면하고 있다는 점에서, 다양한 가치 시스템을 수용하고 주어진 안전 선호와 정밀하게 정렬할 필요가 있다.

Method: 유해/선령 샘플을 사용하여 이미지 프롬프트를 최적화함으로써, 단일 모델이 다양한 가치 시스템에 적응하고 주어진 안전 선호에 맞게 정렬할 수 있게 하는 최적화 기반의 시각적 프롬프트 프레임워크인 Magic Image를 제안한다.

Result: 다양한 데이터 세트에서 모델 성능을 유지하면서 개선된 안전성과 효율성의 균형을 보여주었다.

Conclusion: 배포 가능한 다중 모달 대형 언어 모델의 안전 정렬을 위한 실용적인 해결책을 제공한다.

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [46] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 이 연구는 멀티모달 가짜 리뷰 탐지 프레임워크를 제안하며, 텍스트와 비주얼 데이터를 통합하여 리뷰의 진위 여부를 예측합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자 생성 리뷰가 소비자 행동에 미치는 영향을 증대하고 신뢰를 강화하기 위해 가짜 리뷰 탐지가 필요합니다.

Method: BERT로 인코딩된 텍스트 특성과 ResNet-50으로 추출한 시각적 특성을 융합한 멀티모달 가짜 리뷰 탐지 프레임워크를 개발했습니다.

Result: 멀티모달 모델이 단일 모달 베이스라인보다 우수하며, 테스트 세트에서 F1 점수 0.934를 달성했습니다.

Conclusion: 이 연구는 디지털 신뢰 보호에 있어 멀티모달 학습의 중요성을 보여줍니다.

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [47] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 이 연구는 소매 가격 최적화를 위한 다중 에이전트 강화 학습의 체계적인 실증 연구를 제시하며, MAPPO와 그래프 주의력 증강 변형인 MAPPO+GAT의 성능을 비교한다.


<details>
  <summary>Details</summary>
Motivation: 소매에서의 동적 가격 책정은 수요 변화에 적응하고 관련 제품 간의 결정을 조정하는 정책이 필요하다.

Method: 실제 거래 데이터에서 파생된 시뮬레이션된 가격 책정 환경을 사용하여 수익, 무작위 시드에 대한 안정성, 제품 간 공정성 및 표준화된 평가 프로토콜 하에서의 훈련 효율성을 평가했다.

Result: MAPPO는 포트폴리오 수준의 가격 제어를 위한 강력하고 재현 가능한 기반을 제공하며, MAPPO+GAT는 제품 그래프를 통해 정보를 공유하여 과도한 가격 변동을 유발하지 않고 성능을 향상시킨다.

Conclusion: 그래프 통합 MARL은 동적 소매 가격 책정에 대해 독립 학습자보다 더 확장 가능하고 안정적인 솔루션을 제공하며, 다중 제품 의사 결정에서 실용적인 이점을 제공합니다.

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [48] [GEPOC Parameters -- Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC은 인구 수준 연구 질문을 분석하기 위한 모델과 방법의 집합으로, 오스트리아의 모델 매개변수를 계산하기 위한 데이터 처리 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 모델이 특정 국가나 지역에 유효하게 적용되기 위해서는 안정적이고 재현 가능한 데이터 프로세스가 필요하다.

Method: 자유롭게 접근 가능한 데이터를 바탕으로 한 데이터 처리 방법의 전체 설명과 함께 데이터 집계, 분해, 융합, 정제 및 스케일링에 사용된 알고리즘을 포함한다.

Result: 가장 중요한 GEPOC 모델인 GEPOC ABM의 매개변수를 계산하는 데 중점을 두었다. 이 모델을 사용하여 광범위한 검증 연구를 수행하였다.

Conclusion: 이 연구는 오스트리아의 자료를 활용한 GEPOC의 유용성을 보여준다.

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [49] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 이 논문에서는 통신 네트워크의 자동화된 문제 해결을 위한 다중 에이전트 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 통신 네트워크의 규모와 복잡성이 증가하고 있어 효과적인 관리와 최적화가 어려워지고 있다.

Method: 에이전트 워크플로를 사용하는 다중 에이전트 시스템을 제안하고, 대규모 언어 모델이 다양한 전문 도구를 조정하여 완전 자동화된 네트워크 문제 해결을 수행하도록 한다.

Result: AI/ML 기반 모니터를 통해 고장을 감지한 후, 여러 에이전트를 동적으로 활성화하여 문제를 진단하고 해결 전략을 권장한다.

Conclusion: 제안된 프레임워크는 무선 접속 네트워크와 코어 네트워크 도메인 모두에서 문제 해결 자동화 속도를 현저히 향상시킨다.

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [50] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: AI 엔지니어 팀을 위한 플랫폼인 Engineering.ai를 제시하며, 이를 통해 복잡한 공학 작업을 자율적으로 수행할 수 있는 가능성을 확인했다.


<details>
  <summary>Details</summary>
Motivation: 현대 공학에서는 전문가들이 서로 협력하여 복잡한 제품을 설계하며, 이는 다학제적 복잡성을 관리하는 데 필수적이지만 상당한 개발 시간과 비용이 요구된다.

Method: Engineering.ai는 최고 엔지니어가 전문 에이전트를 조율하는 계층적 다중 에이전트 아키텍처를 채택하여, 각 에이전트는 LLM과 도메인 특정 지식을 활용한다. 에이전트 간 협업은 데이터 출처와 재현성을 보장하기 위해 파일 기반 커뮤니케이션을 통해 이루어진다.

Result: UAV 날개 최적화를 통해 시스템의 신뢰성을 검증하였으며, 400개 이상의 매개변수 구성에서 100% 성공률을 기록하였다.

Conclusion: 에이전트 기반 AI 엔지니어는 복잡한 공학 작업을 자율적으로 수행할 가능성을 보여준다.

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [51] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 에이전틱 AI 프레임워크는 외부 도구를 통합하여 수동 텍스트 오라클을 자율 문제 해결자로 변화시킵니다. 이 논문은 CPU 중심의 관점에서 에이전틱 AI 작업의 시스템 병목 현상을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI 작업에서 CPU 중심의 시스템 병목 현상을 이해하고 특성화하기 위해서입니다.

Method: 오케스트레이터 및 의사 결정 구성 요소, 추론 경로 동역학 및 에이전틱 흐름의 반복성을 기반으로 에이전틱 AI를 체계적으로 특성화합니다. 그런 다음, 대표적인 다섯 개의 에이전틱 AI 작업을 선택하여 지연 시간, 처리량 및 에너지 지표를 프로파일링합니다.

Result: CPU가 지연 시간에 미치는 영향을 분석한 결과, 도구 처리의 최대 90.6%가 CPU에서 발생하며, 처리량은 CPU 또는 GPU 요인에 의해 병목 현상을 겪는 것으로 나타났습니다.

Conclusion: 결과를 바탕으로 CPU 및 GPU 인식 마이크로 배칭(CGAM)과 혼합 에이전틱 작업 스케줄링(MAWS) 최적화를 제안하며, 이는 에이전틱 AI 성능을 향상시킬 수 있습니다.

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [52] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-AGI 벤치마크는 인공지능 일반화 능력 향상을 추적하는 데 중요한 역할을 한다. 이 논문은 원래 ARC-AGI 훈련 데이터 세트를 확장하는 오픈 소스 생성기 ARC-GEN을 소개한다.


<details>
  <summary>Details</summary>
Motivation: ARC-AGI는 인공지능 일반화 능력의 진전을 추적하기 위한 도전적인 벤치마크로, 효율적인 기술 습득을 측정하는 데 초점을 맞추고 있다.

Method: ARC-GEN을 활용하여 원래의 ARC-AGI 훈련 데이터 세트를 가능한 한 충실하게 확장하고, 모든 400개 작업을 포괄하는 포괄적이고 모방적인 생성기를 개발하였다.

Result: ARC-GEN은 ARC-AGI의 분포 속성과 특성을 잘 반영하여, 더 많은 샘플 쌍을 생성할 수 있게 한다.

Conclusion: 이 생성기를 이용해 2025 Google Code Golf Championship에 제출된 프로그램의 정확성을 검증할 수 있는 정적 벤치마크 세트를 수립할 수 있다.

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [53] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)의 법률 작업 통합이 가속화되면서 LLM의 신뢰성을 평가할 벤치마크의 필요성이 대두되었다. 본 논문에서는 법률 추론의 취약성을 평가하기 위해 CLAUSE라는 벤치마크를 도입하고, 7500개의 실제 수정된 계약서를 생성하여 LLM의 세밀한 차이 탐지 능력을 연구했다.


<details>
  <summary>Details</summary>
Motivation: LLM이 법률 분야에서 신뢰성을 갖추기 위한 시스템적 테스트 부족에 대한 문제를 해결하고자 한다.

Method: 7500개의 실제 수정된 계약서를 생성하고, 10개의 이상 징후 카테고리를 만들어 법적 신뢰성을 위해 RAG 시스템으로 검증하는 새로운 인물 중심의 파이프라인을 개발하였다.

Result: 주요 LLM들이 내재된 법적 결함을 탐지하고 그 중요성을 설명하는 능력이 있음을 평가했으나, 미세한 오류를 놓치고 법적으로 정당화하는 데 어려움을 겪는 주요 약점이 발견되었다.

Conclusion: 법률 AI의 추론 실패를 식별하고 수정하는 경로를 제시한다.

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [54] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 이 논문에서는 이진 마법 정사각형(BMS)을 생성하는 간단한 알고리즘을 제안합니다. 이 알고리즘은 항상 유효한 BMS를 생성하며, 비정사각형 BMS에 대한 연구도 포함되어 있습니다.


<details>
  <summary>Details</summary>
Motivation: 이진 마법 정사각형을 생성하는 효율적인 알고리즘을 개발하고 BMS의 존재 조건을 공식화하는 것이 본 연구의 목적입니다.

Method: 우리는 귀납법을 통해 우리의 알고리즘이 항상 최적의 이론적 복잡도로 유효한 BMS를 반환함을 보여줍니다. 이후 비정사각형 BMS에 대한 조건을 공식화하고, 변형된 알고리즘을 제안합니다.

Result: 제안된 알고리즘은 검증 가능한 BMS를 생성하며, GPU 가속을 이용하여 여러 BMS를 병렬로 생성할 수 있는 Python 패키지로 구현되어 공개되었습니다.

Conclusion: 이 연구는 BMS 생성의 효율성을 개선하고, 비정사각형 BMS에 대한 조건을 명확히 하였으며, 실용적인 응용을 위한 코드를 제공함으로써 향후 연구에 기여할 것입니다.

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [55] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 이 연구는 단일 에이전트 기반의 교통 신호 제어 모델을 제안하여 교통 혼잡을 효과적으로 완화한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 프레임워크가 확장성 문제를 가지고 있어 단일 에이전트 프레임워크의 필요성이 대두되었다.

Method: 단일 에이전트 RL 기반의 지역 적응형 교통 신호 제어 모델을 제안하며, 상태, 행동 및 보상 함수 정의를 포함한다.

Result: 제안된 모델이 다중 교차로 제어를 통해 대규모 지역 혼잡 수준을 효과적으로 완화함을 보여준다.

Conclusion: 탐색 차량 데이터를 활용하여 신뢰성 높은 추정을 가능하게 하고, 제안된 방법의 광범위한 배치 가능성을 향상시킨다.

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [56] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 이 논문에서는 RL로 후속 학습된 비전-언어 모델(VLM)의 공간적 추론 능력을 개선하기 위한 새로운 프레임워크인 Ariadne를 제안합니다. 이 프레임워크는 합성 미로를 이용해 다단계 공간 추론을 수행하며, RLVR을 통해 VLM을 훈련합니다. 그 결과, 기존 모델이 0%였던 문제에서 50% 이상의 정확도를 달성했습니다.


<details>
  <summary>Details</summary>
Motivation: VLM의 후속 학습이 본래 VLM의 능력 한계를 정말로 확장할 수 있는지를 조사하기 위함입니다.

Method: 합성 미로를 활용하여 다단계 공간 추론을 수행하고, 난이도 인식 커리큘럼 내에서 RLVR을 사용하여 VLM을 훈련합니다.

Result: 후속 RLVR 훈련 후, VLM은 기존 모델이 0%인 문제 세트에서 50% 이상의 정확도를 달성했습니다. 또한, 합성 미로에서 훈련했음에도 불구하고, 실용적인 벤치마크에서 OOD 일반화 평가 시 MapBench에서 평균 16%, ReasonMap에서 24%의 제로샷 개선을 달성했습니다.

Conclusion: 이 연구는 모델의 기본 한계를 확장함과 동시에 실제 공간 추론에 대한 일반화를 향상시킵니다. 그러나 본 연구는 사전 학습 데이터의 불투명성으로 인해 후속 학습 단계에 한정된다는 점을 인식합니다.

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [57] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 샘플링된 추론 경로를 증가시키는 것의 trade-off를 분석한 연구로, 대형 언어 모델에서 자가 일관성이 향상되는 한계를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 자가 일관성에서 샘플링된 추론 경로를 증가시키는 것의 trade-off를 이해하기 위함이다.

Method: Gemini 2.5 모델을 사용하여 HotpotQA와 Math-500 데이터셋에서 다양한 샘플링된 추론 경로의 출력을 비교하였다.

Result: 더 큰 모델은 더 안정적이고 일관된 개선곡선을 보였으며, 중간 샘플링 이후 성능 향상이 완화됨을 확인하였다.

Conclusion: 자가 일관성은 여전히 유용하지만, 고샘플 설정은 계산 비용 대비 거의 이점이 없다.

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [58] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: EHR 예측을 위한 연구에서 다양한 방법론을 비교하였으며, 카운트 기반 모델과 혼합 에이전트 방법의 성능이 비슷함을 발견했다.


<details>
  <summary>Details</summary>
Motivation: 구조화된 전자 건강 기록(EHR)은 임상 예측에 필수적이다.

Method: EHRSHOT 데이터셋을 사용하여 세 가지 방법론을 평가하였다: LightGBM과 TabPFN을 기반으로 한 두 가지 시간 구간의 옥타르롤업으로 구축된 카운트 기반 모델, 사전 훈련된 순차적 변환기(CLMBR), 그리고 표 기반 기록을 자연어 요약으로 변환하고 텍스트 분류기로 이어지는 혼합 에이전트 파이프라인.

Result: EHRSHOT 데이터셋을 사용하여 여덟 가지 결과를 평가했으며, 평가 과제들에서 카운트 기반 모델과 혼합 에이전트 방법 간의 성과가 비슷하게 나왔다.

Conclusion: 카운트 기반 모델은 구조화된 EHR 벤치마킹에 대한 강력한 후보로 남아있다.

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [59] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 인간 여행자의 학습 및 행동 조정을 효과적으로 모델링하는 것이 시스템 평가와 계획에 중요하다. 본 논문에서는 대규모 언어 모델(LLM) 에이전트를 활용한 새로운 이중 에이전트 프레임워크를 제안한다. 이 방법은 인간 여행자의 학습 및 적응 행동을 시뮬레이션하는 LLM 여행자 에이전트와, 이들의 페르소나를 훈련시키는 LLM 보정 에이전트로 구성되어 있다. 실제 데이터 세트를 기반으로 기존 LLM 기반 방법보다 성능이 우수하며, 여행자의 학습 과정을 포착하는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 인간 여행자의 행동이 복잡한 인지 및 결정 과정을 포함하기 때문에, 교통 시스템과 상호작용하면서 그들의 행동을 학습하고 조정하는 과정을 효과적으로 모델링하는 것이 중요하다.

Method: 우리는 온라인 데이터 스트림으로부터 학습 및 적응 행동을 지속적으로 학습하고 LLM 에이전트와 인간 여행자 간의 정렬을 가능하게 하는 새로운 이중 에이전트 프레임워크를 제안한다. 이는 메모리 시스템과 학습 가능한 페르소나를 갖춘 LLM 여행자 에이전트 세트를 포함한다.

Result: 우리의 접근 방식은 개별 행동 정렬과 총체적 시뮬레이션 정확도 모두에서 기존 LLM 기반 방법보다 크게 향상된 성과를 보인다.

Conclusion: 우리 프레임워크는 여행자의 학습과 적응을 시뮬레이션하기 위한 적응형이며 행동적으로 현실적인 에이전트를 생성하는 새로운 접근법을 제공하며, 이는 교통 시뮬레이션 및 정책 분석에 이점을 가져올 수 있다.

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [60] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 이 연구는 암 병리 보고서에서 TNM 병기 추출의 도전 과제를 극복하기 위한 두 가지 지식 끌어내기 방법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 환자의 예후 및 치료 계획을 위해 암 병기가 중요하지만, 비구조적 병리 보고서에서 병리학적 TNM 병기를 추출하는 것은 지속적인 도전 과제입니다.

Method: 첫 번째 방법, 장기 기억을 통한 지식 끌어내기(KEwLTM)는 주석이 없는 병리 보고서에서 직접 병기 규칙을 유도하기 위해 반복적인 프롬프팅 전략을 사용합니다. 두 번째 방법, 검색 증강 생성 기반의 지식 끌어내기(KEwRAG)는 관련 지침에서 규칙을 사전 추출한 후 적용하여 해석력을 향상시키고 반복적인 검색 오버헤드를 피합니다.

Result: KEwLTM은 효과적인 Zero-Shot Chain-of-Thought(ZSCOT) 추론에서 KEwRAG보다 우수한 성과를 보였고, ZSCOT 추론이 덜 효과적일 때 KEwRAG이 더 나은 성과를 달성했습니다.

Conclusion: 이 두 방법은 암 병기의 자동화에 대해 향상된 해석력을 가진 확장 가능하고 성능이 높은 해결책으로 사용될 수 있습니다.

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [61] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG라는 새로운 프레임워크를 통해 언어 모델의 성능을 개선하면서 효율성을 유지하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLMs는 매력적인 기능을 보여주지만, 파라메트릭 지식에 의존하는 것은 부정확성을 초래한다.

Method: ET2RAG는 훈련이 필요 없는 방법으로, 가장 관련성 높은 문서를 검색하고 LLM을 보강하여 응답의 다양성을 효율적으로 생성한다.

Result: ET2RAG는 세 가지 작업에서 성능을 크게 향상시킨다: 오픈 도메인 질문 응답, 레시피 생성 및 이미지 캡셔닝.

Conclusion: 응답 길이를 관리함으로써 계산 비용과 성능 간의 균형을 달성할 수 있다.

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [62] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 이 논문은 복잡한 작업 수행 중 단일 에이전트의 한계를 해결하고 대규모 언어 모델을 기반으로 한 모듈형 작업 분해와 동적 협업을 위한 다중 에이전트 아키텍처를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업 수행 중 단일 에이전트의 한계로 인해 작업 분해와 협업에서의 비효율성을 해결하기 위해.

Method: 작업 설명을 통합된 의미 표현으로 변환한 후, 모듈형 분해 메커니즘을 도입하여 전체 목표를 여러 계층의 하위 작업으로 분해하고 동적 스케줄링과 라우팅 메커니즘을 통해 에이전트 간의 실시간 협업을 가능하게 함.

Result: 실험 결과 제안된 방법이 기존 접근 방식보다 성능과 강인성에서 우수하며, 작업 복잡성과 통신 오버헤드 간의 더 나은 균형을 달성함.

Conclusion: 이 연구는 언어 기반 작업 분해 및 동적 협업이 다중 에이전트 시스템에서 효과적이고 실행 가능함을 보여주며, 복잡한 환경에서의 작업 수행을 위한 체계적인 솔루션을 제공합니다.

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [63] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 본 논문은 NeuComBack라는 새로운 벤치마크 데이터셋을 소개하며, LLM을 활용한 신경 컴파일의 성능을 평가하고, LLM의 내부 프롬프트 전략을 발전시키는 자기 발전 프롬프트 최적화 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 컴파일러 개발과 유지에 필요한 복잡성과 높은 인적 자원이 문제로 지적된다. LLM의 발전이 신경 컴파일이라는 새로운 패러다임을 제공하지만, 실질적인 채택에는 여러 장애물이 존재한다.

Method: NeuComBack라는 IR-어셈블리 컴파일을 위한 데이터셋을 설계하고, 신경 컴파일의 기초적인 워크플로우를 정의하며, 최신 LLM의 역량을 평가한다.

Result: 새로운 성능 기준을 설정하고, 제안된 자기 발전 프롬프트 최적화 방법이 LLM 생성 어셈블리 코드의 기능적 정확성과 성능을 향상시킨 결과를 보였다.

Conclusion: 제안한 방법은 LLM이 생성한 어셈블리 코드의 기능적 정확도를 x86_64에서 44%에서 64%로, aarch64에서 36%에서 58%로 개선하였다. 특히, 16개의 올바르게 생성된 x86_64 프로그램 중 14개(87.5%)는 clang-O3 성능을 초과하였다.

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [64] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 대규모 언어 모델(LLM)의 취약점을 파악하는 것은 안전성을 개선하기 위해 중요하다. 이 논문은 새로운 메타 최적화 프레임워크인 AMIS를 소개하여 jailbreak 프롬프트와 점수 템플릿을 공동으로 발전시켜 더 나은 성과를 달성한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 안전성을 개선하기 위해 그들의 내재된 취약점을 파악하는 일이 중요하다.

Method: AMIS(Align to MISalign)는 jailbreak 프롬프트와 점수 템플릿을 이원 구조를 통해 공동으로 발전시키는 메타 최적화 프레임워크다.

Result: AMIS는 AdvBench와 JBB-Behaviors에서 평가한 결과, Claude-3.5-Haiku에서 88.0% ASR, Claude-4-Sonnet에서 100.0% ASR을 달성하며 기존 기준을 대폭 초과했다.

Conclusion: AMIS는 현재 최고 수준의 성능을 달성함으로써 jailbreak 프로세스를 혁신하는 데 기여한다.

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [65] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 이 연구는 인공지능 관점에서 이중 작업 패러다임 내의 시간 처리 간섭을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 인간의 타이밍 연구와 유사한 emergent behavior를 가진 AI 모델의 행동을 이해하고, 이를 통해 생물학적 시스템에서 관찰되는 행동을 비교하고자 한다.

Method: Overcooked 환경의 단순화된 버전에서 두 가지 변형, 단일 작업(T) 및 이중 작업(T+N)을 구현하고, 두 작업 각각에 대해 두 개의 깊은 강화 학습(DRL) 에이전트를 훈련하였다.

Result: 이중 작업 에이전트(T+N)는 단일 작업 에이전트(T)와 비교하여 시간 과잉 생산을 나타냈으며, 이 결과는 네 개의 목표 지속 시간 모두에서 일관되었다.

Conclusion: 에이전트의 LSTM 층에서의 신경 역학에 대한 초기 분석은 전용 또는 내재적인 타이머에 대한 명확한 증거를 제공하지 않았으므로, 에이전트의 시간 유지 메커니즘을 이해하기 위한 추가 조사가 필요하다.

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [66] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 본 논문에서는 의학과 같은 고위험 분야의 AI 모델에 대한 설명이 신뢰성을 결여할 수 있음을 지적하고, 이를 해결하기 위해 상호작용하는 에이전트를 제안한다. 이 에이전트는 진단 추론을 지원하기 위해 외부 시각적 증거를 전략적으로 찾는 정책을 학습하며, 이 정책은 강화 학습을 통해 최적화된다. 실험 결과, 이 행동 기반 추론 과정이 정확도를 크게 개선하며, 에이전트의 설명이 결정 과정에 필수적임을 입증하기 위한 인과 개입 방법이 도입되었다.


<details>
  <summary>Details</summary>
Motivation: AI 모델의 설명이 검증 가능성을 결여하여 신뢰를 저해할 수 있음을 인식하고 해결하고자 했다.

Method: 외부 시각적 증거를 지원하기 위해 정책을 학습하는 상호작용 에이전트를 구상하며, 이 정책은 강화 학습을 통해 최적화된다.

Result: 비상호작용 기준선에 비해 Brier 점수를 18% 감소시켜 calibrated accuracy가 유의미하게 향상되었다.

Conclusion: 본 연구는 검증 가능하고 신뢰성 있는 추론 능력을 갖춘 AI 시스템 구축을 위한 실용적인 프레임워크를 제공한다.

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [67] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 이 연구는 능동적인 의료 AI 시스템으로의 변환을 통해 pre-consultation 과정을 개선하는 계층적 다중 에이전트 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 전 세계의 의료 시스템은 환자 수의 증가와 제한된 상담 시간으로 큰 어려움에 직면하고 있습니다.

Method: 8개의 에이전트 아키텍처를 개발하여 pre-consultation을 4개의 주요 작업으로 분해했습니다: Triage, History of Present Illness 수집, Past History 수집, Chief Complaint 생성.

Result: 프레임워크는 주요 부서 triage의 정확도가 87.0%, 2차 부서 분류의 정확도가 80.5%에 도달했습니다.

Conclusion: 자율 AI 시스템의 잠재력을 입증하며, 임상 환경에서 pre-consultation의 효율성과 품질을 향상시킬 수 있습니다.

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [68] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: 본 논문은 TPS-Bench를 도입하여 다양한 도구를 필요로 하는 복합 문제 해결 능력을 평가하는 LLM 에이전트를 벤치마크합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트가 여러 도구를 요구하는 실제 문제를 해결할 수 있는지 여부를 탐구하기 위해.

Method: 200개의 복합 작업을 수집하여 LLM 에이전트의 툴 계획 및 스케줄링 능력을 평가하는 TPS-Bench를 소개.

Result: 대부분의 모델이 합리적인 도구 계획을 수행할 수 있으나 스케줄링에서 차이를 보임. GLM-4.5는 64.72%의 작업 완료율을 보였으나 긴 실행 시간으로 고통받음.

Conclusion: 강화 학습(RL)을 통해 스케줄링 효율성을 개선할 수 있는 가능성을 보여줌.

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [69] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 우리는 지속 가능성과 관련된 기업 소셜 미디어 콘텐츠를 분석하기 위한 다중 모달 분석 파이프라인을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 지속 가능성과 관련된 기업 커뮤니케이션의 복잡성을 해결하기 위해.

Method: 대규모 언어 모델과 비전-언어 모델을 활용하여 기업 트윗과 시각적 지속 가능성 커뮤니케이션을 분석하는 방법론을 제안한다.

Result: 17개 지속 가능 개발 목표(SDGs)에 대한 기업 트윗과 그 패턴을 분석하여 산업 간 차이점과 추세를 밝혀낸다.

Conclusion: 자동 레이블 생성 및 의미적 시각 클러스터링 방법론은 대규모 소셜 미디어 분석에 유연하게 적용될 수 있다.

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [70] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 법률 QA 분야에 적합한 하이브리드 시스템을 통해 법률 상담의 신뢰성과 업데이트 가능성을 보장한다.


<details>
  <summary>Details</summary>
Motivation: AI가 사법 분야에 침투함에 따라 법률 질문 응답의 진위성과 추적 가능성을 보장하는 것이 중요해졌다.

Method: RAG(검색 증강 생성)과 다중 모델 앙상블을 통합하여 신뢰할 수 있고 감사 가능하며 지속적으로 업데이트 가능한 법률 QA 에이전트를 제공한다.

Result: 하이브리드 접근 방식이 단일 모델 기준선과 기존 RAG 파이프라인보다 F1, ROUGE-L, LLM-as-a-Judge 지표에서 성능이 크게 향상되었다.

Conclusion: 제안한 시스템은 환각을 줄이며 답변 품질과 법적 준수를 개선하는 동시에 법적 시나리오에서 미디어 포렌식 기술의 실용적 적용을 진전시킨다.

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [71] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLM 에이전트는 깊은 추론이 필요한 협소한 환경에서 뛰어나지만, 다양한 도구와 스키마가 요구되는 복잡한 환경에서는 취약하다. 본 논문에서는 실제 테스트베드 데이터나 API에 접근하지 않고도 LLM이 현실적인 환경 피드백을 시뮬레이션할 수 있음을 보여준다. 그리고 이를 바탕으로 두 가지 프레임워크를 제안한다: Simia-SFT와 Simia-RL.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 강점을 활용하여 복잡한 환경에서도 강건성을 유지할 수 있는 방법을 찾기 위함이다.

Method: Simia-SFT는 작은 시드 세트를 다채로운 궤적으로 확대하여 SFT 데이터를 합성하는 파이프라인이다. Simia-RL은 LLM 시뮬레이션 피드백을 통해 실제 환경 구현 없이 RL 훈련을 가능하게 하는 프레임워크이다.

Result: 세 가지 벤치마크에서 일관된 개선을 보였으며, GPT-4o를 초과하고 o4-mini에 접근하는 성과를 기록했다.

Conclusion: Simia-SFT와 Simia-RL은 환경 공학 없이도 에이전트 훈련을 확장 가능하게 하여, 기존의 무거운 구현을 유연한 LLM 기반 시뮬레이션으로 대체할 수 있음을 보여준다.

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [72] [DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture](https://arxiv.org/abs/2511.00447)
*Ruofan Liu,Yun Lin,Jin Song Dong*

Main category: cs.CR

TL;DR: DRIP은 대형 언어 모델을 공격으로부터 보호하기 위한 훈련 시간 방어책으로, 지시 의미와 데이터 의미 간의 강력한 분리를 시행하여 효용성을 저하시키지 않고도 보안을 강화한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 지시를 따르는 능력이 뛰어나지만, 이를 통해 지시를 변경하거나 방해하는 공격에 노출된다. 모델의 핵심 취약점은 지시적 의도와 설명적 내용을 구별할 수 없는 것이다.

Method: DRIP은 의미론적 모델링 관점을 기반으로 한 훈련 시간 방어책으로, 지시 의미와 데이터 의미 사이의 강력한 분리를 시행한다. 두 가지 경량화된 메커니즘을 도입한다: (1) 의미적 분리를 수행하는 토큰 단위의 비지시 전환과 (2) 생성 중 실제 최상위 지시의 영향을 강화하기 위한 지속적인 의미적 앵커를 제공하는 잔여 융합 경로가 그것이다.

Result: LLaMA-8B 및 Mistral-7B에서 수행된 실험 결과, DRIP은 StruQ, SecAlign, ISE, PFT와 같은 최신 방어 방법보다 뛰어난 성능을 보이며, 역할 분리를 49% 개선하고 적응형 공격의 성공률을 66% 감소시킨다.

Conclusion: DRIP은 AlpacaEval, IFEval 및 MT-Bench에서 비방어 모델과 유사한 효용성을 가지며, 경량 표현 편집의 힘과 역할 인식 감독이 적응형 프롬프트 주입에 대한 LLM의 보안을 강화하는 데 중요함을 강조한다.

Abstract: Large language models (LLMs) have demonstrated impressive
instruction-following capabilities. However, these capabilities also expose
models to prompt injection attacks, where maliciously crafted inputs overwrite
or distract from the intended instructions. A core vulnerability lies in the
model's lack of semantic role understanding: it cannot distinguish directive
intent from descriptive content, leading it to execute instruction-like phrases
embedded in data.
  We propose DRIP, a training-time defense grounded in a semantic modeling
perspective, which enforces robust separation between instruction and data
semantics without sacrificing utility. DRIP introduces two lightweight yet
complementary mechanisms: (1) a token-wise de-instruction shift that performs
semantic disentanglement, weakening directive semantics in data tokens while
preserving content meaning; and (2) a residual fusion pathway that provides a
persistent semantic anchor, reinforcing the influence of the true top-level
instruction during generation. Experimental results on LLaMA-8B and Mistral-7B
across three prompt injection benchmarks (SEP, AlpacaFarm, and InjecAgent)
demonstrate that DRIP outperforms state-of-the-art defenses, including StruQ,
SecAlign, ISE, and PFT, improving role separation by 49%, and reducing attack
success rate by 66% for adaptive attacks. Meanwhile, DRIP's utility is on par
with the undefended model across AlpacaEval, IFEval, and MT-Bench. Our findings
underscore the power of lightweight representation edits and role-aware
supervision in securing LLMs against adaptive prompt injection.

</details>


### [73] [Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems](https://arxiv.org/abs/2511.01268)
*Minseok Kim,Hankook Lee,Hyungjoon Koo*

Main category: cs.CR

TL;DR: RAGDefender는 RAG 배치에서 데이터 오염 공격에 대한 효율적인 방어 메커니즘으로, 경량 기계 학습 기법을 활용하여 추가 모델 훈련이나 추론 없이 적대적 콘텐츠를 탐지하고 필터링한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)은 웹 기반 서비스로 널리 채택되고 있지만, 생성된 환각 콘텐츠 및 최신 정보에 접근하지 못하는 등의 문제에 직면해 있다.

Method: RAGDefender는 RAG 시스템의 검색 후 단계에서 작동하며, 경량 기계 학습 기법을 사용하여 적대적 콘텐츠를 탐지하고 필터링한다.

Result: RAGDefender는 여러 모델 및 적대적 시나리오에서 기존 방어 방법보다 일관되게 우수한 성능을 보인다. 예를 들어, 적대적 구문이 정당한 구문보다 4배 많은 경우 Gemini 모델에 대한 공격 성공률을 0.89에서 0.02로 낮춘다.

Conclusion: RAGDefender는 데이터 오염 공격에 대한 효율적인 방어 솔루션으로, 기존의 고급 방어 시스템에 비해 더 낮은 공격 성공률을 달성한다.

Abstract: Large language models (LLMs) are reshaping numerous facets of our daily
lives, leading widespread adoption as web-based services. Despite their
versatility, LLMs face notable challenges, such as generating hallucinated
content and lacking access to up-to-date information. Lately, to address such
limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising
direction by generating responses grounded in external knowledge sources. A
typical RAG system consists of i) a retriever that probes a group of relevant
passages from a knowledge base and ii) a generator that formulates a response
based on the retrieved content. However, as with other AI systems, recent
studies demonstrate the vulnerability of RAG, such as knowledge corruption
attacks by injecting misleading information. In response, several defense
strategies have been proposed, including having LLMs inspect the retrieved
passages individually or fine-tuning robust retrievers. While effective, such
approaches often come with substantial computational costs.
  In this work, we introduce RAGDefender, a resource-efficient defense
mechanism against knowledge corruption (i.e., by data poisoning) attacks in
practical RAG deployments. RAGDefender operates during the post-retrieval
phase, leveraging lightweight machine learning techniques to detect and filter
out adversarial content without requiring additional model training or
inference. Our empirical evaluations show that RAGDefender consistently
outperforms existing state-of-the-art defenses across multiple models and
adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR)
against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for
RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber
legitimate ones by a factor of four (4x).

</details>


### [74] [Beyond Static Thresholds: Adaptive RRC Signaling Storm Detection with Extreme Value Theory](https://arxiv.org/abs/2511.01391)
*Dang Kien Nguyen,Rim El Malki,Filippo Rebecchi,Raymond Knopp,Melek Önen*

Main category: cs.CR

TL;DR: 이 논문은 5G 및 그 이후의 네트워크에서 사용자 장비(UE)와 기지국(gNodeB 또는 gNB) 간의 통신이 신호 폭풍에 취약할 수 있음을 보여주며, 이를 방지하기 위해 EVT 기반의 적응형 임계값 감지 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 5G 및 그 이후의 네트워크에서 RRC 계층의 신호 폭풍으로 인한 가용성 위협을 다루기 위해 본 연구를 수행하였다.

Method: Extreme Value Theory (EVT)를 기반으로 한 적응형 임계값 감지 시스템을 제안하며, 실제 RRC 트래픽 데이터를 사용하여 시뮬레이션 공격 시나리오를 적용하여 평가하였다.

Result: 감지 시스템이 공격을 식별하고 합법적인 고 트래픽 상황과 구별할 수 있는 높은 정확도(93% 이상)와 정밀도, 재현율을 보여주었다.

Conclusion: 제안된 시스템은 다양한 트래픽 조건에서도 잘 작동하며, 복잡한 조건에서도 낮은 감지 지연을 유지한다.

Abstract: In 5G and beyond networks, the radio communication between a User Equipment
(UE) and a base station (gNodeB or gNB), also known as the air interface, is a
critical component of network access and connectivity. During the connection
establishment procedure, the Radio Resource Control (RRC) layer can be
vulnerable to signaling storms, which threaten the availability of the radio
access control plane. These attacks may occur when one or more UEs send a large
number of connection requests to the gNB, preventing new UEs from establishing
connections. In this paper, we investigate the detection of such threats and
propose an adaptive threshold-based detection system based on Extreme Value
Theory (EVT). The proposed solution is evaluated numerically by applying
simulated attack scenarios based on a realistic threat model on top of
real-world RRC traffic data from an operator network. We show that, by
leveraging features from the RRC layer only, the detection system can not only
identify the attacks but also differentiate them from legitimate high-traffic
situations. The adaptive threshold calculated using EVT ensures that the system
can work under diverse traffic conditions. The results show high accuracy,
precision, and recall values (above 93%), and a low detection latency even
under complex conditions.

</details>
