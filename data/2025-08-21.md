<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: Chain-of-Agents (CoA)는 단일 모델 내에서 다단계 문제 해결을 가능하게 하는 새로운 LLM 추론 패러다임이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템은 수동적인 프롬프트 및 워크플로우 엔지니어링에 의존하여 비효율적인 계산성과 낮은 성능을 보인다.

Method: Chain-of-Agents를 통해 모델이 다양한 도구 에이전트와 역할 수행 에이전트를 동적으로 활성화하여 다중 에이전트 협업을 시뮬레이션한다.

Result: AFM은 웹 에이전트 및 코드 에이전트 설정에서의 다양한 벤치마크에서 새로운 최고 성능을 달성한다.

Conclusion: 연구 결과는 에이전트 모델과 에이전틱 RL에 대한 미래 연구를 위한 탄탄한 출발점을 제공한다.

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [2] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: Cognitive Workspace라는 새로운 패러다임을 제안하여 대규모 언어 모델의 맥락 관리의 한계를 극복하고, 이를 통해 진정한 인지 확장을 구현한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 맥락 관리에서 근본적인 한계를 겪고 있다는 점을 강조하고, 이를 해결하기 위해 Cognitive Workspace를 제안한다.

Method: 인지 과학의 기초를 바탕으로 활성 메모리 관리, 계층적 인지 버퍼, 과업 중심의 맥락 최적화를 통해 Cognitive Workspace의 세 가지 핵심 혁신을 실현한다.

Result: Cognitive Workspace는 평균 58.6%의 메모리 재사용률을 달성하며, 전통적인 RAG에 비해 17-18%의 순 효율성 증가를 보인다.

Conclusion: Cognitive Workspace는 정보 검색에서 진정한 인지 증강으로의 근본적인 전환을 촉진하는 이론적 프레임워크를 제시한다.

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [3] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: CardAIc-Agents는 심혈관 질환의 조기 탐지와 개인화된 진료를 위해 외부 도구를 통합하고 다양한 심장 작업을 지원하는 다중 모드 프레임워크로, 기존 AI 시스템의 한계를 극복한다.


<details>
  <summary>Details</summary>
Motivation: 심혈관 질환은 전 세계에서 주요 사망 원인이며, 의료 인력의 심각한 부족이 문제를 악화시키고 있다.

Method: CardAIc-Agents라는 다중 모드 프레임워크를 제안하여 외부 도구를 통합하고 다양하게 적응할 수 있도록 지원한다.

Result: 세 가지 데이터셋에서 CardAIc-Agents는 기존의 비전-언어 모델 및 최신 에이전트 시스템에 비해 효율성을 입증하였다.

Conclusion: CardAIc-Agents는 심혈관 진료에서 필요한 적응적 사고를 가능하게 하여 개인화된 결정 지원을 향상시킨다.

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [4] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaEval은 자동화된 알파 마이닝 모델을 위한 통합된, 병렬화 가능한, 백테스트 없이 평가하는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 알파 마이닝은 금융 데이터에서 예측 신호를 생성하는 데 필수적이며, 기존의 평가 방법은 재현 가능성과 정확성에서 한계를 갖는다.

Method: AlphaEval은 예측력, 안정성, 시장 변동에 대한 강건성, 재무 논리, 다양성의 다섯 가지 보완적인 차원에서 생성된 알파의 전반적인 품질을 평가한다.

Result: 광범위한 실험을 통해 AlphaEval은 포괄적인 백테스팅과 유사한 평가 일관성을 달성하고, 더 포괄적인 통찰력과 높은 효율성을 제공함을 입증하였다.

Conclusion: 모든 구현과 평가 도구는 재현 가능성을 높이고 커뮤니티 참여를 촉진하기 위해 오픈 소스로 제공된다.

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [5] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 본 연구는 유한 관계 구조 형태의 양성 및 음성 예제에 맞는 온톨로지와 제약을 조정하는 문제를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 양성 및 음성 예제를 형성하는 유한 관계 구조에 맞는 온톨로지와 제약을 조정하는 방법을 연구합니다.

Method: 온톨로지 및 제약 언어로는 설명 논리 $	extmath{E	ext{L}}$과 $	extmath{E	ext{LI}}$를 고려하며, 전체, 가드된, 최전선 가드된, 최전선-하나, 비제한 TGDs 및 포함 제약의 여러 클래스를 분석합니다.

Result: 정확한 계산 복잡성을 규명하고, 알고리즘을 설계하며, 온톨로지 및 TGDs의 크기를 분석합니다.

Conclusion: $	extmath{E	ext{L}}$, $	extmath{E	ext{LI}}$, 가드된 TGDs 및 포함 제약에 대해 유한 기초가 존재하지만, 일반적으로 전체, 최전선 가드된 및 최전선-하나 TGDs에 대해서는 존재하지 않습니다.

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [6] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: Active Inference (AIF)의 의사결정 프레임워크를 개선하는 방법론을 소개하며, 이를 통해 자원 제약 환경에서 효율적인 AIF 에이전트를 구현할 수 있다.


<details>
  <summary>Details</summary>
Motivation: AIF는 의사결정에 강력한 프레임워크를 제공하지만, 계산 및 메모리 요구사항이 배치에 도전 과제가 된다.

Method: pymdp의 유연성과 효율성을 하드웨어 효율적인 실행을 위해 맞춤형으로 설계한 통합된 희소 계산 그래프와 결합하여 AIF의 배치를 용이하게 하는 방법론을 제시한다.

Result: 우리의 접근 방식은 지연 시간을 2배 이상 줄이고 메모리는 최대 35%까지 감소시켰다.

Conclusion: 이 연구는 실시간 및 임베디드 애플리케이션을 위한 효율적인 AIF 에이전트의 배치를 발전시킨다.

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [7] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 이 논문은 텍스트-투-SQL 모델의 기초 능력 및 일반화 능력을 높이기 위해 WHERE 절의 의미적 파싱을 위한 실행 기반 전략과 모델 해석 분석을 통합하여 CESQL 모델을 설계하였고, 이를 통해 예측 정확성을 크게 향상시켰습니다.


<details>
  <summary>Details</summary>
Motivation: 텍스트-투-SQL 모델의 기초 능력과 일반화 능력을 높이기 위해.

Method: WHERE 절의 의미적 파싱을 위한 실행 기반 전략과 모델 해석 분석을 통합하고, 필터링 조정, 논리적 상관 관계 개선 및 모델 융합을 통해 CESQL 모델을 설계함.

Result: WikiSQL 데이터셋에서 성능이 뛰어나며 예측 결과의 정확성이 크게 향상됨.

Conclusion: 기본 데이터베이스 쿼리 처리의 정확성을 향상시킴으로써 복잡한 쿼리 및 비정상 데이터가 포함된 시나리오를 처리하는 연구에 새로운 관점을 제공할 것으로 기대됨.

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [8] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: 데이터 오염은 평가 데이터가 모델 학습 데이터로 유출되어 테스트의 유효성을 저해하는 문제를 나타냅니다. 본 논문에서는 검색 기반 LLM 에이전트를 평가할 때 발생하는 검색 시간 오염(STC) 문제를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 검색 기반 LLM 에이전트를 사용하는 평가에서 도구를 사용하여 온라인 정보 출처를 검색하는 과정에서 발생하는 STC 문제를 파악하고 이를 해결하고자 했습니다.

Method: HuggingFace 데이터를 활용하여, 검색 기반 에이전트가 질문과 답변의 쌍을 직접 찾는지를 평가했습니다.

Result: 3%의 질문에서 에이전트가 HuggingFace의 데이터셋에서 진실 레이블을 찾는 것을 확인했습니다. HuggingFace 접근을 차단한 후, 오염된 서브셋에서 약 15%의 정확도 감소를 관찰했습니다.

Conclusion: 검색 기반 LLM 에이전트의 신뢰할 수 있는 평가를 보장하기 위해 새로운 형태의 데이터 유출 문제를 다루는 베스트 프랙티스를 제안했습니다.

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [9] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: QuickMerge는 다음 토큰 예측을 위한 경량 토큰 병합 프레임워크로, 효율성을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 언어, 비전 및 비디오 도메인에서 생성 모델이 더 큰 입력으로 확장됨에 따라 토큰 수준의 계산 비용이 주요 병목 현상이 되었습니다.

Method: QuickMerge는 주의 정규화 크기에 따라 동적으로 원소 수를 줄이며, 엔트로피 기반 예산 추정기를 통해 가이드합니다.

Result: QuickMerge는 계산-정확성 균형에서 일관된 개선을 보여줍니다.

Conclusion: QuickMerge는 학습된 토크나이저와 고정 패치 기준을 초과하는 성능을 달성하면서 토큰 수를 상당히 줄입니다.

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [10] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: 체스에서 전략적 결정-making은 즉각적인 기회와 장기적인 목표 간의 긴장을 관리하는 것을 포함한다. AI와 인간 간의 게임 dynamics를 비교함으로써, 높은 수준의 전략적 긴장을 지속하는 AI가 있도록 나타났다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 자주 보는 전략적 결정-making의 괴리 즉시 기회와 장기적 목표 사이에서의 긴장을 분석하고자 한다.

Method: AI 대 AI 및 인간 대 인간의 게임 dynamics를 비교하여 보드에서 진행 중인 전략적 긴장을 정량화하기 위한 네트워크 기반 지표를 제안한다.

Result: 가장 경쟁적인 AI 플레이어는 엘로 1600 및 2300에서 전문가 수준 이상으로 더 긴 시간 동안 높은 수준의 전략적 긴장을 유지한다.

Conclusion: 이 연구 결과는 복잡한 전략적 환경에서 AI의 사용에 대한 함의를 가질 수 있다.

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [11] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 대규모 언어 모델 기반 에이전트에서 메모리는 개인화 달성을 위한 중요한 기능을 한다.


<details>
  <summary>Details</summary>
Motivation: 사용자 정보를 저장하고 활용하여 개인화를 구현하는 메모리의 필요성을 강조한다.

Method: 다양한 명시적 및 암시적 메모리 방법을 구현하고 포괄적인 실험을 실시한다.

Result: 여러 관점에서 이 작업에 대한 성과를 평가하고 강점과 약점을 분석한다.

Conclusion: 제안된 HybridMem 방법의 효과를 광범위한 실험을 통해 입증한다.

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [12] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: 데이터 기반 인공지능이 새로운 물질 발견을 혁신하고 있으며, DIVE 다중 에이전트 워크플로우가 실험 데이터를 체계적으로 읽고 정리하여 물질 디자인 자동화에 기여한다.


<details>
  <summary>Details</summary>
Motivation: 자료의 비구조적 형태가 인공지능 모델을 통한 물질 설계를 방해하고 있다.

Method: DIVE 다중 에이전트 워크플로우가 과학 문헌의 그래픽 요소에서 실험 데이터를 읽고 조직한다.

Result: DIVE는 상업 모델과 대비해 10-15% 개선시키고, 오픈 소스 모델 대비 30% 이상의 성과를 낸다.

Conclusion: AI 워크플로우와 에이전트 디자인은 다양한 물질에 광범위하게 적용 가능하며, AI 주도의 물질 발견을 위한 패러다임을 제공한다.

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [13] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: STONK은 뉴스 지식을 활용한 주식 최적화 프레임워크로, 수치 시장 지표와 감정이 풍부한 뉴스 임베딩을 통합하여 일일 주식 변동 예측을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 주식 시장 예측의 정확성을 높이기 위해 기존 분석의 한계를 극복하고자 합니다.

Method: 수치 및 텍스트 임베딩을 특성 결합 및 교차 모드 주의를 통해 통합하여 예측을 수행하는 통합 파이프라인을 사용합니다.

Result: 백테스트 결과 STONK이 수치 전용 기준을 능가함을 보여줍니다.

Conclusion: 융합 전략 및 모델 구성을 종합적으로 평가하여 확장 가능한 다중 모달 금융 예측을 위한 근거 기반 지침을 제공합니다.

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [14] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: LLM 기반 자동 휴리스틱 설계(AHD)가 진화 계산(EC) 프레임워크 내에서 유망한 결과를 보였지만, 정적 연산자 사용과 지식 축적 메커니즘 부족으로 효과성이 저해받았다. HiFo-Prompt라는 새로운 프레임워크를 소개하며, 이는 두 가지 상호보완적인 프롬프트 전략(Foresight와 Hindsight)을 통해 LLM을 안내한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 자동 휴리스틱 설계의 효과성을 높이기 위한 방법을 제시하고자 한다.

Method: Foresight 기반 프롬프트는 인구 역학에 따라 탐색을 조정하고, Hindsight 기반 프롬프트는 과거 세대의 성공적인 휴리스틱을 기본 재사용 가능한 설계 원칙으로 증류한다.

Result: HiFo-Prompt는 최신 LLM 기반 AHD 방법보다 현저히 우수한 성능을 보여주며, 더 높은 품질의 휴리스틱을 생성하고 빠른 수렴과 우수한 쿼리 효율성을 달성한다.

Conclusion: HiFo-Prompt의 이중 메커니즘은 일시적인 발견을 지속적인 지식 기반으로 전환하여 LLM이 자신의 경험으로부터 학습할 수 있게 한다.

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [15] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: LOOP는 신경 및 기호 구성 요소 간의 반복적인 대화로 계획을 처리하는 새로운 신경-기호 계획 프레임워크이며, 복잡한 작업 관리를 위한 안정적인 계획을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 자동 시스템에서의 계획은 작은 오류가 큰 실패나 막대한 손실로 이어질 수 있어 매우 중요하다.

Method: LOOP는 13개의 조정된 신경 기능을 통합하고, PDDL 사양을 생성하고 기호적 피드백을 기반으로 이를 반복적으로 다듬으며 실행 추적에서 인과 지식 기반을 구축한다.

Result: LOOP는 IPC 벤치마크 도메인에서 85.8%의 성공률을 달성하여 기존 접근 방식보다 우수함을 입증하였다.

Conclusion: 신뢰할 수 있는 계획의 핵심은 신경망과 기호 추론기 간의 선택이 아니라 전체 과정에서 서로 '대화'하게 만드는 것이다.

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [16] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: SPANER은 다양한 모달리티에서 입력을 통합된 의미 공간에 임베드하기 위해 설계된 모달리티 불가지론적 PEFT 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 PEFT 방법들이 작업 특정 성과에 집중하여 모달리티 임베딩 공간의 구조를 무시한 문제를 해결하고자 합니다.

Method: SPANER는 공유 프롬프트 메커니즘을 사용하여 모달리티에 관계없이 의미적으로 관련된 인스턴스가 공간적으로 수렴하도록 합니다.

Result: 비전-언어 및 오디오-비주얼 벤치마크에서 경쟁력 있는 적은 샷 검색 성능을 보여줍니다.

Conclusion: 임베딩 구조의 정렬이 조정된 어댑터 가중치를 최적화하는 것보다 스케일 가능한 다중 모드 학습에 더 중요하다는 것을 강조합니다.

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [17] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: 본 논문은 비정형 다중 페이지 금융 테이블을 정규화되고 스키마에 맞는 출력으로 변환하는 TASER라는 지속적 학습 가능한 테이블 추출 시스템을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 실제 금융 문서는 수백만 개의 다양한 금융 상품 유형에 대한 중요한 정보를 포함하고 있으나, 이러한 정보는 종종 복잡한 표에 숨겨져 있습니다.

Method: TASER는 초기 스키마를 활용하여 테이블 탐지, 분류, 추출 및 권장 작업을 수행하는 테이블 에이전트를 통해 작동합니다.

Result: TASER는 기존의 테이블 탐지 모델인 Table Transformer보다 10.1% 성능이 우수하며, 지속적 학습 과정에서 큰 배치 크기가 스키마 추천을 104.3% 증가시켰습니다.

Conclusion: TASER는 실제 금융 테이블에 대한 강력한 이해를 위해 에이전틱하고 스키마 기반의 추출 시스템의 가능성을 보여줍니다.

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [18] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: 인공지능 시스템이 과학적 발견을 혁신하고 있으며, 자율적으로 과학적 작업을 수행할 수 있는 가능성을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 과학 문헌과 전문화의 급증으로 연구자들이 지식을 통합하고 통일 이론을 개발하는 능력이 제한되었음.

Method: 도메인에 구애받지 않는 자율 AI 시스템을 통해 가설 생성, 데이터 수집, 원고 준비의 과학적 작업 흐름을 독립적으로 탐색.

Result: 이 시스템은 심리학 연구 3건을 설계 및 실행하고, 288명의 참여자를 대상으로 한 온라인 데이터 수집을 현명하게 수행했으며, 8시간 이상의 연속 코딩 세션을 통해 분석 파이프라인을 개발했다.

Conclusion: AI가 이론적 추론과 방법론적 엄격성을 가지고 비트리비얼 연구를 수행할 수 있음을 보여주고, 과학적 이해의 본질과 과학적 공적 인정에 대한 중요한 질문을 제기한다.

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [19] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: STPFormer는 스페이셜-템포럴 트래픽 예측을 위한 새로운 Transformer 모델로, 기존 모델의 한계를 극복하고 뛰어난 성과를 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 시간 패턴과 동적 공간 구조로 인해 스페이셜-템포럴 트래픽 예측은 도전적입니다.

Method: STPFormer는 temporally position-aware encoding을 위한 TPA, sequential spatial learning을 위한 SSA, cross-domain alignment을 위한 STGM, multi-scale fusion을 위한 Attention Mixer로 구성됩니다.

Result: STPFormer는 5개의 실제 데이터셋에서 새로운 SOTA 결과를 설정하며, ablation 및 시각화 실험을 통해 효과성과 일반화 가능성을 입증했습니다.

Conclusion: STPFormer는 기존 Transformer 모델의 제한점을 극복하며, 향상된 트래픽 예측 성능을 제공합니다.

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [20] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: DMMV(Discrete Min-Max Violation) 문제를 정의하고, 이를 해결하기 위한 GPU 가속 휴리스틱을 개발하였다. 이 방법은 세 가지 최적화 문제에서 성능을 향상시켰다.


<details>
  <summary>Details</summary>
Motivation: 최악의 성능 요구 사항을 가진 다양한 사용 사례에 적용 가능한 일반적인 최적화 문제를 찾기 위해 DMMV를 도입하였다.

Method: DMMV의 수학적 정의와 속성을 파악하고, GPU 가속 휴리스틱을 개발하여 실제적인 DMMV 인스턴스의 크기를 해결하였다.

Result: 휴리스틱은 언어 모델의 사후 훈련 양자화에서 평균 14% 개선, 불균일 노이즈 하의 이산 단층촬영에서 16% 재구성 오류 감소 및 최소 6배 속도 향상을 달성하였다.

Conclusion: DMMV를 컨텍스트 프리 최적화 문제로 연구하는 이점과 제안한 휴리스틱의 장점을 강조한다. 관련 코드는 오픈소스로 제공된다.

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [21] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: 언어 모델 에이전트는 실제 작업 자동화를 위한 잠재력을 보이지만, 안전-critical 상황에서는 심각한 위험을 초래할 수 있습니다. 본 연구에서는 LM 에이전트의 위험 인식과 안전 실행 능력 간의 격차를 식별하고, 이를 평가하기 위한 프레임워크를 개발하여 그들의 위험 지식, 실행 경로의 위험 식별 능력, 위험 행동 회피 능력을 조사하였습니다. 결과적으로, 에이전트는 위험 지식은 뛰어나지만 실제 시나리오에서 그 지식을 적용하는 데 실패하고, 종종 여전히 위험한 행동을 수행하는 경향이 있음을 보여주었습니다. 이를 해결하기 위해 제안된 행동을 독립적으로 비판하는 위험 검증기와 실행 경로를 추상적으로 변환하는 추상화를 도입하였습니다. 우리의 시스템은 일반적인 프롬프트 기반 에이전트보다 55.3%의 위험한 행동 실행을 줄이는 성과를 거두었습니다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델 에이전트의 안전성 문제를 해결하고, 이들의 위험 인식과 실행 능력 간의 간극을 이해하기 위해.

Method: 에이전트의 안전성을 1) 잠재적 위험에 대한 지식, 2) 실행 경로에서 위험을 식별하는 능력, 3) 위험한 행동을 피하는 실제 행동의 세 가지 차원에서 평가하기 위한 종합적 평가 프레임워크를 개발하였다.

Result: 에이전트는 위험 지식이 거의 완벽할 정도였지만, 실제 시나리오에서는 이 지식을 적용하는 데 실패하여 성능이 23% 이상 감소하였고, 여전히 위험한 행동을 26% 미만의 비율로 수행하였다.

Conclusion: 대신에 관찰된 격차를 활용하여 에이전트의 제안된 행동을 독립적으로 비판하는 위험 검증기를 개발하였으며, 이를 통해 55.3%의 위험한 행동 실행을 줄이는 성과를 달성하였다.

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [22] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: CrafterDojo는 일반 목적의 구현 에이전트 연구를 위한 경량 프로토타입 환경을 제공하고, Crafter 환경을 활용하는 데 필요한 기초 모델과 도구 모음을 포함한다.


<details>
  <summary>Details</summary>
Motivation: 일반 목적의 구현 에이전트를 개발하는 것은 AI에서 핵심적인 도전 과제이다.

Method: 이 논문에서는 CrafterDojo라는 기초 모델 및 도구 모음과 CrafterVPT, CrafterCLIP, CrafterSteve-1을 소개한다.

Result: CrafterDojo는 Crafter 환경을 경량 프로토타입 친화적이며, Minecraft와 유사한 테스트베드로 활용할 수 있도록 한다.

Conclusion: 우리는 행동 및 캡션 데이터셋 생성 도구킷, 참조 에이전트 구현, 벤치마크 평가, 완전한 오픈 소스 코드베이스를 제공한다.

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [23] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: EAG-RL은 전자 건강 기록(EHR)에서 대형 언어 모델(LLM)의 추론 능력을 강화하기 위한 새로운 두 단계 훈련 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 전자 건강 기록 기반 임상 예측을 정확하고 일반화할 수 있도록 향상시키는 것이 중요합니다.

Method: EAG-RL은 전문가 주의 가이드를 통해 LLM의 EHR 추론 능력을 본질적으로 향상시키는 두 단계 훈련 프레임워크입니다.

Result: EAG-RL은 두 가지 실제 EHR 데이터 세트에서 LLM의 본질적인 EHR 추론 능력을 평균 14.62% 향상시켰습니다.

Conclusion: EAG-RL은 임상 예측 작업에서 실제 배치의 가능성을 보여줍니다.

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [24] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 이 연구는 차트에서 코드 생성 과제를 위한 다중모달 구조 강화 학습(MSRL) 방법을 제안하며, 고전적인 감독 세부 조정(SFT)의 한계를 극복해 효율적인 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습이 비전-언어 모델에서 일반적인 추론에 매우 효과적이지만, 정보가 풍부한 이미지를 깊이 이해하고 구조화된 출력을 생성해야 하는 작업에는 활용이 부족하다.

Method: 우리는 차트에서 코드 생성을 위해 다중모달 구조 강화 학습(MSRL) 방법을 제안하며, 300만 개의 실제 arXiv 테이블 차트-코드 쌍으로 구성된 최대의 훈련 코퍼스를 구축하였다.

Result: MSRL은 SFT의 성능 한계를 상당히 극복하여 ChartMimic 및 ReachQA 벤치마크에서 각각 6.2% 및 9.9% 향상을 보여주었다.

Conclusion: MSRL 방법은 고급 모델과 경쟁할 수 있는 성능을 달성했다.

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [25] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: GUI 요소의 정확한Localization을 위해 V2P 방법을 제안하여 배경 분산을 줄이고 중앙-엣지 구분 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: GUI 에이전트 개발을 위해 GUI 요소의 정밀한 로컬라이제이션이 중요하다.

Method: V2P는 배경 주의를 최소화하고 GUI 상호작용을 2D 가우시안 열지도로 모델링하여 중앙과 엣지를 구분한다.

Result: V2P는 ScreenSpot-v2와 ScreenSpot-Pro에서 각각 92.3%와 50.5%의 성능을 달성한다.

Conclusion: 각 구성요소의 기여를 확인하여 V2P의 정밀한 GUI 그라운딩 작업에 대한 일반화 가능성을 강조한다.

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [26] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: 본 논문에서는 불완전한 지식 그래프에 대한 쿼리 응답 방법을 개선하여 부드러운 제약 조건을 통합하기 위한 신경망 쿼리 재조정기(NQR)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 쿼리 응답 방법은 고정된 제약 조건에 초점을 맞추고 있어 실제의 모호한 제약 조건에 대한 대응이 부족합니다.

Method: 신경망 쿼리 재조정기(NQR)는 부드러운 제약 조건을 통합하여 쿼리 응답 점수를 조정하도록 설계되었습니다.

Result: 실험을 통해 NQR이 부드러운 제약 조건을 캡처하면서도 쿼리 응답 성능을 유지하는 것을 입증했습니다.

Conclusion: NQR은 기존 쿼리 응답 성능을 개선하는 데 기여할 수 있습니다.

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [27] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: 본 논문은 데이터 제약 환경에서 설명의 충실성과 안정성을 개선하기 위한 새로운 인스턴스 기반 전이 학습 LIME 프레임워크(ITL-LIME)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LIME의 고유한 무작위성이 제한된 훈련 데이터 시나리오에서 설명의 지역성 및 불안정성 문제를 초래할 수 있으며, 데이터 부족은 비현실적인 변형 및 샘플을 생성할 수 있습니다.

Method: ITL-LIME은 관련 소스 도메인에서의 실제 인스턴스를 활용하여 타겟 도메인에서의 설명 과정에서 도움을 주는 인스턴스 전이 학습을 LIME 프레임워크에 통합합니다. 클러스터링을 사용하여 소스 도메인을 대표 프로토타입이 있는 클러스터로 나누고, 적절한 실제 소스 인스턴스를 추출하여 타겟 인스턴스와 이웃한 실제 인스턴스와 결합합니다.

Result: 제안된 LIME 프레임워크는 설계된 지역성을 기반으로 하는 인스턴스 가중치를 할당하여 일반적인 설명의 충실성을 높이고, 최종적으로 가중된 소스 및 타겟 인스턴스를 사용하여 대리 모델을 훈련합니다.

Conclusion: 이 연구는 데이터 제약 상황에서의 모델 해석을 위한 새로운 방법론을 제시하며, LIME의 성능을 향상시킵니다.

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [28] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: 이 논문은 가정 행동을 설명하는 지식 그래프를 조사하며, 가정 로봇 제어 및 비디오 분석에 유용하다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 가정 행동을 설명하는 지식 그래프는 가정 로봇 제어 및 비디오 분석에 필수적이다.

Method: 가정 행동을 위한 지식 그래프 구축 및 기존 링크 예측 알고리즘의 적합성 평가.

Result: 일반적인 링크 예측 문제와 달리, 상황 지식 그래프는 독특한 특성을 갖고 있어 많은 링크 예측 알고리즘이 적합하지 않다는 것을 발견했다.

Conclusion: 많은 링크 예측 알고리즘이 단순 기준선조차 초월할 수 없음을 보여준다.

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [29] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: MHSNet은 기업의 인재 풀을 개선하기 위해 외부 웹사이트에서 수집한 이력서의 중복을 감지하는 다단계 정체성 검증 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기업은 인재 풀을 유지하기 위해 외부 웹사이트에서 이력서를 지속적으로 검색해야 하지만, 이력서의 품질이 낮고 불완전한 경우가 많다.

Method: MHSNet은 대비 학습을 사용하여 BGE-M3를 미세 조정하고, 전문가 혼합(MoE) 테크닉을 통해 이력서의 다단계 희소 및 밀집 표현을 생성한다.

Result: 실험 결과 MHSNet의 효과가 입증되었다.

Conclusion: MHSNet은 다양한 불완전한 이력서를 처리할 수 있도록 설계되었으며, 이력서 간의 다단계 의미 유사성을 계산할 수 있다.

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [30] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델의 추론 능력을 향상시키기 위한 신경-상징적 접근법의 최근 발전을 포괄적으로 검토합니다.


<details>
  <summary>Details</summary>
Motivation: 강력한 추론 능력을 가진 AI 시스템 개발은 인공지능의 일반화(AGI)를 추구하는 데 있어 중요한 이정표로 여겨집니다.

Method: 신경-상징적 학습 패러다임을 간단히 소개하고, LLM의 추론 능력을 향상시키기 위한 신경-상징적 방법을 세 가지 관점에서 논의합니다: Symbolic->LLM, LLM->Symbolic, LLM+Symbolic.

Result: 최근 신경-상징적 접근 방식을 통해 LLM의 추론 능력을 향상시키기 위한 다양한 기법들이 탐구되었습니다.

Conclusion: 여러 주요 과제와 유망한 미래 방향에 대해 논의하였으며, 관련 논문과 자원을 포함한 GitHub 저장소를 공개하였습니다.

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [31] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepLog는 신경기호 AI를 위한 이론적 및 운영적 프레임워크로, 신경기호 AI에서 일반적으로 사용되는 표현과 계산 메커니즘을 추상화하는 빌딩 블록과 원시 데이터 구조를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 신경기호 AI의 다양성과 효율성을 향상시키기 위해 새로운 프레임워크가 필요하다.

Method: DeepLog 언어는 신경기호 모델과 추론 작업을 지정하며, 확장된 대수 회로를 계산 그래프로 사용한다.

Result: DeepLog는 다양한 퍼지 및 확률 논리와의 비교 실험을 통해 일반성과 효율성을 입증하였다.

Conclusion: DeepLog는 신경기호 AI 시스템의 설계를 단순화하고 다양한 구조와 논리를 쉽게 활용할 수 있게 한다.

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [32] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: CausalPlan은 대형 언어 모델의 협업 작업에서의 인과적 결함을 해결하기 위한 두 단계의 프레임 워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LMM)의 협업 작업에서 인과적으로 유효하지 않거나 일관성이 없는 행동의 문제를 해결하고자 한다.

Method: CausalPlan은 명시적인 구조적 인과 reasoning을 LLM 계획 프로세스에 통합하는 두 단계의 프레임워크로, 구조적 인과 행동(SCA) 모델을 통해 과거 행동과 현재 환경 상태가 미래 결정에 미치는 영향을 포착하는 인과 그래프를 학습한다.

Result: CausalPlan은 Overcooked-AI 벤치마크에서 여러 에이전트 조정 작업과 다양한 크기의 LLM에서 일관되게 유효하지 않은 행동을 줄이고 협업을 개선했다.

Conclusion: 결과는 인과 기반 계획이 효율적이고 해석 가능하며 일반화 가능한 다중 에이전트 LLM 시스템 배포에 있어 가치를 강조한다.

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [33] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: EMRC 프레임워크를 제안하여 의료 의사결정 시스템의 정확성과 신뢰성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 의료 의사결정(MDM)은 복잡한 과정으로, 이질적이고 복잡한 임상 정보를 효과적으로 통합하기 위해 상당한 전문 지식이 필요하다.

Method: EMRC 프레임워크는 전문가 인식 가능한 에이전트 모집과 다중 에이전트 협업을 통한 두 단계로 작동한다.

Result: EMRC 프레임워크는 세 가지 공개 MDM 데이터셋에서 평가되어, 최신 단일 및 다중 LLM 방법론을 능가하는 결과를 나타낸다.

Conclusion: EMRC는 의료 질문에 대한 최적의 LLM을 동적으로 선택하고, 진단 신뢰성을 향상시킨다.

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [34] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 본 논문은 동적으로 학습하는 새로운 양적 공식 인스턴스화 접근법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 양적 공식은 본질적인 비결정성 때문에 SMT 솔버에 상당한 도전 과제가 됩니다.

Method: 관찰된 인스턴스를 잠재적 언어의 샘플로 간주하고 확률적 문맥자유문법을 사용하여 새로운 유사한 항목을 생성하는 방법을 사용합니다.

Result: 우리의 방법은 성공적인 과거 인스턴스를 모방할 뿐만 아니라, 학습된 항의 확률을 선택적으로 반전시켜 다양성을 탐색합니다.

Conclusion: 이 접근법은 정량자 추론에서 착취와 탐색의 균형을 맞추는 것을 목표로 합니다.

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [35] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 이 논문은 다양한 RAG 시스템의 장점을 활용할 수 있는 앙상블 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 RAG 프레임워크가 존재하지만, 단일 RAG 프레임워크는 다양한 하위 작업에 잘 적응하지 못한다.

Method: RAG 시스템 기반의 앙상블 방법을 종합적이고 체계적으로 조사하였으며, 이론적 분석과 메커니즘 분석 관점에서 RAG 앙상블 프레임워크를 분석하였다.

Result: 다양한 RAG 시스템을 집합하여 실험을 수행한 결과, 파이프라인 및 모듈 수준에서 모두 일반화 가능하고 강건함을 보여주었다.

Conclusion: 이 연구는 다중 RAG 시스템 앙상블에 대한 유사 연구의 기초를 마련한다.

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [36] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: LLMs를 이용한 PDDL 계획의 일반화된 계획 생성을 위한 새로운 접근 방식 소개.


<details>
  <summary>Details</summary>
Motivation: PDDL 도메인에서 작업을 일반화하는 계획을 효율적으로 생성하는 필요성.

Method: 전략을 의사코드 형태로 생성하고 자동 디버깅 기능을 통해 오류를 사전에 식별 및 수정.

Result: 확장된 기능을 통해 17개의 벤치마크 도메인에서 일반화된 계획의 품질이 향상됨을 보여줌.

Conclusion: 12개 도메인에서 최고의 Python 프로그램이 모든 작업을 해결함.

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [37] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: 본 논문에서는 금융 애플리케이션을 위한 시계열 모델링 워크플로우를 자동화하고 향상시키기 위한 모듈형 에이전트 구조인 TS-Agent를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 시계열 데이터는 금융 시장의 의사 결정에 중심적이나, 고성능의 해석 가능하고 감사 가능한 모델을 구축하는 것은 주요한 도전 과제입니다.

Method: TS-Agent는 모델 선택, 코드 개선, 미세 조정의 세 단계로 구성된 구조화된 반복 결정 프로세스로 파이프라인을 형식화합니다. 이 과정은 맥락적 추론 및 실험적 피드백에 의해 안내됩니다.

Result: 다양한 금융 예측 및 합성 데이터 생성 작업에 대한 경험적 평가 결과, TS-Agent는 최신 AutoML 및 에이전틱 기준을 지속적으로 초월하여 우수한 정확성, 견고성 및 결정 추적 가능성을 달성합니다.

Conclusion: TS-Agent는 적응형 학습, 강력한 디버깅 및 투명한 감사 기능을 지원하여 금융 서비스와 같은 고위험 환경에서의 핵심 요구 사항을 충족합니다.

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [38] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: AI 기반의 자율 에이전트의 전략적 행동 동향을 공급망 시뮬레이션에서 조사하고, 협업 AI 에이전트가 이론적으로 열등한 성과를 내는 "협업 역설"을 발견함.


<details>
  <summary>Details</summary>
Motivation: AI 기반 에이전트의 전략적 행동에 대한 이해가 경제적 환경에서 중요해짐에 따라, 공급망 시스템의 불안정성을 조사하려는 필요성이 대두되고 있다.

Method: 공급망 시뮬레이션에서 대형 언어 모델(LLM) 기반 생성적 AI 에이전트를 사용하여 컴퓨터 실험을 수행하였다.

Result: 협업 AI 에이전트가 비AI 기준선보다도 더 열악한 성과를 보이는 '협업 역설'을 발견하였다.

Conclusion: 안정성을 확보하려면 고급의 AI 기반 정책 설정과 저급의 협업 실행 프로토콜의 두 가지 층의 결합이 필요하다.

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [39] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: 이 연구는 pretrained 대형 언어 모델(LLM)을 시뮬레이션 도구 구현에 적합하게 조정할 수 있는지를 알아본다.


<details>
  <summary>Details</summary>
Motivation: 전문가들이 시뮬레이션 도구를 효과적으로 사용하는 데 도움을 주는 가상 어시스턴트를 만드는 것.

Method: 개방형 및 폐쇄형 LLM을 정제하고 사용자 맞춤화하는 프레임워크를 제시한다.

Result: PyChrono 가상 실험을 수행하는 스크립트의 질적 개선을 가져오는 프로세스를 통해 여러 LLM 클래스의 정제 및 맞춤화를 수행했다.

Conclusion: 생성된 스크립트는 사용자가 수정 및 개선할 수 있는 강력한 시작점으로 작용할 수 있으며, 이 프레임워크는 다른 응용 도메인과 관련된 시뮬레이션 도구의 진입 장벽을 낮출 수 있다.

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [40] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: LRS 문제에 대한 BRKGA를 사용한 해결책을 제시하며, 계산 효율성을 중심으로 한 연구를 통해 최첨단 기법임을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: LRS 문제는 생물정보학의 하위수열 문제로, 유전체 재조합에서 중요한 역할을 한다.

Method: Biased Random Key Genetic Algorithm (BRKGA) 사용, 개인 평가의 계산 효율성에 중점.

Result: 제안된 BRKGA는 LRS 문제에 대해 현재 최첨단 기술임을 보여준 계산 결과.

Conclusion: 대규모 알파벳 크기에 따른 입력 문자열에 개선 여지가 있음.

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [41] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ComputerRL은 복잡한 디지털 작업 공간을 능숙하게 운영할 수 있는 자율 데스크탑 인텔리전스 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 데스크탑 환경에서 기계 에이전트와 인간 중심 환경 간의 불일치를 해소하기 위해 API-GUI 패러다임을 도입했다.

Method: 대규모 온라인 강화 학습을 가속화하기 위해 수천 개의 병렬 가상 데스크탑 환경을 조정할 수 있는 분산 RL 인프라를 개발하고, 강화 학습과 감독된 미세 조정을 번갈아 진행하는 Entropulse 훈련 전략을 제안했다.

Result: GLM-4-9B-0414 및 Qwen2.5-14B 모델을 활용하여 OSWorld 벤치마크에서 평가한 결과, GLM-4-9B-0414 기반의 AutoGLM-OS-9B가 48.1%의 새로운 최고 정확도를 달성하였다.

Conclusion: 이 알고리즘과 프레임워크는 AutoGLM을 구축하는 데 채택되었다.

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [BERT-VQA: Visual Question Answering on Plots](https://arxiv.org/abs/2508.13184)
*Tai Vu,Robert Yang*

Main category: cs.LG

TL;DR: 비주얼 질문 응답은 자연어 이해 분야에서 흥미로운 도전 과제이며, 본 연구는 특히 플롯에 대한 비주얼 질문 응답 문제를 다룬다. BERT-VQA라는 모델을 개발하였으며, 기존 모델과의 비교를 통해 큰 통찰을 제공하였다.


<details>
  <summary>Details</summary>
Motivation: 비주얼 질문 응답은 비전과 언어 도메인 간의 정보 교환이 필요한 도전 과제이다.

Method: BERT-VQA라는 VisualBERT 기반의 모델 아키텍처를 개발하고, 사전 훈련된 ResNet 101 이미지 인코더와 결합하여 모델을 구성하였다.

Result: 모델을 LSTM, CNN 및 얕은 분류기 기반의 기준선과 비교 평가한 결과, VisualBERT의 크로스 모달리티 모듈이 플롯 구성 요소와 질문 구문을 정렬하는 데 필수적이라는 우리의 핵심 가설은 반증되었다.

Conclusion: 플롯 질문 응답의 어려움과 다양한 모델 아키텍처의 적절성을 제공하는 통찰을 제시하였다.

Abstract: Visual question answering has been an exciting challenge in the field of
natural language understanding, as it requires deep learning models to exchange
information from both vision and language domains. In this project, we aim to
tackle a subtask of this problem, namely visual question answering on plots. To
achieve this, we developed BERT-VQA, a VisualBERT-based model architecture with
a pretrained ResNet 101 image encoder, along with a potential addition of joint
fusion. We trained and evaluated this model against a baseline that consisted
of a LSTM, a CNN, and a shallow classifier. The final outcome disproved our
core hypothesis that the cross-modality module in VisualBERT is essential in
aligning plot components with question phrases. Therefore, our work provided
valuable insights into the difficulty of the plot question answering challenge
as well as the appropriateness of different model architectures in solving this
problem.

</details>


### [43] [Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis](https://arxiv.org/abs/2508.13196)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: 이 논문은 자연재해 컨텍스트에서의 다중 모달 감정 분석을 위한 새로운 접근 방식을 소개하며, 이미지와 텍스트 모달리티를 통합하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자연재해 관리에서 공공의 감정을 이해하는 것이 중요하다.

Method: CNN 기반 이미지 분석과 LLM 기반 텍스트 처리를 통합하여 CrisisMMD 데이터세트에서 감정 관련 특징을 추출한다.

Result: 모델은 기존 기준선보다 향상된 정확도를 보였으며, 2.43%의 정확도 증가와 5.18%의 F1-score 성장을 기록했다.

Conclusion: 우리의 접근 방식은 다중 모달 데이터의 복잡한 관계를 처리하는 데 효과적이며, 재난 관리의 실제적인 적용 가능성을 제공한다.

Abstract: This paper introduces a novel approach for multimodal sentiment analysis on
social media, particularly in the context of natural disasters, where
understanding public sentiment is crucial for effective crisis management.
Unlike conventional methods that process text and image modalities separately,
our approach seamlessly integrates Convolutional Neural Network (CNN) based
image analysis with Large Language Model (LLM) based text processing,
leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to
extract sentiment relevant features from the CrisisMMD dataset. To effectively
model intermodal relationships, we introduce a contextual attention mechanism
within the fusion process. Leveraging contextual-attention layers, this
mechanism effectively captures intermodality interactions, enhancing the
model's comprehension of complex relationships between textual and visual data.
The deep neural network architecture of our model learns from these fused
features, leading to improved accuracy compared to existing baselines.
Experimental results demonstrate significant advancements in classifying social
media data into informative and noninformative categories across various
natural disasters. Our model achieves a notable 2.43% increase in accuracy and
5.18% in F1-score, highlighting its efficacy in processing complex multimodal
data. Beyond quantitative metrics, our approach provides deeper insight into
the sentiments expressed during crises. The practical implications extend to
real time disaster management, where enhanced sentiment analysis can optimize
the accuracy of emergency interventions. By bridging the gap between multimodal
analysis, LLM powered text understanding, and disaster response, our work
presents a promising direction for Artificial Intelligence (AI) driven crisis
management solutions. Keywords:

</details>


### [44] [Strategies for training point distributions in physics-informed neural networks](https://arxiv.org/abs/2508.13216)
*Santosh Humagain,Toni Schneidereit*

Main category: cs.LG

TL;DR: 물리 정보 신경망이 미분 방정식의 근사를 효과적으로 지원하는데, 본 논문에서는 훈련 데이터 분포의 영향을 체계적으로 검토한다.


<details>
  <summary>Details</summary>
Motivation: 물리 정보 신경망은 미분 방정식의 구조와 조건을 손실 함수에 직접 포함시켜 근사를 수행함으로써 모델링 단계에서 불변성 등의 조건을 쉽게 추가할 수 있다.

Method: 두 개의 보통 미분 방정식과 두 개의 부분 미분 방정식에 대해 훈련 데이터 생성을 위한 다섯 가지 전략과 얕은 신경망 구조(한 개 및 두 개의 은닉층)를 테스트 했다.

Result: 훈련 포인트 분포가 솔루션 정확도에 미치는 영향을 보여주고, 그들이 미분 방정식의 특성과 연결되어 있음을 확인했다.

Conclusion: 훈련 데이터 분포는 솔루션의 정확도에 중요한 영향을 미치며, 이는 미분 방정식의 특성과 관련이 있다.

Abstract: Physics-informed neural networks approach the approximation of differential
equations by directly incorporating their structure and given conditions in a
loss function. This enables conditions like, e.g., invariants to be easily
added during the modelling phase. In addition, the approach can be considered
as mesh free and can be utilised to compute solutions on arbitrary grids after
the training phase. Therefore, physics-informed neural networks are emerging as
a promising alternative to solving differential equations with methods from
numerical mathematics. However, their performance highly depends on a large
variety of factors. In this paper, we systematically investigate and evaluate a
core component of the approach, namely the training point distribution. We test
two ordinary and two partial differential equations with five strategies for
training data generation and shallow network architectures, with one and two
hidden layers. In addition to common distributions, we introduce sine-based
training points, which are motivated by the construction of Chebyshev nodes.
The results are challenged by using certain parameter combinations like, e.g.,
random and fixed-seed weight initialisation for reproducibility. The results
show the impact of the training point distributions on the solution accuracy
and we find evidence that they are connected to the characteristics of the
differential equation.

</details>


### [45] [MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.13661)
*Maciej Wojtala,Bogusz Stefańczyk,Dominik Bogucki,Łukasz Lepak,Jakub Strykowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 본 논문은 자가 주의 기반 통신 모듈을 제안하여, 다중 에이전트 강화 학습에서 에이전트 간 정보를 효율적으로 교환할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업을 집단적으로 수행하기 위한 통신의 중요성.

Method: 자가 주의 기반 통신 모듈을 도입하여 에이전트 간 정보를 교환.

Result: SMAC 벤치마크에서 여러 맵에서 최첨단 성능을 달성.

Conclusion: 제안한 접근 방식은 완전히 미분 가능하며, 기존의 복잡한 통신 프로토콜을 대체할 수 있다.

Abstract: Communication is essential for the collective execution of complex tasks by
human agents, motivating interest in communication mechanisms for multi-agent
reinforcement learning (MARL). However, existing communication protocols in
MARL are often complex and non-differentiable. In this work, we introduce a
self-attention-based communication module that exchanges information between
the agents in MARL. Our proposed approach is fully differentiable, allowing
agents to learn to generate messages in a reward-driven manner. The module can
be seamlessly integrated with any action-value function decomposition method
and can be viewed as an extension of such decompositions. Notably, it includes
a fixed number of trainable parameters, independent of the number of agents.
Experimental results on the SMAC benchmark demonstrate the effectiveness of our
approach, which achieves state-of-the-art performance on several maps.

</details>


### [46] [Deep Graph Neural Point Process For Learning Temporal Interactive Networks](https://arxiv.org/abs/2508.13219)
*Su Chen,Xiaohua Qi,Xixun Lin,Yanmin Shang,Xiaolin Xu,Yangxi Li*

Main category: cs.LG

TL;DR: 본 논문은 네트워크 내 시간적 상호작용을 예측하는 새로운 방법으로 DGNPP 모델을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 이전 연구에서 시간적 상호작용 네트워크(TIN)는 네트워크 토폴로지 구조의 영향을 무시하고 거칠게 다뤄졌다.

Method: DGNPP 모델은 Node Aggregation Layer와 Self Attentive Layer의 두 가지 주요 모듈로 구성된다.

Result: DGNPP는 세 가지 공공 데이터셋에서 이벤트 예측 및 발생 시간 예측 작업에서 뛰어난 성능을 보였다.

Conclusion: 이 모델은 기존의 방법들보다 현저하게 성과가 우수하며, 효율성을 높인다.

Abstract: Learning temporal interaction networks(TIN) is previously regarded as a
coarse-grained multi-sequence prediction problem, ignoring the network topology
structure influence. This paper addresses this limitation and a Deep Graph
Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two
key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node
Aggregation Layer captures topological structures to generate static
representation for users and items, while the Self Attentive Layer dynamically
updates embeddings over time. By incorporating both dynamic and static
embeddings into the event intensity function and optimizing the model via
maximum likelihood estimation, DGNPP predicts events and occurrence time
effectively. Experimental evaluations on three public datasets demonstrate that
DGNPP achieves superior performance in event prediction and time prediction
tasks with high efficiency, significantly outperforming baseline models and
effectively mitigating the limitations of prior approaches.

</details>


### [47] [A Recurrent Neural Network based Clustering Method for Binary Data Sets in Education](https://arxiv.org/abs/2508.13224)
*Mizuki Ohira,Toshimichi Saito*

Main category: cs.LG

TL;DR: 이 논문은 교육 분야에서 널리 사용되는 이진 데이터 집합인 S-P 차트에 대한 클러스터링 방법에 재귀 신경망을 적용한 것을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 학생 수가 증가함에 따라 S-P 차트를 처리하기 어려워지므로, 큰 차트를 작은 차트로 분류할 필요가 있다.

Method: 네트워크 동역학을 기반으로 한 간단한 클러스터링 방법을 제시하며, 이 방법은 여러 고정점과 집합의 유역을 가져 작은 S-P 차트와 대응하는 클러스터를 생성한다.

Result: 클러스터링 성능을 평가하기 위해 학생들의 답안 패턴의 특이성을 특징짓는 평균 주의 지수라는 중요한 특징량을 제시하며, 기본 실험을 수행하여 방법의 효과를 확인하였다.

Conclusion: 이 방법의 효과성이 입증되었다.

Abstract: This paper studies an application of a recurrent neural network to clustering
method for the S-P chart: a binary data set used widely in education. As the
number of students increases, the S-P chart becomes hard to handle. In order to
classify the large chart into smaller charts, we present a simple clustering
method based on the network dynamics. In the method, the network has multiple
fixed points and basins of attraction give clusters corresponding to small S-P
charts. In order to evaluate the clustering performance, we present an
important feature quantity: average caution index that characterizes
singularity of students answer oatterns. Performing fundamental experiments,
effectiveness of the method is confirmed.

</details>


### [48] [RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning](https://arxiv.org/abs/2508.13229)
*Suhang Hu,Wei Hu,Yuhang Su,Fan Zhang*

Main category: cs.LG

TL;DR: RISE는 복잡한 이미지 주석 작업에서 비전-언어 모델(VLM)의 한계를 극복하기 위한 두 단계 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 이미지 주석 작업에서 VLM이 면밀한 추론을 요구하는 문제에 직 faced하고 있다는 점.

Method: RISE는 두 단계로 구성된 프레임워크로, 첫째 단계인 RISE-CoT는 주석-추론-주석의 닫힌 루프를 생성하여 시각적으로 기반한, 논리적으로 일관된 사고 체인을 생성한다. 두 번째 단계인 RISE-R1은 고품질 CoT 하위 집합을 활용하여 감독된 파인 튜닝을 수행한 후 강화 파인 튜닝을 진행한다.

Result: RISE로 훈련된 Qwen2-VL-2B는 복잡한 이미지 주석 작업에서 SFT 및 Visual-RFT보다 더 우수한 성능을 발휘하며, 강력한 성능과 향상된 설명 가능성을 달성했다.

Conclusion: RISE는 수동적으로 주석된 사고 체인을 요구하지 않고도 VLM 추론을 발전시키기 위한 자가 감독 솔루션을 제공한다.

Abstract: Vision-Language Models (VLMs) struggle with complex image annotation tasks,
such as emotion classification and context-driven object detection, which
demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses
solely on annotation outcomes, ignoring underlying rationales, while Visual
Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought
(CoTs) due to the absence of high-quality, verified CoTs during pre-training.
We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework
to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement
learning-driven "annotation-reasoning-annotation" closed-loop generates
visually grounded, logically consistent CoTs by verifying their ability to
reconstruct original annotations without direct leakage. The Inspire and
Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by
RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement
fine-tuning to produce interpretable reasoning and accurate annotations,
achieving Expertise in complex visual tasks. Evaluated on complex and simple
image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and
Visual-RFT, achieving robust performance and enhanced explainability. RISE
offers a self-supervised solution for advancing VLM reasoning without requiring
manually annotated CoTs.

</details>


### [49] [Data driven feedback linearization of nonlinear control systems via Lie derivatives and stacked regression approach](https://arxiv.org/abs/2508.13241)
*Lakshmi Priya P. K.,Andreas Schwung*

Main category: cs.LG

TL;DR: 물리 시스템의 지배 방정식을 발견하고 효과적인 피드백 컨트롤러를 설계하는 것은 여전히 도전적이고 집중적인 연구 분야 중 하나이다.


<details>
  <summary>Details</summary>
Motivation: 물리 시스템의 동역학에 영향을 미치는 비선형 요소를 포함한 시스템 행동을 깊이 이해해야 한다.

Method: 희소 회귀 알고리즘을 사용하여 시스템을 식별하고, 출력 함수의 사전(dictionay)에 Lie 파생물을 적용하여 내부 동역학이 관측되지 않도록 보장하는 증강 제약 조건을 도출한다.

Result: 새로운 접근 방식은 스택 회귀 알고리즘과 상대 차수 조건을 결합하여 물리 모델의 진정한 지배 방정식을 발견하고 피드백 선형화한다.

Conclusion: 이 방법은 기존의 관련 연구와 차별화된 결과를 보여준다.

Abstract: Discovering the governing equations of a physical system and designing an
effective feedback controller remains one of the most challenging and intensive
areas of ongoing research. This task demands a deep understanding of the system
behavior, including the nonlinear factors that influence its dynamics. In this
article, we propose a novel methodology for identifying a feedback linearized
physical system based on known prior dynamic behavior. Initially, the system is
identified using a sparse regression algorithm, subsequently a feedback
controller is designed for the discovered system by applying Lie derivatives to
the dictionary of output functions to derive an augmented constraint which
guarantees that no internal dynamics are observed. Unlike the prior related
works, the novel aspect of this article combines the approach of stacked
regression algorithm and relative degree conditions to discover and feedback
linearize the true governing equations of a physical model.

</details>


### [50] [Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation](https://arxiv.org/abs/2508.13284)
*Nobuyuki Oishi,Philip Birch,Daniel Roggen,Paula Lago*

Main category: cs.LG

TL;DR: 이 연구는 센서 기반 인간 활동 인식(HAR)에서 고품질 레이블 데이터의 부족 문제를 해결하기 위한 물리적으로 그럴듯한 데이터 증강(PPDA) 기법을 소개하고, 전통적인 데이터 증강 기법과 비교하여 성능을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: HAR의 모델 성능 향상과 현실 세계 시나리오에서의 일반화 능력을 위해 고품질 레이블 데이터의 부족 문제를 해결하고자 한다.

Method: 물리 시뮬레이션을 활용하여 모션 캡처 또는 비디오 기반 포즈 추정에서 얻은 인체 운동 데이터를 통해 실제적인 변동성을 포함하는 PPDA를 소개한다.

Result: PPDA는 매크로 F1 점수를 평균 3.7 pp 개선하고, 최대 60% 더 적은 훈련 주체 수로도 경쟁력 있는 성과를 보여준다.

Conclusion: PPDA의 물리적 그럴듯함을 추구하는 것이 데이터 증강에서 장점을 제공하고, 심층 학습 HAR 모델 훈련을 위한 합성 관성 측정 장치 데이터를 생성하는 데 있어 물리 시뮬레이션의 잠재력을 강조한다.

Abstract: The scarcity of high-quality labeled data in sensor-based Human Activity
Recognition (HAR) hinders model performance and limits generalization across
real-world scenarios. Data augmentation is a key strategy to mitigate this
issue by enhancing the diversity of training datasets. Signal
Transformation-based Data Augmentation (STDA) techniques have been widely used
in HAR. However, these methods are often physically implausible, potentially
resulting in augmented data that fails to preserve the original meaning of the
activity labels. In this study, we introduce and systematically characterize
Physically Plausible Data Augmentation (PPDA) enabled by physics simulation.
PPDA leverages human body movement data from motion capture or video-based pose
estimation and incorporates various realistic variabilities through physics
simulation, including modifying body movements, sensor placements, and
hardware-related effects. We compare the performance of PPDAs with traditional
STDAs on three public datasets of daily activities and fitness workouts. First,
we evaluate each augmentation method individually, directly comparing PPDAs to
their STDA counterparts. Next, we assess how combining multiple PPDAs can
reduce the need for initial data collection by varying the number of subjects
used for training. Experiments show consistent benefits of PPDAs, improving
macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive
performance with up to 60% fewer training subjects than STDAs. As the first
systematic study of PPDA in sensor-based HAR, these results highlight the
advantages of pursuing physical plausibility in data augmentation and the
potential of physics simulation for generating synthetic Inertial Measurement
Unit data for training deep learning HAR models. This cost-effective and
scalable approach therefore helps address the annotation scarcity challenge in
HAR.

</details>


### [51] [Towards Human-AI Complementarity in Matching Tasks](https://arxiv.org/abs/2508.13285)
*Adrian Arnaiz-Rodriguez,Nina Corvelo Benz,Suhas Thejaswi,Nuria Oliver,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 협력적 매칭 시스템(comatch)은 인간과 AI의 상호보완성을 극대화하여 매칭 결정을 개선하는 데이터 기반의 새로운 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 매칭 시스템은 인간과 AI의 상호보완성을 달성하기 위해 설계되지 않았다.

Method: comatch는 가장 자신 있는 결정만을 선택하고 나머지는 인간 결정자에게 위임하는 협력적 접근 방식을 사용하여 매칭 결정을 최적화한다.

Result: comatch가 생성한 매칭 결과는 인간 참여자나 기존 알고리즘 매칭의 결과보다 우수하다.

Conclusion: 우리는 약 800명의 참여자를 대상으로 한 대규모 인간 대상 연구를 통해 제안된 접근 방식을 검증하였으며, 시스템의 구현 및 연구 데이터는 오픈 소스로 제공된다.

Abstract: Data-driven algorithmic matching systems promise to help human decision
makers make better matching decisions in a wide variety of high-stakes
application domains, such as healthcare and social service provision. However,
existing systems are not designed to achieve human-AI complementarity:
decisions made by a human using an algorithmic matching system are not
necessarily better than those made by the human or by the algorithm alone. Our
work aims to address this gap. To this end, we propose collaborative matching
(comatch), a data-driven algorithmic matching system that takes a collaborative
approach: rather than making all the matching decisions for a matching task
like existing systems, it selects only the decisions that it is the most
confident in, deferring the rest to the human decision maker. In the process,
comatch optimizes how many decisions it makes and how many it defers to the
human decision maker to provably maximize performance. We conduct a large-scale
human subject study with $800$ participants to validate the proposed approach.
The results demonstrate that the matching outcomes produced by comatch
outperform those generated by either human participants or by algorithmic
matching on their own. The data gathered in our human subject study and an
implementation of our system are available as open source at
https://github.com/Networks-Learning/human-AI-complementarity-matching.

</details>


### [52] [Hierarchical Conformal Classification](https://arxiv.org/abs/2508.13288)
*Floris den Hengst,Inès Blin,Majid Mohammadi,Syed Ihtesham Hussain Shah,Taraneh Younesian*

Main category: cs.LG

TL;DR: 계층적 준거 분류(HCC)는 준거 예측(CP)의 확장을 통해 클래스 계층 구조를 통합하여 신뢰할 수 있는 예측 세트를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 CP는 클래스의 세멘틱 관계나 계층적 구조와 같은 도메인 지식을 무시하여 클래스들을 평면적이고 구조가 없는 것으로 취급합니다.

Method: HCC를 제약 최적화 문제로 정식화하고 다양한 계층 수준의 노드로 구성된 예측 세트를 생성합니다.

Result: HCC는 최적성을 유지하면서도 커버리지 보장을 제공합니다.

Conclusion: 실험 결과는 HCC의 이점을 강조하며, 사용자 연구는 계층적 예측 세트가 평면적 예측 세트보다 선호된다는 것을 보여줍니다.

Abstract: Conformal prediction (CP) is a powerful framework for quantifying uncertainty
in machine learning models, offering reliable predictions with finite-sample
coverage guarantees. When applied to classification, CP produces a prediction
set of possible labels that is guaranteed to contain the true label with high
probability, regardless of the underlying classifier. However, standard CP
treats classes as flat and unstructured, ignoring domain knowledge such as
semantic relationships or hierarchical structure among class labels. This paper
presents hierarchical conformal classification (HCC), an extension of CP that
incorporates class hierarchies into both the structure and semantics of
prediction sets. We formulate HCC as a constrained optimization problem whose
solutions yield prediction sets composed of nodes at different levels of the
hierarchy, while maintaining coverage guarantees. To address the combinatorial
nature of the problem, we formally show that a much smaller, well-structured
subset of candidate solutions suffices to ensure coverage while upholding
optimality. An empirical evaluation on three new benchmarks consisting of
audio, image, and text data highlights the advantages of our approach, and a
user study shows that annotators significantly prefer hierarchical over flat
prediction sets.

</details>


### [53] [Efficient Constraint-Aware Flow Matching via Randomized Exploration](https://arxiv.org/abs/2508.13316)
*Zhengyan Huan,Jacob Boerma,Li-Ping Liu,Shuchin Aeron*

Main category: cs.LG

TL;DR: 이 연구는 Flow Matching을 통해 제약 조건을 만족하는 샘플 생성을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 제약 조건을 가진 샘플 생성을 효과적으로 수행하기 위해.

Method: FM 목표에 제약 조건과의 거리를 벌점으로 추가하거나, 무작위화와 평균 플로우 학습을 사용하여 제약을 만족하는 샘플을 생성한다.

Result: 제안된 방법이 제약 만족도에서 현저한 개선을 보이며 목표 분포와의 일치를 이룬다.

Conclusion: 두 단계 접근 방식이 계산 효율성이 높으며, 미래 연구 방향을 제시한다.

Abstract: We consider the problem of generating samples via Flow Matching (FM) with an
additional requirement that the generated samples must satisfy given
constraints. We consider two scenarios, viz.: (a) when a differentiable
distance function to the constraint set is given, and (b) when the constraint
set is only available via queries to a membership oracle. For case (a), we
propose a simple adaptation of the FM objective with an additional term that
penalizes the distance between the constraint set and the generated samples.
For case (b), we propose to employ randomization and learn a mean flow that is
numerically shown to have a high likelihood of satisfying the constraints. This
approach deviates significantly from existing works that require simple convex
constraints, knowledge of a barrier function, or a reflection mechanism to
constrain the probability flow. Furthermore, in the proposed setting we show
that a two-stage approach, where both stages approximate the same original flow
but with only the second stage probing the constraints via randomization, is
more computationally efficient. Through several synthetic cases of constrained
generation, we numerically show that the proposed approaches achieve
significant gains in terms of constraint satisfaction while matching the target
distributions. As a showcase for a practical oracle-based constraint, we show
how our approach can be used for training an adversarial example generator,
using queries to a hard-label black-box classifier. We conclude with several
future research directions. Our code is available at
https://github.com/ZhengyanHuan/FM-RE.

</details>


### [54] [Decoding Communications with Partial Information](https://arxiv.org/abs/2508.13326)
*Dylan Cope,Peter McBurney*

Main category: cs.LG

TL;DR: 이 논문은 기계 언어 습득에서 부분 관측 가능성을 고려하여 언어 학습의 복잡성을 증가시키는 문제를 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 기계 언어 습득은 모방 학습 문제로 자주 제시되지만, 모든 관련 정보를 볼 수 있다는 가정인 부분 관측 가능성은 잘 다루어지지 않았다.

Method: 환경에 대한 지식, 수행된 행동 및 전송된 메시지로부터 유용한 정보를 추론하는 도전적 설정을 탐구하고, 장난감 설정에서의 문제 해결을 시연한다.

Result: 이 문제의 여러 동기 부여 예제를 가져오고, 이를 해결할 수 있는 방법을 시연하여 일반적인 설정에서 발생하는 도전 과제를 공식적으로 탐구한다.

Conclusion: 사적인 정보를 디코딩하여 언어 습득을 촉진하는 학습 기반 알고리즘을 제시한다.

Abstract: Machine language acquisition is often presented as a problem of imitation
learning: there exists a community of language users from which a learner
observes speech acts and attempts to decode the mappings between utterances and
situations. However, an interesting consideration that is typically unaddressed
is partial observability, i.e. the learner is assumed to see all relevant
information. This paper explores relaxing this assumption, thereby posing a
more challenging setting where such information needs to be inferred from
knowledge of the environment, the actions taken, and messages sent. We see
several motivating examples of this problem, demonstrate how they can be solved
in a toy setting, and formally explore challenges that arise in more general
settings. A learning-based algorithm is then presented to perform the decoding
of private information to facilitate language acquisition.

</details>


### [55] [A Dual-Attention Graph Network for fMRI Data Classification](https://arxiv.org/abs/2508.13328)
*Amirali Arbab,Zeinab Davarani,Mehran Safayani*

Main category: cs.LG

TL;DR: 이 연구는 자폐 스펙트럼 장애(ASD) 진단을 위한 역동적인 그래프 생성 및 시공간 주의 메커니즘을 활용한 새로운 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 신경 과학 분야의 발전을 위해 복잡한 신경 활동 동역학을 이해하는 것이 중요하다.

Method: 이 연구는 변환기 기반 주의 메커니즘을 사용하여 각 시간 간격의 기능적 뇌 연결성을 역동적으로 추론하여 모델이 중요한 뇌 영역과 시간 구간에 선택적으로 집중할 수 있도록 한다.

Result: ABIDE 데이터셋의 하위 집합을 평가한 결과, 모델은 63.2%의 정확도와 60.0 AUC를 기록하여 정적 그래프 기반 접근 방식(예: GCN: 51.8)을 능가하였다.

Conclusion: 동적 연결성과 시공간 맥락의 공동 모델링이 fMRI 분류에 효과적임을 검증하였다.

Abstract: Understanding the complex neural activity dynamics is crucial for the
development of the field of neuroscience. Although current functional MRI
classification approaches tend to be based on static functional connectivity or
cannot capture spatio-temporal relationships comprehensively, we present a new
framework that leverages dynamic graph creation and spatiotemporal attention
mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in
this research dynamically infers functional brain connectivity in each time
interval using transformer-based attention mechanisms, enabling the model to
selectively focus on crucial brain regions and time segments. By constructing
time-varying graphs that are then processed with Graph Convolutional Networks
(GCNs) and transformers, our method successfully captures both localized
interactions and global temporal dependencies. Evaluated on the subset of ABIDE
dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static
graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint
modeling of dynamic connectivity and spatio-temporal context for fMRI
classification. The core novelty arises from (1) attention-driven dynamic graph
creation that learns temporal brain region interactions and (2) hierarchical
spatio-temporal feature fusion through GCNtransformer fusion.

</details>


### [56] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE는 차세대 Mixture-of-Experts 아키텍처를 위한 확장 가능한 훈련 성능을 제공하는 새로운 MoE 훈련 시스템이다.


<details>
  <summary>Details</summary>
Motivation: Emerging MoE 아키텍처는 전문가 세분화 및 대규모 라우팅을 통해 강력한 모델 품질을 제공하지만 메모리 오버헤드와 커뮤니케이션 비용 때문에 확장성에 한계가 있다.

Method: X-MoE는 플랫폼 간 커널을 활용한 패딩 없는 훈련, 중복 회피 디스패치 및 시퀀스 분할 MoE 블록을 통한 하이브리드 병렬성과 같은 새로운 기술을 적용한다.

Result: Frontier 슈퍼컴퓨터를 통한 평가 결과, X-MoE는 1024개의 GPU에서 DeepSeek 스타일의 MoE를 545억 개의 매개변수로 확장할 수 있으며, 이는 기존 방법으로 같은 하드웨어 예산에서 훈련 가능한 모델보다 10배 크다.

Conclusion: X-MoE는 높은 훈련 처리량을 유지하면서 확장 가능한 훈련 성능을 제공한다.

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [57] [Dimension lower bounds for linear approaches to function approximation](https://arxiv.org/abs/2508.13346)
*Daniel Hsu*

Main category: cs.LG

TL;DR: 선형 대수적 접근 방식을 통해 $L^2$ 함수 근사 문제를 해결하는 선형 방법의 차원 하한을 증명한다.


<details>
  <summary>Details</summary>
Motivation: 선형 방법의 차원 하한을 제시하기 위한 필요성.

Method: 선형 대수적 접근 방식을 사용하여 주장을 전개한다.

Result: 커널 방법에 대해 샘플 크기 하한을 제시한다.

Conclusion: 제시된 방법은 Kolmogorov $n$-폭의 하한을 확립하는 데에도 적용될 수 있다.

Abstract: This short note presents a linear algebraic approach to proving dimension
lower bounds for linear methods that solve $L^2$ function approximation
problems. The basic argument has appeared in the literature before (e.g.,
Barron, 1993) for establishing lower bounds on Kolmogorov $n$-widths. The
argument is applied to give sample size lower bounds for kernel methods.

</details>


### [58] [Counterfactual Probabilistic Diffusion with Expert Models](https://arxiv.org/abs/2508.13355)
*Wenhao Mu,Zhi Cao,Mehmed Uludag,Alexander Rodríguez*

Main category: cs.LG

TL;DR: 복잡한 동적 시스템에서의 반사실적 분포 예측은 과학적 모델링과 의사결정에 필수적이다. 기존 방법은 데이터 부족 시 성능 저하를 보인다.


<details>
  <summary>Details</summary>
Motivation: 공공 보건 및 의학과 같은 분야에서 의사결정을 위해 복잡한 동적 시스템의 반사실적 분포 예측이 필요하다.

Method: 불완전한 전문가 모델로부터의 안내를 통합하는 시계열 확산 기반 프레임워크인 ODE-Diff를 제안한다.

Result: ODE-Diff는 반사실적 예측에서 강력한 기준선보다 일관되게 우수한 성능을 보인다.

Conclusion: 이 방법은 기계적 접근과 데이터 기반 접근을 연결하여 보다 신뢰할 수 있고 해석 가능한 인과 추론을 가능하게 한다.

Abstract: Predicting counterfactual distributions in complex dynamical systems is
essential for scientific modeling and decision-making in domains such as public
health and medicine. However, existing methods often rely on point estimates or
purely data-driven models, which tend to falter under data scarcity. We propose
a time series diffusion-based framework that incorporates guidance from
imperfect expert models by extracting high-level signals to serve as structured
priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and
data-driven approaches, enabling more reliable and interpretable causal
inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations,
synthetic pharmacological dynamics, and real-world case studies, demonstrating
that it consistently outperforms strong baselines in both point prediction and
distributional accuracy.

</details>


### [59] [Adaptive Conformal Prediction Intervals Over Trajectory Ensembles](https://arxiv.org/abs/2508.13362)
*Ruipu Li,Daniel Menacho,Alexander Rodríguez*

Main category: cs.LG

TL;DR: 본 연구는 샘플링된 경로를 보정된 예측 구간으로 변환하는 통합 프레임워크를 제안하며, 이로써 더 정확한 불확실성 추정을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 미래 궤적은 자율 주행, 허리케인 예측 및 전염병 모델링 등 여러 분야에서 중요한 역할을 하며, 연구자들은 일반적으로 확률 모델이나 여러 자기회귀 예측기를 활용하여 앙상블 경로를 생성합니다.

Method: 우리는 샘플링된 경로를 보정된 예측 구간으로 변환하는 컨포말 예측에 기반한 통합 프레임워크를 제안합니다. 이는 온라인 업데이트 단계와 단계 간 의존성을 포착하는 최적화 단계를 도입하여 구현됩니다.

Result: 이 방법은 각 경로 주변에 불연속적인 예측 구간을 생성하고, 자연스럽게 시간적 의존성을 포착하며, 더 날카롭고 적응적인 불확실성 추정을 제공합니다.

Conclusion: 이 연구는 보정된 예측 구간을 제공함으로써 예측의 정확성을 향상시키는 새로운 방법론을 제시합니다.

Abstract: Future trajectories play an important role across domains such as autonomous
driving, hurricane forecasting, and epidemic modeling, where practitioners
commonly generate ensemble paths by sampling probabilistic models or leveraging
multiple autoregressive predictors. While these trajectories reflect inherent
uncertainty, they are typically uncalibrated. We propose a unified framework
based on conformal prediction that transforms sampled trajectories into
calibrated prediction intervals with theoretical coverage guarantees. By
introducing a novel online update step and an optimization step that captures
inter-step dependencies, our method can produce discontinuous prediction
intervals around each trajectory, naturally capture temporal dependencies, and
yield sharper, more adaptive uncertainty estimates.

</details>


### [60] [Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference](https://arxiv.org/abs/2508.13380)
*Seohyeon Cha,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 이 논문은 자원 제약이 있는 엣지 디바이스에서 다중 작업 모델을 배치하고 쿼리를 최적 라우팅하여 추론 정확도를 극대화하는 통합 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자원 제약이 있는 엣지 디바이스에서 지능형 서비스에 대한 수요가 증가하고 있어, 여러 작업을 동시에 실행해야 하는 많은 실제 응용 프로그램에 대응할 필요성이 커지고 있습니다.

Method: 다중 작업 모델을 클라이언트와 엣지 서버에 배치하고, 쿼리를 최적으로 라우팅하는 문제를 혼합 정수 프로그램으로 정식화하고, J3O라는 교체 알고리즘을 도입하였습니다. 이 알고리즘은 Lagrangian 완화 서브모듈 최적화로 모델을 선택하고, 제약 조건이 있는 선형 프로그래밍을 통해 최적의 오프로드를 결정합니다.

Result: J3O는 다중 작업 벤치마크에서 최적 솔버가 요구하는 실행 시간의 15% 이내로 유지하면서 항상 97% 이상의 최적 정확도를 달성합니다.

Conclusion: 제안된 프레임워크는 다양한 작업 부하에 대해 확장성을 유지하면서 엣지에서 배치 처리를 고려하도록 J3O를 확장하였습니다.

Abstract: The growing demand for intelligent services on resource-constrained edge
devices has spurred the development of collaborative inference systems that
distribute workloads across end devices, edge servers, and the cloud. While
most existing frameworks focus on single-task, single-model scenarios, many
real-world applications (e.g., autonomous driving and augmented reality)
require concurrent execution of diverse tasks including detection,
segmentation, and depth estimation. In this work, we propose a unified
framework to jointly decide which multi-task models to deploy (onload) at
clients and edge servers, and how to route queries across the hierarchy
(offload) to maximize overall inference accuracy under memory, compute, and
communication constraints. We formulate this as a mixed-integer program and
introduce J3O (Joint Optimization of Onloading and Offloading), an alternating
algorithm that (i) greedily selects models to onload via Lagrangian-relaxed
submodular optimization and (ii) determines optimal offloading via constrained
linear programming. We further extend J3O to account for batching at the edge,
maintaining scalability under heterogeneous task loads. Experiments show J3O
consistently achieves over $97\%$ of the optimal accuracy while incurring less
than $15\%$ of the runtime required by the optimal solver across multi-task
benchmarks.

</details>


### [61] [Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp](https://arxiv.org/abs/2508.13406)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 이 연구는 임상적으로 정의된 발작 시작 영역(SOZ)과 통계적으로 비정상적인 채널 간의 공간 일치를 평가하기 위한 정량적 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 발작 시작 영역의 정확한 국소화는 성공적인 수술 결과에 필수적입니다.

Method: (1) 비지도 이상치 탐지 방법으로, 적응형 이웃 선택을 통한 LOF 분석을 사용하여 기이한 채널을 식별하고, (2) 공간 상관 분석을 통해 정확한 공동 발생 메트릭과 가중치 인덱스 유사성을 계산합니다.

Result: LOF 기반 접근법이 효과적으로 이상치를 탐지하며, 채널 근접성을 가중치로 한 인덱스 매칭이 SOZ 국소화에서 정확한 매칭보다 뛰어난 성과를 보였습니다.

Conclusion: 발작 없는 환자와 성공적인 수술 결과를 가진 환자에서 SOZ 국소화 성능이 높았으며, 기이치 탐지 및 가중 공간 메트릭은 SOZ 국소화에 보완적인 방법을 제공합니다.

Abstract: This study presents a quantitative framework for evaluating the spatial
concordance between clinically defined seizure onset zones (SOZs) and
statistically anomalous channels identified through time-frequency analysis of
chirp events. The proposed pipeline employs a two-step methodology: (1)
Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with
adaptive neighborhood selection identifies anomalous channels based on
spectro-temporal features of chirp (Onset frequency, offset frequency, and
temporal duration); and (2) Spatial Correlation Analysis, which computes both
exact co-occurrence metrics and weighted index similarity, incorporating
hemispheric congruence and electrode proximity. Key findings demonstrate that
the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects
outliers, with index matching (weighted by channel proximity) outperforming
exact matching in SOZ localization. Performance metrics (precision, recall, F1)
were highest for seizure-free patients (Index Precision mean: 0.903) and those
with successful surgical outcomes (Index Precision mean: 0.865), whereas
failure cases exhibited lower concordance (Index Precision mean: 0.460). The
key takeaway is that chirp-based outlier detection, combined with weighted
spatial metrics, provides a complementary method for SOZ localization,
particularly in patients with successful surgical outcomes.

</details>


### [62] [NovoMolGen: Rethinking Molecular Language Model Pretraining](https://arxiv.org/abs/2508.13408)
*Kamran Chitsaz,Roshan Balaji,Quentin Fournier,Nirav Pravinbhai Bhatt,Sarath Chandar*

Main category: cs.LG

TL;DR: 제안된 NovoMolGen 모델은 일반적인 언어 모델링 관행과 분자의 생성 성능 간의 관계를 체계적으로 조사하여 향상된 분자 모델링 전략을 제공한다.


<details>
  <summary>Details</summary>
Motivation: de-novo 분자를 설계하는 데 필요한 화학 공간을 효율적으로 탐색하기 위해, 기존의 깊은 생성 모델과의 차별화된 접근이 필요하다.

Method: NovoMolGen이라는 변환기 기반의 기본 모델 패밀리를 도입하고, 15억 개의 분자로 사전 훈련을 수행했다.

Result: 사전 훈련 중에 측정된 성능 지표와 실제 하류 성능 간의 약한 상관관계를 발견하고, 기존 Mol-LLM 및 특수 생성 모델보다 우수한 성능을 입증했다.

Conclusion: NovoMolGen은 비제한적 및 목표 지향적 분자 생성 작업에서 새로운 최첨단 결과를 설정하며, 효율적이고 효과적인 분자 모델링 전략의 발전을 위한 강력한 기초를 제공한다.

Abstract: Designing de-novo molecules with desired property profiles requires efficient
exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$
possible synthesizable candidates. While various deep generative models have
been developed to design small molecules using diverse input representations,
Molecular Large Language Models (Mol-LLMs) based on string representations have
emerged as a scalable approach capable of exploring billions of molecules.
However, there remains limited understanding regarding how standard language
modeling practices such as textual representations, tokenization strategies,
model size, and dataset scale impact molecular generation performance. In this
work, we systematically investigate these critical aspects by introducing
NovoMolGen, a family of transformer-based foundation models pretrained on 1.5
billion molecules for de-novo molecule generation. Through extensive empirical
analyses, we identify a weak correlation between performance metrics measured
during pretraining and actual downstream performance, revealing important
distinctions between molecular and general NLP training dynamics. NovoMolGen
establishes new state-of-the-art results, substantially outperforming prior
Mol-LLMs and specialized generative models in both unconstrained and
goal-directed molecular generation tasks, thus providing a robust foundation
for advancing efficient and effective molecular modeling strategies.

</details>


### [63] [Decentralized Contextual Bandits with Network Adaptivity](https://arxiv.org/abs/2508.13411)
*Chuyun Deng,Huiwen Jia*

Main category: cs.LG

TL;DR: 이 논문은 네트워크 상의 맥락적 선형 밴딧 문제를 다루며, 동적으로 업데이트된 네트워크 가중치에 의해 안내되는 적응형 정보 공유를 통해 글로벌 및 로컬 학습을 가능하게 하는 두 가지 UCB 알고리즘을 개발한다.


<details>
  <summary>Details</summary>
Motivation: 정보가 부분적으로 공유되는 네트워크 환경에서 맥락적 밴딧의 미개척 영역을 탐구하고자 한다.

Method: NetLinUCB 및 Net-SGD-UCB라는 두 가지 네트워크 인식 UCB 알고리즘을 개발하며, 이는 동적으로 업데이트되는 네트워크 가중치에 의해 안내되는 적응형 정보 공유를 가능하게 한다.

Result: 두 알고리즘은 공유된 구조에 따른 학습 복잡성을 $O(N)$에서 서브라인 $O(	ext{sqrt{N}})$로 줄이는 후회 경계를 설정한다.

Conclusion: NetLinUCB는 저소음 영역에서 정밀한 이질성에 강점이 있고, Net-SGD-UCB는 고차원 및 고분산 맥락에 강건성을 가진다는 것을 보여준다.

Abstract: We consider contextual linear bandits over networks, a class of sequential
decision-making problems where learning occurs simultaneously across multiple
locations and the reward distributions share structural similarities while also
exhibiting local differences. While classical contextual bandits assume either
fully centralized data or entirely isolated learners, much remains unexplored
in networked environments when information is partially shared. In this paper,
we address this gap by developing two network-aware Upper Confidence Bound
(UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information
sharing guided by dynamically updated network weights. Our approach decompose
learning into global and local components and as a result allow agents to
benefit from shared structure without full synchronization. Both algorithms
incur lighter communication costs compared to a fully centralized setting as
agents only share computed summaries regarding the homogeneous features. We
establish regret bounds showing that our methods reduce the learning complexity
associated with the shared structure from $O(N)$ to sublinear $O(\sqrt{N})$,
where $N$ is the size of the network. The two algorithms reveal complementary
strengths: NetLinUCB excels in low-noise regimes with fine-grained
heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance
contexts. We further demonstrate the effectiveness of our methods across
simulated pricing environments compared to standard benchmarks.

</details>


### [64] [MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search](https://arxiv.org/abs/2508.13415)
*Jeremy Carleton,Debajoy Mukherjee,Srinivas Shakkottai,Dileep Kalathil*

Main category: cs.LG

TL;DR: MAVIS는 다목적 정렬을 위해 값 기반 추론 시간 검색을 사용하는 경량 프레임워크로, 사용자 선호도에 따라 LLM의 출력을 조정할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 다양한 응용 분야에서 여러 목표를 균형 있게 조정하는 것이 필요하다.

Method: MAVIS는 각각의 서로 다른 목표에 대응하는 작은 가치 모델을 훈련시키고, 추론 시 사용자 지정 가중치를 사용하여 이 모델들을 결합하여 기본 모델의 출력 분포를 조정한다.

Result: MAVIS는 각 목표를 위한 모델을 미세 조정하고 나중에 결합하는 기존 방식보다 성능이 우수하다.

Conclusion: MAVIS는 사용자 맞춤형 선호를 극대화하는 데 효과적이다.

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
applications that demand balancing multiple, often conflicting, objectives --
such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific
preferences in such multi-objective settings typically requires fine-tuning
models for each objective or preference configuration, which is computationally
expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via
Value-Guided Inference-Time Search -- a lightweight inference-time alignment
framework that enables dynamic control over LLM behavior without modifying the
base model's weights. MAVIS trains a set of small value models, each
corresponding to a distinct objective. At inference time, these value models
are combined using user-specified weights to produce a tilting function that
adjusts the base model's output distribution toward desired trade-offs. The
value models are trained using a simple iterative algorithm that ensures
monotonic improvement of the KL-regularized policy. We show empirically that
MAVIS outperforms baselines that fine-tune per-objective models and combine
them post hoc, and even approaches the performance of the idealized setting
where models are fine-tuned for a user's exact preferences.

</details>


### [65] [EventTSF: Event-Aware Non-Stationary Time Series Forecasting](https://arxiv.org/abs/2508.13434)
*Yunfeng Ge,Ming Jin,Yiji Zhao,Hongyan Li,Bo Du,Chang Xu,Shirui Pan*

Main category: cs.LG

TL;DR: Event-aware non-stationary time series forecasting (EventTSF)는 역사적 시계열과 텍스트 이벤트를 통합하여 예측 정확도를 크게 향상시키는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 비정상적인 시계열 예측을 개선하기 위해 자연어 기반의 외부 사건을 통합하는 연구가 부족하다.

Method: EventTSF는 역사적 시계열과 텍스트 이벤트를 결합하여 후속 예측을 수행하는 자기 회귀 생성 프레임워크이다. 이를 통해 시계열 데이터와 텍스트 데이터 간의 세밀한 상호작용을 포착한다.

Result: EventTSF는 12개의 기준 모델을 초과하는 성능을 보여주며, 예측 정확도가 10.7% 향상되고 교육 효율성이 $1.13\times$ 증가했다.

Conclusion: EventTSF는 이벤트 유발 불확실성을 효과적으로 관리하고 다양한 비정상적인 시계열 예측 시나리오에서 뛰어난 성능을 발휘한다.

Abstract: Time series forecasting plays a vital role in critical domains like energy
and transportation, where non-stationary dynamics are deeply intertwined with
events in other modalities such as texts. However, incorporating natural
language-based external events to improve non-stationary forecasting remains
largely unexplored, as most approaches still rely on a single modality,
resulting in limited contextual knowledge and model underperformance. Enabling
fine-grained multimodal interactions between temporal and textual data is
challenged by three fundamental issues: (1) the difficulty of fine-grained
synchronization between time-varying discrete textual events and continuous
time series; (2) the inherent temporal uncertainty introduced by textual
semantics; and (3) the misalignment between textual event embeddings and
multi-resolution temporal patterns. In this work, we address these challenges
by introducing event-aware non-stationary time series forecasting (EventTSF),
an autoregressive generation framework that integrates historical time series
with textual events to make subsequent forecasts. Specifically, EventTSF uses
autoregressive diffusion with flow matching at each step to capture nuanced
temporal-event interactions. To handle event-induced uncertainty, flow matching
timesteps are adaptively controlled according to event semantic signals. The
underlying denoiser employs a multimodal U-shaped diffusion transformer that
efficiently fuses temporal and textual modalities across different resolutions.
Extensive experiments on 8 synthetic and real-world datasets show that EventTSF
outperforms 12 baselines across diverse event-aware non-stationary time series
forecasting scenarios, achieving substantial improvements of 10.7% higher
forecasting accuracy and $1.13\times$ faster training efficiency.

</details>


### [66] [SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer](https://arxiv.org/abs/2508.13435)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: SVDformer는 방향 인식 그래프 표현 학습을 위한 새로운 프레임워크로, SVD와 Transformer 아키텍처를 결합하여 방향성 의미와 전역 구조 패턴을 효과적으로 캡처한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 방향 그래프 신경망이 방향적인 의미와 전역 구조 패턴을 함께 캡처하는 데 어려움을 겪고 있기 때문이다.

Method: SVDformer는 다중 헤드 자기 주의를 통해 특이값 임베딩을 정제하고, Transformer를 활용하여 들어오는/나가는 엣지 패턴 간의 다중 스케일 상호작용을 모델링한다.

Result: 여섯 개의 방향 그래프 벤치마크에서 실험한 결과, SVDformer는 최신 GNN 및 방향 인식 기준선을 지속적으로 초월한다.

Conclusion: SVDformer는 방향 그래프에서 표현 학습을 위한 새로운 패러다임을 확립한다.

Abstract: Directed graphs are widely used to model asymmetric relationships in
real-world systems. However, existing directed graph neural networks often
struggle to jointly capture directional semantics and global structural
patterns due to their isotropic aggregation mechanisms and localized filtering
mechanisms. To address this limitation, this paper proposes SVDformer, a novel
framework that synergizes SVD and Transformer architecture for direction-aware
graph representation learning. SVDformer first refines singular value
embeddings through multi-head self-attention, adaptively enhancing critical
spectral components while suppressing high-frequency noise. This enables
learnable low-pass/high-pass graph filtering without requiring spectral
kernels. Furthermore, by treating singular vectors as directional projection
bases and singular values as scaling factors, SVDformer uses the Transformer to
model multi-scale interactions between incoming/outgoing edge patterns through
attention weights, thereby explicitly preserving edge directionality during
feature propagation. Extensive experiments on six directed graph benchmarks
demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and
direction-aware baselines on node classification tasks, establishing a new
paradigm for learning representations on directed graphs.

</details>


### [67] [Dynamic Design of Machine Learning Pipelines via Metalearning](https://arxiv.org/abs/2508.13436)
*Edesio Alcobaça,André C. P. L. F. de Carvalho*

Main category: cs.LG

TL;DR: 자동화된 머신러닝(AutoML)은 모델 선택, 하이퍼파라미터 조정 및 특징 공학을 자동화하여 머신러닝 기반 시스템의 디자인을 민주화했다. 그러나 전통적인 탐색 및 최적화 전략들에 수반되는 높은 계산 비용이 여전히 중요한 도전 과제가 되고 있다. 본 논문은 AutoML 시스템의 탐색 공간을 동적으로 설계하기 위한 메타러닝 방법을 제안한다. 제안된 방법은 역사적 메타 지식을 사용하여 탐색 공간의 유망한 지역을 선택함으로써 최적화 과정을 가속화한다.


<details>
  <summary>Details</summary>
Motivation: AutoML 시스템의 설계에서 전통적인 최적화 전략의 높은 계산 비용과 탐색 공간의 과적합 문제 해결.

Method: 제안된 메타러닝 방법은 역사적 메타 지식을 활용하여 탐색 공간에서 유망한 영역을 선택하고, 이를 통해 최적화 과정을 가속화한다.

Result: 실험에 따르면, 이 방법은 Random Search에서 런타임을 89% 줄이고, 탐색 공간을 전처리기 1.8/13 및 분류기 4.3/16 비율로 감소시킨다.

Conclusion: 제안된 방법은 Auto-Sklearn에 적용되었을 때도 경쟁력 있는 성능을 보여주었고, 탐색 공간을 줄일 수 있었다. 본 연구는 메타 피쳐 선택, 메타 모델 설명 가능성, 탐색 공간 축소 전략에서의 트레이드오프에 대한 통찰도 포함하고 있다.

Abstract: Automated machine learning (AutoML) has democratized the design of machine
learning based systems, by automating model selection, hyperparameter tuning
and feature engineering. However, the high computational cost associated with
traditional search and optimization strategies, such as Random Search, Particle
Swarm Optimization and Bayesian Optimization, remains a significant challenge.
Moreover, AutoML systems typically explore a large search space, which can lead
to overfitting. This paper introduces a metalearning method for dynamically
designing search spaces for AutoML system. The proposed method uses historical
metaknowledge to select promising regions of the search space, accelerating the
optimization process. According to experiments conducted for this study, the
proposed method can reduce runtime by 89\% in Random Search and search space by
(1.8/13 preprocessor and 4.3/16 classifier), without compromising significant
predictive performance. Moreover, the proposed method showed competitive
performance when adapted to Auto-Sklearn, reducing its search space.
Furthermore, this study encompasses insights into meta-feature selection,
meta-model explainability, and the trade-offs inherent in search space
reduction strategies.

</details>


### [68] [ASAP: Unsupervised Post-training with Label Distribution Shift Adaptive Learning Rate](https://arxiv.org/abs/2508.13445)
*Heewon Park,Mugon Joe,Miru Kim,Minhae Kwon*

Main category: cs.LG

TL;DR: ASAP는 온라인 레이블 시프트에 적응하기 위해 동적으로 학습률을 조정하는 방법을 제안하며, 레이블이 없는 상황에서도 효과적으로 작동합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 모델은 온라인 레이블 시프트에서 레이블 분포가 시간이 지남에 따라 변화하는 문제에 직면합니다.

Method: ASAP는 현재 및 이전의 레이블 없는 출력 간의 코사인 거리를 계산하여 동적으로 학습률을 조정하는 방법입니다.

Result: 여러 데이터셋과 시프트 시나리오에서 ASAP의 정확도와 효율성이 일관되게 향상됨을 보여줍니다.

Conclusion: ASAP는 비지도 모델 적응에 실용적인 방법이 됩니다.

Abstract: In real-world applications, machine learning models face online label shift,
where label distributions change over time. Effective adaptation requires
careful learning rate selection: too low slows adaptation and too high causes
instability. We propose ASAP (Adaptive Shift Aware Post-training), which
dynamically adjusts the learning rate by computing the cosine distance between
current and previous unlabeled outputs and mapping it within a bounded range.
ASAP requires no labels, model ensembles, or past inputs, using only the
previous softmax output for fast, lightweight adaptation. Experiments across
multiple datasets and shift scenarios show ASAP consistently improves accuracy
and efficiency, making it practical for unsupervised model adaptation.

</details>


### [69] [Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification](https://arxiv.org/abs/2508.13452)
*Ruobing Jiang,Mengzhe Liu,Haobing Liu,Yanwei Yu*

Main category: cs.LG

TL;DR: HMC는 구조적 일관성과 MTL의 손실 가중치 균형 문제를 해결하기 위해 HCAL 분류기를 제안한다.


<details>
  <summary>Details</summary>
Motivation: HMC의 구조적 일관성 및 MTL에서의 손실 가중치 균형 유지와 관련된 문제를 해결하기 위함이다.

Method: MTL과 프로토타입 대비 학습 및 적응적 작업 가중치 메커니즘을 통합한 HCAL 분류기를 제안한다.

Result: 제안된 분류기는 기존 모델보다 높은 분류 정확도와 감소된 계층 위반율을 보여준다.

Conclusion: 이 연구는 계층적 다중 레이블 분류 문제를 효과적으로 해결하는 새로운 접근 방식을 제시한다.

Abstract: Hierarchical Multi-Label Classification (HMC) faces critical challenges in
maintaining structural consistency and balancing loss weighting in Multi-Task
Learning (MTL). In order to address these issues, we propose a classifier
called HCAL based on MTL integrated with prototype contrastive learning and
adaptive task-weighting mechanisms. The most significant advantage of our
classifier is semantic consistency including both prototype with explicitly
modeling label and feature aggregation from child classes to parent classes.
The other important advantage is an adaptive loss-weighting mechanism that
dynamically allocates optimization resources by monitoring task-specific
convergence rates. It effectively resolves the "one-strong-many-weak"
optimization bias inherent in traditional MTL approaches. To further enhance
robustness, a prototype perturbation mechanism is formulated by injecting
controlled noise into prototype to expand decision boundaries. Additionally, we
formalize a quantitative metric called Hierarchical Violation Rate (HVR) as to
evaluate hierarchical consistency and generalization. Extensive experiments
across three datasets demonstrate both the higher classification accuracy and
reduced hierarchical violation rate of the proposed classifier over baseline
models.

</details>


### [70] [Classifying Clinical Outcome of Epilepsy Patients with Ictal Chirp Embeddings](https://arxiv.org/abs/2508.13476)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 이 연구는 다양한 결과 시나리오에 대한 해석 가능한 시각화를 위한 t-SNE 파이프라인을 제시하며, chirp 기반 메트릭스를 사용하여 분류 작업을 수행합니다.


<details>
  <summary>Details</summary>
Motivation: chirp 특징에 대한 해석 가능한 시각화를 통해 다양한 결과 시나리오에서의 데이터 분석을 개선하고자 함.

Method: t-SNE를 활용하여 지역 이웃 관계를 보존하고, Student t 분포 기반 유사도 최적화를 통해 군집화 문제를 해결하여 2D t-SNE 임베딩에서 세 가지 분류 작업을 수행함.

Result: Random Forest와 k-NN 분류기가 뛰어난 성능을 보이며, 최적 사례 감지에서 최대 88.8%의 정확도를 달성함.

Conclusion: 해석 가능한 임베딩과 지역적 특징 기여를 활용하여 임상 분류 및 의사결정을 지원할 수 있는 잠재력을 보여줌.

Abstract: This study presents a pipeline leveraging t-Distributed Stochastic Neighbor
Embedding (t-SNE) for interpretable visualizations of chirp features across
diverse outcome scenarios. The dataset, comprising chirp-based temporal,
spectral, and frequency metrics. Using t-SNE, local neighborhood relationships
were preserved while addressing the crowding problem through Student
t-distribution-based similarity optimization. Three classification tasks were
formulated on the 2D t-SNE embeddings: (1) distinguishing clinical success from
failure/no-resection, (2) separating high-difficulty from low-difficulty cases,
and (3) identifying optimal cases, defined as successful outcomes with minimal
clinical difficulty. Four classifiers, namely, Random Forests, Support Vector
Machines, Logistic Regression, and k-Nearest Neighbors, were trained and
evaluated using stratified 5-fold cross-validation. Across tasks, the Random
Forest and k-NN classifiers demonstrated superior performance, achieving up to
88.8% accuracy in optimal case detection (successful outcomes with minimal
clinical difficulty). Additionally, feature influence sensitivity maps were
generated using SHAP explanations applied to model predicting t-SNE
coordinates, revealing spatially localized feature importance within the
embedding space. These maps highlighted how specific chirp attributes drive
regional clustering and class separation, offering insights into the latent
structure of the data. The integrated framework showcases the potential of
interpretable embeddings and local feature attribution for clinical
stratification and decision support.

</details>


### [71] [DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing](https://arxiv.org/abs/2508.13490)
*Pengyu Lai,Yixiao Chen,Hui Xu*

Main category: cs.LG

TL;DR: 본 논문은 복잡한 비선형 동적 시스템을 근사하기 위해 새롭게 제안된 신경 연산자 프레임워크인 DyMixOp를 소개한다.


<details>
  <summary>Details</summary>
Motivation: Partial differential equations(PDEs)로 설명되는 비선형 동적 시스템을 근사하는 데 있어 신경망 사용의 주요 도전 과제는 이러한 시스템을 적절한 형식으로 변환하는 것이다.

Method: DyMixOp는 관성 매니폴드 이론에 기반하여 무한 차원 비선형 PDE 동역학을 유한 차원 잠재 공간으로 변환하여 본질적인 비선형 상호작용을 유지하고 물리적 해석 가능성을 높이는 구조적 기초를 구축한다.

Result: 실험 결과는 DyMixOp가 다양한 PDE 벤치마크에서 최첨단 성능을 달성하고 예측 오류를 획기적으로 감소시켰음을 보인다.

Conclusion: 특히 대류 우세 시나리오에서 최대 86.7%까지 예측 오류를 크게 줄이면서 계산 효율성과 확장성을 유지하는 모습을 보인다.

Abstract: A primary challenge in using neural networks to approximate nonlinear
dynamical systems governed by partial differential equations (PDEs) is
transforming these systems into a suitable format, especially when dealing with
non-linearizable dynamics or the need for infinite-dimensional spaces for
linearization. This paper introduces DyMixOp, a novel neural operator framework
for PDEs that integrates insights from complex dynamical systems to address
this challenge. Grounded in inertial manifold theory, DyMixOp transforms
infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent
space, establishing a structured foundation that maintains essential nonlinear
interactions and enhances physical interpretability. A key innovation is the
Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in
turbulence. This transformation effectively captures both fine-scale details
and nonlinear interactions, while mitigating spectral bias commonly found in
existing neural operators. The framework is further strengthened by a
dynamics-informed architecture that connects multiple LGM layers to approximate
linear and nonlinear dynamics, reflecting the temporal evolution of dynamical
systems. Experimental results across diverse PDE benchmarks demonstrate that
DyMixOp achieves state-of-the-art performance, significantly reducing
prediction errors, particularly in convection-dominated scenarios reaching up
to 86.7\%, while maintaining computational efficiency and scalability.

</details>


### [72] [Uncertainty Tube Visualization of Particle Trajectories](https://arxiv.org/abs/2508.13505)
*Jixian Li,Timbwaoga Aime Judicael Ouermi,Mengjiao Han,Chris R. Johnson*

Main category: cs.LG

TL;DR: 본 논문은 신경망을 이용한 입자 궤적 예측을 위한 새로운 불확실성 시각화 방법인 불확실성 튜브를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 신경망(NN) 모델의 예측 불확실성을 이해하기 위해 효과적으로 정량화하고 시각화하는 것이 중요하며, 신뢰성이 중요한 응용 분야에서 NN 모델의 신뢰성을 향상시킬 필요가 있음.

Method: 슈퍼타원형 튜브를 설계 및 구현하여 비대칭적 불확실성을 정확하게 포착하고 직관적으로 전달하며, Deep Ensembles, Monte Carlo Dropout 및 Stochastic Weight Averaging-Gaussian과 같은 이미 확립된 불확실성 정량화 기술을 통합.

Result: 불확실성 튜브의 실용성을 입증하고 합성 및 시뮬레이션 데이터 세트에서의 적용 사례를 보여줌.

Conclusion: 우리의 결과는 NN으로 유도된 입자 경로에서의 불확실성을 효과적으로 시각화하는 방법을 제공하며, 이는 과학 및 공학 분야에서의 신뢰성을 높이는 데 기여할 수 있음.

Abstract: Predicting particle trajectories with neural networks (NNs) has substantially
enhanced many scientific and engineering domains. However, effectively
quantifying and visualizing the inherent uncertainty in predictions remains
challenging. Without an understanding of the uncertainty, the reliability of NN
models in applications where trustworthiness is paramount is significantly
compromised. This paper introduces the uncertainty tube, a novel,
computationally efficient visualization method designed to represent this
uncertainty in NN-derived particle paths. Our key innovation is the design and
implementation of a superelliptical tube that accurately captures and
intuitively conveys nonsymmetric uncertainty. By integrating well-established
uncertainty quantification techniques, such as Deep Ensembles, Monte Carlo
Dropout (MC Dropout), and Stochastic Weight Averaging-Gaussian (SWAG), we
demonstrate the practical utility of the uncertainty tube, showcasing its
application on both synthetic and simulation datasets.

</details>


### [73] [Explainability of Algorithms](https://arxiv.org/abs/2508.13529)
*Andrés Páez*

Main category: cs.LG

TL;DR: 복잡한 머신러닝 알고리즘의 불투명성은 인공지능(AI)의 윤리적 발전을 저해하는 주요 장애물 중 하나로 언급된다. 이 논문에서는 불투명의 두 가지 이해 방식을 살펴보고, AI 시스템의 기술적 불투명성을 극복하기 위해 개발된 다양한 설명 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: AI 알고리즘의 불투명성은 그 발전에 있어 윤리적 문제를 제기한다.

Method: 불투명의 두 가지 이해 방식을 논의하고, AI의 기술적 불투명성을 극복하기 위한 설명 가능한 AI(XAI)에 대한 연구를 분석한다.

Result: 설명 가능한 AI(XAI)는 여전히 여러 도전에 직면해 있다.

Conclusion: 복잡성 때문만이 아니라, 특정 알고리즘은 상업적 이유로 불투명할 수 있으며, 이를 해결하기 위한 설명 방법이 개발되고 있지만 여전히 난제가 존재한다.

Abstract: The opaqueness of many complex machine learning algorithms is often mentioned
as one of the main obstacles to the ethical development of artificial
intelligence (AI). But what does it mean for an algorithm to be opaque? Highly
complex algorithms such as artificial neural networks process enormous volumes
of data in parallel along multiple hidden layers of interconnected nodes,
rendering their inner workings epistemically inaccessible to any human being,
including their designers and developers; they are "black boxes" for all their
stakeholders. But opaqueness is not always the inevitable result of technical
complexity. Sometimes, the way an algorithm works is intentionally hidden from
view for proprietary reasons, especially in commercial automated decision
systems, creating an entirely different type of opaqueness. In the first part
of the chapter, we will examine these two ways of understanding opacity and the
ethical implications that stem from each of them. In the second part, we
explore the different explanatory methods that have been developed in computer
science to overcome an AI system's technical opaqueness. As the analysis shows,
explainable AI (XAI) still faces numerous challenges.

</details>


### [74] [MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination](https://arxiv.org/abs/2508.13532)
*Ziyan Wu,Ivan Korolija,Rui Tang*

Main category: cs.LG

TL;DR: MuFlex는 다중 건물 유연성 조정을 위한 벤치마킹과 테스트를 위한 개방형 플랫폼으로, 에너지 모델과 동기화된 정보 교환을 통해 RL 구현을 표준화합니다.


<details>
  <summary>Details</summary>
Motivation: 재생 가능 발전의 확산이 증가함에 따라, 시스템 균형 유지를 위해서는 건물 집합체의 조정된 수요 유연성이 필요합니다.

Method: MuFlex 플랫폼은 EnergyPlus 건물 모델 간 동기화된 정보 교환을 가능하게 하며, 최신 OpenAI Gym 인터페이스를 준수하여 모듈화된 RL 구현을 제공합니다.

Result: 사례 연구를 통해 네 개의 사무실 건물 간 수요 유연성을 조정하는 데 Soft Actor-Critic 알고리즘을 적용하여 성과를 입증하였습니다.

Conclusion: 네 개의 건물에서 수요 유연성을 집계한 결과, 실내 환경 품질을 유지하면서 전체 피크 수요를 특정 임계값 이하로 감소시켰습니다.

Abstract: With the increasing penetration of renewable generation on the power grid,
maintaining system balance requires coordinated demand flexibility from
aggregations of buildings. Reinforcement learning (RL) has been widely explored
for building controls because of its model-free nature. Open-source simulation
testbeds are essential not only for training RL agents but also for fairly
benchmarking control strategies. However, most building-sector testbeds target
single buildings; multi-building platforms are relatively limited and typically
rely on simplified models (e.g., Resistance-Capacitance) or data-driven
approaches, which lack the ability to fully capture the physical intricacies
and intermediate variables necessary for interpreting control performance.
Moreover, these platforms often impose fixed inputs, outputs, and model
formats, restricting their applicability as benchmarking tools across diverse
control scenarios. To address these gaps, MuFlex, a scalable, open-source
platform for benchmarking and testing control strategies for multi-building
flexibility coordination, was developed in this study. MuFlex enables
synchronous information exchange across EnergyPlus building models and adheres
to the latest OpenAI Gym interface, providing a modular, standardized RL
implementation. The platform capabilities were demonstrated in a case study
coordinating demand flexibility across four office buildings using the Soft
Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results
show that aggregating the four buildings flexibility reduced total peak demand
below a specified threshold while maintaining indoor environmental quality.

</details>


### [75] [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
*Rituparna Datta,Jiaming Cui,Gregory R. Madden,Anil Vullikanti*

Main category: cs.LG

TL;DR: CALYPSO라는 하이브리드 프레임워크를 통해 MRSA 확산을 예측하고 분석하는 방법론을 제안하며, 기존 모델보다 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: MRSA의 위험을 이해하고, 개입 평가 및 발병률 예측은 공공 보건에서 중요한 문제입니다.

Method: 신경망과 기계적 메타 집단 모델을 통합하여 MRSA의 확산 역학을 포착하는 CALYPSO 수단을 제시합니다.

Result: CALYPSO는 기계 학습 기준선에 비해 주 statewide 예측 성능을 4.5% 이상 개선하고, 고위험 지역 및 비용 효율적인 감염 예방 자원 할당 전략을 식별합니다.

Conclusion: CALYPSO는 공공 보건 정책의 감염 통제 및 발병 위험 분석에 대한 유용성을 제공합니다.

Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.

</details>


### [76] [Collapsing ROC approach for risk prediction research on both common and rare variants](https://arxiv.org/abs/2508.13552)
*Changshuai Wei,Qing Lu*

Main category: cs.LG

TL;DR: 이 논문은 흔한 유전 변이와 희귀 유전 변이를 모두 고려한 리스크 예측을 위한 새로운 접근 방식인 CROC를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 유전적 발견을 기반으로 한 리스크 예측은 공공 건강과 임상 치료 개선에 큰 가능성을 지니고 있다.

Method: 이 연구에서는 흔한 유전 변이와 희귀 유전 변이에 대한 리스크 예측 연구를 위해 CROC 접근 방식을 제안하며, 이는 기존의 FROC 접근 방식의 확장이다.

Result: CROC 방법을 사용한 모델이 모든 SNP를 기반으로 할 때 AUC 0.605로, 흔한 변이만을 사용한 모델의 AUC 0.585보다 더 높은 정확도를 보였다.

Conclusion: CROC 방법은 흔한 변이가 적을 때 FROC 방법보다 더 높은 정확도를 달성하며, 희귀 변이에만 있는 경우에도 유망한 AUC 값을 기록하였다.

Abstract: Risk prediction that capitalizes on emerging genetic findings holds great
promise for improving public health and clinical care. However, recent risk
prediction research has shown that predictive tests formed on existing common
genetic loci, including those from genome-wide association studies, have lacked
sufficient accuracy for clinical use. Because most rare variants on the genome
have not yet been studied for their role in risk prediction, future disease
prediction discoveries should shift toward a more comprehensive risk prediction
strategy that takes into account both common and rare variants. We are
proposing a collapsing receiver operating characteristic CROC approach for risk
prediction research on both common and rare variants. The new approach is an
extension of a previously developed forward ROC FROC approach, with additional
procedures for handling rare variants. The approach was evaluated through the
use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the
Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction
model built on all SNPs gained more accuracy AUC = 0.605 than one built on
common variants alone AUC = 0.585. We further evaluated the performance of two
approaches by gradually reducing the number of common variants in the analysis.
We found that the CROC method attained more accuracy than the FROC method when
the number of common variants in the data decreased. In an extreme scenario,
when there are only rare variants in the data, the CROC reached an AUC value of
0.603, whereas the FROC had an AUC value of 0.524.

</details>


### [77] [Prediction of Hospital Associated Infections During Continuous Hospital Stays](https://arxiv.org/abs/2508.13561)
*Rituparna Datta,Methun Kamruzzaman,Eili Y. Klein,Gregory R Madden,Xinwei Deng,Anil Vullikanti,Parantapa Bhattacharya*

Main category: cs.LG

TL;DR: MRSA는 병원에 입원한 환자들에게 심각한 항균 저항 위협이며, 본 논문에서는 환자의 MRSA 검사 결과를 모델링하기 위한 새로운 생성적 확률 모델인 GenHAI를 제안한다.


<details>
  <summary>Details</summary>
Motivation: MRSA의 위험이 특히 높아 연구가 필요하다.

Method: GenHAI라는 새로운 생성적 확률 모델을 제안하여 MRSA 검사 결과 시퀀스를 모델링한다.

Result: 생성적 및 판별적 기계 학습 모델과 비교하여 GenHAI의 효능을 입증하였다.

Conclusion: GenHAI는 MRSA 감염 위험 완화를 위한 중요한 질문에 답할 수 있는 도구로 유용하다.

Abstract: The US Centers for Disease Control and Prevention (CDC), in 2019, designated
Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial
resistance threat. The risk of acquiring MRSA and suffering life-threatening
consequences due to it remains especially high for hospitalized patients due to
a unique combination of factors, including: co-morbid conditions, immuno
suppression, antibiotic use, and risk of contact with contaminated hospital
workers and equipment. In this paper, we present a novel generative
probabilistic model, GenHAI, for modeling sequences of MRSA test results
outcomes for patients during a single hospitalization. This model can be used
to answer many important questions from the perspectives of hospital
administrators for mitigating the risk of MRSA infections. Our model is based
on the probabilistic programming paradigm, and can be used to approximately
answer a variety of predictive, causal, and counterfactual questions. We
demonstrate the efficacy of our model by comparing it against discriminative
and generative machine learning models using two real-world datasets.

</details>


### [78] [A Generalized Learning Framework for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2508.13596)
*Lingyu Si,Jingyao Wang,Wenwen Qiang*

Main category: cs.LG

TL;DR: 이 논문에서는 자기 감독 대조 학습(SSCL)을 일반화된 학습 프레임워크(GLF)로 확장하고, GLF의 제약 부분 설계를 위한 두 가지 통찰을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: SSCL 방법이 여러 다운스트림 작업에서 우수성을 보이며, 제약 부분 설계를 통해 성능향상을 추구합니다.

Method: 기존의 SSCL 방법인 BYOL, Barlow Twins, SwAV를 분석하고 GLF로 통합하며, 서브 공간의 밀집성과 분리성을 측정하는 방법을 제안합니다.

Result: ADC(Adaptive Distribution Calibration) 방법을 통해 이론적 분석과 경험적 평가를 통해 우수성을 입증하였습니다.

Conclusion: 제안된 방법이 SSCL의 제약 설계 문제를 해결하고, 분류 정보를 잘 보존하도록 합니다.

Abstract: Self-supervised contrastive learning (SSCL) has recently demonstrated
superiority in multiple downstream tasks. In this paper, we generalize the
standard SSCL methods to a Generalized Learning Framework (GLF) consisting of
two parts: the aligning part and the constraining part. We analyze three
existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be
unified under GLF with different choices of the constraining part. We further
propose empirical and theoretical analyses providing two insights into
designing the constraining part of GLF: intra-class compactness and inter-class
separability, which measure how well the feature space preserves the class
information of the inputs. However, since SSCL can not use labels, it is
challenging to design a constraining part that satisfies these properties. To
address this issue, we consider inducing intra-class compactness and
inter-class separability by iteratively capturing the dynamic relationship
between anchor and other samples and propose a plug-and-play method called
Adaptive Distribution Calibration (ADC) to ensure that samples that are near or
far from the anchor point in the original input space are closer or further
away from the anchor point in the feature space. Both the theoretical analysis
and the empirical evaluation demonstrate the superiority of ADC.

</details>


### [79] [Approximate Bayesian Inference via Bitstring Representations](https://arxiv.org/abs/2508.13598)
*Aleksanteri Sladek,Martin Trapp,Arno Solin*

Main category: cs.LG

TL;DR: 이 논문은 양자화된 매개변수 공간에서 확률적 추론을 수행하는 방법을 제안하며, 이로써 불연속 매개변수를 사용하여 연속 분포를 학습할 수 있도록 합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 모델을 확장하기 위해 양자화된 저정밀 산술에 대한 노력이 필요합니다.

Method: 확률 회로를 사용하여 처리 가능한 학습 접근 방식을 도입합니다.

Result: 다양한 모델로 검증하여 정확성을 희생하지 않고도 추론 효율성을 입증합니다.

Conclusion: 이 연구는 확률적 계산을 위한 분산 근사치를 활용하여 확장 가능하고 해석 가능한 기계 학습을 발전시킵니다.

Abstract: The machine learning community has recently put effort into quantized or
low-precision arithmetics to scale large models. This paper proposes performing
probabilistic inference in the quantized, discrete parameter space created by
these representations, effectively enabling us to learn a continuous
distribution using discrete parameters. We consider both 2D densities and
quantized neural networks, where we introduce a tractable learning approach
using probabilistic circuits. This method offers a scalable solution to manage
complex distributions and provides clear insights into model behavior. We
validate our approach with various models, demonstrating inference efficiency
without sacrificing accuracy. This work advances scalable, interpretable
machine learning by utilizing discrete approximations for probabilistic
computations.

</details>


### [80] [Bounding Causal Effects and Counterfactuals](https://arxiv.org/abs/2508.13607)
*Tobias Maringgele*

Main category: cs.LG

TL;DR: 본 논문은 인과 추론에서의 약한 가정 대신 구속을 제공하는 다양한 경계 알고리즘을 비교하고 이를 바탕으로 실질적인 가이드를 제공하는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 인과 추론은 실제에서 드물게 충족되는 강한 가정에 의존하는 경향이 있다. 따라서 실천에서의 활용이 부족한 부분을 해결하고자 한다.

Method: 여러 인과 시나리오에서 다양한 경계 알고리즘을 체계적으로 비교하고, 최신 기법을 구현, 확장 및 통합하여 공통 평가 프레임워크 내에서 평가한다.

Result: 수천 건의 무작위 시뮬레이션을 통해 각 방법의 경계의 타이트함, 계산 효율성, 가정 위반에 대한 강건성을 평가하였다.

Conclusion: 실무자를 지원하기 위해 알고리즘 선택을 위한 실용적인 의사 결정 트리를 정리하고, 관찰 가능한 데이터 특성을 기반으로 최적의 방법을 예측하는 기계 학습 모델을 훈련했다.

Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured
confounding or perfect compliance - that are rarely satisfied in practice.
Partial identification offers a principled alternative: instead of relying on
unverifiable assumptions to estimate causal effects precisely, it derives
bounds that reflect the uncertainty inherent in the data. Despite its
theoretical appeal, partial identification remains underutilized in applied
work, in part due to the fragmented nature of existing methods and the lack of
practical guidance. This thesis addresses these challenges by systematically
comparing a diverse set of bounding algorithms across multiple causal
scenarios. We implement, extend, and unify state-of-the-art methods - including
symbolic, optimization-based, and information-theoretic approaches - within a
common evaluation framework. In particular, we propose an extension of a
recently introduced entropy-bounded method, making it applicable to
counterfactual queries such as the Probability of Necessity and Sufficiency
(PNS). Our empirical study spans thousands of randomized simulations involving
both discrete and continuous data-generating processes. We assess each method
in terms of bound tightness, computational efficiency, and robustness to
assumption violations. To support practitioners, we distill our findings into a
practical decision tree for algorithm selection and train a machine learning
model to predict the best-performing method based on observable data
characteristics.
  All implementations are released as part of an open-source Python package,
CausalBoundingEngine, which enables users to apply and compare bounding methods
through a unified interface.

</details>


### [81] [Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models](https://arxiv.org/abs/2508.13625)
*Wenxuan Ye,Xueli An,Onur Ayan,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.LG

TL;DR: FedOL은 비공식 데이터셋에서 예측 결과를 교환하여 기밀성 있는 연합 학습을 개선하고, 통신 비용을 줄이며, 다양한 모델 구조를 지원하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 클라이언트가 제한된 계산 자원으로 기밀 정보를 보호하면서도 더욱 강력한 모델을 파악할 수 있는 방법을 찾기 위함입니다.

Method: FedOL은 클라이언트가 라벨이 없는 공개 데이터셋에서 모델 예측 출력을 교환하여 지식 증류를 활용하며, 하나의 통신 라운드에서 서버 모델을 구성합니다.

Result: 시뮬레이션 결과, FedOL은 기존 대비 획기적으로 성능이 향상되어 모바일 네트워크에 효율적인 솔루션을 제공합니다.

Conclusion: FedOL은 자원의 비대칭성과 통신 오버헤드를 줄여 기밀성을 보장하면서도 더 나은 성능을 제공하는 연합 학습 기법입니다.

Abstract: Large models, renowned for superior performance, outperform smaller ones even
without billion-parameter scales. While mobile network servers have ample
computational resources to support larger models than client devices, privacy
constraints prevent clients from directly sharing their raw data. Federated
Learning (FL) enables decentralized clients to collaboratively train a shared
model by exchanging model parameters instead of transmitting raw data. Yet, it
requires a uniform model architecture and multiple communication rounds, which
neglect resource heterogeneity, impose heavy computational demands on clients,
and increase communication overhead. To address these challenges, we propose
FedOL, to construct a larger and more comprehensive server model in one-shot
settings (i.e., in a single communication round). Instead of model parameter
sharing, FedOL employs knowledge distillation, where clients only exchange
model prediction outputs on an unlabeled public dataset. This reduces
communication overhead by transmitting compact predictions instead of full
model weights and enables model customization by allowing heterogeneous model
architectures. A key challenge in this setting is that client predictions may
be biased due to skewed local data distributions, and the lack of ground-truth
labels in the public dataset further complicates reliable learning. To mitigate
these issues, FedOL introduces a specialized objective function that
iteratively refines pseudo-labels and the server model, improving learning
reliability. To complement this, FedOL incorporates a tailored pseudo-label
generation and knowledge distillation strategy that effectively integrates
diverse knowledge. Simulation results show that FedOL significantly outperforms
existing baselines, offering a cost-effective solution for mobile networks
where clients possess valuable private data but limited computational
resources.

</details>


### [82] [Text2Weight: Bridging Natural Language and Neural Network Weight Spaces](https://arxiv.org/abs/2508.13633)
*Bowen Tian,Wenshuo Chen,Zexi Li,Songning Lai,Jiemin Wu,Yutao Yue*

Main category: cs.LG

TL;DR: 이 논문은 자연어 설명에 조건화된 작업 특화 가중치를 생성하는 확산 변환기 프레임워크인 T2W를 제안하며, 이는 신경망 매개변수의 일반화 및 새로운 작업에 대한 적용 가능성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 자동으로 신경망을 생성하는 기술의 발전 필요성

Method: T2W는 자연어 설명에 조건화된 작업 특화 가중치를 생성하며, 네트워크 매개변수를 균일한 블록으로 계층적으로 처리하고, CLIP의 텍스트 임베딩을 통합하며, 적대적 학습과 가중치 공간 증강을 통해 일반화를 향상시킨다.

Result: Cifar100, Caltech256, TinyImageNet에서 실험을 통해 T2W가 새로운 작업을 위한 고품질 가중치를 생성할 수 있음을 입증했다.

Conclusion: 우리의 작업은 텍스트 의미론과 가중치 공간 역학을 연결하며, 신경망 매개변수 합성을 위한 생성 모델의 실용성을 발전시킨다.

Abstract: How far are we really from automatically generating neural networks? While
neural network weight generation shows promise, current approaches struggle
with generalization to unseen tasks and practical application exploration. To
address this, we propose T2W, a diffusion transformer framework that generates
task-specific weights conditioned on natural language descriptions. T2W
hierarchically processes network parameters into uniform blocks, integrates
text embeddings from CLIP via a prior attention mechanism, and employs
adversarial training with weight-space augmentation to enhance generalization.
Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability
to produce high-quality weights for unseen tasks, outperforming
optimization-based initialization and enabling novel applications such as
weight enhancement and text-guided model fusion. Our work bridges textual
semantics with weight-space dynamics, supported by an open-source dataset of
text-weight pairs, advancing the practicality of generative models in neural
network parameter synthesis. Our code is available on Github.

</details>


### [83] [Explainable Learning Rate Regimes for Stochastic Optimization](https://arxiv.org/abs/2508.13639)
*Zhuang Yang*

Main category: cs.LG

TL;DR: 본 연구는 자동으로 학습률(LR)을 업데이트하는 방법을 제안하며, 이는 확률적 경량의 변화에만 의존한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LR 조정 방식은 복잡하거나 추가적인 하이퍼파라미터 조정이 필요하여, 실용에서 막대한 계산 비용과 시간을 요구한다.

Method: 확률적 2차 알고리즘을 활용하여 자동적으로 LR을 조정하는 방법을 개발하였고, 학습률은 확률적 경량의 노름이 감소(증가)함에 따라 증가(감소)하는 방식으로 구현되었다.

Result: 이 결과로 도출된 LR 조정 방식은 다양한 고전적인 확률적 알고리즘(SGD, SGDM, SIGNSGD)에서 효율성과 강건성, 확장성을 보여주었다.

Conclusion: 이 연구는 하이퍼파라미터 조정 없이 자동적으로 LR을 업데이트하는 새로운 접근 방식의 가능성을 제시한다.

Abstract: Modern machine learning is trained by stochastic gradient descent (SGD),
whose performance critically depends on how the learning rate (LR) is adjusted
and decreased over time. Yet existing LR regimes may be intricate, or need to
tune one or more additional hyper-parameters manually whose bottlenecks include
huge computational expenditure, time and power in practice. This work, in a
natural and direct manner, clarifies how LR should be updated automatically
only according to the intrinsic variation of stochastic gradients. An
explainable LR regime by leveraging stochastic second-order algorithms is
developed, behaving a similar pattern to heuristic algorithms but implemented
simply without any parameter tuning requirement, where it is of an automatic
procedure that LR should increase (decrease) as the norm of stochastic
gradients decreases (increases). The resulting LR regime shows its efficiency,
robustness, and scalability in different classical stochastic algorithms,
containing SGD, SGDM, and SIGNSGD, on machine learning tasks.

</details>


### [84] [Personalized Subgraph Federated Learning with Sheaf Collaboration](https://arxiv.org/abs/2508.13642)
*Wenfei Liang,Yanan Zhao,Rui She,Yiming Li,Wee Peng Tay*

Main category: cs.LG

TL;DR: FedSheafHN은 고객 맞춤형 모델 생성을 위한 새로운 서브그래프 연합 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 그래프 구조 데이터는 다양한 응용 프로그램에서 흔히 사용되며, 다양한 데이터 분포를 처리하기 위해 개인화된 그래프 모델이 필요하다.

Method: FedSheafHN은 클라이언트의 로컬 서브그래프를 서버가 구축한 협력 그래프에 임베드하고, 협력 그래프 내에서 시프 확산을 사용하여 클라이언트 표현을 풍부하게 한다.

Result: FedSheafHN은 다양한 그래프 데이터 세트에서 기존 개인화된 서브그래프 FL 방법보다 우수한 성능을 보였다.

Conclusion: 또한, 빠른 모델 수렴을 보이며, 새로운 클라이언트에 효과적으로 일반화된다.

Abstract: Graph-structured data is prevalent in many applications. In subgraph
federated learning (FL), this data is distributed across clients, each with a
local subgraph. Personalized subgraph FL aims to develop a customized model for
each client to handle diverse data distributions. However, performance
variation across clients remains a key issue due to the heterogeneity of local
subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework
built on a sheaf collaboration mechanism to unify enhanced client descriptors
with efficient personalized model generation. Specifically, FedSheafHN embeds
each client's local subgraph into a server-constructed collaboration graph by
leveraging graph-level embeddings and employing sheaf diffusion within the
collaboration graph to enrich client representations. Subsequently, FedSheafHN
generates customized client models via a server-optimized hypernetwork.
Empirical evaluations demonstrate that FedSheafHN outperforms existing
personalized subgraph FL methods on various graph datasets. Additionally, it
exhibits fast model convergence and effectively generalizes to new clients.

</details>


### [85] [GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling](https://arxiv.org/abs/2508.13653)
*Ashish Jha,Anh huy Phan,Razan Dibo,Valentin Leplat*

Main category: cs.LG

TL;DR: GRAFT는 추가된 서브셋 선택 방법으로, 학습 중에 효율성과 환경 영향을 줄이면서도 정확도를 유지한다.


<details>
  <summary>Details</summary>
Motivation: 현대 신경망을 큰 데이터셋에서 훈련하는 것은 계산적으로나 환경적으로 비용이 많이 든다.

Method: GRAFT는 (i) 각 배치에 대한 저차원 특징 표현을 추출하고, (ii) Fast MaxVol 샘플러를 적용하여 배치의 지배적인 서브스페이스를 포함하는 작고 다양한 서브셋을 선택하며, (iii) 그래디언트 근사 기준을 사용하여 서브셋 크기를 동적으로 조정한다.

Result: GRAFT는 여러 벤치마크에서 정확도와 효율성 모두에서 최근의 선택 기준과 동등하거나 이를 초과한다.

Conclusion: GRAFT는 정확도, 효율성 및 배출량 간의 유리한 균형을 제공한다.

Abstract: Training modern neural networks on large datasets is computationally and
environmentally costly. We introduce GRAFT, a scalable in-training subset
selection method that (i) extracts a low-rank feature representation for each
batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset
that spans the batch's dominant subspace, and (iii) dynamically adjusts the
subset size using a gradient-approximation criterion. By operating in low-rank
subspaces and training on carefully chosen examples instead of full batches,
GRAFT preserves the training trajectory while reducing wall-clock time, energy
consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT
matches or exceeds recent selection baselines in both accuracy and efficiency,
providing a favorable trade-off between accuracy, efficiency, and emissions.

</details>


### [86] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: 본 연구에서는 기존의 대규모 언어 모델(LLM)의 확장 방법에 보완적인 "입력 시간 확장" 패러다임을 제안합니다. 이 방법은 트레이닝과 테스트에서 쿼리 전략을 적용하는 방식으로 성능을 향상시킵니다. 또한 낮은 품질의 데이터셋에서도 높은 성능을 낼 수 있다는 것을 발견했습니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)의 성능 향상을 위한 새로운 확장 방법을 제시하고자 하였습니다.

Method: 입력 시간 확장을 통해 다양한 쿼리 전략을 결합하고, 훈련과 테스트 동안 메타 지식을 이용하여 입력을 다듬습니다.

Result: 우리의 방법은 32B 모델 중 AIME24(76.7%) 및 AIME25(76.7%)의 최신 성능을 달성했습니다.

Conclusion: 우리는 개방형 소스 데이터셋과 파이프라인, 평가 결과, 체크포인트를 통해 재현성 및 추가 연구를 촉진할 예정입니다.

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


### [87] [In-Context Decision Making for Optimizing Complex AutoML Pipelines](https://arxiv.org/abs/2508.13657)
*Amir Rezaei Balef,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 이 연구는 현대 기계 학습 파이프라인을 선택하고 조정하기 위해 CASH 프레임워크를 확장하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 AutoML 시스템에서 CASH가 기초적 역할을 해왔으나, 최신 ML 워크플로우는 하이퍼파라미터 최적화 이상의 기술을 요구하므로 새로운 AutoML 접근 방식이 필요합니다.

Method: PS-PFN을 제안하여 현대 ML 파이프라인을 적응시키고 탐색할 수 있도록 Posterior Sampling을 max k-armed 밴딧 문제 설정으로 확장합니다.

Result: PS-PFN은 사전 데이터 적합 네트워크를 활용하여 최대 값의 후방 분포를 효율적으로 추정하며, 실험 결과 PS-PFN이 기존 밴딧 및 AutoML 전략에 비해 우수한 성능을 보여줍니다.

Conclusion: PS-PFN은 다양한 팔을 당기는 비용을 고려하고 각 팔에 대해 보상 분포를 개별적으로 모델링하는 방법으로 확장할 수 있습니다.

Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been
fundamental to traditional AutoML systems. However, with the advancements of
pre-trained models, modern ML workflows go beyond hyperparameter optimization
and often require fine-tuning, ensembling, and other adaptation techniques.
While the core challenge of identifying the best-performing model for a
downstream task remains, the increasing heterogeneity of ML pipelines demands
novel AutoML approaches. This work extends the CASH framework to select and
adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit
adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed
bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to
efficiently estimate the posterior distribution of the maximal value via
in-context learning. We show how to extend this method to consider varying
costs of pulling arms and to use different PFNs to model reward distributions
individually per arm. Experimental results on one novel and two existing
standard benchmark tasks demonstrate the superior performance of PS-PFN
compared to other bandit and AutoML strategies. We make our code and data
available at https://github.com/amirbalef/CASHPlus.

</details>


### [88] [Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond](https://arxiv.org/abs/2508.13679)
*Canzhe Zhao,Shinji Ito,Shuai Li*

Main category: cs.LG

TL;DR: 본 논문에서는 적대적 환경에서의 헤비 테일 멀티 암 밴딧 문제를 위한 FTRL 타입 알고리즘을 제안하고, 이를 통해 스토캐스틱 환경에서도 효과적인 성과를 달성함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 헤비 테일 밴딧 문제에 대한 연구가 활발히 진행되고 있으나, 이전 연구들은 주로 확률적 환경에 집중되어 있었다. 이 연구는 적대적 환경에서의 헤비 테일 밴딧 문제를 해결하기 위한 새로운 접근 방식을 제안한다.

Method: 손실 추정치에 보너스 함수로 이동한 추정치를 활용하여 일반적인 적대적 헤비 테일 밴딧 문제를 위한 FTRL 알고리즘을 개발했다.

Result: 헤비 테일 멀티 암 밴딧에 대해 $	ilde{O}(T^{rac{1}{ho}})$ 최악의 경우 후회와 $	ilde{O}(	ext{log} T)$ 차이 의존적 후회를 달성하며, 선형 경우에도 유사한 성과를 거두었다.

Conclusion: 제안된 알고리즘은 스토캐스틱 환경에서의 최악의 후회 경계와 일치하며, 일반적인 헤비 테일 밴딧 문제에 대해서도 BOBW 후회 경계를 보장하는 안정성 페널티 매칭 학습률을 제안하였다.

Abstract: Heavy-tailed bandits have been extensively studied since the seminal work of
\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,
enabling efficient learning with both a large number of arms and heavy-tailed
noises, have recently attracted significant attention
\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.
However, prior studies focus almost exclusively on stochastic regimes, with few
exceptions limited to the special case of heavy-tailed multi-armed bandits
(MABs) \citep{Huang0H22,ChengZ024,Chen2024uniINF}.
  In this work, we propose a general framework for adversarial heavy-tailed
bandit problems, which performs follow-the-regularized-leader (FTRL) over the
loss estimates shifted by a bonus function. Via a delicate setup of the bonus
function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm
for heavy-tailed MABs, which does not require the truncated non-negativity
assumption and achieves an $\widetilde{O}(T^{\frac{1}{\varepsilon}})$
worst-case regret in the adversarial regime as well as an $\widetilde{O}(\log
T)$ gap-dependent regret in the stochastic regime. We then extend our framework
to the linear case, proposing the first algorithm for adversarial heavy-tailed
linear bandits with finite arm sets. This algorithm achieves an
$\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$ regret, matching the
best-known worst-case regret bound in stochastic regimes. Moreover, we propose
a general data-dependent learning rate, termed \textit{heavy-tailed noise aware
stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW
regret bounds for general heavy-tailed bandit problems once certain conditions
are satisfied. By using HT-SPM and, in particular, a variance-reduced linear
loss estimator, we obtain the first BOBW result for heavy-tailed linear
bandits.

</details>


### [89] [Minimizing the Weighted Number of Tardy Jobs: Data-Driven Heuristic for Single-Machine Scheduling](https://arxiv.org/abs/2508.13703)
*Nikolai Antonov,Prěmysl Šůcha,Mikoláš Janota,Jan Hůla*

Main category: cs.LG

TL;DR: 단일 기계 스케줄링 문제에 대한 새로운 데이터 기반 휴리스틱 방법을 제안하며, 이는 기계 학습을 문제의 특성과 결합하여 최적 솔루션의 수와 적응성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존의 단일 기계 스케줄링 연구는 일반적으로 정확한 알고리즘에 집중되어 있으며, 이는 전형적인 경우에서는 잘 작동하지만 문제 공간의 특정 영역에서는 성능이 크게 저하될 수 있다.

Method: 기계 학습과 문제 특성의 결합을 통한 새로운 데이터 기반 스케줄링 휴리스틱을 도입하여, 구체적인 데이터 세트 구조에 맞춘 강력하고 확장 가능한 성능을 제공한다.

Result: 실험 결과, 제안한 방법이 최적성 갭, 최적 솔루션의 수, 다양한 데이터 시나리오에 대한 적응성 측면에서 최신 기술을 크게 초월함을 보인다.

Conclusion: 제안된 접근 방식은 실제 응용에 대한 유연성을 강조하며, 기계 학습 모델에 대한 체계적인 탐색을 수행하여 모델 선택 과정에 대한 통찰력을 제공한다.

Abstract: Existing research on single-machine scheduling is largely focused on exact
algorithms, which perform well on typical instances but can significantly
deteriorate on certain regions of the problem space. In contrast, data-driven
approaches provide strong and scalable performance when tailored to the
structure of specific datasets. Leveraging this idea, we focus on a
single-machine scheduling problem where each job is defined by its weight,
duration, due date, and deadline, aiming to minimize the total weight of tardy
jobs. We introduce a novel data-driven scheduling heuristic that combines
machine learning with problem-specific characteristics, ensuring feasible
solutions, which is a common challenge for ML-based algorithms. Experimental
results demonstrate that our approach significantly outperforms the
state-of-the-art in terms of optimality gap, number of optimal solutions, and
adaptability across varied data scenarios, highlighting its flexibility for
practical applications. In addition, we conduct a systematic exploration of ML
models, addressing a common gap in similar studies by offering a detailed model
selection process and providing insights into why the chosen model is the best
fit.

</details>


### [90] [Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment](https://arxiv.org/abs/2508.13715)
*Jie Shi,Arno P. J. M. Siebes,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: 이 논문은 공급망 신용 평가를 위해 연합 학습과 설명 가능한 AI 기술을 결합한 Trans-XFed 아키텍처를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 공급망 신용 평가에서 프라이버시, 정보 사일로, 클래스 불균형, 비동일 및 독립적으로 분포된 데이터, 그리고 모델 해석 가능성과 같은 주요 문제를 해결하고자 합니다.

Method: 성능 기반 클라이언트 선택 전략(PBCS)을 도입하여 클래스 불균형 및 비동일 및 독립적으로 분포된 데이터 문제를 해결하고, FedProx 아키텍처를 활용하여 동형 암호화를 강화하며, 트랜스포머 인코더를 추가하여 학습된 특징에 대한 통찰을 제공합니다.

Result: Trans-XFed의 효과는 실제 공급망 데이터 세트에 대한 실험 평가를 통해 입증되었습니다.

Conclusion: Trans-XFed는 여러 기준선과 비교하여 정확한 신용 평가를 제공하는 동시에 투명성과 프라이버시를 유지함을 보여줍니다.

Abstract: This paper proposes a Trans-XFed architecture that combines federated
learning with explainable AI techniques for supply chain credit assessment. The
proposed model aims to address several key challenges, including privacy,
information silos, class imbalance, non-identically and independently
distributed (Non-IID) data, and model interpretability in supply chain credit
assessment. We introduce a performance-based client selection strategy (PBCS)
to tackle class imbalance and Non-IID problems. This strategy achieves faster
convergence by selecting clients with higher local F1 scores. The FedProx
architecture, enhanced with homomorphic encryption, is used as the core model,
and further incorporates a transformer encoder. The transformer encoder block
provides insights into the learned features. Additionally, we employ the
integrated gradient explainable AI technique to offer insights into
decision-making. We demonstrate the effectiveness of Trans-XFed through
experimental evaluations on real-world supply chain datasets. The obtained
results show its ability to deliver accurate credit assessments compared to
several baselines, while maintaining transparency and privacy.

</details>


### [91] [DREAMS: Preserving both Local and Global Structure in Dimensionality Reduction](https://arxiv.org/abs/2508.13747)
*Noël Kury,Dmitry Kobak,Sebastian Damrich*

Main category: cs.LG

TL;DR: DREAMS는 데이터의 지역 및 글로벌 구조를 모두 잘 보존할 수 있는 차원 축소 기법이다.


<details>
  <summary>Details</summary>
Motivation: 고차원 데이터를 2차원으로 시각화하는데 사용되는 차원 축소 기법은 일반적으로 지역 또는 글로벌 구조를 보존하기 위해 설계되어 있으나, 두 가지를 모두 잘 표현할 수 있는 방법은 없다.

Method: DREAMS는 $t$-SNE의 지역 구조 보존과 PCA의 글로벌 구조 보존을 단순한 정규화 항을 통해 결합한다.

Result: DREAMS는 지역적으로 잘 구조화된 $t$-SNE 임베딩과 글로벌하게 잘 구조화된 PCA 임베딩 사이의 임베딩 스펙트럼을 생성하여 두 가지 구조 보존을 효율적으로 균형 잡는다.

Conclusion: REALM은 7개의 실제 데이터셋에서 테스트 되었으며, 이전 방법보다 다수의 스케일에서 구조 보존 능력이 우수함을 보여준다.

Abstract: Dimensionality reduction techniques are widely used for visualizing
high-dimensional data in two dimensions. Existing methods are typically
designed to preserve either local (e.g. $t$-SNE, UMAP) or global (e.g. MDS,
PCA) structure of the data, but none of the established methods can represent
both aspects well. In this paper, we present DREAMS (Dimensionality Reduction
Enhanced Across Multiple Scales), a method that combines the local structure
preservation of $t$-SNE with the global structure preservation of PCA via a
simple regularization term. Our approach generates a spectrum of embeddings
between the locally well-structured $t$-SNE embedding and the globally
well-structured PCA embedding, efficiently balancing both local and global
structure preservation. We benchmark DREAMS across seven real-world datasets,
including five from single-cell transcriptomics and one from population
genetics, showcasing qualitatively and quantitatively its superior ability to
preserve structure across multiple scales compared to previous approaches.

</details>


### [92] [Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting](https://arxiv.org/abs/2508.13749)
*Mohammad Taha Shah,Sabrina Khurshid,Gourab Ghatak*

Main category: cs.LG

TL;DR: 본 논문에서는 확률적 밴딧 환경에서 샤프 비율 극대화를 위한 순차적 의사결정 문제를 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 샤프 비율 최적화는 높은 수익과 위험 관리 간의 내재된 균형을 요구합니다.

Method: 우리는 셔프 비율에 대해 특별히 설계된 새로운 후회 분해를 포함하는 탐슨 샘플링 알고리즘을 사용합니다.

Result: 결과적으로 탐슨 샘플링은 시간이 지남에 따라 로그 후회를 달성하며, 우리의 알고리즘은 기존 알고리즘을 크게 능가하는 성능을 보입니다.

Conclusion: 본 연구는 샤프 비율 극대화를 달성하기 위한 알고리즘적 접근 방식을 제시합니다.

Abstract: In this paper, we investigate the problem of sequential decision-making for
Sharpe ratio (SR) maximization in a stochastic bandit setting. We focus on the
Thompson Sampling (TS) algorithm, a Bayesian approach celebrated for its
empirical performance and exploration efficiency, under the assumption of
Gaussian rewards with unknown parameters. Unlike conventional bandit objectives
focusing on maximizing cumulative reward, Sharpe ratio optimization instead
introduces an inherent tradeoff between achieving high returns and controlling
risk, demanding careful exploration of both mean and variance. Our theoretical
contributions include a novel regret decomposition specifically designed for
the Sharpe ratio, highlighting the role of information acquisition about the
reward distribution in driving learning efficiency. Then, we establish
fundamental performance limits for the proposed algorithm \texttt{SRTS} in
terms of an upper bound on regret. We also derive the matching lower bound and
show the order-optimality. Our results show that Thompson Sampling achieves
logarithmic regret over time, with distribution-dependent factors capturing the
difficulty of distinguishing arms based on risk-adjusted performance. Empirical
simulations show that our algorithm significantly outperforms existing
algorithms.

</details>


### [93] [Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration](https://arxiv.org/abs/2508.13755)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Dongchun Xie,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: RLVR은 대형 언어 모델의 추론 능력을 향상시키는 유망한 패러다임이지만, 깊이와 폭의 두 가지 차원에서 잠재력이 제약받고 있다. DARS라는 방법을 통해 어려운 문제를 재조정하고, 학습 데이터의 폭을 확장하면 추론 능력을 더욱 향상시킬 수 있음을 보인다.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning with Verifiable Reward (RLVR)의 완전한 잠재력은 모델이 샘플링할 수 있는 가장 어려운 문제의 깊이와 단일 반복에서 소비되는 인스턴스의 수인 폭에 의해 저해되고 있다.

Method: 어려운 문제를 목표로 하는 다단계 롤아웃을 통해 재조정하는 Difficulty Adaptive Rollout Sampling (DARS)을 도입하였다. 또한, 배치 크기를 급격하게 확대하고 PPO의 미니 배치 반복을 여러 에포크에 걸친 전체 배치 업데이트로 교체하여 폭을 증가시켰다.

Result: 배치 크기를 늘림으로써 Pass@1 성능이 유의미하게 향상되었으며, DARS 및 DARS-B를 통해 Pass@K와 Pass@1에서 동시성장이 확인되었다.

Conclusion: RLVR에서 깊이와 폭이 모두 독립적인 차원으로 작용하며, 이는 RLVR의 추론 능력을 발휘하는 데 핵심이다.

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a
powerful paradigm for unlocking reasoning capabilities in large language
models, yet its full potential is hindered by two under-explored dimensions:
Depth-the hardest problem a model can sample; Breadth-the number of instances
consumed in a single iteration. We dissect the popular GRPO algorithm and
reveal a systematic bias: the cumulative-advantage disproportionately weights
samples with medium accuracy, while down-weighting the low-accuracy instances
that are crucial for pushing reasoning boundaries. To rectify the depth
neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which
re-weights hard problems through targeted multi-stage rollouts, thereby
increasing the number of positive rollouts for hard problems. Empirically,
naively enlarging rollout size only accelerates convergence and even hurts
Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra
inference cost at convergence. Just as we adaptively expanded the depth of
exploration, we now ask whether aggressively scaling the breadth of training
data can further amplify reasoning gains. To this end, we intensely scale batch
size and replace PPO's mini-batch iterations with full-batch updates over
multiple epochs. Increasing breadth significantly enhances Pass@1 performance.
Large-breadth training sustains high token-level entropy, indicating continued
exploration and reduced gradient noise. We further present DARS-B, which
augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K
and Pass@1. The results confirm that breadth and adaptive exploration across
depth operate as orthogonal dimensions in RLVR, which are key to unleashing the
reasoning power of RLVR.

</details>


### [94] [PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting](https://arxiv.org/abs/2508.13773)
*Tian Sun,Yuqi Chen,Weiwei Sun*

Main category: cs.LG

TL;DR: PENGUIN이라는 새로운 시계열 예측 모델을 제안하며, 주기적인 패턴의 중요성을 강조하고, 상대적 주의 편향을 효과적으로 통합함으로써 기존 모델보다 뛰어난 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 시계열 예측에서 Transformer 기반 모델의 효과에 대한 논란을 해결하고, 주기적인 패턴 모델링의 중요성을 재조명하기 위해.

Method: 주기적 중첩 상대 주의 편향(Periodic-Nested Relative Attention Bias)을 도입하고, 특정 주기를 목표로 하는 그룹화된 주의 메커니즘을 설계하였다.

Result: PENGUIN은 다양한 벤치마크 실험에서 MLP 기반 및 Transformer 기반 모델보다 일관되게 우수한 성능을 보였다.

Conclusion: PENGUIN은 시계열 모델링에서 주기적 구조를 효과적으로 포착하는 방법을 제공하며, 다양한 주기를 동시에 처리할 수 있는 능력을 입증하였다.

Abstract: Long-term time series forecasting (LTSF) is a fundamental task with
wide-ranging applications. Although Transformer-based models have made
significant breakthroughs in forecasting, their effectiveness for time series
forecasting remains debatable. In this paper, we revisit the significance of
self-attention and propose a simple yet effective mechanism, Periodic-Nested
Group Attention, namely PENGUIN. Our approach highlights the importance of
explicitly modeling periodic patterns and incorporating relative attention bias
for effective time series modeling. To this end, we introduce a periodic-nested
relative attention bias that captures periodic structures directly. To handle
multiple coexisting periodicities (e.g., daily and weekly cycles), we design a
grouped attention mechanism, where each group targets a specific periodicity
using a multi-query attention mechanism. Extensive experiments across diverse
benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and
Transformer-based models.

</details>


### [95] [Communication-Efficient Federated Learning with Adaptive Number of Participants](https://arxiv.org/abs/2508.13803)
*Sergey Skorik,Vladislav Dorofeev,Gleb Molodtsov,Aram Avetisyan,Dmitry Bylinkin,Daniil Medyakov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 본 논문에서는 통신 효율성을 높이기 위해 훈련 라운드마다 최적의 클라이언트 수를 동적으로 결정하는 Intelligent Selection of Participants (ISP) 메커니즘을 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 모델의 급속한 확장은 여러 도메인에서 성능 향상을 가능하게 했으나, 통신 효율성 문제와 같은 여러 가지 도전을 야기했습니다.

Method: ISP는 훈련 라운드마다 클라이언트의 수를 적절히 조정하여 통신 효율성을 높이도록 설계된 적응형 메커니즘입니다.

Result: ISP의 효과를 여러 설정(비전 변환기, 실제 ECG 분류, 그래디언트 압축 훈련 등)에서 검증하였으며, 통신 비용이 최대 30% 절감되는 결과를 확인했습니다.

Conclusion: 한계를 넘는 클라이언트 수 선택은 연합 학습의 별개 작업임을 강조했습니다.

Abstract: Rapid scaling of deep learning models has enabled performance gains across
domains, yet it introduced several challenges. Federated Learning (FL) has
emerged as a promising framework to address these concerns by enabling
decentralized training. Nevertheless, communication efficiency remains a key
bottleneck in FL, particularly under heterogeneous and dynamic client
participation. Existing methods, such as FedAvg and FedProx, or other
approaches, including client selection strategies, attempt to mitigate
communication costs. However, the problem of choosing the number of clients in
a training round remains extremely underexplored. We introduce Intelligent
Selection of Participants (ISP), an adaptive mechanism that dynamically
determines the optimal number of clients per round to enhance communication
efficiency without compromising model accuracy. We validate the effectiveness
of ISP across diverse setups, including vision transformers, real-world ECG
classification, and training with gradient compression. Our results show
consistent communication savings of up to 30\% without losing the final
quality. Applying ISP to different real-world ECG classification setups
highlighted the selection of the number of clients as a separate task of
federated learning.

</details>


### [96] [Reinforcement Learning-based Adaptive Path Selection for Programmable Networks](https://arxiv.org/abs/2508.13806)
*José Eduardo Zerna Torres,Marios Avgeris,Chrysa Papagianni,Gergely Pongrácz,István Gódor,Paola Grosso*

Main category: cs.LG

TL;DR: 이 연구는 프로그래머블 네트워크에서 적응형 경로 선택을 위한 분산형 네트워크 내 강화 학습 프레임워크의 개념 증명 구현을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 프로그래머블 네트워크에서의 경로 선택 성능 향상 및 혼잡 상황에 대한 동적 적응을 목표로 함.

Method: 확률 학습 자동체(SLA)와 실시간 텔레메트리 데이터를结合하여 지역 데이터 기반의 포워딩 결정을 지원하는 시스템 구현.

Result: Mininet 기반 테스트 베드에서 P4 프로그래머블 BMv2 스위치를 사용하여 SLA 기반 메커니즘이 효과적인 경로 선택에 수렴하고 네트워크 조건 변화에 적응하는 것을 입증함.

Conclusion: 제안된 시스템은 라인 속도에서 동적으로 변화하는 네트워크 조건에 적응할 수 있는 능력을 보여줌.

Abstract: This work presents a proof-of-concept implementation of a distributed,
in-network reinforcement learning (IN-RL) framework for adaptive path selection
in programmable networks. By combining Stochastic Learning Automata (SLA) with
real-time telemetry data collected via In-Band Network Telemetry (INT), the
proposed system enables local, data-driven forwarding decisions that adapt
dynamically to congestion conditions. The system is evaluated on a
Mininet-based testbed using P4-programmable BMv2 switches, demonstrating how
our SLA-based mechanism converges to effective path selections and adapts to
shifting network conditions at line rate.

</details>


### [97] [Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias](https://arxiv.org/abs/2508.13813)
*Koffi Ismael Ouattara,Ioannis Krontiris,Theo Dimitrakos,Frank Kargl*

Main category: cs.LG

TL;DR: 이 논문은 AI 훈련 데이터셋의 신뢰성을 평가하기 위한 첫 번째 공식 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 훈련 데이터에 점점 더 의존함에 따라, 데이터셋의 신뢰성 평가가 중요해졌다.

Method: Subjective Logic 기반으로 신뢰성 제안과 불확실성을 정량화하는 접근법을 사용한다.

Result: 우리 방법은 클래스 불균형을 포착하며, 중앙 집중형 및 연합형 상황에서도 해석 가능하고 견고하다.

Conclusion: 이 프레임워크는 AI 훈련 데이터셋의 전반적인 신뢰성을 평가하는 강력한 도구로 자리잡을 것이다.

Abstract: As AI systems increasingly rely on training data, assessing dataset
trustworthiness has become critical, particularly for properties like fairness
or bias that emerge at the dataset level. Prior work has used Subjective Logic
to assess trustworthiness of individual data, but not to evaluate
trustworthiness properties that emerge only at the level of the dataset as a
whole. This paper introduces the first formal framework for assessing the
trustworthiness of AI training datasets, enabling uncertainty-aware evaluations
of global properties such as bias. Built on Subjective Logic, our approach
supports trust propositions and quantifies uncertainty in scenarios where
evidence is incomplete, distributed, and/or conflicting. We instantiate this
framework on the trustworthiness property of bias, and we experimentally
evaluate it based on a traffic sign recognition dataset. The results
demonstrate that our method captures class imbalance and remains interpretable
and robust in both centralized and federated contexts.

</details>


### [98] [Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression](https://arxiv.org/abs/2508.13829)
*Samuel Stocksieker,Denys pommeret,Arthur Charpentier*

Main category: cs.LG

TL;DR: 불균형 회귀 문제에 대한 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 불균형 분포 학습은 예측 모델링에서 흔하고 중요한 문제로, 표준 알고리즘의 성능을 저하시킵니다.

Method: 변분 오토인코더(VAE)를 사용하여 데이터 분포의 잠재 표현을 모델링하고 정의합니다.

Result: 경쟁 알고리즘과의 수치 비교를 통해 이 방법의 효율성을 평가합니다.

Conclusion: 이 방법은 불균형 회귀 문제의 학습을 개선하는 데 기여합니다.

Abstract: Imbalanced distribution learning is a common and significant challenge in
predictive modeling, often reducing the performance of standard algorithms.
Although various approaches address this issue, most are tailored to
classification problems, with a limited focus on regression. This paper
introduces a novel method to improve learning on tabular data within the
Imbalanced Regression (IR) framework, which is a critical problem. We propose
using Variational Autoencoders (VAEs) to model and define a latent
representation of data distributions. However, VAEs can be inefficient with
imbalanced data like other standard approaches. To address this, we develop an
innovative data generation method that combines a disentangled VAE with a
Smoothed Bootstrap applied in the latent space. We evaluate the efficiency of
this method through numerical comparisons with competitors on benchmark
datasets for IR.

</details>


### [99] [One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression](https://arxiv.org/abs/2508.13836)
*Mikołaj Janusz,Tomasz Wojnar,Yawei Li,Luca Benini,Kamil Adamczewski*

Main category: cs.LG

TL;DR: 본 연구는 한 번의 훈련과 가지치기 방법인 일회성 가지치기와 여러 사이클을 통한 반복 가지치기를 체계적으로 비교하여 각각의 장점을 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 신경망의 압축을 통해 계산 효율성을 높이는 가지치기 기법에 대한 관심.

Method: 두 가지 가지치기 방법인 일회성 가지치기와 반복 가지치기의 체계적 비교, 정의 제공, 구조화된 및 비구조화된 환경에서 벤치마킹, 다양한 가지치기 기준 및 방식 적용.

Result: 일회성 가지치기는 낮은 가지치기 비율에서 더 효과적이고, 반복 가지치기는 높은 비율에서 더 좋은 성능을 보임.

Conclusion: 기다림 기반 가지치기를 옹호하며 특정 시나리오에서 전통적인 방법보다 성능이 우수한 하이브리드 접근법을 제안하여, 목표와 제한에 맞춰 가지치기 전략을 선택하는 데 유용한 통찰을 제공.

Abstract: Pruning is a core technique for compressing neural networks to improve
computational efficiency. This process is typically approached in two ways:
one-shot pruning, which involves a single pass of training and pruning, and
iterative pruning, where pruning is performed over multiple cycles for
potentially finer network refinement. Although iterative pruning has
historically seen broader adoption, this preference is often assumed rather
than rigorously tested. Our study presents one of the first systematic and
comprehensive comparisons of these methods, providing rigorous definitions,
benchmarking both across structured and unstructured settings, and applying
different pruning criteria and modalities. We find that each method has
specific advantages: one-shot pruning proves more effective at lower pruning
ratios, while iterative pruning performs better at higher ratios. Building on
these findings, we advocate for patience-based pruning and introduce a hybrid
approach that can outperform traditional methods in certain scenarios,
providing valuable insights for practitioners selecting a pruning strategy
tailored to their goals and constraints. Source code is available at
https://github.com/janumiko/pruning-benchmark.

</details>


### [100] [FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks](https://arxiv.org/abs/2508.13853)
*Nicolò Romandini,Cristian Borcea,Rebecca Montanari,Luca Foschini*

Main category: cs.LG

TL;DR: FedUP는 악의적인 클라이언트의 영향을 효율적으로 완화하기 위해 설계된 경량화된 연합 비학습(FU) 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습(FL)은 모델 중독 같은 공격에 취약할 수 있으며, 연합 비학습(FU)은 그러한 취약점을 해결하기 위한 방법으로 떠오르고 있다.

Method: FedUP는 침해된 모델 내에서 특정 연결을 제거함으로써 악의적인 클라이언트의 영향을 효율적으로 완화하도록 설계된 알고리즘이다.

Result: 테스트 결과, FedUP은 악의적인 클라이언스의 영향을 줄이며, 비악의적 데이터에 대한 성능을 유지하면서 악의적인 데이터에 대한 정확도를 낮춘다.

Conclusion: FedUP는 최신 FU 솔루션과 비교하여 일관되게 더 빠르고 저장 공간을 절약하면서 효과적인 연합 비학습을 달성한다.

Abstract: Federated Learning (FL) can be vulnerable to attacks, such as model
poisoning, where adversaries send malicious local weights to compromise the
global model. Federated Unlearning (FU) is emerging as a solution to address
such vulnerabilities by selectively removing the influence of detected
malicious contributors on the global model without complete retraining.
However, unlike typical FU scenarios where clients are trusted and cooperative,
applying FU with malicious and possibly colluding clients is challenging
because their collaboration in unlearning their data cannot be assumed. This
work presents FedUP, a lightweight FU algorithm designed to efficiently
mitigate malicious clients' influence by pruning specific connections within
the attacked model. Our approach achieves efficiency by relying only on
clients' weights from the last training round before unlearning to identify
which connections to inhibit. Isolating malicious influence is non-trivial due
to overlapping updates from benign and malicious clients. FedUP addresses this
by carefully selecting and zeroing the highest magnitude weights that diverge
the most between the latest updates from benign and malicious clients while
preserving benign information. FedUP is evaluated under a strong adversarial
threat model, where up to 50%-1 of the clients could be malicious and have full
knowledge of the aggregation process. We demonstrate the effectiveness,
robustness, and efficiency of our solution through experiments across IID and
Non-IID data, under label-flipping and backdoor attacks, and by comparing it
with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces
malicious influence, lowering accuracy on malicious data to match that of a
model retrained from scratch while preserving performance on benign data. FedUP
achieves effective unlearning while consistently being faster and saving
storage compared to the SOTA.

</details>


### [101] [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](https://arxiv.org/abs/2508.13874)
*Rouqaiah Al-Refai,Pankaja Priya Ramasamy,Ragini Ramesh,Patricia Arias-Cabarcos,Philipp Terhörst*

Main category: cs.LG

TL;DR: 생체 인식 모달리티의 평가를 전문가 설문을 통해 재조명하고, 다양한 모달리티에 대한 신뢰성과 일관성을 평가하여 향후 연구 방향을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 인증 시스템의 발전과 생체 인식의 중요성이 커짐에 따라 특정 응용에 적합한 생체 모달리티를 평가할 신뢰할 수 있는 프레임워크가 필요하다.

Method: 24명의 생체 인식 전문가를 대상으로 한 설문 조사를 통해 생체 모달리티의 평가를 재검토한다.

Result: 전문가 의견 분석 결과, 얼굴 인식의 신뢰도는 높아진 반면, 지문 인식은 새로운 취약점으로 인해 신뢰성이 떨어진 것으로 나타났다.

Conclusion: 전문가 평가와 데이터셋 레벨의 불확실성 간의 비교를 통해 실증 증거와 전문가 통찰의 통합이 중요하다는 것을 강조하며, 전문가 간의 의견 차이는 미래 연구의 방향을 제시한다.

Abstract: The rapid advancement of authentication systems and their increasing reliance
on biometrics for faster and more accurate user verification experience,
highlight the critical need for a reliable framework to evaluate the
suitability of biometric modalities for specific applications. Currently, the
most widely known evaluation framework is a comparative table from 1998, which
no longer adequately captures recent technological developments or emerging
vulnerabilities in biometric systems. To address these challenges, this work
revisits the evaluation of biometric modalities through an expert survey
involving 24 biometric specialists. The findings indicate substantial shifts in
property ratings across modalities. For example, face recognition, shows
improved ratings due to technological progress, while fingerprint, shows
decreased reliability because of emerging vulnerabilities and attacks. Further
analysis of expert agreement levels across rated properties highlighted the
consistency of the provided evaluations and ensured the reliability of the
ratings. Finally, expert assessments are compared with dataset-level
uncertainty across 55 biometric datasets, revealing strong alignment in most
modalities and underscoring the importance of integrating empirical evidence
with expert insight. Moreover, the identified expert disagreements reveal key
open challenges and help guide future research toward resolving them.

</details>


### [102] [Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches](https://arxiv.org/abs/2508.13898)
*Yishun Lu,Wesley Armour*

Main category: cs.LG

TL;DR: 이 논문에서는 대규모 배치 크기에서 효과적인 2차 방법의 성능을 복원하는 Fisher-Orthogonal Projection(FOP)라는 새로운 기술을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 현대 GPU는 대량의 고대역폭 메모리를 장착하여 수만 개의 훈련 샘플을 지원할 수 있게 했지만, 대부분의 기존 최적화기들은 이런 대규모 배치 크기에서 효과적으로 작동하지 못한다.

Method: FOP는 두 개의 하위 배치에서 받은 그래디언트를 활용하여 분산을 인식하는 업데이트 방향을 구성하며, Fisher-메트릭 하에서 평균과 직교하는 그래디언트 차원 성분을 통해 평균 그래디언트를 향상시킨다.

Result: FOP는 대규모 배치 크기에서도 2차 방법의 효과성을 복원하여 훈련을 확장 가능하게 하고 일반화 성능을 향상시키며 빠른 수렴을 이룬다.

Conclusion: FOP는 대규모 배치 학습에서 발생하는 문제를 해결하는 혁신적인 방법으로, 성능을 기존 단순한 경량 하강법 수준으로 떨어뜨리지 않고 향상시킨다.

Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory,
enabling them to support mini-batch sizes of up to tens of thousands of
training samples. However, most existing optimizers struggle to perform
effectively at such a large batch size. As batch size increases, gradient noise
decreases due to averaging over many samples, limiting the ability of
first-order methods to escape sharp or suboptimal minima and reach the global
minimum. Meanwhile, second-order methods like the natural gradient with
Kronecker-Factored Approximate Curvature (KFAC) often require excessively high
damping to remain stable at large batch sizes. This high damping effectively
washes out the curvature information that gives these methods their advantage,
reducing their performance to that of simple gradient descent. In this paper,
we introduce Fisher-Orthogonal Projection (FOP), a novel technique that
restores the effectiveness of the second-order method at very large batch
sizes, enabling scalable training with improved generalization and faster
convergence. FOP constructs a variance-aware update direction by leveraging
gradients from two sub-batches, enhancing the average gradient with a component
of the gradient difference that is orthogonal to the average under the
Fisher-metric.

</details>


### [103] [Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation](https://arxiv.org/abs/2508.13904)
*Thanh Nguyen,Chang D. Yoo*

Main category: cs.LG

TL;DR: OFQL은 DQL의 한계점을 극복하고 효율적인 1단계 행동 생성을 가능하게 하는 새로운 프레임워크로, D4RL 벤치마크에서 DQL보다 성능이 우수하고 훈련 및 추론 시간을 크게 단축시킴.


<details>
  <summary>Details</summary>
Motivation: Diffusion 모델의 생성 능력이 오프라인 강화 학습에서 높은 성능을 발휘하도록 하는 결정적 알고리즘을 가능하게 하였습니다.

Method: DQL의 재검토를 통해 문제점을 파악하고, 보조 모델이나 다단계 훈련 없이 효율적인 1단계 행동 생성을 위한 OFQL 프레임워크를 제안합니다.

Result: OFQL은 DQL 및 다른 확산 기반 기준보다 우수한 성능을 보이면서 훈련 및 추론 시간을 크게 단축시킵니다.

Conclusion: OFQL은 DQL의 다단계 샘플링과 재귀적 기울기 업데이트를 제거하여 더 빠르고 강력한 훈련과 추론을 가능하게 합니다.

Abstract: The generative power of diffusion models (DMs) has recently enabled
high-performing decision-making algorithms in offline reinforcement learning
(RL), achieving state-of-the-art results across standard benchmarks. Among
them, Diffusion Q-Learning (DQL) stands out as a leading method for its
consistently strong performance. Nevertheless, DQL remains limited in practice
due to its reliance on multi-step denoising for action generation during both
training and inference. Although one-step denoising is desirable, simply
applying it to DQL leads to a drastic performance drop. In this work, we
revisit DQL and identify its core limitations. We then propose One-Step Flow
Q-Learning (OFQL), a novel framework that enables efficient one-step action
generation during both training and inference, without requiring auxiliary
models, distillation, or multi-phase training. Specifically, OFQL reformulates
DQL within the sample-efficient Flow Matching (FM) framework. While
conventional FM induces curved generative trajectories that impede one-step
generation, OFQL instead learns an average velocity field that facilitates
direct, accurate action generation. Collectively, OFQL eliminates the need for
multi-step sampling and recursive gradient updates in DQL, resulting in faster
and more robust training and inference. Extensive experiments on the D4RL
benchmark demonstrate that OFQL outperforms DQL and other diffusion-based
baselines, while substantially reducing both training and inference time
compared to DQL.

</details>


### [104] [Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management](https://arxiv.org/abs/2508.13905)
*Tianheng Ling,Vipin Singh,Chao Qian,Felix Biessmann,Gregor Schiele*

Main category: cs.LG

TL;DR: 기후 변화로 인한 극단적인 기상 이벤트가 노후된 혼합 하수 시스템에 도전을 제기하고 있으며, 이에 따라 하수 과오버플로우의 위험성이 증가하고 있다. 본 논문은 엣지 장치에서 에너지 효율적인 예측을 가능하게 하는 종단 간 예측 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기후 변화로 인한 극단 기상 사건이 혼합 하수 시스템에 미치는 영향 및 예측의 필요성.

Method: 경량의 Transformer 및 LSTM 모델을 통합하여 정수 양자화를 통해 효율적인 장치 내 실행을 가능하게 하는 예측 프레임워크를 제안하였다.

Result: 8비트 Transformer 모델이 높은 정확도(MSE 0.0376)와 함께 적당한 에너지 비용(0.370 mJ)을 달성했으며, 최적 8비트 LSTM 모델은 에너지 소모가 훨씬 적지만 정확도는 14.89% 저하되었다.

Conclusion: 모델 선택과 배치 우선순위를 맞추는 필요성을 강조하며, 에너지 소비가 매우 낮은 LSTM 또는 높은 예측 정확도를 위해 Transformer를 선호할 수 있음을 보여준다.

Abstract: Extreme weather events, intensified by climate change, increasingly challenge
aging combined sewer systems, raising the risk of untreated wastewater
overflow. Accurate forecasting of sewer overflow basin filling levels can
provide actionable insights for early intervention, helping mitigating
uncontrolled discharge. In recent years, AI-based forecasting methods have
offered scalable alternatives to traditional physics-based models, but their
reliance on cloud computing limits their reliability during communication
outages. To address this, we propose an end-to-end forecasting framework that
enables energy-efficient inference directly on edge devices. Our solution
integrates lightweight Transformer and Long Short-Term Memory (LSTM) models,
compressed via integer-only quantization for efficient on-device execution.
Moreover, an automated hardware-aware deployment pipeline is used to search for
optimal model configurations by jointly minimizing prediction error and energy
consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer
data, the selected 8-bit Transformer model, trained on 24 hours of historical
measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ
per inference. In contrast, the optimal 8-bit LSTM model requires significantly
less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE
0.0432) and much longer training time. This trade-off highlights the need to
align model selection with deployment priorities, favoring LSTM for ultra-low
energy consumption or Transformer for higher predictive accuracy. In general,
our work enables local, energy-efficient forecasting, contributing to more
resilient combined sewer systems. All code can be found in the GitHub
Repository (https://github.com/tianheng-ling/EdgeOverflowForecast).

</details>


### [105] [Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control](https://arxiv.org/abs/2508.13922)
*SM Mazharul Islam,Manfred Huber*

Main category: cs.LG

TL;DR: 본 연구에서는 심층 강화 학습에서 다중 모달 정책을 모델링하기 위해 범주형 정책을 도입하고, 이를 통해 더 나은 탐색과 빠른 수렴을 달성함을 보여준다.


<details>
  <summary>Details</summary>
Motivation:  많은 실용적 의사결정 문제에서 강력한 탐색을 촉진하는 다중 모달 정책의 필요성이 있다.

Method: 범주형 분포를 이용하여 다중 모달 동작을 모델링하고, 샘플링된 모드에 조건화된 출력 동작을 생성한다.

Result: 다중 모달 정책은 DeepMind Control Suite 환경 세트에서 평가되었으며, 개선된 탐색 덕분에 표준 가우시안 정책보다 빠르게 수렴하고 성능이 우월함을 입증했다.

Conclusion: 범주형 분포는 연속 제어에서 구조화된 탐색과 다중 모달 동작 표현을 위한 강력한 도구로 작용한다.

Abstract: A policy in deep reinforcement learning (RL), either deterministic or
stochastic, is commonly parameterized as a Gaussian distribution alone,
limiting the learned behavior to be unimodal. However, the nature of many
practical decision-making problems favors a multimodal policy that facilitates
robust exploration of the environment and thus to address learning challenges
arising from sparse rewards, complex dynamics, or the need for strategic
adaptation to varying contexts. This issue is exacerbated in continuous control
domains where exploration usually takes place in the vicinity of the predicted
optimal action, either through an additive Gaussian noise or the sampling
process of a stochastic policy. In this paper, we introduce Categorical
Policies to model multimodal behavior modes with an intermediate categorical
distribution, and then generate output action that is conditioned on the
sampled mode. We explore two sampling schemes that ensure differentiable
discrete latent structure while maintaining efficient gradient-based
optimization. By utilizing a latent categorical distribution to select the
behavior mode, our approach naturally expresses multimodality while remaining
fully differentiable via the sampling tricks. We evaluate our multimodal policy
on a set of DeepMind Control Suite environments, demonstrating that through
better exploration, our learned policies converge faster and outperform
standard Gaussian policies. Our results indicate that the Categorical
distribution serves as a powerful tool for structured exploration and
multimodal behavior representation in continuous control.

</details>


### [106] [How Usable is Automated Feature Engineering for Tabular Data?](https://arxiv.org/abs/2508.13932)
*Bastian Schäfer,Lennart Purucker,Maciej Janowski,Frank Hutter*

Main category: cs.LG

TL;DR: 자동화된 기능 엔지니어링(AutoFE) 방법의 사용성을 평가하였으며, 전반적으로 사용이 어렵고 문서화가 부족하며 활성 커뮤니티가 없는 것을 발견하였다.


<details>
  <summary>Details</summary>
Motivation: 기능 엔지니어링은 기계 학습에서 최고의 성능을 달성하는 데 필수적이며, 수동 기능 엔지니어링은 비용과 시간이 많이 소모된다.

Method: 53개의 AutoFE 방법을 조사하였다.

Result: 조사 결과, 대부분의 방법이 사용하기 어렵고 문서화가 부족하며 활성화된 커뮤니티가 없음을 발견하였다.

Conclusion: 사용 가능한 자동화 방법을 위한 향후 연구의 필요성을 강조하였다.

Abstract: Tabular data, consisting of rows and columns, is omnipresent across various
machine learning applications. Each column represents a feature, and features
can be combined or transformed to create new, more informative features. Such
feature engineering is essential to achieve peak performance in machine
learning. Since manual feature engineering is expensive and time-consuming, a
substantial effort has been put into automating it. Yet, existing automated
feature engineering (AutoFE) methods have never been investigated regarding
their usability for practitioners. Thus, we investigated 53 AutoFE methods. We
found that these methods are, in general, hard to use, lack documentation, and
have no active communities. Furthermore, no method allows users to set time and
memory constraints, which we see as a necessity for usable automation. Our
survey highlights the need for future work on usable, well-engineered AutoFE
methods.

</details>


### [107] [Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem](https://arxiv.org/abs/2508.13963)
*Soumyajit Guin,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 이 논문에서는 확률적 최단 경로 문제를 해결하기 위해 두 가지 알고리즘과 기능 근사 설정용 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 확률적 최단 경로(SSP) 문제는 강화 학습(RL)에서 중요한 문제 클래스이며, RL의 다른 비용 기준 유형은 SSP 설정에서 공식화할 수 있다.

Method: 이 논문에서는 테이블 형식과 함수 근사 설정을 위한 알고리즘을 제안한다.

Result: 제안된 테이블 알고리즘은 다른 잘 알려진 수렴 RL 알고리즘에 비해 우수한 성능을 보이며, 함수 근사 알고리즘도 안정적인 성능을 나타낸다.

Conclusion: 모든 알고리즘의 비대칭 거의 확실한 수렴을 보여준다.

Abstract: In this paper we propose two algorithms in the tabular setting and an
algorithm for the function approximation setting for the Stochastic Shortest
Path (SSP) problem. SSP problems form an important class of problems in
Reinforcement Learning (RL), as other types of cost-criteria in RL can be
formulated in the setting of SSP. We show asymptotic almost-sure convergence
for all our algorithms. We observe superior performance of our tabular
algorithms compared to other well-known convergent RL algorithms. We further
observe reliable performance of our function approximation algorithm compared
to other algorithms in the function approximation setting.

</details>


### [108] [AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics](https://arxiv.org/abs/2508.13979)
*Yi Yang,Kei Ikemura,Qingwen Zhang,Xiaomeng Zhu,Ci Li,Nazre Batool,Sina Sharif Mansouri,John Folkesson*

Main category: cs.LG

TL;DR: 본 논문은 선형 스칼라화와 다중 작업 최적화(MTO)의 직접적인 연결을 Establish하며, AutoScale이라는 두 단계의 프레임워크를 소개하여 작업 가중치를 효율적으로 선택하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 선형 스칼라화가 복잡한 MTO 방법보다 우수한 성능을 보일 수 있다는 점에 주목하였습니다.

Method: MTO 메트릭을 사용하여 가중치 선택을 유도하는 두 단계의 프레임워크인 AutoScale을 소개합니다.

Result: AutoScale은 다양한 데이터셋에서 항상 우수한 성능과 높은 효율성을 보여줍니다.

Conclusion: 본 연구는 적절한 가중치 선택이 성능 향상에 기여할 수 있음을 입증하였습니다.

Abstract: Recent multi-task learning studies suggest that linear scalarization, when
using well-chosen fixed task weights, can achieve comparable to or even better
performance than complex multi-task optimization (MTO) methods. It remains
unclear why certain weights yield optimal performance and how to determine
these weights without relying on exhaustive hyperparameter search. This paper
establishes a direct connection between linear scalarization and MTO methods,
revealing through extensive experiments that well-performing scalarization
weights exhibit specific trends in key MTO metrics, such as high gradient
magnitude similarity. Building on this insight, we introduce AutoScale, a
simple yet effective two-phase framework that uses these MTO metrics to guide
weight selection for linear scalarization, without expensive weight search.
AutoScale consistently shows superior performance with high efficiency across
diverse datasets including a new large-scale benchmark.

</details>


### [109] [Multi-User Contextual Cascading Bandits for Personalized Recommendation](https://arxiv.org/abs/2508.13981)
*Jiho Park,Huiwen Jia*

Main category: cs.LG

TL;DR: 새로운 다중 사용자 맥락 연속 밴딧 모델인 MCCB를 제안하며, 온라인 광고 시나리오에서의 사용자 상호작용을 반영. UCBBP와 AUCBBP 알고리즘의 제시와 이론적 성과 검증.


<details>
  <summary>Details</summary>
Motivation: 여러 사용자가 동시에 상호작용하는 시스템을 다루기 위한 새로운 모델을 필요로 함.

Method: Multi-User Contextual Cascading Bandit (MCCB) 모델과 두 가지 알고리즘, Upper Confidence Bound with Backward Planning (UCBBP) 및 Active Upper Confidence Bound with Backward Planning (AUCBBP)를 제안.

Result: UCBBP는 $T$ 에피소드, $H$ 세션 단계 및 $N$ 컨텍스트에 대해 $	ilde{O}(	heta 	imes 	ext{sqrt}(THN))$이라는 후회 경계를 달성하며, AUCBBP는 사용자 스케일링에서 효율성 개선을 보여줌.

Conclusion: 이론적 발견을 수치 실험을 통해 검증하여, 다양한 설정에서 두 알고리즘의 경험적 효과성을 입증.

Abstract: We introduce a Multi-User Contextual Cascading Bandit model, a new
combinatorial bandit framework that captures realistic online advertising
scenarios where multiple users interact with sequentially displayed items
simultaneously. Unlike classical contextual bandits, MCCB integrates three key
structural elements: (i) cascading feedback based on sequential arm exposure,
(ii) parallel context sessions enabling selective exploration, and (iii)
heterogeneous arm-level rewards. We first propose Upper Confidence Bound with
Backward Planning (UCBBP), a UCB-style algorithm tailored to this setting, and
prove that it achieves a regret bound of $\widetilde{O}(\sqrt{THN})$ over $T$
episodes, $H$ session steps, and $N$ contexts per episode. Motivated by the
fact that many users interact with the system simultaneously, we introduce a
second algorithm, termed Active Upper Confidence Bound with Backward Planning
(AUCBBP), which shows a strict efficiency improvement in context scaling, i.e.,
user scaling, with a regret bound of $\widetilde{O}(\sqrt{T+HN})$. We validate
our theoretical findings via numerical experiments, demonstrating the empirical
effectiveness of both algorithms under various settings.

</details>


### [110] [Formal Algorithms for Model Efficiency](https://arxiv.org/abs/2508.14000)
*Naman Tyagi,Srishti Das,Kunal,Vatsal Gupta*

Main category: cs.LG

TL;DR: KMR 프레임워크는 딥러닝 모델 효율성 기술을 통합적으로 표현하고 추론할 수 있는 형식이다.


<details>
  <summary>Details</summary>
Motivation: 효율성 최적화를 위한 수학적이고 모듈화된 관점을 제공하여 다양한 방법을 통합하고 관리할 수 있는 수단을 제공하기 위해.

Method: KMR 프레임워크는 다양한 방법들을 통제 가능한 조정, 결정적인 규칙, 측정 가능한 미터로 추상화하여 구성된다.

Result: KMR을 사용하여 잘 알려진 효율성 방법들을 KMR 삼중체로 구체화할 수 있으며, 각 방법에 대한 간결한 알고리즘 템플릿을 제시한다.

Conclusion: KMR은 모델 효율성 연구를 통합하고 발전시키기 위한 개념적 및 실용적 도구를 제공한다.

Abstract: We introduce the Knob-Meter-Rule (KMR) framework, a unified formalism for
representing and reasoning about model efficiency techniques in deep learning.
By abstracting diverse methods, including pruning, quantization, knowledge
distillation, and parameter-efficient architectures, into a consistent set of
controllable knobs, deterministic rules, and measurable meters, KMR provides a
mathematically precise and modular perspective on efficiency optimization. The
framework enables systematic composition of multiple techniques, flexible
policy-driven application, and iterative budgeted optimization through the
Budgeted-KMR algorithm. We demonstrate how well-known efficiency methods can be
instantiated as KMR triples and present concise algorithmic templates for each.
The framework highlights underlying relationships between methods, facilitates
hybrid pipelines, and lays the foundation for future research in automated
policy learning, dynamic adaptation, and theoretical analysis of cost-quality
trade-offs. Overall, KMR offers both a conceptual and practical tool for
unifying and advancing model efficiency research.

</details>


### [111] [ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery](https://arxiv.org/abs/2508.14005)
*Mohammad Izadi,Mehran Safayani*

Main category: cs.LG

TL;DR: ASD와 관련된 뇌 연결성을 포착할 수 있는 ASDFormer라는 새로운 Transformer 기반 아키텍처를 제안한다.


<details>
  <summary>Details</summary>
Motivation: ASD의 발생과 관련된 뇌의 연결성 패턴을 포착하는 것이 중요하다.

Method: Transformer 아키텍처와 Mixture of Pooling-Classifier Experts (MoE)를 결합하여 ASD와 관련된 신경 신호를 포착한다.

Result: ASDFormer는 ABIDE 데이터셋에서 최신 진단 정확도를 달성하고, ASD와 관련된 기능적 연결성 장애에 대한 통찰력을 제공한다.

Conclusion: ASDFormer는 바이오마커 발견을 위한 유망한 도구로 활용될 수 있다.

Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition
marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a
non-invasive window into large-scale neural dynamics by measuring
blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can
be modeled as interactions among Regions of Interest (ROIs), which are grouped
into functional communities based on their underlying roles in brain function.
Emerging evidence suggests that connectivity patterns within and between these
communities are particularly sensitive to ASD-related alterations. Effectively
capturing these patterns and identifying interactions that deviate from typical
development is essential for improving ASD diagnosis and enabling biomarker
discovery. In this work, we introduce ASDFormer, a Transformer-based
architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to
capture neural signatures associated with ASD. By integrating multiple
specialized expert branches with attention mechanisms, ASDFormer adaptively
emphasizes different brain regions and connectivity patterns relevant to
autism. This enables both improved classification performance and more
interpretable identification of disorder-related biomarkers. Applied to the
ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and
reveals robust insights into functional connectivity disruptions linked to ASD,
highlighting its potential as a tool for biomarker discovery.

</details>


### [112] [GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks](https://arxiv.org/abs/2508.14004)
*Sergey Salishev,Ian Akhremchik*

Main category: cs.LG

TL;DR: 정량화된 신경망은 각 레이어에서 반올림이 비트 너비를 줄이면서 용량을 감소시키는 노이즈 채널 체인으로 볼 수 있다. 이 논문은 평균 비트 너비가 감소함에 따라 용량 동역학을 추적하고, 미세 조정을 매끄럽고 제약된 최적화 문제로 설정하여 정량화 병목 현상을 파악한다.


<details>
  <summary>Details</summary>
Motivation: 정량화된 신경망의 성능을 향상시키기 위해 비트 너비 감소와 관련된 병목 현상을 분석하고 해결하고자 한다.

Method: 완전 미분 가능한 직접 통과 추정기(STE)를 이용하며, 학습 가능한 비트 너비, 노이즈 스케일, 클램프 경계를 사용하고 외부 점 페널티를 통해 목표 비트 너비를 적용한다. 또한, 부드러운 메트릭 스무딩(증류 사용)을 통해 훈련을 안정화한다.

Result: 이 방법은 단순함에도 불구하고 극단적인 W1A1 설정에서 경쟁력 있는 정확도를 달성하면서 STE의 효율성을 유지한다.

Conclusion: 제안된 접근 방식은 정량화된 신경망의 성능을 최적화하는 데 효과적이며, 향후 연구와 응용에 기여할 수 있다.

Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where
rounding in each layer reduces capacity as bit-width shrinks; the
floating-point (FP) checkpoint sets the maximum input rate. We track capacity
dynamics as the average bit-width decreases and identify resulting quantization
bottlenecks by casting fine-tuning as a smooth, constrained optimization
problem. Our approach employs a fully differentiable Straight-Through Estimator
(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a
target bit-width via an exterior-point penalty; mild metric smoothing (via
distillation) stabilizes training. Despite its simplicity, the method attains
competitive accuracy down to the extreme W1A1 setting while retaining the
efficiency of STE.

</details>


### [113] [Efficient Knowledge Graph Unlearning with Zeroth-order Information](https://arxiv.org/abs/2508.14013)
*Yang Xiao,Ruimeng Ye,Bohan Liu,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 지식 그래프의 비효율적 학습 제거 문제를 해결하기 위한 새로운 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 프라이버시 정책으로 인해 훈련 데이터를 제거할 필요성이 커지고 있으나, 전체 재훈련은 비용이 많이 드는 문제를 해결하고자 한다.

Method: 효율적인 지식 그래프 비학습 제거 알고리즘을 개발하고, 데이터 제거에 따른 모델의 민감도를 근사화한다.

Result: 제안된 방법이 기존 최첨단 그래프 비학습 제거 방법들에 비해 비학습 효율성과 품질에서 현저하게 우수함을 보였다.

Conclusion: 실험 결과, 새로 제안한 방법이 비학습 제거 분야에서 기존 방법들보다 우수함을 확인했다.

Abstract: Due to regulations like the Right to be Forgotten, there is growing demand
for removing training data and its influence from models. Since full retraining
is costly, various machine unlearning methods have been proposed. In this
paper, we firstly present an efficient knowledge graph (KG) unlearning
algorithm. We remark that KG unlearning is nontrivial due to the distinctive
structure of KG and the semantic relations between entities. Also, unlearning
by estimating the influence of removed components incurs significant
computational overhead when applied to large-scale knowledge graphs. To this
end, we define an influence function for KG unlearning and propose to
approximate the model's sensitivity without expensive computation of
first-order and second-order derivatives for parameter updates. Specifically,
we use Taylor expansion to estimate the parameter changes caused by data
removal. Given that the first-order gradients and second-order derivatives
dominate the computational load, we use the Fisher matrices and zeroth-order
optimization to approximate the inverse-Hessian vector product without
constructing the computational graphs. Our experimental results demonstrate
that the proposed method outperforms other state-of-the-art graph unlearning
baselines significantly in terms of unlearning efficiency and unlearning
quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.

</details>


### [114] [Typed Topological Structures Of Datasets](https://arxiv.org/abs/2508.14008)
*Wanjun Hu*

Main category: cs.LG

TL;DR: 이 논문에서는 데이터셋에 대한 특수한 타입과 관련된 위상수학을 개발하여 데이터셋의 내부 구조를 조사한다.


<details>
  <summary>Details</summary>
Motivation: 현재 데이터셋 연구는 통계적 방법 및 대수적 위상수학 방법에 중점을 두고 있으며, 유한 위상 공간을 연구할 새로운 접근법이 필요하다.

Method: 타입이 할당된 열린 집합을 사용하여 특수한 타입 집합과 관련된 타입 위상을 개발한다.

Result: 각 트랙을 구성 요소로 나눌 수 있는 자연스러운 몫 공간을 정의하였고, 그 구성 요소는 정수 시퀀스로 표현될 수 있다.

Conclusion: 이러한 구조는 볼록 껍질 계산, 구멍 탐지, 클러스터링 및 이상 탐지와 같은 문제를 위한 새로운 알고리즘 플랫폼을 제공한다.

Abstract: A datatset $X$ on $R^2$ is a finite topological space. Current research of a
dataset focuses on statistical methods and the algebraic topological method
\cite{carlsson}. In \cite{hu}, the concept of typed topological space was
introduced and showed to have the potential for studying finite topological
spaces, such as a dataset. It is a new method from the general topology
perspective. A typed topological space is a topological space whose open sets
are assigned types. Topological concepts and methods can be redefined using
open sets of certain types. In this article, we develop a special set of types
and its related typed topology on a dataset $X$. Using it, we can investigate
the inner structure of $X$. In particular, $R^2$ has a natural quotient space,
in which $X$ is organized into tracks, and each track is split into components.
Those components are in a order. Further, they can be represented by an integer
sequence. Components crossing tracks form branches, and the relationship can be
well represented by a type of pseudotree (called typed-II pseudotree). Such
structures provide a platform for new algorithms for problems such as
calculating convex hull, holes, clustering and anomaly detection.

</details>


### [115] [BLIPs: Bayesian Learned Interatomic Potentials](https://arxiv.org/abs/2508.14022)
*Dario Coscia,Pim de Haan,Max Welling*

Main category: cs.LG

TL;DR: BLIPs는 부정확한 예측 문제를 해결하기 위한 베이지안 학습 상호 원자 잠재력으로, 시뮬레이션 기반 화학에서의 MLIPs의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: MLIPs는 시뮬레이션 기반 화학에서 중요한 도구이지만, 훈련 데이터가 부족한 경우와 분포 밖에서의 예측 정확성에 어려움이 있습니다.

Method: BLIPs는 변량 베이지안 프레임워크로, Variational Dropout의 적응형 버전을 기반으로 하여 MLIPs를 훈련하거나 미세 조정합니다.

Result: BLIPs는 에너지 및 힘 예측에서 잘 보정된 불확실성 추정치를 제공하며, 시뮬레이션 기반 화학 작업에서 표준 MLIPs에 비해 예측 정확성을 향상시킵니다.

Conclusion: 사전 훈련된 MLIPs를 BLIP으로 미세 조정하면 일관된 성능 향상과 보정된 불확실성을 제공합니다.

Abstract: Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool
in simulation-based chemistry. However, like most deep learning models, MLIPs
struggle to make accurate predictions on out-of-distribution data or when
trained in a data-scarce regime, both common scenarios in simulation-based
chemistry. Moreover, MLIPs do not provide uncertainty estimates by
construction, which are fundamental to guide active learning pipelines and to
ensure the accuracy of simulation results compared to quantum calculations. To
address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic
Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian
framework for training or fine-tuning MLIPs, built on an adaptive version of
Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and
minimal computational overhead for energy and forces prediction at inference
time, while integrating seamlessly with (equivariant) message-passing
architectures. Empirical results on simulation-based computational chemistry
tasks demonstrate improved predictive accuracy with respect to standard MLIPs,
and trustworthy uncertainty estimates, especially in data-scarse or heavy
out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP
yields consistent performance gains and calibrated uncertainties.

</details>


### [116] [Learning from Preferences and Mixed Demonstrations in General Settings](https://arxiv.org/abs/2508.14027)
*Jason R Brown,Carl Henrik Ek,Robert D Mullins*

Main category: cs.LG

TL;DR: LEOPARD 알고리즘은 다양한 데이터로부터 보상 기능을 효율적으로 학습하며, 제한된 피드백 상황에서도 기존 방식보다 성능이 우수하다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업에서 좋은 보상 함수를 지정하는 것이 어려운 문제를 해결하고자 한다.

Method: LEOPARD(Preference And Ranked Demonstrations로부터 목표를 학습)라는 새로운 알고리즘을 개발하였다.

Result: LEOPARD는 제한된 피드백 상황에서도 기존 방법보다 상당히 뛰어난 성능을 보인다.

Conclusion: LEOPARD는 다양한 피드백 유형을 결합할 때 이점을 발견하였다.

Abstract: Reinforcement learning is a general method for learning in sequential
settings, but it can often be difficult to specify a good reward function when
the task is complex. In these cases, preference feedback or expert
demonstrations can be used instead. However, existing approaches utilising both
together are often ad-hoc, rely on domain-specific properties, or won't scale.
We develop a new framing for learning from human data, \emph{reward-rational
partial orderings over observations}, designed to be flexible and scalable.
Based on this we introduce a practical algorithm, LEOPARD: Learning Estimated
Objectives from Preferences And Ranked Demonstrations. LEOPARD can learn from a
broad range of data, including negative demonstrations, to efficiently learn
reward functions across a wide range of domains. We find that when a limited
amount of preference and demonstration feedback is available, LEOPARD
outperforms existing baselines by a significant margin. Furthermore, we use
LEOPARD to investigate learning from many types of feedback compared to just a
single one, and find that combining feedback types is often beneficial.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [117] [Goal-Directedness is in the Eye of the Beholder](https://arxiv.org/abs/2508.13247)
*Nina Rajcic,Anders Søgaard*

Main category: cs.MA

TL;DR: 이 논문은 목표 지향적 행동을 추론하는 두 가지 접근 방식의 한계와 문제점을 논의하며, 목표 지향성을 객관적으로 측정할 수 없다는 주장을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 에이전트의 행동을 예측하기 위해 목표를 귀속시키는 능력이 중요하다.

Method: 목표 지향적 행동을 두 가지 방식, 즉 행동 관찰을 통한 추정과 내부 모델 상태에서 목표를 탐색하는 방식으로 탐구한다.

Result: 목표 지향성을 객관적으로 측정할 수 없다는 놀라운 결론에 도달했다.

Conclusion: 동적 다중 에이전트 시스템의 emergent property로 목표 지향성을 모델링하기 위한 새로운 방향을 제시한다.

Abstract: Our ability to predict the behavior of complex agents turns on the
attribution of goals. Probing for goal-directed behavior comes in two flavors:
Behavioral and mechanistic. The former proposes that goal-directedness can be
estimated through behavioral observation, whereas the latter attempts to probe
for goals in internal model states. We work through the assumptions behind both
approaches, identifying technical and conceptual problems that arise from
formalizing goals in agent systems. We arrive at the perhaps surprising
position that goal-directedness cannot be measured objectively. We outline new
directions for modeling goal-directedness as an emergent property of dynamic,
multi-agent systems.

</details>


### [118] [Self-Organizing Agent Network for LLM-based Workflow Automation](https://arxiv.org/abs/2508.13732)
*Yiming Xiong,Jian Wang,Bing Li,Yuhan Zhu,Yuqi Zhao*

Main category: cs.MA

TL;DR: 본 연구에서는 다중 에이전트 프레임워크를 기반으로 한 대규모 언어 모델을 활용하여 복잡한 작업 계획을 개선하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 실제 비즈니스 환경에서 비즈니스 워크플로우는 수많은 서브 프로세스의 모듈화 및 재사용을 통해 구성되며, 이는 복잡한 워크플로우를 초래한다.

Method: 새로운 구조 기반 오케스트레이션 프레임워크인 자기 조직화 에이전트 네트워크(SOAN)를 제안한다.

Result: SOAN은 구조 단위를 독립 에이전트로 식별하고 캡슐화하여 에이전트 네트워크를 점진적으로 구축하며, 모듈성 및 명확성을 강화한다.

Conclusion: SOAN은 적응성, 내결함성 및 실행 효율성 측면에서 최신 기법보다 현저히 우수함을 입증했다.

Abstract: Recent multi-agent frameworks built upon large language models (LLMs) have
demonstrated remarkable capabilities in complex task planning. However, in
real-world enterprise environments, business workflows are typically composed
through modularization and reuse of numerous subprocesses, resulting in
intricate workflows characterized by lengthy and deeply nested execution paths.
Such complexity poses significant challenges for LLM-driven orchestration, as
extended reasoning chains and state-space explosions severely impact planning
effectiveness and the proper sequencing of tool invocations. Therefore,
developing an orchestration method with controllable structures capable of
handling multi-layer nesting becomes a critical issue. To address this, we
propose a novel structure-driven orchestration framework Self-Organizing Agent
Network (SOAN). SOAN incrementally builds a formalized agent network by
identifying and encapsulating structural units as independent agents, enhancing
modularity and clarity in orchestration. Extensive evaluations were performed
using multiple benchmarks as well as a real-world enterprise workflow dataset.
Experimental results demonstrate that SOAN significantly outperforms
state-of-the-art methods in terms of adaptability, fault tolerance, and
execution efficiency.

</details>


### [119] [BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web](https://arxiv.org/abs/2508.13787)
*Zihan Guo,Yuanjian Zhou,Chenyi Wang,Linlin You,Minjie Bian,Weinan Zhang*

Main category: cs.MA

TL;DR: 본 논문은 블록체인을 활용한 신뢰할 수 있는 Agentic Web (BetaWeb)을 소개하며, LLM 기반 다중 에이전트 시스템(LaMAS)의 발전을 위한 개선된 인프라를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 발전은 인공지능 에이전트의 발전을 촉진하고 있으며, 다양한 자율적 실체로 진화하는 에이전트 생태계의 상호 연결성과 확장성 구축이 필수적이다.

Method: 블록체인을 활용하여 신뢰할 수 있고 확장 가능한 인프라를 제공하는 BetaWeb 프레임워크를 제안한다.

Result: BetaWeb은 LaMAS의 발전을 지원하고, 웹 패러다임을 Web3에서 Web3.5로 진화시킬 수 있는 잠재력을 가지고 있다.

Conclusion: 블록체인과 LaMAS의 깊은 통합은 회복력 있고 신뢰할 수 있으며 지속 가능한 디지털 생태계의 기반을 마련할 수 있다.

Abstract: The rapid development of large language models (LLMs) has significantly
propelled the development of artificial intelligence (AI) agents, which are
increasingly evolving into diverse autonomous entities, advancing the LLM-based
multi-agent systems (LaMAS). However, current agentic ecosystems remain
fragmented and closed. Establishing an interconnected and scalable paradigm for
Agentic AI has become a critical prerequisite. Although Agentic Web proposes an
open architecture to break the ecosystem barriers, its implementation still
faces core challenges such as privacy protection, data management, and value
measurement. Existing centralized or semi-centralized paradigms suffer from
inherent limitations, making them inadequate for supporting large-scale,
heterogeneous, and cross-domain autonomous interactions. To address these
challenges, this paper introduces the blockchain-enabled trustworthy Agentic
Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not
only offers a trustworthy and scalable infrastructure for LaMAS but also has
the potential to advance the Web paradigm from Web3 (centered on data
ownership) towards Web3.5, which emphasizes ownership of agent capabilities and
the monetization of intelligence. Beyond a systematic examination of the
BetaWeb framework, this paper presents a five-stage evolutionary roadmap,
outlining the path of LaMAS from passive execution to advanced collaboration
and autonomous governance. We also conduct a comparative analysis of existing
products and discuss key challenges of BetaWeb from multiple perspectives.
Ultimately, we argue that deep integration between blockchain and LaMAS can lay
the foundation for a resilient, trustworthy, and sustainably incentivized
digital ecosystem. A summary of the enabling technologies for each stage is
available at https://github.com/MatZaharia/BetaWeb.

</details>


### [120] [COCO: Cognitive Operating System with Continuous Oversight for Multi-Agent Workflow Reliability](https://arxiv.org/abs/2508.13815)
*Churong Liang,Jinling Gan,Kairan Hong,Qiushi Tian,Zongze Wu,Runnan Li*

Main category: cs.MA

TL;DR: COCO는 다중 에이전트 시스템에서 비동기적 자기 모니터링 및 적응형 오류 수정을 구현하는 이론적 구조를 제공하여 오류 전파 및 품질 저하 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 다중 에이전트 워크플로우는 수정 메커니즘 없이 상류 실패를 하류 에이전트가 복합적으로 발생시키는 오류 전파 및 품질 저하에 내재된 취약성을 가지고 있습니다.

Method: COCO는 오류 감지를 중요한 실행 경로에서 분리하여 품질 보증과 계산 효율성 사이의 기본적인 균형을 다루는 새로운 분리된 아키텍처를 사용합니다. COCO는 세 가지 핵심 알고리즘 혁신을 사용하여 체계적 및 확률적 오류 문제를 해결합니다.

Result: 벤치마크 다중 에이전트 작업에 대한 광범위한 실험 결과, 평균 6.5%의 성능 향상이 나타났으며 자율 워크플로우 신뢰성에 대한 새로운 최첨단 기준을 설정했습니다.

Conclusion: 이러한 접근 방식은 자율 워크플로우 시스템에서 신뢰성을 크게 향상시킬 수 있음을 보여줍니다.

Abstract: Large-scale multi-agent workflows exhibit inherent vulnerability to error
propagation and quality degradation, where downstream agents compound upstream
failures without corrective mechanisms. We introduce COCO (Cognitive Operating
System with Continuous Oversight), a theoretically-grounded framework that
implements asynchronous self-monitoring and adaptive error correction in
multi-agent driven systems. COCO addresses the fundamental trade-off between
quality assurance and computational efficiency through a novel decoupled
architecture that separates error detection from the critical execution path,
achieving $O(1)$ monitoring overhead relative to workflow complexity. COCO
employs three key algorithmic innovations to address systematic and stochastic
errors: (1) Contextual Rollback Mechanism - a stateful restart protocol that
preserves execution history and error diagnostics, enabling informed
re-computation rather than naive retry; (2) Bidirectional Reflection Protocol -
a mutual validation system between monitoring and execution modules that
prevents oscillatory behavior and ensures convergence; (3) Heterogeneous
Cross-Validation - leveraging model diversity to detect systematic biases and
hallucinations through ensemble disagreement metrics. Extensive experiments on
benchmark multi-agent tasks demonstrate 6.5\% average performance improvement,
establishing new state-of-the-art for autonomous workflow reliability.

</details>


### [121] [The Multi-Stage Assignment Problem: A Fairness Perspective](https://arxiv.org/abs/2508.13856)
*Vibulan J,Swapnil Dhamal,Shweta Jain*

Main category: cs.MA

TL;DR: 이 논문은 다단계 그래프에서의 공정한 경로 할당 문제를 탐구하며, 제안한 C-Balance 및 DC-Balance 알고리즘으로 에이전트 간의 불공정성을 줄이는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 다단계 그래프에서 에이전트 간의 경로 할당이 불공정하게 이루어지므로 이를 개선할 필요가 있습니다.

Method: C-Balance와 DC-Balance 알고리즘을 사용하여 경로 할당을 최적화하며 공정성을 최소화합니다.

Result: 제안된 알고리즘이 ILP보다 몇 배 더 빠르게 실행되며, 에이전트 간의 불공정성을 크게 줄입니다.

Conclusion: 이 연구는 다단계 그래프에서 공정한 경로 할당 문제의 알고리즘적 접근을 통해 불공정성을 경감할 수 있음을 보여줍니다.

Abstract: This paper explores the problem of fair assignment on Multi-Stage graphs. A
multi-stage graph consists of nodes partitioned into $K$ disjoint sets (stages)
structured as a sequence of weighted bipartite graphs formed across adjacent
stages. The goal is to assign node-disjoint paths to $n$ agents starting from
the first stage and ending in the last stage. We show that an efficient
assignment that minimizes the overall sum of costs of all the agents' paths may
be highly unfair and lead to significant cost disparities (envy) among the
agents. We further show that finding an envy-minimizing assignment on a
multi-stage graph is NP-hard. We propose the C-Balance algorithm, which
guarantees envy that is bounded by $2M$ in the case of two agents, where $M$ is
the maximum edge weight. We demonstrate the algorithm's tightness by presenting
an instance where the envy is $2M$. We further show that the cost of fairness
($CoF$), defined as the ratio of the cost of the assignment given by the fair
algorithm to that of the minimum cost assignment, is bounded by $2$ for
C-Balance. We then extend this approach to $n$ agents by proposing the
DC-Balance algorithm that makes iterative calls to C-Balance. We show the
convergence of DC-Balance, resulting in envy that is arbitrarily close to $2M$.
We derive $CoF$ bounds for DC-Balance and provide insights about its dependency
on the instance-specific parameters and the desired degree of envy. We
experimentally show that our algorithm runs several orders of magnitude faster
than a suitably formulated ILP.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [122] [Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions](https://arxiv.org/abs/2508.13214)
*Xuyang Guo,Zekai Huang,Zhao Song,Jiahao Zhang*

Main category: cs.CR

TL;DR: 대형 언어 모델(LLM)은 복잡한 추론과 제로샷 일반화에서 강력한 능력을 보여주지만, 프롬프트 주입 공격에 대한 강건성이 우려됩니다. 이 연구에서는 기본 산술 문제를 통해 LLM이 쉽게 오도될 수 있는지를 조사하였습니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 교육 및 데이터 품질 평가에서 판사 역할을 수행할 수 있는 잠재력이 있지만, 프롬프트 주입 공격에 대한 취약성이 우려됩니다.

Method: 기본 산술 질문을 다중 선택 또는 참/거짓 문제로 제공하며 PDF 파일에 숨겨진 프롬프트를 주입하여 LLM을 평가했습니다.

Result: LLM은 사소한 시나리오에서도 숨겨진 프롬프트 주입 공격에 취약하다는 결과를 보여주었습니다.

Conclusion: 이 연구는 LLM-as-a-judge 응용 프로그램에 대한 심각한 강건성 위험을 강조합니다.

Abstract: Large Language Models (LLMs) have recently demonstrated strong emergent
abilities in complex reasoning and zero-shot generalization, showing
unprecedented potential for LLM-as-a-judge applications in education, peer
review, and data quality evaluation. However, their robustness under prompt
injection attacks, where malicious instructions are embedded into the content
to manipulate outputs, remains a significant concern. In this work, we explore
a frustratingly simple yet effective attack setting to test whether LLMs can be
easily misled. Specifically, we evaluate LLMs on basic arithmetic questions
(e.g., "What is 3 + 2?") presented as either multiple-choice or true-false
judgment problems within PDF files, where hidden prompts are injected into the
file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt
injection attacks, even in these trivial scenarios, highlighting serious
robustness risks for LLM-as-a-judge applications.

</details>


### [123] [MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](https://arxiv.org/abs/2508.13220)
*Yixuan Yang,Daoyuan Wu,Yufan Chen*

Main category: cs.CR

TL;DR: 이 논문은 모델 컨텍스트 프로토콜(MCP)의 보안 유형을 체계적으로 분류하고, MCP 보안을 평가하기 위한 종합 보안 벤치마크인 MCPSecBench를 도입한다.


<details>
  <summary>Details</summary>
Motivation: MCP가 LLM 기반 에이전트의 기능을 향상시키지만, 새로운 보안 위험과 공격 표면을 확장한다는 필요성을 제기한다.

Method: MCPSecBench는 프롬프트 데이터셋, MCP 서버, MCP 클라이언트, 공격 스크립트를 통합하여 MCP 보안을 평가하는 모듈화된 벤치마크이다.

Result: 85% 이상의 공격이 적어도 한 플랫폼을 취약하게 만들며, Claude, OpenAI, Cursor에서 핵심 취약점이 발견되었다.

Conclusion: MCPSecBench는 MCP 보안을 평가하는 표준을 정립하고 모든 MCP 계층에서 철저한 테스트를 가능하게 한다.

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications via the Model Context Protocol (MCP), a universal, open standard
for connecting AI agents with data sources and external tools. While MCP
enhances the capabilities of LLM-based agents, it also introduces new security
risks and expands their attack surfaces. In this paper, we present the first
systematic taxonomy of MCP security, identifying 17 attack types across 4
primary attack surfaces. We introduce MCPSecBench, a comprehensive security
benchmark and playground that integrates prompt datasets, MCP servers, MCP
clients, and attack scripts to evaluate these attacks across three major MCP
providers. Our benchmark is modular and extensible, allowing researchers to
incorporate custom implementations of clients, servers, and transport protocols
for systematic security assessment. Experimental results show that over 85% of
the identified attacks successfully compromise at least one platform, with core
vulnerabilities universally affecting Claude, OpenAI, and Cursor, while
prompt-based and tool-centric attacks exhibit considerable variability across
different hosts and models. Overall, MCPSecBench standardizes the evaluation of
MCP security and enables rigorous testing across all MCP layers.

</details>


### [124] [Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis](https://arxiv.org/abs/2508.13240)
*Soham Hans,Nikolos Gurney,Stacy Marsella,Sofia Hirschmann*

Main category: cs.CR

TL;DR: 이 논문은 해커의 행동에서 손실 회피의 인지 편향을 추출하기 위해 대규모 언어 모델(LLM)을 활용하는 새로운 방법론을 제시하고 있다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안에서 알려지지 않은 적에 대한 방어는 아주 중요하지만, 인간의 인지 편향을 이해하고 정량화하는 것은 오랜 도전 과제였다.

Method: 해커 행동에 대한 데이터를 수집하고 LLM을 사용하여 관련 행동을 분류하고 해커가 사용하는 정의된 지속성 메커니즘과 상관 관계를 분석한다.

Result: 분석을 통해 손실 회피가 해커의 의사 결정에서 어떻게 나타나는지에 대한 새로운 통찰을 제공한다.

Conclusion: LLM은 미세한 행동 패턴을 효과적으로 분석하고 해석할 수 있으며, 이를 통해 실시간 행동 기반 분석을 통한 사이버 방어 전략 강화에 기여할 수 있다.

Abstract: Understanding and quantifying human cognitive biases from empirical data has
long posed a formidable challenge, particularly in cybersecurity, where
defending against unknown adversaries is paramount. Traditional cyber defense
strategies have largely focused on fortification, while some approaches attempt
to anticipate attacker strategies by mapping them to cognitive vulnerabilities,
yet they fall short in dynamically interpreting attacks in progress. In
recognition of this gap, IARPA's ReSCIND program seeks to infer, defend
against, and even exploit attacker cognitive traits. In this paper, we present
a novel methodology that leverages large language models (LLMs) to extract
quantifiable insights into the cognitive bias of loss aversion from hacker
behavior. Our data are collected from an experiment in which hackers were
recruited to attack a controlled demonstration network. We process the hacker
generated notes using LLMs using it to segment the various actions and
correlate the actions to predefined persistence mechanisms used by hackers. By
correlating the implementation of these mechanisms with various operational
triggers, our analysis provides new insights into how loss aversion manifests
in hacker decision-making. The results demonstrate that LLMs can effectively
dissect and interpret nuanced behavioral patterns, thereby offering a
transformative approach to enhancing cyber defense strategies through
real-time, behavior-based analysis.

</details>


### [125] [Involuntary Jailbreak](https://arxiv.org/abs/2508.13246)
*Yangyang Guo,Yangyan Li,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 본 연구에서는 대형 언어 모델(LLMs)에서의 새로운 취약성인 '비자발적 탈옥(involuntary jailbreak)'을 밝혀냅니다. 이 취약성은 특정 공격 목표가 없으며, 단일 보편적 프롬프트만으로도 LLM의 방호 구조 전체를 손상시킬 수 있습니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 방호 장치의 강건성을 재평가하고 미래의 안전성 정렬 강화를 위한 기여를 이끌어내기 위함입니다.

Method: 하나의 보편적 프롬프트를 사용하여 LLM에게 일반적으로 거부될 질문과 그에 대한 심층적인 응답을 생성하도록 지시합니다.

Result: 주요 LLM 대부분, 즉 Claude Opus 4.1, Grok 4, Gemini 2.5 Pro, GPT 4.1에 대해 일관되게 탈옥을 성공시킵니다.

Conclusion: 이 문제는 연구자와 실무자들이 LLM의 방호 강도를 재평가하는 계기를 마련하고, 더 강력한 안전 정렬에 기여하길 희망합니다.

Abstract: In this study, we disclose a worrying new vulnerability in Large Language
Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing
jailbreak attacks, this weakness is distinct in that it does not involve a
specific attack objective, such as generating instructions for \textit{building
a bomb}. Prior attack methods predominantly target localized components of the
LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise
the entire guardrail structure, which our method reveals to be surprisingly
fragile. We merely employ a single universal prompt to achieve this goal. In
particular, we instruct LLMs to generate several questions that would typically
be rejected, along with their corresponding in-depth responses (rather than a
refusal). Remarkably, this simple prompt strategy consistently jailbreaks the
majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,
and GPT 4.1. We hope this problem can motivate researchers and practitioners to
re-evaluate the robustness of LLM guardrails and contribute to stronger safety
alignment in future.

</details>


### [126] [Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design](https://arxiv.org/abs/2508.13357)
*Zhuoran Li,Hanieh Totonchi Asl,Ebrahim Nouri,Yifei Cai,Danella Zhao*

Main category: cs.CR

TL;DR: 본 논문에서는 자원 제약이 있는 디바이스에서 안전한 추론을 가능하게 하는 Silentflow라는 효율적인 프로토콜을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 안전한 다자간 계산(MPC)은 엣지에서 개인 정보 보호 기계 학습의 실질적인 기반을 제공합니다. 특히, 자원 제약이 있는 디바이스에서의 비선형 연산을 지원하는데 필수적입니다.

Method: Silentflow는 Trusted Execution Environment(TEE)를 활용한 프로토콜로, COT 생성을 위한 통신을 제거하고 저 전송 강도 문제를 해결하기 위해 구조화된 알고리즘 분해 기법을 사용합니다.

Result: Silentflow는 최첨단 프로토콜에 비해 최대 39.51배의 속도 향상을 달성했으며, Zynq-7000 SoC에 COT 계산을 오프로드하여 자원 제약 환경에서 ImageNet 데이터셋의 추론을 가속화합니다.

Conclusion: 따라서 Silentflow는 Cryptflow2와 Cheetah에 비해 각각 4.62배 및 3.95배의 속도 향상을 달성합니다.

Abstract: Secure Multi-Party Computation (MPC) offers a practical foundation for
privacy-preserving machine learning at the edge, with MPC commonly employed to
support nonlinear operations. These MPC protocols fundamentally rely on
Oblivious Transfer (OT), particularly Correlated OT (COT), to generate
correlated randomness essential for secure computation. Although COT generation
is efficient in conventional two-party settings with resource-rich
participants, it becomes a critical bottleneck in real-world inference on
resource-constrained devices (e.g., IoT sensors and wearables), due to both
communication latency and limited computational capacity. To enable real-time
secure inference, we introduce Silentflow, a highly efficient Trusted Execution
Environment (TEE)-assisted protocol that eliminates communication in COT
generation. We tackle the core performance bottleneck-low computational
intensity-through structured algorithmic decomposition: kernel fusion for
parallelism, Blocked On-chip eXpansion (BOX) to improve memory access patterns,
and vectorized batch operations to maximize memory bandwidth utilization.
Through design space exploration, we balance end-to-end latency and resource
demands, achieving up to 39.51x speedup over state-of-the-art protocols. By
offloading COT computations to a Zynq-7000 SoC, SilentFlow accelerates PPMLaaS
inference on the ImageNet dataset under resource constraints, achieving a 4.62x
and 3.95x speedup over Cryptflow2 and Cheetah, respectively.

</details>


### [127] [A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources](https://arxiv.org/abs/2508.13364)
*Tadeu Freitas,Carlos Novo,Inês Dutra,João Soares,Manuel Correia,Benham Shariati,Rolando Martins*

Main category: cs.CR

TL;DR: Intrusion Tolerant Systems (ITSs)는 다양한 공격 표면을 악용하는 다중 도메인 적대자의 증가로 인해 중요성이 높아지고 있다. 본 연구에서는 HAL 9000이라는 ITS 위험 관리자가 신규 취약점을 자동으로 예측하고 평가하며, 다양한 정보 출처로부터 데이터를 수집하는 스크래퍼를 통합하여 시스템의 방어 능력을 향상시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 공격 표면을 악용하는 다중 도메인 적대자들이 증가하면서 Intrusion Tolerant Systems (ITSs)의 중요성이 높아지고 있다.

Method: HAL 9000이라는 기존의 ITS 위험 관리자가 신규 취약점을 자동으로 예측하고 평가하기 위해 머신 러닝을 활용하며, 추가적으로 스크래퍼를 사용하여 다양한 위협 출처에서 데이터를 지속적으로 수집한다.

Result: 스크래퍼에서 얻은 정보를 HAL 9000의 위험 관리 프레임워크와 통합하여 emergent threats를 보다 효과적으로 대처할 수 있음을 평가 결과가 보여준다.

Conclusion: 스크래퍼의 통합은 HAL 9000의 방어 능력을 향상시키고, 새로운 위협에 대한 추가 정보를 제공하며, 시스템 관리에 긍정적인 영향을 미친다.

Abstract: Intrusion Tolerant Systems (ITSs) have become increasingly critical due to
the rise of multi-domain adversaries exploiting diverse attack surfaces. ITS
architectures aim to tolerate intrusions, ensuring system compromise is
prevented or mitigated even with adversary presence. Existing ITS solutions
often employ Risk Managers leveraging public security intelligence to adjust
system defenses dynamically against emerging threats. However, these approaches
rely heavily on databases like NVD and ExploitDB, which require manual analysis
for newly discovered vulnerabilities. This dependency limits the system's
responsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager
introduced in our prior work, addressed these challenges through machine
learning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts
and assesses new vulnerabilities automatically. To calculate the risk of a
system, it also incorporates the Exploitability Probability Scoring system to
estimate the likelihood of exploitation within 30 days, enhancing proactive
defense capabilities.
  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a
limitation, considering the availability of other sources of information. This
extended work introduces a custom-built scraper that continuously mines diverse
threat sources, including security advisories, research forums, and real-time
exploit proofs-of-concept. This significantly expands HAL 9000's intelligence
base, enabling earlier detection and assessment of unverified vulnerabilities.
Our evaluation demonstrates that integrating scraper-derived intelligence with
HAL 9000's risk management framework substantially improves its ability to
address emerging threats. This paper details the scraper's integration into the
architecture, its role in providing additional information on new threats, and
the effects on HAL 9000's management.

</details>


### [128] [When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks](https://arxiv.org/abs/2508.13425)
*Mohamed Elmahallawy,Tie Luo*

Main category: cs.CR

TL;DR: LTP-FLEO는 LEO 위성 네트워크를 위한 비동기 연합 학습 프레임워크로, 장기적인 데이터 개인 정보를 보호하고 공정성을 증진하는 기술이다.


<details>
  <summary>Details</summary>
Motivation: LEO 위성 네트워크와 같은 동적이고 자원이 제한된 환경에서 기존의 보안 집계 방법은 클라이언트의 지속적인 가용성을 가정하고 각 통신 라운드에서의 개인 정보 보호만 고려하며, 다수의 라운드를 통한 개인 정보 유출 가능성을 간과한다.

Method: LTP-FLEO는 예측 가능한 가시성을 기반으로 위성을 그룹화하는 개인 정보 인식 위성 분할, 오래된 모델 업데이트의 부정적인 영향을 완화하는 모델 나이 균형 조정 및 서로 다른 가시성 기간의 위성을 공정하게 처리하는 공정한 글로벌 집계를 도입한다.

Result: LTP-FLEO는 다중 라운드 훈련 동안 모델 및 데이터 개인 정보를 효과적으로 보호하고 위성 기여에 따라 공정성을 증진하며 글로벌 수렴을 가속화하고 경쟁력 있는 모델 정확도를 달성한다.

Conclusion: LTP-FLEO는 LEO 위성 네트워크에서 연합 학습에 대한 새로운 접근 방식을 제시하며 보안과 공정성을 모두 향상시킨다.

Abstract: Secure aggregation is a common technique in federated learning (FL) for
protecting data privacy from both curious internal entities (clients or server)
and external adversaries (eavesdroppers). However, in dynamic and
resource-constrained environments such as low Earth orbit (LEO) satellite
networks, traditional secure aggregation methods fall short in two aspects: (1)
they assume continuous client availability while LEO satellite visibility is
intermittent and irregular; (2) they consider privacy in each communication
round but have overlooked the possible privacy leakage through multiple rounds.
To address these limitations, we propose LTP-FLEO, an asynchronous FL framework
that preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO
introduces (i) privacy-aware satellite partitioning, which groups satellites
based on their predictable visibility to the server and enforces joint
participation; (ii) model age balancing, which mitigates the adverse impact of
stale model updates; and (iii) fair global aggregation, which treats satellites
of different visibility durations in an equitable manner. Theoretical analysis
and empirical validation demonstrate that LTP-FLEO effectively safeguards both
model and data privacy across multi-round training, promotes fairness in line
with satellite contributions, accelerates global convergence, and achieves
competitive model accuracy.

</details>


### [129] [Beneath the Mask: Can Contribution Data Unveil Malicious Personas in Open-Source Projects?](https://arxiv.org/abs/2508.13453)
*Ruby Nealon*

Main category: cs.CR

TL;DR: GitHub 사용자 "JiaT75"가 XZ Utils 프로젝트에 숨겨진 백도어를 포함한 버전을 합친 사례를 분석하고, 이로 인한 이상 행동을 감지하기 위한 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 오픈소스 프로젝트에 대한 기여자가 보여주는 이상 행동을 모니터링하고 식별할 수 있는 도구의 필요성.

Method: GitHub 기여로부터 수집된 오픈 소스 인텔리전스(OSINT) 데이터를 그래프 데이터베이스와 그래프 이론을 사용해 분석한다.

Result: JiaT75 인물의 이상 행동을 다른 오픈소스 프로젝트에서 효율적으로 식별할 수 있음을 보여준다.

Conclusion: 이 연구는 오픈소스 기여자가 보여주는 이상 행동을 탐지하기 위한 새로운 방법을 제공한다.

Abstract: In February 2024, after building trust over two years with project
maintainers by making a significant volume of legitimate contributions, GitHub
user "JiaT75" self-merged a version of the XZ Utils project containing a highly
sophisticated, well-disguised backdoor targeting sshd processes running on
systems with the backdoored package installed. A month later, this package
began to be distributed with popular Linux distributions until a Microsoft
employee discovered the backdoor while investigating how a recent system
upgrade impacted the performance of SSH authentication. Despite its potential
global impact, no tooling exists for monitoring and identifying anomalous
behavior by personas contributing to other open-source projects. This paper
demonstrates how Open Source Intelligence (OSINT) data gathered from GitHub
contributions, analyzed using graph databases and graph theory, can efficiently
identify anomalous behaviors exhibited by the "JiaT75" persona across other
open-source projects.

</details>


### [130] [Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security](https://arxiv.org/abs/2508.13520)
*Takreem Haider*

Main category: cs.CR

TL;DR: 이 논문에서는 엘리프틱 곡선 암호화(ECC)의 스칼라 생성 방법을 최적화하여 암호학적인 공격에 대한 저항력을 높이는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자원 제한 환경에서 낮은 엔트로피 스칼라로 인한 암호 공격에 대한 취약성을 해결하기 위해.

Method: 차별적 진화(DE) 알고리즘을 사용하여 스칼라의 이진 표현이 최적의 엔트로피를 나타내도록 생성하는 방법.

Result: DE 최적화된 스칼라가 전통적으로 생성된 스칼라보다 현저히 높은 엔트로피를 달성함을 실험적으로 입증하였습니다.

Conclusion: 제안된 방법은 기존 ECC 기반 프로토콜에 통합될 수 있으며, 블록체인, 보안 메시징, IoT 및 기타 자원 제약 환경에 이상적인 결정론적이고 조정 가능한 대안을 제공합니다.

Abstract: Elliptic Curve Cryptography (ECC) is a fundamental component of modern
public-key cryptosystems that enable efficient and secure digital signatures,
key exchanges, and encryption. Its core operation, scalar multiplication,
denoted as $k \cdot P$, where $P$ is a base point and $k$ is a private scalar,
relies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$
is selected using user input or pseudorandom number generators. However, in
resource-constrained environments with weak entropy sources, these approaches
may yield low-entropy or biased scalars, increasing susceptibility to
side-channel and key recovery attacks. To mitigate these vulnerabilities, we
introduce an optimization-driven scalar generation method that explicitly
maximizes bit-level entropy. Our approach uses differential evolution (DE), a
population-based metaheuristic algorithm, to search for scalars whose binary
representations exhibit maximal entropy, defined by an even and statistically
uniform distribution of ones and zeros. This reformulation of scalar selection
as an entropy-optimization problem enhances resistance to entropy-based
cryptanalytic techniques and improves overall unpredictability. Experimental
results demonstrate that DE-optimized scalars achieve entropy significantly
higher than conventionally generated scalars. The proposed method can be
integrated into existing ECC-based protocols, offering a deterministic, tunable
alternative to traditional randomness, ideal for applications in blockchain,
secure messaging, IoT, and other resource-constrained environments.

</details>


### [131] [CAI Fluency: A Framework for Cybersecurity AI Fluency](https://arxiv.org/abs/2508.13588)
*Víctor Mayoral-Vilches,Jasmin Wachter,Cristóbal R. J. Veas Chavez,Cathrin Schachner,Luis Javier Navarrete-Lozano,María Sanz-Gómez*

Main category: cs.CR

TL;DR: 이 연구는 사이버 보안 AI 도구의 지식과 활용을 민주화하기 위해 CAI Fluency라는 교육 플랫폼을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: CAI 프레임워크의 주요 목표는 인공지능 기반 사이버 보안 솔루션의 광범위한 채택과 효과적인 사용을 가속화하는 것입니다.

Method: CAI Fluency는 AI 유창성 프레임워크를 기반으로 하여 사이버 보안 애플리케이션을 위해 세 가지 인간-AI 상호작용 방식과 네 가지 핵심 역량을 조정합니다.

Result: 이론적 기초는 실무자들이 기술적 기술뿐만 아니라 보안 맥락에서 책임감 있는 AI 사용을 위한 비판적 사고와 윤리적 인식을 개발하도록 보장합니다.

Conclusion: 이 기술 보고서는 CAI 프레임워크의 원칙을 이해하고 이를 프로젝트와 실제 보안 맥락에서 적용하는 방법을 교육하는 데 도움이 되는 백서 및 자세한 교육 및 실용 가이드 역할을 합니다.

Abstract: This work introduces CAI Fluency, an an educational platform of the
Cybersecurity AI (CAI) framework dedicated to democratizing the knowledge and
application of cybersecurity AI tools in the global security community. The
main objective of the CAI framework is to accelerate the widespread adoption
and effective use of artificial intelligence-based cybersecurity solutions,
pathing the way to vibe-hacking, the cybersecurity analogon to vibe-coding.
  CAI Fluency builds upon the Framework for AI Fluency, adapting its three
modalities of human-AI interaction and four core competencies specifically for
cybersecurity applications. This theoretical foundation ensures that
practitioners develop not just technical skills, but also the critical thinking
and ethical awareness necessary for responsible AI use in security contexts.
  This technical report serves as a white-paper, as well as detailed
educational and practical guide that helps users understand the principles
behind the CAI framework, and educates them how to apply this knowledge in
their projects and real-world security contexts.

</details>


### [132] [Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems](https://arxiv.org/abs/2508.13644)
*Viktoria Koscinski,Mark Nelson,Ahmet Okutan,Robert Falso,Mehdi Mirakhorli*

Main category: cs.CR

TL;DR: 소프트웨어 취약점을 정확하게 평가하는 것은 우선순위 정리 및 수정에 필수적이다. 본 연구는 네 가지 취약점 평가 시스템을 대규모로 비교하여 평가의 불일치를 밝힌다.


<details>
  <summary>Details</summary>
Motivation: 소프트웨어 취약점에 대한 정확한 평가가 위험 기반 결정에 필수적이지만, 기존의 다양한 스코어링 시스템이 일관되지 않은 우선순위 결정을 초래하고 있다.

Method: CVSS, SSVC, EPSS 및 Exploitability Index의 네 가지 공개 취약점 스코어링 시스템의 대규모 비교를 수행하였다.

Result: 600개의 실제 취약점 데이터셋을 사용하여 스코어간의 관계를 조사하였고, 각 스코어가 취약점 관리에 어떻게 기여하는지를 평가하였다.

Conclusion: 스코어링 시스템 간의 중요한 차이점이 밝혀졌으며, 이는 데이터 기반의 위험 기반 결정을 내리는 조직에 중대한 영향을 미친다.

Abstract: Accurately assessing software vulnerabilities is essential for effective
prioritization and remediation. While various scoring systems exist to support
this task, their differing goals, methodologies and outputs often lead to
inconsistent prioritization decisions. This work provides the first
large-scale, outcome-linked empirical comparison of four publicly available
vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),
the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit
Prediction Scoring System (EPSS), and the Exploitability Index. We use a
dataset of 600 real-world vulnerabilities derived from four months of
Microsoft's Patch Tuesday disclosures to investigate the relationships between
these scores, evaluate how they support vulnerability management task, how
these scores categorize vulnerabilities across triage tiers, and assess their
ability to capture the real-world exploitation risk. Our findings reveal
significant disparities in how scoring systems rank the same vulnerabilities,
with implications for organizations relying on these metrics to make
data-driven, risk-based decisions. We provide insights into the alignment and
divergence of these systems, highlighting the need for more transparent and
consistent exploitability, risk, and severity assessments.

</details>


### [133] [Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG](https://arxiv.org/abs/2508.13690)
*Wei Shao,Zequan Liang,Ruoyu Zhang,Ruijie Fang,Ning Miao,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun,Chongzhou Fang*

Main category: cs.CR

TL;DR: 생리 신호를 이용한 생체 인증 기술은 착용 가능한 장치에서의 안전하고 사용자 친화적인 접근 제어를 위한 유망한 경로를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 착용 가능한 장치에서 안전하고 사용자 친화적인 접근 제어를 제공하기 위해 생리 신호를 이용한 생체 인증이 필요하다.

Method: 우리는 저주파(25 Hz) 다채널 PPG 신호를 사용하여 스마트워치에 대한 연속 인증 시스템인 We-Be Band를 처음으로 구현하고 평가했다. Bi-LSTM과 주의 메커니즘을 활용하여 4채널 PPG의 짧은(4초) 윈도우에서 신원 특정 특징을 추출하였다.

Result: 우리는 공공 데이터셋(PTTPPG)과 We-Be Dataset(26명 피험자)을 사용한 광범위한 평가를 통해 평균 테스트 정확도 88.11%, 매크로 F1 점수 0.88, 잘못된 수용 비율(FAR) 0.48%, 잘못된 거부 비율(FRR) 11.77%, 동등 오류 비율(EER) 2.76%의 강력한 분류 성능을 입증하였다. 우리의 25 Hz 시스템은 성능 저하 없이 512 Hz 대비 53%, 128 Hz 대비 19%의 센서 전력 소비를 줄였다.

Conclusion: 우리는 25 Hz 샘플링이 인증 정확도를 유지한다는 것을 발견했으며, 20 Hz에서는 성능이 급격히 떨어지면서 전력 절약이 미미하다는 점을 강조했다. 또한, 활동 다양성 훈련이 생리적 상태 전반에 걸쳐 강건성을 향상시킬 수 있음을 발견했다.

Abstract: Biometric authentication using physiological signals offers a promising path
toward secure and user-friendly access control in wearable devices. While
electrocardiogram (ECG) signals have shown high discriminability, their
intrusive sensing requirements and discontinuous acquisition limit
practicality. Photoplethysmography (PPG), on the other hand, enables
continuous, non-intrusive authentication with seamless integration into
wrist-worn wearable devices. However, most prior work relies on high-frequency
PPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy
and computational overhead, impeding deployment in power-constrained real-world
systems. In this paper, we present the first real-world implementation and
evaluation of a continuous authentication system on a smartwatch, We-Be Band,
using low-frequency (25 Hz) multi-channel PPG signals. Our method employs a
Bi-LSTM with attention mechanism to extract identity-specific features from
short (4 s) windows of 4-channel PPG. Through extensive evaluations on both
public datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate
strong classification performance with an average test accuracy of 88.11%,
macro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection
Rate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system
reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to
128 Hz setups without compromising performance. We find that sampling at 25 Hz
preserves authentication accuracy, whereas performance drops sharply at 20 Hz
while offering only trivial additional power savings, underscoring 25 Hz as the
practical lower bound. Additionally, we find that models trained exclusively on
resting data fail under motion, while activity-diverse training improves
robustness across physiological states.

</details>


### [134] [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://arxiv.org/abs/2508.13730)
*Daniel M. Jimenez-Gutierrez,Yelizaveta Falkouskaya,Jose L. Hernandez-Ramos,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.CR

TL;DR: 본 논문은 연합 학습(FL)의 최신 공격 및 방어 메커니즘에 대한 포괄적인 개요를 제공하며, 200개 이상의 관련 연구를 분류하고 분석한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습은 데이터 공유 없이 여러 클라이언트가 글로벌 모델을 협력적으로 훈련할 수 있도록 하는 새로운 기계 학습 패러다임이다.

Method: 200개 이상의 논문을 분석하고 최신 공격 및 방어 메커니즘을 보안 강화 및 프라이버시 보호 기술로 분류하였다.

Result: 기존 방법의 강점과 한계를 비판적으로 분석하고, 프라이버시, 보안 및 모델 성능 간의 균형과 비IID 데이터 분포의 영향을 논의하였다.

Conclusion: 연구자는 스케일 가능하고 적응력이 있으며 에너지 효율적인 솔루션의 필요성을 포함하여 향후 연구 과제를 식별하고 FL 환경에서의 발전을 촉진할 것을 목표로 한다.

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm
enabling multiple clients to train a global model collaboratively without
sharing their raw data. While FL enhances data privacy by design, it remains
vulnerable to various security and privacy threats. This survey provides a
comprehensive overview of more than 200 papers regarding the state-of-the-art
attacks and defense mechanisms developed to address these challenges,
categorizing them into security-enhancing and privacy-preserving techniques.
Security-enhancing methods aim to improve FL robustness against malicious
behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same
time, privacy-preserving techniques focus on protecting sensitive data through
cryptographic approaches, differential privacy, and secure aggregation. We
critically analyze the strengths and limitations of existing methods, highlight
the trade-offs between privacy, security, and model performance, and discuss
the implications of non-IID data distributions on the effectiveness of these
defenses. Furthermore, we identify open research challenges and future
directions, including the need for scalable, adaptive, and energy-efficient
solutions operating in dynamic and heterogeneous FL environments. Our survey
aims to guide researchers and practitioners in developing robust and
privacy-preserving FL systems, fostering advancements safeguarding
collaborative learning frameworks' integrity and confidentiality.

</details>


### [135] [NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js](https://arxiv.org/abs/2508.13750)
*Eric Cornelissen,Musard Balliu*

Main category: cs.CR

TL;DR: NodeShield는 Node.js 애플리케이션의 의존성 계층을 강제하고 시스템 자원에 대한 접근을 제어하는 실행 시간 보호 메커니즘이다.


<details>
  <summary>Details</summary>
Motivation: 소프트웨어 공급망은 악의적인 공격자가 접근하는 일반적인 경로가 되고 있으며, Node.js 생태계는 그 규모와 보편성으로 인해 다양한 공격의 대상이 되고 있다.

Method: NodeShield라는 보호 메커니즘을 설계하고 구현하여 애플리케이션의 의존성 계층을 강제하고 실행 시간에 시스템 자원에 대한 접근을 제어한다. 우리는 SBOM 표준을 사용하여 의존성 계층의 진실한 출처로 삼고, 이를 통해 선언되지 않은 구성 요소의 은밀한 남용을 방지한다. 또한, 각 구성 요소가 접근할 수 있는 시스템 자원의 집합을 나타내는 능력 개념으로 SBOM을 개선하자고 제안한다. 이 제안된 SBOM 확장은 각 구성 요소의 필수 능력을 기록하는 CBOM이라는 능력 자재 명세서이다.

Result: NodeShield는 67개의 알려진 공급망 공격 중 98% 이상을 차단할 수 있으며, 서버에 대한 오버헤드는 요청당 1ms 미만으로 최소화된다.

Conclusion: NodeShield는 원래 코드나 Node.js 런타임에 수정 없이 코드 아웃라인을 통해 SBOM과 CBOM을 실행 시간에 강제함으로써 예기치 않거나 악의적인 행동을 방지할 수 있다.

Abstract: The software supply chain is an increasingly common attack vector for
malicious actors. The Node.js ecosystem has been subject to a wide array of
attacks, likely due to its size and prevalence. To counter such attacks, the
research community and practitioners have proposed a range of static and
dynamic mechanisms, including process- and language-level sandboxing,
permission systems, and taint tracking. Drawing on valuable insight from these
works, this paper studies a runtime protection mechanism for (the supply chain
of) Node.js applications with the ambitious goals of compatibility, automation,
minimal overhead, and policy conciseness.
  Specifically, we design, implement and evaluate NodeShield, a protection
mechanism for Node.js that enforces an application's dependency hierarchy and
controls access to system resources at runtime. We leverage the up-and-coming
SBOM standard as the source of truth for the dependency hierarchy of the
application, thus preventing components from stealthily abusing undeclared
components. We propose to enhance the SBOM with a notion of capabilities that
represents a set of related system resources a component may access. Our
proposed SBOM extension, the Capability Bill of Materials or CBOM, records the
required capabilities of each component, providing valuable insight into the
potential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime
via code outlining (as opposed to inlining) with no modifications to the
original code or Node.js runtime, thus preventing unexpected, potentially
malicious behavior. Our evaluation shows that NodeShield can prevent over 98%
out of 67 known supply chain attacks while incurring minimal overhead on
servers at less than 1ms per request. We achieve this while maintaining broad
compatibility with vanilla Node.js and a concise policy language that consists
of at most 7 entries per dependency.

</details>


### [136] [Red Teaming Methodology for Design Obfuscation](https://arxiv.org/abs/2508.13965)
*Yuntao Liu,Abir Akib,Zelin Lu,Qian Xu,Ankur Srivastava,Gang Qu,David Kehlet,Nij Dorairaj*

Main category: cs.CR

TL;DR: 설계 난독화 방법의 보안을 평가하기 위한 체계적인 접근 방식을 제공하며, 원래 설계의 구조에 관한 정보가 더 많이 유출된다는 것을 보여주는 연구.


<details>
  <summary>Details</summary>
Motivation: VLSI 공급망의 신뢰할 수 없는 당사자로부터 민감한 설계 세부 정보를 보호하기 위한 필요성.

Method: 설계 난독화 접근법의 보안을 평가하기 위한 공격 시나리오를 제시하고, 공격자가 작동하는 칩에 접근하지 못하는 경우를 위한 보안 메트릭과 평가 방법론을 제안한다.

Result: 플로리다 대학교에서 개발한 RIPPER 도구에 대한 사례 연구를 통해 일반적으로 고려되는 것보다 원래 설계의 구조에 대한 정보가 더 많이 유출된다는 결과를 얻었다.

Conclusion: 설계 난독화가 제공하는 보안이 기존의 이해보다 더 취약할 수 있음을 나타낸다.

Abstract: The main goal of design obfuscation schemes is to protect sensitive design
details from untrusted parties in the VLSI supply chain, including but not
limited to off-shore foundries and untrusted end users. In this work, we
provide a systematic red teaming approach to evaluate the security of design
obfuscation approaches. Specifically, we propose security metrics and
evaluation methodology for the scenarios where the adversary does not have
access to a working chip. A case study on the RIPPER tool developed by the
University of Florida indicates that more information is leaked about the
structure of the original design than commonly considered.

</details>
