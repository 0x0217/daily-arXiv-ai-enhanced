<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 75]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 24]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation](https://arxiv.org/abs/2508.10494)
*Jiulin Li,Ping Huang,Yexin Li,Shuo Chen,Juewen Hu,Ye Tian*

Main category: cs.LG

TL;DR: MAGUS는 다중 모달 이해 및 생성을 통합하는 모듈형 프레임워크로, 유연성과 확장성을 갖춘 혁신적인 솔루션을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 실제 세계의 다중 모달 애플리케이션에서는 텍스트, 이미지, 오디오 및 비디오를 포함한 모달 간의 이해 및 생성을 가능하게 하는 any-to-any 기능이 요구됩니다.

Method: MAGUS는 인지(Cognition) 및 심의(Deliberation)라는 두 가지 분리된 단계를 통해 다중 모달 이해 및 생성을 통합하는 모듈형 프레임워크입니다. 인지 단계에서는 역할 조건에 따른 세 가지 다중 모달 LLM 에이전트(Perceiver, Planner, Reflector)가 구조화된 이해 및 계획을 수행하기 위해 협력적인 대화에 참여합니다. 심의 단계에서는 LLM 기반의 추론과 확산 기반 생성이 상호 강화되는 방식으로 조율되는 Growth-Aware Search 메커니즘을 통합합니다.

Result: 여러 벤치마크에 걸친 실험 결과, MAGUS는 강력한 기준선 및 최신 시스템을 능가하는 성능을 나타냅니다. 특히, MME 벤치마크에서는 MAGUS가 강력한 폐쇄 소스 모델인 GPT-4o를 초월합니다.

Conclusion: MAGUS는 플러그 앤 플레이 확장성, 확장 가능한 모달 변환 및 의미적 정렬을 지원하며 공동 훈련 없이도 작동합니다.

Abstract: Real-world multimodal applications often require any-to-any capabilities,
enabling both understanding and generation across modalities including text,
image, audio, and video. However, integrating the strengths of autoregressive
language models (LLMs) for reasoning and diffusion models for high-fidelity
generation remains challenging. Existing approaches rely on rigid pipelines or
tightly coupled architectures, limiting flexibility and scalability. We propose
MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that
unifies multimodal understanding and generation via two decoupled phases:
Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration
within a shared textual workspace. In the Cognition phase, three
role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -
engage in collaborative dialogue to perform structured understanding and
planning. The Deliberation phase incorporates a Growth-Aware Search mechanism
that orchestrates LLM-based reasoning and diffusion-based generation in a
mutually reinforcing manner. MAGUS supports plug-and-play extensibility,
scalable any-to-any modality conversion, and semantic alignment - all without
the need for joint training. Experiments across multiple benchmarks, including
image, video, and audio generation, as well as cross-modal instruction
following, demonstrate that MAGUS outperforms strong baselines and
state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the
powerful closed-source model GPT-4o.

</details>


### [2] [OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services](https://arxiv.org/abs/2508.09992)
*Daniel Groos*

Main category: cs.LG

TL;DR: 이 논문은 공공 데이터를 기반으로 한 오픈 소스 판타지 프리미어 리그 예측 방법인 OpenFPL을 제시하여 선수 성과에 대한 고도로 정확한 예측에 대한 접근성을 민주화하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 판타지 프리미어 리그는 참가자들이 최고의 선수를 선택하도록 유도하며, 정확한 성과 예측은 참가자에게 경쟁에서 우위를 점할 수 있게 한다.

Method: OpenFPL은 네 개의 시즌(2020-21~2023-24) 동안의 Fantasy Premier League 및 Understat 데이터를 기반으로 최적화된 포지션 별 앙상블 모델로 구성된다.

Result: OpenFPL은 2024-25 시즌 데이터를 사용한 예측 테스트에서 상업적으로 저명한 서비스와 유사한 정확도를 달성하였다. 또한 높은 수익 선수에 대한 상업적 벤치마크를 초과하였다.

Conclusion: 이 결과는 장기적인 이적 및 전략 계획을 지원하며 최종 결정에 대한 정보를 제공한다.

Abstract: Fantasy Premier League engages the football community in selecting the
Premier League players who will perform best from gameweek to gameweek. Access
to accurate performance forecasts gives participants an edge over competitors
by guiding expectations about player outcomes and reducing uncertainty in squad
selection. However, high-accuracy forecasts are currently limited to commercial
services whose inner workings are undisclosed and that rely on proprietary
data. This paper aims to democratize access to highly accurate forecasts of
player performance by presenting OpenFPL, an open-source Fantasy Premier League
forecasting method developed exclusively from public data. Comprising
position-specific ensemble models optimized on Fantasy Premier League and
Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL
achieves accuracy comparable to a leading commercial service when tested
prospectively on data from the 2024-25 season. OpenFPL also surpasses the
commercial benchmark for high-return players ($>$ 2 points), which are most
influential for rank gains. These findings hold across one-, two-, and
three-gameweek forecast horizons, supporting long-term planning of transfers
and strategies while also informing final-day decisions.

</details>


### [3] [xRFM: Accurate, scalable, and interpretable feature learning models for tabular data](https://arxiv.org/abs/2508.10053)
*Daniel Beaglehole,David Holzmüller,Adityanarayanan Radhakrishnan,Mikhail Belkin*

Main category: cs.LG

TL;DR: xRFM은 데이터의 지역 구조에 적응하고 대량의 학습 데이터에 확장할 수 있는 기능 학습 커널 기계와 트리 구조를 결합한 알고리즘으로, $31$개의 다른 방법에 비해 최고의 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 표 형식의 데이터에서의 추론은 현대 기술과 과학의 기초이며, 이 예측 작업에 대한 최선의 실천은 거의 변화가 없다.

Method: xRFM은 기능 학습 커널 기계와 트리 구조를 결합한 알고리즘이다.

Result: xRFM은 $100$개의 회귀 데이터셋에서 최고의 성능을 달성하고, $200$개의 분류 데이터셋에서도 경쟁력이 있어 GBDTs를 능가한다.

Conclusion: xRFM은 Average Gradient Outer Product를 통해 해석 가능성을 본래 제공한다.

Abstract: Inference from tabular data, collections of continuous and categorical
variables organized into matrices, is a foundation for modern technology and
science. Yet, in contrast to the explosive changes in the rest of AI, the best
practice for these predictive tasks has been relatively unchanged and is still
primarily based on variations of Gradient Boosted Decision Trees (GBDTs). Very
recently, there has been renewed interest in developing state-of-the-art
methods for tabular data based on recent developments in neural networks and
feature learning methods. In this work, we introduce xRFM, an algorithm that
combines feature learning kernel machines with a tree structure to both adapt
to the local structure of the data and scale to essentially unlimited amounts
of training data.
  We show that compared to $31$ other methods, including recently introduced
tabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performance
across $100$ regression datasets and is competitive to the best methods across
$200$ classification datasets outperforming GBDTs. Additionally, xRFM provides
interpretability natively through the Average Gradient Outer Product.

</details>


### [4] [A Personalized Exercise Assistant using Reinforcement Learning (PEARL): Results from a four-arm Randomized-controlled Trial](https://arxiv.org/abs/2508.10060)
*Amy Armento Lee,Narayan Hegde,Nina Deliu,Emily Rosenzweig,Arun Suggala,Sriram Lakshminarasimhan,Qian He,John Hernandez,Martin Seneviratne,Rahul Singh,Pradnesh Kalkar,Karthikeyan Shanmugam,Aravindan Raghuveer,Abhimanyu Singh,My Nguyen,James Taylor,Jatin Alla,Sofia S. Villar,Hulya Emir-Farinas*

Main category: cs.LG

TL;DR: PEARL 연구는 Fitbit 앱을 통해 개인화된 신체 활동 유도 내용을 제공하는 강화 학습 알고리즘을 평가한 첫 번째 대규모 임상 시험으로, 신체 활동을 증가시키는 데 성과를 보였다.


<details>
  <summary>Details</summary>
Motivation: 신체 활동 부족은 글로벌 건강 문제로, 개인화된 신체 활동 촉진이 필요하다.

Method: 13,463명의 Fitbit 사용자를 대조군, 무작위군, 고정군, 강화 학습군의 네 그룹으로 나누고, 행동 과학 원칙에 기반한 155개의 유도를 제공했다.

Result: 강화 학습군은 1개월과 2개월 동안 모든 그룹에 비해 신체 활동이 증가했다.

Conclusion: 행동 과학에 기반한 개인화된 디지털 건강 개입을 위한 확장 가능하고 효과적인 강화 학습 접근법의 잠재력을 입증했다.

Abstract: Consistent physical inactivity poses a major global health challenge. Mobile
health (mHealth) interventions, particularly Just-in-Time Adaptive
Interventions (JITAIs), offer a promising avenue for scalable, personalized
physical activity (PA) promotion. However, developing and evaluating such
interventions at scale, while integrating robust behavioral science, presents
methodological hurdles. The PEARL study was the first large-scale, four-arm
randomized controlled trial to assess a reinforcement learning (RL) algorithm,
informed by health behavior change theory, to personalize the content and
timing of PA nudges via a Fitbit app.
  We enrolled and randomized 13,463 Fitbit users into four study arms: control,
random, fixed, and RL. The control arm received no nudges. The other three arms
received nudges from a bank of 155 nudges based on behavioral science
principles. The random arm received nudges selected at random. The fixed arm
received nudges based on a pre-set logic from survey responses about PA
barriers. The RL group received nudges selected by an adaptive RL algorithm. We
included 7,711 participants in primary analyses (mean age 42.1, 86.3% female,
baseline steps 5,618.2).
  We observed an increase in PA for the RL group compared to all other groups
from baseline to 1 and 2 months. The RL group had significantly increased
average daily step count at 1 month compared to all other groups: control (+296
steps, p=0.0002), random (+218 steps, p=0.005), and fixed (+238 steps,
p=0.002). At 2 months, the RL group sustained a significant increase compared
to the control group (+210 steps, p=0.0122). Generalized estimating equation
models also revealed a sustained increase in daily steps in the RL group vs.
control (+208 steps, p=0.002). These findings demonstrate the potential of a
scalable, behaviorally-informed RL approach to personalize digital health
interventions for PA.

</details>


### [5] [Measuring Time Series Forecast Stability for Demand Planning](https://arxiv.org/abs/2508.10063)
*Steven Klee,Yuntian Xia*

Main category: cs.LG

TL;DR: 본 논문은 생산 시스템에서의 수요 예측의 안정성과 일관성을 강조하며, 최신 기계 학습 모델이 안정성을 향상시키면서도 예측 정확도를 유지하거나 개선할 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 예측의 중요성과 기존 모델에 비해 안정성과 일관성이 더 가치 있는 경우가 많음을 강조하기 위함입니다.

Method: 최신 기계 학습 모델(Chronos, DeepAR, PatchTST 등)의 안정성과 정확성을 M5 대회 및 Favorita 식료품 판매와 같은 공공 데이터 세트를 통해 사례 연구하여 측정합니다.

Result: 앙상블 모델이 예측 정확도를 크게 악화시키지 않거나 심지어 개선하면서도 안정성을 향상시켰음을 보여주었습니다.

Conclusion: 생산 시스템에서 배포되는 모델의 예측 안정성에 대한 추가 연구의 필요성을 제안합니다.

Abstract: Time series forecasting is a critical first step in generating demand plans
for supply chains. Experiments on time series models typically focus on
demonstrating improvements in forecast accuracy over existing/baseline
solutions, quantified according to some accuracy metric. There is no doubt that
forecast accuracy is important; however in production systems, demand planners
often value consistency and stability over incremental accuracy improvements.
Assuming that the inputs have not changed significantly, forecasts that vary
drastically from one planning cycle to the next require high amounts of human
intervention, which frustrates demand planners and can even cause them to lose
trust in ML forecasting models. We study model-induced stochasticity, which
quantifies the variance of a set of forecasts produced by a single model when
the set of inputs is fixed. Models with lower variance are more stable.
  Recently the forecasting community has seen significant advances in forecast
accuracy through the development of deep machine learning models for time
series forecasting. We perform a case study measuring the stability and
accuracy of state-of-the-art forecasting models (Chronos, DeepAR, PatchTST,
Temporal Fusion Transformer, TiDE, and the AutoGluon best quality ensemble) on
public data sets from the M5 competition and Favorita grocery sales. We show
that ensemble models improve stability without significantly deteriorating (or
even improving) forecast accuracy. While these results may not be surprising,
the main point of this paper is to propose the need for further study of
forecast stability for models that are being deployed in production systems.

</details>


### [6] [Constrained Decoding of Diffusion LLMs with Context-Free Grammars](https://arxiv.org/abs/2508.10111)
*Niels Mündler,Jasper Dekoninck,Martin Vechev*

Main category: cs.LG

TL;DR: 본 논문에서는 확산 모델의 제약 디코딩을 위한 새로운 방법을 제안하여, 문법적으로 정확한 출력 생성을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLMs)은 다양한 분야에서 좋은 성능을 보이지만, 형식 언어에 명시된 구문 제약을 지키는 것이 필요한 실용적 응용에 한계가 있다.

Method: 본 논문은 제약 디코딩을 더 일반적인 덧셈 보완 문제로 축소한 후, 목표 언어와 정규 언어의 교차점이 비어 있는지 결정하는 문제로 이를 다시 축소하고, 컨텍스트 자유 언어에 대한 효율적인 알고리즘을 제시한다.

Result: C++ 코드 보완 및 JSON 내 구조화된 데이터 추출과 같은 응용에서, 우리의 방법은 거의 완벽한 구문 정확성을 달성하며 기능적 정확성을 지속적으로 유지하거나 개선한다.

Conclusion: 우리의 효율성 최적화는 계산 오버헤드를 실용적으로 유지한다.

Abstract: Large language models (LLMs) have shown promising performance across diverse
domains. Many practical applications of LLMs, such as code completion and
structured data extraction, require adherence to syntactic constraints
specified by a formal language. Yet, due to their probabilistic nature, LLM
output is not guaranteed to adhere to such formal languages. Prior work has
proposed constrained decoding as a means to restrict LLM generation to
particular formal languages. However, existing works are not applicable to the
emerging paradigm of diffusion LLMs, when used in practical scenarios such as
the generation of formally correct C++ or JSON output. In this paper we address
this challenge and present the first constrained decoding method for diffusion
models, one that can handle formal languages captured by context-free grammars.
We begin by reducing constrained decoding to the more general additive
infilling problem, which asks whether a partial output can be completed to a
valid word in the target language. This problem also naturally subsumes the
previously unaddressed multi-region infilling constrained decoding. We then
reduce this problem to the task of deciding whether the intersection of the
target language and a regular language is empty and present an efficient
algorithm to solve it for context-free languages. Empirical results on various
applications, such as C++ code infilling and structured data extraction in
JSON, demonstrate that our method achieves near-perfect syntactic correctness
while consistently preserving or improving functional correctness. Importantly,
our efficiency optimizations ensure that the computational overhead remains
practical.

</details>


### [7] [Less is More: Learning Graph Tasks with Just LLMs](https://arxiv.org/abs/2508.10115)
*Sola Shirai,Kavitha Srinivas,Julian Dolby,Michael Katz,Horst Samulowitz,Shirin Sohrabi*

Main category: cs.LG

TL;DR: 대형 언어 모델(LLMs)은 그래프를 통해 많은 문제를 해결할 수 있는 가능성이 있으며, 연구를 통해 LLM이 그래프 작업을 학습하고 일반화할 수 있는지를 살폈다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델에서 그래프를 통한 추론이 많은 문제 해결에 도움을 줄 수 있다는 점에서, 기존 방식의 장단점을 분석할 필요가 있다.

Method: LLM에 지시적 연쇄적 사고 솔루션을 통해 그래프 작업을 해결하도록 훈련시켰다.

Result: 소규모 LLM도 훈련받은 방식으로 그래프 작업을 해결할 수 있었으며, 새로운 작업과 그래프 구조에도 일반화 가능하였다.

Conclusion: 전문 그래프 인코더 없이도 LLM이 그래프 작업을 해결하고 일반화할 수 있음을 보여준다.

Abstract: For large language models (LLMs), reasoning over graphs could help solve many
problems. Prior work has tried to improve LLM graph reasoning by examining how
best to serialize graphs as text and by combining GNNs and LLMs. However, the
merits of such approaches remain unclear, so we empirically answer the
following research questions: (1) Can LLMs learn to solve fundamental graph
tasks without specialized graph encoding models?, (2) Can LLMs generalize
learned solutions to unseen graph structures or tasks?, and (3) What are the
merits of competing approaches to learn graph tasks? We show that even small
LLMs can learn to solve graph tasks by training them with instructive
chain-of-thought solutions, and this training generalizes, without specialized
graph encoders, to new tasks and graph structures.

</details>


### [8] [From Intent to Execution: Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation](https://arxiv.org/abs/2508.10118)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Mengyang Zhao,Teng Fu,Bin Li,Xiangyang Xue*

Main category: cs.LG

TL;DR: CAD-RL은 CAD 모델링 코드 생성을 위한 새로운 강화 학습 프레임워크로, 설계 의도를 더 효과적으로 실행 가능한 코드로 변환하기 위한 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 현재 CAD 워크플로우는 광범위한 도메인 전문 지식과 수동 모델링 노력을 요구하므로 자동화의 필요성이 있다.

Method: CAD-RL은 CoT 기반의 Cold Start와 목표 지향 강화 학습 후 훈련을 결합하여, 실행 가능성 보상, 기하학적 정확도 보상, 외부 평가 보상이라는 세 가지 작업 특화 보상을 사용한다.

Result: 실험 결과 CAD-RL은 기존 VLM에 비해 추론 품질, 출력 정확도 및 코드 실행 가능성에서显著한 개선을 달성하였다.

Conclusion: CAD-RL은 CAD 모델링의 자동화 가능성을 높일 수 있으며, 제안된 방법론과 데이터셋은 향후 연구에 기여할 것이다.

Abstract: Computer-Aided Design (CAD) plays a vital role in engineering and
manufacturing, yet current CAD workflows require extensive domain expertise and
manual modeling effort. Recent advances in large language models (LLMs) have
made it possible to generate code from natural language, opening new
opportunities for automating parametric 3D modeling. However, directly
translating human design intent into executable CAD code remains highly
challenging, due to the need for logical reasoning, syntactic correctness, and
numerical precision. In this work, we propose CAD-RL, a multimodal
Chain-of-Thought (CoT) guided reinforcement learning post training framework
for CAD modeling code generation. Our method combines CoT-based Cold Start with
goal-driven reinforcement learning post training using three task-specific
rewards: executability reward, geometric accuracy reward, and external
evaluation reward. To ensure stable policy learning under sparse and
high-variance reward conditions, we introduce three targeted optimization
strategies: Trust Region Stretch for improved exploration, Precision Token Loss
for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce
noisy supervision. To support training and benchmarking, we release ExeCAD, a
noval dataset comprising 16,540 real-world CAD examples with paired natural
language and structured design language descriptions, executable CADQuery
scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves
significant improvements in reasoning quality, output precision, and code
executability over existing VLMs.

</details>


### [9] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 이 논문에서는 수학적 추론과 같은 도전적인 도메인에서 LLM의 고급 추론을 개선하기 위해 Nested-ReFT라는 새로운 강 reinforcement fine-tuning 프레임워크를 소개합니다. 이 프레임워크는 훈련 중 행동 모델로서 타겟 모델의 일부 레이어를 사용하여 추론 비용을 줄이고, 편향을 완화하는 세 가지 변형을 탐구합니다.


<details>
  <summary>Details</summary>
Motivation: 코스트가 큰 기존의 ReFT 프레임워크의 단점을 해결하고자 함.

Method: Nested-ReFT 프레임워크에서는 타겟 모델의 일부 레이어가 행동 모델로 작용하여 훈련 중 오프 정책 완성을 생성합니다.

Result: 여러 수학적 추론 벤치마크 및 모델 크기에서 token/sec로 측정한 계산 효율성이 향상됨을 보여줍니다.

Conclusion: Nested-ReFT는 편향된 경향을 최소화하면서도 baseline ReFT 성능에 일치하는 성능을 유지합니다.

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [10] [rETF-semiSL: Semi-Supervised Learning for Neural Collapse in Temporal Data](https://arxiv.org/abs/2508.10147)
*Yuhan Xie,William Cappelletti,Mahsa Shoaran,Pascal Frossard*

Main category: cs.LG

TL;DR: 본 논문에서는 심층 신경망을 위한 신규 반자율 프리트레이닝 전략을 제안하고, 다양한 다변량 시계열 데이터셋에서 이 전략이 이전의 방안들보다 성능이 우수함을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열의 복잡한 패턴을 효과적으로 캡처하기 위해, 심층 신경망은 동적 데이터를 잘 표현해야 합니다.

Method: 우리는 기존의 반자율 학습 방법을 개선하여, Neural Collapse 현상을 만족하는 잠재 표현을 강화하는 새로운 반자율 프리트레이닝 전략을 제안합니다. 이를 위해 회전 등각 타이트 프레임-분류기와 가짜 라벨링을 사용하여 몇 개의 라벨이 붙은 샘플로 심층 인코더를 프리트레이닝합니다. 또한, 임베딩 분리를 강제하면서 시간 동역학을 효과적으로 캡처하기 위해 생성적 전제 작업을 통합하고, 새로운 시퀀스 증강 전략을 정의합니다.

Result: 우리의 방법은 LSTM, 변환기 및 상태 공간 모델에 적용했을 때 이전의 전제 작업들보다 성능이 현저히 우수함을 보였습니다.

Conclusion: 이러한 결과는 이론적으로 기반한 임베딩 기하학과 프리트레이닝 목표를 정렬하는 것이 유익함을 강조합니다.

Abstract: Deep neural networks for time series must capture complex temporal patterns,
to effectively represent dynamic data. Self- and semi-supervised learning
methods show promising results in pre-training large models, which -- when
finetuned for classification -- often outperform their counterparts trained
from scratch. Still, the choice of pretext training tasks is often heuristic
and their transferability to downstream classification is not granted, thus we
propose a novel semi-supervised pre-training strategy to enforce latent
representations that satisfy the Neural Collapse phenomenon observed in
optimally trained neural classifiers. We use a rotational equiangular tight
frame-classifier and pseudo-labeling to pre-train deep encoders with few
labeled samples. Furthermore, to effectively capture temporal dynamics while
enforcing embedding separability, we integrate generative pretext tasks with
our method, and we define a novel sequential augmentation strategy. We show
that our method significantly outperforms previous pretext tasks when applied
to LSTMs, transformers, and state-space models on three multivariate time
series classification datasets. These results highlight the benefit of aligning
pre-training objectives with theoretically grounded embedding geometry.

</details>


### [11] [Out-of-Distribution Detection using Counterfactual Distance](https://arxiv.org/abs/2508.10148)
*Maria Stoica,Francesco Leofante,Alessio Lomuscio*

Main category: cs.LG

TL;DR: 본 논문에서는 카운터팩추얼 설명을 활용한 OOD 데이터 탐지 방법을 제안하고, CIFAR 및 ImageNet 데이터셋에서 우수한 성능을 보임을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 시스템의 안전한 사용을 위해 정확하고 설명 가능한 OOD 탐지가 필요하다.

Method: 입력이 주어지면 카운터팩추얼 설명을 활용하여 결정 경계까지의 거리를 계산하는 사후 OOD 탐지 방법을 제안한다.

Result: CIFAR-10에서 93.50% AUROC 및 25.80% FPR95를, CIFAR-100에서는 97.05% AUROC 및 13.79% FPR95, ImageNet-200에서는 92.55% AUROC 및 33.55% FPR95를 기록하여 기존 방법보다 우수한 성능을 보인다.

Conclusion: 제안한 방법은 최신 기술과 일치하며, OOD 탐지 결과를 해석하는 데 카운터팩추얼 설명을 효과적으로 사용할 수 있다.

Abstract: Accurate and explainable out-of-distribution (OOD) detection is required to
use machine learning systems safely. Previous work has shown that feature
distance to decision boundaries can be used to identify OOD data effectively.
In this paper, we build on this intuition and propose a post-hoc OOD detection
method that, given an input, calculates the distance to decision boundaries by
leveraging counterfactual explanations. Since computing explanations can be
expensive for large architectures, we also propose strategies to improve
scalability by computing counterfactuals directly in embedding space.
Crucially, as the method employs counterfactual explanations, we can seamlessly
use them to help interpret the results of our detector. We show that our method
is in line with the state of the art on CIFAR-10, achieving 93.50% AUROC and
25.80% FPR95. Our method outperforms these methods on CIFAR-100 with 97.05%
AUROC and 13.79% FPR95 and on ImageNet-200 with 92.55% AUROC and 33.55% FPR95
across four OOD datasets

</details>


### [12] [Characterizing Evolution in Expectation-Maximization Estimates for Overspecified Mixed Linear Regression](https://arxiv.org/abs/2508.10154)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 혼합 모델은 실용적인 효과와 이론적 기반으로 인해 많은 주목을 받았다. 그러나 모델 과대표화로 인한 모델 오사양은 여전히 도전 과제이다. 본 논문에서는 두 구성 요소 혼합 선형 회귀의 EM 알고리즘의 행동을 분석하고 이를 수학적으로 구명하였다. 이를 통해 초기 추정치의 균형 상태에 따라 수렴 속도가 다르다는 것을 확인하였다.


<details>
  <summary>Details</summary>
Motivation: 혼합 모델의 실용성과 이론적 기초에 대한 연구를 통해, 모델 오사양 문제를 해결하고자 하는 필요성이 있다.

Method: 혼합 선형 회귀의 비대칭 초기 추정치를 통해 EM 알고리즘의 수렴 성질을 분석하고, 이를 이론적으로 정립하였다.

Result: 초기 추정의 균형 여부에 따라 회귀 매개변수의 수렴 속도가 선형 또는 비선형임을 밝혀내었다. 구체적으로, 비대칭 초기 추정일 때의 수렴 속도는 $O(rac{1}{	ext{n}})$이고, 대칭 초기 추정에서는 $O(rac{1}{	ext{n}^{1/2}})$로 나타났다.

Conclusion: 이는 오사양된 모델 설정과 고유 신호 대 잡음 비율(SNR) 범위에서 분석을 확장하는 데 있어 중요한 기초 자료가 된다.

Abstract: Mixture models have attracted significant attention due to practical
effectiveness and comprehensive theoretical foundations. A persisting challenge
is model misspecification, which occurs when the model to be fitted has more
mixture components than those in the data distribution. In this paper, we
develop a theoretical understanding of the Expectation-Maximization (EM)
algorithm's behavior in the context of targeted model misspecification for
overspecified two-component Mixed Linear Regression (2MLR) with unknown
$d$-dimensional regression parameters and mixing weights. In Theorem 5.1 at the
population level, with an unbalanced initial guess for mixing weights, we
establish linear convergence of regression parameters in $O(\log(1/\epsilon))$
steps. Conversely, with a balanced initial guess for mixing weights, we observe
sublinear convergence in $O(\epsilon^{-2})$ steps to achieve the
$\epsilon$-accuracy at Euclidean distance. In Theorem 6.1 at the finite-sample
level, for mixtures with sufficiently unbalanced fixed mixing weights, we
demonstrate a statistical accuracy of $O((d/n)^{1/2})$, whereas for those with
sufficiently balanced fixed mixing weights, the accuracy is $O((d/n)^{1/4})$
given $n$ data samples. Furthermore, we underscore the connection between our
population level and finite-sample level results: by setting the desired final
accuracy $\epsilon$ in Theorem 5.1 to match that in Theorem 6.1 at the
finite-sample level, namely letting $\epsilon = O((d/n)^{1/2})$ for
sufficiently unbalanced fixed mixing weights and $\epsilon = O((d/n)^{1/4})$
for sufficiently balanced fixed mixing weights, we intuitively derive iteration
complexity bounds $O(\log (1/\epsilon))=O(\log (n/d))$ and
$O(\epsilon^{-2})=O((n/d)^{1/2})$ at the finite-sample level for sufficiently
unbalanced and balanced initial mixing weights. We further extend our analysis
in overspecified setting to low SNR regime.

</details>


### [13] [Benchmark-Driven Selection of AI: Evidence from DeepSeek-R1](https://arxiv.org/abs/2508.10173)
*Petr Spelda,Vit Stritecky*

Main category: cs.LG

TL;DR: 사고 언어 모델의 평가가 중요해지고 있으며, 이 모델들이 기존 능력을 결합하여 작업 완료 전의 중간 단계를 생성할 수 있다는 것이 관찰되었습니다. 이 연구는 영향력 있는 벤치마크를 학습 커리큘럼으로 사용함으로써 성과를 향상시킬 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 사고가 대형 언어 모델의 다음 확장 차원이 됨에 따라, 중요한 작업에서 모델의 능력을 세심하게 연구할 필요가 있다.

Method: 영향력 있는 벤치마크를 학습을 위한 커리큘럼으로 활용하여 DeepSeek-R1 모델에서의 성능을 평가하였다.

Result: 더 나은 성능은 항상 테스트 시간 알고리즘 개선이나 모델 크기 때문만이 아니라, 효과적인 벤치마크를 사용함으로써도 이루어질 수 있다.

Conclusion: 따라서 일부 벤치마크는 보이지 않는 테스트 세트가 아니라 훈련을 위한 커리큘럼으로 볼 수 있다.

Abstract: Evaluation of reasoning language models gained importance after it was
observed that they can combine their existing capabilities into novel traces of
intermediate steps before task completion and that the traces can sometimes
help them to generalize better than past models. As reasoning becomes the next
scaling dimension of large language models, careful study of their capabilities
in critical tasks is needed. We show that better performance is not always
caused by test-time algorithmic improvements or model sizes but also by using
impactful benchmarks as curricula for learning. We call this benchmark-driven
selection of AI and show its effects on DeepSeek-R1 using our sequential
decision-making problem from Humanity's Last Exam. Steering development of AI
by impactful benchmarks trades evaluation for learning and makes novelty of
test tasks key for measuring generalization capabilities of reasoning models.
Consequently, some benchmarks could be seen as curricula for training rather
than unseen test sets.

</details>


### [14] [An Explainable AI based approach for Monitoring Animal Health](https://arxiv.org/abs/2508.10210)
*Rahul Janaa,Shubham Dixit,Mrityunjay Sharma,Ritesh Kumar*

Main category: cs.LG

TL;DR: 이 연구는 데이터 기반 농업 관행을 보여주고, 기계 학습을 통해 축산업에서 가축 건강 모니터링과 수익 최적화를 향상시키기 위한 노력을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 우유 농가가 모든 동물을 추적하기 어려운 문제를 해결하기 위해 축산 건강 모니터링과 수익 최적화가 필요하다.

Method: 3축 가속도계 센서의 지속적인 데이터 수집과 강력한 기계 학습 방법론을 통해 가축 활동에 대한 실행 가능한 정보를 제공하고, 블루투스 기반 IoT 장치와 4G 네트워크를 이용하여 실시간 분석을 수행한다.

Result: k-최근접 이웃 분류기가 가장 뛰어난 성능을 보였으며, AUC는 훈련 세트에서 평균 0.98, 표준 편차 0.0026, 테스트 세트에서 0.99로 나타났다.

Conclusion: 투명성을 보장하기 위해 SHAP와 같은 설명 가능한 AI 프레임워크를 사용하여 주요 특징의 중요성을 해석하고, 지속 가능한 축산 관리에 대한 설명 가능하고 실용적인 기계 학습 모델 개발을 지원한다.

Abstract: Monitoring cattle health and optimizing yield are key challenges faced by
dairy farmers due to difficulties in tracking all animals on the farm. This
work aims to showcase modern data-driven farming practices based on explainable
machine learning(ML) methods that explain the activity and behaviour of dairy
cattle (cows). Continuous data collection of 3-axis accelerometer sensors and
usage of robust ML methodologies and algorithms, provide farmers and
researchers with actionable information on cattle activity, allowing farmers to
make informed decisions and incorporate sustainable practices. This study
utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for
seamless data transmission, immediate analysis, inference generation, and
explains the models performance with explainability frameworks. Special
emphasis is put on the pre-processing of the accelerometers time series data,
including the extraction of statistical characteristics, signal processing
techniques, and lag-based features using the sliding window technique. Various
hyperparameter-optimized ML models are evaluated across varying window lengths
for activity classification. The k-nearest neighbour Classifier achieved the
best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the
training set and 0.99 on testing set). In order to ensure transparency,
Explainable AI based frameworks such as SHAP is used to interpret feature
importance that can be understood and used by practitioners. A detailed
comparison of the important features, along with the stability analysis of
selected features, supports development of explainable and practical ML models
for sustainable livestock management.

</details>


### [15] [AI-Driven Detection and Analysis of Handwriting on Seized Ivory: A Tool to Uncover Criminal Networks in the Illicit Wildlife Trade](https://arxiv.org/abs/2508.10219)
*Will Fein,Ryan J. Horwitz,John E. Brown III,Amit Misra,Felipe Oviedo,Kevin White,Juan M. Lavista Ferres,Samuel K. Wasser*

Main category: cs.LG

TL;DR: AI 기반 방법으로 밀매된 코끼리 상아의 수기 표시를 분석하여 밀매 네트워크의 단서를 제공.


<details>
  <summary>Details</summary>
Motivation: 코끼리 개체수 감소 문제와 밀매 네트워크의 어려운 차단.

Method: AI 기반 파이프라인을 사용하여 압수된 상아의 수기 표시를 추출 및 분석.

Result: 18,000개 이상의 표시를 추출하고 184개의 반복되는 '서명 표시'를 식별.

Conclusion: AI의 잠재력을 활용하여 야생동물 범죄를 중단하는 데 기여할 수 있다.

Abstract: The transnational ivory trade continues to drive the decline of elephant
populations across Africa, and trafficking networks remain difficult to
disrupt. Tusks seized by law enforcement officials carry forensic information
on the traffickers responsible for their export, including DNA evidence and
handwritten markings made by traffickers. For 20 years, analyses of tusk DNA
have identified where elephants were poached and established connections among
shipments of ivory. While the links established using genetic evidence are
extremely conclusive, genetic data is expensive and sometimes impossible to
obtain. But though handwritten markings are easy to photograph, they are rarely
documented or analyzed. Here, we present an AI-driven pipeline for extracting
and analyzing handwritten markings on seized elephant tusks, offering a novel,
scalable, and low-cost source of forensic evidence. Having collected 6,085
photographs from eight large seizures of ivory over a 6-year period
(2014-2019), we used an object detection model to extract over 17,000
individual markings, which were then labeled and described using
state-of-the-art AI tools. We identified 184 recurring "signature markings"
that connect the tusks on which they appear. 20 signature markings were
observed in multiple seizures, establishing forensic links between these
seizures through traffickers involved in both shipments. This work complements
other investigative techniques by filling in gaps where other data sources are
unavailable. The study demonstrates the transformative potential of AI in
wildlife forensics and highlights practical steps for integrating handwriting
analysis into efforts to disrupt organized wildlife crime.

</details>


### [16] [Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine](https://arxiv.org/abs/2508.10228)
*Abdelmoula El Yazizi,Samee U. Khan,Yaroslav Koshka*

Main category: cs.LG

TL;DR: D-Wave 양자 풀림기에 대한 샘플 품질 평가를 위해 로컬 밸리 중심 접근법이 적용되었다. D-Wave의 샘플과 고전적으로 훈련된 RBM의 Gibbs 샘플을 비교한 결과, D-Wave의 샘플은 더 높은 LV 수를 가지지만, 두 기법 간의 지역 최솟값은 다르게 나타났다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 D-Wave 양자 풀림기를 사용한 샘플링의 품질을 평가하기 위해 LV 중심 접근법을 도입하려는 목적이 있다.

Method: D-Wave와 Gibbs 샘플을 얻고, LVs의 수와 관련된 지역 최솟값의 에너지를 비교하였다.

Result: D-Wave의 양자 풀림 시간을 줄여도 LV 수에 유의미한 증가가 없었으며, D-Wave 샘플은 Gibbs 샘플보다 더 많은 LV에 속했지만, 두 기법 간 차이가 많았다.

Conclusion: 이 연구 결과는 D-Wave 기반 샘플링에서 이전 조사가 의미 있는 개선을 달성하지 못한 이유를 설명하며, 고전-양자 결합 접근법을 사용할 가능성을 보여준다.

Abstract: A local-valley (LV) centered approach to assessing the quality of sampling
from Restricted Boltzmann Machines (RBMs) was applied to the latest generation
of the D-Wave quantum annealer. D-Wave and Gibbs samples from a classically
trained RBM were obtained at conditions relevant to the
contrastive-divergence-based RBM learning. The samples were compared for the
number of the LVs to which they belonged and the energy of the corresponding
local minima. No significant (desirable) increase in the number of the LVs has
been achieved by decreasing the D-Wave annealing time. At any training epoch,
the states sampled by the D-Wave belonged to a somewhat higher number of LVs
than in the Gibbs sampling. However, many of those LVs found by the two
techniques differed. For high-probability sampled states, the two techniques
were (unfavorably) less complementary and more overlapping. Nevertheless, many
potentially "important" local minima, i.e., those having intermediate, even if
not high, probability values, were found by only one of the two sampling
techniques while missed by the other. The two techniques overlapped less at
later than earlier training epochs, which is precisely the stage of the
training when modest improvements to the sampling quality could make meaningful
differences for the RBM trainability. The results of this work may explain the
failure of previous investigations to achieve substantial (or any) improvement
when using D-Wave-based sampling. However, the results reveal some potential
for improvement, e.g., using a combined classical-quantum approach.

</details>


### [17] [Interpretable Machine Learning Model for Early Prediction of Acute Kidney Injury in Critically Ill Patients with Cirrhosis: A Retrospective Study](https://arxiv.org/abs/2508.10233)
*Li Sun,Shuheng Chen,Junyi Fan,Yong Si,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Maryam Pishgar*

Main category: cs.LG

TL;DR: 이 연구는 간경변 환자에서 조기 급성 신장 손상(AKI) 예측을 위한 해석 가능한 기계 학습 모델을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 간경변은 높은 사망률과 빈번한 합병증을 동반하는 진행성 간 질환이며, AKI는 입원 환자의 50%에서 발생하여 이환율을 악화시킨다.

Method: MIMIC-IV v2.2 데이터베이스의 회고적 분석을 수행하였고, ICU에 입원한 1240명의 성인 간경변 환자를 식별하였다. 실험에서는 LASSO 특징 선택 및 SMOTE 클래스 균형화가 포함되었다. 여섯 가지 알고리즘을 훈련 및 평가하였다.

Result: LightGBM이 최상의 성능을 보였으며, 주요 예측 변수는 알려진 간경변-AKI 기전과 일치하는 변수들이다.

Conclusion: LightGBM 기반 모델은 간경변 ICU 환자에서 조기 AKI 위험 예측을 가능하게 하며, 외부 검증 및 전자 건강 기록 시스템 통합이 필요하다.

Abstract: Background: Cirrhosis is a progressive liver disease with high mortality and
frequent complications, notably acute kidney injury (AKI), which occurs in up
to 50% of hospitalized patients and worsens outcomes. AKI stems from complex
hemodynamic, inflammatory, and metabolic changes, making early detection
essential. Many predictive tools lack accuracy, interpretability, and alignment
with intensive care unit (ICU) workflows. This study developed an interpretable
machine learning model for early AKI prediction in critically ill patients with
cirrhosis.
  Methods: We conducted a retrospective analysis of the MIMIC-IV v2.2 database,
identifying 1240 adult ICU patients with cirrhosis and excluding those with ICU
stays under 48 hours or missing key data. Laboratory and physiological
variables from the first 48 hours were extracted. The pipeline included
preprocessing, missingness filtering, LASSO feature selection, and SMOTE class
balancing. Six algorithms-LightGBM, CatBoost, XGBoost, logistic regression,
naive Bayes, and neural networks-were trained and evaluated using AUROC,
accuracy, F1-score, sensitivity, specificity, and predictive values.
  Results: LightGBM achieved the best performance (AUROC 0.808, 95% CI
0.741-0.856; accuracy 0.704; NPV 0.911). Key predictors included prolonged
partial thromboplastin time, absence of outside-facility 20G placement, low pH,
and altered pO2, consistent with known cirrhosis-AKI mechanisms and suggesting
actionable targets.
  Conclusion: The LightGBM-based model enables accurate early AKI risk
stratification in ICU patients with cirrhosis using routine clinical variables.
Its high negative predictive value supports safe de-escalation for low-risk
patients, and interpretability fosters clinician trust and targeted prevention.
External validation and integration into electronic health record systems are
warranted.

</details>


### [18] [SoK: Data Minimization in Machine Learning](https://arxiv.org/abs/2508.10836)
*Robin Staab,Nikola Jovanović,Kimberly Mai,Prakhar Ganesh,Martin Vechev,Ferdinando Fioretto,Matthew Jagielski*

Main category: cs.LG

TL;DR: 데이터 최소화(DM)의 원칙은 특정 작업에 엄격히 필요한 데이터만 수집하는 것을 강조하며, GDPR 및 CPRA와 같은 데이터 보호 규정의 기초적인 원칙이다. DM과 관련된 연구 분야인 머신러닝에서의 데이터 최소화(DMML)를 제안하고, 이를 효율적으로 적용할 수 있는 포괄적인 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 최소화 원칙은 규제 위반 시 막대한 경제적 제재를 동반하므로, 이를 지원하는 체계적 접근이 필요하다.

Method: DMML을 위한 통합 데이터 파이프라인, 적대자, 최소화 포인트를 포함하는 포괄적인 프레임워크를 제시.

Result: 문헌을 체계적으로 검토하고 연구자와 실무자가 DM 원칙을 효과적으로 적용할 수 있도록 돕는 구조적 개요를 제공한다.

Conclusion: 우리의 연구는 AI/ML에서 데이터 최소화 전략의 더욱 넓은 채택을 촉진한다.

Abstract: Data minimization (DM) describes the principle of collecting only the data
strictly necessary for a given task. It is a foundational principle across
major data protection regulations like GDPR and CPRA. Violations of this
principle have substantial real-world consequences, with regulatory actions
resulting in fines reaching hundreds of millions of dollars. Notably, the
relevance of data minimization is particularly pronounced in machine learning
(ML) applications, which typically rely on large datasets, resulting in an
emerging research area known as Data Minimization in Machine Learning (DMML).
At the same time, existing work on other ML privacy and security topics often
addresses concerns relevant to DMML without explicitly acknowledging the
connection. This disconnect leads to confusion among practitioners,
complicating their efforts to implement DM principles and interpret the
terminology, metrics, and evaluation criteria used across different research
communities. To address this gap, our work introduces a comprehensive framework
for DMML, including a unified data pipeline, adversaries, and points of
minimization. This framework allows us to systematically review the literature
on data minimization and \emph{DM-adjacent} methodologies, for the first time
presenting a structured overview designed to help practitioners and researchers
effectively apply DM principles. Our work facilitates a unified DM-centric
understanding and broader adoption of data minimization strategies in AI/ML.

</details>


### [19] [Can Transformers Break Encryption Schemes via In-Context Learning?](https://arxiv.org/abs/2508.10235)
*Jathin Korrapati,Patrick Mendoza,Aditya Tomar,Abein Abraham*

Main category: cs.LG

TL;DR: 이 연구는 ICL을 암호학적 함수 학습에 적용하여 암호 해독 성능을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구는 트랜스포머 모델이 간단한 함수 클래스에서 일반화할 수 있음을 보여주었다. 본 연구에서는 이러한 ICL을 암호 해독에 적용할 새로운 방법을 제안하고 있다.

Method: ICL을 사용하여 주어진 (암호문, 평문) 쌍을 통해 모델이 기저의 치환을 추론하고 새로운 암호문 단어를 해독하도록 훈련시킨다.

Result: 모델이 정형화된 추론 문제에서 암호 기법을 효과적으로 해독할 수 있는지를 평가하였다.

Conclusion: 트랜스포머 모델의 귀납적 편향과 일반화 능력을 ICL 패러다임 하에서 잘 평가할 수 있는 기회를 제공한다.

Abstract: In-context learning (ICL) has emerged as a powerful capability of
transformer-based language models, enabling them to perform tasks by
conditioning on a small number of examples presented at inference time, without
any parameter updates. Prior work has shown that transformers can generalize
over simple function classes like linear functions, decision trees, even neural
networks, purely from context, focusing on numerical or symbolic reasoning over
underlying well-structured functions. Instead, we propose a novel application
of ICL into the domain of cryptographic function learning, specifically
focusing on ciphers such as mono-alphabetic substitution and Vigen\`ere
ciphers, two classes of private-key encryption schemes. These ciphers involve a
fixed but hidden bijective mapping between plain text and cipher text
characters. Given a small set of (cipher text, plain text) pairs, the goal is
for the model to infer the underlying substitution and decode a new cipher text
word. This setting poses a structured inference challenge, which is well-suited
for evaluating the inductive biases and generalization capabilities of
transformers under the ICL paradigm. Code is available at
https://github.com/adistomar/CS182-project.

</details>


### [20] [Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models](https://arxiv.org/abs/2508.10243)
*Taibiao Zhao,Mingxuan Sun,Hao Wang,Xiaobing Chen,Xiangwei Zhou*

Main category: cs.LG

TL;DR: HPMI는 모델 아키텍처를 변경하지 않고 소규모의 원 자료와 기본 지식만으로 백도어 공격을 수행하는 새로운 방법이다.


<details>
  <summary>Details</summary>
Motivation: 트랜스포머 모델이 백도어 공격에 취약하다는 최근 연구를 반영하여, 리트레이닝이나 모델 아키텍처 변화를 요구하지 않는 공격 방법이 필요하다.

Method: Head-wise Pruning and Malicious Injection (HPMI)라는 새로운 공격 방법을 제안하며, 가장 중요하지 않은 헤드를 제거하고 미리 훈련된 악성 헤드를 주입하여 백도어를 설정한다.

Result: HPMI는 1) 클린 정확도 손실이 거의 없고, 2) 99.55% 이상의 공격 성공률을 달성하며, 3) 네 가지 고급 방어 메커니즘을 우회한다.

Conclusion: HPMI는 기존 리트레이닝 의존 공격보다 더 뛰어난 은폐성과 견고성을 제공하면서, 클린 정확도에 미치는 영향을 최소화한다.

Abstract: Transformer models have demonstrated exceptional performance and have become
indispensable in computer vision (CV) and natural language processing (NLP)
tasks. However, recent studies reveal that transformers are susceptible to
backdoor attacks. Prior backdoor attack methods typically rely on retraining
with clean data or altering the model architecture, both of which can be
resource-intensive and intrusive. In this paper, we propose Head-wise Pruning
and Malicious Injection (HPMI), a novel retraining-free backdoor attack on
transformers that does not alter the model's architecture. Our approach
requires only a small subset of the original data and basic knowledge of the
model architecture, eliminating the need for retraining the target transformer.
Technically, HPMI works by pruning the least important head and injecting a
pre-trained malicious head to establish the backdoor. We provide a rigorous
theoretical justification demonstrating that the implanted backdoor resists
detection and removal by state-of-the-art defense techniques, under reasonable
assumptions. Experimental evaluations across multiple datasets further validate
the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy
loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four
advanced defense mechanisms. Additionally, relative to state-of-the-art
retraining-dependent attacks, HPMI achieves greater concealment and robustness
against diverse defense strategies, while maintaining minimal impact on clean
accuracy.

</details>


### [21] [Convergence Analysis of Max-Min Exponential Neural Network Operators in Orlicz Space](https://arxiv.org/abs/2508.10248)
*Satyaranjan Pradhan,Madan Mohan Soren*

Main category: cs.LG

TL;DR: 본 연구에서는 지수 신경망 연산자를 사용하여 함수 근사를 위한 Max Min 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 함수 근사를 향상시키기 위한 새로운 방법론 개발.

Method: Max Min Kantorovich 유형의 지수 신경망 연산자를 개발하고, 점별 및 균일 수렴을 연구합니다.

Result: 로그 모듈 연속성을 사용하여 수렴의 차수 분석 및 속도 추정을 수행합니다.

Conclusion: Orlicz 공간 설정 내에서 Max Min Kantorovich 유형 지수 신경망 연산자의 수렴 거동을 조사하고, 적절한 커널 및 시그모이드 활성화 함수를 통해 함수의 근사 오류를 그래픽적으로 나타냅니다.

Abstract: In this current work, we propose a Max Min approach for approximating
functions using exponential neural network operators. We extend this framework
to develop the Max Min Kantorovich-type exponential neural network operators
and investigate their approximation properties. We study both pointwise and
uniform convergence for univariate functions. To analyze the order of
convergence, we use the logarithmic modulus of continuity and estimate the
corresponding rate of convergence. Furthermore, we examine the convergence
behavior of the Max Min Kantorovich type exponential neural network operators
within the Orlicz space setting. We provide some graphical representations to
illustrate the approximation error of the function through suitable kernel and
sigmoidal activation functions.

</details>


### [22] [Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters](https://arxiv.org/abs/2508.10253)
*Guanzi Yao,Heyao Liu,Linyan Dai*

Main category: cs.LG

TL;DR: 이 논문은 클라우드 네이티브 데이터베이스 시스템의 자원 동적성과 스케줄링 복잡성 문제를 다루고 있으며, 다중 에이전트 강화 학습 기반의 적응형 자원 오케스트레이션 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 네이티브 데이터베이스 시스템에서 발생하는 높은 자원 동적성과 스케줄링 복잡성 문제를 해결하기 위해 필요합니다.

Method: 이 방법은 이기종 역할 기반 에이전트 모델링 메커니즘을 도입하여 다양한 자원 엔티티가 각각의 정책 표현을 채택하도록 합니다.

Result: 실험 결과, 제안된 방법이 전통적인 접근 방식보다 여러 주요 메트릭에서 우수한 성능을 보였습니다.

Conclusion: 이 방법은 실제 대규모 스케줄링 환경에서 효과적으로 오케스트레이션 작업을 처리할 수 있다는 것을 확인했습니다.

Abstract: This paper addresses the challenges of high resource dynamism and scheduling
complexity in cloud-native database systems. It proposes an adaptive resource
orchestration method based on multi-agent reinforcement learning. The method
introduces a heterogeneous role-based agent modeling mechanism. This allows
different resource entities, such as compute nodes, storage nodes, and
schedulers, to adopt distinct policy representations. These agents are better
able to reflect diverse functional responsibilities and local environmental
characteristics within the system. A reward-shaping mechanism is designed to
integrate local observations with global feedback. This helps mitigate policy
learning bias caused by incomplete state observations. By combining real-time
local performance signals with global system value estimation, the mechanism
improves coordination among agents and enhances policy convergence stability. A
unified multi-agent training framework is developed and evaluated on a
representative production scheduling dataset. Experimental results show that
the proposed method outperforms traditional approaches across multiple key
metrics. These include resource utilization, scheduling latency, policy
convergence speed, system stability, and fairness. The results demonstrate
strong generalization and practical utility. Across various experimental
scenarios, the method proves effective in handling orchestration tasks with
high concurrency, high-dimensional state spaces, and complex dependency
relationships. This confirms its advantages in real-world, large-scale
scheduling environments.

</details>


### [23] [Federated Anomaly Detection for Multi-Tenant Cloud Platforms with Personalized Modeling](https://arxiv.org/abs/2508.10255)
*Yuxi Wang,Heyao Liu,Nyutian Long,Guanzi Yao*

Main category: cs.LG

TL;DR: 이 논문은 다중 임차인 클라우드 환경에서의 이상 탐지 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 임차인 클라우드 환경에서 데이터 프라이버시 유출, 이질적인 리소스 행동, 중앙 집중식 모델링의 한계를 해결하는 것이 필요하다.

Method: 다중 임차인을 포함한 연합 학습 프레임워크를 설정하고, 각 임차인이 사전 리소스 사용 데이터로 로컬 모델을 훈련시킨다.

Result: 실험은 클라우드 플랫폼의 실제 텔레메트리 데이터를 사용하여 시뮬레이션된 다중 임차인 환경을 구성하고 제안된 방법의 강건성과 탐지 정확성을 입증하였다.

Conclusion: 제안된 방법은 다양한 복잡한 시나리오에서도 안정적인 성능을 유지하며 클라우드 컴퓨팅 환경에서 지능형 리소스 모니터링 및 이상 진단을 위한 실제적인 잠재력을 강조한다.

Abstract: This paper proposes an anomaly detection method based on federated learning
to address key challenges in multi-tenant cloud environments, including data
privacy leakage, heterogeneous resource behavior, and the limitations of
centralized modeling. The method establishes a federated training framework
involving multiple tenants. Each tenant trains the model locally using private
resource usage data. Through parameter aggregation, a global model is
optimized, enabling cross-tenant collaborative anomaly detection while
preserving data privacy. To improve adaptability to diverse resource usage
patterns, a personalized parameter adjustment mechanism is introduced. This
allows the model to retain tenant-specific feature representations while
sharing global knowledge. In the model output stage, the Mahalanobis distance
is used to compute anomaly scores. This enhances both the accuracy and
stability of anomaly detection. The experiments use real telemetry data from a
cloud platform to construct a simulated multi-tenant environment. The study
evaluates the model's performance under varying participation rates and noise
injection levels. These comparisons demonstrate the proposed method's
robustness and detection accuracy. Experimental results show that the proposed
method outperforms existing mainstream models across key metrics such as
Precision, Recall, and F1-Score. It also maintains stable performance in
various complex scenarios. These findings highlight the method's practical
potential for intelligent resource monitoring and anomaly diagnosis in cloud
computing environments.

</details>


### [24] [Source Component Shift Adaptation via Offline Decomposition and Online Mixing Approach](https://arxiv.org/abs/2508.10257)
*Ryuta Matsuno*

Main category: cs.LG

TL;DR: 이 논문은 소스 구성 요소 이동 적응에 관한 것으로, 과거 훈련 데이터를 기반으로 새로운 데이터 스트림에 대한 예측을 업데이트하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 온라인 학습 방법이 반복적인 이동을 효과적으로 활용하지 못하고, 모델 풀 기반 방법이 개별 소스 구성 요소를 정확하게 포착하지 못하는 문제를 해결하고자 함.

Method: 오프라인 분해와 온라인 혼합 접근 방식을 통해 소스 구성 요소 이동 적응 방법을 제안함. 처음에는 EM 알고리즘을 통해 과거 훈련 데이터만을 사용해 예측 모델을 결정한 후, 온라인 볼록 최적화를 통해 예측 모델의 혼합 가중치를 업데이트함.

Result: 다양한 실제 회귀 데이터 세트에서 실시한 실험 결과, 제안한 방법이 기준선보다 뛰어난 성능을 보이며 누적 테스트 손실을 최대 67.4%까지 줄임.

Conclusion: 이론적 유도 덕분에 제안한 방법은 이동의 특성을 완전히 활용하여 기존 방법보다 우수한 적응 성능을 달성함.

Abstract: This paper addresses source component shift adaptation, aiming to update
predictions adapting to source component shifts for incoming data streams based
on past training data. Existing online learning methods often fail to utilize
recurring shifts effectively, while model-pool-based methods struggle to
capture individual source components, leading to poor adaptation. In this
paper, we propose a source component shift adaptation method via an offline
decomposition and online mixing approach. We theoretically identify that the
problem can be divided into two subproblems: offline source component
decomposition and online mixing weight adaptation. Based on this, our method
first determines prediction models, each of which learns a source component
solely based on past training data offline through the EM algorithm. Then, it
updates the mixing weight of the prediction models for precise prediction
through online convex optimization. Thanks to our theoretical derivation, our
method fully leverages the characteristics of the shifts, achieving superior
adaptation performance over existing methods. Experiments conducted on various
real-world regression datasets demonstrate that our method outperforms
baselines, reducing the cumulative test loss by up to 67.4%.

</details>


### [25] [Uncertainty-Aware Prediction of Parkinson's Disease Medication Needs: A Two-Stage Conformal Prediction Approach](https://arxiv.org/abs/2508.10284)
*Ricardo Diaz-Rincon,Muxuan Liang,Adolfo Ramirez-Zamora,Benjamin Shickel*

Main category: cs.LG

TL;DR: 파킨슨병 약물 관리의 도전 과제를 해결하기 위한 예측 프레임워크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 파킨슨병(PD)의 이질적인 질병 진행 및 치료 반응으로 인해 약물 관리에 독특한 도전이 있다.

Method: 약물 필요성을 최대 2년 미리 예측하는 정합 예측 프레임워크를 개발하였다.

Result: 플로리다 대학교 건강의 전자 건강 기록을 이용하여 약물 변경이 필요할 환자를 식별하고, 필요한 레보도파 일일 용량 조정을 예측하였다.

Conclusion: 불확실성을 정량화함으로써 레보도파 투여에 대한 근거 기반 결정을 가능하게 하여 증상 조절 최적화 및 부작용 최소화를 도모한다.

Abstract: Parkinson's Disease (PD) medication management presents unique challenges due
to heterogeneous disease progression and treatment response. Neurologists must
balance symptom control with optimal dopaminergic dosing based on functional
disability while minimizing side effects. This balance is crucial as inadequate
or abrupt changes can cause levodopa-induced dyskinesia, wearing off, and
neuropsychiatric effects, significantly reducing quality of life. Current
approaches rely on trial-and-error decisions without systematic predictive
methods. Despite machine learning advances, clinical adoption remains limited
due to reliance on point predictions that do not account for prediction
uncertainty, undermining clinical trust and utility. Clinicians require not
only predictions of future medication needs but also reliable confidence
measures. Without quantified uncertainty, adjustments risk premature escalation
to maximum doses or prolonged inadequate symptom control. We developed a
conformal prediction framework anticipating medication needs up to two years in
advance with reliable prediction intervals and statistical guarantees. Our
approach addresses zero-inflation in PD inpatient data, where patients maintain
stable medication regimens between visits. Using electronic health records from
631 inpatient admissions at University of Florida Health (2011-2021), our
two-stage approach identifies patients likely to need medication changes, then
predicts required levodopa equivalent daily dose adjustments. Our framework
achieved marginal coverage while reducing prediction interval lengths compared
to traditional approaches, providing precise predictions for short-term
planning and wider ranges for long-term forecasting. By quantifying
uncertainty, our approach enables evidence-based decisions about levodopa
dosing, optimizing symptom control while minimizing side effects and improving
life quality.

</details>


### [26] [SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning](https://arxiv.org/abs/2508.10298)
*Weijian Mai,Jiamin Wu,Yu Zhu,Zhouheng Yao,Dongzhan Zhou,Andrew F. Luo,Qihao Zheng,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: SynBrain은 시각 자극을 신경 반응으로 변환하는 생성적 프레임워크로, 생물학적 변동성을 모델링하면서 기본적인 기능 일관성을 유지한다.


<details>
  <summary>Details</summary>
Motivation: 시각 자극이 대뇌 반응으로 변환되는 과정을 이해하는 것이 계산 신경과학의 근본적인 도전 과제이다.

Method: SynBrain은 시각 의미에서 신경 반응으로의 변환을 확률적이고 생물학적으로 해석 가능한 방식으로 시뮬레이션하는 생성적 프레임워크를 제안한다.

Result: SynBrain은 실험 결과로 주제별 시각-뇌 자기공명영상(fMRI) 인코딩 성능에서 최첨단 방법을 초월한다.

Conclusion: SynBrain은 기능적 일관성을 드러내며 생물학적 신경 변동성에 의해 형성된 해석 가능한 패턴을 포착한다.

Abstract: Deciphering how visual stimuli are transformed into cortical responses is a
fundamental challenge in computational neuroscience. This visual-to-neural
mapping is inherently a one-to-many relationship, as identical visual inputs
reliably evoke variable hemodynamic responses across trials, contexts, and
subjects. However, existing deterministic methods struggle to simultaneously
model this biological variability while capturing the underlying functional
consistency that encodes stimulus information. To address these limitations, we
propose SynBrain, a generative framework that simulates the transformation from
visual semantics to neural responses in a probabilistic and biologically
interpretable manner. SynBrain introduces two key components: (i) BrainVAE
models neural representations as continuous probability distributions via
probabilistic learning while maintaining functional consistency through visual
semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic
transmission pathway, projecting visual semantics into the neural response
manifold to facilitate high-fidelity fMRI synthesis. Experimental results
demonstrate that SynBrain surpasses state-of-the-art methods in
subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain
adapts efficiently to new subjects with few-shot data and synthesizes
high-quality fMRI signals that are effective in improving data-limited
fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional
consistency across trials and subjects, with synthesized signals capturing
interpretable patterns shaped by biological neural variability. The code will
be made publicly available.

</details>


### [27] [Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning](https://arxiv.org/abs/2508.10299)
*Danni Peng,Yuan Wang,Kangning Cai,Peiyan Ning,Jiming Xu,Yong Liu,Rick Siow Mong Goh,Qingsong Wei,Huazhu Fu*

Main category: cs.LG

TL;DR: 본 연구에서는 의료 분야에서의 연합 학습을 위한 혁신적인 프레임워크인 Federated Knowledge-Enhanced Initialization을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 의료 환경의 급속한 변화에 따라 개별 클라이언트가 과거 경험을 활용하여 새로운 작업이나 질병에 빠르게 적응하는 것이 중요합니다.

Method: FedKEI는 과거 지식으로부터의 교차 클라이언트 및 교차 작업 이전을 활용하여 학습을 위한 초기화를 생성하는 새로운 프레임워크입니다. 이는 서버에서의 글로벌 클러스터링 프로세스를 시작으로 클러스터 간 무게 최적화 및 개별 클러스터 내에서의 최적화를 포함합니다.

Result: 세 가지 벤치마크 데이터셋에 대한 광범위한 실험을 통해 FedKEI가 최신 방법들에 비해 새로운 질병에 적응하는 데 있어 장점을 보임을 증명했습니다.

Conclusion: FedKEI는 의료 분야에서의 개인화된 지식 이전을 통해 연합 학습의 효율성을 크게 향상시킬 수 있습니다.

Abstract: In healthcare, federated learning (FL) is a widely adopted framework that
enables privacy-preserving collaboration among medical institutions. With large
foundation models (FMs) demonstrating impressive capabilities, using FMs in FL
through cost-efficient adapter tuning has become a popular approach. Given the
rapidly evolving healthcare environment, it is crucial for individual clients
to quickly adapt to new tasks or diseases by tuning adapters while drawing upon
past experiences. In this work, we introduce Federated Knowledge-Enhanced
Initialization (FedKEI), a novel framework that leverages cross-client and
cross-task transfer from past knowledge to generate informed initializations
for learning new tasks with adapters. FedKEI begins with a global clustering
process at the server to generalize knowledge across tasks, followed by the
optimization of aggregation weights across clusters (inter-cluster weights) and
within each cluster (intra-cluster weights) to personalize knowledge transfer
for each new task. To facilitate more effective learning of the inter- and
intra-cluster weights, we adopt a bi-level optimization scheme that
collaboratively learns the global intra-cluster weights across clients and
optimizes the local inter-cluster weights toward each client's task objective.
Extensive experiments on three benchmark datasets of different modalities,
including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI's
advantage in adapting to new diseases compared to state-of-the-art methods.

</details>


### [28] [A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.10315)
*Keke Gai,Dongjue Wang,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.LG

TL;DR: CLIP-Fed는 비동질적인 클라이언트 데이터 분포에서의 백도어 공격 방어를 위한 새로운 프레임워크로, 비전-언어 사전훈련 모델의 제로-샷 학습 능력을 활용한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 연합 학습 백도어 방어 방법은 동질적인 클라이언트 데이터 분포 또는 깨끗한 서버 데이터셋의 존재를 가정하여 실제 적용에서 한계를 겪는다.

Method: CLIP-Fed는 사전 집계 및 사후 집계 방어 전략을 통합하고, 멀티모달 대형 언어 모델과 주파수 분석을 사용하여 클라이언트 샘플 없이 서버 데이터셋을 구성 및 증강한다.

Result: CLIP-Fed는 대표적인 데이터셋에 대한 광범위한 실험을 통해 그 효과성을 검증하였으며, 현재 최첨단 방법들과 비교해 평균 ASR을 CIFAR-10에서 2.03%, CIFAR-10-LT에서 1.35% 줄이면서 평균 MA를 각각 7.92%, 0.48% 향상시켰다.

Conclusion: CLIP-Fed는 비동질적인 클라이언트 데이터 분포를 다루면서도 모델 성능을 유지하는 데 성공하였다.

Abstract: Existing backdoor defense methods in Federated Learning (FL) rely on the
assumption of homogeneous client data distributions or the availability of a
clean serve dataset, which limits the practicality and effectiveness. Defending
against backdoor attacks under heterogeneous client data distributions while
preserving model performance remains a significant challenge. In this paper, we
propose a FL backdoor defense framework named CLIP-Fed, which leverages the
zero-shot learning capabilities of vision-language pre-training models. By
integrating both pre-aggregation and post-aggregation defense strategies,
CLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.
To address privacy concerns and enhance the coverage of the dataset against
diverse triggers, we construct and augment the server dataset using the
multimodal large language model and frequency analysis without any client
samples. To address class prototype deviations caused by backdoor samples and
eliminate the correlation between trigger patterns and target labels, CLIP-Fed
aligns the knowledge of the global model and CLIP on the augmented dataset
using prototype contrastive loss and Kullback-Leibler divergence. Extensive
experiments on representative datasets validate the effectiveness of CLIP-Fed.
Compared to state-of-the-art methods, CLIP-Fed achieves an average reduction in
ASR, i.e., 2.03\% on CIFAR-10 and 1.35\% on CIFAR-10-LT, while improving
average MA by 7.92\% and 0.48\%, respectively.

</details>


### [29] [Welfare-Centric Clustering](https://arxiv.org/abs/2508.10345)
*Claire Jie Zhang,Seyed A. Esmaeili,Jamie Morgenstern*

Main category: cs.LG

TL;DR: 이 연구에서는 복지 중심의 클러스터링 접근 방식을 모델링하여 그룹의 효용을 기반으로 한 새로운 알고리즘을 제안하고, 기존의 공정한 클러스터링 방법보다 더 나은 성능을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 공정 클러스터링 접근법은 그룹 간의 공정한 대표성이나 그룹별 클러스터링 비용을 균등화하는 데 초점을 맞추었음. 그러나 최근의 연구는 이러한 공정 개념이 바람직하지 않거나 직관적이지 않은 클러스터링 결과를 초래할 수 있음을 보여주었다.

Method: 그룹의 효용을 거리 및 비례 대표성에 기반하여 모델링하고, Rawlsian(평등주의) 목표와 공리주의 목표에 따른 두 가지 최적화 문제를 형식화함. 이를 위해 새로운 알고리즘을 도입하고 이론적 보장을 증명함.

Result: 여러 실제 데이터셋에 대한 실험적 평가 결과, 제안한 방법이 기존의 공정 클러스터링 기준선보다 유의미하게 우수한 성능을 보임을 입증.

Conclusion: 복지 중심의 클러스터링 접근 방식을 통해 공정 클러스터링의 성능이 향상됨을 확인할 수 있음.

Abstract: Fair clustering has traditionally focused on ensuring equitable group
representation or equalizing group-specific clustering costs. However,
Dickerson et al. (2025) recently showed that these fairness notions may yield
undesirable or unintuitive clustering outcomes and advocated for a
welfare-centric clustering approach that models the utilities of the groups. In
this work, we model group utilities based on both distances and proportional
representation and formalize two optimization objectives based on
welfare-centric clustering: the Rawlsian (Egalitarian) objective and the
Utilitarian objective. We introduce novel algorithms for both objectives and
prove theoretical guarantees for them. Empirical evaluations on multiple
real-world datasets demonstrate that our methods significantly outperform
existing fair clustering baselines.

</details>


### [30] [A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks](https://arxiv.org/abs/2508.10346)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh*

Main category: cs.LG

TL;DR: 본 논문에서는 IoMT 환경에 적합한 다단계 침입 탐지 시스템을 제안하며, 제로데이 공격을 탐지하고 알려진 위협과 미지의 위협을 구별하는 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 의료용 사물인터넷(IoMT)은 헬스케어 혁명을 이끌고 있지만, 사이버 공격에 취약하다.

Method: 다단계 IoMT IDS 프레임워크를 제안하며, 첫 번째 레이어는 거칠게 트래픽을 필터링하고 후속 레이어는 공격 유형을 식별한다.

Result: CICIoMT2024 데이터셋에서 99.77% 정확도와 97.8% F1 점수를 보여준다.

Conclusion: 첫 번째 레이어는 새로운 데이터셋 없이도 높은 정확도로 제로데이 공격을 탐지할 수 있어 IoMT 환경에서의 강한 적용 가능성을 보장한다.

Abstract: The Internet of Medical Things (IoMT) is driving a healthcare revolution but
remains vulnerable to cyberattacks such as denial of service, ransomware, data
hijacking, and spoofing. These networks comprise resource constrained,
heterogeneous devices (e.g., wearable sensors, smart pills, implantables),
making traditional centralized Intrusion Detection Systems (IDSs) unsuitable
due to response delays, privacy risks, and added vulnerabilities. Centralized
IDSs require all sensors to transmit data to a central server, causing delays
or network disruptions in dense environments. Running IDSs locally on IoMT
devices is often infeasible due to limited computation, and even lightweight
IDS components remain at risk if updated models are delayed leaving them
exposed to zero-day attacks that threaten patient health and data security. We
propose a multi level IoMT IDS framework capable of detecting zero day attacks
and distinguishing between known and unknown threats. The first layer (near
Edge) filters traffic at a coarse level (attack or not) using meta-learning or
One Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far
Edge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024
dataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first
layer detects zero-day attacks with high accuracy without needing new datasets,
ensuring strong applicability in IoMT environments. Additionally, the
meta-learning approach achieves high.

</details>


### [31] [Semantic Communication with Distribution Learning through Sequential Observations](https://arxiv.org/abs/2508.10350)
*Samer Lahoud,Kinda Khawam*

Main category: cs.LG

TL;DR: 이 논문은 의미 통신에서 배포 학습을 조사하며, 수신자가 순차적 관찰을 통해 기본적인 의미 분포를 추론해야 함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존 통신에서 의미 전달로의 패러다임 전환을 통해 의미 통신의 중요성을 강조하고, 통계적 학습의 기본 조건을 제시하기 위해.

Method: 수신자가 모르는 사전 정보를 활용하여 의미 통신에서 배포 학습을 위한 기본 조건을 설정하고, 효과적인 전송 매트릭스의 랭크 및 분포 추정 수렴 속도를 분석했다.

Result: 전송 매트릭스의 완전 랭크가 요구되며, 의미 왜곡으로의 추정 오류 전환을 정량화하였다. 실험 결과는 시스템 컨디셔닝이 학습 속도와 성능에 미치는 중요한 영향을 입증했다.

Conclusion: 의미 통신에서의 통계적 학습의 엄격한 특성을 제공하고, 즉각적인 성과와 적응 능력의 균형을 맞춘 시스템 설계를 위한 원칙을 제시한다.

Abstract: Semantic communication aims to convey meaning rather than bit-perfect
reproduction, representing a paradigm shift from traditional communication.
This paper investigates distribution learning in semantic communication where
receivers must infer the underlying meaning distribution through sequential
observations. While semantic communication traditionally optimizes individual
meaning transmission, we establish fundamental conditions for learning source
statistics when priors are unknown. We prove that learnability requires full
rank of the effective transmission matrix, characterize the convergence rate of
distribution estimation, and quantify how estimation errors translate to
semantic distortion. Our analysis reveals a fundamental trade-off: encoding
schemes optimized for immediate semantic performance often sacrifice long-term
learnability. Experiments on CIFAR-10 validate our theoretical framework,
demonstrating that system conditioning critically impacts both learning rate
and achievable performance. These results provide the first rigorous
characterization of statistical learning in semantic communication and offer
design principles for systems that balance immediate performance with
adaptation capability.

</details>


### [32] [eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing](https://arxiv.org/abs/2508.10370)
*Jiyong Kim,Jaeho Lee,Jiahao Lin,Alish Kanani,Miao Sun,Umit Y. Ogras,Jaehyun Park*

Main category: cs.LG

TL;DR: eMamba는 Mamba 모델을 엣지 플랫폼에 배포하기 위해 설계된 완전한 하드웨어 가속 프레임워크로, 복잡한 정규화 계층을 경량 하드웨어 인식 대안으로 대체하여 계산 효율성을 극대화하며, 근사화 감지 신경 구조 검색을 통해 학습 가능한 매개변수를 조정한다. 실험 결과는 경쟁력 있는 정확도를 유지하면서 대폭 향상된 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: Mamba는 자원 제약이 있는 엣지 디바이스에서 특히 유망한 SSM 기반 모델로, 하드웨어 가속 프레임워크가 필요하다.

Method: eMamba는 Mamba 모델을 엣지 플랫폼에서 배포하기 위해 복잡한 정규화 계층을 경량 하드웨어 인식 대안으로 대체하고, SiLU 활성화 및 지수화와 같은 비싼 연산을 근사화하여 계산 효율성을 최대화한다. 또한 근사화-aware 신경 아키텍처 검색을 수행하여 매개변수를 조정한다.

Result: Fashion-MNIST, CIFAR-10 및 MARS 데이터셋을 사용한 평가에서 eMamba는 1.63-19.9배 적은 매개변수로 최신 기술과 유사한 정확도를 달성하고, WikiText2 데이터셋에서 다양한 시퀀스 길이에 걸쳐 안정적인 혼란도를 나타낸다.

Conclusion: eMamba는 AMD ZCU102 FPGA 및 ASIC에 대해 전반적인 파이프라인을 양자화 및 구현하여 성능 향상을 보여준다.

Abstract: State Space Model (SSM)-based machine learning architectures have recently
gained significant attention for processing sequential data. Mamba, a recent
sequence-to-sequence SSM, offers competitive accuracy with superior
computational efficiency compared to state-of-the-art transformer models. While
this advantage makes Mamba particularly promising for resource-constrained edge
devices, no hardware acceleration frameworks are currently optimized for
deploying it in such environments. This paper presents eMamba, a comprehensive
end-to-end hardware acceleration framework explicitly designed for deploying
Mamba models on edge platforms. eMamba maximizes computational efficiency by
replacing complex normalization layers with lightweight hardware-aware
alternatives and approximating expensive operations, such as SiLU activation
and exponentiation, considering the target applications. Then, it performs an
approximation-aware neural architecture search (NAS) to tune the learnable
parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,
and MARS, an open-source human pose estimation dataset, show eMamba achieves
comparable accuracy to state-of-the-art techniques using 1.63-19.9$\times$
fewer parameters. In addition, it generalizes well to large-scale natural
language tasks, demonstrating stable perplexity across varying sequence lengths
on the WikiText2 dataset. We also quantize and implement the entire eMamba
pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm
technology. Experimental results show 4.95-5.62$\times$ lower latency and
2.22-9.95$\times$ higher throughput, with 4.77$\times$ smaller area,
9.84$\times$ lower power, and 48.6$\times$ lower energy consumption than
baseline solutions while maintaining competitive accuracy.

</details>


### [33] [A Unified Evaluation Framework for Multi-Annotator Tendency Learning](https://arxiv.org/abs/2508.10393)
*Liyun Zhang,Jingcheng Ke,Shenli Fan,Xuanmeng Sha,Zheng Lian*

Main category: cs.LG

TL;DR: 본 논문은 개별 경향 학습(ITL) 방법의 유효성을 평가할 수 있는 최초의 통합 평가 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: ITL 방법이 개별 경향을 제대로 포착하고 의미 있는 행동 설명을 제공하는지 평가할 수 있는 프레임워크가 필요합니다.

Method: 차별화된 두 가지 지표(DIC와 BAE)를 포함한 통합 평가 프레임워크를 개발했습니다.

Result: 제안된 평가 프레임워크의 유효성을 검증하기 위한 광범위한 실험을 수행했습니다.

Conclusion: 제안한 프레임워크는 ITL 방법의 성능 평가에 기여할 것입니다.

Abstract: Recent works have emerged in multi-annotator learning that shift focus from
Consensus-oriented Learning (CoL), which aggregates multiple annotations into a
single ground-truth prediction, to Individual Tendency Learning (ITL), which
models annotator-specific labeling behavior patterns (i.e., tendency) to
provide explanation analysis for understanding annotator decisions. However, no
evaluation framework currently exists to assess whether ITL methods truly
capture individual tendencies and provide meaningful behavioral explanations.
To address this gap, we propose the first unified evaluation framework with two
novel metrics: (1) Difference of Inter-annotator Consistency (DIC) quantifies
how well models capture annotator tendencies by comparing predicted
inter-annotator similarity structures with ground-truth; (2) Behavior Alignment
Explainability (BAE) evaluates how well model explanations reflect annotator
behavior and decision relevance by aligning explainability-derived with
ground-truth labeling similarity structures via Multidimensional Scaling (MDS).
Extensive experiments validate the effectiveness of our proposed evaluation
framework.

</details>


### [34] [XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization](https://arxiv.org/abs/2508.10395)
*Aditya Tomar,Coleman Hooper,Minjae Lee,Haocheng Xi,Rishabh Tiwari,Wonjun Kang,Luca Manolache,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: XQuant는 LLM 추론을 위한 메모리 소비를 획기적으로 줄이고 인퍼런스 성능을 향상시키는 새로운 방법론을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 인퍼런스는 많은 다운스트림 애플리케이션에 중요한 작업으로 부상했지만, 메모리 요구 사항과 대역폭 문제로 효율적인 인퍼런스 수행이 어렵습니다.

Method: XQuant는 저비트 양자화를 사용하여 메모리 소비를 줄이고 KV 캐시 방법과 비교하여 상당한 정확도 이점을 제공합니다.

Result: XQuant를 사용하여 우리가 달성한 결과는 FP16 기준에 비해 최대 7.7배의 메모리 절약과 0.1 미만의 당혹감 저하가 있습니다.

Conclusion: XQuant는 하드웨어 플랫폼의 빠르게 증가하는 계산 능력을 활용하여 메모리 병목 현상을 제거하고, 다양한 모델에서 FP16 정확도에 가까운 성능을 달성합니다.

Abstract: Although LLM inference has emerged as a critical workload for many downstream
applications, efficiently inferring LLMs is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of LLM inference. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
XQuant, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through low-bit quantization with substantial
accuracy benefits relative to state-of-the-art KV cache quantization methods.
We accomplish this by quantizing and caching the layer input activations X,
instead of using standard KV caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2$\times$
memory savings compared to KV caching. By applying XQuant, we achieve up to
$\sim 7.7\times$ memory savings with $<0.1$ perplexity degradation compared to
the FP16 baseline. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
XQuant-CL, which exploits the cross-layer similarity in the X embeddings for
extreme compression. Across different models, XQuant-CL attains up to
10$\times$ memory savings relative to the FP16 baseline with only 0.01
perplexity degradation, and 12.5$\times$ memory savings with only $0.1$
perplexity degradation. XQuant exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art KV cache quantization methods and achieving
near-FP16 accuracy across a wide range of models.

</details>


### [35] [SC2Arena and StarEvolve: Benchmark and Self-Improvement Framework for LLMs in Complex Decision-Making Tasks](https://arxiv.org/abs/2508.10428)
*Pengbo Shen,Yaqing Wang,Ni Mu,Yao Luan,Runpeng Xie,Senhao Yang,Lexiang Wang,Hao Hu,Shuang Xu,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 대규모 언어 모델 평가에 대한 새로운 벤치마크와 프레임워크를 제안하여 복잡한 의사결정에서의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: AI의 전략적 계획 및 실시간 적응 능력을 향상시키기 위해 대규모 언어 모델(LLM)을 평가하는 것이 필수적입니다.

Method: SC2Arena라는 벤치마크를 통해 모든 플레이 가능한 종족과 저수준 행동 공간을 지원하고, StarEvolve라는 계층적 프레임워크를 통해 전략 계획과 전술 실행을 통합합니다.

Result: 실험 결과, StarEvolve는 전략적 계획에서 우수한 성능을 달성함을 보여주었습니다.

Conclusion: 이 연구는 이전의 벤치마크로는 불가능했던 일반화 에이전트 개발에 대한 귀중한 통찰력을 제공합니다.

Abstract: Evaluating large language models (LLMs) in complex decision-making is
essential for advancing AI's ability for strategic planning and real-time
adaptation. However, existing benchmarks for tasks like StarCraft II fail to
capture the game's full complexity, such as its complete game context, diverse
action spaces, and all playable races. To address this gap, we present
SC2Arena, a benchmark that fully supports all playable races, low-level action
spaces, and optimizes text-based observations to tackle spatial reasoning
challenges. Complementing this, we introduce StarEvolve, a hierarchical
framework that integrates strategic planning with tactical execution, featuring
iterative self-correction and continuous improvement via fine-tuning on
high-quality gameplay data. Its key components include a
Planner-Executor-Verifier structure to break down gameplay, and a scoring
system for selecting high-quality training samples. Comprehensive analysis
using SC2Arena provides valuable insights into developing generalist agents
that were not possible with previous benchmarks. Experimental results also
demonstrate that our proposed StarEvolve achieves superior performance in
strategic planning. Our code, environment, and algorithms are publicly
available.

</details>


### [36] [Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models](https://arxiv.org/abs/2508.10435)
*Tianxiao Cao,Kyohei Atarashi,Hisashi Kashima*

Main category: cs.LG

TL;DR: 샤프니스 인식 최소화(SAM)는 과매개변수 모델에서 일반화 향상을 위한 효과적인 최적화 기법임.


<details>
  <summary>Details</summary>
Motivation: SAM의 숨겨진 규제 효과를 보다 일반적인 텐서화된 모델에서 분석하고자 함.

Method: 노름 편차(Norm Deviation)라는 개념을 도입하고, 이를 SAM의 그래디언트 흐름 분석을 통해 발전시키는 방법을 제시함.

Result: DAS가 SAM에 비해 경쟁력 있는 성능 또는 향상된 성능을 달성하며 계산 오버헤드를 줄임을 확인함.

Conclusion: DAS는 데이터 적응 방식으로 코어 노름을 스케일링하여 SAM의 규제 행동을 모방하는 간단하면서도 효과적인 방법임.

Abstract: Sharpness-Aware Minimization (SAM) has been proven to be an effective
optimization technique for improving generalization in overparameterized
models. While prior works have explored the implicit regularization of SAM in
simple two-core scale-invariant settings, its behavior in more general
tensorized or scale-invariant models remains underexplored. In this work, we
leverage scale-invariance to analyze the norm dynamics of SAM in general
tensorized models. We introduce the notion of \emph{Norm Deviation} as a global
measure of core norm imbalance, and derive its evolution under SAM using
gradient flow analysis. We show that SAM's implicit control of Norm Deviation
is governed by the covariance between core norms and their gradient magnitudes.
Motivated by these findings, we propose a simple yet effective method,
\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this
regularization behavior by scaling core norms in a data-adaptive manner. Our
experiments across tensor completion, noisy training, model compression, and
parameter-efficient fine-tuning confirm that DAS achieves competitive or
improved performance over SAM, while offering reduced computational overhead.

</details>


### [37] [RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations](https://arxiv.org/abs/2508.10455)
*Asiful Arefeen,Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: RealAC는 현실적이고 실행 가능한 반사실적인 설명을 생성하기 위한 도메인 비종속 프레임워크로, 입력 특성의 최소 변경을 설명하여 AI의 결정을 이해할 수 있도록 합니다.


<details>
  <summary>Details</summary>
Motivation: AI의 결정을 이해 가능한 방식으로 설명하기 위해서는 현실적이고 실행 가능한 반사실적 설명이 필요합니다.

Method: RealAC는 사실적인 인스턴스와 반사실적인 인스턴스 간의 특성 쌍의 동시 분포를 정렬하여 복잡한 특성 간 의존성을 자동으로 보존합니다.

Result: RealAC는 세 가지 합성 데이터셋과 두 개의 실제 데이터셋에서 평가하며 현실성과 실행 가능성의 균형을 유지합니다.

Conclusion: 우리 방법은 최신 기술 기준 및 대형 언어 모델 기반 반사실적 생성 기법보다 우수한 성능을 보이며 인과성을 인식하고 사용자 중심의 반사실적 생성 방법을 제공합니다.

Abstract: Counterfactual explanations provide human-understandable reasoning for
AI-made decisions by describing minimal changes to input features that would
alter a model's prediction. To be truly useful in practice, such explanations
must be realistic and feasible -- they should respect both the underlying data
distribution and user-defined feasibility constraints. Existing approaches
often enforce inter-feature dependencies through rigid, hand-crafted
constraints or domain-specific knowledge, which limits their generalizability
and ability to capture complex, nonlinear relations inherent in data. Moreover,
they rarely accommodate user-specified preferences and suggest explanations
that are causally implausible or infeasible to act upon. We introduce RealAC, a
domain-agnostic framework for generating realistic and actionable
counterfactuals. RealAC automatically preserves complex inter-feature
dependencies without relying on explicit domain knowledge -- by aligning the
joint distributions of feature pairs between factual and counterfactual
instances. The framework also allows end-users to ``freeze'' attributes they
cannot or do not wish to change by suppressing change in frozen features during
optimization. Evaluations on three synthetic and two real datasets demonstrate
that RealAC balances realism with actionability. Our method outperforms
state-of-the-art baselines and Large Language Model-based counterfactual
generation techniques in causal edge score, dependency preservation score, and
IM1 realism metric and offers a solution for causality-aware and user-centric
counterfactual generation.

</details>


### [38] [X-Node: Self-Explanation is All We Need](https://arxiv.org/abs/2508.10461)
*Prajit Sengupta,Islem Rekik*

Main category: cs.LG

TL;DR: X-Node는 각 노드가 예측 과정의 일환으로 자신의 설명을 생성하는 자기 설명 GNN 프레임워크로, GNN의 해석 가능성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 고위험 임상 응용 프로그램에서 해석 가능성이 필수적이지만, 기존의 GNN 설명 기법은 개별 노드 결정이나 지역적 추론에 대한 제한된 통찰만 제공하여 신뢰성을 제한한다.

Method: X-Node는 각 노드가 자신의 예측 과정의 일환으로 설명을 생성하도록 하고, 구조화된 컨텍스트 벡터를 활용하여 노드의 잠재 임베딩을 재구성하고 자연어 설명을 생성하며, GNN의 메시지 전송 파이프라인으로 설명을 되먹이는 '텍스트 주입' 메커니즘을 활용한다.

Result: MedMNIST와 MorphoMNIST에서 파생된 두 개의 그래프 데이터셋에 대해 GCN, GAT, GIN 백본과 통합하여 X-Node를 평가하였다.

Conclusion: X-Node는 신뢰할 수 있는 각 노드에 대한 설명을 생성하면서 경쟁력 있는 분류 정확도를 유지함을 보여준다.

Abstract: Graph neural networks (GNNs) have achieved state-of-the-art results in
computer vision and medical image classification tasks by capturing structural
dependencies across data instances. However, their decision-making remains
largely opaque, limiting their trustworthiness in high-stakes clinical
applications where interpretability is essential. Existing explainability
techniques for GNNs are typically post-hoc and global, offering limited insight
into individual node decisions or local reasoning. We introduce X-Node, a
self-explaining GNN framework in which each node generates its own explanation
as part of the prediction process. For every node, we construct a structured
context vector encoding interpretable cues such as degree, centrality,
clustering, feature saliency, and label agreement within its local topology. A
lightweight Reasoner module maps this context into a compact explanation
vector, which serves three purposes: (1) reconstructing the node's latent
embedding via a decoder to enforce faithfulness, (2) generating a natural
language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)
guiding the GNN itself via a "text-injection" mechanism that feeds explanations
back into the message-passing pipeline. We evaluate X-Node on two graph
datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,
and GIN backbones. Our results show that X-Node maintains competitive
classification accuracy while producing faithful, per-node explanations.
Repository: https://github.com/basiralab/X-Node.

</details>


### [39] [Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers](https://arxiv.org/abs/2508.10480)
*Panagiotis D. Grontas,Antonio Terpin,Efe C. Balta,Raffaello D'Andrea,John Lygeros*

Main category: cs.LG

TL;DR: 본 논문에서는 볼록 제약 조건을 만족하는 신경망의 출력을 위한 새로운 층인 $\\Pi$net을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 볼록 제약 조건을 만족하면서도 빠르고 안정적인 해결책을 제공하기 위해.

Method: 운영자 분리를 통한 전방향 패스의 빠르고 신뢰할 수 있는 투영과 암묵 함수 정리를 사용한 역전파.

Result: 단일 문제를 해결할 때 전통적인 솔버보다 빠르고, 문제 배치에 대해서는 현저히 더 빠름을 입증하였습니다.

Conclusion: $\\Pi$net은 다중 차량 동작 계획 문제에서 비볼록 궤적 선호를 다루고 있으며, 효과적인 튜닝 휴리스틱을 갖춘 JAX로 구현된 GPU 준비 패키지로 제공됩니다.

Abstract: We introduce an output layer for neural networks that ensures satisfaction of
convex constraints. Our approach, $\Pi$net, leverages operator splitting for
rapid and reliable projections in the forward pass, and the implicit function
theorem for backpropagation. We deploy $\Pi$net as a feasible-by-design
optimization proxy for parametric constrained optimization problems and obtain
modest-accuracy solutions faster than traditional solvers when solving a single
problem, and significantly faster for a batch of problems. We surpass
state-of-the-art learning approaches in terms of training time, solution
quality, and robustness to hyperparameter tuning, while maintaining similar
inference times. Finally, we tackle multi-vehicle motion planning with
non-convex trajectory preferences and provide $\Pi$net as a GPU-ready package
implemented in JAX with effective tuning heuristics.

</details>


### [40] [GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation](https://arxiv.org/abs/2508.10471)
*Xinrui Li,Qilin Fan,Tianfu Wang,Kaiwen Wei,Ke Yu,Xu Zhang*

Main category: cs.LG

TL;DR: 연합 그래프 학습(FGL)은 여러 클라이언트가 개인의 분산 그래프 데이터를 공유하지 않고도 강력한 그래프 신경망을 협력적으로 훈련할 수 있게 한다. 그러나 FGL은 클라이언트 간 비독립 동질성에 심각한 도전에 직면해 있으며, 이는 클래스 불균형으로 인해 글로벌 모델이 다수 클래스에 편향되고 희귀하지만 중요한 사건을 식별하지 못하는 문제를 초래한다. 이를 해결하기 위해 제안하는 GraphFedMIG는 이 문제를 연합 생성 데이터 증강 작업으로 재구성하는 새로운 FGL 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 연합 그래프 학습에서의 비독립 동질성을 극복하고 클래스 불균형으로 인한 문제를 해결하기 위한 필요성.

Method: GraphFedMIG는 계층적 생성 적대 신경망을 사용하여 각 클라이언트가 고충실도 특성 표현을 합성하기 위한 로컬 생성기를 훈련시킨다. 클라이언트를 클러스터로 그룹화하여 각 클러스터가 전용 구분기를 공유하도록 한다. 또한, 상호 정보에 의해 유도되는 메커니즘을 통해 클라이언트 생성기의 진화를 안내한다.

Result: 제안된 GraphFedMIG는 네 가지 실제 데이터셋에 대한 광범위한 실험을 통해 다른 기준선과 비교하여 우수성을 입증하였다.

Conclusion: GraphFedMIG는 연합 그래프 학습의 클래스 불균형 문제를 효과적으로 해결할 수 있는 새로운 접근 방식을 제공한다.

Abstract: Federated graph learning (FGL) enables multiple clients to collaboratively
train powerful graph neural networks without sharing their private,
decentralized graph data. Inherited from generic federated learning, FGL is
critically challenged by statistical heterogeneity, where non-IID data
distributions across clients can severely impair model performance. A
particularly destructive form of this is class imbalance, which causes the
global model to become biased towards majority classes and fail at identifying
rare but critical events. This issue is exacerbated in FGL, as nodes from a
minority class are often surrounded by biased neighborhood information,
hindering the learning of expressive embeddings. To grapple with this
challenge, we propose GraphFedMIG, a novel FGL framework that reframes the
problem as a federated generative data augmentation task. GraphFedMIG employs a
hierarchical generative adversarial network where each client trains a local
generator to synthesize high-fidelity feature representations. To provide
tailored supervision, clients are grouped into clusters, each sharing a
dedicated discriminator. Crucially, the framework designs a mutual
information-guided mechanism to steer the evolution of these client generators.
By calculating each client's unique informational value, this mechanism
corrects the local generator parameters, ensuring that subsequent rounds of
mutual information-guided generation are focused on producing high-value,
minority-class features. We conduct extensive experiments on four real-world
datasets, and the results demonstrate the superiority of the proposed
GraphFedMIG compared with other baselines.

</details>


### [41] [On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations](https://arxiv.org/abs/2508.10490)
*Amir Mehrpanah,Matteo Gamba,Kevin Smith,Hossein Azizpour*

Main category: cs.LG

TL;DR: ReLU 네트워크의 설명 가능성을 분석하고, 매끄럽고 신뢰할 수 있는 설명을 위한 새로운 스펙트럼 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: ReLU 네트워크는 시각 데이터에서 흔히 사용되지만, 예측에서 개별 픽셀에 의존하여 설명이 시끄럽고 해석하기 어렵다.

Method: 스펙트럼 프레임워크를 사용하여 설명에서 매끄러움과 신뢰성의 기여를 정량화하고 규제하는 방법을 제시한다.

Result: 서바겐트 기반의 매끄럽게 하는 방식이 설명을 왜곡시켜 '설명 격차'를 발생시킴을 발견했다.

Conclusion: 이론적 발견을 다양한 디자인 선택, 데이터셋, 및 변형을 통해 검증하였다.

Abstract: ReLU networks, while prevalent for visual data, have sharp transitions,
sometimes relying on individual pixels for predictions, making vanilla
gradient-based explanations noisy and difficult to interpret. Existing methods,
such as GradCAM, smooth these explanations by producing surrogate models at the
cost of faithfulness. We introduce a unifying spectral framework to
systematically analyze and quantify smoothness, faithfulness, and their
trade-off in explanations. Using this framework, we quantify and regularize the
contribution of ReLU networks to high-frequency information, providing a
principled approach to identifying this trade-off. Our analysis characterizes
how surrogate-based smoothing distorts explanations, leading to an
``explanation gap'' that we formally define and measure for different post-hoc
methods. Finally, we validate our theoretical findings across different design
choices, datasets, and ablations.

</details>


### [42] [EDAPT: Towards Calibration-Free BCIs with Continual Online Adaptation](https://arxiv.org/abs/2508.10474)
*Lisa Haxel,Jaivardhan Kapoor,Ulf Ziemann,Jakob H. Macke*

Main category: cs.LG

TL;DR: EDAPT는 뇌-컴퓨터 인터페이스의 정확도 저하 문제를 해결하기 위해 지속적인 모델 적응을 통해 보정 필요성을 없애는 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: BCI의 정확도 저하 문제와 사용자 간 신호의 변동성으로 인해 빈번한 재보정이 필요하여 실제 배치에 한계를 두기 위함이다.

Method: EDAPT는 다수 사용자의 데이터를 이용해 기준 디코더를 훈련한 후, 신경 패턴 변화에 따라 감독된 미세 조정을 통해 지속적으로 개인화된 모델로 발전시킨다.

Result: EDAPT는 9개의 데이터 세트에서 3개의 BCI 작업에 대해 테스트되었으며, 전통적이고 정적인 방법보다 일관되게 정확도를 향상시킨 것으로 나타났다.

Conclusion: EDAPT는 BCI 도입에 대한 주요 장벽을 줄여주는 보정 없는 BCI로의 실용적인 경로를 제공한다.

Abstract: Brain-computer interfaces (BCIs) suffer from accuracy degradation as neural
signals drift over time and vary across users, requiring frequent recalibration
that limits practical deployment. We introduce EDAPT, a task- and
model-agnostic framework that eliminates calibration through continual model
adaptation. EDAPT first trains a baseline decoder using data from multiple
users, then continually personalizes this model via supervised finetuning as
the neural patterns evolve during use. We tested EDAPT across nine datasets
covering three BCI tasks, and found that it consistently improved accuracy over
conventional, static methods. These improvements primarily stem from combining
population-level pretraining and online continual finetuning, with unsupervised
domain adaptation providing further gains on some datasets. EDAPT runs
efficiently, updating models within 200 milliseconds on consumer-grade
hardware. Finally, decoding accuracy scales with total data budget rather than
its allocation between subjects and trials. EDAPT provides a practical pathway
toward calibration-free BCIs, reducing a major barrier to BCI deployment.

</details>


### [43] [Contrastive ECOC: Learning Output Codes for Adversarial Defense](https://arxiv.org/abs/2508.10491)
*Che-Yu Chou,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 본 논문에서는 자동화된 코드북 학습을 위한 세 가지 모델을 제안하며, 이 모델들은 대조 학습을 기반으로 하여 데이터를 통해 직접적이고 적응적으로 학습할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 다중 클래스 분류에서 기존의 원-핫 인코딩 방법은 최적의 인코딩 메커니즘이 아닐 수 있으며, 수동으로 설계되거나 무작위로 생성된 코드북에 의존하는 기존 ECOC 방법의 한계가 있다.

Method: 대조 학습을 기반으로 한 세 가지 모델을 통해 코드북을 자동으로 학습할 수 있도록 했다.

Result: 제안된 모델들은 네 개의 데이터셋에서 두 개의 기준 모델에 비해 적대적 공격에 대해 우수한 견고성을 보인다.

Conclusion: 이 연구는 자동화된 코드북 학습이 다중 클래스 분류에서의 성능을 향상시킬 수 있음을 보여준다.

Abstract: Although one-hot encoding is commonly used for multiclass classification, it
is not always the most effective encoding mechanism. Error Correcting Output
Codes (ECOC) address multiclass classification by mapping each class to a
unique codeword used as a label. Traditional ECOC methods rely on manually
designed or randomly generated codebooks, which are labor-intensive and may
yield suboptimal, dataset-agnostic results. This paper introduces three models
for automated codebook learning based on contrastive learning, allowing
codebooks to be learned directly and adaptively from data. Across four
datasets, our proposed models demonstrate superior robustness to adversarial
attacks compared to two baselines. The source is available at
https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.

</details>


### [44] [Confounding is a Pervasive Problem in Real World Recommender Systems](https://arxiv.org/abs/2508.10479)
*Alexander Merkov,David Rohde,Alexandre Gilotte,Benjamin Heymann*

Main category: cs.LG

TL;DR: 이 논문은 추천 시스템에서 관찰되지 않은 혼란변수가 어떻게 발생하는지를 보여주고, 일반적인 관행이 오히려 혼란을 초래할 수 있음을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 추천 시스템에 관찰되지 않은 혼란변수 문제를 해결하기 위한 필요성이 있다.

Method: 기능 공학, A/B 테스트, 모듈화와 같은 일반적인 관행의 영향을 분석하고 시뮬레이션 연구를 통해 이를 입증한다.

Result: 일반적인 관행들이 추천 시스템에 혼란을 도입하여 성능을 저하시킬 수 있음을 보여준다.

Conclusion: 실제 시스템에서 혼란의 영향을 줄이거나 피하기 위한 실용적인 제안을 제공한다.

Abstract: Unobserved confounding arises when an unmeasured feature influences both the
treatment and the outcome, leading to biased causal effect estimates. This
issue undermines observational studies in fields like economics, medicine,
ecology or epidemiology. Recommender systems leveraging fully observed data
seem not to be vulnerable to this problem. However many standard practices in
recommender systems result in observed features being ignored, resulting in
effectively the same problem. This paper will show that numerous common
practices such as feature engineering, A/B testing and modularization can in
fact introduce confounding into recommendation systems and hamper their
performance. Several illustrations of the phenomena are provided, supported by
simulation studies with practical suggestions about how practitioners may
reduce or avoid the affects of confounding in real systems.

</details>


### [45] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 장기 강화 학습에서 보상 희소성 문제를 해결하기 위해, 소프트웨어 공학(SWE) 작업에 적합한 새로운 RL 프레임워크인 SWE-oriented RL Framework와 보상 누적 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 장기 강화 학습 작업에서 보상 희소성을 극복하고 소프트웨어 공학의 복잡한 요구를 충족시키기 위해.

Method: SWE-oriented RL Framework를 도입하고 Gated Reward Accumulation (G-RA) 방식을 사용하여 고수준 보상이 미리 정의된 기준을 충족할 때만 즉각적인 보상을 누적한다.

Result: 실험 결과 G-RA는 완료율과 수정율을 각각 증가시키며 보상 불일치로 인한 정책 저하를 방지한다.

Conclusion: 장기 강화 학습에서 균형 잡힌 보상 누적의 중요성을 강조하며, 실용적인 해결책을 제시한다.

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [46] [Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures](https://arxiv.org/abs/2508.10489)
*Jonas Ulmen,Ganesh Sundaram,Daniel Görges*

Main category: cs.LG

TL;DR: 이 논문은 JEPAs를 활용하여 연속 시간 동적 시스템을 이용한 세계 모델 작성 기술을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 재구성 기반 방법보다 더 발전된 Joint Embedding Predictive Architectures(JEPAs)의 출현에 따른 모델링 접근 방법의 필요성.

Method: 순차 임베딩을 신경형일반미분방정식(neural ODE)과 통합하고, 계약 임베딩 및 Lipschitz 상수를 강제하는 손실 함수를 사용하여 잠재 상태 공간을 구성한다.

Result: 단순 진자 시스템을 위해 이미지 데이터만을 사용하여 구조화된 잠재 상태 공간 모델을 생성함으로써 접근법의 효과를 입증한다.

Conclusion: 로봇 공학에 광범위한 응용이 있는 보다 일반적인 제어 알고리즘 및 추정 기술 개발을 위한 새로운 기술을 제시한다.

Abstract: With the advent of Joint Embedding Predictive Architectures (JEPAs), which
appear to be more capable than reconstruction-based methods, this paper
introduces a novel technique for creating world models using continuous-time
dynamic systems from arbitrary observation data. The proposed method integrates
sequence embeddings with neural ordinary differential equations (neural ODEs).
It employs loss functions that enforce contractive embeddings and Lipschitz
constants in state transitions to construct a well-organized latent state
space. The approach's effectiveness is demonstrated through the generation of
structured latent state-space models for a simple pendulum system using only
image data. This opens up a new technique for developing more general control
algorithms and estimation techniques with broad applications in robotics.

</details>


### [47] [FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection](https://arxiv.org/abs/2508.10594)
*Yunfeng Zhao,Yixin Liu,Shiyuan Li,Qingfeng Chen,Yu Zheng,Shirui Pan*

Main category: cs.LG

TL;DR: FreeGAD는 훈련이 필요 없는 효과적인 그래프 이상 탐지 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 깊이 기반 이상 탐지 방법의 높은 배포 비용과 낮은 확장성 문제를 해결하기 위해.

Method: Affinity-gated residual encoder를 활용하여 이상 인식 표현을 생성하고, 앵커 노드를 식별하여 통계적 편차를 계산합니다.

Result: FreeGAD는 다양한 도메인의 여러 벤치마크 데이터셋에서 이상 탐지 성능, 효율성 및 확장성을 개선합니다.

Conclusion: 훈련이나 반복 최적화 없이도 우수한 성능을 보입니다.

Abstract: Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the
majority within a graph, playing a crucial role in applications such as social
networks and e-commerce. Despite the current advancements in deep
learning-based GAD, existing approaches often suffer from high deployment costs
and poor scalability due to their complex and resource-intensive training
processes. Surprisingly, our empirical findings suggest that the training phase
of deep GAD methods, commonly perceived as crucial, may actually contribute
less to anomaly detection performance than expected. Inspired by this, we
propose FreeGAD, a novel training-free yet effective GAD method. Specifically,
it leverages an affinity-gated residual encoder to generate anomaly-aware
representations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal
and anomalous guides, followed by calculating anomaly scores through
anchor-guided statistical deviations. Extensive experiments demonstrate that
FreeGAD achieves superior anomaly detection performance, efficiency, and
scalability on multiple benchmark datasets from diverse domains, without any
training or iterative optimization.

</details>


### [48] [On Spectral Properties of Gradient-based Explanation Methods](https://arxiv.org/abs/2508.10595)
*Amir Mehrpanah,Erik Englesson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 딥 네트워크의 행동을 이해하는 것은 결과에 대한 신뢰를 높이는 데 중요하다. 본 연구는 설명 방법을 형식적으로 분석하기 위해 새로운 확률적 및 스펙트럼 관점을 채택하고 각종 디자인 선택에서 발생하는 스펙트럴 편향을 밝혀냈다.


<details>
  <summary>Details</summary>
Motivation: 딥 네트워크의 결과에 대한 신뢰성을 높이기 위해 네트워크의 행동을 이해하는 것이 중요하다.

Method: 확률적 및 스펙트럼 관점을 채택하여 설명 방법을 형식적으로 분석하고, 스퀘어드 그래디언트와 입력 교란과 같은 일반적인 디자인 선택을 조사했다.

Result: 스펙트럴 편향이 그래디언트 사용에서 비롯된다는 것을 발견하고, SmoothGrad와 같은 설명 방법의 교란 하이퍼파라미터 선택이 일관되지 않은 설명으로 이어질 수 있음을 규명했다.

Conclusion: 표준 교란 스케일을 결정하는 메커니즘과 SpectralLens라는 집계 방법을 제안하며, 이론적 결과를 정량적 평가를 통해 입증했다.

Abstract: Understanding the behavior of deep networks is crucial to increase our
confidence in their results. Despite an extensive body of work for explaining
their predictions, researchers have faced reliability issues, which can be
attributed to insufficient formalism. In our research, we adopt novel
probabilistic and spectral perspectives to formally analyze explanation
methods. Our study reveals a pervasive spectral bias stemming from the use of
gradient, and sheds light on some common design choices that have been
discovered experimentally, in particular, the use of squared gradient and input
perturbation. We further characterize how the choice of perturbation
hyperparameters in explanation methods, such as SmoothGrad, can lead to
inconsistent explanations and introduce two remedies based on our proposed
formalism: (i) a mechanism to determine a standard perturbation scale, and (ii)
an aggregation method which we call SpectralLens. Finally, we substantiate our
theoretical results through quantitative evaluations.

</details>


### [49] [SPHENIC: Topology-Informed Multi-View Clustering for Spatial Transcriptomics](https://arxiv.org/abs/2508.10646)
*Chenkai Guo,Yikai Zhu,Jing Yangum,Renxiang Guan,Por Lip Yee,Guangdun Peng,Dayu Hu*

Main category: cs.LG

TL;DR: SPHENIC은 공간 위치 정보를 통합하여 세포 하위 집단 식별을 위한 클러스터링의 통찰력을 향상시키는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법의 두 가지 한계점을 극복하기 위해.

Method: SPHENIC은 불변 위상적 특성을 클러스터링 네트워크에 통합하고, 공간 제약 및 분포 최적화 모듈(SCDOM)을 설계하여 고품질 공간 임베딩을 생성한다.

Result: SPHENIC은 14개의 벤치마크 공간 전사체 조각에서 우수한 성능을 보였으며, 기존 최첨단 방법보다 3.31%-6.54% 더 나은 성능을 달성했다.

Conclusion: SPHENIC은 세포의 진정한 분포를 반영하는 정밀 클러스터링을 가능하게 함으로써 공간 클러스터링 작업에서 뛰어난 성과를 보여주었다.

Abstract: By incorporating spatial location information, spatial-transcriptomics
clustering yields more comprehensive insights into cell subpopulation
identification. Despite recent progress, existing methods have at least two
limitations: (i) topological learning typically considers only representations
of individual cells or their interaction graphs; however, spatial
transcriptomic profiles are often noisy, making these approaches vulnerable to
low-quality topological signals, and (ii) insufficient modeling of spatial
neighborhood information leads to low-quality spatial embeddings. To address
these limitations, we propose SPHENIC, a novel Spatial Persistent Homology
Enhanced Neighborhood Integrative Clustering method. Specifically, SPHENIC
incorporates invariant topological features into the clustering network to
achieve stable representation learning. Additionally, to construct high-quality
spatial embeddings that reflect the true cellular distribution, we design the
Spatial Constraint and Distribution Optimization Module (SCDOM). This module
increases the similarity between a cell's embedding and those of its spatial
neighbors, decreases similarity with non-neighboring cells, and thereby
produces clustering-friendly spatial embeddings. Extensive experiments on 14
benchmark spatial transcriptomic slices demonstrate that SPHENIC achieves
superior performance on the spatial clustering task, outperforming existing
state-of-the-art methods by 3.31%-6.54% over the best alternative.

</details>


### [50] [REFN: A Reinforcement-Learning-From-Network Framework against 1-day/n-day Exploitations](https://arxiv.org/abs/2508.10701)
*Tianlong Yu,Lihong Liu,Ziyi Zhou,Fudu Xing,Kailong Wang,Yang Yang*

Main category: cs.LG

TL;DR: REFN은 1일 및 n일 취약점을 방지하기 위해 LLMs(대형 언어 모델)을 훈련시키는 프레임워크로, 온라인 네트워크 보상을 활용하여 확장성을 보장하고 에지 보안 게이트웨이에서 통합 배포를 통해 호환성을 확보합니다.


<details>
  <summary>Details</summary>
Motivation: 1일 및 n일 취약점의 악용은 네트워크 장치에 심각한 위협을 초래합니다. 기존 방어 방식은 한계가 있습니다.

Method: REFN은 강화 학습(RL)을 사용하여 LLM이 자율적으로 네트워크 필터를 생성하도록 훈련합니다.

Result: REFN은 22개의 1일 및 n일 악용 패밀리에 대해 평가되었으며, 다른 대안보다 21.1% 높은 정확도를 보여줍니다.

Conclusion: REFN은 대규모 1일 및 n일 악용 방지를 위한 LLM 훈련의 초기 단계로 작용합니다.

Abstract: The exploitation of 1 day or n day vulnerabilities poses severe threats to
networked devices due to massive deployment scales and delayed patching
(average Mean Time To Patch exceeds 60 days). Existing defenses, including host
based patching and network based filtering, are inadequate due to limited
scalability across diverse devices, compatibility issues especially with
embedded or legacy systems, and error prone deployment process (manual patch
validation). To address these issues, we introduce REFN (Reinforcement Learning
From Network), a novel framework that trains Large Language Models (LLMs) to
autonomously generate network filters to prevent 1 day or n day exploitations.
REFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven
by online network rewards instead of traditional Human Feedback (RLHF). REFN
guarantees compatibility via unified deployment on edge security gateways
(Amazon Eero). REFN provides robustness via online validation using real
network traffic. Crucially, REFN addresses three core challenges in training
LLMs for exploit prevention: 1) expanding current LLMs limited vulnerability
fixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging
current LLMs language to network gaps through an RL From VNF Pipeline that
translates language context (vulnerability description) into network
enforcement, 3) addressing the LLM hallucination and non determinism via the
Online Agentic Validation that penalizes erroneous outputs. Evaluated across 22
families of 1 day or n day exploits, REFN demonstrates effectiveness (21.1
percent higher accuracy than alternatives), efficiency (Mean Time To Patch of
3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an
initial step toward training LLMs to rapidly prevent massive scale 1 day or n
day exploitations.

</details>


### [51] [Nonlocal Monte Carlo via Reinforcement Learning](https://arxiv.org/abs/2508.10520)
*Dmitrii Dobrynin,Masoud Mohseni,John Paul Strachan*

Main category: cs.LG

TL;DR: 비균형 비지역 몬테 카를로(NMC) 알고리즘을 사용한 딥 강화 학습(RL) 기반의 최적화 접근법을 제안하며, 기존의 MCMC 알고리즘보다 더 나은 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 복합비용 함수의 최적화는 오랜 도전 과제로, 기존 MCMC 알고리즘이 어려운 벤치마크에서 효과적이지 못하다는 것을 보여준다.

Method: NMC 알고리즘과 딥 강화 학습을 결합하여 비지역 전이 정책을 훈련시킨다.

Result: 딥 강화 학습을 통해 훈련된 솔버는 에너지 변화와 기하학적 정보를 사용하여 효율적으로 학습하며, 기존 방법들보다 향상된 성능을 발휘한다.

Conclusion: 훈련된 정책은 잔여 에너지, 솔루션 도출 시간, 솔루션 다양성 측면에서 기존 MCMC 기반 알고리즘보다 개선된 성능을 보인다.

Abstract: Optimizing or sampling complex cost functions of combinatorial optimization
problems is a longstanding challenge across disciplines and applications. When
employing family of conventional algorithms based on Markov Chain Monte Carlo
(MCMC) such as simulated annealing or parallel tempering, one assumes
homogeneous (equilibrium) temperature profiles across input. This instance
independent approach was shown to be ineffective for the hardest benchmarks
near a computational phase transition when the so-called overlap-gap-property
holds. In these regimes conventional MCMC struggles to unfreeze rigid
variables, escape suboptimal basins of attraction, and sample high-quality and
diverse solutions. In order to mitigate these challenges, Nonequilibrium
Nonlocal Monte Carlo (NMC) algorithms were proposed that leverage inhomogeneous
temperature profiles thereby accelerating exploration of the configuration
space without compromising its exploitation. Here, we employ deep reinforcement
learning (RL) to train the nonlocal transition policies of NMC which were
previously designed phenomenologically. We demonstrate that the resulting
solver can be trained solely by observing energy changes of the configuration
space exploration as RL rewards and the local minimum energy landscape geometry
as RL states. We further show that the trained policies improve upon the
standard MCMC-based and nonlocal simulated annealing on hard uniform random and
scale-free random 4-SAT benchmarks in terms of residual energy,
time-to-solution, and diversity of solutions metrics.

</details>


### [52] [Electromagnetic Simulations of Antennas on GPUs for Machine Learning Applications](https://arxiv.org/abs/2508.10713)
*Murat Temiz,Vemund Bakken*

Main category: cs.LG

TL;DR: 본 연구는 GPU 기반의 개방형 전자기 시뮬레이션 소프트웨어를 이용한 안테나 시뮬레이션 프레임워크를 제안하고, 기계 학습 및 최적화 응용을 위한 결과를 상업용 소프트웨어와 비교한다.


<details>
  <summary>Details</summary>
Motivation: 안테나 설계 및 최적화의 기계 학습 응용을 위한 안테나 시뮬레이션의 필요성.

Method: GPU를 활용하여 미리 정의된 또는 임의의 안테나 형상 매개변수를 가진 다수의 안테나를 시뮬레이션하여 데이터 세트를 생성하는 방법.

Result: 입문자용 GPU가 고급 CPU보다 계산 성능이 월등하며, 고급 게임 GPU는 고급 CPU에 비해 약 18배 더 뛰어난 성능을 제공한다.

Conclusion: 오픈소스 EM 시뮬레이션 소프트웨어가 상업용 소프트웨어와 유사한 결과를 제공할 수 있음을 보여준다.

Abstract: This study proposes an antenna simulation framework powered by graphics
processing units (GPUs) based on an open-source electromagnetic (EM) simulation
software (gprMax) for machine learning applications of antenna design and
optimization. Furthermore, it compares the simulation results with those
obtained through commercial EM software. The proposed software framework for
machine learning and surrogate model applications will produce antenna data
sets consisting of a large number of antenna simulation results using GPUs.
Although machine learning methods can attain the optimum solutions for many
problems, they are known to be data-hungry and require a great deal of samples
for the training stage of the algorithms. However, producing a sufficient
number of training samples in EM applications within a limited time is
challenging due to the high computational complexity of EM simulations.
Therefore, GPUs are utilized in this study to simulate a large number of
antennas with predefined or random antenna shape parameters to produce data
sets. Moreover, this study also compares various machine learning and deep
learning models in terms of antenna parameter estimation performance. This
study demonstrates that an entry-level GPU substantially outperforms a high-end
CPU in terms of computational performance, while a high-end gaming GPU can
achieve around 18 times more computational performance compared to a high-end
CPU. Moreover, it is shown that the open-source EM simulation software can
deliver similar results to those obtained via commercial software in the
simulation of microstrip antennas when the spatial resolution of the
simulations is sufficiently fine.

</details>


### [53] [Projected Coupled Diffusion for Test-Time Constrained Joint Generation](https://arxiv.org/abs/2508.10531)
*Hao Luan,Yi Xian Goh,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: PCD는 제약 조건을 가진 공동 생성을 위한 새로운 테스트 시간 프레임워크로, 여러 사전 훈련된 확산 모델에서의 샘플 생성 시 제약 조건을 만족시키며 과도한 재훈련 없이 효과적인 결과를 보인다.


<details>
  <summary>Details</summary>
Motivation: 테스트 시간 샘플링의 수정은 전체 확산 모델을 재훈련하지 않고도 주어진 목표를 달성하기 위해 생성 프로세스를 편향시키는 중요한 확장으로 부상하고 있다.

Method: 본 연구에서 우리는 PCD라는 새로운 테스트 시간 프레임워크를 제안하여, 결합된 가이던스 항을 생성 동역학에 도입하고 각 확산 단계에서 하드 제약을 적용하는 프로젝션 단계를 포함시켰다.

Result: 적용 시나리오에서 PCD의 효과를 empirically 입증하며, 샘플들 간의 결합 효과를 개선하고 제약 조건을 과도한 계산 비용 없이 보장하는 결과를 보여준다.

Conclusion: PCD는 이미지 쌍 생성, 물체 조작 및 다중 로봇 운동 계획과 같은 응용 시나리오에서 유용한 프레임워크로 입증되었다.

Abstract: Modifications to test-time sampling have emerged as an important extension to
diffusion algorithms, with the goal of biasing the generative process to
achieve a given objective without having to retrain the entire diffusion model.
However, generating jointly correlated samples from multiple pre-trained
diffusion models while simultaneously enforcing task-specific constraints
without costly retraining has remained challenging. To this end, we propose
Projected Coupled Diffusion (PCD), a novel test-time framework for constrained
joint generation. PCD introduces a coupled guidance term into the generative
dynamics to encourage coordination between diffusion models and incorporates a
projection step at each diffusion step to enforce hard constraints.
Empirically, we demonstrate the effectiveness of PCD in application scenarios
of image-pair generation, object manipulation, and multi-robot motion planning.
Our results show improved coupling effects and guaranteed constraint
satisfaction without incurring excessive computational costs.

</details>


### [54] [APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares](https://arxiv.org/abs/2508.10732)
*Kejia Fan,Jianheng Tang,Zhirui Yang,Feijiang Han,Jiaxu Li,Run He,Yajiang Huang,Anfeng Liu,Houbing Herbert Song,Yunhuai Liu,Huiping Zhuang*

Main category: cs.LG

TL;DR: 개인화된 연합 학습(PFL)은 비독립적으로 동질적이지 않은 데이터(non-IID)에 취약하여 개인화 모델 전달에 도전을 줌. 이 문제를 해결하기 위해 APFL 방식 제안.


<details>
  <summary>Details</summary>
Motivation: PFL의 개인화 모델을 개별 클라이언트에게 효과적으로 전달하는 것이 필요하다.

Method: 이중 스트림 최소 제곱법을 통한 분석적 개인화 연합 학습(APFL) 접근 방식 제안.

Result: APFL은 정확도에서 최소 1.10%-15.45%의 이점을 제공하며, 다양한 데이터셋에서 기존 방법과 비교하여 우수성을 입증함.

Conclusion: APFL은 데이터 이질성에 관계없이 개인화된 모델을 동일하게 유지하며, 개인화의 효율성을 증가시킨다.

Abstract: Personalized Federated Learning (PFL) has presented a significant challenge
to deliver personalized models to individual clients through collaborative
training. Existing PFL methods are often vulnerable to non-IID data, which
severely hinders collective generalization and then compromises the subsequent
personalization efforts. In this paper, to address this non-IID issue in PFL,
we propose an Analytic Personalized Federated Learning (APFL) approach via
dual-stream least squares. In our APFL, we use a foundation model as a frozen
backbone for feature extraction. Subsequent to the feature extractor, we
develop dual-stream analytic models to achieve both collective generalization
and individual personalization. Specifically, our APFL incorporates a shared
primary stream for global generalization across all clients, and a dedicated
refinement stream for local personalization of each individual client. The
analytical solutions of our APFL enable its ideal property of heterogeneity
invariance, theoretically meaning that each personalized model remains
identical regardless of how heterogeneous the data are distributed across all
other clients. Empirical results across various datasets also validate the
superiority of our APFL over state-of-the-art baselines, with advantages of at
least 1.10%-15.45% in accuracy.

</details>


### [55] [Driving Accurate Allergen Prediction with Protein Language Models and Generalization-Focused Evaluation](https://arxiv.org/abs/2508.10541)
*Brian Shing-Hei Wong,Joshua Mincheol Kim,Sin-Hang Fung,Qing Xiong,Kelvin Fu-Kiu Ao,Junkang Wei,Ran Wang,Dan Michelle Wang,Jingying Zhou,Bo Feng,Alfred Sze-Lok Cheng,Kevin Y. Yip,Stephen Kwok-Wing Tsui,Qin Cao*

Main category: cs.LG

TL;DR: Applm은 1000억 개 매개변수를 가진 xTrimoPGLM 단백질 언어 모델을 활용하여 알레르겐 단백질을 정확하게 식별하는 컴퓨팅 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 알레르겐 단백질의 정확한 식별은 공공 건강에 중요한 도전 과제입니다.

Method: Applm은 xTrimoPGLM 단백질 언어 모델을 사용하여 여러 현실 세계 시나리오와 유사한 작업을 수행합니다.

Result: Applm은 일곱 개의 최신 방법보다 consistently 돋보이는 성능을 보여줍니다.

Conclusion: Applm을 오픈 소스 소프트웨어로 제공하며, 향후 연구를 지원하기 위해 정제된 벤치마크 데이터 세트를 제공합니다.

Abstract: Allergens, typically proteins capable of triggering adverse immune responses,
represent a significant public health challenge. To accurately identify
allergen proteins, we introduce Applm (Allergen Prediction with Protein
Language Models), a computational framework that leverages the 100-billion
parameter xTrimoPGLM protein language model. We show that Applm consistently
outperforms seven state-of-the-art methods in a diverse set of tasks that
closely resemble difficult real-world scenarios. These include identifying
novel allergens that lack similar examples in the training set, differentiating
between allergens and non-allergens among homologs with high sequence
similarity, and assessing functional consequences of mutations that create few
changes to the protein sequences. Our analysis confirms that xTrimoPGLM,
originally trained on one trillion tokens to capture general protein sequence
characteristics, is crucial for Applm's performance by detecting important
differences among protein sequences. In addition to providing Applm as
open-source software, we also provide our carefully curated benchmark datasets
to facilitate future research.

</details>


### [56] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: 본 논문은 검증 가능한 보상(RLVR)에서 Pass@k 보상을 사용하여 정책 모델을 훈련시키고 탐색 능력을 개선하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 검증 가능한 보상으로 일반적으로 Pass@1을 사용하는 RLVR은 탐색과 활용의 균형을 맞추는 데 어려움을 겪고 있으며, 이로 인해 정책은 보수적인 행동을 선호하고 지역 최적점으로 수렴한다.

Method: Pass@k를 보상으로 사용하여 정책 모델을 훈련시킨 후 탐색 능력의 개선을 관찰하고, Pass@k 훈련의 장점에 대한 분석 솔루션을 도출하여 효율적이고 효과적인 과정을 제시한다.

Result: 탐색과 활용은 본질적으로 상충하는 목표가 아니며 서로를 상호 강화할 수 있다는 점을 발견하고, Pass@k 훈련이 직접적으로 장점 함수를 설계하는 것과 관련이 있음을 강조한다.

Conclusion: RLVR을 위한 장점 설계를 탐색하여 유망한 결과를 보여주고, 향후 연구 방향을 제시 한다.

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [57] [Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets](https://arxiv.org/abs/2508.10758)
*Nicolas Lapautre,Maria Marchenko,Carlos Miguel Patiño,Xin Zhou*

Main category: cs.LG

TL;DR: 이 논문에서는 물리 시스템의 대규모 데이터셋에서 트랜스포머의 효율성을 높이기 위해 Erwin 아키텍처와 Native Sparse Attention(NSA) 메커니즘을 결합하는 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 물리 시스템 데이터셋에서 트랜스포머의 잠재력을 극대화하고 주의 메커니즘의 제곱 규모 문제를 극복하는 것이 필요하다.

Method: NSAD 메커니즘을 비순차 데이터에 맞게 조정하고, Erwin NSA 모델을 구현하여 세 가지 물리 과학 데이터셋에서 평가했다.

Result: 신뢰성과 성능을 높이며, 원래의 Erwin 모델과 동등하거나 그 이상의 성능을 달성했다.

Conclusion: Erwin 논문의 실험 결과를 재현하여 그 구현의 유효성을 검증했다.

Abstract: Unlocking the potential of transformers on datasets of large physical systems
depends on overcoming the quadratic scaling of the attention mechanism. This
work explores combining the Erwin architecture with the Native Sparse Attention
(NSA) mechanism to improve the efficiency and receptive field of transformer
models for large-scale physical systems, addressing the challenge of quadratic
attention complexity. We adapt the NSA mechanism for non-sequential data,
implement the Erwin NSA model, and evaluate it on three datasets from the
physical sciences -- cosmology simulations, molecular dynamics, and air
pressure modeling -- achieving performance that matches or exceeds that of the
original Erwin model. Additionally, we reproduce the experimental results from
the Erwin paper to validate their implementation.

</details>


### [58] [Technical Report: Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot](https://arxiv.org/abs/2508.10581)
*Jeroen Berrevoets,Julianna Piskorz,Robert Davis,Harry Amad,Jim Weatherall,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: CATE-B는 치료 효과 추정을 위한 오픈 소스 시스템으로, 사용자가 치료 효과 추정 과정을 안내받을 수 있도록 대규모 언어 모델을 활용한다.


<details>
  <summary>Details</summary>
Motivation: 관찰 데이터에서 치료 효과를 추정하는 것은 의료, 경제 및 공공 정책 등 다양한 분야에서 중요하지만 복잡한 작업이다.

Method: CATE-B는 인과적인 발견과 대규모 언어 모델 기반의 엣지 방향 설정을 통해 구조적 인과 모델을 구축하고, 새로운 최소 불확실성 조정 세트 기준을 통해 견고한 조정 세트를 식별하며, 인과 구조와 데이터 세트 특성에 맞는 회귀 방법을 선택하도록 돕는다.

Result: CATE-B는 다양한 도메인과 인과적 복잡성을 아우르는 벤치마크 작업 세트를 발표하여 재현성과 평가를 촉진한다.

Conclusion: CATE-B는 인과 추론과 지능형 상호작용 지원을 결합하여 엄격한 인과 분석에 대한 장벽을 낮추고 자동화된 치료 효과 추정의 새로운 벤치마크 클래스의 기초를 마련한다.

Abstract: Estimating treatment effects (TE) from observational data is a critical yet
complex task in many fields, from healthcare and economics to public policy.
While recent advances in machine learning and causal inference have produced
powerful estimation techniques, their adoption remains limited due to the need
for deep expertise in causal assumptions, adjustment strategies, and model
selection. In this paper, we introduce CATE-B, an open-source co-pilot system
that uses large language models (LLMs) within an agentic framework to guide
users through the end-to-end process of treatment effect estimation. CATE-B
assists in (i) constructing a structural causal model via causal discovery and
LLM-based edge orientation, (ii) identifying robust adjustment sets through a
novel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting
appropriate regression methods tailored to the causal structure and dataset
characteristics. To encourage reproducibility and evaluation, we release a
suite of benchmark tasks spanning diverse domains and causal complexities. By
combining causal inference with intelligent, interactive assistance, CATE-B
lowers the barrier to rigorous causal analysis and lays the foundation for a
new class of benchmarks in automated treatment effect estimation.

</details>


### [59] [Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection](https://arxiv.org/abs/2508.10785)
*Shouju Wang,Yuchen Song,Sheng'en Li,Dongmian Zou*

Main category: cs.LG

TL;DR: 본 연구에서는 불공정한 결과를 초래할 수 있는 GNN 기반의 그래프 이상 탐지(GAD) 모델의 공정성을 다루기 위해, DECAF-GAD라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 그래프 이상 탐지(GAD)는 다양한 분야에서 점점 더 중요한 작업이 되고 있으며, GNN의 발전으로 GAD 방법이 성능 개선을 실현하였습니다. 그러나 GAD의 공정성 문제는 충분히 탐구되지 않았습니다.

Method: DECAF-GAD는 민감한 속성을 학습된 표현에서 분리하기 위해 구조적 인과 모델(SCM)을 도입하고, 이를 기반으로 한 특수화된 오토인코더 아키텍처와 공정성 가이드 손실 함수를 이용합니다.

Result: DECAF-GAD는 경쟁력 있는 이상 탐지 성능을 발휘하며, 기존 GAD 방법에 비해 공정성 지표를 유의미하게 향상시킵니다.

Conclusion: DECAF-GAD는 GAD 성능을 유지하면서 편향을 완화하는 효과적인 접근 방식을 제시합니다.

Abstract: Graph anomaly detection (GAD) has become an increasingly important task
across various domains. With the rapid development of graph neural networks
(GNNs), GAD methods have achieved significant performance improvements.
However, fairness considerations in GAD remain largely underexplored. Indeed,
GNN-based GAD models can inherit and amplify biases present in training data,
potentially leading to unfair outcomes. While existing efforts have focused on
developing fair GNNs, most approaches target node classification tasks, where
models often rely on simple layer architectures rather than autoencoder-based
structures, which are the most widely used architecturs for anomaly detection.
To address fairness in autoencoder-based GAD models, we propose
\textbf{D}is\textbf{E}ntangled \textbf{C}ounterfactual \textbf{A}dversarial
\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving
GAD performance. Specifically, we introduce a structural causal model (SCM) to
disentangle sensitive attributes from learned representations. Based on this
causal framework, we formulate a specialized autoencoder architecture along
with a fairness-guided loss function. Through extensive experiments on both
synthetic and real-world datasets, we demonstrate that DECAF-GAD not only
achieves competitive anomaly detection performance but also significantly
enhances fairness metrics compared to baseline GAD methods. Our code is
available at https://github.com/Tlhey/decaf_code.

</details>


### [60] [GNN-based Unified Deep Learning](https://arxiv.org/abs/2508.10583)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: 이 논문은 의료 이미징에서의 일반화 문제를 해결하기 위해 통합 학습 개념을 제안하며, 다양한 모델을 공유 그래프 공간에서 통합하여 일반화 능력을 향상시키는 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: 의료 이미징에서의 도메인 차단 시나리오에서 깊이 학습 모델의 일반화 문제를 해결하고, 각 병원이 지역 데이터에 맞는 별도의 모델을 훈련해야 하는 필요성을 인식한다.

Method: 각 모델을 그래프 표현으로 인코딩하여 공유 그래프 학습 공간에서 통합할 수 있는 통합 학습 패러다임을 제안하고, 이를 통해 최적화를 안내하는 GNN을 활용한다.

Result: MorphoMNIST 및 PneumoniaMNIST, BreastMNIST를 포함한 두 개의 MedMNIST 벤치마크에서 평가한 결과, 통합 학습이 독특한 분포에서 훈련된 모델이 혼합된 분포에서 테스트될 때 성능을 향상시킨다.

Conclusion: 유니파이드 GNN을 통해 다양한 아키텍처 및 분포를 아우르는 파라미터 공유와 지식 전이를 지원하여 일반화 능력을 향상시킨다.

Abstract: Deep learning models often struggle to maintain generalizability in medical
imaging, particularly under domain-fracture scenarios where distribution shifts
arise from varying imaging techniques, acquisition protocols, patient
populations, demographics, and equipment. In practice, each hospital may need
to train distinct models - differing in learning task, width, and depth - to
match local data. For example, one hospital may use Euclidean architectures
such as MLPs and CNNs for tabular or grid-like image data, while another may
require non-Euclidean architectures such as graph neural networks (GNNs) for
irregular data like brain connectomes. How to train such heterogeneous models
coherently across datasets, while enhancing each model's generalizability,
remains an open problem. We propose unified learning, a new paradigm that
encodes each model into a graph representation, enabling unification in a
shared graph learning space. A GNN then guides optimization of these unified
models. By decoupling parameters of individual models and controlling them
through a unified GNN (uGNN), our method supports parameter sharing and
knowledge transfer across varying architectures (MLPs, CNNs, GNNs) and
distributions, improving generalizability. Evaluations on MorphoMNIST and two
MedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unified
learning boosts performance when models are trained on unique distributions and
tested on mixed ones, demonstrating strong robustness to unseen data with large
distribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN

</details>


### [61] [Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer](https://arxiv.org/abs/2508.10587)
*Xuanhao Mu,Gökhan Demirel,Yuzhe Zhang,Jianlei Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: 이 논문은 고해상도 데이터를 필요로 하지 않고 Generative Adversarial Transformers (GATs)를 이용한 새로운 업샘플링 방법을 제안하여 기존의 방법보다 정확도를 향상시켰다.


<details>
  <summary>Details</summary>
Motivation: 에너지 네트워크 설계 및 운영에서 시간 해상도의 격차를 줄이기 위해 시간을 재표본화할 필요성이 있다.

Method: 고해상도 데이터에 접근하지 않고 교육할 수 있는 Generative Adversarial Transformers (GATs)를 사용한 새로운 업샘플링 방법을 제안한다.

Result: 기존 보간 방법과 비교하여 업샘플링 작업의 평균 제곱근 오차(RMSE)를 9% 줄이고, 모델 예측 제어(MPC) 응용 시나리오의 정확도를 13% 개선했다.

Conclusion: 제안된 방법은 고해상도 데이터의 필요 없이 효율적으로 작동할 수 있으며, 업샘플링 정확도를 향상시킨다.

Abstract: To bridge the temporal granularity gap in energy network design and operation
based on Energy System Models, resampling of time series is required. While
conventional upsampling methods are computationally efficient, they often
result in significant information loss or increased noise. Advanced models such
as time series generation models, Super-Resolution models and imputation models
show potential, but also face fundamental challenges. The goal of time series
generative models is to learn the distribution of the original data to generate
high-resolution series with similar statistical characteristics. This is not
entirely consistent with the definition of upsampling. Time series
Super-Resolution models or imputation models can degrade the accuracy of
upsampling because the input low-resolution time series are sparse and may have
insufficient context. Moreover, such models usually rely on supervised learning
paradigms. This presents a fundamental application paradox: their training
requires the high-resolution time series that is intrinsically absent in
upsampling application scenarios. To address the mentioned upsampling issue,
this paper introduces a new method utilizing Generative Adversarial
Transformers (GATs), which can be trained without access to any ground-truth
high-resolution data. Compared with conventional interpolation methods, the
introduced method can reduce the root mean square error (RMSE) of upsampling
tasks by 9%, and the accuracy of a model predictive control (MPC) application
scenario is improved by 13%.

</details>


### [62] [Oops!... They Stole it Again: Attacks on Split Learning](https://arxiv.org/abs/2508.10598)
*Tanveer Khan,Antonis Michalas*

Main category: cs.LG

TL;DR: 이 논문은 Split Learning(SL)의 보안 공격을 체계적으로 검토하고 방어 방법을 분석하여 SL의 프라이버시 문제를 개선하기 위한 정보를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: SL의 분산 특성은 새로운 보안 문제를 야기하므로, 잠재적인 공격을 포괄적으로 탐구할 필요가 있습니다.

Method: SL에 대한 다양한 공격을 분류하고, 기존 방어 방법들을 분석했습니다.

Result: 보안 격차를 드러내고, 기존 방어의 효과와 한계를 강조했습니다.

Conclusion: SL의 프라이버시 문제를 개선하기 위한 열린 도전과 향후 방향을 식별했습니다.

Abstract: Split Learning (SL) is a collaborative learning approach that improves
privacy by keeping data on the client-side while sharing only the intermediate
output with a server. However, the distributed nature of SL introduces new
security challenges, necessitating a comprehensive exploration of potential
attacks. This paper systematically reviews various attacks on SL, classifying
them based on factors such as the attacker's role, the type of privacy risks,
when data leaks occur, and where vulnerabilities exist. We also analyze
existing defense methods, including cryptographic methods, data modification
approaches, distributed techniques, and hybrid solutions. Our findings reveal
security gaps, highlighting the effectiveness and limitations of existing
defenses. By identifying open challenges and future directions, this work
provides valuable information to improve SL privacy issues and guide further
research.

</details>


### [63] [Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.10608)
*Davide Guidobene,Lorenzo Benedetti,Diego Arapovic*

Main category: cs.LG

TL;DR: 다목적 강화 학습(MORL)은 여러 모순되는 목표를 동시에 최적화하는 것을 목표로 하는 전통적인 강화 학습(RL)의 일반화입니다.


<details>
  <summary>Details</summary>
Motivation: MORL의 문제를 해결하고 샘플 효율성을 개선하는 것이 중요합니다.

Method: 정책 기울기 방법(PGM)을 사용하고, 분산 감소 기법을 적용하여 샘플 복잡성을 줄입니다.

Result: 제안된 접근법은 샘플 효율성을 향상시키며, MORL의 성능을 개선합니다.

Conclusion: MORL의 성능 개선과 함께 대규모 상태-행동 공간에서의 확장성을 유지합니다.

Abstract: Multi-Objective Reinforcement Learning (MORL) is a generalization of
traditional Reinforcement Learning (RL) that aims to optimize multiple, often
conflicting objectives simultaneously rather than focusing on a single reward.
This approach is crucial in complex decision-making scenarios where agents must
balance trade-offs between various goals, such as maximizing performance while
minimizing costs. We consider the problem of MORL where the objectives are
combined using a non-linear scalarization function. Just like in standard RL,
policy gradient methods (PGMs) are amongst the most effective for handling
large and continuous state-action spaces in MORL. However, existing PGMs for
MORL suffer from high sample inefficiency, requiring large amounts of data to
be effective. Previous attempts to solve this problem rely on overly strict
assumptions, losing PGMs' benefits in scalability to large state-action spaces.
In this work, we address the issue of sample efficiency by implementing
variance-reduction techniques to reduce the sample complexity of policy
gradients while maintaining general assumptions.

</details>


### [64] [Beyond Random Sampling: Instance Quality-Based Data Partitioning via Item Response Theory](https://arxiv.org/abs/2508.10628)
*Lucas Cardoso,Vitor Santos,José Ribeiro Filho,Ricardo Prudêncio,Regiane Kawasaki,Ronnie Alves*

Main category: cs.LG

TL;DR: 기계 학습 모델의 견고한 검증을 위한 새로운 데이터 파티셔닝 전략을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 데이터 파티셔닝 방법은 각 인스턴스의 내재적 품질을 무시하는 경우가 많다.

Method: 아이템 반응 이론(IRT) 매개변수를 사용하여 모델 검증 단계에서 데이터 세트를 분할하는 방법론을 제안한다.

Result: IRT 기반 파티셔닝 전략이 여러 기계 학습 모델의 성능에 미치는 영향을 평가하여 고유한 이질성과 정보성 하위 그룹을 드러내었다.

Conclusion: 높은 추측 인스턴스로 훈련하면 모델 성능이 크게 떨어지며, 일부 파티션은 50% 미만의 정확도를 기록하였다.

Abstract: Robust validation of Machine Learning (ML) models is essential, but
traditional data partitioning approaches often ignore the intrinsic quality of
each instance. This study proposes the use of Item Response Theory (IRT)
parameters to characterize and guide the partitioning of datasets in the model
validation stage. The impact of IRT-informed partitioning strategies on the
performance of several ML models in four tabular datasets was evaluated. The
results obtained demonstrate that IRT reveals an inherent heterogeneity of the
instances and highlights the existence of informative subgroups of instances
within the same dataset. Based on IRT, balanced partitions were created that
consistently help to better understand the tradeoff between bias and variance
of the models. In addition, the guessing parameter proved to be a determining
factor: training with high-guessing instances can significantly impair model
performance and resulted in cases with accuracy below 50%, while other
partitions reached more than 70% in the same dataset.

</details>


### [65] [Energy-Based Models for Predicting Mutational Effects on Proteins](https://arxiv.org/abs/2508.10629)
*Patrick Soga,Zhenyu Lei,Yinhan He,Camille Bilodeau,Jundong Li*

Main category: cs.LG

TL;DR: 본 연구에서는 단백질 복합체의 결합 자유 에너지 변화($\Delta\Delta G$) 예측을 위한 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 단백질 공학 및 단백질-단백질 상호작용 공학에서 결합 자유 에너지 변화($\Delta\Delta G$) 예측은 약물 발견에 중요한 과제이다.

Method: 단백질 복합체의 기하학적 배치를 추정하기 위해 에너지 기반 모델을 활용하는 새로운 접근 방식을 제안하며, $\Delta\Delta G$를 시퀀스 기반 구성 요소와 구조 기반 구성 요소로 분해한다.

Result: 기존의 최첨단 구조 및 시퀀스 기반 깊은 학습 방법들보다 $\Delta\Delta G$ 예측 및 SARS-CoV-2에 대한 항체 최적화에서 우월성을 입증하였다.

Conclusion: 새로운 에너지 기반 물리적 유도 편향을 도입함으로써 단백질 복합체의 결합 자유 에너지 변화를 정확하게 예측할 수 있는 방법을 개발하였다.

Abstract: Predicting changes in binding free energy ($\Delta\Delta G$) is a vital task
in protein engineering and protein-protein interaction (PPI) engineering for
drug discovery. Previous works have observed a high correlation between
$\Delta\Delta G$ and entropy, using probabilities of biologically important
objects such as side chain angles and residue identities to estimate
$\Delta\Delta G$. However, estimating the full conformational distribution of a
protein complex is generally considered intractable. In this work, we propose a
new approach to $\Delta\Delta G$ prediction that avoids this issue by instead
leveraging energy-based models for estimating the probability of a complex's
conformation. Specifically, we novelly decompose $\Delta\Delta G$ into a
sequence-based component estimated by an inverse folding model and a
structure-based component estimated by an energy model. This decomposition is
made tractable by assuming equilibrium between the bound and unbound states,
allowing us to simplify the estimation of degeneracies associated with each
state. Unlike previous deep learning-based methods, our method incorporates an
energy-based physical inductive bias by connecting the often-used sequence
log-odds ratio-based approach to $\Delta\Delta G$ prediction with a new
$\Delta\Delta E$ term grounded in statistical mechanics. We demonstrate
superiority over existing state-of-the-art structure and sequence-based deep
learning methods in $\Delta\Delta G$ prediction and antibody optimization
against SARS-CoV-2.

</details>


### [66] [Conditional Information Bottleneck for Multimodal Fusion: Overcoming Shortcut Learning in Sarcasm Detection](https://arxiv.org/abs/2508.10644)
*Yihua Wang,Qi Jia,Cong Xu,Feiyu Chen,Yuhan Liu,Haotian Zhang,Liang Jin,Lu Liu,Zhichun Wang*

Main category: cs.LG

TL;DR: 멀티모달 비꼬기 탐지의 복잡한 문제를 다루며, 효과적인 모드 융합의 필요성을 강조하고, MCIB 모델을 통해 최상의 성능을 달성함.


<details>
  <summary>Details</summary>
Motivation: 비꼬기는 인간 의사소통의 복잡한 감정을 전달하며, 이를 효과적으로 탐지하는 것이 중요하다.

Method: MUStARD++ 데이터셋의 단축 신호를 제거하고, 비꼬기 탐지를 위한 Multimodal Conditional Information Bottleneck (MCIB) 모델을 도입하였다.

Result: MCIB 모델이 최상의 성능을 달성하였으며, 단축 학습에 의존하지 않았다.

Conclusion: 효과적인 모드 융합에 초점을 맞추는 것이 복잡한 감정 인식에 필수적임을 확인했다.

Abstract: Multimodal sarcasm detection is a complex task that requires distinguishing
subtle complementary signals across modalities while filtering out irrelevant
information. Many advanced methods rely on learning shortcuts from datasets
rather than extracting intended sarcasm-related features. However, our
experiments show that shortcut learning impairs the model's generalization in
real-world scenarios. Furthermore, we reveal the weaknesses of current modality
fusion strategies for multimodal sarcasm detection through systematic
experiments, highlighting the necessity of focusing on effective modality
fusion for complex emotion recognition. To address these challenges, we
construct MUStARD++$^{R}$ by removing shortcut signals from MUStARD++. Then, a
Multimodal Conditional Information Bottleneck (MCIB) model is introduced to
enable efficient multimodal fusion for sarcasm detection. Experimental results
show that the MCIB achieves the best performance without relying on shortcut
learning.

</details>


### [67] [Geospatial Diffusion for Land Cover Imperviousness Change Forecasting](https://arxiv.org/abs/2508.10649)
*Debvrat Varshney,Vibhas Vats,Bhartendu Pandey,Christa Brelsford,Philipe Dias*

Main category: cs.LG

TL;DR: 본 연구는 제너레이티브 AI를 활용한 토지 이용 및 토지 피복 변화 예측을 제안하며, 역사적 데이터에 기반한 데이터 합성 문제로 프레임화한다.


<details>
  <summary>Details</summary>
Motivation: 지역 지구 시스템 모델들이 미래 기후 시나리오에서 고해상도로 수문학적 및 대기적 프로세스를 예측하는 데 있어서 점점 더 뛰어난 능력을 보여주지만, 토지 이용 및 토지 피복 변화 예측의 능력은 뒤처져 있다.

Method: LULC 예측을 역사적 및 보조 데이터 원천에 조건화된 데이터 합성 문제로 설정하고, 이를 위한 제너레이티브 모델의 바람직한 성질을 논의한다. 구체적으로, 우리는 전체 미국에 걸친 역사적 데이터를 사용하여 불침투성 예측을 실험하고, 10년 예측을 위한 확산 모델을 훈련하여 변화가 전혀 없다고 가정한 기준선과 성능을 비교한다.

Result: 12개 대도시 지역에서 훈련 시 제외된 연도에 대한 평가 결과, 평균 해상도가 $0.7	imes0.7km^2$ 이상일 때 우리의 모델은 기준선보다 낮은 평균 절대 오차(MAE)를 나타낸다.

Conclusion: 이 연구는 제너레이티브 모델이 역사적 데이터에서 미래 변화를 예측하는 데 중요한 시공간 패턴을 포착할 수 있음을 잘 보여주며, 향후 연구는 지구의 물리적 특성에 대한 보조 정보를 통합하고 드라이버 변수를 통해 다양한 시나리오의 시뮬레이션을 지원할 것이다.

Abstract: Land cover, both present and future, has a significant effect on several
important Earth system processes. For example, impervious surfaces heat up and
speed up surface water runoff and reduce groundwater infiltration, with
concomitant effects on regional hydrology and flood risk. While regional Earth
System models have increasing skill at forecasting hydrologic and atmospheric
processes at high resolution in future climate scenarios, our ability to
forecast land-use and land-cover change (LULC), a critical input to risk and
consequences assessment for these scenarios, has lagged behind. In this paper,
we propose a new paradigm exploiting Generative AI (GenAI) for land cover
change forecasting by framing LULC forecasting as a data synthesis problem
conditioned on historical and auxiliary data-sources. We discuss desirable
properties of generative models that fundament our research premise, and
demonstrate the feasibility of our methodology through experiments on
imperviousness forecasting using historical data covering the entire
conterminous United States. Specifically, we train a diffusion model for
decadal forecasting of imperviousness and compare its performance to a baseline
that assumes no change at all. Evaluation across 12 metropolitan areas for a
year held-out during training indicate that for average resolutions $\geq
0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding
corroborates that such a generative model can capture spatiotemporal patterns
from historical data that are significant for projecting future change.
Finally, we discuss future research to incorporate auxiliary information on
physical properties about the Earth, as well as supporting simulation of
different scenarios by means of driver variables.

</details>


### [68] [Graph Learning via Logic-Based Weisfeiler-Leman Variants and Tabularization](https://arxiv.org/abs/2508.10651)
*Reijo Jaakkola,Tomi Janhunen,Antti Kuusisto,Magdalena Ortiz,Matias Selin,Mantas Šimkus*

Main category: cs.LG

TL;DR: 본 논문에서는 Weisfeiler-Leman 알고리즘의 변형을 사용하여 그래프 데이터를 표 형식으로 변환한 후, 표 형식 데이터에 대한 방법을 적용하는 새로운 그래프 분류 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 그래프 분류에서의 새로운 접근 방식을 통해 기존의 방법들에 비해 더 효율적인 성능을 목표로 합니다.

Method: 그래프 데이터를 Weisfeiler-Leman 알고리즘의 다양한 변형을 통해 표 형식으로 변환한 다음, 이러한 데이터를 기반으로 한 방법을 적용합니다.

Result: 제안된 접근 방식은 최첨단 그래프 신경망 및 그래프 커널의 정확도를 유지하면서도 데이터셋에 따라 더 시간적 또는 메모리 효율적인 결과를 보여줍니다.

Conclusion: 그래프 데이터셋으로부터 해석 가능한 모달 논리 공식을 직접 추출하는 방법을 논의합니다.

Abstract: We present a novel approach for graph classification based on tabularizing
graph data via variants of the Weisfeiler-Leman algorithm and then applying
methods for tabular data. We investigate a comprehensive class of
Weisfeiler-Leman variants obtained by modifying the underlying logical
framework and establish a precise theoretical characterization of their
expressive power. We then test two selected variants on twelve benchmark
datasets that span a range of different domains. The experiments demonstrate
that our approach matches the accuracy of state-of-the-art graph neural
networks and graph kernels while being more time or memory efficient, depending
on the dataset. We also briefly discuss directly extracting interpretable modal
logic formulas from graph datasets.

</details>


### [69] [MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control](https://arxiv.org/abs/2508.10684)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Guan-Horng Liu,Yongxin Chen,Molei Tao*

Main category: cs.LG

TL;DR: 본 논문에서는 이산 상태 공간에서 샘플을 생성하기 위한 신경 샘플러 학습 문제를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 이산 상태 공간에서의 샘플 생성을 위한 신경 샘플러 학습이 통계 물리학, 기계 학습, 조합 최적화 등 여러 분야에서 중요한 과제입니다.

Method: 우리는 연속 시간 마르코프 체계의 확률적 최적 제어에 기반하여 학습 목표의 가족을 통해 두 경로 측정을 정렬하여 이산 신경 샘플러를 훈련하기 위한 새로운 프레임워크인 	extbf{M}asked 	extbf{D}iffusion 	extbf{N}eural 	extbf{S}ampler (	extbf{MDNS})를 제안합니다.

Result: MDNS는 다양한 통계적 특성을 가진 분포에 대한 광범위한 실험을 통해 효율성과 확장성을 입증했으며, 매우 높은 문제 차원에도 불구하고 목표 분포에서 정확하게 샘플링할 수 있도록 학습하였고, 다른 학습 기반 기준선보다 큰 마진으로 성능을 초과 달성했습니다.

Conclusion: 제안된 프레임워크의 효능과 잠재력을 입증하기 위한 포괄적인 이탈 분석 및 확장 연구도 제공됩니다.

Abstract: We study the problem of learning a neural sampler to generate samples from
discrete state spaces where the target probability mass function
$\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an
important task in fields such as statistical physics, machine learning,
combinatorial optimization, etc. To better address this challenging task when
the state space has a large cardinality and the distribution is multi-modal, we
propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural
$\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete
neural samplers by aligning two path measures through a family of learning
objectives, theoretically grounded in the stochastic optimal control of the
continuous-time Markov chains. We validate the efficiency and scalability of
MDNS through extensive experiments on various distributions with distinct
statistical properties, where MDNS learns to accurately sample from the target
distributions despite the extremely high problem dimensions and outperforms
other learning-based baselines by a large margin. A comprehensive study of
ablations and extensions is also provided to demonstrate the efficacy and
potential of the proposed framework.

</details>


### [70] [IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data](https://arxiv.org/abs/2508.10775)
*Dong Xu,Zhangfan Yang,Jenna Xinyi Yao,Shuangbao Song,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: IBEX는 구조 기반 약물 설계에서 단백질-리간드 복합체 데이터 부족 문제를 해결하기 위한 정보 병목 이론을 활용한 파이프라인이다.


<details>
  <summary>Details</summary>
Motivation: 구조 기반 약물 발견에서 단백질-리간드 복합체의 공개 데이터가 부족하기 때문에 기존 파이프라인들이 학습편향에 과적합되는 문제를 해결하고자 한다.

Method: PAC-Bayesian 정보 병목 이론을 사용하여 각 샘플의 정보 밀도를 정량화하고, Scaffold Hopping 작업을 통해 모델의 효과적인 용량과 전이 성능을 개선한다.

Result: IBEX는 제로샷 도킹 성공률을 53%에서 64%로 개선하고, 평균 Vina 점수를 -7.41에서 -8.07로 상승시켜, 100개의 포켓 중 57곳에서 최상의 중앙 Vina 에너지를 달성하였다.

Conclusion: IBEX는 25%의 QED 증가, 개선된 유효성과 다양성을 제공하며 외삽 오류를 눈에 띄게 줄인다.

Abstract: Three-dimensional generative models increasingly drive structure-based drug
discovery, yet it remains constrained by the scarce publicly available
protein-ligand complexes. Under such data scarcity, almost all existing
pipelines struggle to learn transferable geometric priors and consequently
overfit to training-set biases. As such, we present IBEX, an
Information-Bottleneck-EXplored coarse-to-fine pipeline to tackle the chronic
shortage of protein-ligand complex data in structure-based drug design.
Specifically, we use PAC-Bayesian information-bottleneck theory to quantify the
information density of each sample. This analysis reveals how different masking
strategies affect generalization and indicates that, compared with conventional
de novo generation, the constrained Scaffold Hopping task endows the model with
greater effective capacity and improved transfer performance. IBEX retains the
original TargetDiff architecture and hyperparameters for training to generate
molecules compatible with the binding pocket; it then applies an L-BFGS
optimization step to finely refine each conformation by optimizing five
physics-based terms and adjusting six translational and rotational degrees of
freedom in under one second. With only these modifications, IBEX raises the
zero-shot docking success rate on CBGBench CrossDocked2020-based from 53% to
64%, improves the mean Vina score from $-7.41 kcal mol^{-1}$ to $-8.07 kcal
mol^{-1}$, and achieves the best median Vina energy in 57 of 100 pockets versus
3 for the original TargetDiff. IBEX also increases the QED by 25%, achieves
state-of-the-art validity and diversity, and markedly reduces extrapolation
error.

</details>


### [71] [Non-Stationary Restless Multi-Armed Bandits with Provable Guarantee](https://arxiv.org/abs/2508.10804)
*Yu-Heng Hung,Ping-Chun Hsieh,Kai Wang*

Main category: cs.LG

TL;DR: 이 논문은 비정상 전이 가정을 갖는 다중 무장 도박 문제를 다루며, 해당 문제를 해결하는 새로운 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 RMAB 알고리즘이 비정상 동적 상황에서의 실제 응용 프로그램에 직면한 문제를 해결하기 위해.

Method: 슬라이딩 윈도우 강화 학습과 상한 신뢰 구간(UCB) 메커니즘을 통합하여 전이 동적 특성과 그 변동성을 동시에 학습하는 새로운 알고리즘을 제안.

Result: 제안된 알고리즘 mabights;가 비정상 RMAB 문제를 위해 최초로 이론적 기반을 제공하며, 느슨한 후회 정의를 활용하여 후회 경계 $	ilde{	ext{O}}(N^{2} B^{rac{1}{4}} T^{rac{3}{4}})$를 달성함을 입증.

Conclusion: 이 연구는 비정상 RMAB에 대한 새로운 이론적 접근을 제공하여, 향후 연구 및 실용적 응용의 기초를 마련한다.

Abstract: Online restless multi-armed bandits (RMABs) typically assume that each arm
follows a stationary Markov Decision Process (MDP) with fixed state transitions
and rewards. However, in real-world applications like healthcare and
recommendation systems, these assumptions often break due to non-stationary
dynamics, posing significant challenges for traditional RMAB algorithms. In
this work, we specifically consider $N$-armd RMAB with non-stationary
transition constrained by bounded variation budgets $B$. Our proposed \rmab\;
algorithm integrates sliding window reinforcement learning (RL) with an upper
confidence bound (UCB) mechanism to simultaneously learn transition dynamics
and their variations. We further establish that \rmab\; achieves
$\widetilde{\mathcal{O}}(N^2 B^{\frac{1}{4}} T^{\frac{3}{4}})$ regret bound by
leveraging a relaxed definition of regret, providing a foundational theoretical
framework for non-stationary RMAB problems for the first time.

</details>


### [72] [Comparison of Data Reduction Criteria for Online Gaussian Processes](https://arxiv.org/abs/2508.10815)
*Thore Wietzke,Knut Graichen*

Main category: cs.LG

TL;DR: 이 논문은 온라인 가우시안 프로세스(GP)의 효율성을 높이기 위한 여러 축소 기준을 비교하고 실제 데이터셋에서 그 성능을 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 가우시안 프로세스는 유연성과 불확실성 정량화 능력으로 인해 회귀 및 시스템 식별에 널리 사용되지만, 계산 복잡성으로 인해 작은 데이터셋에만 적용될 수 있습니다. 이는 스트리밍 환경에서 많은 데이터 포인트가 쌓이게 되어 문제가 됩니다.

Method: 여러 축소 기준의 통합 비교를 제공하고 이들의 계산 복잡성과 축소 행동을 분석합니다. 이 기준들은 벤치마크 함수와 실제 데이터셋, 동적 시스템 식별 작업에서 평가됩니다.

Result: 접수 기준이 제안되어 중복 데이터 포인트를 추가로 필터링합니다.

Conclusion: 이 연구는 온라인 GP 알고리즘에 적합한 기준을 선택하기 위한 실제 가이드라인을 제공합니다.

Abstract: Gaussian Processes (GPs) are widely used for regression and system
identification due to their flexibility and ability to quantify uncertainty.
However, their computational complexity limits their applicability to small
datasets. Moreover in a streaming scenario, more and more datapoints accumulate
which is intractable even for Sparse GPs. Online GPs aim to alleviate this
problem by e.g. defining a maximum budget of datapoints and removing redundant
datapoints. This work provides a unified comparison of several reduction
criteria, analyzing both their computational complexity and reduction behavior.
The criteria are evaluated on benchmark functions and real-world datasets,
including dynamic system identification tasks. Additionally, acceptance
criteria are proposed to further filter out redundant datapoints. This work
yields practical guidelines for choosing a suitable criterion for an online GP
algorithm.

</details>


### [73] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 이 논문은 메모리 증강 변환기의 발전과 신경 과학 원칙을 통합하여 장기 기억과 지속 학습의 한계를 극복하는 방법을 모색합니다.


<details>
  <summary>Details</summary>
Motivation: 메모리는 지능의 기본 요소로, 생물학적 및 인공지능 시스템에서 학습, 추론 및 적응을 가능하게 합니다.

Method: 신경 과학 원칙과 메모리 증강 변환기의 공학적 발전을 통합한 통합 프레임워크를 제시합니다.

Result: 최근의 진전을 기능적 목표, 메모리 표현 및 통합 메커니즘의 세 가지 분류적 차원으로 정리합니다.

Conclusion: 인지적으로 영감을 받은 평생 학습 변환기 구조를 위한 로드맵을 제공합니다.

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


### [74] [Efficiently Verifiable Proofs of Data Attribution](https://arxiv.org/abs/2508.10866)
*Ari Karchmer,Seth Neel,Martin Pawelczyk*

Main category: cs.LG

TL;DR: 이 논문은 데이터 귀속의 신뢰 문제를 해결하기 위해 상호 검증 패러다임을 제안하며, 불신 임의 증명자와 자원이 제한된 검증자 간의 상호 작용을 통해 효율적인 데이터 귀속 검증을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 계산 자원이 제한된 당사자들이 제공된 데이터 귀속이 '좋은'지 신뢰할 수 있는 방법을 찾기 위해, 불신 및 계산 능력이 강력한 증명자가 데이터를 학습하고 검증자와 상호 작용하는 접근법을 제안한다.

Method: 불신과 계산 능력이 강한 증명자가 데이터 귀속을 학습하고, 자원이 제한된 검증자와의 상호 작용을 통해 데이터 귀속을 검증하는 프로토콜을 제안한다.

Result: 프로토콜은 데이터 귀속이 최적 데이터 귀속에 대해 평균 제곱 오차 측면에서 {	extepsilon}-가까운 경우를 확률 1-{	extdelta}로 수용하며, 증명자가 프로토콜을 임의로 벗어나면 확률 {	extdelta}로 탐지된다.

Conclusion: 검증자의 작업량은 독립적인 모델 재훈련 횟수로 측정되며, 데이터셋 크기와 관계없이 O(1/{	extepsilon})으로 조정된다. 이는 다양한 귀속 작업에 널리 적용 가능하다.

Abstract: Data attribution methods aim to answer useful counterfactual questions like
"what would a ML model's prediction be if it were trained on a different
dataset?" However, estimation of data attribution models through techniques
like empirical influence or "datamodeling" remains very computationally
expensive. This causes a critical trust issue: if only a few computationally
rich parties can obtain data attributions, how can resource-constrained parties
trust that the provided attributions are indeed "good," especially when they
are used for important downstream applications (e.g., data pricing)? In this
paper, we address this trust issue by proposing an interactive verification
paradigm for data attribution. An untrusted and computationally powerful Prover
learns data attributions, and then engages in an interactive proof with a
resource-constrained Verifier. Our main result is a protocol that provides
formal completeness, soundness, and efficiency guarantees in the sense of
Probably-Approximately-Correct (PAC) verification. Specifically, if both Prover
and Verifier follow the protocol, the Verifier accepts data attributions that
are {\epsilon}-close to the optimal data attributions (in terms of the Mean
Squared Error) with probability 1-{\delta}. Conversely, if the Prover
arbitrarily deviates from the protocol, even with infinite compute, then this
is detected (or it still yields data attributions to the Verifier) except with
probability {\delta}. Importantly, our protocol ensures the Verifier's
workload, measured by the number of independent model retrainings it must
perform, scales only as O(1/{\epsilon}); i.e., independently of the dataset
size. At a technical level, our results apply to efficiently verifying any
linear function over the boolean hypercube computed by the Prover, making them
broadly applicable to various attribution tasks.

</details>


### [75] [A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design](https://arxiv.org/abs/2508.10899)
*Haydn Thomas Jones,Natalie Maus,Josh Magnus Ludan,Maggie Ziyu Huan,Jiaming Liang,Marcelo Der Torossian Torres,Jiatao Liang,Zachary Ives,Yoseph Barash,Cesar de la Fuente-Nunez,Jacob R. Gardner,Mark Yatskar*

Main category: cs.LG

TL;DR: AI 기반 발견은 디자인 시간을 크게 줄이고 신약의 효과를 높일 수 있다. 그러나 실험적 사전 지식의 부족으로 인해 암묵적 제약을 위반할 위험이 있다. 이 연구에서는 약물 설계를 위한 사전 데이터를 포함하는 새로운 데이터셋을 소개하며, 이를 통해 안전하고 효과적인 분자를 제안하도록 모델을 최적화할 수 있다.


<details>
  <summary>Details</summary>
Motivation: AI를 활용한 신약 발견 과정의 효율성을 높이는 필요성이 있다.

Method: L사전 데이터를 사용하여 LLM, CLIP, LLava 아키텍처를 훈련시켜 텍스트와 설계 목표를 공동으로 추론하는 방식으로 접근하였다.

Result: L사전 데이터로 훈련된 모델은 TDC 태스크에서 기존의 더 큰 모델들과 비교해 우수한 성능을 보였다.

Conclusion: 제안된 데이터셋은 안전하고 효과적인 신약 디자인에 기여할 수 있는 가능성이 있으며, 추후 문헌이 증가함에 따라 확장된 버전을 제공할 예정이다.

Abstract: AI-driven discovery can greatly reduce design time and enhance new
therapeutics' effectiveness. Models using simulators explore broad design
spaces but risk violating implicit constraints due to a lack of experimental
priors. For example, in a new analysis we performed on a diverse set of models
on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules
proposed had high probability of being mutagenic. In this work, we introduce
\ourdataset, a dataset of priors for design problems extracted from literature
describing compounds used in lab settings. It is constructed with LLM pipelines
for discovering therapeutic entities in relevant paragraphs and summarizing
information in concise fair-use facts. \ourdataset~ consists of 32.3 million
pairs of natural language facts, and appropriate entity representations (i.e.
SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,
CLIP, and LLava architectures to reason jointly about text and design targets
and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is
highly effective for creating models with strong priors: in supervised
prediction problems that use our data as pretraining, our best models with 15M
learnable parameters outperform larger 2B TxGemma on both regression and
classification TDC tasks, and perform comparably to 9B models on average.
Models built with \ourdataset~can be used as constraints while optimizing for
novel molecules in GuacaMol, resulting in proposals that are safer and nearly
as effective. We release our dataset at
\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},
and will provide expanded versions as available literature grows.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 이 논문은 최적 의사결정을 위한 최적화 모델링의 자동화를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 현실 문제 해결에 있어 최적화 모델링의 유용성이 높지만, 운영 연구 전문가의 상당한 전문 지식이 필요하다.

Method: 대형 언어 모델(LLM)의 발전을 통해 수학적 모델링 절차를 자동화하는 새로운 기회를 탐구한다.

Result: 기준 데이터셋에 대해 높은 오류율이 발견되었고, 이를 정리하여 새로운 리더보드를 구축하였다.

Conclusion: 현재 방법론의 한계를 파악하고 향후 연구 기회를 제시한다.

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [77] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: Amazon Nova AI Challenge는 대학팀 간의 경쟁을 통해 안전한 AI 발전을 도모하고 있으며, 자동화된 레드팀 봇과 안전한 AI 보조 도구를 개발하는 데 초점을 맞추고 있다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 안전성 확보는 중요한 과제가 되고 있으며, 이를 해결하기 위해 Amazon에서는 Trusted AI 트랙을 도입했다.

Method: 본 대회는 각 팀이 AI 코딩 보조 도구와 대화를 통하며 안전 정렬을 테스트하는 대결 형식으로 진행된다. 또한 고품질 주석 데이터가 제공된다.

Result: 팀들은 추론 기반 안전 정렬, 강력한 모델 가드레일, 다중 턴 탈옥, 대규모 언어 모델(LLM)의 효율적인 프로빙과 같은 첨단 기술을 개발했다.

Conclusion: 이 논문은 AI 소프트웨어 개발 안전성 문제를 해결하기 위한 대학팀과 Amazon Nova AI Challenge 팀의 진전을 outline하고 있다.

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [78] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 이 논문은 뉴스 기사에서 허위 정보를 탐지하기 위해 관계 추출을 사용하는 다중 에이전트 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 디지털 플랫폼에서 허위 정보의 광범위한 확산은 정보 무결성에 심각한 도전 과제를 제기한다.

Method: 제안된 Agentic AI 시스템은 네 개의 에이전트를 결합한다: 기계 학습 에이전트(로지스틱 회귀), 위키피디아 지식 확인 에이전트(명명된 개체 인식을 활용), 일관성 탐지 에이전트(LLM 프롬프트 엔지니어링 사용), 웹 스크래핑 데이터 분석기(팩트 체크를 위한 관계 삼중항 추출). 이 시스템은 모델 컨텍스트 프로토콜(MCP)을 통해 조율되어 구성 요소 간의 공유 컨텍스트와 실시간 학습을 제공한다.

Result: 결과는 다중 에이전트 앙상블이 95.3%의 정확도와 0.964의 F1 점수를 달성하며, 개별 에이전트와 전통적인 방법을 크게 초월함을 보여준다.

Conclusion: 개별 에이전트의 오분류율로부터 수학적으로 유도된 가중 집계 방법은 알고리즘 임계값 최적화보다 우수하다. 모듈형 아키텍처는 시스템이 쉽게 확장 가능하게 하면서도 결정 과정의 세부 사항을 유지한다.

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [79] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 대형 언어 모델의 발전은 지능형 에이전트의 자율성 및 동적 다중 에이전트 조정을 강조하는 새로운 AI 패러다임인 에이전틱 AI를 가져왔다. 이 논문은 주요 에이전틱 AI 프레임워크의 비교 분석을 제공하며, 에이전트 통신 프로토콜을 심층적으로 검토한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 AI 패러다임을 변화시키고 있으며, 에이전틱 AI의 발전과 연구 방향 모색이 필요하다.

Method: CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno, Google ADK, MetaGPT와 같은 주요 프레임워크를 체계적으로 검토하고, 다양한 에이전트 통신 프로토콜을 분석하였다.

Result: 에이전틱 AI 시스템의 기초적인 분류체계를 확립하고, 향후 연구 방향을 제안하였다.

Conclusion: 이 연구는 자율 AI 시스템의 발전을 위한 포괄적인 참고 자료로서의 역할을 한다.

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [80] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 이 논문은 자연어 프롬프트를 처리하고 인터넷 기반 콘텐츠를 자율적으로 검색 및 활용하는 심층 연구 에이전트(DRA)에 초점을 맞추고 있다.


<details>
  <summary>Details</summary>
Motivation: 최근 DRA의 성능이 인상적이나 대부분이 독점 시스템에 국한되어 있으며, 오픈소스 DRA의 필요성이 대두되었다.

Method: 우리는 BrowseComp 벤치마크를 ODR(Open Deep Research)과 기존 독점 시스템 간의 비교에 적합하도록 조정하였다. 또한 BC-Small이라는 더 작은 벤치마크를 제안하였다.

Result: ODR 및 Anthropic과 Google의 두 가지 독점 시스템을 BC-Small에서 비교한 결과, 세 시스템 모두 60개의 질문에 대한 테스트 세트에서 0% 정확도를 기록하였다.

Conclusion: ODR에 세 가지 전략적 개선 사항을 도입하여 ODR+ 모델을 구현하였으며, 이는 기존의 모든 시스템 중에서 BC-Small에서 10%의 성공률을 기록하였다.

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [81] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 이 논문은 대규모 추론 모델(LRM)의 생성 길이를 줄이는 효율적인 방법을 조사하고, 이는 추론의 질을 유지하면서도 계산 비용을 절감합니다.


<details>
  <summary>Details</summary>
Motivation: LRM의 긴 출력이 계산 비용을 증가시키고 과도한 사고를 초래하면서 추론의 효과성과 효율성 간의 균형을 맞추는 데 어려움이 있습니다.

Method: 생성 경로 분포를 분석하고 난이도 추정을 통해 생성된 경로를 필터링합니다. 이후 Bradley-Terry 손실 기반 프레임워크에서 다양한 선호 최적화 방법의 수렴 행동을 분석합니다.

Result: 우리는 LCPO(Length Controlled Preference Optimization)를 제안하여 NLL 손실과 관련된 암묵적 보상을 직접적으로 균형 있게 조절합니다. 이는 제한된 데이터와 훈련으로도 길이 선호를 효과적으로 학습할 수 있습니다.

Conclusion: 우리의 방법은 여러 벤치마크에서 평균 출력 길이를 50% 이상 줄이며, 추론 성능을 유지하는 데 효과적임을 입증합니다. 이 연구는 LRM이 효율적인 추론으로 안내되는 계산 효율적인 접근 방식의 잠재력을 강조합니다.

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [82] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI는 동적 솔루션 공간 탐색을 통해 기존 AutoML 방법의 한계를 극복하는 새로운 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 최근 LLM 기반 AutoML 시스템은 인상적인 기능을 보여주지만, 제한된 탐색 전략과 실행 병목 현상과 같은 심각한 한계에 직면해 있습니다.

Method: KompeteAI는 아이디어를 독립적으로 처리하는 기존 MCTS 방법과 달리 상위 후보를 결합하는 통합 단계를 도입합니다. 또한, Kaggle 노트북과 arXiv 논문에서 아이디어를 통합하여 가설 공간을 확장합니다.

Result: KompeteAI는 개발 파이프라인 평가를 6.9배 가속화하며, MLE-Bench에서 평균 3egexp{}의 성능 향상을 보여줍니다.

Conclusion: Kompete-bench를 새롭게 제안하여 MLE-Bench의 한계를 해결하며, 여기서 KompeteAI는 최첨단 결과를 달성합니다.

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [83] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 이 연구는 사건의 엔트로피 잠재력 개념이 인공지능의 불확실성 정량화, 의사결정 및 해석 가능성을 어떻게 향상시킬 수 있는지를 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템에서 불확실성을 명확히 하고 관리하는 방법이 필요합니다.

Method: 물리학에서의 원래 수식화를 바탕으로 사건 중심의 척도를 도입하여 AI에 맞게 조정합니다.

Result: 정책 평가, 내재 보상 설계, 설명 가능한 AI 및 이상 탐지에서의 응용을 탐구합니다.

Conclusion: 엔트로피 잠재력 프레임워크는 AI에서 불확실성을 관리하기 위한 이론적으로 기반을 둔 해석 가능하고 다재다능한 접근 방식을 제공합니다.

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [84] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 본 논문에서는 그래픽 디자인 평가를 위한 에이전틱 디자인 리뷰 시스템(AgenticDRS)을 제안하고, 이를 통해 다수의 에이전트가 협력하여 디자인을 분석하는 방법을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 그래픽 디자인 평가를 다각적으로 수행하기 위해 전문가 리뷰어의 피드백을 집계하는 방식이 필요하다.

Method: 다수의 에이전트가 메타 에이전트에 의해 조율되어 디자인을 분석하고, 그래프 매칭을 기반으로 한 새로운 인컨텍스트 예시 선택 접근 방식을 사용한다.

Result: 최신 성능 기준과 비교한 철저한 실험 평가를 통해 에이전틱-DRS의 효능이 입증되었다.

Conclusion: 이 연구가 실용적이면서도 덜 탐구된 연구 방향에 관심을 불러일으킬 것으로 기대한다.

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [85] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 대형 언어 모델의 이해 능력과 추론 능력은 착각이며, 이들은 실제로 진정한 이해와 추론 능력을 가질 수 없다.


<details>
  <summary>Details</summary>
Motivation: AIGC 도구의 발전으로 LLM의 능력에 대한 과장된 주장에 의문을 제기하고자 한다.

Method: LLM의 작동 원리의 본질적 한계를 설명한다.

Result: LLM은 진정한 올바른 추론 능력을 가질 수 없는 이유를 명확히 한다.

Conclusion: LLM의 이해와 추론 능력은 환상에 불과하다.

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [86] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 본 연구에서는 과도한 사고 문제를 해결하기 위해 효과적인 단계를 장려하고 비효과적인 단계에 대한 패널티를 부여하는 새로운 보상 메커니즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LRM의 과도한 사고로 인한 비효율성을 해결하고자 한다.

Method: 단계별 성과에 기반한 새로운 규칙 기반 검증 가능 보상 메커니즘(VSRM)을 제안한다.

Result: VSRM을 PPO 및 Reinforce++와 통합해 실험한 결과, 다양한 기준에서 출력 길이를 상당히 줄이면서 원래의 추론 성능을 유지한다.

Conclusion: 우리의 접근 방식이 비효과적인 단계를 억제하고 효과적인 추론을 장려함으로써 과도한 사고 문제를 근본적으로 완화함을 보여준다.

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [87] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: Dianping-Trust-Safety 팀은 META CRAG-MM 챌린지에서 다중 모드 다중 턴 질의 응답 시스템을 구축하는 솔루션을 제시하였으며, 다양한 작업에서 우수한 성과를 거두었다.


<details>
  <summary>Details</summary>
Motivation: META CRAG-MM 챌린지는 다중 모드 다중 턴 질의 응답 시스템을 구축하는 것이 필요하며, 이를 통해 다양한 출처의 정보를 통합하고 복잡한 질문을 처리할 수 있는 시스템 개발의 중요성을 보여준다.

Method: 이 논문에서는 이미지 기반 모의 지식 그래프에서 구조화된 데이터를 사용하여 질문에 답하는 솔루션(작업 1), 지식 그래프와 웹 검색 결과를 통합하는 정보 합성(작업 2), 그리고 다중 턴 대화를 처리하는 방법(작업 3)에 대한 접근 방식을 제시한다.

Result: 작업 1에서 52.38%의 주요 차이로 1위를 차지했으며, 작업 3에서는 3위를 차지하였다.

Conclusion: 이 연구는 주의 깊은 교육과 강화 학습의 통합이 우리 교육 파이프라인에서 효과적임을 입증한다.

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [88] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 본 논문에서는 이종 에이전트 간의 안정적인 정책 업데이트를 위한 HATRPO(이종 에이전트 신뢰 영역 정책 최적화)를 제안하며, KL 발산을 통한 에이전트별 신뢰 지역 제약을 적용하여 훈련을 안정화합니다.


<details>
  <summary>Details</summary>
Motivation: 이종 에이전트 환경에서의 느리고 지역적으로 최적화된 업데이트 문제를 해결하고자 합니다.

Method: HATRPO-W 및 HATRPO-G라는 두 가지 접근 방식을 제안하여 에이전트 간 KL 발산 임계값을 할당합니다.

Result: 우리의 방법이 HATRPO의 성능을 크게 향상시켜 더 빠른 수렴 및 더 높은 최종 보상을 달성했습니다.

Conclusion: HATRPO-W 및 HATRPO-G 모두 22.5% 이상의 성능 향상을 달성하였으며, HATRPO-W는 더 안정적인 학습 동력을 보여 주었습니다.

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [89] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 이 연구는 정보가 희박한 환경에서의 상상력 있는 추론 능력에 대한 LLM의 수용 능력을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 벤치마크가 동적이고 탐색적인 추론 과정을 포착하지 못하는 문제를 해결하고자 합니다.

Method: 전통적인 '거북이 수프' 게임을 기반으로 한 포괄적인 연구 프레임워크를 도입하고, 800개의 퍼즐을 포함한 TurtleSoup-Bench와 LLM의 성능을 평가하기 위한 Mosaic-Agent를 제안합니다.

Result: 선도적인 LLM과의 실험에서 명확한 능력 한계와 일반적인 실패 패턴, 인간과 비교할 때 큰 성능 차이를 발견했습니다.

Conclusion: 우리의 연구는 LLM의 상상력 있는 추론에 대한 새로운 통찰을 제공하고 탐색 에이전트 행동에 대한 향후 연구의 기초를 마련합니다.

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [90] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG는 효과적인 지식 집합 및 검색 전략을 결합하여 대형 언어 모델의 성능을 향상시키는 새로운 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 RAG 방법은 맥락상 결함이 있거나 불완전한 정보를 검색함에 따라 효과성이 저하됩니다.

Method: LeanRAG는 엔티티 클러스터를 형성하고 집합 수준 요약 간의 명시적 관계를 구축하는 새로운 의미 집계 알고리즘을 사용합니다.

Result: LeanRAG는 4개의 QA 벤치마크에서 기존 방법보다 응답 품질에서 현저히 향상되었으며, 검색 중복성을 46% 줄였습니다.

Conclusion: LeanRAG는 그래프에서 경로 검색과 관련된 상당한 오버헤드를 완화하고 중복 정보 검색을 최소화하는 데 기여합니다.

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [91] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: 의료 기록에서 약물 추천의 중요성을 강조하고, 기존 모델이 데이터의 결함에 어떻게 대응하는지를 설명. HiRef라는 새로운 프레임워크를 제안하며, 이 프레임워크가 어떻게 일반화 능력을 향상시키는지를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 환자의 장기적인 의료 기록을 활용하여 의사들이 시기적절한 결정을 내릴 수 있도록 약물 추천을 지원하는 것이 중요하다.

Method: HiRef는 계층적 의료 온톨로지에서 인코딩된 계층적 의미와 실제 EHR에서 파생된 공동 발생 패턴을 결합한 통합 프레임워크이다. 온톨로지 항목을 쌍곡선 공간에 삽입하여 트리 형태의 관계를 자연스럽게 포착하고 지식 전이를 통해 일반화 성능을 향상시킨다.

Result: EHR 벤치마크인 MIMIC-III 및 MIMIC-IV에서 강력한 성능을 달성하며, 시뮬레이션된 보이지 않는 코드 환경에서도 높은 정확도를 유지한다.

Conclusion: HiRef의 회복력은 보이지 않는 의료 코드에 대한 저항성으로 입증되며, 학습된 희소 그래프 구조와 의료 코드 매핑의 심층 분석에 의해 지원된다.

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [92] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: MM-Food-100K은 검증 가능한 출처를 가진 10만 샘플의 다중 모달 음식 지능 데이터셋이다.


<details>
  <summary>Details</summary>
Motivation: 음식 이미지에 대한 다양한 정보를 포함하는 고품질 데이터베이스를 구축하기 위해.

Method: Codatta 기여 모델을 사용하여 6주간 87,000명의 기여자로부터 수집된 120만 개의 음식 이미지 코퍼스의 약 10%를 공개된 데이터셋으로 정제함.

Result: 대규모 비전-언어 모델을 이미지 기반 영양 예측에 적합하게 조정하여 일관된 성능 향상을 보이며 MM-Food-100K 부분집합에 대한 결과를 보고.

Conclusion: MM-Food-100K를 공공이 무료로 접근할 수 있도록 공개하며 잠재적인 상업적 접근을 위해 약 90%는 보유하고, 기여자와 수익을 공유할 예정이다.

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [93] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0는 MLLM의 수학적 추론 능력을 종합적으로 향상시키기 위해 구조화된 수학 지식 시스템, 모델 중심 데이터 공간 모델링 및 강화 학습 기반 훈련 패러다임을 통합하는 시스템이다.


<details>
  <summary>Details</summary>
Motivation: MLLM은 다양한 작업에서 뛰어난 능력을 보여주지만 복잡한 수학적 추론에 여전히 어려움을 겪고 있다.

Method: 우리는 구조화된 수학 지식 시스템과 모델 중심의 데이터 공간 모델링, 강화 학습 기반 훈련 패러다임이 통합된 We-Math 2.0을 도입한다.

Result: MathBook-RL은 네 가지 널리 사용되는 벤치마크에서 기존 기준과 경쟁력 있게 성능을 발휘하며 MathBookEval에서 강력한 결과를 달성하였다.

Conclusion: 이 결과는 수학적 추론의 일반화 가능성이 유망하다는 것을 시사한다.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [94] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: 본 논문에서는 학술적 지식 그래프(SKG)에서 자연어 질문을 SPARQL 질의로 변환하는 LLM(대형 언어 모델) 기반 접근 방식의 한계를 극복하기 위해 FIRESPARQL이라는 모듈형 프레임워크를 제안합니다. 이 프레임워크는 fine-tuned LLM을 핵심 구성 요소로 하며, RAG와 SPARQL 질의 수정 레이어를 통해 컨텍스트를 선택적으로 제공합니다. 실험 결과, fine-tuning이 최상의 성능을 달성하여 질의 정확도에서 0.90 ROUGE-L, 결과 정확도에서 0.85 RelaxedEM을 기록했습니다.


<details>
  <summary>Details</summary>
Motivation: 학술적 지식 그래프(SKG)에서의 질문 응답은 복잡한 학술 콘텐츠와 그래프의 복잡한 구조로 인해 어려운 작업입니다.

Method: FIRESPARQL이라는 모듈형 프레임워크를 제안하고, fine-tuned LLM을 핵심 구성 요소로 활용하며, RAG와 SPARQL 질의 수정 레이어를 통해 추가 컨텍스트를 제공합니다.

Result: fine-tuning을 통해 질의 정확도 0.90 ROUGE-L, 결과 정확도 0.85 RelaxedEM의 성과를 달성했습니다.

Conclusion: FIRESPARQL은 SKG 특정 콘텐츠와 스키마에 대한 노출 한계를 극복하고, LLM 기반 접근 방식을 개선하여 높은 성능을 보였습니다.

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [95] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 사용자 쿼리에 기반한 다중 위치 검색을 위한 공간 단서 쿼리 시스템인 SEQ-GPT를 제안하며, 자연어를 활용한 효과적인 검색 경험을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 위치 검색 작업을 수행할 때 사용자의 경험이 제한적임을 인식하고, 사용자 지정 예제를 바탕으로 여러 위치를 동시에 검색할 수 있는 가능성을 탐구하기 위해.

Method: 대형 언어 모델(LLM)을 활용하여 사용자 인터랙션을 강화하고, 자연어와 구조화된 공간 데이터를 연계하는 맞춤형 LLM 적응 파이프라인을 제안하며, 다이얼로그 합성 및 다중 모델 협력을 통해 검색 프로세스를 개선한다.

Result: SEQ-GPT는 사용자의 쿼리 세부정보를 명확히 할 수 있는 기능과 사용자 피드백에 따라 검색을 동적으로 조정할 수 있는 기능을 제공하며, 현실적인 데이터와 애플리케이션 시나리오를 통해 공간 검색을 확대하는 엔드 투 엔드 시연을 제공한다.

Conclusion:  SEQ-GPT는 사용자가 자연어로 복잡한 위치 검색을 보다 효과적으로 수행할 수 있도록 지원하며, 공간 검색의 범위를 넓힐 수 있는 가능성을 보여준다.

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [96] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 본 연구에서는 진단 과정에서 의사의 역할을 AI가 주도하도록 재편성하여, AI가 전체 진단 프로세스를 이끌 수 있도록 하는 새로운 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI의 활용이 임상 진단의 효율성을 높이고, 의사의 업무 부담을 줄이는 데 기여할 수 있도록 하기 위해, AI의 역할을 재조정할 필요가 있다.

Method: DxDirector-7B라는 대형 언어 모델을 이용하여, 최소한의 의사 개입으로 전체 진단 과정을 이끌도록 설계하였다.

Result: DxDirector-7B는 기존의 의료 LLM 및 일반 LLM에 비해 더 높은 진단 정확성을 달성하고 의사의 업무 부담을 상당히 감소시켰다.

Conclusion: 이 연구는 AI가 진단 과정의 주도적인 역할을 수행하여 의사의 업무 부담을 줄이는 새로운 가능성을 제시한다.

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [97] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS는 Chest X-Ray(CXR) 추론을 위한 최초의 다중 모드 프레임워크로, 의료 작업에서의 안전성 및 신뢰성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 도구 보강 에이전트 시스템은 의사 결정의 신뢰를 저하시키고 안전 위험을 초래하는 블랙박스 추론 단계, 의료 작업에 본질적으로 중요한 불완전한 다중 모드 통합, 경직되고 계산적으로 비효율적인 에이전트 파이프라인에 의해 제한됩니다.

Method: PASS는 다중 도구 그래프에서 에이전트 워크플로를 적응적으로 샘플링하여 해석 가능한 확률로 주석이 달린 의사 결정 경로를 생성합니다. 이를 위해 과거 CXR 추론 작업에서 학습된 작업 조건 분포를 활용합니다.

Result: PASS는 다양한 벤치마크에서 강력한 기준선보다 여러 메트릭(정확도, AUC 등)에서 유의미하게 우수한 성과를 보였습니다.

Conclusion: 통합 의료 에이전트 시스템의 해석 가능성, 적응성 및 다중 모드 처리의 새로운 패러다임 전환을 추진합니다.

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [98] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: LM과 인간 선호 간의 정렬은 신뢰할 수 있는 AI 시스템 구축에 중요하다. DPO와 같은 방법을 통해 정렬을 최적화하지만, 정적 및 정책 데이터를 비교할 때 효과의 차이를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델(LM)과 인간의 선호를 정렬하는 것은 신뢰할 수 있는 AI 시스템을 구축하는 데 필수적이다.

Method: 정적 선호 데이터에서 정책을 최적화하는 DPO를 사용하고, 정책 샘플링을 통합하여 LM 정렬을 개선했다.

Result: 정책 데이터가 항상 최적이 아니며, 정적 데이터와 정책 데이터 간의 효과성 차이가 나타났다. 예를 들어, Llama-3에서는 3배, Zephyr에서는 0.4배의 효과성을 보였다.

Conclusion: 정렬 과정을 선호 주입 단계와 선호 미세 조정 단계로 나누고, 이를 통해 경계를 식별하는 효과적인 알고리즘을 제안하였다.

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [99] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 이 논문은 복잡한 영역에서 대규모 언어 모델의 추론 능력을 향상시키기 위한 새롭고 편향 없는 추정기를 구축하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 복잡한 과제를 해결하는 데 어려움을 겪고 있으며, 특히 수학에서의 추론 능력 향상 필요.

Method: ComMCS라는 방법을 통해 현재 및 이후 단계의 MC 추정치를 선형 결합하여 편향 없는 추정기를 구축합니다.

Result: ComMCS가 MATH-500의 Best-of-32 샘플링 실험에서 회귀 기반 최적화 방법보다 2.8점, 비분산 감소 기준선보다 2.2점을 초과했습니다.

Conclusion: 이 방법은 추가적인 LLM 추론 비용 없이 예측 가능한 분산 감소를 달성함을 이론적으로 보여줍니다.

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [100] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: Multi-Subspace Representation Steering(MSRS)는 여러 속성을 효과적으로 조정하기 위한 새로운 프레임워크로, 각 속성에 직교적인 하위 공간을 할당하여 속성 간 간섭을 줄이고 모델의 표현 공간 내에서 그 영향을 분리합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들이 여러 특성을 동시에 조정할 때 간섭 및 바람직하지 않은 절충점을 초래하는 문제를 해결하고자 하는 필요성이 있습니다.

Method: MSRS는 하위 공간 표현 미세 조정을 통해 다중 특성을 효과적으로 조정하는 새로운 접근 방식을 제공하며, 하이브리드 하위 공간 조합 전략을 통합합니다.

Result: 실험 결과 MSRS는 특성 간 충돌을 현저히 줄이고 기존 방법들을 앞지르며 다양한 후속 작업에 효과적으로 일반화됨을 보여줍니다.

Conclusion: MSRS는 대규모 언어 모델의 행동을 정밀하게 조정할 수 있는 강력한 도구로 입증되었습니다.

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [101] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: STEP는 대화 맥락과 지식 그래프 정보를 효과적으로 통합하여 추천의 정확성과 대화 품질을 향상시키는 대화형 추천 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 기존 대화형 추천 시스템은 사용자 선호도와 대화 맥락의 깊은 의미를 포착하는 데 어려움을 겪고 있다.

Method: STEP은 사전 학습된 언어 모델을 중심으로 학습 커리큘럼에 따라 맥락-지식 융합 및 경량 작업별 프롬프트 조정을 결합한다.

Result: STEP은 두 개의 프롬프트를 통한 정보를 활용하여 사용자 의도에 맞춘 응답 생성을 유도하고, 지식에 일관된 후보 제품 순위를 조정하여 추천의 정확성과 대화 품질을 높인다.

Conclusion: 실험 결과, STEP은 두 개의 공개 데이터 세트에서 추천 정확도와 대화 품질에서 기존 방법들을 초월하였다.

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [102] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM은 생물의학 분야에서 온톨로지 일치를 지원하는 대형 언어 모델 기반의 프레임워크로, 의미적 표현을 풍부하게 하고 정밀도를 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 온톨로지 일치(OM)는 생물의학 분야에서 이질적인 지식 소스 간의 의미적 상호 운용성과 통합을 가능하게 하는 핵심적인 역할을 한다.

Method: GenOM은 텍스트 정의를 생성하여 온톨로지 개념의 의미적 표현을 풍부하게 하고, 임베딩 모델로 정렬 후보를 검색하며, 정밀도를 높이기 위해 정확한 일치 기반 도구를 통합한다.

Result: OAEI Bio-ML 트랙에서 수행된 광범위한 실험은 GenOM이 종종 경쟁력 있는 성능을 달성할 수 있음을 보여주며, 전통적인 OM 시스템 및 최근 LLM 기반 방법들을 초월한다.

Conclusion: 추가적인 아블레이션 연구를 통해 의미적 풍부화와 몇 가지 샷 프롬프트의 효과가 확인되어, 프레임워크의 강건성과 적응성을 강조한다.

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [103] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 이 논문은 그래프 신경망(GNN)과 심층 강화 학습(RL)을 결합하여 일반화된 계획을 수행하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 문제에서 계획을 더 효과적으로 해결하기 위해 희소하고 목표 인식 GNN 표현을 도입함으로써 현재 방법의 한계를 극복하고 싶었습니다.

Method: 희소하고 목표 인식의 GNN 표현을 사용하여 관련된 지역 관계를 선택적으로 인코딩하고 목표와 관련된 공간적 특징을 명시적으로 통합합니다.

Result: 이 방법은 이전에 촘촘한 그래프 표현으로 처리할 수 없었던 더 큰 격자 크기에 효과적으로 확장되며 정책 일반화 및 성공률을 크게 향상시킵니다.

Conclusion: 우리의 연구 결과는 현실적인 대규모 일반화 계획 작업을 해결하기 위한 실용적인 기초를 제공합니다.

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [104] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: AI 생성 콘텐츠의 확산이 misinformation의 위험을 증가시키고 있다. 사람들이 AI 생성 콘텐츠에 어떻게 반응하는지를 이해하기 위해 MhAIM Dataset을 통해 대규모 분석을 수행하고, T-Lens 시스템을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 생성 콘텐츠의 확산에 따른 misinformation 위험 증가에 대응하기 위해, 콘텐츠가 인간의 인식과 행동에 미치는 영향을 연구하고자 하였다.

Method: MhAIM Dataset을 통한 대규모 분석과 T-Lens 시스템 개발로, 인간 반응 예측 정보를 통합한 LLM 기반 에이전트 시스템을 제안한다.

Result: 인간 중심 연구를 통해 사람들이 텍스트와 시각 자료를 포함한 AI 콘텐츠를 더 잘 식별하며, 새로운 신뢰성, 영향력, 개방성 메트릭스를 제안하였다.

Conclusion: 이 연구는 AI, 인간 인지 및 정보 수신 간의 복잡한 상호작용을 강조하며, AI 기반 misinformation의 위험을 완화하기 위한 실행 가능한 전략을 제안한다.

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [105] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 대형 언어 모델은 데이터와 매개변수를 확장함으로써 구조화되고 일반화 가능한 내부 표현을 획득한다고 가정되지만, 이 논문에서는 클리니컬 트라이얼 자연어 추론 벤치마크를 통해 이 가정을 검증한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 사실 접근 실패와 추론 실패를 구분하기 위한 새로운 벤치마크 필요성.

Method: 임상 시험 자연어 추론 벤치마크와 GKMRV 프로브를 사용하여 모델 성능 평가.

Result: 모델은 GKMRV 정확도가 높지만 주요 추론 작업에서는 낮은 성능을 보인다.

Conclusion: 현재 LLM은 관련 임상 지식은 보유하지만 이를 신뢰성 있게 적용하는 데 필요한 구조적 내부 표현이 부족하다.

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [106] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: AI 시스템의 결정 지원을 위한 설명 가능성이 강조되지만, 시각 장애인을 포함한 접근 가능성 문제는 충분히 탐구되지 않았다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 의사결정 지원이 증가하면서, 사용자들이 더 많은 정보를 바탕으로 선택을 할 수 있도록 설명 가능성이 중요해졌다.

Method: 이 논문은 두 가지 접근 방식을 통해 XAI의 접근 가능성 격차를 조사한다. 첫째, 79개의 연구 문헌 리뷰를 통해 XAI 기술의 평가가 장애인을 포함하지 않는 경우가 많고, 대부분의 설명이 본질적으로 시각적인 형식에 의존하고 있음을 밝힌다. 둘째, 포괄적인 XAI 디자인을 위한 4단계 방법론적 개념 증명을 제시한다.

Result: 초기 연구 결과에 따르면, 비시각 사용자에게는 단순화된 설명이 더 이해하기 쉽고, 공정한 해석을 위해서는 다중 모드 프레젠테이션이 필요하다.

Conclusion: 이 논문은 시각 장애인을 포함한 사용자에게 적합한 XAI 기술의 필요성을 강조한다.

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [107] [REALISM: A Regulatory Framework for Coordinated Scheduling in Multi-Operator Shared Micromobility Services](https://arxiv.org/abs/2508.10166)
*Heng Tan,Hua Yan,Yukun Yuan,Guang Wang,Yu Yang*

Main category: cs.MA

TL;DR: 이 논문은 도시의 다중 운영자 공유 마이크로모빌리티 서비스의 조정을 위한 REALISM 규제 프레임워크를 설계하고, 이 프레임워크가 차량 사용의 공평성과 평균 수요 충족에서 성능 향상을 이룸을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 공유 마이크로모빌리티의 확산이 도시의 사회적 문제를 초래하고 있으며, 다중 운영자 간 비협조적인 행동으로 인해 문제를 악화시키고 있다.

Method: REALISM은 도시 목표 달성과 운영자의 개별 기여를 기준으로 각 운영자에게 점수를 배정하는 규제 프레임워크이다. 공정성을 고려한 점수 할당을 측정하고 최적화하는 절차를 개발하여 운영자와 도시 규제자가 상호 작용하게 한다.

Result: REALISM 프레임워크는 시카고의 실제 e-스쿠터 사용 데이터를 바탕으로 평가되었으며, 차량 사용의 공평성에서 최소 39.93%의 성능 향상을 달성하고, 도시 전체의 평균 수요 충족에서도 1.82%의 향상을 보였다.

Conclusion: REALISM 프레임워크는 공유 마이크로모빌리티 서비스를 최적화하는 데 있어 효과적이며, 도시 규제자의 기대와 부합하는 서비스를 제공할 수 있다.

Abstract: Shared micromobility (e.g., shared bikes and electric scooters), as a kind of
emerging urban transportation, has become more and more popular in the world.
However, the blooming of shared micromobility vehicles brings some social
problems to the city (e.g., overloaded vehicles on roads, and the inequity of
vehicle deployment), which deviate from the city regulator's expectation of the
service of the shared micromobility system. In addition, the multi-operator
shared micromobility system in a city complicates the problem because of their
non-cooperative self-interested pursuits. Existing regulatory frameworks of
multi-operator vehicle rebalancing generally assume the intrusive control of
vehicle rebalancing of all the operators, which is not practical in the real
world. To address this limitation, we design REALISM, a regulatory framework
for coordinated scheduling in multi-operator shared micromobility services that
incorporates the city regulator's regulations in the form of assigning a score
to each operator according to the city goal achievements and operators'
individual contributions to achieving the city goal, measured by Shapley value.
To realize the fairness-aware score assignment, we measure the fairness of
assigned scores and use them as one of the components to optimize the score
assignment model. To optimize the whole framework, we develop an alternating
procedure to make operators and the city regulator interact with each other
until convergence. We evaluate our framework based on real-world e-scooter
usage data in Chicago. Our experiment results show that our method achieves a
performance gain of at least 39.93% in the equity of vehicle usage and 1.82% in
the average demand satisfaction of the whole city.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [108] [A Robust Pipeline for Differentially Private Federated Learning on Imbalanced Clinical Data using SMOTETomek and FedProx](https://arxiv.org/abs/2508.10017)
*Rodrigo Tertulino*

Main category: cs.CR

TL;DR: 이 연구는 환자 개인정보 보호를 보장하면서 분산 데이터에서 모델 학습을 가능하게 하는 연합 학습(FL) 프레임워크를 통해 심혈관 위험 예측을 다룬다. 데이터 불균형 문제를 해결하기 위해 하이브리드 SMOTETomek 기법을 통합하고, 비독립 데이터에 대해 최적화된 FedProx 알고리즘을 적용하여 임상적으로 유용한 모델을 개발했다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습은 환자 프라이버시를 보호하면서 진료 연구를 위한 혁신적인 방법론을 제시하지만, 데이터 불균형과 사생활 보호 간의 복잡한 균형을 맞추는 데 어려움이 있다.

Method: 하이브리드 합성 소수자 과표집 기법(SMOTETomek)을 클라이언트 수준에서 통합하고, 조정된 FedProx 알고리즘을 사용하여 비독립적 데이터에 대한 프레임워크 최적화를 수행했다.

Result: 최종 결과는 개인 정보 예산(엡실론)과 모델 재현률 간의 비선형적 트레이드오프를 보여주며, 최적화된 FedProx가 표준 FedAvg보다 일관되게 우수한 성능을 발휘함을 revealed 했다.

Conclusion: 본 연구는 실제 의료 데이터에 적용 가능한 효과적이고 안전하며 정확한 진단 도구를 개발하기 위한 실용적인 방법론적 청사진을 제공한다.

Abstract: Federated Learning (FL) presents a groundbreaking approach for collaborative
health research, allowing model training on decentralized data while
safeguarding patient privacy. FL offers formal security guarantees when
combined with Differential Privacy (DP). The integration of these technologies,
however, introduces a significant trade-off between privacy and clinical
utility, a challenge further complicated by the severe class imbalance often
present in medical datasets. The research presented herein addresses these
interconnected issues through a systematic, multi-stage analysis. An FL
framework was implemented for cardiovascular risk prediction, where initial
experiments showed that standard methods struggled with imbalanced data,
resulting in a recall of zero. To overcome such a limitation, we first
integrated the hybrid Synthetic Minority Over-sampling Technique with Tomek
Links (SMOTETomek) at the client level, successfully developing a clinically
useful model. Subsequently, the framework was optimized for non-IID data using
a tuned FedProx algorithm. Our final results reveal a clear, non-linear
trade-off between the privacy budget (epsilon) and model recall, with the
optimized FedProx consistently out-performing standard FedAvg. An optimal
operational region was identified on the privacy-utility frontier, where strong
privacy guarantees (with epsilon 9.0) can be achieved while maintaining high
clinical utility (recall greater than 77%). Ultimately, our study provides a
practical methodological blueprint for creating effective, secure, and accurate
diagnostic tools that can be applied to real-world, heterogeneous healthcare
data.

</details>


### [109] [A Comparative Performance Evaluation of Kyber, sntrup761, and FrodoKEM for Post-Quantum Cryptography](https://arxiv.org/abs/2508.10023)
*Samet Ünsal*

Main category: cs.CR

TL;DR: 본 논문은 포스트 양자 암호(PQC) 알고리즘의 보안, 성능 및 실제 적용 가능성을 비교합니다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨터의 공격에 안전한 암호화 알고리즘 개발의 필요성.

Method: Kyber, sntrup761, FrodoKEM 같은 주요 포스트 양자 암호 알고리즘을 비교 분석.

Result: 각 알고리즘의 강점과 약점을 강조하고 향후 연구 방향에 대한 통찰을 제공합니다.

Conclusion: 포스트 양자 암호의 현재 상태와 양자 컴퓨팅 시대의 미래 전망을 이해하기 위한 기초가 됩니다.

Abstract: Post-quantum cryptography (PQC) aims to develop cryptographic algorithms that
are secure against attacks from quantum computers. This paper compares the
leading postquantum cryptographic algorithms, such as Kyber, sntrup761, and
FrodoKEM, in terms of their security, performance, and real-world
applicability. The review highlights the strengths and weaknesses of each
algorithm and provides insights into future research directions. We also
discuss the challenges of transitioning from classical to post-quantum systems
and the potential impacts on various industries. This paper serves as a
foundation for understanding the current state of post-quantum cryptography and
its future prospects in the quantum computing era.

</details>


### [110] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 본 연구에서는 Large Language Models(LLMs)에 대한 제어 공격을 방어하기 위한 새로운 방법인 Context Filtering 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLMs의 성능이 향상되었지만, 다양한 jailbreak 공격이 안전성과 윤리적 측면에서 위험을 초래하고 있습니다.

Method: Context Filtering 모델은 신뢰할 수 없는 맥락을 걸러내고 실제 사용자 의도를 포함한 주요 프롬프트를 식별하는 입력 전처리 방법입니다.

Result: 우리 모델은 jailbreak 공격의 성공률을 최대 88%까지 줄이면서 원래 LLM의 성능을 유지합니다.

Conclusion: 모델은 화이트박스와 블랙박스 모델 모두에 적용할 수 있으며, 모델의 미세 조정 없이 안전성을 향상시킬 수 있는 플러그 앤 플레이 방식입니다.

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [111] [Cognitive Cybersecurity for Artificial Intelligence: Guardrail Engineering with CCS-7](https://arxiv.org/abs/2508.10033)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: 언어 모델은 전통적인 행동 정렬을 벗어난 인간과 유사한 인지 취약성을 보인다. 이 논문은 인간 인지 보안 연구에 근거한 7가지 취약성의 분류법(CCS-7)를 제시하고, '생각 먼저, 항상 검증하기'(TFVA) 수업을 통해 인지 보안을 7.9% 개선함을 입증하였다. 그 결과, 아키텍처에 따라 취약성이 다르게 나타나며, 특정 모델에서는 오류율이 최대 135%까지 증가했다. 이는 인지 안전성을 아키텍처별 공학 문제로 재조명하며, 배포 전 아키텍처 인식 테스트의 필요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델의 인지 취약성을 이해하고 이를 개선하기 위한 연구

Method: 151명을 대상으로 하는 무작위 대조 시험을 통해 '생각 먼저, 항상 검증하기'(TFVA) 교육을 실시했다.

Result: 12,180개의 실험을 통해 TFVA 스타일의 가드레일이 아키텍처에 따라 다르게 작용하는 취약성을 드러냈다.

Conclusion: 하나의 아키텍처에서 효과적인 개입이 다른 아키텍처에서는 실패하거나 해를 끼칠 수 있음을 보여주며, 배포 전 아키텍처 인식 인지 안전성 테스트가 필요하다는 점을 강조한다.

Abstract: Language models exhibit human-like cognitive vulnerabilities, such as
emotional framing, that escape traditional behavioral alignment. We present
CCS-7 (Cognitive Cybersecurity Suite), a taxonomy of seven vulnerabilities
grounded in human cognitive security research. To establish a human benchmark,
we ran a randomized controlled trial with 151 participants: a "Think First,
Verify Always" (TFVA) lesson improved cognitive security by +7.9% overall. We
then evaluated TFVA-style guardrails across 12,180 experiments on seven diverse
language model architectures. Results reveal architecture-dependent risk
patterns: some vulnerabilities (e.g., identity confusion) are almost fully
mitigated, while others (e.g., source interference) exhibit escalating
backfire, with error rates increasing by up to 135% in certain models. Humans,
in contrast, show consistent moderate improvement. These findings reframe
cognitive safety as a model-specific engineering problem: interventions
effective in one architecture may fail, or actively harm, another, underscoring
the need for architecture-aware cognitive safety testing before deployment.

</details>


### [112] [Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems](https://arxiv.org/abs/2508.10035)
*Varsha Sen,Biswash Basnet*

Main category: cs.CR

TL;DR: 이 논문은 스마트 그리드 인프라, 특히 가정용 지역 네트워크(HAN)에서의 잘못된 데이터 삽입 공격(FDIA)을 탐지하고 분류하기 위한 기계 학습 기반 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 그리드 인프라가 실시간 모니터링과 제어를 사용하므로 FDIAs가 심각한 위협이 된다.

Method: LIGHTWEIGHT 인공지능 신경망(ANN)과 양방향 LSTM을 사용하여 에너지 소비, 비용, 시간 맥락을 통한 실시간 탐지 및 다양한 공격 유형 분류를 수행한다.

Result: 실험 결과는 제안된 모델이 FDIAs를 효과적으로 식별하고 분류하며, 스마트 그리드에 대한 회복성을 향상시키는 확장 가능한 솔루션을 제공함을 보여준다.

Conclusion: 이 연구는 주거 단말에서 스마트 그리드 사이버 보안을 강화하는 지능형 데이터 기반 방어 메커니즘 구축에 기여한다.

Abstract: False Data Injection Attacks (FDIAs) pose a significant threat to smart grid
infrastructures, particularly Home Area Networks (HANs), where real-time
monitoring and control are highly adopted. Owing to the comparatively less
stringent security controls and widespread availability of HANs, attackers view
them as an attractive entry point to manipulate aggregated demand patterns,
which can ultimately propagate and disrupt broader grid operations. These
attacks undermine the integrity of smart meter data, enabling malicious actors
to manipulate consumption values without activating conventional alarms,
thereby creating serious vulnerabilities across both residential and
utility-scale infrastructures. This paper presents a machine learning-based
framework for both the detection and classification of FDIAs using residential
energy data. A real-time detection is provided by the lightweight Artificial
Neural Network (ANN), which works by using the most vital features of energy
consumption, cost, and time context. For the classification of different attack
types, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and
sigmoid attack shapes through learning sequential dependencies in the data. A
synthetic time-series dataset was generated to emulate realistic household
behaviour. Experimental results demonstrate that the proposed models are
effective in identifying and classifying FDIAs, offering a scalable solution
for enhancing grid resilience at the edge. This work contributes toward
building intelligent, data-driven defence mechanisms that strengthen smart grid
cybersecurity from residential endpoints.

</details>


### [113] [Certifiably robust malware detectors by design](https://arxiv.org/abs/2508.10038)
*Pierre-Francois Gimenez,Sarath Sivaprasad,Mario Fritz*

Main category: cs.CR

TL;DR: 본 논문은 정적 악성 코드 분석을 위한 새로운 모델 아키텍처를 제안하며, 머신러닝 기반 탐지 방법과의 비교를 통해 견고한 악성 코드 탐지를 달성하는 방법을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 정적 악성 코드 분석의 스케일러블한 탐지를 위한 머신러닝 기술의 사용과 악성 코드에 대한 적대적 예제의 생성 필요성.

Method: 견고한 악성 코드 탐지를 위한 새로운 모델 아키텍처 제안 및 모든 견고한 탐지기가 특정 구조로 분해될 수 있음을 보여줌.

Result: 제안된 접근 방법이 머신러닝 기반 악성 코드 탐지 방법과 비교하여 탐지 성능을 제한적으로 감소시키면서도 견고한 탐지를 가능하게 함.

Conclusion: 이 프레임워크는 다양한 악성 코드 탐지 방법의 성능을 강화할 수 있는 가능성을 지닌다.

Abstract: Malware analysis involves analyzing suspicious software to detect malicious
payloads. Static malware analysis, which does not require software execution,
relies increasingly on machine learning techniques to achieve scalability.
Although such techniques obtain very high detection accuracy, they can be
easily evaded with adversarial examples where a few modifications of the sample
can dupe the detector without modifying the behavior of the software. Unlike
other domains, such as computer vision, creating an adversarial example of
malware without altering its functionality requires specific transformations.
We propose a new model architecture for certifiably robust malware detection by
design. In addition, we show that every robust detector can be decomposed into
a specific structure, which can be applied to learn empirically robust malware
detectors, even on fragile features. Our framework ERDALT is based on this
structure. We compare and validate these approaches with machine-learning-based
malware detection methods, allowing for robust detection with limited reduction
of detection performance.

</details>


### [114] [Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries](https://arxiv.org/abs/2508.10039)
*Wenqiang Wang,Yan Xiao,Hao Lin,Yangshijie Zhang,Xiaochun Cao*

Main category: cs.CR

TL;DR: CEMA는 다양한 작업에서 텍스트 적대 공격을 효과적으로 수행할 수 있는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 작업 적대 공격은 제한된 쿼리와 단일 작업 유형에만 초점을 맞춰 실제 사용 사례에 덜 효과적이다.

Method: CEMA는 다양한 작업 간의 적대 텍스트의 전이 가능성을 이용하여 훈련이 간단한 대체 모델을 사용하여 다중 작업 시나리오를 단순화한다.

Result: CEMA는 100개의 쿼리로도 공격 성공률을 크게 향상시키며 상업적 API와 다양한 모델에 적용 가능하다.

Conclusion: CEMA는 다양한 작업을 대상으로 하는 적대적 공격에 있어 강력하고 효과적인 도구임을 보여준다.

Abstract: Current multi-task adversarial text attacks rely on abundant access to shared
internal features and numerous queries, often limited to a single task type. As
a result, these attacks are less effective against practical scenarios
involving black-box feedback APIs, limited queries, or multiple task types. To
bridge this gap, we propose \textbf{C}luster and \textbf{E}nsemble
\textbf{M}ulti-task Text Adversarial \textbf{A}ttack (\textbf{CEMA}), an
effective black-box attack that exploits the transferability of adversarial
texts across different tasks. CEMA simplifies complex multi-task scenarios by
using a \textit{deep-level substitute model} trained in a
\textit{plug-and-play} manner for text classification, enabling attacks without
mimicking the victim model. This approach requires only a few queries for
training, converting multi-task attacks into classification attacks and
allowing attacks across various tasks.
  CEMA generates multiple adversarial candidates using different text
classification methods and selects the one that most effectively attacks
substitute models.
  In experiments involving multi-task models with two, three, or six
tasks--spanning classification, translation, summarization, and text-to-image
generation--CEMA demonstrates significant attack success with as few as 100
queries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google
Translate), large language models (e.g., ChatGPT 4o), and image-generation
models (e.g., Stable Diffusion V2), showcasing its versatility and
effectiveness in real-world applications.

</details>


### [115] [Quantum Prime Factorization: A Novel Approach Based on Fermat Method](https://arxiv.org/abs/2508.10041)
*Julien Mellaerts*

Main category: cs.CR

TL;DR: 본 논문에서는 합성 홀수의 인수분해를 위한 새로운 양자 알고리즘을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 합성 홀수의 인수분해 문제를 효율적으로 해결하기 위해.

Method: 클래식 페르마 방법을 개선하고, 이를 양자 어닐러를 위한 최적화 문제로 다시 공식화함.

Result: 양자 장치로 인수분해된 가장 큰 수 8,689,739를 인수분해했습니다.

Conclusion: 우리의 연구는 합성 홀수의 인수분해에 대한 새로운 접근을 제시하며, 양자 컴퓨팅의 응용 가능성을 확대합니다.

Abstract: In this paper, we introduce a novel quantum algorithm for the factorization
of composite odd numbers. This work makes two significant contributions. First,
we present a new improvement to the classical Fermat method, fourfold reducing
the computational complexity of factoring. Second, we reformulate Fermat
factorization method as an optimization problem suitable for Quantum Annealers
which allowed us to factorize 8,689,739, the biggest number ever factorized
using a quantum device to our knowledge.

</details>


### [116] [FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2508.10042)
*Jane Carney,Kushal Upreti,Gaby G. Dagher,Tim Andersen*

Main category: cs.CR

TL;DR: 분산 학습에서 데이터 독성을 감지하기 위한 블록체인 기반 프레임워크인 \\Sys를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 딥러닝의 한계를 극복하고 IoT 기기의 개인 데이터를 이용한 공동 훈련을 가능하게 하기 위해.

Method: 각 클라이언트에서 생성된 판별 모델을 사용해 모델 업데이트에서 데이터 독성을 감지하는 프레임워크를 제시한다.

Result: \\Sys는 데이터 독성 공격에 강력하며, 판별 모델의 생성이 확장 가능하다는 것을 보여준다.

Conclusion: 본 연구는 블록체인 기술을 활용하여 분산 학습의 데이터 독성 문제를 해결할 수 있는 가능성을 제시한다.

Abstract: Federated learning enhances traditional deep learning by enabling the joint
training of a model with the use of IoT device's private data. It ensures
privacy for clients, but is susceptible to data poisoning attacks during
training that degrade model performance and integrity. Current poisoning
detection methods in federated learning lack a standardized detection method or
take significant liberties with trust. In this paper, we present \Sys, a novel
blockchain-enabled poison detection framework in federated learning. The
framework decentralizes the role of the global server across participating
clients. We introduce a judge model used to detect data poisoning in model
updates. The judge model is produced by each client and verified to reach
consensus on a single judge model. We implement our solution to show \Sys is
robust against data poisoning attacks and the creation of our judge model is
scalable.

</details>


### [117] [Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/abs/2508.10043)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CR

TL;DR: LLM과 자율 에이전트를 결합할 때 보안 문제가 발생하며, MAESTRO 프레임워크를 통해 에이전틱 AI의 취약점을 노출하고 평가하여 제거했다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트를 사용하는 네트워크 모니터링 및 의사결정 시스템에서 LLM과의 결합으로 인한 보안 문제를 다루기 위함이다.

Method: MAESTRO 프레임워크를 사용하여 7 계층 위협 모델링 아키텍처를 통해 에이전틱 AI의 취약점을 평가하고 제거하는 프로토타입 시스템을 구축했다.

Result: 두 가지 실용적인 위협 사례(서비스 거부 공격 및 메모리 오염)를 확인했으며, 이로 인해 성능 저하가 발생했다.

Conclusion: 이 시스템은 실용적인 위협 매핑 및 리스크 평가에 유효하며, 메모리 무결성 강화를 위한 다층 방어 접근 방식이 필요하다.

Abstract: When combining Large Language Models (LLMs) with autonomous agents, used in
network monitoring and decision-making systems, this will create serious
security issues. In this research, the MAESTRO framework consisting of the
seven layers threat modeling architecture in the system was used to expose,
evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent
system was constructed and implemented, using Python, LangChain, and telemetry
in WebSockets, and deployed with inference, memory, parameter tuning, and
anomaly detection modules. Two practical threat cases were confirmed as
follows: (i) resource denial of service by traffic replay denial-of-service,
and (ii) memory poisoning by tampering with the historical log file maintained
by the agent. These situations resulted in measurable levels of performance
degradation, i.e. telemetry updates were delayed, and computational loads were
increased, as a result of poor system adaptations. It was suggested to use a
multilayered defense-in-depth approach with memory isolation, validation of
planners and anomaly response systems in real-time. These findings verify that
MAESTRO is viable in operational threat mapping, prospective risk scoring, and
the basis of the resilient system design. The authors bring attention to the
importance of the enforcement of memory integrity, paying attention to the
adaptation logic monitoring, and cross-layer communication protection that
guarantee the agentic AI reliability in adversarial settings.

</details>


### [118] [Generative AI for Cybersecurity of Energy Management Systems: Methods, Challenges, and Future Directions](https://arxiv.org/abs/2508.10044)
*Aydin Zaboli,Junho Hong*

Main category: cs.CR

TL;DR: 이 논문은 에너지 관리 시스템(EMS)을 위한 포괄적인 보안 프레임워크를 제시하여 사이버 보안 취약점 및 시스템 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 에너지 관리 시스템의 보안 문제를 체계적으로 해결하고자 한다.

Method: 멀티 포인트 공격/오류 모델과 생성 AI 기반 이상 탐지 시스템을 통해 보안 취약점을 식별하고 해결하는 방식으로 접근한다.

Result: IEEE 14-Bus 시스템에서 프레임워크의 효과성을 검증하고, 시각 분석을 통해 불일치를 발견하였다.

Conclusion: 이 프레임워크는 수치 분석과 시각적 패턴 인식을 결합하여 사이버 위협 및 시스템 오류에 대한 보호를 강화한다.

Abstract: This paper elaborates on an extensive security framework specifically
designed for energy management systems (EMSs), which effectively tackles the
dynamic environment of cybersecurity vulnerabilities and/or system problems
(SPs), accomplished through the incorporation of novel methodologies. A
comprehensive multi-point attack/error model is initially proposed to
systematically identify vulnerabilities throughout the entire EMS data
processing pipeline, including post state estimation (SE) stealth attacks, EMS
database manipulation, and human-machine interface (HMI) display corruption
according to the real-time database (RTDB) storage. This framework acknowledges
the interconnected nature of modern attack vectors, which utilize various
phases of supervisory control and data acquisition (SCADA) data flow. Then,
generative AI (GenAI)-based anomaly detection systems (ADSs) for EMSs are
proposed for the first time in the power system domain to handle the scenarios.
Further, a set-of-mark generative intelligence (SoM-GI) framework, which
leverages multimodal analysis by integrating visual markers with rules
considering the GenAI capabilities, is suggested to overcome inherent spatial
reasoning limitations. The SoM-GI methodology employs systematic visual
indicators to enable accurate interpretation of segmented HMI displays and
detect visual anomalies that numerical methods fail to identify. Validation on
the IEEE 14-Bus system shows the framework's effectiveness across scenarios,
while visual analysis identifies inconsistencies. This integrated approach
combines numerical analysis with visual pattern recognition and linguistic
rules to protect against cyber threats and system errors.

</details>


### [119] [NetMoniAI: An Agentic AI Framework for Network Security & Monitoring](https://arxiv.org/abs/2508.10052)
*Pallavi Zambare,Venkata Nikhil Thanikella,Nikhil Padmanabh Kottur,Sree Akhil Akula,Ying Liu*

Main category: cs.CR

TL;DR: NetMoniAI는 분산 분석과 경량 중앙 조정을 통합한 자동 네트워크 모니터링 및 보안 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 네트워크 보안 문제를 해결하기 위해 자동화된 모니터링 체계를 개발하는 것이 필요하다.

Method: 노드에서 자율적인 마이크로 에이전트가 로컬 트래픽 분석과 이상 탐지를 수행하며, 중앙 컨트롤러가 노드 간 통찰을 집계하여 공격을 감지한다.

Result: NetMoniAI는 자원 제약 하에서도 성능이 확장 가능하며 중복성을 줄이고 응답 시간을 개선하는 동시에 정확성을 유지한다.

Conclusion: 전체 프레임워크는 오픈 소스로 제공되어 연구자와 실무자들이 다양한 네트워크 환경과 위협 시나리오에서 복제하고 검증할 수 있도록 한다.

Abstract: In this paper, we present NetMoniAI, an agentic AI framework for automatic
network monitoring and security that integrates decentralized analysis with
lightweight centralized coordination. The framework consists of two layers:
autonomous micro-agents at each node perform local traffic analysis and anomaly
detection. A central controller then aggregates insights across nodes to detect
coordinated attacks and maintain system-wide situational awareness. We
evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.
Results confirm that the two-tier agentic-AI design scales under resource
constraints, reduces redundancy, and improves response time without
compromising accuracy. To facilitate broader adoption and reproducibility, the
complete framework is available as open source. This enables researchers and
practitioners to replicate, validate, and extend it across diverse network
environments and threat scenarios. Github link:
https://github.com/pzambare3/NetMoniAI

</details>


### [120] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: 디지털 워터마킹을 활용하여 기계적 불학습의 효율성을 높이는 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 삭제 권리에 대한 수요 증가로 기계적 불학습이 중요한 도구가 되었다.

Method: 디지털 워터마킹을 활용하여 데이터를 전략적으로 수정하고 기계적 불학습을 촉진한다.

Result: Water4MU는 이미지 분류 및 생성 작업에서 효과적으로 기계적 불학습을 수행한다.

Conclusion: 기존 방법을 능가하여 도전적인 불학습 시나리오에서도 우수한 성능을 보인다.

Abstract: With the increasing demand for the right to be forgotten, machine unlearning
(MU) has emerged as a vital tool for enhancing trust and regulatory compliance
by enabling the removal of sensitive data influences from machine learning (ML)
models. However, most MU algorithms primarily rely on in-training methods to
adjust model weights, with limited exploration of the benefits that data-level
adjustments could bring to the unlearning process. To address this gap, we
propose a novel approach that leverages digital watermarking to facilitate MU
by strategically modifying data content. By integrating watermarking, we
establish a controlled unlearning mechanism that enables precise removal of
specified data while maintaining model utility for unrelated tasks. We first
examine the impact of watermarked data on MU, finding that MU effectively
generalizes to watermarked data. Building on this, we introduce an
unlearning-friendly watermarking framework, termed Water4MU, to enhance
unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO)
framework: at the upper level, the watermarking network is optimized to
minimize unlearning difficulty, while at the lower level, the model itself is
trained independently of watermarking. Experimental results demonstrate that
Water4MU is effective in MU across both image classification and image
generation tasks. Notably, it outperforms existing methods in challenging MU
scenarios, known as "challenging forgets".

</details>


### [121] [An Architecture for Distributed Digital Identities in the Physical World](https://arxiv.org/abs/2508.10185)
*René Mayrhofer,Michael Roland,Tobias Höller,Philipp Hofer,Mario Lins*

Main category: cs.CR

TL;DR: 탈중앙화된 디지털 신원 아키텍처를 설계하고 분석하여 물리적 거래에서의 개인 신원 관리를 개선한다.


<details>
  <summary>Details</summary>
Motivation: 디지털 신원은 디지털 및 물리적 서비스 거래를 중재하는 데 점점 더 중요해지고 있으며, 중앙 집중식 관리에는 가용성과 개인 정보 보호 문제에 대한 우려가 있다.

Method: 우리는 다양한 센서와 신원 기관, 특성 검증자 및 개인 신원 에이전트(PIA)를 포함한 분산형 디지털 신원 아키텍처를 설계하고 구현했다.

Result: 제안된 아키텍처는 거래가 완전히 탈중앙화된 방식으로 수행되며, 우리가 현재 중앙 조정을 가정하는 구성 요소는 선택 사항이다.

Conclusion: 컨셉 증명 구현을 통해 아키텍처와 초기 프로토콜의 실제 타당성을 보여주었다.

Abstract: Digital identities are increasingly important for mediating not only digital
but also physical service transactions. Managing such identities through
centralized providers can cause both availability and privacy concerns: single
points of failure and control are ideal targets for global attacks on
technical, organizational, or legal fronts. We design, analyze, and build a
distributed digital identity architecture for physical world transactions in
common scenarios like unlocking doors, public transport, or crossing country
borders. This architecture combines (biometric and other) sensors, (established
and upcoming) identity authorities, attribute verifiers, and a new core
component we call the \emph{Personal Identity Agent (PIA)} that represents
individuals with their identity attributes in the digital domain. All
transactions are conducted in a completely decentralized manner, and the
components for which we currently assume central coordination are optional and
only used for assisting with service discovery and latency reduction. We
present a first protocol between these parties and formally verify that it
achieves relevant security properties based on a realistic threat model
including strong global adversaries. A proof-of-concept implementation
demonstrates practical feasibility of both architecture and initial protocol
for applications that can tolerate end-to-end latencies in the range of a few
seconds.

</details>


### [122] [Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations](https://arxiv.org/abs/2508.10212)
*Md Sazedur Rahman,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CR

TL;DR: 이 논문에서는 지하 광산 운영의 데이터 수집에서 발생할 수 있는 개인정보 보호 문제를 해결하기 위해 MineDetect라는 방어 연합 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 지하 광산의 센서 데이터는 개인 정보 보호 및 안전과 관련된 중요한 정보를 포함하고 있으며, 이를 안전하게 처리하는 방법이 필요하다.

Method: MineDetect는 공격 모델을 감지하고 분리하며, 낮은 품질의 데이터가 있는 광산의 영향을 완화하는 방어 연합 학습 프레임워크를 개발하였다.

Result: 다양한 데이터 세트에서의 종합적인 시뮬레이션 결과 MineDetect는 기존 방법들보다 강인성과 정확도에서 우수한 성능을 보여준다.

Conclusion: MineDetect는 지하 광산의 안전성과 운영 효과성을 향상시키기 위한 중요한 발전을 이루었다.

Abstract: Underground mining operations rely on distributed sensor networks to collect
critical data daily, including mine temperature, toxic gas concentrations, and
miner movements for hazard detection and operational decision-making. However,
transmitting raw sensor data to a central server for training deep learning
models introduces significant privacy risks, potentially exposing sensitive
mine-specific information. Federated Learning (FL) offers a transformative
solution by enabling collaborative model training while ensuring that raw data
remains localized at each mine. Despite its advantages, FL in underground
mining faces key challenges: (i) An attacker may compromise a mine's local
model by employing techniques such as sign-flipping attacks or additive noise,
leading to erroneous predictions; (ii) Low-quality (yet potentially valuable)
data, caused by poor lighting conditions or sensor inaccuracies in mines may
degrade the FL training process. In response, this paper proposes MineDetect, a
defense FL framework that detects and isolates the attacked models while
mitigating the impact of mines with low-quality data. MineDetect introduces two
key innovations: (i) Detecting attacked models (maliciously manipulated) by
developing a history-aware mechanism that leverages local and global averages
of gradient updates; (ii) Identifying and eliminating adversarial influences
from unreliable models (generated by clients with poor data quality) on the FL
training process. Comprehensive simulations across diverse datasets demonstrate
that MineDetect outperforms existing methods in both robustness and accuracy,
even in challenging non-IID data scenarios. Its ability to counter adversarial
influences while maintaining lower computational efficiency makes it a vital
advancement for improving safety and operational effectiveness in underground
mining.

</details>


### [123] [BERTector: Intrusion Detection Based on Joint-Dataset Learning](https://arxiv.org/abs/2508.10327)
*Haoyang Hu,Xun Huang,Chenyu Wu,Shiwen Liu,Zhichao Lian,Shuangquan Zhang*

Main category: cs.CR

TL;DR: IDS의 일반화 및 강건성 문제를 해결하기 위한 새로운 훈련 패러다임과 BERT 기반의 BERTector 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 네트워크 트래픽의 이질성과 공격 패턴의 다양성 때문에 IDS가 일반화 및 강건성 문제에 직면하고 있습니다.

Method: BERTector는 트래픽 인식 시맨틱 토큰화와 하이브리드 데이터 세트를 통한 감독형 미세 조정, 효율적인 훈련을 위한 저_rank 적응(LoRA)이라는 세 가지 주요 구성 요소를 통합합니다.

Result: BERTector는 최신 감지 정확도, 강력한 크로스 데이터 세트 일반화 능력, 및 적대적 변동에 대한 뛰어난 강건성을 보여줍니다.

Conclusion: 이 연구는 복잡하고 동적인 네트워크 환경에서 현대 IDS를 위한 통합되고 효율적인 솔루션을 확립합니다.

Abstract: Intrusion detection systems (IDS) are facing challenges in generalization and
robustness due to the heterogeneity of network traffic and the diversity of
attack patterns. To address this issue, we propose a new joint-dataset training
paradigm for IDS and propose a scalable BERTector framework based on BERT.
BERTector integrates three key components: NSS-Tokenizer for traffic-aware
semantic tokenization, supervised fine-tuning with a hybrid dataset, and
low-rank adaptation (LoRA) for efficient training. Extensive experiments show
that BERTector achieves state-of-the-art detection accuracy, strong
cross-dataset generalization capabilities, and excellent robustness to
adversarial perturbations. This work establishes a unified and efficient
solution for modern IDS in complex and dynamic network environments.

</details>


### [124] [Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches](https://arxiv.org/abs/2508.10431)
*Chris Cao,Gururaj Saileshwar*

Main category: cs.CR

TL;DR: MIRAGE 랜덤화 캐시에서 AES 키를 회수하는 공격에 대한 주장이 잘못된 모델링에서 비롯된 것임을 보여줌.


<details>
  <summary>Details</summary>
Motivation: MIRAGE 랜덤화 캐시를 대상으로 한 공격의 유효성을 검증하기 위해.

Method: MIRAGE의 시뮬레이션을 분석하고 고정 시드를 사용하여 발생하는 문제점을 지적함.

Result: 난수 생성기의 초기 시드를 랜덤화하자 AES T-테이블 접근과 공격자의 실행 시간 간의 상관관계가 사라짐.

Conclusion: 보고된 정보 유출은 잘못된 모델링의 결과이며, 실제 MIRAGE의 취약성이 아님을 입증함.

Abstract: Recent work presented at USENIX Security 2025 claims that occupancy-based
attacks can recover AES keys from the MIRAGE randomized cache. In this paper,
we examine these claims and find that they arise from fundamental modeling
flaws. Most critically, the authors' simulation of MIRAGE uses a constant seed
to initialize the random number generator used for global evictions in MIRAGE,
causing every AES encryption they trace to evict the same deterministic
sequence of cache lines. This artificially creates a highly repeatable timing
pattern that is not representative of a realistic implementation of MIRAGE,
where eviction sequences vary randomly between encryptions. When we instead
randomize the eviction seed for each run, reflecting realistic operation, the
correlation between AES T-table accesses and attacker runtimes disappears, and
the attack fails. These findings show that the reported leakage is an artifact
of incorrect modeling, and not an actual vulnerability in MIRAGE.

</details>


### [125] [AlDBaran: Towards Blazingly Fast State Commitments for Blockchains](https://arxiv.org/abs/2508.10493)
*Bernhard Kauer,Aleksandr Petrosyan,Benjamin Livshits*

Main category: cs.CR

TL;DR: AlDBaran은 고속 블록체인 시스템을 위한 인증된 데이터베이스의 중요한 발전을 나타내며, 50 Gbps의 네트워크 처리량으로 효율적인 상태 업데이트를 처리할 수 있는 구조를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 현대 블록체인 시스템의 무결성을 유지하기 위해 인증된 데이터베이스의 필요성이 존재한다.

Method: 디스크 I/O 작업을 제거하고, 프리패칭 전략을 구현하며, 메르클 트리의 업데이트 메커니즘을 개선하여 인증된 데이터 구조를 전개했다.

Result: AlDBaran은 초당 약 4800만 업데이트를 처리할 수 있으며, 역사적 상태 증명을 지원하여 다양한 신규 애플리케이션을 가능하게 한다.

Conclusion: AlDBaran은 자원 제한 환경에서도 매력적인 솔루션으로, 성능을 더욱 향상시킬 수 있는 모듈화된 역사적 데이터 기능을 제공한다.

Abstract: The fundamental basis for maintaining integrity within contemporary
blockchain systems is provided by authenticated databases. Our analysis
indicates that a significant portion of the approaches applied in this domain
fail to sufficiently meet the stringent requirements of systems processing
transactions at rates of multi-million TPS. AlDBaran signifies a substantial
advancement in authenticated databases. By eliminating disk I/O operations from
the critical path, implementing prefetching strategies, and refining the update
mechanism of the Merkle tree, we have engineered an authenticated data
structure capable of handling state updates efficiently at a network throughput
of 50 Gbps. This throughput capacity significantly surpasses any empirically
documented blockchain throughput, guaranteeing the ability of even the most
high-throughput blockchains to generate state commitments effectively.
  AlDBaran provides support for historical state proofs, which facilitates a
wide array of novel applications. For instance, the deployment of AlDBaran
could enable blockchains that do not currently support state commitments to
offer functionalities for light clients and/or implement rollups.
  When benchmarked against alternative authenticated data structure projects,
AlDBaran exhibits superior performance and simplicity. In particular, AlDBaran
achieves speeds of approximately 48 million updates per second using an
identical machine configuration. This characteristic renders AlDBaran an
attractive solution for resource-limited environments, as its historical data
capabilities can be modularly isolated (and deactivated), which further
enhances performance. On consumer-level portable hardware, it achieves
approximately 8 million updates/s in an in-memory setting and 5 million
updates/s with snapshots at sub-second intervals, illustrating compelling and
cost-effective scalability.

</details>


### [126] [Codes on any Cayley Graph have an Interactive Oracle Proof of Proximity](https://arxiv.org/abs/2508.10510)
*Hugo Delavenne,Louise Lallemand*

Main category: cs.CR

TL;DR: 이 논문은 흐 blooming IOPP를 일반화하여 다양한 코드를 지원하며, 높은 음속 매개변수를 유지하면서 복잡도 매개변수를 약간 낮추는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 코드 기반 SNARKs의 핵심 구성 요소인 IOPP의 적용 범위 확대를 목표로 한다.

Method: [DMR25]에서 소개된 꽃 IOPP를 (2, n)-정규 Tanner 코드에서 Cayley 그래프의 엣지에 인덱스된 기호를 가진 모든 코드로 일반화한다.

Result: 제안된 일반화는 약간의 복잡도 매개변수 감소와 함께 높은 음속 매개변수를 유지한다.

Conclusion: 일부 Cayley 그래프의 우수한 전개 특성 덕분에 상수 비율과 상수 최소 거리 코드를 적용할 수 있게 되었다.

Abstract: Interactive Oracle Proofs of Proximity (IOPP) are at the heart of code-based
SNARKs, a family of zeroknowledge protocols. The first and most famous one is
the FRI protocol [BBHR18a], that efficiently tests proximity to Reed-Solomon
codes. This paper generalizes the flowering IOPP introduced in [DMR25] for some
specific (2, n)-regular Tanner codes to a much broader variety of codes: any
code with symbols indexed on the edges of a Cayley graph. The flowering
protocol of [DMR25] had a soundness parameter much lower than the FRI protocol
[BCI + 23], and complexity parameters that could compete with the FRI
[BBHR18a]. The lower soundness and the absence of restriction on the base field
may lead to other practical speedups, however the codes considered in [DMR25]
have an o(1) minimum distance. The generalization proposed in this paper
preserves the soundness parameter with a slight decrease of the complexity
parameters, while allowing being applied on codes with constant rate and
constant minimum distance thanks to the good expansion properties of some
families of Cayley graphs.

</details>


### [127] [A Transformer-Based Approach for DDoS Attack Detection in IoT Networks](https://arxiv.org/abs/2508.10636)
*Sandipan Dey,Payal Santosh Kate,Vatsala Upadhyay,Abhishek Vaish*

Main category: cs.CR

TL;DR: 본 논문은 IoT 장치에서 DDoS 공격을 탐지하기 위해 Transformer 모델을 사용하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: DDoS 공격은 IoT 장치의 보안에 큰 위협이 되며 네트워크 인프라에 심각한 피해를 줄 수 있습니다. IoT 장치는 자원 제약으로 인해 이러한 공격에 취약합니다. 기존의 DDoS 탐지 방법은 IoT 네트워크의 동적 특성과 공격의 확장성, 프로토콜의 다양성, 높은 트래픽 양 및 장치 행동의 변동성을 처리하기에 충분히 효율적이지 않습니다.

Method: 제안된 모델은 네트워크 트래픽 데이터에서 특징을 추출하고 이를 자기 주의 메커니즘을 사용하여 처리합니다. 이를 통해 DDoS 공격 탐지를 수행합니다.

Result: 실제 데이터셋을 사용한 실험 결과, 제안된 접근 방식이 전통적인 기계 학습 기술보다 우수함을 보여주었으며, 정확도, 정밀도, 재현율 및 F1-score를 비교하여 검증할 수 있었습니다.

Conclusion: 이 연구의 결과는 Transformer 모델이 IoT 장치에서 DDoS 공격을 탐지하는 효과적인 솔루션이 될 수 있으며, 실제 IoT 환경에서 배포될 가능성이 있음을 보여줍니다.

Abstract: DDoS attacks have become a major threat to the security of IoT devices and
can cause severe damage to the network infrastructure. IoT devices suffer from
the inherent problem of resource constraints and are therefore susceptible to
such resource-exhausting attacks. Traditional methods for detecting DDoS
attacks are not efficient enough to cope with the dynamic nature of IoT
networks, as well as the scalability of the attacks, diversity of protocols,
high volume of traffic, and variability in device behavior, and variability of
protocols like MQTT, CoAP, making it hard to implement security across all the
protocols. In this paper, we propose a novel approach, i.e., the use of
Transformer models, which have shown remarkable performance in natural language
processing tasks, for detecting DDoS attacks on IoT devices. The proposed model
extracts features from network traffic data and processes them using a
self-attention mechanism. Experiments conducted on a real-world dataset
demonstrate that the proposed approach outperforms traditional machine learning
techniques, which can be validated by comparing both approaches' accuracy,
precision, recall, and F1-score. The results of this study show that the
Transformer models can be an effective solution for detecting DDoS attacks on
IoT devices and have the potential to be deployed in real-world IoT
environments.

</details>


### [128] [MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks](https://arxiv.org/abs/2508.10639)
*Anyuan Sang,Lu Zhou,Li Yang,Junbo Jia,Huipeng Yang,Pengbin Feng,Jianfeng Ma*

Main category: cs.CR

TL;DR: MirGuard는 그래프 조작 공격에 강한 이상 탐지 프레임워크로, Logic-Aware Noise Injection을 통해 의미적으로 유효한 그래프 뷰를 생성하며, Logic-Preserving Contrastive Learning을 활용하여 모델의 견고성을 높입니다.


<details>
  <summary>Details</summary>
Motivation: PIDS가 그래프 조작 공격에 취약하다는 기존 연구 결과를 바탕으로, 실제적으로 사용할 수 있는 강력한 탐지 솔루션이 필요합니다.

Method: Logic-Aware Noise Injection(LNI)을 통해 의미적 유효성을 유지하는 그래프 뷰를 생성하고, Logic-Preserving Contrastive Learning 프레임워크를 사용하여 모델의 표현력을 향상시킵니다.

Result: MirGuard는 여러 프로베넌스 데이터셋에서 다양한 그래프 조작 공격에 대해 기존의 최첨단 탐지기보다 뛰어난 견고성을 보였습니다.

Conclusion: MirGuard는 현대 사이버 보안 도전 과제에 대한 강력하고 효과적인 솔루션을 제공하는 첫 번째 목표 지향적 연구입니다.

Abstract: Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have
become essential tools for anomaly detection in host systems due to their
ability to capture rich contextual and structural information, as well as their
potential to detect unknown attacks. However, recent studies have shown that
these systems are vulnerable to graph manipulation attacks, where attackers
manipulate the graph structure to evade detection. While some previous
approaches have discussed this type of attack, none have fully addressed it
with a robust detection solution, limiting the practical applicability of
PIDSes.
  To address this challenge, we propose MirGuard, a robust anomaly detection
framework that combines logic-aware multi-view augmentation with contrastive
representation learning. Rather than applying arbitrary structural
perturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to
generate semantically valid graph views, ensuring that all augmentations
preserve the underlying causal semantics of the provenance data. These views
are then used in a Logic-Preserving Contrastive Learning framework, which
encourages the model to learn representations that are invariant to benign
transformations but sensitive to adversarial inconsistencies. Comprehensive
evaluations on multiple provenance datasets demonstrate that MirGuard
significantly outperforms state-of-the-art detectors in robustness against
various graph manipulation attacks without sacrificing detection performance
and efficiency. Our work represents the first targeted study to enhance PIDS
against such adversarial threats, providing a robust and effective solution to
modern cybersecurity challenges.

</details>


### [129] [A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis](https://arxiv.org/abs/2508.10652)
*Richa Dasila,Vatsala Upadhyay,Samo Bobek,Abhishek Vaish*

Main category: cs.CR

TL;DR: 딥 러닝 모델은 사이버 보안에서 악성 코드 탐지와 대응에 중요한 역할을 하지만, 그 투명하지 않은 특성 때문에 해석이 어렵다. 본 연구에서는 설명 가능한 인공지능(XAI) 기법을 통합하여 이러한 모델의 해석 가능성과 신뢰성을 향상시키고, 동적 악성 코드 분석에 다층 퍼셉트론(MLP)의 효능을 탐구하였다.


<details>
  <summary>Details</summary>
Motivation: 딥 러닝 모델의 결정 과정을 이해하기 어려운 문제를 해결하고, 사이버 보안에서의 신뢰성을 높이고자 한다.

Method: XAI 기법을 통합하고, MLP를 포함한 여러 모델의 악성 코드 분류 효과를 평가한다.

Result: MLP가 메타모픽 악성 코드 탐지에 효과적이며, 다른 모델들(CNN, RNN, CNN-LSTM)과 비교하여 투명성을 확보한다.

Conclusion: 딥 러닝 모델의 내부 작동을 이해할 수 있도록 돕고, 사이버 보안 분야에서의 예측 능력에 대한 이해도를 높인다.

Abstract: Deep learning models are one of the security strategies, trained on extensive
datasets, and play a critical role in detecting and responding to these threats
by recognizing complex patterns in malicious code. However, the opaque nature
of these models-often described as "black boxes"-makes their decision-making
processes difficult to understand, even for their creators. This research
addresses these challenges by integrating Explainable AI (XAI) techniques to
enhance the interpretability and trustworthiness of malware detection models.
In this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware
analysis has been considered, a less explored area, and its efficacy in
detecting Metamorphic Malware, and further the effectiveness and transparency
of MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating
these models through the lens of Explainable AI (XAI). This comprehensive
approach aims to demystify the internal workings of deep learning models,
promoting a better understanding and trust in their predictive capabilities in
cybersecurity contexts. Such in-depth analysis and implementation haven't been
done to the best of our knowledge.

</details>


### [130] [Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/abs/2508.10677)
*Amine Tellache,Abdelaziz Amara Korba,Amdjed Mokhtari,Horea Moldovan,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 본 연구는 LLM 기반의 RAG 프레임워크를 통해 사이버 위협 인텔리전스를 자동화하고 효과적으로 사고 대응을 개선하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사이버 위협 완화를 위한 효과적인 사고 대응이 필수적이지만, 보안팀은 경고 피로, 높은 오탐지율, 방대한 비구조화된 사이버 위협 인텔리전스 문서들로 인해 압도되고 있다.

Method: 이 연구에서는 동적으로 검색된 사이버 위협 인텔리전스를 통합하여 사고 대응을 자동화하고 향상시키는 RAG 기반의 새로운 프레임워크를 소개한다. 이는 NLP 기반 유사성 검색과 외부 CTI 플랫폼에 대한 표준화된 쿼리를 결합한 하이브리드 검색 메커니즘을 포함한다.

Result: 실제 및 시뮬레이션 경고에 대한 경험적 검증을 통해, 우리의 접근 방식이 사고 대응의 정확성, 맥락화 및 효율성을 향상시켜 분석가의 작업 부담을 덜고 응답 지연을 줄임을 입증하였다.

Conclusion: 이 연구는 LLM 기반 사이버 위협 인텔리전스 융합이 자율 보안 운영을 발전시키고 지능적이며 적응 가능한 사이버 보안 프레임워크의 기초를 마련할 수 있는 잠재력을 강조한다.

Abstract: Effective incident response (IR) is critical for mitigating cyber threats,
yet security teams are overwhelmed by alert fatigue, high false-positive rates,
and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents.
While CTI holds immense potential for enriching security operations, its
extensive and fragmented nature makes manual analysis time-consuming and
resource-intensive. To bridge this gap, we introduce a novel
Retrieval-Augmented Generation (RAG)-based framework that leverages Large
Language Models (LLMs) to automate and enhance IR by integrating dynamically
retrieved CTI. Our approach introduces a hybrid retrieval mechanism that
combines NLP-based similarity searches within a CTI vector database with
standardized queries to external CTI platforms, facilitating context-aware
enrichment of security alerts. The augmented intelligence is then leveraged by
an LLM-powered response generation module, which formulates precise,
actionable, and contextually relevant incident mitigation strategies. We
propose a dual evaluation paradigm, wherein automated assessment using an
auxiliary LLM is systematically cross-validated by cybersecurity experts.
Empirical validation on real-world and simulated alerts demonstrates that our
approach enhances the accuracy, contextualization, and efficiency of IR,
alleviating analyst workload and reducing response latency. This work
underscores the potential of LLM-driven CTI fusion in advancing autonomous
security operations and establishing a foundation for intelligent, adaptive
cybersecurity frameworks.

</details>


### [131] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: LLM 기반 에이전트의 사용이 개인 정보 보호에 대한 위협을 증가시킬 수 있으며, 이를 해결하기 위해 공격자와 방어자의 지침을 개선하는 검색 기반 프레임워크를 제안.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 사용이 개인 정보 보호에 중대한 위협을 초래할 가능성이 있다.

Method: 개인정보에 민감한 에이전트 상호작용을 시뮬레이션하여 공격자와 방어자의 지침을 개선하는 검색 기반 프레임워크를 제안한다.

Result: 공격 전략이 직접적인 요청에서 개인정보 도용 및 동의 위조와 같은 정교한 다회전 전술로 발전하며, 방어 전략이 규칙 기반 제약에서 신원 확인 상태 기계로 발전한다.

Conclusion: 발견된 공격 및 방어 전략은 다양한 시나리오와 백본 모델에 따라 전이 가능하여 개인 정보 인식 에이전트를 구축하는 데 강력한 실제 유용성을 보인다.

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>
