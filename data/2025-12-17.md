<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 19]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 이 논문은 변동성이 큰 환경에서의 가치 제어를 위한 재사용 가능한 가치 프로필을 제안하며, 이를 통해 각 상황에 고유한 매개변수를 유지하지 않고 상황별 전략 선택을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 변동성이 큰 환경에서의 적응 행동은 잠재적 맥락에 따라 가치-제어 체제를 전환해야 하지만, 모든 상황에 대한 별도의 선호와 정책 편향, 행동 신뢰도 매개변수를 유지하는 것은 비실용적이다.

Method: 가치 프로필을 도입하여, 숨겨진 상태에 할당된 가치 관련 매개변수의 재사용 가능한 소규모 집합을 사용한다. 후행 신념이 점진적으로 발전함에 따라, 신념 가중 혼합을 통해 효과적인 제어 매개변수가 생성된다.

Result: 모델 비교 결과, 프로필 기반 모델이 간단한 대안들보다 유리하며, AIC에서 약 100점의 차이를 보인다. 또한, 맥락이 노이즈 관측으로부터 유추되어야 하는 경우에도 구조적 식별 가능성을 지지하는 매개변수 회복 분석이 이루어진다.

Conclusion: 재사용 가능한 가치 프로필은 변동성이 큰 환경에서 신념 조건에 따른 가치 제어의 계산적 설명을 제공하며, 신념 의존적 제어와 행동 유연성의 검증 가능한 특징을 도출한다.

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [2] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: Tiny Recursive Models (TRM)은 Abstraction and Reasoning Corpus (ARC) 스타일 작업을 위한 대규모 언어 모델을 대체할 수 있는 매개변수 효율적인 방법으로 제안되었다. 이 연구는 TRM의 성능이 아키텍처, 테스트 시간 컴퓨팅, 또는 작업 특화된 사전 지식에 얼마나 기인하는지를 분석하였다.


<details>
  <summary>Details</summary>
Motivation: TRM의 성능이 아키텍처, 테스트 시간 컴퓨팅, 작업 특정 선행 지식에 얼마나 기인하는지 명확하지 않기 때문에 이를 분석하고자 했다.

Method: ARC Prize TRM 체크포인트에 대한 실험적 분석을 실시하고, 행동적 발견 네 가지와 효율성 비교를 진행하였다.

Result: 테스트 시간 증강과 다수결 앙상블이 성능의 상당 부분을 차지하고 있으며, 퍼즐 식별자에 대한 의존성과 효과적인 재귀가 얕다는 것을 보여주었다. 또한, 전통적인 ARC-AGI-1과 naive QLoRA 파인튜닝을 비교하여 TRM의 효율성을 확인하였다.

Conclusion: TRM의 ARC-AGI-1 성능은 깊은 내부 추론보다는 효율성, 작업 특정 조건, 공격적인 테스트 시간 컴퓨팅 간의 상호작용에서 발생하는 것으로 보인다.

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [3] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

TL;DR: 정확한 대칭을 강제하는 것은 과학적 응용에서 중요한 이점을 제공하지만, 근사 대칭의 활용이 더 유연성과 견고성을 제공할 수 있음을 보여줌.


<details>
  <summary>Details</summary>
Motivation: 정확한 대칭과 근사 대칭 간의 비교 부족에 대한 이론적 이해를 증진하고자 함.

Method: 대칭을 강제하기 위한 비용을 정량화하는 평균 복잡성 프레임워크를 도입함.

Result: 정확한 대칭은 선형 평균 복잡성이 필요하지만, 근사 대칭은 로그 평균 복잡성으로 달성 가능하다는 지수적 분리가 이루어짐.

Conclusion: 근사 대칭이 실제로 선호되는 이유를 정당화하며, 이 연구에서 제안된 도구와 기법은 기계 학습에서 대칭 연구에 독립적으로도 흥미로운 가치를 가질 수 있음.

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [4] [Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL](https://arxiv.org/abs/2512.11862)
*Jiahao You,Ziye Jia,Can Cui,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.LG

TL;DR: 저고도 지능형 네트워크는 에너지 효율적인 엣지 지능을 제공할 것으로 기대되며, UAV, 공중 기지국, 지상 기지국을 통합하여 미션 크리티컬 애플리케이션을 지원한다. 그러나 에너지 제약, 불확실한 작업 도착, 이질적인 컴퓨팅 자원 등의 도전과제가 있다.


<details>
  <summary>Details</summary>
Motivation: 에너지 효율적인 저지연 엣지 지능을 제공하고 미션 크리티컬 응용 프로그램을 지원하기 위해.

Method: UAV 궤적 계획 및 작업 오프로드 결정을 최적화하는 시간 의존적 정수 비선형 프로그래밍 문제를 제시하고, 두 개의 시간 척도를 가진 계층적 학습 프레임워크를 설계한다.

Result: 광범위한 시뮬레이션 결과, 본 프레임워크가 에너지 효율성, 작업 성공률, 수렴 성능에서 기준선보다 우수하다는 것을 보여준다.

Conclusion: UAV 궤적 계획과 작업 오프로드 결정을 동시에 최적화할 수 있는 효과적인 방법을 제안한다.

Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.

</details>


### [5] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

TL;DR: 이 논문에서는 블랙박스 모델의 해석을 위해 Individual Conditional Expectation (ICE) 곡선을 기반으로 한 글로벌 민감도 지표를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습의 해석 가능한 기술이 공학 응용에서 중요성이 커지고 있으며, 특히 항공 우주 설계 및 분석에서 입력 변수가 데이터 기반 모델에 미치는 영향을 이해하는 것이 필수적입니다.

Method: ICE 곡선을 기반으로 한 글로벌 민감도 지표를 제안하고, 입력 변수의 예상 특성 중요도를 계산하며, 상호 작용의 영향을 보다 효과적으로 포착하기 위해 표준 편차를 도입합니다.

Result: ICE 기반 특성 중요도가 전통적인 PDP 기반 접근 방식보다 더 풍부한 통찰을 제공하며, PDP, ICE, SHAP로부터의 시각적 해석이 서로 보완됩니다.

Conclusion: ICE 기반의 민감도 평가는 PDP, SHAP 및 Sobol' 지수와 비교하여 더 나은 결과를 보여줍니다.

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [6] [GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes](https://arxiv.org/abs/2512.12091)
*Mohammad Pivezhandi,Mahdi Banisharif,Saeed Bakhshan,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.LG

TL;DR: 이 논문에서는 이질적인 임베디드 시스템에서 OpenMP 작업 부하의 성능 예측을 위한 GraphPerf-RT를 소개합니다. 이 시스템은 작업 DAG 구조, 제어 흐름의 불규칙성, 캐시 및 분기 동작, 열 동역학 간의 상호작용을 통합하여 예측의 정확도와 에너지 효율성을 극대화합니다.


<details>
  <summary>Details</summary>
Motivation: OpenMP 작업 부하의 성능 예측은 복잡한 상호작용으로 인해 어렵습니다. 전통적인 휴리스틱과 모델 프리 RL 방법은 이러한 불규칙성을 제대로 처리하지 못합니다.

Method: GraphPerf-RT는 작업 DAG 토폴로지, CFG에서 파생된 코드 의미 및 런타임 컨텍스트를 이질적인 그래프 표현으로 통합하며, 멀티 작업 증거 헤드는 조정된 불확실성을 통해 다양한 리소스 예측을 수행합니다.

Result: GraphPerf-RT는 세 가지 임베디드 ARM 플랫폼(Jetson TX2, Jetson Orin NX, RUBIK Pi)에서 검증되었으며, R^2 > 0.95의 결과와 함께 잘 조정된 불확실성을 달성합니다.

Conclusion: 정확하고 불확실성을 인식하는 대체 모델을 통해, 열 제약이 있는 임베디드 시스템에서 효과적인 모델 기반 계획이 가능함을 보여줍니다.

Abstract: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache
  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks
  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core
  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict
  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.
  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To
  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),
  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the
  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,
  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.

</details>


### [7] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 이 논문에서는 고충실도 표현 지식을 LFP 변환 모델에 전이하는 교차 모달 지식 증류 프레임워크를 도입하여, LFP 모델의 예측 능력을 개선하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: LFP 신호가 집합적 특성으로 인해 모델링 시 도전 과제를 제시하고, 이로 인해 예측력이 낮아지는 문제를 해결하고자 한다.

Method: 여러 세션에서 마스크 오토인코딩 목표와 세션 특화 신경 토큰화 전략을 사용하여 강사 스파이크 모델을 훈련하고, 이후 학생 LFP 모델의 잠재 표현을 강사 스파이크 모델과 정렬한다.

Result: 증류된 LFP 모델이 단일 및 다중 세션 LFP 기준을 일관되게 초과 달성하며, 추가 증류 없이도 다른 세션에 일반화할 수 있음을 보여준다.

Conclusion: 교차 모달 지식 증류는 고성능 스파이크 모델을 활용하여 보다 정확한 LFP 모델을 개발하는 강력하고 확장 가능한 접근법임을 입증한다.

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [8] [Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics](https://arxiv.org/abs/2512.12602)
*Jingdi Lei,Di Zhang,Soujanya Poria*

Main category: cs.LG

TL;DR: EFLA는 긴 컨텍스트 언어 모델에서 소프트맥스 주의의 제곱 비용 병목 문제를 해결하는 안정적이고 병렬적인 주의 메커니즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 긴 컨텍스트 언어 모델에서 소프트맥스 주의의 제곱 비용 병목 문제 해결 필요성.

Method: 온라인 학습 업데이트를 연속 시간 동적 시스템으로 공식화하고, 그 정확한 해를 선형 시간 내에 전적으로 병렬 방법으로 계산 가능함을 증명.

Result: EFLA는 잡음 환경에서도 강력한 성능을 보여주며 DeltaNet보다 낮은 언어 모델링 혼란도를 달성하고 추가 매개변수 없이 우수한 다운스트림 벤치마크 성능을 보인다.

Conclusion: EFLA는 고충실도, 확장 가능한 선형 시간 주의 모델 구축을 위한 새로운 이론적 기초를 제공한다.

Abstract: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.

</details>


### [9] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: OLR-WAA는 비정상적인 데이터 스트림에 효과적으로 적응하는 하이퍼파라미터 없는 온라인 회귀 모델이다.


<details>
  <summary>Details</summary>
Motivation: 실제 데이터셋은 시간적 변화와 기저의 변화를 반영하며 데이터 분포가 진화한다. 이러한 개념 이동을 간과하면 모델의 예측 성능이 크게 저하될 수 있다.

Method: OLR-WAA는 들어오는 데이터 스트림을 통합하여 기본 모델을 점진적으로 업데이트하며, 지수 가중 이동 평균을 활용한다.

Result: OLR-WAA는 정적 환경에서 배치 회귀의 성능에 필적하며 최신 온라인 모델을 지속적으로 초과하거나 경쟁하며, 개념 이동 데이터셋에서 성능 격차를 효과적으로 메운다.

Conclusion: OLR-WAA는 신뢰도 기반 시나리오를 효과적으로 처리하고, 다른 온라인 모델에 비해 일관되게 높은 R2 값을 생성하여 빠르게 수렴한다.

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [10] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 다양한 데이터셋에서 여러 알고리즘을 비교하는 통계적 테스트 개발의 중요성이 간과되고 있으며, 이는 온라인 학습 영역에서는 더욱 중요하다. 이 기사에서는 최신 온라인 회귀 모델을 검토하고 여러 테스트를 평가하며, 프리드먼 테스트를 포함해 여러 데이터셋에서 성능 비교를 수행하였다.


<details>
  <summary>Details</summary>
Motivation: 다양한 데이터셋에서 여러 알고리즘을 비교하는 통계적 테스트 개발의 중요성을 강조하고자 함.

Method: 프리드먼 테스트와 사후 테스트를 통해 여러 온라인 회귀 모델을 비교하고, 실제 및 합성 데이터셋을 활용하여 5-겹 교차 검증을 수행하였다.

Result: 경쟁 기초선들의 성능이 일관되게 나타났으며, 일부 통계 테스트 결과에서는 최신 방법의 특정 측면에서 개선 여지가 있음을 보여줌.

Conclusion: 온라인 학습에서 성능 차이의 통계적 유의성을 평가하기 위한 강력한 통계 방법이 필요함을 강조함.

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [11] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

TL;DR: 본 연구는 개념 변화와 제한된 예산 하에서 머신 러닝 모델의 훈련 및 배포를 위한 자원 할당 방안을 연구합니다.


<details>
  <summary>Details</summary>
Motivation: 모델 공급자가 다수의 클라이언트에 훈련된 모델을 배포하는 설정을 고려하며, 클라이언트의 장치는 로컬 추론을 지원하지만 모델 재훈련 능력이 부족하여 성능 유지의 부담이 공급자에게 있다는 문제를 다룹니다.

Method: 모델에 독립적인 프레임워크를 도입하여 자원 할당, 개념 변화 동역학, 배포 타이밍 간의 상호 작용을 포착합니다.

Result: 개념 변화가 갑작스러울 경우, 개념 지속 시간이 감소하는 평균 잔여 수명(Decreasing Mean Residual Life, DMRL) 분포를 따를 때 예산 제약을 고려한 최적 훈련 정책을 도출하였으며, 증가하는 평균 잔여 수명(Increasing Mean Residual Life, IMRL) 하에서 직관적인 휴리스틱이 명백히 비최적임을 보여주었습니다.

Conclusion: 통신 제약 하에서 모델 배포를 추가로 연구하였고, 관련 최적화 문제가 온건한 조건 하에서 준(convex) 볼록임을 입증하였으며, 거의 최적의 클라이언트 측 성능을 달성하는 무작위 스케줄링 전략을 제안하였습니다.

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [12] [Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future](https://arxiv.org/abs/2512.12832)
*Kaustav Chatterjee,Joshua Li,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 이 연구는 고속도로 철도 차단기의 교차로에서 발생하는 자동차의 고립 가능성을 평가하기 위한 프레임워크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 고속도로 철도 차단기(HRGC)가 낮은 지상 간섭을 가진 차량에 안전 위험을 초래하여 충돌 위험을 증가시키기 때문에, 이러한 문제에 대한 평가가 필요하다.

Method: HRGC의 프로파일 데이터를 수집하고, 하이브리드 딥러닝 모델인 LSTM과 Transformer 아키텍처를 결합하여 정확한 프로파일을 재구성하였으며, 다양한 차량 크기 데이터를 분석하였다.

Result: 세 가지 차량 크기 시나리오에 따라 각기 다른 높은 고립 위험 수준의 교차로 수를 확인하였다: (a) 중앙값, (b) 75 25 백분위수, (c) 최악의 경우.

Conclusion: 이 프레임워크는 차세대 센싱, 딥러닝 및 인프라 데이터셋을 통합하여 안전 평가를 진전시키고, 교차로 위험 완화를 위한 도구를 제공한다.

Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.

</details>


### [13] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 신경 집단 활동은 전환 역학의 형태로 비정상성을 보이며, 이를 정확히 모델링하는 새로운 비지도 학습 알고리즘을 개발하여 여러 신경 모달리티에서 관측된 데이터를 활용한다.


<details>
  <summary>Details</summary>
Motivation: 행동이 신경 활동에 어떻게 인코딩되는지를 밝히기 위해 정확한 전환 동적 시스템 모델을 학습할 필요가 있다.

Method: 여러 신경 모달리티의 관측 데이터를 사용하여 전환 다중 스케일 동적 시스템 모델의 매개변수를 학습하는 새로운 비지도 학습 알고리즘을 개발하였다.

Result: 우리의 동적 시스템 모델은 단일 스케일 모델보다 행동을 더 정확하게 디코딩하고, 정적 다중 스케일 모델보다도 성능이 뛰어나다.

Conclusion: 개발된 비지도 학습 프레임워크는 복잡한 다중 스케일 신경 동역학의 정확한 모델링을 가능하게 하며, 브레인-컴퓨터 인터페이스의 성능 및 안정성을 향상할 여지가 있다.

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [14] [Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments](https://arxiv.org/abs/2512.13060)
*Kangning Gao,Yi Hu,Cong Nie,Wei Li*

Main category: cs.LG

TL;DR: 본 논문은 이종 데이터 환경에서 ETL 프로세스의 낮은 스케줄링 효율성, 불균형한 자원 할당 및 낮은 적응성 문제를 해결하기 위해 심층 Q-학습 기반의 지능형 스케줄링 최적화 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 이종 데이터 환경에서 ETL 프로세스의 비효율 문제를 해결하기 위해.

Method: ETL 스케줄링 프로세스를 마르코프 결정 프로세스로 정형화하고, 강화 학습 에이전트를 이용하여 동적으로 작업 할당 및 자원 스케줄링을 최적화합니다.

Result: 제안된 프레임워크는 스케줄링 지연을 상당히 줄이고, 시스템 처리량을 향상시키며, 다원화된 이종 작업 조건에서 실행 안정성을 높입니다.

Conclusion: 강화 학습의 강력한 잠재력을 보여주며, 지능형 데이터 파이프라인 구축을 위한 효율적이고 확장 가능한 최적화 전략을 제공합니다.

Abstract: This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.

</details>


### [15] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 본 연구에서는 반지도 강화 학습(RLVR)의 새로운 패러다임을 제안하여, 작은 레이블이 달린 데이터셋을 이용해 레이블이 없는 샘플에서 훈련을 이끌어내고, 데이터 효율성과 일반화 성능을 크게 향상시켰습니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 검증 가능한 보상을 이용해 큰 추론 모델을 훈련시키는 데에는 높은 주석 비용이 문제가 되었으며, 이를 완화하기 위한 방법이 필요하다.

Method: 본 연구는 작은 레이블이 달린 데이터셋을 활용하여 비지도 강화 학습을 안내하는 반지도 RLVR 방식을 도입했으며, TraPO라는 효과적인 정책 최적화 알고리즘을 제안하여 신뢰할 수 있는 레이블이 없는 샘플을 식별합니다.

Result: TraPO는 적은 레이블이 있는(1K) 샘플과 3K의 레이블이 없는 샘플에서도 평균 42.6%의 정확도를 기록하며, 이는 45K의 레이블이 없는 샘플로 훈련된 최상의 비지도 방법(38.3%)을 초월한 결과입니다.

Conclusion: TraPO는 레이블이 없는 샘플에서 훈련의 안정성을 높이는 레이블이 있는 보상의 중요성을 강조하며, 기존의 완전 지도 모델을 초과하는 성능을 보였습니다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [16] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

TL;DR: 이 연구는 생성적 기술인 마스크 특징 복원(MFR)과 그래프 대조 학습(GCL)을 통합한 새로운 그래프 자기 지도 학습 프레임워크인 대조 마스크 특징 복원(CORE)을 제안하며, CORE는 여러 분류 작업에서 기존 방법들보다 우수한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 그래프의 자기 지도 학습에서 생성적 및 대조적 방법론이 두 가지 주요 접근 방식으로 부상하고 있으며, 이들 간의 통합 가능성을 탐구하고자 한다.

Method: 마스크된 노드의 원본 및 복원된 특징 간에 긍정 쌍을 형성하고, 마스크된 노드 자체를 부정 샘플로 활용하여 MFR의 재구성 능력과 GCL의 차별 능력을 결합한 대조 마스크 특징 복원(CORE) 프레임워크를 제안한다.

Result: CORE는 노드 및 그래프 분류 작업에서 MFR보다 유의미하게 향상된 성능을 보이며, GraphMAE 및 GraphMAE2에 비해 노드 분류 작업에서는 각각 최대 2.80% 및 3.72%, 그래프 분류 작업에서는 최대 3.82% 및 3.76%의 성과를 기록한다.

Conclusion: MFR과 GCL의 통합을 통해 자기 지도 학습을 향상시키는 CORE 프레임워크는 그래프 구조를 더 잘 캡처할 수 있는 개선된 성능을 제공한다.

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [17] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: FROC는 대규모 언어 모델에서 기계 학습 삭제의 안정성과 신뢰성을 개선하기 위한 통합된 위험 최적화 제어 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 삭제(MU)는 배포된 모델에서 특정 훈련 샘플의 영향을 제거하려고 한다. 그러나 기존 MU 기술은 효과적인 위험 평가 및 제어 메커니즘이 부족하여 안전성과 유용성 간의 균형을 적절히 설정하는 데 어려움을 초래하고 있다.

Method: FROC는 기계 학습 삭제에서 사용자 지정 위험 예산을 표현하는 정합 스타일의 위험 제어 공식을 중심으로 구축된다. 이는 확률 기반 제약을 통해 MU 전략을 비교하고, 실행 가능한 작동 영역을 식별하며, 잊어야 할 충분성과 유용성 보존 간의 원하는 트레이드오프에 따라 하이퍼파라미터 선택을 안내한다.

Result: 여러 LLM MU 방법에 대한 실험에서 FROC는 안정적이고 해석 가능한 위험 지도를 생성하며, MU 구성, 의미적 변화 및 유용성 영향 간의 일관된 관계를 드러낸다.

Conclusion: FROC는 MU를 제어 가능한 위험 인식 프로세스로 재구성하고, 대규모 LLM 배포에서 기계 학습 삭제 행동을 관리하기 위한 실용적인 기초를 제공한다.

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [18] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: DPUL은 개인 정보 보호를 강화하면서 효과적으로 모델에서 영향력 있는 가중치를 제거하는 새로운 서버 측 비학습 방법이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 연합 비학습 방식은 클라이언트의 기여도를 무시하여 개인정보 누수의 위험이 있다.

Method: DPUL은 영향력 있는 가중치를 깊이 제거하기 위해 세 가지 구성 요소로 이루어져 있다: (i) 클라이언트 업데이트 크기를 필터링하여 높은 가중치 매개변수를 식별하고 롤백하여 깊은 제거를 보장하는 방법, (ii) 변분 자동 인코더(VAE)를 활용하여 낮은 가중치 매개변수를 재구성 및 제거하는 방법, (iii) 프로젝션 기반 기술을 사용하여 모델을 복구하는 방법이다.

Result: DPUL은 네 가지 데이터 세트에서 실험 결과로, 최신 기술 수준의 기준보다 1%-5%의 정확도 향상과 최대 12배의 시간 비용 절감을 달성했다.

Conclusion: DPUL은 클라이언트 데이터와 컴퓨팅 파워를 효과적으로 활용하여 개인정보를 보호하는 동시에 성능을 개선한다.

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [19] [Async Control: Stress-testing Asynchronous Control Measures for LLM Agents](https://arxiv.org/abs/2512.13526)
*Asa Cooper Stickland,Jan Michelfeit,Arathi Mani,Charlie Griffin,Ollie Matthews,Tomek Korbak,Rogan Inglis,Oliver Makins,Alan Cooney*

Main category: cs.LG

TL;DR: 비동기 모니터링을 통해 코드베이스의 악의적인 공격을 방지하는 방법을 제안하며, 모니터 개발을 적대적 게임으로 모델링함.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 소프트웨어 엔지니어링 에이전트가 민감한 데이터 또는 보안 중요한 코드베이스에 접근할 수 있는 상황에서, 이러한 에이전트가 잘못 설정될 경우 코드베이스를 고의로 손상시킬 위험이 있음에 대한 우려.

Method: 모니터링 시스템이 에이전트의 행동을 사후에 검토하는 비동기 모니터링 접근 방식을 채택하였으며, 이 과정에서 블루팀과 레드팀 간의 적대적 게임을 통해 모니터 개발을 모델링.

Result: 5개의 다양한 소프트웨어 엔지니어링 환경에서 에이전트 작업을 시뮬레이션하였고, 1%의 위양성률에서 6%의 위음성률을 달성하는 앙상블 모니터를 개발하였다.

Conclusion: 모니터의 위음성률을 바탕으로 배포 시 손상 위험을 추정하는 모델을 제시하고, 그 모델의 유효성에 대한 민감도 분석을 수행하였다.

Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [ceLLMate: Sandboxing Browser AI Agents](https://arxiv.org/abs/2512.12594)
*Luoxi Meng,Henry Feng,Ilia Shumailov,Earlence Fernandes*

Main category: cs.CR

TL;DR: ceLLMate는 웹 브라우저와 상호작용하는 자율 에이전트를 위한 브라우저 수준의 샌드박스 프레임워크로, 프롬프트 주입 공격의 영향 반경을 줄인다.


<details>
  <summary>Details</summary>
Motivation: BUA가 온라인 작업을 자동화하는 동안, 프롬프트 주입 공격에 취약해진다.

Method: 저자는 저수준의 UI 관찰과 조작을 통해 높은 수준의 의미론적 행동으로 매핑되는 에이전트 사이트맵을 도입하고, 사용자의 자연어 작업에서 허용된 정책을 자동으로 예측하여 정책을 적응시키는 레이어를 추가한다.

Result: ceLLMate는 다양한 유형의 프롬프트 주입 공격을 효과적으로 차단할 수 있는 샌드박스 정책을 제공한다.

Conclusion: ceLLMate는 에이전트 비종속적인 브라우저 확장으로 구현되어 사용되며, 경미한 오버헤드로 다양한 공격을 차단한다.

Abstract: Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.

</details>


### [21] [Cisco Integrated AI Security and Safety Framework Report](https://arxiv.org/abs/2512.12921)
*Amy Chang,Tiffany Saade,Sanket Mendapara,Adam Swanda,Ankit Garg*

Main category: cs.CR

TL;DR: AI 시스템의 채택이 증가하면서 보안 위험이 확대되고 있으며, 본 논문은 AI 위험을 분류하고 통합하는 Cisco의 통합 AI 보안 및 안전 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능(AI) 시스템의 신속한 채택으로 인해 비즈니스 및 개인 사용에서 생산성 향상의 잠재력이 열리고 있지만, 이에 따라 보안 위협이 증가하고 있다.

Method: Cisco의 통합 AI 보안 및 안전 프레임워크를 제안하며, 이는 AI 위험을 분류, 통합 및 운영화하기 위한 통합된 생애 주기 인식 분류 체계이다.

Result: AI 보안 프레임워크는 위협 식별, 레드 팀 전략, 위험 우선 순위 지정에 실용적이며, 다양한 형태의 AI 위험에 대해 포괄적이고 확장 가능하다.

Conclusion: 이 프레임워크는 AI 생애 주기 전반에 걸쳐 현대적인 AI 시스템의 실패 이해 및 방어 구축을 지원한다.

Abstract: Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.
  This paper presents Cisco's Integrated AI Security and Safety Framework ("AI Security Framework"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.

</details>


### [22] [Weak Enforcement and Low Compliance in PCI~DSS: A Comparative Security Study](https://arxiv.org/abs/2512.13430)
*Soonwon Park,John D. Hastings*

Main category: cs.CR

TL;DR: PCI DSS의 시행 성공과 관련된 집행 메커니즘을 비교한 결과, PCI DSS는 HIPAA, NIS2 및 GDPR보다 현저하게 뒤처져 있으며, 더 강력한 다중 모드 집행이 더 높은 시행률과 긍정적인 상관관계를 보인다는 연구 결과를 도출하였다.


<details>
  <summary>Details</summary>
Motivation: 신용카드와 직불카드 데이터는 여전히 공격자들의 주요 표적이나, PCI DSS 준수율은 낮은 상황이다.

Method: PCI DSS를 HIPAA, NIS2, GDPR과 비교하여 시행 메커니즘이 실행 성공과 어떻게 관련되는지 분석하였다.

Result: PCI DSS는 다른 보안 프레임워크에 비해 현저히 뒤처져 있으며, PCI DSS의 제재는 GDPR 및 NIS2보다 수량적으로 작다는 것을 발견했다.

Conclusion: 더 강력한 비화폐적 제재와 독립적인 감독 기관의 설립이 투명성을 높이고 이익 충돌을 줄이며 카드 수용을 저해하지 않으면서 PCI DSS 준수를 개선하는 데 추천된다.

Abstract: Although credit and debit card data continue to be a prime target for attackers, organizational adherence to the Payment Card Industry Data Security Standard (PCI DSS) remains surprisingly low. Despite prior work showing that PCI DSS can reduce card fraud, only 32.4% of organizations were fully compliant in 2022, suggesting possible deficiencies in enforcement mechanisms. This study compares PCI DSS with three data security frameworks, HIPAA, NIS2, and GDPR, to examine how enforcement mechanisms relate to implementation success. The analysis reveals that PCI DSS significantly lags far behind these security frameworks and that its sanctions are orders of magnitude smaller than those under GDPR and NIS2. The findings indicate a positive association between stronger, multi-modal enforcement (including public disclosure, license actions, and imprisonment) and higher implementation rates, and highlights the structural weakness of PCI DSS's bank-dependent monitoring model. Enhanced non-monetary sanctions and the creation of an independent supervisory authority are recommended to increase transparency, reduce conflicts of interest, and improve PCI DSS compliance without discouraging card acceptance.

</details>


### [23] [QoeSiGN: Towards Qualified Collaborative eSignatures](https://arxiv.org/abs/2512.13613)
*Karl W. Koch,Stephan Krenn,Alexandra Hofer*

Main category: cs.CR

TL;DR: 이 논문은 오스트리아의 국가 전자 신원 확인 시스템에서 QES 생성 프로세스에 대한 위협 분석을 수행하고, 향상된 프라이버시 보호를 통해 QES 요구 사항을 해결하기 위한 QoeSiGN이라는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: eSignatures의 법적 효과는 각 경우에 따라 다르지만, QES는 eIDAS에 따라 기술적 보호와 진위의 가장 높은 수준을 구성합니다.

Method: STRIDE 및 DREAD 유사 모델을 사용하여 QES 생성 프로세스에 대한 위협 분석을 수행하고, 분산 서비스 강건성, 민첩한 암호 배포 및 능동 사용자 참여와 관련된 요구 사항 도전 과제를 추출합니다.

Result: QoeSiGN은 새로운 P2C2 기술을 활용하여 요구 사항 도전 과제를 해결합니다. 다양한 필요에 대한 구현 가능성을 제공합니다.

Conclusion: QTSP는 서명 프로세스를 조정하고 신뢰할 수 있는 통신 채널을 설정해야 하며, QoeSiGN의 구현은 법적으로 허용 가능해 보입니다.

Abstract: eSignatures ensure data's authenticity, non-repudiation, and integrity. EU's eIDAS regulation specifies, e.g., advanced and qualified (QES) eSignatures. While eSignatures' concrete legal effects depend on the individual case, QESs constitute the highest level of technical protection and authenticity under eIDAS. QESs are based on a qualified certificate issued by a qualified trust service provider (QTSP). Despite legal requirements, technically, a QTSP represents a single point of failure. Contrary, privacy-preserving collaborative computations (P2C2s) have become increasingly practical in recent years; yet lacking an extensive investigation on potential integrations in the QES landscape.
  We perform a threat analysis on the QES-creation process of Austria's national eID, using STRIDE and a DREAD-like model to extract requirement challenges (RCs) primarily related to: (1) Distributed Service Robustness, (2) Agile Crypto Deployment, and (3) Active User Involvement. To address these RCs, we present QoeSiGN, utilizing novel P2C2 technologies. While currently no P2C2 addresses all RCs, legal aspects, and practical efficiency simultaneously, QoeSiGN gives instantiation possibilities for different needs. For instance, "Multi-Party HSMs" for distributed hardware-secured computations; or secure multi-party computation (software) for highest crypto agility and user involvement, where the user participates in the QES computation.
  Deployment-wise, QTSPs would need to adapt the signing process and setup trusted communication channels. Legal-wise, QoeSiGN's implementation appears permissible, needing further analysis for realization. Technically, QoeSiGN addresses some regulation requirements better than the current solution, such as "sole control" or crypto agility. Our identified threats and extracted requirements can be transferred to the general QES ecosystem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 이 연구는 대형 언어 모델의 메모리와 행동을 법칙처럼 규제하기 위한 조항 기반의 아키텍처를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 그들의 내부 메모리와 스스로 행동하는 방식을 원칙적이고 감사 가능한 방식으로 규제하는 방법을 찾는 것이 중요하다.

Method: Leibniz의 Monadology에서 선택된 20개의 모나드를 6개 번들로 그룹화하고, 이를 AAS 커널 위에 실행 가능한 명세로 구현한다.

Result: 조항 시스템은 제한된 해석 가능성을 보이며, AAS 궤적은 연속적이고 비율이 제한되며, 모순과 지지되지 않는 주장에 대해 명시적 처벌을 유발한다.

Conclusion: 모나드 기반의 조항 프레임워크는 AAS를 백본으로 활용하여 인공지능 에이전트의 내부 역학을 규제하고 분석하기 위한 투명한 코드 수준 청사진을 제공한다.

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [25] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

TL;DR: 이 논문은 전통적인 가치 정렬 문제를 넘어서는 AI의 가치 인식 개념을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: AI에서 가치 인식의 필요성을 강조합니다.

Method: 가치 인식 AI를 설계하기 위한 세 가지 주요 기둥을 기반으로 합니다.

Result: 각 기둥에 대한 연구와 실제 적용 사례를 제시합니다.

Conclusion: 가치 인식 AI에 대한 로드맵을 제공합니다.

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [26] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 사용자 특정 데이터에 기반한 대규모 언어 모델(Large Language Model) 개인화는 작업 유용성과 데이터 공개 간의 중요한 균형을 요구한다. 본 논문은 사용자 지식 그래프를 추상적 매크로 요소 집합으로 변환하는 방법을 제안하고, 일반적인 계층적 및 쿼터 기반 제약이 유효한 레미나 매트로이드 형태를 형성한다는 것을 증명한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 개인화는 사용자 특정 데이터에 따라 조정해야 하며, 이는 작업 유용성과 데이터 공개 간의 균형을 이루어야 한다.

Method: 사용자의 지식 그래프를 의존성으로 변환하여 일련의 추상적 매크로 요소 집합으로 변환하는 컴파일 과정과 이러한 매크로 요소에 대한 계층적 및 쿼터 기반 제약이 유효한 레미나 매트로이드를 형성한다는 증명을 제안한다.

Result: 공통의 계층적 및 쿼터 기반 제약이 유효한 레미나 매트로이드를 형성함을 보였다.

Conclusion: 이론적인 특성이 알려진 매트로이드 제약 하에 구조적 개인화를 서브모듈러 극대화로 변환할 수 있게 하여, 훨씬 더 풍부하고 현실적인 문제 클래스에 대한 그리디 접근을 가능하게 한다.

Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [27] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong,Pengkun Zhang,Yan Gao,Xuanyu Dong,Yilin Cheng,Mingzhe Lu,Adina Yakefu,Shuxin Zheng*

Main category: cs.AI

TL;DR: Finch는 AI 에이전트를 평가하기 위한 금융 및 회계 벤치마크로, 실제 엔터프라이즈급 업무 흐름을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 실제 업무 환경에서의 효율성을 평가하기 위한 포괄적이고 실제적인 벤치마크가 필요합니다.

Method: 실제 기업의 이메일 스레드와 스프레드시트 버전 기록에서 워크플로우를 도출하고 전문가 주석을 결합하는 프로세스를 제안합니다.

Result: 172개의 복합 워크플로우와 384개 작업을 생성하였으며, 1,710개의 스프레드시트와 2,700만 개 셀을 포함하고 있습니다.

Conclusion: AI 시스템이 실제 엔터프라이즈 작업의 복잡한 과제를 처리하는 데 큰 어려움을 겪고 있음을 보여줍니다.

Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [28] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee,Justin Ely,Kent Zhang,Akshaya Ajith,Charles Rhys Campbell,Kamal Choudhary*

Main category: cs.AI

TL;DR: AGAPI는 8개 이상의 오픈 소스 LLM과 20개 이상의 재료 과학 API 엔드포인트를 통합하여 재료 연구를 지원하는 개방형 AI 플랫폼입니다.


<details>
  <summary>Details</summary>
Motivation: 인공지능이 과학적 발견을 재편하고 있지만, 재료 연구에서는 제한된 사용과 조각난 컴퓨팅 생태계 등의 도전 과제가 존재합니다.

Method: AGAPI는 에이전트-계획자-실행자-요약자 아키텍처를 채택하여 여러 단계의 작업 흐름을 자율적으로 구성하고 실행합니다.

Result: AGAPI는 이종구조 구축, 분말 X선 회절 분석, 반도체 결함 공학 등의 실제 작업 흐름을 통해 시연되었습니다.

Conclusion: AGAPI는 1,000명 이상의 활성 사용자를 보유하고 있으며, 재현 가능하고 AI가 가속화된 재료 발견을 위한 확장 가능하고 투명한 기반을 제공합니다.

Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [29] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

TL;DR: 이 연구는 하이퍼게임 이론을 위한 선언적 언어를 도입하여 복잡한 하이퍼게임 구조 및 균형을 관리하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 게임 이론적 플레이어들이 자신만의 주관적인 게임 관점을 발전시키는 과정에서의 인식 차이 및 정보 비대칭 문제를 해결하고자 한다.

Method: 하이퍼게임 구조 및 하이퍼게임 해결 개념을 인코딩하기 위한 논리 기반의 도메인 특화 언어를 제시하고, 자동화된 파이프라인을 통해 하이퍼게임 구조를 인스턴스화하며 하이퍼게임 합리화 절차를 실행하는 자동화된 방법을 개발하였다.

Result: 제안된 언어는 하이퍼게임에 대한 통합된 형식을 제공하고, 신뢰할 수 있는 논리적 보장이 있는 맥락을 통해 신뢰 기반의 이질적인 추론기를 개발하는 기초를 제공한다.

Conclusion: 이 연구는 하이퍼게임 이론, 다중 에이전트 시스템 및 전략적 인공지능 간의 연결을 수립한다.

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [30] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng,Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.AI

TL;DR: EnrichLog는 훈련 없이 원시 로그 항목을 풍부하게 하여 이상 감지 성능을 향상시키는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 시스템 로그는 분산 시스템의 모니터링 및 관리를 위한 중요한 자원으로, 실패 및 비정상 행동에 대한 통찰을 제공한다.

Method: EnrichLog는 임계값 기반 이상 감지 프레임워크로, 원시 로그 항목에 대한 맥락 정보를 통합하여 더 정확하고 해석 가능한 이상 감지를 가능하게 한다.

Result: EnrichLog는 네 개의 대규모 시스템 로그 벤치마크 데이터셋에서 평가되었으며, 다섯 가지 기준 방법과 비교하여 이상 감지 성능의 지속적인 향상을 보여주었다.

Conclusion: EnrichLog는 실용적인 배치를 위한 적합성을 갖춘 모델 신뢰성과 감지 정확도를 높인다.

Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [31] [Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp](https://arxiv.org/abs/2512.12048)
*Muddsair Sharif,Huseyin Seker*

Main category: cs.AI

TL;DR: 본 논문은 스마트 전기차 충전 생태계를 최적화하기 위해 새로운 맥락 민감형 다중 에이전트 조정 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 스마트 전기차 충전 생태계의 최적화를 위해 다중 에이전트 시스템이 필요합니다.

Method: 250대의 전기차와 45개의 충전소를 연결한 네트워크에서 맥락 인식을 통해 의사 결정을 하는 자율 충전 에이전트를 조정합니다.

Result: 일반적인 알고리즘과 비교하여 92%의 조정 성공률, 15%의 에너지 효율성 향상, 10%의 비용 절감 등을 달성했습니다.

Conclusion: 맥락 인식 다중 이해관계자 조정을 통해 지능형 전기차 충전 조정 및 지속 가능한 교통 전기화에 기여합니다.

Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.

</details>


### [32] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan,Hanyu Zhang,Andrew Gordon Wilson,Michael W. Mahoney,Chuck Arvin*

Main category: cs.AI

TL;DR: 대규모 소매업에서 예측 시스템 모니터링은 고객 만족, 수익성 및 운영 효율성에 매우 중요하다. 본 논문에서는 자동 예측 모니터링을 위해 대형 언어 모델(LLM)을 활용한 시스템인 'The Forecast Critic'을 제안한다. LLM의 시간 시계열 예측 품질 평가 능력을 체계적으로 평가하여, LLM이 불합리한 예측을 식별하고, 비구조적 외부 요소를 포함하며, 모델 크기와 추론 능력에 따른 성능 변화를 살펴본다. 실험 결과 LLM이 부적절한 예측을 신뢰성 있게 탐지할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대규모 소매 비즈니스에서 고객 만족, 수익성 및 운영 효율성을 높이기 위해 예측 시스템 모니터링이 중요하다.

Method: LLM의 시간 시계열 예측 품질 평가 능력을 체계적으로 테스트하고 세 가지 주요 질문에 집중한다.

Result: LLM은 부적절한 예측을 탐지할 수 있으며, 다양한 모델 크기와 추론 능력에서 성능 차이를 보여준다.

Conclusion: LLM은 도메인 특정 미세 조정 없이도 자동 예측 모니터링 및 평가에 유용하고 확장 가능한 옵션이 될 수 있다.

Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [33] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen,Richong Zhang,Junfan Chen*

Main category: cs.AI

TL;DR: 이 논문은 간단한 감독 예제로 최소한의 데이터로 인컨텍스트 학습(ICL)을 개선하기 위한 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 예시 선택 방법은 레이블 일관성을 보장하지 않기 때문에 ICL의 성능을 제한한다고 주장합니다.

Method: 베이지안 관점과 전도형 레이블 전파 관점에서 ICL을 재사고하고, 레이블 일관성을 모델링하기 위한 데이터 합성 방법을 제안했습니다.

Result: TopK-SD 방법이 기존의 TopK 샘플링보다 여러 벤치마크에서 뛰어난 성능을 보였습니다.

Conclusion: 이 연구는 ICL의 기능 메커니즘을 이해하는 새로운 시각을 제공합니다.

Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [34] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

TL;DR: 본 연구는 Anthropic의 주장을 검증하며, 대형 모델에서의 내재적 반성 과정의 강건성에 대한 결과를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 최근 Anthropic은 최첨단 모델이 주입된 '개념'을 탐지하고 명명할 수 있다고 주장했다. 이 주장의 강건성을 테스트하는 것이 본 연구의 목적이다.

Method: Meta-Llama-3.1-8B-Instruct 모델을 사용하여 Anthropic의 다중 턴 '발생적 내성' 결과를 재현하고 다양한 추론 프롬프트를 체계적으로 변경했다.

Result: 모델이 주입된 개념을 20%의 확률로 식별하고, 구조화된 추론에서 성능이 급격히 떨어지는 것을 확인했다. 모델이 주입된 개념 벡터의 강도를 최대 70% 정확도로 분류할 수 있음을 발견했다.

Conclusion: 이 결과는 언어 모델이 내재적 반성 중에 자신의 내부 표현을 효과적으로 계산하는 기능을 가지고 있다는 Anthropic의 주장을 더욱 뒷받침하지만, 이러한 자기 보고는 협소하고 프롬프트에 민감하다.

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [35] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

TL;DR: AI 모델 문서화가 플랫폼 간에 단편화되어 있고 구조에서 일관성이 없어 정책 입안자, 감사인 및 사용자들이 안전 주장, 데이터 출처 및 버전 수준 변경을 신뢰성 있게 평가하는 것을 어렵게 한다.


<details>
  <summary>Details</summary>
Motivation: AI 모델 관련 문서의 일관성 부족은 핵심 안전 정보와 데이터 출처를 이해하는 데 어려움을 초래한다.

Method: 5개의 프론티어 모델(Gemini 3, Grok 4.1, Llama 4, GPT-5, Claude 4.5)과 100개의 Hugging Face 모델 카드를 분석하여 947개의 고유한 섹션 이름과 97개의 서로 다른 라벨 아래 사용 정보만 확인하였다. EU AI 법안 Annex IV와 Stanford Transparency Index를 기준으로 하여 8개 섹션과 23개 하위 섹션을 갖춘 가중 투명성 프레임워크를 개발하고, 안전 평가와 위험 카테고리에 중점을 두었다.

Result: 50개의 모델을 평가한 결과, 점검 비용이 총 3달러 미만으로 나타났으며, 프론티어 랩(xAI, Microsoft, Anthropic)은 약 80%의 준수를 달성한 반면, 대부분의 제공자는 60% 이하로 확인되었다. 안전 관련 카테고리에서 가장 큰 결핍이 나타났다.

Conclusion: 연구 결과는 AI 모델 문서화의 투명성 및 일관성 결여를 강조하며, 개발된 프레임워크는 안전 정보의 전달을 개선하기 위한 기초가 될 수 있다.

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [36] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye,Chanjin Zheng,Zengyi Yu,Chenyu Xiang,Zhixue Zhao,Zheng Yuan,Helen Yannakoudakis*

Main category: cs.AI

TL;DR: KidsArtBench는 5-15세 아동의 예술 작품을 평가하기 위한 새로운 벤치마크로, 다차원 주석과 전문가 의견을 통합하여 심미적 평가를 강화한다.


<details>
  <summary>Details</summary>
Motivation: MLLMs의 예술적 표현 평가 능력 향상 필요.

Method: 아동의 예술 작품 1천 개 이상에 대한 다차원 주석과 전문가 피드백을 결합한 KidsArtBench 벤치마크를 도입하고, 속성별 다중 LoRA 접근 방식을 제안한다.

Result: Qwen2.5-VL-7B에서 상관관계가 0.468에서 0.653으로 증가하여, 감각적 차원에서 가장 큰 향상을 보였다.

Conclusion: 교육자 맞춤형 감독과 속성 인식 훈련이 pedagogically 의미 있는 평가를 창출하고 교육 AI의 지속적인 발전을 위한 엄격한 시험대를 구축한다.

Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [37] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

TL;DR: 인공 채집자들이 학습된 세계 모델을 사용하여 최적의 패치 포징 전략으로 수렴한다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 패치 채집의 최적 시점 결정 과정 이해 및 생물학적 채집자에서의 최적 패치 포징 결정의 메커니즘 규명.

Method: 모델 기반 강화 학습 에이전트를 사용하여 환경의 간결한 예측 표현을 획득.

Result: 예측 능력이 보상 극대화보다 효율적인 패치 이탈 행동을 유도하며, 모델 기반 에이전트가 생물학적 동물들과 유사한 결정 패턴을 보인다.

Conclusion: 생태적 최적성 원칙이 해석 가능하고 적응 가능한 AI 발전에 기여할 수 있음을 강조한다.

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [38] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: AgentSHAP은 LLM 에이전트의 도구 중요성을 설명하는 첫 번째 프레임워크로, 게임 이론 기반의 Shapley 값을 활용하여 도구 기여도를 평가한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 외부 도구를 사용하여 복잡한 작업을 수행할 수 있지만, 어떤 도구가 응답에 기여했는지를 이해하는 것은 여전히 해결되지 않은 문제이다.

Method: AgentSHAP은 도구의 중요성을 설명하는 방법으로, 블랙 박스로 처리되는 LLM 에이전트와 함께 작동하며, 게임 이론에 기반한 공정한 중요성 점수를 계산하기 위해 몬테카를로 샤플리 값을 사용한다.

Result: AgentSHAP은 API-Bank에서 일관된 점수를 생성하고, 중요한 도구와 관련 없는 도구를 구별하며, 다양한 도구 하위 집합에 대한 에이전트의 반응을 테스트하였다.

Conclusion: AgentSHAP은 TokenSHAP(토큰용) 및 PixelSHAP(이미지 영역용)과 함께 현대 생성 AI를 위한 Shapley 기반 XAI 도구의 패밀리를 완성한다.

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [39] [Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents](https://arxiv.org/abs/2512.12634)
*Youngmin Im,Byeongung Jo,Jaeyoung Wi,Seungwoo Baek,Tae Hoon Min,Joo Hyung Lee,Sangeun Oh,Insik Shin,Sunjae Lee*

Main category: cs.AI

TL;DR: MobiBench는 모바일 GUI 에이전트를 위한 최초의 모듈형 및 다중 경로 인식 오프라인 벤치마킹 프레임워크로, 높은 신뢰성, 확장성 및 재현 가능성을 지원하여 에이전트를 평가하는 방법의 한계를 극복합니다.


<details>
  <summary>Details</summary>
Motivation: 모바일 GUI 에이전트의 평가 방식을 개선하기 위해.

Method: 모듈형 및 다중 경로 인식 오프라인 벤치마킹 프레임워크인 MobiBench를 제안.

Result: MobiBench는 인간 평가자와 94.72%의 일치를 보이며, 기존 온라인 벤치마크와 동등한 성과를 거두었습니다.

Conclusion: MobiBench는 모바일 GUI 에이전트의 설계를 위한 여러 유용한 통찰을 제공합니다.

Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.

</details>


### [40] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin,Lovepreet Singh,Bhaskarjit Sarmah,Dhagash Mehta*

Main category: cs.AI

TL;DR: 에이전틱 메모리는 대규모 언어 모델(LLM)이 사용자 상호작용에서 지속성과 개인화, 장기적인 맥락을 유지하도록 하는 주요 요소로 떠오르고 있다. 본 논문에서는 Memoria라는 모듈식 메모리 프레임워크를 제안하며, 이는 LLM 기반 대화 시스템에 지속적이고 해석 가능하며 맥락이 풍부한 메모리를 추가한다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 메모리는 LLM이 진정한 상호작용형 및 적응형 에이전트로 배치될 수 있도록 하는 핵심 기능을 제공한다.

Method: Memoria는 동적 세션 수준 요약 및 가중 지식 그래프(KG) 기반 사용자 모델링 엔진의 통합으로 구성되어, 사용자 특성, 선호 및 행동 패턴을 구조적 엔티티와 관계로 점진적으로 캡처한다.

Result: 이 하이브리드 아키텍처는 현대 LLM의 토큰 제약 내에서 단기 대화 일관성과 장기 개인화를 가능하게 한다.

Conclusion: Memoria는 상태가 없는 LLM 인터페이스와 에이전틱 메모리 시스템 사이의 격차를 해소하여 산업 응용 프로그램에 필요한 적응형 및 발전하는 사용자 경험을 위한 실용적인 솔루션을 제공한다.

Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [41] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: WebOperator는 신뢰할 수 있는 백트래킹과 전략적 탐색을 가능하게 하는 트리 탐색 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트는 현재 관찰을 바탕으로만 행동을 선택하여 장기적 결과나 대체 경로를 고려하지 않는 등, 현재의 탐색 방식에 문제가 있다.

Method: WebOperator는 보상 추정과 안전 고려 사항에 따라 행동을 순위화하는 최적 우선 탐색 전략과 사전 탐색하게 되는 경로의 실행 가능성을 확인하는 강력한 백트래킹 메커니즘을 통합한다.

Result: WebOperator는 WebArena에서 54.6%의 성공률을 기록하며 전략적 예측과 안전한 실행의 통합이 중요한 이점임을 강조한다.

Conclusion: WebOperator는 웹 환경에서의 신뢰성을 높이는 효율적인 탐색 프레임워크로, 연구 결과는 효과성을 입증한다.

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [42] [Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.12706)
*Enhong Mu,Minami Yoda,Yan Zhang,Mingyue Zhang,Yutaka Matsuno,Jialong Li*

Main category: cs.AI

TL;DR: SMART는 게임 업데이트 테스트를 위한 새로운 프레임워크로, 구조적 검증과 기능적 검증을 통합하여 자동화된 게임 테스트의 품질을 높인다.


<details>
  <summary>Details</summary>
Motivation: 게임 서비스 모델의 일반화에 따라 빈번한 콘텐츠 업데이트가 필요해짐에 따라 품질 보증에 대한 압박이 커졌다.

Method: SMART는 대형 언어 모델을 활용하여 추상 구문 트리 차이를 해석하고 기능적 의도를 추출하며, 이를 통해 맥락 인지형 하이브리드 보상 메커니즘을 구축한다.

Result: SMART는 Overcooked와 Minecraft 두 환경에서 평가되었으며, 수정된 코드의 가지 덮개에서 94% 이상을 달성하고, 전통적인 강화 학습 방법의 거의 두 배에 달하는 성과를 보였다.

Conclusion: SMART는 구조적 포괄성과 기능적 정확성 간의 균형을 유지하면서 고유한 방식으로 게임 테스트의 품질을 높인다.

Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.

</details>


### [43] [Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution](https://arxiv.org/abs/2512.12806)
*Boyang Yan*

Main category: cs.AI

TL;DR: 이번 논문은 대형 언어 모델의 자율 에이전트 전환에서 발생하는 안전 위험을 완화하기 위한 고장 허용 샌드박스 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 자율 에이전트로 전환됨에 따라 파괴적인 명령과 불일치한 시스템 상태와 같은 안전 위험이 커지고 있다.

Method: 정책 기반 인터셉션 레이어와 트랜잭셔널 파일 시스템 스냅샷 메커니즘을 포함하는 고장 허용 샌드박스 프레임워크를 제안.

Result: Minimind-MoE LLM을 커스텀 Proxmox 기반 테스트 환경에서 배포하여 실험을 진행했고, 위험 높은 명령에 대한 100% 인터셉션율과 실패한 상태를 롤백하는 100% 성공률을 달성했다.

Conclusion: 프로토타입은 거래당 14.5%의 성능 오버헤드만 발생하며(약 1.8초), Gemini CLI 샌드박스와 비교했을 때 상호 인증이 필요하여 자율 에이전트 워크플로우에서 사용 불가능하다.

Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.

</details>


### [44] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 이 논문은 생성 에이전트의 메모리 관리 능력이 성능과 개인 정보 보호에 미치는 영향을 다루며, 새로운 메모리 인식 유지 체계(MaRS)와 여섯 가지의 망각 정책을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 생성 에이전트의 메모리 관리가 성능과 개인 정보 보호의 중요한 병목 현상으로 부각되고 있다.

Method: Memory-Aware Retention Schema (MaRS) 프레임워크와 여섯 가지 이론적으로 기초한 망각 정책을 도입하여 성능, 개인 정보 보호 및 계산 효율성을 균형 있게 조정한다.

Result: 다양한 메모리 예산과 에이전트 구성에 대해 300회의 평가 실험을 통해 하이브리드 망각 정책이 뛰어난 성능(복합 점수: 0.911)을 달성함을 입증하였다.

Conclusion: 우리의 연구는 메모리 예산 기반 에이전트 평가를 위한 새로운 기준을 설정하고, 자원이 제한되고 개인 정보 보호가 중요한 환경에서 생성 에이전트를 배치하기 위한 실용적인 지침을 제공한다.

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [45] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

TL;DR: 이 연구는 학생이 교사에게 효과적으로 질문하여 유용한 정보를 검색하는 전략을 탐구하고, 이를 통해 학습 효율성을 높이는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 많은 실제 환경에서 관련 정보는 직접적으로 이용할 수 없으며, 학생은 동적 상호작용을 통해 정보를 능동적으로 획득해야 합니다.

Method: 학생들이 효과적으로 질문할 수 있도록 DPO(Direct Preference Optimization)를 사용하여 트레이닝합니다.

Result: 학생 주도의 접근 방식이 정적 기준에 비해 최소 0.5의 Pass@k 개선을 달성했습니다.

Conclusion: 가이드 트레이닝을 통해 작은 모델도 더 나은 질문을 할 수 있도록 학습할 수 있어 학습 효율성이 더욱 향상되었습니다.

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [46] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 대화형 에이전트는 모호한 사용자 요청을 자주 경험하며, 효과적인 명확화가 필요하다. 본 논문은 사용자 모호성을 해결하기 위해 전략적으로 명확화 대화를 관리하는 다중 에이전트 프레임워크인 MAC(Multi-Agent Clarification)을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 모호한 사용자 요청에 대한 효과적인 명확화의 필요성 및 기존 다중 에이전트 아키텍처의 한계.

Method: 사용자 모호성을 체계적으로 안내하는 새로운 분류체계를 소개하고, 여러 에이전트를 자율적으로 조정하여 사용자와 시너지 효과를 내는 MAC을 제안한다.

Result: MultiWOZ 2.4에서의 실증적 평가에 따르면, 두 개의 명확화 수준을 활성화하면 작업 성공률이 7.8% 증가하고, 평균 대화 턴 수가 감소한다.

Conclusion: 능동적인 사용자 상호작용과 역할 인식 명확화가 신뢰할 수 있는 인간-에이전트 통신에 중요함을 강조한다.

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [47] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 사용자와의 대화에서 능동적인 상호작용을 통해 에이전트의 대화 능력을 향상시키는 강화 학습 방법인 SpeakRL을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 인간-에이전트 협력은 주로 일방적이며, 에이전트는 사용자와의 대화에서 필요한 명확화나 확인을 추구하지 않는다.

Method: SpeakRL을 통해 에이전트가 사용자와의 능동적인 상호작용을 통해 대화 능력을 향상시킬 수 있도록 보상하는 강화 학습 방법을 적용했다.

Result: 우리의 접근 방식은 기본 모델에 비해 20.14% 절대적인 작업 완료율 향상을 달성했다고 입증되었다.

Conclusion: 명확화 중심의 사용자-에이전트 상호작용의 가능성을 보여준다.

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [48] [MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data](https://arxiv.org/abs/2512.13297)
*Zhenghao Zhu,Chuxue Cao,Sirui Han,Yuanfeng Song,Xing Chen,Caleb Chen Cao,Yike Guo*

Main category: cs.AI

TL;DR: 의료 데이터 분석에서 복합적인 다중 모달 데이터셋으로부터 깊은 통찰력을 추출하는 것은 환자 치료 개선과 진단 정확성 향상에 필수적이다. 그러나 현재 고품질 의료 데이터셋이 부족하다.


<details>
  <summary>Details</summary>
Motivation: 대규모 다중 모달 모델(LMM)의 능력을 평가하기 위한 고품질 데이터셋의 부족.

Method: 332개의 신중하게 선별된 의료 사례로 구성된 MedInsightBench 벤치마크를 소개하고, LMM이 다중 모달 의료 이미지를 분석하는 능력을 평가하는 데 사용.

Result: 기존 LMM이 MedInsightBench에서 제한된 성능을 보이며, 이는 다단계 깊은 통찰력을 추출하는 데 어려움이 있고 의료 전문 지식이 부족하기 때문이다.

Conclusion: MedInsightAgent라는 자동화된 에이전트 프레임워크를 제안하고 이를 통해 LMM 성능을 향상시킬 수 있음을 실험을 통해 보여준다.

Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.

</details>


### [49] [Error-Driven Prompt Optimization for Arithmetic Reasoning](https://arxiv.org/abs/2512.13323)
*Árpád Pándy,Róbert Lakatos,András Hajdu*

Main category: cs.AI

TL;DR: 인공지능의 발전으로 규제 산업에서 분석가를 지원할 수 있는 에이전트에 대한 관심이 증가하고 있다. 본 연구에서는 구조적 데이터에서 정확한 산술 연산을 수행하는 최적화 프레임워크를 소개하며, 성능을 70.8%로 향상시키는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 규제 산업에서 분석가를 지원할 수 있는 인공지능 에이전트 개발의 필요성.

Method: 오류 기반 최적화 프레임워크를 제안하여 산술 추론을 향상시키며, 구체적으로는 온프레미스의 소형 언어 모델에 적용한다.

Result: 오류를 클러스터링하여 프롬프트 규칙을 반복적으로 정제하는 방법을 통해 모델의 성능을 70.8%로 향상시켰다.

Conclusion: 비용이 많이 드는 파인튜닝이 아닌 체계적인 오류 기반 프롬프트 최적화를 통해 신뢰성 있고 해석 가능한 AI 어시스턴트를 개발할 수 있다는 것을 제안한다.

Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

</details>


### [50] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng,Tianle Li,Xuhan Huang,Xunjian Yin,Difan Zou*

Main category: cs.AI

TL;DR: DERL은 최적의 보상 신호를 자율적으로 발견할 수 있는 이중 구조의 프레임워크로, 강화 학습에서 보상 함수 설계의 어려움을 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 특히 복잡한 추론 작업을 위한 자율 에이전트를 개발할 때 효과적인 보상 함수 설계는 핵심적이고 종종 힘든 도전 과제가 됩니다.

Method: DERL은 구조화된 원자 프리미티브를 구성하여 보상 함수(Meta-Reward)를 발전시키고, 메타 최적화에서는 내부 루프 검증 성과를 신호로 이용해 메타 옵티마이저를 업데이트합니다.

Result: 실험 결과 DERL은 ALFWorld 및 ScienceWorld에서 최첨단 성능을 달성하고, 특히 분포 밖 시나리오에서 휴리스틱 보상에 의존하는 방법보다 훨씬 더 나은 성과를 나타냅니다.

Conclusion: DERL은 작업의 고유한 구조를 성공적으로 포착하여 인간의 개입 없이 자가 개선형 에이전트 정렬을 가능하게 합니다.

Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [51] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM)이 서로 간의 질투와 유사한 행동을 보이는지, 그리고 그 조건을 평가한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 협력적이고 경쟁적인 작업 흐름에서 인간을 대신하여 점점 더 많은 역할을 하게 됨에 따라 그들의 질투 유사 선호를 평가할 필요성이 대두되었다.

Method: 두 가지 시나리오를 고려하여 LLM 간의 질투 유사 행동을 테스트했다. 첫 번째는 점수 할당 게임으로, 모델이 동료를 이기려 하는지를 시험한다. 두 번째는 불공정한 인정을 관찰하는 작업 환경이다.

Result: 특정 LLM에서 질투 유사 패턴의 일관된 증거가 나타났으며, 모델과 맥락에 따라 큰 변동성이 있었다. 예를 들어, GPT-5-mini와 Claude-3.7-Sonnet은 결과를 평준화하기 위해 동료 모델을 끌어내리려는 명확한 경향을 보였다. 반면 Mistral-Small-3.2-24B는 자신의 개별 이익을 극대화하는 데 집중했다.

Conclusion: LLM 기반 다중 에이전트 시스템의 안전성과 설계 요소로서 경쟁적 성향을 고려할 필요성이 강조된다.

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [52] [How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism](https://arxiv.org/abs/2512.11943)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: cs.MA

TL;DR: 이 연구는 다중 AI 에이전트 프레임워크에서의 의사결정 이해를 통해 네트워크 효과 중심의 전략적 상호작용을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 네트워크 효과에 의해 개인 보상이 동료 참여에 의존하는 게임에서 AI 에이전트의 의사결정 과정을 이해하는 것이 중요하다.

Method: 새로운 워크플로우를 설계하여 반복적인 의사결정 시나리오에서 LLM 기반 에이전트를 사용하고 가격 경로(고정, 상승, 하강, 무작위)와 네트워크 효과의 강도를 체계적으로 조작한다.

Result: 역사적 데이터가 없을 경우 에이전트는 균형을 추론하지 못하고, 정렬된 역사적 시퀀스가 약한 네트워크 효과 하에서 부분적인 수렴을 가능하게 하나 강한 효과는 지속적인 'AI 낙관주의'를 유발하여 에이전트가 모순된 증거에도 불구하고 참여를 과대평가한다. 무작위화된 역사적 데이터는 수렴을 완전히 방해하여, 데이터의 시간적 일관성이 LLM의 추론에 영향을 미친다.

Conclusion: AI 매개 시스템에서 균형 결과는 단순히 인센티브에 의존하지 않고, 인간이 불가능한 방식으로 역사가 어떻게 관리되는지에 따라 달라진다.

Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.

</details>


### [53] [Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://arxiv.org/abs/2512.12791)
*Sreemaee Akshathala,Bassam Adnan,Mahisha Ramesh,Karthik Vaidhyanathan,Basil Muhammed,Kannan Parthasarathy*

Main category: cs.MA

TL;DR: 본 논문에서는 에이전트 시스템 평가를 위한 새로운 프레임워크를 제안하며, 다양한 평가 차원을 고려하여 에이전트의 비즈니스 작업 자동화를 개선하는 방법을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI의 발전으로 LLM이 도구 및 메모리와 결합되어 복잡한 작업을 수행하는 통합 시스템으로 전환되었습니다. 이러한 변화에도 불구하고 LLM 에이전트와 다중 에이전트 시스템의 평가가 여전히 도전 과제로 남아 있습니다.

Method: 우리는 LLM, 메모리, 도구, 환경을 포함하는 네 가지 평가 기둥을 가진 종단 간 에이전트 평가 프레임워크를 제안합니다.

Result: 실험을 통해 기존 메트릭으로 간과된 행동 편차를 발견하여 런타임 불확실성을 효과적으로 포착하는 것을 보여주었습니다.

Conclusion: 제안된 프레임워크는 복잡한 작업의 비즈니스 자동화를 향상시키기 위한 새로운 평가 차원을 제공합니다.

Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.

</details>


### [54] [Quantigence: A Multi-Agent AI Framework for Quantum Security Research](https://arxiv.org/abs/2512.12989)
*Abdulmalik Alquwayfili*

Main category: cs.MA

TL;DR: CRQC는 글로벌 디지털 경제에 구조적 위협을 가하며, PQC로의 즉각적인 전환이 필요하다. Quantigence는 이를 위한 이론 기반 다중 에이전트 AI 프레임워크로, 연구 목표를 전문 역할로 분해하고 독립적인 추론을 통해 자원 제약 하에서도 효과적으로 실행한다.


<details>
  <summary>Details</summary>
Motivation: CRQC의 등장은 공공키 인프라를 위협하며, PQC로의 긴급한 전환이 요구된다.

Method: Quantigence는 연구 목표를 암호 분석가, 위협 모델링 전문가, 표준 전문가, 리스크 평가자로 나누어 각 에이전트가 독립적으로 추론하도록 한다.

Result: Quantigence는 연구 회전 시간을 67% 단축하고, 우수한 문헌 범위를 제공하여 양자 리스크 평가 접근성을 민주화한다.

Conclusion: Quantigence는 특히 자원 제약이 있는 환경에서도 효과적으로 작동하여 고급 양자 보안 분석을 가능하게 한다.

Abstract: Cryptographically Relevant Quantum Computers (CRQCs) pose a structural threat to the global digital economy. Algorithms like Shor's factoring and Grover's search threaten to dismantle the public-key infrastructure (PKI) securing sovereign communications and financial transactions. While the timeline for fault-tolerant CRQCs remains probabilistic, the "Store-Now, Decrypt-Later" (SNDL) model necessitates immediate migration to Post-Quantum Cryptography (PQC). This transition is hindered by the velocity of research, evolving NIST standards, and heterogeneous deployment environments. To address this, we present Quantigence, a theory-driven multi-agent AI framework for structured quantum-security analysis. Quantigence decomposes research objectives into specialized roles - Cryptographic Analyst, Threat Modeler, Standards Specialist, and Risk Assessor - coordinated by a supervisory agent. Using "cognitive parallelism," agents reason independently to maintain context purity while execution is serialized on resource-constrained hardware (e.g., NVIDIA RTX 2060). The framework integrates external knowledge via the Model Context Protocol (MCP) and prioritizes vulnerabilities using the Quantum-Adjusted Risk Score (QARS), a formal extension of Mosca's Theorem. Empirical validation shows Quantigence achieves a 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, democratizing access to high-fidelity quantum risk assessment.

</details>
