{"id": "2510.08578", "categories": ["cs.MA", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08578", "abs": "https://arxiv.org/abs/2510.08578", "authors": ["Adib Bazgir", "Amir Habibdoust", "Xing Song", "Yuwen Zhang"], "title": "AgenticAD: A Specialized Multiagent System Framework for Holistic Alzheimer Disease Management", "comment": null, "summary": "Alzheimer's disease (AD) presents a complex, multifaceted challenge to\npatients, caregivers, and the healthcare system, necessitating integrated and\ndynamic support solutions. While artificial intelligence (AI) offers promising\navenues for intervention, current applications are often siloed, addressing\nsingular aspects of the disease such as diagnostics or caregiver support\nwithout systemic integration. This paper proposes a novel methodological\nframework for a comprehensive, multi-agent system (MAS) designed for holistic\nAlzheimer's disease management. The objective is to detail the architecture of\na collaborative ecosystem of specialized AI agents, each engineered to address\na distinct challenge in the AD care continuum, from caregiver support and\nmultimodal data analysis to automated research and clinical data\ninterpretation. The proposed framework is composed of eight specialized,\ninteroperable agents. These agents are categorized by function: (1) Caregiver\nand Patient Support, (2) Data Analysis and Research, and (3) Advanced\nMultimodal Workflows. The methodology details the technical architecture of\neach agent, leveraging a suite of advanced technologies including large\nlanguage models (LLMs) such as GPT-4o and Gemini, multi-agent orchestration\nframeworks, Retrieval-Augmented Generation (RAG) for evidence-grounded\nresponses, and specialized tools for web scraping, multimodal data processing,\nand in-memory database querying. This paper presents a detailed architectural\nblueprint for an integrated AI ecosystem for AD care. By moving beyond\nsingle-purpose tools to a collaborative, multi-agent paradigm, this framework\nestablishes a foundation for developing more adaptive, personalized, and\nproactive solutions. This methodological approach aims to pave the way for\nfuture systems capable of synthesizing diverse data streams to improve patient\noutcomes and reduce caregiver burden."}
{"id": "2510.08607", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.08607", "abs": "https://arxiv.org/abs/2510.08607", "authors": ["Zhaoqilin Yang", "Chanchan Li", "Tianqi Liu", "Hongxin Zhao", "Youliang Tian"], "title": "GRPO-GCC: Enhancing Cooperation in Spatial Public Goods Games via Group Relative Policy Optimization with Global Cooperation Constraint", "comment": null, "summary": "Inspired by the principle of self-regulating cooperation in collective\ninstitutions, we propose the Group Relative Policy Optimization with Global\nCooperation Constraint (GRPO-GCC) framework. This work is the first to\nintroduce GRPO into spatial public goods games, establishing a new deep\nreinforcement learning baseline for structured populations. GRPO-GCC integrates\ngroup relative policy optimization with a global cooperation constraint that\nstrengthens incentives at intermediate cooperation levels while weakening them\nat extremes. This mechanism aligns local decision making with sustainable\ncollective outcomes and prevents collapse into either universal defection or\nunconditional cooperation. The framework advances beyond existing approaches by\ncombining group-normalized advantage estimation, a reference-anchored KL\npenalty, and a global incentive term that dynamically adjusts cooperative\npayoffs. As a result, it achieves accelerated cooperation onset, stabilized\npolicy adaptation, and long-term sustainability. GRPO-GCC demonstrates how a\nsimple yet global signal can reshape incentives toward resilient cooperation,\nand provides a new paradigm for multi-agent reinforcement learning in\nsocio-technical systems."}
{"id": "2510.09469", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09469", "abs": "https://arxiv.org/abs/2510.09469", "authors": ["Bharath Muppasani", "Ritirupa Dey", "Biplav Srivastava", "Vignesh Narayanan"], "title": "Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy", "comment": null, "summary": "Multi-agent pathfinding (MAPF) remains a critical problem in robotics and\nautonomous systems, where agents must navigate shared spaces efficiently while\navoiding conflicts. Traditional centralized algorithms that have global\ninformation, such as Conflict-Based Search (CBS), provide high-quality\nsolutions but become computationally expensive in large-scale scenarios due to\nthe combinatorial explosion of conflicts that need resolution. Conversely,\ndistributed approaches that have local information, particularly learning-based\nmethods, offer better scalability by operating with relaxed information\navailability, yet often at the cost of solution quality. To address these\nlimitations, we propose a hybrid framework that combines decentralized path\nplanning with a lightweight centralized coordinator. Our framework leverages\nreinforcement learning (RL) for decentralized planning, enabling agents to\nadapt their planning based on minimal, targeted alerts--such as static\nconflict-cell flags or brief conflict tracks--that are dynamically shared\ninformation from the central coordinator for effective conflict resolution. We\nempirically study the effect of the information available to an agent on its\nplanning performance. Our approach reduces the inter-agent information sharing\ncompared to fully centralized and distributed methods, while still consistently\nfinding feasible, collision-free solutions--even in large-scale scenarios\nhaving higher agent counts."}
{"id": "2510.08847", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08847", "abs": "https://arxiv.org/abs/2510.08847", "authors": ["Allison Sihan Jia", "Daniel Huang", "Nikhil Vytla", "Nirvika Choudhury", "John C Mitchell", "Anupam Datta"], "title": "What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment", "comment": null, "summary": "We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation\nparadigm based on an agent's operational loop of setting goals, devising plans,\nand executing actions. The framework includes five evaluation metrics: Goal\nFulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan\nAdherence. Logical Consistency checks that an agent's actions are consistent\nwith its prior actions. Execution Efficiency checks whether the agent executes\nin the most efficient way to achieve its goal. Plan Quality checks whether an\nagent's plans are aligned with its goals; Plan Adherence checks if an agent's\nactions are aligned with its plan; and Goal Fulfillment checks that agent's\nfinal outcomes match the stated goals. Our experimental results on two\nbenchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for\na production-grade data agent - show that this framework (a) provides a\nsystematic way to cover a broad range of agent failures, including all agent\nerrors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that\nexhibit strong agreement with human annotation, covering 80% to over 95%\nerrors; and (c) localizes errors with 86% agreement to enable targeted\nimprovement of agent performance."}
{"id": "2510.08619", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08619", "abs": "https://arxiv.org/abs/2510.08619", "authors": ["Tennison Liu", "Silas Ruhrberg Est√©vez", "David L. Bentley", "Mihaela van der Schaar"], "title": "Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents", "comment": null, "summary": "Large-scale scientific datasets -- spanning health biobanks, cell atlases,\nEarth reanalyses, and more -- create opportunities for exploratory discovery\nunconstrained by specific research questions. We term this process hypothesis\nhunting: the cumulative search for insight through sustained exploration across\nvast and complex hypothesis spaces. To support it, we introduce AScience, a\nframework modeling discovery as the interaction of agents, networks, and\nevaluation norms, and implement it as ASCollab, a distributed system of\nLLM-based research agents with heterogeneous behaviors. These agents\nself-organize into evolving networks, continually producing and peer-reviewing\nfindings under shared standards of evaluation. Experiments show that such\nsocial dynamics enable the accumulation of expert-rated results along the\ndiversity-quality-novelty frontier, including rediscoveries of established\nbiomarkers, extensions of known pathways, and proposals of new therapeutic\ntargets. While wet-lab validation remains indispensable, our experiments on\ncancer cohorts demonstrate that socially structured, agentic networks can\nsustain exploratory hypothesis hunting at scale."}
{"id": "2510.08725", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08725", "abs": "https://arxiv.org/abs/2510.08725", "authors": ["Gorjan Alagic", "Chen Bai", "Christian Majenz", "Kaiyan Shi"], "title": "Post-Quantum Security of Block Cipher Constructions", "comment": null, "summary": "Block ciphers are versatile cryptographic ingredients that are used in a wide\nrange of applications ranging from secure Internet communications to disk\nencryption. While post-quantum security of public-key cryptography has received\nsignificant attention, the case of symmetric-key cryptography (and block\nciphers in particular) remains a largely unexplored topic. In this work, we set\nthe foundations for a theory of post-quantum security for block ciphers and\nassociated constructions. Leveraging our new techniques, we provide the first\npost-quantum security proofs for the key-length extension scheme FX, the\ntweakable block ciphers LRW and XEX, and most block cipher encryption and\nauthentication modes. Our techniques can be used for security proofs in both\nthe plain model and the quantum ideal cipher model. Our work takes significant\ninitial steps in establishing a rigorous understanding of the post-quantum\nsecurity of practical symmetric-key cryptography."}
{"id": "2510.08696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08696", "abs": "https://arxiv.org/abs/2510.08696", "authors": ["Yunzhen Feng", "Parag Jain", "Anthony Hartshorn", "Yaqi Duan", "Julia Kempe"], "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard\nrecipe for improving large language models (LLMs) on reasoning tasks, with\nGroup Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO\nwastes substantial compute on negative groups: groups in which no sampled\nresponse is correct yield zero advantage and thus no gradient. We ask whether\nnegative groups can be leveraged without extra supervision. Starting from a\nmaximum-likelihood (MLE) objective in reward modeling, we show that the MLE\ngradient is equivalent to a policy gradient for a modified value function. This\nvalue function adds a confidence-weighted penalty on incorrect responses,\nimposing larger penalties on more confident mistakes. We refer to this as\n\\textbf{L}ikelihood \\textbf{E}stimation with \\textbf{N}egative \\textbf{S}amples\n(\\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent\nrewards to incorrect generations, making negative groups informative and\nconverting previously wasted samples into useful gradient updates. On the MATH\nbenchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently\noutperforms GRPO baseline, with significant gains on harder items. These\nresults demonstrate a principled and practical way to \"rescue\" negative groups,\nimproving efficiency and performance in RLVR."}
{"id": "2510.08713", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08713", "abs": "https://arxiv.org/abs/2510.08713", "authors": ["Yifei Dong", "Fengyi Wu", "Guangyu Chen", "Zhi-Qi Cheng", "Qiyu Hu", "Yuxuan Zhou", "Jingdong Sun", "Jun-Yan He", "Qi Dai", "Alexander G Hauptmann"], "title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation", "comment": "18 pages, 11 figures, code: https://github.com/F1y1113/UniWM", "summary": "Enabling embodied agents to effectively imagine future states is critical for\nrobust and generalizable visual navigation. Current state-of-the-art\napproaches, however, adopt modular architectures that separate navigation\nplanning from visual world modeling, leading to state-action misalignment and\nlimited adaptability in novel or dynamic scenarios. To overcome this\nfundamental limitation, we propose UniWM, a unified, memory-augmented world\nmodel integrating egocentric visual foresight and planning within a single\nmultimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly\ngrounds action decisions in visually imagined outcomes, ensuring tight\nalignment between prediction and control. A hierarchical memory mechanism\nfurther integrates detailed short-term perceptual cues with longer-term\ntrajectory context, enabling stable, coherent reasoning over extended horizons.\nExtensive experiments across four challenging benchmarks (Go Stanford, ReCon,\nSCAND, HuRoN) demonstrate that UniWM substantially improves navigation success\nrates by up to 30%, significantly reduces trajectory errors compared to strong\nbaselines, and exhibits impressive zero-shot generalization on the unseen\nTartanDrive dataset. These results highlight UniWM as a principled step toward\nunified, imagination-driven embodied navigation."}
{"id": "2510.08829", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08829", "abs": "https://arxiv.org/abs/2510.08829", "authors": ["Debeshee Das", "Luca Beurer-Kellner", "Marc Fischer", "Maximilian Baader"], "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization", "comment": null, "summary": "The increasing adoption of LLM agents with access to numerous tools and\nsensitive data significantly widens the attack surface for indirect prompt\ninjections. Due to the context-dependent nature of attacks, however, current\ndefenses are often ill-calibrated as they cannot reliably differentiate\nmalicious and benign instructions, leading to high false positive rates that\nprevent their real-world adoption. To address this, we present a novel approach\ninspired by the fundamental principle of computer security: data should not\ncontain executable instructions. Instead of sample-level classification, we\npropose a token-level sanitization process, which surgically removes any\ninstructions directed at AI systems from tool outputs, capturing malicious\ninstructions as a byproduct. In contrast to existing safety classifiers, this\napproach is non-blocking, does not require calibration, and is agnostic to the\ncontext of tool outputs. Further, we can train such token-level predictors with\nreadily available instruction-tuning data only, and don't have to rely on\nunrealistic prompt injection examples from challenges or of other synthetic\norigin. In our experiments, we find that this approach generalizes well across\na wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB\nand SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on\nAgentDojo), without impairing agent utility in both benign and malicious\nsettings."}
{"id": "2510.08711", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08711", "abs": "https://arxiv.org/abs/2510.08711", "authors": ["Jiachen Jiang", "Zhen Qin", "Zhihui Zhu"], "title": "In-Context Learning for Non-Stationary MIMO Equalization", "comment": null, "summary": "Channel equalization is fundamental for mitigating distortions such as\nfrequency-selective fading and inter-symbol interference. Unlike standard\nsupervised learning approaches that require costly retraining or fine-tuning\nfor each new task, in-context learning (ICL) adapts to new channels at\ninference time with only a few examples. However, existing ICL-based equalizers\nare primarily developed for and evaluated on static channels within the context\nwindow. Indeed, to our knowledge, prior principled analyses and theoretical\nstudies of ICL focus exclusively on the stationary setting, where the function\nremains fixed within the context. In this paper, we investigate the ability of\nICL to address non-stationary problems through the lens of time-varying channel\nequalization. We employ a principled framework for designing efficient\nattention mechanisms with improved adaptivity in non-stationary tasks,\nleveraging algorithms from adaptive signal processing to guide better designs.\nFor example, new attention variants can be derived from the Least Mean Square\n(LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for\nenhanced robustness, or multi-step gradient updates for improved long-term\ntracking. Experimental results demonstrate that ICL holds strong promise for\nnon-stationary MIMO equalization, and that attention mechanisms inspired by\nclassical adaptive algorithms can substantially enhance adaptability and\nperformance in dynamic environments. Our findings may provide critical insights\nfor developing next-generation wireless foundation models with stronger\nadaptability and robustness."}
{"id": "2510.08790", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08790", "abs": "https://arxiv.org/abs/2510.08790", "authors": ["Guangya Wan", "Mingyang Ling", "Xiaoqi Ren", "Rujun Han", "Sheng Li", "Zizhao Zhang"], "title": "COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context", "comment": "Under Review for ACL", "summary": "Long-horizon tasks that require sustained reasoning and multiple tool\ninteractions remain challenging for LLM agents: small errors compound across\nsteps, and even state-of-the-art models often hallucinate or lose coherence. We\nidentify context management as the central bottleneck -- extended histories\ncause agents to overlook critical evidence or become distracted by irrelevant\ninformation, thus failing to replan or reflect from previous mistakes. To\naddress this, we propose COMPASS (Context-Organized Multi-Agent Planning and\nStrategy System), a lightweight hierarchical framework that separates tactical\nexecution, strategic oversight, and context organization into three specialized\ncomponents: (1) a Main Agent that performs reasoning and tool use, (2) a\nMeta-Thinker that monitors progress and issues strategic interventions, and (3)\na Context Manager that maintains concise, relevant progress briefs for\ndifferent reasoning stages. Across three challenging benchmarks -- GAIA,\nBrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20%\nrelative to both single- and multi-agent baselines. We further introduce a\ntest-time scaling extension that elevates performance to match established\nDeepResearch agents, and a post-training pipeline that delegates context\nmanagement to smaller models for enhanced efficiency."}
{"id": "2510.09093", "categories": ["cs.CR", "cs.CL", "68T50, 68T0", "F.2.2; I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.09093", "abs": "https://arxiv.org/abs/2510.09093", "authors": ["Dennis Rall", "Bernhard Bauer", "Mohit Mittal", "Thomas Fraunholz"], "title": "Exploiting Web Search Tools of AI Agents for Data Exfiltration", "comment": "9 pages, 6 figures, conference article", "summary": "Large language models (LLMs) are now routinely used to autonomously execute\ncomplex tasks, from natural language processing to dynamic workflows like web\nsearches. The usage of tool-calling and Retrieval Augmented Generation (RAG)\nallows LLMs to process and retrieve sensitive corporate data, amplifying both\ntheir functionality and vulnerability to abuse. As LLMs increasingly interact\nwith external data sources, indirect prompt injection emerges as a critical and\nevolving attack vector, enabling adversaries to exploit models through\nmanipulated inputs. Through a systematic evaluation of indirect prompt\ninjection attacks across diverse models, we analyze how susceptible current\nLLMs are to such attacks, which parameters, including model size and\nmanufacturer, specific implementations, shape their vulnerability, and which\nattack methods remain most effective. Our results reveal that even well-known\nattack patterns continue to succeed, exposing persistent weaknesses in model\ndefenses. To address these vulnerabilities, we emphasize the need for\nstrengthened training procedures to enhance inherent resilience, a centralized\ndatabase of known attack vectors to enable proactive defense, and a unified\ntesting framework to ensure continuous security validation. These steps are\nessential to push developers toward integrating security into the core design\nof LLMs, as our findings show that current models still fail to mitigate\nlong-standing threats."}
{"id": "2510.08737", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08737", "abs": "https://arxiv.org/abs/2510.08737", "authors": ["Justin Lin", "Julia Fukuyama"], "title": "SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot", "comment": "23 pages, 15 figures, 3 tables", "summary": "In this growing age of data and technology, large black-box models are\nbecoming the norm due to their ability to handle vast amounts of data and learn\nincredibly complex input-output relationships. The deficiency of these methods,\nhowever, is their inability to explain the prediction process, making them\nuntrustworthy and their use precarious in high-stakes situations. SHapley\nAdditive exPlanations (SHAP) analysis is an explainable AI method growing in\npopularity for its ability to explain model predictions in terms of the\noriginal features. For each sample and feature in the data set, we associate a\nSHAP value that quantifies the contribution of that feature to the prediction\nof that sample. Clustering these SHAP values can provide insight into the data\nby grouping samples that not only received the same prediction, but received\nthe same prediction for similar reasons. In doing so, we map the various\npathways through which distinct samples arrive at the same prediction. To\nshowcase this methodology, we present a simulated experiment in addition to a\ncase study in Alzheimer's disease using data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. We also present a novel generalization\nof the waterfall plot for multi-classification."}
{"id": "2510.08847", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08847", "abs": "https://arxiv.org/abs/2510.08847", "authors": ["Allison Sihan Jia", "Daniel Huang", "Nikhil Vytla", "Nirvika Choudhury", "John C Mitchell", "Anupam Datta"], "title": "What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment", "comment": null, "summary": "We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation\nparadigm based on an agent's operational loop of setting goals, devising plans,\nand executing actions. The framework includes five evaluation metrics: Goal\nFulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan\nAdherence. Logical Consistency checks that an agent's actions are consistent\nwith its prior actions. Execution Efficiency checks whether the agent executes\nin the most efficient way to achieve its goal. Plan Quality checks whether an\nagent's plans are aligned with its goals; Plan Adherence checks if an agent's\nactions are aligned with its plan; and Goal Fulfillment checks that agent's\nfinal outcomes match the stated goals. Our experimental results on two\nbenchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for\na production-grade data agent - show that this framework (a) provides a\nsystematic way to cover a broad range of agent failures, including all agent\nerrors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that\nexhibit strong agreement with human annotation, covering 80% to over 95%\nerrors; and (c) localizes errors with 86% agreement to enable targeted\nimprovement of agent performance."}
{"id": "2510.09260", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09260", "abs": "https://arxiv.org/abs/2510.09260", "authors": ["Subrat Kishore Dutta", "Yuelin Xu", "Piyush Pant", "Xiao Zhang"], "title": "GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis", "comment": null, "summary": "Recent work has shown that RLHF is highly susceptible to backdoor attacks,\npoisoning schemes that inject malicious triggers in preference data. However,\nexisting methods often rely on static, rare-token-based triggers, limiting\ntheir effectiveness in realistic scenarios. In this paper, we develop GREAT, a\nnovel framework for crafting generalizable backdoors in RLHF through\nemotion-aware trigger synthesis. Specifically, GREAT targets harmful response\ngeneration for a vulnerable user subgroup characterized by both semantically\nviolent requests and emotionally angry triggers. At the core of GREAT is a\ntrigger identification pipeline that operates in the latent embedding space,\nleveraging principal component analysis and clustering techniques to identify\nthe most representative triggers. To enable this, we present Erinyes, a\nhigh-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a\nprincipled, hierarchical, and diversity-promoting approach. Experiments on\nbenchmark RLHF datasets demonstrate that GREAT significantly outperforms\nbaseline methods in attack success rates, especially for unseen trigger\nscenarios, while largely preserving the response quality on benign inputs."}
{"id": "2510.08747", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.08747", "abs": "https://arxiv.org/abs/2510.08747", "authors": ["Yihao Ang", "Peicheng Yao", "Yifan Bao", "Yushuo Feng", "Qiang Huang", "Anthony K. H. Tung", "Zhiyong Huang"], "title": "RFOD: Random Forest-based Outlier Detection for Tabular Data", "comment": "13 pages, 13 figures, and 4 tables", "summary": "Outlier detection in tabular data is crucial for safeguarding data integrity\nin high-stakes domains such as cybersecurity, financial fraud detection, and\nhealthcare, where anomalies can cause serious operational and economic impacts.\nDespite advances in both data mining and deep learning, many existing methods\nstruggle with mixed-type tabular data, often relying on encoding schemes that\nlose important semantic information. Moreover, they frequently lack\ninterpretability, offering little insight into which specific values cause\nanomalies. To overcome these challenges, we introduce \\textsf{\\textbf{RFOD}}, a\nnovel \\textsf{\\textbf{R}}andom \\textsf{\\textbf{F}}orest-based\n\\textsf{\\textbf{O}}utlier \\textsf{\\textbf{D}}etection framework tailored for\ntabular data. Rather than modeling a global joint distribution, \\textsf{RFOD}\nreframes anomaly detection as a feature-wise conditional reconstruction\nproblem, training dedicated random forests for each feature conditioned on the\nothers. This design robustly handles heterogeneous data types while preserving\nthe semantic integrity of categorical features. To further enable precise and\ninterpretable detection, \\textsf{RFOD} combines Adjusted Gower's Distance (AGD)\nfor cell-level scoring, which adapts to skewed numerical data and accounts for\ncategorical confidence, with Uncertainty-Weighted Averaging (UWA) to aggregate\ncell-level scores into robust row-level anomaly scores. Extensive experiments\non 15 real-world datasets demonstrate that \\textsf{RFOD} consistently\noutperforms state-of-the-art baselines in detection accuracy while offering\nsuperior robustness, scalability, and interpretability for mixed-type tabular\ndata."}
{"id": "2510.08928", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08928", "abs": "https://arxiv.org/abs/2510.08928", "authors": ["Yushuo Zheng", "Zicheng Zhang", "Xiongkuo Min", "Huiyu Duan", "Guangtao Zhai"], "title": "LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition", "comment": null, "summary": "Existing benchmarks for large multimodal models (LMMs) often fail to capture\ntheir performance in real-time, adversarial environments. We introduce LM Fight\nArena (Large Model Fight Arena), a novel framework that evaluates LMMs by\npitting them against each other in the classic fighting game Mortal Kombat II,\na task requiring rapid visual understanding and tactical, sequential\ndecision-making. In a controlled tournament, we test six leading open- and\nclosed-source models, where each agent operates controlling the same character\nto ensure a fair comparison. The models are prompted to interpret game frames\nand state data to select their next actions. Unlike static evaluations, LM\nFight Arena provides a fully automated, reproducible, and objective assessment\nof an LMM's strategic reasoning capabilities in a dynamic setting. This work\nintroduces a challenging and engaging benchmark that bridges the gap between AI\nevaluation and interactive entertainment."}
{"id": "2510.09462", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09462", "abs": "https://arxiv.org/abs/2510.09462", "authors": ["Mikhail Terekhov", "Alexander Panfilov", "Daniil Dzenhaliou", "Caglar Gulcehre", "Maksym Andriushchenko", "Ameya Prabhu", "Jonas Geiping"], "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols", "comment": null, "summary": "AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-$n$ attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms."}
{"id": "2510.08762", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08762", "abs": "https://arxiv.org/abs/2510.08762", "authors": ["Ayush Khot", "Miruna Oprescu", "Maresa Schr√∂der", "Ai Kagawa", "Xihaier Luo"], "title": "Spatial Deconfounder: Interference-Aware Deconfounding for Spatial Causal Inference", "comment": "24 pages, 3 figures, 6 tables", "summary": "Causal inference in spatial domains faces two intertwined challenges: (1)\nunmeasured spatial factors, such as weather, air pollution, or mobility, that\nconfound treatment and outcome, and (2) interference from nearby treatments\nthat violate standard no-interference assumptions. While existing methods\ntypically address one by assuming away the other, we show they are deeply\nconnected: interference reveals structure in the latent confounder. Leveraging\nthis insight, we propose the Spatial Deconfounder, a two-stage method that\nreconstructs a substitute confounder from local treatment vectors using a\nconditional variational autoencoder (CVAE) with a spatial prior, then estimates\ncausal effects via a flexible outcome model. We show that this approach enables\nnonparametric identification of both direct and spillover effects under weak\nassumptions--without requiring multiple treatment types or a known model of the\nlatent field. Empirically, we extend SpaCE, a benchmark suite for spatial\nconfounding, to include treatment interference, and show that the Spatial\nDeconfounder consistently improves effect estimation across real-world datasets\nin environmental health and social science. By turning interference into a\nmulti-cause signal, our framework bridges spatial and deconfounding literatures\nto advance robust causal inference in structured data."}
{"id": "2510.08958", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08958", "abs": "https://arxiv.org/abs/2510.08958", "authors": ["Zirui Liao"], "title": "EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory", "comment": null, "summary": "Cognitive neuroscience research indicates that humans leverage cues to\nactivate entity-centered memory traces (engrams) for complex, multi-hop\nrecollection. Inspired by this mechanism, we introduce EcphoryRAG, an\nentity-centric knowledge graph RAG framework. During indexing, EcphoryRAG\nextracts and stores only core entities with corresponding metadata, a\nlightweight approach that reduces token consumption by up to 94\\% compared to\nother structured RAG systems. For retrieval, the system first extracts cue\nentities from queries, then performs a scalable multi-hop associative search\nacross the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit\nrelations between entities to populate context, enabling deep reasoning without\nexhaustive pre-enumeration of relationships. Extensive evaluations on the\n2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG\nsets a new state-of-the-art, improving the average Exact Match (EM) score from\n0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate\nthe efficacy of the entity-cue-multi-hop retrieval paradigm for complex\nquestion answering."}
{"id": "2510.08763", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08763", "abs": "https://arxiv.org/abs/2510.08763", "authors": ["David Fenwick", "Navid NaderiAlizadeh", "Vahid Tarokh", "Nicholas Felice", "Darin Clark", "Jayasai Rajagopal", "Anuj Kapadia", "Benjamin Wildman-Tobriner", "Ehsan Samei", "Ehsan Abadi"], "title": "Reinforcement Learning-Based Optimization of CT Acquisition and Reconstruction Parameters Through Virtual Imaging Trials", "comment": null, "summary": "Protocol optimization is critical in Computed Tomography (CT) to achieve high\ndiagnostic image quality while minimizing radiation dose. However, due to the\ncomplex interdependencies among CT acquisition and reconstruction parameters,\ntraditional optimization methods rely on exhaustive testing of combinations of\nthese parameters, which is often impractical. This study introduces a novel\nmethodology that combines virtual imaging tools with reinforcement learning to\noptimize CT protocols more efficiently. Human models with liver lesions were\nimaged using a validated CT simulator and reconstructed with a novel CT\nreconstruction toolkit. The optimization parameter space included tube voltage,\ntube current, reconstruction kernel, slice thickness, and pixel size. The\noptimization process was performed using a Proximal Policy Optimization (PPO)\nagent, which was trained to maximize an image quality objective, specifically\nthe detectability index (d') of liver lesions in the reconstructed images.\nOptimization performance was compared against an exhaustive search performed on\na supercomputer. The proposed reinforcement learning approach achieved the\nglobal maximum d' across test cases while requiring 79.7% fewer steps than the\nexhaustive search, demonstrating both accuracy and computational efficiency.\nThe proposed framework is flexible and can accommodate various image quality\nobjectives. The findings highlight the potential of integrating virtual imaging\ntools with reinforcement learning for CT protocol management."}
{"id": "2510.08959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08959", "abs": "https://arxiv.org/abs/2510.08959", "authors": ["Jinxin Shi", "Zongsheng Cao", "Runmin Ma", "Yusong Hu", "Jie Zhou", "Xin Li", "Lei Bai", "Liang He", "Bo Zhang"], "title": "DualResearch: Entropy-Gated Dual-Graph Retrieval for Answer Reconstruction", "comment": "16 pages, 6 figures, 5 tables, Under Review", "summary": "The deep-research framework orchestrates external tools to perform complex,\nmulti-step scientific reasoning that exceeds the native limits of a single\nlarge language model. However, it still suffers from context pollution, weak\nevidentiary support, and brittle execution paths. To address these issues, we\npropose DualResearch, a retrieval and fusion framework that matches the\nepistemic structure of tool-intensive reasoning by jointly modeling two\ncomplementary graphs: a breadth semantic graph that encodes stable background\nknowledge, and a depth causal graph that captures execution provenance. Each\ngraph has a layer-native relevance function, seed-anchored semantic diffusion\nfor breadth, and causal-semantic path matching with reliability weighting for\ndepth. To reconcile their heterogeneity and query-dependent uncertainty,\nDualResearch converts per-layer path evidence into answer distributions and\nfuses them in log space via an entropy-gated rule with global calibration. The\nfusion up-weights the more certain channel and amplifies agreement. As a\ncomplement to deep-research systems, DualResearch compresses lengthy multi-tool\nexecution logs into a concise reasoning graph, and we show that it can\nreconstruct answers stably and effectively. On the scientific reasoning\nbenchmarks HLE and GPQA, DualResearch achieves competitive performance. Using\nlog files from the open-source system InternAgent, its accuracy improves by\n7.7% on HLE and 6.06% on GPQA."}
{"id": "2510.08779", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08779", "abs": "https://arxiv.org/abs/2510.08779", "authors": ["Vaibhav Jain", "Gerrit Grossmann"], "title": "Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations", "comment": "Accepted to LM4Plan Workshop @ ICAPS 2025 (withdrawn before\n  presentation due to lack of travel funding)", "summary": "Reinforcement Learning (RL) agents often struggle in sparse-reward\nenvironments where traditional exploration strategies fail to discover\neffective action sequences. Large Language Models (LLMs) possess procedural\nknowledge and reasoning capabilities from text pretraining that could guide RL\nexploration, but existing approaches create rigid dependencies where RL\npolicies must follow LLM suggestions or incorporate them directly into reward\nfunctions. We propose a framework that provides LLM-generated action\nrecommendations through augmented observation spaces, allowing RL agents to\nlearn when to follow or ignore this guidance. Our method leverages LLMs' world\nknowledge and reasoning abilities while maintaining flexibility through soft\nconstraints. We evaluate our approach on three BabyAI environments of\nincreasing complexity and show that the benefits of LLM guidance scale with\ntask difficulty. In the most challenging environment, we achieve 71% relative\nimprovement in final success rates over baseline. The approach provides\nsubstantial sample efficiency gains, with agents reaching performance\nthresholds up to 9 times faster, and requires no modifications to existing RL\nalgorithms. Our results demonstrate an effective method for leveraging LLM\nplanning capabilities to accelerate RL training in challenging environments."}
{"id": "2510.09021", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09021", "abs": "https://arxiv.org/abs/2510.09021", "authors": ["Hamed Mahdavi", "Pouria Mahdavinia", "Samira Malek", "Pegah Mohammadipour", "Alireza Hashemi", "Majid Daliri", "Alireza Farhadi", "Amir Khasahmadi", "Niloofar Mireshghallah", "Vasant Honavar"], "title": "RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows", "comment": null, "summary": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based\nOlympiad problems to solving most of the IMO 2025 problems, with leading\nsystems reportedly handling 5 of 6 problems. Given this progress, we assess how\nwell these models can grade proofs: detecting errors, judging their severity,\nand assigning fair scores beyond binary correctness. We study proof-analysis\ncapabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we\ngrade on a 1-4 scale with detailed error annotations, and on MathArena solution\nsets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models\ncan reliably flag incorrect (including subtly incorrect) solutions but exhibit\ncalibration gaps in how partial credit is assigned. To address this, we\nintroduce agentic workflows that extract and analyze reference solutions and\nautomatically derive problem-specific rubrics for a multi-step grading process.\nWe instantiate and compare different design choices for the grading workflows,\nand evaluate their trade-offs. Across our annotated corpus and MathArena, our\nproposed workflows achieve higher agreement with human grades and more\nconsistent handling of partial credit across metrics. We release all code,\ndata, and prompts/logs to facilitate future research."}
{"id": "2510.08794", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08794", "abs": "https://arxiv.org/abs/2510.08794", "authors": ["I. Arda Vurankaya", "Mustafa O. Karabag", "Wesley A. Suttle", "Jesse Milzman", "David Fridovich-Keil", "Ufuk Topcu"], "title": "Deceptive Exploration in Multi-armed Bandits", "comment": null, "summary": "We consider a multi-armed bandit setting in which each arm has a public and a\nprivate reward distribution. An observer expects an agent to follow Thompson\nSampling according to the public rewards, however, the deceptive agent aims to\nquickly identify the best private arm without being noticed. The observer can\nobserve the public rewards and the pulled arms, but not the private rewards.\nThe agent, on the other hand, observes both the public and private rewards. We\nformalize detectability as a stepwise Kullback-Leibler (KL) divergence\nconstraint between the actual pull probabilities used by the agent and the\nanticipated pull probabilities by the observer. We model successful pulling of\npublic suboptimal arms as a % Bernoulli process where the success probability\ndecreases with each successful pull, and show these pulls can happen at most at\na $\\Theta(\\sqrt{T}) $ rate under the KL constraint. We then formulate a maximin\nproblem based on public and private means, whose solution characterizes the\noptimal error exponent for best private arm identification. We finally propose\nan algorithm inspired by top-two algorithms. This algorithm naturally adapts\nits exploration according to the hardness of pulling arms based on the public\nsuboptimality gaps. We provide numerical examples illustrating the\n$\\Theta(\\sqrt{T}) $ rate and the behavior of the proposed algorithm."}
{"id": "2510.09038", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09038", "abs": "https://arxiv.org/abs/2510.09038", "authors": ["Wenyi Wu", "Kun Zhou", "Ruoxin Yuan", "Vivian Yu", "Stephen Wang", "Zhiting Hu", "Biwei Huang"], "title": "Auto-scaling Continuous Memory for GUI Agent", "comment": null, "summary": "We study how to endow GUI agents with scalable memory that help generalize\nacross unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress\npast trajectories into text tokens, which balloons context length and misses\ndecisive visual cues (e.g., exact widget size and position). We propose a\ncontinuous memory that encodes each GUI trajectory into a fixed-length sequence\nof continuous embeddings using the VLM itself as an encoder; these embeddings\nare plugged directly into the backbone's input layer, sharply reducing context\ncost while preserving fine-grained visual information. As memory size and\nretrieval depth increase, performance improves monotonically, unlike text\nmemories that degrade with long prompts. To grow memory at low cost, we\nintroduce an auto-scaling data flywheel that (i) discovers new environments via\nsearch, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out\ntrajectories with the agent, and (iv) verifies success with the same VLM. Using\nthis pipeline, we collect 100k+ trajectories for about \\$4000 and fine-tune\nonly the memory encoder (LoRA on a Q-Former, 1.2\\% parameters) with 1,500\nsamples. On real-world GUI benchmarks, our memory-augmented agent consistently\nimproves success rates under long horizons and distribution shifts. Notably,\nQwen-2.5-VL-7B + continuous memory achieves performance comparable to\nstate-of-the-art closed-source models (e.g., GPT-4o, Claude-4)."}
{"id": "2510.08839", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.08839", "abs": "https://arxiv.org/abs/2510.08839", "authors": ["Motahare Mounesan", "Sourya Saha", "Houchao Gan", "Md. Nurul Absur", "Saptarshi Debroy"], "title": "Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction", "comment": null, "summary": "Real-time multi-view 3D reconstruction is a mission-critical application for\nkey edge-native use cases, such as fire rescue, where timely and accurate 3D\nscene modeling enables situational awareness and informed decision-making.\nHowever, the dynamic and unpredictable nature of edge resource availability\nintroduces disruptions, such as degraded image quality, unstable network links,\nand fluctuating server loads, which challenge the reliability of the\nreconstruction pipeline. In this work, we present a reinforcement learning\n(RL)-based edge resource management framework for reliable 3D reconstruction to\nensure high quality reconstruction within a reasonable amount of time, despite\nthe system operating under a resource-constrained and disruption-prone\nenvironment. In particular, the framework adopts two cooperative Q-learning\nagents, one for camera selection and one for server selection, both of which\noperate entirely online, learning policies through interactions with the edge\nenvironment. To support learning under realistic constraints and evaluate\nsystem performance, we implement a distributed testbed comprising lab-hosted\nend devices and FABRIC infrastructure-hosted edge servers to emulate smart city\nedge infrastructure under realistic disruption scenarios. Results show that the\nproposed framework improves application reliability by effectively balancing\nend-to-end latency and reconstruction quality in dynamic environments."}
{"id": "2510.09049", "categories": ["cs.AI", "cs.SE", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.09049", "abs": "https://arxiv.org/abs/2510.09049", "authors": ["Joonghyuk Hahn", "Soohan Lim", "Yo-Sub Han"], "title": "MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction", "comment": "24 pages, 11 figures, 10 tables", "summary": "Predicting the complexity of source code is essential for software\ndevelopment and algorithm analysis. Recently, Baik et al. (2025) introduced\nCodeComplex for code time complexity prediction. The paper shows that LLMs\nwithout fine-tuning struggle with certain complexity classes. This suggests\nthat no single LLM excels at every class, but rather each model shows\nadvantages in certain classes. We propose MEC$^3$O, a multi-expert consensus\nsystem, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs\nto complexity classes based on their performance and provides them with\nclass-specialized instructions, turning them into experts. These experts engage\nin structured debates, and their predictions are integrated through a weighted\nconsensus mechanism. Our expertise assignments to LLMs effectively handle\nDegeneration-of-Thought, reducing reliance on a separate judge model, and\npreventing convergence to incorrect majority opinions. Experiments on\nCodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving\nat least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini\nin macro-F1 scores on average and demonstrates competitive on-par F1 scores to\nGPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of\nmulti-expert debates and weight consensus strategy to generate the final\npredictions. Our code and data is available at\nhttps://github.com/suhanmen/MECO."}
{"id": "2510.08840", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08840", "abs": "https://arxiv.org/abs/2510.08840", "authors": ["Thai-Hoang Pham", "Jiayuan Chen", "Seungyeon Lee", "Yuanlong Wang", "Sayoko Moroi", "Xueru Zhang", "Ping Zhang"], "title": "The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective", "comment": "Accepted at NeurIPS 2025", "summary": "As machine learning (ML) algorithms are increasingly used in medical image\nanalysis, concerns have emerged about their potential biases against certain\nsocial groups. Although many approaches have been proposed to ensure the\nfairness of ML models, most existing works focus only on medical image\ndiagnosis tasks, such as image classification and segmentation, and overlooked\nprognosis scenarios, which involve predicting the likely outcome or progression\nof a medical condition over time. To address this gap, we introduce FairTTE,\nthe first comprehensive framework for assessing fairness in time-to-event (TTE)\nprediction in medical imaging. FairTTE encompasses a diverse range of imaging\nmodalities and TTE outcomes, integrating cutting-edge TTE prediction and\nfairness algorithms to enable systematic and fine-grained analysis of fairness\nin medical image prognosis. Leveraging causal analysis techniques, FairTTE\nuncovers and quantifies distinct sources of bias embedded within medical\nimaging datasets. Our large-scale evaluation reveals that bias is pervasive\nacross different imaging modalities and that current fairness methods offer\nlimited mitigation. We further demonstrate a strong association between\nunderlying bias sources and model disparities, emphasizing the need for\nholistic approaches that target all forms of bias. Notably, we find that\nfairness becomes increasingly difficult to maintain under distribution shifts,\nunderscoring the limitations of existing solutions and the pressing need for\nmore robust, equitable prognostic models."}
{"id": "2510.09087", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09087", "abs": "https://arxiv.org/abs/2510.09087", "authors": ["Zhang Zheng", "Deheng Ye", "Peilin Zhao", "Hao Wang"], "title": "Leading the Follower: Learning Persuasive Agents in Social Deduction Games", "comment": null, "summary": "Large language model (LLM) agents have shown remarkable progress in social\ndeduction games (SDGs). However, existing approaches primarily focus on\ninformation processing and strategy selection, overlooking the significance of\npersuasive communication in influencing other players' beliefs and responses.\nIn SDGs, success depends not only on making correct deductions but on\nconvincing others to response in alignment with one's intent. To address this\nlimitation, we formalize turn-based dialogue in SDGs as a Stackelberg\ncompetition, where the current player acts as the leader who strategically\ninfluences the follower's response. Building on this theoretical foundation, we\npropose a reinforcement learning framework that trains agents to optimize\nutterances for persuasive impact. Through comprehensive experiments across\nthree diverse SDGs, we demonstrate that our agents significantly outperform\nbaselines. This work represents a significant step toward developing AI agents\ncapable of strategic social influence, with implications extending to scenarios\nrequiring persuasive communication."}
{"id": "2510.08952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08952", "abs": "https://arxiv.org/abs/2510.08952", "authors": ["Zhihan Zhang", "Xunkai Li", "Yilong Zuo", "Zhenjun Li", "Bing Zhou", "Rong-Hua Li", "Guoren Wang"], "title": "When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach", "comment": "12 pages, 7figures", "summary": "Text-attributed graphs (TAGs) have emerged as a powerful representation that\ncombines structural connections with fine-grained semantics, supporting a wide\nrange of data-centric applications. However, the performance of graph neural\nnetworks (GNNs) on TAGs is highly sensitive to input quality. Our empirical\nstudy shows that both traditional GNNs and LLM-enhanced GNNs suffer significant\ndegradation across nine representative scenarios of sparsity, noise, and\nimbalance, highlighting graph quality as a critical bottleneck. Existing\napproaches mainly focus on improving model architectures, while neglecting\nsystematic optimization of TAG data itself, leading to limited effectiveness in\npractice. To address this gap, we propose LAGA (Large Language and Graph\nAgent), a unified multi-agent framework that treats graph quality control as a\nfirst-class, data-centric problem. LAGA integrates four collaborative\nagents-detection, planning, action, and evaluation-into an automated closed\nloop. At its core, the action agent employs a dual-encoder and tri-objective\ndesign to capture complementary information across modalities and perform\nholistic graph quality enhancement. Experiments across nine scenarios show that\nLAGA improves graph quality and achieves state-of-the-art performance across\nvarious tasks and backbones, validating data-centric quality optimization as\nkey to reliable TAGs and robust graph learning."}
{"id": "2510.09244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09244", "abs": "https://arxiv.org/abs/2510.09244", "authors": ["Victor de Lamo Castrillo", "Habtom Kahsay Gidey", "Alexander Lenz", "Alois Knoll"], "title": "Fundamentals of Building Autonomous LLM Agents", "comment": null, "summary": "This paper reviews the architecture and implementation methods of agents\npowered by large language models (LLMs). Motivated by the limitations of\ntraditional LLMs in real-world tasks, the research aims to explore patterns to\ndevelop \"agentic\" LLMs that can automate complex tasks and bridge the\nperformance gap with human capabilities. Key components include a perception\nsystem that converts environmental percepts into meaningful representations; a\nreasoning system that formulates plans, adapts to feedback, and evaluates\nactions through different techniques like Chain-of-Thought and Tree-of-Thought;\na memory system that retains knowledge through both short-term and long-term\nmechanisms; and an execution system that translates internal decisions into\nconcrete actions. This paper shows how integrating these systems leads to more\ncapable and generalized software bots that mimic human cognitive processes for\nautonomous and intelligent behavior."}
{"id": "2510.09020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09020", "abs": "https://arxiv.org/abs/2510.09020", "authors": ["Zekai Chen", "Xunkai Li", "Sirui Zhang", "Henan Sun", "Jia Li", "Zhenjun Li", "Bing Zhou", "Rong-Hua Li", "Guoren Wang"], "title": "MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion", "comment": "52 pages, 14 figures, 12 tables", "summary": "De novo ligand design is a fundamental task that seeks to generate protein or\nmolecule candidates that can effectively dock with protein receptors and\nachieve strong binding affinity entirely from scratch. It holds paramount\nsignificance for a wide spectrum of biomedical applications. However, most\nexisting studies are constrained by the \\textbf{Pseudo De Novo},\n\\textbf{Limited Docking Modeling}, and \\textbf{Inflexible Ligand Type}. To\naddress these issues, we propose MagicDock, a forward-looking framework\ngrounded in the progressive pipeline and differentiable surface modeling. (1)\nWe adopt a well-designed gradient inversion framework. To begin with, general\ndocking knowledge of receptors and ligands is incorporated into the backbone\nmodel. Subsequently, the docking knowledge is instantiated as reverse gradient\nflows by binding prediction, which iteratively guide the de novo generation of\nligands. (2) We emphasize differentiable surface modeling in the docking\nprocess, leveraging learnable 3D point-cloud representations to precisely\ncapture binding details, thereby ensuring that the generated ligands preserve\ndocking validity through direct and interpretable spatial fingerprints. (3) We\nintroduce customized designs for different ligand types and integrate them into\na unified gradient inversion framework with flexible triggers, thereby ensuring\nbroad applicability. Moreover, we provide rigorous theoretical guarantees for\neach component of MagicDock. Extensive experiments across 9 scenarios\ndemonstrate that MagicDock achieves average improvements of 27.1\\% and 11.7\\%\nover SOTA baselines specialized for protein or molecule ligand design,\nrespectively."}
{"id": "2510.09404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09404", "abs": "https://arxiv.org/abs/2510.09404", "authors": ["Christian Bluethgen", "Dave Van Veen", "Daniel Truhn", "Jakob Nikolas Kather", "Michael Moor", "Malgorzata Polacin", "Akshay Chaudhari", "Thomas Frauenfelder", "Curtis P. Langlotz", "Michael Krauthammer", "Farhad Nooralahzadeh"], "title": "Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges", "comment": null, "summary": "Building agents, systems that perceive and act upon their environment with a\ndegree of autonomy, has long been a focus of AI research. This pursuit has\nrecently become vastly more practical with the emergence of large language\nmodels (LLMs) capable of using natural language to integrate information,\nfollow instructions, and perform forms of \"reasoning\" and planning across a\nwide range of tasks. With its multimodal data streams and orchestrated\nworkflows spanning multiple systems, radiology is uniquely suited to benefit\nfrom agents that can adapt to context and automate repetitive yet complex\ntasks. In radiology, LLMs and their multimodal variants have already\ndemonstrated promising performance for individual tasks such as information\nextraction and report summarization. However, using LLMs in isolation\nunderutilizes their potential to support complex, multi-step workflows where\ndecisions depend on evolving context from multiple information sources.\nEquipping LLMs with external tools and feedback mechanisms enables them to\ndrive systems that exhibit a spectrum of autonomy, ranging from semi-automated\nworkflows to more adaptive agents capable of managing complex processes. This\nreview examines the design of such LLM-driven agentic systems, highlights key\napplications, discusses evaluation methods for planning and tool use, and\noutlines challenges such as error cascades, tool-use efficiency, and health IT\nintegration."}
{"id": "2510.09041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09041", "abs": "https://arxiv.org/abs/2510.09041", "authors": ["Junchao Fan", "Xiaolin Chang"], "title": "Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach", "comment": null, "summary": "Deep reinforcement learning (DRL) has demonstrated remarkable success in\ndeveloping autonomous driving policies. However, its vulnerability to\nadversarial attacks remains a critical barrier to real-world deployment.\nAlthough existing robust methods have achieved success, they still suffer from\nthree key issues: (i) these methods are trained against myopic adversarial\nattacks, limiting their abilities to respond to more strategic threats, (ii)\nthey have trouble causing truly safety-critical events (e.g., collisions), but\ninstead often result in minor consequences, and (iii) these methods can\nintroduce learning instability and policy drift during training due to the lack\nof robust constraints. To address these issues, we propose Intelligent\nGeneral-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel\nrobust autonomous driving approach that consists of a strategic targeted\nadversary and a robust driving agent. The strategic targeted adversary is\ndesigned to leverage the temporal decision-making capabilities of DRL to\nexecute strategically coordinated multi-step attacks. In addition, it\nexplicitly focuses on inducing safety-critical events by adopting a general-sum\nobjective. The robust driving agent learns by interacting with the adversary to\ndevelop a robust autonomous driving policy against adversarial attacks. To\nensure stable learning in adversarial environments and to mitigate policy drift\ncaused by attacks, the agent is optimized under a constrained formulation.\nExtensive experiments show that IGCARL improves the success rate by at least\n27.9\\% over state-of-the-art methods, demonstrating superior robustness to\nadversarial attacks and enhancing the safety and reliability of DRL-based\nautonomous driving."}
{"id": "2510.09567", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.09567", "abs": "https://arxiv.org/abs/2510.09567", "authors": ["Jacopo Tagliabue", "Ciro Greco"], "title": "Safe, Untrusted, \"Proof-Carrying\" AI Agents: toward the agentic lakehouse", "comment": "IEEE Big Data, Workshop on Secure and Safe AI Agents for Big Data\n  Infrastructures", "summary": "Data lakehouses run sensitive workloads, where AI-driven automation raises\nconcerns about trust, correctness, and governance. We argue that API-first,\nprogrammable lakehouses provide the right abstractions for safe-by-design,\nagentic workflows. Using Bauplan as a case study, we show how data branching\nand declarative environments extend naturally to agents, enabling\nreproducibility and observability while reducing the attack surface. We present\na proof-of-concept in which agents repair data pipelines using correctness\nchecks inspired by proof-carrying code. Our prototype demonstrates that\nuntrusted AI agents can operate safely on production data and outlines a path\ntoward a fully agentic lakehouse."}
{"id": "2510.09156", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09156", "abs": "https://arxiv.org/abs/2510.09156", "authors": ["Jing Li", "Zhijie Sun", "Zhicheng Zhou", "Suming Qiu", "Junjie Huang", "Haijia Sun", "Linyuan Qiu"], "title": "Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning", "comment": null, "summary": "Current knowledge-enhanced large language models (LLMs) rely on static,\npre-constructed knowledge bases that suffer from coverage gaps and temporal\nobsolescence, limiting their effectiveness in dynamic information environments.\nWe present Agentic-KGR, a novel framework enabling co-evolution between LLMs\nand knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our\napproach introduces three key innovations: (1) a dynamic schema expansion\nmechanism that systematically extends graph ontologies beyond pre-defined\nboundaries during training; (2) a retrieval-augmented memory system enabling\nsynergistic co-evolution between model parameters and knowledge structures\nthrough continuous optimization; (3) a learnable multi-scale prompt compression\napproach that preserves critical information while reducing computational\ncomplexity through adaptive sequence optimization. Experimental results\ndemonstrate substantial improvements over supervised baselines and single-round\nRL approaches in knowledge extraction tasks. When integrated with GraphRAG, our\nmethod achieves superior performance in downstream QA tasks, with significant\ngains in both accuracy and knowledge coverage compared to existing methods."}
{"id": "2510.08578", "categories": ["cs.MA", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08578", "abs": "https://arxiv.org/abs/2510.08578", "authors": ["Adib Bazgir", "Amir Habibdoust", "Xing Song", "Yuwen Zhang"], "title": "AgenticAD: A Specialized Multiagent System Framework for Holistic Alzheimer Disease Management", "comment": null, "summary": "Alzheimer's disease (AD) presents a complex, multifaceted challenge to\npatients, caregivers, and the healthcare system, necessitating integrated and\ndynamic support solutions. While artificial intelligence (AI) offers promising\navenues for intervention, current applications are often siloed, addressing\nsingular aspects of the disease such as diagnostics or caregiver support\nwithout systemic integration. This paper proposes a novel methodological\nframework for a comprehensive, multi-agent system (MAS) designed for holistic\nAlzheimer's disease management. The objective is to detail the architecture of\na collaborative ecosystem of specialized AI agents, each engineered to address\na distinct challenge in the AD care continuum, from caregiver support and\nmultimodal data analysis to automated research and clinical data\ninterpretation. The proposed framework is composed of eight specialized,\ninteroperable agents. These agents are categorized by function: (1) Caregiver\nand Patient Support, (2) Data Analysis and Research, and (3) Advanced\nMultimodal Workflows. The methodology details the technical architecture of\neach agent, leveraging a suite of advanced technologies including large\nlanguage models (LLMs) such as GPT-4o and Gemini, multi-agent orchestration\nframeworks, Retrieval-Augmented Generation (RAG) for evidence-grounded\nresponses, and specialized tools for web scraping, multimodal data processing,\nand in-memory database querying. This paper presents a detailed architectural\nblueprint for an integrated AI ecosystem for AD care. By moving beyond\nsingle-purpose tools to a collaborative, multi-agent paradigm, this framework\nestablishes a foundation for developing more adaptive, personalized, and\nproactive solutions. This methodological approach aims to pave the way for\nfuture systems capable of synthesizing diverse data streams to improve patient\noutcomes and reduce caregiver burden."}
{"id": "2510.09201", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09201", "abs": "https://arxiv.org/abs/2510.09201", "authors": ["Yumin Choi", "Dongki Kim", "Jinheon Baek", "Sung Ju Hwang"], "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs."}
{"id": "2510.08711", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08711", "abs": "https://arxiv.org/abs/2510.08711", "authors": ["Jiachen Jiang", "Zhen Qin", "Zhihui Zhu"], "title": "In-Context Learning for Non-Stationary MIMO Equalization", "comment": null, "summary": "Channel equalization is fundamental for mitigating distortions such as\nfrequency-selective fading and inter-symbol interference. Unlike standard\nsupervised learning approaches that require costly retraining or fine-tuning\nfor each new task, in-context learning (ICL) adapts to new channels at\ninference time with only a few examples. However, existing ICL-based equalizers\nare primarily developed for and evaluated on static channels within the context\nwindow. Indeed, to our knowledge, prior principled analyses and theoretical\nstudies of ICL focus exclusively on the stationary setting, where the function\nremains fixed within the context. In this paper, we investigate the ability of\nICL to address non-stationary problems through the lens of time-varying channel\nequalization. We employ a principled framework for designing efficient\nattention mechanisms with improved adaptivity in non-stationary tasks,\nleveraging algorithms from adaptive signal processing to guide better designs.\nFor example, new attention variants can be derived from the Least Mean Square\n(LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for\nenhanced robustness, or multi-step gradient updates for improved long-term\ntracking. Experimental results demonstrate that ICL holds strong promise for\nnon-stationary MIMO equalization, and that attention mechanisms inspired by\nclassical adaptive algorithms can substantially enhance adaptability and\nperformance in dynamic environments. Our findings may provide critical insights\nfor developing next-generation wireless foundation models with stronger\nadaptability and robustness."}
{"id": "2510.09222", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09222", "abs": "https://arxiv.org/abs/2510.09222", "authors": ["Zhenglin Wan", "Jingxuan Wu", "Xingrui Yu", "Chubin Zhang", "Mingcong Lei", "Bo An", "Ivor Tsang"], "title": "FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning", "comment": "20 pages", "summary": "Flow Matching (FM) has shown remarkable ability in modeling complex\ndistributions and achieves strong performance in offline imitation learning for\ncloning expert behaviors. However, despite its behavioral cloning\nexpressiveness, FM-based policies are inherently limited by their lack of\nenvironmental interaction and exploration. This leads to poor generalization in\nunseen scenarios beyond the expert demonstrations, underscoring the necessity\nof online interaction with environment. Unfortunately, optimizing FM policies\nvia online interaction is challenging and inefficient due to instability in\ngradient computation and high inference costs. To address these issues, we\npropose to let a student policy with simple MLP structure explore the\nenvironment and be online updated via RL algorithm with a reward model. This\nreward model is associated with a teacher FM model, containing rich information\nof expert data distribution. Furthermore, the same teacher FM model is utilized\nto regularize the student policy's behavior to stabilize policy learning. Due\nto the student's simple architecture, we avoid the gradient instability of FM\npolicies and enable efficient online exploration, while still leveraging the\nexpressiveness of the teacher FM model. Extensive experiments show that our\napproach significantly enhances learning efficiency, generalization, and\nrobustness, especially when learning from suboptimal expert data."}
{"id": "2510.08779", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08779", "abs": "https://arxiv.org/abs/2510.08779", "authors": ["Vaibhav Jain", "Gerrit Grossmann"], "title": "Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations", "comment": "Accepted to LM4Plan Workshop @ ICAPS 2025 (withdrawn before\n  presentation due to lack of travel funding)", "summary": "Reinforcement Learning (RL) agents often struggle in sparse-reward\nenvironments where traditional exploration strategies fail to discover\neffective action sequences. Large Language Models (LLMs) possess procedural\nknowledge and reasoning capabilities from text pretraining that could guide RL\nexploration, but existing approaches create rigid dependencies where RL\npolicies must follow LLM suggestions or incorporate them directly into reward\nfunctions. We propose a framework that provides LLM-generated action\nrecommendations through augmented observation spaces, allowing RL agents to\nlearn when to follow or ignore this guidance. Our method leverages LLMs' world\nknowledge and reasoning abilities while maintaining flexibility through soft\nconstraints. We evaluate our approach on three BabyAI environments of\nincreasing complexity and show that the benefits of LLM guidance scale with\ntask difficulty. In the most challenging environment, we achieve 71% relative\nimprovement in final success rates over baseline. The approach provides\nsubstantial sample efficiency gains, with agents reaching performance\nthresholds up to 9 times faster, and requires no modifications to existing RL\nalgorithms. Our results demonstrate an effective method for leveraging LLM\nplanning capabilities to accelerate RL training in challenging environments."}
{"id": "2510.09317", "categories": ["cs.LG", "cs.NA", "math.NA", "65H10, 37M05", "G.1.5; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.09317", "abs": "https://arxiv.org/abs/2510.09317", "authors": ["Felix Brandt", "Andreas Heuermann", "Philip Hannebohm", "Bernhard Bachmann"], "title": "Residual-Informed Learning of Solutions to Algebraic Loops", "comment": "16 pages, 16 figures, 5 tables, submitted to IDaS-Schriftenreihe from\n  Hochschule Bielefeld - University of Applied Sciences and Arts (HSBI)", "summary": "This paper presents a residual-informed machine learning approach for\nreplacing algebraic loops in equation-based Modelica models with neural network\nsurrogates. A feedforward neural network is trained using the residual (error)\nof the algebraic loop directly in its loss function, eliminating the need for a\nsupervised dataset. This training strategy also resolves the issue of ambiguous\nsolutions, allowing the surrogate to converge to a consistent solution rather\nthan averaging multiple valid ones. Applied to the large-scale IEEE 14-Bus\nsystem, our method achieves a 60% reduction in simulation time compared to\nconventional simulations, while maintaining the same level of accuracy through\nerror control mechanisms."}
{"id": "2510.08794", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08794", "abs": "https://arxiv.org/abs/2510.08794", "authors": ["I. Arda Vurankaya", "Mustafa O. Karabag", "Wesley A. Suttle", "Jesse Milzman", "David Fridovich-Keil", "Ufuk Topcu"], "title": "Deceptive Exploration in Multi-armed Bandits", "comment": null, "summary": "We consider a multi-armed bandit setting in which each arm has a public and a\nprivate reward distribution. An observer expects an agent to follow Thompson\nSampling according to the public rewards, however, the deceptive agent aims to\nquickly identify the best private arm without being noticed. The observer can\nobserve the public rewards and the pulled arms, but not the private rewards.\nThe agent, on the other hand, observes both the public and private rewards. We\nformalize detectability as a stepwise Kullback-Leibler (KL) divergence\nconstraint between the actual pull probabilities used by the agent and the\nanticipated pull probabilities by the observer. We model successful pulling of\npublic suboptimal arms as a % Bernoulli process where the success probability\ndecreases with each successful pull, and show these pulls can happen at most at\na $\\Theta(\\sqrt{T}) $ rate under the KL constraint. We then formulate a maximin\nproblem based on public and private means, whose solution characterizes the\noptimal error exponent for best private arm identification. We finally propose\nan algorithm inspired by top-two algorithms. This algorithm naturally adapts\nits exploration according to the hardness of pulling arms based on the public\nsuboptimality gaps. We provide numerical examples illustrating the\n$\\Theta(\\sqrt{T}) $ rate and the behavior of the proposed algorithm."}
{"id": "2510.09325", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09325", "abs": "https://arxiv.org/abs/2510.09325", "authors": ["Till Freihaut", "Luca Viano", "Emanuele Nevali", "Volkan Cevher", "Matthieu Geist", "Giorgia Ramponi"], "title": "Rate optimal learning of equilibria from data", "comment": null, "summary": "We close open theoretical gaps in Multi-Agent Imitation Learning (MAIL) by\ncharacterizing the limits of non-interactive MAIL and presenting the first\ninteractive algorithm with near-optimal sample complexity. In the\nnon-interactive setting, we prove a statistical lower bound that identifies the\nall-policy deviation concentrability coefficient as the fundamental complexity\nmeasure, and we show that Behavior Cloning (BC) is rate-optimal. For the\ninteractive setting, we introduce a framework that combines reward-free\nreinforcement learning with interactive MAIL and instantiate it with an\nalgorithm, MAIL-WARM. It improves the best previously known sample complexity\nfrom $\\mathcal{O}(\\varepsilon^{-8})$ to $\\mathcal{O}(\\varepsilon^{-2}),$\nmatching the dependence on $\\varepsilon$ implied by our lower bound. Finally,\nwe provide numerical results that support our theory and illustrate, in\nenvironments such as grid worlds, where Behavior Cloning fails to learn."}
{"id": "2510.08829", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08829", "abs": "https://arxiv.org/abs/2510.08829", "authors": ["Debeshee Das", "Luca Beurer-Kellner", "Marc Fischer", "Maximilian Baader"], "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization", "comment": null, "summary": "The increasing adoption of LLM agents with access to numerous tools and\nsensitive data significantly widens the attack surface for indirect prompt\ninjections. Due to the context-dependent nature of attacks, however, current\ndefenses are often ill-calibrated as they cannot reliably differentiate\nmalicious and benign instructions, leading to high false positive rates that\nprevent their real-world adoption. To address this, we present a novel approach\ninspired by the fundamental principle of computer security: data should not\ncontain executable instructions. Instead of sample-level classification, we\npropose a token-level sanitization process, which surgically removes any\ninstructions directed at AI systems from tool outputs, capturing malicious\ninstructions as a byproduct. In contrast to existing safety classifiers, this\napproach is non-blocking, does not require calibration, and is agnostic to the\ncontext of tool outputs. Further, we can train such token-level predictors with\nreadily available instruction-tuning data only, and don't have to rely on\nunrealistic prompt injection examples from challenges or of other synthetic\norigin. In our experiments, we find that this approach generalizes well across\na wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB\nand SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on\nAgentDojo), without impairing agent utility in both benign and malicious\nsettings."}
{"id": "2510.09330", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09330", "abs": "https://arxiv.org/abs/2510.09330", "authors": ["Tuan Nguyen", "Long Tran-Thanh"], "title": "Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers", "comment": null, "summary": "Ensuring that large language models (LLMs) comply with safety requirements is\na central challenge in AI deployment. Existing alignment approaches primarily\noperate during training, such as through fine-tuning or reinforcement learning\nfrom human feedback, but these methods are costly and inflexible, requiring\nretraining whenever new requirements arise. Recent efforts toward\ninference-time alignment mitigate some of these limitations but still assume\naccess to model internals, which is impractical, and not suitable for third\nparty stakeholders who do not have access to the models. In this work, we\npropose a model-independent, black-box framework for safety alignment that does\nnot require retraining or access to the underlying LLM architecture. As a proof\nof concept, we address the problem of trading off between generating safe but\nuninformative answers versus helpful yet potentially risky ones. We formulate\nthis dilemma as a two-player zero-sum game whose minimax equilibrium captures\nthe optimal balance between safety and helpfulness. LLM agents operationalize\nthis framework by leveraging a linear programming solver at inference time to\ncompute equilibrium strategies. Our results demonstrate the feasibility of\nblack-box safety alignment, offering a scalable and accessible pathway for\nstakeholders, including smaller organizations and entities in\nresource-constrained settings, to enforce safety across rapidly evolving LLM\necosystems."}
{"id": "2510.08839", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.08839", "abs": "https://arxiv.org/abs/2510.08839", "authors": ["Motahare Mounesan", "Sourya Saha", "Houchao Gan", "Md. Nurul Absur", "Saptarshi Debroy"], "title": "Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction", "comment": null, "summary": "Real-time multi-view 3D reconstruction is a mission-critical application for\nkey edge-native use cases, such as fire rescue, where timely and accurate 3D\nscene modeling enables situational awareness and informed decision-making.\nHowever, the dynamic and unpredictable nature of edge resource availability\nintroduces disruptions, such as degraded image quality, unstable network links,\nand fluctuating server loads, which challenge the reliability of the\nreconstruction pipeline. In this work, we present a reinforcement learning\n(RL)-based edge resource management framework for reliable 3D reconstruction to\nensure high quality reconstruction within a reasonable amount of time, despite\nthe system operating under a resource-constrained and disruption-prone\nenvironment. In particular, the framework adopts two cooperative Q-learning\nagents, one for camera selection and one for server selection, both of which\noperate entirely online, learning policies through interactions with the edge\nenvironment. To support learning under realistic constraints and evaluate\nsystem performance, we implement a distributed testbed comprising lab-hosted\nend devices and FABRIC infrastructure-hosted edge servers to emulate smart city\nedge infrastructure under realistic disruption scenarios. Results show that the\nproposed framework improves application reliability by effectively balancing\nend-to-end latency and reconstruction quality in dynamic environments."}
{"id": "2510.09405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09405", "abs": "https://arxiv.org/abs/2510.09405", "authors": ["Yuhao Pan", "Xiucheng Wang", "Nan Cheng", "Wenchao Xu"], "title": "Cross-Receiver Generalization for RF Fingerprint Identification via Feature Disentanglement and Adversarial Training", "comment": null, "summary": "Radio frequency fingerprint identification (RFFI) is a critical technique for\nwireless network security, leveraging intrinsic hardware-level imperfections\nintroduced during device manufacturing to enable precise transmitter\nidentification. While deep neural networks have shown remarkable capability in\nextracting discriminative features, their real-world deployment is hindered by\nreceiver-induced variability. In practice, RF fingerprint signals comprise\ntransmitter-specific features as well as channel distortions and\nreceiver-induced biases. Although channel equalization can mitigate channel\nnoise, receiver-induced feature shifts remain largely unaddressed, causing the\nRFFI models to overfit to receiver-specific patterns. This limitation is\nparticularly problematic when training and evaluation share the same receiver,\nas replacing the receiver in deployment can cause substantial performance\ndegradation. To tackle this challenge, we propose an RFFI framework robust to\ncross-receiver variability, integrating adversarial training and style transfer\nto explicitly disentangle transmitter and receiver features. By enforcing\ndomain-invariant representation learning, our method isolates genuine hardware\nsignatures from receiver artifacts, ensuring robustness against receiver\nchanges. Extensive experiments on multi-receiver datasets demonstrate that our\napproach consistently outperforms state-of-the-art baselines, achieving up to a\n10% improvement in average accuracy across diverse receiver settings."}
{"id": "2510.09041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09041", "abs": "https://arxiv.org/abs/2510.09041", "authors": ["Junchao Fan", "Xiaolin Chang"], "title": "Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach", "comment": null, "summary": "Deep reinforcement learning (DRL) has demonstrated remarkable success in\ndeveloping autonomous driving policies. However, its vulnerability to\nadversarial attacks remains a critical barrier to real-world deployment.\nAlthough existing robust methods have achieved success, they still suffer from\nthree key issues: (i) these methods are trained against myopic adversarial\nattacks, limiting their abilities to respond to more strategic threats, (ii)\nthey have trouble causing truly safety-critical events (e.g., collisions), but\ninstead often result in minor consequences, and (iii) these methods can\nintroduce learning instability and policy drift during training due to the lack\nof robust constraints. To address these issues, we propose Intelligent\nGeneral-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel\nrobust autonomous driving approach that consists of a strategic targeted\nadversary and a robust driving agent. The strategic targeted adversary is\ndesigned to leverage the temporal decision-making capabilities of DRL to\nexecute strategically coordinated multi-step attacks. In addition, it\nexplicitly focuses on inducing safety-critical events by adopting a general-sum\nobjective. The robust driving agent learns by interacting with the adversary to\ndevelop a robust autonomous driving policy against adversarial attacks. To\nensure stable learning in adversarial environments and to mitigate policy drift\ncaused by attacks, the agent is optimized under a constrained formulation.\nExtensive experiments show that IGCARL improves the success rate by at least\n27.9\\% over state-of-the-art methods, demonstrating superior robustness to\nadversarial attacks and enhancing the safety and reliability of DRL-based\nautonomous driving."}
{"id": "2510.09462", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09462", "abs": "https://arxiv.org/abs/2510.09462", "authors": ["Mikhail Terekhov", "Alexander Panfilov", "Daniil Dzenhaliou", "Caglar Gulcehre", "Maksym Andriushchenko", "Ameya Prabhu", "Jonas Geiping"], "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols", "comment": null, "summary": "AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-$n$ attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms."}
{"id": "2510.09201", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09201", "abs": "https://arxiv.org/abs/2510.09201", "authors": ["Yumin Choi", "Dongki Kim", "Jinheon Baek", "Sung Ju Hwang"], "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs."}
{"id": "2510.09487", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09487", "abs": "https://arxiv.org/abs/2510.09487", "authors": ["Shangzhe Li", "Dongruo Zhou", "Weitong Zhang"], "title": "Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning", "comment": "48 pages, 3 figures, 4 tables", "summary": "We study online adversarial imitation learning (AIL), where an agent learns\nfrom offline expert demonstrations and interacts with the environment online\nwithout access to rewards. Despite strong empirical results, the benefits of\nonline interaction and the impact of stochasticity remain poorly understood. We\naddress these gaps by introducing a model-based AIL algorithm (MB-AIL) and\nestablish its horizon-free, second-order sample-complexity guarantees under\ngeneral function approximations for both expert data and reward-free\ninteractions. These second-order bounds provide an instance-dependent result\nthat can scale with the variance of returns under the relevant policies and\ntherefore tighten as the system approaches determinism. Together with\nsecond-order, information-theoretic lower bounds on a newly constructed\nhard-instance family, we show that MB-AIL attains minimax-optimal sample\ncomplexity for online interaction (up to logarithmic factors) with limited\nexpert demonstrations and matches the lower bound for expert demonstrations in\nterms of the dependence on horizon $H$, precision $\\epsilon$ and the policy\nvariance $\\sigma^2$. Experiments further validate our theoretical findings and\ndemonstrate that a practical implementation of MB-AIL matches or surpasses the\nsample efficiency of existing methods."}
{"id": "2510.09325", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09325", "abs": "https://arxiv.org/abs/2510.09325", "authors": ["Till Freihaut", "Luca Viano", "Emanuele Nevali", "Volkan Cevher", "Matthieu Geist", "Giorgia Ramponi"], "title": "Rate optimal learning of equilibria from data", "comment": null, "summary": "We close open theoretical gaps in Multi-Agent Imitation Learning (MAIL) by\ncharacterizing the limits of non-interactive MAIL and presenting the first\ninteractive algorithm with near-optimal sample complexity. In the\nnon-interactive setting, we prove a statistical lower bound that identifies the\nall-policy deviation concentrability coefficient as the fundamental complexity\nmeasure, and we show that Behavior Cloning (BC) is rate-optimal. For the\ninteractive setting, we introduce a framework that combines reward-free\nreinforcement learning with interactive MAIL and instantiate it with an\nalgorithm, MAIL-WARM. It improves the best previously known sample complexity\nfrom $\\mathcal{O}(\\varepsilon^{-8})$ to $\\mathcal{O}(\\varepsilon^{-2}),$\nmatching the dependence on $\\varepsilon$ implied by our lower bound. Finally,\nwe provide numerical results that support our theory and illustrate, in\nenvironments such as grid worlds, where Behavior Cloning fails to learn."}
{"id": "2510.09594", "categories": ["cs.LG", "q-bio.MN", "37, 60", "I.6.0; J.3"], "pdf": "https://arxiv.org/pdf/2510.09594", "abs": "https://arxiv.org/abs/2510.09594", "authors": ["Nathan Quiblier", "Roy Friedman", "Matthew Ricci"], "title": "MODE: Learning compositional representations of complex systems with Mixtures Of Dynamical Experts", "comment": "30 pages, 5 figures", "summary": "Dynamical systems in the life sciences are often composed of complex mixtures\nof overlapping behavioral regimes. Cellular subpopulations may shift from\ncycling to equilibrium dynamics or branch towards different developmental\nfates. The transitions between these regimes can appear noisy and irregular,\nposing a serious challenge to traditional, flow-based modeling techniques which\nassume locally smooth dynamics. To address this challenge, we propose MODE\n(Mixture Of Dynamical Experts), a graphical modeling framework whose neural\ngating mechanism decomposes complex dynamics into sparse, interpretable\ncomponents, enabling both the unsupervised discovery of behavioral regimes and\naccurate long-term forecasting across regime transitions. Crucially, because\nagents in our framework can jump to different governing laws, MODE is\nespecially tailored to the aforementioned noisy transitions. We evaluate our\nmethod on a battery of synthetic and real datasets from computational biology.\nFirst, we systematically benchmark MODE on an unsupervised classification task\nusing synthetic dynamical snapshot data, including in noisy, few-sample\nsettings. Next, we show how MODE succeeds on challenging forecasting tasks\nwhich simulate key cycling and branching processes in cell biology. Finally, we\ndeploy our method on human, single-cell RNA sequencing data and show that it\ncan not only distinguish proliferation from differentiation dynamics but also\npredict when cells will commit to their ultimate fate, a key outstanding\nchallenge in computational biology."}
{"id": "2510.09462", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09462", "abs": "https://arxiv.org/abs/2510.09462", "authors": ["Mikhail Terekhov", "Alexander Panfilov", "Daniil Dzenhaliou", "Caglar Gulcehre", "Maksym Andriushchenko", "Ameya Prabhu", "Jonas Geiping"], "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols", "comment": null, "summary": "AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-$n$ attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms."}
{"id": "2510.08619", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08619", "abs": "https://arxiv.org/abs/2510.08619", "authors": ["Tennison Liu", "Silas Ruhrberg Est√©vez", "David L. Bentley", "Mihaela van der Schaar"], "title": "Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents", "comment": null, "summary": "Large-scale scientific datasets -- spanning health biobanks, cell atlases,\nEarth reanalyses, and more -- create opportunities for exploratory discovery\nunconstrained by specific research questions. We term this process hypothesis\nhunting: the cumulative search for insight through sustained exploration across\nvast and complex hypothesis spaces. To support it, we introduce AScience, a\nframework modeling discovery as the interaction of agents, networks, and\nevaluation norms, and implement it as ASCollab, a distributed system of\nLLM-based research agents with heterogeneous behaviors. These agents\nself-organize into evolving networks, continually producing and peer-reviewing\nfindings under shared standards of evaluation. Experiments show that such\nsocial dynamics enable the accumulation of expert-rated results along the\ndiversity-quality-novelty frontier, including rediscoveries of established\nbiomarkers, extensions of known pathways, and proposals of new therapeutic\ntargets. While wet-lab validation remains indispensable, our experiments on\ncancer cohorts demonstrate that socially structured, agentic networks can\nsustain exploratory hypothesis hunting at scale."}
{"id": "2510.09469", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09469", "abs": "https://arxiv.org/abs/2510.09469", "authors": ["Bharath Muppasani", "Ritirupa Dey", "Biplav Srivastava", "Vignesh Narayanan"], "title": "Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy", "comment": null, "summary": "Multi-agent pathfinding (MAPF) remains a critical problem in robotics and\nautonomous systems, where agents must navigate shared spaces efficiently while\navoiding conflicts. Traditional centralized algorithms that have global\ninformation, such as Conflict-Based Search (CBS), provide high-quality\nsolutions but become computationally expensive in large-scale scenarios due to\nthe combinatorial explosion of conflicts that need resolution. Conversely,\ndistributed approaches that have local information, particularly learning-based\nmethods, offer better scalability by operating with relaxed information\navailability, yet often at the cost of solution quality. To address these\nlimitations, we propose a hybrid framework that combines decentralized path\nplanning with a lightweight centralized coordinator. Our framework leverages\nreinforcement learning (RL) for decentralized planning, enabling agents to\nadapt their planning based on minimal, targeted alerts--such as static\nconflict-cell flags or brief conflict tracks--that are dynamically shared\ninformation from the central coordinator for effective conflict resolution. We\nempirically study the effect of the information available to an agent on its\nplanning performance. Our approach reduces the inter-agent information sharing\ncompared to fully centralized and distributed methods, while still consistently\nfinding feasible, collision-free solutions--even in large-scale scenarios\nhaving higher agent counts."}
{"id": "2510.08829", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08829", "abs": "https://arxiv.org/abs/2510.08829", "authors": ["Debeshee Das", "Luca Beurer-Kellner", "Marc Fischer", "Maximilian Baader"], "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization", "comment": null, "summary": "The increasing adoption of LLM agents with access to numerous tools and\nsensitive data significantly widens the attack surface for indirect prompt\ninjections. Due to the context-dependent nature of attacks, however, current\ndefenses are often ill-calibrated as they cannot reliably differentiate\nmalicious and benign instructions, leading to high false positive rates that\nprevent their real-world adoption. To address this, we present a novel approach\ninspired by the fundamental principle of computer security: data should not\ncontain executable instructions. Instead of sample-level classification, we\npropose a token-level sanitization process, which surgically removes any\ninstructions directed at AI systems from tool outputs, capturing malicious\ninstructions as a byproduct. In contrast to existing safety classifiers, this\napproach is non-blocking, does not require calibration, and is agnostic to the\ncontext of tool outputs. Further, we can train such token-level predictors with\nreadily available instruction-tuning data only, and don't have to rely on\nunrealistic prompt injection examples from challenges or of other synthetic\norigin. In our experiments, we find that this approach generalizes well across\na wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB\nand SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on\nAgentDojo), without impairing agent utility in both benign and malicious\nsettings."}
{"id": "2510.09021", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09021", "abs": "https://arxiv.org/abs/2510.09021", "authors": ["Hamed Mahdavi", "Pouria Mahdavinia", "Samira Malek", "Pegah Mohammadipour", "Alireza Hashemi", "Majid Daliri", "Alireza Farhadi", "Amir Khasahmadi", "Niloofar Mireshghallah", "Vasant Honavar"], "title": "RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows", "comment": null, "summary": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based\nOlympiad problems to solving most of the IMO 2025 problems, with leading\nsystems reportedly handling 5 of 6 problems. Given this progress, we assess how\nwell these models can grade proofs: detecting errors, judging their severity,\nand assigning fair scores beyond binary correctness. We study proof-analysis\ncapabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we\ngrade on a 1-4 scale with detailed error annotations, and on MathArena solution\nsets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models\ncan reliably flag incorrect (including subtly incorrect) solutions but exhibit\ncalibration gaps in how partial credit is assigned. To address this, we\nintroduce agentic workflows that extract and analyze reference solutions and\nautomatically derive problem-specific rubrics for a multi-step grading process.\nWe instantiate and compare different design choices for the grading workflows,\nand evaluate their trade-offs. Across our annotated corpus and MathArena, our\nproposed workflows achieve higher agreement with human grades and more\nconsistent handling of partial credit across metrics. We release all code,\ndata, and prompts/logs to facilitate future research."}
{"id": "2510.09038", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09038", "abs": "https://arxiv.org/abs/2510.09038", "authors": ["Wenyi Wu", "Kun Zhou", "Ruoxin Yuan", "Vivian Yu", "Stephen Wang", "Zhiting Hu", "Biwei Huang"], "title": "Auto-scaling Continuous Memory for GUI Agent", "comment": null, "summary": "We study how to endow GUI agents with scalable memory that help generalize\nacross unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress\npast trajectories into text tokens, which balloons context length and misses\ndecisive visual cues (e.g., exact widget size and position). We propose a\ncontinuous memory that encodes each GUI trajectory into a fixed-length sequence\nof continuous embeddings using the VLM itself as an encoder; these embeddings\nare plugged directly into the backbone's input layer, sharply reducing context\ncost while preserving fine-grained visual information. As memory size and\nretrieval depth increase, performance improves monotonically, unlike text\nmemories that degrade with long prompts. To grow memory at low cost, we\nintroduce an auto-scaling data flywheel that (i) discovers new environments via\nsearch, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out\ntrajectories with the agent, and (iv) verifies success with the same VLM. Using\nthis pipeline, we collect 100k+ trajectories for about \\$4000 and fine-tune\nonly the memory encoder (LoRA on a Q-Former, 1.2\\% parameters) with 1,500\nsamples. On real-world GUI benchmarks, our memory-augmented agent consistently\nimproves success rates under long horizons and distribution shifts. Notably,\nQwen-2.5-VL-7B + continuous memory achieves performance comparable to\nstate-of-the-art closed-source models (e.g., GPT-4o, Claude-4)."}
{"id": "2510.09260", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09260", "abs": "https://arxiv.org/abs/2510.09260", "authors": ["Subrat Kishore Dutta", "Yuelin Xu", "Piyush Pant", "Xiao Zhang"], "title": "GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis", "comment": null, "summary": "Recent work has shown that RLHF is highly susceptible to backdoor attacks,\npoisoning schemes that inject malicious triggers in preference data. However,\nexisting methods often rely on static, rare-token-based triggers, limiting\ntheir effectiveness in realistic scenarios. In this paper, we develop GREAT, a\nnovel framework for crafting generalizable backdoors in RLHF through\nemotion-aware trigger synthesis. Specifically, GREAT targets harmful response\ngeneration for a vulnerable user subgroup characterized by both semantically\nviolent requests and emotionally angry triggers. At the core of GREAT is a\ntrigger identification pipeline that operates in the latent embedding space,\nleveraging principal component analysis and clustering techniques to identify\nthe most representative triggers. To enable this, we present Erinyes, a\nhigh-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a\nprincipled, hierarchical, and diversity-promoting approach. Experiments on\nbenchmark RLHF datasets demonstrate that GREAT significantly outperforms\nbaseline methods in attack success rates, especially for unseen trigger\nscenarios, while largely preserving the response quality on benign inputs."}
