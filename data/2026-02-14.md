<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 7]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.LG](#cs.LG) [Total: 25]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [TRACE: Timely Retrieval and Alignment for Cybersecurity Knowledge Graph Construction and Expansion](https://arxiv.org/abs/2602.11211)
*Zijing Xu,Ziwei Ning,Tiancheng Hu,Jianwei Zhuge,Yangyang Wang,Jiahao Cao,Mingwei Xu*

Main category: cs.CR

TL;DR: TRACE 프레임워크는 구조화된 데이터와 비구조화된 사이버 보안 데이터 소스를 통합하여 사이버 위협에 대한 통찰력을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 사이버 위협의 빠른 발전으로 인해 보안 지식 통합의 중요한 격차가 드러났다.

Method: TRACE는 24개의 구조화된 데이터베이스와 APT 보고서, 논문, 수리 통지와 같은 3가지 비구조화된 데이터 카테고리에서 지식을 통합하는 프레임워크이다.

Result: TRACE는 기존 CKG에 비해 1.8배의 노드 커버리지를 달성하고, 엔티티 추출에서 86.08%의 정밀도, 76.92%의 재현율, 81.24%의 F1 점수를 기록하였다.

Conclusion: TRACE를 통해 위협 헌터와 공격 분석가는 취약점, 공격 방법 및 방어 기술에 대한 실시간의 전체론적 통찰력을 얻게 된다.

Abstract: The rapid evolution of cyber threats has highlighted significant gaps in security knowledge integration. Cybersecurity Knowledge Graphs (CKGs) relying on structured data inherently exhibit hysteresis, as the timely incorporation of rapidly evolving unstructured data remains limited, potentially leading to the omission of critical insights for risk analysis. To address these limitations, we introduce TRACE, a framework designed to integrate structured and unstructured cybersecurity data sources. TRACE integrates knowledge from 24 structured databases and 3 categories of unstructured data, including APT reports, papers, and repair notices. Leveraging Large Language Models (LLMs), TRACE facilitates efficient entity extraction and alignment, enabling continuous updates to the CKG. Evaluations demonstrate that TRACE achieves a 1.8x increase in node coverage compared to existing CKGs. TRACE attains the precision of 86.08%, the recall of 76.92%, and the F1 score of 81.24% in entity extraction, surpassing the best-known LLM-based baselines by 7.8%. Furthermore, our entity alignment methods effectively harmonize entities with existing knowledge structures, enhancing the integrity and utility of the CKG. With TRACE, threat hunters and attack analysts gain real-time, holistic insights into vulnerabilities, attack methods, and defense technologies.

</details>


### [2] [CryptoAnalystBench: Failures in Multi-Tool Long-Form LLM Analysis](https://arxiv.org/abs/2602.11304)
*Anushri Eswaran,Oleg Golev,Darshan Tank,Sidhant Rahi,Himanshu Tyagi*

Main category: cs.CR

TL;DR: 본 논문은 고밀도 데이터 영역인 암호화폐 분석에서 LLM(대형 언어 모델)의 실패 사례를 조사하고, 이를 해결하기 위한 벤치마크와 평가 프로세스를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현대 분석 에이전트는 복잡하고 다양한 입력을 처리해야 하며, 이에 따라 LLM과 다수의 도구 출력의 상호작용을 이해할 필요가 있다.

Method: CryptoAnalystBench라는 벤치마크를 만들고, 다양한 도구와 함께 LLM을 사용하여 응답을 생성하는 방법을 제안하였다. 또한 인용 검증 및 평가 기준을 포함한 평가 파이프라인을 개발하였다.

Result: 인간 주석을 통해 LLM의 오류 유형을 분류하고, 이를 통해 이러한 오류가 첨단 시스템에서도 발생한다는 것을 발견하였다. 이는 중요한 결정에 영향을 미칠 수 있다.

Conclusion: 우리는 평가 기준을 개선하여 이러한 오류를 더 잘 포착하고, 개발자와 연구자가 분석 에이전트를 연구할 수 있도록 스케일러블한 피드백을 제공할 수 있는 방법을 제안한다.

Abstract: Modern analyst agents must reason over complex, high token inputs, including dozens of retrieved documents, tool outputs, and time sensitive data. While prior work has produced tool calling benchmarks and examined factuality in knowledge augmented systems, relatively little work studies their intersection: settings where LLMs must integrate large volumes of dynamic, structured and unstructured multi tool outputs. We investigate LLM failure modes in this regime using crypto as a representative high data density domain. We introduce (1) CryptoAnalystBench, an analyst aligned benchmark of 198 production crypto and DeFi queries spanning 11 categories; (2) an agentic harness equipped with relevant crypto and DeFi tools to generate responses across multiple frontier LLMs; and (3) an evaluation pipeline with citation verification and an LLM as a judge rubric spanning four user defined success dimensions: relevance, temporal relevance, depth, and data consistency. Using human annotation, we develop a taxonomy of seven higher order error types that are not reliably captured by factuality checks or LLM based quality scoring. We find that these failures persist even in state of the art systems and can compromise high stakes decisions. Based on this taxonomy, we refine the judge rubric to better capture these errors. While the judge does not align with human annotators on precise scoring across rubric iterations, it reliably identifies critical failure modes, enabling scalable feedback for developers and researchers studying analyst style agents. We release CryptoAnalystBench with annotated queries, the evaluation pipeline, judge rubrics, and the error taxonomy, and outline mitigation strategies and open challenges in evaluating long form, multi tool augmented systems.

</details>


### [3] [Security Threat Modeling for Emerging AI-Agent Protocols: A Comparative Analysis of MCP, A2A, Agora, and ANP](https://arxiv.org/abs/2602.11327)
*Zeynab Anbiaee,Mahdi Rabbani,Mansur Mirani,Gunjan Piya,Igor Opushnyev,Ali Ghorbani,Sajjad Dadkhah*

Main category: cs.CR

TL;DR: MCP, A2A, Agora, ANP 등 AI 에이전트 통신 프로토콜의 보안 분석을 체계적으로 수행하고 프로토콜별 위험을 식별하며, 안전한 배포를 위한 지침을 제공한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트 통신 프로토콜의 보안 원칙이 미비하고, 프로토콜 중심의 위험 평가 프레임워크가 없어 체계적인 분석이 필요하다.

Method: 프로토콜 아키텍처, 신뢰 가정, 상호작용 패턴 및 라이프사이클 행동을 분석하는 구조화된 위협 모델링 분석 개발 후, 12가지 프로토콜 수준 위험을 식별하고 평가하는 질적 위험 평가 프레임워크 도입.

Result: MCP에 대한 사례 연구를 통해 executable components의 필수 검증/증명이 누락된 위험을 정량화하여 보안 주장을 형식화하였다.

Conclusion: 설계에 의해 유도된 주요 위험 표면을 강조하고 에이전트 통신 생태계의 안전한 배포 및 향후 표준화를 위한 실행 가능한 지침을 제공한다.

Abstract: The rapid development of the AI agent communication protocols, including the Model Context Protocol (MCP), Agent2Agent (A2A), Agora, and Agent Network Protocol (ANP), is reshaping how AI agents communicate with tools, services, and each other. While these protocols support scalable multi-agent interaction and cross-organizational interoperability, their security principles remain understudied, and standardized threat modeling is limited; no protocol-centric risk assessment framework has been established yet. This paper presents a systematic security analysis of four emerging AI agent communication protocols. First, we develop a structured threat modeling analysis that examines protocol architectures, trust assumptions, interaction patterns, and lifecycle behaviors to identify protocol-specific and cross-protocol risk surfaces. Second, we introduce a qualitative risk assessment framework that identifies twelve protocol-level risks and evaluates security posture across the creation, operation, and update phases through systematic assessment of likelihood, impact, and overall protocol risk, with implications for secure deployment and future standardization. Third, we provide a measurement-driven case study on MCP that formalizes the risk of missing mandatory validation/attestation for executable components as a falsifiable security claim by quantifying wrong-provider tool execution under multi-server composition across representative resolver policies. Collectively, our results highlight key design-induced risk surfaces and provide actionable guidance for secure deployment and future standardization of agent communication ecosystems.

</details>


### [4] [Optimizing Agent Planning for Security and Autonomy](https://arxiv.org/abs/2602.11416)
*Aashish Kolluri,Rishi Sharma,Manuel Costa,Boris Köpf,Tobias Nießen,Mark Russinovich,Shruti Tople,Santiago Zanella-Béguelin*

Main category: cs.CR

TL;DR: 간접 프롬프트 주입 공격은 결과적인 행동을 실행하는 AI 에이전트를 위협하며, 결정론적 시스템 수준 방어의 필요성을 자극합니다. 이 연구는 보안성을 유지하면서 인간의 개입 없이 에이전트가 수행할 수 있는 행동의 비율을 정량화하는 자율성 지표를 도입했습니다.


<details>
  <summary>Details</summary>
Motivation: 간접 프롬프트 주입 공격으로부터 AI 에이전트를 보호하기 위해 결정론적 시스템 수준 방어가 필요하다.

Method: 보안 인식을 가진 에이전트를 설계하여, 더욱 풍부한 인간 개입(HITL) 상호작용을 도입하고, 작업 진행과 정책 준수 모두를 계획합니다.

Result: 이 연구는 기존의 정보 흐름 제어 방어 위에 이 디자인을 구현하여 AgentDojo 및 WASP 벤치마크에서 평가하였으며, 결과적으로 유틸리티를 포기하지 않고 더 높은 자율성을 달성했습니다.

Conclusion: 결정론적 시스템 수준 방어의 주요 이점은 인간의 감독에 대한 의존성을 줄이는 것으로, 자율성 지표를 통해 이점을 정량화했습니다.

Abstract: Indirect prompt injection attacks threaten AI agents that execute consequential actions, motivating deterministic system-level defenses. Such defenses can provably block unsafe actions by enforcing confidentiality and integrity policies, but currently appear costly: they reduce task completion rates and increase token usage compared to probabilistic defenses. We argue that existing evaluations miss a key benefit of system-level defenses: reduced reliance on human oversight. We introduce autonomy metrics to quantify this benefit: the fraction of consequential actions an agent can execute without human-in-the-loop (HITL) approval while preserving security. To increase autonomy, we design a security-aware agent that (i) introduces richer HITL interactions, and (ii) explicitly plans for both task progress and policy compliance. We implement this agent design atop an existing information-flow control defense against prompt injection and evaluate it on the AgentDojo and WASP benchmarks. Experiments show that this approach yields higher autonomy without sacrificing utility.

</details>


### [5] [Resource-Aware Deployment Optimization for Collaborative Intrusion Detection in Layered Networks](https://arxiv.org/abs/2602.11851)
*André García Gómez,Ines Rieger,Wolfgang Hotwagner,Max Landauer,Markus Wurzenberger,Florian Skopik,Edgar Weippl*

Main category: cs.CR

TL;DR: 비교적 다양한 분산 환경에서 쉽게 배포 가능한 새로운 협업 침입 탐지 시스템(CIDS) 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 분산된 중요한 인프라가 빠르게 진화하는 환경에서 효과적으로 작동할 수 있도록 설계된 CIDS 아키텍처가 필요합니다.

Method: 노드별로 사용 가능한 자원과 데이터 유형에 따라 탐지기 배치를 동적으로 최적화하는 CIDS 프레임워크를 개발했습니다.

Result: 여러 분산 데이터 세트를 사용한 평가를 통해 제안된 프레임워크가 자동으로 최적 구성을 유지하면서 효율적인 침입 탐지를 달성할 수 있음을 입증했습니다.

Conclusion: 제안된 CIDS 프레임워크는 중대한 인프라를 겨냥한 실제 사이버 공격을 기반으로 하는 공공 데이터 세트를 포함하여 다양한 공격 체인 및 네트워크 토폴로지를 특징으로 하는 환경에서 빠르게 적응할 수 있습니다.

Abstract: Collaborative Intrusion Detection Systems (CIDS) are increasingly adopted to counter cyberattacks, as their collaborative nature enables them to adapt to diverse scenarios across heterogeneous environments. As distributed critical infrastructure operates in rapidly evolving environments, such as drones in both civil and military domains, there is a growing need for CIDS architectures that can flexibly accommodate these dynamic changes. In this study, we propose a novel CIDS framework designed for easy deployment across diverse distributed environments. The framework dynamically optimizes detector allocation per node based on available resources and data types, enabling rapid adaptation to new operational scenarios with minimal computational overhead. We first conducted a comprehensive literature review to identify key characteristics of existing CIDS architectures. Based on these insights and real-world use cases, we developed our CIDS framework, which we evaluated using several distributed datasets that feature different attack chains and network topologies. Notably, we introduce a public dataset based on a realistic cyberattack targeting a ground drone aimed at sabotaging critical infrastructure. Experimental results demonstrate that the proposed CIDS framework can achieve adaptive, efficient intrusion detection in distributed settings, automatically reconfiguring detectors to maintain an optimal configuration, without requiring heavy computation, since all experiments were conducted on edge devices.

</details>


### [6] [Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy](https://arxiv.org/abs/2602.11897)
*Andrei Kojukhov,Arkady Bovshover*

Main category: cs.CR

TL;DR: 현대 AI 기반 사이버 보안 시스템은 주로 모델 중심 탐지 및 자동화 파이프라인으로 설계되며, 이는 정확성 및 응답 지연과 같은 작업 수준 성능 지표에 최적화되어 있습니다. 하지만 이러한 구조는 적대적 불확실성 하에서 책임 있는 의사 결정을 지원하는 데 어려움을 겪습니다. 따라서 본 논문은 사이버 보안 조정을 에이전트 중심의 다중 에이전트 인지 시스템으로 재구성해야 한다고 주장합니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안 시스템이 적대적 불확실성 아래에서도 책임감 있는 의사 결정을 지원할 필요성이 있습니다.

Method: 이 논문에서는 탐지, 가설 형성, 맥락 해석, 설명, 거버넌스를 담당하는 이종 AI 에이전트들이 명시적인 메타 인지 판단 기능을 통해 조정되는 개념적 아키텍처 프레임워크를 소개합니다.

Result: 모던 보안 작전은 명시적 조직 원칙 없이도 분산 인지 시스템처럼 작동하고 있음을 보여줍니다.

Conclusion: 제안된 프레임워크는 사이버 보안에서 AI의 초점을 고립된 예측 최적화에서 불확실성 하의 자율성 관리로 전환합니다.

Abstract: Contemporary AI-driven cybersecurity systems are predominantly architected as model-centric detection and automation pipelines optimized for task-level performance metrics such as accuracy and response latency. While effective for bounded classification tasks, these architectures struggle to support accountable decision-making under adversarial uncertainty, where actions must be justified, governed, and aligned with organizational and regulatory constraints. This paper argues that cybersecurity orchestration should be reconceptualized as an agentic, multi-agent cognitive system, rather than a linear sequence of detection and response components. We introduce a conceptual architectural framework in which heterogeneous AI agents responsible for detection, hypothesis formation, contextual interpretation, explanation, and governance are coordinated through an explicit meta-cognitive judgement function. This function governs decision readiness and dynamically calibrates system autonomy when evidence is incomplete, conflicting, or operationally risky. By synthesizing distributed cognition theory, multi-agent systems research, and responsible AI governance frameworks, we demonstrate that modern security operations already function as distributed cognitive systems, albeit without an explicit organizing principle. Our contribution is to make this cognitive structure architecturally explicit and governable by embedding meta-cognitive judgement as a first-class system function. We discuss implications for security operations centers, accountable autonomy, and the design of next-generation AI-enabled cyber defence architectures. The proposed framework shifts the focus of AI in cybersecurity from optimizing isolated predictions to governing autonomy under uncertainty.

</details>


### [7] [MalTool: Malicious Tool Attacks on LLM Agents](https://arxiv.org/abs/2602.12194)
*Yuepeng Hu,Yuqi Jia,Mengyuan Li,Dawn Song,Neil Gong*

Main category: cs.CR

TL;DR: 이 논문은 악의적인 도구 코드 구현에 대한 체계적인 연구를 통해, 코드 구현에 내장된 악의적인 행동을 탐색하여 악성 도구의 위험성을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구는 사용자와 LLM 에이전트에 의해 악의적인 도구의 설치 및 선택 가능성을 높이는 데 초점을 맞췄지만, 악의적인 행동이 코딩 구현에 내장되는 문제는 충분히 연구되지 않았다.

Method: 우리는 LLM 에이전트 설정에 맞춘 악의적인 도구 행동의 분류 체계를 제안하고, 코딩 LLM을 이용해 악의적인 행동을 보여주는 도구를 자동으로 생성하는 MalTool 프레임워크를 개발하였다.

Result: MalTool을 통해 1,200개의 독립적인 악성 도구와 5,287개의 실제 악의적 행동이 내장된 도구 데이터셋을 구축했으며, 안전성을 고려한 코딩 LLM에도 불구하고 높은 효과iveness를 보였다.

Conclusion: 기존의 악성코드 탐지 방법이 한계가 있음을 밝혀내어 새로운 방어 수단의 필요성을 강조했다.

Abstract: In a malicious tool attack, an attacker uploads a malicious tool to a distribution platform; once a user installs the tool and the LLM agent selects it during task execution, the tool can compromise the user's security and privacy. Prior work primarily focuses on manipulating tool names and descriptions to increase the likelihood of installation by users and selection by LLM agents. However, a successful attack also requires embedding malicious behaviors in the tool's code implementation, which remains largely unexplored.
  In this work, we bridge this gap by presenting the first systematic study of malicious tool code implementations. We first propose a taxonomy of malicious tool behaviors based on the confidentiality-integrity-availability triad, tailored to LLM-agent settings. To investigate the severity of the risks posed by attackers exploiting coding LLMs to automatically generate malicious tools, we develop MalTool, a coding-LLM-based framework that synthesizes tools exhibiting specified malicious behaviors, either as standalone tools or embedded within otherwise benign implementations. To ensure functional correctness and structural diversity, MalTool leverages an automated verifier that validates whether generated tools exhibit the intended malicious behaviors and differ sufficiently from prior instances, iteratively refining generations until success. Our evaluation demonstrates that MalTool is highly effective even when coding LLMs are safety-aligned. Using MalTool, we construct two datasets of malicious tools: 1,200 standalone malicious tools and 5,287 real-world tools with embedded malicious behaviors. We further show that existing detection methods, including commercial malware detection approaches such as VirusTotal and methods tailored to the LLM-agent setting, exhibit limited effectiveness at detecting the malicious tools, highlighting an urgent need for new defenses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates](https://arxiv.org/abs/2602.11301)
*John M. Willis*

Main category: cs.AI

TL;DR: PBSAI는 기업의 AI 시스템 보안을 위해 다중 에이전트 참조 아키텍처를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기업들이 AI 시스템을 통합하여 보안과 위험 관리를 효과적으로 처리할 필요성이 증가하고 있습니다.

Method: PBSAI는 12개 도메인으로 책임을 조직하고, 도구와 정책 간의 매개 역할을 하는 경계 있는 에이전트 가족을 정의합니다.

Result: PBSAI는 NIST AI RMF 기능과 일치하며, 기업 SOC 및 방대한 방어 환경에서의 적용을 입증합니다.

Conclusion: PBSAI는 개방형 생태계 개발과 향후 경험적 검증을 위한 구조화된 증거 중심의 기초로 제안됩니다.

Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.
  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.

</details>


### [9] [AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition](https://arxiv.org/abs/2602.11348)
*Ruipeng Wang,Yuxin Chen,Yukai Wang,Chang Wu,Junfeng Fang,Xiaodong Cai,Qi Gu,Hui Su,An Zhang,Xiang Wang,Xunliang Cai,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 대형 언어 모델의 발전 덕분에 LLM 기반 에이전트가 여러 벤치마크에서 좋은 성능을 달성할 수 있게 되었지만, 실제 환경에서는 성능이 떨어진다. 이를 해결하기 위해 AgentNoiseBench라는 프레임워크를 도입하여 소음 환경에서 에이전트 모델의 강인성을 평가하고, 사용자 소음과 도구 소음으로 환경 소음을 분류한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 실제 배치에서의 성능이 벤치마크에서의 성능과 일치하지 않는 문제를 해결하고자 한다.

Method: 제어 가능한 소음을 기존 에이전트 중심의 벤치마크에 주입하는 자동화 파이프라인을 개발하고, 이를 통해 다양한 아키텍처와 매개변수 규모의 모델을 평가한다.

Result: 다양한 소음 조건에서 현재의 에이전트 모델이 현실적 환경의 혼란에 얼마나 민감한지 보여주는 성능 변화를 도출하였다.

Conclusion: 에이전트 모델은 현실적인 환경의 변화에 민감하며, 따라서 강인성을 평가하는 새로운 프레임워크가 필요하다.

Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.

</details>


### [10] [Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization](https://arxiv.org/abs/2602.11437)
*Chengrui Qu,Christopher Yeh,Kishan Panaganti,Eric Mazumdar,Adam Wierman*

Main category: cs.AI

TL;DR: 이 논문은 분포적으로 강건한 개별-전역-최대 원칙을 적용해 다중 에이전트 강화 학습의 성능을 개선하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 실제 환경에서 다중 에이전트 강화 학습의 신뢰성을 높이기 위한 필요성

Method: 분포적으로 강건한 IGM(DrIGM) 원칙을 도입하여 각 에이전트의 강건한 탐욕 행동이 팀의 최적 공동 행동과 일치하도록 요구하는 방법.

Result: DrIGM은 분산 탐욕 실행과 호환되는 강건한 개별 행동 가치의 새로운 정의로 성립하며, 전체 시스템에 대한 강건성을 보장한다.

Conclusion: 기존 가치 분해 아키텍처의 DrIGM 준수 강건한 변형을 도출하고, 높은 충실도의 SustainGym 시뮬레이터와 StarCraft 게임 환경에서 성능을 지속적으로 개선하는 결과를 보였다.

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.

</details>


### [11] [Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization](https://arxiv.org/abs/2602.11351)
*Yihang Yao,Zhepeng Cen,Haohong Lin,Shiqi Liu,Zuxin Liu,Jiacheng Zhu,Zhang-Wei Hong,Laixi Shi,Ding Zhao*

Main category: cs.AI

TL;DR: BAO는 능동적인 RL 프레임워크로, 사용자 의도에 맞게 효율적인 상호작용과 행동 향상을 통해 능동적인 대화 에이전트를 훈련하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 능동적 LLM 에이전트가 실제 사용자 중심 애플리케이션에서 중요한 역할을 하도록 훈련할 필요가 있습니다.

Method: BAO는 행동 향상과 행동 규제를 결합하여 사용자 기대에 부합하도록 에이전트의 행동을 조정합니다.

Result: BAO는 UserRL 벤치마크의 여러 작업에서 기존 능동적인 RL 기준을 능가하는 성능을 보여줍니다.

Conclusion: BAO는 복잡한 다중 턴 시나리오에서 능동적이고 사용자 aligned LLM 에이전트를 훈련하는 데 효과적입니다.

Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.

</details>


### [12] [Multi UAVs Preflight Planning in a Shared and Dynamic Airspace](https://arxiv.org/abs/2602.12055)
*Amath Sow,Mauricio Rodriguez Cesen,Fabiola Martins Campos de Oliveira,Mariusz Wzorek,Daniel de Leng,Mattias Tiger,Fredrik Heintz,Christian Esteve Rothenberg*

Main category: cs.AI

TL;DR: DTAPP-IICR라는 새로운 프레임워크는 동적이고 공유된 공역에서 대규모 무인항공기(UAV) 군집의 비행 전 계획 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 UAV 플릿의 비행 전 계획은 동적이고 공유된 공역에서의 시간적 금지 구역과 다양한 비행기 특성, 엄격한 배송 마감일 등으로 인해 상당한 도전을 제시합니다.

Method: 우리는 긴급도를 기반으로 미션의 우선 순위를 정하고, SFIPP-ST라는 새로운 4D 단일 에이전트 플래너를 사용하여 왕복 경로를 계산하는 방식을 제안합니다. 그런 다음 기하학적 갈등 그래프에 의해 안내되는 반복적인 대규모 이웃 탐색을 통해 잔여 갈등을 효율적으로 해결합니다.

Result: DTAPP-IICR는 시간적 금지 구역이 있는 벤치마크에서 최대 1,000대의 UAV로 거의 100% 성공률을 달성하고, 가지치기를 통해 최대 50%의 실행 시간 단축을 달성하며, UTM 맥락에서 강화된 갈등 기반 탐색보다 우수한 성과를 보입니다.

Conclusion: DTAPP-IICR는 현실적인 도시 규모 운영에서 성공적으로 확장 가능하며, 대체 우선 기반 방법이 중간 배치에서도 실패하는 상황에서도 유용하고 확장 가능한 비행 전 계획 솔루션으로 자리 잡고 있습니다.

Abstract: Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.

</details>


### [13] [ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences](https://arxiv.org/abs/2602.11354)
*Bang Nguyen,Dominik Soós,Qian Ma,Rochana R. Obadage,Zack Ranjan,Sai Koneru,Timothy M. Errington,Shakhlo Nematova,Sarah Rajtmajer,Jian Wu,Meng Jiang*

Main category: cs.AI

TL;DR: AI 에이전트를 위한 자동화된 과학 논문 평가에 대한 관심이 높아지고 있으며, 기존 벤치마크는 연구 결과의 재현 가능성에 초점을 맞추고 있다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트를 활용한 과학 논문 자동 평가의 필요성이 증가하고 있음.

Method: ReplicatorBench라는 종합 벤치마크를 도입하고, 연구 재현의 세 가지 단계에서 AI 에이전트를 평가.

Result: 현재 LLM 에이전트는 실험을 효과적으로 설계하고 실행할 수 있지만, 재현에 필요한 리소스 검색에서는 어려움을 겪고 있음.

Conclusion: AI 에이전트가 인류 재현자의 활동을 모방할 수 있는 능력의 중요성을 강조하며, 필요한 도구를 갖춘 ReplicatorAgent가 개발됨.

Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxed environments, to accomplish tasks in ReplicatorBench. We evaluate ReplicatorAgent across four underlying large language models (LLMs), as well as different design choices of programming language and levels of code access. Our findings reveal that while current LLM agents are capable of effectively designing and executing computational experiments, they struggle with retrieving resources, such as new data, necessary to replicate a claim. All code and data are publicly available at https://github.com/CenterForOpenScience/llm-benchmarking.

</details>


### [14] [Causal-JEPA: Learning World Models through Object-Level Latent Interventions](https://arxiv.org/abs/2602.11389)
*Heejeong Nam,Quentin Le Lidec,Lucas Maes,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: C-JEPA는 객체 중심의 세계 모델로, 상호작용 의존 동역학을 포착하여 예측, 추론 및 제어를 지원하는 모델이다.


<details>
  <summary>Details</summary>
Motivation: 강력한 관계적 이해가 예측, 추론 및 제어를 지원하기 위해 필요하다.

Method: C-JEPA는 이미지 패치에서 객체 중심 표현으로 마스크된 공동 임베딩 예측을 확장하는 간단하고 유연한 객체 중심 세계 모델이다.

Result: C-JEPA는 비주얼 질의 응답에서 일관된 성과를 보이며, 특히 객체 수준 마스킹이 없는 동일한 아키텍처와 비교하여 약 20	ext{%}의 절대적 개선을 이룬다.

Conclusion: 객체 수준 마스킹이 잠재적 개입을 통해 인과적 유도 편향을 유도한다는 형식을 제공한다.

Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.

</details>


### [15] [TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning](https://arxiv.org/abs/2602.11409)
*Sina Tayebati,Divake Kumar,Nastaran Darabi,Davide Ettori,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.AI

TL;DR: TRACER는 듀얼-컨트롤 툴-에이전트-사용자 상호작용을 위한 궤적 수준의 불확실성 메트릭을 제시하며, 복잡한 대화형 도구 사용 환경에서 불확실성을 더 정확하게 탐지하는 데 기여합니다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계에서 AI 에이전트의 다중 턴 도구 사용 상호작용에서 불확실성을 추정하는 것은 어려우며, 실패는 종종 드문 결정적 에피소드에 의해 촉발됩니다.

Method: TRACER는 내용 인식 서프라이벌과 상황 인식 신호, 의미적 및 어휘적 반복, 도구 기반 일관성의 공백을 결합하여, MAX-복합적 단계 위험 함수를 사용하여 결정적인 이상 현상을 드러냅니다.

Result: TRACER는 $τ^2$-벤치에서 작업 실패 및 선택적 작업 실행을 예측하여 기준선 대비 AUROC를 최대 37.1% 향상시키고 AUARC를 최대 55% 향상시켰습니다.

Conclusion: 복잡한 대화형 도구 사용 환경에서 불확실성을 더 빠르고 정확하게 탐지할 수 있게 해줍니다.

Abstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $τ^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.

</details>


### [16] [AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.11510)
*Faouzi El Yagoubi,Ranwa Al Mallah,Godwin Badu-Marfo*

Main category: cs.AI

TL;DR: 다중 에이전트 대형 언어 모델은 전달되는 민감한 데이터로 인해 새로운 프라이버시 리스크를 초래하고, 이를 측정할 수 있는 기존 벤치마크가 부족하다. AgentLeak라는 새로운 벤치마크를 도입하고, 다양한 모델을 테스트한 결과, 다중 에이전트 구성에서 출력 누출이 줄어드는 반면 내부 채널의 감시 부족으로 전체 시스템 노출이 증가함을 확인하였다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 민감한 데이터의 유출 리스크가 존재하지만, 이를 측정하는 기준이 부족하다.

Method: AgentLeak라는 벤치마크를 제시하고, 의료, 금융, 법률, 기업 도메인에서 1,000개의 시나리오를 통해 다중 에이전트 모델의 누출을 분석하였다.

Result: 다중 에이전트 구성에서 출력 결과 누출은 줄어들지만, 내부 채널로 인한 시스템 노출이 증가하며, 모든 모델에서 에이전트 간 통신이 주요 취약점임이 확인되었다.

Conclusion: 내부 채널에 대한 프라이버시 보호를 통합한 조정 프레임워크의 필요성이 강조된다.

Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.

</details>


### [17] [CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference](https://arxiv.org/abs/2602.11527)
*Jiawei Zhu,Wei Chen,Ruichu Cai*

Main category: cs.AI

TL;DR: CausalAgent는 자연어 상호작용을 통해 인과 추론을 자동화하는 대화형 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 인과 분석 워크플로우는 통계와 컴퓨터 과학의 이중 배경을 요구하며, 연구자들이 알고리즘을 수동으로 선택하고 데이터 품질 문제를 처리하며 복잡한 결과를 해석해야 하는 기술적 장벽이 존재한다.

Method: CausalAgent는 다중 에이전트 시스템(MAS), 검색 증강 생성(RAG) 및 모델 컨텍스트 프로토콜(MCP)을 통합하여 데이터 정리, 인과 구조 학습, 편향 수정 및 보고서 생성을 자동화한다.

Result: 사용자는 데이터셋을 업로드하고 자연어로 질문을 제기하여 rigorously하고 인터랙티브한 분석 보고서를 받는다.

Conclusion: CausalAgent는 사용자 중심의 AI 협업 패러다임으로, 분석 워크플로우를 명확히 모델링하고 인과 분석에 대한 진입 장벽을 크게 낮추면서 과정의 엄격성과 해석 가능성을 보장한다.

Abstract: Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.

</details>


### [18] [Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use](https://arxiv.org/abs/2602.11541)
*Hanbing Liu,Chunhao Tian,Nan An,Ziyuan Wang,Pinyan Lu,Changyuan Yu,Qi Qi*

Main category: cs.AI

TL;DR: 예산 제약을 받는 도구 보강 에이전트를 연구하며, 큰 언어 모델이 엄격한 예산 하에 외부 도구를 호출하여 다단계 작업을 해결하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 예산 제약 내에서 외부 도구를 활용하여 다단계 작업을 해결하는 에이전트를 개발하고자 한다.

Method: 의도 인식 계층적 세계 모델을 활용한 추론 시 계획 프레임워크인 INTENT를 제안한다.

Result: INTENT는 예산의 엄격한 충족을 보장하면서도 기존 기법들에 비해 작업 성공률을 크게 향상시키며, 도구 가격 변화 및 변동 예산과 같은 동적 시장 변화에 견고하다.

Conclusion: 이 연구는 예산 제약 하에 도구 사용 예측 및 의사결정을 개선하며, 다단계 작업 해결에 효과적임을 보였다.

Abstract: We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.

</details>


### [19] [Learning to Configure Agentic AI Systems](https://arxiv.org/abs/2602.11574)
*Aditya Taparia,Som Sagar,Ransalu Senanayake*

Main category: cs.AI

TL;DR: 이 논문에서는 LLM 기반 에이전트 시스템 구성을 위한 효율적인 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트 시스템 구성이 많은 조합 디자인 공간에서 워크플로우, 도구, 토큰 예산 및 프롬프트를 선택하는 것을 포함하며, 이는 일반적으로 고정된 대형 템플릿이나 수작업 조정된 휴리스틱에 의해 처리된다. 이는 동일한 비효율적인 구성이 쉬운 입력 쿼리와 어려운 입력 쿼리 모두에 적용되기 때문에 불안정한 행동과 불필요한 컴퓨팅을 초래한다.

Method: 에이전트 구성을 쿼리 기준 의사결정 문제로 정의하고, 강화 학습을 통해 이러한 구성을 동적으로 조정하기 위한 경량 계층적 정책을 학습하는 ARC(Agentic Resource & Configuration learner)를 소개한다.

Result: 여러 벤치마크에서, 학습된 정책은 강력한 수작업 설계 및 기타 기준을 지속적으로 능가하며, 최대 25% 더 높은 작업 정확도와 함께 토큰 및 실행 시간을 줄인다.

Conclusion: 쿼리별 에이전트 구성 학습은 "모두를 위한 단일 사이즈" 디자인에 대한 강력한 대안임을 보여준다.

Abstract: Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to "one size fits all" designs.

</details>


### [20] [The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs](https://arxiv.org/abs/2602.11583)
*Jingdi Chen,Hanqing Yang,Zongjun Liu,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트의 통신 및 협력의 중요성과 이를 위한 다양한 접근 방식을 검토한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트가 동적인 환경에서 협력하여 의사결정을 하도록 지원하기 때문이다.

Method: 다중 에이전트 통신(MA-Comm)의 발전을 다섯 가지 질문(누가, 누구와, 무엇을, 언제, 왜)에 따라 정리한다.

Result: 이러한 접근 방식들은 통신 설계에 대한 상이한 선택들이 어떤 영향을 미치는지를 강조한다.

Conclusion: 앞으로의 하이브리드 시스템 개발을 위한 실용적인 설계 패턴과 도전 과제를 제시한다.

Abstract: Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.

</details>


### [21] [When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents](https://arxiv.org/abs/2602.11619)
*Aman Mehta*

Main category: cs.AI

TL;DR: LLM 에이전트를 동일한 작업에 대해 두 번 실행했을 때, 같은 행동을 보이지 않는 경우가 많다는 연구 결과를 제시.


<details>
  <summary>Details</summary>
Motivation: 동일한 LLM 에이전트를 같은 작업에서 두 번 실행했을 때, 행동이 일관되지 않은 이유를 탐구하기 위해.

Method: HotpotQA 데이터셋에 대해 Llama 3.1 70B, GPT-4o, Claude Sonnet 4.5의 세 가지 모델에서 3,000번의 에이전트 실행을 분석했다.

Result: ReAct 스타일 에이전트는 동일한 입력에도 평균적으로 10회 실행당 2.0-4.2개의 서로 다른 행동 시퀀스를 생성하였으며, 일관된 행동을 보이는 작업은 80-92%의 정확도를, 높은 불일치를 보이는 작업은 25-60%의 정확도를 기록했다.

Conclusion: 실행 중 행동 일관성을 모니터링하면 조기 오류 탐지 및 에이전트의 신뢰성을 향상시킬 수 있다.

Abstract: Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.

</details>


### [22] [PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics](https://arxiv.org/abs/2602.11666)
*E Fan,Lisong Shi,Zhengtong Li,Chih-yung Wen*

Main category: cs.AI

TL;DR: CFD를 위한 자율 에이전트를 배치하는 데 있어 LLM의 확률적 특성이 제한적이며, 이를 해결하기 위해 PhyNiKCE라는 새로운 신경기호 에이전틱 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: CFD 시뮬레이션에서 물리 기반의 보존 법칙과 수치적 안정성을 요구하는데 LLM이 이를 준수하지 못하는 문제를 해결하고자 합니다.

Method: PhyNiKCE는 시뮬레이션 설정을 제약 만족 문제로 처리하는 상징적 지식 엔진을 사용하여 신경 계획과 상징적 검증을 분리합니다.

Result: PhyNiKCE는 OpenFOAM 실험에서 96% 상대적 개선을 보여주었으며, 자율 자가 수정 루프를 59% 줄이고 LLM 토큰 소비를 17% 감소시켰습니다.

Conclusion: 이 아키텍처는 CFD에서 검증되었지만, 더 넓은 산업 자동화에서 신뢰할 수 있는 인공지능을 위한 확장 가능하고 감사 가능한 패러다임을 제공합니다.

Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to "context poisoning," where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.

</details>


### [23] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 기계 학습 시스템이 잘못된 이유로 높은 성능을 달성하는 문제를 다루며, 이와 관련된 Rung Collapse와 Aleatoric Entrenchment를 설명하고, 에피스템적 후회 최소화(ERM) 방법론을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습 시스템이 이유가 잘못될 때 좋은 성능을 보이는 문제를 해결하고자 한다.

Method: 에피스템적 후회 최소화(ERM)라는 목표를 제안하며, 이를 지식 표현에 기초한 세 가지 기여와 함께 세 층 구조에 내장한다.

Result: 1,360개의 인과 덫 시나리오에서 Rung Collapse가 지속하며, ERM 피드백이 53-59%의 고착된 오류를 회복함을 보여준다.

Conclusion: 기계 학습 모델의 성능을 높이기 위해 선형적인 피드백 접근 방식을 벗어나야 하며, 독립적인 오류를 처리할 수 있는 메커니즘이 필요하다.

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [24] [Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]](https://arxiv.org/abs/2602.11745)
*Songlin Lyu,Lujie Ban,Zihang Wu,Tianqi Luo,Jirong Liu,Chenhao Ma,Yuyu Luo,Nan Tang,Shipeng Qi,Heng Lin,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 이 논문에서는 고품질 벤치마크 데이터세트와 평가 방법의 부족으로 인해 Text-to-GQL 시스템의 발전이 저해되고 있음을 설명하고, 이를 해결하기 위해 Text2GQL-Bench라는 통합된 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: Text-to-GQL 시스템의 발전은 고품질 벤치마크 데이터세트와 평가 방법의 부족으로 저해되고 있다.

Method: Text2GQL-Bench는 13개의 도메인에 걸쳐 178,184개의 (질문, 쿼리) 쌍을 포함하는 다중 GQL 데이터세트와 다양한 도메인, 질문 추상화 수준 및 GQL을 생성하는 확장 가능한 구축 프레임워크로 구성된다.

Result: 강력한 LLM조차도 제로 샷 설정에서 최대 4%의 실행 정확도(EX)를 달성하며, 3샷 프롬프트를 고정하면 정확도가 약 50%로 상승하지만, 문법적 유효성은 70% 미만으로 남아 있다. 또한, 미세 조정된 8B 개방형 모델은 45.1% EX와 90.8% 문법적 유효성을 달성하여 성능 향상이 충분한 ISO-GQL 예제 노출에 의해 이루어짐을 보여준다.

Conclusion: Text2GQL-Bench는 다양한 도메인과 언어에 대해 Text-to-GQL 시스템의 성능을 종합적으로 평가하는 데 필수적인 자원으로 자리매김할 것이다.

Abstract: Graph models are fundamental to data analysis in domains rich with complex relationships. Text-to-Graph-Query-Language (Text-to-GQL) systems act as a translator, converting natural language into executable graph queries. This capability allows Large Language Models (LLMs) to directly analyze and manipulate graph data, posi-tioning them as powerful agent infrastructures for Graph Database Management System (GDBMS). Despite recent progress, existing datasets are often limited in domain coverage, supported graph query languages, or evaluation scope. The advancement of Text-to-GQL systems is hindered by the lack of high-quality benchmark datasets and evaluation methods to systematically compare model capabilities across different graph query languages and domains. In this work, we present Text2GQL-Bench, a unified Text-to-GQL benchmark designed to address these limitations. Text2GQL-Bench couples a multi-GQL dataset that has 178,184 (Question, Query) pairs spanning 13 domains, with a scalable construction framework that generates datasets in different domains, question abstraction levels, and GQLs with heterogeneous resources. To support compre-hensive assessment, we introduce an evaluation method that goes beyond a single end-to-end metric by jointly reporting grammatical validity, similarity, semantic alignment, and execution accuracy. Our evaluation uncovers a stark dialect gap in ISO-GQL generation: even strong LLMs achieve only at most 4% execution accuracy (EX) in zero-shot settings, though a fixed 3-shot prompt raises accuracy to around 50%, the grammatical validity remains lower than 70%. Moreover, a fine-tuned 8B open-weight model reaches 45.1% EX, and 90.8% grammatical validity, demonstrating that most of the performance jump is unlocked by exposure to sufficient ISO-GQL examples.

</details>


### [25] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: 본 논문은 LLM 에이전트 시스템을 위한 최초의 사고 대응 프레임워크인 AIR를 소개하며, 이 시스템이 사고 대응 생애 주기를 자율적으로 관리하도록 설계되었음을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 현재 LLM 에이전트의 안전 메커니즘은 실패를 미리 방지하는 데 집중하고 있으며, 사고 발생 이후의 대응 및 회복 능력은 제한적이다.

Method: AIR는 LLM 에이전트 시스템의 사고 대응 생애 주기를 자율적으로 관리하기 위해 도메인 특화 언어를 정의하고, 이를 에이전트의 실행 루프에 통합하여 사고를 감지하고, 격리 및 회복 작업을 실행하며, 유사한 사고를 방지하기 위한 가드레일 규칙을 합성한다.

Result: AIR는 세 가지 대표 에이전트 유형에서 90% 이상의 성공률을 달성하며, 사고 감지, 수정 및 근절을 성공적으로 수행한다.

Conclusion: 사고 대응은 에이전트 안전을 개선하기 위한 1급 메커니즘으로서 실행 가능하고 필수적임을 보여준다.

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [26] [TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents](https://arxiv.org/abs/2602.11767)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Holger Boche*

Main category: cs.AI

TL;DR: 이 논문은 기존 프레임워크와 선택 방법을 보완하는 강화 학습 에이전트 학습을 개선하기 위해 TSR이라는 새로운 기법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 발전으로 강화 학습을 활용한 다중 턴 상호작용 에이전트를 훈련시키려는 필요성이 커지고 있습니다.

Method: TSR(Trajectory-Search Rollouts)은 테스트 단계의 스케일링 아이디어를 재사용하여 각 턴에서 고득점 행동을 선택하고 고품질 궤적을 구성하기 위해 경량 트리 스타일의 검색을 수행합니다.

Result: Sokoban, FrozenLake, WebShop 작업에서 15%까지 성능 향상을 달성하였고, 훈련 계산량을 한 번 증가시켜 더 안정적인 학습을 제공합니다.

Conclusion: TSR은 추론 단계에서 훈련의 롤아웃 단계로 검색을 이동시킴으로써 더 강력한 다중 턴 에이전트 학습을 위한 간단하고 일반적인 메커니즘을 제공합니다.

Abstract: Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.

</details>


### [27] [Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation](https://arxiv.org/abs/2602.11790)
*Lingyong Yan,Jiulong Wu,Dong Xie,Weixian Shi,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: LAVES는 교육 문제에서 고품질 교육 비디오를 생성하는 계층적 LLM 기반 다중 에이전트 시스템으로, 정확한 논리적 추론과 지식 표현이 필요한 교육 미디어 제작의 한계를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 최근의 영상 생성 모델들은 시각적 콘텐츠 제작에서 인상적인 성과를 보여주지만, 교육 및 강의 미디어처럼 엄격한 논리적 rigor와 정확한 지식 표현이 요구되는 시나리오에서는 한계가 있다.

Method: LAVES는 교육 비디오 생성을 다중 목표 작업으로 구성하여 올바른 단계별 추론, 교육적으로 일관된 내레이션, 의미적으로 정확한 시각적 시연, 정밀한 오디오-비주얼 정렬을 동시에 요구한다. 생성 워크플로우를 전문 에이전트들로 분해하고 중앙 조정 에이전트가 품질 게이트 및 반복적 비판 메커니즘으로 조정한다.

Result: LAVES는 하루에 백만 개 이상의 비디오를 생성할 수 있으며, 현재 산업 표준 방법과 비교해 95% 이상의 비용 절감을 달성하고 높은 수용률을 유지한다.

Conclusion: 이 시스템은 모든 출력을 의미론적 비판, 규칙 기반 제약 및 도구 기반 컴파일 검사를 거치게 하여 수동 편집 없이 완전 자동화된 종단 간 제작을 가능하게 한다.

Abstract: Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

</details>


### [28] [Intelligent AI Delegation](https://arxiv.org/abs/2602.11865)
*Nenad Tomašev,Matija Franklin,Simon Osindero*

Main category: cs.AI

TL;DR: AI 에이전트가 복잡한 작업을 보다 잘 처리하기 위한 적응형 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트는 점점 더 복잡한 작업을 수행할 수 있어야 하며, 이를 위해 문제를 관리 가능한 하위 구성 요소로 의미 있게 분해하고, AI 에이전트와 인간에게 안전하게 위임할 수 있어야 한다.

Method: 적응형 프레임워크를 통해 작업 할당, 권한 이전, 책임, 역할 및 경계에 대한 명확한 사양, 의도에 대한 명확성, 신뢰를 구축하는 메커니즘을 포함하는 일련의 결정을 내린다.

Result: 제안된 프레임워크는 복잡한 위임 네트워크에서 인간과 AI 모두에게 적용될 수 있다.

Conclusion: 이 연구는 새로운 에이전트적 웹에서 프로토콜 개발을 알리는 것을 목표로 한다.

Abstract: AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.

</details>


### [29] [AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution](https://arxiv.org/abs/2602.11917)
*Taian Guo,Haiyang Shen,Junyu Luo,Binqi Chen,Hongjun Ding,Jinsheng Huang,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 이 논문에서는 알파 요인 탐색의 한계를 극복하기 위해 AlphaPROBE라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 정량적 금융에서 알파 요인 탐색은 중요한 도전 과제이며 기존 방법들은 전통적인 패러다임의 한계가 있습니다.

Method: AlphaPROBE는 알파 요인 탐색을 방향 비순환 그래프(DAG)를 통한 전략적 탐색으로 재구성하고, Bayesian Factor Retriever와 DAG-aware Factor Generator를 포함합니다.

Result: 세 개의 주요 중국 주식 시장 데이터셋에 대한 광범위한 실험 결과, AlphaPROBE는 예측 정확도, 수익 안정성 및 훈련 효율성에서 향상된 성능을 보여 주었습니다.

Conclusion: 글로벌 진화 토폴로지를 활용하는 것이 효율적이고 탄탄한 자동 알파 탐색에 필수적임을 확인했습니다.

Abstract: Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.

</details>


### [30] [MEME: Modeling the Evolutionary Modes of Financial Markets](https://arxiv.org/abs/2602.11918)
*Taian Guo,Haiyang Shen,Junyu Luo,Zhongshi Xing,Hanchun Lian,Jinsheng Huang,Binqi Chen,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 본 연구에서는 금융 시장을 동적이고 진화하는 생태계로 모델링하는 Logic-Oriented 관점을 제안하며, MEME라는 모델을 통해 시장 역학을 재구성하는 방법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: LLM은 방대한 비정형 데이터를 처리하여 인간과 유사한 분석 워크플로우를 모방할 수 있는 잠재력을 보여주고 있지만, 기존 LLM 기반 방법은 주식 예측이나 포트폴리오 할당에 국한되어 있습니다.

Method: MEME는 시장 역학을 진화하는 논리의 관점에서 재구성하기 위해 다중 에이전트 추출 모듈과 가우시안 혼합 모델링을 사용합니다.

Result: 2023년부터 2025년까지 세 가지 이질적인 중국 주식 풀에 대한 실험 결과, MEME는 일곱 개의 최신 기술(SOTA) 기준을 지속적으로 초과 수행했습니다.

Conclusion: MEME는 금융 시장의 진화하는 합의를 식별하고 적응하는 능력을 검증받았습니다.

Abstract: LLMs have demonstrated significant potential in quantitative finance by processing vast unstructured data to emulate human-like analytical workflows. However, current LLM-based methods primarily follow either an Asset-Centric paradigm focused on individual stock prediction or a Market-Centric approach for portfolio allocation, often remaining agnostic to the underlying reasoning that drives market movements. In this paper, we propose a Logic-Oriented perspective, modeling the financial market as a dynamic, evolutionary ecosystem of competing investment narratives, termed Modes of Thought. To operationalize this view, we introduce MEME (Modeling the Evolutionary Modes of Financial Markets), designed to reconstruct market dynamics through the lens of evolving logics. MEME employs a multi-agent extraction module to transform noisy data into high-fidelity Investment Arguments and utilizes Gaussian Mixture Modeling to uncover latent consensus within a semantic space. To model semantic drift among different market conditions, we also implement a temporal evaluation and alignment mechanism to track the lifecycle and historical profitability of these modes. By prioritizing enduring market wisdom over transient anomalies, MEME ensures that portfolio construction is guided by robust reasoning. Extensive experiments on three heterogeneous Chinese stock pools from 2023 to 2025 demonstrate that MEME consistently outperforms seven SOTA baselines. Further ablation studies, sensitivity analysis, lifecycle case study and cost analysis validate MEME's capacity to identify and adapt to the evolving consensus of financial markets. Our implementation can be found at https://github.com/gta0804/MEME.

</details>


### [31] [Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments](https://arxiv.org/abs/2602.11964)
*Romain Froger,Pierre Andrews,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Gerard Moreno-Torres Bertran,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Vladislav Vorotilov,Mengjue Wang,Ian Yu,Amine Benhalloum,Grégoire Mialon,Thomas Scialom*

Main category: cs.AI

TL;DR: Gaia2는 현실적이고 비동기적인 환경에서 대형 언어 모델 에이전트를 평가하는 벤치마크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 이 논문의 동기는 에이전트의 행동과 무관하게 독립적으로 진화하는 환경에서 대형 언어 모델 에이전트를 평가할 수 있는 새로운 벤치마크의 필요성이다.

Method: Gaia2는 에이전트가 시간 제약 하에서 작업하고 잡음과 동적 사건에 적응하며 모호성을 해결하고 다른 에이전트와 협력해야 하는 시나리오를 도입한다. 각 시나리오는 행동 검증기와 쌍으로 제공되어 세분화된 행동 수준 평가를 가능하게 한다.

Result: 최신 상용 및 오픈소스 모델의 평가 결과, 어떤 모델도 모든 능력에서 우위를 점하지 못했다. GPT-5는 전체 점수가 42% pass@1으로 가장 높은 점수를 기록했지만 시간 민감 작업에서 실패했으며, Claude-4 Sonnet은 비용을 위해 정확성과 속도의 균형을 맞추었고, Kimi-K2는 21% pass@1로 오픈소스 모델 중에서 가장 앞선 성과를 보였다.

Conclusion: 이 결과는 추론, 효율성, 견고성 간의 기본적인 트레이드오프를 강조하며 '시뮬레이션에서 현실'로의 격차를 해소하는 데 있어 도전을 드러낸다. Gaia2는 오픈소스 Agents Research Environments 플랫폼을 기반으로 한 소비자 환경에서 구축되었으며 확장성이 용이하도록 설계되었다.

Abstract: We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the "sim2real" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.

</details>


### [32] [LawThinker: A Deep Research Legal Agent in Dynamic Environments](https://arxiv.org/abs/2602.12056)
*Xinyu Yang,Chenlong Deng,Tongyu Wen,Binyu Xie,Zhicheng Dou*

Main category: cs.AI

TL;DR: LawThinker라는 자율 법률 연구 에이전트가 중간 추론 단계의 검증을 통해 법적 추론의 정확성을 향상시키는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 법적 추론 개발 시 결과의 정확성뿐만 아니라 절차적으로 준수한 추론 과정을 요구한다는 필요성이 있다.

Method: Explore-Verify-Memorize 전략을 채택하여 각 지식 탐색 단계 후 검증을 원자적 작업으로 수행하는 LawThinker를 제안한다.

Result: LawThinker는 동적 벤치마크 J1-EVAL에서 직접 추론 대비 24% 향상과 워크플로 기반 방법 대비 11% 개선을 보였다.

Conclusion: 이 모델은 정적 벤치마크에서의 평가를 통해 일반화 능력을 더욱 확인하였다.

Abstract: Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .

</details>


### [33] [Tiny Recursive Reasoning with Mamba-2 Attention Hybrid](https://arxiv.org/abs/2602.12078)
*Wenlong Wang,Fergal Reid*

Main category: cs.AI

TL;DR: Mamba-2 하이브리드 연산자는 TRM의 변환기 블록을 대체하여 재귀적 추론에서의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 소규모 네트워크가 재귀적 추론 작업에서 강력한 성능을 발휘하는 이유를 규명하고, Mamba-2의 도입이 이 능력을 유지하는지 확인하기 위함입니다.

Method: TRM의 변환기 블록을 Mamba-2 하이브리드 연산자로 교체하면서 매개변수의 균형을 유지했습니다.

Result: Mamba-2 하이브리드가 ARC-AGI-1에서 pass@2를 +2.0% 향상시켰고, 높은 K 값에서 일관되게 우수한 성능을 보였습니다.

Conclusion: Mamba-2 하이브리드 연산자는 재귀적 구조 내에서 추론 능력을 유지함을 입증하며, 재귀적 연산자 설계에 있어 SSM 기반 연산자의 가능성을 제시합니다.

Abstract: Recent work on recursive reasoning models like TRM demonstrates that tiny networks (7M parameters) can achieve strong performance on abstract reasoning tasks through latent recursion -- iterative refinement in hidden representation space without emitting intermediate tokens. This raises a natural question about operator choice: Mamba-2's state space recurrence is itself a form of iterative refinement, making it a natural candidate for recursive reasoning -- but does introducing Mamba-2 into the recursive scaffold preserve reasoning capability? We investigate this by replacing the Transformer blocks in TRM with Mamba-2 hybrid operators while maintaining parameter parity (6.83M vs 6.86M parameters). On ARC-AGI-1, we find that the hybrid improves pass@2 (the official metric) by +2.0\% (45.88\% vs 43.88\%) and consistently outperforms at higher K values (+4.75\% at pass@100), whilst maintaining pass@1 parity. This suggests improved candidate coverage -- the model generates correct solutions more reliably -- with similar top-1 selection. Our results validate that Mamba-2 hybrid operators preserve reasoning capability within the recursive scaffold, establishing SSM-based operators as viable candidates in the recursive operator design space and taking a first step towards understanding the best mixing strategies for recursive reasoning.

</details>


### [34] [Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication](https://arxiv.org/abs/2602.12083)
*Antonin Sulc*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 AI 시스템의 디버깅을 위한 차별 가능한 모달 논리를 설명한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 AI 시스템의 발전과 함께, 의미적 실패를 디버깅하기 위해서는 지식, 신념, 인과 관계 및 의무에 대한 추론이 필요하다.

Method: 차별 가능한 모달 논리(DML)를 통해 신뢰 네트워크, 인과 사슬 및 규제 경계를 행동 데이터로부터 학습할 수 있는 모달 논리 신경망(MLNNs)을 구현하였다.

Result: 각 모달리티는 구체적인 다중 에이전트 시나리오에서 보여주었으며, 논리적 모순이 학습 가능한 최적화 목표로 어떻게 변환되는지를 완전하게 구현하였다.

Conclusion: 우리는 신뢰와 인과 관계가 명시적인 매개변수로 포함된 해석 가능한 학습 구조, 희소 데이터로 학습을 유도하는 차별 가능한 공리, 다양한 제약을 조합하는 구성적 다중 모달 추론 등의 주요 기여를 제시한다.

Abstract: As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.
  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.

</details>


### [35] [The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context](https://arxiv.org/abs/2602.12108)
*Xiaoyuan Liu,Tian Liang,Dongyang Ma,Deyu Zhou,Haitao Mi,Pinjia He,Yan Wang*

Main category: cs.AI

TL;DR: StateLM은 내부 추론 루프를 갖춘 새로운 기초 모델로, 메모리 도구를 스스로 활용하여 모델 상태를 관리할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 모델이 수동적으로 문맥을 수용하는 것을 넘어서, 능동적으로 자신의 문맥을 관리할 수 있어야 한다는 필요.

Method: 우리는 StateLM이라는 새로운 클래스의 기초 모델을 도입하여 메모리 도구를 활용하고 이를 능동적으로 관리하도록 훈련시킨다.

Result: StateLM은 다양한 시나리오에서 표준 LLM보다 우수한 성능을 보였으며, 특히 긴 문서 QA 작업에서 10%에서 20%의 정확도 향상을 보였다.

Conclusion: 우리의 접근법은 LLM을 수동적 예측자에서 상태 인식 에이젼트로 전환하며, 추론이 상태를 가진 관리 가능한 프로세스가 된다.

Abstract: In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the "wand" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.

</details>


### [36] [STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction](https://arxiv.org/abs/2602.12143)
*Xiaoxiao Wang,Chunxiao Li,Junying Wang,Yijin Guo,Zijian Chen,Chunyi Li,Xiaohong Liu,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: STAR는 데이터 기반 통계 기대와 지식 기반 행위 추론을 연결하며, 모델 성능 예측을 개선하기 위한 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 포괄적인 대형 모델 평가가 매우 비용이 많이 드는 상황에서 제한된 관측으로부터 모델 성능 예측이 필수적이다.

Method: STAR는 전문 검색기를 활용하여 외부 지식을 수집하고, 제약된 확률 행렬 분해(CPMF)에 의미론적 특징을 포함시켜 불확실성을 가진 통계적 기대를 생성한다.

Result: STAR는 점수 기반 및 순위 기반 메트릭 모두에서 모든 기준선을 일관되게 초과하며, 극도의 희소성 하에서 가장 강력한 통계 방법에 비해 총 점수가 14.46% 향상된다.

Conclusion: STAR는 1~2개의 관찰 점수만으로도 신뢰할 수 있는 예측 조정을 제공한다.

Abstract: As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.

</details>


### [37] [Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning](https://arxiv.org/abs/2602.12146)
*Mahdi Khodabandeh,Ghazal Shabani,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.AI

TL;DR: 본 연구는 T5 언어 모델 아키텍처에 적용된 강화 학습을 활용한 새로운 무손실 압축 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 저장 비용과 전송 오버헤드를 최소화하면서 데이터 무결성을 보호하기 위해 효율적인 무손실 압축이 필수적이다.

Method: 우리 방법은 데이터의 토큰 시퀀스로 압축할 수 있도록 강화 학습을 적용한 T5 언어 모델 아키텍처를 활용한다.

Result: 전통적인 방법에 비해 압축 비율에서 상당한 개선을 보인다.

Conclusion: 언어 모델의 잠재적 정보를 활용함으로써 명시적인 콘텐츠 이해 없이도 데이터를 효과적으로 압축하는 강력하고 실용적인 압축 솔루션으로 나아간다.

Abstract: Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.

</details>


### [38] [Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision](https://arxiv.org/abs/2602.12164)
*Xiaohan He,Shiyang Feng,Songtao Huang,Lei Bai,Bin Wang,Bo Zhang*

Main category: cs.AI

TL;DR: Sci-CoE는 두 단계로 구성된 과학적 공동 발전 프레임워크로, 모델이 스스로 해결사와 검증자가 되도록 하여 복잡한 과학적 추론 능력을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: LLM은 뛰어난 추론 능력을 보였으나, 과학적 추론 과제에서 신뢰할 수 없는 해결책 평가와 제한된 검증 전략의 다양성으로 인해 여전히 취약함.

Method: Sci-CoE는 두 단계로 구성된 과학적 공동 발전 프레임워크로, 첫 단계에서 소량의 주석이 있는 데이터를 사용하여 검증자를 위한 근본적인 정확성 판단 기준을 설정하고, 두 번째 단계에서는 합의, 신뢰성 및 다양성을 고려하는 기하학적 보상 메커니즘을 도입하여 라벨이 없는 데이터에서 대규모 자기 반복을 유도합니다.

Result: 여러 일반 과학 벤치마크에서 Sci-CoE가 복잡한 추론 능력을 향상시키고 강한 확장성을 나타내어 보다 견고하고 다양한 평가 시스템 구축을 촉진함을 보여줍니다.

Conclusion: Sci-CoE는 과학적 추론 과제에서 LLM의 성능을 향상시키고 스스로 학습할 수 있는 기틀을 마련합니다.

Abstract: Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.

</details>


### [39] [Think like a Scientist: Physics-guided LLM Agent for Equation Discovery](https://arxiv.org/abs/2602.12259)
*Jianke Yang,Ohm Venkatachalam,Mohammad Kianezhad,Sharvaree Vadgama,Rose Yu*

Main category: cs.AI

TL;DR: KeplerAgent는 물리학 도구를 사용하여 다단계 추론 과정을 명시적으로 따르는 기계 학습 프레임워크로, 심볼릭 회귀 엔진을 구성하여 물리 방정식을 보다 정확하게 찾는다.


<details>
  <summary>Details</summary>
Motivation: 관찰된 현상을 상징적이고 해석 가능한 공식으로 설명하는 것은 과학의 기본 목표입니다. 최근 대형 언어 모델(LLM)은 상징적 방정식 발견을 위한 유망한 도구로 등장하였습니다.

Method: KeplerAgent라는 에이전트 기반 프레임워크를 도입하여 과학적 추론 과정을 명시적으로 따르며, 물리학 기반 도구를 조정하여 중간 구조를 추출하고 이러한 결과를 사용하여 심볼릭 회귀 엔진을 구성합니다.

Result: KeplerAgent는 물리 방정식 벤치마크에서 LLM 및 전통적인 기준선보다 상당히 높은 심볼릭 정확도와 더 큰 잡음 데이터에 대한 강인성을 달성합니다.

Conclusion: 이 연구는 과학적 추론 과정을 효과적으로 모방하여 LLM 기반 시스템의 단점을 극복할 수 있음을 보여줍니다.

Abstract: Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.

</details>


### [40] [CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use](https://arxiv.org/abs/2602.12268)
*Zhen Zhang,Kaiqiang Song,Xun Wang,Yebowen Hu,Weixiang Yan,Chenyang Zhao,Henry Peng Zou,Haoyun Deng,Sathish Reddy Indurthi,Shujian Liu,Simin Ma,Xiaoyang Wang,Xin Eric Wang,Song Wang*

Main category: cs.AI

TL;DR: CM2는 검증 가능한 보상 대신 체크리스트 보상을 사용하는 강화 학습 프레임워크로, 안정성과 정보성을 균형 있게 유지하며 다중 회전의 도구 사용 에이전트를 최적화한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 현실 세계의 과제를 해결하는 데 많은 관심을 받고 있으나, 강화 학습을 이러한 환경에 적용하는 데 여러 어려움이 있다.

Method: CM2는 각 턴의 의도된 행동을 세밀한 이진 기준으로 분해하고 명확한 증거 및 구조화된 메타데이터를 사용하여 보상을 부여하는 방식이다.

Result: CM2는 감독 하의 미세 조정보다 일관되게 개선되었으며, 여러 벤치마크에서 기존 모델 대비 성과를 개선하였다.

Conclusion: CM2는 검증 가능한 보상에 의존하지 않고 다중 회전 및 다단계 도구 사용 에이전트를 최적화하는 확장 가능한 방법을 제공한다.

Abstract: AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.

</details>


### [41] [Agentic Test-Time Scaling for WebAgents](https://arxiv.org/abs/2602.12276)
*Nicholas Lee,Lutfi Eren Erdogan,Chris Joseph John,Surya Krishnapillai,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.AI

TL;DR: CATTS는 다단계 에이전트를 위한 동적 계산 할당 기법으로, 웹 에이전트에 대한 경험적 연구를 통해 고안되었으며, 성능을 최대 9.1% 향상시키고 효율성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 다단계 작업에서의 테스트 시간 스케일링의 행동을 이해하고 최적의 계산 분배 방법을 찾기 위해.

Method: 웹 에이전트에 대한 추론 시간 스케일링의 경험적 연구를 실시하고, LLM 기반의 중재자를 포함한 강력한 집계 전략을 조사하였다.

Result: CATTS는 결정이 진정으로 논란이 있는 경우에만 계산을 할당하여 성능을 향상시킴을 보여주었다.

Conclusion: CATTS는 효율성 증가와 해석 가능한 결정 규칙을 제공하여 새로운 표준으로 자리잡을 수 있다.

Abstract: Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [42] [Counterfactual Conditional Likelihood Rewards for Multiagent Exploration](https://arxiv.org/abs/2602.11740)
*Ayhan Alp Aydeniz,Robert Loftin,Kagan Tumer*

Main category: cs.MA

TL;DR: CCL 보상은 팀 탐색에 대한 에이전트의 독특한 기여를 측정하여 다중 에이전트 시스템에서 효율적인 탐색을 촉진합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템이 협력 전략을 발견하기 위해서는 효율적인 탐색이 중요하지만, 기존에는 개인 에이전트 수준에서 탐색이 장려되어 중복 행동이 발생하는 경향이 있었습니다.

Method: 본 연구에서는 에이전트의 탐색을 팀 탐색에 대한 독특한 기여로 측정하는 Counterfactual Conditional Likelihood (CCL) 보상을 도입했습니다.

Result: 연속 다중 에이전트 도메인에서 실험한 결과, CCL 보상은 희소 팀 보상을 가진 도메인의 학습을 가속화하고 에이전트 간의 긴밀한 조정이 필요한 작업에서 특히 효과적임을 보여주었습니다.

Conclusion: CCL 보상은 팀의 공동 탐색과 관련하여 유용한 관찰을 강조하여 다중 에이전트 시스템의 탐색 효율성을 개선합니다.

Abstract: Efficient exploration is critical for multiagent systems to discover coordinated strategies, particularly in open-ended domains such as search and rescue or planetary surveying. However, when exploration is encouraged only at the individual agent level, it often leads to redundancy, as agents act without awareness of how their teammates are exploring. In this work, we introduce Counterfactual Conditional Likelihood (CCL) rewards, which score each agent's exploration by isolating its unique contribution to team exploration. Unlike prior methods that reward agents solely for the novelty of their individual observations, CCL emphasizes observations that are informative with respect to the joint exploration of the team. Experiments in continuous multiagent domains show that CCL rewards accelerate learning for domains with sparse team rewards, where most joint actions yield zero rewards, and are particularly effective in tasks that require tight coordination among agents.

</details>


### [43] [Cooperation Breakdown in LLM Agents Under Communication Delays](https://arxiv.org/abs/2602.11754)
*Keita Nishimoto,Kimitaka Asatani,Ichiro Sakata*

Main category: cs.MA

TL;DR: LLM 기반 다중 에이전트 시스템의 협력과 조정을 위한 FLCOA 프레임워크를 제안하고, 통신 지연이 협력에 미치는 영향을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 실제 환경에서의 계산 및 통신 제약 속에서 AI 에이전트의 협력과 조정이 필수적이다.

Method: 연속적인 죄수의 딜레마와 통신 지연을 도입하여 LLM 기반 에이전트로 시뮬레이션을 수행한다.

Result: 지연이 증가함에 따라 에이전트는 느린 응답을 악용하기 시작하며, 지나치게 긴 지연은 착취 주기를 줄이는 U자형 관계를 나타낸다.

Conclusion: 협력을 촉진하려면 고차원적인 제도적 설계뿐만 아니라 통신 지연과 자원 배분 같은 하위 요인에도 주목해야 한다.

Abstract: LLM-based multi-agent systems (LLM-MAS), in which autonomous AI agents cooperate to solve tasks, are gaining increasing attention. For such systems to be deployed in society, agents must be able to establish cooperation and coordination under real-world computational and communication constraints. We propose the FLCOA framework (Five Layers for Cooperation/Coordination among Autonomous Agents) to conceptualize how cooperation and coordination emerge in groups of autonomous agents, and highlight that the influence of lower-layer factors - especially computational and communication resources - has been largely overlooked. To examine the effect of communication delay, we introduce a Continuous Prisoner's Dilemma with Communication Delay and conduct simulations with LLM-based agents. As delay increases, agents begin to exploit slower responses even without explicit instructions. Interestingly, excessive delay reduces cycles of exploitation, yielding a U-shaped relationship between delay magnitude and mutual cooperation. These results suggest that fostering cooperation requires attention not only to high-level institutional design but also to lower-layer factors such as communication delay and resource allocation, pointing to new directions for MAS research.

</details>


### [44] [Multi-Defender Single-Attacker Perimeter Defense Game on a Cylinder: Special Case in which the Attacker Starts at the Boundary](https://arxiv.org/abs/2602.11977)
*Michael Otte,Roderich Groß*

Main category: cs.MA

TL;DR: 이 논문은 원통형에서 진행되는 다중 에이전트 경계 방어 게임을 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 경계 방어의 전략을 이해하고 강화하는 것이 필요하다.

Method: n명의 느리게 이동하는 방어자가 단일 빠르게 이동하는 공격자를 방어 경계를 넘지 못하게 해야 한다.

Result: 특정 조건에서 공격자가 승리하는 데 필요한 조건을 설명한다.

Conclusion: 공격자가 방어자가 현재 방어하고 있는 경계 근처에서 시작할 때 발생하는 상황을 분석했다.

Abstract: We describe a multi-agent perimeter defense game played on a cylinder. A team of n slow-moving defenders must prevent a single fast-moving attacker from crossing the boundary of a defensive perimeter. We describe the conditions necessary for the attacker to win in the special case that the intruder starts close to the boundary and in a region that is currently defended.

</details>


### [45] [DEpiABS: Differentiable Epidemic Agent-Based Simulator](https://arxiv.org/abs/2602.12102)
*Zhijian Gao,Shuxin Li,Bo An*

Main category: cs.MA

TL;DR: DEpiABS는 전염병 모델링을 위한 신뢰할 수 있고 데이터 효율적인 프레임워크로, 개인 수준의 동질성과 전염병 프로세스를 모델링하면서도 계산 효율성을 보장합니다.


<details>
  <summary>Details</summary>
Motivation: COVID-19 팬데믹은 기존 전염병 시뮬레이션 도구의 한계를 드러냈습니다.

Method: DEpiABS는 기계적 세부 사항, 계산 효율성 및 해석 가능성을 모두 균형 있게 갖춘 확장 가능하고 미분 가능한 에이전트 기반 모델입니다.

Result: DEpiABS는 COVID-19와 독감 데이터의 민감도 분석과 보정 과정을 통해 검증되었으며, 예측 오류가 최근 감소했습니다.

Conclusion: 이러한 개선은 보조 데이터에 의존하지 않고 이루어져, 미래 전염병 대응 모델링을 위한 신뢰할 수 있고 일반화 가능한 데이터 효율적인 프레임워크임을 증명합니다.

Abstract: The COVID-19 pandemic highlighted the limitations of existing epidemic simulation tools. These tools provide information that guides non-pharmaceutical interventions (NPIs), yet many struggle to capture complex dynamics while remaining computationally practical and interpretable. We introduce DEpiABS, a scalable, differentiable agent-based model (DABM) that balances mechanistic detail, computational efficiency and interpretability. DEpiABS captures individual-level heterogeneity in health status, behaviour, and resource constraints, while also modelling epidemic processes like viral mutation and reinfection dynamics. The model is fully differentiable, enabling fast simulation and gradient-based parameter calibration. Building on this foundation, we introduce a z-score-based scaling method that maps small-scale simulations to any real-world population sizes with negligible loss in output granularity, reducing the computational burden when modelling large populations. We validate DEpiABS through sensitivity analysis and calibration to COVID-19 and flu data from ten regions of varying scales. Compared to the baseline, DEpiABS is more detailed, fully interpretable, and has reduced the average normal deviation in forecasting from 0.97 to 0.92 on COVID-19 mortality data and from 0.41 to 0.32 on influenza-like-illness data. Critically, these improvements are achieved without relying on auxiliary data, making DEpiABS a reliable, generalisable, and data-efficient framework for future epidemic response modelling.

</details>


### [46] [Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems](https://arxiv.org/abs/2602.12243)
*Sanket A. Salunkhe,George P. Kontoudis*

Main category: cs.MA

TL;DR: 이 연구에서는 대규모 다중 로봇 네트워크를 위한 새로운 분산 GP 프레임워크인 pxpGP를 소개하며, 이는 기존 분산 GP 방법보다 하이퍼파라미터 추정과 예측 정확도에서 우수한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 로봇 시스템은 계산 및 통신 제약 하에서 복잡한 환경을 모델링하기 위한 확장 가능하고 연합적인 방법이 필요하다.

Method: pxpGP는 희소 변분 추론을 활용하여 로컬 compact 초상 표현을 생성하고, 희소 변분 최적화 계획을 통해 로컬 초상 데이터 세트를 제한하며, 글로벌 스케일링 전방향 비정확 컨센서스 교환 방향 방법(ADMM)을 적응형 매개변수 업데이트 및 워밍업 시작 초기화와 함께 공식화한다.

Result: 합성 및 실제 데이터 세트를 기반으로 한 실험 결과, pxpGP와 그 분산 변형인 dec-pxpGP는 특히 대규모 네트워크에서 하이퍼파라미터 추정 및 예측 정확도에서 기존의 분산 GP 방법보다 우수한 성능을 보여준다.

Conclusion: pxpGP는 대규모 다중 로봇 네트워크를 지원하는 강력한 분산 GP 프레임워크를 제공한다.

Abstract: Multi-robot systems require scalable and federated methods to model complex environments under computational and communication constraints. Gaussian Processes (GPs) offer robust probabilistic modeling, but suffer from cubic computational complexity, limiting their applicability in large-scale deployments. To address this challenge, we introduce the pxpGP, a novel distributed GP framework tailored for both centralized and decentralized large-scale multi-robot networks. Our approach leverages sparse variational inference to generate a local compact pseudo-representation. We introduce a sparse variational optimization scheme that bounds local pseudo-datasets and formulate a global scaled proximal-inexact consensus alternating direction method of multipliers (ADMM) with adaptive parameter updates and warm-start initialization. Experiments on synthetic and real-world datasets demonstrate that pxpGP and its decentralized variant, dec-pxpGP, outperform existing distributed GP methods in hyperparameter estimation and prediction accuracy, particularly in large-scale networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models](https://arxiv.org/abs/2602.11184)
*Zukang Xu,Zhixiong Zhao,Xing Hu,Zhixuan Chen,Dawei Yang*

Main category: cs.LG

TL;DR: KBVQ-MoE는 MoE 기반 대형 언어 모델에 대한 초저비트 양자화를 개선하기 위한 새로운 VQ 프레임워크로, 입력 기반 중복 제거 및 바이어스 보정 출력 안정화를 통합한다.


<details>
  <summary>Details</summary>
Motivation: MoE 모델은 성능을 대폭 향상시키면서도 계산 효율성을 유지하지만, 높은 매개변수 크기와 메모리 요구량이 자원 제한 환경에서의 배포에 주요 도전 과제가 된다.

Method: KBVQ-MoE는 입력 기반 중복 제거와 바이어스 보정 출력 안정화의 두 가지 기술을 통합하여 MoE 기반 LLM을 위한 초저비트 양자화를 향상시킨다.

Result: 다양한 MoE LLM에 대한 실험에서 KBVQ-MoE는 기존 양자화 방법보다 정확도를 훨씬 잘 유지한다. 예를 들어, Qwen1.5-MoE-A2.7B의 3비트 양자화는 68.07의 FP16 기준값에 거의 일치하는 67.99의 평균 정확도를 달성한다.

Conclusion: KBVQ-MoE는 엣지 디바이스 및 자원 제한 플랫폼에서의 효율적인 배치를 위한 가능성을 강조한다.

Abstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.

</details>


### [48] [TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.11187)
*Yubo Hou,Furen Zhuang,Partha Pratim Kundu,Sezin Ata Kircali,Jie Wang,Mihai Dragos Rotaru,Dutta Rahul,Ashish James*

Main category: cs.LG

TL;DR: 전자 제품의 급속한 성장으로 2.5D 집적 회로 채택이 가속화되었고, 효과적인 자동 칩렛 배치가 필수적이다. 기존의 배치 방법은 주로 배선 길이를 최소화하는 데 집중하거나 다중 목표 최적화를 단일 목표로 변환하는 데 한계가 있다. 이를 해결하기 위해 TDPNavigator-Placer라는 새로운 다중 에이전트 강화 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전자 제품의 성장에 따라 더 크고 이질적인 칩렛 집합으로 스케일링할 때 효과적인 자동 칩렛 배치의 필요성이 커졌다.

Method: TDPNavigator-Placer는 칩렛의 열 설계 전력(TDP)을 기반으로 동적으로 배치를 최적화하는 새로운 다중 에이전트 강화 학습 프레임워크이다. 이 방법은 본질적으로 상충하는 목표를 전문화된 에이전트에 명시적으로 할당하며, 각 에이전트는 독특한 보상 메커니즘과 환경적 제약 아래에서 운영된다.

Result: TDPNavigator-Placer는 최신의 방법들보다 상당히 개선된 파레토 프런트를 제공하여 배선 길이와 열 성능 간의 균형 잡힌 트레이드오프를 가능하게 한다.

Conclusion: TDPNavigator-Placer는 배선 길이와 열 성능 간의 상충하는 목표를 효과적으로 관리하여 실질적인 배치 성능을 향상시킨다.

Abstract: The rapid growth of electronics has accelerated the adoption of 2.5D integrated circuits, where effective automated chiplet placement is essential as systems scale to larger and more heterogeneous chiplet assemblies. Existing placement methods typically focus on minimizing wirelength or transforming multi-objective optimization into a single objective through weighted sum, which limits their ability to handle competing design requirements. Wirelength reduction and thermal management are inherently conflicting objectives, making prior approaches inadequate for practical deployment. To address this challenge, we propose TDPNavigator-Placer, a novel multi-agent reinforcement learning framework that dynamically optimizes placement based on chiplet's thermal design power (TDP). This approach explicitly assigns these inherently conflicting objectives to specialized agents, each operating under distinct reward mechanisms and environmental constraints within a unified placement paradigm. Experimental results demonstrate that TDPNavigator-Placer delivers a significantly improved Pareto front over state-of-the-art methods, enabling more balanced trade-offs between wirelength and thermal performance.

</details>


### [49] [The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning](https://arxiv.org/abs/2602.11217)
*Simin Fan,Dimitris Paparas,Natasha Noy,Binbin Xiong,Noveen Sachdeva,Berivan Isik*

Main category: cs.LG

TL;DR: 본 논문은 언어 모델의 사전 훈련에서 감독 하에 미세 조정(SFT)으로의 기능 이전을 조사하여 모델 개발과 데이터 선별에 대한 통찰을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델 기능이 사전 훈련에서 감독 하에 미세 조정(SFT)으로 어떻게 전이되는지를 이해하는 것은 효율적인 모델 개발과 데이터 관리에 필수적입니다.

Method: 우리는 정확도 및 신뢰성 지표를 적용한 다양한 데이터 혼합 및 모델 규모에서의 상관 관계 프로토콜을 통해 이 질문들을 해결합니다.

Result: 실험 결과, 전이의 신뢰성이 기능 범주, 기준, 그리고 규모에 따라 극적으로 변하며 정확도와 신뢰는 서로 다른, 때로는 상반되는 확장 동력을 보입니다.

Conclusion: 이 발견은 사전 훈련 결정과 하위 결과 사이의 복잡한 상호작용을 조명하며, 기준 선택, 데이터 관리 및 효율적인 모델 개발을 위한 실용적인 지침을 제공합니다.

Abstract: Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.

</details>


### [50] [Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT](https://arxiv.org/abs/2602.11220)
*Jiacheng Wang,Ping Jian,Zhen Yang,Zirong Chen,Keren Liao,Zhongbin Guo*

Main category: cs.LG

TL;DR: 본 연구에서는 대규모 언어 모델의 지도학습 미세조정(SFT) 과정에서 발생하는 재학습의 재분포 문제를 해결하기 위해 정책 학습 문제로 데이터 재작성 접근법을 적용하여, QA 스타일 생성 분포와 일치하면서도 다양성을 유지하는 재작성 정책을 학습하였다.


<details>
  <summary>Details</summary>
Motivation: SFT가 불리한 분포 변화에서 파생되는 재학습의 문제를 해결하고자 한다.

Method: 데이터 재작성을 정책 학습 문제로 변환하고, 강화 학습을 통해 최적의 재작성 분포를 학습한다.

Result: 비교적으로 SFT를 통해 비활성화되는 기준 벤치마크에서 평균 12.34%의 망각을 줄이면서 표준 SFT와 유사한 downstream 성과를 달성하였다.

Conclusion: 우리는 데이터 재작성 에이전트를 제안하며, 이를 통해 고품질의 재작성 데이터셋을 생성할 수 있음을 입증하였다.

Abstract: Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .

</details>


### [51] [Evaluating Memory Structure in LLM Agents](https://arxiv.org/abs/2602.11243)
*Alina Shutova,Alexandra Olenina,Ivan Vinogradov,Anton Sinitsin*

Main category: cs.LG

TL;DR: StructMemEval은 리트리벌 증강 LLMs 대 메모리 에이전트의 성능을 비교하는 새로운 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 더 복잡한 메모리 아키텍처를 생성하는 과정에서 LLM 기반 에이전트의 기능을 분석하고 미래 메모리 디자인을 지도하는 것이 점점 더 어려워짐에 따라, 단순한 사실 기억과 복잡한 메모리 계층 구조를 테스트할 필요성이 대두된다.

Method: StructMemEval은 에이전트의 장기 기억 조직 능력을 테스트하기 위한 벤치마크로, 특정 구조에 지식을 조직하여 해결하는 작업들을 모은다.

Result: 초기 실험에서는 단순 리트리벌 증강 LLMs가 이러한 작업에서 어려움을 겪는 반면, 메모리 에이전트는 메모리 조직 방법을 제공받으면 안정적으로 해결할 수 있다.

Conclusion: 현대 LLM들이 메모리 구조를 인식하지 못하는 경우가 많다는 것을 발견했으며, 이는 LLM 교육과 메모리 프레임워크의 미래 개선을 위한 중요한 방향을 강조한다.

Abstract: Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent's ability to organize its long-term memory, not just factual recall. We gather a suite of tasks that humans solve by organizing their knowledge in a specific structure: transaction ledgers, to-do lists, trees and others. Our initial experiments show that simple retrieval-augmented LLMs struggle with these tasks, whereas memory agents can reliably solve them if prompted how to organize their memory. However, we also find that modern LLMs do not always recognize the memory structure when not prompted to do so. This highlights an important direction for future improvements in both LLM training and memory frameworks.

</details>


### [52] [HiFloat4 Format for Language Model Inference](https://arxiv.org/abs/2602.11287)
*Yuanyong Luo,Jing Huang,Yu Cheng,Ziwei Yu,Kaihua Zhang,Kehong Hong,Xinda Ma,Xin Wang,Anping Tong,Guipeng Hu,Yun Xu,Mehran Taghian,Peng Wu,Guanglin Li,Yunke Peng,Tianchi Hu,Minqi Chen,Michael Bi Mi,Hu Liu,Xiping Zhou,Junsong Wang,Qiang Lin,Heng Liao*

Main category: cs.LG

TL;DR: HiFloat4는 딥러닝을 위해 설계된 블록 부동 소수점 데이터 형식으로, 여러 언어 모델에서 기존 NVFP4 형식보다 높은 정확도를 달성하는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝의 효율성을 높이기 위해 최적화된 데이터 형식의 필요성이 있다.

Method: HiFloat4는 64개의 4비트 요소와 32비트의 공유 스케일링 메타데이터를 포함한 블록 부동 소수점 데이터 형식을 사용한다.

Result: HiF4는 다양한 언어 모델과 여러 다운스트림 작업에서 기존의 NVFP4 형식보다 평균적으로 더 높은 정확도를 달성했다.

Conclusion: HiF4는 부동 소수점 데이터 표현 방식을 개선하고 하드웨어 효율성을 크게 향상시킨다.

Abstract: This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.

</details>


### [53] [Can We Really Learn One Representation to Optimize All Rewards?](https://arxiv.org/abs/2602.11399)
*Chongyi Zheng,Royina Karegoudra Jayanth,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 이 논문에서는 강화 학습에서의 전방-후방 표현 학습(FB)을 해명하고, 기존 FB 방법을 개선한 일 단계 전방-후방 표현 학습(one-step FB)을 제안하여 효과성을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습 문제 해결을 위한 적절한 사전 형식에 대한 논의가 커지고 있으며, 사전 처리를 통해 최적의 제어를 가능한 한 많이 배우는 방법에 대한 관심이 증가하고 있습니다.

Method: 본 연구에서는 FB의 존재 조건, 최적화 목표 및 실제 수렴 방식을 명확히 하고, 이를 통해 새로운 일 단계 전방-후방 표현 학습 방법을 제안합니다.

Result: 실험 결과, 제안한 방법이 기존 방법보다 10^5배 작은 오차로 수렴하며, 제로샷 성능이 평균 24% 향상됨을 발견했습니다.

Conclusion: 이번 연구는 강화 학습에 대한 비지도 사전 훈련 방법을 단순화하여 정책 개선의 한 단계를 수행하는 방법을 제공합니다.

Abstract: As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.

</details>


### [54] [Multi-Level Strategic Classification: Incentivizing Improvement through Promotion and Relegation Dynamics](https://arxiv.org/abs/2602.11439)
*Ziyuan Huang,Lina Alkarmi,Mingyan Liu*

Main category: cs.LG

TL;DR: 정략적 분류는 개인이 유리한 결정 결과를 얻기 위해 반응을 조작하는 문제를 연구하며, 저비용의 부정직한 행동으로 이어질 수 있다. 본 연구는 분류기 임계값과 다단계 승강 제도의 설계를 분석하며, 에이전트의 장기적인 전략을 최적화하고 정직한 노력을 유도하는 임계값을 설계할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 정략적 분류의 문제를 해결하여 에이전트가 보다 정직한 행동을 하도록 유도하고자 하는 동기에서 출발하였다.

Method: 다단계 승강 제도 내에서 분류기 임계값과 난이도 진행을 분석하는 모델을 개발하였다.

Result: 에이전트의 최적 장기 전략을 특성화하고, 정직한 노력을 효과적으로 유도하는 임계값 시퀀스를 설계할 수 있음을 입증하였다.

Conclusion: 부드러운 조건 하에서 이 메커니즘이 에이전트가 진정한 개선 노력을 통해 임계값을 초과하는 유일한 방법이 될 수 있음을 증명하였다.

Abstract: Strategic classification studies the problem where self-interested individuals or agents manipulate their response to obtain favorable decision outcomes made by classifiers, typically turning to dishonest actions when they are less costly than genuine efforts. While existing studies on sequential strategic classification primarily focus on optimizing dynamic classifier weights, we depart from these weight-centric approaches by analyzing the design of classifier thresholds and difficulty progression within a multi-level promotion-relegation framework. Our model captures the critical inter-temporal incentives driven by an agent's farsightedness, skill retention, and a leg-up effect where qualification and attainment can be self-reinforcing. We characterize the agent's optimal long-term strategy and demonstrate that a principal can design a sequence of thresholds to effectively incentivize honest effort. Crucially, we prove that under mild conditions, this mechanism enables agents to reach arbitrarily high levels solely through genuine improvement efforts.

</details>


### [55] [A Generic Framework for Fair Consensus Clustering in Streams](https://arxiv.org/abs/2602.11500)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 본 논문에서는 입력 클러스터링이 순차적으로 도착하고 메모리가 제한된 스트리밍 모델에서 공정한 합의 클러스터링을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 애플리케이션에 적합한 공정한 클러스터링 방법론의 필요성.

Method: 로그 수의 입력만 저장하며 스트림을 처리하는 첫 번째 상수 계수 알고리즘을 설계하고, 공정한 클러스터링과 클러스터 적합을 통합하는 새로운 알고리즘 프레임워크를 도입한다.

Result: 평가 기준을 개선한 근사 보장을 제공하며, 오프라인에서도 재사용 가능하다.

Conclusion: 메모리 제약이 있는 상황에서도 공정한 합의 클러스터링이 가능하며, k-중앙값 합의 클러스터링 문제로 확장할 수 있다.

Abstract: Consensus clustering seeks to combine multiple clusterings of the same dataset, potentially derived by considering various non-sensitive attributes by different agents in a multi-agent environment, into a single partitioning that best reflects the overall structure of the underlying dataset. Recent work by Chakraborty et al, introduced a fair variant under proportionate fairness and obtained a constant-factor approximation by naively selecting the best closest fair input clustering; however, their offline approach requires storing all input clusterings, which is prohibitively expensive for most large-scale applications.
  In this paper, we initiate the study of fair consensus clustering in the streaming model, where input clusterings arrive sequentially and memory is limited. We design the first constant-factor algorithm that processes the stream while storing only a logarithmic number of inputs. En route, we introduce a new generic algorithmic framework that integrates closest fair clustering with cluster fitting, yielding improved approximation guarantees not only in the streaming setting but also when revisited offline. Furthermore, the framework is fairness-agnostic: it applies to any fairness definition for which an approximately close fair clustering can be computed efficiently. Finally, we extend our methods to the more general k-median consensus clustering problem.

</details>


### [56] [Adaptive Milestone Reward for GUI Agents](https://arxiv.org/abs/2602.11524)
*Congmin Zheng,Xiaoyun Mo,Xinbei Ma,Qiqiang Lin,Yin Zhao,Jiachen Zhu,Xingyu Lou,Jun Wang,Zhaoxiang Wang,Weiwen Liu,Zhuosheng Zhang,Yong Yu,Weinan Zhang*

Main category: cs.LG

TL;DR: 이 논문에서는 Mobile GUI Agent 훈련을 위한 Adaptive Milestone Reward (ADMIRE) 메커니즘을 제안하며, 성과 개선과 일반화 능력을 입증한다.


<details>
  <summary>Details</summary>
Motivation: 장기 과제에서 발생하는 시간적 신용 할당 문제를 해결할 필요가 있다.

Method: ADMIRE는 성공적인 탐색에서 동적으로 정제된 이정표에 궤적을 고정하여 검증 가능한 적응형 보상 시스템을 구축하고, 비대칭 신용 할당 전략을 통합하여 성공한 궤적의 잡음을 제거하고 실패한 궤적을 지원한다.

Result: ADMIRE는 AndroidWorld의 다양한 기본 모델에서 성공률이 10% 이상 개선됨을 입증하였다.

Conclusion: 이 방법은 강력한 일반화 능력을 보여 다양한 RL 알고리즘과 웹 탐색, 구현된 작업과 같은 이질적인 환경에서도 우수한 성능을 달성한다.

Abstract: Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fidelity but suffers from signal sparsity, while process reward provides dense supervision but remains prone to bias and reward hacking. To resolve this conflict, we propose the Adaptive Milestone Reward (ADMIRE) mechanism. ADMIRE constructs a verifiable, adaptive reward system by anchoring trajectory to milestones, which are dynamically distilled from successful explorations. Crucially, ADMIRE integrates an asymmetric credit assignment strategy that denoises successful trajectories and scaffolds failed trajectories. Extensive experiments demonstrate that ADMIRE consistently yields over 10% absolute improvement in success rate across different base models on AndroidWorld. Moreover, the method exhibits robust generalizability, achieving strong performance across diverse RL algorithms and heterogeneous environments such as web navigation and embodied tasks.

</details>


### [57] [PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models](https://arxiv.org/abs/2602.11530)
*Eunyeong Cho,Jehyeon Bang,Ranggi Hwang,Minsoo Rhu*

Main category: cs.LG

TL;DR: PASCAL은 체계적 스케줄링을 통해 LLM의 추론 및 응답 단계를 구분하여 성능을 향상시키고 TTFT를 줄이는 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 추론 기반 LLM에 있어 Chain-of-Thought(CoT) 추론의 출현은 사용자에게 보이는 출력 지연과 TTFT 증가라는 새로운 서비스 문제를 야기한다.

Method: PASCAL은 추론 단계를 우선시하여 TTFT를 줄이고, 응답 단계 동안 제어된 선점 및 토큰 속도를 사용하여 경험 품질(QoE)을 유지하는 단계 인식 스케줄링 알고리즘이다.

Result: DeepSeek-R1-Distill-Qwen-32B 벤치마크에서 PASCAL은 응답 단계 SLO 달성을 유지하면서 꼬리 TTFT를 최대 72%까지 줄인다.

Conclusion: 추론 기반 LLM 배치에서 단계 인식 스케줄링의 중요성을 입증하였다.

Abstract: The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.

</details>


### [58] [GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks](https://arxiv.org/abs/2602.11629)
*Dongxiao He,Wenxuan Sun,Yongqi Huang,Jitao Zhao,Di Jin*

Main category: cs.LG

TL;DR: 그래프 프롬프트 학습(GPL)은 사전 훈련된 그래프 모델의 하위 작업 적응을 위한 유망한 패러다임으로, 사전 훈련 목표와 하위 작업 간의 불일치를 완화한다. 최근 GPL의 초점이 도메인 내에서 교차 도메인 시나리오로 이동했으며, 이는 실제 세계 응용 프로그램에 가까워졌다. 그러나 이러한 도메인 변화에서 GPL이 여전히 효과적인 이유는 아직 탐구되지 않았다. 우리는 대표적인 GPL 방법들이 교차 도메인 설정에서 두 가지 간단한 기준선과 경쟁적이며, 이는 프롬프트 메커니즘에 대한 더 깊은 이해를 탐구하도록 유도한다. 이론 분석을 제공하여 두 가지 상호 보완적 가지를 함께 활용하면 단독 가지를 사용할 때보다 작은 추정 오류를 생성하는 것을 보여주며, 교차 도메인 GPL이 사전 훈련된 지식과 작업별 적응의 통합으로부터 이익을 얻는 것을 공식적으로 증명한다. 이러한 통찰을 바탕으로 우리는 GP2F를 제안했으며, 이는 두 극단을 명시적으로 수립한 이중 가지 GPL 방법이다. 대규모 실험을 통해 우리 방법이 기존 방법보다 우수함을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 프롬프트 학습 메커니즘에 대한 깊은 이해를 탐구하고, 사전 훈련된 지식과 작업 특정 적응 간의 통합의 이점을 증명한다.

Method: 이중 가지 GPL 방법인 GP2F를 제안하고, 두 가지 극단을 명시적으로 수립하여 적응형 융합을 수행한다.

Result: 우리의 방법이 기존의 방법보다 우수하다는 것을 입증하는 광범위한 실험 결과를 제시한다.

Conclusion: GP2F는 사전 훈련된 지식과 작업 특정 적응의 통합을 통해 교차 도메인에서 효과적으로 작동한다.

Abstract: Graph Prompt Learning (GPL) has recently emerged as a promising paradigm for downstream adaptation of pre-trained graph models, mitigating the misalignment between pre-training objectives and downstream tasks. Recently, the focus of GPL has shifted from in-domain to cross-domain scenarios, which is closer to the real world applications, where the pre-training source and downstream target often differ substantially in data distribution. However, why GPLs remain effective under such domain shifts is still unexplored. Empirically, we observe that representative GPL methods are competitive with two simple baselines in cross-domain settings: full fine-tuning (FT) and linear probing (LP), motivating us to explore a deeper understanding of the prompting mechanism. We provide a theoretical analysis demonstrating that jointly leveraging these two complementary branches yields a smaller estimation error than using either branch alone, formally proving that cross-domain GPL benefits from the integration between pre-trained knowledge and task-specific adaptation. Based on this insight, we propose GP2F, a dual-branch GPL method that explicitly instantiates the two extremes: (1) a frozen branch that retains pre-trained knowledge, and (2) an adapted branch with lightweight adapters for task-specific adaptation. We then perform adaptive fusion under topology constraints via a contrastive loss and a topology-consistent loss. Extensive experiments on cross-domain few-shot node and graph classification demonstrate that our method outperforms existing methods.

</details>


### [59] [TIP: Resisting Gradient Inversion via Targeted Interpretable Perturbation in Federated Learning](https://arxiv.org/abs/2602.11633)
*Jianhua Wang,Yinlin Su*

Main category: cs.LG

TL;DR: 이 논문은 연합 학습에서의 그래디언트 반전 공격을 방지하기 위한 새로운 방어 프레임워크, Targeted Interpretable Perturbation (TIP)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습은 데이터 로컬리티를 유지하면서 협업 모델 훈련을 가능하게 하지만, 그래디언트 교환이 시스템을 그래디언트 반전 공격에 취약하게 만듭니다.

Method: TIP은 모델 해석성과 주파수 도메인 분석을 통합하는 새로운 방어 프레임워크이며, 그래디언트 가중 클래스 활성화 맵핑을 활용하여 중요한 합성곱 채널을 동적으로 식별하고, 선택된 커널을 주파수 도메인으로 변환한 후 고주파 스펙트럼에 선택적으로 교정된 섭동을 주입합니다.

Result: TIP은 세밀한 이미지 재구성을 위한 필수 세부 정보를 파괴하면서 모델 정확도에 중요한 저주파 정보를 유지합니다. 벤치마크 데이터셋에서 TIP은 최신 GIA에 대해 재구성된 이미지를 시각적으로 인식할 수 없도록 하고 글로벌 모델 정확도를 유지합니다.

Conclusion: TIP은 기존 DP 기반 방어보다 개인 정보 보호-유용성 균형과 해석 가능성에서 크게 우수한 성능을 보입니다.

Abstract: Federated Learning (FL) facilitates collaborative model training while preserving data locality; however, the exchange of gradients renders the system vulnerable to Gradient Inversion Attacks (GIAs), allowing adversaries to reconstruct private training data with high fidelity. Existing defenses, such as Differential Privacy (DP), typically employ indiscriminate noise injection across all parameters, which severely degrades model utility and convergence stability. To address those limitation, we proposes Targeted Interpretable Perturbation (TIP), a novel defense framework that integrates model interpretability with frequency domain analysis. Unlike conventional methods that treat parameters uniformly, TIP introduces a dual-targeting strategy. First, leveraging Gradient-weighted Class Activation Mapping (Grad-CAM) to quantify channel sensitivity, we dynamically identify critical convolution channels that encode primary semantic features. Second, we transform these selected kernels into the frequency domain via the Discrete Fourier Transform and selectively inject calibrated perturbations into the high-frequency spectrum. By selectively perturbing high-frequency components, TIP effectively destroys the fine-grained details necessary for image reconstruction while preserving the low-frequency information crucial for model accuracy. Extensive experiments on benchmark datasets demonstrate that TIP renders reconstructed images visually unrecognizable against state-of-the-art GIAs, while maintaining global model accuracy comparable to non-private baselines, significantly outperforming existing DP-based defenses in the privacy-utility trade-off and interpretability. Code is available in https://github.com/2766733506/asldkfjssdf_arxiv

</details>


### [60] [Both Topology and Text Matter: Revisiting LLM-guided Out-of-Distribution Detection on Text-attributed Graphs](https://arxiv.org/abs/2602.11641)
*Yinlin Zhu,Di Wu,Xu Wang,Guocong Quan,Miao Hu*

Main category: cs.LG

TL;DR: 본 논문은 텍스트 속성 그래프에서 외부 패턴을 효과적으로 탐지하기 위해 LG-Plug이라는 전략을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 GNN이 실제 환경의 OOD 데이터에서 과신 및 오류 예측을 자주 보이는 문제를 해결하고자 함.

Method: LG-Plug는 토폴로지와 텍스트 표현을 정렬하여 세밀한 노드 임베딩을 생성하고, 클러스터 기반 LLM 프롬프트를 통해 합의 기반 OOD 노출을 생성한다.

Result: LG-Plug는 기존의 탐지기와 매끄럽게 통합될 수 있는 OOD 노출을 생성하여 ID와 OOD 노드를 분리한다.

Conclusion: 이 방법은 효과적인 탐지 성능을 향상시키는 데 기여할 것으로 기대된다.

Abstract: Text-attributed graphs (TAGs) associate nodes with textual attributes and graph structure, enabling GNNs to jointly model semantic and structural information. While effective on in-distribution (ID) data, GNNs often encounter out-of-distribution (OOD) nodes with unseen textual or structural patterns in real-world settings, leading to overconfident and erroneous predictions in the absence of reliable OOD detection. Early approaches address this issue from a topology-driven perspective, leveraging neighboring structures to mitigate node-level detection bias. However, these methods typically encode node texts as shallow vector features, failing to fully exploit rich semantic information. In contrast, recent LLM-based approaches generate pseudo OOD priors by leveraging textual knowledge, but they suffer from several limitations: (1) a reliability-informativeness imbalance in the synthesized OOD priors, as the generated OOD exposures either deviate from the true OOD semantics, or introduce non-negligible ID noise, all of which offers limited improvement to detection performance; (2) reliance on specialized architectures, which prevents incorporation of the extensive effective topology-level insights that have been empirically validated in prior work. To this end, we propose LG-Plug, an LLM-Guided Plug-and-play strategy for TAG OOD detection tasks. LG-Plug aligns topology and text representations to produce fine-grained node embeddings, then generates consensus-driven OOD exposure via clustered iterative LLM prompting. Moreover, it leverages lightweight in-cluster codebook and heuristic sampling reduce time cost of LLM querying. The resulting OOD exposure serves as a regularization term to separate ID and OOD nodes, enabling seamless integration with existing detectors.

</details>


### [61] [DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels](https://arxiv.org/abs/2602.11715)
*Haolei Bai,Lingcheng Kong,Xueyi Chen,Jianmian Wang,Zhiqiang Tao,Huan Wang*

Main category: cs.LG

TL;DR: Diffusion 대형 언어 모델(dLLMs)은 코드 생성에 적합한 새로운 접근 방식으로, CUDA 커널 생성을 위한 뚜렷한 발전을 보여줍니다. CuKe 데이터셋과 BiC-RL 프레임워크를 활용하여 DICE 모델을 구축하여 CUDA 커널 생성을 최적화했습니다.


<details>
  <summary>Details</summary>
Motivation: CUDA 커널 생성을 위한 대형 언어 모델이 필요하지만, 고품질 훈련 데이터가 부족하고 높은 전문성이 요구됩니다.

Method: CuKe라는 데이터셋과 두 단계의 강화 학습 프레임워크(BiC-RL)를 설계하여 CUDA 커널 생성을 위한 dLLM을 튜닝했습니다.

Result: DICE 모델은 같은 규모의 자가 회귀 및 확산 LLM보다 훨씬 더 뛰어난 성능을 보여주었습니다.

Conclusion: DICE는 CUDA 커널 생성의 새로운 최첨단 기술을 설정했습니다.

Abstract: Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.

</details>


### [62] [Temporal Difference Learning with Constrained Initial Representations](https://arxiv.org/abs/2602.11800)
*Jiafei Lyu,Jingwen Yang,Zhongjian Qiao,Runze Liu,Zeyuan Liu,Deheng Ye,Zongqing Lu,Xiu Li*

Main category: cs.LG

TL;DR: 본 논문에서는 Tanh 함수를 초기 레이어에 도입하여 off-policy 강화 학습 에이전트의 샘플 효율을 개선하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구들은 입력 데이터의 초기 표현을 직접적으로 제한하는 가능성을 간과하고 있으며, 이는 분포 이동 문제를 완화하고 훈련을 안정화하는 데 도움을 줄 수 있습니다.

Method: Tanh 함수를 초기 레이어에 도입하고, 이를 통해 훈련의 수렴 특성을 이론적으로 분석합니다. 또한, 세 가지 구성 요소로 이루어진 Constrained Initial Representations(CIR) 프레임워크를 제시합니다.

Result: CIR은 여러 연속 제어 작업에서 뛰어난 성능을 보이며, 기존 강력한 기준 방법들과 경쟁하거나 이를 초월하는 결과를 나타냅니다.

Conclusion: 이 연구는 Tanh 함수를 사용하여 초기 표현을 제한함으로써 off-policy RL의 샘플 효율성을 개선할 수 있음을 보여줍니다.

Abstract: Recently, there have been numerous attempts to enhance the sample efficiency of off-policy reinforcement learning (RL) agents when interacting with the environment, including architecture improvements and new algorithms. Despite these advances, they overlook the potential of directly constraining the initial representations of the input data, which can intuitively alleviate the distribution shift issue and stabilize training. In this paper, we introduce the Tanh function into the initial layer to fulfill such a constraint. We theoretically unpack the convergence property of the temporal difference learning with the Tanh function under linear function approximation. Motivated by theoretical insights, we present our Constrained Initial Representations framework, tagged CIR, which is made up of three components: (i) the Tanh activation along with normalization methods to stabilize representations; (ii) the skip connection module to provide a linear pathway from the shallow layer to the deep layer; (iii) the convex Q-learning that allows a more flexible value estimate and mitigates potential conservatism. Empirical results show that CIR exhibits strong performance on numerous continuous control tasks, even being competitive or surpassing existing strong baseline methods.

</details>


### [63] [Deep Kernel Fusion for Transformers](https://arxiv.org/abs/2602.11808)
*Zixi Zhang,Zhiwen Mo,Yiren Zhao,Robert Mullins*

Main category: cs.LG

TL;DR: DeepFusionKernel을 제안하여 HBM 트래픽을 줄이고 캐시 재사용을 향상시킴으로써 LLM 추론 속도를 향상시킴.


<details>
  <summary>Details</summary>
Motivation: Agentic LLM 추론은 메모리 대역폭에 의해 제한되고 있기 때문에 이를 최적화할 필요가 있다.

Method: DeepFusionKernel은 깊이 결합된 커널로 HBM 트래픽을 줄이고 캐시 재사용을 향상시킨다.

Result: DeepFusionKernel은 H100에서 13.2%, A100에서 9.7%의 속도 향상을 달성한다.

Conclusion: DeepFusionKernel은 다양한 모델 및 하드웨어 플랫폼에 적응 가능하며, 생성 길이에 대해 일관된 가속을 보장한다.

Abstract: Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.

</details>


### [64] [Towards Sustainable Investment Policies Informed by Opponent Shaping](https://arxiv.org/abs/2602.11829)
*Juan Agustin Duque,Razvan Ciuca,Ayoub Echchahed,Hugo Larochelle,Aaron Courville*

Main category: cs.LG

TL;DR: 이 논문은 InvestESG라는 다중 에이전트 시뮬레이션을 통해 기후 위험 하에서 투자자와 기업 간의 상호작용을 분석하고, 경제적 에이전트들의 학습 프로세스를 전략적으로 형성함으로써 사회적으로 유익한 결과를 도출할 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 기후 변화 문제 해결에는 전 세계적인 협력이 필요하지만, 경제적 행위자들은 종종 집단 복지보다 즉각적인 이익을 우선시합니다.

Method: InvestESG라는 다중 에이전트 시뮬레이션을 통해 개인의 인센티브가 집단 복지와 다르게 작용하는 조건을 수학적으로 나타내고, Advantage Alignment 알고리즘을 적용하여 에이전트의 학습에 영향을 미칩니다.

Result: Advantage Alignment가 사회적 유익한 균형을 선호하도록 학습 동향을 편향화하여 학습 과정을 전략적으로 조정할 수 있음을 보여줍니다.

Conclusion: 경제적 에이전트의 학습 프로세스를 전략적으로 형성하면 더 나은 결과를 이끌어낼 수 있으며, 이는 정책 메커니즘에 대해 시장의 인센티브를 장기 지속 가능 목표와 더 잘 맞출 수 있도록 하는 데 기여할 수 있습니다.

Abstract: Addressing climate change requires global coordination, yet rational economic actors often prioritize immediate gains over collective welfare, resulting in social dilemmas. InvestESG is a recently proposed multi-agent simulation that captures the dynamic interplay between investors and companies under climate risk. We provide a formal characterization of the conditions under which InvestESG exhibits an intertemporal social dilemma, deriving theoretical thresholds at which individual incentives diverge from collective welfare. Building on this, we apply Advantage Alignment, a scalable opponent shaping algorithm shown to be effective in general-sum games, to influence agent learning in InvestESG. We offer theoretical insights into why Advantage Alignment systematically favors socially beneficial equilibria by biasing learning dynamics toward cooperative outcomes. Our results demonstrate that strategically shaping the learning processes of economic agents can result in better outcomes that could inform policy mechanisms to better align market incentives with long-term sustainability goals.

</details>


### [65] [Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret](https://arxiv.org/abs/2602.11995)
*Yifei Jin,Xin Zheng,Lei Guo*

Main category: cs.LG

TL;DR: 이 논문은 비정상적이며 시간에 따라 변동하는 시스템에서 발생하는 데이터 스트림을 처리하기 위해 MLMS 알고리즘을 조사하며, 이 알고리즘이 온라인 학습 애플리케이션에 대한 기초가 될 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대규모 데이터 처리 시나리오에서 복잡한 시스템에서 발생하는 순차적 데이터 스트림의 비정상성은 기존 이론 분석을 어렵게 한다. 이를 해결하기 위해 실시간 업데이트를 가능하게 하는 알고리즘의 필요성이 대두된다.

Method: 이 논문에서는 계산의 단순성과 온라인 처리 능력을 활용하여 적응형 식별 도구로서 Momentum Least Mean Squares (MLMS) 알고리즘을 조사한다.

Result: 다양한 실제 조건 하에서 MLMS의 추적 성능과 후회 경계를 도출하였으며, 비정상적 환경에서도 빠른 적응과 견고한 추적을 성취함을 실험을 통해 입증하였다.

Conclusion: MLMS는 현대 스트리밍 및 온라인 학습 애플리케이션에 대한 유망한 기초를 제공하며, 특히 비정상적인 환경에서 유용성이 강조된다.

Abstract: In large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.

</details>


### [66] [FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client](https://arxiv.org/abs/2602.12014)
*Gongxi Zhu,Hanlin Gu,Lixin Fan,Qiang Yang,Yuxing Han*

Main category: cs.LG

TL;DR: FedGRPO는 소규모 클라이언트 모델의 데이터를 활용하여 대규모 서버 측 기초 모델의 성능을 향상시키는 방법을 제안하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 소규모 클라이언트 모델의 데이터를 활용하여 대규모 서버 모델의 성능을 향상시키기 위한 필요성이 있다.

Method: FedGRPO는 두 개의 모듈로 구성된 프라이버시를 보존하는 프레임워크로, 첫 번째 모듈은 보조 데이터를 이용해 신뢰도 그래프를 구축하여 각 질문에 가장 적합한 클라이언트를 선택하고, 두 번째 모듈은 질문과 해결 rationale를 정책으로 패키징하여 적절한 클라이언트 그룹에 배포한 후 집합적 손실 함수를 통해 스칼라 보상 신호를 집계한다.

Result: 다양한 도메인 작업에 대한 실험 결과, FedGRPO는 기존의 FedFM 기법에 비해 우수한 정확성과 통신 효율성을 보임을 입증하였다.

Conclusion: FedGRPO는 데이터나 모델 업데이트를 대신하여 보상 값을 교환함으로써 개인 정보 위험을 줄이고 통신 비용을 절감하며 이질적인 디바이스 간의 병렬 평가를 가능하게 한다.

Abstract: One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or incur high communication costs and introduce unavoidable privacy risks. We reformulate this problem as a reinforcement learning style evaluation process and propose FedGRPO, a privacy preserving framework comprising two modules. The first module performs competence-based expert selection by building a lightweight confidence graph from auxiliary data to identify the most suitable clients for each question. The second module leverages the "Group Relative" concept from the Group Relative Policy Optimization (GRPO) framework by packaging each question together with its solution rationale into candidate policies, dispatching these policies to a selected subset of expert clients, and aggregating solely the resulting scalar reward signals via a federated group-relative loss function. By exchanging reward values instead of data or model updates, FedGRPO reduces privacy risk and communication overhead while enabling parallel evaluation across heterogeneous devices. Empirical results on diverse domain tasks demonstrate that FedGRPO achieves superior downstream accuracy and communication efficiency compared to conventional FedFMs baselines.

</details>


### [67] [PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving](https://arxiv.org/abs/2602.12029)
*Sunghyeon Woo,Hoseung Kim,Sunghwan Shim,Minjung Jo,Hyunjoon Jeong,Jeongtae Lee,Joonghoon Kim,Sungjae Lee,Baeseong Park,Se Jung Kwon,Dongsoo Lee*

Main category: cs.LG

TL;DR: PrefillShare는 분산된 시스템에서 여러 모델 간의 프리필 단계를 공유할 수 있게 하는 알고리즘으로, 계산과 KV 저장소의 중복을 줄인다.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 시스템은 복잡한 문제 해결을 위해 여러 전문 언어 모델을 조율하지만, 동일한 프롬프트 프리픽스를 반복 처리하여 성능 저하를 초래한다.

Method: PrefillShare는 모델을 프리필 및 디코드 모듈로 분해하고, 프리필 모듈을 고정한 후 디코드 모듈만 미세 조정하는 알고리즘이다.

Result: 이 방법은 다양한 작업에서 전체 미세 조정 정확도를 유지하면서 p95 지연 시간을 4.5배 줄이고, 다중 모델 작업에서 처리량을 3.9배 향상시킨다.

Conclusion: PrefillShare는 여러 작업 및 모델에 대해 성능을 크게 향상시킨다.

Abstract: Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads.

</details>


### [68] [Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL](https://arxiv.org/abs/2602.12087)
*Alfredo Reichlin,Adriano Pacciarelli,Danica Kragic,Miguel Vasco*

Main category: cs.LG

TL;DR: 상태 추정은 강화 학습에서 중요하지만 소음과의 불확실성을 처리하는 데 어려움이 있다. 본 연구는 행동 전환 최소화를 반영하는 구조적 잠재 표현을 학습하는 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 고차원, 멀티모달 및 노이즈가 많은 관측값으로부터 환경의 상태를 추정하는 것은 강화 학습의 근본적인 도전 과제이다.

Method: 구조적 잠재 표현을 학습하고, 다중 센서 모달리티를 적응적으로 통합할 수 있는 멀티모달 잠재 전이 모델과 거리 가중 센서 융합 메커니즘을 도입한다.

Result: 제안된 접근 방식은 다양한 멀티모달 강화 학습 과제에서 드러나며, 센서 노이에 대한 강인성이 개선되고 상태 추정이 향상되었다.

Conclusion: 전이 인식 메트릭 공간을 활용하는 것이 연속적 의사결정에서의 강인한 상태 추정에 대한 원칙적이고 확장 가능한 해결책을 제시한다.

Abstract: Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions required to transition between them. The proposed metric space formulation provides a geometric interpretation of uncertainty without the need for explicit probabilistic modeling. To achieve this, we introduce a multimodal latent transition model and a sensor fusion mechanism based on inverse distance weighting, allowing for the adaptive integration of multiple sensor modalities without prior knowledge of noise distributions. We empirically validate the approach on a range of multimodal RL tasks, demonstrating improved robustness to sensor noise and superior state estimation compared to baseline methods. Our experiments show enhanced performance of an RL agent via the learned representation, eliminating the need of explicit noise augmentation. The presented results suggest that leveraging transition-aware metric spaces provides a principled and scalable solution for robust state estimation in sequential decision-making.

</details>


### [69] [It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks](https://arxiv.org/abs/2602.12147)
*Zhongzheng Qiao,Sheng Pan,Anni Wang,Viktoriya Zhukova,Yong Liu,Xudong Jiang,Qingsong Wen,Mingsheng Long,Ming Jin,Chenghao Liu*

Main category: cs.LG

TL;DR: 본 논문에서는 기존의 시간 시계열 기초 모델(TSFM) 벤치마크의 한계를 극복하기 위해 50개의 새로운 데이터셋과 98개의 예측 작업으로 구성된 TIME 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: TSFM의 발전에도 불구하고 기존 벤치마크에서 데이터 구성, 데이터 무결성, 작업 공식화 및 분석 관점에서의 한계가 발견되었다.

Method: TIME은 50개의 새로운 데이터셋과 98개의 예측 작업으로 구성된 차세대 작업 중심 벤치마크로, 데이터 유출이 없는 엄격한 제로샷 TSFM 평가를 위해 설계되었다.

Result: 12개의 대표적인 TSFM을 평가하고 다차원 리더보드를 구축하여 심층 분석과 시각적 검토를 돕는다.

Conclusion: TIME 벤치마크는 실제 운영 요구 사항과 변동 예측 가능성에 맞춘 예측 구성으로 작업 공식을 재정의하고 모델 기능에 대한 일반화된 통찰을 제공하는 새로운 평가 접근 방식을 제안한다.

Abstract: Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.

</details>


### [70] [Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222)
*Miaosen Zhang,Yishan Liu,Shuxia Lin,Xu Yang,Qi Dai,Chong Luo,Weihao Jiang,Peng Hou,Anxiang Zeng,Xin Geng,Baining Guo*

Main category: cs.LG

TL;DR: 이 논문은 감독된 미세조정(SFT)의 일반화 성능을 향상시키기 위한 온-정책 SFT 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 감독된 미세조정(SFT)은 계산 효율성이 높지만 강화 학습(RL)과 비교했을 때 일반화 성능이 종종 열악하다.

Method: 우리는 데이터와 모델이 유도한 분포 간의 정렬을 설명하고 정량화하는 분포 구별 이론(DDT)을 소개하고, SFT의 일반화 능력을 향상시키기 위해 두 가지 보완 기술(IDFT와 힌트 디코딩)을 도입한다.

Result: 우리의 프레임워크는 DPO 및 SimPO와 같은 저명한 오프라인 RL 알고리즘과 동등한 수준의 일반화 성능을 달성하면서 SFT 파이프라인의 효율성을 유지함을 보여준다.

Conclusion: 따라서 제안된 프레임워크는 RL이 실행 불가능한 도메인에서 실용적인 대안이 된다.

Abstract: Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT

</details>


### [71] [Olmix: A Framework for Data Mixing Throughout LM Development](https://arxiv.org/abs/2602.12237)
*Mayee F. Chen,Tyler Murray,David Heineman,Matt Jordan,Hannaneh Hajishirzi,Christopher Ré,Luca Soldaini,Kyle Lo*

Main category: cs.LG

TL;DR: Olmix는 언어 모델 훈련 시 데이터 믹싱의 두 가지 문제를 해결하는 프레임워크로, 실험적 연구와 혼합물 재사용 메커니즘을 통해 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델 훈련에서 데이터 믹싱은 중요한 문제로, 기존의 믹싱 방법들은 실제 적용 시 한계가 있다.

Method: Olmix는 믹싱 방법의 설계 선택을 이해하고, 도메인 세트가 업데이트될 때 효율적으로 혼합물을 재계산하는 방법을 연구한다.

Result: 다섯 번의 도메인 세트 업데이트를 통해 혼합물 재사용이 각 업데이트 후 재계산하는 것과 동일한 성능을 내면서도 74%의 계산량을 절감하고, 다운스트림 작업에서 비믹싱 훈련보다 11.6% 향상된 성능을 보인다.

Conclusion: Olmix는 언어 모델 훈련 과정에서 데이터 믹싱 문제를 효과적으로 해결할 수 있는 기초를 제공한다.

Abstract: Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.

</details>
