<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 15]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 7]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis](https://arxiv.org/abs/2512.08952)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.LG

TL;DR: 이 논문은 대화형 로봇의 훈련을 위한 가상 시뮬레이터를 개발하여, 하드웨어 부담 없이 심리적 문제(우울증 및 PTSD)에 대한 대화 지원을 개선하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 인간형 로봇을 사용자와 테스트하는 것은 느리고 마모를 초래하며 반복성과 다양성을 제한합니다. 그러나 스크리닝 에이전트는 대화의 타이밍, 음조, 백채널, 얼굴 및 발음의 주의를 기울여야 합니다.

Method: 로봇을 대화형 에이전트로 가상화하고, 인터뷰 데이터를 변환하여 동기화된 언어, 시선/얼굴 및 머리-몸 자세를 가진 276명의 Unreal Engine MetaHuman 환자를 생성합니다. 이 시뮬레이터는 비언어적 역학을 포함한 학습 정책을 사용하고 안전한 학습 루프를 구축하여 타이밍과 관계를 주된 제어 변수로 다룹니다.

Result: TD3(쌍둥이 지연 DDPG) 제어기가 PPO 및 CEM보다 더 높은 성과를 달성하였으며, 점진적인 보상과 안정적인 속도로 거의 완벽한 커버리지를 기록했습니다.

Conclusion: 이 연구는 276명의 대화형 환자를 생성하는 에이전트 중심의 시뮬레이터, 비언어적 반증을 포함한 안전한 학습 루프, TD3와 PPO/CEM의 비교 연구, 임상 감독의 로봇 파일럿을 가능하게 하는 언급된 성과를 포함한 기여를 제공합니다.

Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.

</details>


### [2] [SEA: Spectral Edge Attacks on Graph Neural Networks](https://arxiv.org/abs/2512.08964)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 이 논문에서는 스펙트럼 강인성 평가를 활용한 새로운 유형의 적대적 공격인 스펙트럼 엣지 공격(SEA)을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: GNN이 그래프 구조 데이터에서 강력한 성능을 내지만, 그래프 구조의 작은 변형에 취약하다는 점에서 출발.

Method: 스펙트럼 임베딩을 계산하여 입력 매니폴드의 가장 취약한 방향을 포착하고, 이를 통해 각 엣지 또는 비엣지에 강인성 점수를 할당하는 방법을 제안합니다.

Result: 스펙트럼 강인성이 높은 엣지를 제거하는 스페이드 유도 삭제 공격과, 취약한 스펙트럼 공간에서 최대한 호환되지 않는 노드들 사이에 엣지를 삽입하는 스페이드 유도 추가 공격을 도입합니다.

Conclusion: 이 공격은 그래프 수준에서 작동하며, 모델을 인지하지만 개념적으로 단순하고 기존 GNN 아키텍처에 그래디언트를 요구하지 않고 통합할 수 있습니다.

Abstract: Graph Neural Networks (GNNs) achieve strong performance on graph-structured data, but are notoriously vulnerable to small, carefully crafted perturbations of the graph structure. Most existing structure-based attacks rely on gradient-based heuristics or local connectivity patterns, and treat edges as equally important candidates for manipulation. In this paper, we propose Spectral Edge Attacks (SEA), a new family of adversarial attacks that explicitly leverage spectral robustness evaluation to guide structural perturbations. Our key idea is to compute a spectral embedding that captures the most fragile directions of the input manifold and to use it to assign a robustness score to each edge or non-edge. Based on these scores, we introduce two complementary attack variants: (i) a Spade-guided deletion attack that removes the most spectrally robust edges, and (ii) a Spade-guided addition attack that inserts edges between nodes that are maximally incompatible in the fragile spectral space. Both attacks operate at the graph level, are model-aware but conceptually simple, and can be plugged into existing GNN architectures without requiring gradients. We describe the spectral formulation, the attack algorithms, and experiments on benchmarks.

</details>


### [3] [Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment](https://arxiv.org/abs/2512.09013)
*Paul Garnier,Pablo Jeken-Rico,Vincent Lannelongue,Chiara Faitini,Aurèle Goetz,Lea Chanvillard,Ramy Nemer,Jonathan Viquerat,Ugo Pelissier,Philippe Meliga,Jacques Sédat,Thomas Liebig,Yves Chau,Elie Hachem*

Main category: cs.LG

TL;DR: 이 연구는 뇌동맥류의 혈류역학을 신속하게 예측할 수 있는 그래프 신경망 대체 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 뇌동맥류의 파열 위험을 정확하게 평가하기 위해, 현재의 전통적인 컴퓨터 유체 역학 시뮬레이션 방식은 비용이 많이 들고 시간이 오래 걸리며 전문적인 기술이 필요합니다.

Method: 우리는 그래프 변환기와 자기 회귀 예측을 결합하여, 환자 맞춤형 동맥류의 복잡한 혈류역학을 1분 이내로 시뮬레이션하는 그래프 신경망 대체 모델을 제안합니다.

Result: 우리 모델은 이전에 보지 못한 환자의 기하학과 유입 조건에서도 일반화되며, 높은 해상도의 혈류 예측을 제공합니다.

Conclusion: 이 모델은 뇌동맥류 분석을 위한 실시간 의사결정 지원 시스템으로 변환하여, 환자 이미징 후 몇 분 내에 해상도 높은 혈류역학 예측을 제공합니다.

Abstract: Intracranial aneurysms remain a major cause of neurological morbidity and mortality worldwide, where rupture risk is tightly coupled to local hemodynamics particularly wall shear stress and oscillatory shear index. Conventional computational fluid dynamics simulations provide accurate insights but are prohibitively slow and require specialized expertise. Clinical imaging alternatives such as 4D Flow MRI offer direct in-vivo measurements, yet their spatial resolution remains insufficient to capture the fine-scale shear patterns that drive endothelial remodeling and rupture risk while being extremely impractical and expensive.
  We present a graph neural network surrogate model that bridges this gap by reproducing full-field hemodynamics directly from vascular geometries in less than one minute per cardiac cycle. Trained on a comprehensive dataset of high-fidelity simulations of patient-specific aneurysms, our architecture combines graph transformers with autoregressive predictions to accurately simulate blood flow, wall shear stress, and oscillatory shear index. The model generalizes across unseen patient geometries and inflow conditions without mesh-specific calibration. Beyond accelerating simulation, our framework establishes the foundation for clinically interpretable hemodynamic prediction. By enabling near real-time inference integrated with existing imaging pipelines, it allows direct comparison with hospital phase-diagram assessments and extends them with physically grounded, high-resolution flow fields.
  This work transforms high-fidelity simulations from an expert-only research tool into a deployable, data-driven decision support system. Our full pipeline delivers high-resolution hemodynamic predictions within minutes of patient imaging, without requiring computational specialists, marking a step-change toward real-time, bedside aneurysm analysis.

</details>


### [4] [Contrast transfer functions help quantify neural network out-of-distribution generalization in HRTEM](https://arxiv.org/abs/2512.09067)
*Luis Rangel DaCosta,Mary C. Scott*

Main category: cs.LG

TL;DR: 신경망의 OOD 일반화 이해는 실험적 워크플로에의 성공적인 배치에 필수적이다. 본 연구에서는 12,000개 이상의 신경망의 OOD 일반화를 조사하고 HRTEM 데이터셋 간 정보를 비교하는 프레임워크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 신경망의 OOD 일반화 이해는 실험적 조건의 변동이 클 때, 그들의 성공적인 배치에 필수적이다.

Method: 고해상도 전송 전자 현미경(HRTEM) 이미징을 위한 신경망 분할 모델을 훈련시키고, 합성 데이터로 12,000개 이상의 신경망의 OOD 일반화를 측정하였다. HRTEM 대비 전이 함수를 활용하여 데이터셋 간 정보 콘텐츠를 비교하고 OOD 도메인 변화를 정량화하는 프레임워크를 발전시켰다.

Result: 신경망 분할 모델은 상당한 성능 안정성을 가지지만, 이미징 조건이 훈련 분포에서 이동함에 따라 성능이 점진적으로 악화됨을 나타냈다.

Conclusion: 본 연구는 원자 구조와 같은 다른 OOD 변화 설명의 한계를 고려하고, 그러한 환경에서 일반화를 이해하기 위한 보완 기법을 논의한다.

Abstract: Neural networks, while effective for tackling many challenging scientific tasks, are not known to perform well out-of-distribution (OOD), i.e., within domains which differ from their training data. Understanding neural network OOD generalization is paramount to their successful deployment in experimental workflows, especially when ground-truth knowledge about the experiment is hard to establish or experimental conditions significantly vary. With inherent access to ground-truth information and fine-grained control of underlying distributions, simulation-based data curation facilitates precise investigation of OOD generalization behavior. Here, we probe generalization with respect to imaging conditions of neural network segmentation models for high-resolution transmission electron microscopy (HRTEM) imaging of nanoparticles, training and measuring the OOD generalization of over 12,000 neural networks using synthetic data generated via random structure sampling and multislice simulation. Using the HRTEM contrast transfer function, we further develop a framework to compare information content of HRTEM datasets and quantify OOD domain shifts. We demonstrate that neural network segmentation models enjoy significant performance stability, but will smoothly and predictably worsen as imaging conditions shift from the training distribution. Lastly, we consider limitations of our approach in explaining other OOD shifts, such as of the atomic structures, and discuss complementary techniques for understanding generalization in such settings.

</details>


### [5] [Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting](https://arxiv.org/abs/2512.09076)
*Moazzam Umer Gondal,Hamad ul Qudous,Asma Ahmad Farhan*

Main category: cs.LG

TL;DR: 도시 대기 오염의 정확한 예측은 공공 건강 보호와 완화 정책을 안내하는 데 필수적이다. 경량 가법 모델인 Facebook Prophet (FBP)과 NeuralProphet (NP)가 베이징의 PM$_{2.5}$ 및 PM$_{10}$ 예측에서 경쟁력 있는 결과를 제공할 수 있는지를 조사하였다.


<details>
  <summary>Details</summary>
Motivation: 도시 대기 오염 예측의 중요성은 공공 건강을 보호하고 완화 정책을 안내하기 위해 필수적이다.

Method: 다년간의 오염물 및 기상 데이터를 사용하여, 상관관계, 상호 정보, mRMR을 이용한 체계적인 특성 선택과 유출 방지 안전 스케일링 및 시간 순서에 따른 데이터 분할을 적용하였다.

Result: FBP가 NP, SARIMAX 및 학습 기반 기준선보다 지속적으로 우수한 성능을 보였으며, 두 오염물 모두에 대해 test $R^2$가 0.94 이상이었다.

Conclusion: 이 연구 결과는 해석 가능한 가법 모델이 전통적인 접근 방식과 복잡한 접근 방식 모두와 경쟁력을 유지함을 보여주며, 정확성, 투명성 및 배치 용이성의 실용적인 균형을 제공한다.

Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.

</details>


### [6] [Natural Geometry of Robust Data Attribution: From Convex Models to Deep Networks](https://arxiv.org/abs/2512.09103)
*Shihao Li,Jiachen Li,Dongmei Chen*

Main category: cs.LG

TL;DR: 이 논문은 모델 예측의 원인 교육 샘플을 식별하는 데이터 귀속 방법이 분포 변화에 민감해 신뢰성을 저해한다는 문제를 해결하기 위해, 강인하게 인증된 귀속을 위한 통합 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 귀속 방법의 신뢰성 문제를 해결하고자 한다.

Method: Convex 모델에서 Wasserstein-Robust Influence Functions(W-RIF)를 도출하고, 딥 네트워크에서 Natural Wasserstein 메트릭을 사용하여 변이성을 측정한다.

Result: CIFAR-10 데이터셋에서 Natural W-TRAK는 68.7%의 순위 쌍을 인증하며, Euclidean 기준의 0%와 비교된다.

Conclusion: Self-Influence 항은 귀속 안정성을 지배하는 Lipschitz 상수와 같으며, 레버리지 기반 이상 탐지 이론적 기반을 제공한다.

Abstract: Data attribution methods identify which training examples are responsible for a model's predictions, but their sensitivity to distributional perturbations undermines practical reliability. We present a unified framework for certified robust attribution that extends from convex models to deep networks. For convex settings, we derive Wasserstein-Robust Influence Functions (W-RIF) with provable coverage guarantees. For deep networks, we demonstrate that Euclidean certification is rendered vacuous by spectral amplification -- a mechanism where the inherent ill-conditioning of deep representations inflates Lipschitz bounds by over $10{,}000\times$. This explains why standard TRAK scores, while accurate point estimates, are geometrically fragile: naive Euclidean robustness analysis yields 0\% certification. Our key contribution is the Natural Wasserstein metric, which measures perturbations in the geometry induced by the model's own feature covariance. This eliminates spectral amplification, reducing worst-case sensitivity by $76\times$ and stabilizing attribution estimates. On CIFAR-10 with ResNet-18, Natural W-TRAK certifies 68.7\% of ranking pairs compared to 0\% for Euclidean baselines -- to our knowledge, the first non-vacuous certified bounds for neural network attribution. Furthermore, we prove that the Self-Influence term arising from our analysis equals the Lipschitz constant governing attribution stability, providing theoretical grounding for leverage-based anomaly detection. Empirically, Self-Influence achieves 0.970 AUROC for label noise detection, identifying 94.1\% of corrupted labels by examining just the top 20\% of training data.

</details>


### [7] [Goal inference with Rao-Blackwellized Particle Filters](https://arxiv.org/abs/2512.09269)
*Yixuan Wang,Dan P. Guralnik,Warren E. Dixon*

Main category: cs.LG

TL;DR: 모바일 에이전트의 목표 추론은 주요한 추정 문제로, 이 논문에서는 Rao-Blackwellized Particle Filter (RBPF)를 변형하여 연구한다.


<details>
  <summary>Details</summary>
Motivation: 모바일 에이전트의 목표를 추론하는 것은 노이즈가 많은 경로 관찰로부터 중요한 문제로, 더 정교한 의도 추론을 위한 발전이 필요하다.

Method: 이 논문에서는 RBPF를 사용하여 에이전트의 동역학이 고정된 형태로 주어졌다는 가정하에, 선형-가우시안 하위 구조를 분석적으로 변별하고 입자 가중치만 업데이트하는 방법을 사용한다.

Result: 우리의 분석은 RBPF 추정으로 인한 KL 다이버전스의 하한과 두 개 추정기 간의 성능 차이를 정량화했다.

Conclusion: 우리의 실험 결과는 일치하는 에이전트에 대한 빠르고 정확한 의도 회복을 보여주며, 향후 의도 은폐 제어기 설계에 대한 연구를 자극한다.

Abstract: Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.

</details>


### [8] [Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices](https://arxiv.org/abs/2512.09313)
*Yuki Oda,Yuta Ono,Hiroshi Nakamura,Hideki Takase*

Main category: cs.LG

TL;DR: Hetero-SplitEE는 이종 IoT 장치들이 협력하여 공유 딥 뉴럴 네트워크를 훈련할 수 있게 하는 새로운 방법입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 분할 학습 접근 방식은 클라이언트의 동질성을 가정하고 있어 이종 장치가 많은 실제 IoT 시스템에는 적용하기 어렵습니다.

Method: Hetero-SplitEE는 이종 조기 종료를 통합한 계층적 훈련 방식을 통해 각 클라이언트가 자신의 계산 용량에 맞춘 고유한 분할 지점을 선택할 수 있게 합니다.

Result: CIFAR-10, CIFAR-100, STL-10 데이터셋에서 우리의 방법은 경쟁력 있는 정확도를 유지하면서 다양한 계산 제약을 효과적으로 지원함을 보여줍니다.

Conclusion: 우리의 접근 방식은 이종 IoT 생태계에서 협력적 딥 러닝의 실용적인 배포를 가능하게 합니다.

Abstract: The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.

</details>


### [9] [KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction](https://arxiv.org/abs/2512.09365)
*Jiayu Qin,Zhengquan Luo,Guy Tadmor,Changyou Chen,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 본 연구는 단백질-분자 상호작용(MPI) 예측을 위한 새로운 접근 방식을 제안하며, 다양한 생물학적 데이터셋을 활용해 고품질의 의사 레이블을 생성하고 MPI 예측 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 단백질-분자 상호작용 예측은 약물 발견과 분자 기능 주석에 중요한 기본 작업이지만, 기존 모델은 레이블이 붙은 데이터의 부족과 생물학적 맥락을 무시하는 두 가지 주요 문제에 직면해 있다.

Method: 우리의 프레임워크는 다양한 생물학적 데이터셋을 집계하고, 최적 수송 기반 접근 방식을 사용하여 알려진 상호작용의 분포를 활용해 레이블을 할당하는 데 있어 고품질의 의사 레이블을 생성한다.

Result: 여러 MPI 데이터셋에서 평가한 결과, 기존 방법들보다 예측 정확도와 제로샷 능력에서 상당한 개선을 보인다.

Conclusion: 우리의 접근 방식은 기존의 단일 또는 이모달 학습의 한계를 극복할 수 있는 새로운 패러다임을 제공하며, 계산 생물학과 약물 발견의 미래 발전을 위해 길을 터준다.

Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.

</details>


### [10] [Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting](https://arxiv.org/abs/2512.09398)
*Hongjun Wang,Jiawei Yong,Jiawei Wang,Shintaro Fukushima,Renhe Jiang*

Main category: cs.LG

TL;DR: 교통 예측은 심층 학습의 발전에도 불구하고 여전히 공간-시간 데이터 마이닝에서 주요 과제로 남아 있으며, 이 논문은 교통 사고 및 규제 데이터를 포함한 두 개의 데이터 세트를 제안하고 새로운 모델 ConFormer를 소개하여 예측 정확도를 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 교통 예측에 있어 외부 요인의 복잡한 영향을 잘 반영하지 못하는 기존 모델의 한계를 극복하고자 함.

Method: 교통 사고 및 규제 데이터를 통합한 두 개의 데이터 세트를 사용하여, 그래프 전파와 가이드된 정규화 계층을 통합한 모델 ConFormer를 제안함.

Result: ConFormer는 기존의 가장 뛰어난 모델인 STAEFormer보다 예측 성능과 효율성에서 뛰어난 결과를 보여줌.

Conclusion: ConFormer는 여러 매트릭스에서 주류 공간-시간 기준선을 지속적으로 초과하며 교통 예측 연구의 발전 가능성을 강조함.

Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.

</details>


### [11] [Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models](https://arxiv.org/abs/2512.09591)
*Magnus Ruud Kjaer,Rahul Thapa,Gauri Ganjoo,Hyatt Moore,Poul Joergen Jennum,Brandon M. Westover,James Zou,Emmanuel Mignot,Bryan He,Andreas Brink-Kjaer*

Main category: cs.LG

TL;DR: 이 논문에서는 수면 분석을 향상시키기 위해 자가 지도 표현 학습(SSRL)을 활용한 스탠포드 수면 벤치를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 수면 분석의 기초 모델 향상을 위해 PSG 데이터의 방대한 양을 활용할 기회가 있지만, 데이터셋과 평가 기준의 부족이 진행을 저해하고 있다.

Method: 스탠포드 수면 벤치라는 대규모 PSG 데이터셋을 소개하고, SSRL 사전 훈련 방법을 체계적으로 평가하여 수면 단계, 무호흡 진단, 나이 추정 및 질병 예측과 같은 여러 작업의 성능을 비교했다.

Result: 여러 사전 훈련 방법들이 수면 단계, 무호흡 진단, 나이 추정에서는 비슷한 성능을 보였지만, 사망 및 질병 예측에서는 대조 학습이 다른 방법보다 현저하게 우수한 성능을 보였다.

Conclusion: 스탠포드 수면 벤치를 비롯한 사전 훈련된 모델 가중치, 훈련 파이프라인 및 평가 코드를 제공함으로써 재현성을 촉진하고 수면 연구를 발전시킬 것이다.

Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.

</details>


### [12] [Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks](https://arxiv.org/abs/2512.09621)
*Jingbo Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen*

Main category: cs.LG

TL;DR: 본 논문은 차세대 차량 통신을 위한 Tripartite Cooperative Semantic Communication 프레임워크를 제안하여 효율적인 엣지 태스크 처리를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 차량 엣지 컴퓨팅과 의미론적 통신을 결합하여 차량 인터넷의 엣지 태스크 처리를 효율화하기 위한 필요성.

Method: V2I 및 V2V 통신을 활용하여 차량 사용자(VU)가 의미론적 태스크 오프로드를 수행할 수 있도록 Tripartite Cooperative Semantic Communication(TCSC) 프레임워크를 제안하며, 이를 위해 혼합 정수 비선형 프로그래밍(MINLP) 문제로 구성된 두 개의 하위 문제를 해결한다.

Result: 제안된 방법(MAPPO-PDN)과 선형 프로그래밍(LP) 사용을 통해 의미론적 심볼 수와 오프로드 비율 최적화를 처리하였으며, 시뮬레이션 결과 본 방법이 다른 알고리즘보다 우수한 성능을 보였다.

Conclusion: 주어진 환경에서 차량 간 및 차량과 인프라 간의 효과적인 의미론적 태스크 오프로드가 가능함을 보여준다.

Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.

</details>


### [13] [Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning](https://arxiv.org/abs/2512.09706)
*Kaichen He,Zihao Wang,Muyao Li,Anji Liu,Yitao Liang*

Main category: cs.LG

TL;DR: CrossAgent는 동적 환경에서 다양한 액션 공간을 처리하고 자율적으로 인터페이스를 선택하는 에이전트 모델로, 800개 이상의 테스트에서 최첨단 성능을 달성했다.


<details>
  <summary>Details</summary>
Motivation: 기존 에이전트는 고정된 작업 공간으로 제한되어 있어 동적 환경에서의 적응력이 부족하다.

Method: CrossAgent는 다양한 액션 공간을 마스터하고, Multi-Turn Group Relative Policy Optimization 알고리즘을 통한 훈련 파이프라인을 통해 학습한다.

Result: CrossAgent는 800개 이상의 작업에서 최첨단 성능을 달성하였다.

Conclusion: 모델이 다양한 액션 공간의 강점을 동적으로 활용함으로써, 고정된 기반 모델을 능가하는 일반화 및 효율성을 보인다.

Abstract: The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA

</details>


### [14] [Knowledge Diversion for Efficient Morphology Control and Policy Transfer](https://arxiv.org/abs/2512.09796)
*Fu Feng,Ruixiao Shi,Yucheng Xie,Jianlu Shen,Jing Wang,Xin Geng*

Main category: cs.LG

TL;DR: DivMorph는 이질적인 에이전트 형태로의 일반화된 정책을 학습하기 위해 모듈러 훈련 패러다임을 제안하며, 기존 방식의 한계를 극복합니다.


<details>
  <summary>Details</summary>
Motivation: 이질적인 에이전트 형태에 걸쳐 일반화되는 보편적인 정책을 학습하고자 함.

Method: DivMorph는 훈련 전에 무작위로 초기화된 Transformer 가중치를 SVD를 통해 인수 단위로 분해하고, 동적 소프트 게이팅을 사용하여 작업 및 형태 임베딩에 따라 이러한 단위를 조절합니다.

Result: DivMorph는 정책 배포의 효율성을 향상시키고, 새로운 작업으로의 정책 전이를 지원하며, 상태-of-the-art 성능을 달성합니다.

Conclusion: DivMorph는 교차 작업 전송에 대해 샘플 효율에서 3배 향상을 이루고, 단일 에이전트 배치에서 모델 크기를 17배 줄였습니다.

Abstract: Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \textit{learngenes} and morphology- and task-specific \textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\times$ reduction in model size for single-agent deployment.

</details>


### [15] [STACHE: Local Black-Box Explanations for Reinforcement Learning Policies](https://arxiv.org/abs/2512.09909)
*Andrew Elashkin,Orna Grumberg*

Main category: cs.LG

TL;DR: STACHE는 강화 학습 에이전트의 행동을 설명하기 위한 종합 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 희소 보상 또는 안전이 중요한 환경에서 강화 학습 에이전트의 예기치 않은 행동을 해결하기 위해 신뢰할 수 있는 디버깅 및 검증 도구의 필요성이 있다.

Method: STACHE는 이산 마르코프 게임 내에서 에이전트의 특정 행동에 대한 지역적 설명을 생성하는 프레임워크로, 강건성 영역과 최소 반사실을 포함한 복합 설명을 생성하는 방법을 사용한다.

Result: Empirical validation on Gymnasium environments shows that our framework explains policy actions and captures the evolution of policy logic during training.

Conclusion: STACHE는 불안정한 행동에서 최적화된 전략으로의 변화를 시각적으로 이해할 수 있도록 하며, 에이전트의 민감도와 의사 결정 경계를 제공합니다.

Abstract: Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [16] [Exposing Vulnerabilities in Counterfeit Prevention Systems Utilizing Physically Unclonable Surface Features](https://arxiv.org/abs/2512.09150)
*Anirudh Nakra,Nayeeb Rashid,Chau-Wai Wong,Min Wu*

Main category: cs.CR

TL;DR: 이 논문은 종이 표면의 독특한 특성을 활용한 인증 방법이 공격자에게 취약할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 위조 제품이 신뢰하지 않는 공급 체인을 통해 공공 건강과 안전에 큰 위험을 초래함에 따라 효과적인 위조 방지 기술 필요성이 증가하고 있다.

Method: 종이-PUF 기반 인증을 위한 운영 프레임워크를 정형화하고, 이는 물리적 및 디지털 영역에서의 시스템 수준의 취약성을 밝힌다. 또한, 물리적 서비스 거부 및 디지털 위조 공격을 설계하여 적절한 인증을 방해한다.

Result: 설계된 공격의 효과는 종이 PUF 기반의 신뢰할 수 있고 강력한 인증을 위한 보안 대책의 필요성을 강조한다.

Conclusion: 제안된 프레임워크는 위조 방지 시스템 설계를 위한 포괄적인 보안 분석을 촉진하며, 다양한 시스템 구성 요소가 어떻게 공격자에 의해 악용될 수 있는지를 설명한다.

Abstract: Counterfeit products pose significant risks to public health and safety through infiltrating untrusted supply chains. Among numerous anti-counterfeiting techniques, leveraging inherent, unclonable microscopic irregularities of paper surfaces is an accurate and cost-effective solution. Prior work of this approach has focused on enabling ubiquitous acquisition of these physically unclonable features (PUFs). However, we will show that existing authentication methods relying on paper surface PUFs may be vulnerable to adversaries, resulting in a gap between technological feasibility and secure real-world deployment. This gap is investigated through formalizing an operational framework for paper-PUF-based authentication. Informed by this framework, we reveal system-level vulnerabilities across both physical and digital domains, designing physical denial-of-service and digital forgery attacks to disrupt proper authentication. The effectiveness of the designed attacks underscores the strong need for security countermeasures for reliable and resilient authentication based on paper PUFs. The proposed framework further facilitates a comprehensive, stage-by-stage security analysis, guiding the design of future counterfeit prevention systems. This analysis delves into potential attack strategies, offering a foundational understanding of how various system components, such as physical features and verification processes, might be exploited by adversaries.

</details>


### [17] [FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection](https://arxiv.org/abs/2512.09264)
*Xiaojing Chen,Dan Li,Lijun Peng,Jun YanŁetter,Zhiqing Guo,Junyang Chen,Xiao Lan,Zhongjie Ba,Yunfeng DiaoŁetter*

Main category: cs.CR

TL;DR: 이 연구에서는 AIGC 감지를 위한 주파수 기반 블랙박스 공격 방법인 FBA$^2$D를 제안하고, 실험을 통해 그 유효성을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: AIGC의 발전에 따라 소셜 미디어에서 허위 정보의 확산에 대한 불안감이 커지고 있으며, AIGC 감지를 위한 효율적인 방어 기법이 필요하다.

Method: FBA$^2$D는 생성된 이미지와 실제 이미지 간의 주파수 영역의 불일치를 이용한 결정 기반 공격으로, 이산 코사인 변환(DCT)을 활용하여 정밀한 스펙트럼 분할을 수행하고, 질의 하위 공간으로 주파수 대역을 선택하여 쿼리 효율성과 이미지 품질을 개선한다.

Result: Synthetic LSUN 데이터셋과 GenImage 데이터셋에 대한 실증 연구를 통해 제안된 방법이 효과적임을 입증하였다.

Conclusion: 이 연구는 실질적인 AIGC 보안 문제를 해결해야 할 시급함을 강조한다.

Abstract: The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.

</details>


### [18] [ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data](https://arxiv.org/abs/2512.09321)
*Ruiqi Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: ObliInjection은 다중 출처 입력 데이터를 가진 LLM 애플리케이션을 목표로 하는 최초의 프롬프트 주입 공격이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 프롬프트 주입 공격들은 전체 입력 데이터가 공격자의 제어 아래에 있는 단일 출처에서 온다고 가정하거나 다른 출처에서 오는 세그먼트의 순서에 대한 불확실성을 무시하여 다중 출처 데이터 영역에서의 성공이 제한적이다.

Method: ObliInjection은 순서에 무관한 손실을 도입하여 LLM이 오염된 세그먼트의 순서와 관계없이 공격자가 선택한 작업을 완료할 가능성을 정량화하고, 순서GCG 알고리즘을 활용하여 오염된 세그먼트를 최적화한다.

Result: 세 가지 데이터셋과 12가지 LLM에 걸친 포괄적인 실험을 통해, ObliInjection은 입력 데이터의 6-100개 세그먼트 중 단 하나만 오염되어 있어도 매우 효과적임을 보여준다.

Conclusion: ObliInjection은 다중 출처 입력 데이터를 사용하는 LLM 애플리케이션에 대한 새로운 공격 기법을 제공하여, 공격자가 세그먼트의 순서를 알지 못해도 성공할 수 있도록 한다.

Abstract: Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data.
  In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated.

</details>


### [19] [ByteShield: Adversarially Robust End-to-End Malware Detection through Byte Masking](https://arxiv.org/abs/2512.09883)
*Daniel Gibert,Felip Manyà*

Main category: cs.CR

TL;DR: 이 논문은 바이트 수준의 마스킹을 활용하여 엔드 투 엔드 악성코드 탐지기를 강화하는 새로운 방어 메커니즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 엔드 투 엔드 악성코드 탐지기가 적대적 공격에 취약하다는 연구 결과에 따라, 기존 연구자들은 무작위 및 (비)무작위 스무딩에 기반한 방어를 제안하였으나 여전히 큰 적대적 페이로드 공격에 취약하다.

Method: 바이트 수준에서 마스킹을 활용하여 입력 파일의 여러 마스킹된 버전을 생성하고 각 버전을 독립적으로 분류한 후 임계값 기반 투표 메커니즘을 적용하여 최종 분류를 생성하는 방어 메커니즘을 제안한다.

Result: 우리는 EMBER 및 BODMAS 데이터셋에서 수행한 실험을 통해, 제안한 방어 기법이 다양한 기능 보존 조작으로 생성된 적대적 예제에 대해 무작위 및 (비)무작위 스무딩 방어보다 우수한 성능을 보이며, 깨끗한 예제에 대해서도 높은 정확도를 유지함을 보여준다.

Conclusion: 이 방어 메커니즘은 적대적 페이로드를 차단하여 모델의 결정에 미치는 영향을 효과적으로 중화시키며, 입력 버전 중 일부는 파일의 원래 의도를 대표하는 것을 보장한다.

Abstract: Research has proven that end-to-end malware detectors are vulnerable to adversarial attacks. In response, the research community has proposed defenses based on randomized and (de)randomized smoothing. However, these techniques remain susceptible to attacks that insert large adversarial payloads. To address these limitations, we propose a novel defense mechanism designed to harden end-to-end malware detectors by leveraging masking at the byte level. This mechanism operates by generating multiple masked versions of the input file, independently classifying each version, and then applying a threshold-based voting mechanism to produce the final classification. Key to this defense is a deterministic masking strategy that systematically strides a mask across the entire input file. Unlike randomized smoothing defenses, which randomly mask or delete bytes, this structured approach ensures coverage of the file over successive versions. In the best-case scenario, this strategy fully occludes the adversarial payload, effectively neutralizing its influence on the model's decision. In the worst-case scenario, it partially occludes the adversarial payload, reducing its impact on the model's predictions. By occluding the adversarial payload in one or more masked versions, this defense ensures that some input versions remain representative of the file's original intent, allowing the voting mechanism to suppress the influence of the adversarial payload. Results achieved on the EMBER and BODMAS datasets demonstrate the suitability of our defense, outperforming randomized and (de)randomized smoothing defenses against adversarial examples generated with a wide range of functionality-preserving manipulations while maintaining high accuracy on clean examples.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [20] [WOLF: Werewolf-based Observations for LLM Deception and Falsehoods](https://arxiv.org/abs/2512.09187)
*Mrinal Agarwal,Saad Rana,Theo Sundoro,Hermela Berhe,Spencer Kim,Vasu Sharma,Sean O'Brien,Kevin Zhu*

Main category: cs.MA

TL;DR: 본 연구는 다중 에이전트 상호작용에서의 기만을 평가하기 위한 새로운 벤치마크인 WOLF를 제안합니다. WOLF는 기만 생성과 탐지를 측정할 수 있는 구조를 제공하여 기만의 역동적인 특성을 반영합니다.


<details>
  <summary>Details</summary>
Motivation: 효과적인 시스템은 정보 은폐와 타인의 기만 행위 탐지를 전략적으로 수행해야 하지만, 기존 평가 방법은 기만을 정적 분류로 축소하여 상호작용적이고 적대적인 특성을 무시합니다.

Method: WOLF는 역할에 기반한 에이전트(마을 사람, 늑대 인간, 점쟁이, 의사)를 포함하는 프로그래밍 가능한 상태 기계 내에서 기만을 설계하고 평가합니다. 각 발언은 개별 분석 단위로 취급되고, 기만의 유형은 표준화된 분류 체계를 통해 구분됩니다.

Result: 늑대 인간은 31%의 턴에서 기만적인 발언을 생산하고, 동료 탐지 정확도는 71-73%입니다. 의심은 턴이 지남에 따라 늑대 인간에 대해 상승하며, 마을 사람과 의사에 대해서는 안정적으로 유지됩니다.

Conclusion: WOLF는 기만 평가를 정적 데이터셋을 넘어 동적이고 통제된 테스트베드로 확장하여 적대적인 다중 에이전트 상호작용에서 기만과 탐지 능력을 측정할 수 있는 방법을 제공합니다.

Abstract: Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.

</details>


### [21] [GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection](https://arxiv.org/abs/2512.09396)
*Zishu Wei,Qixiang Ma,Xavier Hu,Yuhang Liu,Hui Zang,Yudong Zhao,Tao Wang,Shengyu Zhang,Fei Wu*

Main category: cs.MA

TL;DR: 이 논문에서는 GUI 자동화를 위한 새로운 MLLM 기반 에이전트 프레임워크인 GAIR를 제안하고, 다양한 GUI 작업을 처리하기 위한 이질적인 모델의 지식을 통합하여 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: GUI 자동화는 문서 처리부터 온라인 쇼핑, CAD, 비디오 편집에 이르기까지 다양한 작업을 포함하며, 각 작업에 대한 이질적인 요구가 존재합니다.

Method: GAIR는 여러 GUI 특정 모델의 정보를 공동 처리하기 위해 범용 MLLM을 도입하고, 이는 의사결정자로서 작동합니다.

Result: GAIR는 GUI 벤치마크에서 광범위한 실험을 통해 그 효과성과 신뢰성을 평가 받았습니다.

Conclusion: GAIR는 다양한 MLLM의 장단점을 반영하여 정보의 수집과 의사결정을 지원하는 구조적 접근 방식을 제공합니다.

Abstract: Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.

</details>


### [22] [Supporting Dynamic Agentic Workloads: How Data and Agents Interact](https://arxiv.org/abs/2512.09548)
*Ioana Giurgiu,Michael E. Nidd*

Main category: cs.MA

TL;DR: 다중 요인 시스템의 한계를 해결하기 위한 에이전트 중심 데이터 패브릭을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 발전은 현재의 데이터 관리 아키텍처의 근본적인 한계를 드러낸다.

Method: 에이전트 중심 데이터 패브릭이라는 통합 아키텍처를 제안하고 주의 기반 데이터 검색, 의미론적 마이크로 캐싱, 예측 데이터 프리패칭, 다수결 기반 데이터 제공 개념을 활용한다.

Result: 우리는 이 개념들을 통해 에이전트들이 데이터를 더 빠르고 효율적으로 접근할 수 있도록 하여 중복 쿼리와 데이터 이동, 추론 부담을 줄였다.

Conclusion: 데이터 시스템을 정적인 실행자로 보는 대신, 적응형 협력자로 새롭게 정의함으로써 동적이고 추론 중심의 에이전트 간에 효율적이고 맥락이 풍부한 데이터 교환을 가능하게 하는 연구 방향을 제시한다.

Abstract: The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog는 대화 생성, 평가 및 기계 해석 용이성을 통합한 MIT 라이센스의 오픈소스 Python 툴킷이다.


<details>
  <summary>Details</summary>
Motivation: 대화형 에이전트의 구축 및 분석을 위한 단일 프레임워크가 필요하다.

Method: 표준화된 Dialog 표현을 중심으로 다양한 기능을 제공한다.

Result: 대화 생성, 평가, 해석의 통합을 통해 시스템의 구축 및 이해를 더 체계적으로 할 수 있다.

Conclusion: SDialog는 연구자들이 대화형 시스템을 보다 체계적으로 구축, 벤치마크 및 이해할 수 있도록 돕는다.

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [24] [Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing](https://arxiv.org/abs/2512.09882)
*Justin W. Lin,Eliot Krzysztof Jones,Donovan Julian Jasper,Ethan Jun-shen Ho,Anna Wu,Arnold Tianyi Yang,Neil Perry,Andy Zou,Matt Fredrikson,J. Zico Kolter,Percy Liang,Dan Boneh,Daniel E. Ho*

Main category: cs.AI

TL;DR: AI 에이전트를 인간 사이버 보안 전문가와 비교 평가한 최초의 연구로, ARTEMIS라는 새로운 에이전트 프레임워크가 탄탄한 결과를 보였다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 사이버 보안 효율성을 평가하기 위해 인간 전문가와의 비교 연구가 필요하다.

Method: 10명의 사이버 보안 전문가와 6개의 기존 AI 에이전트 및 ARTEMIS를 실시간 기업 환경에서 평가하였다.

Result: ARTEMIS는 전체적으로 두 번째에 올라 9개의 유효 취약점을 발견하고 82%의 유효 제출율을 기록했다.

Conclusion: AI 에이전트는 체계적 열거, 병렬 활용 및 비용 면에서 장점이 있지만, 높은 허위 긍정률과 GUI 기반 작업에서의 어려움과 같은 주요 능력 격차가 존재한다.

Abstract: We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.

</details>


### [25] [Architectures for Building Agentic AI](https://arxiv.org/abs/2512.09458)
*Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 이 장에서는 에이전트적이고 생성적인 AI의 신뢰성이 주로 구조적 속성이라는 주장을 한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트적 시스템의 신뢰성을 이해하고 이를 향상시키기 위한 방법을 제시하기 위해.

Method: 에이전트적 시스템을 목표 지향적이며 도구를 사용하는 의사 결정자로 정의하고, 원칙적인 구성요소화 및 규율 있는 인터페이스를 통해 신뢰성이 어떻게 형성되는지를 보여준다.

Result: 다양한 유형의 에이전트 및 그들의 설계 가이드를 제안하며, 각 패턴이 신뢰성의 범위와 실패 모드를 어떻게 변화시키는지를 분석한다.

Conclusion: 에이전트 시스템의 설계에 대한 안내를 정제하고, 다양한 요인들이 신뢰성에 미치는 영향을 강조한다.

Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.

</details>


### [26] [An End-to-end Planning Framework with Agentic LLMs and PDDL](https://arxiv.org/abs/2512.09629)
*Emanuele La Malfa,Ping Zhu,Samuele Marro,Sara Bernardini,Michael Wooldridge*

Main category: cs.AI

TL;DR: 본 연구는 검증자를 지원하는 계획을 위한 종단 간 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 인간의 자연어로 작성된 사양을 처리하고 계획 문제를 해결하기 위해 효율적인 방법을 찾기 위해서입니다.

Method: 오케스트레이터가 자연어 사양을 PDDL 모델로 변환하고, 서브 모듈(에이전트)을 사용하여 도메인과 문제를 반복적으로 개선합니다.

Result: 다양한 도메인과 작업에서 우리의 프레임워크의 유연성과 효과성을 입증하였습니다.

Conclusion: LLM을 도움으로 종단 간 계획을 향한 중대한 발전을 나타내며, 기존의 PDDL 계획 엔진 및 검증기와 통합 가능합니다.

Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.

</details>


### [27] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 이 논문은 인식적으로 이질적인 에이전트 간의 신념, 동기 및 영향력을 모델링하기 위한 기하학적 프레임워크를 개발한다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 다양한 인지적 배경을 가진 에이전트 간의 효과적인 의사소통과 의미 전달의 이해를 돕기 위해 필요하다.

Method: 각 에이전트는 개인화된 가치 공간으로 표현되며, 신념은 구조화된 벡터로 정형화된다. 이 신념의 전달은 선형 해석 맵을 통해 이루어지며, 이 맵의 널 공간을 피해야 신념이 소통을 지속할 수 있다.

Result: ‘널 공간 없음의 리더십 조건’이라는 주요 결과를 도출하고, 이는 리더십을 설득이나 권위의 속성이 아닌 표현 가능성의 속성으로 정의한다.

Conclusion: 이러한 인지-기하학적 관점은 인간 및 인공 시스템의 영향력의 인식론적 경계를 명확히 하고, 이질적인 에이전트 간의 신념 역학 분석을 위한 일반적인 기초를 제공한다.

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [28] [Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation](https://arxiv.org/abs/2512.09736)
*Jingtian Yan,Zhifei Li,William Kang,Stephen F. Smith,Jiaoyang Li*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 경로 찾기(MAPF) 알고리즘의 성능을 현실적인 실행 환경에서 이해하기 위한 주요 설계 선택의 영향을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: MAPF 알고리즘은 산업 창고 및 자동화 제조 시설에서 로봇이 신뢰성 있게 운영될 수 있도록 필요하다.

Method: 세 가지 기본 요소를 체계적으로 연구하여 설계 선택이 성능에 미치는 영향을 분석합니다: 솔루션 최적성과 실행 성능의 관계, 동역학 모델링의 부정확성이 시스템 성능에 미치는 민감도, 모델 정확성과 계획 최적성 간의 상호작용.

Result: 실험적으로 이러한 요소를 분석하여 설계 선택이 현실적 시나리오에서 성능에 미치는 영향을 이해합니다.

Conclusion: 실질적인 실제 배포를 위한 커뮤니티를 이끌기 위한 개방적인 도전 과제와 연구 방향을 강조합니다.

Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.

</details>


### [29] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: 본 연구는 SCOPE라는 새로운 계획 모델을 소개하여 LLM의 효율성을 높이고, 이를 통해 텍스트 기반 환경에서의 목표 분해 작업을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 텍스트 기반 환경에서의 장기 계획은 개방형 행동 공간, 모호한 관찰 및 희박한 피드백으로 인해 상당한 도전 과제를 나타낸다.

Method: SCOPE는 초기화 시 LLM이 생성한 하위 목표를 활용하여 경량화된 학생 모델을 사전 교육하는 일회성 계층적 계획자이다.

Result: SCOPE는 LLM 기반의 기존 방법과 비교할 때 효율성을 크게 개선하고, 텍스트 기반 계획 작업에서 하위 목표 분해의 강력한 출발점이 된다.

Conclusion: SCOPE는 LLM 기반의 계층적 에이전트보다 우수한 성공률과 빠른 추론 시간을 달성하였다.

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>
