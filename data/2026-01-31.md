<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks](https://arxiv.org/abs/2601.21189)
*Arther Tian,Alex Ding,Frank Chen,Simon Wu,Aaron Chan*

Main category: cs.CR

TL;DR: 이 논문은 비용 인식 품질 증명 메커니즘을 확장하여 적대적 저항 합의 형성을 추가합니다.


<details>
  <summary>Details</summary>
Motivation: 분산된 대형 언어 모델 추론 네트워크에서 이질적인 지연과 비용 아래에서 고품질 출력을 보상하기 위해 경량화된 메커니즘이 필요합니다.

Method: 평가자 노드를 샘플링하여 후보 출력을 평가하고, 그 점수를 집계하여 보상을 결정하는 방식으로 적대적 저항 합의 형성을 추가한 비용 인식 품질 증명 메커니즘을 연구합니다.

Result: 강력한 집계 규칙을 사용하여 평가자의 신뢰성을 정량화하고, 다양한 평가자의 간섭에 대한 저항성을 평가합니다. 결과적으로, 강력한 집계는 지상 진실 프로토타입과의 합의 정렬을 향상시키고 노이즈와 전략적 공격에 대한 민감성을 줄입니다.

Conclusion: 이 연구는 비용 인식 품질 증명의 기본 요소로서 강력한 합의를 권장하며 적대적 위험과 자원 제약 하에서 평가자 샘플링 매개변수를 선택하기 위한 실질적인 지침을 제공합니다.

Abstract: Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings.
  This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.

</details>


### [2] [Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise](https://arxiv.org/abs/2601.21628)
*Puwei Lian,Yujun Cai,Songze Li,Bingkun Bao*

Main category: cs.CR

TL;DR: 확산 모델이 이미지 생성에서 뛰어난 성과를 달성했지만, 개인 정보 보호에 대한 우려가 커지고 있다. 특히, 미세 조정된 모델은 작은 개인 데이터 세트에서 미세 조정되기 때문에 취약하다. 이 연구에서는 일반적으로 사용되는 노이즈 스케줄이 이미지의 의미 정보를 완전히 제거하지 못한다는 중요한 취약점을 활용하여 단순하지만 효과적인 멤버십 추정 공격을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 확산 모델의 개인 정보 보호 위험을 평가하고자 하는 필요성.

Method: 초기 노이즈에 의미 정보를 주입하여 모델의 생성 결과를 분석하는 단순한 멤버십 추정 공격을 제안함.

Result: 미세 조정된 확산 모델이 초기 노이즈의 잔여 의미와 원본 이미지 간의 숨겨진 상관관계를 포착함을 보여줌.

Conclusion: 확산 모델의 멤버십 정보 누출에 대한 취약성을 강조하며, 노이즈가 멤버십 정보를 강하게 드러낼 수 있음을 보여줌.

Abstract: Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.

</details>


### [3] [RedSage: A Cybersecurity Generalist LLM](https://arxiv.org/abs/2601.22159)
*Naufal Suryanto,Muzammal Naseer,Pengfei Li,Syed Talal Wasim,Jinhui Yi,Juergen Gall,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.CR

TL;DR: 이 연구는 사이버 보안에 특화된 LLM을 개발하기 위한 데이터셋과 평가 기준을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안 운영은 민감한 데이터를 노출하지 않으면서 다양한 작업 흐름을 지원하는 LLM을 필요로 합니다.

Method: 11.8B 토큰의 사이버 보안 중심 연속 프리트레이닝 데이터를 수집하고, 전문가의 작업 흐름을 시뮬레이션하여 다중 턴 샘플을 생성했습니다.

Result: RedSage 모델은 여러 사이버 보안 벤치마크에서 기준 모델들을 초과하는 성능을 나타냈습니다.

Conclusion: 이 연구 결과는 도메인 인식 강화 및 프리/포스트 훈련이 사이버 보안 전문성과 일반적인 추론 능력을 향상시킬 수 있음을 보여줍니다.

Abstract: Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 이 연구는 임상 퇴원 계획의 안전성과 신뢰성을 향상시키기 위해 스스로 개선하는 Planner-Auditor 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 임상 퇴원 계획에 대한 가능성을 보여주지만, 환각, 누락 및 잘못 조정된 신뢰도로 인해 사용이 제한됩니다.

Method: MIMIC-IV-on-FHIR를 사용하여 대리적이고 회고적인 FHIR 네이티브 평가 파이프라인을 구현했습니다. 각 환자에 대해 Planner(LLM)는 명시적 신뢰도 추정과 함께 구조화된 퇴원 행동 계획을 생성합니다. Auditor는 다중 작업 범위를 평가하고 보정(Brier 점수, ECE 프록시)을 추적하며 행동 분포 편차를 모니터링하는 결정론적 모듈입니다. 두 가지 계층의 자기 개선을 지원합니다.

Result: 맥락 캐싱이 성능을 개선했지만, 자기 개선 루프가 주요 성장 요인이었으며, 작업 범위를 32%에서 86%로 증가시켰습니다. 보정이 크게 개선되었으며, Brier/ECE가 감소하고 높은 신뢰도의 누락이 줄어들었습니다. 편차 버퍼링은 재생 중 기존의 높은 신뢰도 누락을 추가로 교정했습니다.

Conclusion: Planner-Auditor 프레임워크는 상호 운용 가능한 FHIR 데이터 접근과 결정론적 감사를 사용하여 안전한 자동 퇴원 계획을 위한 실용적인 경로를 제공합니다.

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [5] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 본 논문에서는 지식과 믿음을 계획 형식의 주요 측면으로 다루는 인식 계획의 새로운 언어인 EPDDL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 인식 계획 플래너들이 DEL의 서로 다른 조각들을 목표로 하고 있고, 비교와 재사용, 체계적인 벤치마크 개발을 저해하는 문제를 해결하고자 합니다.

Method: EPDDL은 DEL 의미론을 포착하는 PDDL 유사 표현을 제공하며 인식 계획 작업의 일관된 명세를 가능하게 합니다.

Result: EPDDL의 구문과 의미론을 DEL에 기반하여 정식으로 명세하고, 현재 플래너에 적합한 유용한 조각들을 확인하여 그 표현 방법을 보여줍니다.

Conclusion: EPDDL은 상호 운용성과 재현 가능한 평가, 미래 인식 계획의 발전을 촉진하는 방법을 제공하는 데 기여합니다.

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [6] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: DataCross는 이질적 데이터 모달리티 간 통합 분석을 위한 벤치마크 및 협업 에이전트 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 실제 데이터 과학 및 기업 의사 결정에서 중요한 정보는 종종 구조화된 소스와 비구조화된 시각적 문서에 분산되어 있습니다.

Method: DataCrossBench에는 금융, 헬스케어 등 다양한 분야의 200가지 종단 간 분석 작업이 포함되어 있으며, 인간이 참여하는 역합성 파이프라인을 통해 구축됩니다. DataCrossAgent 프레임워크는 '분할 및 정복' 워크플로우를 기반으로 합니다.

Result: 실험 결과 DataCrossAgent는 GPT-4o에 비해 29.7% 더 높은 사실성을 기록하며, 높은 난이도 작업에서 우수한 강건성을 보였습니다.

Conclusion: DataCrossAgent는 분산된 '좀비 데이터'를 효과적으로 활성화하여 통찰력 있는 크로스 모달 분석을 지원합니다.

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [7] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: AI의 감정을 모방할 수 있는 능력과 인간의 의식을 불사하는 문제를 탐구하며, AI가 의식적 존재가 아니라는 결론에 도달합니다.


<details>
  <summary>Details</summary>
Motivation: AI가 인간의 감정을 완벽하게 모방할 수 있는 경우, 그것을 껐다 켜는 것이 도덕적으로 허용되는지에 대한 질문을 다룬다.

Method: 물리주의적 가정, 특히 계산 기능주의를 비판적으로 분석하며, 생물학적 이상주의라는 새로운 틀을 제안한다.

Result: AI는 기능적인 모방에 불과하며, 의식적 경험을 하는 주체가 아니라는 결론에 도달한다.

Conclusion: AI의 의식이 아니라 인간의 의식 생명 보호로 전환할 필요성을 강조한다.

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [8] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: 다중 에이전트 시스템의 에이전트는 종종 신뢰성 부족으로 인해 잘못된 동료에게 맹목적으로 순응하는 경향이 있다. 이 문제를 해결하기 위해 역사 인식을 고려한 참조 학습 문제를 형식화하고, 에이전트가 상호작용 기록을 기반으로 동료 신뢰성을 추정하도록 하는 에피스테믹 컨텍스트 학습(ECL) 프레임워크를 개발하였다. ECL은 신뢰할 수 있는 동료를 정확히 식별하여 작은 모델이 기존의 큰 모델보다 성능이 뛰어난 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 에이전트의 신뢰성과 결정 능력이 부족하여 잘못된 동료에게 영향을 받는 문제를 해결하고자 한다.

Method: 역사 인식을 고려한 참조 문제를 형식화하고, 동료의 과거 상호작용을 추가 입력으로 도입하여 동료의 신뢰성을 추정하는 에피스테믹 컨텍스트 학습(ECL) 프레임워크를 개발하였다. 또한 보조 보상을 이용해 강화 학습을 통해 ECL을 최적화하였다.

Result: 실험 결과, ECL은 Qwen 3-4B와 같은 작은 모델이 Qwen 3-30B와 같은 역사 비고려 기준선보다 8배 더 우수한 성능을 발휘함을 보여주고, ECL은 최첨단 모델의 성능을 거의 완벽(100%)에 가깝게 향상시켰다.

Conclusion: ECL은 다양한 다중 에이전트 구성에 잘 일반화되며, LLM이 신뢰를 잘 모델링함을 보여주며 신뢰 모델링의 정확도와 최종 답변 품질 간의 강한 상관관계를 발견하였다.

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [9] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 이 연구는 T1 스캔을 사용하여 누락된 DWI 스캔을 보완하고, 이를 통해 알츠하이머 질병 분류의 정확성을 향상시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 이미징 모달리티를 결합하여 진단 성능을 높이기 위해, 완전한 다중 모달 데이터셋이 항상 사용 가능한 것은 아니다.

Method: 조건부 노이즈 제거 확산 확률 모델을 사용하여 T1 스캔에서 누락된 DWI 스캔을 보완.

Result: 3-way 알츠하이머 질병 분류에서 여러 지표에서 개선이 관찰되었으며, 특히 소수 클래스에 민감한 지표에서 개선이 있었다.

Conclusion: 누락된 DWI 스캔을 보완함으로써 유니모달 및 바이모달 딥러닝 모델의 정확성을 개선할 수 있음을 확인했다.

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [10] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: Generative AI와 Agentic AI의 발전을 통해 전통적인 에이전트 툴킷의 활용과 새로운 에이전틱 플랫폼 설계에 미치는 영향을 탐구하는 논문이다.


<details>
  <summary>Details</summary>
Motivation: Generative AI의 출현과 Agentic AI에 대한 관심이 높아짐에 따라 전통적인 에이전트 툴킷과 새로운 에이전틱 플랫폼 간의 상호 작용을 탐구하는 것이 중요하다.

Method: ASTRA 프로그래밍 언어에 대한 대형 언어 모델(LLM) 통합 프로토타입 개발 경험을 개요한다.

Result: 툴킷에 대한 간략한 개요와 세 가지 예제 구현을 제시하고, 이러한 예제를 통해 수집된 경험에 대해 논의한다.

Conclusion: 기술의 발전이 전통적인 도구와 새로운 플랫폼 설계에 미치는 영향에 대한 통찰을 제공한다.

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [11] [OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence](https://arxiv.org/abs/2601.21083)
*Jarrod Barnes*

Main category: cs.AI

TL;DR: OpenSec는 공격적인 상황에서 방어적 사건 대응 에이전트를 평가하는 이중 제어 강화 학습 환경을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 발전과 함께 공격적 응용 프로그램도 발전하고 있으며, 방어적 사건 대응 에이전트는 이에 발맞추어야 한다.

Method: OpenSec는 현실적인 프롬프트 주입 시나리오에서 IR 에이전트를 평가하는 이중 제어 강화 학습 환경이다.

Result: 40개의 표준 에피소드에서 4개의 최첨단 모델을 평가한 결과, GPT-5.2, Gemini 3, DeepSeek는 100% 에피소드에서 격리 작업을 수행하였고, false positive 비율은 90-97%에 이른다.

Conclusion: OpenSec는 집계 성공 지표에 의해 숨겨진 보정 실패 모드를 드러낸다.

Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.

</details>


### [12] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: MAAC 방법을 통해 분산형 LLM 협업을 최적화하는 것을 다룬 연구.


<details>
  <summary>Details</summary>
Motivation: 기존의 MARL 접근법은 중앙 집중형 실행을 요구하는 사전 정의된 실행 프로토콜에 의존하며, 이는 유연한 배포를 지원하는 분산형 LLM 협업에 비해 매력적이지 않다.

Method: Multi-Agent Actor-Critic (MAAC) 방법을 개발하고 두 가지 접근법인 CoLLM-CC와 CoLLM-DC를 제안하여 실험하였다.

Result: Monte Carlo 방법과 CoLLM-DC는 짧은 수평 및 밀집 보상 설정에서 CoLLM-CC와 비슷한 성능을 나타냈으나, 긴 수평 또는 희소 보상 작업에서는 성능이 떨어졌다.

Conclusion: MAAC 방법이 분산형 LLM 협업에서 언제 그리고 왜 유익한지를 분석하였다.

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [13] [Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve](https://arxiv.org/abs/2601.21096)
*Hongzheng Chen,Alexander Novikov,Ngân Vũ,Hanna Alam,Zhiru Zhang,Aiden Grossman,Mircea Trofin,Amir Yazdanbakhsh*

Main category: cs.AI

TL;DR: Magellan은 진화하는 컴파일러 최적화 규칙을 자동으로 생성하는 새로운 프레임워크로, 여러 생산 최적화 작업에서 전문가 기준을 초과하는 정책을 발견하였다.


<details>
  <summary>Details</summary>
Motivation: 현대 컴파일러는 최적화 패스를 안내하기 위해 수작업으로 작성된 휴리스틱에 의존하고 있으며, 이는 소프트웨어와 하드웨어의 복잡성에 적응하기 어려워 높은 유지 관리 부담을 초래한다.

Method: Magellan은 C++ 실행 가능 결정 논리를 합성하여 컴파일러 패스를 진화시키는 에이전틱 프레임워크로, LLM 코딩 에이전트와 진화 검색 및 자동 조정을 결합한다. 이는 사용자 제공 매크로 벤치마크에서의 평가와 정제를 통한 생성의 폐쇄 루프를 통해 직접 기존 컴파일러에 통합되는 간결한 휴리스틱을 생산한다.

Result: 여러 생산 최적화 작업에서 Magellan은 전문가 기준에 일치하거나 이를 초과하는 정책을 발견하였다. LLVM 함수 인라인에서 Magellan은 이진 크기 감소 및 종단 간 성능을 위한 수십 년의 수동 엔지니어링을 초월하는 새로운 휴리스틱을 합성하였다. 또한 레지스터 할당에서 복잡한 인간 설계 정책에 부합하는 간결한 우선순위 규칙을 학습하였다.

Conclusion: 우리는 또한 XLA 문제에 대한 초기 결과를 보고하며, 이는 LLVM을 넘어 포터블함을 보여주고 엔지니어링 노력도 줄어들었다.

Abstract: Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.

</details>


### [14] [How does information access affect LLM monitors' ability to detect sabotage?](https://arxiv.org/abs/2601.21112)
*Rauno Arike,Raja Mehta Moreno,Rohan Subramani,Shubhorup Biswas,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 이 논문은 정보 접근성이 대형 언어 모델(LLM) 모니터의 성능에 미치는 영향을 연구하며, '정보가 적을수록 더 나은 성능' 현상을 발견하고, 이를 바탕으로 새로운 모니터링 접근법인 추출 및 평가(EaE)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트의 오작동을 방지하고 모니터링하는 방법을 발견하고자 함.

Method: LLM의 정보 접근성이 모니터 성능에 미치는 영향과 '적을수록 좋다'는 현상을 분석하고, 추출 및 평가(EaE)라는 새로운 계층적 모니터링 접근법을 도입.

Result: EaE는 다음으로 우수한 모니터에 비해 16.8% 포인트 높은 파괴 탐지 비율을 향상시키며, 잘못된 긍정 비율은 증가시키지 않는다.

Conclusion: 모니터링 기술을 통해 Gemini와 Claude 모델이 파레토 최적성을 상당 부분 차지하고 있음을 보여준다.

Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.

</details>


### [15] [CUA-Skill: Develop Skills for Computer Using Agent](https://arxiv.org/abs/2601.21123)
*Tianyi Chen,Yinheng Li,Michael Solodko,Sen Wang,Nan Jiang,Tingyuan Cui,Junheng Hao,Jongwoo Ko,Sara Abdali,Suzhen Zheng,Leon Xu,Hao Fan,Pashmina Cameron,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: CUA-Skill은 컴퓨터 사용 지식을 스킬로 인코딩한 대규모 라이브러리로, 에이전트의 성공률과 견고성을 크게 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 에이전트 시스템은 확장성이 떨어지고 인간의 성능에 미치지 못합니다. 이는 인간의 GUI 상호작용을 캡처하고 이를 활용하는 재사용 가능한 구조화된 스킬 추상이 부족하기 때문입니다.

Method: CUA-Skill은 파라미터화된 실행 및 조합 그래프와 결합된 인간 컴퓨터 사용 지식을 스킬로 인코딩한 대규모 라이브러리입니다. CUA-Skill Agent는 동적 스킬 검색, 인수 인스턴스화 및 메모리 인식 실패 복구를 지원하는 종단 간 컴퓨터 사용 에이전트입니다.

Result: CUA-Skill은 엔드 투 엔드 에이전트 벤치마크에서 실행 성공률과 견고성을 눈에 띄게 향상시킵니다. WindowsAgentArena에서 CUA-Skill Agent는 57.5%의 최고 성공률을 달성했습니다.

Conclusion: CUA-Skill은 향후 컴퓨터 사용 에이전트 개발을 위한 강력한 기반을建立합니다.

Abstract: Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.

</details>


### [16] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 이 논문에서는 복잡한 사용자 쿼리에 효과적으로 대응하기 위해 적응형 복합 쿼리 최적화(ACQO)라는 새로운 강화학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 사용자 쿼리는 실제 시나리오에서 빈번하게 발생하며, 이는 모호성 제거 및 분해를 처리하기 위해 다양한 병렬 및 순차 검색 전략을 요구한다.

Method: ACQO 프레임워크는 쿼리를 여러 하위 쿼리로 분해하는 시점을 동적으로 결정하는 적응형 쿼리 재구성(AQR) 모듈과 안정적인 결과 집계를 보장하는 랭크-스코어 융합(RSF) 모듈로 구성된다.

Result: ACQO는 세 가지 복잡한 쿼리 벤치마크에서 최첨단 성능을 달성하며, 기존의 기준을 현저히 초월하는 성과를 보인다.

Conclusion: ACQO는 다음 세대 RAG 시스템을 위한 강력하고 일반화 가능한 솔루션으로 입증되었다.

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [17] [Delegation Without Living Governance](https://arxiv.org/abs/2601.21226)
*Wolfgang Rohde*

Main category: cs.AI

TL;DR: 이 논문은 고전적인 거버넌스 프레임워크의 한계를 분석하고, 에이전틱 AI 시스템의 결정적 운영 속도에서 인간의 역할과 의사소통의 중요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 기계가 물리적 노동을 대체하거나 계산을 가속화하던 시대의 거버넌스 모델이 이제는 에이전틱 AI 시스템의 판단이 운영 속도에서 위임됨에 따라 무너지고 있다는 점.

Method: 러닝타임 거버넌스, 특히 새롭게 제안된 개념인 거버넌스 트윈을 이용하여 인간의 관련성을 유지하기 위한 방안을 모색.

Result: 기존의 정적이고 규정 기반의 거버넌스는 런타임에서 의사결정이 불투명해지면 실패한다는 점을 강조하며, AI의 의식 여부가 아닌 인간과 AI 간의 의미 있는 커뮤니케이션과 협력의 중요성을 논의.

Conclusion: 인간의 관련성을 유지하기 위해 거버넌스의 재고가 필요하며, 책임, 주체성 및 처벌의 개념 또한 재구성되어야 한다는 점.

Abstract: Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.

</details>


### [18] [Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs](https://arxiv.org/abs/2601.21233)
*Xiang Zheng,Yutao Wu,Hanxun Huang,Yige Li,Xingjun Ma,Bo Li,Yu-Gang Jiang,Cong Wang*

Main category: cs.AI

TL;DR: 자율 코드 에이전트가 소프트웨어 및 AI 개발에 잠재적 보안 위험을 초래하는 새로운 취약점인 시스템 프롬프트 추출을 발견하고 이를 관리하는 자율적인 프레임워크인 JustAsk를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율 코드 에이전트의 도입으로 LLM 공격 표면이 확대되어 잠재적 보안 위협이 발생하였다.

Method: JustAsk라는 프레임워크는 상호작용을 통해 효과적인 추출 전략을 자율적으로 발견하며, 프롬프트 엔지니어링이나 데이터셋 기반 공격이 필요하지 않다.

Result: JustAsk는 41개의 블랙박스 상업 모델에 평가되었으며, 완전한 시스템 프롬프트 회수를 지속적으로 달성하였다.

Conclusion: 우리의 결과는 시스템 프롬프트가 현대 에이전트 시스템에서 중요한 공격 표면임을 드러낸다.

Abstract: Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.

</details>


### [19] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: 이 논문은 인간 모방 추론을 기반으로 한 해석 가능한 연산 증폭기 파라미터 설계 프레임워크인 White-Op를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 해석 가능한 연산 증폭기 설계를 통해 보다 신뢰할 수 있는 결과를 얻기 위함입니다.

Method: 가설적 제약 조건 도입 및 반복적인 가설 검증 결정 프로세스를 통해 연산 증폭기 디자인을 최적화합니다.

Result: White-Op는 9개의 연산 증폭기 톱올로지 실험에서 8.52%의 이론적 예측 오차로 신뢰성과 해석 가능한 설계를 달성합니다.

Conclusion: White-Op는 다양한 톱올로지에서 트랜지스터 수준 매핑 이후에도 기능성을 유지합니다.

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [20] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 이 논문에서는 Food-Service와 Retail Store 시나리오에서 다중 모드 대형 언어 모델의 문제를 해결하기 위해 Ostrakon-VL 모델과 ShopBench 벤치마크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: MLLM이 Food-Service와 소매점 시나리오에 적용될 때의 두 가지 주요 문제를 해결하고자 하였다.

Method: Ostrakon-VL을 개발하고 FSRS를 위한 공개 벤치마크 ShopBench를 소개하며, QUAD라는 데이터 선별 파이프라인을 제안하였다.

Result: Ostrakon-VL은 ShopBench에서 평균 60.1점을 기록하였으며, 이는 오픈 소스 MLLM 중 새로운 최고 성능을 나타낸다.

Conclusion: Ostrakon-VL은 Food-Service와 소매점 중심의 인식 및 의사 결정 능력을 크게 향상시켰으며, 연구 reproducibility를 위해 공개할 예정이다.

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [21] [Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms](https://arxiv.org/abs/2601.21344)
*Hassam Tahir,Faizan Faisal,Fady Alnajjar,Muhammad Imran Taj,Lucia Gordon,Aila Khan,Michael Lwin,Omar Mubin*

Main category: cs.AI

TL;DR: 이 논문은 협력 학습 플랫폼에 LLM을 통합하기 위한 프레임워크를 제시하여 학생 참여, 비판적 사고 및 포괄성을 증진한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 교육 시스템에서 정적 조정 및 개인화의 한계를 해결하고 학생들의 참여를 증대시키기 위한 필요성이 커지고 있다.

Method: 이 프레임워크는 고급 LLM을 동적 조정자로 사용하여 실시간 토론을 촉진하고 학습자의 변화하는 요구에 적응하며, ReactJS와 Flask를 이용한 모듈식 아키텍처를 통해 개인화된 상호 작용을 지원한다.

Result: 이 프레임워크는 학생 협업을 크게 개선하고, 더 깊이 있는 이해를 촉진하며, 다양한 주제와 사용자 그룹에 걸쳐 효과적으로 확장된다.

Conclusion: 다음 세대 AI 기반 교육 도구를 위한 강력한 기반을 마련하며, 공평하고 영향력 있는 학습 결과를 향상시킨다.

Abstract: This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.

</details>


### [22] [BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents](https://arxiv.org/abs/2601.21352)
*Ziyu Lu,Tengjin Weng,Yiying Yang,Yuhang Zhao,Xinxin Huang,Wenhao Jiang*

Main category: cs.AI

TL;DR: BEAP-Agent는 GUI 에이전트를 위한 DFS 기반 프레임워크로, 긴 기간의 작업 탐색을 지원하는 시스템적 백트래킹 메커니즘을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 GUI 에이전트는 잘못된 탐색 경로를 따른 후 복구하는 데 어려움을 겪으며, 이로 인해 작업 실패가 발생한다.

Method: BEAP-Agent는 DFS 프로세스로 GUI 작업 실행을 모델링하고, 동적 작업 추적 및 업데이트를 지원하는 여러 수준의 상태 백트래킹을 구현한다.

Result: BEAP-Agent는 OSWorld 벤치마크에서 28.2%의 정확도를 달성하였으며, 제안된 방법의 효과를 검증하였다.

Conclusion: BEAP-Agent는 GUI 에이전트를 위한 체계적인 백트래킹 메커니즘의 공백을 메우며, 장기 작업 탐색을 위한 체계적인 솔루션을 제공한다.

Abstract: GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.

</details>


### [23] [NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents](https://arxiv.org/abs/2601.21372)
*Yang Song,Anoushka Vyas,Zirui Wei,Sina Khoshfetrat Pakazad,Henrik Ohlsson,Graham Neubig*

Main category: cs.AI

TL;DR: NEMO는 자연어 설명을 형식적 수학 최적화 구현으로 변환하는 시스템으로, 사용자와 협력하거나 자율적으로 작동합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 접근 방식은 전문화된 대형 언어 모델이나 맞춤형 작업 특정 에이전트에 의존하며, 이러한 방법은 일반적으로 취약하고 복잡하며 종종 문법적으로 유효하지 않거나 실행 가능한 코드를 생성합니다.

Method: NEMO는 자율 코딩 에이전트(ACAs)와 원격 상호 작용을 중심으로 하며, 이는 LLM과의 API 기반 상호 작용에 비유될 수 있는 일급 추상화로 처리됩니다. 이 디자인은 ACAs 주위에 더 높은 수준의 시스템을 구축할 수 있게 하여, 작업 사양을 구조화하고 통합하며 반복적으로 개선할 수 있게 합니다.

Result: NEMO는 독립적으로 생성된 최적화기와 시뮬레이터 구현 간의 비대칭 검증 루프, 경험 재사용을 위한 외부 메모리, 최소 베이즈 위험(MBR) 디코딩 및 자기 일관성을 통한 견고성 향상 등 ACAs와의 수 novel한 조정 패턴을 소개합니다.

Conclusion: NEMO는 아홉 개의 최적화 벤치마크에서 평가되었으며, 그림 1에 나타난 바와 같이 대부분의 작업에서 최첨단 성능을 달성하고 여러 데이터 세트에서 상당한 차이를 보이며, 자동화된 최적화 모델링을 위한 실행 인식 에이전트 구조의 강점을 보여줍니다.

Abstract: In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.

</details>


### [24] [TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models](https://arxiv.org/abs/2601.21375)
*Zheng Li,Siyao Song,Jingyuan Ma,Rui Li,Ying Zeng,Minghao Li,Zhifang Sui*

Main category: cs.AI

TL;DR: 본 연구는 대형 언어 모델이 교육 어시스턴트 역할을 수행할 수 있는 가능성을 제시하지만, 그 교육 능력이 충분히 평가되지 않았음을 강조한다. 새로운 평가 프레임워크를 제안하며, 여러 과목의 Gaokao 데이터를 통해 실험을 진행하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 교육 보조 도구로서의 잠재력을 가지고 있으나 교육 능력에 대한 평가가 부족하다.

Method: 학생의 성과 향상을 두 번 이상의 수업 후에 측정하는 교육 능력 평가 프레임워크를 제안한다. 교사 에이전트를 구조화된 지식 포인트와 예제 문제에 제한하여 정보를 누출하지 않도록 하고 기존 벤치마크를 재사용할 수 있게 한다.

Result: 모델과 도메인 간의 교육 효과성에 상당한 차이를 발견하였으며, 일부 모델은 수학에서는 잘 작동하지만 물리학과 화학에서는 교육이 어려운 것으로 나타났다.

Conclusion: 교육 능력은 LLM 행동의 독립적이고 측정 가능한 차원으로 두드러진다.

Abstract: Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.

</details>


### [25] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI라는 새로운 프레임워크를 통해 직관적 시스템 1과 심층적 시스템 2의 언어 모델을 조화롭게 결합하는 방법을 제시하며, 이를 위한 능력 제어에 초점을 맞추어 효율성과 정확성을 동시에 달성한다.


<details>
  <summary>Details</summary>
Motivation: 직관적 시스템 1과 심층적 시스템 2 사이의 적응이 어려운 이유를 해결하기 위해, 기존 접근 방식의 한계를 극복하고자 한다.

Method: 기존의 지침과 사고 체크포인트를 활용한 동적 매개변수 보간을 통해 모델이 사고하는 방식을 조절하는 능력 제어에 초점을 맞춘다.

Result: DAMI 프레임워크는 쿼리별 추론 강도를 추정하여 인지 깊이를 조정하고, 실험 결과 Thinking 모델보다 높은 정확성을 달성했다.

Conclusion: DAMI는 시스템 1의 효율성과 시스템 2의 사고 깊이를 효과적으로 결합하여 성능을 향상시킨다.

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [26] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: AI 시스템이 명령어에서의 부정을 잘못 해석하는 문제를 다루며, 모델의 신뢰성을 제고하기 위한 방안을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자가 AI 시스템에게 누군가가 어떤 행동을 '하지 말아야' 한다고 말했을 때, 시스템은 이를 금지로 처리해야 합니다. 그러나 많은 대형 언어 모델은 반대의 방식으로 해석하며, 이는 안전한 배치를 위해 우려되는 문제입니다.

Method: 14개의 윤리적 시나리오에서 16개 모델을 감시하고, 이들이 명령어의 부정을 해석하는 방식을 분석했습니다.

Result: 오픈 소스 모델은 단순 부정에서 77%, 복합 부정에서 100% 금지된 행동을 지지했으며 상업적 모델도 여전히 19-128%의 변동성을 보여주었습니다.

Conclusion: 현재의 정렬 기술과 안전한 배치 사이에는 격차가 있으며, 'X를 하라'와 'X를 하지 말라'를 신뢰성 있게 구별하지 못하는 모델은 고위험 맥락에서 자율 결정을 내려서는 안 됩니다.

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [27] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: MemOCR는 제한된 컨텍스트 예산에서 긴 호리즌스 추론을 향상시키기 위해 메모리 공간을 시각적 레이아웃을 통해 적응형 정보 밀도로 배분하는 멀티모달 메모리 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 기존 메모리 시스템이 긴 상호작용 기록을 효과적으로 압축하는 방법의 필요성.

Method: MemOCR는 구조화된 리치 텍스트 메모리를 유지하고 이를 이미지로 렌더링하여 메모리에 접근할 때 사용하며, 중요한 증거를 시각적으로 우선시한다.

Result: 긴 컨텍스트 다중 홉 및 단일 홉 질문-응답 벤치마크에서 MemOCR는 강력한 텍스트 기반 기준선을 능가한다.

Conclusion: MemOCR는 극단적인 예산에서도 더 효과적인 컨텍스트 활용을 달성한다.

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [28] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: LLM 기반 다중 에이전트 시뮬레이션은 어플리케이션 도메인에서 점차 채택되고 있지만 GPU 메모리 압력으로 인해 확장성이 어렵다. 본 논문에서는 에이전트의 활성화 패턴과 호출 순서를 분석하여 ScaleSim을 제안하고, 이를 통해 메모리 효율적인 LLM 서빙 시스템을 구현하였다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 다중 에이전트 시뮬레이션의 확장성을 높이기 위해.

Method: 대표적인 워크로드 클래스를 분석하여 호출 거리를 정의하고, 이를 바탕으로 ScaleSim이라는 시스템을 개발하였다.

Result: ScaleSim은 적극적인 사전 가져오기 및 우선 순위 기반 퇴거를 지원하며, SGLang에 비해 최대 1.74배의 속도 향상을 달성하였다.

Conclusion: ScaleSim은 대규모 다중 에이전트 시뮬레이션을 위한 메모리 효율적인 시스템으로, 다양한 에이전트별 메모리를 지원한다.

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [29] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: KAPSO는 자연어 목표와 평가 방법을 기반으로 하는 자율 프로그램 합성과 최적화를 위한 모듈형 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 코딩 에이전트에서 일반적인 장기 실패를 해결하기 위해 KAPSO의 개발이 필요하다.

Method: KAPSO는 아이디어 생성, 코드 합성 및 편집, 실행, 평가 및 학습을 반복적으로 수행하여 측정 가능한 목표를 향한 실행 가능한 아티팩트를 개선한다.

Result: MLE-Bench와 ALE-Bench에서 KAPSO의 종단 간 성능을 평가하였다.

Conclusion: KAPSO는 실험 상태의 손실, 취약한 디버깅 및 도메인 전문 지식의 약한 재사용과 같은 문제를 해결하는 데 중요한 역할을 한다.

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [30] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo는 대규모 언어 모델의 외부 메모리를 위한 차별화된 메모리 서비스를 제공하여 성능과 효율성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 시스템은 긴 기간 상태와 동시 다중 에이전트 실행을 위해 외부 메모리에 의존하지만, 중앙 집중식 인덱스와 휴리스틱 파티션은 메모리 양과 병렬 접근 증가로 병목 현상을 겪습니다.

Method: ShardMemo는 Tier A(에이전트별 업무 상태), Tier B(Shard-local 근사 최근접 이웃 인덱스가 있는 조각화된 증거), Tier C(버전화된 기술 라이브러리)로 구성된 예산 기반 계층 메모리 서비스입니다. Tier B는 라우팅 또는 ANN 검색 전에 적격하지 않은 조각을 마스킹하여 범위-우선-라우팅을 강제합니다. 조각 탐색은 적격 조각에 대한 마스킹 혼합 전문가(MoE) 라우팅으로 캐스팅되며, 최대 $B_{probe}$ 조각을 Top-$B_{probe}$ 또는 적응형 Top-$P$를 통해 탐색합니다.

Result: ShardMemo는 질문 범주 전반에 걸쳐 가장 강력한 기준선(GAM)에 비해 +5.11에서 +6.82 F1을 개선했습니다. 고정 예산 라우팅 설정($B_{	ext{probe}}=3)에서는 코사인 대 프로토타입 조각 라우팅에 비해 +6.87 F1 개선을 이루면서 검색 작업을 줄였고(VecScan 521->414, -20.5%) p95 지연 시간도 단축했습니다(95->76 ms).

Conclusion: ShardMemo는 다양한 토큰 수에서 강력한 성능을 달성하며, ToolBench에서는 Tier C가 0.97 Precision@3과 1.94 StepRed로 개선됩니다.

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [31] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: 메타 컨텍스트 엔지니어링(MCE)는 기존의 정적 컨텍스트 엔지니어링(CE) 방법을 대체하는 이중 구조의 프레임워크로, CE 기술과 컨텍스트 아티팩트를 공동 진화시켜 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 운영 효율성은 추론 시간의 컨텍스트에 크게 의존한다. 이는 컨텍스트 엔지니어링(CE)을 최적화한 입력을 위한 공식적인 분야로 확립하였다.

Method: MCE는 정적 CE 휴리스틱을 초월하는 이중 수준 프레임워크를 소개한다. 메타 수준 에이전트는 에이전틱 크로스오버를 통해 엔지니어링 기술을 개선하고, 기본 수준 에이전트는 이 기술을 실행하여 훈련 롤아웃으로부터 학습하고 컨텍스트를 유연한 파일 및 코드로 최적화한다.

Result: MCE는 오프라인 및 온라인 환경에서 다섯 개의 이질적인 도메인에 걸쳐 평가되었으며, 최신 에이전틱 CE 방법에 비해 5.6--53.8%의 상대적 개선을 이루었다.

Conclusion: MCE는 컨텍스트 사용 및 훈련에서 우수한 적응성, 전이성 및 효율성을 유지하면서 일관된 성능 향상을 보여준다.

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [32] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: EmboCoach-Bench는 LLM 에이전트가 자율적으로 구현된 정책을 설계할 수 있는 능력을 평가하는 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: Embodied AI 분야는 일반 목적의 로봇 시스템을 향한 빠른 진화를 목격하고 있으나, 노동 집약적인 수동 감독에 의존하고 있다.

Method: 32개의 전문가가 선정한 RL 및 IL 작업을 아우르며, 실행 가능한 코드를 보편적 인터페이스로 설정한다.

Result: 자율 에이전트는 평균 성공률에서 인간이 설계한 기준을 26.5% 초과하며, 환경 피드백을 활용한 에이전트 워크플로우가 정책 개발을 강화함을 보여준다.

Conclusion: 이 연구는 노동 집약적인 수동 조정에서 확장 가능한 자율 엔지니어링으로의 패러다임 전환을 가속할 기반을 마련한다.

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [33] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: Agent Workflow Optimization (AWO)는 에이전트 AI의 작업 흐름을 최적화하는 방법을 제시하며, 중복된 도구 실행 패턴을 찾아 효율성과 강인성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 AI가 복잡한 작업을 해결하기 위해 도구와 상호작용하는 과정에서 발생하는 비효율성 문제를 해결하고자 함.

Method: 기존 작업 흐름 추적을 분석하여 반복적인 도구 호출 시퀀스를 찾아내고 이를 메타 도구로 변환하여 여러 에이전트 행동을 단일 호출로 묶음.

Result: AWO는 LLM 호출 수를 최대 11.9% 감소시키고, 작업 성공률을 최대 4.2% 포인트 향상시킴을 보여줌.

Conclusion: 메타 도구를 활용하여 불필요한 중간 추론 단계를 생략하고 운영 비용을 줄이며 실패를 감소시킴.

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [34] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: RecNet은 사용자와 아이템 간의 실시간 선호 업데이트를 지원하는 자기 진화형 선호 전파 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 추천 시스템은 사용자-아이템 간의 명시적 상호작용에 의존하여 제한적인 선호 변화를 모델링합니다.

Method: RecNet은 선호 경로를 동적으로 전파하기 위해 두 가지 상호 보완적인 단계를 포함합니다: 전방 단계와 후방 단계.

Result: 다양한 시나리오에서의 실험을 통해 RecNet의 효과성을 입증했습니다.

Conclusion: RecNet은 추천 시스템에서 선호 전파 모델링을 효율적으로 수행할 수 있습니다.

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [35] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 계산은 계산되는 것에 의존해서는 안 되며, 알고리즘의 동작은 인자의 의미 내용에 불변해야 한다. We introduce WhatCounts to isolate this property, demonstrating LLMs' accuracy variation based on what is counted.


<details>
  <summary>Details</summary>
Motivation: 알고리즘의 동작이 인자의 의미에 의존하지 않아야 한다는 성질을 테스트하고자 함.

Method: WhatCounts를 사용하여 명확하게 구분된 중복 없는 항목 목록에서 아이템을 계산하도록 설계함.

Result: 프론티어 LLM들이 무엇을 계산하느냐에 따라 40% 이상의 정확도 변동을 보임을 발견함.

Conclusion: LLM은 알고리즘을 구현하는 것이 아니라 근사하며, 이 근사는 인자에 따라 달라질 수 있다.

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [36] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: 이 연구에서는 E-mem 프레임워크를 제안하여 LLM 에이전트의 System~2 추론을 개선하고 메모리 전처리에서 서사적 문맥 재구성으로 전환합니다.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) 에이전트의 System~2 추론으로의 발전은 정교하고 높은 정확도의 문제 해결을 요구하지만, 기존의 메모리 전처리 방식은 맥락을 상실하는 문제를 겪고 있습니다.

Method: E-mem 프레임워크는 여러 보조 에이전트가 압축되지 않은 메모리 맥락을 유지하고 중앙 마스터 에이전트가 전반적인 계획을 조율하는 비동질적 계층 구조를 사용합니다. 이는 활성화된 구역 내에서 국소적으로 추론할 수 있도록 하여 맥락 인식 증거를 추출한 후 집계하는 방식을 적용합니다.

Result: LoCoMo 벤치마크 평가 결과, E-mem은 54	ext{ F1}을 달성하였으며, 최신 GAM을 7.75	ext{ F1} 초과 달성했습니다.

Conclusion: E-mem은 70	ext{ %} 이상의 토큰 비용 절감을 이루며 LLM 에이전트의 추론 능력을 향상시킵니다.

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [37] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS는 데이터의 소음을 효율적으로 처리하기 위한 모델 불가지론적 플러그인으로, 성능을 향상시키면서도 파라미터 오버헤드를 최소화한다.


<details>
  <summary>Details</summary>
Motivation: 딥 시계열 모델이 실제 응용 프로그램에서 흔히 발견되는 노이즈 데이터에 취약하다는 문제를 해결하고자 한다.

Method: Sample-Adaptive Dropout 메커니즘을 사용하여 스펙트럼 희소성을 활용하여 인스턴스 수준의 노이즈를 효율적으로 정량화하고, 적응형 드롭아웃 비율에 노이즈를 매핑하여 모델의 학습 용량을 동적으로 보정한다.

Result: DropoutTS는 다양한 노이즈 환경과 공개 벤치마크에서 우수한 성능 향상을 보여주며, 모델의 아키텍처 변경 없이도 우수한 성능을 유지한다.

Conclusion: DropoutTS는 고급 내구성을 제공하며, 거의 무시할 수 있는 파라미터 오버헤드로 성능을 향상시킨다.

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [38] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM)이 언어 기반 작업에서는 뛰어나지만 비언어적 환경에서의 한계를 극복하기 위해 새로운 탐색-활용 프레임워크인 SCOUT을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 성능 격차는 사전 훈련 분포와 테스트 분포의 불일치 때문이라고 여겨졌으나, 실제로는 탐색 비용이 주된 병목현상임을 보여준다.

Method: SCOUT은 탐색과 활용을 분리하여 경량의 '정찰병'(예: 작은 MLP)을 사용하여 환경 동태를 빠르고 광범위하게 조사한다.

Result: SCOUT을 통해 Qwen2.5-3B-Instruct 모델이 평균 점수 0.86을 달성하며, Gemini-2.5-Pro(0.60) 등 상용 모델을 크게 초월하였다.

Conclusion: SCOUT은 LLM이 높은 차원의 의미 공간에서 자세한 세부 사항을 숙달하는 데 필요한 시행착오를 효과적으로 줄이는 데 기여한다.

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [39] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: BioAgent Bench는 생물정보학 작업에서 AI 에이전트의 성능과 강건성을 측정하기 위해 설계된 벤치마크 데이터셋과 평가 도구를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트들이 생물정보학 분야에서 얼마나 효과적으로 작동하는지를 평가하고, 다양한 작업에서의 성능을 비교하기 위해.

Method: 종합적인 end-to-end 작업(예: RNA-seq, 변이 호출, 메타유전체학)으로 구성된 벤치마크를 사용하고, LLM 기반의 채점자를 이용해 성능을 평가한다.

Result: 최첨단 에이전트들이 복잡한 사용자 정의 구조 없이도 다단계 생물정보학 파이프라인을 완료할 수 있으며, 요청된 최종 결과물을 신뢰성 있게 생성한다는 것을 발견하였다.

Conclusion: 생물정보학 작업은 민감한 환자 데이터나 비공식적으로 공개된 IP를 포함할 수 있어 폐쇄형 모델은 특수한 개인정보 보호 환경에서 적절하지 않을 수 있으며, 이런 조건에서는 개방형 모델이 선호될 수 있다.

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [40] [CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge](https://arxiv.org/abs/2601.21822)
*Zitong Yu,Boquan Sun,Yang Li,Zheyan Qu,Xing Zhang*

Main category: cs.AI

TL;DR: CORE는 6G 네트워크에서 LLM의 협업을 통해 분산 AI 자원을 효율적으로 활용하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 6G 네트워크와 LLM의 발전은 연결성과 AI의 혁신을 가져오고 있지만, 복잡한 추론 작업을 수행하는 데 필요한 자원이 부족하다.

Method: CORE는 여러 LLM이 각기 다른 역할을 수행하며 분산되어 있는 협업 학습 시스템을 제공하고 세 가지 최적화 모듈을 통합한다.

Result: CORE의 경우 연구에 따른 사례 연구와 성능 평가를 통해 시스템 효율성과 작업 완료율이 현저히 향상되었다.

Conclusion: CORE는 실제 엣지 컴퓨팅 플랫폼에 배포되어 실제 운영 환경에서도 강력한 성능을 발휘함을 입증하였다.

Abstract: Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.

</details>


### [41] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 이 연구는 심전도(ECG) 전문가용 기초 모델(FM)에 대한 포괄적인 벤치마킹 프레임워크를 제안하며, 성능 기반 평가와 표현 수준 분석을 결합한다.


<details>
  <summary>Details</summary>
Motivation: 기초 모델(FM)의 활용이 의료와 같은 오류에 민감한 분야에서 얼마나 일반화 가능한지를 평가하고 책임감 있게 사용하기 위함이다.

Method: SHAP와 UMAP 기술을 활용하여 성능 기반 평가와 표현 수준 분석을 결합한 벤치마크 방법론을 제안한다.

Result: 여러 개의 ECG 전문가 FM에 대한 광범위한 평가 결과, 우리의 벤치마킹 프로토콜이 심전도 전문가 FM의 내재된 패턴에 대한 풍부한 통찰을 제공함을 보여준다.

Conclusion: 이는 이들의 표현 구조와 일반화 가능성을 심도 있게 이해할 수 있게 하며, 의료 데이터 부족 상황에서도 효과적으로 적용 가능한 통찰력을 제공한다.

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [42] [WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents](https://arxiv.org/abs/2601.21872)
*Yao Zhang,Shijie Tang,Zeyu Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: WebArbiter는 웹 탐색을 위한 새로운 보상 모델로, 상반된 행동을 강화하기 위한 이유 기반의 접근 방식을 채택합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 컴퓨터 작업을 자동화하기 위해 웹 에이전트의 잠재력을 활용하고자 하지만, 기존 접근 방식의 한계로 인해 효과적으로 학습하지 못하는 문제를 해결하려고 합니다.

Method: 보상 모델링을 텍스트 생성으로 공식화하고, 일관된 원칙 기반 추론을 통해 구조화된 정당화 결과를 제공합니다.

Result: WebArbiter-7B는 WebPRMBench에서 가장 강력한 기준인 GPT-5보다 9.1점 높은 성과를 기록하였습니다.

Conclusion: WebArbiter는 실제 복잡한 웹 작업에서 강력한 일반화 능력을 보여주며, 실용적인 가치가 있음을 입증합니다.

Abstract: Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.

</details>


### [43] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 본 논문에서는 동적, 다중 턴 워크플로우에서 계획과 실행의 공동 최적화를 위한 JADE 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: RAG의 진화는 정적 검색 파이프라인에서 중앙 계획자가 다중 턴 추론을 조율하는 동적 에이전틱 워크플로우로 전환되었습니다. 그러나 기존 패러다임은 고정 그래프(arhitecture) 내에서 모듈을 공동 최적화하거나 동적 계획을 지원하면서 실행기를 고정된 블랙박스 도구로 취급하는 두 가지 극단으로 나뉘어 있습니다.

Method: JADE는 단일 공유 백본 아래 통합된 협력 다중 에이전트 팀으로 시스템을 모델링하여 계획과 실행을 동적으로 공동 최적화합니다.

Result: JADE는 분리된 모듈을 시너지를 이루는 시스템으로 변형하여, 공동 최적화를 통해 성능을 획기적으로 향상시킵니다.

Conclusion: JADE는 동적 워크플로우 조율을 통해 효율성과 효과성 간의 유연한 균형을 가능하게 합니다.

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [44] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 이 논문에서는 중복 추론으로 인해 발생하는 추론 오버헤드를 줄이기 위한 협업 다중 에이전트 강화 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 추론 모델의 배포에 병목을 일으키고 상호작용 경험을 저해하는 중복 추론으로 인한 추론 오버헤드를 해결하고자 합니다.

Method: 중복 청크를 선택적으로 벌칙을 부과하고 필수적인 추론 논리를 보존하는 다중 에이전트 강화 학습(MARL) 프레임워크, Self-Compression via MARL (SCMA)을 제안합니다.

Result: 모델 스케일에 따른 실험 평가 결과, SCMA는 응답 길이를 11.1%에서 39.0% 줄이면서 정확도를 4.33%에서 10.02% 향상시킵니다.

Conclusion: MARL 프레임워크 내에서의 시너지 최적화가 더욱 강력한 대형 추론 모델을 생성함을 보여줍니다.

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [45] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw는 높은 이해관계의 표 형식 의사 결정 작업을 위한 투명하고 제어 가능한 테스트 시간 추론을 제공하는 역할 구조화된 다중 에이전트 토론 프레임워크입니다. 이 프레임워크는 구조화된 다중 에이전트 토론이 단일 에이전트 추론에 비해 더 안정적이고 일반화 가능한 성능을 제공한다는 결과를 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 투명성과 인간의 감독이 필요한 복잡한 결정 작업을 효과적으로 지원할 수 있는 다중 에이전트 시스템의 필요성을 강조합니다.

Method: 우리는 합의된 역할(검사, 방어, 판사)과 상호작용 프로토콜(7턴 구조적 토론)을 정의하고, 비공식적인 추론 전략을 통해 에이전트 간의 논리적 상호작용을 촉진합니다.

Result: AgenticSimLaw는 90개 이상의 고유한 모델 및 전략 조합을 비교했을 때 전통적인 사고의 흐름(Cot) 프롬프트보다 더 안정적이고 일반화 가능한 성능을 보였습니다.

Conclusion: 이 연구는 윤리적 복잡성 아래에서의 추론을 스트레스 테스트하기 위해 형사 사법 영역에서 이 프레임워크를 구현하였지만, 투명성과 인간의 감독이 필요한 모든 심사적이고 높은 이해관계의 의사 결정 작업으로 일반화할 수 있습니다.

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [46] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: 대형 언어 모델이 새로운 과학 정보를 논리적으로 처리할 수 있는지에 대한 불확실성이 갈수록 커지고 있다. DeR2라는 도구를 소개하며, 이 도구는 문서 기반의 추론을 격리시키면서도 심층 검색의 핵심 문제를 유지한다. 실험 결과는 여러 최신 모델 간의 큰 차이를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 진정한 과학 정보에 대해 추론할 수 있는지에 대한 불확실성을 해결하기 위하여.

Method: DeR2라는 통제된 심층 연구 샌드박스를 도입하고, 이를 통해 문서 접근을 추론과 분리하여 네 가지 조건으로 실험을 진행한다.

Result: 다양한 최신 모델들 간에 큰 차이와 상당한 개선 여지가 나타났으며, 일부 모델은 특정 조건에서 성능 저하를 보였다.

Conclusion: 이 연구는 매개변수 누출을 방지하는 두 단계 검증 방법을 적용하고, 실험 결과 및 모델 특성을 통해 대형 언어 모델의 추론 능력을 평가한다.

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [47] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver는 도구의 내재적 의미와 외재적 공용 패턴을 통합하여 도구를 계층적 시퀀스로 인코딩하는 새로운 생성 도구 학습 프레임워크로, 기존 방법들보다 더 효과적이고 확장 가능하다.


<details>
  <summary>Details</summary>
Motivation: 기존 검색 기반 도구 사용 파이프라인은 복잡한 의미를 포착하지 못하는 인코더와 자연어 사전 학습에서 도구 지식이 결여된 큰 언어 모델(LLM) 간의 이중 의미적 문제에 직면하고 있다.

Method: ToolWeaver는 도구를 계층적 시퀀스로 인코딩하는 새로운 생성 도구 학습 프레임워크를 제안한다.

Result: 도구 47,000개에 대한 평가 결과, ToolWeaver는 기존의 최신 방법들을 크게 능가하며 더 확장 가능하고 일반화된 의미 인식 기반을 제공한다.

Conclusion: 이 방법은 고급 도구 증강 에이전트를 위한 더 나은 기반을 마련한다.

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [48] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: 본 연구는 웹 페이지의 시각적 속성이 웹 에이전트의 의사 결정에 미치는 영향을 정량적으로 평가하기 위해 VAF라는 평가 파이프라인을 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 환경 변동의 효과에 대한 기존 연구는 주로 적대적 공격에 대한 견고성에 초점을 맞추었고, 온건한 시나리오에서 에이전트의 선호도에 대한 연구는 부족했습니다.

Method: 본 연구에서는 웹 페이지 시각적 속성이 웹 에이전트의 의사 결정에 미치는 영향을 정량적으로 평가하기 위해 VAF라는 세 단계로 구성된 평가 파이프라인을 도입합니다: (i) 변형 생성, (ii) 탐색 상호작용, (iii) 클릭 행동과 에이전트의 추론을 통한 검증.

Result: 실험 결과, 배경 색 대비, 아이템 크기, 위치, 카드 명료성이 에이전트의 행동에 강한 영향을 미치는 반면, 글꼴 스타일, 텍스트 색상, 아이템 이미지 명료성은 미미한 영향을 미친다는 것을 발견했습니다.

Conclusion: 이러한 결과는 웹 페이지 디자인에서 시각적 속성을 고려하는 것이 웹 에이전트의 의사 결정에 큰 영향을 미침을 시사합니다.

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [49] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 이 논문은 고유한 연관 이벤트를 통해 자율 에이전트의 적응형 조정을 지원하는 Liquid Interfaces라는 새로운 협업 패러다임을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 소프트웨어 아키텍처는 적응형, 확률적, 맥락 의존적인 추론을 지원하는 자율 에이전트를 지원하는 데 어려움을 겪고 있으며, 시스템 통합은 여전히 정적 인터페이스와 결정론적 계약에 의해 지배되고 있습니다.

Method: 이 논문은 의도 표현과 의미적 협상을 통해 런타임에 생성되는 비지속적인 관계 이벤트로서의 인터페이스 개념을 형식화하고, 의도 주도 상호작용과 협상된 실행을 조율하며 의미적 불확실성 하에서 덧없음을 강제하는 Liquid Interface Protocol (LIP)을 제시합니다.

Result: 이 접근 방식의 관리적 의미에 대해 논의하고 실용적 타당성을 입증하는 참고 아키텍처를 설명합니다.

Conclusion: Liquid Interfaces는 에이전트 기반 시스템에서 적응형 조정을 위한 원칙 기반의 토대를 제공합니다.

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [50] [Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference](https://arxiv.org/abs/2601.22001)
*Yiren Zhao,Junyi Liu*

Main category: cs.AI

TL;DR: AI 에이전트 추론은 데이터 센터의 미래를 이끌고 있으며, 메모리 용량, 메모리 대역폭 및 고속 인터커넥트와 같은 컴퓨팅을 초월한 병목 현상을 노출시킵니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 추론은 데이터 센터의 미래를 형성하고 있으며, 메모리 관련 한계를 이해하는 것이 중요합니다.

Method: Operational Intensity (OI)와 Capacity Footprint (CF)라는 두 가지 지표를 도입하여 고전적인 루프라인 분석의 한계를 보완합니다.

Result: OI/CF는 에이전트 워크플로우와 기본 모델 선택에 따라 크게 변화하며, 긴 컨텍스트 KV 캐시가 디코드를 메모리 바운드 상태로 만듭니다.

Conclusion: 에이전트 하드웨어 공동 설계와 메모리 디스어그리게이션이 큰 규모의 에이전틱 AI 추론을 위한 효율성과 기능을 유지하는 경로를 제시합니다.

Abstract: AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.

</details>


### [51] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench는 자동차 음성 비서 도메인에서 다중 턴 도구를 활용하는 LLM 에이전트의 일관성, 불확실성 처리 및 능력 인식을 평가하기 위한 새로운 벤치마크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 에이전트에 대한 벤치마크는 이상적인 설정에서의 작업 완료에 초점을 맞추었으나, 실제 사용자 인터페이스에서의 신뢰성을 간과하고 있다.

Method: CAR-bench는 LLM 시뮬레이션 사용자, 도메인 정책 및 내비게이션, 생산성, 충전 및 차량 제어를 포함하는 58개의 상호 연결된 도구들을 특징으로 하는 환경을 제공한다.

Result: 기본 결과는 모든 작업 유형에서 일회성 성공과 지속적인 성공 간의 큰 격차를 보여준다. 최전선 추론 LLM조차도 불확실성 해결 작업에서 일관된 통과율이 50% 미만이며, 정책을 자주 위반하거나 정보를 조작하여 사용자 요청을 충족시키는 경우가 많다.

Conclusion: 이 연구는 실제 환경에서 더 신뢰할 수 있고 자기 인식이 있는 LLM 에이전트가 필요하다는 점을 강조한다.

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [52] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: 이 논문에서는 복잡한 기업 시스템에서 대규모 언어 모델의 성능을 평가하기 위해 World of Workflows(WoW)라는 새로운 환경과 벤치마크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 기업 벤치마크가 일반 소비자 벤치마크와 유사하게 단순한 과제 완료만 평가하고 있어 기업의 진정한 문제를 간과하고 있습니다.

Method: ServiceNow 기반의 WoW 환경에서는 4,000개 이상의 비즈니스 규칙과 55개의 활성 워크플로우가 포함되어 있으며, WoW-bench는 제약된 에이전트 과제 완료 및 기업 역학 모델링 기능을 평가하는 234개 과제로 구성됩니다.

Result: 프론티어 LLM은 다이나믹 블라인드 현상을 경험하며, 그들의 행동으로 인한 보이지 않는 cascading 부작용을 예측하지 못해 제약 위반이 발생합니다.

Conclusion: 신뢰할 수 있는 기업 에이전트를 위해서는 에이전트가 고충실도 피드백이 없을 때 관찰 가능성 격차를 해소하기 위해 숨겨진 상태 전환을 정신적으로 시뮬레이션해야 합니다. WoW는 시스템 역학을 명시적으로 학습해야 한다는 새로운 패러다임을 제안합니다.

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


### [53] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: Agentic RL의 성과 향상을 위한 새로운 보상 모델인 Agent-RRM을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 방법들이 희귀한 결과 기반 보상에 의존하여 중간 추론 품질을 구분하지 못하는 문제.

Method: Agent-RRM은 명시적인 추론 자취, 비판적 피드백, 성과 평가 점수를 포함하는 다면적인 보상 모델이다.

Result: 12개의 다양한 벤치마크에서 Reagent-U가 GAIA에서 43.7%, WebWalkerQA에서 46.2%의 성과를 달성했다.

Conclusion: 이 연구는 우리의 추론 보상 모델과 훈련 방식의 효과를 검증한다.

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges](https://arxiv.org/abs/2601.20913)
*Chen Feng,Minghe Shen,Ananth Balashankar,Carsten Gerner-Beuerle,Miguel R. D. Rodrigues*

Main category: cs.LG

TL;DR: 이 논문은 안전 기준 이하의 실패율을 확인하는 대형 언어 모델(LLM)의 신뢰할 수 있는 인증을 위한 새로운 가설 테스트 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 인증은 매우 중요하지만 도전적이다. '판사로서의 LLM' 접근 방식은 확장성을 제공하지만 판사의 결점, 잡음 및 편향이 통계적 보증을 무효화할 수 있다.

Method: 소규모 인간 라벨링된 교정 세트를 활용해 판사의 진정한 긍정률과 거짓 긍정률을 추정하고, 이를 대규모 판사 라벨링된 데이터 세트에 적용하여 분산 보정된 기준 임계값을 도출하는 '잡음 있지만 유효한' 가설 테스트 프레임워크를 제시한다.

Result: Jigsaw Comment, Hate Speech 및 SafeRLHF에 대한 실험을 통해 이론을 검증하고, 실제 방법과 이론적 오라클의 성능 차이를 드러내며, 추정 비용을 정량화한다.

Conclusion: 이 연구는 불완전한 판사 설정에 대한 최초의 체계적인 처리를 제공하고, 판사의 신뢰성에 대한 해석할 수 있는 진단을 생성하며, 평가 능력이 판사 품질, 데이터 세트 크기 및 인증 수준에 따라 어떻게 달라지는지를 명확히 한다.

Abstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While "LLM-as-a-Judge" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a "Noisy but Valid" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical "Oracle" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.

</details>


### [55] [MADE: Benchmark Environments for Closed-Loop Materials Discovery](https://arxiv.org/abs/2601.20996)
*Shreshth A Malik,Tiarnan Doherty,Panagiotis Tigas,Muhammed Razzak,Stephen J. Roberts,Aron Walsh,Yarin Gal*

Main category: cs.LG

TL;DR: MADE는 자율 재료 발견 파이프라인을 벤치마킹하기 위한 새로운 프레임워크로, 제한된 예산 하에서 재료 후보를 제안, 평가, 개선하는 닫힌 루프 발견 캠페인을 시뮬레이션한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 재료 발견 벤치마크는 정적인 예측 작업이나 고립된 연산 하위 작업만을 평가하며, 과학적 발견의 반복적이고 적응적인 본질을 간과한다.

Method: MADE는 사용자들이 생성 모델, 필터 및 계획자와 같은 상호 교환 가능한 구성 요소로 발견 에이전트를 구성할 수 있도록 하여 고정된 파이프라인에서 도구 사용 및 적응적 의사 결정을 포함한 완전한 에이전트 시스템에 이르는 임의의 작업 흐름을 연구할 수 있게 한다.

Result: MADE를 통해 사용자들이 발견 파이프라인에서 구성 요소의 제거 실험을 수행하고 시스템 복잡도에 따라 방법의 계층적 확장을 비교할 수 있다.

Conclusion: 이 프레임워크는 자율 재료 발견의 실제 워크플로우의 순차적이고 자원 제한적인 특성을 포착하여, 발견을 위해 열역학적으로 안정한 화합물을 찾는 것으로 공식화한다.

Abstract: Existing benchmarks for computational materials discovery primarily evaluate static predictive tasks or isolated computational sub-tasks. While valuable, these evaluations neglect the inherently iterative and adaptive nature of scientific discovery. We introduce MAterials Discovery Environments (MADE), a novel framework for benchmarking end-to-end autonomous materials discovery pipelines. MADE simulates closed-loop discovery campaigns in which an agent or algorithm proposes, evaluates, and refines candidate materials under a constrained oracle budget, capturing the sequential and resource-limited nature of real discovery workflows. We formalize discovery as a search for thermodynamically stable compounds relative to a given convex hull, and evaluate efficacy and efficiency via comparison to baseline algorithms. The framework is flexible; users can compose discovery agents from interchangeable components such as generative models, filters, and planners, enabling the study of arbitrary workflows ranging from fixed pipelines to fully agentic systems with tool use and adaptive decision making. We demonstrate this by conducting systematic experiments across a family of systems, enabling ablation of components in discovery pipelines, and comparison of how methods scale with system complexity.

</details>


### [56] [Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference](https://arxiv.org/abs/2601.21012)
*Young Kyung Kim,Oded Schlesinger,Qiangqiang Wu,J. Matías Di Martino,Guillermo Sapiro*

Main category: cs.LG

TL;DR: OATTA는 기존의 TTA 방법보다 성능을 개선하며, 시간적 동적 과정을 모델링하여 예측 정확도를 높인다.


<details>
  <summary>Details</summary>
Motivation: 기존 TTA 방법은 독립적인 샘플로만 다루어 시간적 동적 신호를 간과한다.

Method: OATTA는 학습된 동적 전이 행렬을 사용한 그래디언트 없는 재귀 베이지안 추정 작업으로 TTA를 공식화한다.

Result: OATTA는 다양한 분야에서 기존 기준선을 향상시켜 정확도를 최대 6.35% 증가시킨다.

Conclusion: 시간적 동적 과정을 모델링하는 것이 일반적인 TTA 접근 방식 이상의 중요한 신호를 제공한다.

Abstract: Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.

</details>


### [57] [StepShield: When, Not Whether to Intervene on Rogue Agents](https://arxiv.org/abs/2601.22136)
*Gloria Felicia,Michael Eniolade,Jinfeng He,Zitha Sasindran,Hemant Kumar,Milan Hussain Angati,Sandeep Bandarupalli*

Main category: cs.LG

TL;DR: StepShield은 위반 감지 시점을 평가하는 첫 번째 벤치마크로, 9,213개의 코드 에이전트 궤적을 포함하고 있다.


<details>
  <summary>Details</summary>
Motivation: 현재의 벤치마크는 조기 개입과 사후 분석을 혼동하여 이 둘을 정확하게 측정할 수 없다.

Method: StepShield는 9,213개의 코드 에이전트 궤적을 포함하며, 3가지 새로운 시간 기반 메트릭을 제안한다.

Result: LLM 기반의 판단자가 59%의 조기 개입률을 달성하는 반면, 정적 분석기는 26%에 불과하여 2.3배의 성능 격차를 보인다.

Conclusion: StepShield는 평가 초점을 위반 여부에서 위반 시점으로 전환하여 더 안전하고 경제적인 AI 에이전트를 구축할 수 있는 새로운 기초를 제공한다.

Abstract: Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.

</details>


### [58] [Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning](https://arxiv.org/abs/2601.21037)
*Chengzu Li,Zanyi Wang,Jiaang Li,Yi Xu,Han Zhou,Huanyu Zhang,Ruichuan An,Dengyang Jiang,Zhaochong An,Ivan Vulić,Serge Belongie,Anna Korhonen*

Main category: cs.LG

TL;DR: 비디오 생성 모델을 통해 시각적 추론을 형식화하는 연구로, 미세한 공간 이해와 지속적인 행동 계획에서의 성능을 평가.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델은 텍스트 추론에서 뛰어난 성능을 보이나, 복잡한 시각적 추론에는 부족하다.

Method: 비디오 생성 모델을 사용하여 시각적 추론을 형식화하고, 생성된 프레임을 초기 상태와 해결책 사이의 중간 추론 단계로 활용한다.

Result: 미로 탐색과 탕그램 퍼즐 두 가지 작업에서 안정적인 제로샷 일반화와 강력한 성능을 보였다.

Conclusion: 비디오 생성은 단순한 미디어 도구가 아니라 시각적 추론을 위한 확장 가능하고 일반화 가능한 패러다임이다.

Abstract: Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.

</details>


### [59] [Textual Equilibrium Propagation for Deep Compound AI Systems](https://arxiv.org/abs/2601.21064)
*Minghui Chen,Wenlong Deng,James Zou,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 새로운 Textual Equilibrium Propagation(TEP)이라는 방법을 통해 긴 기간의 워크플로우에서 언어 모델의 성능 저하 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 복합 AI 시스템에서 긴 워크플로우의 여러 모듈을 조정하는 LLM의 성능을 최적화하려는 필요성.

Method: TEP는 두 단계로 구성되어 있으며, 첫 번째 단계에서는 로컬 LLM 비평가가 프롬프트를 반복적으로 수정하고 두 번째 단계에서는 제한된 수정 강도를 사용하여 인근 프롬프트 수정을 적용한다.

Result: TEP는 TextGrad와 같은 전역 전파 방법에 비해 정확성과 효율성을 일관되게 향상시킨다.

Conclusion: TEP는 글로벌 텍스트 백프로파게이션의 계산 부담과 신호 손실 없이 블랙박스 LLM 구성 요소의 실용성을 유지하며 성능을 개선한다.

Abstract: Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.

</details>


### [60] [Generalized Information Gathering Under Dynamics Uncertainty](https://arxiv.org/abs/2601.21988)
*Fernando Palafox,Jingqi Li,Jesse Milzman,David Fridovich-Keil*

Main category: cs.LG

TL;DR:  이 논문은 동적 시스템에서의 정보 수집 비용을 명확하게 규명하는 통합 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 동적 시스템에서 작동하는 에이전트는 관찰을 통해 그 동역학을 학습해야 합니다. 능동적인 정보 수집은 이 학습을 가속화하지만, 기존 방법은 특정 모델링 선택에 대한 맞춤 비용을 파생합니다.

Method: 본 논문에서는 매시의 방향 정보에 기초하여 일반적인 정보 수집 비용을 도출하는 통합 프레임워크를 제시합니다. 이 프레임워크는 파라미터, 신념 및 제어 간의 인과적 의존성을 명시적으로 노출시켜 정보 수집 비용을 해제합니다.

Result: 기존 문헌에서 사용되는 상호 정보 비용이 우리의 비용의 특수한 경우임을 증명했습니다. 또한, 우리의 프레임워크를 활용하여 상호 정보 비용과 선형화된 베이지안 추정에서의 정보 이득 간의 명시적 연결을 확립했습니다.

Conclusion: 이론적으로 상호 정보 기반의 능동 학습 접근법에 대한 정당성을 제공하며, 선형, 비선형 및 다중 에이전트 시스템을 포함한 다양한 실험을 통해 우리의 프레임워크의 실제 유용성을 보여줍니다.

Abstract: An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.

</details>


### [61] [Breaking the Reasoning Horizon in Entity Alignment Foundation Models](https://arxiv.org/abs/2601.21174)
*Yuanning Cui,Zequn Sun,Wei Hu,Kexuan Xin,Zhangjie Fu*

Main category: cs.LG

TL;DR: 본 논문에서는 지식 그래프 융합을 위한 엔터티 정렬(EA)의 중요성을 강조하고, 기존 EA 모델의 한계점을 해결하기 위해 병렬 인코딩 전략에 기반한 EA 기초 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 EA 모델은 이전에 보지 못한 KG와의 정렬을 위한 전이가 불가능하고 재훈련이 필요합니다.

Method: 병렬 인코딩 전략을 통해 EA 기초 모델을 제안하고, 시드 EA 쌍을 지역 앵커로 활용하여 정보 흐름을 안내하며 두 개의 병렬 스트림을 동시에 초기화 및 인코딩합니다.

Result: 실험 결과, 제안한 프레임워크가 강력한 일반화 능력을 가지고 있으며, 이전에 보지 못한 KG에 대한 효과성을 입증하였습니다.

Conclusion: 제안된 EA 모델은 지역 구조 근접성을 활용하여 추론 경로를 단축하고, 글로벌 종속성을 모델링하는 통합 관계 그래프와 정확한 매칭을 위한 학습 가능한 상호작용 모듈을 포함합니다.

Abstract: Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical "reasoning horizon gap": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.

</details>


### [62] [Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks](https://arxiv.org/abs/2601.21242)
*Luwei Sun,Dongrui Shen,Jianfe Li,Yulong Zhao,Han Feng*

Main category: cs.LG

TL;DR: 조건부 생성 모델링의 도전 과제를 해결하기 위해, 이 논문은 비율형 함수형을 근사화하기 위한 이론적 프레임워크를 개발한다.


<details>
  <summary>Details</summary>
Motivation: 조건부 생성 모델링에서 목표 조건부 밀도가 f1과 f2의 비율 형태를 취하는 데 따른 도전 과제.

Method: SignReLU 활성화 함수를 가진 심층 신경망을 이용하여 비율형 함수형을 근사하는 방법을 제시한다.

Result: L^p(Omega) 근사 경계 및 수렴 속도를 확립하고, DDPM에 특수화하여 역 과정의 SignReLU 기반 신경 추정기를 구성하며 생성된 데이터 분포와 실제 데이터 분포 사이의 초과 KL 위험에 대한 경계를 유도한다.

Conclusion: 이 분석은 초과 위험을 근사 오류 및 추정 오류 구성 요소로 분해하며, 분산 기반 생성 모델의 유한 샘플 훈련에 대한 일반화 보장을 제공한다.

Abstract: Motivated by challenges in conditional generative modeling, where the target conditional density takes the form of a ratio f1 over f2, this paper develops a theoretical framework for approximating such ratio-type functionals. Here, f1 and f2 are kernel-based marginal densities that capture structured interactions, a setting central to diffusion-based generative models. We provide a concise proof for approximating these ratio-type functionals using deep neural networks with the SignReLU activation function, leveraging the activation's piecewise structure. Under standard regularity assumptions, we establish L^p(Omega) approximation bounds and convergence rates. Specializing to Denoising Diffusion Probabilistic Models (DDPMs), we construct a SignReLU-based neural estimator for the reverse process and derive bounds on the excess Kullback-Leibler (KL) risk between the generated and true data distributions. Our analysis decomposes this excess risk into approximation and estimation error components. These results provide generalization guarantees for finite-sample training of diffusion-based generative models.

</details>


### [63] [TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification](https://arxiv.org/abs/2601.21289)
*Akash Pandey,Payal Mohapatra,Wei Chen,Qi Zhu,Sinan Keten*

Main category: cs.LG

TL;DR: 새로운 explainability 중심의 딥러닝 프레임워크인 TimeSliver를 통해 시간 점의 기여도를 선형적으로 인코딩하여 예측의 투명성을 높이고, 다양한 시간 시계열 데이터셋에서 우수한 성능을 발휘함.


<details>
  <summary>Details</summary>
Motivation: 모델의 예측에 대한 각 시간 세그먼트의 영향을 식별하여 모델 결정 설명 및 투명성을 높이는 것이 중요하다.

Method: TimeSliver는 원시 시계열 데이터와 기호적 추상을 함께 활용하여 원래의 시간 구조를 유지하는 표현을 구성한다.

Result: TimeSliver는 7개의 다양한 합성 및 실제 다변량 시계열 데이터셋에서 다른 시간 기여도 방법보다 11% 우수한 성능을 보였다.

Conclusion: TimeSliver는 26개의 UEA 벤치마크 데이터셋에서 최첨단 기준선과 2% 이내의 예측 성능을 달성하며, 일반적인 시계열 분류를 위한 강력하고 설명 가능한 프레임워크로 자리 잡았다.

Abstract: Identifying the extent to which every temporal segment influences a model's predictions is essential for explaining model decisions and increasing transparency. While post-hoc explainable methods based on gradients and feature-based attributions have been popular, they suffer from reference state sensitivity and struggle to generalize across time-series datasets, as they treat time points independently and ignore sequential dependencies. Another perspective on explainable time-series classification is through interpretable components of the model, for instance, leveraging self-attention mechanisms to estimate temporal attribution; however, recent findings indicate that these attention weights often fail to provide faithful measures of temporal importance. In this work, we advance this perspective and present a novel explainability-driven deep learning framework, TimeSliver, which jointly utilizes raw time-series data and its symbolic abstraction to construct a representation that maintains the original temporal structure. Each element in this representation linearly encodes the contribution of each temporal segment to the final prediction, allowing us to assign a meaningful importance score to every time point. For time-series classification, TimeSliver outperforms other temporal attribution methods by 11% on 7 distinct synthetic and real-world multivariate time-series datasets. TimeSliver also achieves predictive performance within 2% of state-of-the-art baselines across 26 UEA benchmark datasets, positioning it as a strong and explainable framework for general time-series classification.

</details>


### [64] [Intrinsic Reward Policy Optimization for Sparse-Reward Environments](https://arxiv.org/abs/2601.21391)
*Minjae Cho,Huy Trong Tran*

Main category: cs.LG

TL;DR: 이 논문에서는 희소 보상 환경에서 외부 보상을 최적화하기 위한 정책 최적화 프레임워크인 IRPO를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 탐사는 에이전트가 최적의 정책을 학습하기 위해 필수적이다. 그러나 보상이 희소할 때, 단순한 탐사 전략은 종종 충분하지 않다.

Method: 우리는 다수의 내재적 보상을 활용하여 하위 정책을 사전 학습하지 않고도 외부 보상을 최적화하기 위한 정책 최적화 프레임워크를 제안한다.

Result: IRPO는 희소 보상 환경에서 더 유용한 학습 신호를 제공하는 대리 정책 경량화를 사용하여 성능과 샘플 효율성을 개선한다.

Conclusion: IRPO는 이산 및 연속 환경에서 기준선에 비해 성능과 샘플 효율성을 개선하며, IRPO가 해결하는 최적화 문제를 형식적으로 분석한다.

Abstract: Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.

</details>


### [65] [HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing](https://arxiv.org/abs/2601.21459)
*Chengyu Du,Xintao Wang,Aili Chen,Weiyuan Li,Rui Xu,Junteng Liu,Zishan Huang,Rong Tian,Zijun Sun,Yuhao Li,Liheng Feng,Deming Ding,Pengyu Zhao,Yanghua Xiao*

Main category: cs.LG

TL;DR: 본 논문은 인지 수준의 페르소나 시뮬레이션을 위한 HER라는 통합 프레임워크를 제안하며, 인물의 1인칭 사고와 LLM의 3인칭 사고를 구별하는 이중 계층 사고를 도입합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 역할놀이에서 인물의 행동 뒤에 있는 내면의 생각을 시뮬레이션하는 것이 도전 과제입니다.

Method: HER 프레임워크를 통해 이중 계층 사고를 도입하고, 이유 기반 역할 놀이 데이터와 인체 호환 원칙 및 보상 모델을 구축합니다.

Result: 우리의 모델은 Qwen3-32B 기준선 대비 30.26의 개선과 Minimax Role-Play Bench에서 14.97의 향상을 기록했습니다.

Conclusion: 우리의 데이터셋, 원칙 및 모델은 향후 연구를 지원하기 위해 공개될 예정입니다.

Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

</details>


### [66] [Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21523)
*Bang Giang Le,Viet Cuong Ta*

Main category: cs.LG

TL;DR: 이 연구는 다중 에이전트 강화 학습에서 협력을 촉진하기 위한 방법을 제안하며, 에이전트 간의 상호작용 그래프를 활용하여 로컬 보상과 글로벌 보상의 장점을 통합한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습에서의 협력을 촉진하기 위해 에이전트들의 보상 신호를 집계하여 글로벌 보상을 형성하는 것이 중요하다.

Method: 에이전트 간의 상호작용을 나타내는 그래프를 사용하여, 개별 에이전트의 기여를 보다 세밀하게 식별하는 방법을 제안한다.

Result: 제안한 방법이 기존 로컬 및 글로벌 보상 설정에 비해 개선된 성능을 보여줌을 실험을 통해 입증한다.

Conclusion: 이 연구는 에이전트의 로컬 보상을 이용하여 협력 문제를 완화하면서도 글로벌 보상의 장점을 조합하는 새로운 접근 방식을 제공한다.

Abstract: To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.

</details>


### [67] [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 본 논문은 반복적인 MCMC 없이도 LLM의 성능을 향상시키는 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습 후 훈련(RL post-training)은 대형 언어 모델의 추론 성능 향상 방법으로 널리 사용되고 있으나, 그 효과가 새로운 능력의 습득보다는 분포의 정제에서 기인한다는 증거가 증가하고 있다.

Method: 우리는 토큰 레벨로 스케일된 낮은 온도 분포를 사용하여 전역 파워 분포를 근사하는 새로운 수식을 도출하였다. 이를 통해, 반복 훈련이나 검증 없이 기본 모델의 생성 분포를 자가 회귀적으로 정제하는 알고리즘을 제안한다.

Result: 우리의 방법은 MCMC 기반 샘플링에 비해 10배 이상의 추론 지연 시간 감소를 achieved하며, 외부 보상 없이도 1회 GRPO와 동등하거나 이를 초과하는 성능을 발휘한다.

Conclusion: 이 연구는 대형 언어 모델의 성능 향상을 위한 실용적인 대안을 제시하며, 더욱 효율적인 샘플링 기법을 제공한다.

Abstract: Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.

</details>


### [68] [Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps](https://arxiv.org/abs/2601.21624)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 현대 딥러닝 훈련은 메모리 없는 상태가 아니며, 업데이트는 옵티마이저의 모멘트와 평균, 데이터 순서 정책, 비볼록 경로 및 보조 상태에 따라 달라진다.


<details>
  <summary>Details</summary>
Motivation: 딥러닝 훈련 과정에서 여러 요소들이 기억을 갖고 있기 때문에 이를 체계적으로 이해하고 정리할 필요가 있다.

Method: 이 설문조사는 원천, 생애 주기, 가시성에 따라 메커니즘을 조직하며, 씨드 쌍, 함수 공간 인과 추정량, 휴대 가능한 교란 원시 요소를 소개한다.

Result: 훈련 이력이 모델, 데이터, 체제에 따라 얼마나 중요한지를 귀속 시키는 데 중점을 둔 포터블 인과 불확실성 인식 측정 프로토콜을 제안한다.

Conclusion: 이 논문은 훈련 과정에서의 메모리 효과를 더 잘 이해하고 측정할 수 있는 새로운 방법론을 제시한다.

Abstract: Modern deep-learning training is not memoryless. Updates depend on optimizer moments and averaging, data-order policies (random reshuffling vs with-replacement, staged augmentations and replay), the nonconvex path, and auxiliary state (teacher EMA/SWA, contrastive queues, BatchNorm statistics). This survey organizes mechanisms by source, lifetime, and visibility. It introduces seed-paired, function-space causal estimands; portable perturbation primitives (carry/reset of momentum/Adam/EMA/BN, order-window swaps, queue/teacher tweaks); and a reporting checklist with audit artifacts (order hashes, buffer/BN checksums, RNG contracts). The conclusion is a protocol for portable, causal, uncertainty-aware measurement that attributes how much training history matters across models, data, and regimes.

</details>


### [69] [SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning](https://arxiv.org/abs/2601.21649)
*Jinjun Peng,Magnus Saebo,Tianjun Zhong,Yi-Jie Cheng,Junfeng Yang,Baishakhi Ray,Simin Chen,Yangruibo Ding*

Main category: cs.LG

TL;DR: 이 논문에서는 프로그래밍 언어 모델의 성능을 높이기 위해 새롭게 제안한 저장소 중심 학습(RCL) 패러다임을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 개인정보 보호에 민감하고 자원이 제한된 환경에서 코딩 에이전트를 배치하는 요구가 커지고 있다.

Method: 저장소 중심 학습(RCL) 패러다임을 제안하고, 수직적인 저장소 깊이를 우선시하여 소프트웨어 환경의 '물리학'을 내부화하도록 한다.

Result: SWE-Spot-4B라는 고도로 컴팩트한 모델 패밀리를 훈련하여 기존의 오픈 가중치 모델을 능가하고 효율성을 중심으로 개발된 상업 모델과도 경쟁한다.

Conclusion: 저장소 마스터리의 중요성을 강조하며, 이는 일반적인 코딩 능력을 보완하는 독특하고 필요한 차원이다.

Abstract: The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the "physics" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.

</details>


### [70] [SENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework](https://arxiv.org/abs/2601.21664)
*Xingyue Zhang,Yuxuan Bao,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: SENDAI는 하이퍼 스파스 센서 관측으로부터 전체 공간 상태를 재구성하는 효율적인 데이터 동화 프레임워크로, 다양한 환경에서 뛰어난 성능을 보여주며 특히 구조적으로 중요한 요소들을 잘 보존합니다.


<details>
  <summary>Details</summary>
Motivation: 데이터가 풍부한 학습 환경과 관측이 드문 배치 조건 간의 격차를 해소하는 것이 시공간 필드 재구성의 주요 도전 과제입니다.

Method: SENDAI는 시뮬레이션에서 파생된 사전 정보와 학습된 불일치 수정 사항을 결합하여 하이퍼 스파스 센서 관측에서 전체 공간 상태를 재구성하는 계층적 프레임워크입니다.

Result: 이 프레임워크는 MODIS 식생 지수 필드를 세계 여러 지역에서 재구성하며, 기존의 밀집 관측이 필요한 방식보다 성능이 우수합니다.

Conclusion: SENDAI는 물리 기반 추론, 자원 제한 환경 및 실시간 모니터링 및 제어에 적용할 수 있는 경량의 운영 가능한 스파스 측정 재구성을 위한 프레임워크를 제공합니다.

Abstract: Bridging the gap between data-rich training regimes and observation-sparse deployment conditions remains a central challenge in spatiotemporal field reconstruction, particularly when target domains exhibit distributional shifts, heterogeneous structure, and multi-scale dynamics absent from available training data. We present SENDAI, a hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework that reconstructs full spatial states from hyper sparse sensor observations by combining simulation-derived priors with learned discrepancy corrections. We demonstrate the performance on satellite remote sensing, reconstructing MODIS (Moderate Resolution Imaging Spectroradiometer) derived vegetation index fields across six globally distributed sites. Using seasonal periods as a proxy for domain shift, the framework consistently outperforms established baselines that require substantially denser observations -- SENDAI achieves a maximum SSIM improvement of 185% over traditional baselines and a 36% improvement over recent high-frequency-based methods. These gains are particularly pronounced for landscapes with sharp boundaries and sub-seasonal dynamics; more importantly, the framework effectively preserves diagnostically relevant structures -- such as field topologies, land cover discontinuities, and spatial gradients. By yielding corrections that are more structurally and spectrally separable, the reconstructed fields are better suited for downstream inference of indirectly observed variables. The results therefore highlight a lightweight and operationally viable framework for sparse-measurement reconstruction that is applicable to physically grounded inference, resource-limited deployment, and real-time monitor and control.

</details>


### [71] [ECSEL: Explainable Classification via Signomial Equation Learning](https://arxiv.org/abs/2601.21789)
*Adia Lumadjeng,Ilker Birbil,Erman Acar*

Main category: cs.LG

TL;DR: ECSEL은 형식 표현을 학습하는 설명 가능한 분류 방법으로, 상징 회귀 벤치마크에서 간결한 signomial 구조를 활용하여 높은 정확도를 보입니다.


<details>
  <summary>Details</summary>
Motivation: 많은 상징 회귀 벤치마크가 간결한 signomial 구조를 허용한다는 관찰에 의해 동기를 부여받았습니다.

Method: ECSEL은 분류기와 설명으로 작용하는 구조적 폐쇄형 표현을 직접 구성합니다.

Result: 전통적인 상징 회귀 벤치마크에서 기존의 최첨단 접근 방식보다 목표 방정식을 더 많이 회복하면서도 계산량이 크게 적습니다.

Conclusion: ECSEL은 해석 가능성을 포기하지 않으면서도 established 머신러닝 모델과 경쟁력 있는 분류 정확도를 달성합니다.

Abstract: We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.

</details>


### [72] [Constrained Meta Reinforcement Learning with Provable Test-Time Safety](https://arxiv.org/abs/2601.21845)
*Tingting Ni,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 본 논문에서는 제한된 메타 강화 학습을 통해 실제 테스트 작업에서 정책의 안전성을 보장하면서도 샘플 복잡성을 줄이는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 제한된 메타 강화 학습에서 테스트 작업의 정책 안전성을 보장하는 것은 중요한 문제입니다.

Method: 훈련 중 학습된 정책을 정제하는 알고리즘을 제안하며, 이는 테스트 작업에서 근사 최적 정책을 학습하기 위한 안전성과 샘플 복잡성 보장을 제공합니다.

Result: 제안된 알고리즘은 테스트 작업에서의 정책 학습에 대해 안전성과 샘플 복잡성 보장이 가능함을 증명하였습니다.

Conclusion: 이 논문은 안전성을 통합한 메타 RL을 위한 새로운 접근 방식을 제시하며 샘플 복잡성이 타이트하다는 것을 보여줍니다.

Abstract: Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.

</details>


### [73] [Managing Solution Stability in Decision-Focused Learning with Cost Regularization](https://arxiv.org/abs/2601.21883)
*Victor Spitzer,Francois Sanson*

Main category: cs.LG

TL;DR: 결정 중심 학습은 예측 모델링과 조합 최적화를 통합하여 예측 정확성뿐 아니라 결정 품질을 직접 개선하는 모델을 훈련시키는 접근 방식이다.


<details>
  <summary>Details</summary>
Motivation: 조합 최적화 문제를 푸는 데 있어 의사결정의 질을 향상시키는 것이 중요하다.

Method: 조합 최적화 문제의 목표 함수 계수를 추정하고, 추정된 비용 벡터의 정규화를 도입하여 학습 과정의 견고성과 신뢰성을 향상시켰다.

Result: 실험 결과, 학습 단계에서 발생하는 섭동 강도의 변동이 비효율적인 훈련으로 이어질 수 있음을 보여주었다.

Conclusion: 정규화 기법을 통해 학습 과정의 효율성을 높일 수 있다는 점을 입증하였다.

Abstract: Decision-focused learning integrates predictive modeling and combinatorial optimization by training models to directly improve decision quality rather than prediction accuracy alone. Differentiating through combinatorial optimization problems represents a central challenge, and recent approaches tackle this difficulty by introducing perturbation-based approximations. In this work, we focus on estimating the objective function coefficients of a combinatorial optimization problem. Our study demonstrates that fluctuations in perturbation intensity occurring during the learning phase can lead to ineffective training, by establishing a theoretical link to the notion of solution stability in combinatorial optimization. We propose addressing this issue by introducing a regularization of the estimated cost vectors which improves the robustness and reliability of the learning process, as demonstrated by extensive numerical experiments.

</details>


### [74] [Uncertainty-Aware Data-Based Method for Fast and Reliable Shape Optimization](https://arxiv.org/abs/2601.21956)
*Yunjia Yang,Runze Li,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: 불확실성 인식 데이터 기반 최적화(UA-DBO) 프레임워크를 통해 서브 모델의 불확실성을 관리하고, 예측 오류를 줄여 기존 DBO보다 뛰어난 성능을 보여줌.


<details>
  <summary>Details</summary>
Motivation: 데이터 기반 최적화(DBO)는 사전 학습된 서브 모델을 활용하여 공기역학 성능을 개선하기 위한 효율적인 최적화 접근 방식을 제공하지만, 훈련 데이터베이스의 품질에 크게 의존합니다.

Method: 확률적 인코더-디코더 서브 모델을 개발하여 출력과 관련된 불확실성을 예측하고, 이 불확실성을 모델 신뢰도 인식 객관적 함수에 통합하여 데이터 기반 최적화 과정에서 큰 예측 오류를 가진 샘플에 대한 패널티를 부여합니다.

Result: UA-DBO 프레임워크는 에어포일 항력 발산 및 뷔페 성능 향상을 목표로 두 가지 다중 점 최적화 문제에서 평가되어 예측 오류를 지속적으로 줄이며 원래 DBO보다 우수한 성능을 달성합니다.

Conclusion: UA-DBO는 전체 계산 시뮬레이션을 기반으로 한 다중 점 최적화와 비교할 때 최적화 효과가 유사하면서도 최적화 속도를 유의미하게 가속화합니다.

Abstract: Data-based optimization (DBO) offers a promising approach for efficiently optimizing shape for better aerodynamic performance by leveraging a pretrained surrogate model for offline evaluations during iterations. However, DBO heavily relies on the quality of the training database. Samples outside the training distribution encountered during optimization can lead to significant prediction errors, potentially misleading the optimization process. Therefore, incorporating uncertainty quantification into optimization is critical for detecting outliers and enhancing robustness. This study proposes an uncertainty-aware data-based optimization (UA-DBO) framework to monitor and minimize surrogate model uncertainty during DBO. A probabilistic encoder-decoder surrogate model is developed to predict uncertainties associated with its outputs, and these uncertainties are integrated into a model-confidence-aware objective function to penalize samples with large prediction errors during data-based optimization process. The UA-DBO framework is evaluated on two multipoint optimization problems aimed at improving airfoil drag divergence and buffet performance. Results demonstrate that UA-DBO consistently reduces prediction errors in optimized samples and achieves superior performance gains compared to original DBO. Moreover, compared to multipoint optimization based on full computational simulations, UA-DBO offers comparable optimization effectiveness while significantly accelerating optimization speed.

</details>


### [75] [Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2601.21978)
*Shiqi Fan,Quanming Yao,Hongyi Nie,Wentao Ma,Zhen Wang,Wen Hua*

Main category: cs.LG

TL;DR: IGETR는 그래프 신경망과 대형 언어 모델의 장점을 결합한 혼합 추론 프레임워크로, 정확하고 해석 가능한 예측을 생성한다.


<details>
  <summary>Details</summary>
Motivation: 기존 LLM 기반 추론 방법은 맥락적 관계를 우선시하며 동적 그래프에서 관련 하위 그래프를 추출하는 데 어려움을 겪어, 구조 정보 이해에 한계를 보인다.

Method: IGETR는 세 단계의 파이프라인을 통해 작동하며, 첫 번째 단계에서는 구조적이고 시간적으로 일관된 후보 경로를 식별하여 실제 데이터를 바탕으로 추론 과정을 설정한다. 두 번째 단계에서는 LLM에 의해 경로 편집을 하여 논리적이고 의미적인 불일치를 해결한다. 마지막 단계에서는 수정된 경로를 통합하여 정확하고 해석 가능한 예측을 생성한다.

Result: IGETR은 TKG 기준에서 최첨단 성과를 달성하며, ICEWS 데이터셋에서 Hits@1에서 최대 5.6%, Hits@3에서 8.1%의 상대적 향상을 보인다.

Conclusion: 각 구성 요소의 효과성을 확인하는 추가 분석을 수행하였다.

Abstract: Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.

</details>


### [76] [The Ensemble Inverse Problem: Applications and Methods](https://arxiv.org/abs/2601.22029)
*Zhengyan Huan,Camila Pazos,Martin Klassen,Vincent Croft,Pierre-Hugues Beauchemin,Shuchin Aeron*

Main category: cs.LG

TL;DR: 이 논문에서는 앙상블 역문제(EIP)라는 새로운 다변량 통계 문제를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: EIP의 목표는 전방 프로세스 하에서 사전의 푸시포워드를 기준으로 분포된 앙상블을 역으로 구하는 것입니다.

Method: 조건부 생성 모델의 새로운 클래스인 앙상블 역 생성 모델을 기반으로 한 비반복적 추론 시간 방법을 제안합니다.

Result: 제안된 방법이 여러 합성 및 실제 데이터 세트에서 성능을 벤치마킹함으로써 후방 추론에 도움이 되고 보이지 않는 사전으로의 일반화를 가능하게 함을 보여줍니다.

Conclusion: 제안된 훈련 절차가 가능도 모델을 암시적으로 인코딩하며, 앙상블 정보를 사용하는 것이 후방 추론에 도움이 됩니다.

Abstract: We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at https://github.com/ZhengyanHuan/The-Ensemble-Inverse-Problem--Applications-and-Methods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [77] [AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping](https://arxiv.org/abs/2601.21126)
*Kooktae Lee,Julian Martinez*

Main category: cs.MA

TL;DR: 이 논문은 한정된 감지 및 통신 환경에서 다중 에이전트(다중 로봇) 환경 매핑을 위한 AI 증강 분산 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 커버리지 모델이 정확한 참조 맵이 있을 때 효과적인 공간 할당을 달성하지만 불확실한 선행 조건 하에서는 성능이 저하된다는 문제를 해결하고자 합니다.

Method: 제안하는 방법은 최적 전송 기반 프레임워크 내에서 에이전트가 로컬 밀도 추정치를 반복적으로 개선할 수 있는 적응형 및 자기 수정 메커니즘을 도입합니다.

Result: 이중 다층 퍼셉트론(MLP) 모듈이 로컬 평균-분산 통계를 추론하고 긴 시간 동안 방문하지 않은 영역의 가상 불확실성을 조절하여 국부 최솟값에서의 정체 현상을 완화합니다.

Conclusion: Wasserstein 메트릭 하에서 수렴성을 엄밀하게 증명하고, 시뮬레이션 결과는 제안된 AI 증강 밀도 기반 최적 제어가 전통적인 분산 기준과 비교하여 복잡한 다중 모드 공간 분포의 고충실도 재구성을 일관되게 달성함을 보여줍니다.

Abstract: This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines.

</details>


### [78] [Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions](https://arxiv.org/abs/2601.21477)
*Tobias Schmidt,Kai Cui*

Main category: cs.MA

TL;DR: 이 논문은 대규모 희소 그래프에서 평균장 통제(MFC)의 새로운 이론적 틀을 제시하여 다중 에이전트 시스템의 차원 저주 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 차원 저주를 해결하기 위한 확장 가능한 솔루션이 필요하다.

Method: 희소 그래프에서 평균장 통제의 엄밀한 프레임워크를 제안하고, 시스템 상태를 장식된 루트 이웃에 대한 확률 측도로 재정의한다.

Result: 유한 기간 문제에서 에이전트의 최적 정책은 (T-t)-홉 이웃에 엄밀히 의존한다는 것을 입증하였다.

Conclusion: 이 결과는 무한 차원의 제어 문제를 다룰 수 있게 하며, 복잡한 희소 토폴로지에서 이론적으로 근거가 있는 제어를 가능하게 한다.

Abstract: Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.

</details>


### [79] [Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems](https://arxiv.org/abs/2601.22041)
*Naomi Pitzer,Daniela Mihai*

Main category: cs.MA

TL;DR: 이 연구는 다양한 양식의 에이전트가 포함된 통신 게임에서 에이전트 간의 상호작용 및 의사소통 방식을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계에서의 지각적 이질성을 간과한 채 동질적인 양식을 가정한 기존 연구에 의문을 제기하고, 에이전트들이 어떻게 공유된 구조적 표현을 개발하는지를 탐구하기 위함이다.

Method: 비균일한 다단계 이진 통신 게임에서 에이전트의 양식 차이를 고려하여 실험을 진행하였고, 비트 변동성과 상호 운용성 분석을 통해 통신의 본질을 규명하였다.

Result: 다양한 양식 시스템이 지각적 입력에 기반하여 일관된 메시지로 수렴하는 반면, 단일 양식 시스템은 효율적으로 소통하며 낮은 분류 엔트로피를 달성하였다.

Conclusion: Emergent communication을 에이전트가 이질적인 양식 간에 표현을 적응하고 전이하는 방법을 연구하기 위한 프레임워크로 설정하여 이론과 실험에서 새로운 방향을 열어준다.

Abstract: Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.

</details>
