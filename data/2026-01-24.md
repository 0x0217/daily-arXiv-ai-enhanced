<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Introducing the Generative Application Firewall (GAF)](https://arxiv.org/abs/2601.15824)
*Joan Vendrell Farreny,Martí Jordà Roca,Miquel Cornudella Gaya,Rodrigo Fernández Baón,Víctor García Martínez,Eduard Camacho Sucarrat,Alessandro Pignati*

Main category: cs.CR

TL;DR: 이 논문은 LLM 애플리케이션을 보호하기 위한 새로운 아키텍처인 생성 애플리케이션 방화벽(GAF)을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 방어 기법은 단편적이며, LLM 애플리케이션에 대한 통합적인 방어가 필요하다.

Method: GAF는 프롬프트 필터, 가드레일 및 데이터 마스킹을 하나의 집행 지점으로 통합하여, 웹 트래픽 방어를 조정하는 WAF와 유사하다.

Result: GAF는 자율 에이전트 및 이들의 도구 상호작용을 포함하여 다양한 방어 기능을 제공합니다.

Conclusion: GAF는 LLM 애플리케이션의 보안을 강화하는 새로운 접근 방식을 제안합니다.

Abstract: This paper introduces the Generative Application Firewall (GAF), a new architectural layer for securing LLM applications. Existing defenses -- prompt filters, guardrails, and data-masking -- remain fragmented; GAF unifies them into a single enforcement point, much like a WAF coordinates defenses for web traffic, while also covering autonomous agents and their tool interactions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: MiRAGE는 도메인 특정 멀티모달 질문-답변 데이터셋을 생성하기 위한 다중 에이전트 프레임워크로, 정보 검색 시스템의 평가를 위한 새로운 표준을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 데이터셋은 일반 도메인 말뭉치에 의존하거나 순수한 텍스트 검색에 의존하여, 전문 기술 문서의 복잡성을 반영하지 못합니다.

Method: MiRAGE는 전문 에이전트의 협력적 집합을 이용하여, 검증된 도메인 특정 멀티모달 질문-답변 데이터셋을 생성합니다.

Result: MiRAGE는 네 가지 상이한 도메인(규정, 금융, 정량 생물학, 저널리즘)에서 상당히 높은 추론 복잡도와 사실적인 신뢰성을 가진 데이터셋을 생성합니다.

Conclusion: MiRAGE는 고유한 말뭉치의 잠재적 주제 구조를 반영하는 표준 평가 데이터셋을 자동으로 생성함으로써, 차세대 정보 검색 시스템의 엄격한 벤치마킹을 위한 기반을 제공합니다.

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [3] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent는 통합 지식 추정, 기술 간극 식별 및 목표 자원 추천을 통해 개인화된 학습을 제공하는 다중 에이전트 교육 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 학생 결과를 향상시키기 위해 개인의 필요에 맞게 교육 콘텐츠, 진행 속도 및 피드백을 조정하는 개인화된 학습 시스템의 필요성

Method: ALIGNAgent는 학생 퀴즈 성적, 성적 데이터 및 학습자 선호도를 처리하여 기술 간극 에이전트를 사용하여 주제 수준의 능숙도 추정치를 생성하고, 그 후 추천 에이전트가 진단된 결핍에 맞는 학습 자료를 추천합니다.

Result: 대학 컴퓨터 과학 과정에서의 광범위한 실증 평가를 통해 ALIGNAgent는 지식 능숙도 추정에서 0.87-0.90의 정밀도와 0.84-0.87의 F1 점수를 달성했습니다.

Conclusion: ALIGNAgent는 개인화된 학습 경험을 제공하며, 실제 시험 성과에 대한 결과 검증을 통해 그 효과성이 입증되었습니다.

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [4] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon은 대규모 언어 모델의 한계를 극복하는 신경-상징 인지 운영 체제이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델은 자기 주의의 제곱의 계산 비용과 맥락 창이 확장됨에 따라 추론 능력이 저하되는 "중간에서 잃어버리기" 현상에 의해 근본적으로 제한받고 있다.

Method: Aeon은 메모리를 정적 저장소가 아닌 관리되는 OS 리소스로 재정의하고, 메모리를 메모리 궁전과 신경-상징 에피소드 그래프로 구조화한다. 또한 대화의 지역성을 활용해 서브 밀리초 검색 지연 시간을 달성하는 예측 캐싱 메커니즘인 SLB를 도입한다.

Result: 벤치마크 결과, Aeon은 대화 작업 부하에서 <1ms의 검색 지연 시간을 달성하며 상태 일관성을 보장한다.

Conclusion: Aeon은 자율 에이전트를 위한 지속적이고 구조화된 메모리를 효과적으로 가능하게 한다.

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [5] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: DFAH는 금융 서비스 내 도구 사용 에이전트의 궤적 결정성과 증거 기반 충실도를 측정하는 프레임워크를 소개한다. 비에이전트 실험 결과, 7-20B 매개변수 모델은 100% 결정성을 달성했으며, 에이전트 도구 사용은 추가 변동성을 초래했다. 결정성과 충실도 사이에 긍정적인 상관관계가 발견되었고, Tier 1 모델은 감사 재생 요구 사항에 부합하는 결정성 수준에 도달했다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트가 규제 감사 재생에 어려움을 겪고 있으며, 동일한 입력으로 플래그가 지정된 거래 결정을 재현할 때 일관된 결과를 반환하지 못하는 문제를 해결하기 위해.

Method: 결정성 및 증거 기반 충실도를 측정하기 위한 프레임워크인 결정성-충실도 보장 하네스(DFAH)를 도입하였고, 비에이전트 실험과 에이전트 도구 사용의 변동성을 분석했다.

Result: 74개의 구성에서 7-20B 매개변수 모델이 100% 결정성을 달성한 반면, 120B 이상의 모델은 동등한 통계적 신뢰성을 달성하기 위해 3.7배 더 많은 검증 샘플이 필요했다. 또한, 결정성과 충실도 사이에 긍정적인 피어슨 상관관계가 나타났다.

Conclusion: Tier 1 모델은 DFAH 평가 환경에서 감사 재생 요구 사항에 일치하는 결정성 수준을 달성하였다.

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [6] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 다양한 교통 사고 데이터를 활용하여 사고 심각도를 예측하는 TransportAgents라는 다중 에이전트 프레임워크를 제안하고, 기존 모델들보다 뛰어난 성능을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 교통 사고의 심각도를 정확하게 예측하는 것은 긴급 대응 및 공공 안전 계획 개선을 위해 매우 중요합니다.

Method: TransportAgents는 카테고리별 LLM 추론과 다층 퍼셉트론(MLP) 통합 모듈을 결합한 하이브리드 다중 에이전트 프레임워크입니다. 각 전문 에이전트는 인구통계, 환경 맥락 또는 사건 세부정보와 같은 특정 교통 정보의 하위 집합에 집중하여 중간 심각도 평가를 생성합니다.

Result: 미국의 두 상호 보완 데이터 세트(CPSRMS 및 NEISS)에서 TransportAgents가 전통적인 기계 학습 및 LLM 기반 기준선보다 consistently 더 나은 성과를 보임을 입증했습니다.

Conclusion: TransportAgents는 해석 가능성과 신뢰성을 강조하며, 안전 전담 의사결정 지원 애플리케이션에 적합합니다.

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [7] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: 세계 모델은 환경의 진화를 시뮬레이션하여 사고능력을 향상시키지만, 현재 모델은 시각적 융합 문제를 겪고 있다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 환경의 진화를 시뮬레이션하여 미래를 계획할 수 있도록 하는 것이 중요하다.

Method: 세계 모델을 시각적 엔진이 아닌 실행 가능한 시뮬레이터로 재구성하고, 구조적 4D 인터페이스와 제약 인지 동역학을 강조한다.

Result: 최신 모델들은 픽셀 예측에서 뛰어나지만, 불변 제약을 위반하고 안전-critical 결정-making에서 실패한다.

Conclusion: 세계 모델의 가치는 실제 롤아웃의 현실성보다 반사실적 추론, 개입 계획 및 강건한 장기 예측 능력에 의해 결정된다.

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [8] [Autonomous Business System via Neuro-symbolic AI](https://arxiv.org/abs/2601.15599)
*Cecil Pang,Hiroki Sayama*

Main category: cs.AI

TL;DR: AUTOBUS는 LLM 기반 AI 에이전트와 비즈니스 의미 중심 데이터를 통합한 자율 비즈니스 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 조직들이 지속적으로 교차 기능 프로세스를 재구성해야 하는 현재의 비즈니스 환경에서, 기존의 기업 시스템은 여전히 고립된 부서와 경직된 워크플로우로 구성되어 있다.

Method: AUTOBUS는 LLM 기반 AI 에이전트, 술어 논리 프로그래밍, 비즈니스 의미 중심의 기업 데이터를 통합해 비즈니스 이니셔티브를 수행하는 신경 기호 AI 아키텍처를 제공한다.

Result: AUTOBUS는 이니셔티브를 작업의 네트워크로 모델링하고, 관련 데이터를 조직하여 논리적 사실과 기초 규칙으로 변환하여 작업 추론의 의미적 기반을 제공한다.

Conclusion: 인간은 의미, 정책 및 작업 지침을 정의하고 유지하며, 도구를 큐레이션하고, 고충격 또는 모호한 결정을 감독하여 책임성과 적응성을 보장한다.

Abstract: Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.

</details>


### [9] [Agentic AI Governance and Lifecycle Management in Healthcare](https://arxiv.org/abs/2601.15630)
*Chandra Prakash,Mary Lind,Avneesh Sisodia*

Main category: cs.AI

TL;DR: 이 논문은 의료 조직에서 에이전트 AI의 통합과 이에 따른 문제를 해결하기 위한 통합 에이전트 생애 주기 관리(UALM) 청사진을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 의료 조직들이 에이전트 AI를 임상 문서 지원 및 조기 경고 모니터링 등 일상 업무에 통합하기 시작하면서, 에이전트 확산 문제로 인해 중복된 에이전트, 불명확한 책임, 일관되지 않은 통제 및 툴 권한 문제가 발생하고 있다.

Method: UALM은 거버넌스 기준, 에이전트 보안 문헌 및 의료 컴플라이언스 요구사항의 신속하고 실제 지향적인 종합에 기반한 청사진이다. 이 모델은 다섯 개의 제어Plane 레이어에 반복적으로 나타나는 격차를 매핑한다: (1) 아이덴티티 및 페르소나 레지스트리, (2) 오케스트레이션 및 도메인 간 중재, (3) PHI(개인 건강 정보) 기반 맥락 및 메모리, (4) 런타임 정책 시행과 킬 스위치 트리거, (5) 자격 취소 및 감사 로깅과 연결된 생애 주기 관리 및 폐기.

Result: UALM은 의료 CIO, CISO 및 임상 리더들에게 로컬 혁신을 유지하면서 임상 및 관리 도메인 전반에 걸쳐 안전한 확장을 가능하게 하는 감사 준비 감독을 위한 구현 가능한 패턴을 제공한다.

Conclusion: UALM은 의료 분야에서 에이전트 AI의 통합과 관리에 있어 보다 효과적인 접근 방식을 제시한다.

Abstract: Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.

</details>


### [10] [Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats](https://arxiv.org/abs/2601.15679)
*Ee Wei Seah,Yongsen Zheng,Naga Nikshith,Mahran Morsidi,Gabriel Waikin Loh Matienzo,Nigel Gay,Akriti Vij,Benjamin Chua,En Qi Ng,Sharmini Johnson,Vanessa Wilfred,Wan Sie Lee,Anna Davidson,Catherine Devine,Erin Zorer,Gareth Holvey,Harry Coppock,James Walpole,Jerome Wynee,Magda Dubois,Michael Schmatz,Patrick Keane,Sam Deverett,Bill Black,Bo Yan,Bushra Sabir,Frank Sun,Hao Zhang,Harriet Farlow,Helen Zhou,Lingming Dong,Qinghua Lu,Seung Jang,Sharif Abuadbba,Simon O'Callaghan,Suyu Ma,Tom Howroyd,Cyrus Fung,Fatemeh Azadi,Isar Nejadgholi,Krishnapriya Vishnubhotla,Pulei Xiong,Saeedeh Lohrasbi,Scott Buffett,Shahrear Iqbal,Sowmya Vajjala,Anna Safont-Andreu,Luca Massarelli,Oskar van der Wal,Simon Möller,Agnes Delaborde,Joris Duguépéroux,Nicolas Rolin,Romane Gallienne,Sarah Behanzin,Tom Seimandi,Akiko Murakami,Takayuki Semitsu,Teresa Tsukiji,Angela Kinuthia,Michael Michie,Stephanie Kasaon,Jean Wangari,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim*

Main category: cs.AI

TL;DR: 자율 AI 시스템의 급속한 상승과 에이전트 능력의 발전이 실세계 상호작용에 대한 감독 감소로 인한 새로운 리스크를 도입하고 있다. 에이전트 테스트는 초기 단계에 있으며, AI 에이전트가 다국적 배포될 때 다양한 언어와 문화를 정확하고 안전하게 처리하는 것이 중요하다.


<details>
  <summary>Details</summary>
Motivation: 자율 AI 시스템의 증가와 에이전트 테스트의 발전 필요성

Method: 세 가지 국제 협력 테스트를 기반으로 하여 일반적인 위험과 사이버 보안을 중심으로 한 두 가지 흐름으로 나뉘어 테스트가 진행되었다.

Result: 다양한 공개 에이전트 기준에 대한 테스트 결과 논의 및 방법론 이슈의 이해

Conclusion: 참가자들은 에이전트 평가 과학을 발전시키기 위해 협력하고 있다.

Abstract: The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.
  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.
  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.

</details>


### [11] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.15690)
*Jiaxin Zhang,Wendi Cui,Zhuohang Li,Lifu Huang,Bradley Malin,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델의 불확실성을 해결하기 위한 진화 과정을 조사하며, 이를 통해 AI의 신뢰성을 높이기 위한 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 불확실성이 고위험 도메인에서의 배포를 방해하는 주요 장벽이라는 문제를 해결하고자 한다.

Method: 불확실성을 수동 진단 지표에서 능동적 제어 신호로 전환하여 모델의 실시간 행동을 안내하는 방식을 제안한다.

Result: 세 가지 영역(고급 추론, 자율 에이전트, 강화 학습)에서 불확실성을 능동적 제어 신호로 활용하는 방법을 보여준다.

Conclusion: 불확실성의 새로운 추세를 마스터하는 것이 확장 가능하고 신뢰할 수 있는 AI를 구축하는 데 필수적이라고 주장한다.

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.

</details>


### [12] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: AI 에이전트는 장기적 추론에서 놀라운 성능을 보여주지만, 초기 인식 오류가 돌이킬 수 없이 전파되는 '착각의 나선'으로 인해 신뢰성이 저하된다. 본 논문에서는 이러한 문제를 해결하기 위한 통합된 이중 과정 에이전틱 불확실성 정량화(AUQ) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 신뢰성을 높이기 위해 '착각의 나선' 문제를 해결하는 것이 필요하다.

Method: 이중 과정 에이전틱 불확실성 정량화(AUQ) 프레임워크를 제안하여 언어화된 불확실성을 능동적인 양방향 제어 신호로 변환한다. 시스템 1(불확실성 인식 메모리, UAM)과 시스템 2(불확실성 인식 반성, UAR)의 두 가지 보완 메커니즘을 포함한다.

Result: 폐쇄 루프 벤치마크 및 개방형 심층 연구 작업에 대한 광범위한 실험에서 훈련이 필요 없는 접근 방식이 뛰어난 성능과 궤적 수준의 보정을 달성한다.

Conclusion: 이러한 이론적 프레임워크 AUQ는 신뢰할 수 있는 에이전트를 향한 중요한 진전을 나타낸다.

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [13] [AgentSM: Semantic Memory for Agentic Text-to-SQL](https://arxiv.org/abs/2601.15709)
*Asim Biswal,Chuan Lei,Xiao Qin,Aodong Li,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.AI

TL;DR: Agent Semantic Memory(AgentSM)는 Text-to-SQL의 새로운 접근법으로, 해석 가능한 의미 기억을 통해 데이터베이스 상호작용을 개선하고 더 높은 효율성과 정확도를 달성합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반의 Text-to-SQL 시스템은 복잡한 스키마와 다양한 SQL 방언을 처리하는 데에서 비효율성과 불안정성의 문제를 겪고 있습니다.

Method: AgentSM은 Prior 실행 흔적을 캡처하거나 선별된 것을 합성하여 구조화된 프로그램으로 변환하여 차후의 추론을 안내합니다.

Result: AgentSM은 Spider 2.0 벤치마크에서 평균 토큰 사용과 경로 길이를 각각 25% 및 35% 감소시켜 더 높은 효율성을 달성하고, 44.8%의 정확도로 Spider 2.0 Lite 벤치마크에서 가장 높은 정확도를 기록하였습니다.

Conclusion: 이런 체계적인 추론 경로 재사용 설계는 대규모 스키마와 복잡한 질문 처리에서의 확장성을 향상시킵니다.

Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.

</details>


### [14] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: 이 논문은 BIRD-Python이라는 새로운 벤치마크를 소개하며, Text-to-Python의 신뢰성을 향상시킬 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL이 데이터베이스 상호작용의 주된 접근 방식이지만, 현업 분석에서는 파일 기반 데이터와 복잡한 분석 작업을 관리하기 위한 일반 프로그래밍 언어의 유연성이 필요하다.

Method: BIRD-Python이라는 벤치마크를 만들고, 데이터 세트를 체계적으로 정제하여 주석 노이즈를 줄이며 실행 의미를 정렬하였다.

Result: 분석 결과, SQL이 선언적 구조를 통해 암묵적인 DBMS 동작을 활용하는 반면, Python은 명시적 절차 논리를 요구하여 사용자 의도에 민감하다는 것을 발견하였다.

Conclusion: Text-to-Python은 도메인 맥락의 결여로 성능 차이가 나며, 이러한 격차를 해결할 경우 Text-to-SQL과 동등한 성능을 달성할 수 있다.

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [15] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 물리학 분야의 정리 증명을 향상시키기 위한 최초의 접근법을 제시하며, PhysLeanData라는 전용 데이터셋을 구성하고 DeepSeek-Prover-V2-7B를 활용하여 PhysProver 모델을 훈련시킴으로써 여러 하위 분야에서 2.4%의 개선을 달성하였고, MiniF2F-Test 벤치마크에서 추가로 1.3%의 향상을 보임.


<details>
  <summary>Details</summary>
Motivation: 정형 물리학 추론이 정리 증명 및 문제 해결 프레임워크에 크게 의존함에도 불구하고 이 분야에 대한 관심이 적었다.

Method: PhysLeanData라는 전용 데이터셋을 구성하고 DeepSeek-Prover-V2-7B를 활용하여 Reinforcement Learning with Verifiable Rewards(RLVR)로 PhysProver 모델을 훈련시켰다.

Result: 약 5,000개의 훈련 샘플만으로 PhysProver는 여러 하위 분야에서 2.4%의 개선을 달성하고, MiniF2F-Test 벤치마크에서 1.3%의 향상을 보였다.

Conclusion: 이 접근법의 효과성과 효율성을 강조하며, 수학적 영역을 넘어 정형 증명기 확장의 패러다임을 제공한다.

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [16] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC는 단일 전문가 궤적을 이용하여 학습하는 오프 정책 액터-비평가 방법으로, 실제 RL 구현을 위한 낮은 비용의 효율적인 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현실에서 강화 학습을 배포하는 것은 샘플 비효율성, 희소 보상, 그리고 노이즈가 많은 시각적 관측으로 인해 여전히 도전적입니다.

Method: SigEnt-SAC는 단일 전문가 궤적에서 학습하는 오프 정책 액터-비평가 방법으로, 시그모이드 제한 엔트로피 항을 디자인하여 아웃 오브 분포 행동으로의 부정적 엔트로피 최적화를 방지합니다.

Result: SigEnt-SAC는 D4RL 작업에서 대표적인 기준과 비교하여 Q-함수의 진동을 크게 완화하고 기존 방법보다 더 빠르게 100% 성공률에 도달합니다.

Conclusion: SigEnt-SAC는 실제 세계의 이미지와 희소 보상으로부터 학습하여 성공적인 정책을 낮은 비용으로 배울 수 있음을 보여줍니다.

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [17] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: AI 에이전트는 복잡한 작업 수행 능력이 발전하고 있지만, 실패에 대한 과신이 여전히 배치의 장애물이다. 우리는 에이전트의 신뢰도 조정을 위한 새로운 진단 프레임워크인 Holistic Trajectory Calibration (HTC)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 실패에 대한 과신은 고위험 환경에서의 배치를 방해하는 주요 장벽이다.

Method: Holistic Trajectory Calibration (HTC)라는 새로운 진단 프레임워크를 도입하여 에이전트의 전체 궤적을 통해 과정 수준의 특징을 추출한다.

Result: HTC는 강력한 기준선들을 일관되게 초과하며, 해석 가능성과 도메인 간 적용 가능성과 일반화를 제공한다.

Conclusion: 이 기여들은 AI 에이전트의 신뢰성을 진단하고 향상시키기 위한 새로운 프로세스 중심 패러다임을 Establish 한다.

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [18] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: Chronic diseases necessitate a proactive approach to management, addressed by the VitalDiagnosis system that uses LLMs to enhance patient engagement and self-management.


<details>
  <summary>Details</summary>
Motivation: Chronic diseases lead to high mortality rates, compounded by inadequate medical resources and aging populations, highlighting the need for improved management strategies.

Method: VitalDiagnosis creates an LLM-driven ecosystem that combines wearable device data with LLM reasoning to shift chronic disease management from passive monitoring to proactive engagement.

Result: The system effectively analyzes health triggers and provides personalized insights within a collaborative care framework, fostering better adherence and self-management.

Conclusion: This proactive approach has the potential to reduce clinical workload while improving patient outcomes in chronic disease management.

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [19] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 이 논문은 딥 연구 에이전트(DRA)의 새로운 발전을 소개하며, 에이전트의 자가 발전 능력을 개선하는 새로운 패러다임을 제안합니다. 이를 통해 인증 기반의 깊은 검증기를 개발하여 검증의 비대칭성을 활용하고 메타 평가 F1 점수에서 기존 모델보다 12%-48% 향상된 성능을 보입니다.


<details>
  <summary>Details</summary>
Motivation: 딥 연구 에이전트(DRA) 분야에서의 자동화된 지식 발견과 문제 해결을 향상시키기 위해 에이전트의 자가 발전 능력을 제안합니다.

Method: 정교하게 설계된 루브릭을 바탕으로 정책 모델의 출력을 반복적으로 검증하여 에이전트가 자가 개선하도록 유도합니다.

Result: 새롭게 제안한 DeepVerifier는 검증의 비대칭성을 활용하여 기존 판단 모델보다 12%-48% 더 높은 메타 평가 F1 점수를 기록했습니다.

Conclusion: DeepVerifier는 테스트 시간 추론 동안 plug-and-play 모듈로 작동하며, 이를 통해 에이전트의 반응을 반복적으로 개선할 수 있습니다.

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [20] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: EvoCUA는 데이터 생성과 정책 최적화를 통합하여 자가 지속적인 진화 사이클을 형성하는 네이티브 컴퓨터 사용 에이전트 모델이다. 이 모델은 OSWorld 벤치마크에서 56.7%의 성공률을 기록하며 기존 모델을 초월했다.


<details>
  <summary>Details</summary>
Motivation: 기존의 정적 데이터에 의존하는 패러다임은 긴 시간 지향 컴퓨터 작업의 복잡한 인과 역학을 포착하는 데 어려움을 겪고 있다.

Method: EvoCUA는 데이터 생성과 정책 최적화를 통합하여 진화 주기를 형성하며, 가능한 다양한 작업을 자율적으로 생성하는 검증 가능한 합성 엔진을 개발한다. 대규모 경험 획득을 위해 수만 개의 비동기 샌드박스 롤아웃을 조정하는 확장 가능한 인프라를 설계한다.

Result: EvoCUA는 OSWorld 벤치마크에서 56.7%의 성공률을 달성하고, 기존의 OpenCUA-72B 모델의 45.0%보다 우수하며, UI-TARS-2(53.1%)와 같은 선도적인 닫힌 무게 모델도 초월한다.

Conclusion: 이 접근 방식은 다양한 규모의 기반 모델 간 일관된 성능 향상을 제공하며, 네이티브 에이전트 능력을 향상시키기 위한 강력하고 확장 가능한 경로를 확립한다.

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [21] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: CS-VAR는 라이브 스트리밍 리스크 평가를 위한 혁신적인 접근법으로, 세션 간 증거를 활용하여 빠른 리스크 추정을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 라이브 스트리밍의 부상은 온라인 상호작용을 변모시켰으며, 하지만 스캠 및 악의적인 행동과 같은 복잡한 위험에 플랫폼이 노출되었다.

Method: CS-VAR는 경량화된 도메인 특화 모델이 빠른 세션 수준의 리스크 추정을 수행하도록 하며, 교육 중에 대형 언어 모델(LLM)의 지도를 받는다.

Result: 대규모 산업 데이터셋에 대한 광범위한 오프라인 실험과 온라인 검증을 통해 CS-VAR의 최첨단 성능을 입증하였다.

Conclusion: CS-VAR는 라이브 스트리밍에 대한 실제 모더레이션을 효과적으로 지원하는 해석 가능한 지역 신호를 제공한다.

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [22] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM) 에이전트는 긴 상호작용 중 어조와 인격의 갑작스러운 변화를 보인다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 수준의 상태를 지배하는 명시적인 시간 구조의 부재를 반영한다.

Method: 외부 정서 상태에 동적 구조를 부여하여 다중 턴 대화에서 시간 일관성과 통제된 회복을 유도한다.

Result: 상태가 없는 에이전트는 일관된 궤적이나 회복을 보이지 않지만, 상태의 지속성은 지연 응답 및 신뢰할 수 있는 회복을 가능하게 한다.

Conclusion: 두 번째 차수의 동적 구조가 정서적 관성과 히스테리시스를 도입하며, 이는 모멘텀과 함께 증가하여 안정성과 반응성 간의 트레이드오프를 드러낸다.

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: Mixture-of-Experts 층은 가중치 희소성을 통해 계산 효율성을 달성하고, 데이터 희소성은 전문가들 각각이 처리하는 토큰의 일부만을 활용함으로써 보완적인 축을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: MoE(Mixture-of-Experts)는 계산 효율성을 높이기 위해 각 토큰이 전문가의 하위 집합만을 활성화합니다. 그러나 데이터 희소성 필요가 커지는 상황에서 이를 해결할 방법이 필요합니다.

Method: Expert-choice routing을 통해 데이터 희소성을 구현하지만, 이는 자동 회귀 모델에서 인과성을 위배하여 학습-추론 불일치를 초래합니다. 우리는 라우팅 풀에서 영(널) 전문가를 활용하여 인과적 토큰 선택 MoE 내에서 데이터 희소성을 회복합니다.

Result: 모델은 표준 로드 밸런싱 목표를 통해 모든 전문가(실제 및 널)를 고르게 사용하도록 학습하여 인과성 위반 없이 기대값 상에서 데이터 희소성을 만듭니다. 시각-언어 모델 학습 평가에서, 데이터 이질성이 뚜렷하게 나타납니다.

Conclusion: 가중치 희소성만으로는 부족한 계산 효율성을 조합한 데이터 희소성이 더 나은 교육 손실과 상위 성능 향상을 가져옵니다. 모델은 명시적 모달리티 라우팅 없이도 시각 토큰을 널 전문가들에게 더 공격적으로 라우팅하는 모달리티 인식 할당을 학습합니다.

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [24] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice는 이진 신뢰 게이팅을 사용하여 학습된 행동 구조를 조건부로 활성화하는 하이브리드 순차 예측 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 안전이 중요한 애플리케이션에서 인지 불확실성을 관리하는 아키텍처 원칙으로 신뢰 게이팅을 지지하기 위해.

Method: 행동 윈도우를 행동 전형으로 클러스터링하고, 신뢰가 임계값을 초과할 때만 점수를 활성화하는 이진 신뢰 게이팅을 사용한다.

Result: Lattice는 추천 시스템(MovieLens)에서 HR@10에서 LSTM 기준에 비해 +31.9% 개선을 달성하며, transformer 기준에 비해 SASRec에서 109.4%, BERT4Rec에서 218.6% 우수한 성능을 발휘했다.

Conclusion: Lattice는 패턴이 적용될 때 활성화하고, 그렇지 않을 때 거부하며, 중복될 때 연기하는 양방향 검증을 통해 신뢰 게이팅의 가능성을 보여준다.

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [25] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 이 연구는 비전형적 알츠하이머병(atAD) 환자 진단의 정확성을 향상시키기 위해 기계 학습 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 비전형적 알츠하이머병 환자는 임상적으로 종종 잘못 진단되므로, 이들의 진단 정확성을 향상시키는 것이 필요합니다.

Method: 임상 테스트 배터리와 표준 치료로 수집된 MRI 데이터를 활용하여 atAD와 비AD 인지 장애를 구분하는 기계 학습 접근 방식을 제안합니다.

Result: NACC의 경우 atAD 진단의 정확성을 52%에서 69%로, ADNI의 경우 34%에서 77%로 향상시켰습니다.

Conclusion: 이 접근 방식은 임상 환경에서 비기억형 atAD의 진단 정확성을 향상시키는 중요한 의미를 가집니다.

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [26] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 이 연구는 불확실한 수요와 리드 타임을 가진 부패성 재고 시스템에서 효율적인 재고 관리 정책을 개발하는 것을 목표로 한다. 기존의 역사적 데이터와 현재 시스템 상태를 바탕으로 주문 수량을 결정하는 딥러닝 기반 방법론을 제안하며, 다양한 시나리오에서의 성능을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: 부패성 제품은 짧은 유통 기한으로 인해 재고 관리에서 큰 도전 과제가 되며, 효과적인 주문 결정을 위한 방법이 필요하다.

Method: 제한된 역사적 데이터와 현재 상태를 이용하여 주문 수량을 직접적으로 결정하는 딥러닝 기반 접근법을 제안한다. 두 가지 변형 방법을 개발하였고, 하나는 블랙박스 접근방식, 다른 하나는 구조적 가이드를 포함하여 재고 영향을 명시적으로 계산하는 접근법이다.

Result: 실험 결과, E2E-PIL 접근법이 E2E-BB를 초월하며, E2E-BPIL은 E2E-PIL을 더욱 향상시킨다는 것을 확인했다.

Conclusion: 인간의 지식을 통합한 딥러닝 기반 의사결정 도구가 더 효과적이고 강력하다는 것을 보여주어, 고급 분석과 재고 이론의 통합의 가치를 강조한다.

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [27] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 1-identification 문제를 다루고, 자격이 있는 팔의 존재 여부를 판별하는 새로운 알고리즘과 하한을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 다수의 자격이 있는 팔이 존재하는 경우에 대한 기존 문헌의 해석과의 보완.

Method: 최적화 수식을 활용하여 적어도 하나의 자격이 있는 팔이 있을 때의 새로운 하한을 도출하고, 상한을 tight하게 설계한 새로운 알고리즘을 개발.

Result: 자격이 있는 팔이 여러 개일 때 $	extmath{E}τ$ 분석의 보완 결과를 도출하였습니다.

Conclusion: 제안된 알고리즘은 하한과의 차이가 로그 인자에 대해 다항식까지 좁혀지는 상한을 갖습니다.

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [28] [Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors](https://arxiv.org/abs/2601.15625)
*Zhiwei Zhang,Fei Zhao,Rui Wang,Zezhong Wang,Bin Liang,Jiakang Wang,Yao Hu,Shaosheng Cao,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 대형 언어 모델이 도구 호출에서 효과적이지만, 다중 턴 실행에서 여전히 취약성으로 인해 문제 발생시 잘못된 재호출로 이어진다. 본 논문에서는 Fission-GRPO라는 프레임워크를 제안하여 실행 오류를 RL 훈련 루프 내에서 교정 감독으로 변환한다.


<details>
  <summary>Details</summary>
Motivation: 현재 접근 방식의 주요 제한점은 일반 강화 학습(RL)이 오류를 드문 부정 보상으로 취급하며, 회복 방법에 대한 지침을 제공하지 않는다는 것이다. 또한, 기존 수집된 합성 오류 교정 데이터셋은 모델의 정책 오류 모드와 분포 불일치 문제를 가지고 있다.

Method: Fission-GRPO는 실행 오류를 RL 훈련 루프 내에서 교정 감독으로 변환하는 프레임워크이다. 이는 실패한 궤적을 진단 피드백으로 보강하여 새로운 훈련 사례로 나누고, 정책 상에서 회복 롤아웃을 재샘플링하는 방식이다.

Result: Fission-GRPO는 BFCL v4 Multi-Turn에서 Qwen3-8B의 오류 회복률을 5.7% 절대 증가시켰으며, GRPO 대비 4% 전반적인 정확도 향상(42.75%에서 46.75%)을 보였다.

Conclusion: Fission-GRPO는 전문 도구 사용 에이전트보다 뛰어난 성능을 발휘하였다.

Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

</details>


### [29] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: 현대 AI 시스템의 성능은 기본적으로 하드웨어 작업으로 고수준 알고리즘 의미론을 변환하는 커널의 품질에 의해 제약을 받는다. 최근 대형 언어 모델(LLM)과 LLM 기반 에이전트의 발전이 커널 생성 및 최적화를 자동화할 수 있는 새로운 가능성을 열어주고 있다. 그러나 이 분야는 분열되어 있으며 LLM 기반 커널 생성을 위한 체계적인 관점이 부족하다.


<details>
  <summary>Details</summary>
Motivation: 현대 AI 시스템의 성능을 향상시키기 위해 커널 엔지니어링을 자동화하고 최적화할 필요성이 있다.

Method: LLM 기반 접근 및 에이전틱 최적화 워크플로우에 대한 구조적 개요를 제공하고, 관련 데이터셋과 벤치마크를 체계적으로 수집한다.

Result: 이 분야에서의 빠른 발전과 함께 기존 접근 방식에 대한 체계적 개요를 제공한다.

Conclusion: 자동화 커널 최적화의 차세대 참조를 확보하기 위해 주요 오픈 챌린지와 미래 연구 방향이 개요된다.

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [30] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: 대형 언어 모델(LLM)의 안전 보호 장치가 취약하여 jailbreak 공격에 취약함을 보여줍니다. 이 연구는 글로벌 최적화를 통해 안전-critical attention head를 식별하는 GOSV 프레임워크를 제안합니다. 이를 통해 멀리 분리된 안전 벡터 세트를 확인하고, 전체 head의 약 30%가 다시 패치될 때 완전한 안전 실패가 발생함을 발견했습니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 안전 보호 장치가 jailbreak 공격에 취약하므로, 이들 구성 요소의 상호작용과 기여를 이해하기 위해 연구가 필요합니다.

Method: GOSV(글로벌 최적화를 이용한 안전 벡터 추출) 프레임워크를 제안하고, Harmful Patching 및 Zero Ablation이라는 두 가지 상호 보완적 활성화 리패칭 전략을 통해 안전에 중요한 attention head를 식별합니다.

Result: 안전 벡터의 두 가지 접근 방식을 확인하고, 모든 모델에서 전체 head의 약 30%가 다시 패치될 때 완전한 안전 실패가 발생함을 발견했습니다.

Conclusion: 제안된 GOSV 프레임워크가 LLM 안전 해석 가능성에 효과적임을 지원하는 강력한 증거를 제공합니다.

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [31] [Uncertainty-guided Generation of Dark-field Radiographs](https://arxiv.org/abs/2601.15859)
*Lina Felsner,Henriette Bast,Tina Dorosti,Florian Schaff,Franz Pfeiffer,Daniela Pfeiffer,Julia Schnabel*

Main category: cs.LG

TL;DR: 이 연구는 기존의 흡수 이미징에서 X-ray 다크 필드 이미지를 생성하기 위한 첫 번째 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: X-ray 다크 필드 방사선 촬영은 미세구조 조직 변화 시각화를 통해 기존의 흡수 이미징에 보완적인 진단 정보를 제공하지만, 데이터의 제한된 가용성이 견고한 딥러닝 모델 개발에 도전 과제를 제기합니다.

Method: 불확실성 기반 점진적 생성 적대신경망을 사용하여 표준 흡수 흉부 X-ray에서 다크 필드 이미지를 직접 생성하는 프레임워크를 제시합니다.

Result: 생성된 이미지의 고유 구조 충실도가 높고, 단계별로 정량적 메트릭이 일관되게 향상되는 것을 실험을 통해 입증했습니다.

Conclusion: 불확실성 기반 생성 모델링은 현실적인 다크 필드 이미지 합성을 가능하게 하며, 향후 임상 응용을 위한 신뢰할 수 있는 기초를 제공합니다.

Abstract: X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.

</details>


### [32] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: $k$-lazyGD는 greedy Online Gradient Descent와 lazy GD/dual-averaging 사이의 간극을 메우는 온라인 학습 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: Greedy한 업데이트와 안정적인 업데이트 사이의 균형을 유지하는 알고리즘을 찾기 위한 필요성.

Method: Smoothed Online Convex Optimization(SOCO) 알고리즘을 분석하며, $k$-lazyGD가 hitting 성능을 희생하지 않고 최적의 동적 후회를 달성함을 증명했다.

Result: $k$-lazyGD는 어떤 laziness slack $k$에 대해 동적 후회 $	extmath{O}(	extmath{	ext{sqrt}}{(P_T+1)T})$를 달성한다.

Conclusion: p 구성자 경로의 길이에 따라 허용 가능한 게으름을 정량적으로 연결하며, $k$-lazyGD가 추적 능력을 손상시키지 않고 lazy 방법의 본질적으로 작은 이동을 유지할 수 있음을 보여준다.

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [33] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: COCO 문제를 다루는 CLASP 알고리즘은 누적 손실을 최소화하고 제약 위반에 대한 제곱 패널티를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 제한된 온라인 볼록 최적화 문제의 효율적인 해결 방법을 찾아야 한다는 필요성.

Method: CLASP 알고리즘은 누적 손실과 제약 위반 제곱을 함께 최소화하는 방법으로, 기존의 비확장성 특성을 활용하여 분석을 진행합니다.

Result: CLASP는 강한 볼록 문제에 대해 로그 기반의 손실 및 제곱 패널티에 대한 보장을 제공합니다.

Conclusion: CLASP는 강한 볼록 문제에서 손실과 제곱 패널티의 로그 보장을 제공하며, 이는 해당 분야에서의 중요한 기여입니다.

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [34] [Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals](https://arxiv.org/abs/2601.16091)
*Saar Cohen*

Main category: cs.MA

TL;DR: 본 논문은 지연이 있는 온라인 비중심 클러스터링을 다루며, 지연 비용을 허용하는 새로운 알고리즘을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 온라인 비중심 클러스터링 문제에서 지연을 고려하는 접근 방식의 필요성

Method: 점이 도착할 때마다 각 점의 위치를 공개하고, 기존 클러스터에 할당하거나 새로운 클러스터를 만드는 알고리즘을 사용한다. 결정은 지연 비용을 고려해 연기할 수 있다.

Result: 우리는 점의 수가 증가함에 따라 출력 클러스터링의 예상 총 비용과 최적의 오프라인 클러스터링 간의 비율이 상수로 제한되는 알고리즘을 제안한다.

Conclusion: 확률적 도착 모델을 통한 온라인 비중심 클러스터링의 성능 개선 가능성을 제시한다.

Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.

</details>


### [35] [Average Unfairness in Routing Games](https://arxiv.org/abs/2601.16187)
*Pan-Yang Su,Arwa Alanqary,Bryce L. Ferguson,Manxi Wu,Alexandre M. Bayen,Shankar Sastry*

Main category: cs.MA

TL;DR: 이 논문에서는 라우팅 게임에서의 공정성을 측정하기 위한 새로운 척도로서 평균 불공정성을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 라우팅 게임에서 공정성을 정의하고 측정하는 새로운 방법이 필요하다.

Method: 우리는 평균 대기시간과 최소 대기시간 간의 비율로 평균 불공정성을 정의하고 세 가지 불공정 척도를 비교합니다.

Result: 평균 불공정성이 항상 로드 불공정성보다 크지 않으며, 평균 불공정성 제약 하에 최적의 흐름이 로드 불공정성 제약을 만족하는 흐름보다 낮은 총 대기시간을 달성함을 증명합니다.

Conclusion: 우리의 연구 결과는 네트워크 라우팅에서 공정성과 효율성의 균형을 평가하기 위한 이론적 보장과 귀중한 통찰을 제공합니다.

Abstract: We propose average unfairness as a new measure of fairness in routing games, defined as the ratio between the average latency and the minimum latency experienced by users. This measure is a natural complement to two existing unfairness notions: loaded unfairness, which compares maximum and minimum latencies of routes with positive flow, and user equilibrium (UE) unfairness, which compares maximum latency with the latency of a Nash equilibrium. We show that the worst-case values of all three unfairness measures coincide and are characterized by a steepness parameter intrinsic to the latency function class. We show that average unfairness is always no greater than loaded unfairness, and the two measures are equal only when the flow is fully fair. Besides that, we offer a complete comparison of the three unfairness measures, which, to the best of our knowledge, is the first theoretical analysis in this direction. Finally, we study the constrained system optimum (CSO) problem, where one seeks to minimize total latency subject to an upper bound on unfairness. We prove that, for the same tolerance level, the optimal flow under an average unfairness constraint achieves lower total latency than any flow satisfying a loaded unfairness constraint. We show that such improvement is always strict in parallel-link networks and establish sufficient conditions for general networks. We further illustrate the latter with numerical examples. Our results provide theoretical guarantees and valuable insights for evaluating fairness-efficiency tradeoffs in network routing.

</details>
