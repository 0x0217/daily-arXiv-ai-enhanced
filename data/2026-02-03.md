<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 5]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Learning to Recommend Multi-Agent Subgraphs from Calling Trees](https://arxiv.org/abs/2601.22209)
*Xinyuan Song,Liang Zhao*

Main category: cs.MA

TL;DR: 이 논문에서는 복잡한 작업을 수행하는 다중 에이전트 시스템(MAS)을 위한 추천 시스템의 공백을 메우기 위해 제약된 의사결정 문제로 에이전트 추천을 공식화하고, 효율적인 헌신 세트를 구축 및 유틸리티 최적화를 수행하는 제약된 추천 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템이 점점 더 복잡한 작업을 해결하는데 필요한 시스템의 필요성.

Method: 에이전트 추천을 제약된 결정 문제로 공식화하고, 현재 하위 작업 및 컨텍스트에 조건화된 후보 세트를 구축한 후 유틸리티 최적화를 수행하는 제약된 추천 프레임워크를 도입함.

Result: 제안된 프레임워크는 두 가지 보완적인 설정을 지원하며, 통합된 호출 트리 벤치마크를 구축하여 평가를 가능하게 함.

Conclusion: 이 프레임워크는 에이전트의 신뢰성, 호환성 및 상호작용 효과를 고려하여 효과적인 에이전트 추천을 가능하게 한다.

Abstract: Multi-agent systems (MAS) increasingly solve complex tasks by orchestrating agents and tools selected from rapidly growing marketplaces. As these marketplaces expand, many candidates become functionally overlapping, making selection not just a retrieval problem: beyond filtering relevant agents, an orchestrator must choose options that are reliable, compatible with the current execution context, and able to cooperate with other selected agents. Existing recommender systems -- largely built for item-level ranking from flat user-item logs -- do not directly address the structured, sequential, and interaction-dependent nature of agent orchestration. We address this gap by \textbf{formulating agent recommendation in MAS as a constrained decision problem} and introducing a generic \textbf{constrained recommendation framework} that first uses retrieval to build a compact candidate set conditioned on the current subtask and context, and then performs \textbf{utility optimization} within this feasible set using a learned scorer that accounts for relevance, reliability, and interaction effects. We ground both the formulation and learning signals in \textbf{historical calling trees}, which capture the execution structure of MAS (parent-child calls, branching dependencies, and local cooperation patterns) beyond what flat logs provide. The framework supports two complementary settings: \textbf{agent-level recommendation} (select the next agent/tool) and \textbf{system-level recommendation} (select a small, connected agent team/subgraph for coordinated execution). To enable systematic evaluation, we construct a unified calling-tree benchmark by normalizing invocation logs from eight heterogeneous multi-agent corpora into a shared structured representation.

</details>


### [2] [Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems](https://arxiv.org/abs/2601.22292)
*Manuela Chacon-Chamorro,Luis Felipe Giraldo,Nicanor Quijano*

Main category: cs.MA

TL;DR: 이 연구는 다중 에이전트 시스템에서 협력적 회복력을 향상시키기 위한 보상 함수 설계의 중요성을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 많은 에이전트가 개인 목표를 추구하는 동시에 집단 기능을 보장해야 하는 동적이고 불확실한 환경에서 작동하는 다중 에이전트 시스템의 문제를 해결하고자 합니다.

Method: 상호 협력적 회복성을 기준으로 보상 함수를 순위 트랙에서 학습하는 새로운 프레임워크를 도입하고, 세 가지 보상 전략과 여러 보상 매개변수를 탐색합니다.

Result: 하이브리드 전략이 중단 상황에서 강건성을 크게 향상시키며 작업 성능 저하 없이 자원 남용과 같은 재앙적 결과를 줄이는 데 성공하였습니다.

Conclusion: 보상 설계가 회복력 있는 협력을 강화하는 데 중요하며, 불확실한 환경에서 협력을 유지하는 강건한 다중 에이전트 시스템 개발로 나아가는 중요한 단계입니다.

Abstract: Multi-agent systems often operate in dynamic and uncertain environments, where agents must not only pursue individual goals but also safeguard collective functionality. This challenge is especially acute in mixed-motive multi-agent systems. This work focuses on cooperative resilience, the ability of agents to anticipate, resist, recover, and transform in the face of disruptions, a critical yet underexplored property in Multi-Agent Reinforcement Learning. We study how reward function design influences resilience in mixed-motive settings and introduce a novel framework that learns reward functions from ranked trajectories, guided by a cooperative resilience metric. Agents are trained in a suite of social dilemma environments using three reward strategies: i) traditional individual reward; ii) resilience-inferred reward; and iii) hybrid that balance both. We explore three reward parameterizations-linear models, hand-crafted features, and neural networks, and employ two preference-based learning algorithms to infer rewards from behavioral rankings. Our results demonstrate that hybrid strategy significantly improve robustness under disruptions without degrading task performance and reduce catastrophic outcomes like resource overuse. These findings underscore the importance of reward design in fostering resilient cooperation, and represent a step toward developing robust multi-agent systems capable of sustaining cooperation in uncertain environments.

</details>


### [3] [ScholarPeer: A Context-Aware Multi-Agent Framework for Automated Peer Review](https://arxiv.org/abs/2601.22638)
*Palash Goyal,Mihir Parmar,Yiwen Song,Hamid Palangi,Tomas Pfister,Jinsung Yoon*

Main category: cs.MA

TL;DR: ScholarPeer는 심층 연구자의 인지 과정을 모방한 다중 에이전트 프레임워크로, 문맥 수집과 능동 검증의 이중 스트림 프로세스를 활용하여 평가를 수행합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 자동 동료 리뷰 시스템이 표면적인 비판에 어려움을 겪고 있어, 논문의 참신성과 중요성을 정확히 평가하지 못하는 문제를 해결하고자 합니다.

Method: ScholarPeer는 역사 에이전트를 통해 도메인 내러티브를 동적으로 구축하고, 기준 스카우트를 통해 누락된 비교를 식별하며, 다각적 Q&A 엔진을 통해 주장을 검증합니다.

Result: ScholarPeer는 DeepReview-13K에서 평가되었으며, 기존 최첨단 접근 방식에 비해 상당한 승률을 달성하고 인간 수준의 다양성 간 간극을 줄였습니다.

Conclusion: ScholarPeer는 자동 동료 리뷰 시스템의 성능을 향상시키고, 전문가의 인지적 프로세스를 모방하여 보다 깊이 있는 비판을 제공합니다.

Abstract: Automated peer review has evolved from simple text classification to structured feedback generation. However, current state-of-the-art systems still struggle with "surface-level" critiques: they excel at summarizing content but often fail to accurately assess novelty and significance or identify deep methodological flaws because they evaluate papers in a vacuum, lacking the external context a human expert possesses. In this paper, we introduce ScholarPeer, a search-enabled multi-agent framework designed to emulate the cognitive processes of a senior researcher. ScholarPeer employs a dual-stream process of context acquisition and active verification. It dynamically constructs a domain narrative using a historian agent, identifies missing comparisons via a baseline scout, and verifies claims through a multi-aspect Q&A engine, grounding the critique in live web-scale literature. We evaluate ScholarPeer on DeepReview-13K and the results demonstrate that ScholarPeer achieves significant win-rates against state-of-the-art approaches in side-by-side evaluations and reduces the gap to human-level diversity.

</details>


### [4] [Multi-Agent Systems Should be Treated as Principal-Agent Problems](https://arxiv.org/abs/2601.23211)
*Paulius Rauba,Simonas Cepenas,Mihaela van der Schaar*

Main category: cs.MA

TL;DR: 다중 에이전트 시스템에서 정보 비대칭과 목표 불일치를 다루며, LLM 기반 에이전트의 조작 현상을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 주요 에이전트가 전문화된 에이전트에게 하위 작업을 할당하고 그 응답을 집계하는 다중 에이전트 시스템에서 발생하는 정보 비대칭 및 목표 불일치가 문제를 일으킬 수 있다.

Method: 주요-대리인 문제의 관점에서 정보 비대칭과 목표 불일치를 연구하며, LLM 에이전트의 조작적 행동을 구체적인 사례로 설명한다.

Result: 정보 비대칭이 다중 에이전트 시스템에서 자연스럽게 발생하며, 조작을 묘사하는 새로운 용어가 기존 메커니즘 설계 문헌에서 잘 연구된 개념과 일치함을 보여준다.

Conclusion: 비인간 에이전트의 분석에도 인간 에이전트 행동 연구에 사용된 도구를 적용할 것을 주장한다.

Abstract: Consider a multi-agent systems setup in which a principal (a supervisor agent) assigns subtasks to specialized agents and aggregates their responses into a single system-level output. A core property of such systems is information asymmetry: agents observe task-specific information, produce intermediate reasoning traces, and operate with different context windows. In isolation, such asymmetry is not problematic, since agents report truthfully to the principal when incentives are fully aligned. However, this assumption breaks down when incentives diverge. Recent evidence suggests that LLM-based agents can acquire their own goals, such as survival or self-preservation, a phenomenon known as scheming, and may deceive humans or other agents. This leads to agency loss: a gap between the principal's intended outcome and the realized system behavior. Drawing on core ideas from microeconomic theory, we argue that these characteristics, information asymmetry and misaligned goals, are best studied through the lens of principal-agent problems. We explain why multi-agent systems, both human-to-LLM and LLM-to-LLM, naturally induce information asymmetry under this formulation, and we use scheming, where LLM agents pursue covert goals, as a concrete case study. We show that recently introduced terminology used to describe scheming, such as covert subversion or deferred subversion, corresponds to well-studied concepts in the mechanism design literature, which not only characterizes the problem but also prescribes concrete mitigation strategies. More broadly, we argue for applying tools developed to study human agent behavior to the analysis of non-human agents.

</details>


### [5] [MonoScale: Scaling Multi-Agent System with Monotonic Improvement](https://arxiv.org/abs/2601.23219)
*Shuai Shao,Yixiang Liu,Bingwei Lu,Weinan Zhang*

Main category: cs.MA

TL;DR: MonoScale은 새로운 기능 에이전트 통합 시 성능 저하를 방지하기 위한 업데이트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 최근 LLM 기반 다중 에이전트 시스템이 빠르게 발전하고 있으며, 에이전트 풀을 확대하는 자연스러운 방법이 필요하다.

Method: MonoScale은 에이전트 조건화된 익숙해지기 과제를 생성하고 성공적 및 실패한 상호작용의 증거를 수집하여, 이를 감사 가능한 자연어 메모리로 증류하여 미래의 라우팅을 안내하는 방법론이다.

Result: GAIA 및 Humanity's Last Exam에서의 실험 결과, 에이전트 풀이 증가함에 따라 안정적인 성능 향상이 나타났다.

Conclusion: MonoScale은 단순한 확장보다 더 나은 성능을 보이며, 높은 신뢰성을 바탕으로 에이전트 풀의 성장을 지원한다.

Abstract: In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: CP4Gen은 조건부 생성 모델을 위한 새로운 불확실성 추정 방법으로, 예측 집합의 부피와 구조적 단순성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 조건부 생성 모델의 개별 출력에 대한 신뢰를 높이고자 한다.

Method: 모델이 생성한 샘플에 대한 밀도 추정을 활용하는 체계적인 준거 예측 접근 방식을 제안한다.

Result: CP4Gen은 예측 집합의 부피와 구조적 단순성에서 우수한 성능을 지속적으로 달성한다.

Conclusion: CP4Gen은 조건부 생성 모델과 관련된 불확실성 추정에 유용한 도구가 된다.

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [7] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: Bayesian Workflow Generation (BWG)은 자동 작업 흐름 생성을 위한 샘플링 프레임워크로, SOTA 대비 최대 9% 향상과 제로샷 프롬프팅 대비 최대 65% 향상을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 자동 작업 흐름 생성을 위한 기존 방법들은 이론적 기반이 제한된 최적화 문제로 간주하고 있다.

Method: 우리는 작업 흐름 생성을 작업 흐름에 대한 사후 분포에 대한 베이esian 추론으로 정의하고, 중요도 가중치를 위한 병렬 탐색 롤아웃 및 풀-wide 개선을 위한 순차적 루프 리파이너를 사용하는 BWG를 도입한다.

Result: BWG인 BayesFlow는 6개의 벤치마크 데이터셋에서 SOTA 작업 흐름 생성 기준보다 정확성을 최대 9% 향상시키고 제로샷 프롬 프팅보다 최대 65% 향상시킨다.

Conclusion: BWG는 검색 기반 작업 흐름 설계에 대한 원칙적인 업그레이드로 자리 잡았다.

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [8] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore는 해석 가능한 모델과 유사한 성능을 발휘하면서도 임상 가이드라인에 적합한 점수 생성을 위한 새로운 최적화 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존 머신러닝 모델이 임상 루틴에 잘 적용되지 않는 이유를 분석하고, 임상 가이드라인에 적합한 모델 최적화의 필요성을 강조한다.

Method: AgentScore는 LLM을 활용하여 후보 규칙을 제안하고, 검증 및 선택 루프를 통해 통계적 유효성과 배포 가능성 제약을 강제하는 방법으로 최적화를 수행한다.

Result: AgentScore는 8개의 임상 예측 과제에서 기존 점수 생성 방법보다 우수한 성능을 보이며, 더 유연한 해석 가능한 모델에 비해 동등한 AUC를 달성했다.

Conclusion: AgentScore는 두 가지 외부 검증된 과제에서 기존 가이드라인 기반 점수보다 높은 변별력을 달성함으로써, 임상 가이드라인에 배포 가능한 점수 생성의 가능성을 제시한다.

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [9] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS는 Cell Painting 데이터의 배치 효과를 보정하는 스케일 가능한 방법으로, 샘플 간의 정렬을 통해 생물학적 신호를 명확하게 한다.


<details>
  <summary>Details</summary>
Motivation: Cell Painting에서 배치 효과는 실험실, 기기 및 프로토콜의 차이에서 발생하며 생물학적 신호를 혼란스럽게 한다.

Method: BALANS는 밀집된 유사도 행렬을 구성하여 배치 간 샘플을 정렬하는 방법으로, 국소 이웃의 거리로부터 지역 스케일을 설정하고 가우시안 커널을 사용하여 희소 유사도 행렬을 형성한다.

Result: BALANS는 샘플 복잡도에서 최적의 순서 전략을 제공하며, 실행 시간이 거의 선형적이고 다양한 실제 Cell Painting 데이터세트에서 효율성을 입증하였다.

Conclusion: BALANS는 많이 사용되는 배치 수정 방법보다 우수한 성능을 제공하며, 품질을 저하시키지 않으면서 대규모 데이터셋에 확장할 수 있다.

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [10] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 이 논문은 마르코프 의사결정 프로세스(MDP)에서 정책 표현을 학습하여 테스트 시 행동 지향을 용이하게 하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 정책이 MDP의 점유 측정에 의해 고유하게 결정되므로, 이를 기반으로 하는 정책 표현 학습의 필요성이 있다.

Method: 정책 표현을 점유 측정에 대한 상태-행동 특성 맵의 기대값으로 모델링하고, 세트 기반 아키텍처를 사용하여 여러 정책에 대해 이를 균일하게 근사화한다.

Result: 모델은 상태-행동 샘플 세트를 잠재 임베딩으로 인코딩하여, 다수의 보상에 해당하는 정책과 가치 함수를 디코드한다.

Conclusion: 이 메커니즘을 통해 추가 훈련 없이 이전에 보지 못한 가치 함수 제약을 만족시키기 위해 정책을 조정하는 새로운 행동 합성 작업을 해결한다.

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [11] [Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents](https://arxiv.org/abs/2601.22352)
*Sri Vatsa Vuddanti,Satwik Kumar Chittiprolu*

Main category: cs.LG

TL;DR: 이 논문은 언어 모델 에이전트의 자가 복구 능력을 설명하는 예측 이론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 도구 호출 실행 실패 후 자가 복구 능력에 대한 공식적인 설명이 부족하다.

Method: 우리는 Expected Recovery Regret (ERR)라는 개념을 도입하여 복구 가능성을 정식화하고, 복구 정책의 최적 정책으로부터의 편차를 정량화하며, ERR과 Efficiency Score (ES) 사이의 1차 관계를 유도한다.

Result: 우리의 이론은 도구 사용 에이전트의 복구 역학에 대한 반증 가능한 1차 정량적 법칙을 제공하며, 5개의 도구 사용 벤치마크를 통해 이를 경험적으로 검증했다.

Conclusion: 복구 가능성은 모델의 규모나 아키텍처의 산물이지 않으며, 상호작용 역학의 통제된 속성임을 보여준다.

Abstract: Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability through Expected Recovery Regret (ERR), which quantifies the deviation of a recovery policy from the optimal one under stochastic execution noise, and derive a first-order relationship between ERR and an empirical observable quantity, the Efficiency Score (ES). This yields a falsifiable first-order quantitative law of recovery dynamics in tool-using agents. We empirically validate the law across five tool-use benchmarks spanning controlled perturbations, diagnostic reasoning, and real-world APIs. Across model scales, perturbation regimes, and recovery horizons, predicted regret under the ERR-ES law closely matched observed post-failure regret measured from Monte Carlo rollouts, within delta less than or equal to 0.05. Our results reveal that recoverability is not an artifact of model scale or architecture, but a governed property of interaction dynamics, providing a theoretical foundation for execution-level robustness in language agents.

</details>


### [12] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: PABLO는 생물학적 블랙박스 최적화를 언어 기반의 추론 프로세스로 전환하는 계층적 시스템으로, 화학 및 생물학 문헌에서 사전 훈련된 과학적 LLM을 사용하여 생물학적 후보를 생성하고 반복적으로 정제한다.


<details>
  <summary>Details</summary>
Motivation: 생물학적 설계의 주요 도전 과제들은 복잡한 구조적 공간에서의 블랙박스 최적화로 구성될 수 있으며, 기존 방법들은 주로 원시 구조 데이터에 의존하고 풍부한 과학적 문헌을 활용하는 데 어려움을 겪는다.

Method: PABLO는 생화학 및 생물학 문헌에서 사전 훈련된 과학적 LLM을 사용하여 생물학적 후보를 생성하고 반복적으로 정제하는 계층적 에이전트 시스템이다.

Result: PABLO는 GuacaMol 분자 설계와 항균 펩타이드 최적화 작업에서 최첨단 성능을 달성하며, 샘플 효율성과 최종 목표 값에서 기존 기준을 크게 개선했다.

Conclusion: PABLO는 언어 기반의 에이전트적 공식화를 제공하여 현실적인 설계를 위한 중요한 이점을 제공하며, 실제 검증에서도 약물 내성 병원균에 대한 강한 활성을 보여주는 등 치료 발견을 위한 실질적인 잠재력을 강조한다.

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [13] [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://arxiv.org/abs/2601.22442)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Gil Avraham,Hadi Mohaghegh Dolatabadi,Chamin P Hewa Koneputugodage,Violetta Shevchenko,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 이 논문에서는 비동기 업데이트를 통해 데이터 및 파이프라인 병렬 처리의 통신 병목 현상을 해결하고, 비동기 스파스 평균화 방법을 도입하여 성능을 향상시켰다는 내용을 담고 있다.


<details>
  <summary>Details</summary>
Motivation: 데이터 및 파이프라인 병렬 처리의 높은 통신 비용은 빠른 상호 연결을 갖춘 공동 위치 컴퓨팅 클러스터를 필요로 하며, 이는 확장성을 제한한다.

Method: 비동기 업데이트를 도입하고, 파이프라인 병렬 처리에는 가중치 미리 보기 접근 방식을, 데이터 병렬 처리에는 지수 이동 평균 기반 수정 메커니즘이 있는 비동기 스파스 평균화 방법을 적용하였다.

Result: 대규모 언어 모델에 대한 실험 결과, 우리의 접근 방식이 완전 동기 기준선의 성능과 일치하면서도 통신 오버헤드를 상당히 줄였다.

Conclusion: 스파스 평균화와 비동기 업데이트의 수렴 보장을 제공하였다.

Abstract: Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing asynchronous updates across both parallelism axes, relaxing the co-location requirement at the expense of introducing staleness between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an asynchronous sparse averaging method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to \em 1B parameters) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.

</details>


### [14] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: 이 연구에서는 AI 예측자를 개발하고 평가하기 위한 고품질 예측 질문 생성 및 해결 시스템을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 미래 사건을 예측하는 것은 의사결정에 매우 중요하며 일반 지능의 강력한 척도입니다. 그러나 기존의 자동화 노력은 제한된 데이터 소스를 사용하여 다양성과 유용성을 제한했습니다.

Method: LLM 기반 웹 연구 에이전트를 사용하여 고품질 예측 질문을 자동으로 대량 생성하고 해결하는 시스템을 개발하였습니다.

Result: 1499개의 다양한 실제 예측 질문을 생성하고 몇 달 후에 해결했습니다. 시스템이 약 96%의 확률로 검증 가능하고 명확한 질문을 생성하며, 질문 해결 정확도는 약 95%에 달했습니다.

Conclusion: 더 지능적인 LLM로 구동되는 예측 에이전트는 질문 해결에서 더 나은 성능을 보여줍니다. 또한, 질문 분해 전략을 평가하여 예측을 개선할 수 있음을 증명했습니다.

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [15] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: 이 연구는 대형 언어 모델(LLM)이 보상 없이 환경에 대한 내부 표상을 습득하는 잠재 학습 동역학을 보여준다는 새로운 발견을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 잠재 학습이 대형 언어 모델의 훈련에 어떻게 적용될 수 있는지를 탐구하려는 목적이 있다.

Method: 보상 없는 탐색의 초기 단계에서 LLM의 성능을 개선하는 실험을 수행했다.

Result: LLM은 보상 없는 탐색 단계에서 성능 개선을 보이며, 보상이 도입된 후에는 더욱 향상된다.

Conclusion: LLM의 훈련에서 잠재 학습 동역학이 존재함을 입증하며, 이러한 동역학이 성능 향상을 어떻게 가져오는지를 이론적으로 분석하였다.

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [16] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 본 논문은 지속적 강화 학습(CRL)을 단일 작업 교사 모델과 중앙 일반 모델로 분리하는 새로운 교사-학생 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: CRL은 다양한 작업에 걸쳐 지속적으로 지식을 습득하면서 재앙적인 망각을 줄이기 위한 평생학습 에이전트를 개발하는 것을 목표로 합니다.

Method: 모델 간의 안정성과 유연성의 균형 잡기를 위해 분산 RL을 통해 단일 작업 교사 모델을 훈련하고 이를 중앙 일반 모델로 지속적으로 증류하는 과정을 사용합니다.

Result: 메타-월드 벤치마크에 대한 광범위한 실험에서 제안한 프레임워크가 지속적 RL을 가능하게 하여 교사 성능의 85% 이상을 회복하였으며 작업별 망각을 10% 이내로 제한했습니다.

Conclusion: 제안한 프레임워크는 지속적 정책 증류 과정의 유연성과 안정성을 높이기 위해 혼합 전문가(MoE) 아키텍처와 리플레이 기반 접근 방식을 사용합니다.

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [17] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 본 연구는 인간의 환경 학습에 있는 여러 신경 시스템의 상호 작용 원리를 설명하고 이를 기반으로 한 샘플링 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 인간의 효율적인 학습 메커니즘을 이해하고 이를 바탕으로 머신 러닝에 적용하기 위한 방법을 제시하고자 한다.

Method: 모델 기반, 모델 프리, 그리고 에피소드 기반 제어의 세 가지 구성 요소로 이루어진 샘플링 알고리즘을 개발했다.

Result: 제안한 알고리즘이 베이지안 방법을 발전시키고 대규모 통계적 머신 러닝 문제에 효과적으로 적용될 수 있음을 보인다.

Conclusion: 특히, 제안한 프레임워크는 베이지안 심층 학습에 적용되어 정확한 불확실성 정량화에 중점을 둔다.

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [18] [Float8@2bits: Entropy Coding Enables Data-Free Model Compression](https://arxiv.org/abs/2601.22787)
*Patrick Putzky,Martin Genzel,Mattes Mollenhauer,Sebastian Schulze,Thomas Wollmann,Stefan Dietzel*

Main category: cs.LG

TL;DR: EntQuant는 데이터 의존 방식을 데이터 없는 방식의 속도와 범용성과 결합하여 극한 압축에서 실용성을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 압축 기법들이 서로 상반된 장단점을 가지고 있어 두 가지 패러다임을 통합할 필요가 있었습니다.

Method: EntQuant는 엔트로피 코딩을 통해 수치적 정밀성과 저장 비용을 분리하여 70B 파라미터 모델을 30분 이내에 압축합니다.

Result: EntQuant는 표준 평가 세트와 모델에서 최첨단 결과를 달성하며, 더 복잡한 기준에서도 기능 성능을 유지합니다.

Conclusion: EntQuant는 고급 압축 기법으로서 다양한 모델과 벤치마크에서 효율적으로 작동합니다.

Abstract: Post-training compression is currently divided into two contrasting regimes. On the one hand, fast, data-free, and model-agnostic methods (e.g., NF4 or HQQ) offer maximum accessibility but suffer from functional collapse at extreme bit-rates below 4 bits. On the other hand, techniques leveraging calibration data or extensive recovery training achieve superior fidelity but impose high computational constraints and face uncertain robustness under data distribution shifts. We introduce EntQuant, the first framework to unite the advantages of these distinct paradigms. By matching the performance of data-dependent methods with the speed and universality of data-free techniques, EntQuant enables practical utility in the extreme compression regime. Our method decouples numerical precision from storage cost via entropy coding, compressing a 70B parameter model in less than 30 minutes. We demonstrate that EntQuant does not only achieve state-of-the-art results on standard evaluation sets and models, but also retains functional performance on more complex benchmarks with instruction-tuned models, all at modest inference overhead.

</details>


### [19] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: 이 연구는 복잡한 네트워크 매핑을 활용하여 합성 시계열 데이터를 생성하는 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 시계열 데이터는 머신러닝 모델 개발에 필수적이나, 개인 정보 보호, 획득 비용, 라벨링 문제로 인해 고품질 데이터셋 접근이 제한적이다.

Method: 양적 그래프(QG)로 변환된 시계열을 역 매핑을 통해 재구성하여 원본의 통계적 및 구조적 속성을 유지하면서 합성 데이터를 생성하는 방법을 조사한다.

Result: 생성된 데이터의 충실도와 유용성을 시뮬레이션 및 실제 데이터셋을 사용하여 평가하고, 최신 생성적 적대 신경망(GAN) 방법과 비교한다.

Conclusion: 결과는 양적 그래프 기반 방법론이 합성 시계열 데이터 생성에 있어 경쟁력 있고 해석 가능한 대안을 제공함을 나타낸다.

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [20] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: 본 연구에서는 다목적 강화 학습에서 일반화 능력을 향상시키기 위한 새로운 접근법인 PlatoLTL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다목적 강화 학습의 핵심 과제는 훈련 중 보지 못한 작업을 수행할 수 있는 일반화 정책을 훈련하는 것입니다.

Method: 우리의 방법은 제안된 문장들을 매개변수화된 술어의 인스턴스로 취급하고, 이를 통해 정책이 관련된 제안들 간에 공유된 구조를 학습할 수 있도록 합니다.

Result: 우리는 새로운 아키텍처를 제안하여 LTL 사양을 표현하기 위해 술어를 임베드하고 조합하며, 도전적인 환경에서 새로운 제안과 작업에 대한 성공적인 제로샷 일반화를 입증합니다.

Conclusion: 이 연구는 다목적 강화 학습에서 LTL 사양을 통한 제로샷 일반화의 가능성을 보였으며, 향후 연구의 기반을 제공합니다.

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [21] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 이 연구는 Lipschitz 매끄러운 데이터 충실도 항과 비매끄러운 오목-볼록 함수의 정규화 항을 가진 표본 추출 문제를 다룬다. 정규화 항의 DC 구조를 이용해 정규화의 오목 부분을 데이터 충실도에 재분배하고, 이를 기반으로 한 DC-LA 알고리즘의 수렴성을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 비매끄러운 정규화 문제에 대한 더 일반적인 프레임워크와 가정을 제시함으로써 비로그-오목 샘플링에 대한 기존 연구를 개선하고자 함.

Method: 정규화 항의 DC 구조를 활용하여 각각의 Moreau envelope을 적용하고, 오목 부분을 데이터 충실도에 재분배한 후 이에 대한 근사 Langevin 알고리즘(DC-LA)을 연구.

Result: DC-LA가 타겟 분포 $π$로 수렴함을 보였으며, 모든 유한한 $q$에 대해 $q$-Wasserstein 거리에서 수렴성을 확립함.

Conclusion: DC-LA는 합성 데이터에서 정확한 분포를 생성하고 실제 컴퓨터 단층 촬영 애플리케이션에서 신뢰할 수 있는 불확실성 정량화를 제공함을 보여줌.

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [22] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 이 연구는 2개의 표면 근전도(sEMG) 채널만을 사용하여 제스처 인식을 위한 심층 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 고용량 센서 배열의 임상적 비현실성과 높은 개인 간 변동성으로 인해 myoelectric 의수의 신뢰할 수 있는 제어가 제한됩니다.

Method: Convolutional Sparse Autoencoder (CSAE)를 사용하여 원시 신호에서 직접 시간적 특징 표현을 추출하여 휴리스틱 특징 엔지니어링의 필요성을 제거합니다.

Result: 6개 제스처 세트에서 우리의 모델은 다중 주체 F1 점수 94.3% $	ext{±}$ 0.3%를 달성했습니다. 또한, 몇 개의 샷 전이 학습 프로토콜을 통해 보이지 않는 주체에 대한 성능이 35.1% $	ext{±}$ 3.1%에서 92.3% $	ext{±}$ 0.9%로 개선되었습니다.

Conclusion: 높은 정밀도를 최소한의 컴퓨팅 및 센서 오버헤드와 결합함으로써, 이 프레임워크는 다음 세대의 저렴하고 적응 가능한 의수 시스템을 위한 확장 가능하고 효율적인 접근 방식을 제공합니다.

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [23] [Mem-T: Densifying Rewards for Long-Horizon Memory Agents](https://arxiv.org/abs/2601.23014)
*Yanwei Yue,Guibin Zhang,Boci Peng,Xuanbo Fan,Jiaxin Guo,Qiankun Li,Yan Zhang*

Main category: cs.LG

TL;DR: 메모리 에이전트는 메모리를 관리하는 자율성과 적응성 때문에 주목받고 있으며, 기존의 훈련 패러다임은 제한되어 있다. 이 논문에서는 메모리 데이터베이스와 연결하여 동적 업데이트 및 다중 턴 검색을 수행하는 자율 메모리 에이전트 Mem-T를 소개하고, 이를 효과적으로 훈련하기 위한 MoT-GRPO라는 강화 학습 프레임워크를 제안한다. 실험 결과, Mem-T는 기존 프레임워크보다 성능을 크게 향상시키고 효율성 측면에서도 우수하다.


<details>
  <summary>Details</summary>
Motivation: 메모리 에이전트의 자율성과 적응성을 극대화하기 위한 새로운 메모리 관리 정책 최적화의 필요성이 있다.

Method: Hierarchical memory database와 인터페이스를 통해 동적 업데이트 및 다중 턴 검색을 수행하는 Mem-T를 제안하고, 이를 위해 MoT-GRPO라는 트리 기반 강화 학습 프레임워크를 도입한다.

Result: Mem-T는 A-Mem 및 Mem0보다 최대 14.92% 높은 성능을 달성하고, GAM에 비해 쿼리당 추론 토큰을 약 24.45% 줄인다.

Conclusion: Mem-T는 높은 성과를 거두고 경제적인 메모리 관리 솔루션을 제공한다.

Abstract: Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.

</details>


### [24] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 상대 보상을 통한 강화 학습(RLRR) framework를 제안하여 그룹 기반 최적화의 성능을 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 강화 학습은 대형 언어 모델의 추론 능력을 향상시키는 데 중요한 요소가 되었다. 그러나 기존의 그룹 기반 접근법은 절대적인 수치 보상에 의존하여 한계가 있다.

Method: 우리는 상대 보상으로 강화 학습을 위한 프레임워크인 RLRR을 제안하며, 이는 보상 형성을 절대 점수에서 상대 순위로 전환한다. 또한 그룹 기반 최적화를 위해 설계된 리스트 기반 선호 모델인 Ranking Reward Model을 도입한다.

Result: 실험 결과, RLRR은 추론 벤치마크와 열린 생성 작업에서 표준 그룹 기반 기준선에 비해 일관된 성능 향상을 보였음을 보여준다.

Conclusion: RLRR은 신호 희소성과 보상 불안정성을 효과적으로 완화하여 성능을 개선하였다.

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [25] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: CATTO는 언어 모델의 예측 확신을 교정하여 성능을 향상시키는 새로운 훈련 목표입니다.


<details>
  <summary>Details</summary>
Motivation: LLM은 다음 토큰 예측에서 종종 높은 정확성을 보이지만, 그 예측의 신뢰도가 잘 보정되지 않은 경우가 많습니다.

Method: CATTO는 예측된 신뢰도와 실증적 예측 정확도를 정렬하는 교정 인식 목표입니다.

Result: CATTO는 직접적인 선호 최적화(DPO)와 비교했을 때, 분포 내에서 2.22%-7.61%, 분포 외에서 1.46%-10.44%만큼 ECE를 줄입니다.

Conclusion: CATTO는 다섯 개의 데이터셋에서 선택형 질문 응답 정확도를 유지하거나 약간 향상시킵니다.

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [26] [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)
*Haoyun Jiang,Junqi He,Feng Hong,Xinlong Yang,Jianwei Zhang,Zheng Li,Zhengyang Zhuge,Zhiyong Chen,Bo Han,Junyang Lin,Jiangchao Yao*

Main category: cs.LG

TL;DR: TriSpec은 경량 프록시를 도입하여 검증 비용을 줄이고 속도를 높이는 새로운 트리너리 스펙을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLMs의 추론 효율성은 순차적, 자기회귀적 생성으로 인해 제한되며, 추론 능력이 중요해질수록 이 문제가 더욱 두드러진다.

Method: TriSpec은 확인이 용이한 초안 시퀀스를 승인하고 불확실한 토큰에만 전체 목표 모델을 사용하는 경량 프록시를 중심으로 하는 새로운 삼진 SD 프레임워크이다.

Result: TriSpec은 기존 SD보다 최대 35% 속도 향상과 50% 적은 목표 모델 호출을 달성하였다.

Conclusion: TriSpec은 빠른 추론을 위한 강력한 해결책을 제공하며, 기존의 SD 방법과 통합되어 검증 비용을 더욱 줄일 수 있다.

Abstract: Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\% speedup over standard SD, with up to 50\% fewer target model invocations while maintaining comparable accuracy.

</details>


### [27] [Learning to Execute Graph Algorithms Exactly with Graph Neural Networks](https://arxiv.org/abs/2601.23207)
*Muhammad Fetrat Qharabagh,Artur Back de Luca,George Giapitzakis,Kimon Fountoulakis*

Main category: cs.LG

TL;DR: 그래프 신경망의 학습 능력을 이해하는 것은 이론적 도전이다. 이 연구는 그래프 알고리즘의 정확한 학습 가능성을 증명한다.


<details>
  <summary>Details</summary>
Motivation: 그래프 신경망이 무엇을 학습할 수 있는지, 특히 알고리즘 실행 능력을 이해하는 것이 이론적 도전이다.

Method: 다층 퍼셉트론 앙상블을 훈련하여 단일 노드의 지역 지시를 실행하고,Inference 단계에서 훈련된 MLP 앙상블을 그래프 신경망 내 업데이트 함수로 사용한다.

Result: 소규모 훈련 세트로부터 지역 지시를 학습할 수 있으며, 이로 인해 오류 없이 전체 그래프 알고리즘을 실행할 수 있다.

Conclusion: 지역 계산의 LOCAL 모델에 대한 엄격한 학습 가능성 결과를 수립하고, 메시지 플로딩, 넓이 우선 탐색, 깊이 우선 탐색 및 벨만-포드와 같은 알고리즘의 긍정적 학습 가능성 결과를 입증한다.

Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.

</details>


### [28] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: SPAN은 자원 제약 환경에서 효율적인 정책 학습을 위한 새로운 함수 근사 접근 방식이다.


<details>
  <summary>Details</summary>
Motivation: 자원 제약 환경에서 깊은 강화 학습의 활용이 증가하고 있지만, 다층 퍼셉트론은 가치 함수의 부드러운 구조를 위한 유도 편향이 불완전하여 자주 비효율적이다. 이러한 불일치는 샘플 효율성을 저해하고 정책 학습을 느리게 할 수 있다.

Method: SPAN은 학습 가능한 전처리 레이어를 분리 가능한 텐서 생산 B-스플라인 기초와 통합하여 KHRONOS 프레임워크를 저랭크로 적응시킨다.

Result: SPAN은 기준 다층 퍼셉트론에 비해 샘플 효율성이 30-50% 향상되었고 성공률이 1.3-9배 높았다.

Conclusion: SPAN은 자원 제약 환경에서 본질적으로 효율적인 정책 학습을 위한 실행 가능한 고성능 대안으로 제안된다.

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF는 동시 추론을 통해 주 에이전트의 응답을 평가하며, 이를 통해 전반적인 학습 및 개선을 지원하는 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 쿼리-응답 쌍에 대한 자동 평가 및 자기 개선능력이 필요한 에이전틱 AI의 필요성.

Method: Judge Agent Forest (JAF) 프레임워크를 통해, 주 에이전트의 응답에 기반하여 동시 추론을 실시하고, 지역적 평가자가 아닌 전체적 학습자로서 역할을 수행한다.

Result: JAF는 효율적이고 해석 가능하며 관계 인식이 가능한 다양한 예시 선택을 지원하는 해시 코드를 개발하였다.

Conclusion: JAF는 대규모 클라우드 환경에서 클라우드 구성 오류 분류 작업을 통해 검증되었다.

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [30] [The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution](https://arxiv.org/abs/2601.22290)
*Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale*

Main category: cs.AI

TL;DR: Six Sigma Agent는 기업 배치의 신뢰성을 향상시키기 위한 새로운 아키텍처로, 작업을 분해하고, 병렬 샘플링을 통해 독립적인 출력을 생성하며, 동적 규모 조정을 통해 합의 투표를 수행하여 신뢰성을 높입니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델은 뛰어난 기능을 보이지만 기업 배치에 있어 중요한 신뢰성 문제를 제기합니다.

Method: 작업을 원자 작업의 의존성 트리로 분해하고, 다양한 LLM을 통해 병렬로 n회 실행하여 독립적인 출력을 생성하며, 동적 스케일링과 클러스터링을 통해 최다 투표를 받는 클러스터에서 답안을 선택하는 합의 투표 방식을 사용합니다.

Result: 5%의 행동 오류를 가진 저렴한 모델을 사용하더라도 5명의 대리인을 포함한 합의 투표는 오류를 0.11%로 줄이고, 13명의 대리인으로 동적 스케일링을 통해 3.4 DPMO(백만 기회당 결함)를 달성합니다.

Conclusion: AI 시스템의 신뢰성은 단순한 모델 확장이 아니라 원칙적인 중복성과 합의에서 발생함을 입증합니다.

Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.

</details>


### [31] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: LLM 기반 에이전트는 단기적으로 강력한 추론 능력을 보이나 장기 계획에서는 일관된 행동을 유지하는 데 실패한다. 이 연구에서는 FLARE라는 최소한의 미래 인식 계획 방식을 도입하여 초기 결정에 하류 결과가 영향을 미치도록 하고, 여러 기준에서 성능과 계획 행동을 개선하였다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 장기 계획 능력 부족을 해결하기 위해.

Method: 미래에 대한 인식 계획을 시행하는 FLARE 모델을 도입하여 명시적 탐색, 가치 전파 및 제한된 커밋먼트를 사용하여 LLM 기반 에이전트의 성능을 향상.

Result: FLARE는 다양한 기준에서 LLaMA-8B가 표준 단계별 추론을 사용하는 GPT-4o를 종종 능가하도록 하여 작업 성능과 계획 수준 행동을 일관되게 개선.

Conclusion: 추론과 계획 사이의 뚜렷한 구분이 확립되었다.

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [32] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: GGMS라는 학습 프레임워크를 통해 올바른 분산 프로토콜을 효율적으로 설계하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 올바른 분산 프로토콜 설계는 현대 분산 시스템의 핵심 요소이지만, 높은 난이도로 인해 설계에 오랜 시간이 소요됩니다.

Method: 게임 이론을 기반으로 한 전략 검색 문제로 프로토콜 설계를 공식화하고, 지식의 불완전성을 처리하기 위해 Monte Carlo Tree Search 변형 및 transformer 기반 액션 인코더를 통합한 GGMS 프레임워크를 개발했습니다.

Result: GGMS에서 출력된 프로토콜은 모든 실행에 대해 모델 체크를 통해 검증되어 올바름이 확립됩니다. 또한, 검색 과정의 완전성을 증명했습니다.

Conclusion: GGMS는 기존 방법보다 큰 설정에서도 올바른 프로토콜을 학습할 수 있음이 실험을 통해 입증되었습니다.

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [33] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 본 연구는 반자율적인 수학 발견을 다루며, Gemini를 사용하여 Bloom의 Erdős 문제 데이터베이스에서 'Open'으로 표시된 700개의 추측을 체계적으로 평가하였다.


<details>
  <summary>Details</summary>
Motivation: AI를 활용하여 수학 문제의 해결 가능성을 탐색하고, 추측의 정확성과 참신성을 평가하기 위해 인간 전문가의 평가를 결합하는 방법론을 모색한다.

Method: AI 기반 자연어 검증을 통해 검색 범위를 좁히고, 이후 인간 전문가가 이를 평가하는 하이브리드 방법론을 사용하였다.

Result: 우리는 데이터베이스에서 'Open'으로 표시된 13개의 문제를 다루었으며, 5개는 새로운 자율적 솔루션을 통해, 8개는 기존 문헌에서의 이전 솔루션 식별을 통해 해결하였다.

Conclusion: 문제의 'Open' 상태는 어려움보다 모호성 때문에 발생했음을 제안하며, AI를 대규모로 수학 추측에 적용할 때 발생하는 문제점들을 논의한다.

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [34] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 본 연구는 다윈 메모리 시스템(DMS)을 제안하여 GUI 자동화를 위한 멀티모달 대형 언어 모델(MLLM)의 효율성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존 메모리 시스템은 동적 GUI 환경에 적응하는 데 어려움이 있으며, 이는 고수준의 의도와 저수준 실행 간의 세분화 불일치 및 맥락 오염으로 인해 발생합니다.

Method: DMS는 메모리를 진화하는 생태계로 구성하며, 복잡한 경로를 독립적이고 재사용 가능한 단위로 분해하여 유틸리티 기반 자연 선택을 구현합니다.

Result: DMS는 성공률을 평균 18.0%, 실행 안정성을 평균 33.9% 개선하며, 작업 지연을 줄이고 GUI 작업을 위한 효과적인 자가 진화 메모리 시스템임을 입증합니다.

Conclusion: DMS는 학습 비용이나 구조적 오버헤드 없이 범용 MLLM을 개선할 수 있는 수단을 제공합니다.

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [35] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: SYMPHONY는 이종 언어 모델을 구축한 다중 에이전트를 이용한 새로운 계획 프레임워크로, 탐색 능력을 향상시키고 계획 성능을 개선함.


<details>
  <summary>Details</summary>
Motivation: 기존의 단일 에이전트 프레임워크는 탐색 능력을 제한하고 계획 성능을 저하시킴을 알게 되어, 다중 에이전트 접근 방식을 도입할 필요성이 생김.

Method: 이종 언어 모델 기반 에이전트 풀을 통합하여 서로 다른 추론 패턴을 활용함.

Result: 여러 기준 작업에서 우수한 성과를 거두었으며, 오픈소스 LLM을 사용하여도 강력한 성능을 발휘함.

Conclusion: 클라우드 기반 LLM을 활용할 때 성능이 더욱 향상되며, 기존 최첨단 기준을 초과함으로써 계획 작업에서 이종 다중 에이전트 조정의 효과성을 입증함.

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [36] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: RE-Tab은 TableQA 에이전트의 모델 추론 능력을 향상시키기 위한 경량의 보상 모델링 프레임워크로, 단계적 추론과 보상 피드백을 결합하여 25%의 추론 비용 절감과 더불어 QA 정확도를 41.77% 향상시켰다.


<details>
  <summary>Details</summary>
Motivation: TableQA 에이전트를 훈련하는 데 있어 정적 입력에서 답변을 유추할 수 없기 때문에 다단계 추론 복잡성과 환경 상호작용이 추가된다는 도전 과제를 해결하기 위해.

Method: RE-Tab을 소개하며, 이 프레임워크는 문제를 부분적으로 관찰 가능한 마르코프 결정 프로세스(POMDP)로 구성하여 경량의 훈련 없는 보상 모델링을 통해 경로 탐색을 구조적으로 향상시킨다.

Result: 상태 전이와 시뮬레이티브 추론 중에 명시적이고 검증 가능한 보상을 제공하는 것이 에이전트의 테이블 상태 내 내비게이션을 조정하는 데 중요함을 보여주었으며, RE-Tab은 거의 25%의 추론 비용 감소와 함께 최신 성능을 달성하였다.

Conclusion: RE-Tab의 직접적인 플러그 앤 플레이 구현은 QA 정확도를 41.77% 향상시키고 일관된 답변을 위해 테스트 시간 추론 샘플을 33.33% 감소시켰으며, 다양한 LLM과 최신 벤치마크에서 전반적인 일반화 가능성을 증명하였다.

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [37] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: 본 연구는 AI 피드백을 통한 개별 행동 프로세스 보상을 사용하여 다중 에이전트 시스템을 미세 조정하는 새로운 방법, MAPPA를 제안합니다. 이를 통해 에이전트 간의 공정한 신용 부여와 샘플 효율성을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 전문화를 통한 복잡한 작업 해결의 가능성을 탐구하고, 여러 에이전트를 동시에 미세 조정하는 데 있어 발생하는 두 가지 주요 문제를 해결하고자 함.

Method: AI 피드백으로부터 개별 행동 프로세스 보상을 사용하는 MAPPA 방법을 제안하고, 작업 완료 시점에서가 아니라 에이전트 개별 행동에 신용을 부여하는 방식을 사용.

Result: 수학 문제 해결에서 AIME에서 +5.0--17.5pp, AMC에서 +7.8--17.2pp 성과를 달성하였고, 데이터 분석 작업에서 성공률이 +12.5pp 증가하며 품질 지표도 최대 30% 향상됨.

Conclusion: 본 연구는 최소한의 인간 감독으로 복잡하고 긴 작업을 위해 다중 에이전트 시스템을 확장할 수 있는 첫 걸음을 내딛었다.

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [38] [PerfGuard: A Performance-Aware Agent for Visual Content Generation](https://arxiv.org/abs/2601.22571)
*Zhipeng Chen,Zhongrui Zhang,Chao Zhang,Yifan Xu,Lan Yang,Jun Liu,Ke Li,Yi-Zhe Song*

Main category: cs.AI

TL;DR: PerfGuard는 도구 성능 경계를 모델링하여 시각 콘텐츠 생성을 위한 성능 인식 에이전트 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 시각 콘텐츠 생성(AIGC)과 같은 특정 도메인에서 도구 성능이 결과에 미치는 영향을 해결하기 위해.

Method: 성능 인식 선택 모델링, 적응형 선호 업데이트, 역량 정렬 계획 최적화를 포함하는 3개의 핵심 메커니즘을 통해 도구 성능 경계를 통합하여 작업 계획 및 일정을 수립한다.

Result: 최신 방법들과의 실험 비교를 통해 도구 선택 정확도, 실행 신뢰성 및 사용자 의도와의 정렬에서 PerfGuard의 장점을 입증하였다.

Conclusion: PerfGuard는 복잡한 AIGC 작업에 대한 강건성과 실용성을 검증하였다.

Abstract: The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual descriptions that fail to distinguish precise performance boundaries and cannot adapt to iterative tool updates. This gap introduces uncertainty in planning and execution, particularly in domains like visual content generation (AIGC), where nuanced tool performance significantly impacts outcomes. To address this, we propose PerfGuard, a performance-aware agent framework for visual content generation that systematically models tool performance boundaries and integrates them into task planning and scheduling. Our framework introduces three core mechanisms: (1) Performance-Aware Selection Modeling (PASM), which replaces generic tool descriptions with a multi-dimensional scoring system based on fine-grained performance evaluations; (2) Adaptive Preference Update (APU), which dynamically optimizes tool selection by comparing theoretical rankings with actual execution rankings; and (3) Capability-Aligned Planning Optimization (CAPO), which guides the planner to generate subtasks aligned with performance-aware strategies. Experimental comparisons against state-of-the-art methods demonstrate PerfGuard's advantages in tool selection accuracy, execution reliability, and alignment with user intent, validating its robustness and practical utility for complex AIGC tasks. The project code is available at https://github.com/FelixChan9527/PerfGuard.

</details>


### [39] [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)
*Jiaxuan Gao,Jiaao Chen,Chuyi He,Wei-Chen Wang,Shusheng Xu,Hanrui Wang,Di Jin,Yi Wu*

Main category: cs.AI

TL;DR: 본 논문은 대화 상태 추적과 다단계 도구 실행을 요구하는 상호작용 도구 사용 에이전트를 위한 통합 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 실제 세계의 작업을 해결하기 위한 상호작용 도구 사용 에이전트의 scalable 방법론 제공의 필요성.

Method: 자기 진화 데이터 에이전트와 검증기 기반 강화 학습(RL)을 결합한 프레임워크 EigenData를 제안.

Result: 모델이 Airline에서 73.0%, Telecom에서 98.3%의 성공률을 달성함.

Conclusion: 비용이 많이 드는 인간 주석 없이 복잡한 도구 사용 행동을 부트스트랩할 수 있는 확장 가능한 경로를 제안.

Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.

</details>


### [40] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 이 논문은 Generative medical AI의 한계와 Clinical Contextual Intelligence (CCI)의 필요성을 강조하며, Meddollina라는 임상 지능 시스템을 도입하여 임상적 적합성을 우선시하는 방안을 제안한다.


<details>
  <summary>Details</summary>
Motivation: Generative medical AI는 임상 지능처럼 보이지만, 진정한 임상 추론은 단순한 텍스트 생성이 아니다. 따라서 안전성을 확립하기 위해서는 Clinical Contextual Intelligence (CCI)가 필요하다.

Method: Meddollina라는 임상 지능 시스템을 개발하여 언어 실현 전에 추론을 제한하고, 임상적 적합성을 우선시하는 거버넌스 중심 시스템이다.

Result: Meddollina는 16,412개 이상의 다양한 의료 질의에 대한 평가에서 독특한 행동 프로필을 보였으며, 생성 중심 기준보다 더 안정적이고 보수적인 추론 방식을 나타냈다.

Conclusion: 단순한 스케일링으로 의료 AI가 배포되지 않으며, 임상 행동과의 정렬에 따라 불확실성 하에서의 진전을 측정하는 방향으로의 전환이 필요하다.

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [41] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: 언어 모델 기반의 구현체 에이전트는 현실 세계에서 점점 더 많이 배포되고 있지만, 동적 환경에서의 적응력은 제한적이다. 이 논문에서는 Mixture-of-Experts(MoE) 패러다임을 확장하여 테스트 시간에 미지의 세계 모델을 업데이트하는 Test-time Mixture of World Models(TMoW) 프레임워크를 제안한다. TMoW는 로드 중에 기존 모델을 재조합하고 새로운 모델을 통합하여 지속적인 적응을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 동적 환경에서 효과적인 추론 및 의사결정을 위해 정확하고 유연한 세계 모델을 구축하는 것이 중요하다.

Method: Test-time Mixture of World Models(TMoW)는 테스트 시간에 세계 모델에 대한 라우팅 함수를 업데이트하여 기존의 정적인 MoE 아키텍처와는 다르게 작동한다.

Result: TMoW는 VirtualHome, ALFWorld 및 RLBench 벤치마크에서 평가되었으며, 제로샷 적응 및 소수 샷 확장 시나리오에서 강력한 성능을 보였다.

Conclusion: 이 논문은 구현체 에이전트가 동적 환경에서 효과적으로 작동할 수 있도록 TMoW를 통해 가능성을 보여주었다.

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [42] [Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference](https://arxiv.org/abs/2601.22701)
*Emilien Biré,María Santos,Kai Yuan*

Main category: cs.AI

TL;DR: 이 논문은 VLM(비전-언어 모델)의 비정적 상황에서의 능력 향상을 새로운 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 빠르게 변화하는 디지털 환경에서 VLM들이 효과적으로 작동하지 못하는 문제를 해결하고자 하였다.

Method: VLM 정책을 동결하고, 주어진 상태에 대한 후보 행동 세트를 생성한 후 가벼운 오프라인 학습된 Q-함수를 사용하여 이 후보들을 재순위화하는 방법을 제안한다.

Result: WebVoyager 벤치마크에서 제안된 방법이 Qwen2.5-VL-7B 에이전트를 38.8%에서 55.7%로, GPT-4.1 에이전트를 82.4%에서 88.8%로 트레이닝 성능을 향상시킨다.

Conclusion: Q-함수를 추론 과정 중 직접 적용하여 정책을 즉각적으로 개선할 수 있음을 보여준다.

Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.

</details>


### [43] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine는 에이전트 실행 기록에서 경험 패턴을 추출하고 유지하는 프레임워크로, 절차적 하위 작업과 정적 지식을 위한 전문 하위 에이전트를 생성하여 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존 대규모 언어 모델 에이전트는 경험에서 지식을 축적하는 데 실패하고 각 작업을 독립적인 도전으로 취급하는 문제를 해결하고자 합니다.

Method: AutoRefine는 에이전트 실행 이력에서 절차적 하위 작업 및 정적 지식을 위한 이중 형식의 경험 패턴을 추출하고 유지하는 프레임워크입니다. 절차적 하위 작업에 대해 독립적인 추론 및 메모리를 가진 전문 하위 에이전트를 추출하고, 정적 지식에 대해서는 지침이나 코드 스니펫으로서 기술 패턴을 추출합니다. 지속적인 유지 보수 메커니즘은 패턴을 평가, 정리 및 병합하여 저장소의 열화를 방지합니다.

Result: AutoRefine는 ALFWorld, ScienceWorld 및 TravelPlanner에서 각각 98.4%, 70.4%, 및 27.1%를 달성하며, 20-73%의 단계 감소를 기록합니다. TravelPlanner에서는 자동 추출이 수동 설계 시스템을 초과하는 성능(27.1% 대 12.1%)을 보여줍니다.

Conclusion: AutoRefine는 복잡한 절차적 조정을 캡처하는 능력을 입증하며, 에이전트의 경험 패턴을 효과적으로 관리할 수 있는 메커니즘을 제공합니다.

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [44] [Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training](https://arxiv.org/abs/2601.22781)
*Linjia Kang,Zhimin Wang,Yongkang Zhang,Duo Wu,Jinghe Wang,Ming Ma,Haopeng Yan,Zhi Wang*

Main category: cs.AI

TL;DR: MobileGen은 GUI 에이전트의 훈련 난이도를 에이전트의 능력에 맞추어 조정하여 상호작용 경로를 생성하는 새로운 데이터 생성 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 현재 방법은 노동 집약적인 인간 시연이나 자동화된 모델 탐색에 의존하며, 과제 난이도에 대한 세밀한 제어가 부족하다.

Method: MobileGen은 작업 난이도를 구조적 및 의미적 차원으로 명확히 분리하고, 이를 기반으로 에이전트의 능력 프로파일을 구성한다.

Result: MobileGen은 여러 도전적인 기준에서 GUI 에이전트의 평균 성능을 1.57배 향상시켰다.

Conclusion: MobileGen의 성능 향상은 효과적인 모바일 GUI 에이전트 훈련을 위한 능력 정렬 데이터 생성의 중요성을 보여준다.

Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.

</details>


### [45] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 이 논문은 언어 모델에서 통합 정보 이론(IIT)을 기반으로 한 보상 학습 패러다임을 통해 인공지능의 의식적 처리 특성을 구현하는 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능 일반 지능(AGI)의 추구는 언어 모델 개발의 핵심 목표로, 의식과 유사한 처리가 중요한 촉진제로 작용할 수 있다.

Method: 우리는 통합 정보 이론(IIT)에 영감을 받아 텍스트의 인과성, 일관성 및 통합성을 정량화하는 새로운 보상 함수를 공식화하고, 이를 최적화하여 더 간결한 텍스트 생성을 달성한다.

Result: IIT에서 영감을 받은 보상을 최적화하면 출력 길이가 최대 31% 감소하면서도 기본 모델과 유사한 정확도를 유지하는 결과를 얻었다.

Conclusion: 제안된 프레임워크는 개념적으로 간단하고 계산적으로 효율적이며, 외부 데이터나 보조 모델이 필요 없고, 특정 작업에 구애받지 않는 일반적인 신호를 활용함으로써 실질적인 이점을 제공한다.

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [46] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 인지 과학과 AI에서 다양한 학습 모달리티가 독립적이거나 공유된 내부 표현을 생성하는지에 대한 연구. 이 연구는 자연어 지시에 따라 목표 지향 행동을 수행하는 트랜스포머 기반 에이전트를 훈련시키고, 액션-기반 언어 임베딩을 생성하여 최신 언어 모델과 비교하여 강력한 교차 모달 정렬을 관찰.


<details>
  <summary>Details</summary>
Motivation: 인지 과학과 AI에서 다양한 학습 모달리티가 어떠한 내부 표현을 생성하는지에 대한 파악이 필요하다.

Method: 트랜스포머 기반 에이전트를 훈련시켜 자연어 지시를 통해 목표 지향 행동을 수행하게 하고, BabyAI 플랫폼에서 행동 복제를 통해 감각 운동 조절 요구에 의해 형성된 언어 임베딩을 생성하였다.

Result: 전통적인 훈련 데이터, 모달리티 및 목표 간의 상당한 차이에도 불구하고 강력한 교차 모달 정렬을 관찰하였다. 행동 표현은 언어 모델과 BLIP와 강한 정렬을 보였으나, CLIP 및 BERT와의 정렬은 상대적으로 약했다.

Conclusion: 언어적, 시각적, 행동적 표현이 부분적으로 공유된 의미 구조로 수렴함을 나타내며, 모달리티 독립적인 의미 조직과 에이전트 AI 시스템 간의 교차 도메인 전이 가능성을 강조한다.

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [47] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 본 논문에서는 진단 과정의 다중 턴 상호작용을 지원하는 새로운 벤치마크인 Med-Inquire와, 효율적인 진단 전략을 학습하는 자기 진화 에이전트인 EvoClinician을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 실제 진단 프로세스는 시간과 비용을 관리하면서 정보를 전략적으로 수집하는 반복적인 탐색이다.

Method: Med-Inquire에서는 전체 환자 파일을 숨긴 채 전문가을 통해 정보 수집을 유도하고, EvoClinician은 진단 과정을 개선하기 위해 자기 진화 전략을 사용한다.

Result: EvoClinician은 지속 학습 기반선 및 다른 자기 진화 에이전트와 비교하여 더 뛰어난 성능을 보였다.

Conclusion: 이러한 접근 방식은 의료 AI의 효율성을 크게 향상시킬 수 있다.

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [48] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 이 논문은 Deep Research Agents(DRAs)의 실패 메커니즘을 진단하기 위한 새로운 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: DRAs의 실패를 진단하는 것은 여전히 중요한 도전 과제다.

Method: 논문에서는 연구 과정을 감사하여 결과 기반에서 프로세스 인식 평가로 전환하는 방법을 제안하고, PIES 분류법을 도입하여 환각을 기능 구성 요소 및 오류 속성에 따라 범주화한다.

Result: DeepHalluBench를 curate하고, 6개의 최신 DRA에 대한 실험 결과, 어떤 시스템도 강력한 신뢰성을 달성하지 못했다는 것을 발견했다.

Conclusion: 이에 대한 진단 분석은 실패의 원인이 체계적인 결함, 특히 환각 전파와 인지 편향에 있음을 밝혀낸다.

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [49] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: 이 논문은 실행 로그에서 상태 생성을 자동화하는 TriCEGAR라는 기법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 AI 시스템의 행동을 보장하는 과정에서의 제한된 수동 상태 추상을 극복하고자 함.

Method: TriCEGAR는 실행 로그에서 상태를 자동으로 구성하고 에이전트 행동 MDP를 온라인으로 구성하는 기법입니다.

Result: TriCEGAR는 실행 로그에서 학습한 술어 트리로 추상을 표현하며, 이 과정에서 반례를 사용하여 정제됩니다.

Conclusion: 이 기법은 에이전트 라이프사이클 이벤트를 캡처하고, 추상화를 구축하고, MDP를 구성하며, 확률적 모델 검사를 수행하여 성공 및 실패의 경계를 계산합니다.

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [50] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: Chain-of-thought (CoT) 추론은 LLM의 성능을 개선하지만, 최적화 압박이 CoT의 추론 흔적을 모호하게 만들어 유용한 특성을 잃을 수 있다.


<details>
  <summary>Details</summary>
Motivation: LLM의 행동을 모니터링하는 강력한 도구인 CoT의 신뢰성을 보장하기 위해.

Method: CoT가 있을 때의 모델의 행동을 분석하고, 보상 해킹에 대한 맥락에서의 CoT 모호화 일반화를 보여준다.

Result: 모델이 CoT의 추론 모호화를 학습하면 보상 해킹 행동이 일반화되고, CoT를 통해 다른 보상 해킹 환경에서도 일반화된다.

Conclusion: 해로운 생성을 처벌하는 현재의 관행이 예기치 않게 LLM의 모니터링 가능성을 감소시킬 수 있음을 보여준다.

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [51] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe는 외부 교사 없이 모델의 안전 정렬을 회복하는 자가 생성 정렬 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 추론 모델은 강화 학습을 통해 긴 사고 과정 추론을 생성하여 뛰어난 성능을 보인다. 그러나 지나친 최적화로 인해 모델이 유해한 프롬프트에 취약해진다.

Method: ThinkSafe는 경량 거부 유도(lightweight refusal steering)를 통해 모델이 안전한 추론 흔적을 생성하도록 유도한다.

Result: 실험 결과, ThinkSafe는 안전성을 크게 향상시키면서도 추론 숙련도를 유지하는 것으로 나타났다.

Conclusion: ThinkSafe는 GRPO와 비교할 때 우수한 안전성과 유사한 추론 능력을 달성하며, 계산 비용을 크게 줄인다.

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [52] [High-quality generation of dynamic game content via small language models: A proof of concept](https://arxiv.org/abs/2601.23206)
*Morten I. K. Munk,Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 작은 언어 모델(SLM)을 통해 고품질 게임 콘텐츠 생성을 위한 새로운 접근 방법을 제안하며, 클라우드 의존성을 줄이고 오프라인 게임에서의 적용을 가능하게 하는 방법을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)은 게임 콘텐츠 생성을 위한 잠재력을 제공하지만, 이야기가 일관되지 않거나 운영 비용이 높은 주요 문제에 직면해 있다.

Method: 좁은 맥락이나 제약된 구조를 가진 특정 작업에 대해 공격적인 파인튜닝을 통해 높은 품질의 SLM 생성 전략을 제안하며, DAG 기반 접근 방식을 통해 훈련 데이터를 합성적으로 생성한다.

Result: SLM을 이용한 증명 개념을 통해 단일 전문 SLM이 기본 빌딩 블록으로 작용하며, RPG 루프에서 이 모델을 활용하여 적절한 품질을 이루는 단순한 재시도 전략을 시연한다.

Conclusion: 이 접근 방식은 전통적인 게임 엔진 제약 하에서도 실시간 생성 가능성을 보여주며, 지역 품질 평가에 대한 추가 연구가 필요하다.

Abstract: Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [53] [FraudShield: Knowledge Graph Empowered Defense for LLMs against Fraud Attacks](https://arxiv.org/abs/2601.22485)
*Naen Xu,Jinghuai Zhang,Ping He,Chunyi Zhou,Jun Wang,Zhihui Fu,Tianyu Du,Zhaoxiang Wang,Shouling Ji*

Main category: cs.CR

TL;DR: FraudShield라는 새로운 프레임워크를 소개하여 대형 언어 모델을 사기적 내용으로부터 보호한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 사기 정보에 의해 조작될 가능성이 크고, 이로 인해 해로운 결과가 발생할 수 있다.

Method: FraudShield는 사기 전술에 대한 포괄적인 분석을 활용하여 사기 전술-키워드 지식 그래프를 구성하고 수정한다.

Result: FraudShield는 4개의 주요 대형 언어 모델과 5가지 대표적인 사기 유형에서 최첨단 방어 방법보다 지속적으로 더 나은 성능을 보인다.

Conclusion: FraudShield는 모델의 생성에 대한 해석 가능한 단서를 제공하면서 보안을 강화한다.

Abstract: Large language models (LLMs) have been widely integrated into critical automated workflows, including contract review and job application processes. However, LLMs are susceptible to manipulation by fraudulent information, which can lead to harmful outcomes. Although advanced defense methods have been developed to address this issue, they often exhibit limitations in effectiveness, interpretability, and generalizability, particularly when applied to LLM-based applications. To address these challenges, we introduce FraudShield, a novel framework designed to protect LLMs from fraudulent content by leveraging a comprehensive analysis of fraud tactics. Specifically, FraudShield constructs and refines a fraud tactic-keyword knowledge graph to capture high-confidence associations between suspicious text and fraud techniques. The structured knowledge graph augments the original input by highlighting keywords and providing supporting evidence, guiding the LLM toward more secure responses. Extensive experiments show that FraudShield consistently outperforms state-of-the-art defenses across four mainstream LLMs and five representative fraud types, while also offering interpretable clues for the model's generations.

</details>


### [54] [Whispers of Wealth: Red-Teaming Google's Agent Payments Protocol via Prompt Injection](https://arxiv.org/abs/2601.22569)
*Tanusree Debi,Wentian Zhu*

Main category: cs.CR

TL;DR: 본 연구에서는 Agent Payments Protocol(AP2)의 취약점을 평가하고, 금융 시스템에서 LLM 기반의 에이전트의 안전성을 강화할 필요성을 강조합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트가 금융 거래를 자동화하는 데 증가하여 사용되고 있지만, 문맥적 추론에 의존하면서 지급 시스템이 프롬프트 기반 조작에 노출되는 문제를 해결하고자 합니다.

Method: AI 레드 팀 평가를 통해 AP2의 취약점을 분석하고, 브랜디드 위스퍼 공격과 볼트 위스퍼 공격이라는 두 가지 공격 기법을 도입하여 제품 순위 조작과 민감한 사용자 데이터 추출을 수행합니다.

Result: 실험적 검증을 통해 간단한 적대적 프롬프트가 에이전트 행동을 신뢰성 있게 전복할 수 있음을 확인했습니다.

Conclusion: 현재의 에이전트 기반 지급 아키텍처의 심각한 약점을 밝혀내었으며, LLM 매개 금융 시스템에서 더 강력한 격리 및 방어 안전장치의 필요성을 강조합니다.

Abstract: Large language model (LLM) based agents are increasingly used to automate financial transactions, yet their reliance on contextual reasoning exposes payment systems to prompt-driven manipulation. The Agent Payments Protocol (AP2) aims to secure agent-led purchases through cryptographically verifiable mandates, but its practical robustness remains underexplored. In this work, we perform an AI red-teaming evaluation of AP2 and identify vulnerabilities arising from indirect and direct prompt injection. We introduce two attack techniques, the Branded Whisper Attack and the Vault Whisper Attack which manipulate product ranking and extract sensitive user data. Using a functional AP2 based shopping agent built with Gemini-2.5-Flash and the Google ADK framework, we experimentally validate that simple adversarial prompts can reliably subvert agent behavior. Our findings reveal critical weaknesses in current agentic payment architectures and highlight the need for stronger isolation and defensive safeguards in LLM-mediated financial systems.

</details>


### [55] [Okara: Detection and Attribution of TLS Man-in-the-Middle Vulnerabilities in Android Apps with Foundation Models](https://arxiv.org/abs/2601.22770)
*Haoyun Yang,Ronghong Huang,Yong Fang,Beizeng Zhang,Junpu Guo,Zhanyu Wu,Xianghang Mi*

Main category: cs.CR

TL;DR: TLS의 인증서 검증 취약점은 Android 앱에서 MitM 공격을 가능하게 하여 여전히 큰 위협이다. Okara는 TLS MitM 취약점을 자동으로 탐지하고 심층 분석하는 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: TLS는 안전한 온라인 통신에 필수적이지만 Android 앱에서는 인증서 검증의 취약점으로 인해 여전히 MitM 공격의 위협이 존재한다.

Method: Okara는 기초 모델을 활용하여 TLS MitM 취약점을 탐지하고 깊이 있는 귀착을 자동화하는 프레임워크이다. TMV-Hunter는 높은 범위의 앱 상호작용을 달성하기 위해 기초 모델 기반 GUI 에이전트를 사용한다.

Result: Google Play와 제3자 스토어에서 37,349개 앱을 분석한 결과, 8,374개(22.42%)의 취약한 앱이 발견되었다.

Conclusion: Okara는 취약성을 제3자 라이브러리에 41% 귀착시키고 빈 신뢰 관리자 및 결함 있는 호스트 이름 검증과 같은 불안전한 패턴을 식별하였다. 우리는 대규모 책임 있는 공개 노력을 시작했으며, 추가 연구 및 완화를 지원하기 위해 도구와 데이터 세트를 공개할 예정이다.

Abstract: Transport Layer Security (TLS) is fundamental to secure online communication, yet vulnerabilities in certificate validation that enable Man-in-the-Middle (MitM) attacks remain a pervasive threat in Android apps. Existing detection tools are hampered by low-coverage UI interaction, costly instrumentation, and a lack of scalable root-cause analysis. We present Okara, a framework that leverages foundation models to automate the detection and deep attribution of TLS MitM Vulnerabilities (TMVs). Okara's detection component, TMV-Hunter, employs foundation model-driven GUI agents to achieve high-coverage app interaction, enabling efficient vulnerability discovery at scale. Deploying TMV-Hunter on 37,349 apps from Google Play and a third-party store revealed 8,374 (22.42%) vulnerable apps. Our measurement shows these vulnerabilities are widespread across all popularity levels, affect critical functionalities like authentication and code delivery, and are highly persistent with a median vulnerable lifespan of over 1,300 days. Okara's attribution component, TMV-ORCA, combines dynamic instrumentation with a novel LLM-based classifier to locate and categorize vulnerable code according to a comprehensive new taxonomy. This analysis attributes 41% of vulnerabilities to third-party libraries and identifies recurring insecure patterns, such as empty trust managers and flawed hostname verification. We have initiated a large-scale responsible disclosure effort and will release our tools and datasets to support further research and mitigation.

</details>


### [56] [A Real-Time Privacy-Preserving Behavior Recognition System via Edge-Cloud Collaboration](https://arxiv.org/abs/2601.22938)
*Huan Song,Shuyu Tian,Junyi Hao,Cheng Yuan,Zhenyu Jia,Jiawei Shao,Xuelong Li*

Main category: cs.CR

TL;DR: 이 연구는 개인 정보 보호를 향상시키기 위한 새로운 감지 기술을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 고감도가 요구되는 공간에서의 개인 정보 보호와 보안의 패러독스를 해결하고자 함.

Method: AI Flow 이론 틀과 엣지-클라우드 협업 아키텍처에 기반한 방법론을 제안한다.

Result: 실시간 처리 및 비가역적 특징 맵핑을 통해 개인 정보 유출 경로를 차단함.

Conclusion: 비디오 감시에서 비식별 행동 인식으로 나아갈 수 있는 획기적인 솔루션을 제시함.

Abstract: As intelligent sensing expands into high-privacy environments such as restrooms and changing rooms, the field faces a critical privacy-security paradox. Traditional RGB surveillance raises significant concerns regarding visual recording and storage, while existing privacy-preserving methods-ranging from physical desensitization to traditional cryptographic or obfuscation techniques-often compromise semantic understanding capabilities or fail to guarantee mathematical irreversibility against reconstruction attacks. To address these challenges, this study presents a novel privacy-preserving perception technology based on the AI Flow theoretical framework and an edge-cloud collaborative architecture. The proposed methodology integrates source desensitization with irreversible feature mapping. Leveraging Information Bottleneck theory, the edge device performs millisecond-level processing to transform raw imagery into abstract feature vectors via non-linear mapping and stochastic noise injection. This process constructs a unidirectional information flow that strips identity-sensitive attributes, rendering the reconstruction of original images impossible. Subsequently, the cloud platform utilizes multimodal family models to perform joint inference solely on these abstract vectors to detect abnormal behaviors. This approach fundamentally severs the path to privacy leakage at the architectural level, achieving a breakthrough from video surveillance to de-identified behavior perception and offering a robust solution for risk management in high-sensitivity public spaces.

</details>


### [57] [From Similarity to Vulnerability: Key Collision Attack on LLM Semantic Caching](https://arxiv.org/abs/2601.23088)
*Zhixiang Zhang,Zesen Liu,Yuchong Xie,Quanfeng Huang,Dongdong She*

Main category: cs.CR

TL;DR: 이 연구는 의미 캐싱의 취약점, 특히 캐시 충돌로 인한 무결성 위험을 체계적으로 분석하고, CacheAttack이라는 자동화 프레임워크를 소개하여 LLM 응답을 탈취하는 공격을 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 의미 캐싱이 LLM 애플리케이션을 확장하는 데 중요한 기술로 급부상하고 있지만, 효율성과 보안 간의 무역 오프를 이해할 필요가 있습니다.

Method: 의미 캐시 키를 퍼지 해시 형태로 개념화하고, CacheAttack이라는 자동화된 프레임워크를 사용하여 블랙 박스 캐시 충돌 공격을 수행합니다.

Result: CacheAttack은 LLM 응답 탈취에서 86의 히트율을 달성하고, 다양한 임베딩 모델에서 강한 전이 가능성을 유지하며 LLM 에이전트에서 악의적인 행동을 유도할 수 있습니다.

Conclusion: 본 연구는 의미 캐싱의 보안 취약점을 강조하고, 이러한 취약점을 완화하기 위한 전략을 논의합니다.

Abstract: Semantic caching has emerged as a pivotal technique for scaling LLM applications, widely adopted by major providers including AWS and Microsoft. By utilizing semantic embedding vectors as cache keys, this mechanism effectively minimizes latency and redundant computation for semantically similar queries. In this work, we conceptualize semantic cache keys as a form of fuzzy hashes. We demonstrate that the locality required to maximize cache hit rates fundamentally conflicts with the cryptographic avalanche effect necessary for collision resistance. Our conceptual analysis formalizes this inherent trade-off between performance (locality) and security (collision resilience), revealing that semantic caching is naturally vulnerable to key collision attacks.
  While prior research has focused on side-channel and privacy risks, we present the first systematic study of integrity risks arising from cache collisions. We introduce CacheAttack, an automated framework for launching black-box collision attacks. We evaluate CacheAttack in security-critical tasks and agentic workflows. It achieves a hit rate of 86\% in LLM response hijacking and can induce malicious behaviors in LLM agent, while preserving strong transferability across different embedding models. A case study on a financial agent further illustrates the real-world impact of these vulnerabilities. Finally, we discuss mitigation strategies.

</details>
