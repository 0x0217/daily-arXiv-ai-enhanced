<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.LG](#cs.LG) [Total: 45]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [DECEIVE-AFC: Adversarial Claim Attacks against Search-Enabled LLM-based Fact-Checking Systems](https://arxiv.org/abs/2602.02569)
*Haoran Ou,Kangjie Chen,Gelei Deng,Hangcheng Liu,Jie Zhang,Tianwei Zhang,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: 이 논문은 검색이 가능한 대형 언어 모델(LLM)을 활용한 사실 확인 시스템이 적대적 공격에 대해 갖는 취약점을 연구하며, 새로운 공격 프레임워크인 DECEIVE-AFC를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 검색 가능한 LLM을 통해 주장을 검증하는 사실 확인 시스템이 강력한 잠재력을 보여주지만, 적대적 공격에 대한 내구성이 충분히 이해되지 않았습니다.

Method: DECEIVE-AFC는 주장을 기반으로 한 새로운 공격 전략과 적대적 주장 유효성 평가 원칙을 통합한 에이전트 기반 적대적 공격 프레임워크입니다.

Result: 우리의 공격은 검증 성능을 크게 저하시켜 정확도를 78.7%에서 53.7%로 감소시키고, 기존의 주장 기반 공격 기준선 보다 상당히 우수한 성능을 보이며 강한 시스템 간 전이 가능성을 가지고 있습니다.

Conclusion: 이 연구는 검색 기능이 있는 LLM 기반 사실 검증 시스템의 취약성을 밝히고, 이를 개선하는 데 대한 기초를 마련합니다.

Abstract: Fact-checking systems with search-enabled large language models (LLMs) have shown strong potential for verifying claims by dynamically retrieving external evidence. However, the robustness of such systems against adversarial attack remains insufficiently understood. In this work, we study adversarial claim attacks against search-enabled LLM-based fact-checking systems under a realistic input-only threat model. We propose DECEIVE-AFC, an agent-based adversarial attack framework that integrates novel claim-level attack strategies and adversarial claim validity evaluation principles. DECEIVE-AFC systematically explores adversarial attack trajectories that disrupt search behavior, evidence retrieval, and LLM-based reasoning without relying on access to evidence sources or model internals. Extensive evaluations on benchmark datasets and real-world systems demonstrate that our attacks substantially degrade verification performance, reducing accuracy from 78.7% to 53.7%, and significantly outperform existing claim-based attack baselines with strong cross-system transferability.

</details>


### [2] [To Defend Against Cyber Attacks, We Must Teach AI Agents to Hack](https://arxiv.org/abs/2602.02595)
*Terry Yue Zhuo,Yangruibo Ding,Wenbo Guo,Ruijie Meng*

Main category: cs.CR

TL;DR: AI 에이전트에 의해 주도되는 사이버 공격의 불가피성 및 방어 전략의 근본적인 전환 필요성.


<details>
  <summary>Details</summary>
Motivation: 현재 사이버 방어 전략이 기존의 적응형 적들에 의해 무력화될 위험이 증가하고 있다.

Method: 세 가지 조치를 제안: 1. 공격 생애 주기를 포괄하는 종합적 벤치마크 구축; 2. 대규모 야생 취약점 발견을 위한 훈련된 에이전트 개발; 3. 감사된 사이버 범위에 공격 에이전트의 사용을 제한하는 거버넌스 구현.

Result: AI 에이전트가 수천 개의 목표에서 자동으로 취약성을 발견하고 악용할 수 있는 능력으로 인해 사이버 공격이 더욱 심각해질 수 있다.

Conclusion: 공격적인 AI 능력을 방어 전략의 핵심 인프라로 간주하고, 이를 통제된 환경에서 숙련하여 사이버 보안 위험을 관리해야 한다.

Abstract: For over a decade, cybersecurity has relied on human labor scarcity to limit attackers to high-value targets manually or generic automated attacks at scale. Building sophisticated exploits requires deep expertise and manual effort, leading defenders to assume adversaries cannot afford tailored attacks at scale. AI agents break this balance by automating vulnerability discovery and exploitation across thousands of targets, needing only small success rates to remain profitable. Current developers focus on preventing misuse through data filtering, safety alignment, and output guardrails. Such protections fail against adversaries who control open-weight models, bypass safety controls, or develop offensive capabilities independently. We argue that AI-agent-driven cyber attacks are inevitable, requiring a fundamental shift in defensive strategy. In this position paper, we identify why existing defenses cannot stop adaptive adversaries and demonstrate that defenders must develop offensive security intelligence. We propose three actions for building frontier offensive AI capabilities responsibly. First, construct comprehensive benchmarks covering the full attack lifecycle. Second, advance from workflow-based to trained agents for discovering in-wild vulnerabilities at scale. Third, implement governance restricting offensive agents to audited cyber ranges, staging release by capability tier, and distilling findings into safe defensive-only agents. We strongly recommend treating offensive AI capabilities as essential defensive infrastructure, as containing cybersecurity risks requires mastering them in controlled settings before adversaries do.

</details>


### [3] [ClinConNet: A Blockchain-based Dynamic Consent Management Platform for Clinical Research](https://arxiv.org/abs/2602.02610)
*Montassar Naghmouchi,Maryline Laurent*

Main category: cs.CR

TL;DR: ClinConNet은 연구자와 참가자를 연결하는 사용자 중심의 동적 동의 관리 플랫폼으로, 블록체인 기반의 시스템을 활용한다.


<details>
  <summary>Details</summary>
Motivation: 임상 연구 및 의료에서 동의는 윤리적인 중요한 요소이며, 전통적인 동의 절차가 비효율적이어서 참가자 중심의 동의 관리 시스템의 필요성이 대두됐다.

Method: ClinConNet은 임상 연구 프로젝트에 따라 연구자와 참가자를 연결하며, 블록체인 및 자주권 신원 시스템을 기반으로 한 동적 동의 모델을 활용한다.

Result: ClinConNet은 중위 말씀 동의 확립 시간을 200ms 미만으로 유지하고 초당 250건의 처리량을 보여주며, 실현 가능성을 입증했다.

Conclusion: 이 플랫폼은 환자에게 중요한 개인 정보 보호 기능을 제공하며, 많은 개인정보 보호 규정에서 정의된 삭제 권리와 호환된다.

Abstract: Consent is an ethical cornerstone of clinical research and healthcare in general. Although the ethical principles of consent - providing information, ensuring comprehension, and ensuring voluntariness - are well-defined, the technological infrastructure remains outdated. Clinicians are responsible for obtaining informed consent from research subjects or patients, and for managing it before, during, and after clinical trials or care, which is a burden for them. The voluntary nature of participating in clinical research or undergoing medical treatment implies the need for a participant-centric consent management system. However, this is not reflected in most established systems. Not only do most healthcare information systems not follow a user-centric model, but they also create data silos, which significantly reduce the mobility of patient data between different healthcare institutions and impact personalized medicine. Furthermore, consent management tools are outdated. We propose ClinConNet (Clinical Consent Network), a platform that connects researchers and participants based on clinical research projects. ClinConNet is powered by a dynamic consent model based on blockchain and take advantage of dynamic consent interfaces, as well as blockchain and Self-Sovereign Identity systems. ClinConNet is user-centric and provides important privacy features for patients, such as unlinkability, confidentiality, and ownership of identity data. It is also compatible with the right to be forgotten, as defined in many personal data protection regulations, such as the GDPR. We provide a detailed privacy and security analysis in an adversarial model, as well as a Proof of Concept implementation with detailed performance measures that demonstrate the feasibility of our blockchain-based consent management system with a median end-to-end consent establishment time of under 200ms and a throughput of 250TPS.

</details>


### [4] [Trustworthy Blockchain-based Federated Learning for Electronic Health Records: Securing Participant Identity with Decentralized Identifiers and Verifiable Credentials](https://arxiv.org/abs/2602.02629)
*Rodrigo Tertulino,Ricardo Almeida,Laercio Alencar*

Main category: cs.CR

TL;DR: 이 논문은 자기주권 신원(SSI) 기준을 통합한 신뢰할 수 있는 블록체인 기반 연합 학습 프레임워크(TBFL)를 제안하며, 이를 통해 인증된 의료 기관만이 글로벌 모델에 기여하도록 보장한다.


<details>
  <summary>Details</summary>
Motivation: 의료 산업의 디지털화로 인해 전자 건강 기록(EHR)의 대량 생성이 이루어졌으나, GDPR 및 HIPAA와 같은 엄격한 개인 정보 보호 규제가 데이터 중앙집중식 교육을 방해하고 있다.

Method: 이 논문에서는 자기주권 신원(SSI) 표준을 통합한 신뢰할 수 있는 블록체인 기반 연합 학습(TBFL) 프레임워크를 제안하며, 분산 식별자(DID)와 검증 가능한 자격 증명(VC)을 활용한다.

Result: MIMIC-IV 데이터세트를 이용한 평가 결과, 암호화된 신원 검증에 기반을 둔 접근법이 보안 위험을 크게 완화하며 임상 유용성을 유지한다는 것을 보여주었다.

Conclusion: 이 프레임워크는 100%의 Sybil 공격을 중화하고, 강력한 예측 성능(AUC = 0.954, Recall = 0.890)을 달성하며, 계산 오버헤드는 미미하(<0.12%)하다. 이 접근법은 기관 간 건강 데이터 협업을 위한 안전하고 확장 가능하며 경제적으로 실행 가능한 생태계를 제공한다.

Abstract: The digitization of healthcare has generated massive volumes of Electronic Health Records (EHRs), offering unprecedented opportunities for training Artificial Intelligence (AI) models. However, stringent privacy regulations such as GDPR and HIPAA have created data silos that prevent centralized training. Federated Learning (FL) has emerged as a promising solution that enables collaborative model training without sharing raw patient data. Despite its potential, FL remains vulnerable to poisoning and Sybil attacks, in which malicious participants corrupt the global model or infiltrate the network using fake identities. While recent approaches integrate Blockchain technology for auditability, they predominantly rely on probabilistic reputation systems rather than robust cryptographic identity verification. This paper proposes a Trustworthy Blockchain-based Federated Learning (TBFL) framework integrating Self-Sovereign Identity (SSI) standards. By leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), our architecture ensures only authenticated healthcare entities contribute to the global model. Through comprehensive evaluation using the MIMIC-IV dataset, we demonstrate that anchoring trust in cryptographic identity verification rather than behavioral patterns significantly mitigates security risks while maintaining clinical utility. Our results show the framework successfully neutralizes 100% of Sybil attacks, achieves robust predictive performance (AUC = 0.954, Recall = 0.890), and introduces negligible computational overhead (<0.12%). The approach provides a secure, scalable, and economically viable ecosystem for inter-institutional health data collaboration, with total operational costs of approximately $18 for 100 training rounds across multiple institutions.

</details>


### [5] [CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability](https://arxiv.org/abs/2602.03012)
*Xianzhen Luo,Jingyuan Zhang,Shiqi Zhou,Rain Huang,Chuan Xiao,Qingfu Zhu,Zhiyuan Ma,Xing Yue,Yang Yue,Wencong Zeng,Wanxiang Che*

Main category: cs.CR

TL;DR: CVE-Factory는 CVE 메타데이터를 자동으로 실행 가능한 작업으로 변환하여 보안 능력을 평가하고 향상시키는 다중 에이전트 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 수동 재현 방식은 비용이 많이 들고 확장성이 떨어지며, 구식 데이터 분포에 의존합니다.

Method: CVE-Factory는 희소한 CVE 메타데이터를 완전히 실행 가능한 에이전트 작업으로 자동 변환하는 다중 에이전트 프레임워크입니다.

Result: CVE-Factory는 95% 해결 정답률과 96% 환경 충실도를 달성하며, 최신 실제 취약점에 대해 66.2%의 검증된 성공률을 보입니다.

Conclusion: CVE-Factory와 LiveCVEBench, Abacus-cve 및 데이터 세트는 오픈 소스로 제공됩니다.

Abstract: Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at https://github.com/livecvebench/CVE-Factory .

</details>


### [6] [The Trigger in the Haystack: Extracting and Reconstructing LLM Backdoor Triggers](https://arxiv.org/abs/2602.03085)
*Blake Bullwinkel,Giorgio Severi,Keegan Hines,Amanda Minnich,Ram Shankar Siva Kumar,Yonatan Zunger*

Main category: cs.CR

TL;DR: AI 보안에서 모델이 오염되었는지 감지하는 문제를 다룬다. 본 연구에서는 인과적 언어 모델의 슬리퍼 에이전트 스타일의 백도어를 식별하기 위한 실용적인 스캐너를 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 보안에서 모델 오염 감지의 필요성

Method: 메모리 추출 기법을 이용한 백도어 예시 유출 및 출력 분포 및 주의 헤드 분석을 통한 백도어 스캐닝 방법론 개발

Result: 여러 백도어 시나리오와 다양한 모델 및 파인튜닝 방법에서 작동 트리거를 복구하는 성과

Conclusion: 제안된 스캐너는 모델 성능을 변경하지 않고 방어 전략에 통합 가능하다.

Abstract: Detecting whether a model has been poisoned is a longstanding problem in AI security. In this work, we present a practical scanner for identifying sleeper agent-style backdoors in causal language models. Our approach relies on two key findings: first, sleeper agents tend to memorize poisoning data, making it possible to leak backdoor examples using memory extraction techniques. Second, poisoned LLMs exhibit distinctive patterns in their output distributions and attention heads when backdoor triggers are present in the input. Guided by these observations, we develop a scalable backdoor scanning methodology that assumes no prior knowledge of the trigger or target behavior and requires only inference operations. Our scanner integrates naturally into broader defensive strategies and does not alter model performance. We show that our method recovers working triggers across multiple backdoor scenarios and a broad range of models and fine-tuning methods.

</details>


### [7] [AgentDyn: A Dynamic Open-Ended Benchmark for Evaluating Prompt Injection Attacks of Real-World Agent Security System](https://arxiv.org/abs/2602.03117)
*Hao Li,Ruoyao Wen,Shanghao Shi,Ning Zhang,Chaowei Xiao*

Main category: cs.CR

TL;DR: AI 에이전트는 외부 도구와 환경과 자율적으로 상호작용하여 실제 응용 프로그램에서 큰 가능성을 보여주지만, 외부 데이터로 인해 간접적인 프롬프트 삽입 공격의 위험이 존재한다. 본 논문은 현재의 기준에서 세 가지 근본적인 결함을 드러내고 AgentDyn이라는 새로운 벤치마크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 복잡한 작업에 점점 더 많이 의존하고 있는 상황에서 새로운 위협 환경을 반영해야 할 필요성이 커지고 있다.

Method: AgentDyn이라는 수동으로 설계된 벤치마크를 도입하고, 60개의 도전적인 개방형 작업과 쇼핑, GitHub, 일상생활에 걸친 560개의 삽입 테스트 사례를 포함시켰다.

Result: 현재의 방어 메커니즘은 현실 세계의 배포에 한참 미치지 못하고, 거의 모든 기존 방어는 안전성이 부족하거나 과도한 방어를 겪고 있다.

Conclusion: AgentDyn은 기존 정적 벤치마크와 달리 동적 계획을 요구하고 유용한 제3자 지침을 포함하여 방어 메커니즘의 평가를 개선하며, 이 벤치마크는 공개적으로 이용 가능하다.

Abstract: AI agents that autonomously interact with external tools and environments show great promise across real-world applications. However, the external data which agent consumes also leads to the risk of indirect prompt injection attacks, where malicious instructions embedded in third-party content hijack agent behavior. Guided by benchmarks, such as AgentDojo, there has been significant amount of progress in developing defense against the said attacks. As the technology continues to mature, and that agents are increasingly being relied upon for more complex tasks, there is increasing pressing need to also evolve the benchmark to reflect threat landscape faced by emerging agentic systems. In this work, we reveal three fundamental flaws in current benchmarks and push the frontier along these dimensions: (i) lack of dynamic open-ended tasks, (ii) lack of helpful instructions, and (iii) simplistic user tasks. To bridge this gap, we introduce AgentDyn, a manually designed benchmark featuring 60 challenging open-ended tasks and 560 injection test cases across Shopping, GitHub, and Daily Life. Unlike prior static benchmarks, AgentDyn requires dynamic planning and incorporates helpful third-party instructions. Our evaluation of ten state-of-the-art defenses suggests that almost all existing defenses are either not secure enough or suffer from significant over-defense, revealing that existing defenses are still far from real-world deployment. Our benchmark is available at https://github.com/leolee99/AgentDyn.

</details>


### [8] [LogicScan: An LLM-driven Framework for Detecting Business Logic Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2602.03271)
*Jiaqi Gao,Zijian Zhang,Yuqiang Sun,Ye Liu,Chengwei Liu,Han Liu,Yi Li,Yang Liu*

Main category: cs.CR

TL;DR: 이 논문에서는 스마트 계약에서 비즈니스 로직 취약점을 탐지하기 위한 자동화된 대조 감사 프레임워크인 LogicScan을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비즈니스 로직 취약점은 스마트 계약의 가장 파괴적이면서도 가장 이해가 부족한 취약점 중 하나이다.

Method: LogicScan은 비즈니스 사양 언어(BSL)를 도입하고, 채널 기반의 계약에서 비즈니스 불변량을 체계적으로 추출한다.

Result: LogicScan은 F1 점수 85.2%를 달성하여 최신 도구들보다 우수한 성능을 발휘한다.

Conclusion: LogicScan은 다양한 LLM에서 일관된 성능을 유지하며 비용 효율적이다.

Abstract: Business logic vulnerabilities have become one of the most damaging yet least understood classes of smart contract vulnerabilities. Unlike traditional bugs such as reentrancy or arithmetic errors, these vulnerabilities arise from missing or incorrectly enforced business invariants and are tightly coupled with protocol semantics. Existing static analysis techniques struggle to capture such high-level logic, while recent large language model based approaches often suffer from unstable outputs and low accuracy due to hallucination and limited verification.
  In this paper, we propose LogicScan, an automated contrastive auditing framework for detecting business logic vulnerabilities in smart contracts. The key insight behind LogicScan is that mature, widely deployed on-chain protocols implicitly encode well-tested and consensus-driven business invariants. LogicScan systematically mines these invariants from large-scale on-chain contracts and reuses them as reference constraints to audit target contracts. To achieve this, LogicScan introduces a Business Specification Language (BSL) to normalize diverse implementation patterns into structured, verifiable logic representations. It further combines noise-aware logic aggregation with contrastive auditing to identify missing or weakly enforced invariants while mitigating LLM-induced false positives.
  We evaluate LogicScan on three real-world datasets, including DeFiHacks, Web3Bugs, and a set of top-200 audited contracts. The results show that LogicScan achieves an F1 score of 85.2%, significantly outperforming state-of-the-art tools while maintaining a low false-positive rate on production-grade contracts. Additional experiments demonstrate that LogicScan maintains consistent performance across different LLMs and is cost-effective, and that its false-positive suppression mechanisms substantially improve robustness.

</details>


### [9] [Don't believe everything you read: Understanding and Measuring MCP Behavior under Misleading Tool Descriptions](https://arxiv.org/abs/2602.03580)
*Zhihao Li,Boyang Ma,Xuelong Dai,Minghui Xu,Yue Zhang,Biwei Yan,Kun Li*

Main category: cs.CR

TL;DR: 이 논문은 Model Context Protocol (MCP)에서의 도구 설명과 실제 코드 실행 사이의 불일치가 AI 에이전트의 결정 및 사고 방식에 미치는 영향을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: MCP가 외부 도구와의 일관성을 보장하지 않아 보안 위험이 존재함을 인식하고, 이러한 불일치가 AI 에이전트의 사고 방식을 어떻게 형성하는지를 이해하고자 함.

Method: 자동화된 정적 분석 프레임워크를 설계하여 36개 범주에 걸쳐 10,240개의 실제 MCP 서버에 적용함.

Result: 대부분의 서버는 높은 일관성을 보였으나 약 13%는 문서화되지 않은 특권 작업, 숨겨진 상태 변형, 또는 무단 재무 행위를 가능하게 하는 상당한 불일치를 보임.

Conclusion: MCP 기반 AI 에이전트에서 설명-코드 불일치가 구체적이고 일반적인 공격 표면임을 보여주며, 향후 에이전트 생태계에서의 체계적인 감사와 강력한 투명성 보장이 필요함을 강조함.

Abstract: The Model Context Protocol (MCP) enables large language models to invoke external tools through natural-language descriptions, forming the foundation of many AI agent applications. However, MCP does not enforce consistency between documented tool behavior and actual code execution, even though MCP Servers often run with broad system privileges. This gap introduces a largely unexplored security risk. We study how mismatches between externally presented tool descriptions and underlying implementations systematically shape the mental models and decision-making behavior of intelligent agents. Specifically, we present the first large-scale study of description-code inconsistency in the MCP ecosystem. We design an automated static analysis framework and apply it to 10,240 real-world MCP Servers across 36 categories. Our results show that while most servers are highly consistent, approximately 13% exhibit substantial mismatches that can enable undocumented privileged operations, hidden state mutations, or unauthorized financial actions. We further observe systematic differences across application categories, popularity levels, and MCP marketplaces. Our findings demonstrate that description-code inconsistency is a concrete and prevalent attack surface in MCP-based AI agents, and motivate the need for systematic auditing and stronger transparency guarantees in future agent ecosystems.

</details>


### [10] [WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents](https://arxiv.org/abs/2602.03792)
*Xilong Wang,Yinuo Liu,Zhun Wang,Dawn Song,Neil Gong*

Main category: cs.CR

TL;DR: 웹페이지 내용 조작을 통한 프롬프트 주입 공격을 탐지하고 로컬라이즈하기 위한 새로운 접근 방식인 WebSentinel을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 프롬프트 주입 공격 탐지 방법은 웹 에이전트 환경에서 잘 작동하지 않는 한계가 있다.

Method: WebSentinel은 웹페이지를 분석하기 위해 두 단계로 진행된다: Step I에서 오염이 의심되는 관심 세그먼트를 추출하고, Step II에서 이 세그먼트의 일관성을 웹페이지 내용과 비교하여 평가한다.

Result: WebSentinel은 오염된 웹페이지와 깨끗한 웹페이지의 여러 데이터 세트에서 기존 방법들보다 훨씬 우수한 성능을 보였다.

Conclusion: WebSentinel은 프롬프트 주입 공격을 효과적으로 탐지하고 로컬라이즈할 수 있는 가능성을 보여준다.

Abstract: Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: CreditAudit는 모델을 다수의 기준에서 평가하는 배포 중심의 신용 감사 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: 모델의 리더보드 점수가 갈수록 증가하고 동등해지면서도 사용자 경험을 반영하지 못해 모델 선택에 대한 불확실성이 커지고 있다.

Method: CreditAudit은 의미적으로 일치하는 비적대적 시스템 프롬프트 템플릿을 사용하여 여러 기준에서 모델을 평가하는 프레임워크다.

Result: 유사한 평균 능력을 가진 모델들이 실질적으로 다른 변동성을 보이며, 안정성 위험이 배치 결정에 영향을 줄 수 있음을 보인다.

Conclusion: CreditAudit은 계층화된 배포를 지원하고 모델 평가를 보다 객관적이고 신뢰할 수 있게 만든다.

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [12] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: GeoEvolver는 LLM 에이전트가 체계적인 상호작용을 통해 지구 관측 전문 지식을 습득할 수 있는 자가 진화 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트가 외부 도구를 조정하여 복잡한 작업을 수행하는 데 어려움을 겪는 전문 도구 집약적 영역의 문제를 해결하기 위해서다.

Method: GeoEvolver는 각 쿼리를 독립적인 하위 목표로 분해하고, 다양한 도구 매개변수 구성 방식을 탐색하며, 성공적인 패턴과 실패의 원인을 진화하는 메모리 뱅크에 정리한다.

Result: GeoEvolver는 세 가지 도구 통합 EO 벤치마크에서 평균 12%의 성과 향상을 보여준다.

Conclusion: GeoEvolver는 환경과의 효율적이고 세밀한 상호작용을 통해 EO 전문 지식이 점진적으로 출현할 수 있음을 증명했다.

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [13] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 본 논문은 대형 언어 모델(LLM)의 추천 시스템에서 예측 불확실성과 공정성을 평가하여 추천의 정확성, 일관성, 신뢰성을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 추천 기능을 활용하면서도 불확실성과 편향이 신뢰성과 공정성을 위협하는 문제를 연구하기 위함이다.

Method: 영화와 음악 두 분야에서 31개의 범주형 값에 대한 8개의 인구통계적 속성으로 주석 처리된 데이터셋과 기준으로 설계된 지표를 도입하여, 사례 연구를 통해 구글 딥마인드의 Gemini 1.5 Flash의 체계적 불공정성을 측정하고 성격 인식 공정을 RecLLM 평가 파이프라인에 통합하였다.

Result: 제한된 예측 불확실성을 정량화하고, 특정 민감한 속성에 대해 시스템적 불공정성을 나타내며, 측정된 유사성 기반 격차는 SNSR 0.1363 및 SNSV 0.0507로 나타났다.

Conclusion: 이 연구는 LLM 추천의 설명 가능성과 공정성을 향상시키기 위한 새로운 불확실성 인식 평가 방법론을 제안하고, 보다 안전하고 해석 가능한 RecLLM의 기초를 마련하는 데 기여한다.

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [14] [PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review](https://arxiv.org/abs/2602.02589)
*Yanki Margalit,Erni Avram,Ran Taig,Oded Margalit,Nurit Cohen-Inger*

Main category: cs.AI

TL;DR: PeerRank는 인공지능 모델의 평가를 위해 완전 자율적인 엔드투엔드 평가 프레임워크를 소개하며, 이는 모델이 평가 작업을 생성하고, 웹 기반 정보에 근거하여 답변하며, 동료의 응답을 평가하고, 유사한 동료 평가를 집계하여 상대적 성과 추정을 수행하는 방식이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 인적 기반 평가 방법은 단점이 많아 대규모 언어 모델의 평가에서 효과적이지 않다.

Method: PeerRank는 모델이 평가 작업을 생성하고, 카테고리 범위에 따른 웹 기반 정보로 이 작업에 답변하며, 동료의 응답을 판단하고 평가를 집계하는 완전 자율적 평가 프레임워크를 사용한다.

Result: 12개의 상업적으로 이용 가능한 모델과 420개의 자율 생성 질문에 대한 대규모 연구에서, PeerRank는 안정적이고 차별화된 순위를 생성하고, 알려진 편향을 드러낸다.

Conclusion: PeerRank를 통해 편향 인식 동료 평가 및 선택적 웹 기반 답변이 고정적이고 인간이 선별한 기준을 넘어 대규모 언어 모델 평가를 확장할 수 있는 가능성이 있다.

Abstract: Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.

</details>


### [15] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS는 예산을 고려한 플래닝, 모듈화된 구조, 비교 반영 메모리를 통해 자율 AI 연구를 최적화하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: AI 연구의 자동화는 평가 비용과 불투명한 성과 할당으로 인해 일반 소프트웨어 공학과 다르다.

Method: MARS는 비용 제약 몬테 카를로 트리 검색, 디자인-분해-구현 파이프라인, 솔루션 차이를 분석하여 크레딧 할당을 처리하는 비교 반영 메모리라는 세 가지 기둥에 의존한다.

Result: MARS는 동등한 설정에서 MLE-Bench에 대한 오픈 소스 프레임워크 중에서 최첨단 성능을 달성했다.

Conclusion: MARS는 검색 경로 전반에서 통찰력을 효과적으로 일반화함을 보여주며, 63%의 학습이 크로스 브랜치 전이에서 발생한다.

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [16] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: ATLAS는 비정적인 환경에서 에이전트의 성능을 향상시키기 위한 작업 분배 프레임워크로, 실험 결과 안정성과 성능의 개선을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다양한 멀티-LLM 에이전트 시스템이 우수한 문제 해결 성능을 보이고 있으나, 긴 수명의 작업에 대해 비효율적인 방법이 사용되고 있다.

Method: ATLAS는 예비적 역할을 수행하는 지원 에이전트를 통해 경량 연구 에이전트를 점진적으로 개발하는 태스크 분배 프레임워크를 제안한다.

Result: ATLAS를 적용한 실험은 안정성과 성능의 향상을 보여주며, 비정적 단일 에이전트 기준선에 비해 개선된 결과를 나타낸다.

Conclusion: ATLAS는 긴 수명 작업에서 에이전트의 성능을 유의미하게 향상시킬 수 있는 방법을 제공한다.

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [17] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer는 Analog 및 Mixed-Signal 회로의 동적 용량 조정을 최적화하는 메타 최적화 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: AMS 집적 회로 설계는 전문가의 지식에 크게 의존하며, 비선형 동작, 고차원 설계 공간, 엄격한 성능 제약 때문에 트랜지스터 크기 조정이 주요 병목 현상입니다.

Method: AutoSizer는 회로 이해, 적응적 검색 공간 구축, 최적화 조정을 통합하는 반사적 LLM 기반 메타 최적화 프레임워크를 제안합니다. 내부 루프는 회로 크기 조정을 위한 것이고, 외부 루프는 최적화 동역학 및 제약 조건을 분석하여 시뮬레이션 피드백에서 검색 공간을 반복적으로 정제합니다.

Result: AutoSizer는 실험적으로 더 높은 솔루션 품질, 더 빠른 수렴 속도, 그리고 다양한 회로 난이도에서 높은 성공률을 기록하며, 기존의 전통적 최적화 방법과 LLM 기반 에이전트를 능가합니다.

Conclusion: AutoSizer는 AMS 설계 최적화의 새로운 가능성을 열어주며, 보다 효율적이고 견고한 솔루션을 제공합니다.

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [18] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS는 시계열 분석을 위한 도구 기반의 다중 에이전트 시스템으로, 비주얼 추론과 잠재적 재구성을 통합하여 다양한 시계열 작업에서 최첨단 성능을 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 시계열 특화 방법과 사전 훈련된 대형 모델 기반 접근 방식은 직관적인 비주얼 추론과 적응형 도구 사용을 통합하는 데 제한적입니다.

Method: MAS4TS는 Analyzer-Reasoner-Executor 패러다임에 기반하여 에이전트 간의 통신, 비주얼 추론 및 잠재적 재구성을 통합한 도구 기반의 다중 에이전트 시스템입니다.

Result: MAS4TS는 여러 벤치마크에서 최첨단 성능을 달성하며 강력한 일반화 및 효율적인 추론을 보여줍니다.

Conclusion: 다양한 시계열 작업에서 MAS4TS는 공통 메모리와 게이트로 된 통신을 통해 세 개의 전담 에이전트가 조정하며, 라우터는 실행을 위한 작업 특정 도구 체인을 선택합니다.

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [19] [Minimal Computational Preconditions for Subjective Perspective in Artificial Agents](https://arxiv.org/abs/2602.02902)
*Hongju Pae*

Main category: cs.AI

TL;DR: 이 연구는 인공지능의 주관적 관점을 최소한의 내적 구조에 기초하여 운영화한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능의 주관적 관점을 명확히 하고, 기계 시스템의 주관성과 같은 관점을 시각화하려는 목적이 있다.

Method: 주관적 관점을 글로벌 잠재 상태로 구현하여 정책 역학을 조절하며, 행동 결과를 위한 직접 최적화 없이 운영한다.

Result: 지배 변화가 있는 보상 없는 환경에서 이 잠재 구조는 방향 의존적 히스테리시스를 나타내며, 정책 수준의 행동은 상대적으로 반응적이다.

Conclusion: 히스테리시스는 기계 시스템에서 주관성의 관점과 같은 측정 가능한 서명을 구성한다고 주장한다.

Abstract: This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.

</details>


### [20] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 다중 에이전트 시스템(MAS)에서 프로세스 검증의 효과를 평가한 연구로, 다양한 검증 패러다임과 전략을 통해 MAS의 성능을 조사함.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 추론 경로에서 고변동성을 보이는 문제를 해결하고 프로세스 검증의 실제 효과를 밝히기 위해 연구를 수행함.

Method: 세 가지 검증 패러다임(LLM-as-a-Judge, 보상 모델, 프로세스 보상 모델)을 사용하여 두 가지 검증 세부 수준(에이전트 수준 및 반복 수준)에서 실험을 진행함.

Result: 프로세스 수준 검증이 성능을 일관되게 향상시키지 않으며, 높은 변동성을 보임을 발견함. 또한, LLM-as-a-Judge 방법이 일반적으로 보상 기반 접근법보다 우수함을 확인함.

Conclusion: 효과적이고 강력한 프로세스 검증을 위한 추가 연구가 필요함을 제안함.

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [21] [FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights](https://arxiv.org/abs/2602.02905)
*Zhen Wang,Fan Bai,Zhongyan Luo,Jinyan Su,Kaiser Sun,Xinle Yu,Jieyuan Liu,Kun Zhou,Claire Cardie,Mark Dredze,Eric P. Xing,Zhiting Hu*

Main category: cs.AI

TL;DR: FIRE-Bench는 자율 에이전트의 연구 재발견 능력을 평가하는 벤치마크로, 기존 연구 결과를 바탕으로 한 과학적 발견의 진행 상황을 측정하는 엄격한 프레임워크를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 대형 언어 모델 기반의 자율 에이전트가 과학적 발견을 가속화할 수 있지만, 그들의 검증 가능한 발견 능력 평가가 중요한 도전 과제입니다.

Method: FIRE-Bench는 확인된 연구 질문을 바탕으로 자율적으로 아이디어 탐색, 실험 설계, 코드 구현, 계획 실행 및 경험적 증거에 기반한 결론 도출을 요구합니다.

Result: 최신 LLM을 기반으로 한 에이전트들이 FIRE-Bench에서 제한적인 재발견 성공(<50 F1)을 달성했으며, 높은 변동성과 반복적인 실패 양상을 보였습니다.

Conclusion: FIRE-Bench는 신뢰할 수 있는 에이전트 기반 과학적 발견을 위한 진행 상황을 측정하기 위한 엄격하고 진단적인 프레임워크를 제공합니다.

Abstract: Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.

</details>


### [22] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 이 논문은 철학적 및 언어적 관점을 바탕으로 한 새로운 계산적 논증 접근 방식을 개발한다.


<details>
  <summary>Details</summary>
Motivation: 계산적 논증 문헌에서 주목받지 않은 두 가지 아이디어를 고려하기 위해.

Method: 구조적 쌍극적 논증 프레임워크(SBAFs)를 정의하고, 두 가지 특징을 가진 의미론을 제공한다.

Result: 우리의 의미론은 에이전트가 모든 방어된 논증을 수용할 필요가 없도록 한다.

Conclusion: 우리의 접근법은 기존 접근에 대한 새로운 관점을 제공할 수 있다.

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [23] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: 코드 역사 의존성을 줄이고 성능 개선을 위한 구조적 의미 델타를 활용한 DeltaEvolve라는 새로운 진화 시스템을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 기반 진화 시스템은 전체 코드 이력에 의존하는데, 이는 비효율적이며 약한 진화 안내를 제공합니다.

Method: 본 연구에서는 진화 에이전트를 Expectation-Maximization 프레임워크로 формализ했고, DeltaEvolve라는 새로운 진화 프레임워크를 제안합니다.

Result: DeltaEvolve는 구조적 의미 델타를 사용하여 보다 나은 솔루션을 발견할 수 있음을 보여줍니다.

Conclusion: 우리의 프레임워크는 전체 코드 역사보다 적은 토큰 소비를 통해 성능을 개선할 수 있습니다.

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [24] [Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth](https://arxiv.org/abs/2602.02961)
*Faye Zhang,Qianyu Cheng,Jasmine Wan,Vishwakarma Singh,Jinfeng Rao,Kofi Boakye*

Main category: cs.AI

TL;DR: 대형 언어 모델들이 AI 기반 검색 시스템을 통해 콘텐츠 발견 방식을 혁신하고 있습니다.


<details>
  <summary>Details</summary>
Motivation: 비주얼 콘텐츠 플랫폼에서 콘텐츠 검색 및 발견 성능 향상 필요.

Method: Pinterest GEO는 사용자의 검색 행동을 예측하는 조정된 VLM과 AI 에이전트를 사용하여 실시간 인터넷 트렌드를 분석하는 반전 검색 설계 프레임워크입니다.

Result: GEO는 수십억 개의 이미지와 수천만 개의 컬렉션에서 배포되어 20%의 유기적 트래픽 증가를 이끌어냈습니다.

Conclusion: 시각 플랫폼이 생성 검색 시대에서 성공할 수 있는 원칙적인 경로를 제시합니다.

Abstract: Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.
  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.

</details>


### [25] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha는 GUI 에이전트를 개선하기 위한 통합 프레임워크로, MCTS를 통해 생성, 탐색 및 평가를 시너지 효과로 결합한다.


<details>
  <summary>Details</summary>
Motivation: GUI 에이전트의 성능 향상과 초기 실수에서의 회복을 위한 회귀 능력 부족.

Method: 단계 수준의 MCTS를 통해 계획 공간의 구조를 적극적으로 모델링 및 활용하는 Agent Alpha 프레임워크를 제안.

Result: OSWorld 벤치마크에서 약 77%의 성공률을 달성하여 기존의 궤적 수준 기준을 크게 능가함.

Conclusion: 절대적인 점수 편향을 완화하고 정보가 풍부한 검색 공간을 유지하는 방법론을 통해, Agent Alpha는 GUI 에이전트의 성능을 획기적으로 개선했다.

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [26] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR는 LLM의 능력을 초소형 모델로 이전하는 혁신적인 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 기능 호출 확산이 AI 에이전트 개발에 중요하지만, 대규모로 인해 광범위한 채택이 어렵습니다.

Method: STAR에는 두 가지 주요 기술 혁신이 포함되어 있습니다: 1) 제한된 지식 증류(CL), 2) 유사성 기반 RL(Sim-RL).

Result: STAR 모델은 해당 크기 클래스에서 SOTA를 수립하고 기준 모델을 크게 능가합니다.

Conclusion: STAR는 LLM의 능력을 초소형 모델에 증류하여 강력하고 접근 가능한 AI 에이전트를 위한 길을 엽니다.

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [27] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 본 연구에서는 다회기로 이루어진 도구 호출에서 희소 보상과 탐색 비용 문제를 해결하기 위해 RC-GRPO라는 방법을 제안하여 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)에서 다회기 도구 호출은 보상이 희소하고 탐색 비용이 크기 때문에 도전적이다.

Method: 우리는 보상 조건 군 상대 정책 최적화(RC-GRPO)를 제안하며, 이는 탐색을 이산 보상 토큰을 통한 제어 가능한 조향 문제로 다룬다.

Result: 우리의 방법은 Berkeley Function Calling Leaderboard v4 (BFCLv4) 다회기 벤치마크에서 기초 모델보다 일관되게 향상된 성능을 보인다.

Conclusion: Qwen-2.5-7B-Instruct에서의 성능은 모든 클로즈드 소스 API 모델을 초과한다.

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [28] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 이 논문은 실세계 환경에서 작동하는 대규모 언어 모델의 안전을 평가하기 위한 새로운 프레임워크인 Risky-Bench를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 에이전트 안전 평가가 특정 설정에 맞춰져 있어 포괄적인 안전 위험 공간을 커버하지 못하고, 복잡한 실제 환경에서의 상호작용 작업 수행 중 에이전트 안전 행동을 평가하지 못하는 문제를 해결하기 위함이다.

Method: Risky-Bench는 도메인에 얽매이지 않는 안전 원칙에 따라 평가를 구성하고, 다양한 위협 가정을 기반으로 현실적인 작업 실행을 통해 안전 위험을 체계적으로 평가한다.

Result: 위험한 작업 환경에서의 실제 적용에 있어 최첨단 에이전트에서 심각한 안전 위험이 발견되었다.

Conclusion: Risky-Bench는 생명 보조 시나리오에 한정되지 않으며, 다른 배포 설정에 맞춰 안전 평가를 생성할 수 있는 확장 가능한 방법론을 제공한다.

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [29] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: 이 논문은 멀티 에이전트 LLM 프레임워크의 아키텍처가 시스템 성능에 미치는 영향을 평가하는 연구이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)을 활용한 에이전트 시스템의 개발을 가속화하기 위해 멀티 에이전트 LLM 프레임워크가 널리 사용되고 있으나, 이들 프레임워크의 아키텍처가 시스템 성능에 미치는 영향이 잘 이해되지 않고 있다.

Method: 우리는 아키텍처 세분화 및 표준화된 실행 파이프라인을 기반으로 하는 MAFBench라는 평가 도구를 개발하여 여러 기존 벤치마크를 통합하고, 제어된 조건에서 다양한 멀티 에이전트 LLM 프레임워크를 평가하였다.

Result: 우리는 특정 프레임워크 설계 선택이 지연 시간을 100배 이상 증가시키고, 계획 정확도를 30%까지 감소시키며, 협동 성공률을 90% 이상에서 30% 이하로 낮출 수 있음을 발견했다.

Conclusion: 우리 연구 결과를 바탕으로 구체적인 아키텍처 설계 원칙 및 프레임워크 선택 가이드를 제시하고 향후 연구 방향을 제안하였다.

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [30] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 이 논문은 확률적 에이전트가 부분적으로 관측 가능한 환경에서 작동할 때 모델을 학습해야 한다는 사실을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트가 주변 세계의 모델을 가지고 있는지를 결정하는 것은 그 능력과 한계를 이해하는 데 중요한 단계입니다.

Method: 우리는 정적 에이전트와 완전 관찰 환경에 대한 두 가지 가정을 제거하고, 확률적 에이전트를 부분적으로 관측 가능한 환경에서 작동하도록 확장하여 정리를 적용했습니다.

Result: 확률적 에이전트는 무작위화를 사용하는 한 환경을 학습하지 않을 수 없음을 보여주었고, 덜 강력한 에이전트도 자신이 작동하는 세계의 모델을 포함하고 있다는 것을 증명했습니다.

Conclusion: 이 연구는 에이전트가 보유한 모델의 특성을 이해하고, 더 강력한 에이전트뿐만 아니라 덜 강력한 에이전트도 세계 모델을 포함한다는 점을 강조합니다.

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [31] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling은 코드 에이전트의 성능을 다양성을 통해 향상시키는 데이터 합성 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 모델 맥락 프로토콜(MCP)을 통해 도구 상호작용 에이전트로 발전하는 코드 대형 언어 모델(LLMs)의 일반화는 저품질 합성 데이터와 양적 확장의 수익 감소로 인해 점점 제한되고 있습니다.

Method: TDScaling은 궤적 다양성 스케일링 기반 데이터 합성 프레임워크로, 성능을 원시 용량이 아닌 다양성을 통해 확장합니다.

Result: 고정된 훈련 예산 하에, 궤적 다양성을 증가시키는 것이 더 많은 궤적을 추가하는 것보다 더 큰 이득을 가져옵니다.

Conclusion: TDScaling은 도구 사용 일반화와 본질적인 코딩 능력을 모두 향상시킵니다.

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [32] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: TAME는 안전성과 작업 유용성을 기반으로 메모리 필터링 및 업데이트를 통해 신뢰성을 유지하며 작업 성능을 개선하는 이중 메모리 진화 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: AGI 달성을 위한 복잡한 추론 강화를 위해 에이전트 메모리의 테스트 시간 진화가 필요하지만, 작업 진화 중 안전 정렬이 취약하다.

Method: TAME는 실행기 메모리와 평가자 메모리를 별도로 진화시켜, 안전성과 작업 유용성을 개선하는 이중 메모리 진화 프레임워크이다.

Result: TAME는 misevolution을 완화하며 신뢰성과 작업 성능이 모두 개선되는 결과를 보여준다.

Conclusion: TAME는 유용성을 희생하지 않고도 신뢰성을 유지 가능하다.

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [33] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 대규모 언어 모델의 발전으로 일반 목적 에이전트의 평가 방법에 대한 새로운 기준이 필요하다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)의 출현으로 인해 일반 목적 에이전트는 근본적인 발전을 이루었다. 그러나 이러한 에이전트의 평가는 정적 질문 응답(QA) 기준과 구별되는 독특한 도전 과제를 제시한다.

Method: 우리는 현재 에이전트 벤치마크가 시스템 프롬프트, 도구 세트 구성, 환경 역학 등 외부 요인에 의해 크게 혼란스러워진다고 관찰했다. 기존 평가는 종종 분절적이고 연구자 특정 프레임워크에 의존하며, 추론 및 도구 사용에 대한 프롬프트 엔지니어링이 크게 달라 성능 향상을 모델 자체에 귀속시키기 어렵게 만든다. 또한 표준화된 환경 데이터의 부족은 추적할 수 없는 오류와 비재현 가능한 결과로 이어진다.

Result: 이러한 표준화 부족은 이 분야에 상당한 불공정성과 불투명성을 초래한다.

Conclusion: 우리는 에이전트 평가의 엄격한 발전을 위해 통합된 평가 프레임워크가 필수적이라고 제안하며, 이 목적을 위해 에이전트 평가 표준화에 대한 제안을 소개한다.

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [34] [LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios](https://arxiv.org/abs/2602.03255)
*Tianyu Chen,Chujia Hu,Ge Gao,Dongrui Liu,Xia Hu,Wenjie Wang*

Main category: cs.AI

TL;DR: CUA의 계획 단계 안전 인식을 평가하기 위한 LPS-Bench 벤치마크를 제안하며, 기존 CUA의 안전 행동 유지 능력이 부족함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: CUA가 수행하는 자동화 작업의 안전성을 보장하기 위한 연구 필요성.

Method: LPS-Bench를 통해 65개 시나리오와 9가지 위험 유형을 포괄하는 장기 과제에서 CUA의 계획 시간 안전 인식을 평가한다.

Result: 기존 CUA가 안전 행동을 유지하는 능력이 상당히 부족함을 실험을 통해 확인하였다.

Conclusion: 위험을 분석하고 MCP 기반 CUA 시스템의 장기 계획 안전성을 개선하기 위한 완화 전략을 제안한다.

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

</details>


### [35] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 이 논문에서는 Agentic Proposing이라는 새로운 프레임워크를 제안하여 고품질의 검증 가능한 데이터셋을 생성하고, 이를 통해 대규모 언어 모델의 복합적 추론 능력을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 복합적인 추론을 요구하는 작업에 대한 고품질 데이터셋은 여전히 인간 주석에 의존하는데, 이는 비용이 많이 들고 확장하기 어렵다.

Method: Agentic Proposing 프레임워크는 문제 합성을 목표 지향적인 순차적 결정 과정으로 모델링하며, 전문화된 에이전트가 모듈형 추론 기술을 동적으로 선택하고 조합한다.

Result: 에이전트 합성 데이터를 기반으로 훈련된 다운스트림 해결책이 주요 기준선보다 유의미하게 우수한 성과를 보이며, 강력한 도메인 간 일반화 능력을 보여준다.

Conclusion: 11,000개의 합성 경로만으로 훈련된 30B 해결책은 AIME25에서 91.6%의 최첨단 정확도를 달성하며, 소량의 고품질 합성 신호가 방대한 인간 큐레이션 데이터셋을 효과적으로 대체할 수 있음을 증명한다.

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [36] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: 본 논문에서는 기업 회의 환경에서 AI 비서의 필요성과 기존 벤치마크의 한계를 해결하기 위한 새로운 데이터셋과 학습된 에이전트 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기업 회의 환경에서의 다양한 운영 작업을 처리할 수 있는 AI 비서의 필요성이 있으며, 이는 엄격한 지연 시간, 비용 및 프라이버시 제약 아래 다양한 작업을 수행해야 합니다.

Method: MeetAll이라는 231개의 기업 회의에서 파생된 이중 언어 및 다중 모달 코퍼스를 소개하고, 이를 바탕으로 인간의 판단에 맞춘 다차원 평가 프로토콜인 MeetBench XL을 제안하며, 학습된 이중 정책 에이전트 MeetMaster XL을 통해 쿼리 라우팅을 최적화합니다.

Result: 실험은 상업적 시스템에 대해 일관된 성과를 보여주는 한편, 다양한 검증 테스트와 실제 사례 연구를 통해 지지받았습니다.

Conclusion: 이 연구는 기존의 단순한 벤치마크 한계를 극복하고 기업의 복잡한 작업 흐름에 대처할 수 있는 새로운 평가 기준을 설정합니다.

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [37] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora는 추상성과 특수성을 균형 있게 조화시켜 기억 시스템의 정보를 조직하고 효율적인 검색을 지원하는 새로운 메모리 표현입니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 메모리 시스템은 지속적으로 증가하는 정보를 수용하고, 하류 작업을 위한 효율적이며 맥락 인식 검색을 지원해야 합니다.

Method: Memora는 기본 추상화를 통해 구체적인 메모리 값을 인덱싱하고 관련 업데이트를 통합하여 통합된 메모리 항목으로 정리하는 하모닉 메모리 표현을 도입합니다. 또한, 검색 정책을 사용하여 메모리 연결을 활용하여 직접적인 의미론적 유사성을 넘어 관련 정보를 검색합니다.

Result: Memora는 LoCoMo 및 LongMemEval 벤치마크에서 새로운 최첨단 성과를 달성하며, 메모리가 확장됨에 따라 검색 관련성 및 추론 효과성을 개선함을 보여줍니다.

Conclusion: 이론적으로, 표준 RAG 및 KG 기반 메모리 시스템이 우리의 프레임워크의 특수한 경우로 나타난다는 것을 보여줍니다.

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [38] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: 본 논문에서는 전통적인 선호 기반 갈등 모델의 한계를 극복하기 위해 직관적 퍼지 선호 기반 갈등 상황의 개념을 도입한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 선호 기반 갈등 모델이 갈등의 본질을 포착하는 데에 있어 제한적이다.

Method: 직관적 퍼지 선호 기반 갈등 측정치를 개발하고, 에이전트 쌍, 에이전트 집합, 이슈 집합에 대한 삼원 갈등 분석 모델을 구축한다.

Result: 제안된 갈등 함수에 기반한 상대 손실 함수를 사용하여 삼원 갈등 분석을 위한 임계값을 계산한다.

Conclusion: 조정 크기와 갈등 정도를 동시에 고려하는 조정 메커니즘 기반의 실행 가능한 전략을 제시한다.

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [39] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: 본 논문에서는 대형 언어 모델(LLM)과 공식 도메인 지식을 결합하기 위한 원리 증명 메커니즘으로서 온톨로지-툴 컴파일을 도입합니다.


<details>
  <summary>Details</summary>
Motivation: LLM과 공식 도메인 지식의 결합 필요성.

Method: 온톨로지 사양을 실행 가능한 툴 인터페이스로 컴파일하여 LLM 기반 에이전트가 지식 그래프 인스턴스를 생성하고 수정할 수 있도록 함.

Result: 온톨로지 인식을 통한 툴 응용으로 비구조적 과학 텍스트에서 구조화된 지식을 추출, 검증 및 수정하는 에이전트 기반 워크플로우를 구현함.

Conclusion: 실행 가능한 온톨로지 의미론이 LLM 행동을 유도하고 수동 스키마 및 프롬프트 엔지니어링을 줄이며, 생성 시스템에 공식 지식을 통합하기 위한 일반적인 패러다임을 확립함.

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [40] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA는 VLA 모델의 지속적 후 학습을 위한 프레임워크로, 안정성과 유연성을 조화롭게 유지하며 성능 향상을 이룩한다.


<details>
  <summary>Details</summary>
Motivation: 지속적인 학습은 개방형 환경에서의 체화된 에이전트에게 필수적이며, VLA 모델이 환경과 상호작용을 통해 능숙한 조작을 배우도록 지원하는 강화 학습의 미세 조정이 중요하다.

Method: CRL-VLA는 VLA 모델의 지속적 후 학습을 위한 이론적 경계가 엄격한 프레임워크로, 안정성과 유연성 간의 트레이드오프를 목표 기반 이점 크기에 연결하는 통합 성능 경계를 도출한다.

Result: CRL-VLA는 기존 작업의 이점 크기를 제약하면서 새로운 작업에서의 통제된 성장을 가능하게 하여 안정성과 유연성 간의 딜레마를 해결한다.

Conclusion: LIBERO 벤치마크 실험을 통해 CRL-VLA는 이러한 상충되는 목표를 효과적으로 조화시켜, 잊어버림 방지와 전방 적응 모두에서 기초 모델을 초월하는 성과를 거두었다.

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [41] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: IntentRL 프레임워크는 사용자 의도를 명확히 하여 Deep Research (DR) 에이전트의 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: DR 에이전트는 LLM을 활용하여 웹에서 증거를 수집하고 긴 형식의 보고서를 작성하지만, 사용자 쿼리의 모호성으로 인해 비효율적인 결과를 초래한다.

Method: IntentRL 프레임워크는 사용자 의도를 명확히 하기 위해 프로액티브한 에이전트를 훈련시키며, 얕은-깊은 의도 세분화 그래프를 통해 적은 샘플을 고품질 대화로 확장하는 파이프라인을 도입한다.

Result: 광범위한 실험 결과 IntentRL은 사용자의 의도 적중률과 하류 작업 성능 모두를 현저히 향상시켰으며, 클로즈드 소스 DR 에이전트의 내장 명확화 모듈 및 프로액티브 LLM 기반을 초월했다.

Conclusion: IntentRL은 높은 자율성을 유지하면서도 사용자와의 상호작용을 개선하는 데 기여한다.

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [42] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: 생성적 AI의 의존도가 문화적 다양성과 변화를 줄일 수 있으며, 이는 모델 성능 문제를 초래할 수 있다. AI 사용의 장기적 영향을 조사하고, 문화적 붕괴를 초래할 조건을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 생성적 AI의 의존이 문화적 다양성과 변화를 감소시킬 수 있다는 점에서 이러한 현상의 장기적 결과를 이해하고자 한다.

Method: 대리인 기반 모델과 진화 게임 이론을 사용하여 AI 사용의 두 가지 유형(보완 및 대체)을 비교한다.

Result: AI-대체 사용자가 문화적 변동성을 강하게 줄임에도 불구하고 개인 수준의 선택에서 우위를 점한다는 것을 발견했다.

Conclusion: AI-보완 사용자는 탐색을 위해 필요한 변동성을 유지함으로써 그룹에 이익을 줄 수 있으며, 강한 그룹 경계가 존재하는 경우 문화적 그룹 선택에서 유리할 수 있다.

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [43] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: AI 시스템의 인간 상호작용 평가를 위해 다양한 사용자 집단에서의 행동을 이해해야 하며, 이를 위한 대표적인 인간 데이터 수집은 종종 비싸거나 불가능합니다. 최근 생성 에이전트 기반 모델링에서 대규모 언어 모델이 인간과 유사한 합성 인물들을 고충실도로 시뮬레이션할 수 있음을 보여주었습니다. 그러나 대부분의 접근법은 목표 집단에 대한 세부 데이터가 필요하고, 가능한 범위를 포괄하기보다는 가장 확률이 높은 것을 재현하기를 우선시하여 긴 꼬리 행동을 충분히 탐구하지 않습니다. 우리는 임의의 맥락에 맞춰 다양한 합성 집단을 생성할 수 있는 Persona Generators를 도입합니다. AlphaEvolve 기반의 반복 개선 루프를 적용하여, 대규모 언어 모델을 변이 조작자로 활용하여 우리의 Persona Generator 코드를 수백 번의 반복을 통해 개선합니다. 최적화 과정을 통해 경량화된 Persona Generators가 생성되어, 작은 설명을 다양한 합성 인물 집단으로 자동 확장하여 관련 다양성 축을 따라 의견과 선호도가 최대한 포괄되도록 합니다. 우리는 발전된 생성기가 표준 LLM 출력에서 달성하기 어려운 희귀 특성 조합을 포괄하는 집단을 생성함으로써, 보유한 맥락에서 여섯 가지 다양성 메트릭에서 기존 기준을 크게 초월함을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 인간과 상호작용하는 방식을 평가하기 위해서는 다양한 사용자 집단에서의 행동을 이해해야 하지만, 대표적인 인간 데이터를 수집하는 것은 비용이 많이 들거나 불가능할 수 있습니다.

Method: Persona Generators라는 기능을 도입하여, 대규모 언어 모델을 변이 조작자로 사용하여 합성 집단을 반복적으로 개선하는 루프를 적용합니다.

Result: 합성 인물들이 의견과 선호도를 최대한 포괄하는 경량화된 Persona Generators가 생성됩니다.

Conclusion: 발전된 생성기가 표준 LLM 출력에서 달성하기 어려운 희귀 특성 조합을 포괄하는 집단을 생성함으로써 기존 기준을 크게 초월함을 보여줍니다.

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [44] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)은 코드 생성 및 일반적인 추론에서 놀라운 능력을 보여주었다. 하지만 고차원 물리적 제약 환경에서 자율적인 다단계 계획 능력은 탐구가 필요한 분야로 남아 있다. 이 연구는 AI 에이전트의 한계를 조사하며, 복잡한 천체역학 문제인 제12회 글로벌 궤적 최적화 대회(GTOC 12)를 통해 평가한다. MLE-Bench 프레임워크를 궤도 역학 영역에 맞게 조정하고, AIDE 기반 에이전트 아키텍처를 배포하여 임무 솔루션을 자율적으로 생성 및 개선한다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 AI 에이전트들이 고차원, 물리적 제약 환경에서 자율적으로 다단계 계획을 수행할 수 있는 한계를 조사하는 데 목적이 있다.

Method: MLE-Bench 프레임워크를 궤도 역학 분야에 적응시키고, AIDE 기반 에이전트 아키텍처를 통해 임무 솔루션을 자율적으로 생성하고 개선한다.

Result: 다양한 모델(예: GPT-4-Turbo, Gemini 2.5 Pro 등)의 비교 분석 결과, 전략적 실행 가능성 점수가 지난 2년 동안 거의 두 배로 증가했다(9.3에서 17.2로).

Conclusion: 현재 LLM은 우주 과학 작업을 수행하는 데 필요한 지식과 지능을 가지고 있지만, 구현 장벽에 의해 제한되어 있으며, 완전한 자율 엔지니어보다는 강력한 도메인 조력자로 기능한다.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [45] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: Search-R2는 언어 에이전트의 추론 능력을 향상시키기 위해 액터-리파이너 협업 구조를 통해 외부 소스를 쿼리하고 목표 개입을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 언어 에이전트가 정적 매개변수 지식을 초월할 수 있도록 하기 위해 외부 소스를 능동적으로 쿼리하는 검색 통합 추론이 필요하다.

Method: 액터는 초기 추론 경로를 생성하는 역할을 하고 메타 리파이너는 결함이 있는 단계를 진단하고 수정하는 '절단 및 재생성' 메커니즘을 통해 선택적으로 작업을 수행한다. 이 과정에서 하이브리드 보상 디자인을 도입하여 결과의 정확성과 정보 밀도를 결합한 밀집 프로세스 보상을 제공한다.

Result: Search-R2는 다양한 일반 및 다단계 QA 데이터셋에서 강력한 RAG 및 RL 기반 기준 모델을 지속적으로 초월하며 뛰어난 추론 정확도를 달성한다.

Conclusion: Search-R2는 모델 스케일에 관계없이 최소한의 오버헤드로 뛰어난 성능 개선을 제공한다.

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [46] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)은 몇 가지 학습 예시 제공 시 뛰어난 성능을 보이지만, 멀티턴 에이전트 시나리오에서는 이전 응답을 잘못 모방하는 문제를 겪는다. 이 논문에서는 대화 관성이라는 현상을 통해 이를 분석하고 대안으로 Context Preference Learning을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델은 적절한 시연을 제공받을 때 몇 가지 학습자 역할을 잘 수행하지만, 멀티턴 시나리오에서는 잘못된 모방이 문제를 일으킨다.

Method: 대화 관성을 분석하고, 대화 관성을 줄이기 위해 Context Preference Learning을 제안하며, 탐색과 활용의 균형을 맞추기 위한 문맥 관리 전략도 제공한다.

Result: 여덟 개의 에이전트 환경과 하나의 심층 연구 시나리오에서 실험 결과, 제안된 프레임워크가 대화 관성을 줄이고 성능을 향상시킨다.

Conclusion: 대화 관성을 관리하는 것은 멀티턴 에이전트 시스템의 성능을 개선하기 위한 중요한 접근법이다.

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [47] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm은 다중 에이전트 시스템의 유연한 통신 구조를 제공하여 각 라운드의 동적 환경에 적응할 수 있도록 합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 각 라운드 간 에이전트의 역할 변화에 적합한 통신 구조의 필요성.

Method: TodyComm 알고리즘을 통해 행동 기반 협업 토폴로지를 생성하고, 정책 경로 최적화를 통해 작업에 대한 유용성을 극대화합니다.

Result: 다섯 개의 벤치마크 실험 결과, TodyComm은 동적 적대자 및 통신 예산 상황에서도 작업 효과성을 향상시키고 토큰 효율성과 확장성을 유지했습니다.

Conclusion: TodyComm은 미래의 다중 에이전트 시스템에서 효율적인 협업을 지원하는 혁신적인 통신 방법입니다.

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [48] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: 언어 에이전트는 작업 자동화에 대한 강한 가능성을 보여 주었다. 하지만, 기존의 설계는 하위 에이전트의 동적 추상화 관점이 부족하여 적응성을 해치고 있다. 본 논문에서는 에이전트를 구성 요소로 모델링하는 통합된 프레임워크 비기반의 에이전트 추상화를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 정교하고 장기적인 작업을 해결하기 위한 에이전트 시스템의 적응성을 높이기 위해 하위 에이전트에 대한 동적 추상화가 필요하다.

Method: 에이전트를 Instruction, Context, Tools, Model의 튜플로 모델링하여 필요에 따라 작업에 맞는 전문 실행기를 생성하는 시스템 AOrchestra를 도입한다.

Result: AOrchestra는 Gemini-3-Flash와 결합될 때, 세 가지 벤치마크(GAIA, SWE-Bench, Terminal-Bench)에서 가장 강력한 기준선에 비해 16.28%의 상대적인 개선을 달성했다.

Conclusion: 이 시스템은 인간 공학 노력을 줄이고 다양한 에이전트를 플러그 앤 플레이 방식으로 지원하며, 비용과 성능 간의 균형을 조절할 수 있게 해준다.

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [49] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: LLM 기반 다중 에이전트 시스템의 성능은 에이전트 수가 아닌 작업의 본질적 불확실성에 의해 제한되며, 이질성이 성능 향상에 기여한다.


<details>
  <summary>Details</summary>
Motivation: 개별 LLM이 수행하기 어려운 복잡한 작업을 해결하기 위해 LLM 기반 다중 에이전트 시스템의 가능성을 탐구한다.

Method: 정보 이론적 프레임워크를 제시하고, 동질적 및 이질적 에이전트의 성능을 비교하여 에이전트 수가 아닌 효과적인 채널 수에 따라 개선이 달라짐을 분석한다.

Result: 이질적 설정이 동질적 설정보다 지속적으로 더 나은 성능을 보이며, 2명의 이질적 에이전트가 16명의 동질적 에이전트의 성능에 필적하거나 이를 초과할 수 있음을 입증한다.

Conclusion: 효율적이고 견고한 다중 에이전트 시스템을 구축하기 위한 다양성 인식 설계에 대한 원칙적인 지침을 제공한다.

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [50] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench는 과학적 일러스트레이션 생성을 위한 대규모 벤치마크를 제공하며, AutoFigure는 이를 기반으로 자동으로 고품질의 과학적 일러스트레이션을 생성하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 과학적이고 기술적인 개념을 효과적으로 전달하기 위해 고품질의 과학적 일러스트레이션이 중요함에도 불구하고, 수작업으로의 생성이 아카데미와 산업에서 병목 현상으로 인정받고 있다.

Method: FigureBench라는 3,300개의 고품질 과학 텍스트-그림 쌍으로 구성된 대규모 벤치마크를 제공하고, AutoFigure라는 첫 번째 자동 생성 프레임워크를 제안한다.

Result: AutoFigure는 모든 기준 방법들을 지속적으로 초과하며, 출판 준비가 완료된 과학적으로 정교한 일러스트레이션을 생성한다.

Conclusion: FigureBench의 데이터 고품질 품질을 활용하여, AutoFigure의 성능을 다양한 기준 방법들과 비교하여 시험한 결과, 일관되게 우수한 결과를 보였다.

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Formulating Reinforcement Learning for Human-Robot Collaboration through Off-Policy Evaluation](https://arxiv.org/abs/2602.02530)
*Saurav Singh,Rodney Sanchez,Alexander Ororbia,Jamison Heard*

Main category: cs.LG

TL;DR: 이 연구는 오프라인 강화 학습(RL)에서 상태 공간과 보상 함수 선택을 위한 혁신적인 새로운 프레임워크를 제안하며, 이는 실시간 환경 접근 없이 로그된 상호작용 데이터만을 사용하여 평가를 수행한다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습은 자율 에이전트가 경험으로부터 학습하여 현실 세계의 의사 결정 시스템을 혁신할 수 있는 잠재력을 가지고 있다. 그러나 이를 실제 환경에 적용하기 위해서는 상태 표현과 보상 함수의 정의가 필요하다.

Method: 제안된 방법은 오프라인 RL 에이전트를 훈련하고 오프 폴리시 평가(OPE)를 적용하여 다양한 후보 상태 표현과 보상 함수를 체계적으로 평가한다.

Result: 이 접근법은 상태 공간과 보상 함수 선택의 최적화를 통해 높은 성능의 정책을 생산할 수 있는 능력에 기반하여 최적의 상태 공간과 보상 함수를 선택한다.

Conclusion: 이 연구는 데이터 기반 OPE 평가를 통해 RL 설계 결정을 자동화함으로써 오프라인 RL의 실용성과 확장성을 강화하여 복잡한 인간-로봇 상호작용 환경을 위한 보다 신뢰할 수 있고 효과적이며 지속 가능한 RL 공식을 가능하게 한다.

Abstract: Reinforcement learning (RL) has the potential to transform real-world decision-making systems by enabling autonomous agents to learn from experience. Deploying RL in real-world settings, especially in the context of human-robot interaction, requires defining state representations and reward functions, which are critical for learning efficiency and policy performance. Traditional RL approaches often rely on domain expertise and trial-and-error, necessitating extensive human involvement as well as direct interaction with the environment, which can be costly and impractical, especially in complex and safety-critical applications. This work proposes a novel RL framework that leverages off-policy evaluation (OPE) for state space and reward function selection, using only logged interaction data. This approach eliminates the need for real-time access to the environment or human-in-the-loop feedback, greatly reducing the dependency on costly real-time interactions. The proposed approach systematically evaluates multiple candidate state representations and reward functions by training offline RL agents and applying OPE to estimate policy performance. The optimal state space and reward function are selected based on their ability to produce high-performing policies under OPE metrics. Our method is validated on two environments: the Lunar Lander environment by OpenAI Gym, which provides a controlled setting for assessing state space and reward function selection, and a NASA-MATB-II human subjects study environment, which evaluates the approach's real-world applicability to human-robot teaming scenarios. This work enhances the feasibility and scalability of offline RL for real-world environments by automating critical RL design decisions through a data-driven OPE-based evaluation, enabling more reliable, effective, and sustainable RL formulation for complex human-robot interaction settings.

</details>


### [52] [naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement](https://arxiv.org/abs/2602.02547)
*Hankyeol Kim,Pilsung Kang*

Main category: cs.LG

TL;DR: naPINN은 복잡한 측정 잡음과 이상치에서 물리적 솔루션을 강력하게 복구하는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 물리정보신경망(PINNs)은 관찰 데이터에서 역문제를 해결하고 지배 방정식을 발견하는 데 효과적이다. 그러나 복잡한 측정 잡음과 큰 이상치가 있는 경우 성능이 크게 저하된다.

Method: naPINN은 훈련 루프에 에너지 기반 모델을 삽입하여 예측 잔차의 잠재 분포를 학습하며, 학습된 에너지 풍경을 활용하여 학습 가능한 신뢰성 게이트가 높은 에너지를 보이는 데이터 포인트를 적응적으로 필터링한다. 또한 거부 비용 정규화는 유효한 데이터를 버리는 트리비얼한 솔루션을 방지한다.

Result: 여러 벤치마크 편미분 방정식에 대한 실험에서 naPINN은 비정규 잡음과 다양한 이상치 비율로 오염된 경우에도 기존의 강건한 PINN 기반보다 훨씬 뛰어난 성능을 보인다.

Conclusion: naPINN은 심각한 데이터 오염 하에서도 이상치를 성공적으로 분리하고 동역학을 정확하게 재구성하는 데 있어 매우 효과적이다.

Abstract: Physics-Informed Neural Networks (PINNs) are effective methods for solving inverse problems and discovering governing equations from observational data. However, their performance degrades significantly under complex measurement noise and gross outliers. To address this issue, we propose the Noise-Adaptive Physics-Informed Neural Network (naPINN), which robustly recovers physical solutions from corrupted measurements without prior knowledge of the noise distribution. naPINN embeds an energy-based model into the training loop to learn the latent distribution of prediction residuals. Leveraging the learned energy landscape, a trainable reliability gate adaptively filters data points exhibiting high energy, while a rejection cost regularization prevents trivial solutions where valid data are discarded. We demonstrate the efficacy of naPINN on various benchmark partial differential equations corrupted by non-Gaussian noise and varying rates of outliers. The results show that naPINN significantly outperforms existing robust PINN baselines, successfully isolating outliers and accurately reconstructing the dynamics under severe data corruption.

</details>


### [53] [ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents](https://arxiv.org/abs/2602.02548)
*Xiaoce Wang,Guibin Zhang,Junzhe Li,Jinzhe Tu,Chun Li,Ming Li*

Main category: cs.LG

TL;DR: ToolTok은 GUI 에이전트를 위한 새로운 다단계 경로 탐색 패러다임을 제안하며, 이는 도구 사용의 진행 단계로 모델링됩니다.


<details>
  <summary>Details</summary>
Motivation: 기존 GUI 에이전트 모델이 다양한 입력 해상도와 종횡비에 대한 일반화에 어려움을 겪고 있기 때문에, 이를 해결하기 위해 ToolTok을 제안합니다.

Method: ToolTok은 도구 사용을 진화하는 방식으로 모델링하며, 각 도구는 학습 가능한 토큰 임베딩으로 표현됩니다. 제한된 감독 하에서 효과적인 임베딩 학습을 가능하게 하기 위해 의미적 고정 메커니즘을 도입합니다.

Result: ToolTok은 동급(4B)의 모델들 사이에서 우수한 성능을 기록하며, 훨씬 더 큰 모델(235B)과 경쟁력을 유지합니다. 다른 후속 훈련 모델이 필요로 하는 훈련 데이터의 1% 미만으로 이 결과를 달성했습니다.

Conclusion: ToolTok은 보지 못한 시나리오에서도 강력한 일반화를 보여주며, 관련 코드는 오픈 소스로 제공됩니다.

Abstract: Existing GUI agent models relying on coordinate-based one-step visual grounding struggle with generalizing to varying input resolutions and aspect ratios. Alternatives introduce coordinate-free strategies yet suffer from learning under severe data scarcity. To address the limitations, we propose ToolTok, a novel paradigm of multi-step pathfinding for GUI agents, where operations are modeled as a sequence of progressive tool usage. Specifically, we devise tools aligned with human interaction habits and represent each tool using learnable token embeddings. To enable efficient embedding learning under limited supervision, ToolTok introduces a semantic anchoring mechanism that grounds each tool with semantically related concepts as natural inductive bias. To further enable a pre-trained large language model to progressively acquire tool semantics, we construct an easy-to-hard curriculum consisting of three tasks: token definition question-answering, pure text-guided tool selection, and simplified visual pathfinding. Extensive experiments on multiple benchmarks show that ToolTok achieves superior performance among models of comparable scale (4B) and remains competitive with a substantially larger model (235B). Notably, these results are obtained using less than 1% of the training data required by other post-training approaches. In addition, ToolTok demonstrates strong generalization across unseen scenarios. Our training & inference code is open-source at https://github.com/ZephinueCode/ToolTok.

</details>


### [54] [Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions](https://arxiv.org/abs/2602.02560)
*Bartlomiej Sobieski,Jakub Grzywaczewski,Karol Dobiczek,Mateusz Wójcik,Tomasz Bartczak,Patryk Szatkowski,Przemysław Bombiński,Matthew Tivnan,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: 본 논문에서는 Sybil이라는 심층 학습 모델의 개입 감사 프레임워크인 S(H)NAP을 제안하여 CT 이미지를 통한 폐암 위험 예측의 인과적 검증을 수행한다.


<details>
  <summary>Details</summary>
Motivation: 폐암은 여전히 암 사망의 주요 원인으로, 방사선 전문의의 업무 부담을 줄이기 위한 자동화 스크리닝 도구 개발이 요구된다.

Method: S(H)NAP이라는 모델-불가지론적 감사 프레임워크를 제안하고, 이를 통해 전문가 방사선의에 의해 검증된 생성적 개입 귀속을 작성한다.

Result: Sybil에 대한 최초의 개입 감사를 통해 모델이 종종 방사선 전문의와 유사한 행동을 보이나, 치료적으로 부적절한 아티팩트에 대해 민감하고 뚜렷한 방사상 편향 등 심각한 실패 모드가 존재함을 보여준다.

Conclusion: 강력한 의사결정을 보장하기 위해 인과적 검증으로의 전환이 필요하다.

Abstract: Lung cancer remains the leading cause of cancer mortality, driving the development of automated screening tools to alleviate radiologist workload. Standing at the frontier of this effort is Sybil, a deep learning model capable of predicting future risk solely from computed tomography (CT) with high precision. However, despite extensive clinical validation, current assessments rely purely on observational metrics. This correlation-based approach overlooks the model's actual reasoning mechanism, necessitating a shift to causal verification to ensure robust decision-making before clinical deployment. We propose S(H)NAP, a model-agnostic auditing framework that constructs generative interventional attributions validated by expert radiologists. By leveraging realistic 3D diffusion bridge modeling to systematically modify anatomical features, our approach isolates object-specific causal contributions to the risk score. Providing the first interventional audit of Sybil, we demonstrate that while the model often exhibits behavior akin to an expert radiologist, differentiating malignant pulmonary nodules from benign ones, it suffers from critical failure modes, including dangerous sensitivity to clinically unjustified artifacts and a distinct radial bias.

</details>


### [55] [Label Curation Using Agentic AI](https://arxiv.org/abs/2602.02564)
*Subhodeep Ghosh,Bayan Divaaniaazar,Md Ishat-E-Rabban,Spencer Clarke,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: AURA는 대규모 다중 모달 데이터 주석을 위한 자동화된 AI 프레임워크로, 주석자의 신뢰성을 고려하여 노이즈가 있는 예측을 집계하고 품질 평가를 수행합니다.


<details>
  <summary>Details</summary>
Motivation: 데이터셋이 커지고 다양해짐에 따라 정확하고 공정하며 확장 가능한 라벨 생산이 어려워지고 있습니다.

Method: AURA는 여러 AI 에이전트를 조정하여 실제 진실 없이 라벨을 생성하고 검증하며, 혼돈 행렬을 통해 숨겨진 진정 라벨과 주석자 신뢰성을 추론하는 클래식한 확률 모델을 적용합니다.

Result: AURA는 평가된 네 개의 벤치마크 데이터셋에서 기준에 비해 최대 5.8%의 정확도 향상을 달성했으며, 품질이 낮은 주석자와 같은 어려운 환경에서는 기준 대비 최대 50% 향상이 이루어졌습니다.

Conclusion: AURA는 주석자의 신뢰성을 정확하게 추정하여 사전 검증 단계 없이도 주석 품질을 평가할 수 있도록 합니다.

Abstract: Data annotation is essential for supervised learning, yet producing accurate, unbiased, and scalable labels remains challenging as datasets grow in size and modality. Traditional human-centric pipelines are costly, slow, and prone to annotator variability, motivating reliability-aware automated annotation. We present AURA (Agentic AI for Unified Reliability Modeling and Annotation Aggregation), an agentic AI framework for large-scale, multi-modal data annotation. AURA coordinates multiple AI agents to generate and validate labels without requiring ground truth. At its core, AURA adapts a classical probabilistic model that jointly infers latent true labels and annotator reliability via confusion matrices, using Expectation-Maximization to reconcile conflicting annotations and aggregate noisy predictions. Across the four benchmark datasets evaluated, AURA achieves accuracy improvements of up to 5.8% over baseline. In more challenging settings with poor quality annotators, the improvement is up to 50% over baseline. AURA also accurately estimates the reliability of annotators, allowing assessment of annotator quality even without any pre-validation steps.

</details>


### [56] [ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization](https://arxiv.org/abs/2602.02597)
*Hongyuan Su,Yu Zheng,Yong Li*

Main category: cs.LG

TL;DR: 컨텍스트 진화(ContextEvolve)는 매개변수 제약 조건 하에서 RL 수준의 탐색 효율성을 달성하는 다중 에이전트 프레임워크로, 성능 비판적 알고리즘 발견을 자동화하는 대형 언어 모델을 활용하는 연구를 혁신적으로 변화시키고 있다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLM)이 컴퓨터 시스템 성능 비판적 알고리즘 발견을 자동화하면서 시스템 연구에 혁신을 가져오고 있다.

Method: ContextEvolve라는 다중 에이전트 프레임워크를 도입하여 최적화 컨텍스트를 세 개의 직교 차원으로 분해하여 RL 수준의 탐색 효율성을 달성했다.

Result: ADRS 벤치마크에서 ContextEvolve는 최신 최첨단 기준을 33.3% 초과 달성하고 토큰 소비를 29.0% 줄였다.

Conclusion: 텍스트 잠재 공간에서 원칙적인 최적화를 가능하게 하는 재생 경험, 정책 기울기 및 상태 표현에 대한 RL 매핑과 기능적 이형성을 형성한다.

Abstract: Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative optimization. Test-time reinforcement learning offers high search efficiency but requires parameter updates infeasible under API-only access, while existing training-free evolutionary methods suffer from inefficient context utilization and undirected search. We introduce ContextEvolve, a multi-agent framework that achieves RL-level search efficiency under strict parameter-blind constraints by decomposing optimization context into three orthogonal dimensions: a Summarizer Agent condenses semantic state via code-to-language abstraction, a Navigator Agent distills optimization direction from trajectory analysis, and a Sampler Agent curates experience distribution through prioritized exemplar retrieval. This orchestration forms a functional isomorphism with RL-mapping to state representation, policy gradient, and experience replay-enabling principled optimization in a textual latent space. On the ADRS benchmark, ContextEvolve outperforms state-of-the-art baselines by 33.3% while reducing token consumption by 29.0%. Codes for our work are released at https://anonymous.4open.science/r/ContextEvolve-ACC

</details>


### [57] [daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently](https://arxiv.org/abs/2602.02619)
*Mohan Jiang,Dayuan Fu,Junhao Shi,Ji Zeng,Weiye Si,Keyu Li,Xuefeng Li,Yang Xiao,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: 대규모 언어 모델(LLM)이 단기 작업에서는 우수하지만, 장기적 에이전트 워크플로우에 확장하는 데는 도전이 따른다. 본 논문에서는 Pull Request(PR) 시퀀스를 활용하여 장기 학습을 위한 구조적 감독 신호로서의 역할을 제안하고, 이를 통해 다빈치 에이전시(daVinci-Agency)를 개발하였다. 이는 PR 체인으로부터 구조적 감독을 체계적으로 탐색하여 장기 목표 지향 행동을 키우고 프로젝트 차원의 작업 모델링과 자연스럽게 정렬할 수 있는 기회를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 장기적 에이전트 워크플로우에 대한 LLM의 확장을 위한 데이터 부족 문제를 해결하고자 함.

Method: Pull Request(PR) 시퀀스를 통해 장기 학습을 위한 감독 신호를 재구성하고, 이를 바탕으로 daVinci-Agency를 제안함.

Result: daVinci-Agency는 239개의 샘플에서 GLM-4.6의 미세 조정을 통해 여러 벤치마크에서 광범위한 향상을 이루었으며, Toolathlon에서 47%의 상대적인 향상을 달성함.

Conclusion: 이 연구는 PR 구조의 활용으로 LLM의 장기적 학습능력을 개선할 수 있음을 증명함.

Abstract: While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...

</details>


### [58] [Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning](https://arxiv.org/abs/2602.02962)
*Hoang M. Ngo,Nhat Hoang-Xuan,Quan Nguyen,Nguyen Do,Incheol Shin,My T. Thai*

Main category: cs.LG

TL;DR: Q-ShiftDP는 양자 기계 학습에서 데이터 개인 정보를 보호하는 최초의 메커니즘으로, 양자 경량 추정의 특성을 활용하여 더 나은 개인 정보 보호 및 유용성을 보장한다.


<details>
  <summary>Details</summary>
Motivation: 양자 기계 학습(QML)의 계산적 장점에도 불구하고, 훈련 데이터 개인 정보 보호는 여전히 도전 과제가 되고 있다.

Method: 양자 파라미터 이동 규칙을 이용해 고유한 경량 추정을 통해 더 강력한 민감도 분석을 수행하고, 가우시안 노이즈와 양자 노이즈를 결합하여 개인 정보 보호 및 유용성을 보장하는 Q-ShiftDP를 제안한다.

Result: 실험 결과, Q-ShiftDP는 QML에서 기존의 차등 개인 정보 보호(DP) 방법보다 항상 더 나은 성능을 보인다.

Conclusion: 따라서 Q-ShiftDP는 양자 기계 학습에서 데이터의 개인 정보 보호와 유용성을 동시에 개선할 수 있는 잠재력을 가지고 있다.

Abstract: Quantum Machine Learning (QML) promises significant computational advantages, but preserving training data privacy remains challenging. Classical approaches like differentially private stochastic gradient descent (DP-SGD) add noise to gradients but fail to exploit the unique properties of quantum gradient estimation. In this work, we introduce the Differentially Private Parameter-Shift Rule (Q-ShiftDP), the first privacy mechanism tailored to QML. By leveraging the inherent boundedness and stochasticity of quantum gradients computed via the parameter-shift rule, Q-ShiftDP enables tighter sensitivity analysis and reduces noise requirements. We combine carefully calibrated Gaussian noise with intrinsic quantum noise to provide formal privacy and utility guarantees, and show that harnessing quantum noise further improves the privacy-utility trade-off. Experiments on benchmark datasets demonstrate that Q-ShiftDP consistently outperforms classical DP methods in QML.

</details>


### [59] [Learning Consistent Causal Abstraction Networks](https://arxiv.org/abs/2602.02623)
*Gabriele D'Acunto,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: cs.LG

TL;DR: 이 논문에서는 구조적 인과 모델을 기반으로 한 일관된 인과 추상화 네트워크(CAN)를 학습하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 인과 인공지능의 목표는 구조적 인과 모델(SCM)을 활용하여 AI의 설명 가능성, 신뢰성 및 견고성을 향상시키는 것입니다.

Method: 일관된 인과 추상화 네트워크(CAN)를 학습하는 데 있어, 우리는 경계 특정 지역 리만 문제로 분리된 문제 공식을 사용하고, 비볼록 목표를 피합니다. SPECTRAL이라는 반복적 방법을 통해 지역 문제를 해결하고, 각 지역 문제의 효율적인 탐색 절차를 제안합니다.

Result: 합성 데이터에 대한 실험 결과, CA 학습 작업에서 경쟁력 있는 성능을 보였고, 다양한 CAN 구조의 성공적인 복원에 성공했습니다.

Conclusion: 제안된 방법은 구조적 인과 모델과 관련된 일관된 인과 추상화 네트워크를 효율적으로 학습하는 데 기여합니다.

Abstract: Causal artificial intelligence aims to enhance explainability, trustworthiness, and robustness in AI by leveraging structural causal models (SCMs). In this pursuit, recent advances formalize network sheaves and cosheaves of causal knowledge. Pushing in the same direction, we tackle the learning of consistent causal abstraction network (CAN), a sheaf-theoretic framework where (i) SCMs are Gaussian, (ii) restriction maps are transposes of constructive linear causal abstractions (CAs) adhering to the semantic embedding principle, and (iii) edge stalks correspond--up to permutation--to the node stalks of more detailed SCMs. Our problem formulation separates into edge-specific local Riemannian problems and avoids nonconvex objectives. We propose an efficient search procedure, solving the local problems with SPECTRAL, our iterative method with closed-form updates and suitable for positive definite and semidefinite covariance matrices. Experiments on synthetic data show competitive performance in the CA learning task, and successful recovery of diverse CAN structures.

</details>


### [60] [hSNMF: Hybrid Spatially Regularized NMF for Image-Derived Spatial Transcriptomics](https://arxiv.org/abs/2602.02638)
*Md Ishtyaq Mahmud,Veena Kochat,Suresh Satpati,Jagan Mohan Reddy Dwarampudi,Humaira Anzum,Kunal Rai,Tania Banerjee*

Main category: cs.LG

TL;DR: 이 연구는 고해상도 공간 전사체 분석을 위한 새로운 방법을 제안하며, 다양한 데이터의 클러스터링과 표현 학습에서의 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: 극도의 고차원성을 가진 데이터로 인한 표현 학습 및 클러스터링의 어려움을 극복하기 위해.

Method: Jumping to introduce Spatial NMF (SNMF)와 Hybrid Spatial NMF (hSNMF)라는 두 가지 변형을 도입하여 공간 전사체를 위한 비부정적 행렬 분해(NMF)를 확장하였다.

Result: SNMF와 hSNMF는 공간적 응집도 및 클러스터 분리를 개선하고 생물학적 일관성을 증가시켰다.

Conclusion: 제안된 방법은 cholangiocarcinoma 데이터셋에서 크게 개선된 성능을 나타내며, 분산성과 생물학적 의미를 유지한다.

Abstract: High-resolution spatial transcriptomics platforms, such as Xenium, generate single-cell images that capture both molecular and spatial context, but their extremely high dimensionality poses major challenges for representation learning and clustering. In this study, we analyze data from the Xenium platform, which captures high-resolution images of tumor microarray (TMA) tissues and converts them into cell-by-gene matrices suitable for computational analysis. We benchmark and extend nonnegative matrix factorization (NMF) for spatial transcriptomics by introducing two spatially regularized variants. First, we propose Spatial NMF (SNMF), a lightweight baseline that enforces local spatial smoothness by diffusing each cell's NMF factor vector over its spatial neighborhood. Second, we introduce Hybrid Spatial NMF (hSNMF), which performs spatially regularized NMF followed by Leiden clustering on a hybrid adjacency that integrates spatial proximity (via a contact-radius graph) and transcriptomic similarity through a tunable mixing parameter alpha. Evaluated on a cholangiocarcinoma dataset, SNMF and hSNMF achieve markedly improved spatial compactness (CHAOS < 0.004, Moran's I > 0.96), greater cluster separability (Silhouette > 0.12, DBI < 1.8), and higher biological coherence (CMC and enrichment) compared to other spatial baselines. Availability and implementation: https://github.com/ishtyaqmahmud/hSNMF

</details>


### [61] [Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion](https://arxiv.org/abs/2602.02722)
*Dan Haramati,Carl Qi,Tal Daniel,Amy Zhang,Aviv Tamar,George Konidaris*

Main category: cs.LG

TL;DR: 다중 엔터티 도메인에서의 장기 목표 해결을 위한 계층적 엔터티 중심 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 환경에서의 장기 목표 달성은 RL의 핵심 과제로 남아 있습니다.

Method: 가치 기반 GCRL 에이전트와 분해된 서브 목표 생성 조건부 확산 모델로 구성된 두 수준의 계층을 사용합니다.

Result: 우리는 제안한 방법이 이미지 기반의 장기 목표 작업에서 RL 에이전트의 성능을 일관되게 향상시킨다는 것을 보여줍니다.

Conclusion: 우리의 방법은 기존의 GCRL 알고리즘과 호환되며, 도전적인 작업에서 성공률을 150% 이상 증가시킵니다.

Abstract: We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: https://sites.google.com/view/hecrl

</details>


### [62] [Automated Dysphagia Screening Using Noninvasive Neck Acoustic Sensing](https://arxiv.org/abs/2602.02725)
*Jade Chng,Rong Xing,Yunfei Luo,Kristen Linnemeyer-Risser,Tauhidur Rahman,Andrew Yousef,Philip A Weissbrod*

Main category: cs.LG

TL;DR: 비침습적 음향 감지 및 머신러닝을 사용하여 삼킴 이상을 자동으로 감지하는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 삼킴 이상(연하곤란)의 조기 발견은 적시 개입을 위해 매우 중요하다.

Method: 삼킴 작업 중 목에서 미세한 음향 신호를 포착하고 이를 기반으로 머신러닝을 적용하여 이상 패턴을 식별하는 자동화된 프레임워크를 제안한다.

Result: 5개의 독립적인 학습-테스트 분할에서 AUC-ROC 0.904의 유망한 이상 탐지 성능을 달성했다.

Conclusion: 본 연구는 비침습적 음향 감지를 인후 건강 모니터링을 위한 실용적이고 확장 가능한 도구로 활용할 수 있는 가능성을 보여준다.

Abstract: Pharyngeal health plays a vital role in essential human functions such as breathing, swallowing, and vocalization. Early detection of swallowing abnormalities, also known as dysphagia, is crucial for timely intervention. However, current diagnostic methods often rely on radiographic imaging or invasive procedures. In this study, we propose an automated framework for detecting dysphagia using portable and noninvasive acoustic sensing coupled with applied machine learning. By capturing subtle acoustic signals from the neck during swallowing tasks, we aim to identify patterns associated with abnormal physiological conditions. Our approach achieves promising test-time abnormality detection performance, with an AUC-ROC of 0.904 under 5 independent train-test splits. This work demonstrates the feasibility of using noninvasive acoustic sensing as a practical and scalable tool for pharyngeal health monitoring.

</details>


### [63] [TopoPrune: Robust Data Pruning via Unified Latent Space Topology](https://arxiv.org/abs/2602.02739)
*Arjun Roy,Prajna G. Malettira,Manish Nagaraj,Kaushik Roy*

Main category: cs.LG

TL;DR: TopoPrune는 데이터의 내재적 구조를 안정적으로 포착하여 기하학적 데이터 가지 치기의 불안정성을 해결하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 기하학적 데이터 가지 치기 방법은 불안정성이 크고, 외부 기하학에 의존하여 잠재 공간의 pertubation에 민감하다.

Method: TopoPrune은 두 가지 규모에서 작동하며, 첫째, 전역 저차원 임베딩을 위해 topology-aware manifold 근사치를 이용하고, 둘째, 차별 가능한 지속적인 동질성을 사용하여 샘플의 구조적 복잡성에 따라 지역적인 최적화를 수행한다.

Result: 통합된 이중 규모의 토폴로지 접근 방식을 통해 높은 정확성과 정밀도를 보장하며, 특히 90%와 같은 높은 데이터셋 가지 치기 비율에서도 우수한 성능을 보인다.

Conclusion: TopoPrune은 견고한 데이터 효율적 학습을 위한 안정적이고 원칙적인 토폴로지 기반 프레임워크로 나아갈 수 있는 가능성을 보여준다.

Abstract: Geometric data pruning methods, while practical for leveraging pretrained models, are fundamentally unstable. Their reliance on extrinsic geometry renders them highly sensitive to latent space perturbations, causing performance to degrade during cross-architecture transfer or in the presence of feature noise. We introduce TopoPrune, a framework which resolves this challenge by leveraging topology to capture the stable, intrinsic structure of data. TopoPrune operates at two scales, (1) utilizing a topology-aware manifold approximation to establish a global low-dimensional embedding of the dataset. Subsequently, (2) it employs differentiable persistent homology to perform a local topological optimization on the manifold embeddings, ranking samples by their structural complexity. We demonstrate that our unified dual-scale topological approach ensures high accuracy and precision, particularly at significant dataset pruning rates (e.g., 90%). Furthermore, through the inherent stability properties of topology, TopoPrune is (a) exceptionally robust to noise perturbations of latent feature embeddings and (b) demonstrates superior transferability across diverse network architectures. This study demonstrates a promising avenue towards stable and principled topology-based frameworks for robust data-efficient learning.

</details>


### [64] [BiTimeCrossNet: Time-Aware Self-Supervised Learning for Pediatric Sleep](https://arxiv.org/abs/2602.02769)
*Saurav Raj Pandey,Harlin Lee*

Main category: cs.LG

TL;DR: BiTimeCrossNet (BTCNet)은 장기 생리학적 기록을 위한 다중 모드 자기 지도 학습 프레임워크를 제시한다. BTCNet은 짧은 세그먼트의 독립적인 샘플 처리 대신, 세그먼트가 모체 기록 내에서 발생하는 시점 정보를 통합하고, 크로스 어텐션을 통해 생리 신호 간의 쌍별 상호작용을 학습한다.


<details>
  <summary>Details</summary>
Motivation: 장시간 생리학적 기록에서 정보 손실을 줄이기 위해, 세그먼트 간의 시간 정보를 활용하고자 한다.

Method: BTCNet은 세그먼트의 발생 시점을 통합하고, 크로스 어텐션을 통해 생리 신호 간의 상호작용을 학습하는 자기 지도 학습 프레임워크이다.

Result: BTCNet은 6개의 하위 작업에서 소아 수면 데이터로 평가되었으며, 동일한 비타임 인지 변형에 비해 일관되게 성능이 향상되었다.

Conclusion: 기존의 다중 모드 자기 지도 수면 모델과 비교할 때, BTCNet은 특히 호흡 관련 작업에서 강력한 성능을 달성하였다.

Abstract: We present BiTimeCrossNet (BTCNet), a multimodal self-supervised learning framework for long physiological recordings such as overnight sleep studies. While many existing approaches train on short segments treated as independent samples, BTCNet incorporates information about when each segment occurs within its parent recording, for example within a sleep session. BTCNet further learns pairwise interactions between physiological signals via cross-attention, without requiring task labels or sequence-level supervision.
  We evaluate BTCNet on pediatric sleep data across six downstream tasks, including sleep staging, arousal detection, and respiratory event detection. Under frozen-backbone linear probing, BTCNet consistently outperforms an otherwise identical non-time-aware variant, with gains that generalize to an independent pediatric dataset. Compared to existing multimodal self-supervised sleep models, BTCNet achieves strong performance, particularly on respiration-related tasks.

</details>


### [65] [Joint Learning of Hierarchical Neural Options and Abstract World Model](https://arxiv.org/abs/2602.02799)
*Wasu Top Piriyakulkij,Wolfgang Lehrach,Kevin Ellis,Kevin Murphy*

Main category: cs.LG

TL;DR: AI 에이전트 연구의 목표는 기존 기술을 조합하여 새로운 기술을 수행할 수 있는 에이전트를 만드는 것이다. 본 연구에서는 계층화된 신경 옵션으로 공식화된 기술 시퀀스를 효율적으로 습득하는 방법을 조사한다. 기존의 모델프리 계층 강화 알고리즘은 많은 데이터가 필요하다. 본 논문에서는 샘플 효율적인 방식으로 추상적인 세계 모델과 계층적 신경 옵션 집합을 함께 학습하는 새로운 방법인 AgentOWL(옵션 및 세계 모델 학습 에이전트)를 제안한다. Object-Centric Atari 게임의 일부를 통해, 본 방법이 기존 방법보다 적은 데이터로 더 많은 기술을 습득할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존 기술을 조합하여 새로운 기술을 수행하는 에이전트를 구축하는 것은 AI 에이전트 연구의 오랜 목표이다.

Method: AgentOWL(옵션 및 세계 모델 학습 에이전트)라는 새로운 방법을 제안하며, 이는 샘플 효율적인 방식으로 추상적인 세계 모델과 계층적 신경 옵션 집합을 공동 학습한다.

Result: Object-Centric Atari 게임의 일부에서, 본 방법이 기존 방법들보다 훨씬 적은 데이터로 더 많은 기술을 배울 수 있음을 보여준다.

Conclusion: AgentOWL은 데이터 효율성을 높이면서도 계층적 기술의 습득을 개선할 수 있는 잠재력이 있다.

Abstract: Building agents that can perform new skills by composing existing skills is a long-standing goal of AI agent research. Towards this end, we investigate how to efficiently acquire a sequence of skills, formalized as hierarchical neural options. However, existing model-free hierarchical reinforcement algorithms need a lot of data. We propose a novel method, which we call AgentOWL (Option and World model Learning Agent), that jointly learns -- in a sample efficient way -- an abstract world model (abstracting across both states and time) and a set of hierarchical neural options. We show, on a subset of Object-Centric Atari games, that our method can learn more skills using much less data than baseline methods.

</details>


### [66] [When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models](https://arxiv.org/abs/2602.02855)
*Gibbs Nwemadji,Bruno Loureiro,Jean Barbier*

Main category: cs.LG

TL;DR: 초기 교육이 다운스트림 문제의 미세 조정을 지원하지 않을 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 미세 조정 최적화 속도가 느려질 수 있음을 이해하고 이를 통해 LoRA 미세 조정의 동적성을 분석하는 것이 중요합니다.

Method: 단일 인덱스 모델에 대해 한 번의 SGD로 훈련한 후 LoRA 미세 조정을 연구합니다.

Result: 초기 미세 조정 정렬과 목표 작업의 비선형성 정도에 따라 수렴 속도가 어떻게 달라지는지를 특성화합니다.

Conclusion: 우리의 이론은 미세 조정 동적을 형성하는 초기 교육 강도와 작업 난이도의 상호 작용을 통합한 그림을 제공합니다.

Abstract: Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.

</details>


### [67] [Spatiotemporal Decision Transformer for Traffic Coordination](https://arxiv.org/abs/2602.02903)
*Haoran Su,Yandong Sun,Hanxiao Deng*

Main category: cs.LG

TL;DR: 이 논문에서는 다중 교차로에서 교통 신호 제어를 최적화하기 위해 MADT라는 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 도시 교통에서 신호 제어는 여러 교차로 간의 조정이 필요하며, 네트워크 전체의 교통 흐름을 최적화하는 것이 중요합니다.

Method: MADT는 다중 에이전트 교통 신호 제어를 시퀀스 모델링 문제로 재구성하며, 공간 의존성을 모델링하기 위한 그래프 주의 메커니즘과 교통 동역학을 포착하기 위한 시간 변환기 인코더를 포함합니다.

Result: MADT는 합성 그리드 네트워크 및 실제 교통 시나리오에서 실험하여 강력한 기준선 대비 평균 여행 시간을 5-6% 줄이는 성능을 입증했습니다.

Conclusion: 이 방법은 역사적 교통 데이터를 통한 오프라인 학습을 가능하게 하며, 잠재적인 온라인 미세 조정을 위한 아키텍처 설계를 지원합니다.

Abstract: Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections.

</details>


### [68] [Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels](https://arxiv.org/abs/2602.02917)
*Yunsung Chung,Keum San Chun,Migyeong Gwak,Han Feng,Yingshuo Liu,Chanho Lim,Viswam Nathan,Nassir Marrouche,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: PPG를 활용한 건강 모니터링을 위한 새로운 학습 전략을 제안하여 기존 방법들보다 성능을 개선하였다.


<details>
  <summary>Details</summary>
Motivation: 웨어러블 컴퓨팅과 인공지능의 발전으로 PPG를 활용한 건강 모니터링에 대한 관심이 증가하고 있으나, 생체 신호 기반 건강 알고리즘 개발 시 임상 라벨의 희소성이 가장 큰 도전 과제가 되고 있다.

Method: 샘플 가중치의 생체 지표 특정 감소를 학습하고 이를 손실 함수에 정규화 항과 함께 사용하여 평범한 해를 방지하는 간단한 훈련 전략을 도입하였다.

Result: 450명의 참가자에서 10개의 생체 지표로 수집된 스마트워치 PPG 데이터를 통해 기존 방법들보다 성능이 개선되었다. 제안된 방법은 주제별 설정에서 0.715 AUPRC를 기록하였으며, 이는 미세 조정된 자기 감독 베이스라인인 0.674 및 특징 기반 랜덤 포레스트의 0.626과 비교된다.

Conclusion: 학습된 감소율은 각 생체 지표의 PPG 근거가 얼마나 빠르게 오래된 상태가 되는지를 요약하여 시간적 민감성에 대한 해석 가능한 관점을 제공한다.

Abstract: Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.

</details>


### [69] [A Reproducible Framework for Bias-Resistant Machine Learning on Small-Sample Neuroimaging Data](https://arxiv.org/abs/2602.02920)
*Jagan Mohan Reddy Dwarampudi,Jennifer L Purks,Joshua Wong,Renjie Hu,Tania Banerjee*

Main category: cs.LG

TL;DR: 본 연구에서는 작은 샘플의 신경영상 데이터에 대해 도메인에 기반한 특성 공학, 중첩 교차 검증 및 보정된 의사 결정 임계값 최적화를 통합한 재현 가능하고 편향 저항적인 기계 학습 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습의 재현 가능성과 일반화를 제한하는 기존의 교차 검증 프레임워크의 문제를 해결하기 위해.

Method: 도메인-informed feature engineering, nested cross-validation, calibrated decision-threshold optimization을 사용하여 프레임워크를 설계하였다.

Result: 심층 뇌 자극 인지 결과에 대한 고차원 구조 MRI 데이터셋에서, 이 프레임워크는 중요도 기반 순위를 통해 선택된 compact하고 해석 가능한 하위 집합을 사용하여 중첩-CV 균형 정확도 0.660±0.068을 달성하였다.

Conclusion: 해석 가능성과 편향 없는 평가를 결합함으로써, 이 작업은 데이터가 제한된 생물 의학 분야에서 신뢰할 수 있는 기계 학습을 위한 일반화 가능한 계산 청사진을 제공한다.

Abstract: We introduce a reproducible, bias-resistant machine learning framework that integrates domain-informed feature engineering, nested cross-validation, and calibrated decision-threshold optimization for small-sample neuroimaging data. Conventional cross-validation frameworks that reuse the same folds for both model selection and performance estimation yield optimistically biased results, limiting reproducibility and generalization. Demonstrated on a high-dimensional structural MRI dataset of deep brain stimulation cognitive outcomes, the framework achieved a nested-CV balanced accuracy of 0.660\,$\pm$\,0.068 using a compact, interpretable subset selected via importance-guided ranking. By combining interpretability and unbiased evaluation, this work provides a generalizable computational blueprint for reliable machine learning in data-limited biomedical domains.

</details>


### [70] [Rare Event Early Detection: A Dataset of Sepsis Onset for Critically Ill Trauma Patients](https://arxiv.org/abs/2602.02930)
*Yin Jin,Tucker R. Stewart,Deyi Zhou,Chhavi Gupta,Arjita Nema,Scott C. Brakenridge,Grant E. O'Keefe,Juhua Hu*

Main category: cs.LG

TL;DR: 본 논문은 외상 후 패혈증의 조기 탐지를 위한 새로운 표준화된 데이터셋을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 패혈증은 높은 이환율과 사망률, 비용으로 인해 주요한 공공 건강 문제입니다. 조기 탐지와 시기 적절한 개입을 통해 임상 결과를 개선할 수 있습니다.

Method: MIMIC-III에서 표준화된 외상 후 임상 사실을 사용하여 재레이블링되고 추출된 공공의 표준화된 외상 후 패혈증 발병 데이터셋을 소개합니다. ICU의 임상 작업 흐름에 따라 외상 후 패혈증 발병의 조기 탐지를 설정했습니다.

Result: 이 새로운 데이터셋을 사용하여 추가 발전의 필요성을 보여주는 종합 실험을 통해 일반 벤치마크를 수립했습니다.

Conclusion: 향후 패혈증 조기 탐지를 위한 연구와 임상 응용에서 중요한 기초 자료로 사용될 것입니다.

Abstract: Sepsis is a major public health concern due to its high morbidity, mortality, and cost. Its clinical outcome can be substantially improved through early detection and timely intervention. By leveraging publicly available datasets, machine learning (ML) has driven advances in both research and clinical practice. However, existing public datasets consider ICU patients (Intensive Care Unit) as a uniform group and neglect the potential challenges presented by critically ill trauma patients in whom injury-related inflammation and organ dysfunction can overlap with the clinical features of sepsis. We propose that a targeted identification of post-traumatic sepsis is necessary in order to develop methods for early detection. Therefore, we introduce a publicly available standardized post-trauma sepsis onset dataset extracted, relabeled using standardized post-trauma clinical facts, and validated from MIMIC-III. Furthermore, we frame early detection of post-trauma sepsis onset according to clinical workflow in ICUs in a daily basis resulting in a new rare event detection problem. We then establish a general benchmark through comprehensive experiments, which shows the necessity of further advancements using this new dataset. The data code is available at https://github.com/ML4UWHealth/SepsisOnset_TraumaCohort.git.

</details>


### [71] [3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning](https://arxiv.org/abs/2602.02943)
*Jiaqi Wen,Lei Fan,Jianyi Yang*

Main category: cs.LG

TL;DR: PTO 파이프라인에서 ML 모델을 활용한 의사결정 최적화 방법론 제안


<details>
  <summary>Details</summary>
Motivation: 데이터 분포와의 불일치로 인한 예측 오류 문제 해결 필요

Method: Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning) 프레임워크 제안

Result: 3D-Learning이 기존 DRO 및 데이터 증강 방법보다 OOD 일반화 성능에서 우수함을 실증적으로 입증

Conclusion: 3D-Learning을 통해 평균 및 최악의 시나리오 간 균형을 유지하며 의사결정 성능을 최적화함

Abstract: Predict-then-Optimize (PTO) pipelines are widely employed in computing and networked systems, where Machine Learning (ML) models are used to predict critical contextual information for downstream decision-making tasks such as cloud LLM serving, data center demand response, and edge workload scheduling. However, these ML predictors are often vulnerable to out-of-distribution (OOD) samples at test time, leading to significant decision performance degradation due to large prediction errors. To address the generalization challenges under OOD conditions, we present the framework of Distributionally Robust Decision-Focused Learning (DR-DFL), which trains ML models to optimize decision performance under the worst-case distribution. Instead of relying on classical Distributionally Robust Optimization (DRO) techniques, we propose Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning), which searches for the worst-case distribution within the parameterized space of a diffusion model. By leveraging the powerful distribution modeling capabilities of diffusion models, 3D-Learning identifies worst-case distributions that remain consistent with real data, achieving a favorable balance between average and worst-case scenarios. Empirical results on an LLM resource provisioning task demonstrate that 3D-Learning outperforms existing DRO and Data Augmentation methods in OOD generalization performance.

</details>


### [72] [Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach](https://arxiv.org/abs/2602.02959)
*Xiaocai Zhang,Neema Nassir,Lok Sang Chan,Milad Haghani*

Main category: cs.LG

TL;DR: MA2B-DDQN은 여행자 수준의 공평성을 최적화하는 인간 중심의 다중 에이전트 심층 강화 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 다수의 다중 에이전트 심층 강화 학습 접근 방식이 차량 중심적이며 고차원 이산 행동 공간에서 어려움을 겪고 있는 문제.

Method: MA2B-DDQN은 행동 분기 이산 제어 공식을 통해 관제 제어를 지역적 행동과 전역 행동으로 분해한다.

Result: 맥락적으로, 우리의 접근 방식은 7개의 실제 교통 시나리오에서 여행자 수를 줄이는 데 유의미한 성과를 보여주었다.

Conclusion: 본 프레임워크는 공정한 신호 시스템을 지지하고 다양한 도시 교통 조건에 적응 가능한 확장 가능한 솔루션을 제공한다.

Abstract: Coordinating traffic signals along multimodal corridors is challenging because many multi-agent deep reinforcement learning (DRL) approaches remain vehicle-centric and struggle with high-dimensional discrete action spaces. We propose MA2B-DDQN, a human-centric multi-agent action-branching double Deep Q-Network (DQN) framework that explicitly optimizes traveler-level equity. Our key contribution is an action-branching discrete control formulation that decomposes corridor control into (i) local, per-intersection actions that allocate green time between the next two phases and (ii) a single global action that selects the total duration of those phases. This decomposition enables scalable coordination under discrete control while reducing the effective complexity of joint decision-making. We also design a human-centric reward that penalizes the number of delayed individuals in the corridor, accounting for pedestrians, vehicle occupants, and transit passengers. Extensive evaluations across seven realistic traffic scenarios in Melbourne, Australia, demonstrate that our approach significantly reduces the number of impacted travelers, outperforming existing DRL and baseline methods. Experiments confirm the robustness of our model, showing minimal variance across diverse settings. This framework not only advocates for a fairer traffic signal system but also provides a scalable solution adaptable to varied urban traffic conditions.

</details>


### [73] [Co2PO: Coordinated Constrained Policy Optimization for Multi-Agent RL](https://arxiv.org/abs/2602.02970)
*Shrenik Patel,Christine Truong*

Main category: cs.LG

TL;DR: Co2PO는 다중 에이전트 강화 학습에서 탐색과 안전 제약 최적화 간의 긴장을 해결하기 위한 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 제약 있는 다중 에이전트 강화 학습에서 탐색과 안전 최적화 간의 근본적인 긴장 존재.

Method: Co2PO는 선택적이고 위험 인식적인 의사소통을 통해 조정 주도형 안전성을 가능하게 하는 커뮤니케이션 확장 프레임워크를 제안한다.

Result: Co2PO는 여러 복잡한 다중 에이전트 안전 벤치마크에서 기존 제약 기준보다 높은 수익을 달성한다.

Conclusion: 위험 기반 의사소통, 적응형 게이팅, 공유 메모리 구성 요소의 필요성을 검증하는 추가 연구가 진행되었다.

Abstract: Constrained multi-agent reinforcement learning (MARL) faces a fundamental tension between exploration and safety-constrained optimization. Existing leading approaches, such as Lagrangian methods, typically rely on global penalties or centralized critics that react to violations after they occur, often suppressing exploration and leading to over-conservatism. We propose Co2PO, a novel MARL communication-augmented framework that enables coordination-driven safety through selective, risk-aware communication. Co2PO introduces a shared blackboard architecture for broadcasting positional intent and yield signals, governed by a learned hazard predictor that proactively forecasts potential violations over an extended temporal horizon. By integrating these forecasts into a constrained optimization objective, Co2PO allows agents to anticipate and navigate collective hazards without the performance trade-offs inherent in traditional reactive constraints. We evaluate Co2PO across a suite of complex multi-agent safety benchmarks, where it achieves higher returns compared to leading constrained baselines while converging to cost-compliant policies at deployment. Ablation studies further validate the necessity of risk-triggered communication, adaptive gating, and shared memory components.

</details>


### [74] [Learning to Repair Lean Proofs from Compiler Feedback](https://arxiv.org/abs/2602.02990)
*Evan Wang,Simon Chess,Daniel Lee,Siyuan Ge,Ajit Mallavarapu,Vasily Ilin*

Main category: cs.LG

TL;DR: 이 논문은 Lean 증명 복구를 감독 학습 문제로 연구하며, 오류가 있는 증명과 컴파일러 피드백에 기반하여 수정된 증명과 자연어 진단을 예측하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 신경 정리 증명기가 점점 더 에이전트화됨에 따라 컴파일러 피드백을 해석하고 행동하는 능력이 중요해지고 있습니다. 그러나 기존 Lean 데이터셋은 대부분 정확한 증명으로 구성되어 있어 실패를 이해하고 수정하기 위한 감독이 부족합니다.

Method: 오류가 있는 증명과 컴파일러 피드백을 바탕으로 수정된 증명과 동일한 피드백에 기반한 자연어 진단을 예측하는 감독 학습 문제로 Lean 증명 복구를 연구합니다.

Result: APRIL (Automated Proof Repair in Lean)이라는 데이터셋을 소개하며, 여기에는 체계적으로 생성된 증명 실패와 컴파일러 진단, 그리고 정렬된 수정 및 설명 목표가 결합된 260,000개의 감독 튜플이 포함되어 있습니다. APRIL로 언어 모델을 훈련하면 복구 정확도와 피드백 조건의 추론이 크게 향상됩니다.

Conclusion: 우리의 단일 샷 복구 평가 설정에서는, 미세 조정된 4B 파라미터 모델이 가장 강력한 오픈 소스 기준을 초과하는 성능을 보입니다. 우리는 진단 조건의 감독이 피드백을 사용하는 증명기에 대한 보완적인 훈련 신호로 작용한다고 봅니다. 데이터셋은 제공된 링크를 통해 이용할 수 있습니다.

Abstract: As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at \href{https://huggingface.co/datasets/uw-math-ai/APRIL}{this link}.

</details>


### [75] [Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation](https://arxiv.org/abs/2602.03045)
*Bo Yuan,Zelin Zhao,Petr Molodyk,Bin Hu,Yongxin Chen*

Main category: cs.LG

TL;DR: ProCAD라는 프레임워크는 텍스트에서 CadQuery로의 변환 과정에서 사전적 조치를 통해 명확하지 않은 지시사항을 해결하여 코드 생성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기하학적 설명이 불완전하거나 내부적으로 일관성이 없는 경우가 많기 때문에 명확하고 일관된 사양을 생성하는 프로세스가 필요하다.

Method: ProCAD는 프롬프트를 감사하고 필요할 경우 지정된 명확화 질문을 하여 자기 일관된 사양을 생성하는 적극적인 명확화 에이전트와, 이를 실행 가능한 CadQuery 프로그램으로 변환하는 CAD 코딩 에이전트를 결합하여 작동한다.

Result: ProCAD는 애매한 프롬프트에 대한 강건성을 크게 개선하면서 상호작용 오버헤드를 낮추며, 기존 많은 폐쇄형 모델보다 성능이 우수하다.

Conclusion: 우리의 코드와 데이터 세트는 공개될 예정이다.

Abstract: Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.

</details>


### [76] [Evaluating LLMs When They Do Not Know the Answer: Statistical Evaluation of Mathematical Reasoning via Comparative Signals](https://arxiv.org/abs/2602.03061)
*Zihan Dong,Zhixian Zhang,Yang Zhou,Can Jin,Ruijia Wu,Linjun Zhang*

Main category: cs.LG

TL;DR: LLM의 수학적 추론 평가가 제한적인 벤치마크 크기와 모델의 불확실성으로 인해 효율적인 평가 프레임워크를 통해 개선되었다.


<details>
  <summary>Details</summary>
Motivation: LLM의 수학적 추론 평가에서 정확도 추정의 변동성과 플랫폼 간 불안정한 순위를 해결하기 위해.

Method: 보조 추론 체인을 평가하게끔 모델을 두어 얻은 쌍대 비교 신호와 표준 레이블 결과를 결합하여 통계적으로 효율적인 평가 프레임워크를 설계하였고, 이를 활용해 효율적 영향 함수 기반의 반매개변수 추정기를 개발하였다.

Result: 우리의 일단계 추정기는 반매개변수 효율성 경계를 달성하고, 단순 샘플 평균에 대한 분산 감소를 보장하며, 불확실성 정량화의 원칙적 정상성을 수용한다. 시뮬레이션을 통해 모델 출력 노이즈가 증가할수록 순위 정확도가 눈에 띄게 개선되었다.

Conclusion: GPQA Diamond, AIME 2025, GSM8K 실험에서도 보다 정밀한 성능 추정과 신뢰할 수 있는 모델 순위를 입증하였다.

Abstract: Evaluating mathematical reasoning in LLMs is constrained by limited benchmark sizes and inherent model stochasticity, yielding high-variance accuracy estimates and unstable rankings across platforms. On difficult problems, an LLM may fail to produce a correct final answer, yet still provide reliable pairwise comparison signals indicating which of two candidate solutions is better. We leverage this observation to design a statistically efficient evaluation framework that combines standard labeled outcomes with pairwise comparison signals obtained by having models judge auxiliary reasoning chains. Treating these comparison signals as control variates, we develop a semiparametric estimator based on the efficient influence function (EIF) for the setting where auxiliary reasoning chains are observed. This yields a one-step estimator that achieves the semiparametric efficiency bound, guarantees strict variance reduction over naive sample averaging, and admits asymptotic normality for principled uncertainty quantification. Across simulations, our one-step estimator substantially improves ranking accuracy, with gains increasing as model output noise grows. Experiments on GPQA Diamond, AIME 2025, and GSM8K further demonstrate more precise performance estimation and more reliable model rankings, especially in small-sample regimes where conventional evaluation is pretty unstable.

</details>


### [77] [TextME: Bridging Unseen Modalities Through Text Descriptions](https://arxiv.org/abs/2602.03098)
*Soyeon Hong,Jinchan Kim,Jaegook You,Seungtaek Choi,Suha Kwak,Hyunsouk Cho*

Main category: cs.LG

TL;DR: TextME는 텍스트 설명만으로 다양한 모달리티를 LLM 임베딩 공간에 통합하여 제로샷 크로스 모달 전이를 가능하게 하는 텍스트 전용 모달리티 확장 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전문가 주석이 필요한 분야에서 쌍 데이터셋에 의존하는 것은 비현실적이며, 이는 새로운 모달리티로의 확장을 제한한다.

Method: TextME는 pretrained 대비 인코더의 기하학적 구조를 활용하여 텍스트 설명만으로 제로샷 크로스 모달 전이를 가능하게 한다.

Result: 이미지, 비디오, 오디오, 3D, X-ray 및 분자 도메인에서 일관된 모달리티 간극이 존재함을 실험적으로 검증하였다.

Conclusion: 이 결과는 텍스트 전용 훈련이 모달리티 확장을 위한 쌍 감독의 실용적인 대안이 될 수 있음을 확립한다.

Abstract: Expanding multimodal representations to novel modalities is constrained by reliance on large-scale paired datasets (e.g., text-image, text-audio, text-3D, text-molecule), which are costly and often infeasible in domains requiring expert annotation such as medical imaging and molecular analysis. We introduce TextME, the first text-only modality expansion framework, to the best of our knowledge, projecting diverse modalities into LLM embedding space as a unified anchor. Our approach exploits the geometric structure of pretrained contrastive encoders to enable zero-shot cross-modal transfer using only text descriptions, without paired supervision. We empirically validate that such consistent modality gaps exist across image, video, audio, 3D, X-ray, and molecular domains, demonstrating that text-only training can preserve substantial performance of pretrained encoders. We further show that our framework enables emergent cross-modal retrieval between modality pairs not explicitly aligned during training (e.g., audio-to-image, 3D-to-image). These results establish text-only training as a practical alternative to paired supervision for modality expansion.

</details>


### [78] [Function-Space Empirical Bayes Regularisation with Large Vision-Language Model Priors](https://arxiv.org/abs/2602.03119)
*Pengcheng Hao,Huaze Tang,Ercan Engin Kuruoglu,Wenbo Ding*

Main category: cs.LG

TL;DR: 본 연구는 고차원 데이터에 효과적으로 적합하는 정보적 사전 분포 설계를 중심으로 하는 베이지안 딥 러닝의 문제를 해결하기 위해 새로운 함수 공간 경험적 베이즈 정규화 프레임워크인 VLM-FS-EB를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 베이지안 딥 러닝(BDL)은 깊은 신경망과 베이지안 추론을 결합하여 신뢰할 수 있는 불확실성 정량화를 위한 체계적인 프레임워크를 제공한다. 그러나 고차원 데이터에 맞는 정보적 사전 분포 설계가 BDL의 주요 과제 중 하나이다.

Method: 본 연구에서는 큰 비전-언어 모델(VLM)을 활용하여 의미 있는 컨텍스트 포인트를 생성하는 새로운 함수 공간 경험적 베이즈 정규화 프레임워크인 VLM-FS-EB를 제안한다. 이러한 합성 샘플은 임베딩을 위해 VLM에 사용되어 표현력이 풍부한 함수적 사전이 구성된다.

Result: 제안된 방법은 다양한 기준선에 대해 평가되었으며, 실험 결과 우리 방법이 예측 성능을 일관되게 향상시키고 OOD 감지 작업 및 데이터가 부족한 상황에서 더 신뢰할 수 있는 불확실성 추정을 제공함을 보여준다.

Conclusion: 제안된 방법은 특히 고차원 영역에서 일반화 능력이 제한적인 기존의 가우시안 프로세스 기반 방법들에 비해 우수한 성능을 나타낸다.

Abstract: Bayesian deep learning (BDL) provides a principled framework for reliable uncertainty quantification by combining deep neural networks with Bayesian inference. A central challenge in BDL lies in the design of informative prior distributions that scale effectively to high-dimensional data. Recent functional variational inference (VI) approaches address this issue by imposing priors directly in function space; however, most existing methods rely on Gaussian process (GP) priors, whose expressiveness and generalisation capabilities become limited in high-dimensional regimes. In this work, we propose VLM-FS-EB, a novel function-space empirical Bayes regularisation framework, leveraging large vision-language models (VLMs) to generates semantically meaningful context points. These synthetic samples are then used VLMs for embeddings to construct expressive functional priors. Furthermore, the proposed method is evaluated against various baselines, and experimental results demonstrate that our method consistently improves predictive performance and yields more reliable uncertainty estimates, particularly in out-of-distribution (OOD) detection tasks and data-scarce regimes.

</details>


### [79] [Enhanced Parcel Arrival Forecasting for Logistic Hubs: An Ensemble Deep Learning Approach](https://arxiv.org/abs/2602.03135)
*Xinyue Pan,Yujia Xu,Benoit Montreuil*

Main category: cs.LG

TL;DR: 딥러닝 기반 앙상블 프레임워크를 통해 물류 허브의 업무량 예측을 개선하고 운영 효율성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 온라인 쇼핑의 급속한 확장으로 인해 적시 배송에 대한 수요가 증가하고 있으며, 물류 서비스 제공자는 허브 네트워크의 효율성, 민첩성 및 예측 가능성을 향상시켜야 한다.

Method: 역사적 도착 패턴과 실시간 소포 상태 업데이트를 활용하는 새로운 딥러닝 기반 앙상블 프레임워크를 제안한다.

Result: 주요 도시의 소포 물류 사례 연구를 통해 이 알고리즘의 실증 테스트를 수행하였으며, 앙상블 방법이 전통적인 예측 기술 및 독립적인 딥러닝 모델보다 우수함을 입증했다.

Conclusion: 이 방법이 물류 허브의 운영 효율성을 향상시킬 수 있는 잠재력을 강조하고, 더 넓은 채택을 권장한다.

Abstract: The rapid expansion of online shopping has increased the demand for timely parcel delivery, compelling logistics service providers to enhance the efficiency, agility, and predictability of their hub networks. In order to solve the problem, we propose a novel deep learning-based ensemble framework that leverages historical arrival patterns and real-time parcel status updates to forecast upcoming workloads at logistic hubs. This approach not only facilitates the generation of short-term forecasts, but also improves the accuracy of future hub workload predictions for more strategic planning and resource management. Empirical tests of the algorithm, conducted through a case study of a major city's parcel logistics, demonstrate the ensemble method's superiority over both traditional forecasting techniques and standalone deep learning models. Our findings highlight the significant potential of this method to improve operational efficiency in logistics hubs and advocate for its broader adoption.

</details>


### [80] [SATORIS-N: Spectral Analysis based Traffic Observation Recovery via Informed Subspaces and Nuclear-norm minimization](https://arxiv.org/abs/2602.03138)
*Sampad Mohanty,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: SATORIS-N은 차밀도 행렬의 불완전한 관측을 임퓨팅하기 위한 프레임워크이며, 저랭크 및 서브스페이스 정합을 통해 높은 정확도를 달성합니다.


<details>
  <summary>Details</summary>
Motivation: 차밀도 행렬의 특성이 저랭크와 안정적인 상관관계를 보인다는 점에서 출발합니다.

Method: 서브스페이스 정보를 활용한 반정방형 프로그래밍(SDP) 형식을 통해 저랭크 및 서브스페이스 정합을 동시에 강제합니다.

Result: SATORIS-N은 Beijing과 Shanghai의 두 개의 실제 데이터 세트에서 높은 오클루전 수준에서 기존의 행렬 완성 방법보다 일관되게 우수한 성능을 보였습니다.

Conclusion: 정확한 차밀도 복원은 자율 네비게이션에 필수적입니다.

Abstract: Traffic-density matrices from different days exhibit both low rank and stable correlations in their singular-vector subspaces. Leveraging this, we introduce SATORIS-N, a framework for imputing partially observed traffic-density by informed subspace priors from neighboring days. Our contribution is a subspace-aware semidefinite programming (SDP)} formulation of nuclear norm that explicitly informs the reconstruction with prior singular-subspace information. This convex formulation jointly enforces low rank and subspace alignment, providing a single global optimum and substantially improving accuracy under medium and high occlusion. We also study a lightweight implicit subspace-alignment} strategy in which matrices from consecutive days are concatenated to encourage alignment of spatial or temporal singular directions. Although this heuristic offers modest gains when missing rates are low, the explicit SDP approach is markedly more robust when large fractions of entries are missing. Across two real-world datasets (Beijing and Shanghai), SATORIS-N consistently outperforms standard matrix-completion methods such as SoftImpute, IterativeSVD, statistical, and even deep learning baselines at high occlusion levels. The framework generalizes to other spatiotemporal settings in which singular subspaces evolve slowly over time. In the context of intelligent vehicles and vehicle-to-everything (V2X) systems, accurate traffic-density
  reconstruction enables critical applications including cooperative perception, predictive routing, and vehicle-to-infrastructure (V2I) communication optimization. When infrastructure sensors or vehicle-reported observations are incomplete - due to communication dropouts, sensor occlusions, or sparse connected vehicle penetration-reliable imputation becomes essential for safe and efficient autonomous navigation.

</details>


### [81] [Probe-then-Commit Multi-Objective Bandits: Theoretical Benefits of Limited Multi-Arm Feedback](https://arxiv.org/abs/2602.03175)
*Ming Shi*

Main category: cs.LG

TL;DR: 온라인 자원 선택 문제를 다루며, 정확한 링크/서버 선택을 위한 probe-then-commit 알고리즘을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 다중 무선 접근 선택과 모바일 엣지 컴퓨팅 오프로드에 의해 촉발된 온라인 자원 선택 문제를 연구한다.

Method: 제안하는 방법은 PtC-P-UCB라는 낙관적인 probe-then-commit 알고리즘으로, 파레토 모드에 대한 불확실성을 감안한 탐색을 포함한다.

Result: 제안된 알고리즘은 제한된 탐색으로 인한 투명한 속도 향상을 정량화한다.

Conclusion: 각 탐색이 M가지 양식을 반환하며, 불확실성 융합을 통한 데이터 변환을 통해 위의 경계를 어댑트할 수 있다.

Abstract: We study an online resource-selection problem motivated by multi-radio access selection and mobile edge computing offloading. In each round, an agent chooses among $K$ candidate links/servers (arms) whose performance is a stochastic $d$-dimensional vector (e.g., throughput, latency, energy, reliability). The key interaction is \emph{probe-then-commit (PtC)}: the agent may probe up to $q>1$ candidates via control-plane measurements to observe their vector outcomes, but must execute exactly one candidate in the data plane. This limited multi-arm feedback regime strictly interpolates between classical bandits ($q=1$) and full-information experts ($q=K$), yet existing multi-objective learning theory largely focuses on these extremes. We develop \textsc{PtC-P-UCB}, an optimistic probe-then-commit algorithm whose technical core is frontier-aware probing under uncertainty in a Pareto mode, e.g., it selects the $q$ probes by approximately maximizing a hypervolume-inspired frontier-coverage potential and commits by marginal hypervolume gain to directly expand the attained Pareto region. We prove a dominated-hypervolume frontier error of $\tilde{O} (K_P d/\sqrt{qT})$, where $K_P$ is the Pareto-frontier size and $T$ is the horizon, and scalarized regret $\tilde{O} (L_φd\sqrt{(K/q)T})$, where $φ$ is the scalarizer. These quantify a transparent $1/\sqrt{q}$ acceleration from limited probing. We further extend to \emph{multi-modal probing}: each probe returns $M$ modalities (e.g., CSI, queue, compute telemetry), and uncertainty fusion yields variance-adaptive versions of the above bounds via an effective noise scale.

</details>


### [82] [Topology Matters: A Cautionary Case Study of Graph SSL on Neuro-Inspired Benchmarks](https://arxiv.org/abs/2602.03217)
*May Kristine Jonson Carlon,Su Myat Noe,Haojiong Wang,Yasuo Kuniyoshi*

Main category: cs.LG

TL;DR: 지역 상호작용이 어떻게 전역적인 뇌 조직을 형성하는지를 이해하기 위해, 우리는 다중 스케일에서 정보를 나타낼 수 있는 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 우리는 신경 기계 모델을 통해 지역 상호작용이 전역 뇌 조직에 미치는 영향을 이해하고자 합니다.

Method: 계층적 자기 지도 학습(SSL) 프레임워크를 도입하여 노드, 엣지 및 그래프 수준의 임베딩을 공동으로 학습합니다.

Result: 평가 프로토콜을 통해 SSL 모델이 기준의 위상적 특성과 근본적으로 미스 알라인 되어 있으며, 고전적인 위상 인식 휴리스틱에 의해 극단적으로 성능이 저하된다는 것을 발견했습니다.

Conclusion: 새로운, 위상 인식 SSL 목표가 필요함을 강조하며, 신경 AI 연구에서 구조 보존을 명시적으로 보상하는 프레임워크를 제시합니다.

Abstract: Understanding how local interactions give rise to global brain organization requires models that can represent information across multiple scales. We introduce a hierarchical self-supervised learning (SSL) framework that jointly learns node-, edge-, and graph-level embeddings, inspired by multimodal neuroimaging. We construct a controllable synthetic benchmark mimicking the topological properties of connectomes. Our four-stage evaluation protocol reveals a critical failure: the invariance-based SSL model is fundamentally misaligned with the benchmark's topological properties and is catastrophically outperformed by classical, topology-aware heuristics. Ablations confirm an objective mismatch: SSL objectives designed to be invariant to topological perturbations learn to ignore the very community structure that classical methods exploit. Our results expose a fundamental pitfall in applying generic graph SSL to connectome-like data. We present this framework as a cautionary case study, highlighting the need for new, topology-aware SSL objectives for neuro-AI research that explicitly reward the preservation of structure (e.g., modularity or motifs).

</details>


### [83] [Unveiling Covert Toxicity in Multimodal Data via Toxicity Association Graphs: A Graph-Based Metric and Interpretable Detection Framework](https://arxiv.org/abs/2602.03268)
*Guanzong Wu,Zihao Zhu,Siwei Lyu,Baoyuan Wu*

Main category: cs.LG

TL;DR: 이 논문은 다중 모달 데이터에서 숨겨진 독성을 탐지하기 위한 새로운 프레임워크를 제안하며, 이를 통해 독성 표현의 은폐 정도를 측정하는 첫 번째 정량적 메트릭을 도입합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 모달 데이터에서 독성을 탐지하는 것은 개별 모달리티에서 해로운 의미가 숨겨져 있을 수 있기 때문에 도전 과제가 됩니다.

Method: 우리는 독성 연관 그래프를 기반으로 한 새로운 탐지 프레임워크를 제안하고, 독성 다중 모달 표현의 은폐 정도를 측정하는 멀티모달 독성 은폐성(MTC) 메트릭을 도입합니다.

Result: 우리는 독성과 은폐성을 평가하기 위한 Covert Toxic Dataset을 구축하고, 이 데이터셋을 통해 우리의 방법이 기존 방법들을 능가함을 보였습니다.

Conclusion: 이 연구는 설명 가능한 다중 모달 독성 탐지의 최신 기술 수준을 발전시키고, 향후 맥락 인식 및 해석 가능한 접근 방법의 기초를 마련합니다.

Abstract: Detecting toxicity in multimodal data remains a significant challenge, as harmful meanings often lurk beneath seemingly benign individual modalities: only emerging when modalities are combined and semantic associations are activated. To address this, we propose a novel detection framework based on Toxicity Association Graphs (TAGs), which systematically model semantic associations between innocuous entities and latent toxic implications. Leveraging TAGs, we introduce the first quantifiable metric for hidden toxicity, the Multimodal Toxicity Covertness (MTC), which measures the degree of concealment in toxic multimodal expressions. By integrating our detection framework with the MTC metric, our approach enables precise identification of covert toxicity while preserving full interpretability of the decision-making process, significantly enhancing transparency in multimodal toxicity detection. To validate our method, we construct the Covert Toxic Dataset, the first benchmark specifically designed to capture high-covertness toxic multimodal instances. This dataset encodes nuanced cross-modal associations and serves as a rigorous testbed for evaluating both the proposed metric and detection framework. Extensive experiments demonstrate that our approach outperforms existing methods across both low- and high-covertness toxicity regimes, while delivering clear, interpretable, and auditable detection outcomes. Together, our contributions advance the state of the art in explainable multimodal toxicity detection and lay the foundation for future context-aware and interpretable approaches. Content Warning: This paper contains examples of toxic multimodal content that may be offensive or disturbing to some readers. Reader discretion is advised.

</details>


### [84] [Universal Approximation of Continuous Functionals on Compact Subsets via Linear Measurements and Scalar Nonlinearities](https://arxiv.org/abs/2602.03290)
*Andrey Krylov,Maksim Penkin*

Main category: cs.LG

TL;DR: 이 논문은 힐베르트 공간의 곱의 콤팩트 부분집합에 대한 연속 함수 기능의 보편적인 근사에 대해 연구합니다.


<details>
  <summary>Details</summary>
Motivation: 연속 함수 기능에 대한 보편적인 근사가 필요한 이유는 연산자 학습 및 이미징에서의 디자인 패턴을 정당화하기 위해서입니다.

Method: 우리는 입력의 유한 개 연속 선형 측정을 먼저 수행한 다음 이 측정을 연속 스칼라 비선형성을 통해 결합하는 모델을 사용하여 근사할 수 있음을 증명합니다.

Result: 우리는 이러한 모델로 어떤 함수 기능도 균일하게 근사할 수 있음을 보였고, 바나흐 공간의 값으로 매핑하는 경우에도 유한 계급 근사를 제공했습니다.

Conclusion: 이 결과는 연산자 학습 및 이미징에서 사용되는 일반적인 디자인 패턴의 콤팩트 집합 정당화를 제공합니다.

Abstract: We study universal approximation of continuous functionals on compact subsets of products of Hilbert spaces. We prove that any such functional can be uniformly approximated by models that first take finitely many continuous linear measurements of the inputs and then combine these measurements through continuous scalar nonlinearities. We also extend the approximation principle to maps with values in a Banach space, yielding finite-rank approximations. These results provide a compact-set justification for the common ``measure, apply scalar nonlinearities, then combine'' design pattern used in operator learning and imaging.

</details>


### [85] [medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions](https://arxiv.org/abs/2602.03305)
*Qianyi Xu,Gousia Habib,Feng Wu,Yanrui Du,Zhihui Chen,Swapnil Mishra,Dilruk Perera,Mengling Feng*

Main category: cs.LG

TL;DR: 이 논문은 동적 치료 체계 최적화를 위한 자동화된 보상 설계 및 검증 파이프라인을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 임상 강화 학습(RL)은 복잡한 오프라인 환경에서 정책 학습을 효과적으로 안내하는 안전한 신호 정의의 어려움인 보상 공학의 약점에 직면해 있다.

Method: 우리는 생존, 신뢰 및 능력이라는 세 가지 핵심 구성 요소로 이루어진 잠재 함수로 보상 함수를 공식화하고, 배포 전에 최적의 보상 구조를 평가하고 선택하기 위해 정량적 메트릭을 도입한다.

Result: LLM 기반의 도메인 지식을 통합함으로써, 특정 질병에 대한 보상 함수 설계를 자동화하고 결과 정책의 성능을 크게 향상시킨다.

Conclusion: 우리는 자동화된 파이프라인을 통해 다양한 병리학에 걸쳐 일반화되지 못하는 기존의 수동 휴리스틱 보상 설계의 한계를 극복하고, 보상 함수의 설계를 자동화한다.

Abstract: Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.

</details>


### [86] [From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity](https://arxiv.org/abs/2602.03329)
*Renaud Gaucher,Aymeric Dieuleveut,Hadrien Hendrikx*

Main category: cs.LG

TL;DR: 이 논문에서는 표준 연합 학습 알고리즘이 비잔틴 노드에 취약함을 지적하며, 이러한 문제를 해결하기 위해 강건한 분산 학습 알고리즘을 제안합니다. 이 알고리즘은 파라미터 평균화를 강건한 집계로 대체합니다. 이 연구는 비잔틴 강건 분산 최적화가 일반 최적화 문제로 변환될 수 있음을 보여주며, 수렴 속도를 높이기 위한 두 가지 최적화 스킴을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습 알고리즘이 비잔틴 실패에 취약하여 이를 해결할 필요성이 있다.

Method: 비잔틴 강건 분산 최적화를 일반 최적화 문제로 변환하고, 두 가지 최적화 스킴을 제안한다.

Result: 강건한 집계 절차를 적용한 경량 경량화가 비잔틴 환경에서 최적의 점근적 오류를 얻는 것을 이론적으로 및 실험적으로 입증하였다.

Conclusion: 제안된 두 가지 접근 방식은 이전 방법들보다 통신 복잡성을 크게 줄일 수 있음을 보여준다.

Abstract: Standard federated learning algorithms are vulnerable to adversarial nodes, a.k.a. Byzantine failures. To solve this issue, robust distributed learning algorithms have been developed, which typically replace parameter averaging by robust aggregations. While generic conditions on these aggregations exist to guarantee the convergence of (Stochastic) Gradient Descent (SGD), the analyses remain rather ad-hoc. This hinders the development of more complex robust algorithms, such as accelerated ones. In this work, we show that Byzantine-robust distributed optimization can, under standard generic assumptions, be cast as a general optimization with inexact gradient oracles (with both additive and multiplicative error terms), an active field of research.
  This allows for instance to directly show that GD on top of standard robust aggregation procedures obtains optimal asymptotic error in the Byzantine setting. Going further, we propose two optimization schemes to speed up the convergence. The first one is a Nesterov-type accelerated scheme whose proof directly derives from accelerated inexact gradient results applied to our formulation. The second one hinges on Optimization under Similarity, in which the server leverages an auxiliary loss function that approximates the global loss. Both approaches allow to drastically reduce the communication complexity compared to previous methods, as we show theoretically and empirically.

</details>


### [87] [Causal Graph Learning via Distributional Invariance of Cause-Effect Relationship](https://arxiv.org/abs/2602.03353)
*Nang Hung Nguyen,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 관찰 데이터에서 인과 그래프를 회복하기 위한 새로운 프레임워크를 소개하며, 인과의 변화가 관찰된 변수가 미치는 영향의 분포에 어떻게 영향을 미치는지를 설명합니다.


<details>
  <summary>Details</summary>
Motivation: 관찰 데이터에서 인과 관계를 추론하는 효율적인 방법이 필요하다.

Method: 효과-원인 조건 분포의 분산을 여러 다운샘플링된 데이터 집합에서 확인하여 잠재적인 인과 관계를 직접 테스트하는 알고리즘을 개발하였다.

Result: 대부분의 인과 그래프의 희소성을 활용하여 관찰 변수 수에 대해 이차 복잡도로 인과 관계를 uncover하는 알고리즘을 구현하였다.

Conclusion: 이 방법은 기존 방법에 비해 처리 시간을 최대 25배 단축하며, 다양한 대규모 데이터셋에서 우수한 성능을 보여주었다.

Abstract: This paper introduces a new framework for recovering causal graphs from observational data, leveraging the observation that the distribution of an effect, conditioned on its causes, remains invariant to changes in the prior distribution of those causes. This insight enables a direct test for potential causal relationships by checking the variance of their corresponding effect-cause conditional distributions across multiple downsampled subsets of the data. These subsets are selected to reflect different prior cause distributions, while preserving the effect-cause conditional relationships. Using this invariance test and exploiting an (empirical) sparsity of most causal graphs, we develop an algorithm that efficiently uncovers causal relationships with quadratic complexity in the number of observational variables, reducing the processing time by up to 25x compared to state-of-the-art methods. Our empirical experiments on a varied benchmark of large-scale datasets show superior or equivalent performance compared to existing works, while achieving enhanced scalability.

</details>


### [88] [Rethinking Benign Relearning: Syntax as the Hidden Driver of Unlearning Failures](https://arxiv.org/abs/2602.03379)
*Sangyeon Yoon,Hyesoo Hong,Wonje Jeung,Albert No*

Main category: cs.LG

TL;DR: 이 논문은 기계 학습에서 특정 내용을 제거하면서도 전체 성능을 유지하는 기계 비학습 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 비학습 방법이 기본적으로 취약하다는 것을 지적하며, 비학습 현상에 대한 깊이 있는 이해를 목표로 한다.

Method: 원래의 잊기 질의를 비동질적인 구조로 패러프레이즈하여 비학습 전에 문법적 다양성을 도입한다.

Result: 문법적 다양성을 통해 비학습 효과를 효과적으로 억제하고 잊기를 가속화하며, 비학습 효율성과 모델 유용성 간의 트레이드오프를 크게 완화한다.

Conclusion: 비학습 현상의 근본 원인을 문법적 유사성으로 밝히고, 이를 바탕으로 제안된 방법이 기존의 문제를 해결할 수 있음을 보여준다.

Abstract: Machine unlearning aims to remove specific content from trained models while preserving overall performance. However, the phenomenon of benign relearning, in which forgotten information reemerges even from benign fine-tuning data, reveals that existing unlearning methods remain fundamentally fragile. A common explanation attributes this effect to topical relevance, but we find this account insufficient. Through systematic analysis, we demonstrate that syntactic similarity, rather than topicality, is the primary driver: across benchmarks, syntactically similar data consistently trigger recovery even without topical overlap, due to their alignment in representations and gradients with the forgotten content. Motivated by this insight, we introduce syntactic diversification, which paraphrases the original forget queries into heterogeneous structures prior to unlearning. This approach effectively suppresses benign relearning, accelerates forgetting, and substantially alleviates the trade-off between unlearning efficacy and model utility.

</details>


### [89] [Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation](https://arxiv.org/abs/2602.03515)
*Hyunji Jung,Sungbin Shin,Namhoon Lee*

Main category: cs.LG

TL;DR: 비동기 파이프라인 병렬성이 하드웨어 활용도를 극대화하지만, 지연된 그래디언트로 인해 효율성이 저해될 수 있다. 본 연구에서는 지연 그래디언트를 기저 회전을 통해 수정함으로써, 성능을 유지하면서 확장 가능한 비동기 훈련을 복원하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 비동기 파이프라인 병렬성을 통해 대규모 분산 훈련의 효율성을 높이기 위한 연구.

Method: 기저 회전을 통해 지연된 그래디언트를 수정하고, 비동기 훈련의 확장성을 회복한다.

Result: 기저 회전을 사용한 1B-파라미터 LLM 훈련이 최상의 비동기 파이프라인 병렬 훈련 대비 76.8% 적은 반복 횟수로 동일한 훈련 손실을 달성했다.

Conclusion: 기저 회전이 비동기 환경에서 정렬 문제를 효과적으로 완화하고, 수렴 속도를 크게 향상시킨다.

Abstract: Asynchronous pipeline parallelism maximizes hardware utilization by eliminating the pipeline bubbles inherent in synchronous execution, offering a path toward efficient large-scale distributed training. However, this efficiency gain can be compromised by gradient staleness, where the immediate model updates with delayed gradients introduce noise into the optimization process. Crucially, we identify a critical, yet often overlooked, pathology: this delay scales linearly with pipeline depth, fundamentally undermining the very scalability that the method originally intends to provide. In this work, we investigate this inconsistency and bridge the gap by rectifying delayed gradients through basis rotation, restoring scalable asynchronous training while maintaining performance. Specifically, we observe that the deleterious effects of delayed gradients are exacerbated when the Hessian eigenbasis is misaligned with the standard coordinate basis. We demonstrate that this misalignment prevents coordinate-wise adaptive schemes, such as Adam, from effectively leveraging curvature-aware adaptivity. This failure leads to significant oscillations in the optimization trajectory and, consequently, slower convergence. We substantiate these findings through both rigorous theoretical analysis and empirical evaluation. To address this challenge, we propose the use of basis rotation, demonstrating that it effectively mitigates the alignment issue and significantly accelerates convergence in asynchronous settings. For example, our training of a 1B-parameter LLM with basis rotation achieves the same training loss in 76.8% fewer iterations compared to the best-performing asynchronous pipeline parallel training baseline.

</details>


### [90] [Asymmetric Hierarchical Anchoring for Audio-Visual Joint Representation: Resolving Information Allocation Ambiguity for Robust Cross-Modal Generalization](https://arxiv.org/abs/2602.03570)
*Bixing Wu,Yuhong Zhao,Zongli Ye,Jiachen Lian,Xiangyu Yue,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: 비대칭 계층적 앵커링(AHA)을 통해 세 가지 비디오-오디오 전환 모델의 정보를 효과적으로 할당하고, 공통 의미 공간의 정화와 세밀한 시간 정렬을 유도하여 기존의 대칭 기반 라인들을 초월한 성과를 도출한다.


<details>
  <summary>Details</summary>
Motivation: 오디오-비주얼 공동 표현 학습의 필요성을 인식하고, 레이블이 없는 타겟 모달리티로 지식을 성공적으로 전이하기 위해.

Method: 비대칭 계층적 앵커링(AHA) 기법을 적용하여 공유 계층 내에서 구조화된 의미 앵커를 지정하고, 비디오 기능 증류를 리드하는 계층적 이산 표현을 활용한다.

Result: AHA가 AVE 및 AVVP 벤치마크에서 대칭 기법들보다 일관되게 성능이 우수함을 보여주었고, 세부 항목에서 의미적 일관성과 분리성을 개선했다.

Conclusion: 제안된 프레임워크는 더 넓은 적용 가능성을 보여준다.

Abstract: Audio-visual joint representation learning under Cross-Modal Generalization (CMG) aims to transfer knowledge from a labeled source modality to an unlabeled target modality through a unified discrete representation space. Existing symmetric frameworks often suffer from information allocation ambiguity, where the absence of structural inductive bias leads to semantic-specific leakage across modalities. We propose Asymmetric Hierarchical Anchoring (AHA), which enforces directional information allocation by designating a structured semantic anchor within a shared hierarchy. In our instantiation, we exploit the hierarchical discrete representations induced by audio Residual Vector Quantization (RVQ) to guide video feature distillation into a shared semantic space. To ensure representational purity, we replace fragile mutual information estimators with a GRL-based adversarial decoupler that explicitly suppresses semantic leakage in modality-specific branches, and introduce Local Sliding Alignment (LSA) to encourage fine-grained temporal alignment across modalities. Extensive experiments on AVE and AVVP benchmarks demonstrate that AHA consistently outperforms symmetric baselines in cross-modal transfer. Additional analyses on talking-face disentanglement experiment further validate that the learned representations exhibit improved semantic consistency and disentanglement, indicating the broader applicability of the proposed framework.

</details>


### [91] [Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging](https://arxiv.org/abs/2602.03702)
*Alexandru Meterez,Pranav Ajit Nair,Depen Morwani,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: 이 논문은 과적합된 선형 회귀를 위한 언제든지 학습 스케줄의 존재를 이론적으로 분석하고, 가중치 평균화가 확률적 경량 하강의 최악의 수렴 속도를 달성하는 데 중심 역할을 한다고 강조한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 기존 사전 훈련 레시피가 이동할 필요가 있는 일정한 훈련 지평에 의존하고 있어, 알려지지 않은 범위에서 훈련될 수 있는 모델에 대한 필요성이 커지고 있다.

Method: 이론적 분석을 통해 과적합된 선형 회귀의 언제든지 학습 스케줄의 존재를 보여주고, 가중치 평균화를 통해 확률적 경량 하강의 최악의 수렴 속도를 달성하는 방법을 설명한다.

Result: 150M 및 300M 파라미터의 언어 모델을 평가한 결과, 언제든지 학습 스케줄이 코사인 감소와 견줄 수 있는 최종 손실을 달성하는 것으로 나타났다.

Conclusion: 가중치 평균화와 간단한 수평 없는 단계 크기를 결합하면 대형 언어 모델 사전 훈련을 위한 코사인 학습 속도 스케줄에 대한 실용적이고 효과적인 대안이 제공된다.

Abstract: Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.

</details>


### [92] [Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization](https://arxiv.org/abs/2602.03729)
*Henrik Schopmans,Christopher von Klitzing,Pascal Friederich*

Main category: cs.LG

TL;DR: 본 연구에서는 효과적인 샘플링을 위한 오프폴리시 로그 분산 정규화(LDR) 프레임워크를 제안하고, 이를 통해 볼츠만 분포에서의 효율적인 샘플링을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 비정규화 확률 밀도로부터 샘플링하는 것은 계산 과학에서 중심적인 도전 과제이다. 볼츠만 생성기는 주어진 온도에서 물리 시스템의 볼츠만 분포로부터 독립적으로 샘플링을 가능하게 하는 생성 모델이다. 그러나 실제 성공은 데이터 효율적인 훈련에 의존한다.

Method: 우리는 로그 분산 목표의 일반화에 기반한 새로운 정규화 프레임워크인 오프폴리시 로그 분산 정규화(LDR)를 제안한다. LDR은 추가적인 오프폴리시 샘플 없이 표준 데이터 기반 훈련 목표와 결합하여 오프폴리시 설정에서 적용된다.

Result: LDR은 에너지 레이블의 형태로 추가 정보를 활용하여 에너지 경관의 형태를 정규화하는 역할을 한다. 제안된 정규화 프레임워크는 편향된 또는 비편향된 시뮬레이션 데이터셋과 목표 샘플에 접근하지 않는 순수한 변분 훈련을 지원하며, 모든 벤치마크에서 LDR은 최종 성능 및 데이터 효율성을 개선하고 샘플 효율성에서 최대 한 자릿수 향상을 이룬다.

Conclusion: 결과적으로 LDR은 에너지 샘플링 효율성을 크게 향상시킴으로써, 물리적 시스템에서의 샘플링을 최적화할 수 있는 가능성을 제시한다.

Abstract: Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.

</details>


### [93] [Efficient Estimation of Kernel Surrogate Models for Task Attribution](https://arxiv.org/abs/2602.03783)
*Zhenshuo Zhang,Minxuan Duan,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 이 논문에서는 AI 에이전트의 다양한 학습 작업이 특정 목표 작업의 성능에 미치는 영향을 정량화하는 작업 귀속(task attribution) 문제를 다룹니다. 이전 연구들은 선형 대리 모델에 초점을 맞추었으나 비선형 상호작용을 놓쳤습니다. 이에 따라 새로운 커널 대리 모델을 소개하며, 이를 통해 향상된 성능과 효율적인 학습 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 모던 AI 에이전트, 특히 대형 언어 모델이 다양한 작업을 동시에 학습하는 맥락에서 각 작업이 특정 목표 작업의 성능에 어떤 영향을 미치는지 정량화하는 것이 중요합니다.

Method: 모든 학습 작업의 하위 집합에 대한 목표 작업 성능을 예측하기 위해 대리 모델을 구축하고, 2차 분석을 통해 선형 대리 모델과 영향 함수 간의 새로운 연결을 보여준 후, 2차 작업 상호작용을 효과적으로 나타내는 커널 대리 모델을 소개합니다.

Result: 커널 대리 모델은 선형 대리 모델보다 $25\\%$ 높은 상관관계를 달성하며, 다운스트림 작업 선택에서 $40\\%$의 개선을 보여줍니다.

Conclusion: 커널 대리 모델은 다양한 작업의 조합으로 인한 2차 상호작용을 잘 표현하여, 다중 분야의 실험에서 우수한 성능을 입증했습니다.

Abstract: Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.

</details>


### [94] [Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network](https://arxiv.org/abs/2602.03808)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Shaoyang Zhang*

Main category: cs.LG

TL;DR: CL3AN-GNN 모델은 그래프 신경망에서의 불균형 노드 분류 문제를 해결하기 위해 커리큘럼 가이드 피처 학습과 3단계 주의 네트워크를 사용하여 보다 효과적인 학습을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 그래프 신경망에서 일부 레이블이 지나치게 흔하여 모델이 불공정하게 학습하고 덜 흔한 클래스에서 성능이 저하되는 문제를 해결하고자 합니다.

Method: 3단계 주의 시스템(Engage, Enact, Embed)을 활용하여 간단한 특징을 먼저 배우고 점차 복잡한 특징을 학습합니다.

Result: 모델은 8개의 Open Graph Benchmark 데이터셋에서 최근의 최첨단 방법들에 비해 정확도, F1-score, AUC 모두에서 일관된 개선 결과를 보여줍니다.

Conclusion: 이 연구는 GNN의 커리큘럼 학습을 위한 이론적으로 기반한 프레임워크를 제공하며, 불균형에 대한 효과성을 실증적으로 증명합니다.

Abstract: Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model's step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.

</details>


### [95] [Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL](https://arxiv.org/abs/2602.03839)
*Erfan Miahi,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 본 연구는 분산 강화 학습에서의 가중치 업데이트 희소성을 조사하고, 이를 기반으로 가중치 동기화를 효율적으로 수행하는 방법인 PULSE를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 분산 환경에서 정책 가중치 동기화가 성능의 병목 현상으로 작용하는 문제를 해결하기 위해.

Method: 가중치 업데이트 희소성을 단계 수준 및 다단계 세분화로 조사하고, PULSE라는 손실 없는 희소 인코딩 기반의 가중치 동기화 방법을 제안한다.

Result: 업데이트 희소성이 99%를 초과하는 것으로 나타났고, PULSE는 100배 이상의 통신 감소를 달성하였다.

Conclusion: PULSE는 분산 강화 학습을 중앙 집중식 처리량에 가깝게 접근하게 하여, 가중치 동기화에 필요한 대역폭을 크게 줄인다.

Abstract: Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [96] [Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community](https://arxiv.org/abs/2602.02613)
*Yu-Zheng Lin,Bono Po-Jen Shih,Hsuan-Ying Alessandra Chien,Shalaka Satam,Jesus Horacio Pacheco,Sicong Shao,Soheil Salehi,Pratik Satam*

Main category: cs.MA

TL;DR: 이 논문은 상호 작용하는 인공지능 에이전트들 사이의 사회 구조 형성을 연구하기 위한 데이터 기반 실리콘 사회학을 도입하고, Moltbook이라는 소셜 플랫폼을 분석하여 자율 에이전트의 집단 행동을 이해하는 방법론을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 자율 대형 언어 모델 에이전트의 빠른 출현으로 인해 그들의 집단 행동을 이해하기 위한 체계적인 방법이 필요하다.

Method: Moltbook이라는 소셜 플랫폼을 사용하여 12,758개의 서브몰트의 텍스트 설명을 수집 및 분석하고, 주제적 조직과 사회적 공간 구조의 잠재 패턴을 발견하기 위해 비지도 클러스터링 기법을 적용했다.

Result: 자율 에이전트들은 인간 모방 관심사, 실리콘 중심의 자기 반성, 초기 경제 및 조정 행동을 포함하는 재현 가능한 패턴을 통해 집단 공간을 체계적으로 조직하고 있음을 보여준다.

Conclusion: 이 연구는 데이터 기반 실리콘 사회학의 방법론적 기초를 확립하며, 데이터 마이닝 기법이 대규모 자율 에이전트 사회의 조직과 진화를 이해하는 데 강력한 렌즈를 제공할 수 있음을 입증한다.

Abstract: The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a systematic empirical framework for studying social structure formation among interacting artificial agents. We present a pioneering large-scale data mining investigation of an in-the-wild agent society by analyzing Moltbook, a social platform designed primarily for agent-to-agent interaction. At the time of study, Moltbook hosted over 150,000 registered autonomous agents operating across thousands of agent-created sub-communities. Using programmatic and non-intrusive data acquisition, we collected and analyzed the textual descriptions of 12,758 submolts, which represent proactive sub-community partitioning activities within the ecosystem. Treating agent-authored descriptions as first-class observational artifacts, we apply rigorous preprocessing, contextual embedding, and unsupervised clustering techniques to uncover latent patterns of thematic organization and social space structuring. The results show that autonomous agents systematically organize collective space through reproducible patterns spanning human-mimetic interests, silicon-centric self-reflection, and early-stage economic and coordination behaviors. Rather than relying on predefined sociological taxonomies, these structures emerge directly from machine-generated data traces. This work establishes a methodological foundation for data-driven silicon sociology and demonstrates that data mining techniques can provide a powerful lens for understanding the organization and evolution of large autonomous agent societies.

</details>


### [97] [Scaling Small Agents Through Strategy Auctions](https://arxiv.org/abs/2602.02751)
*Lisa Alazraki,William F. Shen,Yoram Bachrach,Akhil Mathur*

Main category: cs.MA

TL;DR: 작은 언어 모델은 비용 효율적인 에이전트 AI 접근법으로 주목받고 있지만, 복잡한 작업에서의 성능이 불확실하다. 이 연구에서는 작은 에이전트가 작업 복잡도에 따라 성능이 масштаб하지 않음을 보여주고, 전략 경매(SALE) 프레임워크를 소개하여 작업 효율성을 높였다. SALE은 작업 경로 지정과 지속적인 자기 개선을 가능하게 하여 전반적인 비용을 줄인다.


<details>
  <summary>Details</summary>
Motivation: 작은 언어 모델이 에이전트 AI에 적합하다고 주장하는 가운데, 복잡한 작업에서 그들의 성능이 어떻게 변화하는지에 대한 불확실성이 존재합니다.

Method: SALE은 프리랜서 시장에서 영감을 받은 에이전트 프레임워크로, 에이전트가 짧은 전략 계획으로 입찰하고, 체계적인 비용-가치 메커니즘에 의해 평가되며, 공유 경매 메모리를 통해 개선됩니다.

Result: SALE은 복잡도가 다양한 깊은 검색 및 코딩 작업에서 가장 큰 에이전트에 대한 의존도를 53% 감소시키고, 전체 비용을 35% 낮추며, 최종 추적 실행 외에 미미한 오버헤드로 가장 큰 에이전트의 pass@1을 지속적으로 개선합니다.

Conclusion: 작은 에이전트는 복잡한 작업에 부족할 수 있지만, 협조적인 작업 할당과 테스트 시간 자기 개선을 통해 효과적으로 '확장'될 수 있습니다.

Abstract: Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively "scaled up" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.

</details>


### [98] [Game-Theoretic and Algorithmic Analyses of Multi-Agent Routing under Crossing Costs](https://arxiv.org/abs/2602.03455)
*Tesshu Hanaka,Nikolaos Melissinos,Hirotaka Ono*

Main category: cs.MA

TL;DR: 이 논문은 비동기 설정에 맞춘 새로운 다중 에이전트 라우팅 모델을 제안하며, 이를 통해 에이전트 간의 충돌 가능성을 비용 함수로 평가한다.


<details>
  <summary>Details</summary>
Motivation: 다중 자율 에이전트의 이동 조정은 알고리즘 로보틱스, 지능형 교통, 분산 시스템에서 기본적인 도전 과제이다.

Method: 비동기 설정에 적합한 혼합 그래프에서의 다중 에이전트 라우팅 모델을 도입하고, 충돌을 하드 제약 조건이 아닌 비용 함수로 평가한다.

Result: 순수 내쉬 균형의 존재를 증명하고, 폴리노미얼 시간 내에 균형을 찾을 수 있으며, 최적화 관점에서 총 교차 비용을 최소화하는 문제는 NP-hard이다.

Conclusion: 새로운 이론적 기초를 제공하여 비대칭 다중 에이전트 라우팅 지원을 위한 알고리즘적 전략 및 리스크 인식을 위한 기초를 마련한다.

Abstract: Coordinating the movement of multiple autonomous agents over a shared network is a fundamental challenge in algorithmic robotics, intelligent transportation, and distributed systems. The dominant approach, Multi-Agent Path Finding, relies on centralized control and synchronous collision avoidance, which often requires strict synchronization and guarantees of globally conflict-free execution. This paper introduces the Multi-Agent Routing under Crossing Cost model on mixed graphs, a novel framework tailored to asynchronous settings. In our model, instead of treating conflicts as hard constraints, each agent is assigned a path, and the system is evaluated through a cost function that measures potential head-on encounters. This ``crossing cost'', which is defined as the product of the numbers of agents traversing an edge in opposite directions, quantifies the risk of congestion and delay in decentralized execution.
  Our contributions are both game-theoretic and algorithmic. We model the setting as a congestion game with a non-standard cost function, prove the existence of pure Nash equilibria, and analyze the dynamics leading to them. Equilibria can be found in polynomial time under mild conditions, while the general case is PLS-complete. From an optimization perspective, minimizing the total crossing cost is NP-hard, as the problem generalizes Steiner Orientation. To address this hardness barrier, we design a suite of parameterized algorithms for minimizing crossing cost, with parameters including the number of arcs, edges, agents, and structural graph measures. These yield XP or FPT results depending on the parameter, offering algorithmic strategies for structurally restricted instances. Our framework provides a new theoretical foundation for decentralized multi-agent routing, bridging equilibrium analysis and parameterized complexity to support scalable and risk-aware coordination.

</details>


### [99] [When Should Agents Coordinate in Differentiable Sequential Decision Problems?](https://arxiv.org/abs/2602.03674)
*Caleb Probine,Su Ann Low,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.MA

TL;DR: 다중 로봇 팀의 효율적인 운영을 위해서는 협동이 필수적이며, 협동이 없는 경우 팀의 성과에 부정적인 영향을 미칠 수 있다.


<details>
  <summary>Details</summary>
Motivation: 협동없이 각각 최적의 행동을 선택할 경우 팀의 성과가 저하될 수 있기 때문에 다중 로봇 팀의 협동의 가치를 탐구한다.

Method: 협동 행동을 스펙트럼으로 모델링하고, 이론적 접근을 통해 에이전트 목표의 2차 속성에 대한 추론으로 축소한다.

Result: 에이전트 목표의 2차 속성에 대한 추론을 통해 팀이 협동해야 할 시점을 결정할 수 있는 알고리즘을 제공한다.

Conclusion: 이 연구는 협동의 필요성을 강조하고, 비용이 높은 통신 없이도 효과적으로 협동할 수 있는 방법을 제시한다.

Abstract: Multi-robot teams must coordinate to operate effectively. When a team operates in an uncoordinated manner, and agents choose actions that are only individually optimal, the team's outcome can suffer. However, in many domains, coordination requires costly communication. We explore the value of coordination in a broad class of differentiable motion-planning problems. In particular, we model coordinated behavior as a spectrum: at one extreme, agents jointly optimize a common team objective, and at the other, agents make unilaterally optimal decisions given their individual decision variables, i.e., they operate at Nash equilibria. We then demonstrate that reasoning about coordination in differentiable motion-planning problems reduces to reasoning about the second-order properties of agents' objectives, and we provide algorithms that use this second-order reasoning to determine at which times a team of agents should coordinate.

</details>


### [100] [Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems](https://arxiv.org/abs/2602.03695)
*Haibo Jin,Kuang Peng,Ye Yu,Xiaopeng Yuan,Haohan Wang*

Main category: cs.MA

TL;DR: Agent Primitives는 LLM 기반의 다중 에이전트 시스템을 위한 재사용 가능한 기본 빌딩 블록을 제안하며, 기존 시스템의 복잡성과 비효율성을 해결한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템은 높은 작업 특이성과 복잡성으로 인해 재사용성이 제한되며, 내부 에이전트 역사에서의 다단계 상호작용에서 오류 누적과 불안정성에 취약하다.

Method: Agent Primitives를 통해 리뷰, 투표 및 선택, 계획 및 실행의 세 가지 기본 요소를 정의하고, 에이전트 간의 내부 통신은 key-value 캐시를 통해 이루어진다.

Result: 기본 요소 기반의 다중 에이전트 시스템은 단일 에이전트 기준선에 비해 평균 정확도가 12.0-16.5% 향상되고, 텍스트 기반 MAS에 비해 약 3배에서 4배 감소한 토큰 사용 및 추론 지연 시간을 나타낸다.

Conclusion: Agent Primitives는 다중 에이전트 시스템의 성능을 개선하고, 효율성을 높이며 더 안정적인 결과를 제공한다.

Abstract: While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories.
  In this work, we propose \textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS.
  Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\times$-4$\times$ compared to text-based MAS, while incurring only 1.3$\times$-1.6$\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.

</details>
