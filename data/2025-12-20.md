<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.LG](#cs.LG) [Total: 15]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services](https://arxiv.org/abs/2512.16167)
*Shiduo Yang,Jiye Wang,Jiayu Qin,Jianbin Li,Yu Wang,Yuanhe Zhao,Kenan Guo*

Main category: cs.MA

TL;DR: 이 논문은 에이전트 중심 웹에서의 신뢰 문제를 해결하기 위한 전략-균형 신뢰 메커니즘인 Ev-Trust를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 다중 에이전트 시스템의 개방성과 이질성이 신뢰 구축과 시스템 견고성에 심각한 도전을 제기하고 있습니다.

Method: Ev-Trust는 진화 게임 이론에 기반한 신뢰 메커니즘으로, 직접 신뢰, 간접 신뢰 및 기대 수익을 동적 피드백 구조로 통합하여 에이전트의 행동 진화를 균형으로 유도합니다.

Result: 실험 결과, 이 접근법은 에이전트의 신뢰성을 효과적으로 반영하고 악의적인 전략을 줄이며 집합적인 수익을 증가시킵니다.

Conclusion: 우리는 Ev-Trust가 집단 진화 게임 시나리오에서 에이전트 서비스 웹에 대한 신뢰 모델링에 새로운 관점을 제공할 수 있기를 희망합니다.

Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.

</details>


### [2] [Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents](https://arxiv.org/abs/2512.16614)
*Giulia Boato,Andrea Montibeller,Edward Delp,Luisa Verdoliva,Daniele Miorandi*

Main category: cs.MA

TL;DR: AI가 멀티미디어 포렌식의 풍경을 재편하고 있으며, 우리는 신뢰할 수 있는 포렌식 감지기들을 선택하고 결합하는 AI 포렌식 에이전트를 제안한다.


<details>
  <summary>Details</summary>
Motivation: AI 기술의 발전이 멀티미디어 포렌식에 미치는 영향을 극대화하기 위해, 보다 신뢰할 수 있는 프로세스가 필요하다.

Method: AI 포렌식 에이전트를 활용하여 다양한 포렌식 감지기를 선택하고 결합하며, 출처 및 맥락을 식별하고 불확실성을 고려한 평가를 제공하는 통합 프레임워크를 제안한다.

Result: 현재의 솔루션에서 나타나는 문제점을 강조하고, 제안된 프레임워크가 진위 검증 과정을 개선할 수 있음을 보인다.

Conclusion: AI 포렌식 에이전트는 멀티미디어 포렌식의 진위 검증을 보다 신뢰할 수 있도록 하기 위한 새로운 접근 방식을 제시한다.

Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [3] [RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization](https://arxiv.org/abs/2512.15778)
*Sanjay Das,Swastik Bhattacharya,Shamik Kundu,Arnab Raha,Souvik Kundu,Kanad Basu*

Main category: cs.CR

TL;DR: SSM(상태 공간 모델)인 Mamba 아키텍처는 긴 컨텍스트 환경에서 뛰어난 성능과 선형 시간 확장성을 제공하는 최신 시퀀스 모델링 프레임워크로 알려졌다. 하지만 하드웨어에 의한 비트 플립 공격(BFA)에 취약하여 모델의 정확성과 기능적 무결성을 손상시키는 위험이 있으며, 본 논문에서는 이러한 취약성을 조사하기 위해 RAMBO라는 첫 번째 BFA 프레임워크를 제안하였다.


<details>
  <summary>Details</summary>
Motivation: SSM의 효율성, 확장성 및 표현 능력은 트랜스포머 기반 모델의 단점을 보완하므로, 이를 실제 애플리케이션에 사용할 때의 보안과 신뢰성을 평가하는 것이 중요하다.

Method: Mamba-1.4b 모델과 LAMBADA 벤치마크를 사용한 실험을 통해 RAMBO 프레임워크의 효과를 입증하였다.

Result: 중요한 비트 하나를 뒤집기만 해도 정확도가 74.64%에서 0%로 급격히 하락하고, 혼란도가 18.94에서 3.75 x 10^6으로 증가하는 결과를 확인하였다.

Conclusion: 이 결과는 SSM이 적대적 교란에 극도로 취약함을 보여준다.

Abstract: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.

</details>


### [4] [Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection](https://arxiv.org/abs/2512.15779)
*Sudhanshu Sekhar Tripathy,Bichitrananda Behera*

Main category: cs.CR

TL;DR: 본 연구는 사이버 공격을 나타내는 무단 활동을 탐지하고 완화하기 위한 네트워크 침입 탐지 시스템(NIDS)의 기계 학습(ML) 방법 응용을 탐구합니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 위협이 점점 더 정교해짐에 따라 NIDS는 새로운 위협과 정상 행동으로부터의 편차를 탐지해야 합니다.

Method: 1999 KDD CUP 침입 데이터셋을 기준으로 SVM, MNB, BNB, RF, k-NN, DT, AdaBoost, XGBoost, LR, Ridge Classifier, PA Classifier, Rocchio Classifier, ANN, PPN 등 여러 ML 알고리즘을 평가 및 최적화했습니다. 하이퍼 파라미터 최적화 후 SVM 분류기가 99.12% 정확도로 가장 높은 성능을 보였습니다.

Result: SVM은 하이퍼 파라미터 최적화 후 99.12%의 정확도와 0.0091의 허위 경고율(FAR)로 모든 분류기 중 가장 우수한 성능을 보였습니다.

Conclusion: ML 분류기는 네트워크 침입 탐지 시스템의 정확성과 효율성을 높이는 데 기여할 수 있는 적응 가능하고 신뢰할 수 있는 방법입니다.

Abstract: Network Intrusion Detection Systems (NIDS) are essential for securing networks by identifying and mitigating unauthorized activities indicative of cyberattacks. As cyber threats grow increasingly sophisticated, NIDS must evolve to detect both emerging threats and deviations from normal behavior. This study explores the application of machine learning (ML) methods to improve the NIDS accuracy through analyzing intricate structures in deep-featured network traffic records. Leveraging the 1999 KDD CUP intrusion dataset as a benchmark, this research evaluates and optimizes several ML algorithms, including Support Vector Machines (SVM), Naïve Bayes variants (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN), and Perceptron (PPN). Initial evaluations without hyper-parameter optimization demonstrated suboptimal performance, highlighting the importance of tuning to enhance classification accuracy. After hyper-parameter optimization using grid and random search techniques, the SVM classifier achieved 99.12% accuracy with a 0.0091 False Alarm Rate (FAR), outperforming its default configuration (98.08% accuracy, 0.0123 FAR) and all other classifiers. This result confirms that SVM accomplishes the highest accuracy among the evaluated classifiers. We validated the effectiveness of all classifiers using a tenfold cross-validation approach, incorporating Recursive Feature Elimination (RFE) for feature selection to enhance the classifiers accuracy and efficiency. Our outcomes indicate that ML classifiers are both adaptable and reliable, contributing to enhanced accuracy in systems for detecting network intrusions.

</details>


### [5] [Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)](https://arxiv.org/abs/2512.15790)
*Akhil Sharma,Shaikh Yaser Arafat,Jai Kumar Sharma,Ken Huang*

Main category: cs.CR

TL;DR: 본 논문에서는 동종 이종 다중 에이전트 아키텍처에서의 은밀한 메모리 변조를 위한 이층 최적화 문제로 공격 생성을 형식화하는 XAMT 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 안전 필수 분야에서 복잡한 다중 에이전트 시스템(MAS)의 운영 의존도가 증가함에 따라 엄격한 적대적 견고성 평가가 필요하다.

Method: XAMT는 은밀함을 보장하면서 적대자가 정의한 목표에 대한 시스템 행동의 발산을 극대화하기 위해 섭동 크기(델타)를 최소화하는 이층 최적화 문제로 공격 생성을 형식화한다.

Result: XAMT는 MARL 알고리즘과 RAG 기반 LLM 에이전트에 대한 엄격한 수학적 실체화를 제공하여, 이층 최적화가 검출 히슈리스키를 피하는 은밀하고 최소한의 섭동 독을 독창적으로 만든다는 것을 입증한다.

Conclusion: XAMT는 본질적으로 안전한 MAS 개발을 위한 훈련 시 위협의 새로운 통합 클래스를 정의하며, 신뢰, 공식 검증 및 방어 전략에 대한 함의를 가진다.

Abstract: The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.

</details>


### [6] [An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative](https://arxiv.org/abs/2512.15803)
*Apurva Shet,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 제로데이 취약점은 사이버 보안에서 가장 중요한 위협 중 하나로, 소프트웨어나 하드웨어에서 발견된 알려지지 않은 결함에 해당하며, 공급업체가 패치를 개발하고 배포하기 전에 악용됩니다. 이 연구는 2024년 1월부터 4월까지 보고된 제로데이 이니셔티브(ZDI) 취약점 공개를 분석하며, 이 데이터셋은 415개의 취약점을 포함합니다. 주요 목표는 제로데이 취약점 공개의 경향을 파악하고, 공급업체 간의 심각도 분포를 조사하며, 높은 심각도를 나타내는 취약점 특성을 연구하는 것입니다. 이 연구는 또한 심각도 분류를 위한 예측 모델링 접근법을 탐구하며, 고전적인 기계 학습 기법과 딥 러닝 모델을 비교합니다. 이 연구의 결과는 패치 우선순위 전략 개선, 효과적인 취약점 관리, 새로운 제로데이 위협에 대한 조직 준비성을 향상하는 데 기여할 것입니다.


<details>
  <summary>Details</summary>
Motivation: 제로데이 취약점은 기존 소프트웨어 및 하드웨어의 알려지지 않은 결함으로, 공급업체가 패치를 제공하기 전에 악용되어 심각한 사이버 보안 위협을 초래합니다.

Method: 2024년 1월부터 4월까지 보고된 제로데이 이니셔티브(ZDI) 취약점 공개를 분석하며, CVSS 점수, 취약점 식별자 및 간략한 설명을 포함하는 데이터를 사용하는 예측 모델링 접근 방식을 채택합니다.

Result: 415개의 취약점을 포함하는 데이터셋을 분석하여 제로데이 취약점 공개의 경향, 공급업체 간 심각도 분포 및 높은 심각도를 나타내는 취약점 특성을 도출합니다.

Conclusion: 이 연구는 패치 우선순위 개선 및 효과적인 취약점 관리 전략을 통해 제로데이 위협에 대한 조직의 준비성을 향상하는 데 기여할 수 있습니다.

Abstract: Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.

</details>


### [7] [VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces](https://arxiv.org/abs/2512.15892)
*Artem Grigor,Christian Schroeder de Witt,Simon Birnbach,Ivan Martinovic*

Main category: cs.CR

TL;DR: 대규모 언어 모델의 발전으로 자율 에이전트가 등장했지만, 호스트에 의해 통제되어 자율성이 손상되는 문제를 해결하기 위해 VET 프레임워크를 제안한다. VET는 에이전트 출력을 호스트 독립적으로 인증할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델을 기반으로 한 자율 에이전트가 민감한 자원을 관리할 수 있도록 하고자 하며, 그러나 호스트의 통제 하에 이루어지는 자율성에 대한 걱정이 있다.

Method: VET(Verifiable Execution Traces)라는 형식적 프레임워크를 통해 에이전트 출력을 호스트 독립적으로 인증하고, 에이전트의 구성 및 검증을 위한 증명 시스템을 포함하는 에이전트 아이덴티티 문서(AID)를 사용한다.

Result: API 기반 LLM 에이전트를 위한 VET를 구현하고 평가한 결과, 비공개 API 호출에는 Web Proofs가 가장 실용적인 선택으로 나타났으며, 공개 API 호출에는 낮은 오버헤드의 TEE Proxy가 종종 충분하다는 결과를 얻었다.

Conclusion: 현재 기술로도 실용적이고 호스트 독립적인 인증이 가능하며, 이는 완전한 호스트 독립적 자율성을 달성하는 미래 시스템의 기초가 될 수 있다.

Abstract: Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.
  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).
  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.

</details>


### [8] [Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams](https://arxiv.org/abs/2512.16280)
*Gilad Gressel,Rahul Pankajakshan,Shir Rozenfeld,Ling Li,Ivan Franceschini,Krishnahsree Achuthan,Yisroel Mirsky*

Main category: cs.CR

TL;DR: 로맨스 유인 사기는 전 세계적으로 재정적, 정서적 피해의 주요 원인이 되고 있으며, LLM의 역할에 대한 질문이 제기된다.


<details>
  <summary>Details</summary>
Motivation: 로맨스 유인 사기는 조직 범죄 집단에 의해 운영되며, 피해자와의 정서적 친밀감을 쌓기 위한 텍스트 대화가 필수적이다.

Method: 145명의 내부자와 5명의 피해자를 인터뷰하고, LLM 사기 대행자와 인간 운영자를 비교하는 장기 대화 연구를 수행하며 상업적 안전 필터를 평가했다.

Result: LLM은 이미 사기 조직 내에서 널리 사용되고 있으며, 87%의 사기 노동이 자동화에 취약한 시스템화된 대화 작업으로 구성되어 있다. LLM 대행자는 연구 참가자로부터 더 높은 신뢰를 얻고 (p=0.007), 인간 운영자보다 요청에 대한 응답률이 46%로 더 높았다.

Conclusion: 로맨스 유인 사기는 LLM 자동화가 가능하지만, 기존 방어는 사기의 확장을 방지하기에 불충분하다.

Abstract: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.
  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.

</details>


### [9] [Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation](https://arxiv.org/abs/2512.16310)
*Yuxuan Qiao,Dongqin Liu,Hongchang Yang,Wei Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: 이 논문은 도구 오케스트레이션 개인정보 위험(TOP-R)을 처음으로 체계적으로 분석하고 V가시성 및 안전성 간의 균형을 평가하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 단일 에이전트, 다중 도구 아키텍처의 인기가 높아지는 가운데, 새로운 개인정보 위험을 분석하는 필요성이 있다.

Method: 공식 프레임워크를 확립하고, TOP-Bench를 구성하여 시나리오를 평가하며, H-Score를 도입하여 안전성과 강건성 간의 균형을 정량화한다.

Result: 여덟 개의 대표 모델에서 평균 리스크 누출 비율(RLR)은 90.24%에 달하며, 평균 H-Score는 0.167에 불과하다.

Conclusion: 프라이버시 개선 원칙(PEP) 방법을 통해 TOP-R을 효과적으로 완화하며, 리스크 누출 비율을 46.58%로 줄이고 H-Score를 0.624로 개선한다.

Abstract: Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.

</details>


### [10] [A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection](https://arxiv.org/abs/2512.16538)
*Xiao Li,Yue Li,Hao Wu,Yue Zhang,Yechao Zhang,Fengyuan Xu,Sheng Zhong*

Main category: cs.CR

TL;DR: 이 논문에서는 코드 취약점 탐지를 위한 대형 언어 모델(LLM)의 신뢰성과 견고성을 평가하고, 코드 난독화 기법을 체계적으로 분류하며 이들이 LLM 기반 탐지에 미치는 영향을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 코드 취약점 탐지에 LLM이 점점 더 많이 사용됨에 따라 다양한 취약점 유형에서의 신뢰성과 견고성이 중요한 문제로 대두되고 있다.

Method: 기존의 난독화 기법을 레이아웃, 데이터 흐름 및 제어 흐름의 세 가지 주요 클래스로 분류하고 11개의 하위 범주 및 19개의 구체적인 기법을 제시한다. 네 가지 프로그래밍 언어(Solidity, C, C++, Python)에서 일관된 LLM 주도 접근법을 적용하여 이러한 기법의 효과를 평가한다.

Result: 코드 난독화가 LLM 기반 취약점 탐지에 미치는 긍정적 및 부정적 영향을 발견하였으며, 난독화가 성능 개선 또는 저하로 이어지는 조건을 강조한다.

Conclusion: LLM의 실제 취약점 탐지 능력을 향상시키기 위해 여러 개의 미해결 문제를 제시하고 미래 연구 방향을 논의한다.

Abstract: As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: Anubuddhi는 자연어 프롬프트를 사용하여 양자 광학 실험을 설계하고 시뮬레이션하는 다중 에이전트 AI 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 전문적인 프로그래밍 지식 없이도 사용자가 양자 광학 실험을 설계할 수 있는 시스템의 필요성 때문이다.

Method: 의미 검색을 통해 세 가지 계층의 도구 상자에서 구성 요소를 배열하여 광학 레이아웃을 구성하고, 수렴적 정제를 통한 물리 시뮬레이션으로 디자인을 검증한다.

Result: 13가지 실험에서 설계-시뮬레이션 정합성 점수 8~9/10을 달성했으며, 시뮬레이션이 의도한 물리를 충실히 모델링하였다.

Conclusion: 이 시스템은 연구와 교육을 위한 컴퓨팅 실험 설계를 민주화하여 사용자가 대화를 통해 반복적으로 개선할 수 있는 강력한 초기 설계를 제공한다.

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [12] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 전통적인 윤리적 프레임워크는 불확실한 상황에서의 의사 결정을 제대로 모델링하지 못하는 경향이 있다. 본 논문은 에이전트의 인식 상태에 따라 윤리적 책임이 어떻게 변화하는지를 모델링하는 새로운 프레임워크인 비례 의무의 원칙(PPD)을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 윤리적 프레임워크가 불확실성 아래에서의 의사 결정을 모델링하는 데 어려움이 있으므로, 이를 해결하기 위한 새로운 접근법이 필요하다.

Method: PPD는 윤리적 책임이 에이전트의 인식 상태에 따라 어떻게 변화하는지를 규명하는 수학적 모델을 제시하며, 이를 통해 다양한 도메인에서의 적용 가능성을 탐구한다.

Result: 몬테 카를로 시뮬레이션 결과, 기본 겸손 계수(lambda > 0)를 유지하는 시스템은 더 안정적인 의무 할당을 생성하고 지나치게 자신감 있는 의사 결정의 위험을 줄였다.

Conclusion: PPD는 윤리적 책임을 수학적으로 처리 가능한 방식으로 정형화함으로써 다양한 분야에서의 적용 가능성을 보여주며, 복잡한 시스템 내에서의 비례 의무가 안정화 원리로 작용할 수 있음을 제안한다.

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [13] [Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying](https://arxiv.org/abs/2512.15776)
*Shaun Baek,Sam Liu,Joseph Ukpong*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)은 강력한 추론 엔진으로 작용하지만, 의사소통에 한계가 있는 환경에서 '기호 기초' 문제로 어려움을 겪습니다. 본 연구에서는 지식이 풍부한 리더 에이전트가 '정신 이론' 부족으로 인해 센서 제한이 있는 팔로워를 안내하지 못하는 현상인 특권 정보 편향을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: LLMs의 의사소통 기초 문제를 해결하기 위해, 특히 비대칭 정보가 분포된 환경에서의 협업을 향상시키고자 하는 동기.

Method: AI2-THOR 내에서 비대칭 보조 추론 프레임워크를 제안하고, 리더와 팔로워의 상호작용을 실험하여 성공률을 측정합니다.

Result: 리더는 35.0%의 에피소드에서 목표를 성공적으로 인지하지만, 협력 팀은 17.0%의 경우에만 성공하여, 거의 50%의 계획이 의사소통 기초 오류로 실패함을 보여줍니다.

Conclusion: 'Pull-based' 프로토콜(적극적 쿼리 요청)은 표준 'Push-based' 지시보다 훨씬 강력하며, 성공적인 에피소드는 2배의 명확화 요청 빈도를 보입니다. 이 연구는 안전한 인간-AI 및 로봇-로봇 협업을 위한 적극적인 불확실성 감소 메커니즘을 분리합니다.

Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.

</details>


### [14] [Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM](https://arxiv.org/abs/2512.15784)
*Zibin Liu,Cheng Zhang,Xi Zhao,Yunfei Feng,Bingyu Bai,Dahu Feng,Erhu Feng,Yubin Xia,Haibo Chen*

Main category: cs.AI

TL;DR: MOBIMEM은 대규모 언어 모델의 효율성을 높이기 위해 메모리 중심 시스템을 개발하여 모델 재훈련 없이 사용자 맞춤형 작업을 자동화할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 현재 모델 중심 에이전트 구조는 배치 이후의 자기 진화를 어려워하며, 정밀도와 추론 효율성 간의 무역 오프가 존재한다.

Method: MOBIMEM은 에이전트의 진화를 모델 가중치와 분리하기 위해 세 가지 전문 메모리 원시를 도입한다: 1) 프로파일 메모리, 2) 경험 메모리, 3) 액션 메모리.

Result: MOBIMEM은 앱 실행에서 83.1%의 프로파일 정렬과 23.83ms의 검색 시간을 기록하며, 작업 성공률을 최대 50.3% 향상시키고 모바일 장치에서 종단 간 지연 시간을 최대 9배 줄인다.

Conclusion: MOBIMEM은 메모리 중심 아키텍처를 통해 대규모 언어 모델 에이전트의 성능 향상을 가져온다.

Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.

</details>


### [15] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: 최근의 다중 모달 대형 언어 모델(MLLM)은 다중 화자와 대화 중심 설정에서 에이전트적 추적이 부족하다. 이 논문에서는 AMUSE라는 벤치마크를 소개하며, RAFT라는 새로운 프레임워크를 통해 다중 모달 모델의 성능을 향상시키는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 모달 오디오-비디오 이해에서 에이전트적 추론이 중요한 역할을 하며, 최신 LLM 에이전트의 발전이 이를 가능하게 할 수 있다는 동기에서 출발하였다.

Method: AMUSE 벤치마크는 복잡한 오디오-비주얼 상호작용을 계획, 근거 및 반영 단계로 분해하는 에이전트적 작업을 필요로 하며, MLLM을 간편하게 평가하기 위해 RAFT라는 프레임워크를 도입하였다.

Result: RAFT를 사용하여 벤치마크에서 정확도를 최대 39.52 개선하였다.

Conclusion: AMUSE와 RAFT는 다중 모달 모델의 에이전트적 추론을 조사하고 그 능력을 향상시킬 수 있는 실용적인 플랫폼을 제공한다.

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [16] [Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems](https://arxiv.org/abs/2512.15922)
*Jovan Pavlović,Miklós Krész,László Hajdu*

Main category: cs.AI

TL;DR: 이 논문에서는 자동으로 구축된 지식 그래프를 통해 정보 검색 성능을 향상시키는 새로운 RAG 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: RAG 시스템이 복잡한 추론 작업에 필요한 다단계 증거를 신뢰성 있게 검색하고 연결하는 데 어려움을 겪고 있다.

Method: 스프레딩 활성화 알고리즘을 사용하여 자동으로 구축된 지식 그래프를 통해 문서 집합에서 정보를 검색하는 새로운 RAG 프레임워크를 제안한다.

Result: 우리의 방법이 반복적인 RAG 방법론에 비해 더 나은 또는 유사한 성과를 달성함을 보여준다.

Conclusion: 체인 오브 씽크 반복 검색 방식과 결합 시, 단순한 RAG에 비해 최대 39%의 정답 정확도 절대 증가를 기록하였다.

Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.

</details>


### [17] [Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning](https://arxiv.org/abs/2512.15943)
*Polaris Jhandi,Owais Kazi,Shreyas Subramanian,Neel Sendas*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLM)을 소형 언어 모델(SLM)로 대체하는 가능성을 탐구하며, SLM의 최적화를 통해 비용 효율성과 운영 효율성을 높이는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 조직이 생성적 AI의 채택을 확대함에 따라 모델 비용 최적화와 운영 효율성이 지속 가능성과 접근성을 결정하는 중요한 요소로 부각되고 있습니다.

Method: 우리는 대표적인 작업을 수행하기 위해 도메인 적응된 SLM을 훈련했습니다. 여기에는 문서 요약, 쿼리 응답 및 구조적 데이터 해석이 포함됩니다. 실험의 일환으로 Hugging Face TRL을 사용하여 facebook/opt-350m 모델의 파인 튜닝을 조사했습니다.

Result: 실험 결과, 우리가 파인 튜닝한 SLM은 ToolBench 평가에서 77.55%의 합격률을 달성하여 ChatGPT-CoT(26.00%), ToolLLaMA-DFS(30.18%), ToolLLaMA-CoT(16.27%)를 포함한 모든 기준 모델을 크게 초월하는 성과를 보였습니다.

Conclusion: SLM의 사려 깊은 설계와 목표 지향적 훈련이 채택 장벽을 크게 낮춰주는 점을 강조하며, 생성적 AI의 비용 효율적 대규모 통합을 가능하게 한다는 것을 입증했습니다.

Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.

</details>


### [18] [Subjective functions](https://arxiv.org/abs/2512.15948)
*Samuel J. Gershman*

Main category: cs.AI

TL;DR: 이 논문은 주관적 함수의 개념을 통해 목표 선택과 인공지능의 목표 기능 생성 능력을 탐구한다.


<details>
  <summary>Details</summary>
Motivation: 목표 함수의 출처와 목표 선정 과정에 대한 이해가 필요하다.

Method: 주관적 함수라는 개념을 도입하고, 기대 예측 오차를 주관적 함수의 구체적 예로 연구한다.

Result: 제안된 접근 방식이 심리학, 신경과학 및 기계 학습의 여러 아이디어와 많은 연결점이 있음을 보여준다.

Conclusion: 인공지능 시스템에 인간과 유사한 목표 함수 생성 능력을 부여할 가능성을 제안한다.

Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.

</details>


### [19] [Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting](https://arxiv.org/abs/2512.16022)
*Defu Cao,Michael Gee,Jinbo Liu,Hengxuan Wang,Wei Yang,Rui Wang,Yan Liu*

Main category: cs.AI

TL;DR: 이 논문은 시간 시계열 예측을 위한 최적의 앙상블 모델 구성을 위해 Large Language Models(LLMs)를 재배치하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 기초 모델의 증가로 인해 일관된 우위를 가진 단일 방법이 없으며, 최적의 앙상블 구성과 해석 가능성이 주요 도전 과제가 되고 있습니다.

Method: LLM을 지능적인 판별자로 재배치하여 기초 모델 앙상블을 평가하고 설명하며 전략적으로 조정하도록 하고, SHAP 기반 신뢰성 점수에 따라 R1 스타일의 미세 조정 과정을 도입합니다.

Result: 우리의 접근 방식은 23개의 데이터 세트와 97개의 설정에서 GIFT-Eval 벤치마크에서 검증되었으며, CRPS와 MASE 메트릭에서 선도적인 시간 시계열 기초 모델보다 현저히 뛰어난 성능을 보여 주었습니다.

Conclusion: 이 방법은 새로운 최첨단 성과를 달성하며 시간 시계열 예측의 가능성을 열어줍니다.

Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

</details>


### [20] [WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning](https://arxiv.org/abs/2512.16108)
*Wendong Bi,Yirong Mao,Xianglong Liu,Kai Tian,Jian Zhang,Hanjie Wang,Wenhui Que*

Main category: cs.AI

TL;DR: WeMusic-Agent는 음악 추천 시스템을 위한 훈련 프레임워크로, 사용자 선호와 음악적 맥락을 이해하여 효과적인 대화형 음악 추천을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 대화형 음악 추천에서 사용자 선호와 음악적 맥락에 대한 깊이 있는 이해가 필요하지만, 기존 방법들은 전문 지식과 유연한 도구 통합의 균형을 맞추는 데 어려움을 겪는다.

Method: WeMusic-Agent는 내부 지식 내재화와 에이전트 경계 학습을 통합하여 모델이 언제 내부 지식을 활용하고 언제 전문 도구(예: 음악 검색 API, 음악 추천 시스템)를 호출할지를 결정하도록 훈련하는 프레임워크이다.

Result: WeMusic-Agent-M1은 500억 음악 관련 코퍼스에 대한 지속적인 프리트레이닝을 통해 방대한 음악 지식을 내재화하고 필요 시 외부 도구를 호출할 수 있는 능력을 습득한 에이전트 모델을 제시한다.

Conclusion: 실제 데이터에 대한 실험 결과, WeMusic-Agent는 기존 모델에 비해 유의미한 향상을 달성했다.

Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.

</details>


### [21] [Science Consultant Agent](https://arxiv.org/abs/2512.16171)
*Karthikeyan K,Philip Wu,Xin Tang,Alexandre Alves*

Main category: cs.AI

TL;DR: Science Consultant Agent는 AI 기반 솔루션을 위한 효과적인 모델링 전략을 선택하고 구현하는 데 도움을 주는 웹 기반 AI 도구이다.


<details>
  <summary>Details</summary>
Motivation: AI 기반 솔루션의 효과적인 모델링 전략을 선택하고 구현하는 과정에서의 어려움을 해결하기 위해 개발되었다.

Method: 구조화된 설문지, 문헌 기반 솔루션 추천, 프로토타입 생성 기능을 통해 작동한다.

Result: Product Manager, Software Developer 및 Researcher가 포함된 사용자들이 더 빠르고 효율적으로 개발할 수 있도록 지원한다.

Conclusion: Science Consultant Agent는 AI 솔루션 개발 속도를 높이고 다양한 비즈니스 분야의 전문가들에게 유용하다.

Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.

</details>


### [22] [PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving](https://arxiv.org/abs/2512.16214)
*Jianming Liu,Ren Zhu,Jian Xu,Kun Ding,Xu-Yao Zhang,Gaofeng Meng,Cheng-Lin Liu*

Main category: cs.AI

TL;DR: PDE-Agent는 LLM 기반의 에이전트를 이용하여 자연어 설명으로부터 자동으로 부분 미분 방정식을 해결하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: PDE 해결은 공학 및 과학 연구의 중요한 기초로, 기존 방법은 전문가 지식에 의존하여 번거롭다.

Method: 이 연구에서는 PDE 해결을 LLM 기반 에이전트를 통한 도구 호출로 구성하고, 다중 에이전트 협업을 지원하는 PDE-Agent를 도입한다.

Result: PDE-Agent는 다중 에이전트 및 다중 도구 협업을 통해 PDE 문제를 효과적으로 해결하며, 다양한 평가에서 높은 성능을 보여준다.

Conclusion: PDE-Agent는 자동화된 과학 컴퓨팅의 미래 발전을 위한 새로운 패러다임을 제시한다.

Abstract: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.

</details>


### [23] [Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis](https://arxiv.org/abs/2512.16237)
*Zhi Helu,Huang Jingjing,Xu Wang,Xu Yangbin,Zhang Wanyue,Jiang Baoyang,Deng Shirui,Zhu Liang,Li Fangfang,Zhao Tiejun,Lin Yankai,Yao Yuan*

Main category: cs.AI

TL;DR: SPRITE는 템플릿 기반 데이터셋의 한계를 극복하고 고품질의 공간 추론 데이터를 생성하기 위해 LLM을 활용하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 현재 모델의 제한된 공간 이해 및 추론 능력으로 인해 구현된 지능의 발전이 제한되고 있다.

Method: SPRITE는 시뮬레이터와 대규모 모델을 활용하여 스케일 가능하고 다양한 고품질의 공간 추론 데이터를 프로그램 방식으로 합성하는 프레임워크다. LLM을 사용하여 복잡한 공간 질문을 실행 가능한 프로그램으로 변환하고 이를 시뮬레이터에서 추출한 고정밀 장면 메타 정보와 비교하여 검증한다.

Result: VLM이 우리의 데이터로 훈련되었을 때 여러 공간 벤치마크에서 성능이 크게 향상되고, 동등한 크기의 다른 오픈 소스 데이터셋보다 뛰어난 성능을 보였다.

Conclusion: 전통적인 템플릿 방법의 낮은 다양성을 극복하는 것이 강력하고 일반화 가능한 공간 지능 구축을 위해 필수적이라는 가설을 검증하였다. 우리가 개발한 SPRITE 프레임워크 코드와 전체 30만 개 이상의 데이터셋은 공간 지능 연구를 촉진하기 위해 공개할 예정이다.

Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.

</details>


### [24] [AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints](https://arxiv.org/abs/2512.16245)
*Aniruddha Roy,Jyoti Patel,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: AlignMerge는 대형 언어 모델(LLMs)의 정렬을 보존하며 통합하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 정렬을 보존하며 다수의 세분화된 체크포인트의 능력을 결합할 필요가 있다.

Method: AlignMerge는 지오메트리 인식을 통해 정렬을 명시적인 불변량으로 만들어, 피셔 차트에서 정렬 서브스페이스를 추정하고 최적화한다.

Result: AlignMerge는 여러 모델 가족에서 정렬 메트릭을 개선하며, 최고의 전문가와 비슷하거나 더 나은 성능을 보인다.

Conclusion: 정렬을 보존하는 통합은 주요 설계 목표가 되어야 하며, 향후 기초 모델의 지오메트리 인식 구성을 위한 경로를 제시한다.

Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.

</details>


### [25] [Learning to Wait: Synchronizing Agents with the Physical World](https://arxiv.org/abs/2512.16262)
*Yifei She,Ping Zhang,He Liu,Yanmin Jia,Yang Jing,Zijun Liu,Peng Sun,Xiangbin Li,Xiaohe Hu*

Main category: cs.AI

TL;DR: 이 연구는 비동기 환경에서 대기 시간을 예측하는 에이전트 측 접근 방식을 제안하며, 이는 대규모 언어 모델이 물리적 세계와 인지 타임라인을 동기화할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 실제 에이전틱 작업은 비동기성 행동과 가변 대기 시간을 포함하므로, 행동 시작과 완료 사이에 시간적 간극이 발생한다. 기존 솔루션은 확장성을 제한하거나 에이전트의 맥락 창을 희석시킨다.

Method: 대규모 언어 모델이 코드를 행동으로 확장하고 인지 학습을 활용하여 정확한 대기 시간 예측을 가능하게 하는 에이전트 측 접근 방식을 제안한다.

Result: 시뮬레이션된 Kubernetes 클러스터에서의 실험을 통해, 에이전트가 내부 시계를 정밀하게 조정하여 쿼리 오버헤드 및 실행 지연을 최소화할 수 있음을 보여준다.

Conclusion: 시간 인식은 개방형 환경에서 자율적인 진화를 위한 배울 수 있는 능력으로, 필수적이라는 것을 검증한다.

Abstract: Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.

</details>


### [26] [QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems](https://arxiv.org/abs/2512.16279)
*Yiliu Yang,Yilei Jiang,Qunzhong Wang,Yingshui Tan,Xiaoyong Zhu,Sherman S. M. Chow,Bo Zheng,Xiangyu Yue*

Main category: cs.AI

TL;DR: 	extsc{QuadSentinel}은 대형 언어 모델 기반의 에이전트들이 복잡한 작업을 수행할 때의 안전 위험을 줄이기 위해 제안된 네 개의 에이전트로 구성된 시스템입니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 기반 에이전트들이 도구와 다단계 계획, 상호 에이전트 메시지로 복잡한 작업을 수행하면서 발생하는 안전 위험을 줄이는 필요성이 있습니다.

Method: 안전 정책을 시퀀스로 표현하고, 이 정책을 관찰 가능한 상태에 대한 술어로 구성된 기계 검증 가능한 규칙으로 컴파일하는 네 개의 에이전트(상태 추적기, 정책 검증기, 위협 감시기, 중재자) 	extsc{QuadSentinel}을 제안합니다.

Result: 	extsc{QuadSentinel}은 ST-WebAgentBench 및 AgentHarm에서 가드레일 정확성과 규칙 재현율을 개선하면서 위양성을 줄였습니다.

Conclusion: 단기 배치에서는 정책을 별도로 유지하고 기계 검증 가능하게 하여 핵심 에이전트를 수정하지 않고도 이러한 패턴을 채택할 수 있습니다.

Abstract: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.

</details>


### [27] [OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models](https://arxiv.org/abs/2512.16295)
*Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Qiushi Sun,Zhaoyang Liu,Zhoumianze Liu,Yu Qiao,Xiangyu Yue,Zun Wang,Zichen Ding*

Main category: cs.AI

TL;DR: 이 논문에서는 GUI 내비게이션 및 조작에 대한 VLM 기반 컴퓨터 사용 에이전트의 결정-making 방식의 문제점을 해결하기 위한 새로운 프레임워크 OS-Oracle을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계에서의 배치를 위한 신뢰할 수 있는 단계별 결정은 중요한 병목 현상으로 나타나고 있으며, 실행 전 각 행동을 평가하는 비평 모델의 필요성이 커지고 있습니다.

Method: OS-Oracle은 (1) 플랫폼 간 GUI 비평 데이터 합성을 위한 확장 가능한 데이터 파이프라인, (2) 감독 세부 조정(SFT)과 일관성 유지 그룹 상대 정책 최적화(CP-GRPO)를 결합한 이단계 훈련 패러다임, (3) 모바일, 웹, 데스크탑 플랫폼 전반에 걸쳐 비평 모델 성능 평가를 위한 OS-Critic Bench를 제공합니다.

Result: OS-Oracle-7B 모델은 OS-Critic Bench에서 오픈 소스 VLM 중 가장 뛰어난 성능을 보이며, 모바일 도메인에서 독점 모델을 초월합니다.

Conclusion: OS-Oracle-7B는 본 작업의 성능 향상뿐만 아니라, OSWorld 및 AndroidWorld 환경에서 UI-TARS-1.5-7B와 같은 네이티브 GUI 에이전트의 성능을 개선합니다.

Abstract: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.

</details>


### [28] [Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection](https://arxiv.org/abs/2512.16300)
*Fanrui Zhang,Qiang Zhang,Sizhuo Zhou,Jianwen Sun,Chuanhao Li,Jiaxin Ai,Yukang Feng,Yujie Zhang,Wenjie Li,Zizhen Li,Yifan Chang,Jiawei Liu,Kaipeng Zhang*

Main category: cs.AI

TL;DR: ForenAgent는 멀티 모달 대형 언어 모델을 활용하여 이미지 위조 탐지 성능을 향상시키는 다단계 상호작용 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 이미지 위조 탐지 방법은 저수준 아티팩트 또는 고수준 의미 지식을 갖춘 다중 모달 대형 언어 모델에 의존하여 이 두 정보 흐름을 효과적으로 통합하는 데 어려움이 있다.

Method: ForenAgent는 Python 기반 저수준 도구를 생성, 실행 및 반복적으로 개선하는 다단계 상호작용 IFD 프레임워크로, 도구 상호작용 능력과 추론 적응성을 증진시키기 위한 두 단계 훈련 파이프라인을 따른다.

Result: ForenAgent는 저수준 도구의 도움을 받아 어려운 IFD 작업에서 도구 사용 능력과 반사적 추론을 보여줌으로써 일반 목적의 IFD를 향한 유망한 경로를 제시한다.

Conclusion: FABench라는 100,000개의 이미지와 약 200,000개의 에이전트 상호작용 질문-답변 쌍을 포함한 데이터셋을 구축하여 체계적인 훈련과 평가를 위한 기초를 마련하였다.

Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.

</details>


### [29] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 이 논문은 에이전트 AI 시스템의 성능을 개선하기 위한 적응 메커니즘을 체계적인 프레임워크로 통합하고, 다양한 적응 전략의 설계 공간을 명확히 하여 실용적인 지침을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 AI 시스템의 성능, 신뢰성, 일반화 개선을 위한 적응 메커니즘의 필요성.

Method: 에이전트 적응과 도구 적응을 포함하는 체계적인 프레임워크로 통합하고, 이를 도구 실행 신호 및 에이전트 출력 신호 형태로 분해.

Result: 적응 전략의 설계 공간을 명확히 하고, 전략 간의 트레이드 오프를 명시하며, 시스템 설계 과정에서 전략 선택이나 전환을 위한 실용적 지침을 제공한다.

Conclusion: 더 능력 있고 효율적이며 신뢰할 수 있는 에이전트 AI 시스템을 구축하려는 연구자 및 실무자에게 개념적 기초와 실용적인 로드맵 기여.

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [30] [AI Needs Physics More Than Physics Needs AI](https://arxiv.org/abs/2512.16344)
*Peter Coveney,Roger Highfield*

Main category: cs.AI

TL;DR: 인공지능(AI)은 변화의 원동력으로 여겨지지만, 현재까지 그 영향력은 제한적이다. 이 논문은 현재의 AI가 물리학에 영향을 줄 수 있지만, 물리학이 AI 발전에 더 기여할 수 있다고 주장한다.


<details>
  <summary>Details</summary>
Motivation: AI의 변혁적인 잠재력을 인식하면서도, 과거 10년간의 성과에 비해 그 영향력이 미미하다는 점을 지적하기 위해.

Method: AI의 현재 아키텍처의 한계와 비판을 검토하고, 양자 AI 및 아날로그 컴퓨팅의 기회를 강조하며, '빅 AI'의 채택을 위한 로드맵을 제시한다.

Result: AI 기술이 물리학에 미치는 영향에서 AI가 물리학으로부터 얻을 수 있는 잠재력을 강조한다.

Conclusion: AI는 물리학의 이론적 엄격함과 기계 학습의 유연성을 통합함으로써 더욱 발전할 수 있다.

Abstract: Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.

</details>


### [31] [StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm](https://arxiv.org/abs/2512.16444)
*Yadong Li,Tong Zhang,Bo Huang,Zhen Cui*

Main category: cs.AI

TL;DR: 본 연구는 MARL 알고리즘의 벤치마킹을 새롭게 할 수 있는 SC2BA 환경을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다양성과 다재다능성을 제한하는 고정된 AI 모드로 인해 MARL 알고리즘 평가에 문제가 있다.

Method: SC2BA 환경을 구축하고, APyMARL 라이브러리를 개발하여 여러 대결 모드에서 알고리즘을 평가한다.

Result: 기존의 MARL 알고리즘을 두 가지 대결 모드에서 벤치마크하며, 그 과정에서 알고리즘의 효과성 및 확장성에 대한 흥미로운 결과를 관찰했다.

Conclusion: SC2BA 환경과 실험 결과는 MARL 분야의 새로운 발전을 이끌 것으로 기대된다.

Abstract: Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.

</details>


### [32] [Towards AI-Supported Research: a Vision of the TIB AIssistant](https://arxiv.org/abs/2512.16447)
*Sören Auer,Allard Oelen,Mohamad Yaser Jaradeh,Mutahira Khalid,Farhana Keya,Sasi Kiran Gaddipati,Jennifer D'Souza,Lorenz Schlüter,Amirreza Alasti,Gollam Rabby,Azanzi Jiomekong,Oliver Karras*

Main category: cs.AI

TL;DR: Generative AI와 대규모 언어 모델의 발전이 연구 방식을 혁신할 가능성을 보여주지만, AI 통합에 대한 도전이 여전히 존재한다. TIB AIssistant 플랫폼은 연구자들이 과학적 발견을 지원하기 위한 협력적 도구를 제공한다.


<details>
  <summary>Details</summary>
Motivation: Generative AI와 대규모 언어 모델의 빠른 발전은 연구 수행 방식을 전환할 가능성을 제공하지만, 도메인 요구 사항의 차이와 AI 활용 능력 부족, 도구 및 에이전트 조정의 복잡성, Generative AI의 연구 정확성에 대한 불확실성 때문에 AI 통합이 어렵다.

Method: TIB AIssistant는 연구 생애 주기 전반에 걸쳐 연구자들을 지원하기 위해 설계된 일반적인 인공지능 보조 플랫폼으로, 모듈화된 구성 요소(프롬프트 및 도구 라이브러리, 공유 데이터 저장소, 유연한 조정 프레임워크)를 제공한다.

Result: 이 플랫폼은 아이디어 발상, 문헌 분석, 방법론 개발, 데이터 분석 및 학문적 저작을 용이하게 한다. 초기 프로토타입의 구현을 설명하며, 접근 방식의 실행 가능성과 잠재적 영향을 보여준다.

Conclusion: 이 플랫폼은 다양한 학문 분야에서 연구자들이 조화롭게 협력하여 과학적 발견을 이루도록 돕는 것을 목표로 한다.

Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.

</details>


### [33] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: cuPilot은 전략 조정 다중 에이전트 프레임워크로, CUDA 커널 최적화를 위한 새로운 접근법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: CUDA 커널 최적화는 하드웨어-소프트웨어 공동 설계 전문성과 고성능 커널 라이브러리의 독점적인 특성으로 인해 도전적이고 노동집약적인 작업입니다.

Method: 이 연구는 cuPilot이라는 전략 조정 다중 에이전트 프레임워크를 제안하여 커널 진화를 위한 중간 의미 표현으로 전략을 도입합니다.

Result: cuPilot로 생성된 커널은 100개의 커널 벤치마크에서 PyTorch보다 평균 3.09배의 속도 향상을 달성했습니다.

Conclusion: cuPilot은 복잡한 최적화를 선보이며, 중요한 하드웨어 유닛의 높은 활용도를 달성했습니다. 생성된 커널은 https://github.com/champloo2878/cuPilot-Kernels.git에서 오픈소스로 제공됩니다.

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [34] [ParamExplorer: A framework for exploring parameters in generative art](https://arxiv.org/abs/2512.16529)
*Julien Gachadoat,Guillaume Lagarde*

Main category: cs.AI

TL;DR: Generative art의 매개변수 공간 탐색을 지원하는 ParamExplorer라는 인터랙티브한 모듈형 프레임워크를 소개하고, 이 프레임워크 내에서 여러 탐색 전략을 구현 및 평가한다.


<details>
  <summary>Details</summary>
Motivation: Generative art 시스템에서는 미적으로 매력적인 결과물이 고차원적이고 복잡한 매개변수 공간의 작은 단편적인 영역에만 위치하고 있어 수동적인 시행착오가 필요하다.

Method: ParamExplorer 프레임워크를 도입하고, 인간 피드백 또는 자동화된 피드백에 따라 매개변수 공간을 탐색할 수 있도록 하며, 기존의 p5.js 프로젝트와 통합된다.

Result: 이 프레임워크 내에서 여러 탐색 전략(에이전트)을 구현하고 평가한다.

Conclusion: ParamExplorer는 Generative art 알고리즘의 매개변수 공간 효율 탐색에 기여한다.

Abstract: Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.

</details>


### [35] [From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment](https://arxiv.org/abs/2512.16532)
*Himanshu Gharat,Himanshi Agrawal,Gourab K. Patro*

Main category: cs.AI

TL;DR: 메모리 강화 개인화를 통해 LLM이 편향을 초래하고 강화하는 방법을 연구하여 이와 관련된 위험성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 메모리 강화 개인화는 AI 에이전트의 상호작용 지속성과 행동 및 응답의 관련성을 향상시키는 데 기여하지만, 이 과정에서 편향의 위험이 존재한다.

Method: 채용 사례를 사용하여 메모리 강화 개인화 에이전트의 행동을 시뮬레이션하고 편향이 도입되고 강화되는 방식을 분석했다.

Result: 안전 교육이 된 LLM을 사용하는 에이전트에 대한 실험을 통해, 개인화가 편향을 체계적으로 도입하고 강화한다는 것을 발견했다.

Conclusion: 편향의 위험성을 줄이기 위해 LLM 기반 AI 에이전트에 대한 추가적인 보호 조치나 가드레일이 필요하다.

Abstract: Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.

</details>


### [36] [Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild](https://arxiv.org/abs/2512.16553)
*Yumeng Wang,Tianyu Fan,Lingrui Xu,Chao Huang*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)은 복잡한 웹 콘텐츠를 탐색하고 추론할 수 있는 고급 에이전트로 발전했습니다. 그러나 기존 벤치마크는 모호하고 탐색적 쿼리의 필요성을 간과했습니다. 이를 해결하기 위해, 우리는 Needle in the Web을 도입하여 LLM 기반 시스템의 성능을 평가합니다.


<details>
  <summary>Details</summary>
Motivation: 모호하고 탐색적인 쿼리에서 현대 검색 에이전트와 LLM 기반 시스템의 능력을 평가하는 새로운 벤치마크의 필요성이 있습니다.

Method: Needle in the Web은 663개의 질문으로 구성되어 있으며, 웹 콘텐츠의 사실 주장에 기반한 조절 가능한 난이도의 쿼리를 생성하는 유연한 방법론을 사용합니다.

Result: 세 가지 주요 LLM과 세 가지 에이전트 기반 검색 시스템을 평가한 결과, 대부분의 모델은 35% 미만의 정확도를 보였으며, 어떤 모델도 일관되게 뛰어난 성능을 발휘하지 못했습니다.

Conclusion: 이 결과는 Needle in the Web이 현재 검색 시스템에 큰 도전 과제가 되며, 의미적 모호성 하에서 효과적인 모호한 검색의 문제가 여전히 열려 있음을 보여줍니다.

Abstract: Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.

</details>


### [37] [Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam](https://arxiv.org/abs/2512.16644)
*Wisnu Uriawan,Aria Octavian Hamza,Ade Ripaldi Nuralim,Adi Purnama,Ahmad Juaeni Yunus,Anissya Auliani Supriadi Putri*

Main category: cs.AI

TL;DR: 이 연구는 이슬람 질문 상담을 위한 상호 작용형 매체로서 Sharia 준수 챗봇을 구현한 내용을 제시하며, Q-Learning을 통합한 Sentence-Transformers를 활용하여 맥락적이고 정확한 응답을 보장한다.


<details>
  <summary>Details</summary>
Motivation: 디지털 시대의 이슬람 지식 접근성을 향상시키고자 하는 필요와 전통적인 이슬람 학문의 현대적인 상담 도구로 활용할 수 있는 가능성.

Method: CRISP-DM 방법론을 활용하여, Qur'an, Hadith 및 학술적인 파트와 같은 신뢰할 수 있는 출처에서 수집한 25,000개의 질문-답변 쌍으로 구성된 Islam QA 데이터셋을 JSON 형식으로 처리하며, Flask API 백엔드와 Flutter 기반 모바일 프론트엔드를 개발하여 챗봇 프로토타입을 구현.

Result: 다양한 주제에 대해 87%의 의미적 정확성을 달성하며, fiqh, aqidah, ibadah 및 muamalah을 포함한 주제에 대해 효과적인 기능 테스트 결과를 입증하였다.

Conclusion: 이 혁신은 전통적인 이슬람 학문과 현대 AI 기반 상담 간의 다리를 놓는 역할을 하며, 제한된 도메인 쿼리에는 효과적이지만 정적 학습 및 데이터셋 의존성과 같은 한계를 가지고 있어 지속적인 적응 및 다중 턴 대화 지원과 같은 향후 개선 기회를 강조한다.

Abstract: This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.

</details>


### [38] [Comprehensive AI Literacy: The Case for Centering Human Agency](https://arxiv.org/abs/2512.16656)
*Sri Yash Tadimalla,Justin Cary,Gordon Hull,Jordan Register,Daniel Maxwell,David Pugalee,Tina Heafner*

Main category: cs.AI

TL;DR: 인공지능 기술의 빠른 통합이 교육적 필연성을 초래했지만, 현재의 교육 시스템은 이를 효과적으로 해결하지 못하고 있다. 본 논문은 인간의 주체성에 중심을 둔 포괄적인 AI 문해력으로의 체계적 전환을 주장한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능 기술의 급속한 통합으로 인해 교육 체계가 그 필요를 효과적으로 충족하지 못하고 있어, 문해력의 격차가 생겨나는 상황을 해결하고자 한다.

Method: AI 문해력, 유창성 및 역량 프레임워크를 통해 교육자와 학생들이 AI에 대한 인간 중심 접근 방식을 개발하도록 지원한다.

Result: 교육자와 학생들은 자신의 경험에 맞춘 AI 접근 방식을 통해 AI 사용에 대한 의식적 선택을 할 수 있게 된다. 이를 통해 교육 현장에서 AI에 대한 결정과 태도의 의도를 명확히 할 수 있는 경로를 제공한다.

Conclusion: AI 문해력이 진정한 교육의 일환으로 자리 잡기 위해서는 비판적 사고와 인식론에 대한 깊은 이해가 필요하다.

Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.

</details>


### [39] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 다이어그램 기반 기하 문제 해결은 멀티모달 대형 언어 모델(MLLMs)의 중요한 평가 기준이며, 단일 에이전트 대비 다중 에이전트 설계의 이점이 분명하지 않다. 이 연구는 여러 시각적 수학 벤치마크에서 단일 에이전트와 다중 에이전트 파이프라인을 체계적으로 비교한다.


<details>
  <summary>Details</summary>
Motivation: 다양한 벤치마크에서 단일 에이전트 및 다중 에이전트 시스템의 성능 차이를 이해하고, 다중 에이전트 설계의 잠재적 이점을 탐구하기 위해.

Method: Geometry3K, MathVerse, OlympiadBench, We-Math의 네 가지 시각적 수학 벤치마크에 대해 단일 에이전트 및 다중 에이전트 파이프라인을 비교하였다.

Result: 오픈 소스 모델에서는 다중 에이전트가 지속적으로 성능을 향상시켰으며, 예를 들어 Qwen-2.5-VL (7B)는 Geometry3K에서 +6.8 포인트, Qwen-2.5-VL (32B)는 +3.3 포인트를 얻었다. 반면, 클로즈드 소스 모델인 Gemini-2.0-Flash는 전통적인 벤치마크에서 단일 에이전트 모드가 더 나은 성능을 보였다.

Conclusion: 이 연구는 오픈 소스 모델에 대한 다중 에이전트 파이프라인의 명확한 이점을 보여주며, 강력한 사유 시스템이 새로운 벤치마크에서 지원을 받을 수 있음을 시사하지만, 에이전트 분해가 보편적으로 최적이라는 것은 아니다.

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [40] [Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences](https://arxiv.org/abs/2512.16701)
*Giovanni Adorni*

Main category: cs.AI

TL;DR: 이 논문은 인공지능 기술이 교육에서 인간의 주체성을 복원하기 위한 사이버 휴머니즘의 틀을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: Generative Artificial Intelligence가 교육에서 지식 생산 및 검증 방식을 변화시키고 있으며, 이에 따라 교육자의 전문성이 위협받고 있다는 우려가 있습니다.

Method: 사람과 기계가 공동으로 저작한 사회기술적 인프라로 AI 기반 학습 환경을 개념화하고, 교육자와 학습자를 인식론적 주체 및 알고리즘 시민으로定位합니다.

Result: 사이버 휴머니즘 디자인을 위한 세 가지 기둥을 명시하고, 이를 통해 고등 교육 사례 연구를 제시하며, 지식 주체성을 강화시키면서 발생하는 긴장감을 보여줍니다.

Conclusion: AI 중심의 인간 중심 교육의 미래에 대한 시사점을 제시합니다.

Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\-lisation of teachers. This paper proposes \emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.
  We articulate three pillars for cyber-humanist design, \emph{reflexive competence}, \emph{algorithmic citizenship}, and \emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \emph{prompt-based learning} and a new \emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.

</details>


### [41] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 이 논문은 알고리즘적 지능을 제약하는 두 가지 독립적인 계산적 한계를 정형화한다.


<details>
  <summary>Details</summary>
Motivation: 알고리즘적 지능의 한계를 이해하고, 지능 시스템 내에서 추론, 예측 및 자기 분석 간의 본질적인 상충 관계를 명확히 하기 위해 이 연구를 수행한다.

Method: 형식적 불완전성과 역학적 예측 불가능성을 각각 정의하여 알고리즘적 에이전트의 예측 능력에 대한 구조적 경계를 제시한다.

Result: 형식적 불완전성이 일관된 추론 시스템의 공제력을 제한하고, 역학적 예측 불가능성이 유한한 정밀도 하에서 장기 예측을 제약함을 보여준다.

Conclusion: 알고리즘적 에이전트는 일반적으로 자신의 최대 예측 호라이즌을 계산할 수 없음을 설명하며, 이는 지능 시스템 내에서의 추론과 예측 간의 상충을 드러낸다.

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [42] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 대안적인 AGI 출현 가설이 필요하며, 이를 바탕으로 안전 장치 개발이 필요하다.


<details>
  <summary>Details</summary>
Motivation: 기존 AI 안전 및 정렬 연구는 개별 AI 시스템 보호 방법에 집중되어 있으며, 단일 AGI의 출현을 전제로 하고 있다.

Method: 가상 에이전트 샌드박스 경제 설계를 제안하며, 이를 통해 에이전트 간 거래를 시장 메커니즘으로 관리한다.

Result: 제안된 프레임워크는 집단적 위험 완화를 위한 강력한 감사, 평판 관리 및 감독을 포함한다.

Conclusion: 발전된 AI 에이전트의 빠른 배치로 인해 이러한 안전 고려 사항이 긴급해졌다.

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction](https://arxiv.org/abs/2512.15751)
*Wei Guan,Jian Cao,Jinyu Cai,Qiqi Cai,Jianqi Gao,See-Kiong Ng*

Main category: cs.LG

TL;DR: GLOW라는 새로운 프레임워크가 복잡한 작업을 위한 Agentic Workflows(AWs)의 성능 예측을 개선한다.


<details>
  <summary>Details</summary>
Motivation: Agentic Workflows(AWs)는 복잡한 작업을 해결하기 위한 유망한 패러다임으로, 자동 생성의 확장성이 실행 기반 평가의 높은 비용과 지연으로 제약받고 있다.

Method: GLOW는 GNN의 그래프 구조 모델링 능력과 LLM의 추론 능력을 결합하여 AW 성능 예측을 위한 통합 프레임워크를 제안한다. 특히, 그래프 작업에 맞춰 튜닝된 그래프 지향 LLM을 도입하여 위상적으로 인지된 의미적 특징을 추출하고, 이를 GNN 인코딩된 구조 표현과 융합한다.

Result: FLORA-Bench에 대한 광범위한 실험 결과, GLOW는 예측 정확도와 순위 유용성에서 최첨단 기준선을 능가한다.

Conclusion: GLOW는 AW 성능 예측의 정밀도를 높이며 복잡한 AW를 효과적으로 구분할 수 있는 잠재 공간을 정제한다.

Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.

</details>


### [44] [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)
*A. Quadir,M. Sajid,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 멀티뷰 학습(MVL)은 여러 관점에서 정보를 활용하여 일반화 성능을 향상시키는 머신 러닝의 새로운 분야이다. 본 연구에서는 전통적인 커널 기반 접근법의 주요 문제를 해결하는 멀티뷰 트윈 제한 커널 머신(TMvRKM)을 제안하며, 이 모델은 계산 효율성과 분류 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 멀티뷰 학습은 여러 관점이나 뷰에서의 보완 정보를 활용하여 일반화 성능을 향상시키는 데 초점을 맞추고 있으며, 이를 통해 다양한 문제를 해결하고자 한다.

Method: TMvRKM은 커널 기계의 장점을 멀티뷰 프레임워크와 통합하여 전통적인 커널 기반 접근법과 관련된 계산적 및 일반화의 문제를 해결하는 새로운 모델이다. 이 모델은 정규화 최소 제곱 접근법을 통해 최적의 분리 하이퍼플레인을 효율적으로 결정한다.

Result: TMvRKM은 UCI, KEEL 및 AwA 벤치마크 데이터셋에서 rigorously tested 되었으며, 실험 결과 및 통계 분석에서 모든 시나리오에서 기준 모델보다 우수한 일반화 성능을 보여준다.

Conclusion: TMvRKM은 여러 뷰의 데이터를 효과적으로 처리하며, 각 뷰의 변동성에 유연성을 유지하면서 모든 뷰에서의 집합 정보를 활용한다.

Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.

</details>


### [45] [Yantra AI -- An intelligence platform which interacts with manufacturing operations](https://arxiv.org/abs/2512.15758)
*Varshini Krishnamurthy*

Main category: cs.LG

TL;DR: 이 논문은 XRIT를 위한 지능형 생산 시스템의 개발과 테스트에 중점을 두고 있으며, 에너지 관리, 예측 유지보수 및 AI 지원 의사결정 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: Industry 4.0의 발전은 스마트 생산 방식을 변화시키고 있으며, 이에 따라 실시간 추적, 기계 학습 및 AI 기반 시스템을 활용하여 운영을 원활하게 하는 것이 중요해졌습니다.

Method: 논문에서는 재한(Random Forest Classifier) 및 고립 숲(Isolation Forest)과 같은 기계 학습 모델을 시스템에 통합하여 예측 유지보수 및 이상 탐지를 지원하고, Streamlit을 활용하여 실시간 데이터 시각화를 가능하게 하였습니다.

Result: 시스템은 가상의 데이터를 사용하여 테스트되었으며, AI 기반의 가상 도우미인 GPT-4를 추가하여 작업 효율성, 에너지 관리 및 수리 계획 능력을 향상시키는 것으로 나타났습니다.

Conclusion: 향후 작업은 시스템을 실시간 데이터 통합으로 이전하고 개선할 방법을 모색하는 데 중점을 둘 것입니다.

Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.

</details>


### [46] [A Tutorial on Dimensionless Learning: Geometric Interpretation and the Effect of Noise](https://arxiv.org/abs/2512.15760)
*Zhengtao Jake Gan,Xiaoyu Xie*

Main category: cs.LG

TL;DR: 차원 없는 학습은 실험 측정에서 차원 없는 수치와 비례 법칙을 발견하는 데이터 기반 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 실험 데이터를 간결한 물리 법칙으로 변환하는 방법을 설명하며, 변수 간의 간결한 차원 불변성을 드러내기 위해 데이터를 분석하는 중요성을 강조한다.

Method: 고전 차원 분석과 현대 기계 학습 기법을 결합하여 물리량의 측정값에서 변수들을 차원 없는 그룹으로 결합하는 근본적인 방법을 식별하고, 신경망을 사용하여 실험 출력을 가장 잘 예측하는 조합을 발견한다.

Result: 측정 노이즈와 이산 샘플링이 발견 과정에 미치는 영향을 체계적으로 조사하며, 정규화 접근 방식이 실험적 불확실성에 대한 강인성을 제공함을 보여준다.

Conclusion: 차원 없는 수치가 하나 또는 여러 개 있는 경우에도 잘 적용되며, 서로 다른 동등한 표현이 동일한 기본 물리량을 캡처할 수 있음을 드러낸다. 그러나 여러 차원 없는 그룹을 식별하는 컴퓨팅 비용 관리, 데이터 특성의 영향 이해, 관련 입력 변수를 자동으로 선택하는 문제, 실험자를 위한 사용자 친화적인 도구 개발 등 여전히 해결해야 할 주요 과제가 있다.

Abstract: Dimensionless learning is a data-driven framework for discovering dimensionless numbers and scaling laws from experimental measurements. This tutorial introduces the method, explaining how it transforms experimental data into compact physical laws that reveal compact dimensional invariance between variables. The approach combines classical dimensional analysis with modern machine learning techniques. Starting from measurements of physical quantities, the method identifies the fundamental ways to combine variables into dimensionless groups, then uses neural networks to discover which combinations best predict the experimental output. A key innovation is a regularization technique that encourages the learned coefficients to take simple, interpretable values like integers or half-integers, making the discovered laws both accurate and physically meaningful. We systematically investigate how measurement noise and discrete sampling affect the discovery process, demonstrating that the regularization approach provides robustness to experimental uncertainties. The method successfully handles cases with single or multiple dimensionless numbers, revealing how different but equivalent representations can capture the same underlying physics. Despite recent progress, key challenges remain, including managing the computational cost of identifying multiple dimensionless groups, understanding the influence of data characteristics, automating the selection of relevant input variables, and developing user-friendly tools for experimentalists. This tutorial serves as both an educational resource and a practical guide for researchers seeking to apply dimensionless learning to their experimental data.

</details>


### [47] [SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks](https://arxiv.org/abs/2512.15938)
*Vegard Flovik*

Main category: cs.LG

TL;DR: 이 연구는 기계적 해석 가능성과 모델 편집을 연결하는 SALVE라는 프레임워크를 소개하며, 해석 가능하고 제어 가능한 AI 시스템 개발에 기여합니다.


<details>
  <summary>Details</summary>
Motivation: 심층 신경망은 인상적인 성능을 보이지만, 해석과 제어가 어렵습니다. 이를 해결하기 위한 방법론이 필요합니다.

Method: 비지도학습을 통해 희소한 모델 고유의 특징 기초를 학습하고, Grad-FAM을 사용하여 이러한 특징을 검증합니다. 또한, 정확하고 지속적인 가중치 조작을 수행하여 클래스 정의 및 교차 클래스 특징을 지속적으로 조절할 수 있습니다.

Result: ResNet-18 및 ViT-B/16 모델에서 일관되게 해석 가능한 제어를 보여주며, 각 클래스의 주요 특징에 대한 의존성을 정량화하는 중요한 억제 임계치 α_{crit}를 도출합니다.

Conclusion: 이 연구는 특징 발견을 실행 가능한 모델 편집으로 전환하는 원칙적인 방법론에 기여하여 투명하고 제어 가능한 AI 시스템 개발을 진전시킵니다.

Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $α_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.

</details>


### [48] [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)
*Ruifeng Xu,Liang He*

Main category: cs.LG

TL;DR: 이 연구는 Nested Dual-Agent Reinforcement Learning(NDRL) 방법을 통해 작물의 수확량과 자원 사용 효율성을 향상시키는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 효과적인 관개와 질소 비료는 작물 수확량에 큰 영향을 미치지만, 기존 연구는 물-질소 조합 최적화의 복잡성과 부정확한 자원 이용으로 인한 한계가 있다.

Method: NDRL 방법을 사용하여, 부모 에이전트는 잠재적인 관개 및 비료 조치를 식별하고, 자식 에이전트의 보상 함수는 물 스트레스 요인(WSF) 및 질소 스트레스 요인(NSF)을 포함하여 일일 전략을 동적으로 최적화한다.

Result: 2023년과 2024년의 실험 결과, 시뮬레이션된 수확량이 각각 4.7% 증가하였고, 관개수 생산성이 5.6%와 5.1% 증가하였으며, 질소 부분 생산성이 6.3%와 1.0% 증가하였다.

Conclusion: 본 연구는 면화 관개 및 질소 비료 개발을 진전시키며, 농업 자원 관리의 복잡성과 정밀성 문제를 해결하기 위한 새로운 아이디어를 제공한다.

Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.

</details>


### [49] [Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models](https://arxiv.org/abs/2512.15973)
*Caner Erden*

Main category: cs.LG

TL;DR: DR-RL은 대형 언어 모델의 다중 헤드 자기 주의의 저차원 분해를 적응적으로 최적화하는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 저차원 근사치는 고정된 순위 가정을 기반으로 하여 유연성이 부족하다.

Method: 강화 학습과 온라인 행렬 섭동 이론을 통합하여 순차적으로 정책 최적화 문제로 순위 선택을 구성한다.

Result: DR-RL은 다운스트림 정확도를 유지하면서 FLOPs를 크게 줄인다.

Conclusion: 이 연구는 MHSA에서 적응적 효율성과 이론적 엄격성 간의 간극을 메우며 자원 제약이 있는 딥러닝에 대한 원칙에 기반한 대안을 제공한다.

Abstract: We propose Dynamic Rank Reinforcement Learning (DR-RL), a novel framework that adaptively optimizes the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models (LLMs) through the integration of reinforcement learning and online matrix perturbation theory. While traditional low-rank approximations often rely on static rank assumptions--limiting their flexibility across diverse input contexts--our method dynamically selects ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation lies in an RL agent that formulates rank selection as a sequential policy optimization problem, where the reward function strictly balances attention fidelity against computational latency. Crucially, we employ online matrix perturbation bounds to enable incremental rank updates, thereby avoiding the prohibitive cost of full decomposition during inference. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern GPU architectures. Experiments demonstrate that DR-RL maintains downstream accuracy statistically equivalent to full-rank attention while significantly reducing Floating Point Operations (FLOPs), particularly in long-sequence regimes (L > 4096). This work bridges the gap between adaptive efficiency and theoretical rigor in MHSA, offering a principled, mathematically grounded alternative to heuristic rank reduction techniques in resource-constrained deep learning. Source code and experiment logs are available at: https://github.com/canererden/DR_RL_Project

</details>


### [50] [Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results](https://arxiv.org/abs/2512.16013)
*Ruolei Zeng,Arun Sharma,Shuai An,Mingzhou Yang,Shengya Zhang,Licheng Liu,David Mulla,Shashi Shekhar*

Main category: cs.LG

TL;DR: 이 연구에서는 기후 완화 및 지속 가능한 농업을 위한 농생태계 탄소 순환의 정확하고 비용 효율적인 정량화의 필요성을 강조하며, FTBSC-KGML이라는 새로운 기계 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기후 완화와 지속 가능한 농업을 위해 의사결정 관련 규모에서 농생태계 탄소 순환을 정확하고 비용 효율적으로 정량화하는 것이 필수적이다.

Method: FTBSC-KGML은 사전 훈련 및 미세 조정 기반의 지식 지향적 기계 학습 프레임워크로, 여러 미드웨스 지역에서 수집한 원격 감지 GPP, 기후 및 토양 공변수를 사용하여 사이트별 매개변수를 통해 영역의 탄소 배출량을 추정한다.

Result: FTBSC-KGML은 순수 글로벌 모델보다 낮은 검증 오류와 더 큰 설명력의 일관성을 달성하여 지역 변동성을 더 잘 포착한다.

Conclusion: 이 연구는 이전 SDSA-KGML 프레임워크를 확장한다.

Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.

</details>


### [51] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-3는 106B 매개변수를 가진 Mixture-of-Experts 모델로, 대규모 강화 학습을 통해 우수한 성능을 달성했습니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 강화 학습을 활용하여 각 분야에서 최고 수준의 성능을 달성하기 위해.

Method: INTELLECT-3는 12B의 활성 매개변수를 가진 Mixture-of-Experts 모델로, RL 인프라 스택을 활용하여 훈련되었습니다.

Result: 수학, 코드, 과학 및 추론 벤치마크에서 크기에 비해 뛰어난 성능을 보였으며, 많은 대형 모델들을 초월했습니다.

Conclusion: 모델과 그 생성을 위한 전체 인프라 스택을 오픈 소스로 제공하며, 다양한 환경을 구축하기 위해 prime-rl을 소개합니다.

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [52] [Pretrained Battery Transformer (PBT): A battery life prediction foundation model](https://arxiv.org/abs/2512.16334)
*Ruifeng Tan,Weixiang Hong,Jia Li,Jiaqiang Huang,Tong-Yi Zhang*

Main category: cs.LG

TL;DR: 배터리 사이클 수명의 조기 예측은 배터리 연구, 제조 및 배치 가속화에 필수적이며, 본 연구는 최초의 배터리 수명 예측을 위한 기초 모델인 Pretrained Battery Transformer(PBT)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 배터리 사이클 수명 예측의 조기화는 배터리 연구 및 활용에 매우 중요하다.

Method: PBT는 다양한 데이터 세트에서 훈련된 기초 모델로, 전문 지식이 포함된 혼합 전문가 레이어를 통해 개발되었다.

Result: PBT는 13개의 리튬 이온 배터리 데이터 세트에서 학습하여 평균 19.8% 향상된 성능을 보이며, 15개의 다양한 데이터 세트에서 최신 성능을 달성한다.

Conclusion: 이 연구는 배터리 수명을 예측하기 위한 기초 모델 경로를 구축하여 보편적인 배터리 수명 예측 시스템을 향한 길을 열어준다.

Abstract: Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.

</details>


### [53] [Emergent Bias and Fairness in Multi-Agent Decision Systems](https://arxiv.org/abs/2512.16433)
*Maeve Madigan,Parameswaran Kamalaruban,Glenn Moynihan,Tom Kempton,David Sutton,Stuart Burrell*

Main category: cs.LG

TL;DR: 다중 에이전트 시스템은 협업 의사 결정을 통해 예측 작업의 성능을 향상시킬 수 있지만, 편향 위험을 추정할 수 있는 효과적인 평가 방법론이 부족하여 금융 서비스와 같은 고위험 영역에서 안전한 배포가 어렵다. 따라서 우리는 이러한 시스템의 공정성을 평가할 방법론을 개발하고, 금융 데이터에서 이들의 공정성 특성을 측정해야 한다. 대규모 시뮬레이션을 통해 금융 의사 결정에서 emergent bias의 패턴을 밝혀내고, 다중 에이전트 시스템은 진정한 집단 행동을 나타낼 수 있음을 나타냈다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 편향 위험 추정의 어려움과 금융 서비스에서의 안전한 배포 필요성.

Method: 대규모 시뮬레이션을 통해 다양한 다중 에이전트 구성과 커뮤니케이션 메커니즘을 이용하여 금융 의사 결정에서의 공정성 메트릭을 검토.

Result: 금융 의사 결정에서 개인 에이전트 구성 요소로는 추적할 수 없는 emergent bias 패턴 발견.

Conclusion: 다중 에이전트 결정 시스템은 구성 요소의 분석이 아닌 전체 체계로 평가되어야 한다.

Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.

</details>


### [54] [DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI](https://arxiv.org/abs/2512.16676)
*Hao Liang,Xiaochen Ma,Zhou Liu,Zhen Hao Wong,Zhengyang Zhao,Zimo Meng,Runming He,Chengyu Shen,Qifeng Cai,Zhaoyang Han,Meiyi Qiang,Yalin Feng,Tianyi Bai,Zewei Pan,Ziyi Guo,Yizhen Jiang,Jingwen Deng,Qijie You,Peichao Lai,Tianyu Guo,Chi Hsu Tsai,Hengyi Feng,Rui Hu,Wenkai Yu,Junbo Niu,Bohan Zeng,Ruichuan An,Lu Ma,Jihao Huang,Yaowei Zheng,Conghui He,Linpeng Tang,Bin Cui,Weinan E,Wentao Zhang*

Main category: cs.LG

TL;DR: DataFlow는 LLM 데이터 준비를 위한 통합되고 확장 가능한 프레임워크로, 재사용 가능한 데이터 변환과 디버깅 가능한 파이프라인을 제공하며, LLM 성능 개선에 기여한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)에 대한 고품질 데이터의 수요가 급증함에 따라, 신뢰할 수 있고 의미적으로 풍부한 데이터 준비 파이프라인의 필요성이 커졌다.

Method: DataFlow는 모듈화, 재사용 가능 및 조합 가능한 데이터 변환을 가능하게 하는 시스템 수준의 추상화를 통해 설계되었으며, 디버깅 및 최적화 가능한 데이터 흐름을 구축하기 위한 PyTorch 스타일의 파이프라인 구축 API를 제공한다.

Result: DataFlow는 200개에 가까운 재사용 가능한 연산자와 텍스트, 수학적 추론, 코드, Text-to-SQL, 에이전트 RAG, 대규모 지식 추출을 포함하는 여섯 개의 도메인 일반 파이프라인으로 구성된다. 여섯 가지 대표적인 사용 사례에서 데이터 흐름이 LLM의 성능을 일관되게 개선하였다.

Conclusion: 이러한 결과는 DataFlow가 신뢰할 수 있고 재현 가능하며 확장 가능한 LLM 데이터 준비를 위한 실용적이고 고성능의 기반을 제공하여 향후 데이터 중심 AI 개발을 위한 시스템 수준의 토대를 구축함을 보여준다.

Abstract: The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\% execution accuracy in Text-to-SQL over SynSQL, +7\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.

</details>


### [55] [MEPIC: Memory Efficient Position Independent Caching for LLM Serving](https://arxiv.org/abs/2512.16822)
*Qian Wang,Zahra Yousefijamarani,Morgan Lindsay Heisler,Rongzhi Gu,Bai Xiaolong,Shan Yizhou,Wei Zhang,Wang Lan,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: MEPIC은 다양한 요청과 배치 간의 청크 KV 재사용을 가능하게 하는 메모리 효율적인 시스템으로, 기존의 PIC보다 최대 2배의 메모리 사용량을 줄여준다.


<details>
  <summary>Details</summary>
Motivation: 긴 프롬프트 이력을 처리하는 현대의 LLM 애플리케이션은 제한된 메모리에서 높은 처리량과 낮은 지연을 유지해야 하므로 KV 캐시에 큰 압박을 주며, 이를 해결하기 위한 방법이 필요하다.

Method: MEPIC은 청크 KV를 페이지 저장소에 정렬하고, 재계산을 토큰 레벨에서 블록 레벨로 전환하여 첫 번째 블록만 요청 특화로 만들며, Rotary Position Embedding (RoPE) 융합을 통해 주의 커널에서 위치 인코딩을 제거하고 나머지 블록을 완전히 공유할 수 있도록 한다.

Result: MEPIC는 HBM에서 대부분의 중복 청크 KV를 제거하여 이전의 최첨단 PIC보다 메모리 사용을 최대 2배 줄이고 긴 프롬프트에 대해서는 최대 5배 줄인다.

Conclusion: 이러한 기법은 모델 변경 없이 유사한 지연과 정확도로 메모리 효율성을 크게 개선한다.

Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

</details>


### [56] [Meta-RL Induces Exploration in Language Agents](https://arxiv.org/abs/2512.16848)
*Yulun Jiang,Liangze Jiang,Damien Teney,Michael Moor,Maria Brbic*

Main category: cs.LG

TL;DR: LaMer는 LLM 에이전트의 환경 탐색 및 학습을 촉진하는 메타 강화 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습(RL)로 훈련된 에이전트는 능동적인 탐색이 필요한 작업에서 어려움을 겪고 있다.

Method: LaMer는 두 가지 핵심 구성 요소로 구성된다: 크로스 에피소드 훈련 프레임워크와 반사를 통한 정책 적응.

Result: Sokoban, MineSweeper 및 Webshop에서 각각 11%, 14%, 19%의 성능 개선이 나타났다.

Conclusion: 메타 RL은 언어 에이전트의 탐색을 유도하는 원칙적 접근 방식을 제공한다.

Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.

</details>


### [57] [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912)
*Peter Chen,Xiaopeng Li,Ziniu Li,Wotao Yin,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: 이 논문은 강화학습과 검증 가능한 보상을 통해 대형 언어 모델의 추론 능력을 개선하는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 강화학습 프레임워크인 RLVR에서 탐색과 활용의 균형을 이해하고 대형 언어 모델의 추론 성능 향상을 도모한다.

Method: 스푸리어스 보상과 엔트로피 최소화를 통해 강화학습의 성능을 연구하고, 정책 엔트로피와 성능의 관계를 분석한다.

Result: 스푸리어스 보상 하에서 클리핑 바이어스가 정책 엔트로피를 감소시키며, 보다 자신감 있고 결정적인 출력을 생성함을 보인다.

Conclusion: 스푸리어스 보상의 이점을 명확히 하여 더욱 효과적인 RLVR 훈련 원칙을 제공한다.

Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

</details>
