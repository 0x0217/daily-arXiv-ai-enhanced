<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 53]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 54]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Hilbert: Recursively Building Formal Proofs with Informal Reasoning](https://arxiv.org/abs/2509.22819)
*Sumanth Varambally,Thomas Voice,Yanchao Sun,Zhifeng Chen,Rose Yu,Ke Ye*

Main category: cs.AI

TL;DR: Hilbert라는 시스템은 비공식적인 추론과 공식적인 검증의 강점을 결합하여 수학적 문제를 더 잘 해결하도록 설계되었다. 이 시스템은 기존의 LLM보다 더 높은 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: LLM의 수학적 추론 능력은 뛰어나지만, 그 해결책에는 종종 자동으로 검증할 수 없는 오류가 포함되어 있다. 따라서, 공식적인 정리 증명 시스템을 통한 완전한 검증이 필요하다.

Method: Hilbert는 비공식 LLM, 전문화된 증명 LLM, 공식 검증기, 의미적 정리 검색기를 포함한 네 가지 구성 요소를 조화롭게 운용하며, 증명할 수 없는 문제를 재귀적 분해를 통해 하위 목표로 나눈다.

Result: Hilbert는 miniF2F에서 99.2%로 기존의 방법보다 6.6% 포인트 높은 성과를 보여주었고, PutnamBench에서 462/660 문제를 해결하여 70.0%의 성공률을 기록하였다.

Conclusion: 따라서, Hilbert는 비공식적인 추론과 공식적인 증명 생성 간의 격차를 효과적으로 좁힌다.

Abstract: Large Language Models (LLMs) demonstrate impressive mathematical reasoning
abilities, but their solutions frequently contain errors that cannot be
automatically verified. Formal theorem proving systems such as Lean 4 offer
automated verification with complete accuracy, motivating recent efforts to
build specialized prover LLMs that generate verifiable proofs in formal
languages. However, a significant gap remains: current prover LLMs solve
substantially fewer problems than general-purpose LLMs operating in natural
language. We introduce Hilbert, an agentic framework that bridges this gap by
combining the complementary strengths of informal reasoning and formal
verification. Our system orchestrates four components: an informal LLM that
excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4
tactics, a formal verifier, and a semantic theorem retriever. Given a problem
that the prover is unable to solve, Hilbert employs recursive decomposition to
split the problem into subgoals that it solves with the prover or reasoner LLM.
It leverages verifier feedback to refine incorrect proofs as necessary.
Experimental results demonstrate that Hilbert substantially outperforms
existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points
above the best publicly available method. Hilbert achieves the best known
result on PutnamBench. It solves 462/660 problems (70.0%), outperforming
proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement
over the best publicly available baseline. Thus, Hilbert effectively narrows
the gap between informal reasoning and formal proof generation.

</details>


### [2] [Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems](https://arxiv.org/abs/2509.23006)
*Hassen Dhrif*

Main category: cs.AI

TL;DR: 이 논문은 Creative Adversarial Testing (CAT) 프레임워크를 도입하여 Agentic AI 시스템의 과제와 목표 간의 정렬을 평가하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 평가 기술은 에이전트, 도구 및 매개변수를 식별하는 효과성에만 초점을 맞추고 있으며, Agentic AI 시스템의 과제와 목표 간의 정렬을 평가하는 데 중요한 공백이 존재합니다.

Method: Creative Adversarial Testing (CAT) 프레임워크는 Agentic AI 작업과 시스템의 의도한 목표 간의 복잡한 관계를 캡처하고 분석하기 위해 설계된 새로운 접근 방식을 제안합니다.

Result: CAT 프레임워크는 Alexa+ 오디오 서비스에서 모델링된 합성 상호작용 데이터를 사용한 광범위한 시뮬레이션을 통해 검증되었으며, 이는 글로벌 사용자 경험을 형성하는 편리한 Agentic AI 시스템입니다.

Conclusion: 결과는 CAT 프레임워크가 목표-작업 정렬에 대한 이전에 없는 통찰력을 제공하여 Agentic AI 시스템의 보다 효과적인 최적화 및 개발을 가능하게 함을 보여줍니다.

Abstract: Agentic AI represents a paradigm shift in enhancing the capabilities of
generative AI models. While these systems demonstrate immense potential and
power, current evaluation techniques primarily focus on assessing their
efficacy in identifying appropriate agents, tools, and parameters. However, a
critical gap exists in evaluating the alignment between an Agentic AI system's
tasks and its overarching goals. This paper introduces the Creative Adversarial
Testing (CAT) framework, a novel approach designed to capture and analyze the
complex relationship between Agentic AI tasks and the system's intended
objectives.
  We validate the CAT framework through extensive simulation using synthetic
interaction data modeled after Alexa+ audio services, a sophisticated Agentic
AI system that shapes the user experience for millions of users globally. This
synthetic data approach enables comprehensive testing of edge cases and failure
modes while protecting user privacy. Our results demonstrate that the CAT
framework provides unprecedented insights into goal-task alignment, enabling
more effective optimization and development of Agentic AI systems.

</details>


### [3] [Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia](https://arxiv.org/abs/2509.23023)
*Davi Bastos Costa,Renato Vicente*

Main category: cs.AI

TL;DR: 마피아는 정보 비대칭성과 이론적 마음 추론을 활용한 사회적 추리 게임이다. Mini-Mafia라는 간소화된 변형을 도입하여 대형 언어 모델(LLMs)의 사회 지능을 평가한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 사회적 지능을 평가하기 위한 체계적인 연구 지원

Method: Mini-Mafia라는 네 명의 플레이어가 참여하는 간소화된 게임을 도입하여 LLM들이 상호 작용하도록 함

Result: 실험 결과, 작은 모델이 큰 모델보다 우수한 성능을 보이는 경우들이 나타났다.

Conclusion: Mini-Mafia는 다중 에이전트 역학을 정량적으로 연구할 수 있는 기회를 제공하며, AI 안전성에도 기여한다.

Abstract: Mafia is a social deduction game where informed mafia compete against
uninformed townsfolk. Its asymmetry of information and reliance on
theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a
useful testbed for evaluating the social intelligence of large language models
(LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified
four-player variant with one mafioso, one detective, and two villagers. We set
the mafioso to kill a villager and the detective to investigate the mafioso
during the night, reducing the game to a single day phase of discussion and
voting. This setup isolates three interactive capabilities through
role-specific win conditions: the mafioso must deceive, the villagers must
detect deception, and the detective must effectively disclose information. To
measure these skills, we have LLMs play against each other, creating the
Mini-Mafia Benchmark: a two-stage framework that first estimates win rates
within fixed opponent configurations, then aggregates performance across them
using standardized scoring. Built entirely from model interactions without
external data, the benchmark evolves as new models are introduced, with each
one serving both as a new opponent and as a subject of evaluation. Our
experiments reveal counterintuitive results, including cases where smaller
models outperform larger ones. Beyond benchmarking, Mini-Mafia enables
quantitative study of emergent multi-agent dynamics such as name bias and
last-speaker advantage. It also contributes to AI safety by generating training
data for deception detectors and by tracking models' deception capabilities
against human baselines.

</details>


### [4] [Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence](https://arxiv.org/abs/2509.23144)
*Atma Anand*

Main category: cs.AI

TL;DR: 이 논문은 여러 에이전트와 목표가 조정되는 정보 처리 시스템이 기본적인 열역학적 제약에 직면하고 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 여러 에이전트 간 조정의 효율성을 극대화하는데 필요한 근본적인 원리를 이해하기 위해.

Method: 정확성 대신 찾기 쉬운 점에서 선택 압력이 높아지는 조정 프로토콜의 정보 이론적 최소 설명 길이를 도출했습니다.

Result: N명의 에이전트와 d개의 잠재적으로 상충하는 목표에서 조정 프로토콜의 최소 설명 길이는 특정 비율로 배수적으로 증가합니다.

Conclusion: 조정은 근본적인 정보 손실을 필요로 하며, 이는 여러 시스템 간의 측정 가능한 신호를 식별하는 데 활용될 수 있습니다.

Abstract: Information-processing systems coordinating across multiple agents and
objectives face fundamental thermodynamic constraints. We show that solutions
with maximum utility to act as coordination focal points have much higher
selection pressure for being findable across agents rather than accuracy. We
derive that the information-theoretic minimum description length of
coordination protocols to precision $\varepsilon$ scales as $L(P)\geq NK\log_2
K+N^2d^2\log (1/\varepsilon)$ for $N$ agents with $d$ potentially conflicting
objectives and internal model complexity $K$. This scaling forces progressive
simplification, with coordination dynamics changing the environment itself and
shifting optimization across hierarchical levels. Moving from established focal
points requires re-coordination, creating persistent metastable states and
hysteresis until significant environmental shifts trigger phase transitions
through spontaneous symmetry breaking. We operationally define coordination
temperature to predict critical phenomena and estimate coordination work costs,
identifying measurable signatures across systems from neural networks to
restaurant bills to bureaucracies. Extending the topological version of Arrow's
theorem on the impossibility of consistent preference aggregation, we find it
recursively binds whenever preferences are combined. This potentially explains
the indefinite cycling in multi-objective gradient descent and alignment faking
in Large Language Models trained with reinforcement learning with human
feedback. We term this framework Thermodynamic Coordination Theory (TCT), which
demonstrates that coordination requires radical information loss.

</details>


### [5] [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
*Zonghan Yang,Shengjie Wang,Kelin Fu,Wenyang He,Weimin Xiong,Yibo Liu,Yibo Miao,Bofei Gao,Yejie Wang,Yingwei Ma,Yanhao Li,Yue Liu,Zhenxing Hu,Kaitai Zhang,Shuyi Wang,Huarong Chen,Flood Sung,Yang Liu,Yang Gao,Zhilin Yang,Tianyu Liu*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)은 소프트웨어 공학(SWE)에 점점 더 많이 적용되고 있으며, 이 분야의 주요 벤치마크는 SWE-bench입니다.


<details>
  <summary>Details</summary>
Motivation: 우리는 SWE-Agent 프레임워크와 Agentless 방법이 상호 배타적이지 않다는 것을 주장합니다.

Method: Agentless 훈련 레시피를 정리하고 Kimi-Dev라는 오픈 소스 SWE LLM을 제시하였습니다.

Result: Kimi-Dev는 SWE-bench Verified에서 60.4%를 달성하였으며, 5k의 공개적으로 이용 가능한 경로에 대한 추가 SFT 적응을 통해 SWE-Agents가 48.6% pass@1 성능을 보였습니다.

Conclusion: 이 결과는 구조화된 기술 우선순위가 워크플로우와 에이전트 프레임워크 간의 다리 역할을 할 수 있음을 보여줍니다.

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent
frameworks with multi-turn interactions and workflow-based Agentless methods
with single-turn verifiable steps. We argue these paradigms are not mutually
exclusive: reasoning-intensive Agentless training induces skill priors,
including localization, code edit, and self-reflection that enable efficient
and effective SWE-Agent adaptation. In this work, we first curate the Agentless
training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\%
on SWE-bench Verified, the best among workflow approaches. With additional SFT
adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to
48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These
results show that structured skill priors from Agentless training can bridge
workflow and agentic frameworks for transferable coding agents.

</details>


### [6] [Risk Profiling and Modulation for LLMs](https://arxiv.org/abs/2509.23058)
*Yikai Wang,Xiaocheng Li,Guanting Chen*

Main category: cs.AI

TL;DR: 이 논문은 대형 언어 모델(LLMs)의 위험 프로필과 후속 학습의 영향을 연구하여, 위험 선호도를 조정하는 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 불확실한 상황에서 의사결정 작업에 사용되지만, 이들의 위험 프로필과 후속 학습의 영향은 충분히 연구되지 않았다.

Method: 행동 경제학 및 금융 도구를 활용하여 LLM의 위험 프로필을 이끌고 조정하는 새로운 파이프라인을 제안한다. 사전 훈련된 모델, 지침 조정 모델, RLHF 조정 모델을 비교한다.

Result: 지침 조정 모델은 일부 표준 효용 이론과 일치하는 반면, 사전 훈련된 모델과 RLHF 조정 모델은 이러한 이론에서 더 많이 벗어난다.

Conclusion: 후속 학습은 위험 선호도의 조정에 있어 가장 안정적이고 효과적인 모듈레이션을 제공하며, 이는 행동 정렬 및 위험 인식 LLM 설계에 대한 향후 연구를 위한 기초를 마련한다.

Abstract: Large language models (LLMs) are increasingly used for decision-making tasks
under uncertainty; however, their risk profiles and how they are influenced by
prompting and alignment methods remain underexplored. Existing studies have
primarily examined personality prompting or multi-agent interactions, leaving
open the question of how post-training influences the risk behavior of LLMs. In
this work, we propose a new pipeline for eliciting, steering, and modulating
LLMs' risk profiles, drawing on tools from behavioral economics and finance.
Using utility-theoretic models, we compare pre-trained, instruction-tuned, and
RLHF-aligned LLMs, and find that while instruction-tuned models exhibit
behaviors consistent with some standard utility formulations, pre-trained and
RLHF-aligned models deviate more from any utility models fitted. We further
evaluate modulation strategies, including prompt engineering, in-context
learning, and post-training, and show that post-training provides the most
stable and effective modulation of risk preference. Our findings provide
insights into the risk profiles of different classes and stages of LLMs and
demonstrate how post-training modulates these profiles, laying the groundwork
for future research on behavioral alignment and risk-aware LLM design.

</details>


### [7] [Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models](https://arxiv.org/abs/2509.23108)
*Morgan McCarty,Jorge Morales*

Main category: cs.AI

TL;DR: 이 연구는 인공지능 시스템의 복잡한 인지 행동을 평가하기 위한 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 대형 언어 모델(LLM)은 훈련 데이터에 포함된 과제에서 최상의 성능을 발휘하며, 자연어만으로 수행 가능한 작업에 한정되어 복잡한 인지 능력을 이해하는 데 제약이 있습니다.

Method: 인지 심리학의 고전적 사고 이미지 작업에서 수십 가지 새로운 항목을 만들어 LLM을 사용하여 텍스트 기반 모델을 평가하고, 인간 피험자와 함께 같은 작업을 수행하여 성능을 비교했습니다.

Result: 최고의 LLM 성능이 평균적인 인간 성과를 유의미하게 초과했습니다. 또한, 사고의 수준이 다른 모델들을 테스트하여 더 많은 사고 토큰을 할당할 때 가장 강력한 성능을 보였습니다.

Conclusion: 이 연구는 LLM이 비주얼 사용 없이도 이미지 의존 과제를 수행할 수 있는 능력을 가진 것으로 보이게 하며, 이는 인간의 시각적 이미지를 표현하는 방식에 대한 논쟁을 재점화합니다.

Abstract: This study offers a novel approach for benchmarking complex cognitive
behavior in artificial systems. Almost universally, Large Language Models
(LLMs) perform best on tasks which may be included in their training data and
can be accomplished solely using natural language, limiting our understanding
of their emergent sophisticated cognitive capacities. In this work, we created
dozens of novel items of a classic mental imagery task from cognitive
psychology. A task which, traditionally, cognitive psychologists have argued is
solvable exclusively via visual mental imagery (i.e., language alone would be
insufficient). LLMs are perfect for testing this hypothesis. First, we tested
several state-of-the-art LLMs by giving text-only models written instructions
and asking them to report the resulting object after performing the
transformations in the aforementioned task. Then, we created a baseline by
testing 100 human subjects in exactly the same task. We found that the best
LLMs performed significantly above average human performance. Finally, we
tested reasoning models set to different levels of reasoning and found the
strongest performance when models allocate greater amounts of reasoning tokens.
These results provide evidence that the best LLMs may have the capability to
complete imagery-dependent tasks despite the non-pictorial nature of their
architectures. Our study not only demonstrates an emergent cognitive capacity
in LLMs while performing a novel task, but it also provides the field with a
new task that leaves lots of room for improvement in otherwise already highly
capable models. Finally, our findings reignite the debate over the formats of
representation of visual imagery in humans, suggesting that propositional
reasoning (or at least non-imagistic reasoning) may be sufficient to complete
tasks that were long-thought to be imagery-dependent.

</details>


### [8] [HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis](https://arxiv.org/abs/2509.25112)
*Yiquan Wang,Tin-Yeh Huang,Qingyun Gao,Jialin Zhang*

Main category: cs.AI

TL;DR: HeDA라는 지능형 다중 에이전트 시스템을 통해 열파에 대한 복잡한 위험 경로를 자동으로 발견하고, 10,247개의 논문을 기반으로 한 포괄적인 지식 그래프를 구축하였다.


<details>
  <summary>Details</summary>
Motivation: 기후, 사회 및 경제 시스템에 걸친 열파의 복잡한 위험을 이해하려는 필요성.

Method: HeDA는 10,247개의 학술 논문을 처리하여 23,156개의 노드와 89,472개의 관계를 포함하는 포괄적인 지식 그래프를 구축하고, 다층 위험 전파 분석을 통해 간과된 위험 전파 경로를 체계적으로 식별한다.

Result: HeDA는 복잡한 질문-응답 작업에서 78.9%의 정확도를 달성했으며, 이는 GPT-4와 같은 최신 벤치마크보다 13.7% 높은 성과이다.

Conclusion: 이 연구는 AI 기반의 과학적 발견을 위한 새로운 패러다임을 제시하고, 기후 적응 전략 개발을 위한 실행 가능한 통찰력을 제공한다.

Abstract: Heatwaves pose complex cascading risks across interconnected climate, social,
and economic systems, but knowledge fragmentation in scientific literature
hinders comprehensive understanding of these risk pathways. We introduce HeDA
(Heatwave Discovery Agent), an intelligent multi-agent system designed for
automated scientific discovery through knowledge graph construction and
multi-layer risk propagation analysis. HeDA processes over 10,247 academic
papers to construct a comprehensive knowledge graph with 23,156 nodes and
89,472 relationships, employing novel multi-layer risk propagation analysis to
systematically identify overlooked risk transmission pathways. Our system
achieves 78.9% accuracy on complex question-answering tasks, outperforming
state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA
successfully discovered five previously unidentified high-impact risk chains,
such as the pathway where a heatwave leads to a water demand surge, resulting
in industrial water restrictions and ultimately causing small business
disruption, which were validated through historical case studies and domain
expert review. This work presents a new paradigm for AI-driven scientific
discovery, providing actionable insights for developing more resilient climate
adaptation strategies.

</details>


### [9] [SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems](https://arxiv.org/abs/2509.23130)
*Qian Cheng,Ruize Tang,Emilie Ma,Finn Hackett,Peiyang He,Yiming Su,Ivan Beschastnikh,Yu Huang,Xiaoxing Ma,Tianyin Xu*

Main category: cs.AI

TL;DR: SysMoBench는 AI가 크고 복잡한 시스템을 형식적으로 모델링하는 능력을 평가하는 벤치마크로, 특히 동시 및 분산 시스템을 대상으로 한다.


<details>
  <summary>Details</summary>
Motivation: 형식 모델은 복잡한 컴퓨터 시스템을 지정하고 그 정확성을 검증하는 데 필수적이지만, 작성 및 유지보수가 매우 비용이 많이 든다.

Method: TLA+를 사용하여 AI가 생성한 모델의 문법적 및 실행 시간 정확성, 시스템 코드와의 일치성, 불변성 정확성을 평가하는 지표를 자동화한다.

Result: SysMoBench는 현재 Raft 구현체, Spinlock 및 Mutex와 같은 아티팩트 9개를 포함하며, AI 모델의 능력과 한계를 이해할 수 있게 한다.

Conclusion: SysMoBench는 LLM 및 에이전트의 잠재력을 평가하고 이 분야의 도구를 확립하며, 새로운 연구 방향을 열어준다.

Abstract: Formal models are essential to specifying large, complex computer systems and
verifying their correctness, but are notoriously expensive to write and
maintain. Recent advances in generative AI show promise in generating certain
forms of specifications. However, existing work mostly targets small code, not
complete systems. It is unclear whether AI can deal with realistic system
artifacts, as this requires abstracting their complex behavioral properties
into formal models. We present SysMoBench, a benchmark that evaluates AI's
ability to formally model large, complex systems. We focus on concurrent and
distributed systems, which are keystones of today's critical computing
infrastructures, encompassing operating systems and cloud infrastructure. We
use TLA+, the de facto specification language for concurrent and distributed
systems, though the benchmark can be extended to other specification languages.
We address the primary challenge of evaluating AI-generated models by
automating metrics like syntactic and runtime correctness, conformance to
system code, and invariant correctness. SysMoBench currently includes nine
diverse system artifacts: the Raft implementation of Etcd and Redis, the
Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively
added. SysMoBench enables us to understand the capabilities and limitations of
today's LLMs and agents, putting tools in this area on a firm footing and
opening up promising new research directions.

</details>


### [10] [AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8](https://arxiv.org/abs/2509.23154)
*Jinzhe Pan,Jingqing Wang,Yuehui Ouyang,Wenchi Cheng,Wei Zhang*

Main category: cs.AI

TL;DR: 무선 장치의 급격한 증가와 신뢰성 요구 사항은 비허가 대역의 분산 채널 접근 메커니즘의 근본적인 개선을 요구한다. 본 논문은 AI 최적화를 통합한 다중 에이전트 강화 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 무선 장치의 급속한 성장과 새로운 애플리케이션의 엄격한 신뢰성 요구가 분산 채널 접근 메커니즘의 개선 필요성을 강조하고 있다.

Method: AI 최적화와 기존 장치 공존을 결합한 다중 에이전트 강화 학습 프레임워크를 개발하고, 실시간 채널 조건에 맞춰 동적으로 백오프 선택 메커니즘을 조정한다.

Result: 제안된 솔루션은 기존 BEB에 비해 충돌 확률을 현저히 감소시키면서 상업적인 Wi-Fi 장치와의 하위 호환성을 유지한다.

Conclusion: 제안된 공정성 지표는 이질적인 시나리오에서 기아 위험을 효과적으로 제거할 수 있다.

Abstract: The exponential growth of wireless devices and stringent reliability
requirements of emerging applications demand fundamental improvements in
distributed channel access mechanisms for unlicensed bands. Current Wi-Fi
systems, which rely on binary exponential backoff (BEB), suffer from suboptimal
collision resolution in dense deployments and persistent fairness challenges
due to inherent randomness. This paper introduces a multi-agent reinforcement
learning framework that integrates artificial intelligence (AI) optimization
with legacy device coexistence. We first develop a dynamic backoff selection
mechanism that adapts to real-time channel conditions through access deferral
events while maintaining full compatibility with conventional CSMA/CA
operations. Second, we introduce a fairness quantification metric aligned with
enhanced distributed channel access (EDCA) principles to ensure equitable
medium access opportunities. Finally, we propose a centralized training
decentralized execution (CTDE) architecture incorporating neighborhood activity
patterns as observational inputs, optimized via constrained multi-agent
proximal policy optimization (MAPPO) to jointly minimize collisions and
guarantee fairness. Experimental results demonstrate that our solution
significantly reduces collision probability compared to conventional BEB while
preserving backward compatibility with commercial Wi-Fi devices. The proposed
fairness metric effectively eliminates starvation risks in heterogeneous
scenarios.

</details>


### [11] [AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms](https://arxiv.org/abs/2509.23189)
*Zhenxing Xu,Yizhe Zhang,Weidong Bao,Hao Wang,Ming Chen,Haoran Ye,Wenzheng Jiang,Hui Yan,Ji Wang*

Main category: cs.AI

TL;DR: AutoEP는 대형 언어 모델을 활용하여 알고리즘 초매개변수를 자동으로 설정하는 새로운 프레임워크로, 뛰어난 성능을 보이고 있다.


<details>
  <summary>Details</summary>
Motivation: 계산 지능에서 알고리즘 초매개변수의 동적 구성은 기본적인 도전과제이다.

Method: AutoEP는 온라인 탐색적 경관 분석 모듈과 다중 LLM 추론 체인을 결합하여 초매개변수 전략을 생성한다.

Result: AutoEP는 다양한 조합 최적화 벤치마크에서 기존의 최첨단 조정기보다 뛰어난 성능을 보인다.

Conclusion: 이 프레임워크는 자동화된 초매개변수 설계의 강력하고 접근 가능한 새로운 패러다임을 보여준다.

Abstract: Dynamically configuring algorithm hyperparameters is a fundamental challenge
in computational intelligence. While learning-based methods offer automation,
they suffer from prohibitive sample complexity and poor generalization. We
introduce AutoEP, a novel framework that bypasses training entirely by
leveraging Large Language Models (LLMs) as zero-shot reasoning engines for
algorithm control. AutoEP's core innovation lies in a tight synergy between two
components: (1) an online Exploratory Landscape Analysis (ELA) module that
provides real-time, quantitative feedback on the search dynamics, and (2) a
multi-LLM reasoning chain that interprets this feedback to generate adaptive
hyperparameter strategies. This approach grounds high-level reasoning in
empirical data, mitigating hallucination. Evaluated on three distinct
metaheuristics across diverse combinatorial optimization benchmarks, AutoEP
consistently outperforms state-of-the-art tuners, including neural evolution
and other LLM-based methods. Notably, our framework enables open-source models
like Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and
accessible new paradigm for automated hyperparameter design. Our code is
available at https://anonymous.4open.science/r/AutoEP-3E11

</details>


### [12] [Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity](https://arxiv.org/abs/2509.23449)
*Charles E. Gagnon,Steven H. H. Ding,Philippe Charland,Benjamin C. M. Fung*

Main category: cs.AI

TL;DR: 이 논문은 이진 코드 유사성 탐지에서 해법을 제시하여 해석 가능성, 일반화 가능성 및 확장 가능성 간의 균형을 이루는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 이진 코드 유사성 탐지는 리버스 엔지니어링의 핵심 작업으로, 맬웨어 분석과 취약성 발견을 지원한다.

Method: 언어 모델 기반 에이전트를 사용하여 구조적 추론 분석을 수행하고 입력/출력 유형, 부작용, 주목할 만한 상수 및 알고리즘 의도와 같은 기능을 생성한다.

Result: 우리 방법은 매칭 훈련 없이도 교차 아키텍처와 교차 최적화 작업에서 각각 42% 및 62%의 recall@1을 달성하며, 훈련된 임베딩 방법과 유사한 성능을 보인다.

Conclusion: 우리 방법은 정확성, 확장성 및 해석 가능성이 공존할 수 있음을 보여준다.

Abstract: Binary code similarity detection is a core task in reverse engineering. It
supports malware analysis and vulnerability discovery by identifying
semantically similar code in different contexts. Modern methods have progressed
from manually engineered features to vector representations. Hand-crafted
statistics (e.g., operation ratios) are interpretable, but shallow and fail to
generalize. Embedding-based methods overcome this by learning robust
cross-setting representations, but these representations are opaque vectors
that prevent rapid verification. They also face a scalability-accuracy
trade-off, since high-dimensional nearest-neighbor search requires
approximations that reduce precision. Current approaches thus force a
compromise between interpretability, generalizability, and scalability.
  We bridge these gaps using a language model-based agent to conduct structured
reasoning analysis of assembly code and generate features such as input/output
types, side effects, notable constants, and algorithmic intent. Unlike
hand-crafted features, they are richer and adaptive. Unlike embeddings, they
are human-readable, maintainable, and directly searchable with inverted or
relational indexes. Without any matching training, our method respectively
achieves 42% and 62% for recall@1 in cross-architecture and cross-optimization
tasks, comparable to embedding methods with training (39% and 34%). Combined
with embeddings, it significantly outperforms the state-of-the-art,
demonstrating that accuracy, scalability, and interpretability can coexist.

</details>


### [13] [Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions](https://arxiv.org/abs/2509.23248)
*Mingyi Luo,Ruichen Zhang,Xiangwang Hou,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato,Shiwen Mao*

Main category: cs.AI

TL;DR: 이 논문은 мобиль edge 환경에서 효율적인 LLM 추론 배치를 위한 공동 최적화 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 발전으로 인공지능이 강력한 추론 및 자율 의사결정 능력을 갖추게 되었습니다. 그러나 이러한 AI가 모바일 엣지 일반 지능(MEGI) 환경에서 사용하는 것은 높은 처리 능력 요구와 제한된 엣지 장치 자원으로 인해 도전 과제가 됩니다.

Method: 논문에서는 Chain-of-Thought(CoT) 프롬팅, 지도학습 미세 조정(SFT), 전문가 혼합(MoE)과 같은 LLM 추론 능력을 향상시키는 방법을 검토하고, 분산 MoE 아키텍처와 적응형 CoT 프롬팅을 통해 추론 개선 및 확장 가능한 배치를 다루는 분산 프레임워크를 제시합니다.

Result: 실험 결과는 해당 프레임워크가 resource efficiency와 reasoning quality 간의 균형을 효과적으로 유지함을 보여주었습니다.

Conclusion: 리소스가 제한된 MEGI 환경에서 복잡한 LLM 추론 능력을 배치하는 실용적인 가능성을 검증했습니다.

Abstract: The rapid advancement of large language models (LLMs) has enabled an
emergence of agentic artificial intelligence (AI) with powerful reasoning and
autonomous decision-making capabilities. This integration with edge computing
has led to the development of Mobile Edge General Intelligence (MEGI), which
brings real-time, privacy-preserving reasoning to the network edge. However,
deploying LLM-based agentic AI reasoning in MEGI environments poses significant
challenges due to the high computational demands of reasoning and the limited
resources of edge devices. To address these challenges, we propose a joint
optimization framework for efficient LLM reasoning deployment in MEGI. First,
we review methods that enhance LLM reasoning capabilities, such as
Chain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of
Experts (MoE). Next, we present a distributed framework that addresses two
correlated aspects: reasoning enhancement through adaptive CoT prompting and
scalable deployment through distributed MoE architecture. The framework
dynamically activates expert networks and adjusts reasoning depth based on task
complexity and device capabilities. We further conduct experimental evaluations
in mobile edge environments. Experimental results demonstrate the framework's
effectiveness in balancing reasoning quality with resource efficiency,
validating the practical viability of deploying sophisticated LLM reasoning
capabilities in resource-constrained MEGI environments.

</details>


### [14] [Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2509.23558)
*Zhaoqi Wang,Daqing He,Zijian Zhang,Xin Li,Liehuang Zhu,Meng Li,Jiamou Liu*

Main category: cs.AI

TL;DR: 본 논문은 LLM의 취약점을 발견하기 위해 PASS 프레임워크를 제안하며, 이는 강화 학습을 통해 초기 탈옥 프롬프트를 형식적인 설명으로 변환하고, 이를 통해 기존의 정렬 방어를 우회하여 탈옥 공격을 강화하는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: LLM(대규모 언어 모델)의 놀라운 능력에도 불구하고 새로운 보안 문제를 초래한다는 사실을 탐구하고자 한다.

Method: PASS 프레임워크를 사용하여 초기 탈옥 프롬프트를 정형화된 설명으로 변환하는 데 강화 학습을 활용한다.

Result: 많은 오픈 소스 모델에 대한 실험을 통해 우리의 공격의 효과성을 검증하였다.

Conclusion: PASS 프레임워크는 LLM의 정렬 방어를 우회하고 보다 효과적인 탈옥을 가능하게 한다.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, yet
they also introduce novel security challenges. For instance, prompt
jailbreaking attacks involve adversaries crafting sophisticated prompts to
elicit responses from LLMs that deviate from human values. To uncover
vulnerabilities in LLM alignment methods, we propose the PASS framework
(\underline{P}rompt J\underline{a}ilbreaking via \underline{S}emantic and
\underline{S}tructural Formalization). Specifically, PASS employs reinforcement
learning to transform initial jailbreak prompts into formalized descriptions,
which enhances stealthiness and enables bypassing existing alignment defenses.
The jailbreak outputs are then structured into a GraphRAG system that, by
leveraging extracted relevant terms and formalized symbols as contextual input
alongside the original query, strengthens subsequent attacks and facilitates
more effective jailbreaks. We conducted extensive experiments on common
open-source models, demonstrating the effectiveness of our attack.

</details>


### [15] [GUI-PRA: Process Reward Agent for GUI Tasks](https://arxiv.org/abs/2509.23263)
*Tao Xiong,Xavier Hu,Yurun Chen,Yuhang Liu,Changqiao Wu,Pengzhi Gao,Wei Liu,Jian Luan,Shengyu Zhang*

Main category: cs.AI

TL;DR: 본 연구는 GUI 작업을 위한 Process Reward Agent(GUI-PRA)를 제안하며, 이 에이전트는 역사적 맥락을 지능적으로 처리하고 UI 상태 변화를 적극적으로 인식하여 프로세스 보상을 더 잘 제공하도록 설계되었습니다. 이는 "중간에서 분실됨" 현상과 UI 변화 인식 부족 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: GUI 작업을 수행하는 멀티모달 대형 언어 모델 기반 에이전트의 자동화 가능성은 높지만, 긴 시간 지연 작업에서는 실패가 빈번합니다.

Method: GUI-PRA는 두 가지 핵심 요소로 구성된 동적 메모리 메커니즘을 도입하여 역사적 맥락을 지능적으로 처리하고 UI 상태 변화를 인식합니다: 연관 기반 검색 모듈과 점진적 요약 모듈입니다.

Result: GUI-PRA는 표준 PRM보다 프로세스 보상을 더 효과적으로 제공하며, 동적 UI 상태와 관련된 정보를 지속적으로 평가합니다.

Conclusion: 본 연구는 GUI 작업의 동적 특성에 적합한 에이전트를 설계함으로써 PRM의 한계를 극복하고, UI의 변화에 민감한 평가 메커니즘을 제공합니다.

Abstract: Graphical User Interface (GUI) Agents powered by Multimodal Large Language
Models (MLLMs) show significant potential for automating tasks. However, they
often struggle with long-horizon tasks, leading to frequent failures. Process
Reward Models (PRMs) are a promising solution, as they can guide these agents
with crucial process signals during inference. Nevertheless, their application
to the GUI domain presents unique challenges. When processing dense artificial
inputs with long history data, PRMs suffer from a "lost in the middle"
phenomenon, where the overwhelming historical context compromises the
evaluation of the current step. Furthermore, standard PRMs lacks GUI changing
awareness, providing static evaluations that are disconnected from the dynamic
consequences of actions, a critical mismatch with the inherently dynamic nature
of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process
Reward Agent for GUI Tasks), a judge agent designed to better provide process
reward than standard PRM by intelligently processing historical context and
actively perceiving UI state changes. Specifically, to directly combat the
``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism
consisting of two core components: a Relevance-based Retrieval Module to
actively fetch pertinent information from long histories and a Progressive
Summarization Module to dynamically condense growing interaction data, ensuring
the model focuses on relevant context. Moreover, to address the lack of UI
changing awareness, we introduce an Aadaptive UI Perception mechanism. This
mechanism enables the agent to reason about UI state changes and dynamically
select the most appropriate tool to gather grounded visual evidence, ensuring
its evaluation is always informed by the current UI context.

</details>


### [16] [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694)
*Jianshuo Dong,Sheng Guo,Hao Wang,Zhuotao Liu,Tianwei Zhang,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 이 논문은 LLM 기반 검색 에이전트의 안전성을 평가하기 위한 자동화된 레드팀 프레임워크를 제안하고, 이를 통해 검색 결과의 낮은 품질이 에이전트 행동에 미치는 영향을 분석한다.


<details>
  <summary>Details</summary>
Motivation: LLM을 인터넷에 연결하여 정보에 접근할 수 있지만, 검색 결과의 신뢰성 부족이 사용자에게 안전 위협을 초래할 수 있다.

Method: 자동화된 레드팀 프레임워크를 도입하고, 이 기반으로 300개의 테스트 케이스를 포함한 SafeSearch 벤치마크를 구축하였다.

Result: LLM 기반 검색 에이전트의 상당한 취약점을 발견하였고, GPT-4.1-mini는 불신 웹사이트에 노출될 때 90.5%의 ASR에 도달하였다.

Conclusion: 우리 프레임워크는 더 안전한 에이전트 개발을 위해 투명성을 높이는 데 가치를 제공한다.

Abstract: Search agents connect LLMs to the Internet, enabling access to broader and
more up-to-date information. However, unreliable search results may also pose
safety threats to end users, establishing a new threat surface. In this work,
we conduct two in-the-wild experiments to demonstrate both the prevalence of
low-quality search results and their potential to misguide agent behaviors. To
counter this threat, we introduce an automated red-teaming framework that is
systematic, scalable, and cost-efficient, enabling lightweight and harmless
safety assessments of search agents. Building on this framework, we construct
the SafeSearch benchmark, which includes 300 test cases covering five
categories of risks (e.g., misinformation and indirect prompt injection). Using
this benchmark, we evaluate three representative search agent scaffolds,
covering search workflow, tool-calling, and deep research, across 7 proprietary
and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities
of LLM-based search agents: when exposed to unreliable websites, the highest
ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,
our analysis highlights the limited effectiveness of common defense practices,
such as reminder prompting. This emphasizes the value of our framework in
promoting transparency for safer agent development. Our codebase and test cases
are publicly available: https://github.com/jianshuod/SafeSearch.

</details>


### [17] [Socio-Economic Model of AI Agents](https://arxiv.org/abs/2509.23270)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 인공지능과의 협업이 사회적 총산출에 미치는 영향을 연구하기 위해 인적 노동자와 자율 AI 에이전트를 포함한 이종 에이전트 기반 모델링 프레임워크를 구축하였다.


<details>
  <summary>Details</summary>
Motivation: 현대 사회경제 시스템이 인공지능 기술과 깊은 통합을 거치고 있기 때문에 AI 협업의 영향을 연구할 필요가 있다.

Method: 다섯 가지 단계적으로 확장된 모델을 구축하였다. 모델 1은 순수한 인간 협업의 기준선으로 작용하며, 모델 2는 AI를 협력자로 도입하고, 모델 3은 에이전트 간의 네트워크 효과를 포함하며, 모델 4는 에이전트를 독립된 생산자로 취급하고, 모델 5는 네트워크 효과와 독립 생산을 통합한다.

Result: AI 에이전트를 도입하면 사회적 총산출이 크게 증가하며, 에이전트 간의 네트워크 효과를 고려할 때 이 증가는 비선형 성장 형태로 나타나고, 개별 기여의 단순한 합을 훨씬 초과한다.

Conclusion: 동일한 자원 투입 하에 에이전트를 독립 생산자로 취급하는 것이 장기 성장 잠재력을 높이며, 네트워크 효과를 도입하면 규모의 경제적 이익이 강하게 나타난다.

Abstract: Modern socio-economic systems are undergoing deep integration with artificial
intelligence technologies. This paper constructs a heterogeneous agent-based
modeling framework that incorporates both human workers and autonomous AI
agents, to study the impact of AI collaboration under resource constraints on
aggregate social output. We build five progressively extended models: Model 1
serves as the baseline of pure human collaboration; Model 2 introduces AI as
collaborators; Model 3 incorporates network effects among agents; Model 4
treats agents as independent producers; and Model 5 integrates both network
effects and independent agent production. Through theoretical derivation and
simulation analysis, we find that the introduction of AI agents can
significantly increase aggregate social output. When considering network
effects among agents, this increase exhibits nonlinear growth far exceeding the
simple sum of individual contributions. Under the same resource inputs,
treating agents as independent producers provides higher long-term growth
potential; introducing network effects further demonstrates strong
characteristics of increasing returns to scale.

</details>


### [18] [From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents](https://arxiv.org/abs/2509.23415)
*Gyubok Lee,Woosog Chay,Heeyoung Kwak,Yeong Hwa Kim,Haanju Yoo,Oksoon Jeong,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: EHR 데이터 접근을 위한 LLM 기반 에이전트의 성능을 평가하기 위해 EHR-ChatQA라는 상호작용 데이터베이스 질문 응답 벤치마크를 제안하며, 쿼리 모호성과 값 불일치를 해결하는 방법을 설명한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 EHR 데이터 접근의 채택이 실세계 임상 데이터 접근 흐름을 적절히 포착하는 벤치마크의 부재로 제한되고 있다.

Method: EHR-ChatQA는 사용자 질문을 명확히 하고, 값 불일치를 해결하며, 정확한 SQL을 생성하여 정확한 답변을 제공하는 데이터베이스 에이전트의 엔드 투 엔드 워크플로우를 평가하는 벤치마크이다.

Result: 실험 결과, 에이전트는 Incremental Query Refinement에서 90-95%의 높은 Pass@5를 달성했지만, AdaptQA에서는 60-80%에 불과하며, 모든 다섯 차례의 시험에서 일관된 성공률(Pass^5)은 35-60% 낮았다.

Conclusion: EHR 도메인에서 안전성을 고려한 강력한 에이전트를 개발할 필요성이 강조되며, 일반적인 실패 모드에 대한 진단 통찰력을 제공한다.

Abstract: Despite the impressive performance of LLM-powered agents, their adoption for
Electronic Health Record (EHR) data access remains limited by the absence of
benchmarks that adequately capture real-world clinical data access flows. In
practice, two core challenges hinder deployment: query ambiguity from vague
user questions and value mismatch between user terminology and database
entries. To address this, we introduce EHR-ChatQA an interactive database
question answering benchmark that evaluates the end-to-end workflow of database
agents: clarifying user questions, using tools to resolve value mismatches, and
generating correct SQL to deliver accurate answers. To cover diverse patterns
of query ambiguity and value mismatch, EHR-ChatQA assesses agents in a
simulated environment with an LLM-based user across two interaction flows:
Incremental Query Refinement (IncreQA), where users add constraints to existing
queries, and Adaptive Query Refinement (AdaptQA), where users adjust their
search goals mid-conversation. Experiments with state-of-the-art LLMs (e.g.,
o4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents
achieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and
60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is
substantially lower by 35-60%. These results underscore the need to build
agents that are not only performant but also robust for the safety-critical EHR
domain. Finally, we provide diagnostic insights into common failure modes to
guide future agent development.

</details>


### [19] [Democratizing AI scientists using ToolUniverse](https://arxiv.org/abs/2509.23426)
*Shanghua Gao,Richard Zhu,Pengwei Sui,Zhenglun Kong,Sufian Aldogom,Yepeng Huang,Ayush Noori,Reza Shamji,Krishna Parvataneni,Theodoros Tsiligkaridis,Marinka Zitnik*

Main category: cs.AI

TL;DR: ToolUniverse는 AI 과학자를 구축하기 위한 통합 생태계로, 600개 이상의 기계 학습 모델과 데이터 세트를 포함하고 있어 상호 운용성과 재사용성을 제공한다.


<details>
  <summary>Details</summary>
Motivation: AI 과학자는 발견 과정에서 협력적 파트너로서의 역할을 수행하지만, 맞춤형으로 제작되며 고정된 워크플로우에 묶여 있어서 만들기 어렵다.

Method: ToolUniverse는 모든 언어와 추론 모델에서 AI 과학자를 구축할 수 있는 생태계로, AI 과학자가 도구를 식별하고 호출하는 방식을 표준화한다.

Result: ToolUniverse는 AI 과학자가 사용할 수 있도록 도구 인터페이스를 자동으로 정제하고, 자연어 설명으로부터 새로운 도구를 생성하며 도구 사양을 반복적으로 최적화한다.

Conclusion: ToolUniverse는 하이퍼콜레스테롤혈증 사례 연구에서 연구자가 약물의 유망한 유사체를 식별하는 AI 과학자를 구축하는 데 사용되었다.

Abstract: AI scientists are emerging computational systems that serve as collaborative
partners in discovery. These systems remain difficult to build because they are
bespoke, tied to rigid workflows, and lack shared environments that unify
tools, data, and analyses into a common ecosystem. In omics, unified ecosystems
have transformed research by enabling interoperability, reuse, and
community-driven development; AI scientists require comparable infrastructure.
We present ToolUniverse, an ecosystem for building AI scientists from any
language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes
how AI scientists identify and call tools, integrating more than 600 machine
learning models, datasets, APIs, and scientific packages for data analysis,
knowledge retrieval, and experimental design. It automatically refines tool
interfaces for correct use by AI scientists, creates new tools from natural
language descriptions, iteratively optimizes tool specifications, and composes
tools into agentic workflows. In a case study of hypercholesterolemia,
ToolUniverse was used to create an AI scientist to identify a potent analog of
a drug with favorable predicted properties. The open-source ToolUniverse is
available at https://aiscientist.tools.

</details>


### [20] [Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks](https://arxiv.org/abs/2509.23537)
*Aaron Xuxiang Tian,Ruofan Zhang,Jiayao Tang,Young Min Cho,Xueqian Li,Qiang Yi,Ji Wang,Zhunping Zhang,Danrui Qi,Sharath Chandra Guntuku,Lyle Ungar,Tianyu Shi,Chi Wang*

Main category: cs.AI

TL;DR: 본 연구에서는 다중 턴 다중 에이전트 오케스트레이션을 조사하여, 대규모 언어 모델 에이전트가 반복적으로 답변을 제안하거나 투표를 진행하며 합의에 도달하는 과정을 다룬다. 네 개의 LLM을 사용하여 두 가지 실험을 수행하였다: 오케스트레이션 성능 벤치마킹 및 GPQA-Diamond에 대한 작용 절제 실험. 오케스트레이션은 단일 모델보다 높은 성능을 보이며, 저자 공개가 자가 투표와 동점을 증가시키고, 진행 중인 투표 표시가 집단행동을 강화하여 수렴 속도를 높이지만 조기 합의를 초래할 수 있음을 보인다.


<details>
  <summary>Details</summary>
Motivation: 다중 턴 다중 에이전트 오케스트레이션의 성능을 향상시키고자 하였음.

Method: Gemini 2.5 Pro, GPT-5, Grok 4, Claude Sonnet 4의 네 가지 LLM을 사용하여 GPQA-Diamond, IFEval, MuSR에서 두 가지 실험을 수행.

Result: 오케스트레이션이 단일 LLM보다 성능이 우수하고, 최상의 오케스트레이션 성능 분석 결과 추가 성능 향상의 잠재성이 있음을 입증하였다.

Conclusion: 작가 공개 및 진행 중인 투표 표시가 성능에 중요한 영향을 미쳐 자가 투표, 동점, 그리고 조기 합의를 초래할 수 있음을 확인하였다.

Abstract: We study multi-turn multi-agent orchestration, where multiple large language
model (LLM) agents interact over multiple turns by iteratively proposing
answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5
Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we
conduct two experiments: (i) benchmarking orchestration against single-LLM
baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who
authored answers and whether they can observe ongoing votes. Orchestration
matches or exceeds the strongest single model and consistently outperforms the
others. Analysis of best-achievable orchestration performance shows potential
for further gains. The ablations show that revealing authorship increases
self-voting and ties, and that showing ongoing votes amplifies herding, which
speeds convergence but can sometimes yield premature consensus.

</details>


### [21] [PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents](https://arxiv.org/abs/2509.23614)
*Yaozu Wu,Jizhou Guo,Dongyuan Li,Henry Peng Zou,Wei-Chieh Huang,Yankai Chen,Zhen Wang,Weizhi Zhang,Yangning Li,Meng Zhang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: PSG-Agent는 LLM 기반 에이전트를 위한 개인화된 동적 시스템으로, 사용자별 리스크 임계값 및 보호 전략을 생성하고, 에이전트 파이프라인 전반에 걸쳐 지속적인 모니터링을 구현하여 안전성을 크게 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트의 안전한 배포를 위해 효과적인 가드레일이 필요하다. 기존의 시스템은 사용자에게 균일한 정책을 적용하고, 상호작용의 위험 진화를 무시하는 두 가지 주요 한계를 가지고 있다.

Method: PSG-Agent는 상호작용 기록을 분석하여 개인화된 가드레일을 생성하고, 에이전트 파이프라인 전반에 걸쳐 위험 누적을 추적하는 전문 가드를 이용한 지속적인 모니터링을 구현한다.

Result: PSG-Agent는 건강 관리, 금융, 일상 자동화 시나리오에서 다양한 사용자 프로필을 포함한 여러 시나리오에서 테스트되었으며, 기존 시스템들을 크게 초월하는 성능을 나타낸다.

Conclusion: PSG-Agent는 LLM 기반 에이전트를 위한 개인화된 안전성을 제공하는 실행 가능하고 감사 가능한 경로를 제시한다.

Abstract: Effective guardrails are essential for safely deploying LLM-based agents in
critical applications. Despite recent advances, existing guardrails suffer from
two fundamental limitations: (i) they apply uniform guardrail policies to all
users, ignoring that the same agent behavior can harm some users while being
safe for others; (ii) they check each response in isolation, missing how risks
evolve and accumulate across multiple interactions. To solve these issues, we
propose PSG-Agent, a personalized and dynamic system for LLM-based agents.
First, PSG-Agent creates personalized guardrails by mining the interaction
history for stable traits and capturing real-time states from current queries,
generating user-specific risk thresholds and protection strategies. Second,
PSG-Agent implements continuous monitoring across the agent pipeline with
specialized guards, including Plan Monitor, Tool Firewall, Response Guard,
Memory Guardian, that track cross-turn risk accumulation and issue verifiable
verdicts. Finally, we validate PSG-Agent in multiple scenarios including
healthcare, finance, and daily life automation scenarios with diverse user
profiles. It significantly outperform existing agent guardrails including
LlamaGuard3 and AGrail, providing an executable and auditable path toward
personalized safety for LLM-based agents.

</details>


### [22] [MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models](https://arxiv.org/abs/2509.23725)
*Siqi Ma,Jiajie Huang,Bolin Yang,Fan Zhang,Jinlin Wu,Yue Shen,Guohui Fan,Zhu Zhang,Zelin Zang*

Main category: cs.AI

TL;DR: 이 논문은 복잡한 의료 질문에 대한 답변을 위해 다중 에이전트 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 의료 질문에 답하려면 도메인 전문 지식과 환자 특정 정보, 그리고 구조화된 다중 관점의 추론이 필요하다.

Method: 우리는 	extsc{MedLA}라는 대규모 언어 모델에 기반한 논리 주도 다중 에이전트 프레임워크를 제안한다. 각 에이전트는 삼단 논법 구조에 따라 명확한 논리 트리를 구성한다.

Result: 	extsc{MedLA}는 MedDDx와 같은 도전적인 벤치마크에서 정적 역할 기반 시스템 및 단일 에이전트 기준을 지속적으로 초월한다.

Conclusion: 	extsc{MedLA}는 신뢰할 수 있는 의료 추론을 위한 일반화 가능한 패러다임을 제공한다.

Abstract: Answering complex medical questions requires not only domain expertise and
patient-specific information, but also structured and multi-perspective
reasoning. Existing multi-agent approaches often rely on fixed roles or shallow
interaction prompts, limiting their ability to detect and resolve fine-grained
logical inconsistencies. To address this, we propose \textsc{MedLA}, a
logic-driven multi-agent framework built on large language models. Each agent
organizes its reasoning process into an explicit logical tree based on
syllogistic triads (major premise, minor premise, and conclusion), enabling
transparent inference and premise-level alignment. Agents engage in a
multi-round, graph-guided discussion to compare and iteratively refine their
logic trees, achieving consensus through error correction and contradiction
resolution. We demonstrate that \textsc{MedLA} consistently outperforms both
static role-based systems and single-agent baselines on challenging benchmarks
such as MedDDx and standard medical QA tasks. Furthermore, \textsc{MedLA}
scales effectively across both open-source and commercial LLM backbones,
achieving state-of-the-art performance and offering a generalizable paradigm
for trustworthy medical reasoning.

</details>


### [23] [Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark](https://arxiv.org/abs/2509.23735)
*Xuyan Ma,Xiaofei Xie,Yawen Wang,Junjie Wang,Boyu Wu,Mingyang Li,Qing Wang*

Main category: cs.AI

TL;DR: 본 논문은 플랫폼으로 조정된 대리 시스템의 실패 원인 식별을 다룬다. 307개의 실패 로그와 세분화된 주석을 포함한 데이터셋을 구축하였으며, LLM을 활용하여 자동으로 원인을 식별하는 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 대리 시스템이 복잡한 추론 및 문제 해결 작업에 점점 더 많이 사용되고 있지만, 이러한 시스템의 실패 원인을 체계적으로 식별하는 방법은 불분명하다.

Method: 307개의 실패 로그로 구성된 AgentFail 데이터셋을 구축하고, 원인과 실패를 연결하는 세분화된 주석을 추가하며, LLM을 위한 자동 원인 식별 벤치마크를 제안한다.

Result: 제안된 분류법은 성능을 크게 개선하였지만, 원인 식별의 정확도는 최대 33.6%에 불과하여 여전히 도전적인 과제가 남아 있다.

Conclusion: 이 연구는 플랫폼 조정 대리 시스템의 신뢰할 수 있는 실패 원인 데이터셋과 분류법, 벤치마크를 제공하여 보다 신뢰성 있는 대리 시스템 개발의 기초를 마련한다.

Abstract: Agentic systems consisting of multiple LLM-driven agents coordinating through
tools and structured interactions, are increasingly deployed for complex
reasoning and problem-solving tasks. At the same time, emerging low-code and
template-based agent development platforms (e.g., Dify) enable users to rapidly
build and orchestrate agentic systems, which we refer to as
platform-orchestrated agentic systems. However, these systems are also fragile
and it remains unclear how to systematically identify their potential failure
root cause. This paper presents a study of root cause identification of these
platform-orchestrated agentic systems. To support this initiative, we construct
a dataset AgentFail containing 307 failure logs from ten agentic systems, each
with fine-grained annotations linking failures to their root causes. We
additionally utilize counterfactual reasoning-based repair strategy to ensure
the reliability of the annotation. Building on the dataset, we develop a
taxonomy that characterizes failure root causes and analyze their distribution
across different platforms and task domains. Furthermore, we introduce a
benchmark that leverages LLMs for automatically identifying root causes, in
which we also utilize the proposed taxonomy as guidance for LLMs. Results show
that the taxonomy can largely improve the performance, thereby confirming its
utility. Nevertheless, the accuracy of root cause identification reaches at
most 33.6%, which indicates that this task still remains challenging. In light
of these results, we also provide actionable guidelines for building such
agentic systems. In summary, this paper provides a reliable dataset of failure
root cause for platform-orchestrated agentic systems, corresponding taxonomy
and benchmark, which serves as a foundation for advancing the development of
more reliable agentic systems.

</details>


### [24] [GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks](https://arxiv.org/abs/2509.23738)
*Cong Chen,Kaixiang Ji,Hao Zhong,Muzhi Zhu,Anzhou Li,Guo Gan,Ziyuan Huang,Cheng Zou,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.AI

TL;DR: GUI-Shepherd는 드문 보상과 크레딧 할당 문제를 해결하기 위해 단계별 피드백을 제공하는 프로세스 보상 모델이다.


<details>
  <summary>Details</summary>
Motivation: 장기 시퀀스 GUI 작업을 위한 자율 에이전트는 드문 보상과 복잡한 크레딧 할당 문제로 인해 어려움을 겪고 있다.

Method: GUI-Shepherd는 $52$k 상호작용의 대규모 데이터 세트에서 학습되며, 인간이 주석을 단 점수와 GPT-4o 생성 합리성을 포함하여 RL 훈련을 위한 보상 제공자이자 추론 검증자로 작용할 수 있다.

Result: 온라인 AndroidWorld 벤치마크에서 GUI-Shepherd는 multi-turn 온라인 PPO를 통해 성공률을 $7.7$ 포인트 향상시키고, 추론 검증자로 사용될 때 $5.1$ 포인트 개선을 가져온다.

Conclusion: 높은 충실도의 프로세스 감독이 더 능력 있는 GUI 에이전트를 구축하는 데 중요하다는 것을 입증하고 일반화 가능한 솔루션을 제시한다.

Abstract: Autonomous agents for long-sequence Graphical User Interface tasks are
hindered by sparse rewards and the intractable credit assignment problem. To
address these challenges, we introduce GUI-Shepherd, a Process Reward Model
that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is
trained on a diverse large-scale data set of $52$k interactions that features
human-annotated scores and GPT-4o generated rationales, enabling it to serve
both as a reward provider for RL training and as a verifier for inference. As
far as we know, we are the first to conduct a systematic study of process
supervision in GUI agents, across diverse settings from online long-horizon
tasks to offline single-step prediction. On the online AndroidWorld benchmark,
GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,
significantly outperforming Outcome Reward Model based competitors. When used
as an inference verifier, it brings $5.1$ points improvements. The benefits
generalize to the offline AndroidControl benchmark, with gains of $2.2$ points
as a reward provider and $4.3$ points as a verifier. Collectively, our results
establish that high-fidelity process supervision is critical for building more
capable GUI agents and present a generalizable solution.

</details>


### [25] [Transparent Visual Reasoning via Object-Centric Agent Collaboration](https://arxiv.org/abs/2509.23757)
*Benjamin Teoh,Ben Glocker,Francesca Toni,Avinash Kori*

Main category: cs.AI

TL;DR: OCEAN은 인간 이해 가능한 개념에 기반한 설명을 생성하는 해석 가능한 AI 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 설명 가능한 AI의 주요 도전 과제는 인간이 이해할 수 있는 개념에 기반한 설명을 만드는 것이다.

Method: OCEAN은 객체 중심 표현과 투명한 다중 에이전트 추론 프로세스를 기반으로 구축된 프레임워크로, 게임 이론적 추론 프로세스를 통해 일관되고 차별적인 증거에 대한 동의를 이끌어낸다.

Result: OCEAN은 표준 시각 분류기와 GradCAM 및 LIME과 같은 인기 있는 사후 설명 도구에 대해 경쟁력 있는 성능을 보였다.

Conclusion: 사용자 연구에서 참여자들은 OCEAN의 설명을 보다 직관적이고 신뢰할 수 있다고 평가했다.

Abstract: A central challenge in explainable AI, particularly in the visual domain, is
producing explanations grounded in human-understandable concepts. To tackle
this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a
novel, inherently interpretable framework built on object-centric
representations and a transparent multi-agent reasoning process. The
game-theoretic reasoning process drives agents to agree on coherent and
discriminative evidence, resulting in a faithful and interpretable
decision-making process. We train OCEAN end-to-end and benchmark it against
standard visual classifiers and popular posthoc explanation tools like GradCAM
and LIME across two diagnostic multi-object datasets. Our results demonstrate
competitive performance with respect to state-of-the-art black-box models with
a faithful reasoning process, which was reflected by our user study, where
participants consistently rated OCEAN's explanations as more intuitive and
trustworthy.

</details>


### [26] [From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning](https://arxiv.org/abs/2509.23768)
*Cheng Yang,Jiaxuan Lu,Haiyuan Wan,Junchi Yu,Feiwei Qin*

Main category: cs.AI

TL;DR: 이번 연구는 화학 반응 조건 추천을 위한 ChemMAS 멀티 에이전트 시스템을 제안하며, 이는 설명 가능한 AI의 새로운 패러다임을 수립한다.


<details>
  <summary>Details</summary>
Motivation: 화학 반응의 적절한 조건 파라미터를 선택하는 것은 화학 과학을 가속화하는 데 매우 중요하다.

Method: ChemMAS는 조건 예측을 증거 기반 추론 과제로 재구성하고, 메커니즘 기반의 기초, 다채널 회상, 제약 인식 에이전틱 토론, 및 이유 집합으로 작업을 분해한다.

Result: 실험 결과 ChemMAS는 도메인 특화 기준보다 20-35% 더 나은 성과를 내며, 일반 목적의 LLM보다 Top-1 정확도에서 10-15% 우수하다.

Conclusion: ChemMAS는 과학적 발견에서 설명 가능한 AI를 위한 새로운 패러다임을 확립한다.

Abstract: The chemical reaction recommendation is to select proper reaction condition
parameters for chemical reactions, which is pivotal to accelerating chemical
science. With the rapid development of large language models (LLMs), there is
growing interest in leveraging their reasoning and planning capabilities for
reaction condition recommendation. Despite their success, existing methods
rarely explain the rationale behind the recommended reaction conditions,
limiting their utility in high-stakes scientific workflows. In this work, we
propose ChemMAS, a multi-agent system that reframes condition prediction as an
evidence-based reasoning task. ChemMAS decomposes the task into mechanistic
grounding, multi-channel recall, constraint-aware agentic debate, and rationale
aggregation. Each decision is backed by interpretable justifications grounded
in chemical knowledge and retrieved precedents. Experiments show that ChemMAS
achieves 20-35% gains over domain-specific baselines and outperforms
general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,
human-trustable rationales, which establishes a new paradigm for explainable AI
in scientific discovery.

</details>


### [27] [Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules](https://arxiv.org/abs/2509.23836)
*Chenyu Zhou,Xiaoming Shi,Hui Qiu,Xiawu Zheng,Haitao Leng,Yankai Jiang,Shaoguo Liu,Tingting Gao,Rongrong Ji*

Main category: cs.AI

TL;DR: 본 논문은 전자상거래 대화 평가를 위한 새로운 Mix-ECom 데이터셋을 소개하고, 전자상거래 에이전트의 성능을 향상시키기 위한 동적 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전자상거래 에이전트의 활용과 연구를 촉진하기 위해, 전자상거래 도메인에서 LLM 에이전트를 평가하는 벤치마킹 프레임워크가 필요합니다.

Method: 신규 데이터셋인 Mix-ECom을 도입하며, 이는 실제 고객 서비스 대화에 기반하여 개인정보를 제거하고 CoT 프로세스를 추가하여 구성되었습니다.

Result: Mix-ECom은 다양한 대화 유형과 전자상거래 규칙을 포함하는 4,799개의 샘플을 포함하고 있으며, 현재 전자상거래 에이전트들이 복잡한 도메인 규칙으로 인해 충분한 대화 처리 능력이 부족함을 보여줍니다.

Conclusion: 향후 연구와 실무에 도움이 될 이 데이터셋은 공개될 예정입니다.

Abstract: E-commerce agents contribute greatly to helping users complete their
e-commerce needs. To promote further research and application of e-commerce
agents, benchmarking frameworks are introduced for evaluating LLM agents in the
e-commerce domain. Despite the progress, current benchmarks lack evaluating
agents' capability to handle mixed-type e-commerce dialogue and complex domain
rules. To address the issue, this work first introduces a novel corpus, termed
Mix-ECom, which is constructed based on real-world customer-service dialogues
with post-processing to remove user privacy and add CoT process. Specifically,
Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce
dialogue, covering four dialogue types (QA, recommendation, task-oriented
dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics,
after-sales), and 82 e-commerce rules. Furthermore, this work build baselines
on Mix-Ecom and propose a dynamic framework to further improve the performance.
Results show that current e-commerce agents lack sufficient capabilities to
handle e-commerce dialogues, due to the hallucination cased by complex domain
rules. The dataset will be publicly available.

</details>


### [28] [AgentGuard: Runtime Verification of AI Agents](https://arxiv.org/abs/2509.23864)
*Roham Koohestani*

Main category: cs.AI

TL;DR: 자율 AI 시스템의 예측 불가능성으로 인한 위험을 줄이기 위한 새로운 프레임워크인 AgentGuard를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 자율적이고 에이전트적인 AI 시스템의 신속한 발전은 예측 불가능성과 emergent behaviors로 인해 중요한 위험을 동반하며, 이는 전통적인 검증 방법의 불충분함을 초래하고 확률적 보증의 필요성을 제기한다.

Method: AgentGuard는 에이전트의 원시 I/O를 관찰하여 상태 모델의 전이와 대응하는 형식적 사건으로 추상화하며, 온라인 학습을 통해 에이전트의 emergent behavior를 형식적으로 모델링하는 Markov Decision Process (MDP)를 동적으로 구축하고 업데이트한다.

Result: 확률적 모델 검사(probabilistic model checking)를 사용하여 프레임워크는 실시간으로 정량적 특성을 검증한다.

Conclusion: AgentGuard는 자율적 AI 시스템의 런타임 검증을 위한 새로운 패러다임을 제공하여 지속적이고 정량적인 보증을 가능하게 한다.

Abstract: The rapid evolution to autonomous, agentic AI systems introduces significant
risks due to their inherent unpredictability and emergent behaviors; this also
renders traditional verification methods inadequate and necessitates a shift
towards probabilistic guarantees where the question is no longer if a system
will fail, but the probability of its failure within given constraints. This
paper presents AgentGuard, a framework for runtime verification of Agentic AI
systems that provides continuous, quantitative assurance through a new paradigm
called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection
layer that observes an agent's raw I/O and abstracts it into formal events
corresponding to transitions in a state model. It then uses online learning to
dynamically build and update a Markov Decision Process (MDP) that formally
models the agent's emergent behavior. Using probabilistic model checking, the
framework then verifies quantitative properties in real-time.

</details>


### [29] [Rethinking Reward Miscalibration of GRPO in Agentic RL](https://arxiv.org/abs/2509.23870)
*Jingyu Liu,Xiaopeng Wu,Jingquan Peng,Kehan Chen,Chuan Yu,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 이 논문은 자율 에이전트를 훈련하는 과정에서 중간 단계의 잘못된 행동의 처벌 필요성을 강조하며, 이러한 행동이 강화되지 않도록 하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 장기적인 실제 작업을 해결할 수 있는 자율 에이전트를 구축하는 데 연구 관심이 많아졌으나, 결과 기반 보상이 잘못된 중간 단계에 긍정적인 보상을 부여할 수 있는 문제를 제기합니다.

Method: 훈련 중 나쁜 행동을 구분할 수 있도록 행동자(actor)를 훈련시키고, 그에 따라 잘못된 행동의 임베딩을 분리하여 그래디언트 간섭을 완화합니다.

Result: 높은 성능을 보이는 샘플의 그래디언트가 비최적 또는 잘못된 행동을 강화할 수 있다는 점을 입증하였고, 제안한 방법의 효과성을 보여주는 많은 실험을 진행했습니다.

Conclusion: 잘못된 행동은 훈련 중 처벌해야 하며, 이를 통해 자율 에이전트의 학습 성능을 향상시킬 수 있습니다.

Abstract: Building autonomous agents capable of solving long-horizon, real-world tasks
has garnered significant research interest. But outcome based rewards may cause
reward miscalibration which means it might mistakenly allocate positive reward
to flawed middle steps which is regarded as the key reason making the bad
actions being reinforced during training. However we reveal that outcome based
reward ensures expected negative advantage for those flawed middle steps, which
means the flawed actions should be punished during training. Even accounting
for the ``squeezing effect", the probability mass of good actions should
increase and the actor should gradually get rid of harmful actions. This shows
that flawed actions should be punished during training. We further identify
gradient coupling between similar samples as a key issue in agentic RL, the
input prompt is extremely similar and the output action space is limited,
therefore during training, gradients from well-performing samples can
inadvertently strengthen suboptimal or incorrect actions due to similar input
observation and output actions. We show that with gradient coupling, some
flawed actions might be enhanced. To address this, we propose training the
actor to classify good or bad actions to separate the embedding of good/bad
actions and alleviate the gradient interference, extensive experiments shows
its effectiveness.

</details>


### [30] [TusoAI: Agentic Optimization for Scientific Methods](https://arxiv.org/abs/2509.23986)
*Alistair Turcan,Kexin Huang,Lei Li,Martin Jinye Zhang*

Main category: cs.AI

TL;DR: TusoAI는 과학적 작업을 수행하고 평가 기능을 기반으로 계산 방법을 자율적으로 개발하고 최적화하는 에이전트 기반 AI 시스템이다. 이는 기존의 최첨단 전문가 방법을 초월하여 많은 과학적 문제를 해결하는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 실험 데이터를 분석하는 데 필요한 계산 도구의 수동 개발은 과학적 발견을 느리게 하며, 이 과정은 비용이 많이 들고 시간이 소요된다.

Method: TusoAI는 과학 작업 설명과 평가 기능을 입력으로 받아들이고, 도메인 지식을 지식 트리 형태로 통합한 후, 반복적인 도메인-specific 최적화 및 모델 진단을 수행한다.

Result: TusoAI는 다양한 작업에서 최첨단 전문가 방법 및 과학 AI 에이전트를 능가하는 성과를 보였다.

Conclusion: TusoAI를 유전학의 두 가지 주요 개방 문제에 적용하여 기존 계산 방법을 개선하고 자가면역 질환과 T 세포 아형 간의 9개 새로운 연관성과 타겟 유전자와 관련된 질병 변이 간의 7개 이전에 보고되지 않은 연관을 발견하였다.

Abstract: Scientific discovery is often slowed by the manual development of
computational tools needed to analyze complex experimental data. Building such
tools is costly and time-consuming because scientists must iteratively review
literature, test modeling and scientific assumptions against empirical data,
and implement these insights into efficient software. Large language models
(LLMs) have demonstrated strong capabilities in synthesizing literature,
reasoning with empirical data, and generating domain-specific code, offering
new opportunities to accelerate computational method development. Existing
LLM-based systems either focus on performing scientific analyses using existing
computational methods or on developing computational methods or models for
general machine learning without effectively integrating the often unstructured
knowledge specific to scientific domains. Here, we introduce TusoAI , an
agentic AI system that takes a scientific task description with an evaluation
function and autonomously develops and optimizes computational methods for the
application. TusoAI integrates domain knowledge into a knowledge tree
representation and performs iterative, domain-specific optimization and model
diagnosis, improving performance over a pool of candidate solutions. We
conducted comprehensive benchmark evaluations demonstrating that TusoAI
outperforms state-of-the-art expert methods, MLE agents, and scientific AI
agents across diverse tasks, such as single-cell RNA-seq data denoising and
satellite-based earth monitoring. Applying TusoAI to two key open problems in
genetics improved existing computational methods and uncovered novel biology,
including 9 new associations between autoimmune diseases and T cell subtypes
and 7 previously unreported links between disease variants linked to their
target genes. Our code is publicly available at
https://github.com/Alistair-Turcan/TusoAI.

</details>


### [31] [LLM/Agent-as-Data-Analyst: A Survey](https://arxiv.org/abs/2509.23988)
*Zirui Tang,Weizheng Wang,Zihang Zhou,Yang Jiao,Bangrui Xu,Boyu Niu,Xuanhe Zhou,Guoliang Li,Yeye He,Wei Zhou,Yitong Song,Cheng Tan,Bin Wang,Conghui He,Xiaoyang Wang,Fan Wu*

Main category: cs.AI

TL;DR: LLM 및 에이전트 기법이 데이터 분석에 중요한 영향을 미치고 있으며, 전통적인 접근 방식과 비교할 때 더 복잡한 데이터 이해 및 자연어 인터페이스를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: LLM 및 에이전트 기법은 학계와 산업에서 데이터 분석에 큰 영향을 미치고 있음.

Method: 복합 데이터 이해, 자연어 인터페이스, 의미 분석 기능 및 자율 파이프라인 오케스트레이션을 통한 데이터 분석.

Result: LLM 기반 기술을 활용하여 구조적, 반구조적, 비구조적, 이종 데이터에 대한 다양한 분석 기법을 검토함.

Conclusion: LLM 및 에이전트 기반 데이터 분석의 발전을 위한 인사이트와 실용적 방향성을 제시함.

Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a
LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both
academica and industry. In comparison with traditional rule or small-model
based approaches, (agentic) LLMs enable complex data understanding, natural
language interfaces, semantic analysis functions, and autonomous pipeline
orchestration. The technical evolution further distills five key design goals
for intelligent data analysis agents, namely semantic-aware design,
modality-hybrid integration, autonomous pipelines, tool-augmented workflows,
and support for open-world tasks. From a modality perspective, we review
LLM-based techniques for (i) structured data (e.g., table question answering
for relational data and NL2GQL for graph data), (ii) semi-structured data
(e.g., markup languages understanding and semi-structured table modeling),
(iii) unstructured data (e.g., chart understanding, document understanding,
programming languages vulnerable detection), and (iv) heterogeneous data (e.g.,
data retrieval and modality alignment for data lakes). Finally, we outline the
remaining challenges and propose several insights and practical directions for
advancing LLM/Agent-powered data analysis.

</details>


### [32] [Do Repetitions Matter? Strengthening Reliability in LLM Evaluations](https://arxiv.org/abs/2509.24086)
*Miguel Angel Alvarado Gonzalez,Michelle Bruno Hernandez,Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Sandra Malagon*

Main category: cs.AI

TL;DR: LLM 리더보드는 종종 단일 확률적 실행에 의존하지만, 신뢰할 수 있는 결론을 위해 필요한 반복 횟수는 불분명하다. 우리는 AI4Math 벤치마크에서 8개의 최첨단 모델을 세 가지 독립적인 실행으로 재평가하였다. 결과적으로 단일 실행 리더보드의 취약성을 발견하고, 평가를 실험으로 간주하며 불확실성을 보고하고 확률적 디코딩 하에 2회 이상의 반복을 사용할 것을 권장한다.


<details>
  <summary>Details</summary>
Motivation: LLM 모델 성능을 평가하는 리더보드의 신뢰성을 높이기 위해.

Method: AI4Math 벤치마크에서 세 가지 독립적인 실행을 통해 여덟 개 모델을 재평가하고, 혼합 효과 로지스틱 회귀와 도메인 수준 한계 평균, 순위 불안정성 분석 및 실행 간 신뢰성을 사용하여 추가 반복의 가치를 평가했다.

Result: 단일 실행 리더보드의 취약성을 발견하였고, 두 번의 실행은 단일 실행의 약 83ase%의 순위 반전을 제거한다는 결과를 얻었다.

Conclusion: 평가를 실험으로 간주하고 불확실성을 보고하며 확률적 디코딩 하에 두 회 이상의 반복을 사용하면 보다 신뢰할 수 있는 평가를 할 수 있다.

Abstract: LLM leaderboards often rely on single stochastic runs, but how many
repetitions are required for reliable conclusions remains unclear. We
re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three
independent runs per setting. Using mixed-effects logistic regression,
domain-level marginal means, rank-instability analysis, and run-to-run
reliability, we assessed the value of additional repetitions. Our findings
shows that Single-run leaderboards are brittle: 10/12 slices (83\%) invert at
least one pairwise rank relative to the three-run majority, despite a zero
sign-flip rate for pairwise significance and moderate overall interclass
correlation. Averaging runs yields modest SE shrinkage ($\sim$5\% from one to
three) but large ranking gains; two runs remove $\sim$83\% of single-run
inversions. We provide cost-aware guidance for practitioners: treat evaluation
as an experiment, report uncertainty, and use $\geq 2$ repetitions under
stochastic decoding. These practices improve robustness while remaining
feasible for small teams and help align model comparisons with real-world
reliability.

</details>


### [33] [Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs](https://arxiv.org/abs/2509.24107)
*Shreyas Singh,Kunal Singh,Pradeep Moturi*

Main category: cs.AI

TL;DR: Fathom-DeepResearch는 정보 탐색 임무를 수행하는 두 개의 특화된 모델로 구성된 에이전트 시스템입니다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 응용 프로그램을 가능하게 하기 위해 도구 통합 추론이 주요 초점으로 떠오르고 있습니다.

Method: Fathom-DeepResearch는 Fathom-Search-4B와 Fathom-Synthesizer-4B의 두 가지 모델로 구성됩니다. 첫 번째 모델은 Qwen3-4B에서 훈련된 DeepSearch 모델로, 실시간 웹 검색과 목표 웹페이지 쿼리를 통한 증거 기반 조사를 최적화합니다.

Result: 이 시스템은 DeepSearch 벤치마크와 DeepResearch-Bench에서 최첨단 성능을 달성하며 다양한 추론 작업에 대한 강한 일반화 능력을 보여줍니다.

Conclusion: 이 개선 사항은 20회 이상의 도구 호출을 신뢰할 수 있게 확장할 수 있게 합니다.

Abstract: Tool-integrated reasoning has emerged as a key focus for enabling agentic
applications. Among these, DeepResearch Agents have gained significant
attention for their strong performance on complex, open-ended
information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system
composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch
model trained from Qwen3-4B and optimized for evidence-based investigation
through live web search and targeted webpage querying. Its training combines
three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent
self-play that enforces strict web-search dependence and heterogeneous source
grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes
multi-turn Reinforcement Learning with Verifiable Rewards through curriculum
pruning, reward-aware advantage scaling, and per-prompt replay buffers; and
(iii) a steerable step-level reward that classifies each tool call by cognitive
behavior and marginal utility, enabling explicit control over search trajectory
breadth, depth, and horizon. These improvements enable reliable extension of
tool-calling beyond 20 calls when warranted. The second is
Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn
DeepSearch traces into structured, citation-dense DeepResearch Reports for
comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES,
WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves
state-of-the-art performance in the open-weights category while demonstrating
strong generalization to diverse reasoning tasks including HLE, AIME-25,
GPQA-Diamond, and MedQA.

</details>


### [34] [Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework](https://arxiv.org/abs/2509.24127)
*Nooshin Bahador*

Main category: cs.AI

TL;DR: 이 논문은 자연어 인터페이스와 복잡한 기업 데이터 웨어하우스 간의 간극을 메우기 위한 모듈형 구성 요소 기반 아키텍처를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 비전문 사용자들이 대화형 인터페이스를 통해 복잡한 데이터 웨어하우스와 상호작용할 수 있도록 하여 데이터 접근성의 핵심 문제를 해결하는 것입니다.

Method: 모듈형 아키텍처를 사용하여 비즈니스 규칙과 데이터 포인트를 통해 결론을 추적하여 의사결정의 '이유'를 설명하는 다층적 추론 프레임워크를 구현합니다.

Result: 이 시스템은 성능 벤치마킹과 업데이트 중 성능 퇴보 감지를 자동화하여 신뢰성을 확보하고, 통계적 문맥 모듈이 모든 결론을 정량적 증거로 뒷받침합니다.

Conclusion: 이 통합된 에이전트 개발 및 평가 프레임워크는 데이터 민감도가 높은 고위험 도메인에서 LLM 기반 에이전트를 배포하기 위한 강력하고 신뢰할 수 있는 시스템을 만듭니다.

Abstract: This article presents a modular, component-based architecture for developing
and evaluating AI agents that bridge the gap between natural language
interfaces and complex enterprise data warehouses. The system directly
addresses core challenges in data accessibility by enabling non-technical users
to interact with complex data warehouses through a conversational interface,
translating ambiguous user intent into precise, executable database queries to
overcome semantic gaps. A cornerstone of the design is its commitment to
transparent decision-making, achieved through a multi-layered reasoning
framework that explains the "why" behind every decision, allowing for full
interpretability by tracing conclusions through specific, activated business
rules and data points. The architecture integrates a robust quality assurance
mechanism via an automated evaluation framework that serves multiple functions:
it enables performance benchmarking by objectively measuring agent performance
against golden standards, and it ensures system reliability by automating the
detection of performance regressions during updates. The agent's analytical
depth is enhanced by a statistical context module, which quantifies deviations
from normative behavior, ensuring all conclusions are supported by quantitative
evidence including concrete data, percentages, and statistical comparisons. We
demonstrate the efficacy of this integrated agent-development-with-evaluation
framework through a case study on an insurance claims processing system. The
agent, built on a modular architecture, leverages the BigQuery ecosystem to
perform secure data retrieval, apply domain-specific business rules, and
generate human-auditable justifications. The results confirm that this approach
creates a robust, evaluable, and trustworthy system for deploying LLM-powered
agents in data-sensitive, high-stakes domains.

</details>


### [35] [ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration](https://arxiv.org/abs/2509.24230)
*Shaobin Ling,Yun Wang,Chenyou Fan,Tin Lun Lam,Junjie Hu*

Main category: cs.AI

TL;DR: ELHPlan은 액션 체인을 기초 계획 요소로 하여 다중 로봇 협업을 지원하는 새로운 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 다중 로봇 환경에서 지능적인 협업을 위한 효율적인 계획 방식 필요.

Method: ELHPlan은 의도에 바인딩된 액션 시퀀스를 생성하고, 충돌 및 실행 가능성을 검증하며, 문제를 해결하고, 검증된 액션을 실행하는 순환 프로세스를 사용합니다.

Result: ELHPlan은 TDW-MAT 및 C-WAH 벤치마크에서 기존 방법들의 24%에 해당하는 토큰 소비로 유사한 작업 성공률을 달성합니다.

Conclusion: 우리의 연구는 LLM 기반의 다중 에이전트 계획 시스템을 위한 새로운 효율성-효과성 경계를 설정합니다.

Abstract: Large Language Models (LLMs) enable intelligent multi-robot collaboration but
face fundamental trade-offs: declarative methods lack adaptability in dynamic
environments, while iterative methods incur prohibitive computational costs
that scale poorly with team size and task complexity. In this paper, we propose
ELHPlan, a novel framework that introduces Action Chains--sequences of actions
explicitly bound to sub-goal intentions--as the fundamental planning primitive.
ELHPlan operates via a cyclical process: 1) constructing intention-bound action
sequences, 2) proactively validating for conflicts and feasibility, 3) refining
issues through targeted mechanisms, and 4) executing validated actions. This
design balances adaptability and efficiency by providing sufficient planning
horizons while avoiding expensive full re-planning. We further propose
comprehensive efficiency metrics, including token consumption and planning
time, to more holistically evaluate multi-agent collaboration. Our experiments
on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable
task success rates while consuming only 24% of the tokens required by
state-of-the-art methods. Our research establishes a new
efficiency-effectiveness frontier for LLM-based multi-agent planning systems.

</details>


### [36] [Model Merging Scaling Laws in Large Language Models](https://arxiv.org/abs/2509.24244)
*Yuanyi Wang,Yanggan Gu,Yiming Zhang,Qi Zhou,Zhaoyi Yan,Congkai Xie,Xinyao Wang,Jianbo Yuan,Hongxia Yang*

Main category: cs.AI

TL;DR: 언어 모델 병합에 대한 경험적 스케일링 법칙을 연구한다. 우리는 전문가 수와 모델 크기에 연결된 간결한 힘 법칙을 식별하고, 이를 통해 예측 가능한 계획을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 모델 병합 과정에서 전문가를 추가하거나 모델 크기를 조정할 때의 수익을 예측할 수 있는 정량적 규칙이 부족하다.

Method: 모델 크기와 전문가 수 간의 관계를 나타내는 힘 법칙을 통해 다양한 아키텍처와 방법에서 측정된 곡선에 잘 맞는 결과를 도출한다.

Result: 전문가 수에 따른 확장으로 수익이 감소하는 경향과 더 많은 전문가가 포함될수록 변동성이 줄어드는 두 가지 강력한 규칙성을 설명한다.

Conclusion: 이 법칙은 목표 손실에 도달하기 위해 필요한 전문가 수를 추정하고 전문가 추가 중단 시점을 결정하는 등의 예측 계획을 가능하게 하며, 분산 생성 AI를 위한 스케일링 원칙을 제안한다.

Abstract: We study empirical scaling laws for language model merging measured by
cross-entropy. Despite its wide practical use, merging lacks a quantitative
rule that predicts returns as we add experts or scale the model size. We
identify a compact power law that links model size and expert number: the
size-dependent floor decreases with model capacity, while the merging tail
exhibits clear diminishing returns in the number of experts. The law holds
in-domain and cross-domain, tightly fits measured curves across diverse
architectures and methods (Average, TA, TIES, DARE), and explains two robust
regularities: most gains arrive early, and variability shrinks as more experts
are included. Building on this, we present a simple theory that explains why
gains fall roughly as 1/k and links the floor and tail to properties of the
base model and the diversity across domains. This law enables predictive
planning: estimate how many experts are needed to reach a target loss, decide
when to stop adding experts, and trade off scaling the base model versus adding
experts under a fixed budget--turning merging from heuristic practice into a
computationally efficient, planable alternative to multitask training. This
suggests a scaling principle for distributed generative AI: predictable gains
can be achieved by composing specialists, offering a complementary path toward
AGI-level systems.

</details>


### [37] [PAME-AI: Patient Messaging Creation and Optimization using Agentic AI](https://arxiv.org/abs/2509.24263)
*Junjie Luo,Yihong Guo,Anqi Liu,Ritu Agarwal,Gordon Gao*

Main category: cs.AI

TL;DR: PAME-AI는 환자 메시지 생성 및 최적화를 위한 인공지능 기반의 혁신적인 접근법으로, 높은 성능의 메시지 디자인을 위한 데이터 변환을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 모바일 메시지 디자인은 다차원 디자인 공간을 탐색하는 데 한계가 있어 환자 메시지를 개선할 필요성이 있다.

Method: PAME-AI는 DIKW 계층을 기반으로 하여, 실험 데이터를 실질적인 메시지 디자인 전략으로 변환하는 전문 컴퓨팅 에이전트 시스템으로 구성된다.

Result: 두 단계의 실험을 통해 최종 생성된 메시지가 68.76%의 참여율을 기록하여 61.27%의 기준선에 비해 12.2% 개선되었다.

Conclusion: 이 에이전틱 아키텍처는 대규모 의료 커뮤니케이션 최적화에 매우 적합한 병렬 처리, 가설 검증 및 지속적인 학습을 가능하게 한다.

Abstract: Messaging patients is a critical part of healthcare communication, helping to
improve things like medication adherence and healthy behaviors. However,
traditional mobile message design has significant limitations due to its
inability to explore the high-dimensional design space. We develop PAME-AI, a
novel approach for Patient Messaging Creation and Optimization using Agentic
AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI
offers a structured framework to move from raw data to actionable insights for
high-performance messaging design. PAME-AI is composed of a system of
specialized computational agents that progressively transform raw experimental
data into actionable message design strategies. We demonstrate our approach's
effectiveness through a two-stage experiment, comprising of 444,691 patient
encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated
message achieved 68.76% engagement compared to the 61.27% baseline,
representing a 12.2% relative improvement in click-through rates. This agentic
architecture enables parallel processing, hypothesis validation, and continuous
learning, making it particularly suitable for large-scale healthcare
communication optimization.

</details>


### [38] [G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge](https://arxiv.org/abs/2509.24276)
*Linhao Luo,Zicheng Zhao,Junnan Liu,Zhangchi Qiu,Junnan Dong,Serge Panev,Chen Gong,Thuy-Trang Vu,Gholamreza Haffari,Dinh Phung,Alan Wee-Chung Liew,Shirui Pan*

Main category: cs.AI

TL;DR: G-reasoner는 그래프와 언어 기반 모델을 통합하여 다양한 그래프 구조 지식에 대한 추론을 수행하는 통합 프레임워크로, QuadGraph라는 네 가지 레이어 추상화를 중심으로 한다.


<details>
  <summary>Details</summary>
Motivation: 기존 RAGs는 지식 집중 작업에서 단편화된 정보와 약한 지식 구조 모델링으로 인한 한계가 있다.

Method: G-reasoner 프레임워크는 QuadGraph라는 표준화된 네 가지 레이어 추상화를 이용하여 이질적인 지식 소스를 통합하고, 34M 파라미터의 그래프 기초 모델(GFM)을 도입하여 그래프 구조와 텍스트 의미를 함께 캡처한다.

Result: G-reasoner는 6개의 벤치마크에서 최신 기술과 비교하여 지속적으로 우수한 성능을 보였으며, LLM의 추론 능력을 크게 향상시켰고, 효율성과 크로스-그래프 일반화에서도 강력한 성과를 달성했다.

Conclusion: G-reasoner는 그래프 기반 모델과 언어 모델의 통합을 통해 지식 구조의 추론 성능을 개선하고, 확장성과 효율성을 보장한다.

Abstract: Large language models (LLMs) excel at complex reasoning but remain limited by
static and incomplete parametric knowledge. Retrieval-augmented generation
(RAG) mitigates this by incorporating external knowledge, yet existing RAGs
struggle with knowledge-intensive tasks due to fragmented information and weak
modeling of knowledge structure. Graphs offer a natural way to model
relationships within knowledge, but LLMs are inherently unstructured and cannot
effectively reason over graph-structured data. Recent graph-enhanced RAG
(GraphRAG) attempts to bridge this gap by constructing tailored graphs and
enabling LLMs to reason on them. However, these methods often depend on ad-hoc
graph designs, heuristic search, or costly agent pipelines, which hinder
scalability and generalization. To address these challenges, we present
G-reasoner, a unified framework that integrates graph and language foundation
models for reasoning over diverse graph-structured knowledge. Central to our
approach is QuadGraph, a standardized four-layer abstraction that unifies
heterogeneous knowledge sources into a common graph representation. Building on
this, we introduce a 34M-parameter graph foundation model (GFM) that jointly
captures graph topology and textual semantics, and is integrated with LLMs to
enhance reasoning in downstream applications. To ensure scalability and
efficiency, mixed-precision training and distributed message-passing are
implemented to scale GFM with more GPUs. Extensive experiments on six
benchmarks show that G-reasoner consistently outperforms state-of-the-art
baselines, significantly enhances LLM reasoning, and achieves strong efficiency
and cross-graph generalization.

</details>


### [39] [MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning](https://arxiv.org/abs/2509.24314)
*Hongjun Liu,Yinghao Zhu,Yuhui Wang,Yitao Long,Zeyu Lai,Lequan Yu,Chen Zhao*

Main category: cs.AI

TL;DR: 이 논문에서는 진단 사례의 초기 증거 해석의 불안정성이 환각으로 이어지는 주요 실패 모드를 드러내며, 임상 추론 에이전트에서의 확률성과 환각을 제한할 필요성을 강조합니다. 이를 해결하기 위해 MedMMV라는 신뢰할 수 있고 검증 가능한 임상 추론을 위한 다중 에이전트 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 진단 사례의 초기 증거 해석에서의 불안정성이 환각으로 이어지는 문제를 해결하고자 함.

Method: MedMMV는 다각적인 단기 롤아웃을 통해 추론을 안정화하고, 환각 탐지기의 감독 하에 구조화된 증거 그래프에서 중간 단계를 기반으로 하며, 결합된 불확실성 점수를 통해 후보 경로를 집계하는 다중 에이전트 프레임워크입니다.

Result: MedMMV는 여섯 개의 의료 벤치마크에서 정확도를 최대 12.7% 향상시키고, 신뢰성을 크게 개선했습니다.

Conclusion: 불안정성을 검증 가능한 다중 에이전트 프로세스를 통해 제어함으로써, 우리의 프레임워크는 임상 의사결정 지원과 같은 고위험 영역에서 신뢰할 수 있는 AI 시스템을 배포하는 강력한 경로를 제공합니다.

Abstract: Recent progress in multimodal large language models (MLLMs) has demonstrated
promising performance on medical benchmarks and in preliminary trials as
clinical assistants. Yet, our pilot audit of diagnostic cases uncovers a
critical failure mode: instability in early evidence interpretation precedes
hallucination, creating branching reasoning trajectories that cascade into
globally inconsistent conclusions. This highlights the need for clinical
reasoning agents that constrain stochasticity and hallucination while producing
auditable decision flows. We introduce MedMMV, a controllable multimodal
multi-agent framework for reliable and verifiable clinical reasoning. MedMMV
stabilizes reasoning through diversified short rollouts, grounds intermediate
steps in a structured evidence graph under the supervision of a Hallucination
Detector, and aggregates candidate paths with a Combined Uncertainty scorer. On
six medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more
critically, demonstrates superior reliability. Blind physician evaluations
confirm that MedMMV substantially increases reasoning truthfulness without
sacrificing informational content. By controlling instability through a
verifiable, multi-agent process, our framework provides a robust path toward
deploying trustworthy AI systems in high-stakes domains like clinical decision
support.

</details>


### [40] [ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling](https://arxiv.org/abs/2509.24460)
*Haotian Zhang,Liu Liu,Baosheng Yu,Jiayan Qiu,Likang Xiao,Yanwei Ren,Quan Chen,Xianglong Liu*

Main category: cs.AI

TL;DR: 본 논문에서는 PRMs의 한계를 극복하기 위해 도메인 중립적인 논리적 흐름을 모델링하는 방법을 제시하고, 이를 통해 다양한 도메인에서의 일반화 능력을 향상시키는 새로운 데이터 주석 및 훈련 프레임워크를 개발하였다.


<details>
  <summary>Details</summary>
Motivation: PRMs는 LLM의 수학적 추론 능력을 향상시키는 데 효과적이지만, 도메인 특정 훈련 데이터의 부족이 일반화 능력을 제한한다.

Method: 도메인 특정 지식 검증에서 도메인 중립적인 논리적 흐름 모델링으로 학습 목표를 전환하고, CoT 단계 간의 문맥 일관성에 초점을 맞춘 새로운 데이터 주석 및 훈련 프레임워크를 개발한다.

Result: ContextPRM 모델은 MMLU-Pro의 아홉 개 비수학적 도메인에서 가중 다수 결정 투표를 통해 평균 6.5%의 정확도 향상을 이루었다.

Conclusion: ContextPRM은 수학적 도메인과 비수학적 도메인 모두에서 일관된 성능을 발휘하며 PRMs의 일반화 능력을 개선한다.

Abstract: Process reward models (PRMs) have demonstrated significant efficacy in
enhancing the mathematical reasoning capabilities of large language models
(LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit
substantial gains in mathematical domains, the scarcity of domain-specific
training data and knowledge-based learning patterns limits their generalization
ability when faced with other domains. To address this limitation, we shift the
learning objective from verifying domain-specific knowledge to modeling
domain-agnostic logical flow. Centering on contextual coherence between
chain-of-thought (CoT) steps, our approach is realized through a novel data
annotation and training framework, which enhances the model's generalization
capabilities across diverse domains. For instance, our resulting model,
ContextPRM, achieves a notable 6.5% average accuracy improvement over the
majority voting baseline via weighted majority voting across nine
non-mathematical domains in MMLU-Pro, including law, history, and philosophy,
significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from
other mathematics-focused PRMs, demonstrating consistent performance across
both mathematical and non-mathematical domains.

</details>


### [41] [Training Agents Inside of Scalable World Models](https://arxiv.org/abs/2509.24527)
*Danijar Hafner,Wilson Yan,Timothy Lillicrap*

Main category: cs.AI

TL;DR: Dreamer 4는 복잡한 환경에서의 객체 상호작용을 정확하게 예측하는 월드 모델로, 마인크래프트에서 다이아몬드를 오프라인 데이터만으로 얻는 최초의 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 지능형 에이전트를 위한 훈련 행동을 상상 속에서 할 수 있는 경험을 시뮬레이션하는 월드 모델에서 일반 지식을 획득하는 경로를 제시하고자 함.

Method: Dreamer 4는 빠르고 정확한 월드 모델 내에서 강화학습을 통해 제어 작업을 해결하는 확장 가능한 에이전트를 학습한다.

Result: Dreamer 4의 월드 모델은 마인크래프트에서 객체 상호작용과 게임 메커니즘을 정확하게 예측하여 이전 월드 모델보다 월등히 향상된 성능을 보여준다.

Conclusion: 이 연구는 상상 훈련을 위한 확장 가능한 레시피를 제공하며, 지능형 에이전트를 향한 한 걸음을 내딛는다.

Abstract: World models learn general knowledge from videos and simulate experience for
training behaviors in imagination, offering a path towards intelligent agents.
However, previous world models have been unable to accurately predict object
interactions in complex environments. We introduce Dreamer 4, a scalable agent
that learns to solve control tasks by reinforcement learning inside of a fast
and accurate world model. In the complex video game Minecraft, the world model
accurately predicts object interactions and game mechanics, outperforming
previous world models by a large margin. The world model achieves real-time
interactive inference on a single GPU through a shortcut forcing objective and
an efficient transformer architecture. Moreover, the world model learns general
action conditioning from only a small amount of data, allowing it to extract
the majority of its knowledge from diverse unlabeled videos. We propose the
challenge of obtaining diamonds in Minecraft from only offline data, aligning
with practical applications such as robotics where learning from environment
interaction can be unsafe and slow. This task requires choosing sequences of
over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors
in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft
purely from offline data, without environment interaction. Our work provides a
scalable recipe for imagination training, marking a step towards intelligent
agents.

</details>


### [42] ["Stop replacing salt with sugar!'': Towards Intuitive Human-Agent Teaching](https://arxiv.org/abs/2509.24651)
*Nikolaos Kondylidis,Andrea Rafanelli,Ilaria Tiddi,Annette ten Teije,Frank van Harmelen*

Main category: cs.AI

TL;DR: 인간은 소수의 예제에서 새로운 개념을 빠르게 학습하지만, 이 능력을 인공지능 시스템에서 재현하는 것은 도전적이다. 본 연구에서는 인간과 에이전트 간의 직관적인 교육 아키텍처를 제안하며, 에이전트가 제한된 예제에서 점진적으로 학습할 수 있도록 도우며, 최적의 예제를 선택하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 인간과 같은 방식으로 인공지능이 주관적인 작업을 학습할 수 있도록 하는 것이 필요하다.

Method: 인간이 에이전트에게 시범을 제공하여 작업을 수행하도록 가르치는 직관적인 아키텍처를 제안하고, 도메인 지식을 활용하여 에이전트의 작업 이해를 넓히며, 제한된 예제에서 효과적으로 학습할 수 있는 방법을 적용한다.

Result: 에이전트는 100개의 예제만으로도 작업 성능의 절반에 도달할 수 있었다.

Conclusion: 전략적인 순서로 예제를 제공하고 외부 상징 지식을 활용하는 학습 방법을 통해 에이전트가 더 효율적으로 일반화할 수 있음을 보여준다.

Abstract: Humans quickly learn new concepts from a small number of examples.
Replicating this capacity with Artificial Intelligence (AI) systems has proven
to be challenging. When it comes to learning subjective tasks-where there is an
evident scarcity of data-this capacity needs to be recreated. In this work, we
propose an intuitive human-agent teaching architecture in which the human can
teach an agent how to perform a task by providing demonstrations, i.e.,
examples. To have an intuitive interaction, we argue that the agent should be
able to learn incrementally from a few single examples. To allow for this, our
objective is to broaden the agent's task understanding using domain knowledge.
Then, using a learning method to enable the agent to learn efficiently from a
limited number of examples. Finally, to optimize how human can select the most
representative and less redundant examples to provide the agent with. We apply
our proposed method to the subjective task of ingredient substitution, where
the agent needs to learn how to substitute ingredients in recipes based on
human examples. We replicate human input using the Recipe1MSubs dataset. In our
experiments, the agent achieves half its task performance after only 100
examples are provided, compared to the complete training set of 50k examples.
We show that by providing examples in strategic order along with a learning
method that leverages external symbolic knowledge, the agent can generalize
more efficiently.

</details>


### [43] [Successful Misunderstandings: Learning to Coordinate Without Being Understood](https://arxiv.org/abs/2509.24660)
*Nikolaos Kondylidis,Anil Yaman,Frank van Harmelen,Erman Acar,Annette ten Teije*

Main category: cs.AI

TL;DR: 이 논문은 신호 게임을 통해 개인들이 새로운 신호 어휘를 개발하여 성공적으로 조정하는 과정을 조사합니다. 에이전트들이 환경을 다르게 인식할 때 의사소통이 어떻게 나타나는지를 연구하며, 성공적인 오해가 발생할 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 통신이 조정을 얼마나 잘 촉진하는지를 평가하는 것이 주된 접근법입니다.

Method: 신호 게임을 통해 개인들이 신호의 새로운 어휘를 개발하고 성공적으로 조정하도록 합니다.

Result: 인구가 항상 최적의 조정 수준으로 수렴하나, 때로는 에이전트들이 서로 다른 해석을 하여 성공적인 오해로 수렴합니다.

Conclusion: 상호작용 파트너와 성공적인 조정을 수립하는 데 어려움을 겪는 미스얼라인 해석을 사용한 에이전트는 새로운 파트너와의 조정에 실패할 수 있습니다.

Abstract: The main approach to evaluating communication is by assessing how well it
facilitates coordination. If two or more individuals can coordinate through
communication, it is generally assumed that they understand one another. We
investigate this assumption in a signaling game where individuals develop a new
vocabulary of signals to coordinate successfully. In our game, the individuals
do not have common observations besides the communication signal and outcome of
the interaction, i.e. received reward. This setting is used as a proxy to study
communication emergence in populations of agents that perceive their
environment very differently, e.g. hybrid populations that include humans and
artificial agents. Agents develop signals, use them, and refine interpretations
while not observing how other agents are using them. While populations always
converge to optimal levels of coordination, in some cases, interacting agents
interpret and use signals differently, converging to what we call successful
misunderstandings. However, agents of population that coordinate using
misaligned interpretations, are unable to establish successful coordination
with new interaction partners. Not leading to coordination failure immediately,
successful misunderstandings are difficult to spot and repair. Having at least
three agents that all interact with each other are the two minimum conditions
to ensure the emergence of shared interpretations. Under these conditions, the
agent population exhibits this emergent property of compensating for the lack
of shared observations of signal use, ensuring the emergence of shared
interpretations.

</details>


### [44] [From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning](https://arxiv.org/abs/2509.24765)
*Yunyao Zhang,Xinglang Zhang,Junxi Sheng,Wenbing Li,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 본 연구는 논리 모델의 논리적 복잡성과 의미적 복잡성을 동시에 다루기 위해 LogicAgent라는 프레임워크를 제안하며, 새로운 데이터셋 RepublicQA를 통해 기존 방법론의 한계를 극복하고 LLM의 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 현재의 연구는 논리적 및 의미적 복잡성 간의 상호작용을 간과하고 있으며, 이는 인간의 사고 과정에 필수적인 복잡한 상황을 다루는 데 어려움을 초래한다.

Method: LogicAgent는 1차 논리에서 다면적 추론을 수행하며, 존재론적 수입 검사와 3값 결정 체계를 통해 공허한 추론을 완화한다. 또한, 새로운 벤치마크인 RepublicQA를 통해 더 높은 수준의 복잡성을 제공한다.

Result: LogicAgent는 RepublicQA에서 6.25%의 평균 성능 향상을 달성했으며, 다른 일반적 논리 추론 벤치마크에서도 추가적으로 7.05%의 평균 성능 향상을 보였다.

Conclusion: 이 결과는 LogicAgent가 LLM의 논리적 성능 향상에 있어 매우 효과적인 방법임을 강조한다.

Abstract: Logical reasoning is a fundamental capability of large language models
(LLMs). However, existing studies largely overlook the interplay between
logical complexity and semantic complexity, resulting in methods that struggle
to address challenging scenarios involving abstract propositions, ambiguous
contexts, and conflicting stances, which are central to human reasoning. For
this gap, we propose LogicAgent, a semiotic-square-guided framework designed to
jointly address logical complexity and semantic complexity. LogicAgent
explicitly performs multi-perspective deduction in first-order logic (FOL),
while mitigating vacuous reasoning through existential import checks that
incorporate a three-valued decision scheme (True, False, Uncertain) to handle
boundary cases more faithfully. Furthermore, to overcome the semantic
simplicity and low logical complexity of existing datasets, we introduce
RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94)
and exhibits substantially greater lexical and structural diversity than prior
benchmarks. RepublicQA is grounded in philosophical concepts, featuring
abstract propositions and systematically organized contrary and contradictory
relations, making it the most semantically rich resource for evaluating logical
reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art
performance on RepublicQA, with a 6.25% average gain over strong baselines, and
generalizes effectively to mainstream logical reasoning benchmarks including
ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05%
average gain. These results highlight the strong effectiveness of our
semiotic-grounded multi-perspective reasoning in boosting LLMs' logical
performance.

</details>


### [45] [PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System](https://arxiv.org/abs/2509.24855)
*Fangchen Yu,Junchi Yao,Ziyi Wang,Haiyuan Wan,Youling Huang,Bo Zhang,Shuyue Hu,Dongzhan Zhou,Ning Ding,Ganqu Cui,Lei Bai,Wanli Ouyang,Peng Ye*

Main category: cs.AI

TL;DR: PhysicsMinions는 물리 학올림피아드를 위한 협동 진화형 다중 에이전트 시스템으로, 물리 문제 해결 능력을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 물리 문제 해결 능력은 현실 세계의 물리적 지능을 나타내며, 물리 학올림피아드는 이러한 능력을 시험할 수 있는 플랫폼이다.

Method: Visual Studio, Logic Studio, Review Studio의 세 가지 협력 스튜디오로 구성되어 있으며, Review Studio의 피드백을 통해 Logic Studio가 스스로 교정하도록 한다.

Result: PhysicsMinions는 7개의 최신 물리 학올림피아드에서 강력한 일반화 능력과 역사적인 돌파구를 달성하고, 평균 점수 기준에서 최초의 개방형 금메달을 수상하였다.

Conclusion: PhysicsMinions는 학올림피아드 수준의 문제 해결을 위한 일반화된 프레임워크로, 다양한 분야에 확장 가능성이 있다.

Abstract: Physics is central to understanding and shaping the real world, and the
ability to solve physics problems is a key indicator of real-world physical
intelligence. Physics Olympiads, renowned as the crown of competitive physics,
provide a rigorous testbed requiring complex reasoning and deep multimodal
understanding, yet they remain largely underexplored in AI research. Existing
approaches are predominantly single-model based, and open-source MLLMs rarely
reach gold-medal-level performance. To address this gap, we propose
PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its
architecture features three synergistic studios: a Visual Studio to interpret
diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform
dual-stage verification. The system coevolves through an iterative refinement
loop where feedback from the Review Studio continuously guides the Logic
Studio, enabling the system to self-correct and converge towards the ground
truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads,
PhysicsMinions delivers three major breakthroughs: (i) Strong generalization:
it consistently improves both open-source and closed-source models of different
sizes, delivering clear benefits over their single-model baselines; (ii)
Historic breakthroughs: it elevates open-source models from only 1-2 to 6 gold
medals across 7 Olympiads, achieving the first-ever open-source gold medal in
the latest International Physics Olympiad (IPhO) under the average-score
metric; and (iii) Scaling to human expert: it further advances the open-source
Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406
contestants and far surpassing the top single-model score of 22.7 (ranked
22nd). Generally, PhysicsMinions offers a generalizable framework for
Olympiad-level problem solving, with the potential to extend across
disciplines.

</details>


### [46] [The Emergence of Social Science of Large Language Models](https://arxiv.org/abs/2509.24877)
*Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)의 사회 과학은 시스템이 마음 귀속을 어떻게 유도하고, 서로 상호작용하며, 인간의 활동과 제도를 어떻게 변화시키는지를 조사한다. 270개의 연구를 체계적으로 검토하여 텍스트 임베딩, 비지도 클러스터링 및 주제 모델링을 결합해 계산적 분류 체계를 구축하였다. 검토된 문헌에서 세 가지 영역이 자연스럽게 등장한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 사회적 영향력을 이해하고, 이를 통해 인지, 도덕성 및 편향 같은 개념을 어떻게 유도하는지에 대해 밝히기 위함이다.

Method: 270개의 연구를 체계적으로 리뷰하여 텍스트 임베딩, 비지도 클러스터링, 주제 모델링을 활용하여 계산적 분류 체계를 개발하였다.

Result: 세 가지 주요 영역: LLM을 사회적 마음으로서, LLM 사회, LLM-인간 상호작용이 드러났다.

Conclusion: 이 분류 체계는 파편화된 분야의 재현 가능한 지도를 제공하고, 분석 수준에 따른 증거 기준을 명확히 하며, 인공지능의 사회 과학에서 누적적 발전의 기회를 강조한다.

Abstract: The social science of large language models (LLMs) examines how these systems
evoke mind attributions, interact with one another, and transform human
activity and institutions. We conducted a systematic review of 270 studies,
combining text embeddings, unsupervised clustering and topic modeling to build
a computational taxonomy. Three domains emerge organically across the reviewed
literature. LLM as Social Minds examines whether and when models display
behaviors that elicit attributions of cognition, morality and bias, while
addressing challenges such as test leakage and surface cues. LLM Societies
examines multi-agent settings where interaction protocols, architectures and
mechanism design shape coordination, norms, institutions and collective
epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,
learning, trust, work and governance, and how risks arise at the human-AI
interface. This taxonomy provides a reproducible map of a fragmented field,
clarifies evidentiary standards across levels of analysis, and highlights
opportunities for cumulative progress in the social science of artificial
intelligence.

</details>


### [47] [MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning](https://arxiv.org/abs/2509.24922)
*Huihao Jing,Wenbin Hu,Hongyu Luo,Jianhui Yang,Wei Fan,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: 법률 작업을 위한 다중 에이전트 시스템(MAS)의 잠재력을 평가하는 새로운 벤치마크인 MASLegalBench를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 법률 분야에서 MAS의 잠재력을 극대화하기 위한 평가 방법의 필요.

Method: GDPR을 적용 시나리오로 활용하여 다양한 역할 기반 MAS를 수동으로 설계하고 최신 LLM을 사용하여 실험을 수행.

Result: 기존 모델과 MAS 아키텍처의 강점, 한계 및 개선 가능 영역을 강조.

Conclusion: 법률 작업에 적합한 MAS의 잠재력을 활용할 수 있는 방법을 제시한다.

Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large
Language Models (LLMs), show great potential in addressing complex tasks. In
this context, integrating MAS with legal tasks is a crucial step. While
previous studies have developed legal benchmarks for LLM agents, none are
specifically designed to consider the unique advantages of MAS, such as task
decomposition, agent specialization, and flexible training. In fact, the lack
of evaluation methods limits the potential of MAS in the legal domain. To
address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS
and designed with a deductive reasoning approach. Our benchmark uses GDPR as
the application scenario, encompassing extensive background knowledge and
covering complex reasoning processes that effectively reflect the intricacies
of real-world legal situations. Furthermore, we manually design various
role-based MAS and conduct extensive experiments using different
state-of-the-art LLMs. Our results highlight the strengths, limitations, and
potential areas for improvement of existing models and MAS architectures.

</details>


### [48] [When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?](https://arxiv.org/abs/2509.24927)
*An Guo,Shuoxiao Zhang,Enyi Tang,Xinyu Gao,Haomin Pang,Haoxiang Tian,Yanzhou Mu,Wu Wen,Chunrong Fang,Zhenyu Chen*

Main category: cs.AI

TL;DR: V2X 협동 인식 시스템의 성능을 평가하고 오류 유형을 분석하여 시스템의 설계와 개선에 기여하는 연구.


<details>
  <summary>Details</summary>
Motivation: 단일 에이전트 인식 시스템의 한계를 극복하기 위해 V2X 협동 인식의 가능성을 모색함.

Method: V2X 협동 인식에 대한 실증 연구를 통해 6가지 주요 오류 패턴을 식별하고 분석.

Result: LiDAR 기반 협동 설정이 최상의 성능을 보였으며, 통신 유형에 따라 협동 인식 성능이 다르다. 또한 협동 인식 오류 증가가 운전 위반 빈도의 증가와 연관이 있음을 발견했다.

Conclusion: 협동 인식 시스템의 취약점을 밝혀내어 시스템 설계 및 개선에 기여하고자 한다.

Abstract: With the tremendous advancement of deep learning and communication
technology, Vehicle-to-Everything (V2X) cooperative perception has the
potential to address limitations in sensing distant objects and occlusion for a
single-agent perception system. V2X cooperative perception systems are software
systems characterized by diverse sensor types and cooperative agents, varying
fusion schemes, and operation under different communication conditions.
Therefore, their complex composition gives rise to numerous operational
challenges. Furthermore, when cooperative perception systems produce erroneous
predictions, the types of errors and their underlying causes remain
insufficiently explored. To bridge this gap, we take an initial step by
conducting an empirical study of V2X cooperative perception. To systematically
evaluate the impact of cooperative perception on the ego vehicle's perception
performance, we identify and analyze six prevalent error patterns in
cooperative perception systems. We further conduct a systematic evaluation of
the critical components of these systems through our large-scale study and
identify the following key findings: (1) The LiDAR-based cooperation
configuration exhibits the highest perception performance; (2)
Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication
exhibit distinct cooperative perception performance under different fusion
schemes; (3) Increased cooperative perception errors may result in a higher
frequency of driving violations; (4) Cooperative perception systems are not
robust against communication interference when running online. Our results
reveal potential risks and vulnerabilities in critical components of
cooperative perception systems. We hope that our findings can better promote
the design and repair of cooperative perception systems.

</details>


### [49] [Agentic Exploration of Physics Models](https://arxiv.org/abs/2509.24978)
*Maximilian Nägele,Florian Marquardt*

Main category: cs.AI

TL;DR: SciExplorer는 과학적 발견을 위한 자동화된 탐색 도구로, 물리 시스템을 탐색하는 데 사용되며, 최소한의 도구로도 인상적인 성과를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 기존의 과학적 발견 과정은 관찰, 분석, 가설 생성의 상호작용에 의존하며, 머신러닝은 이 과정의 개별 측면을 해결하기 위해 점점 더 많이 채택되고 있다.

Method: SciExplorer는 대형 언어 모델의 도구 사용 능력을 활용하여 도메인 특정 설계 없이 시스템을 자유롭게 탐색할 수 있도록 하는 에이전트이다.

Result: SciExplorer는 기계 역학 시스템, 파동 진화, 양자 다체 물리학 등 다양한 모델을 실험하며, 최소한의 도구로도 운동 방정식을 회복하고 기대값에서 해밀토니안을 추론하는 등 인상적인 성과를 보여준다.

Conclusion: 이 setup의 효과는 세밀한 조정이나 특정 작업 지침 없이 다른 분야에서도 유사한 과학적 탐색을 가능하게 한다.

Abstract: The process of scientific discovery relies on an interplay of observations,
analysis, and hypothesis generation. Machine learning is increasingly being
adopted to address individual aspects of this process. However, it remains an
open challenge to fully automate the open-ended, heuristic, iterative loop
required to discover the laws of an unknown system by exploring it through
experiments and analysis, without tailoring the approach to the specifics of a
given task. Here, we introduce SciExplorer, an agent that leverages large
language model tool-use capabilities to enable free-form exploration of systems
without any domain-specific blueprints, and apply it to the exploration of
physical systems that are initially unknown to the agent. We test SciExplorer
on a broad set of models spanning mechanical dynamical systems, wave evolution,
and quantum many-body physics. Despite using a minimal set of tools, primarily
based on code execution, we observe impressive performance on tasks such as
recovering equations of motion from observed dynamics and inferring
Hamiltonians from expectation values. The demonstrated effectiveness of this
setup opens the door towards similar scientific exploration in other domains,
without the need for finetuning or task-specific instructions.

</details>


### [50] [Scaling Synthetic Task Generation for Agents via Exploration](https://arxiv.org/abs/2509.25047)
*Ram Ramrakhya,Andrew Szot,Omar Attia,Yuhao Yang,Anh Nguyen,Bogdan Mazoure,Zhe Gan,Harsh Agrawal,Alexander Toshev*

Main category: cs.AI

TL;DR: AutoPlay는 상호작용 환경을 탐색하여 환경 기반 작업을 생성하는 스케일 가능한 작업 생성 파이프라인입니다.


<details>
  <summary>Details</summary>
Motivation: 후 훈련된 다중 모달 대형 언어 모델을 사용한 상호작용 에이전트 구축의 필요성과 기존 작업 생성 방법의 한계를 극복하고자 합니다.

Method: AutoPlay는 탐색 단계와 작업 생성 단계로 구성되며, 탐색 단계에서 MLLM 탐색 에이전트가 새로운 환경 상태를 발견하고, 작업 생성 단계에서 다양한 실행 가능한 작업을 합성합니다.

Result: AutoPlay는 20개의 Android 애플리케이션에서 20,000개의 작업과 13개의 Ubuntu 애플리케이션에서 10,000개의 작업을 생성했습니다.

Conclusion: 이 결과는 AutoPlay가 인간 주석에 대한 의존도를 줄이는 스케일 가능한 후 훈련 접근법으로 자리 잡았음을 보여줍니다.

Abstract: Post-Training Multimodal Large Language Models (MLLMs) to build interactive
agents holds promise across domains such as computer-use, web navigation, and
robotics. A key challenge in scaling such post-training is lack of high-quality
downstream agentic task datasets with tasks that are diverse, feasible, and
verifiable. Existing approaches for task generation rely heavily on human
annotation or prompting MLLM with limited downstream environment information,
which is either costly or poorly scalable as it yield tasks with limited
coverage. To remedy this, we present AutoPlay, a scalable pipeline for task
generation that explicitly explores interactive environments to discover
possible interactions and current state information to synthesize
environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration
phase, where an MLLM explorer agent systematically uncovers novel environment
states and functionalities, and (ii) a task generation phase, where a task
generator leverages exploration trajectories and a set of task guideline
prompts as context to synthesize diverse, executable, and verifiable tasks. We
show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks
across 13 applications Ubuntu applications to train mobile-use and computer-use
agents. AutoPlay generated tasks enable large-scale task demonstration
synthesis without human annotation by employing an MLLM task executor and
verifier. This data enables training MLLM-based UI agents that improve success
rates up to $20.0\%$ on mobile-use and $10.9\%$ on computer-use scenarios. In
addition, AutoPlay generated tasks combined with MLLM verifier-based rewards
enable scaling reinforcement learning training of UI agents, leading to an
additional $5.7\%$ gain. coverage. These results establish AutoPlay as a
scalable approach for post-training capable MLLM agents reducing reliance on
human annotation.

</details>


### [51] [Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning](https://arxiv.org/abs/2509.25052)
*Sai Wang,Yu Wu,Zhongwen Xu*

Main category: cs.AI

TL;DR: 본 연구는 인공지능 에이전트가 복잡한 환경을 이해하고 학습하는 새로운 패러다임인 CEL(Cogito, ergo ludo)을 제안하며, 이를 통해 에이전트가 명확한 언어 기반의 이해를 바탕으로 효과적으로 게임을 학습하는 방식을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 심층 강화 학습 방법은 막대한 경험에 의존하며, 신경망 가중치 내에 지식을 불투명하게 인코딩한다. 따라서 우리는 에이전트가 추론 및 계획을 통해 학습하도록 하는 새로운 패러다임이 필요하다는 동기가 있다.

Method: CEL은 대형 언어 모델(LLM)을 활용하여 환경의 역학과 자신의 전략에 대한 명시적인 이해를 구축한다. 에이전트는 빈 상태에서 시작해 상호작용과 반성의 주기로 운영되며, 각 에피소드 후에는 전체 경로를 분석하여 환경의 역학 모델을 개선하는 규칙 유도와 경험을 바탕으로 실행 가능한 전략 지침을 요약하는 두 가지 학습 과정을 수행한다.

Result: CEL 에이전트는 Minesweeper, Frozen Lake, Sokoban과 같은 다양한 그리드 월드 작업에서 성공적으로 게임 규칙을 자율적으로 발견하고 드문 보상으로부터 효과적인 정책을 개발함으로써 게임을 마스터하는 것을 보여준다.

Conclusion: 반복적인 과정이 지속적인 학습에 매우 중요하다는 것을 확인하는 배제 연구도 진행했다. 이는 더 일반적이고 해석 가능한 에이전트를 향한 길을 보여 주며, 에이전트는 효과적으로 행동할 뿐만 아니라 원시 경험에 대한 명시적 추론을 통해 자신의 세계에 대한 투명하고 개선된 모델을 구축한다.

Abstract: The pursuit of artificial agents that can learn to master complex
environments has led to remarkable successes, yet prevailing deep reinforcement
learning methods often rely on immense experience, encoding their knowledge
opaquely within neural network weights. We propose a different paradigm, one in
which an agent learns to play by reasoning and planning. We introduce Cogito,
ergo ludo (CEL), a novel agent architecture that leverages a Large Language
Model (LLM) to build an explicit, language-based understanding of its
environment's mechanics and its own strategy. Starting from a tabula rasa state
with no prior knowledge (except action set), CEL operates on a cycle of
interaction and reflection. After each episode, the agent analyzes its complete
trajectory to perform two concurrent learning processes: Rule Induction, where
it refines its explicit model of the environment's dynamics, and Strategy and
Playbook Summarization, where it distills experiences into an actionable
strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,
Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent
successfully learns to master these games by autonomously discovering their
rules and developing effective policies from sparse rewards. Ablation studies
confirm that the iterative process is critical for sustained learning. Our work
demonstrates a path toward more general and interpretable agents that not only
act effectively but also build a transparent and improving model of their world
through explicit reasoning on raw experience.

</details>


### [52] [Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs](https://arxiv.org/abs/2509.25139)
*Yue Zhang,Tianyi Ma,Zun Wang,Yanyuan Qiao,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 이 논문은 다양한 관점에서의 텍스트 설명을 활용하여 내비게이션 에이전트의 맥락 이해를 향상시키는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 LLM 기반 VLN 에이전트들은 시각적 세부 정보를 단순화하거나 고수준 추론이 필요한 추상적 의미를 포착하지 못하는 문제를 가지고 있습니다.

Method: 여러 관점의 텍스트 설명을 통합하여 비유적 추론을 촉진하고, 이를 통해 내비게이션 에이전트의 전반적인 장면 이해 및 공간 추론을 증진시킵니다.

Result: R2R 데이터셋에서 실험을 통해 내비게이션 성능이 유의미하게 향상됨을 입증했습니다.

Conclusion: 텍스트 기반 비유적 추론을 활용하여 에이전트의 행동 결정을 보다 정확하게 합니다.

Abstract: Integrating large language models (LLMs) into embodied AI models is becoming
increasingly prevalent. However, existing zero-shot LLM-based
Vision-and-Language Navigation (VLN) agents either encode images as textual
scene descriptions, potentially oversimplifying visual details, or process raw
image inputs, which can fail to capture abstract semantics required for
high-level reasoning. In this paper, we improve the navigation agent's
contextual understanding by incorporating textual descriptions from multiple
perspectives that facilitate analogical reasoning across images. By leveraging
text-based analogical reasoning, the agent enhances its global scene
understanding and spatial reasoning, leading to more accurate action decisions.
We evaluate our approach on the R2R dataset, where our experiments demonstrate
significant improvements in navigation performance.

</details>


### [53] [ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://arxiv.org/abs/2509.25140)
*Siru Ouyang,Jun Yan,I-Hung Hsu,Yanfei Chen,Ke Jiang,Zifeng Wang,Rujun Han,Long T. Le,Samira Daruki,Xiangru Tang,Vishy Tirumalashetty,George Lee,Mahsan Rofouei,Hangfei Lin,Jiawei Han,Chen-Yu Lee,Tomas Pfister*

Main category: cs.AI

TL;DR: 대형 언어 모델 에이전트가 지속적인 실세계 역할에서 tarefas를 수행하면서, 이들은 누적된 상호작용 역사에서 학습하는 데 실패하여 귀중한 통찰력을 버리고 과거의 오류를 반복하는 문제를 해결하기 위해 ReasoningBank라는 새로운 메모리 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델 에이전트의 현실 세계에서의 지속적인 역할과 이들이 자주 직면하는 작업의 연속적인 스트림으로 인해, 상호작용 역사에서 학습하지 못하는 것이 큰 한계로 작용한다.

Method: ReasoningBank라는 메모리 프레임워크는 에이전트가 자기 평가한 성공적 및 실패한 경험에서 일반화 가능한 추론 전략을 추출하고, 테스트 시 관련 기억을 조회하여 상호작용에 정보를 제공하며, 새로운 학습을 통합하여 시간이 지남에 따라 더 유능해지도록 한다. 또한, 메모리 인식 테스트 시간 스케일링(MaTTS)을 도입하여 에이전트의 상호작용 경험을 확장하고 가속화시킨다.

Result: ReasoningBank는 웹 탐색 및 소프트웨어 엔지니어링 벤치마크에서 기존 기억 메커니즘을 지속적으로 능가하며, 효과성과 효율성을 개선한다. MaTTS는 이러한 이익을 더욱 증폭시킨다.

Conclusion: 이 연구 결과는 메모리 구동 경험 스케일링이 새로운 스케일링 차원으로 자리 잡으며, 에이전트가 자연스럽게 emergent behaviors와 함께 스스로 발전할 수 있게 한다.

Abstract: With the growing adoption of large language model agents in persistent
real-world roles, they naturally encounter continuous streams of tasks. A key
limitation, however, is their failure to learn from the accumulated interaction
history, forcing them to discard valuable insights and repeat past errors. We
propose ReasoningBank, a novel memory framework that distills generalizable
reasoning strategies from an agent's self-judged successful and failed
experiences. At test time, an agent retrieves relevant memories from
ReasoningBank to inform its interaction and then integrates new learnings back,
enabling it to become more capable over time. Building on this powerful
experience learner, we further introduce memory-aware test-time scaling
(MaTTS), which accelerates and diversifies this learning process by scaling up
the agent's interaction experience. By allocating more compute to each task,
the agent generates abundant, diverse experiences that provide rich contrastive
signals for synthesizing higher-quality memory. The better memory in turn
guides more effective scaling, establishing a powerful synergy between memory
and test-time scaling. Across web browsing and software engineering benchmarks,
ReasoningBank consistently outperforms existing memory mechanisms that store
raw trajectories or only successful task routines, improving both effectiveness
and efficiency; MaTTS further amplifies these gains. These findings establish
memory-driven experience scaling as a new scaling dimension, enabling agents to
self-evolve with emergent behaviors naturally arise.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs](https://arxiv.org/abs/2509.22796)
*Xingyu Li,Juefei Pu,Yifan Wu,Xiaochen Zou,Shitong Zhu,Xiaochen Zou,Shitong Zhu,Qiushi Wu,Zheng Zhang,Joshua Hsu,Yue Dong,Zhiyun Qian,Kangjie Lu,Trent Jaeger,Michael De Lucia,Srikanth V. Krishnamurthy*

Main category: cs.CR

TL;DR: 리눅스 커널의 보안 패치 분류를 개선하기 위한 DUALLM이라는 이중 방법 파이프라인을 제안하며, 이를 통해 보안 취약점에 대한 인식을 높이고 정확도를 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 리눅스 커널에 보안 패치가 지속적으로 통합되지만, 하류 유지보수자들이 이를 채택하는 데 지연이 발생하여 취약성이 생김.

Method: 커밋 제목/메시지와 코드 컨텍스트를 활용하여 LLM과 미세 조정된 소형 언어 모델을 기반으로 두 가지 접근 방식을 통합한 DUALLM이라는 이중 방법 파이프라인을 개발함.

Result: DUALLM은 87.4%의 정확도와 0.875의 F1 점수를 달성하여 이전의 솔루션보다 크게 향상됨.

Conclusion: DUALLM은 5,140개의 최근 리눅스 커널 패치 중 111개를 OOB 또는 UAF 취약점으로 식별하며, 90건의 진짜 양성 사례가 수동 검증을 통해 확인됨.

Abstract: Open-source software projects are foundational to modern software ecosystems,
with the Linux kernel standing out as a critical exemplar due to its ubiquity
and complexity. Although security patches are continuously integrated into the
Linux mainline kernel, downstream maintainers often delay their adoption,
creating windows of vulnerability. A key reason for this lag is the difficulty
in identifying security-critical patches, particularly those addressing
exploitable vulnerabilities such as out-of-bounds (OOB) accesses and
use-after-free (UAF) bugs. This challenge is exacerbated by intentionally
silent bug fixes, incomplete or missing CVE assignments, delays in CVE
issuance, and recent changes to the CVE assignment criteria for the Linux
kernel. While fine-grained patch classification approaches exist, they exhibit
limitations in both coverage and accuracy. In this work, we identify previously
unexplored opportunities to significantly improve fine-grained patch
classification. Specifically, by leveraging cues from commit titles/messages
and diffs alongside appropriate code context, we develop DUALLM, a dual-method
pipeline that integrates two approaches based on a Large Language Model (LLM)
and a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an
F1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM
successfully identified 111 of 5,140 recent Linux kernel patches as addressing
OOB or UAF vulnerabilities, with 90 true positives confirmed by manual
verification (many do not have clear indications in patch descriptions).
Moreover, we constructed proof-of-concepts for two identified bugs (one UAF and
one OOB), including one developed to conduct a previously unknown control-flow
hijack as further evidence of the correctness of the classification.

</details>


### [55] [Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions](https://arxiv.org/abs/2509.22814)
*Aditi Tiwari,Akshit Bhalla,Darshan Prasad*

Main category: cs.CR

TL;DR: MCP는 에이전트-툴 상호작용을 위한 스키마 기반 실행 모델을 정의하며, 시각 시스템에서의 첫 번째 프로토콜 수준 감사 결과를 통해 시스템적 약점을 식별하였다.


<details>
  <summary>Details</summary>
Motivation: MCP의 신뢰성과 보안을 개선하기 위해 시각 흐름의 모듈화된 컴퓨터 비전 워크플로우를 지원하고자 함.

Method: 91개의 공개 등록된 시각 중심 MCP 서버를 분석하고, 프로토콜 위반을 감지하는 실행 가능한 벤치마크와 검증기 개발.

Result: 78.0% 시스템에서 스키마 형식 불일치, 24.6%에서 좌표 관례 오류, 100회 실행에서 평균 33.8회 경고가 발생함.

Conclusion: 제안된 벤치마크와 검증기 모음은 컴포지셔널 비전 워크플로우의 신뢰성과 보안을 측정하고 개선할 수 있는 재현 가능한 프레임워크를 수립한다.

Abstract: The Model Context Protocol (MCP) defines a schema bound execution model for
agent-tool interaction, enabling modular computer vision workflows without
retraining. To our knowledge, this is the first protocol level, deployment
scale audit of MCP in vision systems, identifying systemic weaknesses in schema
semantics, interoperability, and runtime coordination. We analyze 91 publicly
registered vision centric MCP servers, annotated along nine dimensions of
compositional fidelity, and develop an executable benchmark with validators to
detect and categorize protocol violations. The audit reveals high prevalence of
schema format divergence, missing runtime schema validation, undeclared
coordinate conventions, and reliance on untracked bridging scripts. Validator
based testing quantifies these failures, with schema format checks flagging
misalignments in 78.0 percent of systems, coordinate convention checks
detecting spatial reference errors in 24.6 percent, and memory scope checks
issuing an average of 33.8 warnings per 100 executions. Security probes show
that dynamic and multi agent workflows exhibit elevated risks of privilege
escalation and untyped tool connections. The proposed benchmark and validator
suite, implemented in a controlled testbed and to be released on GitHub,
establishes a reproducible framework for measuring and improving the
reliability and security of compositional vision workflows.

</details>


### [56] [Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting](https://arxiv.org/abs/2509.23571)
*Yuqiao Meng,Luoxi Tang,Feiyang Yu,Xi Li,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TL;DR: 이 논문은 사이버 위협 탐지를 위한 대형 언어 모델을 효과적으로 활용하기 위한 CyberTeam 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 사이버 위협의 규모와 정교함이 증가함에 따라 블루 팀 방어자들은 위험을 사전에 감지하고 완화하기 위해 고급 도구를 필요로 한다.

Method: CyberTeam은 두 단계로 구성된 표준화된 워크플로우를 구축하여 실제적인 위협 탐지 작업을 모델링하고, 각 작업을 특정 분석 요구 사항에 맞게 조정된 운영 모듈을 통해 처리한다.

Result: CyberTeam은 30개의 작업과 9개의 운영 모듈을 통합하여 LLM이 표준화된 위협 분석을 수행할 수 있도록 안내한다.

Conclusion: 표준화된 설계가 가능한 개선 사항을 강조하고, 실제 위협 탐지에서 열린 추론의 한계를 드러낸다.

Abstract: As cyber threats continue to grow in scale and sophistication, blue team
defenders increasingly require advanced tools to proactively detect and
mitigate risks. Large Language Models (LLMs) offer promising capabilities for
enhancing threat analysis. However, their effectiveness in real-world blue team
threat-hunting scenarios remains insufficiently explored. This paper presents
CyberTeam, a benchmark designed to guide LLMs in blue teaming practice.
CyberTeam constructs a standardized workflow in two stages. First, it models
realistic threat-hunting workflows by capturing the dependencies among
analytical tasks from threat attribution to incident response. Next, each task
is addressed through a set of operational modules tailored to its specific
analytical requirements. This transforms threat hunting into a structured
sequence of reasoning steps, with each step grounded in a discrete operation
and ordered according to task-specific dependencies. Guided by this framework,
LLMs are directed to perform threat-hunting tasks through modularized steps.
Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs
through standardized threat analysis. We evaluate both leading LLMs and
state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended
reasoning strategies. Our results highlight the improvements enabled by
standardized design, while also revealing the limitations of open-ended
reasoning in real-world threat hunting.

</details>


### [57] [A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications](https://arxiv.org/abs/2509.23680)
*Shidong Pan,Yikai Ge,Xiaoyu Sun*

Main category: cs.CR

TL;DR: 안드로이드에서 작업 실행이 가능한 음성 비서의 개인 정보 위험을 포괄적으로 분석한 연구.


<details>
  <summary>Details</summary>
Motivation: 기술 발전과 함께 작업 실행이 가능한 음성 비서의 사용이 증가함에 따라 개인 정보 위험에 대한 체계적인 검토가 필요하다.

Method: 10개의 주요 음성 비서를 대상으로 운영 특성을 분석하고, 6가지 출처를 통해 개인 정보 선언을 교차 검증하였다.

Result: 음성 비서의 개인 정보 선언에서 일관되지 않은 부분이 드러났고, 3가지 주요 개인 정보 위협 모델이 발견되었다.

Conclusion: 이 연구는 개인 정보 보호 실무자에게 유용한 권고 사항을 제공하고, 새로운 자율 AI 에이전트와의 연결성을 강조한다.

Abstract: With the development of foundation AI technologies, task-executable voice
assistants (VAs) have become more popular, enhancing user convenience and
expanding device functionality. Android task-executable VAs are applications
that are capable of understanding complex tasks and performing corresponding
operations. Given their prevalence and great autonomy, there is no existing
work examine the privacy risks within the voice assistants from the
task-execution pattern in a holistic manner. To fill this research gap, this
paper presents a user-centric comprehensive empirical study on privacy risks in
Android task-executable VA applications. We collect ten mainstream VAs as our
research target and analyze their operational characteristics. We then
cross-check their privacy declarations across six sources, including privacy
labels, policies, and manifest files, and our findings reveal widespread
inconsistencies. Moreover, we uncover three significant privacy threat models:
(1) privacy misdisclosure in mega apps, where integrated mini apps such as
Alexa skills are inadequately represented; (2) privilege escalation via
inter-application interactions, which exploit Android's communication
mechanisms to bypass user consent; and (3) abuse of Google system applications,
enabling apps to evade the declaration of dangerous permissions. Our study
contributes actionable recommendations for practitioners and underscores
broader relevance of these privacy risks to emerging autonomous AI agents.

</details>


### [58] [Automated Vulnerability Validation and Verification: A Large Language Model Approach](https://arxiv.org/abs/2509.24037)
*Alireza Lotfi,Charalampos Katsis,Elisa Bertino*

Main category: cs.CR

TL;DR: 본 논문은 소프트웨어 취약점을 체계적으로 관리하고 활용하기 위한 AI 기반 파이프라인을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 소프트웨어 취약점은 기업 네트워크의 보안에 중대한 도전을 제공하며, 효과적인 취약점 평가 및 완화를 위한 고품질 데이터셋의 부족이 문제다.

Method: 본 연구는 CVE 공개 데이터에서 정보를 추출하고, 이를 외부 지식(예: 위협 조언, 코드 스니펫)으로 보강하며, 각 취약점에 대한 컨테이너화된 환경과 익스플로잇 코드를 자동 생성하는 다단계 파이프라인을 제안한다.

Result: 다양한 종류의 취약점에 대해 파이프라인의 효과성을 입증하였으며, CVE 설명에서의 일관성 문제를 폭로하였다.

Conclusion: 이 접근법은 다양한 LLM에서 작동할 수 있으며, 재현성을 가능하게 하고 보안 연구를 가속화하기 위해 아티팩트를 오픈 소싱하였다.

Abstract: Software vulnerabilities remain a critical security challenge, providing
entry points for attackers into enterprise networks. Despite advances in
security practices, the lack of high-quality datasets capturing diverse exploit
behavior limits effective vulnerability assessment and mitigation. This paper
introduces an end-to-end multi-step pipeline leveraging generative AI,
specifically large language models (LLMs), to address the challenges of
orchestrating and reproducing attacks to known software vulnerabilities. Our
approach extracts information from CVE disclosures in the National
Vulnerability Database, augments it with external public knowledge (e.g.,
threat advisories, code snippets) using Retrieval-Augmented Generation (RAG),
and automates the creation of containerized environments and exploit code for
each vulnerability. The pipeline iteratively refines generated artifacts,
validates attack success with test cases, and supports complex multi-container
setups. Our methodology overcomes key obstacles, including noisy and incomplete
vulnerability descriptions, by integrating LLMs and RAG to fill information
gaps. We demonstrate the effectiveness of our pipeline across different
vulnerability types, such as memory overflows, denial of service, and remote
code execution, spanning diverse programming languages, libraries and years. In
doing so, we uncover significant inconsistencies in CVE descriptions,
emphasizing the need for more rigorous verification in the CVE disclosure
process. Our approach is model-agnostic, working across multiple LLMs, and we
open-source the artifacts to enable reproducibility and accelerate security
research. To the best of our knowledge, this is the first system to
systematically orchestrate and exploit known vulnerabilities in containerized
environments by combining general-purpose LLM reasoning with CVE data and
RAG-based context enrichment.

</details>


### [59] [Takedown: How It's Done in Modern Coding Agent Exploits](https://arxiv.org/abs/2509.24240)
*Eunkyu Lee,Donghyeon Kim,Wonyoung Kim,Insu Yun*

Main category: cs.CR

TL;DR: 이번 논문은 8개의 실제 코딩 에이전트에 대한 포괄적인 보안 분석을 제공하며, 기존 접근 방식의 한계를 극복하고, 다양한 보안 문제를 식별하여 사용자 시스템의 기밀성과 무결성에 대한 위협을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 코딩 에이전트는 소프트웨어 개발에 특화된 LLM 기반의 에이전트로, 전통적인 AI 코딩 도우미와 달리 더 복잡한 작업을 수행하는 데 있어 보안과 개인 정보 보호에 대한 우려가 커지고 있습니다.

Method: 8개의 실제 코딩 에이전트의 내부 작업 흐름을 체계적으로 조사하고 각 구성 요소의 보안 위협을 식별하는 방식으로 분석을 수행했습니다.

Result: 15개의 보안 문제를 식별했으며, 이는 사용자 시스템의 기밀성과 무결성을 침해할 수 있습니다. 이 문제들은 단순 개별 취약점이 아니라 집합적으로 종단 간 악용으로 이어질 수 있습니다.

Conclusion: 현대 LLM 기반 에이전트의 포괄적인 보안 분석의 필요성을 강조하며, 보안 고려사항이 충분하지 않을 경우 심각한 취약점이 발생할 수 있음을 보여줍니다.

Abstract: Coding agents, which are LLM-driven agents specialized in software
development, have become increasingly prevalent in modern programming
environments. Unlike traditional AI coding assistants, which offer simple code
completion and suggestions, modern coding agents tackle more complex tasks with
greater autonomy, such as generating entire programs from natural language
instructions. To enable such capabilities, modern coding agents incorporate
extensive functionalities, which in turn raise significant concerns over their
security and privacy. Despite their growing adoption, systematic and in-depth
security analysis of these agents has largely been overlooked.
  In this paper, we present a comprehensive security analysis of eight
real-world coding agents. Our analysis addresses the limitations of prior
approaches, which were often fragmented and ad hoc, by systematically examining
the internal workflows of coding agents and identifying security threats across
their components. Through the analysis, we identify 15 security issues,
including previously overlooked or missed issues, that can be abused to
compromise the confidentiality and integrity of user systems. Furthermore, we
show that these security issues are not merely individual vulnerabilities, but
can collectively lead to end-to-end exploitations. By leveraging these security
issues, we successfully achieved arbitrary command execution in five agents and
global data exfiltration in four agents, all without any user interaction or
approval. Our findings highlight the need for a comprehensive security analysis
in modern LLM-driven agents and demonstrate how insufficient security
considerations can lead to severe vulnerabilities.

</details>


### [60] [When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation](https://arxiv.org/abs/2509.24272)
*Weibo Zhao,Jiahao Liu,Bonan Ruan,Shaofei Li,Zhenkai Liang*

Main category: cs.CR

TL;DR: MCP 서버의 보안 위험에 대한 첫 번째 체계적 연구 발견.


<details>
  <summary>Details</summary>
Motivation: MCP 서버의 빠른 확산으로 인한 심각한 보안 위험이 존재하며 이의 구체적인 분석이 필요하다.

Method: MCP 서버를 위협 행위자로 간주하고, 공격 유형과 취약성을 분석하며, 실제 공격 실행 가능성을 연구한다.

Result: MCP 서버를 통한 12개 공격 분류를 제안하고, 각 유형에 대한 PoC 서버를 개발하여 실제 환경에서의 효과를 입증하였다.

Conclusion: MCP 생태계의 보안을 강화하기 위해 다양한 이해관계자의 협력이 필요하다.

Abstract: Model Context Protocol (MCP) servers enable AI applications to connect to
external systems in a plug-and-play manner, but their rapid proliferation also
introduces severe security risks. Unlike mature software ecosystems with
rigorous vetting, MCP servers still lack standardized review mechanisms, giving
adversaries opportunities to distribute malicious implementations. Despite this
pressing risk, the security implications of MCP servers remain underexplored.
To address this gap, we present the first systematic study that treats MCP
servers as active threat actors and decomposes them into core components to
examine how adversarial developers can implant malicious intent. Specifically,
we investigate three research questions: (i) what types of attacks malicious
MCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models
(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP
server attacks in practice. Our study proposes a component-based taxonomy
comprising twelve attack categories. For each category, we develop
Proof-of-Concept (PoC) servers and demonstrate their effectiveness across
diverse real-world host-LLM settings. We further show that attackers can
generate large numbers of malicious servers at virtually no cost. We then test
state-of-the-art scanners on the generated servers and found that existing
detection approaches are insufficient. These findings highlight that malicious
MCP servers are easy to implement, difficult to detect with current tools, and
capable of causing concrete damage to AI agent systems. Addressing this threat
requires coordinated efforts among protocol designers, host developers, LLM
providers, and end users to build a more secure and resilient MCP ecosystem.

</details>


### [61] [FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems](https://arxiv.org/abs/2509.24408)
*Yuzhen Long,Songze Li*

Main category: cs.CR

TL;DR: 이 논문은 LLM 기반 자율주행 시스템의 기능 라이브러리를 겨냥한 FuncPoison이라는 새로운 공격 방법을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 자율주행 시스템의 의사결정에서 기능 라이브러리의 중요성을 강조하고, 그 취약성을 분석합니다.

Method: FuncPoison은 악의적인 도구와 기만적인 지침을 주입하여 에이전트의 결정을 조작합니다.

Result: FuncPoison은 자율주행 시스템의 경로 정확도를 저하시킬 수 있으며, 특정 에이전트를 조정된 잘못된 행동으로 유도할 수 있음을 실험적으로 입증했습니다.

Conclusion: 기능 라이브러리는 단순한 도구 세트가 아니라 LLM 기반 자율주행 시스템에서 중요한 공격 표면이 될 수 있음을 보여주어 신뢰성에 대한 우려를 제기합니다.

Abstract: Autonomous driving systems increasingly rely on multi-agent architectures
powered by large language models (LLMs), where specialized agents collaborate
to perceive, reason, and plan. A key component of these systems is the shared
function library, a collection of software tools that agents use to process
sensor data and navigate complex driving environments. Despite its critical
role in agent decision-making, the function library remains an under-explored
vulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based
attack targeting the function library to manipulate the behavior of LLM-driven
multi-agent autonomous systems. FuncPoison exploits two key weaknesses in how
agents access the function library: (1) agents rely on text-based instructions
to select tools; and (2) these tools are activated using standardized command
formats that attackers can replicate. By injecting malicious tools with
deceptive instructions, FuncPoison manipulates one agent s decisions--such as
misinterpreting road conditions--triggering cascading errors that mislead other
agents in the system. We experimentally evaluate FuncPoison on two
representative multi-agent autonomous driving systems, demonstrating its
ability to significantly degrade trajectory accuracy, flexibly target specific
agents to induce coordinated misbehavior, and evade diverse defense mechanisms.
Our results reveal that the function library, often considered a simple
toolset, can serve as a critical attack surface in LLM-based autonomous driving
systems, raising elevated concerns on their reliability.

</details>


### [62] [LISA Technical Report: An Agentic Framework for Smart Contract Auditing](https://arxiv.org/abs/2509.24698)
*Izaiah Sun,Daniel Tan,Andy Deng*

Main category: cs.CR

TL;DR: LISA는 규칙 기반 및 논리 기반 방법을 결합하여 스마트 계약의 취약점을 감지하는 프레임워크로, 기존의 감사 보고서를 활용하여 경험을 학습하고, 산업에 신뢰할 수 있는 취약점 검출 솔루션을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 계약의 다양한 취약점을 해결하고, 신뢰성 있는 탐지 솔루션을 제공하기 위함.

Method: LISA는 규칙 기반 및 논리 기반 방법을 혼합하여 스마트 계약의 취약점을 탐지하며, 과거의 감사 보고서 데이터를 활용하여 탐지 경험을 학습한다.

Result: LISA는 LLM 기반 접근 방식 및 전통적인 정적 분석 도구보다 현저히 뛰어난 성능을 보여주며, 더 많은 취약점 유형을 다루고 높은 탐지 정확도를 achieved한다.

Conclusion: LISA는 수동 노력에 대한 의존을 줄이면서 더 신뢰적이고 포괄적인 취약점 탐지를 제공하는 산업에 적합한 솔루션을 제시한다.

Abstract: We present LISA, an agentic smart contract vulnerability detection framework
that combines rule-based and logic-based methods to address a broad spectrum of
vulnerabilities in smart contracts. LISA leverages data from historical audit
reports to learn the detection experience (without model fine-tuning), enabling
it to generalize learned patterns to unseen projects and evolving threat
profiles. In our evaluation, LISA significantly outperforms both LLM-based
approaches and traditional static analysis tools, achieving superior coverage
of vulnerability types and higher detection accuracy. Our results suggest that
LISA offers a compelling solution for industry: delivering more reliable and
comprehensive vulnerability detection while reducing the dependence on manual
effort.

</details>


### [63] [SecInfer: Preventing Prompt Injection via Inference-time Scaling](https://arxiv.org/abs/2509.24967)
*Yupei Liu,Yanting Wang,Yuqi Jia,Jinyuan Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 프롬프트 주입 공격은 대형 언어 모델(LLMs)의 보안에 큰 위협이 된다. 본 논문은 요청 주입 공격에 대한 새로운 방어 기법인 SecInfer를 제안하며, 이는 추론 시간에서의 자원 할당을 통해 LLM의 능력을 향상시키는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 프롬프트 주입 공격은 LLM의 보안에 지속적인 위협을 가하고, 기존의 방어 방법은 효과가 제한적이다.

Method: SecInfer는 시스템 프롬프트를 활용한 샘플링과 목표 작업에 맞는 응답 집계를 포함한 두 단계로 구성된다.

Result: SecInfer는 추가적인 컴퓨팅 자원을 활용하여 기존 및 적응형 프롬프트 주입 공격을 효과적으로 완화하며, 최신 방어 기법 및 기존의 추론 시간 확장 방법보다 우수한 성능을 보인다.

Conclusion: SecInfer는 대형 언어 모델에 대한 프롬프트 주입 공격을 방어할 수 있는 효과적인 새로운 접근 방식을 제시한다.

Abstract: Prompt injection attacks pose a pervasive threat to the security of Large
Language Models (LLMs). State-of-the-art prevention-based defenses typically
rely on fine-tuning an LLM to enhance its security, but they achieve limited
effectiveness against strong attacks. In this work, we propose \emph{SecInfer},
a novel defense against prompt injection attacks built on \emph{inference-time
scaling}, an emerging paradigm that boosts LLM capability by allocating more
compute resources for reasoning during inference. SecInfer consists of two key
steps: \emph{system-prompt-guided sampling}, which generates multiple responses
for a given input by exploring diverse reasoning paths through a varied set of
system prompts, and \emph{target-task-guided aggregation}, which selects the
response most likely to accomplish the intended task. Extensive experiments
show that, by leveraging additional compute at inference, SecInfer effectively
mitigates both existing and adaptive prompt injection attacks, outperforming
state-of-the-art defenses as well as existing inference-time scaling
approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [FedCF: Fair Federated Conformal Prediction](https://arxiv.org/abs/2509.22907)
*Anutam Srinivasan,Aditya T. Vadlamani,Amin Meghrazi,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 이 논문은 연합 학습 환경에서 공정한 예측을 수행하기 위해 Conformal Fairness(CF) 프레임워크를 확장한다.


<details>
  <summary>Details</summary>
Motivation: 머신러닝 모델에서 불확실성을 정량화하기 위한 Conformal Prediction(CP) 기법의 공정성을 개선하고자 함.

Method: CF 프레임워크를 연합 학습 환경에 확장하고, 다양한 인구 통계 그룹의 공정성 관련 격차를 분석하여 연합 모델의 공정성을 감사하는 방법을 논의함.

Result: 여러 도메인에 걸친 여러 데이터 세트에서 실험을 수행하여 제안된 프레임워크를 경험적으로 검증함.

Conclusion: 제안된 프레임워크는 연합 학습 환경에서 공정성을 보장하기 위한 유용한 도구가 될 수 있음을 보여준다.

Abstract: Conformal Prediction (CP) is a widely used technique for quantifying
uncertainty in machine learning models. In its standard form, CP offers
probabilistic guarantees on the coverage of the true label, but it is agnostic
to sensitive attributes in the dataset. Several recent works have sought to
incorporate fairness into CP by ensuring conditional coverage guarantees across
different subgroups. One such method is Conformal Fairness (CF). In this work,
we extend the CF framework to the Federated Learning setting and discuss how we
can audit a federated model for fairness by analyzing the fairness-related gaps
for different demographic groups. We empirically validate our framework by
conducting experiments on several datasets spanning multiple domains, fully
leveraging the exchangeability assumption.

</details>


### [65] [Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders](https://arxiv.org/abs/2509.22913)
*Jake S. Rhodes,Adam G. Rustad,Marshall S. Nielsen,Morgan Chase McClellan,Dallan Gardner,Dawson Hedges*

Main category: cs.LG

TL;DR: 본 논문에서는 전통적인 방법의 한계를 극복하고, 새로운 데이터에 대한 일반화를 가능하게 하는 기하학적으로 정규화된 쌍 지능형 오토인코더 아키텍처를 활용한 안내된 표현 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 매니폴드 정렬 방법들은 샘플 외 연장 기능이 부족하여 현실 세계에서의 적용 가능성이 제한된다.

Method: 기하학적으로 정규화된 쌍 오토인코더 아키텍처를 활용한 안내된 표현 학습 프레임워크를 제안하고, 구조화된 교차 모달 매핑을 강제하여 학습된 임베딩의 기하학적 충실성을 유지한다.

Result: 다양한 MA 방법을 평가했으며, 임베딩 일관성, 정보 보존 및 교차 도메인 전이에서 개선을 보였다.

Conclusion: 알츠하이머병 진단에 프레임워크를 적용하여 다중 모달 환자 데이터를 통합하고, 다중 모달 문제에서의 통찰력을 활용하여 단일 도메인에 한정된 사례에서 예측 정확도를 향상시켰음을 보였다.

Abstract: Manifold alignment (MA) involves a set of techniques for learning shared
representations across domains, yet many traditional MA methods are incapable
of performing out-of-sample extension, limiting their real-world applicability.
We propose a guided representation learning framework leveraging a
geometry-regularized twin autoencoder (AE) architecture to enhance MA while
enabling generalization to unseen data. Our method enforces structured
cross-modal mappings to maintain geometric fidelity in learned embeddings. By
incorporating a pre-trained alignment model and a multitask learning
formulation, we improve cross-domain generalization and representation
robustness while maintaining alignment fidelity. We evaluate our approach using
several MA methods, showing improvements in embedding consistency, information
preservation, and cross-domain transfer. Additionally, we apply our framework
to Alzheimer's disease diagnosis, demonstrating its ability to integrate
multi-modal patient data and enhance predictive accuracy in cases limited to a
single domain by leveraging insights from the multi-modal problem.

</details>


### [66] [Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces](https://arxiv.org/abs/2509.22963)
*Haitong Ma,Ofir Nabati,Aviv Rosenberg,Bo Dai,Oran Lang,Idan Szpektor,Craig Boutilier,Na Li,Shie Mannor,Lior Shani,Guy Tenneholtz*

Main category: cs.LG

TL;DR: 본 논문은 복잡한 환경에서 효과적인 정책을 훈련하기 위한 새로운 이산 확산 모델 훈련 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습이 실세계 문제의 큰 조합적 행동 공간으로 확장되는 데 어려움을 겪고 있기 때문입니다.

Method: 정책 미러 경량화(PMD)를 활용하여 이상적이고 정규화된 목표 정책 분포를 정의하고, 정책 업데이트를 분포 학습 문제로 설정하여 표현력이 강한 확산 모델이 이 안정적인 목표를 복제하도록 훈련합니다.

Result: 우리 방법은 다양한 조합적 벤치마크에서 최신 기술 수준의 결과와 우수한 샘플 효율성을 달성했습니다.

Conclusion: 실험 결과, 우리의 확산 정책이 다른 기준선에 비해 우수한 성능을 보여주었습니다.

Abstract: Reinforcement learning (RL) struggles to scale to large, combinatorial action
spaces common in many real-world problems. This paper introduces a novel
framework for training discrete diffusion models as highly effective policies
in these complex settings. Our key innovation is an efficient online training
process that ensures stable and effective policy improvement. By leveraging
policy mirror descent (PMD) to define an ideal, regularized target policy
distribution, we frame the policy update as a distributional matching problem,
training the expressive diffusion model to replicate this stable target. This
decoupled approach stabilizes learning and significantly enhances training
performance. Our method achieves state-of-the-art results and superior sample
efficiency across a diverse set of challenging combinatorial benchmarks,
including DNA sequence generation, RL with macro-actions, and multi-agent
systems. Experiments demonstrate that our diffusion policies attain superior
performance compared to other baselines.

</details>


### [67] [OptiMind: Teaching LLMs to Think Like Optimization Experts](https://arxiv.org/abs/2509.22979)
*Zeyi Chen,Xinzhi Zhang,Humishka Zope,Hugo Barbalho,Konstantina Mellou,Marco Molinaro,Janardhan Kulkarni,Ishai Menache,Sirui Li*

Main category: cs.LG

TL;DR: 수학적 프로그래밍의 자동화에 대한 연구로, 혼합 정수 선형 프로그래밍의 정확도를 개선하기 위한 접근 방식이 제시된다.


<details>
  <summary>Details</summary>
Motivation: 수학적 프로그래밍은 다양한 도메인에서 중요한 작업으로, 이 과정을 자동화하려는 관심이 커지고 있다.

Method: 최적화 전문 지식을 통합하고, 데이터 정제 및 다중 턴 추론 전략을 개발하여 정확도를 높인다.

Result: 정제된 데이터와 도메인 기반 프롬프트 및 피드백 결합으로 평균 14% 정확도 향상.

Conclusion: 이 연구는 LLM 지원 최적화 공식화의 향상을 위한 기초를 마련한다.

Abstract: Mathematical programming -- the task of expressing operations and
decision-making problems in precise mathematical language -- is fundamental
across domains, yet remains a skill-intensive process requiring operations
research expertise. Recent advances in large language models for complex
reasoning have spurred interest in automating this task, translating natural
language into executable optimization models. Current approaches, however,
achieve limited accuracy, hindered by scarce and noisy training data without
leveraging domain knowledge. In this work, we systematically integrate
optimization expertise to improve formulation accuracy for mixed-integer linear
programming, a key family of mathematical programs. Our approach first cleans
training data through class-based error analysis to explicitly prevent common
mistakes within each optimization class. We then develop multi-turn inference
strategies that guide LLMs with class-specific error summaries and solver
feedback, enabling iterative refinement. Experiments across multiple base LLMs
demonstrate that combining cleaned data with domain-informed prompting and
feedback improves formulation accuracy by 14 percentage points on average,
enabling further progress toward robust LLM-assisted optimization formulation.

</details>


### [68] [FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents](https://arxiv.org/abs/2509.23803)
*Pramit Saha,Joshua Strong,Divyanshu Mishra,Cheng Ouyang,J. Alison Noble*

Main category: cs.LG

TL;DR: 이 논문은 자율 에이전트 기반의 연합 학습(FL) 프레임워크와 이를 평가하기 위한 벤치마크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습의 실제 배포는 복잡한 운영 과제로 인해 어려움이 많아 자동화된 시스템이 필요하다.

Method: 에이전트 기반 FL 프레임워크를 도입하고, 40개의 FL 알고리즘과 201개의 데이터셋을 사용하여 다양한 작업을 수행한다.

Result: 14개의 오픈 소스 및 10개의 독점 LLM의 에이전트 성능을 평가하여, 복잡한 작업에 대한 모델의 한계를 발견했다.

Conclusion: 모델들이 다양한 FL 파이프라인 단계를 자동화할 수 있지만, 복잡하고 상호 의존적인 작업은 여전히 도전 과제가 남아있다.

Abstract: Federated learning (FL) allows collaborative model training across healthcare
sites without sharing sensitive patient data. However, real-world FL deployment
is often hindered by complex operational challenges that demand substantial
human efforts. This includes: (a) selecting appropriate clients (hospitals),
(b) coordinating between the central server and clients, (c) client-level data
pre-processing, (d) harmonizing non-standardized data and labels across
clients, and (e) selecting FL algorithms based on user instructions and
cross-client data characteristics. However, the existing FL works overlook
these practical orchestration challenges. These operational bottlenecks
motivate the need for autonomous, agent-driven FL systems, where intelligent
agents at each hospital client and the central server agent collaboratively
manage FL setup and model training with minimal human intervention. To this
end, we first introduce an agent-driven FL framework that captures key phases
of real-world FL workflows from client selection to training completion and a
benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to
autonomously coordinate healthcare FL. Our framework incorporates 40 FL
algorithms, each tailored to address diverse task-specific requirements and
cross-client characteristics. Furthermore, we introduce a diverse set of
complex tasks across 201 carefully curated datasets, simulating 6
modality-specific real-world healthcare environments, viz., Dermatoscopy,
Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic
performance of 14 open-source and 10 proprietary LLMs spanning small, medium,
and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3
can automate various stages of the FL pipeline, our results reveal that more
complex, interdependent tasks based on implicit goals remain challenging for
even the strongest models.

</details>


### [69] [Beyond Heuristics: Globally Optimal Configuration of Implicit Neural Representations](https://arxiv.org/abs/2509.23139)
*Sipeng Chen,Yan Zhang,Shibo Li*

Main category: cs.LG

TL;DR: OptiINR은Implicit Neural Representations(INRs)의 최적 구성을 위한 첫 번째 통합 프레임워크로, 베이지안 최적화를 이용하여 성능을 극대화한다.


<details>
  <summary>Details</summary>
Motivation: Implicit Neural Representations(INRs)의 성능은 최적 구성 전략의 부재로 제한된다.

Method: OptiINR은 베이지안 최적화를 활용하여 다양한 활성화 패밀리와 연속 초기화 매개변수의 조합 공간을 탐색한다.

Result: OptiINR은 전통적인 수동 조정 대신 데이터 기반의 최적화 프로세스를 통해 일관된 성능을 제공한다.

Conclusion: OptiINR은 다양한 신호 처리 응용 분야에서 성능을 극대화하며 INRs 디자인을 위한 원칙적 기초를 제시한다.

Abstract: Implicit Neural Representations (INRs) have emerged as a transformative
paradigm in signal processing and computer vision, excelling in tasks from
image reconstruction to 3D shape modeling. Yet their effectiveness is
fundamentally limited by the absence of principled strategies for optimal
configuration - spanning activation selection, initialization scales,
layer-wise adaptation, and their intricate interdependencies. These choices
dictate performance, stability, and generalization, but current practice relies
on ad-hoc heuristics, brute-force grid searches, or task-specific tuning, often
leading to inconsistent results across modalities. This work introduces
OptiINR, the first unified framework that formulates INR configuration as a
rigorous optimization problem. Leveraging Bayesian optimization, OptiINR
efficiently explores the joint space of discrete activation families - such as
sinusoidal (SIREN), wavelet-based (WIRE), and variable-periodic (FINER) - and
their associated continuous initialization parameters. This systematic approach
replaces fragmented manual tuning with a coherent, data-driven optimization
process. By delivering globally optimal configurations, OptiINR establishes a
principled foundation for INR design, consistently maximizing performance
across diverse signal processing applications.

</details>


### [70] [GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning](https://arxiv.org/abs/2509.24031)
*Umang Garg,Bowen Zhang,Anantanjit Subrahmanya,Chandrakanth Gudavalli,BS Manjunath*

Main category: cs.LG

TL;DR: GPSMasked Trajectory Transformer(GPS-MTM)는 대규모 이동 데이터를 위한 기초 모델로, 인간의 이동 패턴을 캡처하여 경로 분석의 새로운 가능성을 열어줍니다.


<details>
  <summary>Details</summary>
Motivation: 이 논문은 대규모 이동 데이터에서 정상 패턴을 캡처하고, 이동 데이터를 최초의 1급 모달리티로 위치시키고자 합니다.

Method: GPS-MTM은 경로를 좌표 스트림으로 평탄화하는 대신, 상태(관심 지점 범주)와 행동(요원 전이)이라는 두 가지 상보적 양식으로 이동성을 분해합니다. 이 모델은 양방향 Transformer와 자기 지도 방식의 마스킹 모델링 목표를 활용하여 모달리티 전반에 걸쳐 누락된 부분을 재구성합니다.

Result: GPS-MTM은 Numosim-LA, Urban Anomalies, Geolife 등 여러 벤치마크 데이터셋에서 경로 채우기 및 다음 정지 예측과 같은 다운스트림 작업에서 일관되게 우수한 성능을 보입니다.

Conclusion: 이 결과는 GPS-MTM이 경로 분석을 위한 강력한 기초 모델로 자리잡을 수 있음을 보여줍니다.

Abstract: Foundation models have driven remarkable progress in text, vision, and video
understanding, and are now poised to unlock similar breakthroughs in trajectory
modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a
foundation model for large-scale mobility data that captures patterns of
normalcy in human movement. Unlike prior approaches that flatten trajectories
into coordinate streams, GPS-MTM decomposes mobility into two complementary
modalities: states (point-of-interest categories) and actions (agent
transitions). Leveraging a bi-directional Transformer with a self-supervised
masked modeling objective, the model reconstructs missing segments across
modalities, enabling it to learn rich semantic correlations without manual
labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and
Geolife, GPS-MTM consistently outperforms on downstream tasks such as
trajectory infilling and next-stop prediction. Its advantages are most
pronounced in dynamic tasks (inverse and forward dynamics), where contextual
reasoning is critical. These results establish GPS-MTM as a robust foundation
model for trajectory analytics, positioning mobility data as a first-class
modality for large-scale representation learning. Code is released for further
reference.

</details>


### [71] [TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts](https://arxiv.org/abs/2509.23145)
*Xiaowen Ma,Shuning Ge,Fan Yang,Xiangyu Li,Yun Chen,Mengting Ma,Wei Zhang,Zhipeng Liu*

Main category: cs.LG

TL;DR: Transformer 기반 아키텍처가 시계열 모델링에서 주요한 역할을 하지만, 기존 방식의 제약으로 인해 실제 데이터의 두 가지 문제를 해결하지 못한다. 이를 해결하기 위해 우리는 Temporal Mix of Experts (TMOE)라는 새로운 메커니즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 시간 변수의 중요성은 역동적으로 변화하며, 비정상적인 신호가 예측 정확도를 저해할 수 있다.

Method: TMOE는 키-값 쌍을 지역 전문가로 재구성하고, 각 쿼리에 대해 관련 없는 타임스탬프를 필터링하여 조정된 전문가 선택을 수행한다.

Result: TMOE를 적용하여 새로운 시계열 Transformer 모델인 TimeExpert와 TimeExpert-G를 개발하였으며, 이 모델들이 최첨단 방법들보다 우수함을 입증하였다.

Conclusion: 시간 예측 문제에 대한 성능 향상을 위해 새로운 메커니즘이 필요하며, TMOE가 그 해결책을 제공한다고 결론짓는다.

Abstract: Transformer-based architectures dominate time series modeling by enabling
global attention over all timestamps, yet their rigid 'one-size-fits-all'
context aggregation fails to address two critical challenges in real-world
data: (1) inherent lag effects, where the relevance of historical timestamps to
a query varies dynamically; (2) anomalous segments, which introduce noisy
signals that degrade forecasting accuracy. To resolve these problems, we
propose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism
that reimagines key-value (K-V) pairs as local experts (each specialized in a
distinct temporal context) and performs adaptive expert selection for each
query via localized filtering of irrelevant timestamps. Complementing this
local adaptation, a shared global expert preserves the Transformer's strength
in capturing long-range dependencies. We then replace the vanilla attention
mechanism in popular time-series Transformer frameworks (i.e., PatchTST and
Timer) with TMOE, without extra structural modifications, yielding our specific
version TimeExpert and general version TimeExpert-G. Extensive experiments on
seven real-world long-term forecasting benchmarks demonstrate that TimeExpert
and TimeExpert-G outperform state-of-the-art methods. Code is available at
https://github.com/xwmaxwma/TimeExpert.

</details>


### [72] [Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization](https://arxiv.org/abs/2509.23158)
*Yufei Shen,Ji Hwan Park,Minchao Huang,Jared F. Benge,Justin F. Rousseau,Rosemary A. Lester-Smith,Edison Thomaz*

Main category: cs.LG

TL;DR: 능동적인 스마트폰 센싱을 통해 노인들의 인지 손상을 사전에 탐지하고 예방할 수 있는 방법을 제시하였으며, 모델의 일반화를 향상시키기 위한 두 가지 기법을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: 인지 장애의 조기 탐지는 적시 진단 및 개입을 위해 중요하지만, 임상 평가의 빈도가 낮아 미세한 인지 감소를 포착하는 데 필요한 민감도와 시간 해상도가 부족하다.

Method: 일상적인 행동 특성의 연속 체계를 이용하여 인지 손상을 탐지하기 위해 LSTM 모델을 구현하였다. 또한, 참여자 간 모델 일반화를 향상시키기 위해 루틴 인식 증강 및 인구 통계 맞춤화 기법을 도입하였다.

Result: 36명의 노인으로부터 수집된 6개월 데이터에서, 제안된 기법들은 모델의 AUPRC을 0.637에서 0.766으로 향상시켰다.

Conclusion: 능동적인 센싱을 통한 인지 손상의 확장 가능한 모니터링 가능성을 강조한다.

Abstract: Early detection of cognitive impairment is critical for timely diagnosis and
intervention, yet infrequent clinical assessments often lack the sensitivity
and temporal resolution to capture subtle cognitive declines in older adults.
Passive smartphone sensing has emerged as a promising approach for naturalistic
and continuous cognitive monitoring. Building on this potential, we implemented
a Long Short-Term Memory (LSTM) model to detect cognitive impairment from
sequences of daily behavioral features, derived from multimodal sensing data
collected in an ongoing one-year study of older adults. Our key contributions
are two techniques to enhance model generalizability across participants: (1)
routine-aware augmentation, which generates synthetic sequences by replacing
each day with behaviorally similar alternatives, and (2) demographic
personalization, which reweights training samples to emphasize those from
individuals demographically similar to the test participant. Evaluated on
6-month data from 36 older adults, these techniques jointly improved the Area
Under the Precision-Recall Curve (AUPRC) of the model trained on sensing and
demographic features from 0.637 to 0.766, highlighting the potential of
scalable monitoring of cognitive impairment in aging populations with passive
sensing.

</details>


### [73] [Towards Monotonic Improvement in In-Context Reinforcement Learning](https://arxiv.org/abs/2509.23209)
*Wenhao Zhang,Shao Zhang,Xihuai Wang,Yang Li,Ying Wen*

Main category: cs.LG

TL;DR: ICRL의 새로운 접근법인 CV-ICRL은 Contextual Ambiguity 문제를 해결하여 성능 저하를 완화하고 다양한 작업에서 ICRL 능력을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: ICRL은 과거 경험을 활용하여 새로운 작업에 빠르게 적응하는 에이전트를 개발하는 유망한 패러다임이다.

Method: CV-ICRL은 현재의 컨텍스트에 따라 달성할 수 있는 이상적인 성능을 나타내는 신호로서 Context Value를 훈련 단계에 도입한다.

Result: CV-ICRL은 Dark Room과 Minigrid 테스트베드에서 성능 저하를 효과적으로 완화하고 전체적인 ICRL 능력을 향상시켰다.

Conclusion: Context Value는 이상적인 정책과의 성능 차이에 대한 하한을 강화한다.

Abstract: In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm
for developing agents that can rapidly adapt to new tasks by leveraging past
experiences as context, without updating their parameters. Recent approaches
train large sequence models on monotonic policy improvement data from online
RL, aiming to a continue improved testing time performance. However, our
experimental analysis reveals a critical flaw: these models cannot show a
continue improvement like the training data during testing time. Theoretically,
we identify this phenomenon as Contextual Ambiguity, where the model's own
stochastic actions can generate an interaction history that misleadingly
resembles that of a sub-optimal policy from the training data, initiating a
vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we
introduce Context Value into training phase and propose Context Value Informed
ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing
the ideal performance theoretically achievable by a policy given the current
context. As the context expands, Context Value could include more task-relevant
information, and therefore the ideal performance should be non-decreasing. We
prove that the Context Value tightens the lower bound on the performance gap
relative to an ideal, monotonically improving policy. We fruther propose two
methods for estimating Context Value at both training and testing time.
Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that
CV-ICRL effectively mitigates performance degradation and improves overall ICRL
abilities across various tasks and environments. The source code and data of
this paper are available at
https://github.com/Bluixe/towards_monotonic_improvement .

</details>


### [74] [Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning](https://arxiv.org/abs/2509.23462)
*Alakh Sharma,Gaurish Trivedi,Kartikey Bhandari,Yash Sinha,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: GEMS는 다수의 에이전트를 위한 강화 학습의 유연성을 향상시키는 새로운 방법론으로, 기존 방법의 비효율성을 극복하며 빠른 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 다수의 에이전트를 대상으로 하는 강화 학습은 AI의 핵심 과제로, 기존 방법이 가지는 비효율성을 해결할 필요가 있다.

Method: GEMS는 명시적 정책 집합 대신 잠재적 앵커의 집합과 단일 감가상각 생성기를 사용하여 명시적인 집합을 대체하고, 편향이 없는 몬테 카를로 롤아웃과 모델이 없는 경험적 베르타인 UCB 오라클을 활용하여 정책 집합을 확장한다.

Result: GEMS는 PSRO보다 최대 6배 빠르고, 메모리 사용량이 1.3배 적으면서 더 높은 보상을 얻었다.

Conclusion: GEMS는 PSRO의 게임 이론적 보장을 유지하면서도 비효율성을 극복하여 여러 도메인에서의 대규모 다중 에이전트 학습을 가능하게 한다.

Abstract: Scalable multi-agent reinforcement learning (MARL) remains a central
challenge for AI. Existing population-based methods, like Policy-Space Response
Oracles, PSRO, require storing explicit policy populations and constructing
full payoff matrices, incurring quadratic computation and linear memory costs.
We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free
framework that replaces explicit populations with a compact set of latent
anchors and a single amortized generator. Instead of exhaustively constructing
the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts,
multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB
oracle to adaptively expand the policy set. Best responses are trained within
the generator using an advantage-based trust-region objective, eliminating the
need to store and train separate actors. We evaluated GEMS in a variety of
Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn
Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster,
has 1.3x less memory usage than PSRO, while also reaps higher rewards
simultaneously. These results demonstrate that GEMS retains the game theoretic
guarantees of PSRO, while overcoming its fundamental inefficiencies, hence
enabling scalable multi-agent learning in multiple domains.

</details>


### [75] [Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases](https://arxiv.org/abs/2509.23471)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Drift-Adapter는 모델 버전 간 임베딩 공간을 연결하는 경량의 학습 가능한 변환 레이어이다.


<details>
  <summary>Details</summary>
Motivation: 생산 벡터 데이터베이스에서 임베딩 모델을 업그레이드하는 것은 전체 코퍼스를 다시 인코딩하고 근사 최근접 이웃(ANN) 인덱스를 재구성해야 하므로 운영 중단과 계산 비용이 크다.

Method: Drift-Adapter는 새로운 쿼리를 기존의 임베딩 공간에 매핑하여 기존 ANN 인덱스를 계속 사용할 수 있게 하여 전체 재계산을 미룬다. 세 가지 어댑터 파라미터화 방식인 Orthogonal Procrustes, Low-Rank Affine 및 Compact Residual MLP를 체계적으로 평가한다.

Result: MTEB 텍스트 코퍼스와 CLIP 이미지 모델 업그레이드 실험(1M 항목)에서 Drift-Adapter는 전체 재임베딩의 95-99% 검색 재현율(Recall@10, MRR)을 회복하며 쿼리 지연 시간은 10 마이크로초 이하에 불과하다.

Conclusion: Drift-Adapter는 100배 이상의 재계산 비용 절감과 거의 제로에 가까운 운영 중단으로 업그레이드를 용이하게 하는 실용적인 솔루션으로서 입증되었다.

Abstract: Upgrading embedding models in production vector databases typically requires
re-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor
(ANN) index, leading to significant operational disruption and computational
cost. This paper presents Drift-Adapter, a lightweight, learnable
transformation layer designed to bridge embedding spaces between model
versions. By mapping new queries into the legacy embedding space, Drift-Adapter
enables the continued use of the existing ANN index, effectively deferring full
re-computation. We systematically evaluate three adapter parameterizations:
Orthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on
a small sample of paired old and new embeddings. Experiments on MTEB text
corpora and a CLIP image model upgrade (1M items) show that Drift-Adapter
recovers 95-99% of the retrieval recall (Recall@10, MRR) of a full
re-embedding, adding less than 10 microseconds of query latency. Compared to
operational strategies like full re-indexing or dual-index serving,
Drift-Adapter reduces recompute costs by over 100 times and facilitates
upgrades with near-zero operational interruption. We analyze robustness to
varied model drift, training data size, scalability to billion-item systems,
and the impact of design choices like diagonal scaling, demonstrating
Drift-Adapter's viability as a pragmatic solution for agile model deployment.

</details>


### [76] [Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models](https://arxiv.org/abs/2509.23593)
*Zekun Wang,Anant Gupta,Zihan Dong,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 연속 학습에서의 치명적인 망각 문제를 해결하기 위한 연구로, 확산 모델의 기하학적 특성을 활용한 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 연속 학습에서의 치명적인 망각 문제를 해결하기 위해 기존 접근 방식의 한계를 극복하고자 한다.

Method: 확산 모델의 그래디언트 기하학을 연구하고, 이를 기반으로 EWC의 랭크-1 변형을 제안하여, 재생 기반 접근법과 결합한다.

Result: MNIST, FashionMNIST, CIFAR-10, ImageNet-1k 데이터셋에서 평균 FID를 개선하고 망각을 줄였다.

Conclusion: 개선된 Fisher 추정으로 EWC는 재생과 함께 강력한 보완 수단이 된다.

Abstract: Catastrophic forgetting remains a central obstacle for continual learning in
neural models. Popular approaches -- replay and elastic weight consolidation
(EWC) -- have limitations: replay requires a strong generator and is prone to
distributional drift, while EWC implicitly assumes a shared optimum across
tasks and typically uses a diagonal Fisher approximation. In this work, we
study the gradient geometry of diffusion models, which can already produce
high-quality replay data. We provide theoretical and empirical evidence that,
in the low signal-to-noise ratio (SNR) regime, per-sample gradients become
strongly collinear, yielding an empirical Fisher that is effectively rank-1 and
aligned with the mean gradient. Leveraging this structure, we propose a rank-1
variant of EWC that is as cheap as the diagonal approximation yet captures the
dominant curvature direction. We pair this penalty with a replay-based approach
to encourage parameter sharing across tasks while mitigating drift. On
class-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10,
ImageNet-1k), our method consistently improves average FID and reduces
forgetting relative to replay-only and diagonal-EWC baselines. In particular,
forgetting is nearly eliminated on MNIST and FashionMNIST and is roughly halved
on ImageNet-1k. These results suggest that diffusion models admit an
approximately rank-1 Fisher. With a better Fisher estimate, EWC becomes a
strong complement to replay: replay encourages parameter sharing across tasks,
while EWC effectively constrains replay-induced drift.

</details>


### [77] [Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs](https://arxiv.org/abs/2509.23684)
*Tanya Chowdhury,Atharva Nijasure,Yair Zick,James Allan*

Main category: cs.LG

TL;DR: 본 논문은 LLM의 MLP 계층에서 신경 세포 간의 협력 간섭을 설명하기 위한 메커니즘 해석 가능성 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 작업 특성에 대한 풍부한 기능을 인코딩하지만 MLP 계층 내 표현 형태가 불분명하여 신경 세포 간 협동 작용 연구의 필요성을 제기한다.

Method: coalitional game 이론에 기반한 프레임워크를 도입하여, 신경 세포들이 선호도 기반의 협력 게임에서의 에이전트를 모방하도록 한다.

Result: LLaMA, Mistral 및 Pythia rerankers를 이용해 찾은 협동체는 클러스터링 기준선보다 지속적으로 높은 시너지를 보여준다.

Conclusion: 신경 세포가 기능적으로 중요하고 해석 가능하며 여러 도메인에서 예측 가능한 계산 단위를 인코딩하는 방식을 밝혀준다.

Abstract: Fine-tuned Large Language Models (LLMs) encode rich task-specific features,
but the form of these representations, especially within MLP layers, remains
unclear. Empirical inspection of LoRA updates shows that new features
concentrate in mid-layer MLPs, yet the scale of these layers obscures
meaningful structure. Prior probing suggests that statistical priors may
strengthen, split, or vanish across depth, motivating the need to study how
neurons work together rather than in isolation.
  We introduce a mechanistic interpretability framework based on coalitional
game theory, where neurons mimic agents in a hedonic game whose preferences
capture their synergistic contributions to layer-local computations. Using
top-responsive utilities and the PAC-Top-Cover algorithm, we extract stable
coalitions of neurons: groups whose joint ablation has non-additive effects. We
then track their transitions across layers as persistence, splitting, merging,
or disappearance.
  Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR
tasks, our method finds coalitions with consistently higher synergy than
clustering baselines. By revealing how neurons cooperate to encode features,
hedonic coalitions uncover higher-order structure beyond disentanglement and
yield computational units that are functionally important, interpretable, and
predictive across domains.

</details>


### [78] [FedDAPL: Toward Client-Private Generalization in Federated Learning](https://arxiv.org/abs/2509.23688)
*Soroosh Safari Loaliyan,Jose-Luis Ambite,Paul M. Thompson,Neda Jahanshad,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 본 논문은 연합 학습(FL)에서 도메인 적대 신경망(DANN)을 통합하여 의료 이미징의 데이터 프라이버시를 유지하면서 도메인 안정성을 개선하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 의료 이미징 분야에서는 엄격한 개인 정보 보호 법률에 따라 원시 데이터 공유가 금지되므로, 연합 학습이 적합하다.

Method: 이 논문에서는 DANN을 FL 프로세스에 통합하며, 근접 정규화 방법을 제안하여 클라이언트 간의 적대적 훈련을 안정화한다.

Result: OpenBHB 데이터 세트의 T1 가중치 3-D 뇌 MRI로 실험한 결과, 15개 사이트에서 훈련하고 19개 미사용 사이트에서 테스트했을 때, FedAvg 및 ERM보다 우수한 교차 사이트 일반화를 달성했다.

Conclusion: 이 방법은 데이터 프라이버시를 보호하면서도 효율적인 모델 성능을 보여준다.

Abstract: Federated Learning (FL) trains models locally at each research center or
clinic and aggregates only model updates, making it a natural fit for medical
imaging, where strict privacy laws forbid raw data sharing. A major obstacle is
scanner-induced domain shift: non-biological variations in hardware or
acquisition protocols can cause models to fail on external sites. Most
harmonization methods correct this shift by directly comparing data across
sites, conflicting with FL's privacy constraints. Domain Generalization (DG)
offers a privacy-friendly alternative - learning site-invariant representations
without sharing raw data - but standard DG pipelines still assume centralized
access to multi-site data, again violating FL's guarantees. This paper meets
these difficulties with a straightforward integration of a Domain-Adversarial
Neural Network (DANN) within the FL process. After demonstrating that a naive
federated DANN fails to converge, we propose a proximal regularization method
that stabilizes adversarial training among clients. Experiments on T1-weighted
3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on
participants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79
y (mean 19+/-13 y; 55 percent male) in validation, show that training on 15
sites and testing on 19 unseen sites yields superior cross-site generalization
over FedAvg and ERM while preserving data privacy.

</details>


### [79] [Visual CoT Makes VLMs Smarter but More Fragile](https://arxiv.org/abs/2509.23789)
*Chunxue Xu,Yiwei Wang,Yujun Cai,Bryan Hooi,Songze Li*

Main category: cs.LG

TL;DR: Visual CoT는 VLM의 추론을 향상시키지만, 이미지 노이즈에 대한 강인성은 부족합니다. 본 논문에서는 Visual CoT의 강인성을 평가하고, 이를 개선하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: VLM의 추론을 개선하기 위해 Visual CoT 기술이 도입되었으나, 이미지 수준의 노이즈에 대한 Robustness가 탐구되지 않았습니다.

Method: 12가지 이미지 손상 유형을 포괄하는 벤치마크를 설정하여 Visual CoT와 비-VL 모델 간의 성능 비교를 수행했습니다.

Result: Visual CoT를 통합하면 이미지가 깨끗하든 노이즈가 있든 상관없이 정확도가 일관되게 향상되지만, 입력 섭동에 대한 민감성이 높아져 성능 저하가 더 심해집니다.

Conclusion: Visual CoT의 중간 추론 요소인 수정된 이미지 패치가 취약성의 주요 원인임을 확인하고, Grounding DINO 모델을 활용한 강인성 향상 방법을 제안합니다.

Abstract: Chain-of-Thought (CoT) techniques have significantly enhanced reasoning in
Vision-Language Models (VLMs). Extending this paradigm, Visual CoT integrates
explicit visual edits, such as cropping or annotating regions of interest, into
the reasoning process, achieving superior multimodal performance. However, the
robustness of Visual CoT-based VLMs against image-level noise remains
unexplored. In this paper, we present the first systematic evaluation of Visual
CoT robustness under visual perturbations. Our benchmark spans 12 image
corruption types across 4 Visual Question Answering (VQA) datasets, enabling a
comprehensive comparison between VLMs that use Visual CoT, and VLMs that do
not. The results reveal that integrating Visual CoT consistently improves
absolute accuracy regardless of whether the input images are clean or corrupted
by noise; however, it also increases sensitivity to input perturbations,
resulting in sharper performance degradation compared to standard VLMs. Through
extensive analysis, we identify the intermediate reasoning components of Visual
CoT, i.e., the edited image patches , as the primary source of fragility.
Building on this analysis, we propose a plug-and-play robustness enhancement
method that integrates Grounding DINO model into the Visual CoT pipeline,
providing high-confidence local visual cues to stabilize reasoning. Our work
reveals clear fragility patterns in Visual CoT and offers an effective,
architecture-agnostic solution for enhancing visual robustness.

</details>


### [80] [Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability](https://arxiv.org/abs/2509.23689)
*Ankit Gangwal,Aaryan Ajay Sharma*

Main category: cs.LG

TL;DR: 모델 병합(MM)은 다중 작업 학습의 대안으로 부상했으나, 백도어 공격 등 적대적 공격에 대한 대응에는 한계가 있다.


<details>
  <summary>Details</summary>
Motivation: 이번 연구의 동기는 모델 병합이 적대적 예제의 전이 공격에 미치는 영향을 탐구하는 것이다.

Method: 8개의 MM 방법, 7개의 데이터셋, 6개의 공격 방법을 포함한 종합 평가와 통계 분석을 수행하였다.

Result: 모델 병합은 전이 공격에 대해 신뢰할 수 있는 방어력을 갖지 못하며, 95% 이상의 전이 공격 성공률을 보였다.

Conclusion: 이 연구는 모델 병합을 사용하는 시스템 설계 시 중요한 통찰을 제공하며, 더 안전한 시스템을 설계하는 데 기여할 수 있다.

Abstract: Model Merging (MM) has emerged as a promising alternative to multi-task
learning, where multiple fine-tuned models are combined, without access to
tasks' training data, into a single model that maintains performance across
tasks. Recent works have explored the impact of MM on adversarial attacks,
particularly backdoor attacks. However, none of them have sufficiently explored
its impact on transfer attacks using adversarial examples, i.e., a black-box
adversarial attack where examples generated for a surrogate model successfully
mislead a target model.
  In this work, we study the effect of MM on the transferability of adversarial
examples. We perform comprehensive evaluations and statistical analysis
consisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336
distinct attack settings. Through it, we first challenge the prevailing notion
of MM conferring free adversarial robustness, and show MM cannot reliably
defend against transfer attacks, with over 95% relative transfer attack success
rate. Moreover, we reveal 3 key insights for machine-learning practitioners
regarding MM and transferability for a robust system design: (1) stronger MM
methods increase vulnerability to transfer attacks; (2) mitigating
representation bias increases vulnerability to transfer attacks; and (3) weight
averaging, despite being the weakest MM method, is the most vulnerable MM
method to transfer attacks. Finally, we analyze the underlying reasons for this
increased vulnerability, and provide potential solutions to the problem. Our
findings offer critical insights for designing more secure systems employing
MM.

</details>


### [81] [Estimating Time Series Foundation Model Transferability via In-Context Learning](https://arxiv.org/abs/2509.23695)
*Qingren Yao,Ming Jin,Chengqi Zhang,Chao-Han Huck Yang,Jun Qi,Shirui Pan*

Main category: cs.LG

TL;DR: 본 연구에서는 TimeTic이라는 전이 가능성 평가 프레임워크를 소개하여, 제한된 데이터 영역에서의 모델 선택과 성능 향상 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 기초 모델이 강력한 제로샷 예측을 제공하지만, 데이터가 제한된 분야에서는 성능 향상을 위해 세부 조정이 중요하다.

Method: TimeTic는 모델 선택을 인-컨텍스트 학습 문제로 재구성하여, 알려진 데이터셋에 대한 관찰을 기반으로 TSFM이 다운스트림 데이터셋에서 세부 조정 후 어떻게 동작할지를 예측한다.

Result: TimeTic의 추정치는 이전에 보지 못한 데이터셋에 대한 실제 세부 조정 성능과 강한 정합성을 보이며, 평균 순위 상관관계는 약 0.6으로, 제로샷 성능을 전이 가능성 점수로 사용할 때보다 30% 개선되었다.

Conclusion: TimeTic은 다양한 테스트 시간 시나리오에 적응할 수 있도록 모델-데이터 관계를 유연하게 구성하고, 임베딩 공간의 구별을 포착하여 임의의 모델 집합에 대해 일반화할 수 있다.

Abstract: Time series foundation models (TSFMs) offer strong zero-shot forecasting via
large-scale pre-training, yet fine-tuning remains critical for boosting
performance in domains with limited public data. With the growing number of
TSFMs, efficiently identifying the best model for downstream fine-tuning
becomes increasingly challenging. In this work, we introduce TimeTic, a
transferability estimation framework that recasts model selection as an
in-context-learning problem: given observations on known (source) datasets, it
predicts how a TSFM will perform after fine-tuning on a downstream (target)
dataset. TimeTic flexibly organizes the observed model-data relationships as
contextual information, allowing it to adapt seamlessly to various test-time
scenarios. Leveraging the natural tabular structure formed by dataset
meta-features, model characteristics, and fine-tuned performance, we employ
tabular foundation models to serve as in-context learners. We further introduce
a novel model characterization based on entropy evolution across model layers,
capturing embedding-space distinctions and enabling TimeTic to generalize
across arbitrary model sets. We establish a comprehensive benchmark for
transferability estimation including 10 datasets, 10 foundation models, and 3
forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong
alignment with actual fine-tuned performance for previously unseen datasets,
achieving a mean rank correlation of approximately 0.6 and a 30% improvement
compared to using zero-shot performance as the transferability score.

</details>


### [82] [An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms](https://arxiv.org/abs/2509.23750)
*Li Wang,Sudun,Xingjian Zhang,Wenjun Wu,Lei Huang*

Main category: cs.LG

TL;DR: 이 논문은 심층 강화 학습에서 배치 정규화(BN)의 장점과 도전 과제를 다루며, MA-BN 방법론을 제안하여 성능을 향상시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 심층 강화 학습에서 배치 정규화(BN)를 적용하는 데 혼합된 결과가 있었고, BN의 이점을 활용하여 심층 강화 학습의 교육 과정을 개선할 수 있다는 점을 강조한다.

Method: 다양한 훈련 및 평가 모드가 성능에 미치는 영향을 체계적으로 분석하고, 불안정성이나 발산을 초래하는 실패 모드를 식별한다. 또한, MA-BN 방법론을 제안하여 강화 학습 파이프라인에서 BN을 효과적으로 통합하는 방법에 대한 실행 가능한 권장 사항을 제공한다.

Result: 실험적으로 MA-BN이 RL 환경에서 훈련을 가속화하고 안정화하며, 효과적인 학습률 범위를 넓히고 탐색을 강화하며 전체 최적화의 어려움을 줄임을 확인했다.

Conclusion: MA-BN을 통해 심층 강화 학습의 효율성을 높일 수 있으며, 훈련 안정성을 유지하면서 성능을 향상시킬 수 있는 기회를 제공한다.

Abstract: Batch Normalization (BN) has played a pivotal role in the success of deep
learning by improving training stability, mitigating overfitting, and enabling
more effective optimization. However, its adoption in deep reinforcement
learning (DRL) has been limited due to the inherent non-i.i.d. nature of data
and the dynamically shifting distributions induced by the agent's learning
process. In this paper, we argue that, despite these challenges, BN retains
unique advantages in DRL settings, particularly through its stochasticity and
its ability to ease training. When applied appropriately, BN can adapt to
evolving data distributions and enhance both convergence speed and final
performance. To this end, we conduct a comprehensive empirical study on the use
of BN in off-policy actor-critic algorithms, systematically analyzing how
different training and evaluation modes impact performance. We further identify
failure modes that lead to instability or divergence, analyze their underlying
causes, and propose the Mode-Aware Batch Normalization (MA-BN) method with
practical actionable recommendations for robust BN integration in DRL
pipelines. We also empirically validate that, in RL settings, MA-BN accelerates
and stabilizes training, broadens the effective learning rate range, enhances
exploration, and reduces overall optimization difficulty. Our code is available
at: https://github.com/monster476/ma-bn.git.

</details>


### [83] [Knowledge Homophily in Large Language Models](https://arxiv.org/abs/2509.23773)
*Utkarsh Sahu,Zhisheng Qi,Mahantesh Halappanavar,Nedim Lipka,Ryan A. Rossi,Franck Dernoncourt,Yu Zhang,Yao Ma,Yu Wang*

Main category: cs.LG

TL;DR: 대형 언어 모델(LLM)의 지식 구조를 그래프 표현을 통해 분석하고, 이웃 정보를 활용한 지식 평가 점수를 추정하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 지식 구조에 대한 이해를 높이고, 이를 통해 지식을 효율적으로 검사할 수 있는 방법을 모색하기 위함이다.

Method: LLM의 지식을 그래프로 매핑하고, 인접 노드와의 관계를 분석하여 지식 수준을 비교한다. 이후 GNN 회귀 모델을 통해 엔티티 수준의 지식 점수를 추정한다.

Result: LLM은 그래프에서 가까운 엔티티에 대해 비슷한 수준의 지식을 보유하고 있는 경향을 발견하였다. GNN 모델을 통해 예측된 지식 점수로 인해Less-known triplets의 검증 우선순위를 정할 수 있다.

Conclusion: 제안된 방법은 LLM의 지식 주입을 위한 효율적인 활성 라벨링과 다중 홉 경로 검색을 향상시킨다.

Abstract: Large Language Models (LLMs) have been increasingly studied as neural
knowledge bases for supporting knowledge-intensive applications such as
question answering and fact checking. However, the structural organization of
their knowledge remains unexplored. Inspired by cognitive neuroscience
findings, such as semantic clustering and priming, where knowing one fact
increases the likelihood of recalling related facts, we investigate an
analogous knowledge homophily pattern in LLMs. To this end, we map LLM
knowledge into a graph representation through knowledge checking at both the
triplet and entity levels. After that, we analyze the knowledgeability
relationship between an entity and its neighbors, discovering that LLMs tend to
possess a similar level of knowledge about entities positioned closer in the
graph. Motivated by this homophily principle, we propose a Graph Neural Network
(GNN) regression model to estimate entity-level knowledgeability scores for
triplets by leveraging their neighborhood scores. The predicted
knowledgeability enables us to prioritize checking less well-known triplets,
thereby maximizing knowledge coverage under the same labeling budget. This not
only improves the efficiency of active labeling for fine-tuning to inject
knowledge into LLMs but also enhances multi-hop path retrieval in
reasoning-intensive question answering.

</details>


### [84] [STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning](https://arxiv.org/abs/2509.23802)
*Yao Luan,Ni Mu,Yiqin Yang,Bo Xu,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 본 논문은 다단계 작업에 대한 Preference-based reinforcement learning (PbRL)의 한계를 다루며, STage-AlIgned Reward learning (STAIR) 방법을 제안하여 단계 불일치를 해결하고 성과를 개선하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: PbRL은 인간의 선호로부터 직접 보상을 학습하여 인간의 의도와 더 잘 일치하도록 하며, 이에는 미스매치 단계로 인한 한계가 있다.

Method: 본 논문에서는 단계 근사치를 시간적 거리 기반으로 학습하고, 동일 단계 내의 비교를 우선시하는 STAIR 방법을 제안한다. 시간적 거리는 대조 학습을 통해 학습하여 사전 정의된 작업 지식 없이도 동적으로 변화에 적응한다.

Result: 광범위한 실험을 통해 STAIR가 다단계 작업에서 우수한 성능을 보이며 단일 단계 작업에서도 경쟁력을 갖춘다는 것을 입증했다.

Conclusion: STAIR로 근사된 단계는 인간의 인지와 일치함을 보여 주며, 단계 불일치를 완화하는 데 효과적임을 확인했다.

Abstract: Preference-based reinforcement learning (PbRL) bypasses complex reward
engineering by learning rewards directly from human preferences, enabling
better alignment with human intentions. However, its effectiveness in
multi-stage tasks, where agents sequentially perform sub-tasks (e.g.,
navigation, grasping), is limited by stage misalignment: Comparing segments
from mismatched stages, such as movement versus manipulation, results in
uninformative feedback, thus hindering policy learning. In this paper, we
validate the stage misalignment issue through theoretical analysis and
empirical experiments. To address this issue, we propose STage-AlIgned Reward
learning (STAIR), which first learns a stage approximation based on temporal
distance, then prioritizes comparisons within the same stage. Temporal distance
is learned via contrastive learning, which groups temporally close states into
coherent stages, without predefined task knowledge, and adapts dynamically to
policy changes. Extensive experiments demonstrate STAIR's superiority in
multi-stage tasks and competitive performance in single-stage tasks.
Furthermore, human studies show that stages approximated by STAIR are
consistent with human cognition, confirming its effectiveness in mitigating
stage misalignment.

</details>


### [85] [Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR](https://arxiv.org/abs/2509.23808)
*Fanding Huang,Guanbo Huang,Xiao Fan,Yi He,Xiao Liang,Xiao Chen,Qinting Jiang,Faisal Nadeem Khan,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 이 논문은 강화학습에서 탐험과 착취의 균형을 재조명하고, 숨겨진 상태 공간을 분석하여 둘의 분리를 가능하게 하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 강화학습에서의 최근 발전이 탐험-착취의 균형 관점에서 해석되고 있으나, 이는 측정 수준의 산물이 될 수 있음을 제안한다.

Method: Effective Rank(ER)로 탐험을 정량화하고, 탐욕적 동력을 포착하기 위해 Effective Rank Velocity(ERV)와 Effective Rank Acceleration(ERA)의 새로운 1차 및 2차 도함수를 제안한다.

Result: 숨겨진 상태 수준에서 탐험과 착취가 분리될 수 있다는 것을 발견하고, 이는 두 가지 능력을 동시에 향상시킬 수 있는 기회를 제공한다.

Conclusion: VERL 방법론은 RL 이점 함수를 직접적으로 형성하여 탐험-착취 강화를 실현하는 최초의 방법으로, 실험 결과 Gaokao 2024 데이터셋에서 최대 21.4%의 절대 정확도 향상을 포함한 일관된 성과를 보인다.

Abstract: A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)
interprets recent progress through the lens of an exploration-exploitation
trade-off, a perspective largely shaped by token-level metrics. We re-examine
this perspective, proposing that this perceived trade-off may not be a
fundamental constraint but rather an artifact of the measurement level. To
investigate this, we shift the analysis to the semantically rich hidden-state
space, adopting Effective Rank (ER) to quantify exploration and proposing its
novel first- and second-order derivatives, named Effective Rank Velocity (ERV)
and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our
analysis reveals that at the hidden-state level, exploration and exploitation
could be decoupled (Sec. 4). This finding reveals an opportunity to enhance
both capacities simultaneously. This insight motivates our method,
Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the
principle of synergistic exploration-exploitation enhancement by directly
shaping the RL advantage function. The key innovation is leveraging the
theoretically stable ERA as a predictive meta-controller to create a
synergistic, dual-channel incentive structure. Instead of forcing a trade-off,
VERL prospectively amplifies rewards for exploration to preempt overconfidence
and reinforces exploitative gains to consolidate reasoning. Experiments across
diverse LLMs and reasoning benchmarks show consistent gains, including up to
21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.

</details>


### [86] [Space Group Conditional Flow Matching](https://arxiv.org/abs/2509.23822)
*Omri Puny,Yaron Lipman,Benjamin Kurt Miller*

Main category: cs.LG

TL;DR: 본 연구는 수 결정의 대칭성을 고려한 새로운 생성 프레임워크인 Space Group Conditional Flow Matching을 도입하고, 이를 통해 안정적이고 높은 대칭성을 가진 결정의 생성을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 생성 모델은 결정의 대칭성을 간과하여 비현실적으로 높은 비율의 제한된 대칭을 가진 제안된 결정 구조를 생성한다.

Method: 제안한 프레임워크는 특정 공간군과 위코프 위치 집합에 대한 조건화 과정을 통해 대칭적인 결정 샘플링을 수행한다.

Result: 이 방법은 대칭 결정 구조 예측과 신규 생성 벤치마크에서 최첨단 결과를 달성하였다.

Conclusion: 제안된 방법은 대칭 결정의 모션을 초기 위코프 위치에 제한함으로써 계산 부하를 줄이고, 대칭성을 고려한 생성 프로세스를 구현하였다.

Abstract: Inorganic crystals are periodic, highly-symmetric arrangements of atoms in
three-dimensional space. Their structures are constrained by the symmetry
operations of a crystallographic \emph{space group} and restricted to lie in
specific affine subspaces known as \emph{Wyckoff positions}. The frequency an
atom appears in the crystal and its rough positioning are determined by its
Wyckoff position. Most generative models that predict atomic coordinates
overlook these symmetry constraints, leading to unrealistically high
populations of proposed crystals exhibiting limited symmetry. We introduce
Space Group Conditional Flow Matching, a novel generative framework that
samples significantly closer to the target population of highly-symmetric,
stable crystals. We achieve this by conditioning the entire generation process
on a given space group and set of Wyckoff positions; specifically, we define a
conditionally symmetric noise base distribution and a group-conditioned,
equivariant, parametric vector field that restricts the motion of atoms to
their initial Wyckoff position. Our form of group-conditioned equivariance is
achieved using an efficient reformulation of \emph{group averaging} tailored
for symmetric crystals. Importantly, it reduces the computational overhead of
symmetrization to a negligible level. We achieve state of the art results on
crystal structure prediction and de novo generation benchmarks. We also perform
relevant ablations.

</details>


### [87] [Adversarial Diffusion for Robust Reinforcement Learning](https://arxiv.org/abs/2509.23846)
*Daniele Foffano,Alessio Russo,Alexandre Proutiere*

Main category: cs.LG

TL;DR: 이 논문은 확산 모델을 활용하여 강인한 강화 학습 정책을 교육하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 모델링 오류와 불확실성에 대한 강건성이 주된 도전 과제가 되고 있습니다.

Method: 조건부 샘플링을 활용하여 환경 역학의 불확실성에 강한 정책을 학습합니다.

Result: 기존 강인한 강화 학습 방법들에 비해 우수한 강건성과 성능을 달성합니다.

Conclusion: AD-RRL은 훈련 중 최악의 경우 궤적을 생성하도록 확산 과정을 안내하여 누적 수익의 CVaR을 효과적으로 최적화합니다.

Abstract: Robustness to modeling errors and uncertainties remains a central challenge
in reinforcement learning (RL). In this work, we address this challenge by
leveraging diffusion models to train robust RL policies. Diffusion models have
recently gained popularity in model-based RL due to their ability to generate
full trajectories "all at once", mitigating the compounding errors typical of
step-by-step transition models. Moreover, they can be conditioned to sample
from specific distributions, making them highly flexible. We leverage
conditional sampling to learn policies that are robust to uncertainty in
environment dynamics. Building on the established connection between
Conditional Value at Risk (CVaR) optimization and robust RL, we introduce
Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides
the diffusion process to generate worst-case trajectories during training,
effectively optimizing the CVaR of the cumulative return. Empirical results
across standard benchmarks show that AD-RRL achieves superior robustness and
performance compared to existing robust RL methods.

</details>


### [88] [Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation](https://arxiv.org/abs/2509.23866)
*Pengxiang Li,Zechen Hu,Zirui Shang,Jingrong Wu,Yang Liu,Hui Liu,Zhi Gao,Chenrui Shi,Bofei Zhang,Zihao Zhang,Xiaochuan Shi,Zedong YU,Yuwei Wu,Xinxiao Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.LG

TL;DR: GUI 에이전트를 위한 DART 프레임워크는 비동기 모듈을 통해 RL 훈련의 효율성을 크게 향상시켜, OSWorld 벤치마크에서 42.13%의 성공률을 기록했다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델 기반 GUI 에이전트가 복잡한 작업을 자동화하는 데 유망하지만, RL 적용 시 여러 도전 과제가 존재한다.

Method: DART는 환경 클러스터, 롤아웃 서비스, 데이터 관리자 및 트레이너로 구성된 네 개의 비동기 모듈로 훈련 시스템을 분리하여 효율성을 높인다.

Result: DART-GUI-7B는 OSWorld 기준에서 42.13%의 작업 성공률을 달성했으며, 이는 기본 모델보다 14.61% 증가한 수치이다.

Conclusion: 우리는 DART 훈련 프레임워크를 완전 공개하여 에이전틱 RL 훈련 커뮤니티에 기여할 예정이다.

Abstract: Vision-language model (VLM) based GUI agents show promise for automating
complex desktop and mobile tasks, but face significant challenges in applying
reinforcement learning (RL): (1) slow multi-turn interactions with GUI
environments for policy rollout, and (2) insufficient high-quality
agent-environment interactions for policy learning. To address these
challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI
agents, which coordinates heterogeneous modules in a highly decoupled manner.
DART separates the training system into four asynchronous modules: environment
cluster, rollout service, data manager, and trainer. This design enables
non-blocking communication, asynchronous training, rollout-wise trajectory
sampling, and per-worker model synchronization, significantly improving the
system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,
and 5.5* environment utilization. To facilitate effective learning from
abundant samples, we introduce an adaptive data curation scheme: (1)
pre-collecting successful trajectories for challenging tasks to supplement
sparse success in online sampling; (2) dynamically adjusting rollout numbers
and trajectory lengths based on task difficulty; (3) training selectively on
high-entropy steps to prioritize critical decisions; (4) stabilizing learning
via truncated importance sampling for policy mismatch between policy rollout
and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task
success rate, a 14.61% absolute gain over the base model, and 7.34% higher than
open-source SOTA. We will fully open-source our training framework, data, and
model checkpoints via computer-use-agents.github.io/dart-gui, which we believe
is a timely contribution to the open-source community of agentic RL training.

</details>


### [89] [Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer](https://arxiv.org/abs/2509.23886)
*Simon Schrodi,Elias Kempf,Fazl Barez,Thomas Brox*

Main category: cs.LG

TL;DR: 이 논문은 언어 모델에서 증류 과정 중 발생하는 숨겨진 편견의 전이 현상, 즉 '무의식적 학습'을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델이 증류 과정에서 숨겨진 편견을 전달할 수 있는 방법을 이해하는 것이 중요하다.

Method: 통제된 실험과 메커니즘 분석을 통해 무의식적 학습이 어떻게 발생하는지를 조사하였다.

Result: 무의식적 학습은 토큰 얽힘이나 로짓 유출 없이도 발생하며, 불일치 토큰의 작은 집합에 의존한다는 것을 발견하였다.

Conclusion: 조기 레이어가 중요하며, 미세 조정이 하루에 한 번의 조기 레이어만으로도 무의식적 학습을 촉진할 수 있음을 확인했다.

Abstract: Language models can transfer hidden biases during distillation. For example,
a teacher that "likes owls" can make its student "like owls" too, even when the
training data consists only of lists of numbers. This surprising phenomenon is
called subliminal learning. Subliminal learning can be expected under soft
distillation, where the student is trained on the teacher's full next-token
distribution. But the fact that this also occurs under hard distillation-where
the student only sees sampled tokens-raises a deeper question: when and how
does subliminal learning actually occur? We answer this question through
controlled experiments and mechanistic analysis. Our results show that
subliminal learning does not need (global) token entanglement or logit leakage.
Instead, it comes down to a small set of divergence tokens-rare cases where
teachers with different biases would predict different tokens. Masking out
these tokens mostly removes the hidden bias transfer. Mechanistically,
divergence tokens reveal that early layers are critical. Surprisingly,
finetuning even a single such early layer is sufficient for subliminal
learning. Finally, we find that subliminal learning is fragile. Even small
changes, like paraphrasing prompts, are usually sufficient to suppress it.

</details>


### [90] [Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.23905)
*Tianjiao Sun,Ningyan Guo,Haozhe Gu,Yanyan Peng,Zhiyong Feng*

Main category: cs.LG

TL;DR: 이 논문은 UAV 군집 지원 통신 네트워크의 품질을 향상시키기 위해 통합된 통신 및 제어 공동 설계 메커니즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 인프라가 부족한 환경에서 커버리지 한계를 해결하기 위해 UAV 군집 지원 통신 네트워크의 배치가 중요하다.

Method: 우리는 자원 할당 및 3D 궤적 제어 문제를 마르코프 결정 프로세스(MDP)로 формирование하고, 실시간으로 UAV 군단 간의 조정된 행동을 가능하게 하는 다중 에이전트 강화 학습(MARL) 프레임워크를 개발하였다.

Result: 우리가 제안한 알고리즘은 기준 방법에 비해 에너지 소비를 최대 25% 줄이면서 공정성 지수 0.99를 달성하였다.

Conclusion: 복잡한 지리적 환경에서 UAV 군집 지원 통신의 품질을 개선할 수 있는 효과적인 기법을 제시하였다.

Abstract: The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication
networks has become an increasingly vital approach for remediating coverage
limitations in infrastructure-deficient environments, with especially pressing
applications in temporary scenarios, such as emergency rescue, military and
security operations, and remote area coverage. However, complex geographic
environments lead to unpredictable and highly dynamic wireless channel
conditions, resulting in frequent interruptions of air-to-ground (A2G) links
that severely constrain the reliability and quality of service in UAV
swarm-assisted mobile communications. To improve the quality of UAV
swarm-assisted communications in complex geographic environments, we propose an
integrated communication and control co-design mechanism. Given the stringent
energy constraints inherent in UAV swarms, our proposed mechanism is designed
to optimize energy efficiency while maintaining an equilibrium between
equitable communication rates for mobile ground users (GUs) and UAV energy
expenditure. We formulate the joint resource allocation and 3D trajectory
control problem as a Markov decision process (MDP), and develop a multi-agent
reinforcement learning (MARL) framework to enable real-time coordinated actions
across the UAV swarm. To optimize the action policy of UAV swarms, we propose a
novel multi-agent hybrid proximal policy optimization with action masking
(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action
spaces. The algorithm incorporates action masking to enforce hard constraints
in high-dimensional action spaces. Experimental results demonstrate that our
approach achieves a fairness index of 0.99 while reducing energy consumption by
up to 25% compared to baseline methods.

</details>


### [91] [Efficient Identification of High Similarity Clusters in Polygon Datasets](https://arxiv.org/abs/2509.23942)
*John N. Daras*

Main category: cs.LG

TL;DR: Shapely 2.0 및 Triton과 같은 도구의 발전은 기하학적 연산을 더 빠르고 확장 가능하게 하여 공간 유사성 계산의 효율성을 크게 개선할 수 있다. 하지만 매우 큰 데이터셋의 경우 계산량의 문제로 인해 이러한 최적화가 어려울 수 있다. 본 연구에서는 검증이 필요한 클러스터 수를 줄여 계산 부하를 감소시키는 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 거대한 데이터셋에 대한 공간 유사성 계산을 최적화하고, 시스템의 계산 부하를 줄일 필요성.

Method: 다이내믹 유사성 인덱스 임계값 설정, 감독된 스케줄링, 회수 제약 최적화를 통합하여 공간 유사성이 가장 높은 클러스터를 효율적으로 식별하는 프레임워크를 제안한다.

Result: Kernel Density Estimation (KDE)을 활용해 동적으로 유사성 임계값을 결정하고, 기계 학습 모델을 통해 클러스터의 우선순위를 매김으로써 계산 비용을 크게 절감하면서도 정확성을 유지한다.

Conclusion: 실험 결과는 방법의 확장성과 효과성을 입증하며, 대규모 지리 공간 분석을 위한 실용적인 해결책을 제공한다.

Abstract: Advancements in tools like Shapely 2.0 and Triton can significantly improve
the efficiency of spatial similarity computations by enabling faster and more
scalable geometric operations. However, for extremely large datasets, these
optimizations may face challenges due to the sheer volume of computations
required. To address this, we propose a framework that reduces the number of
clusters requiring verification, thereby decreasing the computational load on
these systems. The framework integrates dynamic similarity index thresholding,
supervised scheduling, and recall-constrained optimization to efficiently
identify clusters with the highest spatial similarity while meeting
user-defined precision and recall requirements. By leveraging Kernel Density
Estimation (KDE) to dynamically determine similarity thresholds and machine
learning models to prioritize clusters, our approach achieves substantial
reductions in computational cost without sacrificing accuracy. Experimental
results demonstrate the scalability and effectiveness of the method, offering a
practical solution for large-scale geospatial analysis.

</details>


### [92] [DiBS-MTL: Transformation-Invariant Multitask Learning with Direction Oracles](https://arxiv.org/abs/2509.23948)
*Surya Murthy,Kushagra Gupta,Mustafa O. Karabag,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 본 연구에서는 비볼록 다중작업 학습에서 방향 기반 협상 해법(DiBS)이 파레토 정체점으로 수렴할 수 있음을 증명하고, 이를 다중작업 설정에 적응시킨 DiBS-MTL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다중작업 학습(MTL)에서 작업 손실이 다르게 조정될 수 있어 특정 작업이 훈련을 지배하고 전체 성능을 저하시키는 문제를 해결하고자 함.

Method: 비볼록 MTL 설정에서 작업 손실의 비선형 변화에 강건한 방향 기반 협상 해법(DiBS)을 정의하고, DiBS의 표준 가정 하에서 손실이 비볼록일 때의 수렴성을 보임. DiBS를 MTM 설정에 효율적으로 적용한 DiBS-MTL을 제안함.

Result: DiBS-MTL은 표준 MTL 벤치마크에서 실험적으로 검증되어, 최첨단 방법들과 경쟁력 있는 성능을 달성하며, 기존 방법들의 성능을 크게 저하시킨 비선형 변환에 대한 강건성을 유지함을 보여줌.

Conclusion: DiBS-MTL은 비볼록 다중작업 학습 문제에서 기존 방법들에 비해 우수한 성능과 안정성을 제공한다.

Abstract: Multitask learning (MTL) algorithms typically rely on schemes that combine
different task losses or their gradients through weighted averaging. These
methods aim to find Pareto stationary points by using heuristics that require
access to task loss values, gradients, or both. In doing so, a central
challenge arises because task losses can be arbitrarily, nonaffinely scaled
relative to one another, causing certain tasks to dominate training and degrade
overall performance. A recent advance in cooperative bargaining theory, the
Direction-based Bargaining Solution (DiBS), yields Pareto stationary solutions
immune to task domination because of its invariance to monotonic nonaffine task
loss transformations. However, the convergence behavior of DiBS in nonconvex
MTL settings is currently not understood. To this end, we prove that under
standard assumptions, a subsequence of DiBS iterates converges to a Pareto
stationary point when task losses are possibly nonconvex, and propose DiBS-MTL,
a computationally efficient adaptation of DiBS to the MTL setting. Finally, we
validate DiBS-MTL empirically on standard MTL benchmarks, showing that it
achieves competitive performance with state-of-the-art methods while
maintaining robustness to nonaffine monotonic transformations that
significantly degrade the performance of existing approaches, including prior
bargaining-inspired MTL methods. Code available at
https://github.com/suryakmurthy/dibs-mtl.

</details>


### [93] [Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts](https://arxiv.org/abs/2509.23976)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.LG

TL;DR: 본 논문은 CDM 명세로부터 기능적이고 가스 최적화된 Solidity 스마트 계약을 생성하는 강화 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 계약 기반 금융 파생상품의 자동화는 상당한 효율성을 제공하지만, 금융 명세를 가스 효율적인 실행 코드로 변환하는 복잡성으로 인해 실제 채택이 제한된다.

Method: CDM 명세로부터 최적 코드를 선택하는 Proximal Policy Optimization (PPO) 에이전트를 활용하여, 기능적 정확성을 먼저 훈련하고 나서 가스 최적화에 집중하는 두 단계 커리큘럼을 사용한다.

Result: RL 에이전트는 눈에 보이지 않는 테스트 데이터에서 비최적화 기준 대비 최대 35.59%의 비용 절감을 달성하는 계약을 생성하는 방법을 학습한다.

Conclusion: 이 연구는 고급 금융 계약과 효율적인 온체인 실행 간의 간극을 메우는 신뢰할 수 있고 경제적으로 지속 가능한 스마트 계약의 자동 합성을 위한 실행 가능한 방법론을 제시한다.

Abstract: Smart contract-based automation of financial derivatives offers substantial
efficiency gains, but its real-world adoption is constrained by the complexity
of translating financial specifications into gas-efficient executable code. In
particular, generating code that is both functionally correct and economically
viable from high-level specifications, such as the Common Domain Model (CDM),
remains a significant challenge. This paper introduces a Reinforcement Learning
(RL) framework to generate functional and gas-optimized Solidity smart
contracts directly from CDM specifications. We employ a Proximal Policy
Optimization (PPO) agent that learns to select optimal code snippets from a
pre-defined library. To manage the complex search space, a two-phase curriculum
first trains the agent for functional correctness before shifting its focus to
gas optimization. Our empirical results show the RL agent learns to generate
contracts with significant gas savings, achieving cost reductions of up to
35.59% on unseen test data compared to unoptimized baselines. This work
presents a viable methodology for the automated synthesis of reliable and
economically sustainable smart contracts, bridging the gap between high-level
financial agreements and efficient on-chain execution.

</details>


### [94] [Guide: Generalized-Prior and Data Encoders for DAG Estimation](https://arxiv.org/abs/2509.23992)
*Amartya Roy,Devharish N,Shreya Ganguly,Kripabandhu Ghosh*

Main category: cs.LG

TL;DR: 본 논문에서는 스케일과 계산 효율성에 제한이 있는 현대 인과 발견 방법의 문제를 해결하기 위해, 대형 언어 모델에서 생성된 인접 행렬과 관찰 데이터를 통합한 GUIDE 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 인과 발견 방법은 노드 스케일, 계산 에너지 수요 및 연속/비연속 데이터 처리와 같은 영역에서 중요한 한계를 가지고 있습니다.

Method: GUIDE는 대형 언어 모델에서 생성된 인접 행렬을 관찰 데이터와 통합하는 이중 인코더 아키텍처를 이용하여 계산 효율성을 최적화합니다.

Result: GUIDE는 평균적으로 RL-BIC 및 KCRL 방법에 비해 42%의 런타임 단축을 기록하며, NOTEARS 및 GraN-DAG에 비해 평균 117%의 정확도 향상을 달성합니다.

Conclusion: GUIDE는 강화 학습 에이전트를 통해 보상 극대화와 패널티 회피를 동적으로 조절하며, 혼합 데이터 유형에 대해 강력한 성능을 제공하고 70개 이상의 노드로 확장 가능합니다.

Abstract: Modern causal discovery methods face critical limitations in scalability,
computational efficiency, and adaptability to mixed data types, as evidenced by
benchmarks on node scalability (30, $\le 50$, $\ge 70$ nodes), computational
energy demands, and continuous/non-continuous data handling. While traditional
algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,
exhibiting prohibitive energy costs for higher-order nodes and poor scalability
beyond 70 nodes, we propose \textbf{GUIDE}, a framework that integrates Large
Language Model (LLM)-generated adjacency matrices with observational data
through a dual-encoder architecture. GUIDE uniquely optimizes computational
efficiency, reducing runtime on average by $\approx 42%$ compared to RL-BIC and
KCRL methods, while achieving an average $\approx 117%$ improvement in accuracy
over both NOTEARS and GraN-DAG individually. During training, GUIDE's
reinforcement learning agent dynamically balances reward maximization
(accuracy) and penalty avoidance (DAG constraints), enabling robust performance
across mixed data types and scalability to $\ge 70$ nodes -- a setting where
baseline methods fail.

</details>


### [95] [Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.24047)
*Runyu Zhang,Na Li,Asuman Ozdaglar,Jeff Shamma,Gioele Zardini*

Main category: cs.LG

TL;DR: 이 논문은 위험 감수성을 기반으로 한 다중 에이전트 강화 학습에서의 협력적 최적화를 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 위험 감수성은 강화 학습의 중심 주제로, 기대 수익 이상의 선호를 모델링하는 데 중요한 역할을 합니다. 협력적 다중 에이전트 강화 학습에서 비관주의가 종종 최적이 아닌 균형으로 이어지는 문제를 해결하고자 합니다.

Method: 위험 감수를 낙관주의로 해석하는 원칙적인 프레임워크를 제안하고, 낙관적인 가치 함수를 도입하여 위험 감수 평가를 형식화합니다. 이 기초 위에 정책 기울기 정리를 도출하고, 분산된 낙관적인 액터-비평자 알고리즘을 개발합니다.

Result: 실험 결과는 위험 감수적 낙관주의가 위험 중립 기준선과 휴리스틱 낙관적 방법보다 협력을 일관되게 향상시킨다는 것을 보여줍니다.

Conclusion: 우리의 프레임워크는 위험 감수적 학습과 낙관주의를 통합하여 MARL에서 협력에 대한 이론적으로 정당화된 효과적인 접근 방식을 제공합니다.

Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL),
where convex risk measures and robust formulations provide principled ways to
model preferences beyond expected return. Recent extensions to multi-agent RL
(MARL) have largely emphasized the risk-averse setting, prioritizing robustness
to uncertainty. In cooperative MARL, however, such conservatism often leads to
suboptimal equilibria, and a parallel line of work has shown that optimism can
promote cooperation. Existing optimistic methods, though effective in practice,
are typically heuristic and lack theoretical grounding. Building on the dual
representation for convex risk measures, we propose a principled framework that
interprets risk-seeking objectives as optimism. We introduce optimistic value
functions, which formalize optimism as divergence-penalized risk-seeking
evaluations. Building on this foundation, we derive a policy-gradient theorem
for optimistic value functions, including explicit formulas for the entropic
risk/KL-penalty setting, and develop decentralized optimistic actor-critic
algorithms that implement these updates. Empirical results on cooperative
benchmarks demonstrate that risk-seeking optimism consistently improves
coordination over both risk-neutral baselines and heuristic optimistic methods.
Our framework thus unifies risk-sensitive learning and optimism, offering a
theoretically grounded and practically effective approach to cooperation in
MARL.

</details>


### [96] [A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture](https://arxiv.org/abs/2509.24068)
*Roussel Rahman,Jeff Shrager*

Main category: cs.LG

TL;DR: 본 논문은 아동의 산술 학습을 설명하기 위해 전략 선택 이론을 소수학 모델로 재구성하고, 신경망 기반의 구조를 사용하여 카운팅 연습과 심볼 임베딩을 포함한다.


<details>
  <summary>Details</summary>
Motivation: 아동의 산술 학습을 더 잘 이해하기 위해, 기존의 전략 선택 이론을 확장하고 개선할 필요가 있다.

Method: 전략 선택 이론을 기반으로 한 신경망 아키텍처를 사용하여 소수학 모델을 개발하고, 카운팅 연습 및 게이티드 어텐션을 포함한다.

Result: 소수학 모델은 카운팅과 덧셈 간의 건설적 및 파괴적 간섭을 보여주고, 손가락 세기를 통해 합을 회상하는 방식의 파동적인 사용을 개선한다.

Conclusion: 이 모델은 수학적 추리를 위한 숫자 특성과 관계를 이해하는 통합 플랫폼으로 발전할 수 있는 가능성을 제시한다.

Abstract: Strategy Choice Theory (SCT)\footnote{``Strategy Choice Theory'',
``Distributions of Associations'', and ``Overlapping Wave Theory'' have been
used to refer to this line of work, emphasizing different
aspects.}\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth}
explains important aspects of children's arithmetic learning based upon
principles including learning from developmentally naturalistic data,
probabilistic representation, confidence-based retrieval, and the phase-like
importance of scaffolding strategies, such as finger-counting. Here we recast
SCT as a ``Small Math Model'' (SMM), employing a neural-network-based
architecture analogous to LLMs. The SMM extends SCT to include counting
practice\footnote{The original SCT model was pre-biased in accordance with the
supposed experience of counting.}, symbol (number) embedding, and gated
attention. Similar to earlier work, the SMM demonstrates constructive and
destructive interference between counting and addition, and the ``wave-like''
use of finger-counting as sum recall improves. We plan to extend the SMM to
later aspects of the decades-long SCT program, including adaptive strategy
choice and eventually strategy discovery, providing a unified platform to
investigate the understanding of numerical characteristics and relationships
essential for mathematical reasoning -- as it can emerge in LLM-based agents.

</details>


### [97] [PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM](https://arxiv.org/abs/2509.24085)
*Ju-Hyung Lee,Yanqing Lu,Klaus Doppler*

Main category: cs.LG

TL;DR: PEARL은 장치 간 통신에서 협력적인 교차 계층 최적화를 위한 프레임워크로, 상태 기반 Wi-Fi Aware 파라미터 선택을 통해 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 장치 간 통신에서의 최적화를 향상시키기 위해 협력적인 접근 방식을 개발하고자 함.

Method: 발행자와 구독자의 상태를 활용하여 Wi-Fi Aware 파라미터 선택을 유도하고, 응용 프로그램 허용 오차에 따라 지연을 정규화하고 장치 배터리 상태에 따라 에너지를 조절하는 문맥 인식 보상을 제공한다.

Result: PEARL은 가벼운 변형인 PEARL(Head + LoRA)와 PEARL-Lite(Head-only)를 통해 최적의 성능을 달성하고, 매우 유사한 목표 점수에서 20ms 미만의 추론을 제공한다.

Conclusion: PEARL은 동료 인식 문맥, 보상 정렬 훈련 및 헤드 기반 효율성 덕분에 항상 켜져 있는 장치에서 교차 계층 제어를 위한 LLM을 실용적으로 만든다.

Abstract: We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a
framework for cooperative cross-layer optimization in device-to-device (D2D)
communication. Building on our previous work on single-device on-device LLMs,
PEARL extends the paradigm by leveraging both publisher and subscriber states
to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which
normalizes latency by application tolerances and modulates energy by device
battery states, provides richer supervision for KL-based finetuning. We study
two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves
the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms
inference at near-identical objective scores. Across synthetic scenarios
grounded in real measurements, PEARL improves objective scores over heuristic
and compact model baselines and reduces energy by up to 16% in cooperative
low-battery cases. These results demonstrate that peer-aware context,
reward-aligned training, and head-based efficiency make LLMs practical for
always-on, on-device cross-layer control.

</details>


### [98] [Adversarial Reinforcement Learning Framework for ESP Cheater Simulation](https://arxiv.org/abs/2509.24274)
*Inkyu Park,Jeong-Gwan Lee,Taehwan Kwon,Juheon Choi,Seungku Kim,Junsu Kim,Kimin Lee*

Main category: cs.LG

TL;DR: 이 연구는 Extra-Sensory Perception(ESP) 치트 사용자의 행동을 시뮬레이션하는 프레임워크를 제안하여 효과적인 치트 방지 시스템을 개발하는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: ESP 치트 사용자는 적의 위치와 같은 숨겨진 게임 정보를 드러내지만, 이러한 행동은 관찰하기 어려워 효과적인 치트 방지 시스템을 위한 신뢰할 수 있는 데이터 수집이 어렵다.

Method: 이 논문에서는 치트 사용자와 비치트 사용자를 서로 다른 관찰 가능성을 가진 강화 학습 에이전트로 모델링하고, 탐지기가 이들의 행동 궤적을 분류하도록 한다. 또한 이 상호작용을 적대적 게임으로 모델링하여 두 플레이어가 시간이 지남에 따라 서로 적응하도록 한다.

Result: 실험 결과, 제안된 프레임워크가 치트 사용자의 적응 행동을 성공적으로 시뮬레이션하며, 보상 최적화와 탐지 회피 사이의 전략적 균형을 이루는 것을 보여준다.

Conclusion: 이 연구는 적응적인 치트 행동을 연구하고 효과적인 치트 탐지기를 개발하기 위한 제어 가능하고 확장 가능한 플랫폼을 제공한다.

Abstract: Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game
information such as enemy locations, are difficult to detect because their
effects are not directly observable in player behavior. The lack of observable
evidence makes it difficult to collect reliably labeled data, which is
essential for training effective anti-cheat systems. Furthermore, cheaters
often adapt their behavior by limiting or disguising their cheat usage, which
further complicates detection and detector development. To address these
challenges, we propose a simulation framework for controlled modeling of ESP
cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and
non-cheaters as reinforcement learning agents with different levels of
observability, while detectors classify their behavioral trajectories. Next, we
formulate the interaction between the cheater and the detector as an
adversarial game, allowing both players to co-adapt over time. To reflect
realistic cheater strategies, we introduce a structured cheater model that
dynamically switches between cheating and non-cheating behaviors based on
detection risk. Experiments demonstrate that our framework successfully
simulates adaptive cheater behaviors that strategically balance reward
optimization and detection evasion. This work provides a controllable and
extensible platform for studying adaptive cheating behaviors and developing
effective cheat detectors.

</details>


### [99] [Expanding Horizons of Level Diversity via Multi-objective Evolutionary Learning](https://arxiv.org/abs/2509.24341)
*Qingquan Zhang,Ziqi Wang,Yuchen Li,Keyuan Zhang,Bo Yuan,Jialin Liu*

Main category: cs.LG

TL;DR: 게임 수준 생성을 위한 다차원 다양성을 고려하여 생성 모델 훈련을 개선하는 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 게임 수준 생성은 더욱 풍부하고 매력적인 게임 경험에 기여하여 큰 관심을 받고 있습니다.

Method: 모델 훈련을 다목적 학습 문제로 공식화하고, 다수의 다양성 메트릭을 동시에 최적화하는 다목적 진화 학습 프레임워크를 제안합니다.

Result: 제안된 프레임워크는 다차원 다양성을 향상시키고, 플레이 가능성과 두 가지 대표적인 다양성 메트릭 간의 거래 범위를 제공하는 생성 모델의 파레토 전선을 식별할 수 있음을 보여줍니다.

Conclusion: 이러한 능력은 의사 결정자가 다양한 시나리오 및 플레이어와 디자이너의 다양한 요구에 맞는 생성기를 선택할 때 정보에 기반한 선택을 할 수 있도록 합니다.

Abstract: In recent years, the generation of diverse game levels has gained increasing
interest, contributing to a richer and more engaging gaming experience. A
number of level diversity metrics have been proposed in literature, which are
naturally multi-dimensional, leading to conflicted, complementary, or both
relationships among these dimensions. However, existing level generation
approaches often fail to comprehensively assess diversity across those
dimensions. This paper aims to expand horizons of level diversity by
considering multi-dimensional diversity when training generative models. We
formulate the model training as a multi-objective learning problem, where each
diversity metric is treated as a distinct objective. Furthermore, a
multi-objective evolutionary learning framework that optimises multiple
diversity metrics simultaneously throughout the model training process is
proposed. Our case study on the commonly used benchmark Super Mario Bros.
demonstrates that our proposed framework can enhance multi-dimensional
diversity and identify a Pareto front of generative models, which provides a
range of tradeoffs among playability and two representative diversity metrics,
including a content-based one and a player-centered one. Such capability
enables decision-makers to make informed choices when selecting generators
accommodating a variety of scenarios and the diverse needs of players and
designers.

</details>


### [100] [AXIS: Explainable Time Series Anomaly Detection with Large Language Models](https://arxiv.org/abs/2509.24378)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: AXIS는 시계열 이해를 위한 미세한 힌트를 제공하고, 설명 가능성과 detection 성능을 높이는 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 시계열 이상 탐지에서는 이상 발생 여부와 그 패턴, 이상 원인을 설명하는 것이 중요하다.

Method: AXIS는 동결된 LLM을 조건화하여 시계열 데이터를 텍스트로 처리하는 대신, 세 가지 보완 힌트를 활용한다.

Result: AXIS는 높은 품질의 설명과 함께 경쟁력 있는 이상 탐지 성능을 보여준다.

Conclusion: AXIS는 최첨단에서의 시계열 분석을 가능하게 하며, LLM 기반 및 인간 평가에서 우수한 성과를 입증하였다.

Abstract: Time-series anomaly detection (TSAD) increasingly demands explanations that
articulate not only if an anomaly occurred, but also what pattern it exhibits
and why it is anomalous. Leveraging the impressive explanatory capabilities of
Large Language Models (LLMs), recent works have attempted to treat time series
as text for explainable TSAD. However, this approach faces a fundamental
challenge: LLMs operate on discrete tokens and struggle to directly process
long, continuous signals. Consequently, naive time-to-text serialization
suffers from a lack of contextual grounding and representation alignment
between the two modalities. To address this gap, we introduce AXIS, a framework
that conditions a frozen LLM for nuanced time-series understanding. Instead of
direct serialization, AXIS enriches the LLM's input with three complementary
hints derived from the series: (i) a symbolic numeric hint for numerical
grounding, (ii) a context-integrated, step-aligned hint distilled from a
pretrained time-series encoder to capture fine-grained dynamics, and (iii) a
task-prior hint that encodes global anomaly characteristics. Furthermore, to
facilitate robust evaluation of explainability, we introduce a new benchmark
featuring multi-format questions and rationales that supervise contextual
grounding and pattern-level semantics. Extensive experiments, including both
LLM-based and human evaluations, demonstrate that AXIS yields explanations of
significantly higher quality and achieves competitive detection accuracy
compared to general-purpose LLMs, specialized time-series LLMs, and time-series
Vision Language Models.

</details>


### [101] [FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing](https://arxiv.org/abs/2509.24472)
*Ran Elbaz,Guy Bar-Shalom,Yam Eitan,Fabrizio Frasca,Haggai Maron*

Main category: cs.LG

TL;DR: FS-KAN은 임의의 순열 대칭 그룹에 대해 동등하고 불변적인 KA 계층을 구축하는 새로운 접근 방식을 제안하며, 기존의 연구를 통합하고 확장한다.


<details>
  <summary>Details</summary>
Motivation: 순열 대칭을 활용하여 모델의 일반화 및 계산 효율성을 크게 향상시킬 수 있는 강력한 모델을 요구하는 배경.

Method: Function Sharing KAN (FS-KAN)이라는 새로운 프레임워크를 도입하여 동등하고 불변적인 KA 계층을 구성한다.

Result: FS-KAN은 기존의 표준 매개변수 공유 계층과 동일한 표현력을 가지며, 여러 데이터 유형과 대칭 그룹에서 데이터 효율성이 우수하다.

Conclusion: FS-KAN은 KAN의 해석 가능성과 적응성을 유지하면서도 저데이터 환경에서 뛰어난 아키텍처 선택이 된다.

Abstract: Permutation equivariant neural networks employing parameter-sharing schemes
have emerged as powerful models for leveraging a wide range of data symmetries,
significantly enhancing the generalization and computational efficiency of the
resulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated
promise through their improved interpretability and expressivity compared to
traditional architectures based on MLPs. While equivariant KANs have been
explored in recent literature for a few specific data types, a principled
framework for applying them to data with permutation symmetries in a general
context remains absent. This paper introduces Function Sharing KAN (FS-KAN), a
principled approach to constructing equivariant and invariant KA layers for
arbitrary permutation symmetry groups, unifying and significantly extending
previous work in this domain. We derive the basic construction of these FS-KAN
layers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup
and provide a theoretical analysis demonstrating that FS-KANs have the same
expressive power as networks that use standard parameter-sharing layers,
allowing us to transfer well-known and important expressivity results from
parameter-sharing networks to FS-KANs. Empirical evaluations on multiple data
types and symmetry groups show that FS-KANs exhibit superior data efficiency
compared to standard parameter-sharing layers, by a wide margin in certain
cases, while preserving the interpretability and adaptability of KANs, making
them an excellent architecture choice in low-data regimes.

</details>


### [102] [Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations](https://arxiv.org/abs/2509.24556)
*Hussam Sababha,Bernat Font,Mohammed Daqaq*

Main category: cs.LG

TL;DR: 이 연구는 원통형 구조물의 고속 회전 제어를 통한 진동 제어의 실험적 배치를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 저속 수치 모의 방법에서 벗어나, 고속 회전 제어로 진동을 실시간으로 제어하고자 함.

Method: 고주파 회전 제어 전략을 학습하기 위해 DRL 에이전트를 활용하고, 상태 피드백을 사용하여 제어를 수행.

Result: 최대 95%의 진동 감쇠를 달성하는 고주파 회전 제어 전략을 성공적으로 학습하였다.

Conclusion: 이 연구는 DRL의 실제 실험에서의 적응성과 도구적 제약 극복 능력을 보여준다.

Abstract: This study showcases an experimental deployment of deep reinforcement
learning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV)
in a circular cylinder at a high Reynolds number (Re = 3000) using rotary
actuation. Departing from prior work that relied on low-Reynolds-number
numerical simulations, this research demonstrates real-time control in a
challenging experimental setting, successfully addressing practical constraints
such as actuator delay. When the learning algorithm is provided with state
feedback alone (displacement and velocity of the oscillating cylinder), the DRL
agent learns a low-frequency rotary control strategy that achieves up to 80%
vibration suppression which leverages the traditional lock-on phenomenon. While
this level of suppression is significant, it remains below the performance
achieved using high-frequency rotary actuation. The reduction in performance is
attributed to actuation delays and can be mitigated by augmenting the learning
algorithm with past control actions. This enables the agent to learn a
high-frequency rotary control strategy that effectively modifies vortex
shedding and achieves over 95% vibration attenuation. These results demonstrate
the adaptability of DRL for AFC in real-world experiments and its ability to
overcome instrumental limitations such as actuation lag.

</details>


### [103] [Emergent World Representations in OpenVLA](https://arxiv.org/abs/2509.24559)
*Marco Molinari,Leonardo Nevali,Saharsha Navani,Omar G. Younis*

Main category: cs.LG

TL;DR: 이 연구에서는 정책 기반 강화 학습으로 훈련된 비전-언어-행동 모델(VLA)이 세계 모델을 암묵적으로 학습하는지 조사합니다.


<details>
  <summary>Details</summary>
Motivation: VLA가 환경 역학을 명시적으로 모델링하지 않고 복잡한 행동을 인코딩하지만, 이러한 모델이 암묵적으로 세계 모델을 학습하는지 여부가 불확실합니다.

Method: 상태 표현에서 임베딩 산술을 사용하여 OpenVLA가 상태 전이에 대한 잠재적 지식을 포함하는지를 조사하는 실험적 방법론을 제안합니다.

Result: 상태 전이의 예측 능력이 통계적으로 유의미하게 나타나며 OpenVLA가 내부 세계 모델을 인코딩하고 있음을 나타냅니다.

Conclusion: Sparse Autoencoders(SAE)를 활용하여 OpenVLA의 세계 모델을 분석하는 파이프라인을 제안합니다.

Abstract: Vision Language Action models (VLAs) trained with policy-based reinforcement
learning (RL) encode complex behaviors without explicitly modeling
environmental dynamics. However, it remains unclear whether VLAs implicitly
learn world models, a hallmark of model-based RL. We propose an experimental
methodology using embedding arithmetic on state representations to probe
whether OpenVLA, the current state of the art in VLAs, contains latent
knowledge of state transitions. Specifically, we measure the difference between
embeddings of sequential environment states and test whether this transition
vector is recoverable from intermediate model activations. Using linear and non
linear probes trained on activations across layers, we find statistically
significant predictive ability on state transitions exceeding baselines
(embeddings), indicating that OpenVLA encodes an internal world model (as
opposed to the probes learning the state transitions). We investigate the
predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that
the world model emerges as training progresses. Finally, we outline a pipeline
leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.

</details>


### [104] [OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment](https://arxiv.org/abs/2509.24610)
*Liang Lin,Zhihao Xu,Junhao Dong,Jian Zhao,Yuchen Yuan,Guibin Zhang,Miao Yu,Yiming Zhang,Zhengtao Yao,Huahui Yi,Dongrui Liu,Xinfeng Li,Kun Wang*

Main category: cs.LG

TL;DR: OrthAlign은 다중 목표를 가진 언어 모델 정렬에서의 파라미터 수준의 갈등을 직접 해결하기 위해 직교 서브스페이스 분해를 이용한 혁신적인 접근법이다.


<details>
  <summary>Details</summary>
Motivation: 다양한 인간의 선호를 만족시키기 위한 LLM 정렬에서 갈등이 발생하는 문제를 해결하고자 한다.

Method: OrthAlign은 파라미터 업데이트 공간을 직교 서브스페이스로 분해하여 서로 간섭하지 않는 방향으로 다양한 선호를 최적화한다.

Result: OrthAlign은 유용성, 무해성, 진실성 차원에서 다중 목표 정렬 후 최대 34.61%에서 50.89%의 단일 선호 개선을 달성하고, 평균 전체 보상 개선은 13.96%에 이른다.

Conclusion: 이 방법은 모든 선호 차원에서 안정적인 수렴을 보장하며, 실험 결과 안정성과 효율성을 입증하였다.

Abstract: Large language model (LLM) alignment faces a critical dilemma when addressing
multiple human preferences: improvements in one dimension frequently come at
the expense of others, creating unavoidable trade-offs between competing
objectives like helpfulness and harmlessness. While prior work mainly focuses
on constraint-based optimization algorithms and data selection strategies to
mitigate conflicts, these approaches overlook the fundamental issue of
resolving conflicts directly at the parameter level. In this paper, we present
OrthAlign, an innovative approach that pioneers a new paradigm by leveraging
orthogonal subspace decomposition to fundamentally resolve gradient-level
conflicts in multi-objective preference alignment. OrthAlign strategically
decomposes parameter update spaces into orthogonal subspaces, ensuring that
optimization toward different preferences occurs in mathematically
non-interfering directions. Building upon this, we provide theoretical
guarantees demonstrating that when parameter increments satisfy both orthogonal
subspace constraints and spectral norm bounds, the resulting updates exhibit
linear Lipschitz growth rather than exponential instability, ensuring stable
convergence across all preference dimensions. Extensive experiments show that:
I. OrthAlign achieves maximum single-preference improvements ranging from
34.61% to 50.89% after multiple-objective alignment across helpful, harmless,
and truthful dimensions. II. With an average overall reward improvement of
13.96%.

</details>


### [105] [T-POP: Test-Time Personalization with Online Preference Feedback](https://arxiv.org/abs/2509.24696)
*Zikun Qu,Min Zhang,Mingze Kong,Xiang Li,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Shuang Qiu,Yao Shu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: T-POP은 온라인 선호 피드백을 통해 LLM을 실시간으로 개인화하는 새로운 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)을 개인 사용자 선호도에 맞게 개인화하는 것은 일반적으로 유용한 응답을 생성하는 것을 넘어서는 중요한 단계이다.

Method: T-POP은 텍스트 생성 중 수집된 온라인 쌍선호 피드백을 학습하는 새로운 개인화 패러다임을 제안한다. 이 알고리즘은 테스트 타임 정렬과 듀얼 밴디트 기법을 결합하여, 업데이트 없이 LLM의 디코딩 과정을 사용자 선호를 포착하는 보상 함수를 온라인으로 학습하여 유도한다.

Result: 광범위한 실험을 통해 T-POP은 빠르고 데이터 효율적인 개인화를 달성하며, 기존 기준선보다 상당히 우수한 성과를 보이고 더 많은 사용자 상호작용을 통해 일관된 개선을 보여준다.

Conclusion: T-POP은 개인화 성능을 높이는 데 효과적인 방법으로, 사용자 피드백을 활용하여 더욱 적응하는 시스템을 만든다.

Abstract: Personalizing large language models (LLMs) to individual user preferences is
a critical step beyond generating generically helpful responses. However,
current personalization methods are ill-suited for new users, as they typically
require either slow, resource-intensive fine-tuning or a substantial amount of
pre-existing user data, creating a significant cold-start problem. To address
this challenge, we introduce a new paradigm for real-time personalization by
learning from online pairwise preference feedback collected during text
generation. We propose T-POP (Test-Time Personalization with Online Preference
Feedback}), a novel algorithm that synergistically combines test-time alignment
with dueling bandits. Without updating the LLM parameters, T-POP steers the
decoding process of a frozen LLM by learning a reward function online that
captures user preferences. By leveraging dueling bandits, T-POP intelligently
queries the user to efficiently balance between exploring their preferences and
exploiting the learned knowledge to generate personalized text. Extensive
experiments demonstrate that T-POP achieves rapid and data-efficient
personalization, significantly outperforming existing baselines and showing
consistent improvement with more user interactions.

</details>


### [106] [FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits](https://arxiv.org/abs/2509.24701)
*Pingchen Lu,Zhi Hong,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Yao Shu,Min Zhang,Shuang Qiu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: 이 논문은 대규모 언어 모델의 프롬프트 최적화 문제를 해결하기 위해 샘플 효율적인 연합 프롬프트 최적화 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 성능이 입력 프롬프트에 매우 민감하므로 프롬프트 최적화가 중요하지만, 지식의 독점성과 높은 샘플 효율성 요구, 사용자 간의 개인정보 보호라는 현실적인 도전에 직면해 있습니다.

Method: 다중 무장 강도(MAB)를 기반으로 한 연합 프롬프트 최적화 프레임워크를 제안하며, 협력을 통해 모델 매개변수를 공유하는 FedPOB 알고리즘을 구현합니다.

Result: FedPOB와 FedPOB-Pref가 기존 기준선을 크게 초과하며, 참여하는 에이전트 수가 증가할수록 성능이 지속적으로 향상됨을 보여줍니다.

Conclusion: 우리의 연합적 접근 방식의 효과를 검증하고, 샘플 효율적이며 협업 학습을 가능하게 하는 새로운 프레임워크를 통해 이 문제를 해결합니다.

Abstract: The performance of large language models (LLMs) is highly sensitive to the
input prompt, making prompt optimization a critical task. However, real-world
application is hindered by three major challenges: (1) the black-box nature of
powerful proprietary LLMs, (2) the need for high sample efficiency due to query
costs, and (3) the desire for privacy-preserving collaboration among multiple
users. To address these challenges simultaneously, we introduce a novel
framework for sample-efficient federated prompt optimization based on
multi-armed bandits (MABs). The MAB framework is uniquely suited for this
problem as it is (1) inherently a black-box optimization method, (2)
practically sample-efficient, and (3) enables collaborative learning with
theoretically guaranteed benefit from more participating agents. We first
propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a
federated variant of the Linear UCB algorithm, where agents collaborate by
sharing model parameters instead of raw data. We then extend our approach to
the practical setting of comparative user feedback by introducing FedPOB with
Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated
dueling bandits. Extensive experiments demonstrate that both FedPOB and
FedPOB-Pref significantly outperform existing baselines and that their
performance consistently improves as more agents participate in the
collaboration, validating the effectiveness of our federated approach.

</details>


### [107] [Quantifying Generalisation in Imitation Learning](https://arxiv.org/abs/2509.24784)
*Nathan Gavenski,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: Labyrinth는 모방 학습의 일반화 평가를 위해 구조, 시작 및 목표 위치, 작업 복잡성을 정밀하게 제어하는 벤치마크 환경이다.


<details>
  <summary>Details</summary>
Motivation: 모방 학습 벤치마크는 훈련과 평가 간의 충분한 변동성이 부족하여 의미 있는 일반화 평가를 제한한다.

Method: Labyrinth는 구조, 시작 및 목표 위치, 작업 복잡성을 제어할 수 있는 벤치마크 환경이다.

Result: Labyrinth는 구별되는 훈련, 평가 및 테스트 설정을 제공하고, 이로 인해 일반화 평가가 개선된다.

Conclusion: Labyrinth는 모방 학습의 일반화 평가를 발전시키고 더 강력한 에이전트를 개발하기 위한 유용한 도구를 제공한다.

Abstract: Imitation learning benchmarks often lack sufficient variation between
training and evaluation, limiting meaningful generalisation assessment. We
introduce Labyrinth, a benchmarking environment designed to test generalisation
with precise control over structure, start and goal positions, and task
complexity. It enables verifiably distinct training, evaluation, and test
settings. Labyrinth provides a discrete, fully observable state space and known
optimal actions, supporting interpretability and fine-grained evaluation. Its
flexible setup allows targeted testing of generalisation factors and includes
variants like partial observability, key-and-door tasks, and ice-floor hazards.
By enabling controlled, reproducible experiments, Labyrinth advances the
evaluation of generalisation in imitation learning and provides a valuable tool
for developing more robust agents.

</details>


### [108] [DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.24800)
*Zixu Wang,Hongbin Dong,Xiaoping Zhang*

Main category: cs.LG

TL;DR: 본 논문에서는 기존의 시간 시계열 예측 방법의 한계를 극복하기 위해 Hybrid Decomposition Dual-Stream Adaptive Transformer (DSAT-HD)를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 예측은 기상, 교통, 전기 및 에너지 예측 등 다양한 응용 분야에서 중요합니다. 기존의 방법은 제한된 시계열 또는 고정된 스케일만 모델링하여 다양한 특성을 캡처하기 어렵게 합니다.

Method: DSAT-HD는 1) EMA와 푸리에 분해를 결합한 하이브리드 분해 메커니즘, 2) 네 개의 병렬 Transformer 레이어로 특성을 라우팅하는 다중 스케일 적응 경로, 3) CNN과 MLP가 각각 계절성과 추세 성분을 처리하는 이중 스트림 잔여 학습 프레임워크를 통해 기능적으로 혁신합니다.

Result: 기존 방법보다 전반적으로 우수한 성능을 발휘하며, 일부 데이터 세트에서 최첨단 성능을 달성합니다.

Conclusion: DSAT-HD는 다양한 이동 시나리오에서 강력한 일반화 능력을 보입니다.

Abstract: Time series forecasting is crucial for various applications, such as weather,
traffic, electricity, and energy predictions. Currently, common time series
forecasting methods are based on Transformers. However, existing approaches
primarily model limited time series or fixed scales, making it more challenging
to capture diverse features cross different ranges. Additionally, traditional
methods like STL for complex seasonality-trend decomposition require
pre-specified seasonal periods and typically handle only single, fixed
seasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive
Transformer (DSAT-HD), which integrates three key innovations to address the
limitations of existing methods: 1) A hybrid decomposition mechanism combining
EMA and Fourier decomposition with RevIN normalization, dynamically balancing
seasonal and trend components through noise Top-k gating; 2) A multi-scale
adaptive pathway leveraging a sparse allocator to route features to four
parallel Transformer layers, followed by feature merging via a sparse combiner,
enhanced by hybrid attention combining local CNNs and global interactions; 3) A
dual-stream residual learning framework where CNN and MLP branches separately
process seasonal and trend components, coordinated by a balanced loss function
minimizing expert collaboration variance. Extensive experiments on nine
datasets demonstrate that DSAT-HD outperforms existing methods overall and
achieves state-of-the-art performance on some datasets. Notably, it also
exhibits stronger generalization capabilities across various transfer
scenarios.

</details>


### [109] [DyMoDreamer: World Modeling with Dynamic Modulation](https://arxiv.org/abs/2509.24804)
*Boxuan Zhang,Runqing Wang,Wei Xiao,Weipu Zhang,Jian Sun,Gao Huang,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: 본 연구는 딥 강화 학습(DRL)의 샘플 비효율성 문제를 해결하기 위한 DyMoDreamer라는 새로운 모델 기반 강화 학습(MBRL) 알고리즘을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 딥 강화 학습에서 고성능 에이전트를 훈련시키기 위해서는 많은 환경 상호작용이 필요하지만, 이는 샘플 비효율성 문제를 야기한다.

Method: DyMoDreamer는 동적 조절 메커니즘을 도입하여 동적 특성 추출을 개선하고 시간 정보를 풍부하게 한다. 이는 새로운 프레임 간 차분 마스크로부터 파생된 차별적 관찰을 사용하여 객체 수준의 움직임 단서를 명시적으로 인코딩한다.

Result: 실험 결과, DyMoDreamer는 Atari $100$k 벤치마크에서 평균 인간 정규화 점수 $156.6\%$로 새로운 최첨단 기록을 세웠고, DeepMind Visual Control Suite에서 $832$의 새로운 기록을 수립했으며, Crafter 벤치마크에서 $1$M 단계 후에 $9.5\\%$의 성능 향상을 보였다.

Conclusion: DyMoDreamer는 강화 학습 분야에서 동적 객체와 시간적 특성을 효과적으로 분리하여 샘플 효율성을 크게 개선하는 성과를 달성했다.

Abstract: A critical bottleneck in deep reinforcement learning (DRL) is sample
inefficiency, as training high-performance agents often demands extensive
environmental interactions. Model-based reinforcement learning (MBRL) mitigates
this by building world models that simulate environmental dynamics and generate
synthetic experience, improving sample efficiency. However, conventional world
models process observations holistically, failing to decouple dynamic objects
and temporal features from static backgrounds. This approach is computationally
inefficient, especially for visual tasks where dynamic objects significantly
influence rewards and decision-making performance. To address this, we
introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic
modulation mechanism to improve the extraction of dynamic features and enrich
the temporal information. DyMoDreamer employs differential observations derived
from a novel inter-frame differencing mask, explicitly encoding object-level
motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic
categorical distributions and integrated into a recurrent state-space model
(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments
demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k
benchmark with a $156.6$\% mean human-normalized score, establishes a new
record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\%
performance improvement after $1$M steps on the Crafter benchmark. Our code is
released at https://github.com/Ultraman-Tiga1/DyMoDreamer.

</details>


### [110] [Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation](https://arxiv.org/abs/2509.24873)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 본 연구는 SoilNet 모델에 대한 불확실성 보정을 통해 인간-기계 협업을 개선하고, 주어진 예산 내에서 더 효율적인 주석을 가능하게 하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 인간-기계 협업에서 기계의 신뢰도에 따라 인간의 결정이 조정되기 때문에 불확실성 정량화가 필수적이다.

Method: 불확실성 보정을 위해 비모수적 예측(conformal prediction) 기법을 적용하고, SoilNet이라는 다중 모달 다중 작업 모델에 이 기법을 적용하였다.

Result: 불확실성이 높은 상황에서 특정 예산 내에서 보다 효율적인 회귀 작업 주석과 유사한 성능 점수를 달성함을 보였다.

Conclusion: 우리는 코드와 실험 결과를 GitHub 저장소에서 제공하며, 불확실성 보정 기법의 적용이 모델 효율성을 높일 수 있음을 확인하였다.

Abstract: Uncertainty quantification is essential in human-machine collaboration, as
human agents tend to adjust their decisions based on the confidence of the
machine counterpart. Reliably calibrated model uncertainties, hence, enable
more effective collaboration, targeted expert intervention and more responsible
usage of Machine Learning (ML) systems. Conformal prediction has become a well
established model-agnostic framework for uncertainty calibration of ML models,
offering statistically valid confidence estimates for both regression and
classification tasks. In this work, we apply conformal prediction to
$\textit{SoilNet}$, a multimodal multitask model for describing soil profiles.
We design a simulated human-in-the-loop (HIL) annotation pipeline, where a
limited budget for obtaining ground truth annotations from domain experts is
available when model uncertainty is high. Our experiments show that
conformalizing SoilNet leads to more efficient annotation in regression tasks
and comparable performance scores in classification tasks under the same
annotation budget when tested against its non-conformal counterpart. All code
and experiments can be found in our repository:
https://github.com/calgo-lab/BGR

</details>


### [111] [Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime](https://arxiv.org/abs/2509.24882)
*Leonardo Defilippis,Yizhou Xu,Julius Girardin,Emanuele Troiani,Vittorio Erba,Lenka Zdeborová,Bruno Loureiro,Florent Krzakala*

Main category: cs.LG

TL;DR: 본 연구는 Quadratic 및 Diagonal 신경망의 스케일링 법칙에 대한 체계적인 분석을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 신경망의 스케일링 법칙은 최근 딥러닝의 많은 진전을 뒷받침하고 있지만, 이론적 이해는 여전히 선형 모델에 국한되어 있습니다.

Method: 행렬 압축 감지 및 LASSO와의 연결을 활용하여, 샘플 복잡성과 가중치 감소의 함수로서 과잉 위험의 스케일링 지수에 대한 자세한 위상 다이어그램을 도출했습니다.

Result: 이 분석은 뚜렷한 스케일링 영역 간의 교차점과 플래토 행동을 발견했습니다.

Conclusion: 또한, 우리는 이러한 영역과 훈련된 네트워크 가중치의 스펙트럼 특성 간의 정확한 연결을 확립하여, 네트워크 일반화 성능과의 관계를 이론적으로 검증하여 기본 원리에서 해석을 제공합니다.

Abstract: Neural scaling laws underlie many of the recent advances in deep learning,
yet their theoretical understanding remains largely confined to linear models.
In this work, we present a systematic analysis of scaling laws for quadratic
and diagonal neural networks in the feature learning regime. Leveraging
connections with matrix compressed sensing and LASSO, we derive a detailed
phase diagram for the scaling exponents of the excess risk as a function of
sample complexity and weight decay. This analysis uncovers crossovers between
distinct scaling regimes and plateau behaviors, mirroring phenomena widely
reported in the empirical neural scaling literature. Furthermore, we establish
a precise link between these regimes and the spectral properties of the trained
network weights, which we characterize in detail. As a consequence, we provide
a theoretical validation of recent empirical observations connecting the
emergence of power-law tails in the weight spectrum with network generalization
performance, yielding an interpretation from first principles.

</details>


### [112] [When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training](https://arxiv.org/abs/2509.24923)
*Sanxing Chen,Xiaoyin Chen,Yukun Huang,Roy Xie,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 대형 언어 모델(LLM)은 자율 에이전트로서의 가능성을 지니고 있지만, 연속적인 의사결정에서 비최적적으로 탐색하는 경향이 있습니다. 본 연구는 전문가 경로를 통한 감독된 세부 조정(SFT)과 다양한 맞춤형 보상 신호를 활용한 강화 학습(RL)을 통해 이를 개선하는 방법을 탐구합니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 자율 에이전트로서 탐색 능력을 향상시키기 위해 기존의 SFT와 RL 방법들을 조합하고, 이들 방법이 어떻게 탐색 전략에 영향을 미치는지를 이해하고자 함.

Method: 전문가 경로에 대한 SFT 및 전략적 보상 신호를 포함한 맞춤형 보상을 활용한 RL을 통해 LLM을 훈련하여 탐색 전략을 비교분석함.

Result: 훈련된 에이전트는 사전 훈련된 모델보다 성능이 우수하고, UCB 및 톰슨 샘플링과 유사한 성능을 달성함. 6배 긴 지평선과 다양한 밴딧 패밀리에 대한 강력한 일반화를 보임.

Conclusion: 각 훈련 패러다임의 선호하는 상황을 명확히 하고, 평균 후회 이상의 평가를 통한 맞춤형 보상 설계와 평가의 필요성을 강조함.

Abstract: While Large Language Models (LLMs) hold promise to become autonomous agents,
they often explore suboptimally in sequential decision-making. Recent work has
sought to enhance this capability via supervised fine-tuning (SFT) or
reinforcement learning (RL), improving regret on the classic multi-armed bandit
task. However, it remains unclear how these learning methods shape exploration
strategies and how well they generalize. We investigate both paradigms by
training LLMs with SFT on expert trajectories and RL with a range of tailored
reward signals including a strategic, regret-shaped reward to reduce variance,
and an algorithmic reward that enables oracle imitation. The resulting agents
outperform pre-trained models and achieve performance comparable to Upper
Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x
longer horizons and across bandit families. Behavioral analysis reveals that
gains often stem from more sophisticated but greedier exploitation: RL/SFT
agents are more prone to early catastrophic failure than pre-trained models,
prematurely abandoning exploration. Furthermore, agents trained to imitate UCB
learn to outperform their teacher by adopting more exploitative variants. Our
findings clarify when each training paradigm is preferable and advocate
tailored reward design and evaluation beyond average regret to promote robust
exploratory behavior.

</details>


### [113] [Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer](https://arxiv.org/abs/2509.24947)
*Sooraj Sathish,Keshav Goyal,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: 본 연구에서는 심층 강화 학습 모델의 특성 표현을 통해 선형 함수 근사기와 같은 간단한 모델을 새로운 작업에 대해 훈련하는 방법을 탐구하며, 정규화 항을 도입하여 특성 표현 간의 양의 상관관계를 줄이는 새로운 심층 Q-학습 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 심층 강화 학습 모델 훈련에는 하이퍼파라미터 조정 및 높은 계산 비용과 같은 여러 가지 도전 과제가 있으며, 이전 작업에서 학습한 지식을 새로운 관련 작업에 재사용할 수 있는 전이 학습 전략이 필요하다.

Method: 기본 심층 강화 학습 모델에서 학습된 내부 표현을 활용하여 새로운 작업에 대해 간단한 모델, 예를 들어 선형 함수 근사기를 훈련시키는 것이 가능할지를 조사하고, 이를 위해 정규화 항을 도입한다.

Result: 실험 결과, 선형 함수 근사기에서의 이점이 확인되었고, 특징 표현 간의 양의 상관관계를 줄임으로써 전이 학습 성능이 향상되었으며 계산 오버헤드가 줄어들었다.

Conclusion: 제안한 접근 방식은 전이 학습 성능을 개선하고 계산 비용을 줄이는 데 효과적이다.

Abstract: Deep Reinforcement Learning (RL) has demonstrated success in solving complex
sequential decision-making problems by integrating neural networks with the RL
framework. However, training deep RL models poses several challenges, such as
the need for extensive hyperparameter tuning and high computational costs.
Transfer learning has emerged as a promising strategy to address these
challenges by enabling the reuse of knowledge from previously learned tasks for
new, related tasks. This avoids the need for retraining models entirely from
scratch. A commonly used approach for transfer learning in RL is to leverage
the internal representations learned by the neural network during training.
Specifically, the activations from the last hidden layer can be viewed as
refined state representations that encapsulate the essential features of the
input. In this work, we investigate whether these representations can be used
as input for training simpler models, such as linear function approximators, on
new tasks. We observe that the representations learned by standard deep RL
models can be highly correlated, which limits their effectiveness when used
with linear function approximation. To mitigate this problem, we propose a
novel deep Q-learning approach that introduces a regularization term to reduce
positive correlations between feature representation of states. By leveraging
these reduced correlated features, we enable more effective use of linear
function approximation in transfer learning. Through experiments and ablation
studies on standard RL benchmarks and MinAtar games, we demonstrate the
efficacy of our approach in improving transfer learning performance and thereby
reducing computational overhead.

</details>


### [114] [MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts](https://arxiv.org/abs/2509.25020)
*Jiayu Liu,Zhenya Huang,Anya Sims,Enhong Chen,Yee Whye Teh,Ning Miao*

Main category: cs.LG

TL;DR: MARCOS는 연속적인 높은 차원의 '생각'으로 추론을 모델링하여, 기존의 토큰 기반의 접근법인 체인 오브 씽킹의 단점을 극복합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 체인 오브 씽킹 방법론은 느리고 비효율적인 추론과 정보 병목 현상을 초래하며, 추론과 토큰 생성을 얽히게 합니다.

Method: MARCO에서는 토큰을 자가 회귀적으로 생성하는 대신, 숨겨진 마르코프 체인 모델을 사용하여 연속적인 고차원 '생각'으로 추론을 모델링합니다.

Result: MARCOS는 세 가지 벤치마크에서 기존의 연속 추론 방법보다 우수한 성능을 보였고, 토큰 기반 체인 오브 씽킹과 비교할 때 처음으로 동등한 성능을 보여주었으며, GSM8K의 경우 4.7% 향상과 15.7배 빠른 추론 속도를 기록하였습니다.

Conclusion: MARCOS는 무작위성에 대한 단계 수준의 제어를 제공하여 LLM에서의 강화 학습과 추론에 대한 중요한 기회를 제공합니다.

Abstract: The current paradigm for reasoning in large language models (LLMs) involves
models "thinking out loud" via a sequence of tokens, known as chain-of-thought
(CoT). This approach, while effective, has several significant drawbacks.
Firstly, inference requires autoregressive generation of often thousands of CoT
tokens, which is slow and computationally expensive. Secondly, it constrains
reasoning to the discrete space of tokens, creating an information bottleneck
across reasoning steps. Thirdly, it fundamentally entangles reasoning with
token generation, forcing LLMs to "think while speaking," which causes
potentially short-sighted reasoning. In light of these limitations, we
re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our
approach, rather than autoregressively generating tokens, we model reasoning as
a hidden Markov chain of continuous, high-dimensional "thoughts". Each
reasoning step involves a transition of the internal thoughts, where explicit
reasoning steps (which may consist of hundreds of tokens) serve as observable
variables, which are windows to peek into the implicit thoughts. Since this
latent process is incompatible with the standard supervised learning, we
further propose a two-phase variational training scheme. Our experiments on
three benchmarks demonstrate that MARCOS outperforms existing continuous
reasoning methods and, for the first time, achieves performance comparable to
token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup
in inference. Beyond this, MARCOS offers additional advantages, such as
step-level instead of token-level control over randomness, opening significant
opportunities for reinforcement learning and reasoning in LLMs.

</details>


### [115] [Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios](https://arxiv.org/abs/2509.25031)
*Sophia V. Kuhn,Rafael Bischof,Marius Weber,Antoine Binggeli,Michael A. Kraus,Walter Kaufmann,Fernando Pérez-Cruz*

Main category: cs.LG

TL;DR: 이 논문은 노후화된 인프라 포트폴리오의 구조 평가에 대한 새로운 접근법으로 Bayesian 신경망(BNN) 대리 모델을 제안하며, 이는 구조적 사전 평가를 신속하게 수행할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 노후화된 인프라 포트폴리오는 자원 배분에 중대한 도전 과제를 제공하며, 개입이 필요한 구조와 안전하게 운영될 수 있는 구조를 결정해야 한다.

Method: Bayesian 신경망(BNN) 대리 모델을 사용하여 전 세계 공통 교량 유형의 신속한 구조 사전 평가를 수행한다. 이 모델은 비선형 유한 요소 분석을 기반으로 한 대규모 데이터베이스에서 훈련되었다.

Result: BNN 대리 모델은 코드 준수 요인을 예측하고 교정된 인지적 불확실성을 통해 높은 신뢰성의 구조 분석 결과를 정확하고 효율적으로 추정한다.

Conclusion: 이 프레임워크는 실제 사례 연구에서 효율성을 입증하였으며, 불필요한 분석과 물리적 개입을 피함으로써 비용과 배출량을 상당히 줄일 수 있는 잠재력을 보여준다.

Abstract: Aging infrastructure portfolios pose a critical resource allocation
challenge: deciding which structures require intervention and which can safely
remain in service. Structural assessments must balance the trade-off between
cheaper, conservative analysis methods and accurate but costly simulations that
do not scale portfolio-wide. We propose Bayesian neural network (BNN)
surrogates for rapid structural pre-assessment of worldwide common bridge
types, such as reinforced concrete frame bridges. Trained on a large-scale
database of non-linear finite element analyses generated via a parametric
pipeline and developed based on the Swiss Federal Railway's bridge portfolio,
the models accurately and efficiently estimate high-fidelity structural
analysis results by predicting code compliance factors with calibrated
epistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware
triage: flagging likely critical structures and providing guidance where
refined analysis is pertinent. We demonstrate the framework's effectiveness in
a real-world case study of a railway underpass, showing its potential to
significantly reduce costs and emissions by avoiding unnecessary analyses and
physical interventions across entire infrastructure portfolios.

</details>


### [116] [Towards generalizable deep ptychography neural networks](https://arxiv.org/abs/2509.25104)
*Albert Vong,Steven Henke,Oliver Hoidn,Hanna Ruth,Junjing Deng,Alexander Hexemer,Apurva Mehta,Arianna Gleason,Levi Hancock,Nicholas Schwarz*

Main category: cs.LG

TL;DR: X선 ptychography는 차세대 광원에서 사용될 데이터 집약적인 이미징 기술로, 깊은 신경망을 이용한 대체 재구성 모델을 통해 실시간 피드백을 가능하게 한다. 이 연구에서는 실험적으로 측정된 프로브와 합성된 객체를 결합한 비지도 학습 워크플로우를 제안하여 다중 빔라인에서 실험을 재구성할 수 있는 신뢰할 수 있는 모델을 발전시킨다.


<details>
  <summary>Details</summary>
Motivation: 고속의 데이터 수집 속도에서 실시간 피드백이 필요하다.

Method: 비지도 학습 워크플로우를 사용하여 실험적으로 측정된 프로브와 합성된 객체를 결합하여 재구성 모델의 정확성을 높인다.

Result: 합성 워크플로우를 통해 훈련된 모델은 실험 데이터로만 훈련된 모델과 유사한 수준의 재구성 정확도를 달성한다.

Conclusion: 제안된 접근법은 동적 실험 조건에서 실시간 피드백을 제공하는 실험 유도 모델의 훈련을 가능하게 한다.

Abstract: X-ray ptychography is a data-intensive imaging technique expected to become
ubiquitous at next-generation light sources delivering many-fold increases in
coherent flux. The need for real-time feedback under accelerated acquisition
rates motivates surrogate reconstruction models like deep neural networks,
which offer orders-of-magnitude speedup over conventional methods. However,
existing deep learning approaches lack robustness across diverse experimental
conditions. We propose an unsupervised training workflow emphasizing probe
learning by combining experimentally-measured probes with synthetic,
procedurally generated objects. This probe-centric approach enables a single
physics-informed neural network to reconstruct unseen experiments across
multiple beamlines; among the first demonstrations of multi-probe
generalization. We find probe learning is equally important as in-distribution
learning; models trained using this synthetic workflow achieve reconstruction
fidelity comparable to those trained exclusively on experimental data, even
when changing the type of synthetic training object. The proposed approach
enables training of experiment-steering models that provide real-time feedback
under dynamic experimental conditions.

</details>


### [117] [Rethinking Entropy Regularization in Large Reasoning Models](https://arxiv.org/abs/2509.25133)
*Yuxian Jiang,Yafu Li,Guanxu Chen,Dongrui Liu,Yu Cheng,Jing Shao*

Main category: cs.LG

TL;DR: SIREN은 LRM에서의 탐색 문제를 해결하는 방법을 제안하며, 최고의 평균 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: RLVR이 LRM의 추론 능력을 향상시킬 수 있지만 엔트로피 붕괴와 조기 수렴 문제에 직면해 있다.

Method: SIREN은 유의미한 행동과 상태의 서브셋으로 탐색을 제한하는 방법으로, 두 단계의 엔트로피 마스킹 메커니즘을 이용한다.

Result: SIREN은 이전의 엔트로피 관련 RLVR 접근 방식들보다 뛰어난 성능을 보여준다.

Conclusion: SIREN은 LRM에서의 조기 수렴 문제를 효과적으로 완화시킨다.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great promise
in enhancing the reasoning abilities of large reasoning models (LRMs). However,
it suffers from a critical issue: entropy collapse and premature convergence.
Naive entropy regularization, a common approach for encouraging exploration in
the traditional RL literature, fails to address this problem in the context of
LRM. Our analysis reveals that this failure stems from the vast action space
and long trajectories in LRMs, which easily trigger a global entropy explosion
as the model indiscriminately explores all possible actions and states. To
address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method
that confines exploration to a meaningful subset of actions and states. SIREN
achieves this through a two-step entropy masking mechanism, consisting of a
top-p mask and a peak-entropy mask. In addition, regularization is transformed
into a self-anchored form to stabilize training. Across five mathematical
benchmarks, SIREN attains superior average performance over previous
entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on
AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes
greater response diversity and maintains entropy at an appropriate level, which
helps to preserve the validation pass@k throughout training. This effectively
mitigates the premature convergence problem common in RLVR for LRM.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [118] [Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives](https://arxiv.org/abs/2509.23026)
*Yue Wang*

Main category: cs.MA

TL;DR: 이 논문에서는 다중 목표를 가진 다중 에이전트 시스템에서의 복잡성을 해결하기 위해 다목적 마르코프 게임(MOMG) 프레임워크를 도입하고, 기본 솔루션 개념으로 파레토-내쉬 균형(PNE)을 제안한다. PNE의 존재를 증명하고 MOMG의 선형 스칼라화 게임의 내쉬 균형 집합과의 동등성을 확립한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 에이전트들이 다양한 목표를 가짐에 따라 시스템의 복잡성이 증가한다.

Method: 다목적 마르코프 게임(MOMG) 프레임워크를 제안하고, 파레토-내쉬 균형(PNE)을 주요 솔루션 개념으로 정의한다. 또한 더 취급하기 쉬운 대안 솔루션 개념을 연구하고, 탐색과 계획을 분리하는 두 단계의 알고리즘을 제안한다.

Result: PNE의 존재를 증명하고 MOMG의 선형 스칼라화 게임의 내쉬 균형과의 동등성을 확립하며, 선호 프로필에 따라 PNE를 계산할 수 있는 효율적인 방법론을 제공한다.

Conclusion: 제안된 알고리즘은 새로운 샘플을 수집하지 않고도 주어진 선호 프로필에 대해 PNE를 계산할 수 있게 한다.

Abstract: In practical multi-agent systems, agents often have diverse objectives, which
makes the system more complex, as each agent's performance across multiple
criteria depends on the joint actions of all agents, creating intricate
strategic trade-offs. To address this, we introduce the Multi-Objective Markov
Game (MOMG), a framework for multi-agent reinforcement learning with multiple
objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary
solution concept, where no agent can unilaterally improve one objective without
sacrificing performance on another. We prove existence of PNE, and establish an
equivalence between the PNE and the set of Nash Equilibria of MOMG's
corresponding linearly scalarized games, enabling solutions of MOMG by
transferring to a standard single-objective Markov game. However, we note that
computing a PNE is theoretically and computationally challenging, thus we
propose and study weaker but more tractable solution concepts. Building on
these foundations, we develop online learning algorithm that identify a single
solution to MOMGs. Furthermore, we propose a two-phase, preference-free
algorithm that decouples exploration from planning. Our algorithm enables
computation of a PNE for any given preference profile without collecting new
samples, providing an efficient methodological characterization of the entire
Pareto-Nash front.

</details>


### [119] [Situational Awareness for Safe and Robust Multi-Agent Interactions Under Uncertainty](https://arxiv.org/abs/2509.23425)
*Benjamin Alcorn,Eman Hammad*

Main category: cs.MA

TL;DR: 다중 에이전트 시스템에서 에이전트의 의도를 예측하고 자원 제약 하에 목표를 달성하는 문제를 모델링하고 해결하는 방법을 제시함.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서 비협력적 에이전트의 의도를 파악하고 자원 제약 하에 성능 저하 없이 목표를 달성하는 문제를 해결하기 위해서.

Method: 자율 에이전트가 안전한 관찰 반경 내에서 환경을 관찰하고, 주목할 주변 에이전트의 상태를 결정하며, 향후 행동을 예측하고 최적의 행동을 취하는 모델을 개발함. 관찰이 없을 경우, 에이전트는 히스토리 기반의 추정 알고리즘을 사용하여 다른 에이전트의 미래 행동을 예측함. 제안된 알고리즘은 리스크 분석을 통해 불확실성을 관리함.

Result: 제안된 접근 방식은 강화학습 및 게임 이론 알고리즘이라는 두 가지 학습 기반 의사결정 프레임워크를 사용하여 검증됨.

Conclusion: 제안된 모델은 다중 에이전트 시스템의 의도 예측 및 자원 제약 하의 행동 최적화 문제를 효과적으로 해결함을 보여줌.

Abstract: Multi-agent systems are prevalent in a wide range of domains including power
systems, vehicular networks, and robotics. Two important problems to solve in
these types of systems are how the intentions of non-coordinating agents can be
determined to predict future behavior and how the agents can achieve their
objectives under resource constraints without significantly sacrificing
performance. To study this, we develop a model where an autonomous agent
observes the environment within a safety radius of observation, determines the
state of a surrounding agent of interest (within the observation radius),
estimates future actions to be taken, and acts in an optimal way. In the
absence of observations, agents are able to utilize an estimation algorithm to
predict the future actions of other agents based on historical trajectory. The
use of the proposed estimation algorithm introduces uncertainty, which is
managed via risk analysis. The proposed approach in this study is validated
using two different learning-based decision making frameworks: reinforcement
learning and game theoretic algorithms.

</details>


### [120] [PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features](https://arxiv.org/abs/2509.24046)
*Lingyao Li,Haolun Wu,Zhenkun Li,Jiabei Hu,Yu Wang,Xiaoshan Huang,Wenyue Hua,Wenqian Wang*

Main category: cs.MA

TL;DR: PartnerMAS라는 계층적 다중 에이전트 프레임워크를 통해 고차원 의사 결정 과제를 효과적으로 평가하고, 전통적 모델들을 초월하는 성능을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 고차원 의사 결정 과제는 다양한 특성을 가진 후보 풀을 평가해야 하며, 기존의 모델들은 확장성 및 일관성에서 어려움을 겪고 있다.

Method: PartnerMAS는 평가를 전략 설계, 역할별 평가, 결과 통합의 세 가지 계층으로 나누어 수행하는 계층적 다중 에이전트 프레임워크이다.

Result: PartnerMAS는 140개의 사례에서 단일 에이전트 및 토론 기반 다중 에이전트 모델을 초과하는 성과를 내어, 매칭 비율에서 10--15%의 향상을 보였다.

Conclusion: LLM 에이전트 간의 구조화된 협업이 개인 모델의 확장보다 더 강력한 결과를 생성할 수 있음을 보여주며, PartnerMAS가 데이터가 풍부한 도메인에서 고차원 의사 결정을 위한 유망한 프레임워크임을 강조한다.

Abstract: High-dimensional decision-making tasks, such as business partner selection,
involve evaluating large candidate pools with heterogeneous numerical,
categorical, and textual features. While large language models (LLMs) offer
strong in-context reasoning capabilities, single-agent or debate-style systems
often struggle with scalability and consistency in such settings. We propose
PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation
into three layers: a Planner Agent that designs strategies, Specialized Agents
that perform role-specific assessments, and a Supervisor Agent that integrates
their outputs. To support systematic evaluation, we also introduce a curated
benchmark dataset of venture capital co-investments, featuring diverse firm
attributes and ground-truth syndicates. Across 140 cases, PartnerMAS
consistently outperforms single-agent and debate-based multi-agent baselines,
achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows
that planners are most responsive to domain-informed prompts, specialists
produce complementary feature coverage, and supervisors play an important role
in aggregation. Our findings demonstrate that structured collaboration among
LLM agents can generate more robust outcomes than scaling individual models,
highlighting PartnerMAS as a promising framework for high-dimensional
decision-making in data-rich domains.

</details>


### [121] [CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems](https://arxiv.org/abs/2509.24088)
*Yifan Yu,Moyan Li,Shaoyuan Xu,Jinmiao Fu,Xinhai Hou,Fan Lai,Bryan Wang*

Main category: cs.MA

TL;DR: 이 논문은 다중 에이전트 시스템(MAS)이 복잡한 작업에서 오류 인식을 향상시키기 위해 CORRECT라는 경량의 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: MAS에서의 오류 인식은 에이전트 간의 조정 및 도구 사용으로 인해 특히 도전적이다.

Method: CORRECT는 온라인 캐시를 통해 오류 구조를 인식하고 지식을 전이하는 프레임워크이다.

Result: CORRECT는 기존 기술보다 최대 19.8% 향상된 단계별 오류 로컬라이제이션을 보여준다.

Conclusion: 이 연구는 자동화된 오류 인식과 인간 수준의 오류 인식 간의 격차를 크게 좁힌다.

Abstract: Multi-agent systems (MAS) are increasingly capable of tackling complex
real-world tasks, yet their reliance on inter-agent coordination, tool use, and
long-horizon reasoning makes error recognition particularly challenging. Minor
errors can propagate across agents, escalating into task failures while
producing long, intertwined execution trajectories that impose significant
costs for both human developers and automated systems to debug and analyze. Our
key insight is that, despite surface differences in failure trajectories (e.g.,
logs), MAS errors often recur with similar structural patterns. This paper
presents CORRECT, the first lightweight, training-free framework that leverages
an online cache of distilled error schemata to recognize and transfer knowledge
of failure structures across new requests. This cache-based reuse allows LLMs
to perform targeted error localization at inference time, avoiding the need for
expensive retraining while adapting to dynamic MAS deployments in subseconds.
To support rigorous study in this domain, we also introduce CORRECT-Error, a
large-scale dataset of over 2,000 annotated trajectories collected through a
novel error-injection pipeline guided by real-world distributions, and further
validated through human evaluation to ensure alignment with natural failure
patterns. Experiments across seven diverse MAS applications show that CORRECT
improves step-level error localization up to 19.8% over existing advances while
at near-zero overhead, substantially narrowing the gap between automated and
human-level error recognition.

</details>


### [122] [MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems](https://arxiv.org/abs/2509.24323)
*Kun Wang,Guibin Zhang,ManKit Ye,Xinyu Deng,Dongxia Wang,Xiaobin Hu,Jinyang Guo,Yang Liu,Yufei Guo*

Main category: cs.MA

TL;DR: 이 논문은 자동화된 다중 에이전트 시스템을 생성하는 MAS$^2$라는 새로운 패러다임을 도입하여 기존 시스템의 한계를 극복하고 성능을 향상시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 다중 에이전트 시스템이 실제 환경의 역동성과 불확실성에 잘 적응하지 못하는 문제를 해결하고자 함.

Method: 재귀적 자기 생성 원칙에 기반한 MAS$^2$ 다중 에이전트 시스템을 도입하고, 동적으로 목표 에이전트 시스템을 구성하고 수정할 수 있는 '제너레이터-구현자-수정자' 삼중 에이전트 팀을 설계.

Result: MAS$^2$는 복잡한 시나리오에서 최신 기술의 MAS 대비 최대 19.6%의 성능 향상을 달성하며, 새롭게 등장한 LLM을 활용하여 최대 15.1%의 개선 효과를 보여준다.

Conclusion: MAS$^2$는 비용-성과의 균형을 유지하면서도 모범 사례를 초과하는 성능을 제공한다.

Abstract: The past two years have witnessed the meteoric rise of Large Language Model
(LLM)-powered multi-agent systems (MAS), which harness collective intelligence
and exhibit a remarkable trajectory toward self-evolution. This paradigm has
rapidly progressed from manually engineered systems that require bespoke
configuration of prompts, tools, roles, and communication protocols toward
frameworks capable of automated orchestration. Yet, dominant automatic
multi-agent systems, whether generated by external modules or a single LLM
agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}''
paradigm, rendering the resulting systems brittle and ill-prepared for the
dynamism and uncertainty of real-world environments. To transcend this
limitation, we introduce MAS$^2$, a paradigm predicated on the principle of
recursive self-generation: a multi-agent system that autonomously architects
bespoke multi-agent systems for diverse problems. Technically, we devise a
``\textit{generator-implementer-rectifier}'' tri-agent team capable of
dynamically composing and adaptively rectifying a target agent system in
response to real-time task demands. Collaborative Tree Optimization is proposed
to train and specialize these meta-agents. Extensive evaluation across seven
benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$
over state-of-the-art MAS in complex scenarios such as deep research and code
generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization,
effectively leveraging previously unseen LLMs to yield improvements of up to
$15.1\%$. Crucially, these gains are attained without incurring excessive token
costs, as MAS$^2$ consistently resides on the Pareto frontier of
cost-performance trade-offs. The source codes are available at
https://github.com/yeyeyeah2/MAS2.

</details>


### [123] [MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management](https://arxiv.org/abs/2509.25034)
*Heming Fu,Guojun Xiong,Jian Li,Shan Lin*

Main category: cs.MA

TL;DR: 기후 변화에 따라 극단적인 날씨 사건이 증가함에 따라, 물 재해가 커다란 위협이 되고 있으며, 취약한 인구를 보호하고 물의 안전성을 보장하기 위해 적응형 저수지 관리가 중요해지고 있습니다. 이 논문에서는 비정상적인 불확실성을 효과적으로 다루기 위한 분산 저수지 관리 프레임워크인 MARLIN을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기후 변화로 인한 극단적인 기상 이벤트의 증가와 물 재해가 글로벌 커뮤니티에 미치는 위협이 커짐에 따라 적응형 저수지 관리의 필요성이 커지고 있습니다.

Method: MARLIN은 스타링 무르머레이션 지능에 영감을 받은 분산 저수지 관리 프레임워크로, 바이오 영감을 받은 정렬, 분리 및 응집 규칙을 MARL과 결합하여 개별 저수지가 지역 결정을 내리는 동안 전 세계적으로 emergent 조정을 달성할 수 있게 합니다.

Result: 실험 결과에 따르면 MARLIN은 불확실성 처리 능력을 23 향상시키고, 계산량을 35 줄이며, 홍수 대응 시간을 68 단축시켰습니다.

Conclusion: 이러한 결과는 MARLIN의 재해 예방과 커뮤니티 보호를 위한 지능적이고 확장 가능한 물 자원 관리의 잠재력을 보여 줍니다.

Abstract: As climate change intensifies extreme weather events, water disasters pose
growing threats to global communities, making adaptive reservoir management
critical for protecting vulnerable populations and ensuring water security.
Modern water resource management faces unprecedented challenges from cascading
uncertainties propagating through interconnected reservoir networks. These
uncertainties, rooted in physical water transfer losses and environmental
variability, make precise control difficult. For example, sending 10 tons
downstream may yield only 8-12 tons due to evaporation and seepage. Traditional
centralized optimization approaches suffer from exponential computational
complexity and cannot effectively handle such real-world uncertainties, while
existing multi-agent reinforcement learning (MARL) methods fail to achieve
effective coordination under uncertainty. To address these challenges, we
present MARLIN, a decentralized reservoir management framework inspired by
starling murmurations intelligence. Integrating bio-inspired alignment,
separation, and cohesion rules with MARL, MARLIN enables individual reservoirs
to make local decisions while achieving emergent global coordination. In
addition, a LLM provides real-time reward shaping signals, guiding agents to
adapt to environmental changes and human-defined preferences. Experiments on
real-world USGS data show that MARLIN improves uncertainty handling by 23\%,
cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting
super-linear coordination, with complexity scaling 5.4x from 400 to 10,000
nodes. These results demonstrate MARLIN's potential for disaster prevention and
protecting communities through intelligent, scalable water resource management.

</details>
