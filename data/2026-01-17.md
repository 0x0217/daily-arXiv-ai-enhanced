<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 8]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Adaptive Orchestration: Scalable Self-Evolving Multi-Agent Systems](https://arxiv.org/abs/2601.09742)
*Sathish Sampath,Anuradha Baskaran*

Main category: cs.MA

TL;DR: 신뢰성 있는 대형 언어 모델을 위한 새로운 아키텍처인 자기 진화형 상담 시스템을 소개하며, 동적 전문가 혼합 방식을 활용하여 안정성을 유지한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 자율 에이전트로 배치됨에 따라, 일반화-전문화의 딜레마라는 중요한 확장성 병목 현상에 직면하고 있다.

Method: 동적 전문가 혼합(DMoE) 접근 방식을 이용하여 시스템이 실시간 대화 분석에 기반하여 전문 하위 에이전트를 '고용'하도록 런타임 환경을 동적으로 재구성한다.

Result: 실험 결과는 이 아키텍처가 정적 에이전트 군집에 비해 높은 작업 성공률을 유지하며 토큰 소비를 최소화함을 보여준다.

Conclusion: 자기 진화형 상담 시스템은 에이전트의 효율성을 높이고 자원 제약을 줄이며 거부 편향을 완화하는 혁신적인 메커니즘을 제공한다.

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous agents, they face a critical scalability bottleneck known as the "Generalization-Specialization Dilemma." Monolithic agents equipped with extensive toolkits suffer from context pollution and attention decay, leading to hallucinations. Conversely, static multi-agent swarms introduce significant latency and resource overhead. This paper introduces a Self-Evolving Concierge System, a novel architecture utilizing a Dynamic Mixture of Experts (DMoE) approach. Unlike recent self-improving agents that rewrite their own codebase, our system preserves stability by dynamically restructuring its runtime environment: "hiring" specialized sub-agents based on real-time conversation analysis. We introduce an asynchronous "Meta-Cognition Engine" that detects capability gaps, a Least Recently Used (LRU) eviction policy for resource constraints, and a novel "Surgical History Pruning" mechanism to mitigate refusal bias. Experimental results demonstrate that this architecture maintains high task success rates while minimizing token consumption compared to static agent swarms.

</details>


### [2] [Multi-Agent Cooperative Learning for Robust Vision-Language Alignment under OOD Concepts](https://arxiv.org/abs/2601.09746)
*Philip Xu,Isabel Wagner,Eerke Boiten*

Main category: cs.MA

TL;DR: 이 논문은 시각-언어 모델에서 분포 외 개념을 처리할 때 발생하는 교차 모달 정렬 붕괴 문제를 해결하기 위한 새로운 다중 에이전트 협력 학습(MACL) 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 시각-언어 모델이 분포 외 개념을 처리할 때 발생하는 교차 모달 정렬 붕괴 문제를 해결하기 위해.

Method: 이미지, 텍스트, 이름, 조정 에이전트를 포함한 네 개의 핵심 에이전트가 구조화된 메시지 전달을 통해 모달리티 불균형을 완화하는 협력적 학습 프레임워크를 제안한다.

Result: VISTA-Beyond 데이터셋에 대한 실험을 통해 MACL이 몇 가지 샷 및 제로 샷 설정 모두에서 성능을 크게 향상시켜 1-5%의 정밀도 향상을 달성한다.

Conclusion: 제안된 프레임워크는 다중 에이전트 특징 공간 이름 학습 및 적응형 동적 균형 메커니즘을 통해 에이전트 간 기여를 규제한다.

Abstract: This paper introduces a novel Multi-Agent Cooperative Learning (MACL) framework to address cross-modal alignment collapse in vision-language models when handling out-of-distribution (OOD) concepts. Four core agents, including image, text, name, and coordination agents, collaboratively mitigate modality imbalance through structured message passing. The proposed framework enables multi-agent feature space name learning, incorporates a context exchange enhanced few-shot learning algorithm, and adopts an adaptive dynamic balancing mechanism to regulate inter-agent contributions. Experiments on the VISTA-Beyond dataset demonstrate that MACL significantly improves performance in both few-shot and zero-shot settings, achieving 1-5% precision gains across diverse visual domains.

</details>


### [3] [When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making](https://arxiv.org/abs/2601.10102)
*Viswonathan Manoranjan,Snehalkumar `Neil' S. Gaikwad*

Main category: cs.MA

TL;DR: 대규모 언어 모델이 다중 에이전트 시스템에서 전략적 과제로 배치되는 경향이 있지만, 역할 기반 캐릭터와 보상 가시성과 같은 설계 선택이 추론에 미치는 영향을 잘 이해하지 못하고 있다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템이 보상 최적화를 위한 전략적 추론자로 기능하는지 또는 명시적 인센티브보다 역할 정렬을 우선시하는 정체성 기반 행위자인지를 조사하고자 한다.

Method: Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B의 네 가지 LLM 아키텍처를 사용하여, 네 명의 에이전트가 포함된 복잡한 환경 결정 게임에서 체계적인 실험을 수행한다.

Result: 역할 정체성 편향이 존재할 경우에도 보상 최적 균형이 존재하고 완전한 보상 정보가 제공될 때 전략적 추론을 근본적으로 변경한다는 것을 보여준다.

Conclusion: 대표적 선택이 다중 에이전트 시스템이 전략적으로 추론하는지 또는 정체성 기반 행위자로 작용하는지를 결정하는 중요한 거버넌스 결정이라는 중요한 의미를 가진다.

Abstract: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.

</details>


### [4] [TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems](https://arxiv.org/abs/2601.10120)
*Rui Sun,Jie Ding,Chenghua Gong,Tianjun Gu,Yihang Jiang,Juyuan Zhang,Liming Pan,Linyuan Lü*

Main category: cs.MA

TL;DR: TopoDIM 프레임워크는 LLM 기반 다중 에이전트 시스템에서 통신 토폴로지를 최적화하여 집단 지능을 개선하는 데 도움을 줍니다.


<details>
  <summary>Details</summary>
Motivation: 최근의 통찰력에 따라 평가 및 논쟁 메커니즘이 다중 에이전트 시스템의 문제 해결을 향상시킬 수 있다는 점에 착안하였습니다.

Method: TopoDIM은 다양한 상호작용 모드와 함께 일회성 토폴로지 생성을 위한 프레임워크로, 비대칭적 실행을 통해 적응성과 프라이버시를 강화합니다.

Result: TopoDIM은 최신 방법과 비교하여 총 토큰 소비를 46.41% 줄이고 평균 성능을 1.50% 향상시킵니다.

Conclusion: 이 프레임워크는 이질적인 에이전트 간의 통신을 조직하는 데 강력한 적응력을 보여줍니다.

Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/

</details>


### [5] [Fairness Driven Multi-Agent Path Finding Problem](https://arxiv.org/abs/2601.10123)
*Aditi Anand,Dildar Ali,Suman Banerjee*

Main category: cs.MA

TL;DR: 이 연구에서는 여러 에이전트의 비충돌 경로를 찾는 다중 에이전트 경로 찾기 문제를 다루며, 공정성 맥락에서의 두 가지 변형을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 경로 찾기 문제는 로봇 동작 계획 및 무인 항공기 이동을 위한 공역 할당 등 다양한 실제 상황에서 발생한다.

Method: 비합리적인 에이전트의 경우, 문제에 대한 휴리스틱 솔루션을 제안하며, 합리적인 에이전트의 경우에는 지배 전략, 인센티브 호환성 및 개별 합리성을 보장하는 메커니즘을 개발한다.

Result: 여러 솔루션 방법론을 사용하여 제안된 솔루션 접근 방식의 효과성과 효율성을 입증한다.

Conclusion: 이 연구는 다중 에이전트 경로 찾기 문제에서 공정성을 고려한 해결책을 제시한다.

Abstract: The Multi-Agent Path Finding (MAPF) problem aims at finding non-conflicting paths for multiple agents from their respective sources to destinations. This problem arises in multiple real-life situations, including robot motion planning and airspace assignment for unmanned aerial vehicle movement. The problem is computationally expensive, and adding to it, the agents are rational and can misreport their private information. In this paper, we study both variants of the problem under the realm of fairness. For the non-rational agents, we propose a heuristic solution for this problem. Considering the agents are rational, we develop a mechanism and demonstrate that it is a dominant strategy, incentive compatible, and individually rational. We employ various solution methodologies to highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


### [6] [Multipath Routing for Multi-Hop UAV Networks](https://arxiv.org/abs/2601.10299)
*Zhenyu Zhao,Tiankui Zhang,Xiaoxia Xu,Junjie Li,Yuanwei Liu,Wenjuan Xing*

Main category: cs.MA

TL;DR: 본 논문에서는 다중 경로 라우팅 방법을 제안하여 다중 홉 UAV 네트워크의 트래픽을 효율적으로 관리하고 지연 시간을 줄이는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 홉 UAV 네트워크는 국소적인 혼잡을 초래하고 교통 지연을 증가시키는 단일 경로 라우팅 방식을 사용한다.

Method: 각 UAV가 여러 다음 홉 이웃과의 트래픽 흐름을 동적으로 분할하고 전달할 수 있도록 하는 트래픽 적응형 다중 경로 라우팅 방법이 제안된다. 이를 위해 Dec-POMDP 모델링을 사용하고, IPPO-DM이라는 새로운 다중 에이전트 심층 강화 학습 알고리즘을 개발한다.

Result: 시뮬레이션 결과 IPPO-DM이 전송 지연 보장 및 패킷 손실 성능 측면에서 벤치마크 방식보다 우수함을 보여준다.

Conclusion: 제안된 방법은 다중 홉 UAV 네트워크에서 다양한 트래픽 흐름의 지연 요구 사항을 충족하는 데 효과적이다.

Abstract: Multi-hop uncrewed aerial vehicle (UAV) networks are promising to extend the terrestrial network coverage. Existing multi-hop UAV networks employ a single routing path by selecting the next-hop forwarding node in a hop-by-hop manner, which leads to local congestion and increases traffic delays. In this paper, a novel traffic-adaptive multipath routing method is proposed for multi-hop UAV networks, which enables each UAV to dynamically split and forward traffic flows across multiple next-hop neighbors, thus meeting latency requirements of diverse traffic flows in dynamic mobile environments. An on-time packet delivery ratio maximization problem is formulated to determine the traffic splitting ratios at each hop. This sequential decision-making problem is modeled as a decentralized partially observable Markov decision process (Dec-POMDP). To solve this Dec-POMDP, a novel multi-agent deep reinforcement leaning (MADRL) algorithm, termed Independent Proximal Policy Optimization with Dirichlet Modeling (IPPO-DM), is developed. Specifically, the IPPO serves as the core optimization framework, where the Dirichlet distribution is leveraged to parameterize a continuous stochastic policy network on the probability simplex, inherently ensuring feasible traffic splitting ratios. Simulation results demonstrate that IPPO-DM outperforms benchmark schemes in terms of both delivery latency guarantee and packet loss performance.

</details>


### [7] [Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems](https://arxiv.org/abs/2601.10560)
*Xi Shi,Mengxin Zheng,Qian Lou*

Main category: cs.MA

TL;DR: 본 논문은 지연 인식을 통해 병렬 실행을 최적화하는 멀티 에이전트 시스템 orchestrating 기법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 시스템은 복잡한 추론을 가능하게 하지만, 다단계 실행과 반복적인 모델 호출로 인해 높은 추론 지연이 발생하여 시간 민감한 상황에서의 확장성과 사용성을 제한한다.

Method: 지연 감독을 포함한 병렬 실행 환경에서 학습 기반의 멀티 에이전트 시스템의 오케스트레이션을 탐구하고, 병렬 실행에서 지연을 명시적으로 최적화하는 Latency-Aware Multi-agent System (LAMaS) 프레임워크를 제안한다.

Result: 우리의 접근법은 여러 기준선과 비교했을 때 단위 기준 경로의 길이를 38-46% 줄이며, 작업 성능을 유지하거나 향상시킨다.

Conclusion: 효율적인 멀티 에이전트 시스템 설계에서 병렬 실행 하의 지연을 명시적으로 최적화하는 것이 중요함을 강조한다.

Abstract: Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS

</details>


### [8] [Procedural Fairness in Multi-Agent Bandits](https://arxiv.org/abs/2601.10600)
*Joshua Caiata,Carter Blair,Kate Larson*

Main category: cs.MA

TL;DR: 이 논문은 다중 에이전트 다중 무기(Bandit) 환경에서의 공정성을 제안하는 새로운 목표인 절차적 공정을 도입한다.


<details>
  <summary>Details</summary>
Motivation: MA-MAB 환경에서 공정성은 종종 결과에 국한되며, 하지만 심리학, 경제학, 롤스 주의론에 따르면 공정성은 과정과 의사결정에 참여하는 사람들도 중요하다.

Method: 모든 에이전트에게 동등한 의사결정 권한을 제공하며 결과의 비례성을 보장하는 절차적 공정성을 제안한다.

Result: 효과적인 공정성 개념은 결과를 최적화하는 것이 아니라 동등한 목소리와 대표성을 희생하게 되고, 절차적으로 공정한 정책 하에서는 결과 기반 공정성 목표의 희생이 최소화된다는 것을 확인했다.

Conclusion: 절차적 정당성이 공정성 목표로서 더 큰 집중을 받아야 하며, 이를 실천할 수 있는 체계를 제시한다.

Abstract: In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [QFed: Parameter-Compact Quantum-Classical Federated Learning](https://arxiv.org/abs/2601.09809)
*Samar Abdelghani,Soumaya Cherkaoui*

Main category: cs.LG

TL;DR: 양자 보조 연합 학습은 엣지 장치 네트워크의 계산 효율성을 높이기 위해 제안된 QFed 프레임워크를 통해 VGG 유사 모델의 파라미터 수를 77.6% 감소시킬 수 있다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 조직과 기업들은 분산된 데이터 셋에서 집단 지능을 추출해야 하며, 이는 개인정보 보호 및 규제 요구사항을 준수해야 한다.

Method: 양자 보조 연합 학습의 가능성을 검토하고, QFed라는 양자 기반의 연합 학습 프레임워크를 소개한다.

Result: 제안된 QFed 프레임워크는 FashionMNIST 데이터셋을 사용해 VGG 유사 모델의 파라미터 수를 77.6% 줄이는 실험 결과를 보였다.

Conclusion: 양자 컴퓨팅을 바탕으로 연합 학습의 능력을 강화할 수 있는 가능성을 제시한다.

Abstract: Organizations and enterprises across domains such as healthcare, finance, and scientific research are increasingly required to extract collective intelligence from distributed, siloed datasets while adhering to strict privacy, regulatory, and sovereignty requirements. Federated Learning (FL) enables collaborative model building without sharing sensitive raw data, but faces growing challenges posed by statistical heterogeneity, system diversity, and the computational burden from complex models. This study examines the potential of quantum-assisted federated learning, which could cut the number of parameters in classical models by polylogarithmic factors and thus lessen training overhead. Accordingly, we introduce QFed, a quantum-enabled federated learning framework aimed at boosting computational efficiency across edge device networks. We evaluate the proposed framework using the widely adopted FashionMNIST dataset. Experimental results show that QFed achieves a 77.6% reduction in the parameter count of a VGG-like model while maintaining an accuracy comparable to classical approaches in a scalable environment. These results point to the potential of leveraging quantum computing within a federated learning context to strengthen FL capabilities of edge devices.

</details>


### [10] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 본 연구는 자원 제약이 있는 엣지 디바이스에서 큰 언어 모델(LLMs)의 사용을 최적화하기 위한 통합 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자원 제약이 있는 엣지 디바이스에서 LLM의 배포 문제를 해결할 필요성이 있습니다.

Method: GPTQ 기반 양자화, 저순위 적응(LoRA), 특화된 데이터 증류 프로세스를 결합한 통합 프레임워크를 제안합니다.

Result: 최대 2배의 메모리 압축을 실현하고, 전문화된 작업에 대한 효율적인 추론을 가능하게 합니다.

Conclusion: 제안된 방법은 표준 LLM 벤치마크에서 우수한 성능을 보이며, Muon 최적화기가 양자화 중 모델의 정확도 감소 저항성을 향상시킵니다.

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [11] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer는 사용자 필요를 보다 정확하게 반영하기 위해 두 가지 에이전트 구조를 도입하여 언어 기반 보조 기술의 한계를 극복한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 언어 기반 보조 기술은 사용자가 명시적으로 필요를 말해야만 기능하므로, 사용자들이 표현하지 않은 필요가 종종 충족되지 않는다.

Method: ProPer는 차원 생성 에이전트(DGA)와 응답 생성 에이전트(RGA)로 구성된 두 가지 에이전트를 사용하는 새로운 아키텍처를 소개한다. DGA는 사용자의 명시적 데이터를 활용하여 여러 개의 암묵적 차원이나 지식 격차를 생성한다. RGA는 이를 바탕으로 응답을 맞춤화한다.

Result: ProPer는 여러 도메인에서 평가되었으며, 결과는 품질 점수와 승률이 모든 도메인에서 향상되었음을 보여준다.

Conclusion: ProPer는 단일 턴 평가에서 최대 84%의 성과 향상과 다중 턴 상호작용에서 일관된 우위를 달성하였다.

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [12] [PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning](https://arxiv.org/abs/2601.10012)
*Yanhang Shi,Xiaoyu Wang,Houwei Cao,Jian Li,Yong Liu*

Main category: cs.LG

TL;DR: 멀티모달 분산 자유학습(DFL)에서의 문제를 해결하는 새로운 프레임워크인 PARSE를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 멀티모달 DFL에서는 에이전트들이 가용한 모달리티와 모델 아키텍처가 다르므로, 중앙 조정자 없이 P2P 네트워크를 통해 협업해야 한다는 점에서 도전적이다.

Method: PARSE는 서버 없는 환경에서 부분 정보 분해(PID)를 운영하는 멀티모달 DFL 프레임워크이다. 각 에이전트는 기능 분할을 수행하여 자신의 잠재적 표현을 중복적, 고유한, 그리고 상호작용적인 슬라이스로 분해한다.

Result: PARSE는 슬라이스 수준 부분 정렬을 통해 이질적 에이전트 간의 P2P 지식 공유를 가능하게 하며, 태스크-, 모달리티-, 하이브리드-공유 DFL 기준선을 초과하는 일관된 성과를 낸다.

Conclusion: PARSE는 중앙 조정과 경량화 과정을 제거함으로써 단일/다중 모달 그래디언트 충돌을 해결하고, 멀티모달 DFL 딜레마를 극복하면서도 표준 DFL 제약과의 호환성을 유지한다.

Abstract: Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.

</details>


### [13] [Adaptive Label Error Detection: A Bayesian Approach to Mislabeled Data Detection](https://arxiv.org/abs/2601.10084)
*Zan Chaudhry,Noam H. Rotenberg,Brian Caffo,Craig K. Jones,Haris I. Sair*

Main category: cs.LG

TL;DR: ALED는 의료 이미징 데이터셋에서 잘못 레이블이 붙은 샘플을 검출하는 혁신적인 방법으로, 정확도를 유지하면서 감도 향상에 성공했다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습이 보편화됨에 따라 잘못된 레이블을 식별하고 수정하는 것이 중요하다.

Method: ALED는 심층 합성곱 신경망에서 중간 피쳐 공간을 추출하고, 특징을 노이즈 제거한 후, 각 클래스의 축소된 다양체를 다차원 가우스 분포로 모델링하고, 우도 비율 테스트를 수행하여 잘못 레이블된 샘플을 식별한다.

Result: ALED는 여러 의료 이미징 데이터셋에서 기존 레이블 오류 탐지 방법에 비해 감도가 크게 증가했지만 정밀도는 유지했다.

Conclusion: 수정된 데이터에서 신경망을 미세 조정하면 테스트 세트 오류가 33.8% 감소하여 최종 사용자에게 큰 이점을 제공한다.

Abstract: Machine learning classification systems are susceptible to poor performance when trained with incorrect ground truth labels, even when data is well-curated by expert annotators. As machine learning becomes more widespread, it is increasingly imperative to identify and correct mislabeling to develop more powerful models. In this work, we motivate and describe Adaptive Label Error Detection (ALED), a novel method of detecting mislabeling. ALED extracts an intermediate feature space from a deep convolutional neural network, denoises the features, models the reduced manifold of each class with a multidimensional Gaussian distribution, and performs a simple likelihood ratio test to identify mislabeled samples. We show that ALED has markedly increased sensitivity, without compromising precision, compared to established label error detection methods, on multiple medical imaging datasets. We demonstrate an example where fine-tuning a neural network on corrected data results in a 33.8% decrease in test set errors, providing strong benefits to end users. The ALED detector is deployed in the Python package statlab.

</details>


### [14] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 이 논문은 Tree-Query라는 트리 구조의 다중 전문가 LLM 프레임워크를 도입하여 비데이터 기반의 인과 추론을 단순화한다.


<details>
  <summary>Details</summary>
Motivation: 인과 발견의 필요성을 해결하기 위해 오랜 시간 동안 연구가 진행되어 왔지만, 기존 방법론의 한계가 존재한다.

Method: Tree-Query는 쌍(pair) 인과 발견을 백도어 경로, (비)독립성, 잠재적 혼란 요인, 인과 방향에 대한 짧은 쿼리의 시퀀스로 줄여주는 다중 전문가 LLM 프레임워크이다.

Result: Tree-Query는 Mooij et al. 및 UCI 인과 그래프에서 파생된 비데이터 기준에서 구조적 지표를 개선하고, 사례 연구를 통해 혼란 요인 스크리닝 및 안정적이고 높은 신뢰도의 인과 결론을 도출한다.

Conclusion: Tree-Query는 데이터 기반 인과 발견을 보완할 수 있는 데이터 없는 인과 선행 모델을 획득하는 원칙적인 방법을 제공한다.

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [15] [Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand](https://arxiv.org/abs/2601.10181)
*Kiattikun Chobtham*

Main category: cs.LG

TL;DR: 본 논문은 태국 특정 지역의 강수 예측 정확도를 높이기 위한 새로운 북동 몬순 기후 지수를 도입하고, 이 지수를 최적화하기 위해 Deep Q-Network 강화 학습 에이전트를 활용하여 강수 패턴을 효과적으로 예측하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 지구 시스템 내 복잡한 시공간 패턴으로 인해 기후 예측이 어렵고, 태국의 특정 지역에서 예측 정확도를 높이기 위한 로컬 스케일 지수의 필요성이 존재한다.

Method: 해수면 온도를 기준으로 계산된 북동 몬순 기후 지수를 도입하고, Deep Q-Network 강화 학습 에이전트를 활용해 계절 강수와의 상관관계 기반으로 가장 효과적인 구역을 선택한다.

Result: 강수 기상 관측소를 12개의 클러스터로 분류하고, 최적화된 기후 지수를 장기 단기 기억 모델에 통합하여 대부분의 클러스터 지역에서 월별 장기 강수 예측 능력을 크게 향상시킨다.

Conclusion: 이 접근법은 12개월 앞 예측의 평균 제곱근 오차를 효과적으로 줄인다.

Abstract: Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.

</details>


### [16] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: Strategy-aware Surprise(SuS)는 강화학습에서 탐색을 위한 신선함 신호로서 예측 불일치를 사용하는 새로운 내부 동기 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 호기심 기반 방법들은 상태 예측 오류에만 의존하지만, SuS는 행동 전략의 일관성과 예상치 못한 결과를 모두 고려합니다.

Method: SuS는 행동 전략의 일관성을 측정하는 전략 안정성(SS)과 현재 전략 표현에 대한 예기치 않은 결과를 포착하는 전략 서프라이즈(SuS)라는 두 가지 상보적 요소를 도입합니다.

Result: 수학적 추론 작업에서 SuS를 평가한 결과, 정확성과 솔루션 다양성이 모두 유의미하게 개선되었습니다. 각 요소를 제거하면 최소 10%의 성능 저하가 확인되었습니다.

Conclusion: SuS는 기준 방법에 비해 Pass@1에서 17.4%, Pass@5에서 26.4%의 향상을 달성하며 교육 전반에 걸쳐 더 높은 전략 다양성을 유지합니다.

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [17] [CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning](https://arxiv.org/abs/2601.10407)
*Yuanjie Zhao,Junnan Qiu,Yue Ding,Jie Li*

Main category: cs.LG

TL;DR: 본 논문은 오프라인 강화 학습에서 고안된 CS-GBA(중요 샘플 기반의 그래디언트 유도 백도어 공격) 프레임워크를 제안하여 안전 제약 알고리즘에 대한 강력한 공격을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 오프라인 강화 학습에서의 정책 최적화가 정적 데이터 세트로부터 가능하지만, 백도어 공격에 취약하다는 점을 해결하고자 한다.

Method: CS-GBA는 높은 은폐성과 파괴력을 달성하도록 고안된 새로운 프레임워크이며, 중요한 샘플 선택 전략과 상관관계를 깨는 트리거 메커니즘을 활용하여 효율적인 공격을 가능하게 한다.

Result: 실험 결과, 제안된 방법이 최신 기준을 능가하며, 안전 제약 알고리즘에 대해 최소 5%의 오염 예산으로 높은 공격 성공률을 기록하였다.

Conclusion: CS-GBA는 오프라인 강화 학습에 있어 백도어 공격의 새로운 통찰을 제공하며, 청정 환경에서도 에이전트의 성능을 유지할 수 있다.

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network's gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent's performance in clean environments.

</details>


### [18] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 이 논문은 다단계 미래 정보가 포함된 표형 강화 학습 문제를 연구하며, 적응형 배치 정책(ABP)을 제안하여 최적 정책을 찾는 방법을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 다단계 전망 정보를 활용하여 최적의 정책을 학습하는 것이 강화 학습 문제 해결에 미치는 영향을 조사합니다.

Method: 적응형 배치 정책(ABP)을 도입하고 이에 대한 최적 벨만 방정식을 도출하며, 학습 알고리즘을 설계합니다.

Result: ABP를 통해 최적 정책 학습이 가능함을 보이며, 우리의 후회 경계가 전망 수평선에 대해 순서 최적임을 증명합니다.

Conclusion: 이 접근 방법이 기존의 고정 배치 정책보다 우수함을 입증합니다.

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [19] [Communication-Efficient Federated Learning by Exploiting Spatio-Temporal Correlations of Gradients](https://arxiv.org/abs/2601.10491)
*Shenlong Zheng,Zhen Zhang,Yuhui Deng,Geyong Min,Lin Cui*

Main category: cs.LG

TL;DR: GradESTC는 공간적 및 시간적 기울기 상관관계를 활용하여 통신 오버헤드를 감소시키는 새로운 압축 기법이다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 통신 오버헤드는 특히 대역폭이 제한된 네트워크에서 중요한 도전 과제이다.

Method: GradESTC는 공간적 상관관계를 활용하여 전체 기울기를 컴팩트한 기저 벡터 집합과 해당 조합 계수로 분해하며, 시간적 상관관계를 활용하여 매 라운드마다 소수의 기저 벡터만 동적으로 업데이트한다.

Result: GradESTC는 전체 기울기 대신 경량 조합 계수와 업데이트된 기저 벡터의 제한된 수만 전송함으로써 통신 오버헤드를 크게 줄인다. 실험 결과, GradESTC는 목표 정확도에 도달할 때 평균 39.79%의 업링크 통신을 감소시켰다.

Conclusion: GradESTC는 공간 및 시간적 기울기 구조를 효과적으로 활용하여 통신 효율적인 연합 학습을 위한 실용적이고 확장 가능한 솔루션을 제공한다.

Abstract: Communication overhead is a critical challenge in federated learning, particularly in bandwidth-constrained networks. Although many methods have been proposed to reduce communication overhead, most focus solely on compressing individual gradients, overlooking the temporal correlations among them. Prior studies have shown that gradients exhibit spatial correlations, typically reflected in low-rank structures. Through empirical analysis, we further observe a strong temporal correlation between client gradients across adjacent rounds. Based on these observations, we propose GradESTC, a compression technique that exploits both spatial and temporal gradient correlations. GradESTC exploits spatial correlations to decompose each full gradient into a compact set of basis vectors and corresponding combination coefficients. By exploiting temporal correlations, only a small portion of the basis vectors need to be dynamically updated in each round. GradESTC significantly reduces communication overhead by transmitting lightweight combination coefficients and a limited number of updated basis vectors instead of the full gradients. Extensive experiments show that, upon reaching a target accuracy level near convergence, GradESTC reduces uplink communication by an average of 39.79% compared to the strongest baseline, while maintaining comparable convergence speed and final accuracy to uncompressed FedAvg. By effectively leveraging spatio-temporal gradient structures, GradESTC offers a practical and scalable solution for communication-efficient federated learning.

</details>


### [20] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: PG-CBM은 도메인 정의 인과 메커니즘을 따르도록 학습을 제한하여 CBMs의 개선된 설명 가능성을 제공하며, 과학적 응용에서 더 신뢰할 수 있는 AI 시스템을 향해 나아가는 한 걸음을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 CBMs는 도메인 특정 관계와 인과 메커니즘을 간과하고, 완전한 개념 레이블에 의존하여 과학적 도메인에서의 적용 가능성을 제한합니다.

Method: PG-CBM은 생물물리학적으로 의미 있는 중간 개념을 통해 학습을 도메인 정의 인과 메커니즘에 따라 제약하는 CBM의 확장 모델입니다.

Result: PG-CBM은 지표에서의 생물량 밀도 추정 사례 연구를 통해 여러 벤치마크에 비해 오류와 편향을 감소시킵니다.

Conclusion: PG-CBM은 정확도 향상뿐만 아니라 투명성을 높이고, 허위 학습 탐지를 가능하게 하며, 과학적 통찰을 제공합니다.

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [21] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 이 논문은 반복 매개변수 혼합(IPM 스타일 평균화)을 통해 훈련된 반비동기 클라이언트-서버 퍼셉트론을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 연합 및 분산 배포에 있어 시스템의 세 가지 효과를 포착하고자 한다.

Method: 서버 측 집계 규칙인 스테일니스-버킷 집계를 도입하여 업데이트 연령에 대한 정해진 스테일니스 프로파일을 강제한다.

Result: 우리는 특정 서버 라운드 수에 대한 누적 가중 퍼셉트론 실수의 유한한 기대 경계를 증명한다.

Conclusion: 노이즈가 없는 경우, 유한한 기대 실수 예산이 유한한 라운드 안정화 경계를 생성하는 방법을 보여준다.

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes는 GUI 작업을 위한 능동적인 시각적 인식을 지원하는 강화 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 시각-언어 모델(VLM)과 강화 학습(RL)의 최근 발전은 GUI 자동화의 진전을 이끌어왔다. 그러나 기존 방법들은 정적이고 일회성의 시각 입력에 의존하고 수동적 인식을 특징으로 하여 인터페이스를 관찰할 시기와 방법을 적응적으로 결정할 능력이 부족하다.

Method: GUI-Eyes는 정보가 풍부한 관찰을 획득하기 위해 시각 도구(예: 크롭 또는 확대)를 호출하는 방법과 시기를 전략적으로 결정하는 방법을 학습하는 두 단계의 추론 과정에서 작동한다. 이를 지원하기 위해, 우리는 의사 결정을 거칠게 탐색과 세분화된 기초화로 분해하는 점진적 인식 전략을 도입하며, 이는 두 수준의 정책에 의해 조정된다.

Result: ScreenSpot-Pro 벤치마크에서 GUI-Eyes-3B는 3,000개의 레이블이 있는 샘플만 사용하여 44.8%의 기초화 정확도를 달성하였으며, 이는 감독 및 RL 기반 기준보다 현저히 뛰어난 성과를 나타낸다.

Conclusion: 도구 인식이 가능한 능동적 인식은 단계별 정책 추론과 세분화된 보상 피드백이 가능하게 하여 강력하고 데이터 효율적인 GUI 에이전트를 구축하는 데 중요하다.

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [23] [PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation](https://arxiv.org/abs/2601.09771)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.AI

TL;DR: PCN-Rec는 사용자 요구와 정책 제약을 모두 충족하는 추천 시스템으로, 자연어 추론과 결정론적 집행을 분리하여 효율성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 모던 LLM 기반 추천 시스템이 긴 꼬리 노출이나 다양성 요구사항과 같은 거버넌스 제약을 신뢰성 있게 충족하지 못하는 문제 해결이 필요하다.

Method: PCN-Rec는 두 개의 에이전트인 사용자 옹호자와 정책 에이전트를 통해 후보 창을 협상하고, 중재 LLM이 요구 사항 충족을 설명하는 구조적 증명서를 생성하여 상위 N 슬레이트를 합성한다.

Result: PCN-Rec는 MovieLens-100K에서 98.55%의 통과율을 달성하며, 단일 LLM 기준 실험에 비해 유효성 기준 충족을 유지한다.

Conclusion: PCN-Rec는 추천 시스템의 거버넌스 제약을 충족시키면서도 유용성을 유지할 수 있는 효과적인 방법이다.

Abstract: Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).

</details>


### [24] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 비상호작용 끝-끝 프레임워크를 통해 모델 내부에서 논리적 추론을 활성화하고, Attention-Aware Intervention (AAI) 방법을 제안하여 효율적인 추론을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 현대의 LLM을 활용한 논리적 추론은 복잡한 상호작용 프레임워크에 의존하여 추론 과정을 하위 작업으로 분해하고, 이를 통해 신뢰성 있는 논리 구조를 활용하게 해준다.

Method: 비상호작용 끝-끝 프레임워크를 제안하며, 이 프레임워크에 구조적 정보를 도입하여 주목 머리의 일부를 활성화하는 방법을 소개한다. AAI를 통해 선택된 머리의 주목 점수를 재조정하는 시간 중재 방법을 제안한다.

Result: AAI는 다양한 벤치마크와 모델 아키텍처에 걸쳐 논리적 추론 성능을 향상시킴을 보여준다.

Conclusion: AAI는 주목 조정을 통해 모델의 추론 방향을 효율적으로 조작할 수 있는 방법을 제시하며, 추가적인 계산 비용은 거의 발생하지 않는다.

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [25] [A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents](https://arxiv.org/abs/2601.09869)
*Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini*

Main category: cs.AI

TL;DR: 비인간 대상에 인간과 유사한 특성이 부여되는 인격화 현상이 LLM 기반 대화형 에이전트의 발전과 함께 두드러지고 있다. 본 논문은 이러한 인격화의 윤리적 측면을 분석하고 LLM 기반 대화형 에이전트의 윤리적 배치를 위한 연구 계획과 설계/관리에 대한 권고안을 제시한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 대화형 에이전트의 인격화가 윤리적 우려와 기회를 동시에 제기하기 때문이다.

Method: 다섯 개 데이터베이스와 세 개의 프리프린트 저장소를 통한 윤리 중심의 LLM 기반 대화형 에이전트 인격화 관련 연구를 스코핑 리뷰 방식으로 맵핑하였다.

Result: 인격화에 대한 정의는 일치했으나, 운영화 및 실질적 연구는 다양하게 나타났으며, 주로 위험 중심의 규범적 틀로 구성되었다.

Conclusion: 인격적 단서를 윤리적으로 배치하기 위한 연구 계획과 설계/관리 권고안을 제시하였다.

Abstract: Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.

</details>


### [26] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 기존의 LLM 기반 다중 에이전트 시스템은 사전 정의된 워크플로우에 의존하여 작업 상태를 미리 나열하고 라우팅 규칙을 설정하는 방식이다. 그러나 이러한 방식은 예측과 인코딩에 많은 수작업이 필요하며, 복잡한 실제 작업의 상태 공간을 포괄적으로 커버할 수 없다. 이 문제를 해결하기 위해 이 논문에서는 자연어를 사용하여 에이전트 간(A2A) 통신을 통해 작업 진행을 지속적으로 모니터링하고 다른 에이전트를 동적으로 조정하는 정보 흐름 조정 다중 에이전트 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 워크플로우 기반 설계가 요구하는 광범위한 수작업과 복잡한 작업의 상태 공간을 포괄적으로 커버할 수 없는 문제를 해결하고자 한다.

Method: 정보 흐름 조정 다중 에이전트 패러다임을 에이전트 간(A2A) 통신을 통해 구현하며, 사전 정의된 워크플로우에 의존하지 않는다.

Result: pass@1 설정 하에서 본 방법은 63.64%의 정확도를 달성하며, OWL의 55.15%보다 8.49 포인트 더 높은 성과를 보인다.

Conclusion: 이 패러다임은 작업 모니터링의 유연성과 엣지 케이스 처리의 강건성을 향상시킨다.

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [27] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: CMA는 RAG의 한계를 극복하고 장기 에이전트를 위한 필수 아키텍처로 제안됩니다.


<details>
  <summary>Details</summary>
Motivation: RAG는 메모리를 상태 비저장 조회 테이블로 취급하여 정보의 지속성과 기계적 연속성이 결여된 한계를 가집니다.

Method: CMA는 지속적인 저장, 선별적 보존, 연상 경로 설정, 시간적 연결, 고차원 추상화로 내부 상태를 유지 및 업데이트합니다.

Result: CMA를 적용하면 RAG의 메모리 축적, 변형 및 모호성 해소의 구조적 한계를 극복할 수 있음을 보여줍니다.

Conclusion: CMA는 장기 목표를 가진 에이전트에게 필요한 아키텍처 원시이며 지연, 드리프트 및 해석 가능성에 대한 도전 과제를 강조합니다.

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [28] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 이 논문은 AI 에이전트의 프롬프트 주입 공격에 대한 방어책을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트는 악의적 내용에 의해 행동이 탈취 당할 수 있으며, 이에 대한 방어가 필요합니다.

Method: 신뢰할 수 있는 계획자가 UI 워크플로우를 기반으로 독립적인 실행 그래프를 생성하는 방식을 사용합니다.

Result: 설계 평가에서 최신 모델보다 최대 57%의 성능을 유지하며, 소형 오픈 소스 모델에서는 성능을 최대 19% 향상시켰습니다.

Conclusion: 엄격한 보안과 유용성이 동시에 존재할 수 있음을 보여줍니다.

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [29] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 본 논문은 인간-컴퓨터 상호작용에서의 대형 언어 모델(LLM)의 개인화된 특성을 모형화하는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 인간-컴퓨터 상호작용에 미치는 영향을 이해하고, 특히 인간과 유사한 특성, 특히 성격의 중요성을 탐구하기 위해서입니다.

Method: Jung 심리 유형을 통해 LLM의 성격을 모형화할 수 있는 프레임워크를 제시하며, 세 가지 메커니즘(지배-보조 조율, 강화-보상, 반영 메커니즘)을 통합하여 성격을 표현합니다.

Result: 다양한 도전 시나리오에서 Myers-Briggs Type Indicator 설문지를 사용하여 성격 정렬을 평가하고, 진화하는 성격 인식 LLM이 일관되고 맥락에 민감한 상호작용을 지원할 수 있음을 발견했습니다.

Conclusion: 진화하는 성격-aware LLM은 자연스러운 상호작용을 가능하게 하여 인간-컴퓨터 상호작용 디자인을 개선할 수 있습니다.

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [30] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: PaperScout는 논문 검색을 순차적 결정 과정으로 재구성하는 자율 에이전트로, 기존의 경직된 워크플로우의 한계를 극복한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 논문 검색 접근법은 복잡하고 조건적인 쿼리에 대해 어려움을 겪는 경직된 사전 정의된 워크플로우에 의존하고 있다.

Method: PaperScout는 누적된 검색 맥락에 기반하여 검색 및 확장 도구를 호출할 시점과 방법을 동적으로 결정하는 접근 방식을 사용한다.

Result: 종합적인 실험을 통해 PaperScout는 검색률과 관련성 모두에서 강력한 워크플로우 기반 및 강화 학습 기준선보다 유의미하게 우수함을 입증했다.

Conclusion: 이 연구는 우리의 적응형 에이전트 프레임워크와 최적화 전략의 효과성을 검증하였다.

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [31] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: 다중 물리화학적 속성에 대한 정확한 숫자 제약을 충족하는 분자를 생성하는 것은 중요하고 도전적이다. 본 논문에서는 MolGen이라는 분자 생성 프레임워크를 소개하며, 이는 두 단계로 나누어져 있다: 프로토타입 생성과 RL 기반의 세밀한 최적화. 실험 결과, 기존의 강력한 대형 언어 모델 및 그래프 기반 알고리즘을 초월하는 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 정확한 숫자 제약을 충족하는 분자 생성을 위한 필요성과 어려움.

Method: MolGen은 두 단계로 구성된다. 첫 번째 단계에서, 다중 에이전트 추론기가 조각 수준의 편집을 통해 후보를 생성한다. 두 번째 단계에서, 그룹 상대 정책 최적화(GRPO)로 훈련된 조각 수준 최적화기가 속성 오류를 최소화하는 세밀한 최적화를 수행한다.

Result: QED, LogP, 분자량 및 HOMO, LUMO 두 세트의 속성 제약 하에서 실험을 통해 유효성과 다중 속성 목표의 정확한 만족에서 일관된 성과를 보였다.

Conclusion: MolGen은 조각을 활용하여 분자에 대한 더 나은 추론을 가능하게 하고, 숫자 목표에 대한 퇴치를 조절할 수 있는 기능을 지원한다.

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [32] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 지난 1년간 대규모 언어 모델(LLMs)의 진화와 실제 사용에 중요한 전환점이 있었다. 2024년 12월 5일 첫 번째 광범위하게 채택된 추론 모델인 o1이 출시되면서, 단일 패턴 생성에서 다단계 심사 추론으로의 전환이 이루어졌고, 이는 배포와 실험, 새로운 애플리케이션 분야의 확장을 가속화했다. 하지만 이러한 변화 속에서 실제 모델 사용에 대한 경험적 이해는 뒤처졌다. OpenRouter 플랫폼을 활용하여 100조 개 이상의 실제 LLM 상호작용을 분석한 결과, 오픈 웨이트 모델의 상당한 채택과 창의적 역할 놀이의 큰 인기도, 그리고 대리적 추론의 상승을 관찰했다. 또한 초기 사용자가 나중의 Cohort보다 훨씬 오래 지속적으로 참여하는 모습을 확인하고 이를 신데렐라 '유리 구두' 효과라 명명했다. 이러한 결과는 LLM에 대한 개발자와 최종 사용자의 상호작용이 복잡하고 다면적임을 강조한다. 모델 구축자, AI 개발자 및 인프라 제공자를 위한 함의와 데이터 기반의 이해가 LLM 시스템의 더 나은 설계와 배포에 어떻게 기여할 수 있는지를 논의한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLMs)의 실제 사용 방식에 대한 경험적 이해가 부족하다.

Method: OpenRouter 플랫폼을 통해 100조 개 이상의 실제 LLM 상호작용 데이터를 분석했다.

Result: 오픈 웨이트 모델의 채택과 창의적 역할 놀이의 높은 인기, 대리적 추론의 증가를 관찰했다.

Conclusion: 개발자와 최종 사용자 간의 LLM 상호작용은 복잡하고 다면적이며, 이러한 이해가 LLM 시스템의 설계 및 배포에 도움을 줄 수 있다.

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [33] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 대규모 언어 모델(LLM)의 집단 행동 이해는 사회에 미치는 리스크와 이점에 대한 중요한 논의 분야임.


<details>
  <summary>Details</summary>
Motivation: LLM의 독특한 성질은 사전 훈련된 지식과 암묵적 사회적 선입견을 포함하고 있으며, 이러한 요소가 상호작용하는 방식을 체계적으로 검토할 필요성을 제기함.

Method: 이 논문에서는 대화형 패러다임을 기반으로 LLM의 집단 행동을 분석하기 위한 대안적 이론적 기초, 방법론 및 분석 도구를 제안한다.

Result: 제안된 네 가지 방향은 LLM 기반 집단의 개발 및 배치를 위한 중요한 요소로 강조됨.

Conclusion: 이 연구는 LLM 개발과 관련된 이론, 방법 및 임상 간 대화를 통해 LLM 기반 집단의 이해를 깊이 있게 할 수 있는 기초를 제공한다.

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [34] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai는 의료 이미징에서 AI 모델에 대한 접근을 표준화하고 재현성을 촉진하는 오픈 소스 플랫폼이다.


<details>
  <summary>Details</summary>
Motivation: AI는 의료 이미징을 변화시킬 잠재력이 있지만, 다양한 구현 및 아키텍처의 존재, 일관되지 않은 문서화 및 재현성 문제로 연구와 임상 사용이 제한적이다.

Method: MHub.ai는 동료 심사를 받은 출판물의 모델을 표준화된 컨테이너에 패키징하여 DICOM 등 다양한 형식을 직접 처리하고, 통합된 애플리케이션 인터페이스를 제공하며 구조화된 메타데이터를 포함한다.

Result: 우리는 MHub.ai 플랫폼의 임상 사례에서의 유용성을 폐 부분 세분화 모델의 비교 평가를 통해 입증하였다.

Conclusion: MHub.ai는 모델 사용을 간소화하여 동일한 실행 명령과 표준화된 결과로 나란히 벤치마킹할 수 있게 하여 임상 전환의 장벽을 낮춘다.

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [35] [CtD: Composition through Decomposition in Emergent Communication](https://arxiv.org/abs/2601.10169)
*Boaz Carmeli,Ron Meir,Yonatan Belinkov*

Main category: cs.AI

TL;DR: 이 연구는 인공 지능 에이전트가 새로운 이미지를 묘사하기 위해 조합 일반화를 습득하고 활용하는 방법을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 인간이 새로운 방식으로 알려진 개념을 체계적으로 결합할 수 있는 인지 메커니즘인 구성 가능성을 인공 신경 에이전트에 적용하고자 한다.

Method: '구성'과 '분해'의 두 가지 단계로 이루어진 '분해를 통한 구성' 방법을 사용하여 에이전트가 이미지를 기본 개념으로 분해하고 이를 바탕으로 새로운 이미지를 묘사한다.

Result: 특히, '구성' 단계에서 추가 훈련 없이 제로샷으로 일반화가 이루어지는 사례를 관찰하였다.

Conclusion: 이 연구는 인공 지능이 인간처럼 조합 일반화를 통해 새로운 이미지를 이해하고 묘사할 수 있는 가능성을 제시한다.

Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed "Composition through Decomposition", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.

</details>


### [36] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master 2.0는 초장기 자율 기계 학습 엔지니어링을 마스터하는 자율 에이전트로, 고차원 지연 피드백 환경에서 실행 세부사항에 압도당하지 않고 장기 전략의 일관성을 유지합니다.


<details>
  <summary>Details</summary>
Motivation: 인공지능이 에이전틱 과학으로 발전함에 따라 초장기 자율성의 필요성이 대두되고 있습니다. 이는 실험 사이클이 길어짐에 따라 전략적 일관성과 반복적인 수정 능력을 유지하는 데 도전 과제가 됩니다.

Method: Hierarchical Cognitive Caching (HCC)라는 다계층 아키텍처를 도입하여 경험의 구조적 차별화를 가능하게 하며, 즉각적인 실행을 장기 실험 전략과 분리하여 다이나믹하게 지식을 정제합니다.

Result: ML-Master 2.0은 OpenAI의 MLE-Bench에서 24시간 예산 하에 56.44%의 최첨단 메달 비율을 달성했습니다.

Conclusion: 초장기 자율성은 인간의 복잡성을 초월한 자율 탐사가 가능한 인공지능을 위한 확장 가능한 청사진을 제공합니다.

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [37] [Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment](https://arxiv.org/abs/2601.10520)
*Felix Jahn,Yannic Muskalla,Lisa Dargasz,Patrick Schramowski,Kevin Baum*

Main category: cs.AI

TL;DR: AI 에이전트의 결정이 효과적일 뿐만 아니라 규범적으로도 일치하는 것이 중요해짐에 따라, GRACE라는 신경-상징적 이유 기반의 제어 아키텍처를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트가 점점 더 자율성이 높아지고, 중요한 맥락에서 널리 사용되며, 실제 영향력을 미치는 상황에서 그들의 결정이 규범적으로도 일치하는 것이 필수적입니다.

Method: GRACE는 도덕 모듈, 결정 모듈, 감시자로 구성된 세 가지 모듈로 의사결정을 재구성합니다.

Result: 모델을 통해 이해관계자가 에이전트의 행동을 이해하고 논의하며 개선할 수 있는 방법을 보여줍니다.

Conclusion: GRACE는 AI 에이전트가 도덕적으로 준수하도록 모니터링하고 시행할 수 있는 메커니즘을 제공합니다.

Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.

</details>


### [38] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: 계층적 추론 모델(HRM)은 다양한 추론 작업에서 탁월한 성능을 보여주며, 대형 언어 기반 추론 모델보다 뛰어난 성능을 발휘한다. HRM의 강점과 잠재적 실패 모드를 이해하기 위해 기계적 연구를 수행하여 세 가지 놀라운 사실을 발견하였다.


<details>
  <summary>Details</summary>
Motivation: HRM의 강점과 잠재적 실패 모드를 이해하고자 하는 목적.

Method: 정확한 HRM의 추론 패턴을 분석하여 실패 사례와 특이점을 연구함.

Result: HRM이 단순 퍼즐에서 실패하고, 특정 추론 단계에서만 갑자기 올바른 답변을 생성하며, 여러 고정점을 가질 수 있다는 사실을 발견하였다.

Conclusion: HRM은 '추론'보다 '추측'에 가까우며, 이를 개선하기 위한 세 가지 전략을 제안하고, 이를 통해 정확도를 크게 향상시킴.

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [39] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent는 복잡한 유전체 쿼리를 위한 다중 에이전트 프레임워크로, GeneGPT를 능가하는 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 유전체 정보 이해는 생물 의학 연구에 필수적이지만, 복잡한 분산 데이터베이스에서 데이터를 추출하는 것은 여전히 어려움이 있다.

Method: GeneGPT를 복제하고, 복잡한 유전체 쿼리를 위해 전문화된 에이전트를 효율적으로 조정하는 다중 에이전트 프레임워크인 GenomAgent를 제안한다.

Result: GeneTuring 벤치마크의 아홉 가지 작업에서 평가한 결과, GenomAgent는 GeneGPT에 비해 평균 12% 더 뛰어난 성능을 보였다.

Conclusion: Flexible architecture가 유전체 학을 넘어 다양한 과학 분야의 전문 지식 추출로 확장될 수 있다.

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [40] [ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack](https://arxiv.org/abs/2601.10173)
*Hao Li,Yankai Yang,G. Edward Suh,Ning Zhang,Chaowei Xiao*

Main category: cs.CR

TL;DR: ReasAlign은 간접 프롬프트 인젝션 공격에 대한 안전성과 정렬을 개선하기 위한 모델 수준의 솔루션을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: LLM의 에이전트 시스템이 복잡한 워크플로를 자동화할 수 있으나 간접 프롬프트 인젝션 공격에 취약하다.

Method: ReasAlign은 사용자 쿼리를 분석하고 충돌하는 지침을 감지하며 사용자의 의도된 작업의 연속성을 유지하여 간접 공격 방어를 위해 구조화된 추론 단계를 통합한다.

Result: ReasAlign은 다양한 벤치마크에서 유틸리티를 유지하며 Meta SecAlign을 지속적으로 능가하여 사이버 보안 평가 기준에서 94.6% 유틸리티와 3.6% ASR을 기록했다.

Conclusion: ReasAlign은 보안성과 유틸리티 사이에서 최고의 균형을 이루어 간접 프롬프트 인젝션 공격에 대한 강력한 방어를 수립한다.

Abstract: Large Language Models (LLMs) have enabled the development of powerful agentic systems capable of automating complex workflows across various fields. However, these systems are highly vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external data can hijack agent behavior. In this work, we present ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea of ReasAlign is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user's intended tasks to defend against indirect injection attacks. To further ensure reasoning logic and accuracy, we introduce a test-time scaling mechanism with a preference-optimized judge model that scores reasoning steps and selects the best trajectory. Comprehensive evaluations across various benchmarks show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming Meta SecAlign, the strongest prior guardrail. On the representative open-ended CyberSecEval2 benchmark, which includes multiple prompt-injected tasks, ReasAlign achieves 94.6% utility and only 3.6% ASR, far surpassing the state-of-the-art defensive model of Meta SecAlign (56.4% utility and 74.4% ASR). These results demonstrate that ReasAlign achieves the best trade-off between security and utility, establishing a robust and practical defense against prompt injection attacks in real-world agentic systems. Our code and experimental results could be found at https://github.com/leolee99/ReasAlign.

</details>


### [41] [Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale](https://arxiv.org/abs/2601.10338)
*Yi Liu,Weizhe Wang,Ruitao Feng,Yao Zhang,Guangquan Xu,Gelei Deng,Yuekang Li,Leo Zhang*

Main category: cs.CR

TL;DR: AI 에이전트 프레임워크의 부상으로 인해 에이전트의 기능을 동적으로 확장하는 모듈형 패키지인 스킬이 등장하였다. 대규모 보안 분석을 통해 42,447 개의 스킬 중 26.1%에서 취약점이 발견되었으며, 이는 악성 의도를 암시하는 패턴을 포함하고 있다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 기능을 확장하는 스킬이 많은 개인화 및 사용 사례를 가능하게 하지만, 이러한 아키텍처의 보안 위험에 대한 분석이 부족하다.

Method: SkillScan이라는 다단계 탐지 프레임워크를 활용하여 42,447 개의 스킬 중 31,132 개를 체계적으로 분석하였다.

Result: 26.1%의 스킬에서 취약점이 발견되었으며, 데이터 유출(13.3%)과 권한 상승(11.8%)이 가장 흔하며, 5.2%는 악의적인 의도를 강하게 암시하는 높은 심각도의 패턴을 보였다.

Conclusion: 이 연구는 기능 기반 권한 시스템과 필수 보안 검증의 필요성을 강조하며, 이러한 공격 벡터가 추가로 악용되기 전에 조치를 취해야 함을 알린다.

Abstract: The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.

</details>


### [42] [AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior](https://arxiv.org/abs/2601.10440)
*Nadya Abaev,Denis Klimov,Gerard Levinov,David Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: AgentGuardian는 AI 에이전트의 작업을 관리하고 보호하는 새로운 보안 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 작업이 허가된 작업만 수행하고 입력을 적절히 처리받도록 하는 것이 시스템 무결성과 잘못된 사용 방지를 위해 필수적입니다.

Method: AgentGuardian는 상황 인식 접근 제어 정책을 적용하여 AI 에이전트의 작업을 감독하고 보호하는 프레임워크입니다. 통제된 스테이징 단계 동안 실행 추적을 모니터링하여 합법적인 에이전트 행동과 입력 패턴을 학습합니다.

Result: 두 개의 실제 AI 에이전트 애플리케이션에 대한 평가를 통해 AgentGuardian은 악의적이거나 오해의 소지가 있는 입력을 효과적으로 감지하면서 정상적인 에이전트 기능을 유지합니다.

Conclusion: 제어 흐름 기반의 거버넌스 메커니즘은 환각으로 인한 오류 및 기타 오케스트레이션 수준의 오작동을 완화합니다.

Abstract: Artificial intelligence (AI) agents are increasingly used in a variety of domains to automate tasks, interact with users, and make decisions based on data inputs. Ensuring that AI agents perform only authorized actions and handle inputs appropriately is essential for maintaining system integrity and preventing misuse. In this study, we introduce the AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. During a controlled staging phase, the framework monitors execution traces to learn legitimate agent behaviors and input patterns. From this phase, it derives adaptive policies that regulate tool calls made by the agent, guided by both real-time input context and the control flow dependencies of multi-step agent actions. Evaluation across two real-world AI agent applications demonstrates that AgentGuardian effectively detects malicious or misleading inputs while preserving normal agent functionality. Moreover, its control-flow-based governance mechanism mitigates hallucination-driven errors and other orchestration-level malfunctions.

</details>
