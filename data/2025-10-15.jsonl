{"id": "2510.09613", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09613", "abs": "https://arxiv.org/abs/2510.09613", "authors": ["Isaac Henry Teuscher"], "title": "Automating the RMF: Lessons from the FedRAMP 20x Pilot", "comment": "Presented at SiRAcon 25, September 9-11, 2025, Boston, MA, USA. 5\n  pages", "summary": "The U.S. Federal Risk and Authorization Management Program (FedRAMP) has long\nrelied on extensive sets of controls and static documentation to assess cloud\nsystems. However, this manual, point-in-time approach has struggled to keep\npace with cloud-native development. FedRAMP 20x, a 2025 pilot program,\nreimagines the NIST Risk Management Framework (RMF): replacing traditional NIST\n800-53 controls with Key Security Indicators (KSIs), using automated,\nmachine-readable evidence, and emphasizing continuous reporting and\nauthorization.\n  This case study presents a practitioner-led field report from an industry\nparticipant who led multiple FedRAMP 20x pilot submissions and engaged directly\nwith the FedRAMP PMO, 3PAOs, and community working groups. It explores how\nKSIs, continuous evidence pipelines, and DevSecOps integration can streamline\nauthorization and improve cyber risk management. The study shows FedRAMP 20x as\na live testbed for implementing the RMF in a cloud-native, automation-first\napproach and shares actionable recommendations for risk professionals seeking\nto modernize compliance and support real-time, risk-informed decision-making."}
{"id": "2510.09615", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09615", "abs": "https://arxiv.org/abs/2510.09615", "authors": ["Meiyin Meng", "Zaixi Zhang"], "title": "A Biosecurity Agent for Lifecycle LLM Biosecurity Alignment", "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into biomedical\nresearch workflows--from literature triage and hypothesis generation to\nexperimental design--yet this expanded utility also heightens dual-use\nconcerns, including the potential misuse for guiding toxic compound synthesis.\nIn response, this study shows a Biosecurity Agent that comprises four\ncoordinated modes across the model lifecycle: dataset sanitization, preference\nalignment, run-time guardrails, and automated red teaming. For dataset\nsanitization (Mode 1), evaluation is conducted on CORD-19, a COVID-19 Open\nResearch Dataset of coronavirus-related scholarly articles. We define three\nsanitization tiers--L1 (compact, high-precision), L2 (human-curated biosafety\nterms), and L3 (comprehensive union)--with removal rates rising from 0.46% to\n70.40%, illustrating the safety-utility trade-off. For preference alignment\n(Mode 2), DPO with LoRA adapters internalizes refusals and safe completions,\nreducing end-to-end attack success rate (ASR) from 59.7% to 3.0%. At inference\n(Mode 3), run-time guardrails across L1-L3 show the expected security-usability\ntrade-off: L2 achieves the best balance (F1 = 0.720, precision = 0.900, recall\n= 0.600, FPR =0.067), while L3 offers stronger jailbreak resistance at the cost\nof higher false positives. Under continuous automated red-teaming (Mode 4), no\nsuccessful jailbreaks are observed under the tested protocol. Taken together,\nour biosecurity agent offers an auditable, lifecycle-aligned framework that\nreduces attack success while preserving benign utility, providing safeguards\nfor the use of LLMs in scientific research and setting a precedent for future\nagent-level security protections."}
{"id": "2510.09620", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09620", "abs": "https://arxiv.org/abs/2510.09620", "authors": ["Jiayun Mo", "Xin Kang", "Tieyan Li", "Zhongding Lei"], "title": "Toward a Unified Security Framework for AI Agents: Trust, Risk, and Liability", "comment": null, "summary": "The excitement brought by the development of AI agents came alongside arising\nproblems. These concerns centered around users' trust issues towards AIs, the\nrisks involved, and the difficulty of attributing responsibilities and\nliabilities. Current solutions only attempt to target each problem separately\nwithout acknowledging their inter-influential nature. The Trust, Risk and\nLiability (TRL) framework proposed in this paper, however, ties together the\ninterdependent relationships of trust, risk, and liability to provide a\nsystematic method of building and enhancing trust, analyzing and mitigating\nrisks, and allocating and attributing liabilities. It can be applied to analyze\nany application scenarios of AI agents and suggest appropriate measures fitting\nto the context. The implications of the TRL framework lie in its potential\nsocietal impacts, economic impacts, ethical impacts, and more. It is expected\nto bring remarkable values to addressing potential challenges and promoting\ntrustworthy, risk-free, and responsible usage of AI in 6G networks."}
{"id": "2510.09633", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09633", "abs": "https://arxiv.org/abs/2510.09633", "authors": ["Bernhard Mueller"], "title": "Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits", "comment": null, "summary": "Hound introduces a relation-first graph engine that improves system-level\nreasoning across interrelated components in complex codebases. The agent\ndesigns flexible, analyst-defined views with compact annotations (e.g.,\nmonetary/value flows, authentication/authorization roles, call graphs, protocol\ninvariants) and uses them to anchor exact retrieval: for any question, it loads\nprecisely the code that matters (often across components) so it can zoom out to\nsystem structure and zoom in to the decisive lines. A second contribution is a\npersistent belief system: long-lived vulnerability hypotheses whose confidence\nis updated as evidence accrues. The agent employs coverage-versus-intuition\nplanning and a QA finalizer to confirm or reject hypotheses. On a five-project\nsubset of ScaBench[1], Hound improves recall and F1 over a baseline LLM\nanalyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest\nprecision trade-off. We attribute these gains to flexible, relation-first\ngraphs that extend model understanding beyond call/dataflow to abstract\naspects, plus the hypothesis-centric loop; code and artifacts are released to\nsupport reproduction."}
{"id": "2510.09676", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09676", "abs": "https://arxiv.org/abs/2510.09676", "authors": ["Shayan Mohajer Hamidi", "En-Hui Yang", "Ben Liang"], "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling", "comment": null, "summary": "Inverse problems, where the goal is to recover an unknown signal from noisy\nor incomplete measurements, are central to applications in medical imaging,\nremote sensing, and computational biology. Diffusion models have recently\nemerged as powerful priors for solving such problems. However, existing methods\neither rely on projection-based techniques that enforce measurement consistency\nthrough heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y}\n\\mid \\boldsymbol{x})$, often resulting in artifacts and instability under\ncomplex or high-noise conditions. To address these limitations, we propose a\nnovel framework called \\emph{coupled data and measurement space diffusion\nposterior sampling} (C-DPS), which eliminates the need for constraint tuning or\nlikelihood approximation. C-DPS introduces a forward stochastic process in the\nmeasurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the\ndata-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a\nclosed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t,\n\\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive\nsampling based on a well-defined posterior distribution. Empirical results\ndemonstrate that C-DPS consistently outperforms existing baselines, both\nqualitatively and quantitatively, across multiple inverse problem benchmarks."}
{"id": "2510.09801", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09801", "abs": "https://arxiv.org/abs/2510.09801", "authors": ["Valerie Chen", "Rohit Malhotra", "Xingyao Wang", "Juan Michelini", "Xuhui Zhou", "Aditya Bharat Soni", "Hoang H. Tran", "Calvin Smith", "Ameet Talwalkar", "Graham Neubig"], "title": "How can we assess human-agent interactions? Case studies in software agent design", "comment": null, "summary": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40\\% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs."}
{"id": "2510.09688", "categories": ["cs.MA", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.09688", "abs": "https://arxiv.org/abs/2510.09688", "authors": ["R. W. S. Pessoa", "M. H. NÃ¦ss", "J. C. Bijos", "C. M. Rebello", "D. Colombo", "L. Schnitman", "I. B. R. Nogueira"], "title": "A Hybrid Agent-Based and System Dynamics Framework for Modelling Project Execution and Technology Maturity in Early-Stage R&D", "comment": null, "summary": "This paper presents a hybrid approach to predict the evolution of\ntechnological maturity in R and D projects, using the oil and gas sector as an\nexample. Integrating System Dynamics (SD) and Agent Based Modelling (ABM)\nallows the proposed multi level framework to capture uncertainties in work\neffort, team size, and project duration, which influence technological\nprogress. While AB SD hybrid models are established in other fields, their use\nin R and D remains limited. The model combines system level feedback structures\ngoverning work phases, rework cycles, and duration with decentralised agents\nsuch as team members, tasks, and controllers, whose interactions generate\nemergent project dynamics. A base case scenario analysed early stage innovation\nprojects with 15 parallel tasks over 156 weeks. A comparative sequential\nscenario showed an 88 percent reduction in rework duration. A second scenario\nassessed mixed parallel sequential task structures with varying team sizes. In\nparallel configurations, increasing team size reduced project duration and\nimproved task completion, with optimal results for teams of four to five\nmembers. These findings align with empirical evidence showing that moderate\nteam expansion enhances coordination efficiency without excessive communication\noverhead. However, larger teams may decrease performance due to communication\ncomplexity and management delays. Overall, the model outputs and framework\nalign with expert understanding, supporting their validity as quantitative\ntools for analysing resource allocation, scheduling efficiency, and technology\nmaturity progression."}
{"id": "2510.09645", "categories": ["cs.CR", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09645", "abs": "https://arxiv.org/abs/2510.09645", "authors": ["Tonmoy Ghosh"], "title": "AdaptAuth: Multi-Layered Behavioral and Credential Analysis for a Secure and Adaptive Authentication Framework for Password Security", "comment": null, "summary": "Password security has been compelled to evolve in response to the growing\ncomputational capabilities of modern systems. However, this evolution has often\nresulted in increasingly complex security practices that alienate users,\nleading to poor compliance and heightened vulnerability. Consequently,\nindividuals remain exposed to attackers through weak or improperly managed\npasswords, underscoring the urgent need for a comprehensive defense mechanism\nthat effectively addresses password-related risks and threats. In this paper,\nwe propose a multifaceted solution designed to revolutionize password security\nby integrating diverse attributes such as the Password Dissection Mechanism,\nDynamic Password Policy Mechanism, human behavioral patterns, device\ncharacteristics, network parameters, geographical context, and other relevant\nfactors. By leveraging learning-based models, our framework constructs detailed\nuser profiles capable of recognizing individuals and preventing nearly all\nforms of unauthorized access or device possession. The proposed framework\nenhances the usability-security paradigm by offering stronger protection than\nexisting standards while simultaneously engaging users in the policy-setting\nprocess through a novel, adaptive approach."}
{"id": "2510.09705", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09705", "abs": "https://arxiv.org/abs/2510.09705", "authors": ["Sudip Khadka", "L. S. Paudel"], "title": "A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation", "comment": null, "summary": "Static feature exclusion strategies often fail to prevent bias when hidden\ndependencies influence the model predictions. To address this issue, we explore\na reinforcement learning (RL) framework that integrates bias mitigation and\nautomated feature selection within a single learning process. Unlike\ntraditional heuristic-driven filter or wrapper approaches, our RL agent\nadaptively selects features using a reward signal that explicitly integrates\npredictive performance with fairness considerations. This dynamic formulation\nallows the model to balance generalization, accuracy, and equity throughout the\ntraining process, rather than rely exclusively on pre-processing adjustments or\npost hoc correction mechanisms. In this paper, we describe the construction of\na multi-component reward function, the specification of the agents action space\nover feature subsets, and the integration of this system with ensemble\nlearning. We aim to provide a flexible and generalizable way to select features\nin environments where predictors are correlated and biases can inadvertently\nre-emerge."}
{"id": "2510.09901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09901", "abs": "https://arxiv.org/abs/2510.09901", "authors": ["Lianhao Zhou", "Hongyi Ling", "Cong Fu", "Yepeng Huang", "Michael Sun", "Wendi Yu", "Xiaoxuan Wang", "Xiner Li", "Xingyu Su", "Junkai Zhang", "Xiusi Chen", "Chenxing Liang", "Xiaofeng Qian", "Heng Ji", "Wei Wang", "Marinka Zitnik", "Shuiwang Ji"], "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics", "comment": null, "summary": "Computing has long served as a cornerstone of scientific discovery. Recently,\na paradigm shift has emerged with the rise of large language models (LLMs),\nintroducing autonomous systems, referred to as agents, that accelerate\ndiscovery across varying levels of autonomy. These language agents provide a\nflexible and versatile framework that orchestrates interactions with human\nscientists, natural language, computer language and code, and physics. This\npaper presents our view and vision of LLM-based scientific agents and their\ngrowing role in transforming the scientific discovery lifecycle, from\nhypothesis discovery, experimental design and execution, to result analysis and\nrefinement. We critically examine current methodologies, emphasizing key\ninnovations, practical achievements, and outstanding limitations. Additionally,\nwe identify open research challenges and outline promising directions for\nbuilding more robust, generalizable, and adaptive scientific agents. Our\nanalysis highlights the transformative potential of autonomous agents to\naccelerate scientific discovery across diverse domains."}
{"id": "2510.09937", "categories": ["cs.MA", "cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09937", "abs": "https://arxiv.org/abs/2510.09937", "authors": ["Shahbaz P Qadri Syed", "He Bai"], "title": "Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective", "comment": null, "summary": "The empirical success of multi-agent reinforcement learning (MARL) has\nmotivated the search for more efficient and scalable algorithms for large scale\nmulti-agent systems. However, existing state-of-the-art algorithms do not fully\nexploit inter-agent coupling information to develop MARL algorithms. In this\npaper, we propose a systematic approach to leverage structures in the\ninter-agent couplings for efficient model-free reinforcement learning. We model\nthe cooperative MARL problem via a Bayesian network and characterize the subset\nof agents, termed as the value dependency set, whose information is required by\neach agent to estimate its local action value function exactly. Moreover, we\npropose a partially decentralized training decentralized execution (P-DTDE)\nparadigm based on the value dependency set. We theoretically establish that the\ntotal variance of our P-DTDE policy gradient estimator is less than the\ncentralized training decentralized execution (CTDE) policy gradient estimator.\nWe derive a multi-agent policy gradient theorem based on the P-DTDE scheme and\ndevelop a scalable actor-critic algorithm. We demonstrate the efficiency and\nscalability of the proposed algorithm on multi-warehouse resource allocation\nand multi-zone temperature control examples. For dense value dependency sets,\nwe propose an approximation scheme based on truncation of the Bayesian network\nand empirically show that it achieves a faster convergence than the exact value\ndependence set for applications with a large number of agents."}
{"id": "2510.09656", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09656", "abs": "https://arxiv.org/abs/2510.09656", "authors": ["Yejun Jang"], "title": "Signing Right Away", "comment": null, "summary": "The proliferation of high-fidelity synthetic media, coupled with exploitable\nhardware vulnerabilities in conventional imaging pipelines, has precipitated a\ncrisis of trust in digital content. Existing countermeasures, from post-hoc\nclassifiers to software-based signing, fail to address the fundamental\nchallenge of establishing an unbreakable link to reality at the moment of\ncapture. This whitepaper introduces Signing Right Away (SRA), a comprehensive\nsecurity architecture that guarantees the provenance of digital media from\n\"silicon to silicon to signed file.\" SRA leverages a four-pillar security\nmodel-Confidentiality, Integrity, Authentication, and Replay Protection, akin\nto the MIPI Camera Security Framework (CSF), but also extends its scope beyond\nthe internal data bus to the creation of a cryptographically sealed,\nC2PA-compliant final asset. By securing the entire imaging pipeline within a\nTrusted Execution Environment (TEE), SRA ensures that every captured image and\nvideo carries an immutable, verifiable proof of origin. This provides a\nfoundational solution for industries reliant on trustworthy visual information,\nincluding journalism, legal evidence, and insurance. We present the SRA\narchitecture, a detailed implementation roadmap informed by empirical\nprototyping, and a comparative analysis that positions SRA as the essential\n\"last mile\" in the chain of content trust."}
{"id": "2510.09764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09764", "abs": "https://arxiv.org/abs/2510.09764", "authors": ["Wanting Mao", "Maxwell A Xu", "Harish Haresamudram", "Mithun Saha", "Santosh Kumar", "James Matthew Rehg"], "title": "Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model", "comment": null, "summary": "Modeling multi-modal time-series data is critical for capturing system-level\ndynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,\nand accelerometry provide complementary perspectives on interconnected\nphysiological processes. While recent self-supervised learning (SSL) advances\nhave improved unimodal representation learning, existing multi-modal approaches\noften rely on CLIP-style contrastive objectives that overfit to easily aligned\nfeatures and misclassify valid cross-modal relationships as negatives,\nresulting in fragmented and non-generalizable embeddings. To overcome these\nlimitations, we propose ProtoMM, a novel SSL framework that introduces a shared\nprototype dictionary to anchor heterogeneous modalities in a common embedding\nspace. By clustering representations around shared prototypes rather than\nexplicit negative sampling, our method captures complementary information\nacross modalities and provides a coherent \"common language\" for physiological\nsignals. In this work, we focus on developing a Pulse Motion foundation model\nwith ProtoMM and demonstrate that our approach outperforms contrastive-only and\nprior multimodal SSL methods, achieving state-of-the-art performance while\noffering improved interpretability of learned features."}
{"id": "2510.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10047", "abs": "https://arxiv.org/abs/2510.10047", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "comment": "14 pages, 7 figures", "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence."}
{"id": "2510.10325", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10325", "abs": "https://arxiv.org/abs/2510.10325", "authors": ["Walid Abdela"], "title": "KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments", "comment": null, "summary": "The seamless integration of physical and digital environments in\nCyber-Physical Systems(CPS), particularly within Industry 4.0, presents\nsignificant challenges stemming from system heterogeneity and complexity.\nTraditional approaches often rely on rigid, data-centric solutions like\nco-simulation frameworks or brittle point-to-point middleware bridges, which\nlack the semantic richness and flexibility required for intelligent, autonomous\ncoordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent\nInfrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS\nleverages a centralized Knowledge Graph (KG) as a dynamic, shared world model,\nproviding a common semantic foundation for a Multi-Agent System(MAS).\nAutonomous agents, representing both physical and digital components, query\nthis KG for decision-making and update it with real-time state information. The\ninfrastructure features a model-driven architecture which facilitates the\nautomatic generation of agents from semantic descriptions, thereby simplifying\nsystem extension and maintenance. By abstracting away underlying communication\nprotocols and providing a unified, intelligent coordination mechanism, KG-MAS\noffers a robust, scalable, and flexible solution for coupling heterogeneous\nphysical and digital robotic environments."}
{"id": "2510.09663", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09663", "abs": "https://arxiv.org/abs/2510.09663", "authors": ["Raju Dhakal", "Prashant Shekhar", "Laxima Niure Kandel"], "title": "Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection", "comment": "Accepted for publication in ICMLA 2025", "summary": "Radio Frequency Fingerprinting (RFF) has evolved as an effective solution for\nauthenticating devices by leveraging the unique imperfections in hardware\ncomponents involved in the signal generation process. In this work, we propose\na Convolutional Neural Network (CNN) based framework for detecting rogue\ndevices and identifying genuine ones using softmax probability thresholding. We\nemulate an attack scenario in which adversaries attempt to mimic the RF\ncharacteristics of genuine devices by training a Generative Adversarial Network\n(GAN) using In-phase and Quadrature (IQ) samples from genuine devices. The\nproposed approach is verified using IQ samples collected from ten different\nADALM-PLUTO Software Defined Radios (SDRs), with seven devices considered\ngenuine, two as rogue, and one used for validation to determine the threshold."}
{"id": "2510.09776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09776", "abs": "https://arxiv.org/abs/2510.09776", "authors": ["Yufa Zhou", "Yixiao Wang", "Surbhi Goel", "Anru R. Zhang"], "title": "Why Do Transformers Fail to Forecast Time Series In-Context?", "comment": "Code: https://github.com/MasterZhou1/ICL-Time-Series", "summary": "Time series forecasting (TSF) remains a challenging and largely unsolved\nproblem in machine learning, despite significant recent efforts leveraging\nLarge Language Models (LLMs), which predominantly rely on Transformer\narchitectures. Empirical evidence consistently shows that even powerful\nTransformers often fail to outperform much simpler models, e.g., linear models,\non TSF tasks; however, a rigorous theoretical understanding of this phenomenon\nremains limited. In this paper, we provide a theoretical analysis of\nTransformers' limitations for TSF through the lens of In-Context Learning (ICL)\ntheory. Specifically, under AR($p$) data, we establish that: (1) Linear\nSelf-Attention (LSA) models $\\textit{cannot}$ achieve lower expected MSE than\nclassical linear models for in-context forecasting; (2) as the context length\napproaches to infinity, LSA asymptotically recovers the optimal linear\npredictor; and (3) under Chain-of-Thought (CoT) style inference, predictions\ncollapse to the mean exponentially. We empirically validate these findings\nthrough carefully designed experiments. Our theory not only sheds light on\nseveral previously underexplored phenomena but also offers practical insights\nfor designing more effective forecasting architectures. We hope our work\nencourages the broader research community to revisit the fundamental\ntheoretical limitations of TSF and to critically evaluate the direct\napplication of increasingly sophisticated architectures without deeper\nscrutiny."}
{"id": "2510.10074", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10074", "abs": "https://arxiv.org/abs/2510.10074", "authors": ["Jiayi Mao", "Liqun Li", "Yanjie Gao", "Zegang Peng", "Shilin He", "Chaoyun Zhang", "Si Qin", "Samia Khalid", "Qingwei Lin", "Saravan Rajmohan", "Sitaram Lanka", "Dongmei Zhang"], "title": "Agentic Troubleshooting Guide Automation for Incident Management", "comment": null, "summary": "Effective incident management in large-scale IT systems relies on\ntroubleshooting guides (TSGs), but their manual execution is slow and\nerror-prone. While recent advances in LLMs offer promise for automating\nincident management tasks, existing LLM-based solutions lack specialized\nsupport for several key challenges, including managing TSG quality issues,\ninterpreting complex control flow, handling data-intensive queries, and\nexploiting execution parallelism. We first conducted an empirical study on 92\nreal-world TSGs, and, guided by our findings, we present StepFly, a novel\nend-to-end agentic framework for troubleshooting guide automation. Our approach\nfeatures a three-stage workflow: the first stage provides a comprehensive guide\ntogether with a tool, TSG Mentor, to assist SREs in improving TSG quality; the\nsecond stage performs offline preprocessing using LLMs to extract structured\nexecution DAGs from unstructured TSGs and to create dedicated Query Preparation\nPlugins (QPPs); and the third stage executes online using a DAG-guided\nscheduler-executor framework with a memory system to guarantee correct workflow\nand support parallel execution of independent steps. Our empirical evaluation\non a collection of real-world TSGs and incidents demonstrates that StepFly\nachieves a ~94% success rate on GPT-4.1, outperforming baselines with less time\nand token consumption. Furthermore, it achieves a remarkable execution time\nreduction of 32.9% to 70.4% for parallelizable TSGs."}
{"id": "2510.10611", "categories": ["cs.MA", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.10611", "abs": "https://arxiv.org/abs/2510.10611", "authors": ["Heng Zhang", "Yuling Shi", "Xiaodong Gu", "Zijian Zhang", "Haochen You", "Lubin Gan", "Yilei Yuan", "Jin Huang"], "title": "HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication", "comment": null, "summary": "Recent advances in large language model-powered multi-agent systems have\ndemonstrated remarkable collective intelligence through effective\ncommunication. However, existing approaches face two primary challenges: (i)\n\\textit{Ineffective group collaboration modeling}, as they rely on pairwise\nedge representations in graph structures, limiting their ability to capture\nrelationships among multiple agents; and (ii) \\textit{Limited task-adaptiveness\nin communication topology design}, leading to excessive communication cost for\nsimple tasks and insufficient coordination for complex scenarios. These issues\nrestrict the scalability and practical deployment of adaptive collaboration\nframeworks. To address these challenges, we propose \\textbf{HyperAgent}, a\nhypergraph-based framework that optimizes communication topologies and\neffectively captures group collaboration patterns using direct hyperedge\nrepresentations. Unlike edge-based approaches, HyperAgent uses hyperedges to\nlink multiple agents within the same subtask and employs hypergraph\nconvolutional layers to achieve one-step information aggregation in\ncollaboration groups. Additionally, it incorporates a variational autoencoder\nframework with sparsity regularization to dynamically adjust hypergraph\ntopologies based on task complexity. Experiments highlight the superiority of\nHyperAgent in both performance and efficiency. For instance, on GSM8K,\nHyperAgent achieves 95.07\\% accuracy while reducing token consumption by\n25.33\\%, demonstrating the potential of hypergraph-based optimization for\nmulti-agent communication."}
{"id": "2510.09689", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09689", "abs": "https://arxiv.org/abs/2510.09689", "authors": ["Haoran Ou", "Kangjie Chen", "Xingshuo Han", "Gelei Deng", "Jie Zhang", "Han Qiu", "Tianwei Zhang"], "title": "CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search", "comment": null, "summary": "Large Language Models (LLMs) excel at tasks such as dialogue, summarization,\nand question answering, yet they struggle to adapt to specialized domains and\nevolving facts. To overcome this, web search has been integrated into LLMs,\nallowing real-time access to online content. However, this connection magnifies\nsafety risks, as adversarial prompts combined with untrusted sources can cause\nsevere vulnerabilities. We investigate red teaming for LLMs with web search and\npresent CREST-Search, a framework that systematically exposes risks in such\nsystems. Unlike existing methods for standalone LLMs, CREST-Search addresses\nthe complex workflow of search-enabled models by generating adversarial queries\nwith in-context learning and refining them through iterative feedback. We\nfurther construct WebSearch-Harm, a search-specific dataset to fine-tune LLMs\ninto efficient red-teaming agents. Experiments show that CREST-Search\neffectively bypasses safety filters and reveals vulnerabilities in modern\nweb-augmented LLMs, underscoring the need for specialized defenses to ensure\ntrustworthy deployment."}
{"id": "2510.09781", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09781", "abs": "https://arxiv.org/abs/2510.09781", "authors": ["Yue Huang", "Hang Hua", "Yujun Zhou", "Pengcheng Jing", "Manish Nagireddy", "Inkit Padhi", "Greta Dolcetti", "Zhangchen Xu", "Subhajit Chaudhury", "Ambrish Rawat", "Liubov Nedoshivina", "Pin-Yu Chen", "Prasanna Sattigeri", "Xiangliang Zhang"], "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "comment": null, "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems."}
{"id": "2510.10117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10117", "abs": "https://arxiv.org/abs/2510.10117", "authors": ["Yunxiang Mo", "Tianshi Zheng", "Qing Zong", "Jiayu Liu", "Baixuan Xu", "Yauwai Yim", "Chunkit Chan", "Jiaxin Bai", "Yangqiu Song"], "title": "DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay", "comment": "EMNLP 2025 Wordplay (Spotlight)", "summary": "Multimodal abductive reasoning--the generation and selection of explanatory\nhypotheses from partial observations--is a cornerstone of intelligence. Current\nevaluations of this ability in vision-language models (VLMs) are largely\nconfined to static, single-agent tasks. Inspired by Dixit, we introduce\nDixitWorld, a comprehensive evaluation suite designed to deconstruct this\nchallenge. DIXITWORLD features two core components: DixitArena, a dynamic,\nmulti-agent environment that evaluates both hypothesis generation (a\n\"storyteller\" crafting cryptic clues) and hypothesis selection (\"listeners\"\nchoosing the target image from decoys) under imperfect information; and\nDixitBench, a static QA benchmark that isolates the listener's task for\nefficient, controlled evaluation. Results from DixitArena reveal distinct,\nrole-dependent behaviors: smaller open-source models often excel as creative\nstorytellers, producing imaginative yet less discriminative clues, whereas\nlarger proprietary models demonstrate superior overall performance,\nparticularly as listeners. Performance on DixitBench strongly correlates with\nlistener results in DixitArena, validating it as a reliable proxy for\nhypothesis selection. Our findings reveal a key trade-off between generative\ncreativity and discriminative understanding in multimodal abductive reasoning,\na central challenge for developing more balanced and capable vision-language\nagents."}
{"id": "2510.10943", "categories": ["cs.MA", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10943", "abs": "https://arxiv.org/abs/2510.10943", "authors": ["Thi-Nhung Nguyen", "Linhao Luo", "Thuy-Trang Vu", "Dinh Phung"], "title": "The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems", "comment": "15 pages, 19 figures, Preprint. Under review", "summary": "Bias in large language models (LLMs) remains a persistent challenge,\nmanifesting in stereotyping and unfair treatment across social groups. While\nprior research has primarily focused on individual models, the rise of\nmulti-agent systems (MAS), where multiple LLMs collaborate and communicate,\nintroduces new and largely unexplored dynamics in bias emergence and\npropagation. In this work, we present a comprehensive study of stereotypical\nbias in MAS, examining how internal specialization, underlying LLMs and\ninter-agent communication protocols influence bias robustness, propagation, and\namplification. We simulate social contexts where agents represent different\nsocial groups and evaluate system behavior under various interaction and\nadversarial scenarios. Experiments on three bias benchmarks reveal that MAS are\ngenerally less robust than single-agent systems, with bias often emerging early\nthrough in-group favoritism. However, cooperative and debate-based\ncommunication can mitigate bias amplification, while more robust underlying\nLLMs improve overall system stability. Our findings highlight critical factors\nshaping fairness and resilience in multi-agent LLM systems."}
{"id": "2510.09952", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09952", "abs": "https://arxiv.org/abs/2510.09952", "authors": ["Cem Topcuoglu", "Kaan Onarlioglu", "Steven Sprecher", "Engin Kirda"], "title": "HTTP Request Synchronization Defeats Discrepancy Attacks", "comment": null, "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."}
{"id": "2510.09796", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "stat.ML", "47A52, 47J30, 65J22, 65K10, 68T01, 68T07, 68W15, 94A08"], "pdf": "https://arxiv.org/pdf/2510.09796", "abs": "https://arxiv.org/abs/2510.09796", "authors": ["Xiaoyu Wang", "Alexandra Valavanis", "Azhir Mahmood", "Andreas Mang", "Martin Benning", "Audrey Repetti"], "title": "A Unified Framework for Lifted Training and Inversion Approaches", "comment": null, "summary": "The training of deep neural networks predominantly relies on a combination of\ngradient-based optimisation and back-propagation for the computation of the\ngradient. While incredibly successful, this approach faces challenges such as\nvanishing or exploding gradients, difficulties with non-smooth activations, and\nan inherently sequential structure that limits parallelisation. Lifted training\nmethods offer an alternative by reformulating the nested optimisation problem\ninto a higher-dimensional, constrained optimisation problem where the\nconstraints are no longer enforced directly but penalised with penalty terms.\nThis chapter introduces a unified framework that encapsulates various lifted\ntraining strategies, including the Method of Auxiliary Coordinates, Fenchel\nLifted Networks, and Lifted Bregman Training, and demonstrates how diverse\narchitectures, such as Multi-Layer Perceptrons, Residual Neural Networks, and\nProximal Neural Networks fit within this structure. By leveraging tools from\nconvex optimisation, particularly Bregman distances, the framework facilitates\ndistributed optimisation, accommodates non-differentiable proximal activations,\nand can improve the conditioning of the training landscape. We discuss the\nimplementation of these methods using block-coordinate descent strategies,\nincluding deterministic implementations enhanced by accelerated and adaptive\noptimisation techniques, as well as implicit stochastic gradient methods.\nFurthermore, we explore the application of this framework to inverse problems,\ndetailing methodologies for both the training of specialised networks (e.g.,\nunrolled architectures) and the stable inversion of pre-trained networks.\nNumerical results on standard imaging tasks validate the effectiveness and\nstability of the lifted Bregman approach compared to conventional training,\nparticularly for architectures employing proximal activations."}
{"id": "2510.10197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10197", "abs": "https://arxiv.org/abs/2510.10197", "authors": ["Siyuan Lu", "Zechuan Wang", "Hongxuan Zhang", "Qintong Wu", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu", "Tao Lin"], "title": "Don't Just Fine-tune the Agent, Tune the Environment", "comment": null, "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce $\\textbf{Environment Tuning}$, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\n$\\textbf{Environment Tuning}$ orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents."}
{"id": "2510.11004", "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11004", "abs": "https://arxiv.org/abs/2510.11004", "authors": ["Haoran Liang", "Yufa Zhou", "Mohammad Talebi Kalaleh", "Qipei Mei"], "title": "Automating Structural Engineering Workflows with Large Language Model Agents", "comment": "Code: https://github.com/DelosLiang/masse", "summary": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural\nEngineering, effectively integrating large language model (LLM)-based agents\nwith real-world engineering workflows. Structural engineering is a fundamental\nyet traditionally stagnant domain, with core workflows remaining largely\nunchanged for decades despite its substantial economic impact and global market\nsize. Recent advancements in LLMs have significantly enhanced their ability to\nperform complex reasoning, long-horizon planning, and precise tool utilization\n-- capabilities well aligned with structural engineering tasks such as\ninterpreting design codes, executing load calculations, and verifying\nstructural capacities. We present a proof-of-concept showing that most\nreal-world structural engineering workflows can be fully automated through a\ntraining-free LLM-based multi-agent system. MASSE enables immediate deployment\nin professional environments, and our comprehensive validation on real-world\ncase studies demonstrates that it can reduce expert workload from approximately\ntwo hours to mere minutes, while enhancing both reliability and accuracy in\npractical engineering scenarios."}
{"id": "2510.10073", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10073", "abs": "https://arxiv.org/abs/2510.10073", "authors": ["Zonghao Ying", "Yangguang Shao", "Jianle Gan", "Gan Xu", "Junjie Shen", "Wenxin Zhang", "Quanchen Zou", "Junzheng Shi", "Zhenfei Yin", "Mingchuan Zhang", "Aishan Liu", "Xianglong Liu"], "title": "SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents", "comment": null, "summary": "Large vision-language model (LVLM)-based web agents are emerging as powerful\ntools for automating complex online tasks. However, when deployed in real-world\nenvironments, they face serious security risks, motivating the design of\nsecurity evaluation benchmarks. Existing benchmarks provide only partial\ncoverage, typically restricted to narrow scenarios such as user-level prompt\nmanipulation, and thus fail to capture the broad range of agent\nvulnerabilities. To address this gap, we present \\tool{}, the first holistic\nbenchmark for evaluating the security of LVLM-based web agents. \\tool{} first\nintroduces a unified evaluation suite comprising six simulated but realistic\nweb environments (\\eg, e-commerce platforms, community forums) and includes\n2,970 high-quality trajectories spanning diverse tasks and attack settings. The\nsuite defines a structured taxonomy of six attack vectors spanning both\nuser-level and environment-level manipulations. In addition, we introduce a\nmulti-layered evaluation protocol that analyzes agent failures across three\ncritical dimensions: internal reasoning, behavioral trajectory, and task\noutcome, facilitating a fine-grained risk analysis that goes far beyond simple\nsuccess metrics. Using this benchmark, we conduct large-scale experiments on 9\nrepresentative LVLMs, which fall into three categories: general-purpose,\nagent-specialized, and GUI-grounded. Our results show that all tested agents\nare consistently vulnerable to subtle adversarial manipulations and reveal\ncritical trade-offs between model specialization and security. By providing (1)\na comprehensive benchmark suite with diverse environments and a multi-layered\nevaluation pipeline, and (2) empirical insights into the security challenges of\nmodern LVLM-based web agents, \\tool{} establishes a foundation for advancing\ntrustworthy web agent deployment."}
{"id": "2510.09845", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09845", "abs": "https://arxiv.org/abs/2510.09845", "authors": ["Nicholas LaHaye", "Thilanka Munashinge", "Hugo Lee", "Xiaohua Pan", "Gonzalo Gonzalez Abad", "Hazem Mahmoud", "Jennifer Wei"], "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data", "comment": "https://2025.ieeeigarss.org/view_paper.php?PaperNum=6389&SessionID=1611", "summary": "This work demonstrates the possibilities for improving wildfire and air\nquality management in the western United States by leveraging the unprecedented\nhourly data from NASA's TEMPO satellite mission and advances in self-supervised\ndeep learning. Here we demonstrate the efficacy of deep learning for mapping\nthe near real-time hourly spread of wildfire fronts and smoke plumes using an\ninnovative self-supervised deep learning-system: successfully distinguishing\nsmoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across\nthe smoke and fire masks generated from different sensing modalities as well as\nsignificant improvement over operational products for the same cases."}
{"id": "2510.10454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10454", "abs": "https://arxiv.org/abs/2510.10454", "authors": ["Sihang Zeng", "Yujuan Fu", "Sitong Zhou", "Zixuan Yu", "Lucas Jing Liu", "Jun Wen", "Matthew Thompson", "Ruth Etzioni", "Meliha Yetisgen"], "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) offer a generalizable approach for modeling\npatient trajectories, but suffer from the long and noisy nature of electronic\nhealth records (EHR) data in temporal reasoning. To address these challenges,\nwe introduce Traj-CoA, a multi-agent system involving chain-of-agents for\npatient trajectory modeling. Traj-CoA employs a chain of worker agents to\nprocess EHR data in manageable chunks sequentially, distilling critical events\ninto a shared long-term memory module, EHRMem, to reduce noise and preserve a\ncomprehensive timeline. A final manager agent synthesizes the worker agents'\nsummary and the extracted timeline in EHRMem to make predictions. In a\nzero-shot one-year lung cancer risk prediction task based on five-year EHR\ndata, Traj-CoA outperforms baselines of four categories. Analysis reveals that\nTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as a\npromisingly robust and generalizable approach for modeling complex patient\ntrajectories."}
{"id": "2510.11108", "categories": ["cs.MA", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11108", "abs": "https://arxiv.org/abs/2510.11108", "authors": ["Xinfeng Li", "Dong Huang", "Jie Li", "Hongyi Cai", "Zhenhong Zhou", "Wei Dong", "XiaoFeng Wang", "Yang Liu"], "title": "A Vision for Access Control in LLM-based Agent Systems", "comment": "10 pages, 1 figure", "summary": "The autonomy and contextual complexity of LLM-based agents render traditional\naccess control (AC) mechanisms insufficient. Static, rule-based systems\ndesigned for predictable environments are fundamentally ill-equipped to manage\nthe dynamic information flows inherent in agentic interactions. This position\npaper argues for a paradigm shift from binary access control to a more\nsophisticated model of information governance, positing that the core challenge\nis not merely about permission, but about governing the flow of information. We\nintroduce Agent Access Control (AAC), a novel framework that reframes AC as a\ndynamic, context-aware process of information flow governance. AAC operates on\ntwo core modules: (1) multi-dimensional contextual evaluation, which assesses\nnot just identity but also relationships, scenarios, and norms; and (2)\nadaptive response formulation, which moves beyond simple allow/deny decisions\nto shape information through redaction, summarization, and paraphrasing. This\nvision, powered by a dedicated AC reasoning engine, aims to bridge the gap\nbetween human-like nuanced judgment and scalable Al safety, proposing a new\nconceptual lens for future research in trustworthy agent design."}
{"id": "2510.10436", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10436", "abs": "https://arxiv.org/abs/2510.10436", "authors": ["Gaurab Chhetri", "Shriyank Somvanshi", "Pavan Hebli", "Shamyo Brotee", "Subasish Das"], "title": "Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey", "comment": "Preprint under active peer review for ACM Computing Surveys", "summary": "Post-quantum cryptography (PQC) is moving from evaluation to deployment as\nNIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the\nspace from foundations to practice. We first develop a taxonomy across\nlattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families,\nsummarizing security assumptions, cryptanalysis, and standardization status. We\nthen compare performance and communication costs using representative,\nimplementation-grounded measurements, and review hardware acceleration (AVX2,\nFPGA/ASIC) and implementation security with a focus on side-channel resistance.\nBuilding upward, we examine protocol integration (TLS, DNSSEC), PKI and\ncertificate hygiene, and deployment in constrained and high-assurance\nenvironments (IoT, cloud, finance, blockchain). We also discuss complementarity\nwith quantum technologies (QKD, QRNGs) and the limits of near-term quantum\ncomputing. Throughout, we emphasize crypto-agility, hybrid migration, and\nevidence-based guidance for operators. We conclude with open problems spanning\nparameter agility, leakage-resilient implementations, and domain-specific\nrollout playbooks. This survey aims to be a practical reference for researchers\nand practitioners planning quantum-safe systems, bridging standards,\nengineering, and operations."}
{"id": "2510.09872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09872", "abs": "https://arxiv.org/abs/2510.09872", "authors": ["Sanjari Srivastava", "Gang Li", "Cheng Chang", "Rishu Garg", "Manpreet Kaur", "Charlene Y. Lee", "Yuezhang Li", "Yining Mao", "Ignacio Cases", "Yanan Xie", "Peng Qi"], "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions", "comment": null, "summary": "Training web agents to navigate complex, real-world websites requires them to\nmaster $\\textit{subtasks}$ - short-horizon interactions on multiple UI\ncomponents (e.g., choosing the correct date in a date picker, or scrolling in a\ncontainer to extract information). We introduce WARC-Bench (Web Archive\nBenchmark), a novel web navigation benchmark featuring 438 tasks designed to\nevaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed\ninteractions with dynamic and realistic webpages using Web ARChive files. We\nshow that WARC-Bench is challenging for leading computer-use models, with the\nhighest observed success rate being 64.8%. To improve open source models on\nsubtask, we explore two common training techniques: supervised fine-tuning\n(SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments\nshow that SFT models obtain a 48.8% success rate on the benchmark. Training\nwith RLVR over SFT checkpoints, even in data-scarce settings, improves the\nscore to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis\nconcludes that mastering these subtasks is essential for robust web planning\nand navigation, and is a capability not extensively evaluated by existing\nbenchmarks."}
{"id": "2510.10461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10461", "abs": "https://arxiv.org/abs/2510.10461", "authors": ["Hongjie Zheng", "Zesheng Shi", "Ping Yi"], "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision", "comment": null, "summary": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated\nremarkable capabilities in isolated medical tasks like diagnosis and image\nanalysis, but struggle with integrated clinical workflows that connect\ndiagnostic reasoning and medication decisions. We identify a core limitation:\nexisting medical AI systems process tasks in isolation without the\ncross-validation and knowledge integration found in clinical teams, reducing\ntheir effectiveness in real-world healthcare scenarios. To transform the\nisolation paradigm into a collaborative approach, we propose MedCoAct, a\nconfidence-aware multi-agent framework that simulates clinical collaboration by\nintegrating specialized doctor and pharmacist agents, and present a benchmark,\nDrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and\ntreatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\%\ndiagnostic accuracy and 67.58\\% medication recommendation accuracy,\noutperforming single agent framework by 7.04\\% and 7.08\\% respectively. This\ncollaborative approach generalizes well across diverse medical domains, proving\nespecially effective for telemedicine consultations and routine clinical\nscenarios, while providing interpretable decision-making pathways."}
{"id": "2510.11410", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11410", "abs": "https://arxiv.org/abs/2510.11410", "authors": ["Anastasia Psarou", "Åukasz Gorczyca", "Dominik GaweÅ", "RafaÅ Kucharski"], "title": "Autonomous vehicles need social awareness to find optima in multi-agent reinforcement learning routing games", "comment": null, "summary": "Previous work has shown that when multiple selfish Autonomous Vehicles (AVs)\nare introduced to future cities and start learning optimal routing strategies\nusing Multi-Agent Reinforcement Learning (MARL), they may destabilize traffic\nsystems, as they would require a significant amount of time to converge to the\noptimal solution, equivalent to years of real-world commuting.\n  We demonstrate that moving beyond the selfish component in the reward\nsignificantly relieves this issue. If each AV, apart from minimizing its own\ntravel time, aims to reduce its impact on the system, this will be beneficial\nnot only for the system-wide performance but also for each individual player in\nthis routing game.\n  By introducing an intrinsic reward signal based on the marginal cost matrix,\nwe significantly reduce training time and achieve convergence more reliably.\nMarginal cost quantifies the impact of each individual action (route-choice) on\nthe system (total travel time). Including it as one of the components of the\nreward can reduce the degree of non-stationarity by aligning agents'\nobjectives. Notably, the proposed counterfactual formulation preserves the\nsystem's equilibria and avoids oscillations.\n  Our experiments show that training MARL algorithms with our novel reward\nformulation enables the agents to converge to the optimal solution, whereas the\nbaseline algorithms fail to do so. We show these effects in both a toy network\nand the real-world network of Saint-Arnoult. Our results optimistically\nindicate that social awareness (i.e., including marginal costs in routing\ndecisions) improves both the system-wide and individual performance of future\nurban systems with AVs."}
{"id": "2510.10932", "categories": ["cs.CR", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10932", "abs": "https://arxiv.org/abs/2510.10932", "authors": ["Zonghuan Xu", "Xiang Zheng", "Xingjun Ma", "Yu-Gang Jiang"], "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models", "comment": "8 pages, 8 tables, 1 figure. Under review", "summary": "With the growing deployment of Vision-Language-Action (VLA) models in\nreal-world embodied AI systems, their increasing vulnerability to backdoor\nattacks poses a serious safety threat. A backdoored VLA agent can be covertly\ntriggered by a pre-injected backdoor to execute adversarial actions,\npotentially causing system failures or even physical harm. Although backdoor\nattacks on VLA models have been explored, prior work has focused only on\nuntargeted attacks, leaving the more practically threatening scenario of\ntargeted manipulation unexamined. In this paper, we study targeted backdoor\nattacks on VLA models and introduce TabVLA, a novel framework that enables such\nattacks via black-box fine-tuning. TabVLA explores two deployment-relevant\ninference-time threat models: input-stream editing and in-scene triggering. It\nformulates poisoned data generation as an optimization problem to improve\nattack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal\nthat the vision channel is the principal attack surface: targeted backdoors\nsucceed with minimal poisoning, remain robust across variations in trigger\ndesign, and are degraded only by positional mismatches between fine-tuning and\ninference triggers. We also investigate a potential detection-based defense\nagainst TabVLA, which reconstructs latent visual triggers from the input stream\nto flag activation-conditioned backdoor samples. Our work highlights the\nvulnerability of VLA models to targeted backdoor manipulation and underscores\nthe need for more advanced defenses."}
{"id": "2510.09877", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09877", "abs": "https://arxiv.org/abs/2510.09877", "authors": ["Kangping Hu", "Stephen Mussmann"], "title": "Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling", "comment": null, "summary": "Over the past couple of decades, many active learning acquisition functions\nhave been proposed, leaving practitioners with an unclear choice of which to\nuse. Bayesian Decision Theory (BDT) offers a universal principle to guide\ndecision-making. In this work, we derive BDT for (Bayesian) active learning in\nthe myopic framework, where we imagine we only have one more point to label.\nThis derivation leads to effective algorithms such as Expected Error Reduction\n(EER), Expected Predictive Information Gain (EPIG), and other algorithms that\nappear in the literature. Furthermore, we show that BAIT (active learning based\non V-optimal experimental design) can be derived from BDT and asymptotic\napproximations. A key challenge of such methods is the difficult scaling to\nlarge batch sizes, leading to either computational challenges (BatchBALD) or\ndramatic performance drops (top-$B$ selection). Here, using a particular\nformulation of the decision process, we derive Partial Batch Label Sampling\n(ParBaLS) for the EPIG algorithm. We show experimentally for several datasets\nthat ParBaLS EPIG gives superior performance for a fixed budget and Bayesian\nLogistic Regression on Neural Embeddings. Our code is available at\nhttps://github.com/ADDAPT-ML/ParBaLS."}
{"id": "2510.10633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10633", "abs": "https://arxiv.org/abs/2510.10633", "authors": ["Jiabao Shi", "Minfeng Qi", "Lefeng Zhang", "Di Wang", "Yingjie Zhao", "Ziying Li", "Yalong Xing", "Ningran Li"], "title": "Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion", "comment": "16 pages, 13 figures", "summary": "Multimodal text-to-image generation remains constrained by the difficulty of\nmaintaining semantic alignment and professional-level detail across diverse\nvisual domains. We propose a multi-agent reinforcement learning framework that\ncoordinates domain-specialized agents (e.g., focused on architecture,\nportraiture, and landscape imagery) within two coupled subsystems: a text\nenhancement module and an image generation module, each augmented with\nmultimodal integration components. Agents are trained using Proximal Policy\nOptimization (PPO) under a composite reward function that balances semantic\nsimilarity, linguistic visual quality, and content diversity. Cross-modal\nalignment is enforced through contrastive learning, bidirectional attention,\nand iterative feedback between text and image. Across six experimental\nsettings, our system significantly enriches generated content (word count\nincreased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion\nmethods, Transformer-based strategies achieve the highest composite score\n(0.521), despite occasional stability issues. Multimodal ensembles yield\nmoderate consistency (ranging from 0.444 to 0.481), reflecting the persistent\nchallenges of cross-modal semantic grounding. These findings underscore the\npromise of collaborative, specialization-driven architectures for advancing\nreliable multimodal generative systems."}
{"id": "2510.10570", "categories": ["cs.LG", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10570", "abs": "https://arxiv.org/abs/2510.10570", "authors": ["Zirui Wan", "Stefan Vlaski"], "title": "Multitask Learning with Learned Task Relationships", "comment": null, "summary": "Classical consensus-based strategies for federated and decentralized learning\nare statistically suboptimal in the presence of heterogeneous local data or\ntask distributions. As a result, in recent years, there has been growing\ninterest in multitask or personalized strategies, which allow individual agents\nto benefit from one another in pursuing locally optimal models without\nenforcing consensus. Existing strategies require either precise prior knowledge\nof the underlying task relationships or are fully non-parametric and instead\nrely on meta-learning or proximal constructions. In this work, we introduce an\nalgorithmic framework that strikes a balance between these extremes. By\nmodeling task relationships through a Gaussian Markov Random Field with an\nunknown precision matrix, we develop a strategy that jointly learns both the\ntask relationships and the local models, allowing agents to self-organize in a\nway consistent with their individual data distributions. Our theoretical\nanalysis quantifies the quality of the learned relationship, and our numerical\nexperiments demonstrate its practical effectiveness."}
{"id": "2510.10990", "categories": ["cs.CR", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.10990", "abs": "https://arxiv.org/abs/2510.10990", "authors": ["Tianze Wang", "Zhaoyu Chen", "Jian Du", "Yingtai Xiao", "Linjun Zhang", "Qiang Yan"], "title": "Secret-Protected Evolution for Differentially Private Synthetic Text Generation", "comment": null, "summary": "Text data has become extremely valuable on large language models (LLMs) and\neven lead to general artificial intelligence (AGI). A lot of high-quality text\nin the real world is private and cannot be freely used due to privacy concerns.\nTherefore, differentially private (DP) synthetic text generation has been\nproposed, aiming to produce high-utility synthetic data while protecting\nsensitive information. However, existing DP synthetic text generation imposes\nuniform guarantees that often overprotect non-sensitive content, resulting in\nsubstantial utility loss and computational overhead. Therefore, we propose\nSecret-Protected Evolution (SecPE), a novel framework that extends private\nevolution with secret-aware protection. Theoretically, we show that SecPE\nsatisfies $(\\mathrm{p}, \\mathrm{r})$-secret protection, constituting a\nrelaxation of Gaussian DP that enables tighter utility-privacy trade-offs,\nwhile also substantially reducing computational complexity relative to baseline\nmethods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE\nconsistently achieves lower Fr\\'echet Inception Distance (FID) and higher\ndownstream task accuracy than GDP-based Aug-PE baselines, while requiring less\nnoise to attain the same level of protection. Our results highlight that\nsecret-aware guarantees can unlock more practical and effective\nprivacy-preserving synthetic text generation."}
{"id": "2510.09976", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09976", "abs": "https://arxiv.org/abs/2510.09976", "authors": ["Mingyang Lyu", "Yinqian Sun", "Erliang Lin", "Huangrui Li", "Ruolin Chen", "Feifei Zhao", "Yi Zeng"], "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\\pi_0$ have\nshown strong generalization by leveraging large-scale demonstrations, yet their\nperformance is still fundamentally constrained by the quality and coverage of\nsupervised data. Reinforcement learning (RL) provides a promising path for\nimproving and fine-tuning VLAs through online interaction. However,\nconventional policy gradient methods are computationally infeasible in the\ncontext of flow-matching based models due to the intractability of the\nimportance sampling process, which requires explicit computation of policy\nratios. To overcome this limitation, we propose Flow Policy Optimization (FPO)\nalgorithm, which reformulates importance sampling by leveraging per-sample\nchanges in the conditional flow-matching objective. Furthermore, FPO achieves\nstable and scalable online reinforcement fine-tuning of the $\\pi_0$ model by\nintegrating structure-aware credit assignment to enhance gradient efficiency,\nclipped surrogate objectives to stabilize optimization, multi-step latent\nexploration to encourage diverse policy updates, and a Q-ensemble mechanism to\nprovide robust value estimation. We evaluate FPO on the LIBERO benchmark and\nthe ALOHA simulation task against supervised, preference-aligned,\ndiffusion-based, autoregressive online RL, and $\\pi_0$-FAST baselines,\nobserving consistent improvements over the imitation prior and strong\nalternatives with stable learning under sparse rewards. In addition, ablation\nstudies and analyses of the latent space dynamics further highlight the\ncontributions of individual components within FPO, validating the effectiveness\nof the proposed computational modules and the stable convergence of the\nconditional flow-matching objective during online RL."}
{"id": "2510.10640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10640", "abs": "https://arxiv.org/abs/2510.10640", "authors": ["Piyush Pant", "Marcellius William Suntoro", "Ayesha Siddiqua", "Muhammad Shehryaar Sharif", "Daniyal Ahmed"], "title": "Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany", "comment": "7 pages. Application:\n  https://equity-aware-geospatial-ai-project.streamlit.app/ Codebase:\n  https://github.com/mwsyow/equity-aware-geospatial-ai-project/", "summary": "This paper presents EA-GeoAI, an integrated framework for demand forecasting\nand equitable hospital planning in Germany through 2030. We combine\ndistrict-level demographic shifts, aging population density, and infrastructure\nbalances into a unified Equity Index. An interpretable Agentic AI optimizer\nthen allocates beds and identifies new facility sites to minimize unmet need\nunder budget and travel-time constraints. This approach bridges GeoAI,\nlong-term forecasting, and equity measurement to deliver actionable\nrecommendations for policymakers."}
{"id": "2510.11062", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11062", "abs": "https://arxiv.org/abs/2510.11062", "authors": ["Yujie Zhao", "Lanxiang Hu", "Yang Wang", "Minmin Hou", "Hao Zhang", "Ke Ding", "Jishen Zhao"], "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs", "comment": null, "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs."}
{"id": "2510.11203", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11203", "abs": "https://arxiv.org/abs/2510.11203", "authors": ["Jiahao Liu", "Bonan Ruan", "Xianglin Yang", "Zhiwei Lin", "Yan Liu", "Yang Wang", "Tao Wei", "Zhenkai Liang"], "title": "TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection", "comment": null, "summary": "LLM-based agents have demonstrated promising adaptability in real-world\napplications. However, these agents remain vulnerable to a wide range of\nattacks, such as tool poisoning and malicious instructions, that compromise\ntheir execution flow and can lead to serious consequences like data breaches\nand financial loss. Existing studies typically attempt to mitigate such\nanomalies by predefining specific rules and enforcing them at runtime to\nenhance safety. Yet, designing comprehensive rules is difficult, requiring\nextensive manual effort and still leaving gaps that result in false negatives.\nAs agent systems evolve into complex software systems, we take inspiration from\nsoftware system security and propose TraceAegis, a provenance-based analysis\nframework that leverages agent execution traces to detect potential anomalies.\nIn particular, TraceAegis constructs a hierarchical structure to abstract\nstable execution units that characterize normal agent behaviors. These units\nare then summarized into constrained behavioral rules that specify the\nconditions necessary to complete a task. By validating execution traces against\nboth hierarchical and behavioral constraints, TraceAegis is able to effectively\ndetect abnormal behaviors. To evaluate the effectiveness of TraceAegis, we\nintroduce TraceAegis-Bench, a dataset covering two representative scenarios:\nhealthcare and corporate procurement. Each scenario includes 1,300 benign\nbehaviors and 300 abnormal behaviors, where the anomalies either violate the\nagent's execution order or break the semantic consistency of its execution\nsequence. Experimental results demonstrate that TraceAegis achieves strong\nperformance on TraceAegis-Bench, successfully identifying the majority of\nabnormal behaviors."}
{"id": "2510.10000", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10000", "abs": "https://arxiv.org/abs/2510.10000", "authors": ["Bach C. Le", "Tung V. Dao", "Binh T. Nguyen", "Hong T. M. Chu"], "title": "Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks", "comment": null, "summary": "Wasserstein distributionally robust optimization (WDRO) provides a framework\nfor adversarial robustness, yet existing methods based on global Lipschitz\ncontinuity or strong duality often yield loose upper bounds or require\nprohibitive computation. In this work, we address these limitations by\nintroducing a primal approach and adopting a notion of exact Lipschitz\ncertificate to tighten this upper bound of WDRO. In addition, we propose a\nnovel Wasserstein distributional attack (WDA) that directly constructs a\ncandidate for the worst-case distribution. Compared to existing point-wise\nattack and its variants, our WDA offers greater flexibility in the number and\nlocation of attack points. In particular, by leveraging the piecewise-affine\nstructure of ReLU networks on their activation cells, our approach results in\nan exact tractable characterization of the corresponding WDRO problem.\nExtensive evaluations demonstrate that our method achieves competitive robust\naccuracy against state-of-the-art baselines while offering tighter certificates\nthan existing methods. Our code is available at\nhttps://github.com/OLab-Repo/WDA"}
{"id": "2510.10649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10649", "abs": "https://arxiv.org/abs/2510.10649", "authors": ["Can Xie", "Ruotong Pan", "Xiangyu Wu", "Yunfei Zhang", "Jiayi Fu", "Tingting Gao", "Guorui Zhou"], "title": "Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant\npromise for enhancing the reasoning capabilities of large language models\n(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage\nsignal across all tokens in a sequence. This coarse-grained approach overlooks\nthe pivotal role of uncertain, high-stakes decisions during reasoning, leading\nto inefficient exploration and the well-documented problem of entropy collapse.\nTo address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a\nmodel-free method that refines credit assignment by leveraging the model's\ninternal uncertainty signals. UCAS operates in two stages: it first modulates\nthe response-level advantage using the model's overall self-confidence, and\nthen applies a token-level penalty based on raw logit certainty. This dual\nmechanism encourages exploration of high-uncertainty paths that yield correct\nanswers while penalizing overconfident yet erroneous reasoning, effectively\nbalancing the exploration-exploitation trade-off. Extensive experiments on five\nmathematical reasoning benchmarks show that UCAS significantly outperforms\nstrong RLVR baselines across multiple model scales, including 1.5B and 7B. Our\nanalysis confirms that UCAS not only achieves higher rewards but also promotes\ngreater reasoning diversity and successfully mitigates entropy collapse."}
{"id": "2510.11246", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11246", "abs": "https://arxiv.org/abs/2510.11246", "authors": ["Pengyu Zhu", "Lijun Li", "Yaxing Lyu", "Li Sun", "Sen Su", "Jing Shao"], "title": "Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems", "comment": null, "summary": "LLM-based multi-agent systems (MAS) demonstrate increasing integration into\nnext-generation applications, but their safety in backdoor attacks remains\nlargely underexplored. However, existing research has focused exclusively on\nsingle-agent backdoor attacks, overlooking the novel attack surfaces introduced\nby agent collaboration in MAS. To bridge this gap, we present the first\nDistributed Backdoor Attack tailored to MAS. We decompose the backdoor into\nmultiple distributed attack primitives that are embedded within MAS tools.\nThese primitives remain dormant individually but collectively activate only\nwhen agents collaborate in a specific sequence, thereby assembling the full\nbackdoor to execute targeted attacks such as data exfiltration. To fully assess\nthis threat, we introduce a benchmark for multi-role collaborative tasks and a\nsandboxed framework to evaluate. Extensive experiments demonstrate that our\nattack achieves an attack success rate exceeding 95% without degrading\nperformance on benign tasks. This work exposes novel backdoor attack surfaces\nthat exploit agent collaboration, underscoring the need to move beyond\nsingle-agent protection. Code and benchmark are available at\nhttps://github.com/whfeLingYu/Distributed-Backdoor-Attacks-in-MAS."}
{"id": "2510.10029", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10029", "abs": "https://arxiv.org/abs/2510.10029", "authors": ["Ruoxing Yang"], "title": "Experience-Efficient Model-Free Deep Reinforcement Learning Using Pre-Training", "comment": null, "summary": "We introduce PPOPT - Proximal Policy Optimization using Pretraining, a novel,\nmodel-free deep-reinforcement-learning algorithm that leverages pretraining to\nachieve high training efficiency and stability on very small training samples\nin physics-based environments. Reinforcement learning agents typically rely on\nlarge samples of environment interactions to learn a policy. However, frequent\ninteractions with a (computer-simulated) environment may incur high\ncomputational costs, especially when the environment is complex. Our main\ninnovation is a new policy neural network architecture that consists of a\npretrained neural network middle section sandwiched between two fully-connected\nnetworks. Pretraining part of the network on a different environment with\nsimilar physics will help the agent learn the target environment with high\nefficiency because it will leverage a general understanding of the\ntransferrable physics characteristics from the pretraining environment. We\ndemonstrate that PPOPT outperforms baseline classic PPO on small training\nsamples both in terms of rewards gained and general training stability. While\nPPOPT underperforms against classic model-based methods such as DYNA DDPG, the\nmodel-free nature of PPOPT allows it to train in significantly less time than\nits model-based counterparts. Finally, we present our implementation of PPOPT\nas open-source software, available at github.com/Davidrxyang/PPOPT."}
{"id": "2510.10675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10675", "abs": "https://arxiv.org/abs/2510.10675", "authors": ["Deven Panchal"], "title": "Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows", "comment": null, "summary": "Generative Agentic AI systems are emerging as a powerful paradigm for\nautomating complex, multi-step tasks. However, many existing frameworks for\nbuilding these systems introduce significant complexity, a steep learning\ncurve, and substantial boilerplate code, hindering rapid prototyping and\ndeployment. This paper introduces simpliflow, a lightweight, open-source Python\nframework designed to address these challenges. simpliflow enables the rapid\ndevelopment and orchestration of linear, deterministic agentic workflows\nthrough a declarative, JSON-based configuration. Its modular architecture\ndecouples agent management, workflow execution, and post-processing, promoting\nease of use and extensibility. By integrating with LiteLLM, it supports over\n100 Large Language Models (LLMs) out-of-the-box. We present the architecture,\noperational flow, and core features of simpliflow, demonstrating its utility\nthrough diverse use cases ranging from software development simulation to\nreal-time system interaction. A comparative analysis with prominent frameworks\nlike LangChain and AutoGen highlights simpliflow's unique position as a tool\noptimized for simplicity, control, and speed in deterministic workflow\nenvironments."}
{"id": "2510.11251", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11251", "abs": "https://arxiv.org/abs/2510.11251", "authors": ["Rui Xu", "Jiawei Chen", "Zhaoxia Yin", "Cong Kong", "Xinpeng Zhang"], "title": "Large Language Models Are Effective Code Watermarkers", "comment": null, "summary": "The widespread use of large language models (LLMs) and open-source code has\nraised ethical and security concerns regarding the distribution and attribution\nof source code, including unauthorized redistribution, license violations, and\nmisuse of code for malicious purposes. Watermarking has emerged as a promising\nsolution for source attribution, but existing techniques rely heavily on\nhand-crafted transformation rules, abstract syntax tree (AST) manipulation, or\ntask-specific training, limiting their scalability and generality across\nlanguages. Moreover, their robustness against attacks remains limited. To\naddress these limitations, we propose CodeMark-LLM, an LLM-driven watermarking\nframework that embeds watermark into source code without compromising its\nsemantics or readability. CodeMark-LLM consists of two core components: (i)\nSemantically Consistent Embedding module that applies functionality-preserving\ntransformations to encode watermark bits, and (ii) Differential Comparison\nExtraction module that identifies the applied transformations by comparing the\noriginal and watermarked code. Leveraging the cross-lingual generalization\nability of LLM, CodeMark-LLM avoids language-specific engineering and training\npipelines. Extensive experiments across diverse programming languages and\nattack scenarios demonstrate its robustness, effectiveness, and scalability."}
{"id": "2510.10041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10041", "abs": "https://arxiv.org/abs/2510.10041", "authors": ["Sahng-Min Han", "Minjae Kim", "Jinho Cha", "Se-woon Choe", "Eunchan Daniel Cha", "Jungwon Choi", "Kyudong Jung"], "title": "FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis", "comment": "35 pages, 11 figures, submitted to Computers in Biology and Medicine\n  (Elsevier, under review)", "summary": "Deep learning in small and imbalanced biomedical datasets remains\nfundamentally constrained by unstable optimization and poor generalization. We\npresent the first biomedical implementation of FOSSIL (Flexible Optimization\nvia Sample-Sensitive Importance Learning), a regret-minimizing weighting\nframework that adaptively balances training emphasis according to sample\ndifficulty. Using softmax-based uncertainty as a continuous measure of\ndifficulty, we construct a four-stage curriculum (Easy-Very Hard) and integrate\nFOSSIL into both convolutional and transformer-based architectures for Mpox\nskin lesion diagnosis. Across all settings, FOSSIL substantially improves\ndiscrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under\nreal-world perturbations, outperforming conventional baselines without\nmetadata, manual curation, or synthetic augmentation. The results position\nFOSSIL as a generalizable, data-efficient, and interpretable framework for\ndifficulty-aware learning in medical imaging under data scarcity."}
{"id": "2510.10813", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10813", "abs": "https://arxiv.org/abs/2510.10813", "authors": ["Enric Junque de Fortuny", "Veronica Roberta Cappelli"], "title": "LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to domains that require\nreasoning about other agents' behavior, such as negotiation, policy design, and\nmarket simulation, yet existing research has mostly evaluated their adherence\nto equilibrium play or their exhibited depth of reasoning. Whether they display\ngenuine strategic thinking, understood as the coherent formation of beliefs\nabout other agents, evaluation of possible actions, and choice based on those\nbeliefs, remains unexplored. We develop a framework to identify this ability by\ndisentangling beliefs, evaluation, and choice in static, complete-information\ngames, and apply it across a series of non-cooperative environments. By jointly\nanalyzing models' revealed choices and reasoning traces, and introducing a new\ncontext-free game to rule out imitation from memorization, we show that current\nfrontier models exhibit belief-coherent best-response behavior at targeted\nreasoning depths. When unconstrained, they self-limit their depth of reasoning\nand form differentiated conjectures about human and synthetic opponents,\nrevealing an emergent form of meta-reasoning. Under increasing complexity,\nexplicit recursion gives way to internally generated heuristic rules of choice\nthat are stable, model-specific, and distinct from known human biases. These\nfindings indicate that belief coherence, meta-reasoning, and novel heuristic\nformation can emerge jointly from language modeling objectives, providing a\nstructured basis for the study of strategic cognition in artificial agents."}
{"id": "2510.11301", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11301", "abs": "https://arxiv.org/abs/2510.11301", "authors": ["Junhua Zhou", "Quanjun Li", "Weixuan Li", "Guang Yu", "Yihua Shao", "Yihang Dong", "Mengqian Wang", "Zimeng Li", "Changwei Gong", "Xuhang Chen"], "title": "TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical Image Security", "comment": "Accepted By BIBM 2025", "summary": "The rise of digital medical imaging, like MRI and CT, demands strong\nencryption to protect patient data in telemedicine and cloud storage. Chaotic\nsystems are popular for image encryption due to their sensitivity and unique\ncharacteristics, but existing methods often lack sufficient security. This\npaper presents the Three-dimensional Diffusion Algorithm and Deep Learning\nImage Encryption system (TDADL-IE), built on three key elements. First, we\npropose an enhanced chaotic generator using an LSTM network with a 1D-Sine\nQuadratic Chaotic Map (1D-SQCM) for better pseudorandom sequence generation.\nNext, a new three-dimensional diffusion algorithm (TDA) is applied to encrypt\npermuted images. TDADL-IE is versatile for images of any size. Experiments\nconfirm its effectiveness against various security threats. The code is\navailable at\n\\href{https://github.com/QuincyQAQ/TDADL-IE}{https://github.com/QuincyQAQ/TDADL-IE}."}
{"id": "2510.10057", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10057", "abs": "https://arxiv.org/abs/2510.10057", "authors": ["Lei Gao", "Shihong Huang", "Shengjie Wang", "Hong Ma", "Feng Zhang", "Hengda Bao", "Qichang Chen", "Weihua Zhou"], "title": "One4Many-StablePacker: An Efficient Deep Reinforcement Learning Framework for the 3D Bin Packing Problem", "comment": null, "summary": "The three-dimensional bin packing problem (3D-BPP) is widely applied in\nlogistics and warehousing. Existing learning-based approaches often neglect\npractical stability-related constraints and exhibit limitations in generalizing\nacross diverse bin dimensions. To address these limitations, we propose a novel\ndeep reinforcement learning framework, One4Many-StablePacker (O4M-SP). The\nprimary advantage of O4M-SP is its ability to handle various bin dimensions in\na single training process while incorporating support and weight constraints\ncommon in practice. Our training method introduces two innovative mechanisms.\nFirst, it employs a weighted reward function that integrates loading rate and a\nnew height difference metric for packing layouts, promoting improved bin\nutilization through flatter packing configurations. Second, it combines clipped\npolicy gradient optimization with a tailored policy drifting method to mitigate\npolicy entropy collapse, encouraging exploration at critical decision nodes\nduring packing to avoid suboptimal solutions. Extensive experiments demonstrate\nthat O4M-SP generalizes successfully across diverse bin dimensions and\nsignificantly outperforms baseline methods. Furthermore, O4M-SP exhibits strong\npractical applicability by effectively addressing packing scenarios with\nstability constraints."}
{"id": "2510.10895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10895", "abs": "https://arxiv.org/abs/2510.10895", "authors": ["Renxuan Tan", "Rongpeng Li", "Fei Wang", "Chenghui Peng", "Shaoyun Wu", "Zhifeng Zhao", "Honggang Zhang"], "title": "LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Medium Access Control (MAC) protocols, essential for wireless networks, are\ntypically manually configured. While deep reinforcement learning (DRL)-based\nprotocols enhance task-specified network performance, they suffer from poor\ngeneralizability and resilience, demanding costly retraining to adapt to\ndynamic environments. To overcome this limitation, we introduce a\ngame-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the\nuplink transmission between a base station and a varying number of user\nequipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),\ncapturing the network's natural hierarchical structure. Within this game,\nLLM-driven agents, coordinated through proximal policy optimization (PPO),\nsynthesize adaptive, semantic MAC protocols in response to network dynamics.\nProtocol action grammar (PAG) is employed to ensure the reliability and\nefficiency of this process. Under this system, we further analyze the existence\nand convergence behavior in terms of a Stackelberg equilibrium by studying the\nlearning dynamics of LLM-empowered unified policies in response to changing\nfollowers. Simulations corroborate that our framework achieves a 77.6% greater\nthroughput and a 65.2% fairness improvement over conventional baselines.\nBesides, our framework generalizes excellently to a fluctuating number of users\nwithout requiring retraining or architectural changes."}
{"id": "2510.11414", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11414", "abs": "https://arxiv.org/abs/2510.11414", "authors": ["Charles Fleming", "Ashish Kundu", "Ramana Kompella"], "title": "Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems using an LLM-Judged TBAC Model", "comment": null, "summary": "The proliferation of autonomous AI agents within enterprise environments\nintroduces a critical security challenge: managing access control for emergent,\nnovel tasks for which no predefined policies exist. This paper introduces an\nadvanced security framework that extends the Task-Based Access Control (TBAC)\nmodel by using a Large Language Model (LLM) as an autonomous, risk-aware judge.\nThis model makes access control decisions not only based on an agent's intent\nbut also by explicitly considering the inherent \\textbf{risk associated with\ntarget resources} and the LLM's own \\textbf{model uncertainty} in its\ndecision-making process. When an agent proposes a novel task, the LLM judge\nsynthesizes a just-in-time policy while also computing a composite risk score\nfor the task and an uncertainty estimate for its own reasoning. High-risk or\nhigh-uncertainty requests trigger more stringent controls, such as requiring\nhuman approval. This dual consideration of external risk and internal\nconfidence allows the model to enforce a more robust and adaptive version of\nthe principle of least privilege, paving the way for safer and more trustworthy\nautonomous systems."}
{"id": "2510.10278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10278", "abs": "https://arxiv.org/abs/2510.10278", "authors": ["Christopher Chiu", "Silviu Pitis", "Mihaela van der Schaar"], "title": "Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models", "comment": null, "summary": "Clinical reasoning in medicine is a hypothesis-driven process where\nphysicians refine diagnoses from limited information through targeted history,\nphysical examination, and diagnostic investigations. In contrast, current\nmedical benchmarks for large language models (LLMs) primarily assess knowledge\nrecall through single-turn questions, where complete clinical information is\nprovided upfront. To address this gap, we introduce VivaBench, a multi-turn\nbenchmark that evaluates sequential clinical reasoning in LLM agents. Our\ndataset consists of 1762 physician-curated clinical vignettes structured as\ninteractive scenarios that simulate a (oral) examination in medical training,\nrequiring agents to actively probe for relevant findings, select appropriate\ninvestigations, and synthesize information across multiple steps to reach a\ndiagnosis. While current LLMs demonstrate competence in diagnosing conditions\nfrom well-described clinical presentations, their performance degrades\nsignificantly when required to navigate iterative diagnostic reasoning under\nuncertainty in our evaluation. Our analysis identified several failure modes\nthat mirror common cognitive errors in clinical practice, including: (1)\nfixation on initial hypotheses, (2) inappropriate investigation ordering, (3)\npremature diagnostic closure, and (4) failing to screen for critical\nconditions. These patterns reveal fundamental limitations in how current LLMs\nreason and make decisions under uncertainty. Through VivaBench, we provide a\nstandardized benchmark for evaluating conversational medical AI systems for\nreal-world clinical decision support. Beyond medical applications, we\ncontribute to the larger corpus of research on agentic AI by demonstrating how\nsequential reasoning trajectories can diverge in complex decision-making\nenvironments."}
{"id": "2510.10909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10909", "abs": "https://arxiv.org/abs/2510.10909", "authors": ["Daoyu Wang", "Mingyue Cheng", "Qi Liu", "Shuo Yu", "Zirui Liu", "Ze Guo"], "title": "PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature", "comment": "12 pages, 9 figures", "summary": "Understanding and reasoning on the web-scale scientific literature is a\ncrucial touchstone for large language model (LLM) based agents designed to\nsupport complex knowledge-intensive tasks. However, existing works are mainly\nrestricted to tool-free tasks within isolated papers, largely due to the lack\nof a benchmark for cross-paper reasoning and multi-tool orchestration in real\nresearch scenarios. In this work, we propose PaperArena, an evaluation\nbenchmark for agents to address real-world research questions that typically\nrequire integrating information across multiple papers with the assistance of\nexternal tools. Given a research question, agents should integrate diverse\nformats across multiple papers through reasoning and interacting with\nappropriate tools, thereby producing a well-grounded answer. To support\nstandardized evaluation, we provide a modular and extensible platform for agent\nexecution, offering tools such as multimodal parsing, context retrieval, and\nprogrammatic computation. Experimental results reveal that even the most\nadvanced LLM powering a well-established agent system achieves merely 38.78%\naverage accuracy. On the hard subset, accuracy drops to only 18.47%,\nhighlighting great potential for improvement. We also present several empirical\nfindings, including that all agents tested exhibit inefficient tool usage,\noften invoking more tools than necessary to solve a task. We invite the\ncommunity to adopt PaperArena to develop and evaluate more capable agents for\nscientific discovery. Our code and data are available\nhttps://github.com/Melmaphother/PaperArena."}
{"id": "2510.11570", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11570", "abs": "https://arxiv.org/abs/2510.11570", "authors": ["Shuo Chen", "Zhen Han", "Haokun Chen", "Bailan He", "Shengyun Si", "Jingpei Wu", "Philip Torr", "Volker Tresp", "Jindong Gu"], "title": "Bag of Tricks for Subverting Reasoning-based Safety Guardrails", "comment": "OpenAI Red-teaming Challenge Winner and Oral Presentation", "summary": "Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),\nsuch as deliberative alignment, have shown strong defense against jailbreak\nattacks. By leveraging LRMs' reasoning ability, these guardrails help the\nmodels to assess the safety of user inputs before generating final responses.\nThe powerful reasoning ability can analyze the intention of the input query and\nwill refuse to assist once it detects the harmful intent hidden by the\njailbreak methods. Such guardrails have shown a significant boost in defense,\nsuch as the near-perfect refusal rates on the open-source gpt-oss series.\nUnfortunately, we find that these powerful reasoning-based guardrails can be\nextremely vulnerable to subtle manipulation of the input prompts, and once\nhijacked, can lead to even more harmful results. Specifically, we first uncover\na surprisingly fragile aspect of these guardrails: simply adding a few template\ntokens to the input prompt can successfully bypass the seemingly powerful\nguardrails and lead to explicit and harmful responses. To explore further, we\nintroduce a bag of jailbreak methods that subvert the reasoning-based\nguardrails. Our attacks span white-, gray-, and black-box settings and range\nfrom effortless template manipulations to fully automated optimization. Along\nwith the potential for scalable implementation, these methods also achieve\nalarmingly high attack success rates (e.g., exceeding 90% across 5 different\nbenchmarks on gpt-oss series on both local host models and online API\nservices). Evaluations across various leading open-source LRMs confirm that\nthese vulnerabilities are systemic, underscoring the urgent need for stronger\nalignment techniques for open-sourced LRMs to prevent malicious misuse. Code is\nopen-sourced at https://chenxshuo.github.io/bag-of-tricks."}
{"id": "2510.10304", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10304", "abs": "https://arxiv.org/abs/2510.10304", "authors": ["Michael Y. Hu", "Benjamin Van Durme", "Jacob Andreas", "Harsh Jhamtani"], "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "comment": null, "summary": "Language model (LM) agents deployed in novel environments often exhibit poor\nsample efficiency when learning from sequential interactions. This\nsignificantly hinders the usefulness of such agents in environments where\ninteraction is costly (for example, when they interact with humans or reset\nphysical systems). While a number of existing LM agent architectures\nincorporate various mechanisms for experience storage and reflection, they make\nlimited use of LMs' abilities to directly generate or reason about full\ncounterfactual trajectories. We introduce ECHO (Experience Consolidation via\nHindsight Optimization), a prompting framework that adapts hindsight experience\nreplay from reinforcement learning for language model agents. ECHO generates\noptimized trajectories for alternative goals that could have been achieved\nduring failed attempts, effectively creating synthetic positive examples from\nunsuccessful interactions. Our approach consists of two components: a hindsight\nrule that uses the language model itself to identify relevant subgoals and\ngenerate optimized trajectories, and an update rule that maintains compressed\ntrajectory representations in memory. We evaluate ECHO on stateful versions of\nXMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a\ncollaborative information-gathering enterprise simulation. Across both domains,\nECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,\nit also outperforms a number of sophisticated agent architectures including\nReflexion and AWM, demonstrating faster adaptation to novel environments\nthrough more effective utilization of past experiences."}
{"id": "2510.10931", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10931", "abs": "https://arxiv.org/abs/2510.10931", "authors": ["SHengjie Ma", "Chenlong Deng", "Jiaxin Mao", "Jiadeng Huang", "Teng Wang", "Junjie Wu", "Changwang Zhang", "Jun wang"], "title": "PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents", "comment": null, "summary": "Retrieval-augmented generation (RAG) agents, such as recent\nDeepResearch-style systems, extend large language models (LLMs) with autonomous\ninformation-seeking capabilities through external tools. While reinforcement\nlearning (RL) has enabled impressive multi-step reasoning, we identify a\npreviously overlooked failure mode, Tool-Call Hacking, where agents inflate\nreward signals by issuing superficially correct tool calls without genuinely\nleveraging the retrieved evidence. This results in (i) mode collapse into\nrepetitive reliance on a single source and (ii) spurious grounding, where\nanswers are only weakly supported by cited content.\n  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL\nframework that enforces verifiable causal links between retrieved evidence,\nreasoning traces, and final answers. PoU operationalizes this through a unified\nstep-wise contract combining syntactic citation validation, perturbation-based\nsensitivity rewards, and answer-evidence alignment objectives, ensuring that\ntool usage remains both interpretable and functionally grounded.\n  Across seven QA benchmarks spanning in-domain, out-of-domain, and\nout-of-tool-distribution settings, PoU consistently outperforms strong\nDeepResearch baselines in factual accuracy, evidence faithfulness, and\ntool-routing balance. These findings highlight the necessity of grounding\nRL-trained agents not merely in task outcomes but in the causal use of\nretrieved information, offering a principled path toward trustworthy\nretrieval-augmented reasoning."}
{"id": "2510.11688", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11688", "abs": "https://arxiv.org/abs/2510.11688", "authors": ["Zicheng Liu", "Lige Huang", "Jie Zhang", "Dongrui Liu", "Yuan Tian", "Jing Shao"], "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities", "comment": "Project webpage available at https://pacebench.github.io/", "summary": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models."}
{"id": "2510.10364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10364", "abs": "https://arxiv.org/abs/2510.10364", "authors": ["Ali Mirzazadeh", "Simon Cadavid", "Kaiwen Zha", "Chao Li", "Sultan Alzahrani", "Manar Alawajy", "Joshua Korzenik", "Kreshnik Hoti", "Charles Reynolds", "David Mischoulon", "John Winkelman", "Maurizio Fava", "Dina Katabi"], "title": "Transformer Model Detects Antidepressant Use From a Single Night of Sleep, Unlocking an Adherence Biomarker", "comment": null, "summary": "Antidepressant nonadherence is pervasive, driving relapse, hospitalization,\nsuicide risk, and billions in avoidable costs. Clinicians need tools that\ndetect adherence lapses promptly, yet current methods are either invasive\n(serum assays, neuroimaging) or proxy-based and inaccurate (pill counts,\npharmacy refills). We present the first noninvasive biomarker that detects\nantidepressant intake from a single night of sleep. A transformer-based model\nanalyzes sleep data from a consumer wearable or contactless wireless sensor to\ninfer antidepressant intake, enabling remote, effortless, daily adherence\nassessment at home. Across six datasets comprising 62,000 nights from >20,000\nparticipants (1,800 antidepressant users), the biomarker achieved AUROC = 0.84,\ngeneralized across drug classes, scaled with dose, and remained robust to\nconcomitant psychotropics. Longitudinal monitoring captured real-world\ninitiation, tapering, and lapses. This approach offers objective, scalable\nadherence surveillance with potential to improve depression care and outcomes."}
{"id": "2510.11085", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11085", "abs": "https://arxiv.org/abs/2510.11085", "authors": ["Yuxinyue Qian", "Jun Liu"], "title": "Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States", "comment": null, "summary": "With the rapid development of artificial intelligence (AI) technology,\nsocio-economic systems are entering a new stage of \"human-AI co-creation.\"\nBuilding upon a previously established multi-level intelligent agent economic\nmodel, this paper conducts simulation-based comparisons of macroeconomic output\nevolution in China and the United States under different mechanisms-AI\ncollaboration, network effects, and AI autonomous production. The results show\nthat: (1) when AI functions as an independent productive entity, the overall\ngrowth rate of social output far exceeds that of traditional human-labor-based\nmodels; (2) China demonstrates clear potential for acceleration in both the\nexpansion of intelligent agent populations and the pace of technological\ncatch-up, offering the possibility of achieving technological convergence or\neven partial surpassing. This study provides a systematic, model-based\nanalytical framework for understanding AI-driven production system\ntransformation and shifts in international competitiveness, as well as\nquantitative insights for relevant policy formulation."}
{"id": "2510.10625", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10625", "abs": "https://arxiv.org/abs/2510.10625", "authors": ["Yuval Golbari", "Navve Wasserman", "Gal Vardi", "Michal Irani"], "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios", "comment": null, "summary": "Determining which data samples were used to train a model-known as Membership\nInference Attack (MIA)-is a well-studied and important problem with\nimplications for data privacy. Black-box methods presume access only to the\nmodel's outputs and often rely on training auxiliary reference models. While\nthey have shown strong empirical performance, they rely on assumptions that\nrarely hold in real-world settings: (i) the attacker knows the training\nhyperparameters; (ii) all available non-training samples come from the same\ndistribution as the training data; and (iii) the fraction of training data in\nthe evaluation set is known. In this paper, we demonstrate that removing these\nassumptions leads to a significant drop in the performance of black-box\nattacks. We introduce ImpMIA, a Membership Inference Attack that exploits the\nImplicit Bias of neural networks, hence removes the need to rely on any\nreference models and their assumptions. ImpMIA is a white-box attack -- a\nsetting which assumes access to model weights and is becoming increasingly\nrealistic given that many models are publicly available (e.g., via Hugging\nFace). Building on maximum-margin implicit bias theory, ImpMIA uses the\nKarush-Kuhn-Tucker (KKT) optimality conditions to identify training samples.\nThis is done by finding the samples whose gradients most strongly reconstruct\nthe trained model's parameters. As a result, ImpMIA achieves state-of-the-art\nperformance compared to both black and white box attacks in realistic settings\nwhere only the model weights and a superset of the training data are available."}
{"id": "2510.10433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10433", "abs": "https://arxiv.org/abs/2510.10433", "authors": ["Zixiang Xu", "Menghui Zhou", "Jun Qi", "Xuanhan Fan", "Yun Yang", "Po Yang"], "title": "Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression", "comment": null, "summary": "Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in\naging populations, posing a significant and escalating burden on global\nhealthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful\ncomputational paradigm for modeling longitudinal AD data, existing frameworks\ndo not account for the time-varying nature of feature correlations. To address\nthis limitation, we propose a novel MTL framework, named Feature Similarity\nLaplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel\nFeature Similarity Laplacian (FSL) penalty that explicitly models the\ntime-varying relationships between features. By simultaneously considering\ntemporal smoothness among tasks and the dynamic correlations among features,\nour model enhances both predictive accuracy and biological interpretability. To\nsolve the non-smooth optimization problem arising from our proposed penalty\nterms, we adopt the Alternating Direction Method of Multipliers (ADMM)\nalgorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework\nachieves state-of-the-art performance, outperforming various baseline methods.\nThe implementation source can be found at https://github.com/huatxxx/MTL-FSL."}
{"id": "2510.11143", "categories": ["cs.AI", "cs.HC", "68U35, 62P30", "I.2.2"], "pdf": "https://arxiv.org/pdf/2510.11143", "abs": "https://arxiv.org/abs/2510.11143", "authors": ["Chuke Chen", "Biao Luo", "Nan Li", "Boxiang Wang", "Hang Yang", "Jing Guo", "Ming Xu"], "title": "Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis", "comment": "19 pages,5 figures", "summary": "The rapid expansion of scientific data has widened the gap between analytical\ncapability and research intent. Existing AI-based analysis tools, ranging from\nAutoML frameworks to agentic research assistants, either favor automation over\ntransparency or depend on manual scripting that hinders scalability and\nreproducibility. We present ARIA (Automated Research Intelligence Assistant), a\nspec-driven, human-in-the-loop framework for automated and interpretable data\nanalysis. ARIA integrates six interoperable layers, namely Command, Context,\nCode, Data, Orchestration, and AI Module, within a document-centric workflow\nthat unifies human reasoning and machine execution. Through natural-language\nspecifications, researchers define analytical goals while ARIA autonomously\ngenerates executable code, validates computations, and produces transparent\ndocumentation. Beyond achieving high predictive accuracy, ARIA can rapidly\nidentify optimal feature sets and select suitable models, minimizing redundant\ntuning and repetitive experimentation. In the Boston Housing case, ARIA\ndiscovered 25 key features and determined XGBoost as the best performing model\n(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous\ndomains demonstrate ARIA's strong performance, interpretability, and efficiency\ncompared with state-of-the-art systems. By combining AI for research and AI for\nscience principles within a spec-driven architecture, ARIA establishes a new\nparadigm for transparent, collaborative, and reproducible scientific discovery."}
{"id": "2510.10937", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10937", "abs": "https://arxiv.org/abs/2510.10937", "authors": ["Qizhou Peng", "Yang Zheng", "Yu Wen", "Yanna Wu", "Yingying Du"], "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems", "comment": null, "summary": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems."}
{"id": "2510.10446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10446", "abs": "https://arxiv.org/abs/2510.10446", "authors": ["Masoud Makrehchi"], "title": "Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation", "comment": "10 pages", "summary": "We analyze a reversed-supervision strategy that searches over labelings of a\nlarge unlabeled set \\(B\\) to minimize error on a small labeled set \\(A\\). The\nsearch space is \\(2^n\\), and the resulting complexity remains exponential even\nunder large constant-factor speedups (e.g., quantum or massively parallel\nhardware). Consequently, arbitrarily fast -- but not exponentially faster --\ncomputation does not obviate the need for informative labels or priors. In\npractice, the machine learning pipeline still requires an initial human\ncontribution: specifying the objective, defining classes, and providing a seed\nset of representative annotations that inject inductive bias and align models\nwith task semantics. Synthetic labels from generative AI can partially\nsubstitute provided their quality is human-grade and anchored by a\nhuman-specified objective, seed supervision, and validation. In this view,\ngenerative models function as \\emph{label amplifiers}, leveraging small\nhuman-curated cores via active, semi-supervised, and self-training loops, while\nhumans retain oversight for calibration, drift detection, and failure auditing.\nThus, extreme computational speed reduces wall-clock time but not the\nfundamental supervision needs of learning; initial human (or human-grade) input\nremains necessary to ground the system in the intended task."}
{"id": "2510.11144", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11144", "abs": "https://arxiv.org/abs/2510.11144", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "title": "$How^{2}$: How to learn from procedural How-to questions", "comment": null, "summary": "An agent facing a planning problem can use answers to how-to questions to\nreduce uncertainty and fill knowledge gaps, helping it solve both current and\nfuture tasks. However, their open ended nature, where valid answers to \"How do\nI X?\" range from executable actions to high-level descriptions of X's\nsub-goals, makes them challenging for AI agents to ask, and for AI experts to\nanswer, in ways that support efficient planning. We introduce $How^{2}$, a\nmemory agent framework that enables agents to ask how-to questions, store the\nanswers, and reuse them for lifelong learning in interactive environments. We\nevaluate our approach in Plancraft, a Minecraft crafting environment, where\nagents must complete an assembly task by manipulating inventory items. Using\nteacher models that answer at varying levels of abstraction, from executable\naction sequences to high-level subgoal descriptions, we show that lifelong\nlearning agents benefit most from answers that are abstracted and decoupled\nfrom the current state. $How^{2}$ offers a way for LLM-based agents to improve\ntheir planning capabilities over time by asking questions in interactive\nenvironments."}
{"id": "2510.11108", "categories": ["cs.MA", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11108", "abs": "https://arxiv.org/abs/2510.11108", "authors": ["Xinfeng Li", "Dong Huang", "Jie Li", "Hongyi Cai", "Zhenhong Zhou", "Wei Dong", "XiaoFeng Wang", "Yang Liu"], "title": "A Vision for Access Control in LLM-based Agent Systems", "comment": "10 pages, 1 figure", "summary": "The autonomy and contextual complexity of LLM-based agents render traditional\naccess control (AC) mechanisms insufficient. Static, rule-based systems\ndesigned for predictable environments are fundamentally ill-equipped to manage\nthe dynamic information flows inherent in agentic interactions. This position\npaper argues for a paradigm shift from binary access control to a more\nsophisticated model of information governance, positing that the core challenge\nis not merely about permission, but about governing the flow of information. We\nintroduce Agent Access Control (AAC), a novel framework that reframes AC as a\ndynamic, context-aware process of information flow governance. AAC operates on\ntwo core modules: (1) multi-dimensional contextual evaluation, which assesses\nnot just identity but also relationships, scenarios, and norms; and (2)\nadaptive response formulation, which moves beyond simple allow/deny decisions\nto shape information through redaction, summarization, and paraphrasing. This\nvision, powered by a dedicated AC reasoning engine, aims to bridge the gap\nbetween human-like nuanced judgment and scalable Al safety, proposing a new\nconceptual lens for future research in trustworthy agent design."}
{"id": "2510.10451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10451", "abs": "https://arxiv.org/abs/2510.10451", "authors": ["Keisuke Fujii", "Kazushi Tsutsui", "Yu Teshima", "Makoto Itoh", "Naoya Takeishi", "Nozomi Nishiumi", "Ryoya Tanaka", "Shunsuke Shigaki", "Yoshinobu Kawahara"], "title": "Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning", "comment": "21 pages, 7 figures", "summary": "Simulators of animal movements play a valuable role in studying behavior.\nAdvances in imitation learning for robotics have expanded possibilities for\nreproducing human and animal movements. A key challenge for realistic\nmulti-animal simulation in biology is bridging the gap between unknown\nreal-world transition models and their simulated counterparts. Because\nlocomotion dynamics are seldom known, relying solely on mathematical models is\ninsufficient; constructing a simulator that both reproduces real trajectories\nand supports reward-driven optimization remains an open problem. We introduce a\ndata-driven simulator for multi-animal behavior based on deep reinforcement\nlearning and counterfactual simulation. We address the ill-posed nature of the\nproblem caused by high degrees of freedom in locomotion by estimating movement\nvariables of an incomplete transition model as actions within an RL framework.\nWe also employ a distance-based pseudo-reward to align and compare states\nbetween cyber and physical spaces. Validated on artificial agents, flies,\nnewts, and silkmoth, our approach achieves higher reproducibility of\nspecies-specific behaviors and improved reward acquisition compared with\nstandard imitation and RL methods. Moreover, it enables counterfactual behavior\nprediction in novel experimental settings and supports multi-individual\nmodeling for flexible what-if trajectory generation, suggesting its potential\nto simulate and elucidate complex multi-animal behaviors."}
{"id": "2510.11281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11281", "abs": "https://arxiv.org/abs/2510.11281", "authors": ["Deepeka Garg", "Sihan Zeng", "Annapoorani L. Narayanan", "Sumitra Ganesh", "Leo Ardon"], "title": "PADME: Procedure Aware DynaMic Execution", "comment": null, "summary": "Learning to autonomously execute long-horizon procedures from natural\nlanguage remains a core challenge for intelligent agents. Free-form\ninstructions such as recipes, scientific protocols, or business workflows\nencode rich procedural knowledge, but their variability and lack of structure\ncause agents driven by large language models (LLMs) to drift or fail during\nexecution. We introduce Procedure Aware DynaMic Execution (PADME), an agent\nframework that produces and exploits a graph-based representation of\nprocedures. Unlike prior work that relies on manual graph construction or\nunstructured reasoning, PADME autonomously transforms procedural text into\nexecutable graphs that capture task dependencies, decision points, and reusable\nsubroutines. Central to PADME is a two-phase methodology; Teach phase, which\nfocuses on systematic structuring, enrichment with executable logic of\nprocedures, followed by Execute phase, which enables dynamic execution in\nresponse to real-time inputs and environment feedback. This separation ensures\nquality assurance and scalability, allowing expert knowledge to be encoded once\nand reliably reused across varying contexts. The graph representation also\nprovides an inductive bias that reduces error accumulation in long-horizon\nreasoning, underscoring the importance of structured procedure modeling for\nreliable agent-driven automation. Empirically, PADME achieves state-of-the-art\nperformance on four diverse benchmarks, including ALFWorld and ScienceWorld.\nThese results demonstrate that agents equipped with graph-based procedure\nrepresentations offer a powerful intermediate abstraction for robust and\ngeneralizable execution."}
{"id": "2510.10570", "categories": ["cs.LG", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10570", "abs": "https://arxiv.org/abs/2510.10570", "authors": ["Zirui Wan", "Stefan Vlaski"], "title": "Multitask Learning with Learned Task Relationships", "comment": null, "summary": "Classical consensus-based strategies for federated and decentralized learning\nare statistically suboptimal in the presence of heterogeneous local data or\ntask distributions. As a result, in recent years, there has been growing\ninterest in multitask or personalized strategies, which allow individual agents\nto benefit from one another in pursuing locally optimal models without\nenforcing consensus. Existing strategies require either precise prior knowledge\nof the underlying task relationships or are fully non-parametric and instead\nrely on meta-learning or proximal constructions. In this work, we introduce an\nalgorithmic framework that strikes a balance between these extremes. By\nmodeling task relationships through a Gaussian Markov Random Field with an\nunknown precision matrix, we develop a strategy that jointly learns both the\ntask relationships and the local models, allowing agents to self-organize in a\nway consistent with their individual data distributions. Our theoretical\nanalysis quantifies the quality of the learned relationship, and our numerical\nexperiments demonstrate its practical effectiveness."}
{"id": "2510.11290", "categories": ["cs.AI", "cs.HC", "I.2.6; J.4"], "pdf": "https://arxiv.org/pdf/2510.11290", "abs": "https://arxiv.org/abs/2510.11290", "authors": ["Sheng Jin", "Haoming Wang", "Zhiqi Gao", "Yongbo Yang", "Bao Chunjia", "Chengliang Wang"], "title": "Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics", "comment": "9 pages, 7 figures, EMNLP conference", "summary": "Large language models (LLMs) based Agents are increasingly pivotal in\nsimulating and understanding complex human systems and interactions. We propose\nthe AI-Agent School (AAS) system, built around a self-evolving mechanism that\nleverages agents for simulating complex educational dynamics. Addressing the\nfragmented issues in teaching process modeling and the limitations of agents\nperformance in simulating diverse educational participants, AAS constructs the\nZero-Exp strategy, employs a continuous \"experience-reflection-optimization\"\ncycle, grounded in a dual memory base comprising experience and knowledge bases\nand incorporating short-term and long-term memory components. Through this\nmechanism, agents autonomously evolve via situated interactions within diverse\nsimulated school scenarios. This evolution enables agents to more accurately\nmodel the nuanced, multi-faceted teacher-student engagements and underlying\nlearning processes found in physical schools. Experiment confirms that AAS can\neffectively simulate intricate educational dynamics and is effective in\nfostering advanced agent cognitive abilities, providing a foundational stepping\nstone from the \"Era of Experience\" to the \"Era of Simulation\" by generating\nhigh-fidelity behavioral and interaction data."}
{"id": "2510.10586", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10586", "abs": "https://arxiv.org/abs/2510.10586", "authors": ["Giulio Ruffini"], "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents", "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)", "summary": "In the algorithmic (Kolmogorov) view, agents are programs that track and\ncompress sensory streams using generative programs. We propose a framework\nwhere the relevant structural prior is simplicity (Solomonoff) understood as\n\\emph{compositional symmetry}: natural streams are well described by (local)\nactions of finite-parameter Lie pseudogroups on geometrically and topologically\ncomplex low-dimensional configuration manifolds (latent spaces). Modeling the\nagent as a generic neural dynamical system coupled to such streams, we show\nthat accurate world-tracking imposes (i) \\emph{structural constraints} --\nequivariance of the agent's constitutive equations and readouts -- and (ii)\n\\emph{dynamical constraints}: under static inputs, symmetry induces conserved\nquantities (Noether-style labels) in the agent dynamics and confines\ntrajectories to reduced invariant manifolds; under slow drift, these manifolds\nmove but remain low-dimensional. This yields a hierarchy of reduced manifolds\naligned with the compositional factorization of the pseudogroup, providing a\ngeometric account of the ``blessing of compositionality'' in deep models. We\nconnect these ideas to the Spencer formalism for Lie pseudogroups and formulate\na symmetry-based, self-contained version of predictive coding in which higher\nlayers receive only \\emph{coarse-grained residual transformations}\n(prediction-error coordinates) along symmetry directions unresolved at lower\nlayers."}
{"id": "2510.11558", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11558", "abs": "https://arxiv.org/abs/2510.11558", "authors": ["Komal Gupta", "Aditya Shrivastava"], "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products", "comment": null, "summary": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta."}
{"id": "2510.10625", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10625", "abs": "https://arxiv.org/abs/2510.10625", "authors": ["Yuval Golbari", "Navve Wasserman", "Gal Vardi", "Michal Irani"], "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios", "comment": null, "summary": "Determining which data samples were used to train a model-known as Membership\nInference Attack (MIA)-is a well-studied and important problem with\nimplications for data privacy. Black-box methods presume access only to the\nmodel's outputs and often rely on training auxiliary reference models. While\nthey have shown strong empirical performance, they rely on assumptions that\nrarely hold in real-world settings: (i) the attacker knows the training\nhyperparameters; (ii) all available non-training samples come from the same\ndistribution as the training data; and (iii) the fraction of training data in\nthe evaluation set is known. In this paper, we demonstrate that removing these\nassumptions leads to a significant drop in the performance of black-box\nattacks. We introduce ImpMIA, a Membership Inference Attack that exploits the\nImplicit Bias of neural networks, hence removes the need to rely on any\nreference models and their assumptions. ImpMIA is a white-box attack -- a\nsetting which assumes access to model weights and is becoming increasingly\nrealistic given that many models are publicly available (e.g., via Hugging\nFace). Building on maximum-margin implicit bias theory, ImpMIA uses the\nKarush-Kuhn-Tucker (KKT) optimality conditions to identify training samples.\nThis is done by finding the samples whose gradients most strongly reconstruct\nthe trained model's parameters. As a result, ImpMIA achieves state-of-the-art\nperformance compared to both black and white box attacks in realistic settings\nwhere only the model weights and a superset of the training data are available."}
{"id": "2510.11588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11588", "abs": "https://arxiv.org/abs/2510.11588", "authors": ["Jiateng Liu", "Zhenhailong Wang", "Xiaojiang Huang", "Yingjie Li", "Xing Fan", "Xiang Li", "Chenlei Guo", "Ruhi Sarikaya", "Heng Ji"], "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents", "comment": "42 pages", "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata."}
{"id": "2510.10694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10694", "abs": "https://arxiv.org/abs/2510.10694", "authors": ["Ying-Kuan Tsai", "Vispi Karkaria", "Yi-Ping Chen", "Wei Chen"], "title": "Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning", "comment": "to be published in Journal of Mechanical Design", "summary": "Control Co-Design (CCD) integrates physical and control system design to\nimprove the performance of dynamic and autonomous systems. Despite advances in\nuncertainty-aware CCD methods, real-world uncertainties remain highly\nunpredictable. Multi-generation design addresses this challenge by considering\nthe full lifecycle of a product: data collected from each generation informs\nthe design of subsequent generations, enabling progressive improvements in\nrobustness and efficiency. Digital Twin (DT) technology further strengthens\nthis paradigm by creating virtual representations that evolve over the\nlifecycle through real-time sensing, model updating, and adaptive\nre-optimization. This paper presents a DT-enabled CCD framework that integrates\nDeep Reinforcement Learning (DRL) to jointly optimize physical design and\ncontroller. DRL accelerates real-time decision-making by allowing controllers\nto continuously learn from data and adapt to uncertain environments. Extending\nthis approach, the framework employs a multi-generation paradigm, where each\ncycle of deployment, operation, and redesign uses collected data to refine DT\nmodels, improve uncertainty quantification through quantile regression, and\ninform next-generation designs of both physical components and controllers. The\nframework is demonstrated on an active suspension system, where DT-enabled\nlearning from road conditions and driving behaviors yields smoother and more\nstable control trajectories. Results show that the method significantly\nenhances dynamic performance, robustness, and efficiency. Contributions of this\nwork include: (1) extending CCD into a lifecycle-oriented multi-generation\nframework, (2) leveraging DTs for continuous model updating and informed\ndesign, and (3) employing DRL to accelerate adaptive real-time decision-making."}
{"id": "2510.11608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11608", "abs": "https://arxiv.org/abs/2510.11608", "authors": ["Shiqi Zhang", "Xinbei Ma", "Yunqing Xu", "Zouying Cao", "Pengrui Lu", "Haobo Yuan", "Tiancheng Shen", "Zhuosheng Zhang", "Hai Zhao", "Ming-Hsuan Yang"], "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook."}
{"id": "2510.10777", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10777", "abs": "https://arxiv.org/abs/2510.10777", "authors": ["Andrey Veprikov", "Arman Bolatov", "Samuel HorvÃ¡th", "Aleksandr Beznosikov", "Martin TakÃ¡Ä", "Slavomir Hanzely"], "title": "Preconditioned Norms: A Unified Framework for Steepest Descent, Quasi-Newton and Adaptive Methods", "comment": "22 pages, 2 figures, 8 tables", "summary": "Optimization lies at the core of modern deep learning, yet existing methods\noften face a fundamental trade-off between adapting to problem geometry and\nleveraging curvature utilization. Steepest descent algorithms adapt to\ndifferent geometries through norm choices but remain strictly first-order,\nwhereas quasi-Newton and adaptive optimizers incorporate curvature information\nbut are restricted to Frobenius geometry, limiting their applicability across\ndiverse architectures. In this work, we propose a unified framework\ngeneralizing steepest descent, quasi-Newton methods, and adaptive methods\nthrough the novel notion of preconditioned matrix norms. This abstraction\nreveals that widely used optimizers such as SGD and Adam, as well as more\nadvanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAP\nand SPlus, all emerge as special cases of the same principle. Within this\nframework, we provide the first systematic treatment of affine and scale\ninvariance in the matrix-parameterized setting, establishing necessary and\nsufficient conditions under generalized norms. Building on this foundation, we\nintroduce two new methods, $\\texttt{MuAdam}$ and $\\texttt{MuAdam-SANIA}$, which\ncombine the spectral geometry of Muon with Adam-style preconditioning. Our\nexperiments demonstrate that these optimizers are competitive with, and in some\ncases outperform, existing state-of-the-art methods. Our code is available at\nhttps://github.com/brain-lab-research/LIB/tree/quasi_descent"}
{"id": "2510.11661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11661", "abs": "https://arxiv.org/abs/2510.11661", "authors": ["Shijie Xia", "Yuhan Sun", "Pengfei Liu"], "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI", "comment": null, "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities."}
{"id": "2510.10807", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.10807", "abs": "https://arxiv.org/abs/2510.10807", "authors": ["Ali Atiah Alzahrani"], "title": "Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation", "comment": "Code available at: https://github.com/AliAtiah/MARCD", "summary": "We study whether regime-conditioned generative scenarios, coupled with a\nconvex CVaR allocator, improve portfolio decisions under regime shifts. We\nintroduce Multi-Agent Regime-Conditioned Diffusion (MARCD), which (i) infers\nlatent regimes via a Gaussian HMM, (ii) trains a diffusion model with a\ntail-weighted objective and a regime-specialized mixture-of-experts (MoE)\ndenoiser to enrich crisis co-movements, and (iii) feeds the generated scenarios\ninto a turnover-aware CVaR epigraph quadratic program with explicit governance.\nIn strict walk-forward tests on liquid multi-asset ETFs (2005-2025), MARCD\noutperforms standard allocators and improves calibration relative to popular\ngenerators. Over 2020-2025 out-of-sample (monthly; 10 bps), MARCD attains\nSharpe 1.23 (BL 1.02) and MaxDD 9.3 percent (BL 14.1 percent), a 34 percent\nreduction, at comparable turnover; stationary block-bootstrap intervals\nindicate the Sharpe uplift is significant at 5 percent. We provide theory\nlinking tail-weighted diffusion to spectral-risk control of the\ndecision-relevant CVaR gap, oracle/consistency results for the regime-MoE\ndenoiser, and Lipschitz/regret guarantees for the allocator. Together, MARCD\noffers a reproducible bridge from tail-faithful scenario modeling to governed\nportfolio decisions with materially improved drawdown control."}
{"id": "2510.11694", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11694", "abs": "https://arxiv.org/abs/2510.11694", "authors": ["Arjun Sahney", "Ram Gorthi", "Cezary Åastowski", "Javier Vega"], "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering", "comment": "8 pages. No figures. Evaluated on MLE-Benchmark 2025", "summary": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints."}
{"id": "2510.10862", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10862", "abs": "https://arxiv.org/abs/2510.10862", "authors": ["Samuel Yuan", "Divyanshu Saxena", "Jiayi Chen", "Nihal Sharma", "Aditya Akella"], "title": "A Joint Learning Approach to Hardware Caching and Prefetching", "comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."}
{"id": "2510.09620", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09620", "abs": "https://arxiv.org/abs/2510.09620", "authors": ["Jiayun Mo", "Xin Kang", "Tieyan Li", "Zhongding Lei"], "title": "Toward a Unified Security Framework for AI Agents: Trust, Risk, and Liability", "comment": null, "summary": "The excitement brought by the development of AI agents came alongside arising\nproblems. These concerns centered around users' trust issues towards AIs, the\nrisks involved, and the difficulty of attributing responsibilities and\nliabilities. Current solutions only attempt to target each problem separately\nwithout acknowledging their inter-influential nature. The Trust, Risk and\nLiability (TRL) framework proposed in this paper, however, ties together the\ninterdependent relationships of trust, risk, and liability to provide a\nsystematic method of building and enhancing trust, analyzing and mitigating\nrisks, and allocating and attributing liabilities. It can be applied to analyze\nany application scenarios of AI agents and suggest appropriate measures fitting\nto the context. The implications of the TRL framework lie in its potential\nsocietal impacts, economic impacts, ethical impacts, and more. It is expected\nto bring remarkable values to addressing potential challenges and promoting\ntrustworthy, risk-free, and responsible usage of AI in 6G networks."}
{"id": "2510.10915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10915", "abs": "https://arxiv.org/abs/2510.10915", "authors": ["Hanchang Cheng", "Weimin Mu", "Fan Liu", "Weilin Zhu", "Can Ma"], "title": "LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection(TSAD) is a critical task in signal processing\nfield, ensuring the reliability of complex systems. Reconstruction-based\nmethods dominate in TSAD. Among these methods, VAE-based methods have achieved\npromising results. Existing VAE-based methods suffer from the limitation of\nsingle-window feature and insufficient leveraging of long-term time and\nfrequency information. We propose a Conditional Variational AutoEncoder with\nLong-term dependency and Probabilistic time-frequency fusion, named LPCVAE.\nLPCVAE introduces LSTM to capture long-term dependencies beyond windows. It\nfurther incorporates a Product-of-Experts (PoE) mechanism for adaptive and\ndistribution-level probabilistic fusion. This design effectively mitigates\ntime-frequency information loss. Extensive experiments on four public datasets\ndemonstrate it outperforms state-of-the-art methods. The results confirm that\nintegrating long-term time and frequency representations with adaptive fusion\nyields a robust and efficient solution for TSAD."}
{"id": "2510.09633", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09633", "abs": "https://arxiv.org/abs/2510.09633", "authors": ["Bernhard Mueller"], "title": "Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits", "comment": null, "summary": "Hound introduces a relation-first graph engine that improves system-level\nreasoning across interrelated components in complex codebases. The agent\ndesigns flexible, analyst-defined views with compact annotations (e.g.,\nmonetary/value flows, authentication/authorization roles, call graphs, protocol\ninvariants) and uses them to anchor exact retrieval: for any question, it loads\nprecisely the code that matters (often across components) so it can zoom out to\nsystem structure and zoom in to the decisive lines. A second contribution is a\npersistent belief system: long-lived vulnerability hypotheses whose confidence\nis updated as evidence accrues. The agent employs coverage-versus-intuition\nplanning and a QA finalizer to confirm or reject hypotheses. On a five-project\nsubset of ScaBench[1], Hound improves recall and F1 over a baseline LLM\nanalyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest\nprecision trade-off. We attribute these gains to flexible, relation-first\ngraphs that extend model understanding beyond call/dataflow to abstract\naspects, plus the hypothesis-centric loop; code and artifacts are released to\nsupport reproduction."}
{"id": "2510.10937", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10937", "abs": "https://arxiv.org/abs/2510.10937", "authors": ["Qizhou Peng", "Yang Zheng", "Yu Wen", "Yanna Wu", "Yingying Du"], "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems", "comment": null, "summary": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems."}
{"id": "2510.09663", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09663", "abs": "https://arxiv.org/abs/2510.09663", "authors": ["Raju Dhakal", "Prashant Shekhar", "Laxima Niure Kandel"], "title": "Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection", "comment": "Accepted for publication in ICMLA 2025", "summary": "Radio Frequency Fingerprinting (RFF) has evolved as an effective solution for\nauthenticating devices by leveraging the unique imperfections in hardware\ncomponents involved in the signal generation process. In this work, we propose\na Convolutional Neural Network (CNN) based framework for detecting rogue\ndevices and identifying genuine ones using softmax probability thresholding. We\nemulate an attack scenario in which adversaries attempt to mimic the RF\ncharacteristics of genuine devices by training a Generative Adversarial Network\n(GAN) using In-phase and Quadrature (IQ) samples from genuine devices. The\nproposed approach is verified using IQ samples collected from ten different\nADALM-PLUTO Software Defined Radios (SDRs), with seven devices considered\ngenuine, two as rogue, and one used for validation to determine the threshold."}
{"id": "2510.10952", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10952", "abs": "https://arxiv.org/abs/2510.10952", "authors": ["Xi Mao", "Zhendong Wang", "Jingyu Li", "Lingchao Mao", "Utibe Essien", "Hairong Wang", "Xuelei Sherry Ni"], "title": "Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant", "comment": null, "summary": "Early detection of Alzheimer's disease (AD) is crucial because its\nneurodegenerative effects are irreversible, and neuropathologic and\nsocial-behavioral risk factors accumulate years before diagnosis. Identifying\nhigher-risk individuals earlier enables prevention, timely care, and equitable\nresource allocation. We predict cognitive performance from social determinants\nof health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset\nderived from the nationally representative Mex-Cog cohort of the 2003 and 2012\nMexican Health and Aging Study (MHAS).\n  Data: The target is a validated composite cognitive score across seven\ndomains-orientation, memory, attention, language, constructional praxis, and\nexecutive function-derived from the 2016 and 2021 MHAS waves. Predictors span\ndemographic, socioeconomic, health, lifestyle, psychosocial, and healthcare\naccess factors.\n  Methodology: Missingness was addressed with a singular value decomposition\n(SVD)-based imputation pipeline treating continuous and categorical variables\nseparately. This approach leverages latent feature correlations to recover\nmissing values while balancing reliability and scalability. After evaluating\nmultiple methods, XGBoost was chosen for its superior predictive performance.\n  Results and Discussion: The framework outperformed existing methods and the\ndata challenge leaderboard, demonstrating high accuracy, robustness, and\ninterpretability. SHAP-based post hoc analysis identified top contributing SDOH\nfactors and age-specific feature patterns. Notably, flooring material emerged\nas a strong predictor, reflecting socioeconomic and environmental disparities.\nOther influential factors, age, SES, lifestyle, social interaction, sleep,\nstress, and BMI, underscore the multifactorial nature of cognitive aging and\nthe value of interpretable, data-driven SDOH modeling."}
{"id": "2510.09676", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09676", "abs": "https://arxiv.org/abs/2510.09676", "authors": ["Shayan Mohajer Hamidi", "En-Hui Yang", "Ben Liang"], "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling", "comment": null, "summary": "Inverse problems, where the goal is to recover an unknown signal from noisy\nor incomplete measurements, are central to applications in medical imaging,\nremote sensing, and computational biology. Diffusion models have recently\nemerged as powerful priors for solving such problems. However, existing methods\neither rely on projection-based techniques that enforce measurement consistency\nthrough heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y}\n\\mid \\boldsymbol{x})$, often resulting in artifacts and instability under\ncomplex or high-noise conditions. To address these limitations, we propose a\nnovel framework called \\emph{coupled data and measurement space diffusion\nposterior sampling} (C-DPS), which eliminates the need for constraint tuning or\nlikelihood approximation. C-DPS introduces a forward stochastic process in the\nmeasurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the\ndata-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a\nclosed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t,\n\\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive\nsampling based on a well-defined posterior distribution. Empirical results\ndemonstrate that C-DPS consistently outperforms existing baselines, both\nqualitatively and quantitatively, across multiple inverse problem benchmarks."}
{"id": "2510.10962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10962", "abs": "https://arxiv.org/abs/2510.10962", "authors": ["Wei Huang", "Yue Liao", "Yukang Chen", "Jianhui Liu", "Haoru Tan", "Si Liu", "Shiming Zhang", "Shuicheng Yan", "Xiaojuan Qi"], "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models", "comment": "15 pages, 13 figures", "summary": "Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and\nvision-language models (VLMs) by increasing capacity through sparse activation.\nHowever, preloading all experts into memory and activating multiple experts per\ninput introduces significant computational and memory overhead, making the\nexpert module a major contributor to model size and inference cost. To address\nthis, we propose MC# (Mixture-Compressor-sharp), a framework that combines\nstatic quantization and dynamic expert pruning by leveraging the significance\nof experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce\nstorage and loading costs, we introduce Pre-Loading Mixed-Precision\nQuantization (PMQ), which optimizes bit allocation via linear programming,\nbalancing expert importance and quantization error for a Pareto-optimal\ntrade-off between size and performance. To reduce runtime computation, Online\nTop-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a\nsubset of experts per token, enabling fine-grained control over activation. By\ncombining PMQ's static bit-width optimization with OTP's dynamic routing, MC#\nachieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC#\nachieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7%\naccuracy drop across five multimodal benchmarks. Additionally, OTP reduces\nexpert activation over 20% with less than 1% performance degradation,\ndemonstrating strong potential for efficient MoE-based model deployment."}
{"id": "2510.09689", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09689", "abs": "https://arxiv.org/abs/2510.09689", "authors": ["Haoran Ou", "Kangjie Chen", "Xingshuo Han", "Gelei Deng", "Jie Zhang", "Han Qiu", "Tianwei Zhang"], "title": "CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search", "comment": null, "summary": "Large Language Models (LLMs) excel at tasks such as dialogue, summarization,\nand question answering, yet they struggle to adapt to specialized domains and\nevolving facts. To overcome this, web search has been integrated into LLMs,\nallowing real-time access to online content. However, this connection magnifies\nsafety risks, as adversarial prompts combined with untrusted sources can cause\nsevere vulnerabilities. We investigate red teaming for LLMs with web search and\npresent CREST-Search, a framework that systematically exposes risks in such\nsystems. Unlike existing methods for standalone LLMs, CREST-Search addresses\nthe complex workflow of search-enabled models by generating adversarial queries\nwith in-context learning and refining them through iterative feedback. We\nfurther construct WebSearch-Harm, a search-specific dataset to fine-tune LLMs\ninto efficient red-teaming agents. Experiments show that CREST-Search\neffectively bypasses safety filters and reveals vulnerabilities in modern\nweb-augmented LLMs, underscoring the need for specialized defenses to ensure\ntrustworthy deployment."}
{"id": "2510.11018", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11018", "abs": "https://arxiv.org/abs/2510.11018", "authors": ["Pranav Ramesh", "Arjun Roy", "Deepak Ravikumar", "Kaushik Roy", "Gopalakrishnan Srinivasan"], "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness", "comment": null, "summary": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods."}
{"id": "2510.09776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09776", "abs": "https://arxiv.org/abs/2510.09776", "authors": ["Yufa Zhou", "Yixiao Wang", "Surbhi Goel", "Anru R. Zhang"], "title": "Why Do Transformers Fail to Forecast Time Series In-Context?", "comment": "Code: https://github.com/MasterZhou1/ICL-Time-Series", "summary": "Time series forecasting (TSF) remains a challenging and largely unsolved\nproblem in machine learning, despite significant recent efforts leveraging\nLarge Language Models (LLMs), which predominantly rely on Transformer\narchitectures. Empirical evidence consistently shows that even powerful\nTransformers often fail to outperform much simpler models, e.g., linear models,\non TSF tasks; however, a rigorous theoretical understanding of this phenomenon\nremains limited. In this paper, we provide a theoretical analysis of\nTransformers' limitations for TSF through the lens of In-Context Learning (ICL)\ntheory. Specifically, under AR($p$) data, we establish that: (1) Linear\nSelf-Attention (LSA) models $\\textit{cannot}$ achieve lower expected MSE than\nclassical linear models for in-context forecasting; (2) as the context length\napproaches to infinity, LSA asymptotically recovers the optimal linear\npredictor; and (3) under Chain-of-Thought (CoT) style inference, predictions\ncollapse to the mean exponentially. We empirically validate these findings\nthrough carefully designed experiments. Our theory not only sheds light on\nseveral previously underexplored phenomena but also offers practical insights\nfor designing more effective forecasting architectures. We hope our work\nencourages the broader research community to revisit the fundamental\ntheoretical limitations of TSF and to critically evaluate the direct\napplication of increasingly sophisticated architectures without deeper\nscrutiny."}
{"id": "2510.11062", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11062", "abs": "https://arxiv.org/abs/2510.11062", "authors": ["Yujie Zhao", "Lanxiang Hu", "Yang Wang", "Minmin Hou", "Hao Zhang", "Ke Ding", "Jishen Zhao"], "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs", "comment": null, "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs."}
{"id": "2510.09781", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09781", "abs": "https://arxiv.org/abs/2510.09781", "authors": ["Yue Huang", "Hang Hua", "Yujun Zhou", "Pengcheng Jing", "Manish Nagireddy", "Inkit Padhi", "Greta Dolcetti", "Zhangchen Xu", "Subhajit Chaudhury", "Ambrish Rawat", "Liubov Nedoshivina", "Pin-Yu Chen", "Prasanna Sattigeri", "Xiangliang Zhang"], "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "comment": null, "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems."}
{"id": "2510.11121", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11121", "abs": "https://arxiv.org/abs/2510.11121", "authors": ["Rongjie Zhu", "Cong Zhang", "Zhiguang Cao"], "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM", "comment": null, "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."}
{"id": "2510.09845", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09845", "abs": "https://arxiv.org/abs/2510.09845", "authors": ["Nicholas LaHaye", "Thilanka Munashinge", "Hugo Lee", "Xiaohua Pan", "Gonzalo Gonzalez Abad", "Hazem Mahmoud", "Jennifer Wei"], "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data", "comment": "https://2025.ieeeigarss.org/view_paper.php?PaperNum=6389&SessionID=1611", "summary": "This work demonstrates the possibilities for improving wildfire and air\nquality management in the western United States by leveraging the unprecedented\nhourly data from NASA's TEMPO satellite mission and advances in self-supervised\ndeep learning. Here we demonstrate the efficacy of deep learning for mapping\nthe near real-time hourly spread of wildfire fronts and smoke plumes using an\ninnovative self-supervised deep learning-system: successfully distinguishing\nsmoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across\nthe smoke and fire masks generated from different sensing modalities as well as\nsignificant improvement over operational products for the same cases."}
{"id": "2510.11164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11164", "abs": "https://arxiv.org/abs/2510.11164", "authors": ["Ilaria Vascotto", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness", "comment": "Accepted at the European Workshop on Trustworthy Artificial\n  Intelligence (TRUST-AI), co-located within ECAI 2025", "summary": "The use of Artificial Intelligence (AI) models in real-world and high-risk\napplications has intensified the discussion about their trustworthiness and\nethical usage, from both a technical and a legislative perspective. The field\nof eXplainable Artificial Intelligence (XAI) addresses this challenge by\nproposing explanations that bring to light the decision-making processes of\ncomplex black-box models. Despite being an essential property, the robustness\nof explanations is often an overlooked aspect during development: only robust\nexplanation methods can increase the trust in the system as a whole. This paper\ninvestigates the role of robustness through the usage of a feature importance\naggregation derived from multiple models ($k$-nearest neighbours, random forest\nand neural networks). Preliminary results showcase the potential in increasing\nthe trustworthiness of the application, while leveraging multiple model's\npredictive power."}
{"id": "2510.09872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09872", "abs": "https://arxiv.org/abs/2510.09872", "authors": ["Sanjari Srivastava", "Gang Li", "Cheng Chang", "Rishu Garg", "Manpreet Kaur", "Charlene Y. Lee", "Yuezhang Li", "Yining Mao", "Ignacio Cases", "Yanan Xie", "Peng Qi"], "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions", "comment": null, "summary": "Training web agents to navigate complex, real-world websites requires them to\nmaster $\\textit{subtasks}$ - short-horizon interactions on multiple UI\ncomponents (e.g., choosing the correct date in a date picker, or scrolling in a\ncontainer to extract information). We introduce WARC-Bench (Web Archive\nBenchmark), a novel web navigation benchmark featuring 438 tasks designed to\nevaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed\ninteractions with dynamic and realistic webpages using Web ARChive files. We\nshow that WARC-Bench is challenging for leading computer-use models, with the\nhighest observed success rate being 64.8%. To improve open source models on\nsubtask, we explore two common training techniques: supervised fine-tuning\n(SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments\nshow that SFT models obtain a 48.8% success rate on the benchmark. Training\nwith RLVR over SFT checkpoints, even in data-scarce settings, improves the\nscore to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis\nconcludes that mastering these subtasks is essential for robust web planning\nand navigation, and is a capability not extensively evaluated by existing\nbenchmarks."}
{"id": "2510.11168", "categories": ["cs.LG", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11168", "abs": "https://arxiv.org/abs/2510.11168", "authors": ["Jinbin Zhang", "Nasib Ullah", "Erik Schultheis", "Rohit Babbar"], "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces", "comment": "Accepted to ICML 2025", "summary": "Large output spaces, also referred to as Extreme multilabel classification\n(XMC), is a setting that arises, e.g., in large-scale tagging and\nproduct-to-product recommendation, and is characterized by the number of labels\nranging from hundreds of thousands to millions. This means that the linear\nclassification head, usually only a tiny fraction of the overall model, turns\ninto the main driver for compute and memory demand. Current state-of-the-art\nXMC methods predominantly rely on FP16-FP32 mixed-precision training, which we\nshow can be unstable, and inefficient in terms of memory usage and\ncomputational overhead. Meanwhile, existing low-precision methods typically\nretain higher precision for the classification layer. In this work, we propose\nELMO, a pure low-precision training framework for XMC models using BFloat16 and\nFloat8 data types. By leveraging Kahan summation and stochastic rounding, we\ndemonstrate that XMC models can be effectively trained entirely in Float8,\nwithout relying on single-precision master weights or tensor scaling.\nLow-precision training, combined with our proposed memory optimizations --\ngradient fusion and chunking -- enables significant reductions in GPU memory\nusage. For example, we train a 3-million-label XMC model with only 6.6 GiB of\nGPU memory, compared to the 39.7 GiB required by the optimized SOTA method,\nRenee without compromising accuracy."}
{"id": "2510.09877", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09877", "abs": "https://arxiv.org/abs/2510.09877", "authors": ["Kangping Hu", "Stephen Mussmann"], "title": "Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling", "comment": null, "summary": "Over the past couple of decades, many active learning acquisition functions\nhave been proposed, leaving practitioners with an unclear choice of which to\nuse. Bayesian Decision Theory (BDT) offers a universal principle to guide\ndecision-making. In this work, we derive BDT for (Bayesian) active learning in\nthe myopic framework, where we imagine we only have one more point to label.\nThis derivation leads to effective algorithms such as Expected Error Reduction\n(EER), Expected Predictive Information Gain (EPIG), and other algorithms that\nappear in the literature. Furthermore, we show that BAIT (active learning based\non V-optimal experimental design) can be derived from BDT and asymptotic\napproximations. A key challenge of such methods is the difficult scaling to\nlarge batch sizes, leading to either computational challenges (BatchBALD) or\ndramatic performance drops (top-$B$ selection). Here, using a particular\nformulation of the decision process, we derive Partial Batch Label Sampling\n(ParBaLS) for the EPIG algorithm. We show experimentally for several datasets\nthat ParBaLS EPIG gives superior performance for a fixed budget and Bayesian\nLogistic Regression on Neural Embeddings. Our code is available at\nhttps://github.com/ADDAPT-ML/ParBaLS."}
{"id": "2510.11184", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11184", "abs": "https://arxiv.org/abs/2510.11184", "authors": ["Zhengyu Chen", "Jinluan Yang", "Teng Xiao", "Ruochen Zhou", "Luan Zhang", "Xiangyu Xi", "Xiaowei Shi", "Wei Wang", "Jinggang Wang"], "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in reasoning and tool utilization. However, the generalization of\ntool-augmented reinforcement learning (RL) across diverse domains remains\nunderexplored. In this work, we investigate the cross-domain generalization of\nan LLM agent equipped with a code interpreter tool, which is exclusively\ntrained on mathematical problem-solving tasks. Despite the restricted training\ndomain, we evaluate the agent's performance across several distinct reasoning\ndomains. The results reveal that RL-based tool usage learned from mathematical\ntasks can be effectively transferred to complex tasks in other domains,\nenabling great task performance and high token efficiency. To facilitate this\ncross-domain transfer, we propose a Tool Generalization Reinforcement Learning\n(TGRL) framework designed to promote domain-agnostic learning and skill\nmigration, encompassing: (i) a standardized tool interface that abstracts\ndomain-specific nuances through consistent formatting and explicit termination,\nfostering transferable invocation patterns; (ii) a dual-component reward system\nthat decomposes rewards to incentivize generalizable behaviors like tool\nefficiency and reasoning abstraction, ensuring alignment and robustness across\ndomain shifts; and (iii) an XML-based prompt template that separates thinking,\ntool calls, and responses to encourage modular, domain-invariant planning and\ncoherent multi-turn interactions. Extensive experiments across diverse\nbenchmarks validate our approach, achieving state-of-the-art performance and\nhighlighting the cross-domain potential of Tool RL for LLM reasoning."}
{"id": "2510.10041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10041", "abs": "https://arxiv.org/abs/2510.10041", "authors": ["Sahng-Min Han", "Minjae Kim", "Jinho Cha", "Se-woon Choe", "Eunchan Daniel Cha", "Jungwon Choi", "Kyudong Jung"], "title": "FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis", "comment": "35 pages, 11 figures, submitted to Computers in Biology and Medicine\n  (Elsevier, under review)", "summary": "Deep learning in small and imbalanced biomedical datasets remains\nfundamentally constrained by unstable optimization and poor generalization. We\npresent the first biomedical implementation of FOSSIL (Flexible Optimization\nvia Sample-Sensitive Importance Learning), a regret-minimizing weighting\nframework that adaptively balances training emphasis according to sample\ndifficulty. Using softmax-based uncertainty as a continuous measure of\ndifficulty, we construct a four-stage curriculum (Easy-Very Hard) and integrate\nFOSSIL into both convolutional and transformer-based architectures for Mpox\nskin lesion diagnosis. Across all settings, FOSSIL substantially improves\ndiscrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under\nreal-world perturbations, outperforming conventional baselines without\nmetadata, manual curation, or synthetic augmentation. The results position\nFOSSIL as a generalizable, data-efficient, and interpretable framework for\ndifficulty-aware learning in medical imaging under data scarcity."}
{"id": "2510.11292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11292", "abs": "https://arxiv.org/abs/2510.11292", "authors": ["Wenbo Wu", "Qingyi Si", "Xiurui Pan", "Ye Wang", "Jie Zhang"], "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences", "comment": null, "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."}
{"id": "2510.10278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10278", "abs": "https://arxiv.org/abs/2510.10278", "authors": ["Christopher Chiu", "Silviu Pitis", "Mihaela van der Schaar"], "title": "Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models", "comment": null, "summary": "Clinical reasoning in medicine is a hypothesis-driven process where\nphysicians refine diagnoses from limited information through targeted history,\nphysical examination, and diagnostic investigations. In contrast, current\nmedical benchmarks for large language models (LLMs) primarily assess knowledge\nrecall through single-turn questions, where complete clinical information is\nprovided upfront. To address this gap, we introduce VivaBench, a multi-turn\nbenchmark that evaluates sequential clinical reasoning in LLM agents. Our\ndataset consists of 1762 physician-curated clinical vignettes structured as\ninteractive scenarios that simulate a (oral) examination in medical training,\nrequiring agents to actively probe for relevant findings, select appropriate\ninvestigations, and synthesize information across multiple steps to reach a\ndiagnosis. While current LLMs demonstrate competence in diagnosing conditions\nfrom well-described clinical presentations, their performance degrades\nsignificantly when required to navigate iterative diagnostic reasoning under\nuncertainty in our evaluation. Our analysis identified several failure modes\nthat mirror common cognitive errors in clinical practice, including: (1)\nfixation on initial hypotheses, (2) inappropriate investigation ordering, (3)\npremature diagnostic closure, and (4) failing to screen for critical\nconditions. These patterns reveal fundamental limitations in how current LLMs\nreason and make decisions under uncertainty. Through VivaBench, we provide a\nstandardized benchmark for evaluating conversational medical AI systems for\nreal-world clinical decision support. Beyond medical applications, we\ncontribute to the larger corpus of research on agentic AI by demonstrating how\nsequential reasoning trajectories can diverge in complex decision-making\nenvironments."}
{"id": "2510.11345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11345", "abs": "https://arxiv.org/abs/2510.11345", "authors": ["Han Lu", "Zichen Liu", "Shaopan Xiong", "Yancheng He", "Wei Gao", "Yanan Wu", "Weixun Wang", "Jiashun Liu", "Yang Li", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Xiaoyang Li", "Yijia Luo", "Zihe Liu", "Ling Pan", "Junchi Yan", "Wei Wang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng"], "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony", "comment": null, "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training."}
{"id": "2510.10304", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10304", "abs": "https://arxiv.org/abs/2510.10304", "authors": ["Michael Y. Hu", "Benjamin Van Durme", "Jacob Andreas", "Harsh Jhamtani"], "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "comment": null, "summary": "Language model (LM) agents deployed in novel environments often exhibit poor\nsample efficiency when learning from sequential interactions. This\nsignificantly hinders the usefulness of such agents in environments where\ninteraction is costly (for example, when they interact with humans or reset\nphysical systems). While a number of existing LM agent architectures\nincorporate various mechanisms for experience storage and reflection, they make\nlimited use of LMs' abilities to directly generate or reason about full\ncounterfactual trajectories. We introduce ECHO (Experience Consolidation via\nHindsight Optimization), a prompting framework that adapts hindsight experience\nreplay from reinforcement learning for language model agents. ECHO generates\noptimized trajectories for alternative goals that could have been achieved\nduring failed attempts, effectively creating synthetic positive examples from\nunsuccessful interactions. Our approach consists of two components: a hindsight\nrule that uses the language model itself to identify relevant subgoals and\ngenerate optimized trajectories, and an update rule that maintains compressed\ntrajectory representations in memory. We evaluate ECHO on stateful versions of\nXMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a\ncollaborative information-gathering enterprise simulation. Across both domains,\nECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,\nit also outperforms a number of sophisticated agent architectures including\nReflexion and AWM, demonstrating faster adaptation to novel environments\nthrough more effective utilization of past experiences."}
{"id": "2510.11409", "categories": ["cs.LG", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11409", "abs": "https://arxiv.org/abs/2510.11409", "authors": ["Lucas Joos", "Daniel A. Keim", "Maximilian T. Fischer"], "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews", "comment": null, "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows."}
{"id": "2510.10325", "categories": ["cs.MA", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10325", "abs": "https://arxiv.org/abs/2510.10325", "authors": ["Walid Abdela"], "title": "KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments", "comment": null, "summary": "The seamless integration of physical and digital environments in\nCyber-Physical Systems(CPS), particularly within Industry 4.0, presents\nsignificant challenges stemming from system heterogeneity and complexity.\nTraditional approaches often rely on rigid, data-centric solutions like\nco-simulation frameworks or brittle point-to-point middleware bridges, which\nlack the semantic richness and flexibility required for intelligent, autonomous\ncoordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent\nInfrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS\nleverages a centralized Knowledge Graph (KG) as a dynamic, shared world model,\nproviding a common semantic foundation for a Multi-Agent System(MAS).\nAutonomous agents, representing both physical and digital components, query\nthis KG for decision-making and update it with real-time state information. The\ninfrastructure features a model-driven architecture which facilitates the\nautomatic generation of agents from semantic descriptions, thereby simplifying\nsystem extension and maintenance. By abstracting away underlying communication\nprotocols and providing a unified, intelligent coordination mechanism, KG-MAS\noffers a robust, scalable, and flexible solution for coupling heterogeneous\nphysical and digital robotic environments."}
{"id": "2510.11498", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11498", "abs": "https://arxiv.org/abs/2510.11498", "authors": ["Yuhang Li", "Chenchen Zhang", "Ruilin Lv", "Ao Liu", "Ken Deng", "Yuanxing Zhang", "Jiaheng Liu", "Wiggin Zhou", "Bo Zhou"], "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding", "comment": null, "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling."}
{"id": "2510.10433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10433", "abs": "https://arxiv.org/abs/2510.10433", "authors": ["Zixiang Xu", "Menghui Zhou", "Jun Qi", "Xuanhan Fan", "Yun Yang", "Po Yang"], "title": "Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression", "comment": null, "summary": "Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in\naging populations, posing a significant and escalating burden on global\nhealthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful\ncomputational paradigm for modeling longitudinal AD data, existing frameworks\ndo not account for the time-varying nature of feature correlations. To address\nthis limitation, we propose a novel MTL framework, named Feature Similarity\nLaplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel\nFeature Similarity Laplacian (FSL) penalty that explicitly models the\ntime-varying relationships between features. By simultaneously considering\ntemporal smoothness among tasks and the dynamic correlations among features,\nour model enhances both predictive accuracy and biological interpretability. To\nsolve the non-smooth optimization problem arising from our proposed penalty\nterms, we adopt the Alternating Direction Method of Multipliers (ADMM)\nalgorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework\nachieves state-of-the-art performance, outperforming various baseline methods.\nThe implementation source can be found at https://github.com/huatxxx/MTL-FSL."}
{"id": "2510.11502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11502", "abs": "https://arxiv.org/abs/2510.11502", "authors": ["Alexis Ross", "Jacob Andreas"], "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors", "comment": null, "summary": "Research on reasoning in language models (LMs) predominantly focuses on\nimproving the correctness of their outputs. But some important applications\nrequire modeling reasoning patterns that are incorrect. For example, automated\nsystems that can reason about and simulate student errors are useful for\nproviding real-time feedback in the classroom or offline practice for\neducators-in-training. This paper presents a new method, MISTAKE, that (1)\nconstructs high-quality synthetic examples of reasoning errors by leveraging\ncycle consistency between incorrect answers and latent misconceptions; and (2)\nuses the generated data to learn models for student simulation, misconception\nclassification, and answer generation. We evaluate MISTAKE on three educational\ntasks and find that it results in (1) higher accuracy when simulating incorrect\nstudent answers based on specific misconceptions, (2) increased performance\ninferring latent misconceptions from observed incorrect answers, and (3) higher\nalignment with expert-written distractor answers when generating incorrect\nanswers (e.g., for multiple-choice tests)."}
{"id": "2510.10446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10446", "abs": "https://arxiv.org/abs/2510.10446", "authors": ["Masoud Makrehchi"], "title": "Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation", "comment": "10 pages", "summary": "We analyze a reversed-supervision strategy that searches over labelings of a\nlarge unlabeled set \\(B\\) to minimize error on a small labeled set \\(A\\). The\nsearch space is \\(2^n\\), and the resulting complexity remains exponential even\nunder large constant-factor speedups (e.g., quantum or massively parallel\nhardware). Consequently, arbitrarily fast -- but not exponentially faster --\ncomputation does not obviate the need for informative labels or priors. In\npractice, the machine learning pipeline still requires an initial human\ncontribution: specifying the objective, defining classes, and providing a seed\nset of representative annotations that inject inductive bias and align models\nwith task semantics. Synthetic labels from generative AI can partially\nsubstitute provided their quality is human-grade and anchored by a\nhuman-specified objective, seed supervision, and validation. In this view,\ngenerative models function as \\emph{label amplifiers}, leveraging small\nhuman-curated cores via active, semi-supervised, and self-training loops, while\nhumans retain oversight for calibration, drift detection, and failure auditing.\nThus, extreme computational speed reduces wall-clock time but not the\nfundamental supervision needs of learning; initial human (or human-grade) input\nremains necessary to ground the system in the intended task."}
{"id": "2510.09633", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09633", "abs": "https://arxiv.org/abs/2510.09633", "authors": ["Bernhard Mueller"], "title": "Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits", "comment": null, "summary": "Hound introduces a relation-first graph engine that improves system-level\nreasoning across interrelated components in complex codebases. The agent\ndesigns flexible, analyst-defined views with compact annotations (e.g.,\nmonetary/value flows, authentication/authorization roles, call graphs, protocol\ninvariants) and uses them to anchor exact retrieval: for any question, it loads\nprecisely the code that matters (often across components) so it can zoom out to\nsystem structure and zoom in to the decisive lines. A second contribution is a\npersistent belief system: long-lived vulnerability hypotheses whose confidence\nis updated as evidence accrues. The agent employs coverage-versus-intuition\nplanning and a QA finalizer to confirm or reject hypotheses. On a five-project\nsubset of ScaBench[1], Hound improves recall and F1 over a baseline LLM\nanalyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest\nprecision trade-off. We attribute these gains to flexible, relation-first\ngraphs that extend model understanding beyond call/dataflow to abstract\naspects, plus the hypothesis-centric loop; code and artifacts are released to\nsupport reproduction."}
{"id": "2510.10451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10451", "abs": "https://arxiv.org/abs/2510.10451", "authors": ["Keisuke Fujii", "Kazushi Tsutsui", "Yu Teshima", "Makoto Itoh", "Naoya Takeishi", "Nozomi Nishiumi", "Ryoya Tanaka", "Shunsuke Shigaki", "Yoshinobu Kawahara"], "title": "Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning", "comment": "21 pages, 7 figures", "summary": "Simulators of animal movements play a valuable role in studying behavior.\nAdvances in imitation learning for robotics have expanded possibilities for\nreproducing human and animal movements. A key challenge for realistic\nmulti-animal simulation in biology is bridging the gap between unknown\nreal-world transition models and their simulated counterparts. Because\nlocomotion dynamics are seldom known, relying solely on mathematical models is\ninsufficient; constructing a simulator that both reproduces real trajectories\nand supports reward-driven optimization remains an open problem. We introduce a\ndata-driven simulator for multi-animal behavior based on deep reinforcement\nlearning and counterfactual simulation. We address the ill-posed nature of the\nproblem caused by high degrees of freedom in locomotion by estimating movement\nvariables of an incomplete transition model as actions within an RL framework.\nWe also employ a distance-based pseudo-reward to align and compare states\nbetween cyber and physical spaces. Validated on artificial agents, flies,\nnewts, and silkmoth, our approach achieves higher reproducibility of\nspecies-specific behaviors and improved reward acquisition compared with\nstandard imitation and RL methods. Moreover, it enables counterfactual behavior\nprediction in novel experimental settings and supports multi-individual\nmodeling for flexible what-if trajectory generation, suggesting its potential\nto simulate and elucidate complex multi-animal behaviors."}
{"id": "2510.09645", "categories": ["cs.CR", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09645", "abs": "https://arxiv.org/abs/2510.09645", "authors": ["Tonmoy Ghosh"], "title": "AdaptAuth: Multi-Layered Behavioral and Credential Analysis for a Secure and Adaptive Authentication Framework for Password Security", "comment": null, "summary": "Password security has been compelled to evolve in response to the growing\ncomputational capabilities of modern systems. However, this evolution has often\nresulted in increasingly complex security practices that alienate users,\nleading to poor compliance and heightened vulnerability. Consequently,\nindividuals remain exposed to attackers through weak or improperly managed\npasswords, underscoring the urgent need for a comprehensive defense mechanism\nthat effectively addresses password-related risks and threats. In this paper,\nwe propose a multifaceted solution designed to revolutionize password security\nby integrating diverse attributes such as the Password Dissection Mechanism,\nDynamic Password Policy Mechanism, human behavioral patterns, device\ncharacteristics, network parameters, geographical context, and other relevant\nfactors. By leveraging learning-based models, our framework constructs detailed\nuser profiles capable of recognizing individuals and preventing nearly all\nforms of unauthorized access or device possession. The proposed framework\nenhances the usability-security paradigm by offering stronger protection than\nexisting standards while simultaneously engaging users in the policy-setting\nprocess through a novel, adaptive approach."}
{"id": "2510.10586", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10586", "abs": "https://arxiv.org/abs/2510.10586", "authors": ["Giulio Ruffini"], "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents", "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)", "summary": "In the algorithmic (Kolmogorov) view, agents are programs that track and\ncompress sensory streams using generative programs. We propose a framework\nwhere the relevant structural prior is simplicity (Solomonoff) understood as\n\\emph{compositional symmetry}: natural streams are well described by (local)\nactions of finite-parameter Lie pseudogroups on geometrically and topologically\ncomplex low-dimensional configuration manifolds (latent spaces). Modeling the\nagent as a generic neural dynamical system coupled to such streams, we show\nthat accurate world-tracking imposes (i) \\emph{structural constraints} --\nequivariance of the agent's constitutive equations and readouts -- and (ii)\n\\emph{dynamical constraints}: under static inputs, symmetry induces conserved\nquantities (Noether-style labels) in the agent dynamics and confines\ntrajectories to reduced invariant manifolds; under slow drift, these manifolds\nmove but remain low-dimensional. This yields a hierarchy of reduced manifolds\naligned with the compositional factorization of the pseudogroup, providing a\ngeometric account of the ``blessing of compositionality'' in deep models. We\nconnect these ideas to the Spencer formalism for Lie pseudogroups and formulate\na symmetry-based, self-contained version of predictive coding in which higher\nlayers receive only \\emph{coarse-grained residual transformations}\n(prediction-error coordinates) along symmetry directions unresolved at lower\nlayers."}
{"id": "2510.09937", "categories": ["cs.MA", "cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09937", "abs": "https://arxiv.org/abs/2510.09937", "authors": ["Shahbaz P Qadri Syed", "He Bai"], "title": "Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective", "comment": null, "summary": "The empirical success of multi-agent reinforcement learning (MARL) has\nmotivated the search for more efficient and scalable algorithms for large scale\nmulti-agent systems. However, existing state-of-the-art algorithms do not fully\nexploit inter-agent coupling information to develop MARL algorithms. In this\npaper, we propose a systematic approach to leverage structures in the\ninter-agent couplings for efficient model-free reinforcement learning. We model\nthe cooperative MARL problem via a Bayesian network and characterize the subset\nof agents, termed as the value dependency set, whose information is required by\neach agent to estimate its local action value function exactly. Moreover, we\npropose a partially decentralized training decentralized execution (P-DTDE)\nparadigm based on the value dependency set. We theoretically establish that the\ntotal variance of our P-DTDE policy gradient estimator is less than the\ncentralized training decentralized execution (CTDE) policy gradient estimator.\nWe derive a multi-agent policy gradient theorem based on the P-DTDE scheme and\ndevelop a scalable actor-critic algorithm. We demonstrate the efficiency and\nscalability of the proposed algorithm on multi-warehouse resource allocation\nand multi-zone temperature control examples. For dense value dependency sets,\nwe propose an approximation scheme based on truncation of the Bayesian network\nand empirically show that it achieves a faster convergence than the exact value\ndependence set for applications with a large number of agents."}
{"id": "2510.10915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10915", "abs": "https://arxiv.org/abs/2510.10915", "authors": ["Hanchang Cheng", "Weimin Mu", "Fan Liu", "Weilin Zhu", "Can Ma"], "title": "LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection(TSAD) is a critical task in signal processing\nfield, ensuring the reliability of complex systems. Reconstruction-based\nmethods dominate in TSAD. Among these methods, VAE-based methods have achieved\npromising results. Existing VAE-based methods suffer from the limitation of\nsingle-window feature and insufficient leveraging of long-term time and\nfrequency information. We propose a Conditional Variational AutoEncoder with\nLong-term dependency and Probabilistic time-frequency fusion, named LPCVAE.\nLPCVAE introduces LSTM to capture long-term dependencies beyond windows. It\nfurther incorporates a Product-of-Experts (PoE) mechanism for adaptive and\ndistribution-level probabilistic fusion. This design effectively mitigates\ntime-frequency information loss. Extensive experiments on four public datasets\ndemonstrate it outperforms state-of-the-art methods. The results confirm that\nintegrating long-term time and frequency representations with adaptive fusion\nyields a robust and efficient solution for TSAD."}
{"id": "2510.11251", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11251", "abs": "https://arxiv.org/abs/2510.11251", "authors": ["Rui Xu", "Jiawei Chen", "Zhaoxia Yin", "Cong Kong", "Xinpeng Zhang"], "title": "Large Language Models Are Effective Code Watermarkers", "comment": null, "summary": "The widespread use of large language models (LLMs) and open-source code has\nraised ethical and security concerns regarding the distribution and attribution\nof source code, including unauthorized redistribution, license violations, and\nmisuse of code for malicious purposes. Watermarking has emerged as a promising\nsolution for source attribution, but existing techniques rely heavily on\nhand-crafted transformation rules, abstract syntax tree (AST) manipulation, or\ntask-specific training, limiting their scalability and generality across\nlanguages. Moreover, their robustness against attacks remains limited. To\naddress these limitations, we propose CodeMark-LLM, an LLM-driven watermarking\nframework that embeds watermark into source code without compromising its\nsemantics or readability. CodeMark-LLM consists of two core components: (i)\nSemantically Consistent Embedding module that applies functionality-preserving\ntransformations to encode watermark bits, and (ii) Differential Comparison\nExtraction module that identifies the applied transformations by comparing the\noriginal and watermarked code. Leveraging the cross-lingual generalization\nability of LLM, CodeMark-LLM avoids language-specific engineering and training\npipelines. Extensive experiments across diverse programming languages and\nattack scenarios demonstrate its robustness, effectiveness, and scalability."}
{"id": "2510.10932", "categories": ["cs.CR", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10932", "abs": "https://arxiv.org/abs/2510.10932", "authors": ["Zonghuan Xu", "Xiang Zheng", "Xingjun Ma", "Yu-Gang Jiang"], "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models", "comment": "8 pages, 8 tables, 1 figure. Under review", "summary": "With the growing deployment of Vision-Language-Action (VLA) models in\nreal-world embodied AI systems, their increasing vulnerability to backdoor\nattacks poses a serious safety threat. A backdoored VLA agent can be covertly\ntriggered by a pre-injected backdoor to execute adversarial actions,\npotentially causing system failures or even physical harm. Although backdoor\nattacks on VLA models have been explored, prior work has focused only on\nuntargeted attacks, leaving the more practically threatening scenario of\ntargeted manipulation unexamined. In this paper, we study targeted backdoor\nattacks on VLA models and introduce TabVLA, a novel framework that enables such\nattacks via black-box fine-tuning. TabVLA explores two deployment-relevant\ninference-time threat models: input-stream editing and in-scene triggering. It\nformulates poisoned data generation as an optimization problem to improve\nattack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal\nthat the vision channel is the principal attack surface: targeted backdoors\nsucceed with minimal poisoning, remain robust across variations in trigger\ndesign, and are degraded only by positional mismatches between fine-tuning and\ninference triggers. We also investigate a potential detection-based defense\nagainst TabVLA, which reconstructs latent visual triggers from the input stream\nto flag activation-conditioned backdoor samples. Our work highlights the\nvulnerability of VLA models to targeted backdoor manipulation and underscores\nthe need for more advanced defenses."}
{"id": "2510.10962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10962", "abs": "https://arxiv.org/abs/2510.10962", "authors": ["Wei Huang", "Yue Liao", "Yukang Chen", "Jianhui Liu", "Haoru Tan", "Si Liu", "Shiming Zhang", "Shuicheng Yan", "Xiaojuan Qi"], "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models", "comment": "15 pages, 13 figures", "summary": "Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and\nvision-language models (VLMs) by increasing capacity through sparse activation.\nHowever, preloading all experts into memory and activating multiple experts per\ninput introduces significant computational and memory overhead, making the\nexpert module a major contributor to model size and inference cost. To address\nthis, we propose MC# (Mixture-Compressor-sharp), a framework that combines\nstatic quantization and dynamic expert pruning by leveraging the significance\nof experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce\nstorage and loading costs, we introduce Pre-Loading Mixed-Precision\nQuantization (PMQ), which optimizes bit allocation via linear programming,\nbalancing expert importance and quantization error for a Pareto-optimal\ntrade-off between size and performance. To reduce runtime computation, Online\nTop-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a\nsubset of experts per token, enabling fine-grained control over activation. By\ncombining PMQ's static bit-width optimization with OTP's dynamic routing, MC#\nachieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC#\nachieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7%\naccuracy drop across five multimodal benchmarks. Additionally, OTP reduces\nexpert activation over 20% with less than 1% performance degradation,\ndemonstrating strong potential for efficient MoE-based model deployment."}
{"id": "2510.11004", "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11004", "abs": "https://arxiv.org/abs/2510.11004", "authors": ["Haoran Liang", "Yufa Zhou", "Mohammad Talebi Kalaleh", "Qipei Mei"], "title": "Automating Structural Engineering Workflows with Large Language Model Agents", "comment": "Code: https://github.com/DelosLiang/masse", "summary": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural\nEngineering, effectively integrating large language model (LLM)-based agents\nwith real-world engineering workflows. Structural engineering is a fundamental\nyet traditionally stagnant domain, with core workflows remaining largely\nunchanged for decades despite its substantial economic impact and global market\nsize. Recent advancements in LLMs have significantly enhanced their ability to\nperform complex reasoning, long-horizon planning, and precise tool utilization\n-- capabilities well aligned with structural engineering tasks such as\ninterpreting design codes, executing load calculations, and verifying\nstructural capacities. We present a proof-of-concept showing that most\nreal-world structural engineering workflows can be fully automated through a\ntraining-free LLM-based multi-agent system. MASSE enables immediate deployment\nin professional environments, and our comprehensive validation on real-world\ncase studies demonstrates that it can reduce expert workload from approximately\ntwo hours to mere minutes, while enhancing both reliability and accuracy in\npractical engineering scenarios."}
{"id": "2510.11108", "categories": ["cs.MA", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11108", "abs": "https://arxiv.org/abs/2510.11108", "authors": ["Xinfeng Li", "Dong Huang", "Jie Li", "Hongyi Cai", "Zhenhong Zhou", "Wei Dong", "XiaoFeng Wang", "Yang Liu"], "title": "A Vision for Access Control in LLM-based Agent Systems", "comment": "10 pages, 1 figure", "summary": "The autonomy and contextual complexity of LLM-based agents render traditional\naccess control (AC) mechanisms insufficient. Static, rule-based systems\ndesigned for predictable environments are fundamentally ill-equipped to manage\nthe dynamic information flows inherent in agentic interactions. This position\npaper argues for a paradigm shift from binary access control to a more\nsophisticated model of information governance, positing that the core challenge\nis not merely about permission, but about governing the flow of information. We\nintroduce Agent Access Control (AAC), a novel framework that reframes AC as a\ndynamic, context-aware process of information flow governance. AAC operates on\ntwo core modules: (1) multi-dimensional contextual evaluation, which assesses\nnot just identity but also relationships, scenarios, and norms; and (2)\nadaptive response formulation, which moves beyond simple allow/deny decisions\nto shape information through redaction, summarization, and paraphrasing. This\nvision, powered by a dedicated AC reasoning engine, aims to bridge the gap\nbetween human-like nuanced judgment and scalable Al safety, proposing a new\nconceptual lens for future research in trustworthy agent design."}
{"id": "2510.11251", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11251", "abs": "https://arxiv.org/abs/2510.11251", "authors": ["Rui Xu", "Jiawei Chen", "Zhaoxia Yin", "Cong Kong", "Xinpeng Zhang"], "title": "Large Language Models Are Effective Code Watermarkers", "comment": null, "summary": "The widespread use of large language models (LLMs) and open-source code has\nraised ethical and security concerns regarding the distribution and attribution\nof source code, including unauthorized redistribution, license violations, and\nmisuse of code for malicious purposes. Watermarking has emerged as a promising\nsolution for source attribution, but existing techniques rely heavily on\nhand-crafted transformation rules, abstract syntax tree (AST) manipulation, or\ntask-specific training, limiting their scalability and generality across\nlanguages. Moreover, their robustness against attacks remains limited. To\naddress these limitations, we propose CodeMark-LLM, an LLM-driven watermarking\nframework that embeds watermark into source code without compromising its\nsemantics or readability. CodeMark-LLM consists of two core components: (i)\nSemantically Consistent Embedding module that applies functionality-preserving\ntransformations to encode watermark bits, and (ii) Differential Comparison\nExtraction module that identifies the applied transformations by comparing the\noriginal and watermarked code. Leveraging the cross-lingual generalization\nability of LLM, CodeMark-LLM avoids language-specific engineering and training\npipelines. Extensive experiments across diverse programming languages and\nattack scenarios demonstrate its robustness, effectiveness, and scalability."}
{"id": "2510.11292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11292", "abs": "https://arxiv.org/abs/2510.11292", "authors": ["Wenbo Wu", "Qingyi Si", "Xiurui Pan", "Ye Wang", "Jie Zhang"], "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences", "comment": null, "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."}
{"id": "2510.11345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11345", "abs": "https://arxiv.org/abs/2510.11345", "authors": ["Han Lu", "Zichen Liu", "Shaopan Xiong", "Yancheng He", "Wei Gao", "Yanan Wu", "Weixun Wang", "Jiashun Liu", "Yang Li", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Xiaoyang Li", "Yijia Luo", "Zihe Liu", "Ling Pan", "Junchi Yan", "Wei Wang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng"], "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony", "comment": null, "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training."}
{"id": "2510.11688", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11688", "abs": "https://arxiv.org/abs/2510.11688", "authors": ["Zicheng Liu", "Lige Huang", "Jie Zhang", "Dongrui Liu", "Yuan Tian", "Jing Shao"], "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities", "comment": "Project webpage available at https://pacebench.github.io/", "summary": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models."}
