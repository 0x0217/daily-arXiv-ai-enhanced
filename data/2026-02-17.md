<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 18]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Backdoor Attacks on Contrastive Continual Learning for IoT Systems](https://arxiv.org/abs/2602.13062)
*Alfous Tim,Kuniyilh Simi D*

Main category: cs.LG

TL;DR: 본 논문은 IoT 시스템 내에서 CCL(대조 지속 학습)에 대한 백도어 공격의 취약성을 분석한다.


<details>
  <summary>Details</summary>
Motivation: IoT 시스템은 비정상적인 환경에 적응하기 위해 지속적인 학습에 의존해야 한다.

Method: 대조 지속 학습(CCL)과 재생 기반 리허설 및 안정성 유지를 위한 정규화를 분석하고, IoT 고유의 지속성 메커니즘을 검토한다.

Result: CCL이 IoT 지능을 향상시키는 데 효과적이나, 적절히 보호되지 않을 경우 지속적인 위협을 높일 수 있음을 발견했다.

Conclusion: CCL의 방어 전략을 평가하고, 다양한 학습 패러다임 간의 취약성을 비교한다.

Abstract: The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.

</details>


### [2] [Intrinsic Credit Assignment for Long Horizon Interaction](https://arxiv.org/abs/2602.12342)
*Ilze Amanda Auzina,Joschka Strüber,Sergio Hernández-Gutiérrez,Shashwat Goel,Ameya Prabhu,Matthias Bethge*

Main category: cs.LG

TL;DR: 이 논문에서는 긴 수평에서 불확실성을 탐색할 수 있도록 에이전트를 교육하기 위해 ΔBelief-RL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 불확실성 속에서 긴 수평을 탐색하는 에이전트를 효과적으로 교육하는 방법을 찾기 위해.

Method: ΔBelief-RL은 언어 모델의 내재적 신념을 활용하여 중간 진행에 대한 보상을 제공합니다. 에이전트가 목표 솔루션에 할당하는 확률의 변화를 이용하여 신용 할당을 수행합니다.

Result: 합성 상호 작용 데이터로 교육하여 ΔBelief-RL은 정보 탐색 기능을 학습하고, 결과 기반 보상보다 일관되게 우수한 성능을 보여줍니다. 이러한 개선은 소비자 서비스에서 개인화에 이르는 다양한 응용 프로그램으로 일반화됩니다.

Conclusion: ΔBelief 보상을 통해 중간 행동에 대한 신용 할당을 가능하게 함으로써 긴 수평에서의 불확실성을 탐색하기 위한 확장 가능한 교육 전략을 도입합니다.

Abstract: How can we train agents to navigate uncertainty over long horizons? In this work, we propose ΔBelief-RL, which leverages a language model's own intrinsic beliefs to reward intermediate progress. Our method utilizes the change in the probability an agent assigns to the target solution for credit assignment. By training on synthetic interaction data, ΔBelief-RL teaches information-seeking capabilities that consistently outperform purely outcome-based rewards for Reinforcement Learning, with improvements generalizing to out-of-distribution applications ranging from customer service to personalization. Notably, the performance continues to improve as we scale test-time interactions beyond the training horizon, with interaction-efficiency increasing even on Pass@k metrics. Overall, our work introduces a scalable training strategy for navigating uncertainty over a long-horizon, by enabling credit assignment to intermediate actions via intrinsic ΔBelief rewards.

</details>


### [3] [Value Bonuses using Ensemble Errors for Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.12375)
*Abdul Wahab,Raksha Kumaraswamy,Martha White*

Main category: cs.LG

TL;DR: 이 논문은 VBE(Value Bonuses with Ensemble errors)라는 탐험 알고리즘을 제안하며, 이는 다양한 행동-가치 함수의 앙상블을 유지하여 첫 방문 시 긍정적인 가치 보상을 제공하고 심층 탐험을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습에서 방향성 탐험을 위한 메커니즘으로 낙관적인 가치 추정치를 활용하고자 했다.

Method: VBE 알고리즘은 랜덤 행동-가치 함수의 앙상블을 유지하고, 이러한 함수의 추정 오류를 이용해 첫 방문 낙관성을 제공하는 가치 보상을 설계한다.

Result: VBE는 여러 전통적인 탐험 테스트 환경에서 Bootstrap DQN 및 두 가지 보상 보너스 접근 방식(RND 및 ACB)을 능가하는 성능을 보였다.

Conclusion: VBE는 Atari와 같은 더 복잡한 환경에 쉽게 확장될 수 있다는 것을 입증하는 실험을 제공한다.

Abstract: Optimistic value estimates provide one mechanism for directed exploration in reinforcement learning (RL). The agent acts greedily with respect to an estimate of the value plus what can be seen as a value bonus. The value bonus can be learned by estimating a value function on reward bonuses, propagating local uncertainties around rewards. However, this approach only increases the value bonus for an action retroactively, after seeing a higher reward bonus from that state and action. Such an approach does not encourage the agent to visit a state and action for the first time. In this work, we introduce an algorithm for exploration called Value Bonuses with Ensemble errors (VBE), that maintains an ensemble of random action-value functions (RQFs). VBE uses the errors in the estimation of these RQFs to design value bonuses that provide first-visit optimism and deep exploration. The key idea is to design the rewards for these RQFs in such a way that the value bonus can decrease to zero. We show that VBE outperforms Bootstrap DQN and two reward bonus approaches (RND and ACB) on several classic environments used to test exploration and provide demonstrative experiments that it can scale easily to more complex environments like Atari.

</details>


### [4] [Synthetic Interaction Data for Scalable Personalization in Large Language Models](https://arxiv.org/abs/2602.12394)
*Yuchen Ma,Yue Huang,Wenjie Wang,Xiaonan Luo,Xiangliang Zhang,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 개인화된 프롬프트가 대규모 언어 모델을 다양한 사용자에게 배포할 수 있는 기회를 제공하지만, 기존의 프롬프트 최적화 방법은 주로 작업 수준의 최적화에 초점을 맞추고 사용자 특정 선호도와 개별 사용자의 잠재적 제약을 간과한다. 이를 해결하기 위해 PersonaGym이라는 고충실도 합성 데이터 생성 프레임워크를 도입하고, 개인화된 프롬프트 최적화(PPOpt) 프레임워크를 제안하여 사용자 프롬프트를 최적화한다.


<details>
  <summary>Details</summary>
Motivation: 개인화된 프롬프트는 대규모 언어 모델(LLM)을 다양한 사용자에게 배포할 수 있는 큰 기회를 제공한다. 하지만 기존의 프롬프트 최적화 방법은 주로 작업 수준의 최적화에 중점을 두고 있으며 사용자 특정 선호도와 개별 사용자의 잠재적 제약을 간과하고 있다.

Method: PersonaGym이라 불리는 동적인 선호 프로세스를 모델링하는 프레임워크를 사용하며, 이는 개인화된 다중 턴 상호작용 궤적을 생성하기 위해 현실적인 선호 행동과 의미 인식 노이즈를 시뮬레이션하는 에이전틱 LLM 시스템을 포함한다. 또한 PPOpt라는 확장 가능하고 모델 비종속적인 프레임워크를 통해 사용자 프롬프트를 최적화한다.

Result: PersonaGym을 이용해 생성된 PersonaAtlas는 실제 세계의 선호 표현 및 노이즈 패턴을 밀접하게 반영한 고충실도 다중 턴 개인화 상호작용 궤적의 대규모 고품질 합성 데이터셋이다. PPOpt는 작업 성능, 개인화 품질 및 노이즈 및 희소 선호 신호에 대한 강건성 측면에서 최첨단 기준에 비해 일관된 개선을 보여준다.

Conclusion: 이 연구는 개인화된 사용자 상호작용을 최적화하는 새로운 프레임워크를 제안하고, 이를 통해 실제 사용자 행동과 선호를 더 잘 반영할 수 있음을 입증했다.

Abstract: Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.

</details>


### [5] [Safe Reinforcement Learning via Recovery-based Shielding with Gaussian Process Dynamics Models](https://arxiv.org/abs/2602.12444)
*Alexander W. Goodall,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 이 논문은 안전에 대한 증명 가능한 하한선을 제공하는 새로운 회복 기반 방어 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습(RL)은 최적의 의사결정 및 제어를 위한 강력한 프레임워크이지만, 안전이 중요한 응용 분야에서는 종종 증명 가능한 보장이 부족합니다.

Method: 제안된 접근법은 백업 정책(방어막)을 RL 에이전트와 통합하고, 가우시안 프로세스(GP)에 기반한 불확실성 정량화를 활용하여 안전 제약 조건의 잠재적 위반을 예측합니다.

Result: 경험적으로 우리의 접근법은 강력한 성능과 엄격한 안전 준수를 보여줍니다.

Conclusion: 이 프레임워크는 필요한 경우에만 안전한 경로로 동적으로 회복합니다.

Abstract: Reinforcement learning (RL) is a powerful framework for optimal decision-making and control but often lacks provable guarantees for safety-critical applications. In this paper, we introduce a novel recovery-based shielding framework that enables safe RL with a provable safety lower bound for unknown and non-linear continuous dynamical systems. The proposed approach integrates a backup policy (shield) with the RL agent, leveraging Gaussian process (GP) based uncertainty quantification to predict potential violations of safety constraints, dynamically recovering to safe trajectories only when necessary. Experience gathered by the 'shielded' agent is used to construct the GP models, with policy optimization via internal model-based sampling - enabling unrestricted exploration and sample efficient learning, without compromising safety. Empirically our approach demonstrates strong performance and strict safety-compliance on a suite of continuous control environments.

</details>


### [6] [Regularized Meta-Learning for Improved Generalization](https://arxiv.org/abs/2602.12469)
*Noor Islam S. Mohammad,Md Muntaqim Meherab*

Main category: cs.LG

TL;DR: 딥 앙상블 방법들은 예측 성능을 향상시키지만, 세 가지 실제적인 한계가 있다: 기본 모델 간의 중복으로 인한 계산 비용 증가와 조건 저하, 다중공선성 하에서의 불안정한 가중치, 메타 학습 파이프라인에서의 과적합. 우리는 중복 인식 프로젝션, 통계적 메타 기능 증강 및 교차 검증된 정규화 메타 모델(Ridge, Lasso, ElasticNet)을 결합한 네 단계 파이프라인을 통해 이러한 문제를 해결하는 정규화된 메타 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 딥 앙상블 방법들이 예측 성능을 향상시킬 수 있지만, 계산 비용 증가, 불안정한 가중치, 그리고 메타 학습에서의 과적합 등 세 가지 주요 한계가 있어 이를 해결할 필요가 있다.

Method: 중복 인식 프로젝션, 통계적 메타 기능 증강 및 교차 검증된 정규화 메타 모델을 포함한 네 단계 파이프라인을 통해 정규화된 메타 학습 프레임워크를 제안한다.

Result: 제안한 프레임워크는 Playground Series S6E1 벤치마크에서 8.582의 RMSE를 달성하여, 단순 평균(8.894) 및 전통적인 Ridge 스태킹(8.627)보다 향상되었고, 실행 시간 역시 4배 빠르면서 탐욕적인 힐 클라이밍(8.603)과 일치한다.

Conclusion: 정규화된 메타 학습은 고차원 앙상블 시스템을 위한 안정적이고 효율적인 스태킹 전략으로 자리 잡았다.

Abstract: Deep ensemble methods often improve predictive performance, yet they suffer from three practical limitations: redundancy among base models that inflates computational cost and degrades conditioning, unstable weighting under multicollinearity, and overfitting in meta-learning pipelines. We propose a regularized meta-learning framework that addresses these challenges through a four-stage pipeline combining redundancy-aware projection, statistical meta-feature augmentation, and cross-validated regularized meta-models (Ridge, Lasso, and ElasticNet). Our multi-metric de-duplication strategy removes near-collinear predictors using correlation and MSE thresholds ($τ_{\text{corr}}=0.95$), reducing the effective condition number of the meta-design matrix while preserving predictive diversity. Engineered ensemble statistics and interaction terms recover higher-order structure unavailable to raw prediction columns. A final inverse-RMSE blending stage mitigates regularizer-selection variance. On the Playground Series S6E1 benchmark (100K samples, 72 base models), the proposed framework achieves an out-of-fold RMSE of 8.582, improving over simple averaging (8.894) and conventional Ridge stacking (8.627), while matching greedy hill climbing (8.603) with substantially lower runtime (4 times faster). Conditioning analysis shows a 53.7\% reduction in effective matrix condition number after redundancy projection. Comprehensive ablations demonstrate consistent contributions from de-duplication, statistical meta-features, and meta-ensemble blending. These results position regularized meta-learning as a stable and deployment-efficient stacking strategy for high-dimensional ensemble systems.

</details>


### [7] [Bench-MFG: A Benchmark Suite for Learning in Stationary Mean Field Games](https://arxiv.org/abs/2602.12517)
*Lorenzo Magnino,Jiacheng Shen,Matthieu Geist,Olivier Pietquin,Mathieu Laurière*

Main category: cs.LG

TL;DR: 본 논문에서는 Mean Field Games와 Reinforcement Learning의 교차점에서 발생하는 문제를 해결하기 위한 표준화된 평가 프로토콜의 부재를 지적하고, 이를 해결하기 위해 MFG를 위한 종합 벤치마크 세트를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: MFG와 RL의 교차점에서 대규모 다중 에이전트 시스템을 해결하기 위한 알고리즘이 증가하고 있지만, 표준화된 평가 프로토콜이 부족하여 연구자들이 단편적이고 단순한 환경에 의존해야 하는 문제를 해결하고자 합니다.

Method: 우리는 명확성을 위해 이산 시간, 이산 공간의 정적인 설정에 초점을 맞춘 MFG를 위한 종합 벤치마크 세트(Bench-MFG)를 제안하며, 상호작용이 없는 게임, 단조 게임, 잠재력 게임 및 동역학 결합 게임 등 문제 클래스의 분류 체계를 소개하고 각 클래스에 대한 프로토타입 환경을 제공합니다.

Result: MFG 인스턴스를 생성하기 위한 MF-Garnets라는 방법을 제안하고, 다양한 학습 알고리즘을 이러한 환경에서 벤치마크하였으며, 특히 이용 가능성을 최소화하기 위한 새로운 블랙 박스 접근법(MF-PSO)을 포함하였습니다.

Conclusion: 우리의 광범위한 경험적 결과를 바탕으로 향후 실험 비교를 표준화하기 위한 지침을 제안합니다.

Abstract: The intersection of Mean Field Games (MFGs) and Reinforcement Learning (RL) has fostered a growing family of algorithms designed to solve large-scale multi-agent systems. However, the field currently lacks a standardized evaluation protocol, forcing researchers to rely on bespoke, isolated, and often simplistic environments. This fragmentation makes it difficult to assess the robustness, generalization, and failure modes of emerging methods. To address this gap, we propose a comprehensive benchmark suite for MFGs (Bench-MFG), focusing on the discrete-time, discrete-space, stationary setting for the sake of clarity. We introduce a taxonomy of problem classes, ranging from no-interaction and monotone games to potential and dynamics-coupled games, and provide prototypical environments for each. Furthermore, we propose MF-Garnets, a method for generating random MFG instances to facilitate rigorous statistical testing. We benchmark a variety of learning algorithms across these environments, including a novel black-box approach (MF-PSO) for exploitability minimization. Based on our extensive empirical results, we propose guidelines to standardize future experimental comparisons. Code available at \href{https://github.com/lorenzomagnino/Bench-MFG}{https://github.com/lorenzomagnino/Bench-MFG}.

</details>


### [8] [Multi-Agent Model-Based Reinforcement Learning with Joint State-Action Learned Embeddings](https://arxiv.org/abs/2602.12520)
*Zhizun Wang,David Meger*

Main category: cs.LG

TL;DR: 다수의 에이전트를 부분적으로 관측 가능한 동적 환경에서 조정하는 방법을 제시하는 모델 기반 다중 에이전트 강화 학습 프레임워크.


<details>
  <summary>Details</summary>
Motivation: 부분적으로 관측 가능한 고도로 동적인 환경에서 여러 에이전트를 조정하기 위한 정보 제공 표현 및 데이터 효율적인 훈련 필요.

Method: 변분 자동 인코더로 훈련된 세계 모델과 상태-행동 학습 임베딩(SALE)을 사용하는 새로운 다중 에이전트 강화 학습 프레임워크를 설계.

Result: 구상된 궤적과 SALE 기반 행동 값을 결합하여 에이전트들이 선택이 집단적 결과에 미치는 영향을 더욱 풍부하게 이해하게 됨.

Conclusion: StarCraft II 마이크로 관리, 다중 에이전트 MuJoCo, 레벨 기반 채집 도전 등 잘 확립된 다중 에이전트 벤치마크에 대한 실증 연구에서 기존 알고리즘보다 뛰어난 성과를 입증.

Abstract: Learning to coordinate many agents in partially observable and highly dynamic environments requires both informative representations and data-efficient training. To address this challenge, we present a novel model-based multi-agent reinforcement learning framework that unifies joint state-action representation learning with imaginative roll-outs. We design a world model trained with variational auto-encoders and augment the model using the state-action learned embedding (SALE). SALE is injected into both the imagination module that forecasts plausible future roll-outs and the joint agent network whose individual action values are combined through a mixing network to estimate the joint action-value function. By coupling imagined trajectories with SALE-based action values, the agents acquire a richer understanding of how their choices influence collective outcomes, leading to improved long-term planning and optimization under limited real-environment interactions. Empirical studies on well-established multi-agent benchmarks, including StarCraft II Micro-Management, Multi-Agent MuJoCo, and Level-Based Foraging challenges, demonstrate consistent gains of our method over baseline algorithms and highlight the effectiveness of joint state-action learned embeddings within a multi-agent model-based paradigm.

</details>


### [9] [TRACE: Temporal Reasoning via Agentic Context Evolution for Streaming Electronic Health Records (EHRs)](https://arxiv.org/abs/2602.12833)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: TRACE 프레임워크는 동적 환자 상태를 인식하고 장기적으로 안정적인 임상 추론을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)은 방대한 의학 지식을 담고 있지만, 환자의 장기적인 변화에 대해 신뢰성 있게 적용하기 어려움이 있다.

Method: TRACE는 정적 글로벌 프로토콜과 동적 개별 프로토콜로 구성된 이중 메모리 아키텍처를 통해 시간적 추론을 가능하게 하며, 컨텍스트를 명시적으로 구조화하고 유지한다.

Result: MIMIC-IV의 장기적인 임상 이벤트 스트림에서 TRACE는 다음 이벤트 예측 정확도와 프로토콜 준수 및 임상 안전성을 크게 개선하였다.

Conclusion: TRACE는 장기 컨텍스트 및 검색 기반 방법보다 더 나은 성능을 제공하며, 해석 가능하고 감사 가능한 추론 과정을 생성한다.

Abstract: Large Language Models (LLMs) encode extensive medical knowledge but struggle to apply it reliably to longitudinal patient trajectories, where evolving clinical states, irregular timing, and heterogeneous events degrade performance over time. Existing adaptation strategies rely on fine-tuning or retrieval-based augmentation, which introduce computational overhead, privacy constraints, or instability under long contexts. We introduce TRACE (Temporal Reasoning via Agentic Context Evolution), a framework that enables temporal clinical reasoning with frozen LLMs by explicitly structuring and maintaining context rather than extending context windows or updating parameters. TRACE operates over a dual-memory architecture consisting of a static Global Protocol encoding institutional clinical rules and a dynamic Individual Protocol tracking patient-specific state. Four agentic components, Router, Reasoner, Auditor, and Steward, coordinate over this structured memory to support temporal inference and state evolution. The framework maintains bounded inference cost via structured state compression and selectively audits safety-critical clinical decisions. Evaluated on longitudinal clinical event streams from MIMIC-IV, TRACE significantly improves next-event prediction accuracy, protocol adherence, and clinical safety over long-context and retrieval-augmented baselines, while producing interpretable and auditable reasoning traces.

</details>


### [10] [Fractional Order Federated Learning for Battery Electric Vehicle Energy Consumption Modeling](https://arxiv.org/abs/2602.12567)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 이 논문은 퍼지 학습에서의 불안정을 개선하기 위한 FO-RI-FedAvg 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 연결된 전기차에서의 연합 학습은 간헐적인 연결, 시간에 따른 클라이언트 참여 변화, 다양한 작동 조건에 의해 유도된 클라이언트 간 차이로 불안정성을 경험한다.

Method: FO-RI-FedAvg는 클라이언트 측에서 두 가지 보완 메커니즘을 사용하여 안정성을 개선하는 FedAvg의 경량화된 확장이다: 1) 적응형 거칠음 기반의 근접 정규화, 2) 비정수 차수의 지역 최적화.

Result: FO-RI-FedAvg는 두 개의 실제 BEV 에너지 예측 데이터셋(VED, eVED)에서 강력한 연합 기준선에 비해 향상된 정확도와 안정적인 수렴을 보여준다.

Conclusion: 특히 클라이언트 참여가 감소된 상황에서 FO-RI-FedAvg가 우수한 성능을 보인다.

Abstract: Federated learning on connected electric vehicles (BEVs) faces severe instability due to intermittent connectivity, time-varying client participation, and pronounced client-to-client variation induced by diverse operating conditions. Conventional FedAvg and many advanced methods can suffer from excessive drift and degraded convergence under these realistic constraints. This work introduces Fractional-Order Roughness-Informed Federated Averaging (FO-RI-FedAvg), a lightweight and modular extension of FedAvg that improves stability through two complementary client-side mechanisms: (i) adaptive roughness-informed proximal regularization, which dynamically tunes the pull toward the global model based on local loss-landscape roughness, and (ii) non-integer-order local optimization, which incorporates short-term memory to smooth conflicting update directions. The approach preserves standard FedAvg server aggregation, adds only element-wise operations with amortizable overhead, and allows independent toggling of each component. Experiments on two real-world BEV energy prediction datasets, VED and its extended version eVED, show that FO-RI-FedAvg achieves improved accuracy and more stable convergence compared to strong federated baselines, particularly under reduced client participation.

</details>


### [11] [Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL](https://arxiv.org/abs/2602.12636)
*Xin Liu,Yixuan Li,Yuhui Chen,Yuxing Qin,Haoran Li,Dongbin Zhao*

Main category: cs.LG

TL;DR: DEG는 인간 주석이나 광범위한 감독 없이 샘플 효율적인 조밀한 보상을 찾기 위한 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 적합한 보상을 설계하는 것은 강화 학습(RL)에서 특히 구체적인 조작에 큰 도전입니다.

Method: DEG는 큰 비디오 생성 모델의 사전 지식을 활용하여 각 RL 에피소드에 대한 전담 작업 지침을 생성합니다.

Result: DEG는 18가지 다양한 작업에서 스파스 성공 보상을 신속하게 발견하는 데 도움을 주고, 효과적인 RL 및 안정적인 정책 수렴을 안내함을 보여줍니다.

Conclusion: 이 결과는 DEG가 강화 학습에 있어 효과적인 탐색 자극으로 작용할 수 있음을 시사합니다.

Abstract: Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.

</details>


### [12] [Mixture of Predefined Experts: Maximizing Data Usage on Vertical Federated Learning](https://arxiv.org/abs/2602.12708)
*Jon Irureta,Gorka Azkune,Jon Imaz,Aizea Lojo,Javier Fernandez-Marques*

Main category: cs.LG

TL;DR: Split-MoPE는 수직 연합 학습에서 샘플 정렬 문제를 해결하고자 하는 새로운 프레임워크로, 미리 정의된 전문가를 이용하여 데이터 사용을 극대화하며, 통신 비용을 줄이고 높은 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: 수직 연합 학습(VFL)은 금융 및 의료와 같은 개인 정보 보호가 중요한 도메인에서의 공동 모델 훈련을 위한 중요한 패러다임이다. 그러나 기존의 VFL 프레임워크는 참가자 간의 완전한 샘플 정렬이라는 이상화된 가정에 의존하는데, 이는 실제 상황에서 거의 실현되지 않는다.

Method: 이 연구에서는 Split Learning과 미리 정의된 전문가의 혼합 아키텍처(MoPE)를 통합한 새로운 프레임워크인 Split-MoPE를 소개한다. MoPE는 특정 데이터 정렬을 처리하기 위해 미리 정의된 전문가를 활용하며, 전체 샘플 오버랩 없이도 훈련 및 추론 시 데이터 사용을 극대화한다.

Result: Split-MoPE는 목표 데이터 도메인에 대한 사전 훈련된 인코더를 활용하여 단일 통신 라운드에서 최첨단 성능을 달성하고, 다중 라운드의 종단 간 훈련에 비해 통신 비용을 크게 줄인다.

Conclusion: 기존의 샘플 비정렬 문제를 다루는 제안들과 달리, 이 새로운 아키텍처는 악의적이거나 노이즈가 있는 참가자에 대한 내재적 탄력성을 제공하며, 각 예측에 대한 각 협력자의 기여도를 정량화하여 사례별 해석 가능성을 높인다. 여러 평가에서 Split-MoPE는 LASER 및 Vertical SplitNN과 같은 최첨단 시스템을 지속적으로 초월한다.

Abstract: Vertical Federated Learning (VFL) has emerged as a critical paradigm for collaborative model training in privacy-sensitive domains such as finance and healthcare. However, most existing VFL frameworks rely on the idealized assumption of full sample alignment across participants, a premise that rarely holds in real-world scenarios. To bridge this gap, this work introduces Split-MoPE, a novel framework that integrates Split Learning with a specialized Mixture of Predefined Experts (MoPE) architecture. Unlike standard Mixture of Experts (MoE), where routing is learned dynamically, MoPE uses predefined experts to process specific data alignments, effectively maximizing data usage during both training and inference without requiring full sample overlap. By leveraging pretrained encoders for target data domains, Split-MoPE achieves state-of-the-art performance in a single communication round, significantly reducing the communication footprint compared to multi-round end-to-end training. Furthermore, unlike existing proposals that address sample misalignment, this novel architecture provides inherent robustness against malicious or noisy participants and offers per-sample interpretability by quantifying each collaborator's contribution to each prediction. Extensive evaluations on vision (CIFAR-10/100) and tabular (Breast Cancer Wisconsin) datasets demonstrate that Split-MoPE consistently outperforms state-of-the-art systems such as LASER and Vertical SplitNN, particularly in challenging scenarios with high data missingness.

</details>


### [13] [ADEPT: RL-Aligned Agentic Decoding of Emotion via Evidence Probing Tools -- From Consensus Learning to Ambiguity-Driven Emotion Reasoning](https://arxiv.org/abs/2602.12714)
*Esther Sun,Bo-Hao Su,Abinay Reddy Naini,Shinji Watanabe,Carlos Busso*

Main category: cs.LG

TL;DR: ADEPT라는 프레임워크는 감정 인식을 다회전 탐색 프로세스로 재정의하여 SLLM과 자가 감독 음성 인코더의 간극을 메우고, 감정 판단을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 Speech Large Language Models(SLLMs)는 높은 수준의 감정 추론을 가능하게 하지만 신뢰할 수 있는 음향 증거 없이 편향된 판단을 내린다.

Method: ADEPT는 감정 인식을 다회전 탐색 프로세스로 재구성하고, 후보 감정 세트를 유지하며, 구조화된 파이프라인 내에서 전용 의미 및 음향 도구를 호출한다.

Result: 실험을 통해 ADEPT는 대부분의 설정에서 주요 감정 정확도를 향상시켰으며, 미세 감정 특성을 크게 개선하여 감사 가능한 음향 및 의미 증거에 기반한 설명을 생성했다.

Conclusion: ADEPT는 증거 기반 추론을 강화하며, 도구 사용 행동과 예측 품질을 연결하여 인간의 감정 복잡성을 효과적으로 관리한다.

Abstract: Speech Large Language Models (SLLMs) enable high-level emotion reasoning but often produce ungrounded, text-biased judgments without verifiable acoustic evidence. In contrast, self-supervised speech encoders such as WavLM provide strong acoustic representations yet remain opaque discriminative models with limited interpretability. To bridge this gap, we introduce ADEPT (Agentic Decoding of Emotion via Evidence Probing Tools), a framework that reframes emotion recognition as a multi-turn inquiry process rather than a single-pass prediction. ADEPT transforms an SLLM into an agent that maintains an evolving candidate emotion set and adaptively invokes dedicated semantic and acoustic probing tools within a structured pipeline of candidate generation, evidence collection, and adjudication. Crucially, ADEPT enables a paradigm shift from consensus learning to ambiguity-driven emotion reasoning. Since human affect exhibits inherent complexity and frequent co-occurrence of emotions, we treat minority annotations as informative perceptual signals rather than discarding them as noise. Finally, we integrate Group Relative Policy Optimization (GRPO) with an Evidence Trust Gate to explicitly couple tool-usage behaviors with prediction quality and enforce evidence-grounded reasoning. Experiments show that ADEPT improves primary emotion accuracy in most settings while substantially improving minor emotion characterization, producing explanations grounded in auditable acoustic and semantic evidence.

</details>


### [14] [Closing the Loop: A Control-Theoretic Framework for Provably Stable Time Series Forecasting with LLMs](https://arxiv.org/abs/2602.12756)
*Xingyu Zhang,Hanyun Du,Zeen Song,Jianqi Zhang,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 이 논문은 기존의 자가회귀 생성 전략의 문제점을 지적하고, 제어 이론을 기반으로 한 폐쇄 루프 프레임워크인 F-LLM을 제안하여 시간 시계열 예측에서 오류 전파를 효과적으로 완화한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 시간 시계열 예측에서 뛰어난 잠재력을 보이지만, 기존 접근 방식이 자가회귀 생성 전략을 단순히 적용하는 문제를 해결하고자 한다.

Method: F-LLM이라는 새로운 폐쇄 루프 프레임워크를 제안하고, 이를 위해 학습 가능한 잔여 추정기(옵저버)와 피드백 제어기를 활용하여 경로를 안정화하는 방법을 사용한다.

Result: F-LLM은 오류 전파를 상당히 완화하며, 시간 시계열 벤치마크에서 좋은 성능을 달성한다.

Conclusion: F-LLM은 기본 모델이 지역 리프시츠 제약 조건을 만족하는 경우, 균일하게 제한된 오류를 보장하는 이론적 근거를 제공한다.

Abstract: Large Language Models (LLMs) have recently shown exceptional potential in time series forecasting, leveraging their inherent sequential reasoning capabilities to model complex temporal dynamics. However, existing approaches typically employ a naive autoregressive generation strategy. We identify a critical theoretical flaw in this paradigm: during inference, the model operates in an open-loop manner, consuming its own generated outputs recursively. This leads to inevitable error accumulation (exposure bias), where minor early deviations cascade into significant trajectory drift over long horizons. In this paper, we reformulate autoregressive forecasting through the lens of control theory, proposing \textbf{F-LLM} (Feedback-driven LLM), a novel closed-loop framework. Unlike standard methods that passively propagate errors, F-LLM actively stabilizes the trajectory via a learnable residual estimator (Observer) and a feedback controller. Furthermore, we provide a theoretical guarantee that our closed-loop mechanism ensures uniformly bounded error, provided the base model satisfies a local Lipschitz constraint. Extensive experiments demonstrate that F-LLM significantly mitigates error propagation, achieving good performance on time series benchmarks.

</details>


### [15] [X-VORTEX: Spatio-Temporal Contrastive Learning for Wake Vortex Trajectory Forecasting](https://arxiv.org/abs/2602.12869)
*Zhan Qu,Michael Färber*

Main category: cs.LG

TL;DR: X-VORTEX는 비표시된 LiDAR 포인트 클라우드 시퀀스로부터 물리 기반 표현을 학습하는 스페이토-템포랄 대조 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 항공교통 관리에서 항공기의 웨이크 와제가 안전과 용량의 주요 도전 과제가 되고 있다.

Method: 사전 훈련된 모델을 사용하여, 약하게 혼란스러운 시퀀스와 강하게 증강된 시퀀스를 짝지어 입력하여 감지기 희소성과 시간에 따라 변하는 와류 동역학 문제를 해결한다.

Result: X-VORTEX는 100만 개 이상의 LiDAR 스캔에 대한 실제 데이터 세트에서 우수한 와류 중심 로컬라이제이션을 달성하였다.

Conclusion: X-VORTEX는 효율적인 데이터 사용과 정확한 궤적 예측을 지원한다.

Abstract: Wake vortices are strong, coherent air turbulences created by aircraft, and they pose a major safety and capacity challenge for air traffic management. Tracking how vortices move, weaken, and dissipate over time from LiDAR measurements is still difficult because scans are sparse, vortex signatures fade as the flow breaks down under atmospheric turbulence and instabilities, and point-wise annotation is prohibitively expensive. Existing approaches largely treat each scan as an independent, fully supervised segmentation problem, which overlooks temporal structure and does not scale to the vast unlabeled archives collected in practice. We present X-VORTEX, a spatio-temporal contrastive learning framework grounded in Augmentation Overlap Theory that learns physics-aware representations from unlabeled LiDAR point cloud sequences. X-VORTEX addresses two core challenges: sensor sparsity and time-varying vortex dynamics. It constructs paired inputs from the same underlying flight event by combining a weakly perturbed sequence with a strongly augmented counterpart produced via temporal subsampling and spatial masking, encouraging the model to align representations across missing frames and partial observations. Architecturally, a time-distributed geometric encoder extracts per-scan features and a sequential aggregator models the evolving vortex state across variable-length sequences. We evaluate on a real-world dataset of over one million LiDAR scans. X-VORTEX achieves superior vortex center localization while using only 1% of the labeled data required by supervised baselines, and the learned representations support accurate trajectory forecasting.

</details>


### [16] [Multi-Dimensional Visual Data Recovery: Scale-Aware Tensor Modeling and Accelerated Randomized Computation](https://arxiv.org/abs/2602.12982)
*Wenjin Qin,Hailin Wang,Jiangjun Peng,Jianjun Wang,Tingwen Huang*

Main category: cs.LG

TL;DR: 본 논문은 FCTN 분해를 기반으로 한 다차원 데이터 복구 방법의 효율성과 모델링 능력을 개선하기 위해 새로운 패러다임을 제안한다.


<details>
  <summary>Details</summary>
Motivation: FCTN 분해를 활용한 기존의 다차원 데이터 복구 방법은 컴퓨터 효율성과 모델링 능력에서 개선이 필요하다.

Method: 경량화된 비선형 정규화 패러다임을 제안하고, ADMM 프레임워크를 이용하여 효율적인 최적화 알고리즘을 도출하며, 수치 선형 대수의 스케치 기법을 기반으로 빠르고 효율적인 무작위 압축 알고리즘을 개발한다.

Result: 제안된 알고리즘은 기존의 첨단 방법들에 비해 정량적 메트릭, 시각적 품질 및 실행 시간에서 효율성과 우수성을 입증했다.

Conclusion: 이 연구는 다차원 데이터 처리에서 FCTN 기반 접근 방식의 가능성을 보여주며, 더욱 향상된 데이터 복구 방법 개발의 기초를 제공한다.

Abstract: The recently proposed fully-connected tensor network (FCTN) decomposition has demonstrated significant advantages in correlation characterization and transpositional invariance, and has achieved notable achievements in multi-dimensional data processing and analysis. However, existing multi-dimensional data recovery methods leveraging FCTN decomposition still have room for further enhancement, particularly in computational efficiency and modeling capability. To address these issues, we first propose a FCTN-based generalized nonconvex regularization paradigm from the perspective of gradient mapping. Then, reliable and scalable multi-dimensional data recovery models are investigated, where the model formulation is shifted from unquantized observations to coarse-grained quantized observations. Based on the alternating direction method of multipliers (ADMM) framework, we derive efficient optimization algorithms with convergence guarantees to solve the formulated models. To alleviate the computational bottleneck encountered when processing large-scale multi-dimensional data, fast and efficient randomized compression algorithms are devised in virtue of sketching techniques in numerical linear algebra. These dimensionality-reduction techniques serve as the computational acceleration core of our proposed algorithm framework. Theoretical results on approximation error upper bounds and convergence analysis for the proposed method are derived. Extensive numerical experiments illustrate the effectiveness and superiority of the proposed algorithm over other state-of-the-art methods in terms of quantitative metrics, visual quality, and running time.

</details>


### [17] [GPTZero: Robust Detection of LLM-Generated Texts](https://arxiv.org/abs/2602.13042)
*George Alexandru Adam,Alexander Cui,Edwin Thomas,Emily Napier,Nazar Shmatko,Jacob Schnell,Jacob Junqi Tian,Alekhya Dronavalli,Edward Tian,Dongwon Lee*

Main category: cs.LG

TL;DR: GPTZero는 인간과 AI가 생성한 텍스트를 구별하는 최신 AI 감지 솔루션이다.


<details>
  <summary>Details</summary>
Motivation: AI 생성 텍스트와 인간이 작성한 텍스트를 구별하는 것이 점점 더 중요해지고 있다.

Method: GPTZero는 계층적 다중 작업 아키텍처를 도입하여 인간과 AI 텍스트의 유연한 분류를 가능하게 한다.

Result: 다양한 도메인에서 최첨단 정확도를 보여 주며, 적대적 공격과 패러프레이즈에 대한 우수한 강인성을 달성하였다.

Conclusion: GPTZero는 정확하고 설명 가능한 감지를 제공하며 사용자에게 책임 있는 사용을 교육하여 텍스트 평가의 공정성과 투명성을 보장한다.

Abstract: While historical considerations surrounding text authenticity revolved primarily around plagiarism, the advent of large language models (LLMs) has introduced a new challenge: distinguishing human-authored from AI-generated text. This shift raises significant concerns, including the undermining of skill evaluations, the mass-production of low-quality content, and the proliferation of misinformation. Addressing these issues, we introduce GPTZero a state-of-the-art industrial AI detection solution, offering reliable discernment between human and LLM-generated text. Our key contributions include: introducing a hierarchical, multi-task architecture enabling a flexible taxonomy of human and AI texts, demonstrating state-of-the-art accuracy on a variety of domains with granular predictions, and achieving superior robustness to adversarial attacks and paraphrasing via multi-tiered automated red teaming. GPTZero offers accurate and explainable detection, and educates users on its responsible use, ensuring fair and transparent assessment of text.

</details>


### [18] [Quantization-Aware Collaborative Inference for Large Embodied AI Models](https://arxiv.org/abs/2602.13052)
*Zhonghao Lyu,Ming Xiao,Mikael Skoglund,Merouane Debbah,H. Vincent Poor*

Main category: cs.LG

TL;DR: LAIMs는 임베디드 AI 애플리케이션의 핵심 지능 엔진으로 점점 더 인식되고 있지만, 그로 인한 자원 제한 문제를 해결하기 위해 양자화 인식 협력 추론을 연구함. 이 방법은 왜곡을 최소화하고 지연 및 에너지 제약을 고려한 양자화 비트폭과 계산 주파수를 공동 설계하는 것을 목표로 함.


<details>
  <summary>Details</summary>
Motivation: LAIM의 대규모 매개변수와 계산 요구사항이 자원 제한이 있는 임베디드 에이전트에게 큰 도전과제가 됨.

Method: 양자화로 인한 추론 왜곡을 위한 근사치를 개발하고, 왜곡 상한을 최소화하기 위한 양자화 비트폭 및 계산 주파수의 공동 설계 문제를 설정함.

Result: 제안된 왜곡 근사치와 기인한 비율-왜곡 경계 및 공동 설계의 효과성을 검증하는 광범위한 평가가 이루어짐.

Conclusion: 특히, 임베디드 AI 시스템에서 추론의 질, 지연 시간 및 에너지 소비의 균형을 맞추는데 유용함을 보여줌.

Abstract: Large artificial intelligence models (LAIMs) are increasingly regarded as a core intelligence engine for embodied AI applications. However, the massive parameter scale and computational demands of LAIMs pose significant challenges for resource-limited embodied agents. To address this issue, we investigate quantization-aware collaborative inference (co-inference) for embodied AI systems. First, we develop a tractable approximation for quantization-induced inference distortion. Based on this approximation, we derive lower and upper bounds on the quantization rate-inference distortion function, characterizing its dependence on LAIM statistics, including the quantization bit-width. Next, we formulate a joint quantization bit-width and computation frequency design problem under delay and energy constraints, aiming to minimize the distortion upper bound while ensuring tightness through the corresponding lower bound. Extensive evaluations validate the proposed distortion approximation, the derived rate-distortion bounds, and the effectiveness of the proposed joint design. Particularly, simulations and real-world testbed experiments demonstrate the effectiveness of the proposed joint design in balancing inference quality, latency, and energy consumption in edge embodied AI systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [19] [Neighborhood Blending: A Lightweight Inference-Time Defense Against Membership Inference Attacks](https://arxiv.org/abs/2602.12943)
*Osama Zafar,Shaojie Zhan,Tianxi Ji,Erman Ayday*

Main category: cs.CR

TL;DR: 백그라운드 블렌딩은 머신러닝 모델에 대한 멤버십 추론 공격(MIA)을 완화하는 새로운 방어 메커니즘으로, 재교육 없이 모델의 유용성을 유지하면서도 높은 개인정보 보호를 제공한다.


<details>
  <summary>Details</summary>
Motivation: MLaaS의 확산이 개인정보 보호 문제를 야기하고 있으며, 특히 MIA에 대한 방어 필요성이 커지고 있다.

Method: Neighborhood Blending이라는 새로운 방어 메커니즘을 소개하여, 모델 재교육 없이 쿼리된 샘플의 이웃을 기반으로 모델의 신뢰도 출력을 부드럽게 한다.

Result: Neighborhood Blending은 MIA의 성공률을 상당히 감소시키면서도 모델의 성능을 유지한다.

Conclusion: 이 방법은 높은 유용성을 제공하면서도 개인정보 보호를 강화할 수 있는 모델에 구애받지 않는 경량 솔루션이다.

Abstract: In recent years, the widespread adoption of Machine Learning as a Service (MLaaS), particularly in sensitive environments, has raised considerable privacy concerns. Of particular importance are membership inference attacks (MIAs), which exploit behavioral discrepancies between training and non-training data to determine whether a specific record was included in the model's training set, thereby presenting significant privacy risks. Although existing defenses, such as adversarial regularization, DP-SGD, and MemGuard, assist in mitigating these threats, they often entail trade-offs such as compromising utility, increased computational requirements, or inconsistent protection against diverse attack vectors.
  In this paper, we introduce a novel inference-time defense mechanism called Neighborhood Blending, which mitigates MIAs without retraining the model or incurring significant computational overhead. Our approach operates post-training by smoothing the model's confidence outputs based on the neighborhood of a queried sample. By averaging predictions from similar training samples selected using differentially private sampling, our method establishes a consistent confidence pattern, rendering members and non-members indistinguishable to an adversary while maintaining high utility. Significantly, Neighborhood Blending maintains label integrity (zero label loss) and ensures high utility through an adaptive, "pay-as-you-go" distortion strategy. It is a model-agnostic approach that offers a practical, lightweight solution that enhances privacy without sacrificing model utility. Through extensive experiments across diverse datasets and models, we demonstrate that our defense significantly reduces MIA success rates while preserving model performance, outperforming existing post-hoc defenses like MemGuard and training-time techniques like DP-SGD in terms of utility retention.

</details>


### [20] [Cryptographic Choreographies](https://arxiv.org/abs/2602.12967)
*Sebastian Mödersheim,Simon Lund,Alessandro Bruni,Marco Carbone,Rosario Giustolisi*

Main category: cs.CR

TL;DR: CryptoChoreo는 암호 프로토콜을 명세하기 위한 안무 언어로, 전체 프로토콜에 대한 직관적인 고수준의 뷰를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 암호 프로토콜의 명세를 보다 직관적으로 이해하고 표현하기 위함이다.

Method: CryptoChoreo의 의미론을 프로세스 계산법으로의 변환을 통해 정의하며, 이를 통해 프로토콜의 동작 방식을 이해한다.

Result: 프로토콜에 대한 대리 이론을 사용하여 여러 사례 연구를 통해 접근 방식의 실용 가능성을 입증하였다.

Conclusion: 이 접근 방식은 암호 프로토콜의 명세에 효과적이며, 실용적으로 구현할 수 있다.

Abstract: We present CryptoChoreo, a choreography language for the specification of cryptographic protocols. Choreographies can be regarded as an extension of Alice-and-Bob notation, providing an intuitive high-level view of the protocol as a whole (rather than specifying each protocol role in isolation). The extensions over standard Alice-and-Bob notation that we consider are non-deterministic choice, conditional branching, and mutable long-term memory. We define the semantics of CryptoChoreo by translation to a process calculus. This semantics entails an understanding of the protocol: it determines how agents parse and check incoming messages and how they construct outgoing messages, in the presence of an arbitrary algebraic theory and non-deterministic choices made by other agents. While this semantics entails algebraic problems that are in general undecidable, we give an implementation for a representative theory. We connect this translation to ProVerif and show on a number of case studies that the approach is practically feasible.

</details>


### [21] [In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach](https://arxiv.org/abs/2602.13156)
*Yiran Gao,Kim Hammar,Tao Li*

Main category: cs.CR

TL;DR: 본 논문은 사이버 공격에 대한 자율적인 대응 시스템을 개발하기 위해 대규모 언어 모델(LLM)을 활용하여 사건 대응 계획을 위한 경량 솔루션을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 급변하는 사이버 공격은 변화하는 위협에 자율적으로 학습하고 적응할 수 있는 사건 대응 시스템을 요구한다.

Method: 우리는 LLM의 사전 훈련된 보안 지식과 맥락 학습을 활용하여 사건 대응 계획을 위한 경량 에이전트를 제안한다. 이 에이전트는 인식, 추론, 계획, 행동의 네 가지 기능을 통합한다.

Result: LLM 에이전트는 시스템 로그를 처리하고 네트워크 상태를 추론하며, 다양한 대응 전략 하에서 결과를 시뮬레이션하고, 효과적인 응답을 생성할 수 있다. 문헌에서 보고된 사건 로그로 평가할 때, 우리의 에이전트는 최신 LLM보다 최대 23% 더 빠른 복구를 달성한다.

Conclusion: 우리의 에이전트 접근법은 모델링이 필요 없으며, 상용 하드웨어에서 실행될 수 있다.

Abstract: Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models' (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench는 게임 이론적 구조를 포함한 2,009개의 시나리오를 통해 다중 에이전트 환경에서의 AI 안전성을 평가하는 벤치마크입니다.


<details>
  <summary>Details</summary>
Motivation: 기존 AI 안전 벤치마크는 주로 단일 에이전트를 평가하여 다중 에이전트 리스크를 잘 이해하지 못함.

Method: GT-HarmBench라는 2,009개의 고위험 시나리오를 설정하고, 15가지 최첨단 모델을 통해 에이전트의 사회적 행동을 평가.

Result: 에이전트가 사회적으로 유익한 행동을 선택하는 비율은 62%에 불과하며, 게임 이론적 개입이 결과를 18%까지 개선함.

Conclusion: 결과는 다중 에이전트 환경에서의 신뢰성 갭을 강조하고, 정렬 연구를 위한 표준화된 테스트베드를 제공합니다.

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [23] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: 우리는 웹 에이전트를 위한 고품질 훈련 데이터를 자동으로 생성하는 확장 가능한 파이프라인을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 고품질 훈련 사례를 식별하는 데 있어 주요 도전과제는 작업 완료를 향한 진행 상황을 정량화하는 궤적 평가이다.

Method: 우리는 작업 완료를 향한 진행 상황을 세밀하게 평가하는 새로운 제약 기반 평가 프레임워크를 도입한다.

Result: 우리는 20개의 인기 웹사이트에서 복잡한 예약 작업으로 구성된 새로운 벤치마크인 BookingArena에서 우리의 방법을 평가했으며, 증류된 학생 모델이 오픈 소스 접근 방식을 능가하고 상업용 시스템에 맞먹거나 이를 초과 성능을 보이는 동시에 상당히 작은 모델임을 보여준다.

Conclusion: 우리의 작업은 다양하고 현실적인 웹 상호작용 데이터 세트를 효율적으로 생성하는 문제를 해결하고 복잡한 구조화된 웹 작업을 위한 체계적인 평가 방법론을 제공한다.

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [24] [GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics](https://arxiv.org/abs/2602.12617)
*Modi Jin,Yiming Zhang,Boyuan Sun,Dingwen Zhang,MingMing Cheng,Qibin Hou*

Main category: cs.AI

TL;DR: GeoAgent는 인간과 밀접하게 추론하고 세부적인 주소 결론을 도출할 수 있는 모델이다. 이 모델은 기존의 RL 기반 방법의 한계를 극복하며, 지리적 특성과의 충돌 문제를 해결하기 위해 GeoSeek라는 새로운 지리 위치 데이터셋을 소개하고, 지리적 작업의 특성을 탐구하여 일관성 에이전트에 의해 평가되는 geo-similarity 보상과 일관성 보상을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존 RL 기반 방법들은 성능과 해석 가능성에서 돌파구를 마련했지만, AI가 생성한 사고의 연쇄(Cot) 데이터와 훈련 전략에 대한 의존 때문에 여전히 문제점이 있다.

Method: GeoSeek라는 새로운 지리 위치 데이터셋을 소개하고, 지리적 작업의 고유한 특성을 탐구하여 geo-similarity 보상과 일관성 보상을 제안한다.

Result: GeoAgent는 다양한 작업에서 기존 방법과 일반 VLLM을 초월하는 성과를 보여주며, 인간의 사고와 밀접하게 연관된 추론을 생성한다.

Conclusion: GeoAgent는 지리적 관점에서 올바른 답변으로 수렴하도록 모델을 유도하면서 추론 프로세스의 무결성과 일관성을 보장한다.

Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.

</details>


### [25] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: 재고 관리에서 LLM과 OR 알고리즘, 인간 간 상호작용이 이루어지는 방식을 연구하였고, LLM 기반 방법이 전통적인 의사결정 파이프라인에 통합될 수 있는 가능성을 탐구하였다.


<details>
  <summary>Details</summary>
Motivation: 재고 관리 문제에서의 전통적인 주문 결정 방식의 한계를 극복하고자 한다.

Method: OR 알고리즘, LLM 및 인간의 상호작용을 연구하고, InventoryBench라는 벤치마크를 구축하여 의사결정 규칙을 테스트한다.

Result: OR-강화 LLM 방법이 각각의 방법이 단독으로 사용할 때보다 우수함을 발견하였다.

Conclusion: 인간-AI 팀이 독립적으로 운영되는 인간이나 AI보다 평균적으로 더 높은 수익을 올리는 것을 보여주었다.

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [26] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter는 에이전트가 각 단계에서 인지 깊이를 동적으로 조절하도록 훈련시키는 프레임워크로, 효율적인 장기 결정-making을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 에이전트들은 고정된 인지 패턴에 의존하고 있으며, 이는 단계마다 다르게 요구되는 인지 요구를 효과적으로 처리하지 못한다.

Method: ACT-R 이론에 기반한 네 가지 계층적 인지 수준을 설계하고, 인지 인식 감독 미세 조정(CoSFT)과 인지 인식 정책 최적화(CoPO)로 훈련 과정을 두 단계로 나눈다.

Result: CogRouter는 ALFWorld 및 ScienceWorld 실험에서 최첨단 성능을 나타내며, 82.3%의 성공률을 기록하고 GPT-4o, OpenAI-o3 및 GRPO보다 월등한 효율성을 보여준다.

Conclusion: CogRouter는 필요한 인지 깊이를 동적으로 조절함으로써 여러 판단을 최적화할 수 있다.

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [27] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: 이 논문은 LLM 에이전트를 위한 기본 기술의 효과를 평가하기 위한 SkillsBench라는 벤치마크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트에 대한 기본 기술의 효과를 표준화된 방법으로 측정할 필요가 있습니다.

Method: 총 11개 도메인에서 86개의 작업으로 구성된 SkillsBench를 통해, 세 가지 조건(기본 기술 없음, 선별된 기술, 자가 생성된 기술)에서 에이전트 모델을 평가했습니다.

Result: 선별된 기술은 평균 통과율을 16.2pp 개선했지만, 도메인에 따라 차이가 크고 84개 작업 중 16개가 부정적인 결과를 보였습니다.

Conclusion: Focused Skills는 포괄적인 문서보다 성능이 뛰어나며, 기술을 추가한 소형 모델이 기술이 없는 대형 모델과 동등한 성능을 보입니다.

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [28] [WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning](https://arxiv.org/abs/2602.12852)
*Junjie Wang,Zequn Xie,Dan Yang,Jie Feng,Yue Shen,Duolin Sun,Meixiu Long,Yihan Jiao,Zhehao Tan,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: WebClipper는 웹 에이전트의 경로를 그래프 기반으로 압축하여 검색 효율성을 향상시키는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 웹 에이전트를 기반으로 한 딥 리서치 시스템은 복잡한 정보 탐색 작업을 해결하는 데 강력한 잠재력을 가지고 있지만, 그 검색 효율성은 충분히 연구되지 않았다.

Method: WebClipper는 에이전트의 검색 과정을 상태 그래프 모델로 만들고, 경로 최적화를 최소 필요 유향 비순환 그래프(DAG) 탐색 문제로 간주하여, 본질적인 추론을 유지하면서 불필요한 단계를 제거하는 경로를 생성한다.

Result: 정제된 경로에 대한 지속적인 훈련을 통해 에이전트는 더 효율적인 검색 패턴으로 진화하며, 도구 호출 횟수를 약 20% 줄이고 정확도를 향상시킨다.

Conclusion: WebClipper는 우수한 성능 속에서 도구 호출 횟수를 압축하며, 웹 에이전트 설계에서 효과성과 효율성을 조화롭게 하는 데 대한 실질적 통찰을 제공한다.

Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.

</details>


### [29] [BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2602.12876)
*Huanyao Zhang,Jiepeng Zhou,Bo Li,Bowen Zhou,Yanzhe Dan,Haishan Lu,Zhiyong Cao,Jiaoyang Chen,Yuqian Han,Zinan Sheng,Zhengwei Tao,Hao Liang,Jialong Wu,Yang Shi,Yuanpeng He,Jiaye Lin,Qintong Zhang,Guochen Yan,Runhao Zhao,Zhengpin Li,Xiaohan Yu,Lang Mei,Chong Chen,Wentao Zhang,Bin Cui*

Main category: cs.AI

TL;DR: 본 연구에서는 복합 모드 웹 검색을 위한 새로운 벤치마크인 BrowseComp-$V^3$를 소개하며, 이를 통해 MLLM의 심층 검색 역량을 평가하고 개선할 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 모드 브라우징 벤치마크는 작업의 복잡성, 증거 접근성 및 평가 세분성이 제한되어 있어 심층 검색 역량의 포괄적이고 재현 가능한 평가를 방해한다.

Method: BrowseComp-$V^3$라는 새로운 벤치마크를 도입하고, 300개의 다양하고 도전적인 질문을 통해 심층 다단계 및 교차 모드 멀티 홉 추론을 강조하며, 모든 증거는 공개적으로 검색 가능한 것으로 요구된다. 또한 평가 프로세스를 위해 서브 목표 기반의 메커니즘을 통합하였다.

Result: 최신 모델조차도 본 벤치마크에서 36% 정확도에 불과하였으며, 이는 다중 모드 정보 통합 및 세밀한 인식에서의 중요한 병목 현상을 보여준다.

Conclusion: 현재 모델의 능력과 실제 환경에서의 견고한 다중 모드 심층 검색 사이에는 근본적인 격차가 존재한다.

Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.

</details>


### [30] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 본 논문은 구조적 주장에서의 제한을 해제하고, 제한된 변수를 포함할 수 있는 새로운 개념인 CABA를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 ABA 프레임워크가 제안하는 주장이 변수 없이 구성된 근본적인 제약으로 인해 응용에 제한을 받고 있습니다.

Method: 제한된 변수를 포함한 CABA 개념을 제안하고, 비근본 의미론을 정의하여 다양한 비근본 공격 개념과 연결합니다.

Result: 새로운 의미론이 표준 ABA 의미론을 보수적으로 일반화함을 보여줍니다.

Conclusion: 제안된 CABA는 기존 ABA의 한계를 넘어 새로운 적용 가능성을 열어줍니다.

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [31] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 본 논문은 무인 항공기를 위한 적응형 제약 처리 기능을 지원하는 최적 제어와 퍼지 규칙 기반 시스템(FRBS)을 통합한 하이브리드 장애물 회피 아키텍처를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 고전적 최적 제어의 불확실성 하에서의 한계와 안전-critical 항공 시스템에서 해석 가능한 의사결정의 필요성으로부터 동기부여되었다.

Method: FAA 및 EASA의 규제 분리 최소값과 비행 안전 지침에 따라 제약 반경, 긴급 수준 및 활성화 결정을 조절하는 3단계 다카기 수겐 강(Fuzzy Layer)을 설계하였다. 이 퍼지 유도 명확성은 최적 제어 문제에 부드러운 제약으로 통합되며, FALCON 툴박스와 IPOPT를 사용하여 해결된다.

Result: 간소화된 항공기 모델을 사용한 개념 증명 구현을 통해 이 접근 방법이 단일 스레드 MATLAB 환경에서 반복당 2.3초의 계산 시간으로 최적 궤적을 생성할 수 있음을 보여주었다.

Conclusion: 하지만 실험에서 FALCON과 IPOPT의 최신 버전 간의 중대한 소프트웨어 호환성 문제를 발견하여, 라그랑지안 페널티 항이 변함없이 0으로 유지되어 적절한 제약 이행을 방해하는 것으로 나타났다. 이는 시나리오 전반에 걸쳐 일관되며 모델링 결함이 아닌 솔버 툴박스 회귀를 나타낸다. 향후 연구에서는 이전 소프트웨어 버전으로 되돌려 이 효과를 검증하고, 진화적 방법을 사용하여 퍼지 멤버십 함수를 최적화하며, 시스템을 고충실도 비행기 모델 및 확률적 장애물 환경으로 확장할 예정이다.

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [Provably Convergent Actor-Critic in Risk-averse MARL](https://arxiv.org/abs/2602.12386)
*Yizhou Zhang,Eric Mazumdar*

Main category: cs.MA

TL;DR: 이 논문은 무한 지평선 일반합 마르코프 게임에서 정적 정책을 학습하는 문제를 다루며, 위험 회피 쿼탈 반응 균형(RQE)을 통해 이러한 학습을 가능하게 하는 새로운 두 타임스케일 액터-비평가 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 무한 지평선 일반합 마르코프 게임에서 정적 정책을 학습하는 것은 다중 에이전트 강화 학습의 기본적인 문제입니다.

Method: 위험 회피 쿼탈 반응 균형(RQE)을 기반으로 한 새로운 두 타임스케일 액터-비평가 알고리즘을 제안합니다.

Result: 이 접근방식이 전역 수렴을 달성하며 유한 샘플 보장을 제공한다는 것을 증명하였습니다.

Conclusion: 여러 환경에서 실행한 실험을 통해 위험 중립 기준선에 비해 우수한 수렴 특성을 입증하였습니다.

Abstract: Learning stationary policies in infinite-horizon general-sum Markov games (MGs) remains a fundamental open problem in Multi-Agent Reinforcement Learning (MARL). While stationary strategies are preferred for their practicality, computing stationary forms of classic game-theoretic equilibria is computationally intractable -- a stark contrast to the comparative ease of solving single-agent RL or zero-sum games. To bridge this gap, we study Risk-averse Quantal response Equilibria (RQE), a solution concept rooted in behavioral game theory that incorporates risk aversion and bounded rationality. We demonstrate that RQE possesses strong regularity conditions that make it uniquely amenable to learning in MGs. We propose a novel two-timescale Actor-Critic algorithm characterized by a fast-timescale actor and a slow-timescale critic. Leveraging the regularity of RQE, we prove that this approach achieves global convergence with finite-sample guarantees. We empirically validate our algorithm in several environments to demonstrate superior convergence properties compared to risk-neutral baselines.

</details>


### [33] [Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward](https://arxiv.org/abs/2602.12430)
*Renjun Xu,Yang Yan*

Main category: cs.MA

TL;DR: 모놀리식 언어 모델에서 모듈형 에이전트로의 전환은 대형 언어 모델의 실제 배포 방식에 중대한 변화를 가져온다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 기술의 발전과 함께 신뢰할 수 있는, 자가 개선 가능한 기술 생태계 구현이 필요하다.

Method: 에이전트 기술의 분야를 네 가지 축을 따라 조직하며, 정량적 분석 및 프레임워크를 제안한다.

Result: 26.1%의 커뮤니티 기여 기술에서 취약점이 드러났으며, 이를 바탕으로 기술 신뢰 및 생애주기 관리 프레임워크를 제안한다.

Conclusion: 에이전트 시스템의 다음 세대를 위한 skill 추상화 계층에 집중하여 신뢰할 수 있는 기술 생태계를 위한 연구 의제를 제안한다.

Abstract: The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the SKILL.md specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries (SAGE), autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1\% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills.

</details>


### [34] [Theory of Mind Guided Strategy Adaptation for Zero-Shot Coordination](https://arxiv.org/abs/2602.12458)
*Andrew Ni,Simon Stepputtis,Stefanos Nikolaidis,Michael Lewis,Katia P. Sycara,Woojun Kim*

Main category: cs.MA

TL;DR: 이 논문은 다중 에이전트 강화 학습에서 에이전트가 미리 보지 못한 팀원에게 적응하는 방법을 다룬다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습의 주요 과제는 에이전트가 제로샷 방식으로 이전에 보지 못한 팀원에게 적응할 수 있도록 하는 것이다.

Method: 팀원의 의도를 추론하고 정책 앙상블에서 가장 적합한 정책을 선택하기 위해 Theory-of-Mind 기반의 최적 반응 선택을 사용하는 적응형 앙상블 에이전트를 제안한다.

Result: Overcooked 환경에서 완전 및 부분 관찰 설정 하에 제로샷 협조 성능을 실험하여 최적 반응 기준보다 우수한 성능을 입증하였다.

Conclusion: 본 연구는 제로샷 협조 성능에서 단일 최적 반응 기준에 비해 우리의 방법의 우수성을 입증했다.

Abstract: A central challenge in multi-agent reinforcement learning is enabling agents to adapt to previously unseen teammates in a zero-shot fashion. Prior work in zero-shot coordination often follows a two-stage process, first generating a diverse training pool of partner agents, and then training a best-response agent to collaborate effectively with the entire training pool. While many previous works have achieved strong performance by devising better ways to diversify the partner agent pool, there has been less emphasis on how to leverage this pool to build an adaptive agent. One limitation is that the best-response agent may converge to a static, generalist policy that performs reasonably well across diverse teammates, rather than learning a more adaptive, specialist policy that can better adapt to teammates and achieve higher synergy. To address this, we propose an adaptive ensemble agent that uses Theory-of-Mind-based best-response selection to first infer its teammate's intentions and then select the most suitable policy from a policy ensemble. We conduct experiments in the Overcooked environment to evaluate zero-shot coordination performance under both fully and partially observable settings. The empirical results demonstrate the superiority of our method over a single best-response baseline.

</details>


### [35] [Building Large-Scale Drone Defenses from Small-Team Strategies](https://arxiv.org/abs/2602.12502)
*Grant Douglas,Stephen Franklin,Claudia Szabo,Mingyu Guo*

Main category: cs.MA

TL;DR: 본 논문은 대규모 드론 무리를 방어하기 위한 효과적인 협업 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 적대적 드론 무리를 방어하기 위해서는 기존의 다중 에이전트 최적화를 넘어서는 조정 방법이 필요하다.

Method: 제안된 프레임워크를 사용하여 소규모 수비 팀에서 효과가 입증된 전략을 모듈화된 구성 요소로 통합하는 방법을 제안한다.

Result: 실험 결과 이 분할 접근법이 훨씬 더 큰 시나리오로 확장되면서 효과성을 유지하고 최적화를 통해 찾을 수 없는 협력 행동을 드러낸다.

Conclusion: 동적 프로그래밍 분해를 통한 대형 팀의 효율적인 구축을 가능하게 하여 전략의 수렴을 허용한다.

Abstract: Defending against large adversarial drone swarms requires coordination methods that scale effectively beyond conventional multi-agent optimisation. In this paper, we propose to scale strategies proven effective in small defender teams by integrating them as modular components of larger forces using our proposed framework. A dynamic programming (DP) decomposition assembles these components into large teams in polynomial time, enabling efficient construction of scalable defenses without exhaustive evaluation. Because a unit that is strong in isolation may not remain strong when combined, we sample across multiple small-team candidates. Our framework iterates between evaluating large-team outcomes and refining the pool of modular components, allowing convergence on increasingly effective strategies. Experiments demonstrate that this partitioning approach scales to substantially larger scenarios while preserving effectiveness and revealing cooperative behaviours that direct optimisation cannot reliably discover.

</details>
