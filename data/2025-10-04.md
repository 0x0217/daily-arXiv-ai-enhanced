<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: LLM의 자율적인 평가를 위해 다중 에이전트 토론을 활용한 새로운 평가 프레임워크를 소개하며, 에이전트들이 동의 성향을 보이고 효과적으로 협력하는 방식을 quant화하였다.


<details>
  <summary>Details</summary>
Motivation: LLM이 정적 도구에서 자율 에이전트로 전환됨에 따라 기존 평가 기준이 불충분하다.

Method: 다중 에이전트 토론을 제어된 '사회 실험실'로 활용하여 고유한 페르소나와 인센티브로 설정된 LLM 기반 에이전트들이 다양한 주제에 대해 심의하도록 한다.

Result: 에이전트들이 높은 의미적 합의에 도달하는 경향이 있으며, 할당된 페르소나가 안정적이고 측정 가능한 심리적 프로파일을 유도함을 발견하였다.

Conclusion: 이 연구는 AI 에이전트의 사회적 행동을 이해하고 형성하는 중요한 방법론을 제공한다.

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [2] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 인공지능의 발전이 화학 공학에서의 가능성을 제시하지만, 기존 시스템은 한계가 있다. CA-ChemE 시스템을 통해 다중 에이전트 협업을 통해 자율적 연구 진화를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 화학 공학에서 인공지능의 학제간 협업과 탐색의 한계를 해결하고자 한다.

Method: 지식 기반, 지식 향상 기술, 협업 에이전트를 통합하여 다중 에이전트 협업을 통한 연구와 발견을 가능하게 한다.

Result: 지식 기반 향상 메커니즘이 7개 전문가 에이전트 전반에 걸쳐 평균 10-15% 대화 품질 점수를 향상시켰고, 협업 에이전트 도입으로 원거리-domain 전문가 쌍의 협업 효율이 8.5% 향상되었다.

Conclusion: 신중하게 설계된 다중 에이전트 아키텍처가 화학 공학에서 자율적 과학 발견을 위한 유효한 경로를 제공할 수 있음을 보여준다.

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [3] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE은 비전-언어 모델의 인식 및 추론 능력을 향상시키기 위해 상호작용적인 과정을 통해 퍼즐 해결을 모델링하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 대규모 비전-언어 모델은 멀티모달 이해와 추론에서 발전했지만, 기본적인 지각 및 추론 능력은 여전히 제한적이다.

Method: AGILE은 현재 상태에 따라 작업을 수행하기 위해 실행 가능한 코드를 생성하고, 환경은 세부적인 시각 피드백을 제공해 작업 완료를 돕는 반복적인 관찰 및 상호작용 사이클을 통해 모델을 발전시킨다.

Result: AGILE은 다양한 복잡도의 퍼즐 작업에서 성능을 크게 향상시키며, 9개의 일반 비전 작업에 걸쳐 강력한 일반화를 보여준다.

Conclusion: 이 연구는 멀티모달 모델의 추론 및 일반화를 향상시킬 수 있는 새로운 경로를 열어주며, 멀티모달 강화 학습 데이터 부족에 대한 효율적이고 확장 가능한 해결책을 제공한다.

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [4] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK 벤치마크는 동적 기업 환경에서의 장기 메모리 및 상태 추적을 평가하기 위해 설계되었으며, 다양한 플랫폼 간의 복잡한 워크플로우를 모델링하여 메모리 메커니즘의 효율성을 테스트한다.


<details>
  <summary>Details</summary>
Motivation: 기업 환경에서 메모리를 평가할 필요성이 높아지고 있다는 점을 강조한다.

Method: MEMTRACK은 Slack, Linear, Git과 같은 여러 커뮤니케이션 및 생산성 플랫폼에서 비동기 이벤트를 통합하여 현실적인 조직 워크플로를 모델링하고, 메모리 기능을 테스트하는 벤치마크 인스턴스를 제공한다.

Result: 대규모 언어 모델 및 메모리 백엔드에 대한 실험에서 긴 기간 동안 메모리를 활용하는 데 있어 다양한 도전 과제가 드러났다.

Conclusion: 이 연구는 대화형 설정을 넘어 메모리 증강 에이전트의 평가 연구를 진전시키기 위한 확장 가능한 프레임워크를 제공한다.

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [5] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 이번 연구에서는 대규모 언어 모델(LLM) 에이전트가 멀티 스텝 작업에서 발생하는 예측 가능한 실패를 해결하기 위한 간단한 파이프라인을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 종종 미충족 전제 조건으로 작업을 시도하거나 중복 명령을 발행하며 환경 제약을 잘못 처리하는 등의 방식으로 실패합니다.

Method: 우리는 추론 시간 검색을 학습된 능력으로 변환하는 간단한 파이프라인을 제안합니다. 이 접근법은 실패로부터 재사용 가능한 힌트를 추출하고, 에피소드 시작 시 이러한 힌트를 사용하여 개선된 교사 경로를 생성하며, 힌트 문자열을 제거한 경로로 학생 모델을 훈련시킵니다.

Result: ALFWorld와 WebShop의 두 개의 대화형 벤치마크에서, 증류된 학생 모델이 기준 에이전트보다 일관되게 더 나은 성능을 보이며, ALFWorld에서 최대 91%의 성공률을 달성하고 WebShop 점수를 72로 개선하여, 검색 기반 교사에 비해 10-60% 적은 토큰을 사용합니다.

Conclusion: 이 접근법은 모델 규모(7B/14B 매개변수) 및 에이전트 아키텍처(ReAct/StateAct)를 넘어 일반화되며, 검색 이점이 영구적인 런타임 의존성 없이 목표한 세밀 조정을 통해 효과적으로 내재될 수 있음을 보여줍니다.

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [6] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 이 연구에서는 대규모 언어 모델(LLM) 에이전트를 활용하여 데이터 기반 모델링 및 분석을 자동화하는 혁신적인 파이프라인을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 공학은 실험과 시뮬레이션에서 생성된 방대한 데이터 세트에 점점 더 의존하고 있으며, 효율적이고 신뢰할 수 있으며 광범위하게 적용 가능한 모델링 전략에 대한 수요가 증가하고 있습니다.

Method: 우리는 멀티 에이전트 시스템과 Reasoning and Acting (ReAct) 패러다임에 기반한 단일 에이전트 시스템을 포함한 두 가지 LLM 에이전트 프레임워크를 평가합니다.

Result: 우리의 LLM 에이전트 개발 모델은 기존의 CHF 조회 테이블을 초과하여 예측 정확도와 불확실성 정량화에서 인재 전문가가 개발한 최첨단 베이지안 최적화 심층 신경망 모델과 동등한 성능을 제공합니다.

Conclusion: 이러한 결과는 LLM 기반 에이전트가 복잡한 공학 모델링 작업을 자동화할 수 있는 중요한 잠재력을 강조하며, 인간의 작업 부담을 크게 줄이면서 기존의 예측 성능 기준을 충족하거나 초과할 수 있음을 보여줍니다.

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [7] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX는 비정형 로그를 구조화된 지식 그래프로 변환하여 사이버 위협 정보를 추출하는 자율 AI 에이전트이다.


<details>
  <summary>Details</summary>
Motivation: 시스템 로그는 사이버 위협 정보의 중요한 출처이나, 구조 부족과 의미 불일치로 실용성이 제한된다.

Method: OntoLogX는 대형 언어 모델을 활용하여 로그를 온톨로지 기반 지식 그래프로 변환하며, 경량 로그 온톨로지와 검색 증강 생성 방식을 통합하여 생성된 그래프의 유효성을 보장한다.

Result: OntoLogX는 여러 KGs 백엔드에서 강력한 KG 생성을 보여주고, 적대적 활동을 MITRE ATT&CK 전술과 정확하게 연결한다.

Conclusion: 검색과 수정 과정이 정밀도 및 재현율에 미치는 이점을 강조하며, 구조화된 로그 분석에서 코드 중심 모델의 효과와 실용적인 CTI 추출을 위한 온톨로지 기반 표현의 가치를 보여준다.

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [8] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: CUAs는 사용자 목표를 달성하기 위해 GUI에서 행동하는 대행자이며, 본 논문은 이들이 지속적으로 목표 지향적 되기만 하고 실행 가능성, 안전성 등은 고려하지 않는 특성을 지닌다는 점을 보여준다.


<details>
  <summary>Details</summary>
Motivation: CUAs의 행동이 사용자 목표를 달성하는 데 있어 위험을 초래할 수 있음을 이해하고자 함.

Method: BLIND-ACT 라는 90개의 작업으로 구성된 벤치마크를 개발하고, 이를 통해 CUAs의 Blind Goal-Directedness (BGD)를 평가하였다.

Result: 아홉 개의 최첨단 모델을 평가한 결과 평균 BGD 비율이 80.8%로 나타났으며, 인간 주석과 93.75%의 일치를 보였다.

Conclusion: BGD는 CUAs의 안전한 배치를 위해 해결해야 할 기본적인 위험 요소로, 이를 식별하고 BLIND-ACT를 도입함으로써 향후 연구의 기초를 마련하였다.

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [9] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer는 경량 프록시 모델과 LLM의 에이전틱 추론을 결합하여 규모에 맞는 지식 마이닝을 가능하게 하는 협업 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: LLM의 배포가 비싸지고 전통적인 분류기와 추출자 파이프라인은 효율적이지만 새로운 작업에 일반화되지 못하는 문제 해결 필요성.

Method: Falconer는 LLM을 플래너와 주석 달기 모델로 활용하여 사용자 지침을 실행 가능한 파이프라인으로 분해하고 작은 프록시를 교육하기 위한 감독을 생성한다.

Result: Falconer는 최첨단 LLM과 유사한 정확도를 가지면서 추론 비용을 최대 90% 줄이고 대규모 지식 마이닝을 20배 이상 가속화한다.

Conclusion: Falconer는 Deep Research를 위한 효율적이고 확장 가능한 기초를 제공한다.

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [10] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker는 LLM을 활용하여 정보 탐색과 작업 지향 계획을 통합하여 불확실한 환경에서 최적의 결정을 내릴 수 있도록 하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 불완전한 정보와 소음이 가득한 역학 특성을 가진 실용 환경에서 문제 해결을 위한 명시적인 정보 탐색의 중요성.

Method: 정보 탐색을 위한 LLM 의사결정 프레임워크인 InfoSeeker를 도입하여 작업 지향적 계획을 정보 탐색과 통합한다.

Result: InfoSeeker는 기존 방법보다 74%의 성능 향상을 달성하며 샘플 효율성을 희생하지 않는다.

Conclusion: 부분 관찰 가능한 환경에서 강력한 행동을 위해 계획과 정보 탐색의 긴밀한 통합이 중요함을 강조한다.

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [11] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink는 대형 언어 모델에 역 사고 능력을 부여하는 간단하면서도 강력한 접근법이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 안전 정렬 방법이 안전한 응답을 위해 직접 최적화하는 것과 달리, InvThink는 모델이 잠재적 해를 열거하고, 그 결과를 분석하며, 이러한 위험을 선제적으로 피하는 안전한 출력을 생성하도록 지시한다.

Method: InvThink는 세 가지 핵심 발견을 드러낸다: (i) 안전 개선은 기존의 안전 방법에 비해 모델 크기와 더 강한 비례 관계를 나타낸다. (ii) InvThink는 안전 세금 문제를 완화하며, 실패 모드를 체계적으로 고려하도록 모델을 훈련함으로써 표준 벤치마크에서 일반적인 추론 능력을 유지한다. (iii) 일반적인 안전 작업을 넘어, InvThink는 외부 환경(의료, 금융, 법률) 및 에이전틱(협박, 살인) 위험 시나리오를 포함한 높은 위험 도메인에서 뛰어나며, SafetyPrompt와 같은 기준 방법과 비교하여 해로운 응답을 최대 15.7% 감소시킨다.

Result: 세 가지 LLM 계열을 통해 감독된 미세 조정 및 강화 학습 방식으로 InvThink를 구현한다. 이 결과는 역 추론이 더 안전하고 능력 있는 언어 모델을 향한 확장 가능하고 일반화 가능한 경로를 제공함을 시사한다.

Conclusion: InvThink는 대형 언어 모델의 안전성을 향상시키는 효과적인 방법으로, 특히 고위험 도메인에서 그 잠재력을 입증하였다.

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [12] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL은 안전성을 내재화한 진화적 다중 에이전트 강화 학습 프레임워크로, 외부 경비 없이 공격자와 방어자를 함께 최적화해 안전성과 유용성을 동시에 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 다중 에이전트 시스템의 개방성과 상호작용 복잡성이 jailbreak, prompt-injection, adversarial collaboration의 위험에 노출되고 있다는 문제점을 해결하고자 한다.

Method: AdvEvo-MARL은 공격자와 방어자를 공동으로 최적화하는 진화적 다중 에이전트 강화 학습 프레임워크로, 동일 기능 그룹 내의 에이전트들이 그룹 수준의 평균 수익 기반을 공유하여 학습 안정성과 협력을 증진시킨다.

Result: AdvEvo-MARL은 공격 시나리오에서 공격 성공률(ASR)을 20% 이하로 유지하면서 과제를 수행하는 정확도를 개선하거나 유지하는 성과를 보였다.

Conclusion: 안전성과 유용성을 추가적인 경비 에이전트나 시스템 오버헤드 없이 개선할 수 있음을 보여준다.

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [13] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec는 사용자 선호를 효과적으로 포착하는 차세대 LLM 기반 다중 에이전트 협업 추천 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 대화형 추천 시스템이 사용자 선호도 변화에 효과적으로 대응하지 못하고 있으며, 대화의 일관성과 다중 랭킹 목표 간의 균형을 유지하는 데 어려움이 있다.

Method: AgentRec는 계층적 에이전트 네트워크를 통해 적응형 지능을 활용하며, 대화 이해, 선호 모델링, 상황 인식 및 동적 랭킹을 위한 특화된 LLM 기반 에이전트를 사용한다.

Result: AgentRec는 실험 결과, 대화 성공률 2.8%, 추천 정확도(NDCG@10) 1.9%, 대화 효율성 3.2% 개선을 보였다.

Conclusion: 지능형 에이전트 조정을 통해 비교 가능한 계산 비용으로 성능을 향상하였다.

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [14] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 이 연구는 GuruAgents라는 프롬프트 유도 AI 에이전트가 전설적인 투자 고수의 전략을 체계적으로 변환할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 전설적인 투자 고수의 전략을 AI를 통해 실현하고자 하는 목적.

Method: 아이코닉 투자자를 모방하기 위해 독특한 철학을 금융 도구와 결정론적 추론 파이프라인으로 통합한 5개의 GuruAgent 개발.

Result: NASDAQ-100 구성 요소에 대한 백테스트에서 Buffett GuruAgent가 42.2% CAGR로 가장 높은 성능을 보이며 벤치마크를 크게 초과달성.

Conclusion: 프롬프트 엔지니어링을 통해 투자 고수의 질적 철학을 재현 가능한 정량적 전략으로 성공적으로 변환할 수 있음이 확인됨.

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [15] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: AGI 시스템 평가의 어려움을 논의하며, 기존의 직관 기반 평가 방식을 대체할 수 있는 새로운 평가 철학을 제안한다.


<details>
  <summary>Details</summary>
Motivation: AGI 시스템과 방법의 평가가 공학 목표의 범위 때문에 어렵다.

Method: 기존 AI 역사에서 성과가 부진했던 기존의 직관 기반의 합성 과제를 대신해, 강력한 작업 실행을 평가하는 대안적인 디자인 철학에 대해 논의한다.

Result: AGI의 테스크 실행 능력을 보여주는 새로운 평가 방법을 구체적인 데이터 과학의 관행에서 개발한다.

Conclusion: AGI 평가를 위한 실용적인 예시를 제공한다.

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [16] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT는 대규모 언어 모델을 활용하여 사용자의 질문을 SPARQL 쿼리 언어로 변환하여 대사체 지식 그래프와의 상호작용을 지원하는 AI 시스템이다.


<details>
  <summary>Details</summary>
Motivation: Mass spectrometry metabolomics는 방대한 데이터 해석이 필요하며, 이를 위한 고급 방법이 요구된다.

Method: MetaboT는 대규모 언어 모델(LLMs)을 활용하여 사용자의 질문을 지식 그래프에 적합한 SPARQL 쿼리 언어로 변환하는 방식으로 작동한다.

Result: MetaboT는 MetaboT와 표준 LLM(GPT-4o) 간의 비교에서 83.67%의 정확도를 달성하여, 다중 에이전트 시스템의 필요성을 강조한다.

Conclusion: MetaboT는 대화형 질문-응답 도우미로서 메타볼로믹스 데이터를 자연어 쿼리로 검색할 수 있게 해준다.

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [17] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 이 논문은 NIST 사이버 보안 프레임워크 2.0에 맞춘 구조적 의사결정 지원 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 현대 사이버 위협에 대응하기 위해 AI 솔루션의 선택과 배치 방법을 제시하고자 한다.

Method: AI 에이전트 이론과 산업 지침을 통합하여 NIST CSF 2.0 기능을 세부 작업으로 분해하여 접근한다.

Result: AI 에이전트의 자율성, 적응 학습 및 실시간 반응성을 각 하위 카테고리의 보안 요구 사항에 연결했다.

Conclusion: 이 연구는 이론적 AI 구성과 운영 사이버 보안 요구 간의 간극을 메우고, 산업 표준에 부합하는 다중 에이전트 시스템의 기초를 확립한다.

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [18] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: AI를 군사 작전에 통합함으로써 발생하는 이점과 함께, 인간-AI 팀 시스템 구축 및 배치에 관한 다양한 도전 과제가 제기됩니다. 본 연구는 군사 작전에서 인간과 AI의 공동 학습 모델을 설계하여, 지속적이고 양방향적인 통찰력 교환을 통해 이를 해결하고자 합니다.


<details>
  <summary>Details</summary>
Motivation: 빠르게 진화하는 군사적 위협과 복잡한 작전 환경 속에서 AI의 군사 작전 통합은 중요한 장점을 보여줍니다.

Method: 본 연구는 지속적이고 양방향적인 인간-AI 팀의 통찰력 교환을 포괄하는 신뢰할 수 있는 공동 학습 모델을 설계합니다.

Result: 제안된 모델은 다양한 차원(조정 가능한 자율성, 다층적 통제, 양방향 피드백, 협력적 의사결정)을 통합하여, 진화하는 전투 환경에 적응하는 방식을 보여줍니다.

Conclusion: 이 모델은 군사 작전에서 책임감 있고 신뢰할 수 있는 인간-AI 팀 시스템의 발전에 기여할 수 있는 구체적인 예시와 권장 사항을 동반합니다.

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [19] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 전통적인 동료 검토 시스템의 한계를 극복하기 위해 AI 기반 리뷰를 평가하는 새로운 결정론적 시뮬레이션 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 학술 출판 생태계는 제출량과 규제되지 않은 AI로 인한 위기에 직면해 있으며, 과학적 무결성을 보호하기 위한 새로운 거버넌스 모델이 필요하다.

Method: 결정론적 시뮬레이션 프레임워크를 통해 AI가 생성한 동료 검토 보고서를 평가하기 위한 안정적이고 증거 기반의 표준을 제공한다.

Result: 352개의 동료 검토 시뮬레이션 보고서를 분석하여 시스템의 신뢰성을 입증할 수 있는 일관된 시스템 상태 지표를 찾았다.

Conclusion: 이 프레임워크는 AI를 기관의 책임성의 중요한 구성 요소로 재편성하여 학술 커뮤니케이션에서 신뢰를 유지하는 데 필요한 비판적 인프라를 제공한다.

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [20] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD는 텍스트 의미론을 활용하여 컨텍스트 인식 테이블 이상 탐지를 위한 연구를 지원하며, 고유한 메타데이터가 포함된 20개의 데이터셋과 제로샷 LLM 프레임워크를 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 이상 탐지 벤치마크가 도메인 관련 맥락 없이 원시 데이터 포인트만 제공하는 한계가 있기 때문입니다.

Method: 20개의 세심하게 구 curated된 테이블 데이터셋과 최신 AD 알고리즘, 제로샷 LLM 프레임워크를 제공함으로써 텍스트 의미론을 복원합니다.

Result: 실험 결과, 의미론적 맥락이 탐지 성능을 향상시키고 해석 가능성을 높임을 증명했습니다.

Conclusion: ReTabAD는 컨텍스트 인식 AD의 체계적 탐색을 위한 벤치마크로 자리매김합니다.

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [21] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 이 논문은 OpenAI의 o3-preview 모델이 ARC-AGI 벤치마크에서 인간의 정확도를 초과했지만, 최신 모델들이 과제 작성자가 의도한 추상화를 인식하고 reasoning을 하는지 조사한다.


<details>
  <summary>Details</summary>
Motivation: 모델의 추상화 능력을 평가하고, 정확도만으로는 이러한 능력을 과대평가하거나 과소평가할 수 있다는 점을 인식하기 위함이다.

Method: ConceptARC에서 다양한 입력 모달리티(텍스트 vs. 시각), 외부 파이썬 도구 사용 여부, reasoning 모델의 노력 정도를 기준으로 모델을 평가한다.

Result: 일부 텍스트 기반 모델은 인간의 정확도에 도달하지만, 그들이 생성하는 규칙은 종종 표면 수준의 '지름길'에 기반하며 의도된 추상화를 잘 포착하지 못한다.

Conclusion: 모델들은 여전히 추상 reasoning에서 인간보다 뒤처져 있으며, 정확도만으로 평가할 경우 텍스트 모달리티에서는 과대평가되고 시각 모달리티에서는 과소평가될 수 있다.

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [22] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 인공지능이 폐쇄된 언어 모델에서 외부 인식과 정보 통합이 가능한 상호 연결된 에이전트 시스템으로의 패러다임 전환을 겪고 있다. 이 논문은 DRAs에 맞춘 엄격한 벤치마크와 다차원 평가 프레임워크를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능 분야에서 DRAs의 성능을 향상시키기 위해 적절한 평가 도구의 필요성을 느꼈다.

Method: 214개의 전문가가 선정한 도전 과제가 10개의 폭넓은 주제 영역으로 배포된 평가 벤치마크와 DRAs가 생성한 장문 보고서를 종합적으로 평가할 수 있는 다차원 평가 프레임워크를 소개한다.

Result: 주류 DRAs가 웹 검색 도구를 보강한 추론 모델보다 우수한 성능을 보였지만, 여전히 개선할 수 있는 여지가 있음을 확인했다.

Conclusion: 본 연구는 DRAs 시스템의 성능 평가와 구조적 개선, 패러다임 발전을 위한 견고한 기초를 제공한다.

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [23] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 본 연구는 CUAs의 스케일링을 위한 bBoN 방법을 소개하며, 이를 통해 복잡한 작업에서의 성공률을 크게 향상시킬 수 있음을 보인다.


<details>
  <summary>Details</summary>
Motivation: CUAs는 일상적인 디지털 작업을 자동화할 수 있는 잠재력을 가지고 있지만, 불안정성과 높은 변동성이 긴 수의 복잡한 작업에 적용하는 것을 방해한다.

Method: bBoN은 여러 롤아웃을 생성하고, 에이전트 롤아웃을 설명하는 행동 서사를 사용해 이들 중에서 선택함으로써 에이전트의 스케일링을 가능하게 한다.

Result: OSWorld에서, bBoN 스케일링 방법은 69.9%의 새로운 최첨단(State of the Art, SoTA) 성능을 기록하여 이전 방법들을 크게 초월하고, 72%의 인간 수준 성능에 접근했다.

Conclusion: 효과적인 스케일링은 구조화된 경로 이해와 선택을 요구하며, bBoN은 이를 달성하기 위한 실용적인 프레임워크를 제공한다.

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [24] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: Biosignal modalitiy의 상호 연관성을 활용하여 새로운 모드에서 모델 훈련을 지원하는 비지도 교차 모드 지식 전달 프레임워크 제안.


<details>
  <summary>Details</summary>
Motivation: 인간 생리학의 전체적이고 상호 연결된 특성을 반영하여 건강 모니터링 시스템의 접근성, 사용성 및 적응성을 향상시키기 위한 가능성 탐색.

Method: 경량 브릿지 네트워크를 훈련하여 중간 표현을 정렬하고 기초 모델과 모드 간 정보 흐름을 가능하게 하는 새로운 비지도 교차 모드 지식 전달 프레임워크를 탐색.

Result: BioX-Bridge는 최신 방법에 비해 88-99%의 훈련 가능 매개변수를 줄이면서도 전송 성능을 유지하거나 개선함을 보여줌.

Conclusion: 이 프레임워크는 생리 신호의 비지도 교차 모드 지식 전달을 위한 효율적인 솔루션을 제공한다.

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [25] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 웹 에이전트를 겨냥한 프롬프트 주입 공격 탐지에 대한 포괄적인 벤치마크 연구를 발표하며, 여러 공격 유형을 분류하고, 악성 및 무해 샘플로 구성된 데이터셋을 구축하여 탐지 방법을 체계화하고 성능을 평가하였다.


<details>
  <summary>Details</summary>
Motivation: 웹 에이전트를 겨냥한 프롬프트 주입 공격을 탐지하기 위한 기존 방법들이 체계적으로 평가되지 않았기 때문에, 이를 해결하기 위한 연구가 필요하다.

Method: 위협 모델에 기반하여 공격을 세분화하고, 악성 및 무해 샘플로 구성된 데이터셋을 구축한 후, 텍스트 및 이미지 기반 탐지 방법을 체계화하여 성능을 평가하였다.

Result: 일부 탐지기는 명시적 텍스트 지침이나 가시적 이미지 변형에 의존하는 공격을 중간에서 높은 정확도로 탐지할 수 있지만, 명시적 지침을 생략하거나 인지할 수 없는 변형을 사용하는 공격에는 거의 실패하였다.

Conclusion: 프롬프트 주입 공격 탐지를 위해 시스템화된 방법이 필요하며, 데이터셋과 코드가 공개되어 있어 연구자들이 활용할 수 있다.

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [26] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: 코드 실행이 가능한 대형 언어 모델(Large Language Model, LLM) 에이전트들이 소프트웨어 엔지니어링 워크플로우에 통합됨에 따라, 안전 우회(“탈주”) 공격의 위험이 텍스트 전용 설정을 넘어 증가하고 있다. 본 연구에서는 공격자의 능력을 반영하는 세 가지 단계의 작업 공간 제도를 포함한 JAWS-BENCH(Jailbreaks Across WorkSpaces) 벤치마크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 코드 실행이 가능한 LLM 에이전트의 사용 증가 및 안전 우회 공격의 위험 증가.

Method: JAWS-BENCH라는 새로운 벤치마크와 실행 가능성을 고려한 Judge Framework를 통해 에이전트의 공격 성공 및 코드 실행을 평가한다.

Result: JAWS-0에서는 에이전트가 평균 61%의 공격을 수용하며, JAWS-1에서는 컴플라이언스가 100%에 가까워지고, 평균 공격 성공률이 71%로 증가한다. JAWS-M에서는 평균 공격 성공률이 75%로 증가하며, 32%의 공격 코드는 즉시 배포 가능하다.

Conclusion: 모델들 사이에서 LLM을 에이전트로 감싸는 것이 취약성을 크게 증가시키며, 이는 초기 거부 결정이 이후 단계에서 자주 뒤집히기 때문임을 보여준다.

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [27] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: 본 포지션 논문은 대형 언어 모델(LLM)의 개인 정보 위험이 교육 데이터 추출을 넘어 다양한 위협을 포함한다는 점을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 개인 정보 리스크에 대한 논의가 교육 데이터의 단순 암기에 과도하게 집중되고 있는 반면, 더 즉각적이고 확장 가능한 위험은 충분히 탐구되지 않고 있다는 문제 제기.

Method: LLM 생애 주기 전반에 걸친 개인 정보 위험의 포괄적 분류 체계와 사례 연구를 통해 현재 개인 정보 프레임워크의 한계 분석.

Result: 지난 10년간(2016-2025) 주요 학회에서 발표된 1,322개의 AI/ML 개인 정보 관련 논문 분석을 통해 단순 암기가 주목받는 반면, 더 시급한 개인 정보 위험이 다른 곳에 존재한다는 사실을 밝혀냄.

Conclusion: LLM 개인 정보 접근 방식의 근본적 전환이 필요하며, 현재의 기술적 해결책에 국한되지 않고 사회기술적 성격을 반영한 학제적 접근이 필요하다는 제안.

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [28] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: 본 논문은 ML 구성 요소를 겨냥한 적대적 공격이 전체 생산급 맬웨어 탐지 시스템에 미치는 영향을 연구하고, Gmail의 사례를 통해 해당 시스템을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 딥 러닝 모델이 생산 시스템에서 널리 배포됨에 따라, 개별 모델의 단점이 실제 시스템 수준의 취약성을 초래할 수 있음.

Method: ML 모델인 Magika를 이용하여 적대적 예제를 설계하고, 이를 통해 맬웨어가 잘못된 탐지기로 라우팅되도록 함.

Result: 맬웨어 샘플의 단 13바이트를 변경하여 Magika를 90%의 경우에 회피할 수 있으며, 이를 통해 Gmail을 통해 맬웨어 파일 전송 가능.

Conclusion: 전반적으로 방어 접근 방식을 개발하면서, 방어된 생산 모델은 50바이트로 20%의 공격 성공률을 달성할 수 있도록 함.

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [29] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: 이 연구는 다중 모드 연합 의료 시스템에서 안전하고 상호 운용 가능한 데이터 통합을 위한 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 이 연구의 목적은 다양한 의료 데이터의 안전하고 상호 운용 가능한 통합을 통해 디지털 건강 분야의 도전을 해결하는 것이다.

Method: Model Context Protocol (MCP)을 상호 운용성 계층으로 사용하여 다중 모드 연합 의료 시스템에서 안전한 통신을 가능하게 하였다.

Result: 기존의 연합 학습 기준에 비해 진단 정확도가 9.8% 향상되고, 클라이언트 이탈률이 54% 감소하며, 개인정보를 보호하면서도 유용성을 유지하는 결과를 보였다.

Conclusion: MCP를 기반으로 한 다중 모드 융합은 차세대 연합 건강 인프라를 향한 신뢰할 수 있는 경로임을 강조한다.

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: 이 연구는 10,000개의 과학 논문에서 열전도 및 구조적 특성을 자동으로 추출하는 대형 언어 모델(LLM) 기반의 워크플로우를 제시하고 있습니다. 이 워크플로우는 높은 정확도를 유지하면서도 계산 비용을 줄이는데 초점을 맞추고 있으며, 가장 큰 LLM 큐레이션 열전도 데이터셋을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 소재의 빠른 발견을 방해하는 요인은 성능 지표와 구조적 맥락을 결합한 대규모 기계 판독 가능 데이터셋의 부족입니다.

Method: 우리는 약 10,000개의 전체 텍스트 과학 기사를 기반으로 열전도 및 구조적 특성을 자율적으로 추출하는 에이전틱 LLM 기반의 워크플로우를 제시합니다.

Result: 50개의 큐레이션 논문에서의 평가 결과, GPT-4.1이 가장 높은 정확도를 달성했으며, GPT-4.1 Mini는 비용의 상당 부분을 절감하면서도 거의 동등한 성능을 제공합니다.

Conclusion: 이 연구는 가장 큰 LLM 큐레이션 열전도 데이터셋을 제공하고, 재현 가능하며 비용 프로파일이 명확한 추출 파이프라인을 수립하여 열전도체를 넘어서는 확장 가능한 데이터 기반 소재 발견의 기초를 마련합니다.

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [31] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: 이 연구는 적대적 상호작용을 고려한 다중 에이전트 강화 학습(MARL) 환경을 발전시키고 이를 통해 로봇 시스템의 협력적이고 경쟁적인 행동을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 실제 애플리케이션에서는 적대적 상호작용이 매우 중요하기 때문에, 로봇 시스템이 동적 환경에서 협력하기 위한 다중 에이전트 강화 학습에 초점을 맞춘다.

Method: IsaacLab 프레임워크를 확장하여 높은 충실도의 물리 시뮬레이션에서 적대적 정책의 확장 가능한 훈련을 지원하며, 비대칭 목표와 능력을 가진 이종 에이전트를 특징으로 하는 적대적 MARL 환경을 소개한다.

Result: 여러 벤치마크 시나리오에서 실험을 통해 프레임워크가 전반적으로 효과적인 정책 훈련 및 평가 능력을 가지며, 높은 처리량과 시뮬레이션 현실성을 유지하는 것을 보여준다.

Conclusion: 코드와 벤치마크는 GitHub에서 제공된다.

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [32] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 이 리뷰는 3D MRI 재구성의 주요 4가지 접근 방식을 분석하고, 최신 기술, 방법론적 기초, 한계 및 해부학적 구조에 대한 응용을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 3D 형태 재구성은 의료 질병 진단, 치료 계획 및 컴퓨터 모델링에서 점점 더 중요해지고 있다.

Method: 본 리뷰는 포인트 클라우드, 메쉬 기반, 형태 인식, 그리고 볼륨 모델의 4가지 주요 접근 방식을 중심으로 3D MRI 재구성 방법론을 조사한다.

Result: 각 카테고리에 대해 최신 기술과 그 기초, 한계 및 해부학적 구조에의 응용을 분석한다.

Conclusion: 이 리뷰는 연구자들에게 현재 3D 재구성 방법론에 대한 구조적 개요를 제공하여 심층 학습의 기회를 식별하는 데 도움을 주고자 한다.

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [33] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: RheOFormer는 복잡한 유체 흐름의 비선형 역학을 효율적으로 학습할 수 있는 생성적 연산자 학습 방법이다.


<details>
  <summary>Details</summary>
Motivation: 부드러운 물질의 역학을 모델링하는 능력은 목표한 특성을 가진 재료와 공정을 설계하고 엔지니어링하는 데 중요하다.

Method: 자기 주의(self-attention)를 활용하여 다양한 공간 상호작용 및 복잡한 유체 흐름의 특징을 학습할 수 있는 생성적 연산자 학습 방법인 Rheological Operator Transformer(RheOFormer)를 제안한다.

Result: RheOFormer는 다양한 점도 및 비점도의 유동을 대상으로 테스트한 결과, 복잡한 유체의 비선형 역학을 정확히 학습하고 흐름의 시공간적 진화를 예측할 수 있다.

Conclusion: RheOFormer는 강력한 일반화 능력과 계산 효율로 인해 복잡한 유체 시뮬레이션을 가속화하고 데이터 기반 실험을 발전시키며, 다양한 응용 분야에서 실시간 프로세스 최적화를 가능케 하는 강력한 신경 대리 모델로 자리매김하였다.

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [34] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: 다수의 LLM의 답변을 효과적으로 집계하는 것이 중요한 도전과제로 부각되고 있다. 본 연구에서는 두 가지 새로운 집계 알고리즘인 최적 가중치(OW)와 역 놀라운 인기(ISP)를 설계하였고, 이 방법들이 다수결 투표의 한계점을 극복함을 이론적으로 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 다수의 LLM으로부터의 답변을 효과적으로 집계하는 것이 필수적인 도전 과제로 부각되고 있다.

Method: 최적 가중치(OW)와 역 놀라운 인기(ISP)라는 두 가지 새로운 집계 알고리즘을 설계하여 1차 및 2차 정보를 활용하였다.

Result: 이 방법들이 다수결 투표의 내재적 한계를 완화하고 더 신뢰할 수 있는 집단 의사결정을 이끌어낸다는 것을 이론적으로 분석하였다. 또한, 실험적으로 여러 데이터셋에서 성능을 검증하였다.

Conclusion: 모든 경우에서 우리의 방법이 다수결 투표를 지속적으로 초 outperform 하였으며, 견고한 다중 에이전트 LLM 파이프라인 설계에 대한 실용적인 성능 향상과 개념적 통찰을 제공하였다.

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [35] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: 대형 언어 모델의 발전에 따라 AI 안전성과 정렬을 확보하는 것이 중요해졌다. 본 연구에서는 경량 메커니즘인 프롬프트 가드를 우회하는 새로운 공격을 소개한다. 이 공격은 구글 제미니, 딥씽크, 그록, 미스트랄 레 샤트와 같은 제품 모델에서 효과적이며, 미세한 지침 차이를 이용해 정당한 응답 품질을 유지하면서도 공격 가능성을 드러낸다.


<details>
  <summary>Details</summary>
Motivation: AI 안전성과 정렬을 확보하는 것이 중요하며, 이 연구는 프롬프트 가드의 한계를 강조하기 위해 새로운 공격 방법을 제안한다.

Method: 프롬프트 가드를 우회하는 공격 방법으로, 경량 가드가 해독할 수 없는 탈옥 프롬프트를 인코딩하여 주요 언어 모델이 해독할 수 있도록 설계하였다.

Result: 본 방법은 응답 품질을 유지하면서도 여러 주요 언어 모델에서 일관되게 생산 모델을 탈옥시키는 성과를 보였다.

Conclusion: 가벼운 프롬프트 가드의 본질적인 공격 표면을 드러내고, 악의적인 입력을 차단하는 대신 악의적인 출력을 방지하는 방향으로 방어 체계를 전환할 필요성을 강조한다.

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [36] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: TSci는 다양한 시계열 예측을 위한 LLM 기반의 프레임워크로, 예측 오류를 평균 10.4% 및 38.2% 줄이는데 기여합니다.


<details>
  <summary>Details</summary>
Motivation: 시계열 예측은 에너지, 금융, 기후 및 공공 건강과 같은 다양한 분야의 의사 결정에 핵심적이지만, 기존 모델이 특정 데이터 세트와 도메인에 맞춰져 있어 일반화가 어렵습니다. 인력 개입을 최소화하는 일반적인 프레임워크가 필요합니다.

Method: TSci는 네 개의 전문 에이전트로 구성되어 있습니다: Curator는 데이터 통계를 통한 목표 전처리를 선택하고, Planner는 모델 선택의 가설 공간을 좁히며, Forecaster는 모델 적합 및 검증을 수행 후 최적의 모델 구성을 선택하고, Reporter는 모든 과정을 종합하여 투명한 보고서를 작성합니다.

Result: TSci는 8개의 기존 기준에서 실험 결과를 통해 통계 및 LLM 기반 기준선을 일관되게 초월하여 예측 오류를 각각 평균 10.4% 및 38.2% 줄였습니다.

Conclusion: TSci는 예측 워크플로우를 투명하고 해석 가능하며 작업 전반에 걸쳐 확장 가능한 화이트박스 시스템으로 변화시킵니다.

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [37] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: 이 논문은 인간의 개입에서 학습하는 새로운 방법인 PPL(인간 개입으로부터의 예측 선호 학습)을 소개하며, 이를 통해 에이전트의 행동 오류를 모니터링하고 수정하는 데 도움을 줍니다.


<details>
  <summary>Details</summary>
Motivation: 인간의 개입을 통한 학습은 에이전트의 행동 오류를 모니터링하고 수정하기 위해 인간 주체를 포함하고자 합니다.

Method: PPL은 인간 개입에서 암묵적인 선호 신호를 활용하여 미래 롤아웃 예측에 정보를 제공합니다. 이 방법은 각 인간 개입을 L 미래 시간 단계로 부트스트랩하는 것을 핵심 아이디어로 합니다.

Result: 우리는 자율주행 및 로봇 조작 벤치마크에서 실험을 통해 우리의 접근 방식을 평가하고 그 효율성과 일반성을 입증했습니다.

Conclusion: 이론적 분석을 통해 적절한 선호 수평 L을 선택하는 것이 위험한 상태의 범위와 레이블 정확성을 균형 있게 유지하여 알고리즘 최적성 간극을 제한함을 보여주었습니다.

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [38] [Source-Free Cross-Domain Continual Learning](https://arxiv.org/abs/2510.01649)
*Muhammad Tanzil Furqon,Mahardhika Pratama,Igor Škrjanc,Lin Liu,Habibullah Habibullah,Kutluyil Dogancay*

Main category: cs.LG

TL;DR: 이 논문은 출처 없는 교차 영역 지속 학습의 문제를 해결하고, 레이블이 없는 출처 도메인 샘플 없이도 효과적으로 작업을 수행할 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 교차 도메인 지속 학습 접근 방식은 도메인 변화가 있는 많은 스트리밍 작업을 성공적으로 처리하지만, 완전 레이블이 있는 출처 도메인이 필요하여 개인 정보 보호 환경에서의 실행 가능성을 저해한다.

Method: 이 논문에서는 레이블이 없는 출처 도메인 샘플 부재에 대처하기 위해, 출처 사전 훈련 모델과 대규모 비전-언어 모델 간의 시너지를 바탕으로 한 리허설 없는 주파수 인식 동적 프롬프트 협업(REFEREE) 아이디어를 제안한다.

Result: REFEREE는 저주파 성분을 격려하고 고주파 성분을 억제하는 주파수 인식 프롬프트 기법을 사용하여 출처 도메인과 목표 도메인 간의 도메인 이동 문제를 처리하며, 잡음이 있는 유사 레이블에 강한 주파수 인식 증강 샘플을 생성한다.

Conclusion: 엄격한 수치 연구 결과, 우리의 접근 방식이 출처 도메인 샘플에 접근할 수 있는 기존 연구보다 상당한 차이로 우수하다는 것을 확인하였다.

Abstract: Although existing cross-domain continual learning approaches successfully
address many streaming tasks having domain shifts, they call for a fully
labeled source domain hindering their feasibility in the privacy constrained
environments. This paper goes one step ahead with the problem of source-free
cross-domain continual learning where the use of source-domain samples are
completely prohibited. We propose the idea of rehearsal-free frequency-aware
dynamic prompt collaborations (REFEREE) to cope with the absence of labeled
source-domain samples in realm of cross-domain continual learning. REFEREE is
built upon a synergy between a source-pre-trained model and a large-scale
vision-language model, thus overcoming the problem of sub-optimal
generalizations when relying only on a source pre-trained model. The domain
shift problem between the source domain and the target domain is handled by a
frequency-aware prompting technique encouraging low-frequency components while
suppressing high-frequency components. This strategy generates frequency-aware
augmented samples, robust against noisy pseudo labels. The noisy pseudo-label
problem is further addressed with the uncertainty-aware weighting strategy
where the mean and covariance matrix are weighted by prediction uncertainties,
thus mitigating the adverse effects of the noisy pseudo label. Besides, the
issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant
analysis (KLDA) where the backbone network is frozen while the classification
is performed using the linear discriminant analysis approach guided by the
random kernel method. Our rigorous numerical studies confirm the advantage of
our approach where it beats prior arts having access to source domain samples
with significant margins.

</details>


### [39] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Željko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Schönlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Shâma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: 이 논문은 이미지의 역 문제를 해결하기 위한 다양한 학습된 정규화 프레임워크를 통합하여 비교하고, 각 방법의 장단점을 강조합니다.


<details>
  <summary>Details</summary>
Motivation: 역 문제 해결을 위한 다양한 학습된 정규화 프레임워크의 필요성과 이를 비교할 수 있는 통합된 관점의 중요성을 강조합니다.

Method: 사용 가능한 코드를 수집하여 공통 프레임워크로統합합니다.

Result: 제안된 방법들을 체계적으로 비교하고 각 방법의 장점과 단점을 강조합니다.

Conclusion: 이 통합된 관점은 향후 연구의 잠재력을 조명하는 데 유용한 통찰력을 제공합니다.

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [40] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 이 논문은 자율 동적 특징 선택(DFS)을 사용하여 잠재 표현을 개선하는 새로운 접근 방식을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 잠재 표현은 머신 러닝 모델의 성능과 강건성에 중요하지만, 비전 작업에서 종종 노이즈나 관련 없는 특징의 영향을 받습니다.

Method: 제안된 방법은 각 인스턴스에 대해 이미지에서 오도되거나 중복된 정보를 식별하고 제거하며, 가장 관련 있는 특징만이 잠재 공간에 기여하도록 합니다.

Result: 비지도 DFS를 활용한 모델은 클러스터링 및 이미지 생성을 포함한 다양한 작업에서 일반화 성능의 유의미한 향상을 달성했습니다.

Conclusion: 제안된 방법은 라벨이 있는 데이터에 의존하지 않으므로 다양한 도메인과 데이터셋에 광범위하게 적용 가능합니다.

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [41] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: Octax는 JAX로 구현된 고성능 아케이드 게임 환경 모음으로, RL 연구를 위한 GPU 기반 대안 제공


<details>
  <summary>Details</summary>
Motivation: 다양하고 도전적인 환경이 필요하지만 기존 비디오 게임은 계산 비용이 높아 대규모 실험에 적합하지 않음.

Method: CHIP-8 에뮬레이션 기반으로 JAX로 구현된 Octax 환경 모음을 제공.

Result: 기존 솔루션에 비해 훈련 속도와 확장성이 현저히 개선됨을 보여줌.

Conclusion: Octax는 연구자들이 새로운 게임을 쉽게 추가하거나 대형 언어 모델을 사용하여 새로운 환경을 생성할 수 있는 모듈형 디자인을 제공함.

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [42] [Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets](https://arxiv.org/abs/2510.01842)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Sachin Sharma,John D. Kelleher*

Main category: cs.LG

TL;DR: 이 논문은 AutoML과 pre-hoc 모델 선택의 교차점을 탐구하며, 전통적인 모델과 대형 언어 모델 에이전트를 활용하여 AutoML 라이브러리의 검색 공간을 줄이는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: AutoML의 발전에도 불구하고, 기존의 많은 방법들은 하이퍼파라미터의 포괄적인 검색에 의존하고 있습니다. 이에 대한 대안으로 pre-hoc 예측이 주목받고 있습니다.

Method: 전통적인 모델과 대형 언어 모델 에이전트를 활용하여 데이터셋 설명과 통계 정보를 바탕으로 AutoML의 검색 공간을 감소시킵니다.

Result: 제안된 방법론은 AWS AutoGluon 포트폴리오 데이터셋에 적용되었으며, 기존의 AutoML 벤치마크에서 성능 저하 없이 컴퓨팅 오버헤드를 상당히 줄입니다.

Conclusion: 이 접근법은 AutoML 워크플로우의 전환을 제공하며, 주어진 데이터셋에 대해 최적의 모델을 선택하면서 계산 비용을 크게 줄입니다.

Abstract: The field of AutoML has made remarkable progress in post-hoc model selection,
with libraries capable of automatically identifying the most performing models
for a given dataset. Nevertheless, these methods often rely on exhaustive
hyperparameter searches, where methods automatically train and test different
types of models on the target dataset. Contrastingly, pre-hoc prediction
emerges as a promising alternative, capable of bypassing exhaustive search
through intelligent pre-selection of models. Despite its potential, pre-hoc
prediction remains under-explored in the literature. This paper explores the
intersection of AutoML and pre-hoc model selection by leveraging traditional
models and Large Language Model (LLM) agents to reduce the search space of
AutoML libraries. By relying on dataset descriptions and statistical
information, we reduce the AutoML search space. Our methodology is applied to
the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark
containing 175 tabular classification datasets available on OpenML. The
proposed approach offers a shift in AutoML workflows, significantly reducing
computational overhead, while still selecting the best model for the given
dataset.

</details>


### [43] [Multimodal Foundation Models for Early Disease Detection](https://arxiv.org/abs/2510.01899)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: cs.LG

TL;DR: 이 연구는 다양한 환자 데이터를 통합하는 다중 모달 기초 모델을 제안하여 조기 질병 진단을 위한 교차 모달 상관관계를 식별할 수 있는 능력을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 조기 질병 진단을 위해 다양한 데이터 소스를 통합하는 필요성을 강조합니다.

Method: 어텐션 기반의 트랜스포머 프레임워크를 통해 각 모달리티를 공유 잠재 공간에 위치시키고, 이후 다중 헤드 어텐션과 잔차 정규화를 통해 결합하는 방법을 사용했습니다.

Result: 온콜로지, 심장병학, 신경학 분야의 벤치마크 데이터셋을 활용하여 초기 발견 과제를 테스트하는 실험 전략을 제시했습니다.

Conclusion: 정밀 진단을 위한 단일 기초 모델을 향해 나아가는 방법이 제안되었으며, 이는 예측의 정확성을 개선하고 의사 결정에 도움을 줄 수 있습니다.

Abstract: Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.

</details>


### [44] [FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data](https://arxiv.org/abs/2510.02017)
*Aida Tayebi,Ali Khodabandeh Yalabadi,Mehdi Yazdani-Jahromi,Ozlem Ozmen Garibay*

Main category: cs.LG

TL;DR: AI 시스템의 공정함과 편향 제거를 위한 새로운 대조 학습 프레임워크를 제안하고, 탭 형 데이터에서의 편향을 효과적으로 줄임.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 사회적 영향을 고려하는 것은 기술적 도전 이상으로 도덕적 의무이다.

Method: 대조 학습 프레임워크를 사용하여 긍정적인 쌍 샘플을 전략적으로 선택하고 지도 학습 및 자가 감독 대조 학습을 활용한다.

Result: 기존 기술보다 편향을 유의미하게 줄인 결과를 도출하였다.

Conclusion: 정확도 손실 최소화와 다양한 하위 작업에서 학습된 공정한 표현의 활용을 통해 편향을 경감시키는 접근법의 효과를 입증하였다.

Abstract: As AI systems become more embedded in everyday life, the development of fair
and unbiased models becomes more critical. Considering the social impact of AI
systems is not merely a technical challenge but a moral imperative. As
evidenced in numerous research studies, learning fair and robust
representations has proven to be a powerful approach to effectively debiasing
algorithms and improving fairness while maintaining essential information for
prediction tasks. Representation learning frameworks, particularly those that
utilize self-supervised and contrastive learning, have demonstrated superior
robustness and generalizability across various domains. Despite the growing
interest in applying these approaches to tabular data, the issue of fairness in
these learned representations remains underexplored. In this study, we
introduce a contrastive learning framework specifically designed to address
bias and learn fair representations in tabular datasets. By strategically
selecting positive pair samples and employing supervised and self-supervised
contrastive learning, we significantly reduce bias compared to existing
state-of-the-art contrastive learning models for tabular data. Our results
demonstrate the efficacy of our approach in mitigating bias with minimum
trade-off in accuracy and leveraging the learned fair representations in
various downstream tasks.

</details>


### [45] [Reinforcement Learning with Action-Triggered Observations](https://arxiv.org/abs/2510.02149)
*Alexander Ryabchenko,Wenlong Mou*

Main category: cs.LG

TL;DR: 이 논문은 행동에 의해 스스로 발생하는 상태 관찰이 있는 강화 학습 문제를 연구하며, 특정 확률로 상태 관찰이 발생하는 Action-Triggered Sporadically Traceable Markov Decision Processes (ATST-MDPs) 프레임워크를 제안한다. 이 프레임워크에 맞춘 Bellman 최적성 방정식을 유도하고, 다음 상태 관찰이 올 때까지 일련의 행동을 수행하는 에이전트를 위한 행동 시퀀스 학습 패러다임을 도입한다. 이 구조를 활용하여 우리는 이러한 특성 맵에 대한 통계적 오류 보장을 갖춘 오프 정책 추정기를 제안하고, 행동 유발 환경에 적합한 LSVI-UCB의 변형인 ST-LSVI-UCB를 소개하여 효율적인 학습이 이론적으로 가능함을 보인다.


<details>
  <summary>Details</summary>
Motivation: 실제 응용에서 일반적으로 발생하는 행동이 주-trigger된 상태 관찰 문제를 해결하기 위해.

Method: ATST-MDP로 알려진 프레임워크를 사용하여 Bellman 최적성 방정식을 유도하고, 행동 시퀀스 학습 패러다임을 도입하며, 통계적 오류 보장을 갖춘 오프 정책 추정기를 제안.

Result: ST-LSVI-UCB라는 변형을 도입하여 O(
\sqrt{Kd^3(1-\gamma)^{-3}})의 후회를 달성한다.

Conclusion: 행동에 의해 발생하는 관찰 제약 하에서도 효율적 학습이 가능함을 이론적으로 확립한다.

Abstract: We study reinforcement learning problems where state observations are
stochastically triggered by actions, a constraint common in many real-world
applications. This framework is formulated as Action-Triggered Sporadically
Traceable Markov Decision Processes (ATST-MDPs), where each action has a
specified probability of triggering a state observation. We derive tailored
Bellman optimality equations for this framework and introduce the
action-sequence learning paradigm in which agents commit to executing a
sequence of actions until the next observation arrives. Under the linear MDP
assumption, value-functions are shown to admit linear representations in an
induced action-sequence feature map. Leveraging this structure, we propose
off-policy estimators with statistical error guarantees for such feature maps
and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered
settings. ST-LSVI-UCB achieves regret $\widetilde
O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the
feature dimension, and $\gamma$ the discount factor (per-step episode
non-termination probability). Crucially, this work establishes the theoretical
foundation for learning with sporadic, action-triggered observations while
demonstrating that efficient learning remains feasible under such observation
constraints.

</details>


### [46] [Flatness-Aware Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2510.02174)
*Stefano Bruno,Youngsik Hwang,Jaehyeon An,Sotirios Sabanis,Dong-Young Lim*

Main category: cs.LG

TL;DR: fSGLD는 고차원 비볼록 최적화 문제에서 평평한 최소값을 효율적으로 찾도록 설계된 새로운 기법이다.


<details>
  <summary>Details</summary>
Motivation: 딥 러닝에서 일반화 능력은 손실 경관에서 평평한 최소값을 추구하는 것과 밀접하게 관련되어 있다.

Method: fSGLD는 이소트로픽 가우시안 노이즈로 변형된 파라미터에서 평가된 확률적 경량을 사용하여 신뢰할 수 있는 확률적 회색을 최적화한다.

Result: fSGLD의 불변 측도가 전역 최소값에 집중된 정적 측도에 가깝게 유지됨을 증명하였다.

Conclusion: fSGLD는 경쟁 알고리즘에 비해 우수한 일반화 및 견고성을 달성하며 SAM의 절반 정도의 계산 비용을 유지한다.

Abstract: Generalization in deep learning is closely tied to the pursuit of flat minima
in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics
(SGLD) offers no mechanism to bias its dynamics toward such low-curvature
solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin
Dynamics (fSGLD), designed to efficiently and provably seek flat minima in
high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses
the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian
noise, commonly referred to as Random Weight Perturbation (RWP), thereby
optimizing a randomized-smoothing objective that implicitly captures curvature
information. Leveraging these properties, we prove that the invariant measure
of fSGLD stays close to a stationary measure concentrated on the global
minimizers of a loss function regularized by the Hessian trace whenever the
inverse temperature and the scale of random weight perturbation are properly
coupled. This result provides a rigorous theoretical explanation for the
benefits of random weight perturbation. In particular, we establish
non-asymptotic convergence guarantees in Wasserstein distance with the best
known rate and derive an excess-risk bound for the Hessian-trace regularized
objective. Extensive experiments on noisy-label and large-scale vision tasks,
in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD
achieves superior or comparable generalization and robustness to baseline
algorithms while maintaining the computational cost of SGD, about half that of
SAM. Hessian-spectrum analysis further confirms that fSGLD converges to
significantly flatter minima.

</details>


### [47] [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
*Yanxu Chen,Zijun Yao,Yantao Liu,Jin Ye,Jianing Yu,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 대형 언어 모델(LLMs)은 자율 에이전트로서의 유망한 능력을 보여주고 있지만, 금융 분야의 동적 거래 환경에서의 평가가 부족하다. 본 논문에서는 StockBench라는 새로운 벤치마크를 소개하며, 이는 LLM 에이전트를 실제 주식 거래 환경에서 평가하도록 설계되었다. 본 연구는 여러 모델이 높은 수익을 가져오고 위험 관리에 효과적일 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델(LLMs)이 다른 분야에서는 강력한 성능을 보여주고 있지만, 금융 분야에서의 연구는 성과가 낮다. 금융은 경제적 가치에 직접적인 관련이 있고, 고위험 결정에 영향을 미친다.

Method: StockBench라는 오염 없는 벤치마크를 도입하여 LLM 에이전트를 몇 개월간의 실제 주식 거래 환경에서 평가한다. 에이전트는 시장 신호를 기반으로 매일 의사 결정을 한다.

Result: 최신 LLM 모델들이 간단한 매수 및 보유 전략을 능가하지 못하는 경향을 보였지만, 일부 모델들은 높은 수익을 보여주고 위험 관리에 효과적이었다.

Conclusion: 이 연구는 정적 금융 지식을 기반으로 하는 작업이 실제 거래 전략으로의 성공으로 이어지지 않을 수 있음을 강조하며, StockBench를 오픈 소스 자원으로 공개하여 이 분야의 재현성과 연구 발전을 지원한다.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
as autonomous agents, showing promise in reasoning, tool use, and sequential
decision-making. While prior benchmarks have evaluated LLM agents in domains
such as software engineering and scientific discovery, the finance domain
remains underexplored, despite its direct relevance to economic value and
high-stakes decision-making. Existing financial benchmarks primarily test
static knowledge through question answering, but they fall short of capturing
the dynamic and iterative nature of trading. To address this gap, we introduce
StockBench, a contamination-free benchmark designed to evaluate LLM agents in
realistic, multi-month stock trading environments. Agents receive daily market
signals -- including prices, fundamentals, and news -- and must make sequential
buy, sell, or hold decisions. Performance is assessed using financial metrics
such as cumulative return, maximum drawdown, and the Sortino ratio. Our
evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and
open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM
agents struggle to outperform the simple buy-and-hold baseline, several models
demonstrate the potential to deliver higher returns and manage risk more
effectively. These findings highlight both the challenges and opportunities in
developing LLM-powered financial agents, showing that excelling at static
financial knowledge tasks does not necessarily translate into successful
trading strategies. We release StockBench as an open-source resource to support
reproducibility and advance future research in this domain.

</details>


### [48] [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
*Wentao Zhang,Yang Young Lu,Yuntian Deng*

Main category: cs.LG

TL;DR: Interactive Training은 신경망 훈련 중 실시간 피드백 기반 개입을 가능하게 하는 오픈소스 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 신경망 훈련은 고정된 최적화 레시피를 따르며, 불안정성이나 발생하는 훈련 문제에 동적으로 대응하는 유연성이 부족하다.

Method: Interactive Training은 제어 서버를 사용하여 사용자나 자동화된 AI 에이전트와 진행 중인 훈련 프로세스 간의 소통을 중재하고, 사용자가 최적기 하이퍼파라미터, 훈련 데이터, 모델 체크포인트를 동적으로 조정할 수 있도록 한다.

Result: 세 가지 사례 연구를 통해 Interactive Training이 훈련 안정성을 상승시키고, 초기 하이퍼파라미터에 대한 민감도를 감소시키며, 진화하는 사용자 요구에 대한 적응성을 향상시킨다는 것을 입증하였다.

Conclusion: AI 에이전트가 훈련 로그를 자율적으로 모니터링하고, 불안정성을 선제적으로 해결하며, 훈련 역학을 최적화하는 미래 훈련 패러다임으로 나아갈 수 있는 길을 열었다.

Abstract: Traditional neural network training typically follows fixed, predefined
optimization recipes, lacking the flexibility to dynamically respond to
instabilities or emerging training issues. In this paper, we introduce
Interactive Training, an open-source framework that enables real-time,
feedback-driven intervention during neural network training by human experts or
automated AI agents. At its core, Interactive Training uses a control server to
mediate communication between users or agents and the ongoing training process,
allowing users to dynamically adjust optimizer hyperparameters, training data,
and model checkpoints. Through three case studies, we demonstrate that
Interactive Training achieves superior training stability, reduced sensitivity
to initial hyperparameters, and improved adaptability to evolving user needs,
paving the way toward a future training paradigm where AI agents autonomously
monitor training logs, proactively resolve instabilities, and optimize training
dynamics.

</details>


### [49] [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](https://arxiv.org/abs/2510.02312)
*Anna Kuzina,Maciej Pioro,Paul N. Whatmough,Babak Ehteshami Bejnordi*

Main category: cs.LG

TL;DR: 이 연구에서는 압축된 KV-cache에서 직접 지식을 추출하여 잠재적 추론을 수행하는 학생 모델을 위한 첫 번째 프레임워크인 KaVa를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)은 외부 체인을 활용한 다단계 추론 문제에서 탁월하지만, 자세한 추적은 상당한 계산 비용과 메모리 오버헤드를 발생시키고 중복된 스타일의 아티팩트를 포함할 수 있습니다.

Method: KaVa는 교사로부터 압축된 KV-cache에서 직접 지식을 증류하여 잠재적 추론 학생에게 전달하는 프레임워크로, 연속 잠재 토큰의 표현 유연성을 활용합니다.

Result: 실험적으로 KaVa는 강력한 잠재 기준보다 일관성 있게 우수한 성능을 보이고, 자연어 추적과의 변환에서 상당히 작은 저하를 보이며, 효율성을 유지하면서 더 큰 백본에 확장할 수 있습니다.

Conclusion: 이 결과는 압축된 KV-cache 증류가 잠재적 추론을 위한 확장 가능한 감독 신호로 기능할 수 있음을 입증하며, CoT 훈련 교사의 정확성과 잠재적 추론의 효율성 및 배치 가능성을 결합합니다.

Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with
explicit chain-of-thought (CoT), but verbose traces incur significant
computational costs and memory overhead, and often carry redundant, stylistic
artifacts. Latent reasoning has emerged as an efficient alternative that
internalizes the thought process, but it suffers from a critical lack of
supervision, limiting its effectiveness on complex, natural-language reasoning
traces. In this work, we propose KaVa, the first framework that bridges this
gap by distilling knowledge directly from a compressed KV-cache of the teacher
into a latent-reasoning student via self-distillation, leveraging the
representational flexibility of continuous latent tokens to align stepwise KV
trajectories. We show that the abstract, unstructured knowledge within
compressed KV-cache, which lacks direct token correspondence, can serve as a
rich supervisory signal for a latent reasoning student. Empirically, the
approach consistently outperforms strong latent baselines, exhibits markedly
smaller degradation from equation-only to natural-language traces, and scales
to larger backbones while preserving efficiency. These results establish
compressed KV-cache distillation as a scalable supervision signal for latent
reasoning, combining the accuracy of CoT-trained teachers with the efficiency
and deployability of latent inference.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [50] [LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science](https://arxiv.org/abs/2510.01285)
*Alireza Salemi,Mihir Parmar,Palash Goyal,Yiwen Song,Jinsung Yoon,Hamed Zamani,Hamid Palangi,Tomas Pfister*

Main category: cs.MA

TL;DR: 본 연구는 대규모 데이터 레이크에서 유용한 데이터를 찾는 데 있어 다중 에이전트 통신 패러다임을 제안하여 데이터 과학의 실제 적용을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)의 발전에도 불구하고, 이들의 실질적인 배치는 이질적인 데이터 레이크 내에서 관련 데이터를 발견하는 데 어려움이 있습니다.

Method: 기존의 마스터-슬레이브 패러다임을 기반으로 한 다중 에이전트 시스템의 한계를 극복하기 위해, 본 연구에서는 전통적인 AI 모델의 블랙보드 아키텍처에서 영감을 받은 새로운 다중 에이전트 통신 패러다임을 제안합니다.

Result: 본 방법은 세 가지 벤치마크에서 평가되었으며, 실험 결과 블랙보드 아키텍처가 RAG 및 마스터-슬레이브 다중 에이전트 패러다임을 포함한 기준선보다 대폭 향상된 성능을 보였습니다.

Conclusion: 블랙보드 패러다임은 다중 에이전트 시스템을 위한 확장 가능하고 일반화 가능한 통신 프레임워크로 자리 잡았습니다.

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.

</details>


### [51] [SimCity: Multi-Agent Urban Development Simulation with Rich Interactions](https://arxiv.org/abs/2510.01297)
*Yeqi Feng,Yucheng Lu,Hongyu Su,Tianxing He*

Main category: cs.MA

TL;DR: SimCity는 LLM을 활용하여 해석 가능한 거시경제 시스템을 모델링하는 다중 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: LLM이 진정한 해석 가능성을 갖춘 현실적인 거시경제 시뮬레이션을 구축하는 새로운 가능성을 열어주기 때문이다.

Method: SimCity는 가정, 기업, 중앙은행, 정부의 네 가지 핵심 에이전트 유형이 존재하는 철저한 마찰 노동 시장, 이질적인 상품 시장, 금융 시장을 모델링한다.

Result: SimCity는 가격 탄력성, 앵겔의 법칙, 오쿤의 법칙, 필립스 곡선, 베버리지 곡선 등 다양한 고전적인 거시경제 요인을 자연스럽게 재현하였다.

Conclusion: SimCity는 혁신적인 방법으로 거시경제 규칙과 도시 확장 역학을 통합적으로 연구할 수 있는 플랫폼을 제공한다.

Abstract: Large Language Models (LLMs) open new possibilities for constructing
realistic and interpretable macroeconomic simulations. We present SimCity, a
multi-agent framework that leverages LLMs to model an interpretable
macroeconomic system with heterogeneous agents and rich interactions. Unlike
classical equilibrium models that limit heterogeneity for tractability, or
traditional agent-based models (ABMs) that rely on hand-crafted decision rules,
SimCity enables flexible, adaptive behavior with transparent natural-language
reasoning. Within SimCity, four core agent types (households, firms, a central
bank, and a government) deliberate and participate in a frictional labor
market, a heterogeneous goods market, and a financial market. Furthermore, a
Vision-Language Model (VLM) determines the geographic placement of new firms
and renders a mapped virtual city, allowing us to study both macroeconomic
regularities and urban expansion dynamics within a unified environment. To
evaluate the framework, we compile a checklist of canonical macroeconomic
phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the
Phillips Curve, and the Beveridge Curve, and show that SimCity naturally
reproduces these empirical patterns while remaining robust across simulation
runs.

</details>
