{"id": "2509.07053", "categories": ["cs.CR", "cs.CY", "J.4; K.4.1; K.4.3; K.5.0; K.5.2; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.07053", "abs": "https://arxiv.org/abs/2509.07053", "authors": ["Paul Benjamin Lowry", "Gregory D. Moody", "Robert Willison", "Clay Posey"], "title": "The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?", "comment": null, "summary": "The Signalgate incident of March 2025, wherein senior US national security\nofficials inadvertently disclosed sensitive military operational details via\nthe encrypted messaging platform Signal, highlights critical vulnerabilities in\norganizational security arising from human error, governance gaps, and the\nmisuse of technology. Although smaller in scale when compared to historical\nbreaches involving billions of records, Signalgate illustrates critical\nsystemic issues often overshadowed by a focus on external cyber threats.\nEmploying a case-study approach and systematic review grounded in the NIST\nCybersecurity Framework, we analyze the incident to identify patterns of\nhuman-centric vulnerabilities and governance challenges common to\norganizational security failures. Findings emphasize three critical points. (1)\nOrganizational security depends heavily on human behavior, with internal actors\noften serving as the weakest link despite advanced technical defenses; (2)\nLeadership tone strongly influences organizational security culture and\nefficacy, and (3) widespread reliance on technical solutions without sufficient\ninvestments in human and organizational factors leads to ineffective practices\nand wasted resources. From these observations, we propose actionable\nrecommendations for enhancing organizational and national security, including\nstrong leadership engagement, comprehensive adoption of zero-trust\narchitectures, clearer accountability structures, incentivized security\nbehaviors, and rigorous oversight. Particularly during periods of\norganizational transition, such as mergers or large-scale personnel changes,\nadditional measures become particularly important. Signalgate underscores the\nneed for leaders and policymakers to reorient cybersecurity strategies toward\naddressing governance, cultural, and behavioral risks."}
{"id": "2509.07131", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07131", "abs": "https://arxiv.org/abs/2509.07131", "authors": ["Nicolò Romandini", "Carlo Mazzocca", "Kai Otsuki", "Rebecca Montanari"], "title": "SoK: Security and Privacy of AI Agents for Blockchain", "comment": "This work has been accepted to the 7th International Conference on\n  Blockchain Computing and Applications (BCCA 2025)", "summary": "Blockchain and smart contracts have garnered significant interest in recent\nyears as the foundation of a decentralized, trustless digital ecosystem,\nthereby eliminating the need for traditional centralized authorities. Despite\ntheir central role in powering Web3, their complexity still presents\nsignificant barriers for non-expert users. To bridge this gap, Artificial\nIntelligence (AI)-based agents have emerged as valuable tools for interacting\nwith blockchain environments, supporting a range of tasks, from analyzing\non-chain data and optimizing transaction strategies to detecting\nvulnerabilities within smart contracts. While interest in applying AI to\nblockchain is growing, the literature still lacks a comprehensive survey that\nfocuses specifically on the intersection with AI agents. Most of the related\nwork only provides general considerations, without focusing on any specific\ndomain. This paper addresses this gap by presenting the first Systematization\nof Knowledge dedicated to AI-driven systems for blockchain, with a special\nfocus on their security and privacy dimensions, shedding light on their\napplications, limitations, and future research directions."}
{"id": "2509.07287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07287", "abs": "https://arxiv.org/abs/2509.07287", "authors": ["Yan Pang", "Wenlong Meng", "Xiaojing Liao", "Tianhao Wang"], "title": "Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm", "comment": "20 pages", "summary": "With the rapid development of large language models, the potential threat of\ntheir malicious use, particularly in generating phishing content, is becoming\nincreasingly prevalent. Leveraging the capabilities of LLMs, malicious users\ncan synthesize phishing emails that are free from spelling mistakes and other\neasily detectable features. Furthermore, such models can generate\ntopic-specific phishing messages, tailoring content to the target domain and\nincreasing the likelihood of success.\n  Detecting such content remains a significant challenge, as LLM-generated\nphishing emails often lack clear or distinguishable linguistic features. As a\nresult, most existing semantic-level detection approaches struggle to identify\nthem reliably. While certain LLM-based detection methods have shown promise,\nthey suffer from high computational costs and are constrained by the\nperformance of the underlying language model, making them impractical for\nlarge-scale deployment.\n  In this work, we aim to address this issue. We propose Paladin, which embeds\ntrigger-tag associations into vanilla LLM using various insertion strategies,\ncreating them into instrumented LLMs. When an instrumented LLM generates\ncontent related to phishing, it will automatically include detectable tags,\nenabling easier identification. Based on the design on implicit and explicit\ntriggers and tags, we consider four distinct scenarios in our work. We evaluate\nour method from three key perspectives: stealthiness, effectiveness, and\nrobustness, and compare it with existing baseline methods. Experimental results\nshow that our method outperforms the baselines, achieving over 90% detection\naccuracy across all scenarios."}
{"id": "2509.07649", "categories": ["cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.07649", "abs": "https://arxiv.org/abs/2509.07649", "authors": ["Ioannis Koufos", "Abdul Rehman Qureshi", "Adrian Asensio", "Allen Abishek", "Efstathios Zaragkas", "Ricard Vilalta", "Maria Souvalioti", "George Xilouris", "Michael-Alexandros Kourtis"], "title": "Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification", "comment": "6 pages, 5 figures, 1 table, to be published in IEEE Xplore", "summary": "Traditional risk assessments rely on manual audits and system scans, often\ncausing operational disruptions and leaving security gaps. To address these\nchallenges, this work presents Security Digital Twin-as-a-Service (SDT-aaS), a\nnovel approach that leverages Digital Twin (DT) technology for automated,\nnon-intrusive security compliance. SDT-aaS enables real-time security\nassessments by mirroring real-world assets, collecting compliance artifacts,\nand creating machine-readable evidence. The proposed work is a scalable and\ninteroperable solution that supports open standards like CycloneDX and Web of\nThings (WoT), facilitating seamless integration and efficient compliance\nmanagement. Empirical results from a moderate-scale infrastructure use case\ndemonstrate its feasibility and performance, paving the way for efficient,\non-demand cybersecurity governance with minimal operational impact."}
{"id": "2509.07234", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.07234", "abs": "https://arxiv.org/abs/2509.07234", "authors": ["Yanlin Zhou", "Manshi Limbu", "Xuesu Xiao"], "title": "Efficient Multi-Agent Coordination via Dynamic Joint-State Graph Construction", "comment": null, "summary": "Multi-agent pathfinding (MAPF) traditionally focuses on collision avoidance,\nbut many real-world applications require active coordination between agents to\nimprove team performance. This paper introduces Team Coordination on Graphs\nwith Risky Edges (TCGRE), where agents collaborate to reduce traversal costs on\nhigh-risk edges via support from teammates. We reformulate TCGRE as a 3D\nmatching problem-mapping robot pairs, support pairs, and time steps-and\nrigorously prove its NP-hardness via reduction from Minimum 3D Matching. To\naddress this complexity, (in the conference version) we proposed efficient\ndecomposition methods, reducing the problem to tractable subproblems:\nJoint-State Graph (JSG): Encodes coordination as a single-agent shortest-path\nproblem. Coordination-Exhaustive Search (CES): Optimizes support assignments\nvia exhaustive pairing. Receding-Horizon Optimistic Cooperative A* (RHOCA*):\nBalances optimality and scalability via horizon-limited planning. Further in\nthis extension, we introduce a dynamic graph construction method\n(Dynamic-HJSG), leveraging agent homogeneity to prune redundant states and\nreduce computational overhead by constructing the joint-state graph\ndynamically. Theoretical analysis shows Dynamic-HJSG preserves optimality while\nlowering complexity from exponential to polynomial in key cases. Empirical\nresults validate scalability for large teams and graphs, with HJSG\noutperforming baselines greatly in runtime in different sizes and types of\ngraphs. This work bridges combinatorial optimization and multi-agent planning,\noffering a principled framework for collaborative pathfinding with provable\nguarantees, and the key idea of the solution can be widely extended to many\nother collaborative optimization problems, such as MAPF."}
{"id": "2509.07013", "categories": ["cs.LG", "q-bio.PE", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.07013", "abs": "https://arxiv.org/abs/2509.07013", "authors": ["Sima Najafzadehkhoei", "George Vega Yon", "Bernardo Modenesi", "Derek S. Meyer"], "title": "Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs", "comment": null, "summary": "Calibrating agent-based epidemic models is computationally demanding. We\npresent a supervised machine learning calibrator that learns the inverse\nmapping from epidemic time series to SIR parameters. A three-layer\nbidirectional LSTM ingests 60-day incidence together with population size and\nrecovery rate, and outputs transmission probability, contact rate, and R0.\nTraining uses a composite loss with an epidemiology-motivated consistency\npenalty that encourages R0 \\* recovery rate to equal transmission probability\n\\* contact rate.\n  In a 1000-scenario simulation study, we compare the calibrator with\nApproximate Bayesian Computation (likelihood-free MCMC). The method achieves\nlower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs\n0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near\nnominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per\ncalibration. Although contact rate and transmission probability are partially\nnonidentifiable, the approach reproduces epidemic curves more faithfully than\nABC, enabling fast and practical calibration. We evaluate it on SIR agent based\nepidemics generated with epiworldR and provide an implementation in R."}
{"id": "2509.07017", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07017", "abs": "https://arxiv.org/abs/2509.07017", "authors": ["Andrew Kiruluta", "Priscilla Burity"], "title": "From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning", "comment": null, "summary": "We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning\nframework that embeds logical rules as spectral templates and performs\ninference directly in the graph spectral domain. By leveraging graph signal\nprocessing (GSP) and frequency-selective filters grounded in the Laplacian\neigenstructure of knowledge graphs, the architecture unifies the\ninterpretability of symbolic reasoning with the scalability and adaptability of\nspectral learning. Beyond the core formulation, we incorporate a comprehensive\nset of extensions, including dynamic graph and basis learning, rational and\ndiffusion filters for sharper spectral selectivity, mixture-of-spectral-experts\nfor modular specialization, proof-guided training with spectral curricula, and\nuncertainty quantification for calibrated confidence. Additional enhancements\nsuch as large language model coupling, co-spectral transfer alignment,\nadversarial robustness, efficient GPU kernels, generalized Laplacians, and\ncausal interventions further expand the versatility of the framework.\n  Empirical evaluation on state-of-the-art reasoning benchmarks such as\nProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior\naccuracy, faster inference, improved robustness to adversarial perturbations,\nand higher interpretability compared to leading baselines including\ntransformers, message-passing neural networks, and neuro-symbolic logic\nprogramming systems. Spectral attribution and proof-band agreement analyses\nconfirm that model decisions align closely with symbolic proof structures,\nwhile transfer experiments validate effective domain adaptation through\nco-spectral alignment. These results establish Spectral NSR as a scalable and\nprincipled foundation for the next generation of reasoning systems, offering\ntransparency, robustness, and generalization beyond conventional approaches."}
{"id": "2509.07764", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07764", "abs": "https://arxiv.org/abs/2509.07764", "authors": ["Haitao Hu", "Peng Chen", "Yanpeng Zhao", "Yuqi Chen"], "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents", "comment": null, "summary": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses."}
{"id": "2509.07571", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07571", "abs": "https://arxiv.org/abs/2509.07571", "authors": ["Xiyu Guo", "Shan Wang", "Chunfang Ji", "Xuefeng Zhao", "Wenhao Xi", "Yaoyao Liu", "Qinglan Li", "Chao Deng", "Junlan Feng"], "title": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference", "comment": null, "summary": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches."}
{"id": "2509.07019", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07019", "abs": "https://arxiv.org/abs/2509.07019", "authors": ["Xinquan Wu", "Xuefeng Yan", "Mingqiang Wei", "Donghai Guan"], "title": "An efficient deep reinforcement learning environment for flexible job-shop scheduling", "comment": null, "summary": "The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial\noptimization problem that has a wide-range of applications in the real world.\nIn order to generate fast and accurate scheduling solutions for FJSP, various\ndeep reinforcement learning (DRL) scheduling methods have been developed.\nHowever, these methods are mainly focused on the design of DRL scheduling\nAgent, overlooking the modeling of DRL environment. This paper presents a\nsimple chronological DRL environment for FJSP based on discrete event\nsimulation and an end-to-end DRL scheduling model is proposed based on the\nproximal policy optimization (PPO). Furthermore, a short novel state\nrepresentation of FJSP is proposed based on two state variables in the\nscheduling environment and a novel comprehensible reward function is designed\nbased on the scheduling area of machines. Experimental results on public\nbenchmark instances show that the performance of simple priority dispatching\nrules (PDR) is improved in our scheduling environment and our DRL scheduling\nmodel obtains competing performance compared with OR-Tools, meta-heuristic, DRL\nand PDR scheduling methods."}
{"id": "2509.07098", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07098", "abs": "https://arxiv.org/abs/2509.07098", "authors": ["Yinheng Li", "Hailey Hultquist", "Justin Wagle", "Kazuhito Koishida"], "title": "Instruction Agent: Enhancing Agent with Expert Demonstration", "comment": null, "summary": "Graphical user interface (GUI) agents have advanced rapidly but still\nstruggle with complex tasks involving novel UI elements, long-horizon actions,\nand personalized trajectories. In this work, we introduce Instruction Agent, a\nGUI agent that leverages expert demonstrations to solve such tasks, enabling\ncompletion of otherwise difficult workflows. Given a single demonstration, the\nagent extracts step-by-step instructions and executes them by strictly\nfollowing the trajectory intended by the user, which avoids making mistakes\nduring execution. The agent leverages the verifier and backtracker modules\nfurther to improve robustness. Both modules are critical to understand the\ncurrent outcome from each action and handle unexpected interruptions(such as\npop-up windows) during execution. Our experiments show that Instruction Agent\nachieves a 60% success rate on a set of tasks in OSWorld that all top-ranked\nagents failed to complete. The Instruction Agent offers a practical and\nextensible framework, bridging the gap between current GUI agents and reliable\nreal-world GUI task automation."}
{"id": "2509.07939", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07939", "abs": "https://arxiv.org/abs/2509.07939", "authors": ["Katsuaki Nakano", "Reza Feyyazi", "Shanchieh Jay Yang", "Michael Zuzak"], "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments"}
{"id": "2509.07036", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07036", "abs": "https://arxiv.org/abs/2509.07036", "authors": ["Federico Cerutti"], "title": "Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators", "comment": "Accepted at the 2nd edition of the Workshop in AI and Finance at\n  ECAI-2025", "summary": "This paper presents a methodological approach to financial time series\nanalysis by combining causal discovery and uncertainty-aware forecasting. As a\ncase study, we focus on four key U.S. macroeconomic indicators -- GDP, economic\ngrowth, inflation, and unemployment -- and we apply the LPCMCI framework with\nGaussian Process Distance Correlation (GPDC) to uncover dynamic causal\nrelationships in quarterly data from 1970 to 2021. Our results reveal a robust\nunidirectional causal link from economic growth to GDP and highlight the\nlimited connectivity of inflation, suggesting the influence of latent factors.\nUnemployment exhibits strong autoregressive dependence, motivating its use as a\ncase study for probabilistic forecasting. Leveraging the Chronos framework, a\nlarge language model trained for time series, we perform zero-shot predictions\non unemployment. This approach delivers accurate forecasts one and two quarters\nahead, without requiring task-specific training. Crucially, the model's\nuncertainty-aware predictions yield 90\\% confidence intervals, enabling\neffective anomaly detection through statistically principled deviation\nanalysis. This study demonstrates the value of combining causal structure\nlearning with probabilistic language models to inform economic policy and\nenhance forecasting robustness."}
{"id": "2509.07260", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07260", "abs": "https://arxiv.org/abs/2509.07260", "authors": ["Xin Wang", "Ting Dang", "Xinyu Zhang", "Vassilis Kostakos", "Michael J. Witbrock", "Hong Jia"], "title": "HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring", "comment": "9 pages, 6 tables, 6 figures", "summary": "Mobile and wearable healthcare monitoring play a vital role in facilitating\ntimely interventions, managing chronic health conditions, and ultimately\nimproving individuals' quality of life. Previous studies on large language\nmodels (LLMs) have highlighted their impressive generalization abilities and\neffectiveness in healthcare prediction tasks. However, most LLM-based\nhealthcare solutions are cloud-based, which raises significant privacy concerns\nand results in increased memory usage and latency. To address these challenges,\nthere is growing interest in compact models, Small Language Models (SLMs),\nwhich are lightweight and designed to run locally and efficiently on mobile and\nwearable devices. Nevertheless, how well these models perform in healthcare\nprediction remains largely unexplored. We systematically evaluated SLMs on\nhealth prediction tasks using zero-shot, few-shot, and instruction fine-tuning\napproaches, and deployed the best performing fine-tuned SLMs on mobile devices\nto evaluate their real-world efficiency and predictive performance in practical\nhealthcare scenarios. Our results show that SLMs can achieve performance\ncomparable to LLMs while offering substantial gains in efficiency and privacy.\nHowever, challenges remain, particularly in handling class imbalance and\nfew-shot scenarios. These findings highlight SLMs, though imperfect in their\ncurrent form, as a promising solution for next-generation, privacy-preserving\nhealthcare monitoring."}
{"id": "2509.07941", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07941", "abs": "https://arxiv.org/abs/2509.07941", "authors": ["Kai Ye", "Liangcai Su", "Chenxiong Qian"], "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented Code Generation", "comment": "This paper has been accepted by the ACM Conference on Computer and\n  Communications Security (CCS) 2025", "summary": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io."}
{"id": "2509.07039", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07039", "abs": "https://arxiv.org/abs/2509.07039", "authors": ["Serra Aksoy"], "title": "Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation", "comment": "28 Pages, 4 Figures", "summary": "Artificial intelligence deployment for automated photovoltaic (PV) monitoring\nfaces interpretability barriers that limit adoption in energy infrastructure\napplications. While deep learning achieves high accuracy in thermal fault\ndetection, validation that model decisions align with thermal physics\nprinciples remains lacking, creating deployment hesitancy where understanding\nmodel reasoning is critical. This study provides a systematic comparison of\nconvolutional neural networks (ResNet-18, EfficientNet-B0) and vision\ntransformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI\nsaliency analysis to assess alignment with thermal physics principles. This\nrepresents the first systematic comparison of CNNs and vision transformers for\nthermal PV fault detection with physics-validated interpretability. Evaluation\non 20,000 infrared images spanning normal operation and 11 fault categories\nshows that Swin Transformer achieves the highest performance (94% binary\naccuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis\nreveals that models learn physically meaningful features, such as localized\nhotspots for cell defects, linear thermal paths for diode failures, and thermal\nboundaries for vegetation shading, consistent with expected thermal signatures.\nHowever, performance varies significantly across fault types: electrical faults\nachieve strong detection (F1-scores >0.90) while environmental factors like\nsoiling remain challenging (F1-scores 0.20-0.33), indicating limitations\nimposed by thermal imaging resolution. The thermal physics-guided\ninterpretability approach provides methodology for validating AI\ndecision-making in energy monitoring applications, addressing deployment\nbarriers in renewable energy infrastructure."}
{"id": "2509.07367", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.07367", "abs": "https://arxiv.org/abs/2509.07367", "authors": ["Cunxi Yu", "Rongjian Liang", "Chia-Tung Ho", "Haoxing Ren"], "title": "Autonomous Code Evolution Meets NP-Completeness", "comment": "31 pages, 11 figures", "summary": "Large language models (LLMs) have recently shown strong coding abilities,\nenabling not only static code generation but also iterative code self-evolving\nthrough agentic frameworks. Recently, AlphaEvolve \\cite{novikov2025alphaevolve}\ndemonstrated that LLM-based coding agents can autonomously improve algorithms\nand surpass human experts, with scopes limited to isolated kernels spanning\nhundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the\nfirst framework to extend LLM-based code evolution to the full repository\nscale, encompassing hundreds of files and tens of thousands of lines of C/C++\ncode. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem\nand a cornerstone of both theory and applications. SATLUTION orchestrates LLM\nagents to directly evolve solver repositories under strict correctness\nguarantees and distributed runtime feedback, while simultaneously self-evolving\nits own evolution policies and rules. Starting from SAT Competition 2024\ncodebases and benchmark, SATLUTION evolved solvers that decisively outperformed\nthe human-designed winners of the SAT Competition 2025, and also surpassed both\n2024 and 2025 champions on the 2024 benchmarks."}
{"id": "2508.18933", "categories": ["cs.AI", "cs.CR", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18933", "abs": "https://arxiv.org/abs/2508.18933", "authors": ["David Egea", "Barproda Halder", "Sanghamitra Dutta"], "title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation", "comment": null, "summary": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis."}
{"id": "2509.07204", "categories": ["cs.LG", "q-bio.QM", "I.2.6; I.2.4; J.3"], "pdf": "https://arxiv.org/pdf/2509.07204", "abs": "https://arxiv.org/abs/2509.07204", "authors": ["Adrien Couetoux", "Thomas Devenyns", "Lise Diagne", "David Champagne", "Pierre-Yves Mousset", "Chris Anagnostopoulos"], "title": "Predicting effect of novel treatments using molecular pathways and real-world data", "comment": null, "summary": "In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in\ntreating a particular disease prior to clinical testing or any real-world use\nhas been challenging. In this paper, we propose a flexible and modular machine\nlearning-based approach for predicting the efficacy of an untested\npharmaceutical for treating a disease. We train a machine learning model using\nsets of pharmaceutical-pathway weight impact scores and patient data, which can\ninclude patient characteristics and observed clinical outcomes. The resulting\nmodel then analyses weighted impact scores of an untested pharmaceutical across\nhuman biological molecule-protein pathways to generate a predicted efficacy\nvalue. We demonstrate how the method works on a real-world dataset with patient\ntreatments and outcomes, with two different weight impact score algorithms We\ninclude methods for evaluating the generalisation performance on unseen\ntreatments, and to characterise conditions under which the approach can be\nexpected to be most predictive. We discuss specific ways in which our approach\ncan be iterated on, making it an initial framework to support future work on\npredicting the effect of untested drugs, leveraging RWD clinical data and drug\nembeddings."}
{"id": "2509.07642", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07642", "abs": "https://arxiv.org/abs/2509.07642", "authors": ["Sascha Kaltenpoth", "Oliver Müller"], "title": "Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment", "comment": "Presented at the 19th International Conference on\n  Wirtschaftsinformatik 2024, W\\\"urzburg, Germany\n  https://aisel.aisnet.org/wi2024/91/", "summary": "Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space."}
{"id": "2509.07325", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07325", "abs": "https://arxiv.org/abs/2509.07325", "authors": ["Alyssa Unell", "Noel C. F. Codella", "Sam Preston", "Peniel Argaw", "Wen-wai Yim", "Zelalem Gero", "Cliff Wong", "Rajesh Jena", "Eric Horvitz", "Amanda K. Hall", "Ruican Rachel Zhong", "Jiachen Li", "Shrey Jain", "Mu Wei", "Matthew Lungren", "Hoifung Poon"], "title": "CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation", "comment": null, "summary": "The National Comprehensive Cancer Network (NCCN) provides evidence-based\nguidelines for cancer treatment. Translating complex patient presentations into\nguideline-compliant treatment recommendations is time-intensive, requires\nspecialized expertise, and is prone to error. Advances in large language model\n(LLM) capabilities promise to reduce the time required to generate treatment\nrecommendations and improve accuracy. We present an LLM agent-based approach to\nautomatically generate guideline-concordant treatment trajectories for patients\nwith non-small cell lung cancer (NSCLC). Our contributions are threefold.\nFirst, we construct a novel longitudinal dataset of 121 cases of NSCLC patients\nthat includes clinical encounters, diagnostic results, and medical histories,\neach expertly annotated with the corresponding NCCN guideline trajectories by\nboard-certified oncologists. Second, we demonstrate that existing LLMs possess\ndomain-specific knowledge that enables high-quality proxy benchmark generation\nfor both model development and evaluation, achieving strong correlation\n(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.\nThird, we develop a hybrid approach combining expensive human annotations with\nmodel consistency information to create both the agent framework that predicts\nthe relevant guidelines for a patient, as well as a meta-classifier that\nverifies prediction accuracy with calibrated confidence scores for treatment\nrecommendations (AUROC=0.800), a critical capability for communicating the\naccuracy of outputs, custom-tailoring tradeoffs in performance, and supporting\nregulatory compliance. This work establishes a framework for clinically viable\nLLM-based guideline adherence systems that balance accuracy, interpretability,\nand regulatory requirements while reducing annotation costs, providing a\nscalable pathway toward automated clinical decision support."}
{"id": "2509.07820", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07820", "abs": "https://arxiv.org/abs/2509.07820", "authors": ["João Paulo Nogueira", "Wentao Sun", "Alonso Silva", "Laith Zumot"], "title": "Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach", "comment": null, "summary": "The rise of large reasoning language models (LRLMs) has unlocked new\npotential for solving complex tasks. These models operate with a thinking\nbudget, that is, a predefined number of reasoning tokens used to arrive at a\nsolution. We propose a novel approach, inspired by the generator/discriminator\nframework in generative adversarial networks, in which a critic model\nperiodically probes its own reasoning to assess whether it has reached a\nconfident conclusion. If not, reasoning continues until a target certainty\nthreshold is met. This mechanism adaptively balances efficiency and reliability\nby allowing early termination when confidence is high, while encouraging\nfurther reasoning when uncertainty persists. Through experiments on the\nAIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)\nimproves baseline accuracy while reducing token usage. Importantly, extended\nmulti-seed evaluations over 64 runs demonstrate that CGR is stable, reducing\nvariance across seeds and improving exam-like performance under penalty-based\ngrading. Additionally, our token savings analysis shows that CGR can eliminate\nmillions of tokens in aggregate, with tunable trade-offs between certainty\nthresholds and efficiency. Together, these findings highlight certainty as a\npowerful signal for reasoning sufficiency. By integrating confidence into the\nreasoning process, CGR makes large reasoning language models more adaptive,\ntrustworthy, and resource efficient, paving the way for practical deployment in\ndomains where both accuracy and computational cost matter."}
{"id": "2509.07604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07604", "abs": "https://arxiv.org/abs/2509.07604", "authors": ["Zhoujun Cheng", "Richard Fan", "Shibo Hao", "Taylor W. Killian", "Haonan Li", "Suqi Sun", "Hector Ren", "Alexander Moreno", "Daqian Zhang", "Tianjun Zhong", "Yuxin Xiong", "Yuanzhe Hu", "Yutao Xie", "Xudong Han", "Yuqi Wang", "Varad Pimpalkhute", "Yonghao Zhuang", "Aaryamonvikram Singh", "Xuezhi Liang", "Anze Xie", "Jianshu She", "Desai Fan", "Chengqian Gao", "Liqun Ma", "Mikhail Yurochkin", "John Maggs", "Xuezhe Ma", "Guowei He", "Zhiting Hu", "Zhengzhong Liu", "Eric P. Xing"], "title": "K2-Think: A Parameter-Efficient Reasoning System", "comment": "To access the K2-Think reasoning system, please visit\n  https://k2think.ai", "summary": "K2-Think is a reasoning system that achieves state-of-the-art performance\nwith a 32B parameter model, matching or surpassing much larger models like\nGPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system\nshows that smaller models can compete at the highest levels by combining\nadvanced post-training and test-time computation techniques. The approach is\nbased on six key technical pillars: Long Chain-of-thought Supervised\nFinetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic\nplanning prior to reasoning, Test-time Scaling, Speculative Decoding, and\nInference-optimized Hardware, all using publicly available open-source\ndatasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art\nscores on public benchmarks for open-source models, while also performing\nstrongly in other areas such as Code and Science. Our results confirm that a\nmore parameter-efficient model like K2-Think 32B can compete with\nstate-of-the-art systems through an integrated post-training recipe that\nincludes long chain-of-thought training and strategic inference-time\nenhancements, making open-source reasoning systems more accessible and\naffordable. K2-Think is freely available at k2think.ai, offering best-in-class\ninference speeds of over 2,000 tokens per second per request via the Cerebras\nWafer-Scale Engine."}
{"id": "2509.07019", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07019", "abs": "https://arxiv.org/abs/2509.07019", "authors": ["Xinquan Wu", "Xuefeng Yan", "Mingqiang Wei", "Donghai Guan"], "title": "An efficient deep reinforcement learning environment for flexible job-shop scheduling", "comment": null, "summary": "The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial\noptimization problem that has a wide-range of applications in the real world.\nIn order to generate fast and accurate scheduling solutions for FJSP, various\ndeep reinforcement learning (DRL) scheduling methods have been developed.\nHowever, these methods are mainly focused on the design of DRL scheduling\nAgent, overlooking the modeling of DRL environment. This paper presents a\nsimple chronological DRL environment for FJSP based on discrete event\nsimulation and an end-to-end DRL scheduling model is proposed based on the\nproximal policy optimization (PPO). Furthermore, a short novel state\nrepresentation of FJSP is proposed based on two state variables in the\nscheduling environment and a novel comprehensible reward function is designed\nbased on the scheduling area of machines. Experimental results on public\nbenchmark instances show that the performance of simple priority dispatching\nrules (PDR) is improved in our scheduling environment and our DRL scheduling\nmodel obtains competing performance compared with OR-Tools, meta-heuristic, DRL\nand PDR scheduling methods."}
{"id": "2509.07872", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07872", "abs": "https://arxiv.org/abs/2509.07872", "authors": ["Yajun Yu", "Steve Jiang", "Robert Timmerman", "Hao Peng"], "title": "Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy", "comment": null, "summary": "Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR)\nis a novel treatment that delivers radiation in pulses of protracted intervals.\nAccurate prediction of gross tumor volume (GTV) changes through regression\nmodels has substantial prognostic value. This study aims to develop a\nmulti-omics based support vector regression (SVR) model for predicting GTV\nchange. A retrospective cohort of 39 patients with 69 brain metastases was\nanalyzed, based on radiomics (MRI images) and dosiomics (dose maps) features.\nDelta features were computed to capture relative changes between two time\npoints. A feature selection pipeline using least absolute shrinkage and\nselection operator (Lasso) algorithm with weight- or frequency-based ranking\ncriterion was implemented. SVR models with various kernels were evaluated using\nthe coefficient of determination (R2) and relative root mean square error\n(RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate\nthe limitation of small data size. Multi-omics models that integrate radiomics,\ndosiomics, and their delta counterparts outperform individual-omics models.\nDelta-radiomic features play a critical role in enhancing prediction accuracy\nrelative to features at single time points. The top-performing model achieves\nan R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows\npromising performance in predicting continuous change of GTV. It provides a\nmore quantitative and personalized approach to assist patient selection and\ntreatment adjustment in PULSAR."}
{"id": "2509.07036", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07036", "abs": "https://arxiv.org/abs/2509.07036", "authors": ["Federico Cerutti"], "title": "Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators", "comment": "Accepted at the 2nd edition of the Workshop in AI and Finance at\n  ECAI-2025", "summary": "This paper presents a methodological approach to financial time series\nanalysis by combining causal discovery and uncertainty-aware forecasting. As a\ncase study, we focus on four key U.S. macroeconomic indicators -- GDP, economic\ngrowth, inflation, and unemployment -- and we apply the LPCMCI framework with\nGaussian Process Distance Correlation (GPDC) to uncover dynamic causal\nrelationships in quarterly data from 1970 to 2021. Our results reveal a robust\nunidirectional causal link from economic growth to GDP and highlight the\nlimited connectivity of inflation, suggesting the influence of latent factors.\nUnemployment exhibits strong autoregressive dependence, motivating its use as a\ncase study for probabilistic forecasting. Leveraging the Chronos framework, a\nlarge language model trained for time series, we perform zero-shot predictions\non unemployment. This approach delivers accurate forecasts one and two quarters\nahead, without requiring task-specific training. Crucially, the model's\nuncertainty-aware predictions yield 90\\% confidence intervals, enabling\neffective anomaly detection through statistically principled deviation\nanalysis. This study demonstrates the value of combining causal structure\nlearning with probabilistic language models to inform economic policy and\nenhance forecasting robustness."}
{"id": "2509.07896", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07896", "abs": "https://arxiv.org/abs/2509.07896", "authors": ["Philipp Lepold", "Jonas Leichtle", "Tobias Röddiger", "Michael Beigl"], "title": "Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings", "comment": null, "summary": "Automatic sleep staging typically relies on gold-standard EEG setups, which\nare accurate but obtrusive and impractical for everyday use outside sleep\nlaboratories. This limits applicability in real-world settings, such as home\nenvironments, where continuous, long-term monitoring is needed. Detecting sleep\nonset is particularly relevant, enabling consumer applications (e.g.\nautomatically pausing media playback when the user falls asleep). Recent\nresearch has shown correlations between in-ear EEG and full-scalp EEG for\nvarious phenomena, suggesting wearable, in-ear devices could allow unobtrusive\nsleep monitoring. We investigated the feasibility of using single-channel\nin-ear electrophysiological (ExG) signals for automatic sleep staging in a\nwearable device by conducting a sleep study with 11~participants (mean age:\n24), using a custom earpiece with a dry eartip electrode (D\\\"atwyler SoftPulse)\nas a measurement electrode in one ear and a reference in the other. Ground\ntruth sleep stages were obtained from an Apple Watch Ultra, validated for sleep\nstaging. Our system achieved 90.5% accuracy for binary sleep detection (Awake\nvs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep)\nusing leave-one-subject-out validation. These findings demonstrate the\npotential of in-ear electrodes as a low-effort, comfortable approach to sleep\nmonitoring, with applications such as stopping podcasts when users fall asleep."}
{"id": "2509.07131", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07131", "abs": "https://arxiv.org/abs/2509.07131", "authors": ["Nicolò Romandini", "Carlo Mazzocca", "Kai Otsuki", "Rebecca Montanari"], "title": "SoK: Security and Privacy of AI Agents for Blockchain", "comment": "This work has been accepted to the 7th International Conference on\n  Blockchain Computing and Applications (BCCA 2025)", "summary": "Blockchain and smart contracts have garnered significant interest in recent\nyears as the foundation of a decentralized, trustless digital ecosystem,\nthereby eliminating the need for traditional centralized authorities. Despite\ntheir central role in powering Web3, their complexity still presents\nsignificant barriers for non-expert users. To bridge this gap, Artificial\nIntelligence (AI)-based agents have emerged as valuable tools for interacting\nwith blockchain environments, supporting a range of tasks, from analyzing\non-chain data and optimizing transaction strategies to detecting\nvulnerabilities within smart contracts. While interest in applying AI to\nblockchain is growing, the literature still lacks a comprehensive survey that\nfocuses specifically on the intersection with AI agents. Most of the related\nwork only provides general considerations, without focusing on any specific\ndomain. This paper addresses this gap by presenting the first Systematization\nof Knowledge dedicated to AI-driven systems for blockchain, with a special\nfocus on their security and privacy dimensions, shedding light on their\napplications, limitations, and future research directions."}
{"id": "2509.07945", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07945", "abs": "https://arxiv.org/abs/2509.07945", "authors": ["Yuan Pu", "Yazhe Niu", "Jia Tang", "Junyu Xiong", "Shuai Hu", "Hongsheng Li"], "title": "One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning", "comment": "43 pages, 19 figures", "summary": "In heterogeneous multi-task learning, tasks not only exhibit diverse\nobservation and action spaces but also vary substantially in intrinsic\ndifficulty. While conventional multi-task world models like UniZero excel in\nsingle-task settings, we find that when handling large-scale heterogeneous\nenvironments, gradient conflicts and the loss of model plasticity often\nconstrain their sample and computational efficiency. In this work, we address\nthese challenges from two perspectives: the single learning iteration and the\noverall learning process. First, we investigate the impact of key design spaces\non extending UniZero to multi-task planning. We find that a Mixture-of-Experts\n(MoE) architecture provides the most substantial performance gains by\nmitigating gradient conflicts, leading to our proposed model,\n\\textit{ScaleZero}. Second, to dynamically balance the computational load\nacross the learning process, we introduce an online, LoRA-based \\textit{dynamic\nparameter scaling} (DPS) strategy. This strategy progressively integrates LoRA\nadapters in response to task-specific progress, enabling adaptive knowledge\nretention and parameter expansion. Empirical evaluations on standard benchmarks\nsuch as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying\nexclusively on online reinforcement learning with one model, attains\nperformance on par with specialized single-task baselines. Furthermore, when\naugmented with our dynamic parameter scaling strategy, our method achieves\ncompetitive performance while requiring only 80\\% of the single-task\nenvironment interaction steps. These findings underscore the potential of\nScaleZero for effective large-scale multi-task learning. Our code is available\nat \\textcolor{magenta}{https://github.com/opendilab/LightZero}."}
{"id": "2509.07287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07287", "abs": "https://arxiv.org/abs/2509.07287", "authors": ["Yan Pang", "Wenlong Meng", "Xiaojing Liao", "Tianhao Wang"], "title": "Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm", "comment": "20 pages", "summary": "With the rapid development of large language models, the potential threat of\ntheir malicious use, particularly in generating phishing content, is becoming\nincreasingly prevalent. Leveraging the capabilities of LLMs, malicious users\ncan synthesize phishing emails that are free from spelling mistakes and other\neasily detectable features. Furthermore, such models can generate\ntopic-specific phishing messages, tailoring content to the target domain and\nincreasing the likelihood of success.\n  Detecting such content remains a significant challenge, as LLM-generated\nphishing emails often lack clear or distinguishable linguistic features. As a\nresult, most existing semantic-level detection approaches struggle to identify\nthem reliably. While certain LLM-based detection methods have shown promise,\nthey suffer from high computational costs and are constrained by the\nperformance of the underlying language model, making them impractical for\nlarge-scale deployment.\n  In this work, we aim to address this issue. We propose Paladin, which embeds\ntrigger-tag associations into vanilla LLM using various insertion strategies,\ncreating them into instrumented LLMs. When an instrumented LLM generates\ncontent related to phishing, it will automatically include detectable tags,\nenabling easier identification. Based on the design on implicit and explicit\ntriggers and tags, we consider four distinct scenarios in our work. We evaluate\nour method from three key perspectives: stealthiness, effectiveness, and\nrobustness, and compare it with existing baseline methods. Experimental results\nshow that our method outperforms the baselines, achieving over 90% detection\naccuracy across all scenarios."}
{"id": "2509.07955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07955", "abs": "https://arxiv.org/abs/2509.07955", "authors": ["Oliver Daniels", "Stuart Armstrong", "Alexandre Maranhão", "Mahirah Fairuz Rahman", "Benjamin M. Marlin", "Rebecca Gorman"], "title": "ACE and Diverse Generalization via Selective Disagreement", "comment": null, "summary": "Deep neural networks are notoriously sensitive to spurious correlations -\nwhere a model learns a shortcut that fails out-of-distribution. Existing work\non spurious correlations has often focused on incomplete\ncorrelations,leveraging access to labeled instances that break the correlation.\nBut in cases where the spurious correlations are complete, the correct\ngeneralization is fundamentally \\textit{underspecified}. To resolve this\nunderspecification, we propose learning a set of concepts that are consistent\nwith training data but make distinct predictions on a subset of novel unlabeled\ninputs. Using a self-training approach that encourages \\textit{confident} and\n\\textit{selective} disagreement, our method ACE matches or outperforms existing\nmethods on a suite of complete-spurious correlation benchmarks, while remaining\nrobust to incomplete spurious correlations. ACE is also more configurable than\nprior approaches, allowing for straight-forward encoding of prior knowledge and\nprincipled unsupervised model selection. In an early application to\nlanguage-model alignment, we find that ACE achieves competitive performance on\nthe measurement tampering detection benchmark \\textit{without} access to\nuntrusted measurements. While still subject to important limitations, ACE\nrepresents significant progress towards overcoming underspecification."}
{"id": "2509.07571", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07571", "abs": "https://arxiv.org/abs/2509.07571", "authors": ["Xiyu Guo", "Shan Wang", "Chunfang Ji", "Xuefeng Zhao", "Wenhao Xi", "Yaoyao Liu", "Qinglan Li", "Chao Deng", "Junlan Feng"], "title": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference", "comment": null, "summary": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches."}
{"id": "2508.18933", "categories": ["cs.AI", "cs.CR", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18933", "abs": "https://arxiv.org/abs/2508.18933", "authors": ["David Egea", "Barproda Halder", "Sanghamitra Dutta"], "title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation", "comment": null, "summary": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis."}
{"id": "2509.07941", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07941", "abs": "https://arxiv.org/abs/2509.07941", "authors": ["Kai Ye", "Liangcai Su", "Chenxiong Qian"], "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented Code Generation", "comment": "This paper has been accepted by the ACM Conference on Computer and\n  Communications Security (CCS) 2025", "summary": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io."}
{"id": "2509.07017", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07017", "abs": "https://arxiv.org/abs/2509.07017", "authors": ["Andrew Kiruluta", "Priscilla Burity"], "title": "From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning", "comment": null, "summary": "We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning\nframework that embeds logical rules as spectral templates and performs\ninference directly in the graph spectral domain. By leveraging graph signal\nprocessing (GSP) and frequency-selective filters grounded in the Laplacian\neigenstructure of knowledge graphs, the architecture unifies the\ninterpretability of symbolic reasoning with the scalability and adaptability of\nspectral learning. Beyond the core formulation, we incorporate a comprehensive\nset of extensions, including dynamic graph and basis learning, rational and\ndiffusion filters for sharper spectral selectivity, mixture-of-spectral-experts\nfor modular specialization, proof-guided training with spectral curricula, and\nuncertainty quantification for calibrated confidence. Additional enhancements\nsuch as large language model coupling, co-spectral transfer alignment,\nadversarial robustness, efficient GPU kernels, generalized Laplacians, and\ncausal interventions further expand the versatility of the framework.\n  Empirical evaluation on state-of-the-art reasoning benchmarks such as\nProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior\naccuracy, faster inference, improved robustness to adversarial perturbations,\nand higher interpretability compared to leading baselines including\ntransformers, message-passing neural networks, and neuro-symbolic logic\nprogramming systems. Spectral attribution and proof-band agreement analyses\nconfirm that model decisions align closely with symbolic proof structures,\nwhile transfer experiments validate effective domain adaptation through\nco-spectral alignment. These results establish Spectral NSR as a scalable and\nprincipled foundation for the next generation of reasoning systems, offering\ntransparency, robustness, and generalization beyond conventional approaches."}
{"id": "2509.07955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07955", "abs": "https://arxiv.org/abs/2509.07955", "authors": ["Oliver Daniels", "Stuart Armstrong", "Alexandre Maranhão", "Mahirah Fairuz Rahman", "Benjamin M. Marlin", "Rebecca Gorman"], "title": "ACE and Diverse Generalization via Selective Disagreement", "comment": null, "summary": "Deep neural networks are notoriously sensitive to spurious correlations -\nwhere a model learns a shortcut that fails out-of-distribution. Existing work\non spurious correlations has often focused on incomplete\ncorrelations,leveraging access to labeled instances that break the correlation.\nBut in cases where the spurious correlations are complete, the correct\ngeneralization is fundamentally \\textit{underspecified}. To resolve this\nunderspecification, we propose learning a set of concepts that are consistent\nwith training data but make distinct predictions on a subset of novel unlabeled\ninputs. Using a self-training approach that encourages \\textit{confident} and\n\\textit{selective} disagreement, our method ACE matches or outperforms existing\nmethods on a suite of complete-spurious correlation benchmarks, while remaining\nrobust to incomplete spurious correlations. ACE is also more configurable than\nprior approaches, allowing for straight-forward encoding of prior knowledge and\nprincipled unsupervised model selection. In an early application to\nlanguage-model alignment, we find that ACE achieves competitive performance on\nthe measurement tampering detection benchmark \\textit{without} access to\nuntrusted measurements. While still subject to important limitations, ACE\nrepresents significant progress towards overcoming underspecification."}
{"id": "2509.07260", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07260", "abs": "https://arxiv.org/abs/2509.07260", "authors": ["Xin Wang", "Ting Dang", "Xinyu Zhang", "Vassilis Kostakos", "Michael J. Witbrock", "Hong Jia"], "title": "HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring", "comment": "9 pages, 6 tables, 6 figures", "summary": "Mobile and wearable healthcare monitoring play a vital role in facilitating\ntimely interventions, managing chronic health conditions, and ultimately\nimproving individuals' quality of life. Previous studies on large language\nmodels (LLMs) have highlighted their impressive generalization abilities and\neffectiveness in healthcare prediction tasks. However, most LLM-based\nhealthcare solutions are cloud-based, which raises significant privacy concerns\nand results in increased memory usage and latency. To address these challenges,\nthere is growing interest in compact models, Small Language Models (SLMs),\nwhich are lightweight and designed to run locally and efficiently on mobile and\nwearable devices. Nevertheless, how well these models perform in healthcare\nprediction remains largely unexplored. We systematically evaluated SLMs on\nhealth prediction tasks using zero-shot, few-shot, and instruction fine-tuning\napproaches, and deployed the best performing fine-tuned SLMs on mobile devices\nto evaluate their real-world efficiency and predictive performance in practical\nhealthcare scenarios. Our results show that SLMs can achieve performance\ncomparable to LLMs while offering substantial gains in efficiency and privacy.\nHowever, challenges remain, particularly in handling class imbalance and\nfew-shot scenarios. These findings highlight SLMs, though imperfect in their\ncurrent form, as a promising solution for next-generation, privacy-preserving\nhealthcare monitoring."}
{"id": "2509.07367", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.07367", "abs": "https://arxiv.org/abs/2509.07367", "authors": ["Cunxi Yu", "Rongjian Liang", "Chia-Tung Ho", "Haoxing Ren"], "title": "Autonomous Code Evolution Meets NP-Completeness", "comment": "31 pages, 11 figures", "summary": "Large language models (LLMs) have recently shown strong coding abilities,\nenabling not only static code generation but also iterative code self-evolving\nthrough agentic frameworks. Recently, AlphaEvolve \\cite{novikov2025alphaevolve}\ndemonstrated that LLM-based coding agents can autonomously improve algorithms\nand surpass human experts, with scopes limited to isolated kernels spanning\nhundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the\nfirst framework to extend LLM-based code evolution to the full repository\nscale, encompassing hundreds of files and tens of thousands of lines of C/C++\ncode. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem\nand a cornerstone of both theory and applications. SATLUTION orchestrates LLM\nagents to directly evolve solver repositories under strict correctness\nguarantees and distributed runtime feedback, while simultaneously self-evolving\nits own evolution policies and rules. Starting from SAT Competition 2024\ncodebases and benchmark, SATLUTION evolved solvers that decisively outperformed\nthe human-designed winners of the SAT Competition 2025, and also surpassed both\n2024 and 2025 champions on the 2024 benchmarks."}
{"id": "2509.07939", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07939", "abs": "https://arxiv.org/abs/2509.07939", "authors": ["Katsuaki Nakano", "Reza Feyyazi", "Shanchieh Jay Yang", "Michael Zuzak"], "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments"}
