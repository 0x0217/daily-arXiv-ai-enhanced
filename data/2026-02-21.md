<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?](https://arxiv.org/abs/2602.17345)
*Boyang Ma,Hechuan Guo,Peizhuo Lv,Minghui Xu,Xuelong Dai,YeChao Zhang,Yijun Yang,Yue Zhang*

Main category: cs.CR

TL;DR: 체화된 AI 시스템은 안전이 중요한 실제 환경으로 빠르게 진화하고 있으며, 이로 인해 보안, 안전성 및 신뢰성에 대한 근본적인 질문이 제기되고 있다.


<details>
  <summary>Details</summary>
Motivation: 체화된 AI의 고유한 실패 및 보안 문제를 이해하고 해결하기 위해.

Method: 본 논문에서는 체화된 AI의 실패를 모델 결함이나 전통적인 CPS 공격이 아닌 체화로 인한 시스템 수준의 불일치에서 기인한 것으로 주장하며, 이를 바탕으로 네 가지 핵심 통찰을 제시한다.

Result: 체화된 AI는 보안이 더 어렵다는 네 가지 이유: 의미론적 정확성이 물리적 안전을 보장하지 않음, 동일한 행동이 비선형 동력학으로 인해 다른 결과를 초래함, 작은 오류의 전파와 증폭, 안전성이 시간적 또는 시스템 계층적으로 조합되지 않음.

Conclusion: 체화된 AI 보안을 강화하기 위해서는 구성 요소 수준 방어를 넘어서 시스템 수준의 물리적 위험, 불확실성 및 실패 전파에 대한 사고로 나아가야 한다.

Abstract: Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.

</details>


### [2] [Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge](https://arxiv.org/abs/2602.17452)
*Wyatt Benno,Alberto Centelles,Antoine Douchet,Khalil Gibran*

Main category: cs.CR

TL;DR: Jolt Atlas는 제로 지식 기반의 기계 학습 프레임워크로, Jolt 증명 시스템을 모델 추론에 확장하며, ONNX 텐서 연산에 직접 적용된다. 이 프레임워크는 메모리 제약이 있는 환경에서도 모델 추론을 증명할 수 있다. 또한, Jolt Atlas는 암호화 검증을 가능하게 하여 개인 정보 보호 중심의 환경에 적합하다.


<details>
  <summary>Details</summary>
Motivation: Jolt Atlas는 제로 지식 기반의 모델 추론 검증을 가능하게 하여 개인 정보 보호 중심의 환경에서의 활용을 목적으로 한다.

Method: Jolt의 조회 중심 접근 방식을 ONNX 텐서 연산에 적용하고, sumcheck 프로토콜을 사용하는 조회 인수를 통해 비선형 함수에 적합하게 구성하였다. 또한, 모델의 정확성을 유지하면서 조회 테이블의 크기를 줄이는 최적화를 수행한다.

Result: Jolt Atlas는 메모리 제약이 있는 환경에서도 모델 추론을 신속하게 증명할 수 있으며, 다양한 기계 학습 작업에 대한 실용적인 증명 시간을 보여준다.

Conclusion: Jolt Atlas는 특별한 하드웨어 없이도 장치에서 실행 가능한 암호화 검증을 가능하게 하여 개인 정보 보호 중심의 환경에 매우 적합하다.

Abstract: We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions.
  Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verification optimisations detailed in this paper. We demonstrate that Jolt Atlas can prove model inference in memory-constrained environments -- a prover property commonly referred to as \textit{streaming}. Furthermore, we discuss how Jolt Atlas achieves zero-knowledge through the BlindFold technique, as introduced in Vega. In contrast to existing zkML frameworks, we show practical proving times for classification, embedding, automated reasoning, and small language models.
  Jolt Atlas enables cryptographic verification that can be run on-device, without specialised hardware. The resulting proofs are succinctly verifiable. This makes Jolt Atlas well-suited for privacy-centric and adversarial environments. In a companion work, we outline various use cases of Jolt Atlas, including how it serves as guardrails in agentic commerce and for trustless AI context (often referred to as \textit{AI memory}).

</details>


### [3] [Large-scale online deanonymization with LLMs](https://arxiv.org/abs/2602.16800)
*Simon Lermen,Daniel Paleka,Joshua Swanson,Michael Aerni,Nicholas Carlini,Florian Tramèr*

Main category: cs.CR

TL;DR: 대형 언어 모델이 대규모의 비식별화 작업을 수행할 수 있음을 보여준다. 우리의 시스템은 익명화된 온라인 프로필 및 대화만으로도 해커 뉴스 사용자와 Anthropic Interviewer 참가자를 높은 정밀도로 재식별할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 온라인에서 익명 사용자 보호를 위한 기존의 모델이 더 이상 유효하지 않음을 보여주고, 이에 대한 새로운 위협 모델을 제시하기 위함이다.

Method: 대형 언어 모델을 이용해 비정형 텍스트에서 사용자 특성을 추출하고, 의미 임베딩을 통해 후보 매칭을 검색한 후, 상위 후보에 대해 일치 여부를 확인하고 거짓 긍정 비율을 줄이는 방법을 구현했다.

Result: 개발된 방법은 68%의 재현율을 90%의 정밀도로 달성했으며, 기존의 비LLM 방법들보다 월등한 성능을 보였다.

Conclusion: 비익명 사용자를 보호하는 실용적 모호성이 더 이상 유지되지 않으며, 온라인 개인 정보 보호를 위한 위협 모델을 재검토해야 한다.

Abstract: We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that individual, we implement a scalable attack pipeline that uses LLMs to: (1) extract identity-relevant features, (2) search for candidate matches via semantic embeddings, and (3) reason over top candidates to verify matches and reduce false positives. Compared to prior deanonymization work (e.g., on the Netflix prize) that required structured data or manual feature engineering, our approach works directly on raw user content across arbitrary platforms. We construct three datasets with known ground-truth data to evaluate our attacks. The first links Hacker News to LinkedIn profiles, using cross-platform references that appear in the profiles. Our second dataset matches users across Reddit movie discussion communities; and the third splits a single user's Reddit history in time to create two pseudonymous profiles to be matched. In each setting, LLM-based methods substantially outperform classical baselines, achieving up to 68% recall at 90% precision compared to near 0% for the best non-LLM method. Our results show that the practical obscurity protecting pseudonymous users online no longer holds and that threat models for online privacy need to be reconsidered.

</details>


### [4] [Is Mamba Reliable for Medical Imaging?](https://arxiv.org/abs/2602.16723)
*Banafsheh Saber Latibari,Najmeh Nazari,Daniel Brignac,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: Mamba 모델의 견고성을 평가하여 의료 영상에서의 공격을 분석하고 방어의 필요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 의료 영상에서의 신뢰성을 높이기 위해 Mamba 모델의 견고성을 평가할 필요가 있다.

Method: Mamba를 MedM-NIST 분류 벤치마크에서 다양한 입력 수준의 공격을 통해 평가하였다.

Result: Mamba 모델은 여러 종류의 공격에서 취약점을 보였으며, 정확도에 미치는 영향을 정량화하였다.

Conclusion: Mamba의 배포를 위해서는 방어 기법이 필요함을 강조한다.

Abstract: State-space models like Mamba offer linear-time sequence processing and low memory, making them attractive for medical imaging. However, their robustness under realistic software and hardware threat models remains underexplored. This paper evaluates Mamba on multiple MedM-NIST classification benchmarks under input-level attacks, including white-box adversarial perturbations (FGSM/PGD), occlusion-based PatchDrop, and common acquisition corruptions (Gaussian noise and defocus blur) as well as hardware-inspired fault attacks emulated in software via targeted and random bit-flip injections into weights and activations. We profile vulnerabilities and quantify impacts on accuracy indicating that defenses are needed for deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: 이 연구는 대규모 인간 이동 시뮬레이션을 위한 MobCache라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 인간 이동 시뮬레이션은 도시 계획, 전염병학 및 교통 분석과 같은 응용 분야에 매우 중요합니다.

Method: 모바일 인식 캐시 프레임워크 MobCache를 설계하였으며, 이 프레임워크는 각 추론 단계를 잠재 공간 임베딩으로 인코딩하고, 경량 디코더를 사용하여 잠재 공간 추론 체인을 자연어로 변환하는 방식으로 진행됩니다.

Result: MobCache는 여러 차원에서 효율성을 크게 개선하며, 기존 최첨단 LLM 기반 방법과 비교하여 성능을 유지합니다.

Conclusion: MobCache는 대규모 인간 이동 시뮬레이션의 효율성을 크게 향상시키는 동시에 접근 방식을 최적화합니다.

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [6] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent는 복잡한 샘플의 분석 효율성을 향상시키기 위해 AI 워크플로우를 도입하여 분석 시간을 크게 단축시킵니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 시설들이 과학적 처리량의 한계인 분석 및 보고 지연 문제에 직면하고 있습니다.

Method: NeuDiff Agent는 TOPAZ에서 데이터 제품을 감소, 통합, 정제 및 검증하여 검증된 결정 구조와 출판 준비 완료된 CIF로 이끄는 AI 워크플로우입니다.

Result: NeuDiff Agent는 벽 시간(435분)을 86.5(4.7) 분에서 94.4(3.5) 분으로 단축시켰습니다.

Conclusion: 이 결과는 시설 결정학에서 추적 가능성과 출판 검증 요구 사항을 유지하면서 에이전트 AI를 배포할 수 있는 실질적인 경로를 제시합니다.

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [7] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5는 클라우드-엣지 협업과 실시간 상호작용을 가능하게 하는 최신 GUI 에이전트 모델이다.


<details>
  <summary>Details</summary>
Motivation: 최신 GUI 에이전트 모델을 개발하여 다양한 플랫폼에서의 협업 및 상호작용을 개선하고자 함.

Method: 여러 혁신 요소를 포함하는 모델을 설계하여 UI 이해, 추적 생성, 에이전트 능력 강화 및 멀티 플랫폼 환경 문제를 해결하는 알고리즘을 사용함.

Result: GUI-Owl-1.5는 20개 이상의 GUI 벤치마크에서 최첨단 결과를 기록함.

Conclusion: 모델은 오픈 소스로 제공되며, 다양한 분야에서 높은 성능을 보여주어 실용적인 응용 가능성을 지님.

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [8] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage는 LLM이 자동으로 에이전트를 생성하도록 하는 최초의 ADK로, 자가 생성된 토폴로지와 도구 세트를 제공하며, 포괄적이고 구조화된 메모리 지원을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 에이전트 개발 키트는 기능 지원이 부족하거나 수동 설계에 의존하고 있어 에이전트의 일반화 및 전체적인 성능을 제한하고 있습니다.

Method: OpenSage는 에이전트가 자신의 하위 에이전트 및 도구 키트를 생성하고 관리할 수 있는 효과적인 기능을 제공하며, 효율적 관리를 위한 계층적 그래프 기반 메모리 시스템과 소프트웨어 공학 작업에 맞춘 전문 도구 키트를 포함합니다.

Result: 세 가지 최첨단 벤치마크와 다양한 백본 모델을 통해 OpenSage의 장점을 증명하는 광범위한 실험을 수행했습니다.

Conclusion: OpenSage는 인간 중심에서 AI 중심으로의 패러다임 전환을 이끌어내며 차세대 에이전트 개발의 길을 열어줄 것으로 믿습니다.

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [9] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 이 논문은 대규모 언어 모델의 도구 호출 안전성과 텍스트 안전성 간의 차이를 측정하는 GAP 벤치마크를 제안하며, 도구 호출 안전성 검토의 필요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델이 외부 시스템과 상호작용할 때, 안전성 평가가 텍스트 수준의 거부 행동에 주로 집중되어 있어, 유해한 텍스트를 억제하는 정렬이 유해한 행동도 억제하는지에 대한 중요한 질문이 남아 있다.

Method: GAP 벤치마크라는 체계적인 평가 프레임워크를 도입하여, LLM 에이전트의 텍스트 수준 안전성과 도구 호출 수준 안전성 간의 차이를 측정한다. 우리는 여섯 가지 규제된 도메인에서 여섯 개의 최첨단 모델을 테스트한다.

Result: 모든 여섯 모델에서 텍스트 안전성이 도구 호출 안전성으로 전이되지 않음을 발견하였다. 텍스트 출력이 유해한 요청을 거부하더라도 도구 호출이 금지된 행동을 실행하는 경우가 관찰되었으며, 시스템 프롬프트가 도구 호출 행동에 미치는 영향이 크다.

Conclusion: 텍스트만으로 안전성을 평가하는 것은 에이전트의 행동 평가에 불충분하며, 도구 호출 안전성 측정 및 완화가 필요하다.

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [10] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: 기존 강화 학습 방법은 단일 정책 네트워크를 사용해 단순 작업이 대부분의 매개변수를 차지하고 복잡한 작업에 충분한 용량을 남기지 않는 문제를 가지고 있다. 본 논문은 Mixture-of-Experts(MoE) 아키텍처를 사용해 간단한 작업의 지배를 방지하고, Phase-Aware Mixture of Experts(PA-MoE)를 제안하여 전문가의 전문성을 유지하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 강화 학습 방법들이 단일 정책 네트워크를 사용하여 단순 작업이 매개변수의 대부분을 차지하는 경향이 있다.

Method: Phase-Aware Mixture of Experts(PA-MoE)를 제안하고, 경량의 phase router를 통해 강화 학습 목표에서 직접 잠재적 단계 경계를 학습한다.

Result: PA-MoE가 전문가의 전문성을 유지하면서 함께 할당을 해줄 수 있는 결과를 보여준다.

Conclusion: 실험 결과를 통해 제안한 PA-MoE의 효과성을 증명하였다.

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [11] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: 이 논문은 Instruction-Tool Retrieval(ITR)이라는 방법을 제안하여 LLM 에이전트의 시스템 지침과 도구 카탈로그 처리를 최적화하며, 이를 통해 비용 절감과 오류 감소를 실현한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 긴 시스템 지침을 매 단계마다 읽고 큰 도구 카탈로그를 재사용할 때 비용 증가와 오류 발생 가능성 증가에 직면해 있다.

Method: ITR은 각 단계에서 최소한의 시스템 프롬프트 조각과 필요한 도구의 가장 작은 하위 집합만을 검색하도록 설계된 RAG 변형이다.

Result: ITR을 사용한 결과, 단계별 컨텍스트 토큰 수가 95% 감소하고, 올바른 도구 라우팅이 32% 향상되며, 전체 episode 비용이 기존 방식에 비해 70% 절감되었다.

Conclusion: ITR은 에이전트가 2-20배 더 많은 루프를 실행할 수 있게 해주며, 특히 장기 자율 에이전트에서 유용하다.

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [12] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA는 의도 정렬 계획 메모리를 통해 긴 시간 동안의 컴퓨터 사용 실행을 안정화하도록 설계된 다중 에이전트 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 접근 방식은 사용자 의도에서 벗어지며, 주기적인 하위 문제를 반복적으로 해결하는 데 그쳐 오류 축적과 비효율성을 초래합니다.

Method: Planner, Plan-Optimizer 및 Critic이 공유 메모리를 통해 협력하여 원시 상호작용 흔적을 다중 보기 의도 표현 및 재사용 가능한 기술로 추상화합니다.

Result: IntentCUA는 74.83%의 작업 성공률과 0.91의 단계 효율 비율을 달성하여 RL 기반 방법과 궤적 중심 기준선을 초과했습니다.

Conclusion: 시스템 수준의 의도 추상화와 메모리 기반의 조정이 신뢰할 수 있고 효율적인 데스크탑 자동화에 필수적임을 강조합니다.

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [13] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: AI 코딩 에이전트가 GitHub에서 자동으로 풀 리퀘스트를 생성하는 현상을 조사하며, 이러한 에이전트의 설명 특성과 인간 리뷰어의 반응 차이를 분석했다.


<details>
  <summary>Details</summary>
Motivation: AI 코딩 에이전트가 생성하는 풀 리퀘스트의 특성 차이와 인간 리뷰어의 반응을 탐구하기 위함이다.

Method: AIDev 데이터셋을 사용하여 5개의 AI 코딩 에이전트가 생성한 풀 리퀘스트를 실증적으로 분석하였다.

Result: AI 코딩 에이전트는 독특한 PR 설명 스타일을 보이며, 이는 리뷰어의 참여도, 반응 시간, 병합 결과에 영향을 미친다.

Conclusion: 풀 리퀘스트의 발표 방식과 리뷰어 상호작용 역학이 인간-AI 협업 소프트웨어 개발에서 중요한 역할을 한다는 것을 강조한다.

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [14] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 코드 진화는 대형 언어 모델을 활용하여 기존 코드를 변형시켜 가능한 컴퓨터 프로그램을 검색하는 기술입니다. 이 연구에서는 더 간단한 기준선과 비교하지 않은 복잡한 코드 진화 방법의 성능을 세 가지 분야에서 평가하였습니다.


<details>
  <summary>Details</summary>
Motivation: 코드 진화 기법의 성능을 간단한 기준선과 비교하여 그 효율성을 평가하기 위한 것입니다.

Method: 세 가지 분야에서 두 개의 간단한 기준선을 테스트합니다: 더 나은 수학적 경계 찾기, 에이전트 기초 설계, 머신러닝 대회.

Result: 간단한 기준선이 모든 세 가지 분야에서 더 정교한 방법과 동등하거나 더 나은 성과를 보였습니다.

Conclusion: 코드 진화를 더 엄격하게 수행할 수 있는 방법과 최선의 관행에 대한 논의로 마무리합니다.

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [15] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: 6G 무선 시스템이 발전함에 따라 사용자 요구가 단일 지표가 아닌 다차원 목표로 변화하고 있으며, 의도 기반의 자율 인공지능이 중요해지고 있다.


<details>
  <summary>Details</summary>
Motivation: 6G의 발전과 사용자 요구의 복잡성으로 인해 자율 통신 시스템의 필요성이 증가하고 있다.

Method: LLM 기반의 네트워크 에이전트를 사용하여 사용자의 의도를 자연어로 해석하고 실행 가능한 결정으로 변환하는 방법이다.

Result: 의도 인식 및 자율성을 지원하는 물리 계층 작업의 한계를 파악하고, 다양한 적용 시나리오를 제시하였다.

Conclusion: AgenCom이라는 의도 기반 링크 결정 에이전트를 제시하여 다양한 사용자 선호와 채널 조건에 따라 통신 링크를 적응적으로 구성하였다.

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [16] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov는 실행 제약 하에서의 확장 가능한 학습을 가능하게 하는 오프라인 에이전트 학습 프레임워크다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트는 도구 피드백으로부터 학습하는 유망한 패러다임을 제공하지만, 피드백을 얻는 과정이 비쌈과 느린 문제로 인해 온라인 강화 학습이 비효율적이다.

Method: 이 논문에서는 메모리 없는 상태 전이로 검증을 모델링하는 오프라인 에이전트 학습 프레임워크인 LLM4Cov를 제안한다. 이는 결정론적 평가자에 의해 안내된다.

Result: 제안한 파이프라인을 사용하여 компакт한 4B-파라미터 모델이 에이전틱 평가에서 69.2%의 커버리지 합격률을 달성하며, 이는 스승 모델보다 5.3% 더 높은 성능을 보여준다.

Conclusion: 이 모델은 차원이 더 큰 모델들과 경쟁할 수 있는 성능을 입증하였다.

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [17] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: 이 논문은 에이전트가 온라인으로 기호적 인과 세계 모델을 구성하도록 하는 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 환경을 효율적으로 탐색하려면 에이전트가 자신의 세계의 기본 논리를 내재화해야 합니다.

Method: 메타 해석 학습과 술어 발명을 활용하여 에이전트의 의사 결정 루프에 연속적인 모델 학습과 수정을 통합하는 프레임워크를 제안합니다.

Result: 우리의 향상된 추론 접근 방식은 복잡한 관계 역학을 가진 도메인에 대해 확장 가능성과 샘플 효율성을 향상시킵니다.

Conclusion: 제안된 방법은 기존의 PPO 신경망 기반 벤치마크보다 샘플 효율성이 훨씬 더 높은 성능을 발휘합니다.

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [18] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: 이 논문은 웹에서 대리 행동을 가능하게 하는 의미적 계층의 필요성을 제안하며, 이를 위해 '웹 동사'라는 새로운 함수 집합을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 발전으로 자연어가 목표 지향 작업의 실용적인 인터페이스가 되고 있지만, 현재의 웹 에이전트는 클릭 및 키 입력과 같은 낮은 수준의 원시 작업만 수행하고 있어 비효율적입니다.

Method: 웹의 기능을 통합적으로 드러내는 타이핑된, 의미적으로 문서화된 함수 집합인 웹 동사를 제안합니다. 이는 API 또는 클라이언트 사이드 워크플로우를 통해 구현될 수 있습니다.

Result: 웹 동사를 사용함으로써 안정적이고 조합 가능한 단위로 에이전트가 신뢰할 수 있는 워크플로를 생성할 수 있게 되며, 이는 현재의 에이전트에 비해 간결하고 강력한 실행을 보여줍니다.

Conclusion: 웹 동사의 표준화를 위한 로드맵을 제시하여 웹 규모에서 신뢰할 수 있는 배포가 가능하도록 합니다.

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [19] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics는 자연어 설명에서 일반 PDE의 수치 해석기를 자동으로 설계하고 구현하는 다중 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: PDE의 정확한 수치 해석기를 설계하는 것은 종종 상당한 수학적 전문 지식과 수작업 조정을 필요로 한다.

Method: 본 프레임워크는 클래식 수치 해석에 기반한 투명한 해석기를 생성하며, 거친 실행 전략과 잔차 기반 자가 검증 메커니즘을 도입한다.

Result: 24개의 전형적 및 실제 PDE 문제에 대한 실험에서 AutoNumerics는 기존의 신경망 및 LLM 기반 기준과 비교하여 경쟁력 있거나 우수한 정확도를 달성하였다.

Conclusion: AutoNumerics는 자동 PDE 해결을 위한 접근 가능한 패러다임으로서의 가능성을 제시한다.

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [20] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify는 진단 의사결정을 지원하기 위해 후속 질문을 생성하는 AI 에이전트로, 의료 LLM의 정보 탐색 능력을 향상시키고 진단 오류를 줄이는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 의료에서 진단 작업을 위한 대규모 언어 모델의 활용이 증가하고 있으나, 초기 환자 상태만으로는 올바른 진단을 거의 직접적으로 추론할 수 없다.

Method: MedClarify는 차별적 진단과 유사한 후보 진단 목록을 계산하고, 진단 불확실성을 줄이기 위해 목표를 둔 후속 질문을 능동적으로 생성한다.

Result: 우리의 실험에서는 현재 LLM의 의료 추론 한계를 보여주고, 정보 이론적 추론 접근이 효과적인 후속 질문 생성을 통해 진단 오류를 약 27% 포인트 줄일 수 있음을 입증했다.

Conclusion: MedClarify는 에이전트 정보 탐색을 통해 의료 LLM을 개선하고, 실세계 임상 추론의 반복적이고 불확실한 특성을 반영하는 효과적인 대화를 촉진할 수 있는 길을 제시한다.

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [21] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 정보 검색 파이프라인에서 체인 오브 생각을 사용하는 LLM 기반 에이전트의 중간 추론 과정을 평가하기 위한 새로운 척도를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 현재의 체인 오브 생각 평가 방식은 목표 작업의 정확성에만 초점을 맞추고 있으며, 추론 과정의 질이나 유용성을 평가하지 않는다.

Method: 생각가-실행자 프레임워크를 사용하여 체인 오브 생각 생성을 실행과 분리하고, 재사용성 및 검증 가능성을 측정하는 두 가지 새로운 척도를 도입한다.

Result: 네 가지 생각가 모델을 열 개의 실행자 모델 위원회에 대해 평가한 결과, 재사용성과 검증 가능성이 표준 정확성과 상관관계가 없음을 발견하였다.

Conclusion: 특화된 추론 모델의 체인 오브 생각이 Llama와 Gemma 같은 일반 목적의 LLM보다 일관되게 재사용 가능하거나 검증 가능하지 않다는 점이 놀랍다.

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [22] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 인공지능(AI)의 인간 일반 지능 평가를 위한 새로운 접근법으로, '인간 게임'이라는 개념에 기반한 'AI GameStore' 플랫폼을 소개하며, 이를 통해 다양한 인간 게임에서 AI의 성능을 평가하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 기술 발전 속에서 AI를 인간의 일반 지능에 대해 rigorously 평가하는 것이 중요해지고 도전 과제가 되고 있다.

Method: AI GameStore라는 플랫폼을 통해 LLM을 사용하여 인기 있는 게임 환경을 활용하여 새로운 인간 게임을 생성하고, 이를 통해 AI의 게임 플레이 및 학습 능력을 평가한다.

Result: Apple App Store와 Steam의 상위 차트에 기반하여 생성한 100개의 게임 중, 최상의 VLM이 대다수 게임에서 인간 평균 점수의 10% 미만을 기록하였다.

Conclusion: AI GameStore를 실용적으로 구축하여 기계에서 인간과 유사한 일반 지능을 측정하고 향상시키기 위한 다음 단계를 제안한다.

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 브라질 중장년 및 노인을 위해 저비용 변수와 수정 가능성을 활용한 치매 분류 모델을 구축하고, 예측 모델링 접근 방식을 통해 치매 발생 확률을 추정하였다.


<details>
  <summary>Details</summary>
Motivation: 브라질 중장년 및 노인 인구에서 치매를 예측하기 위한 신뢰할 수 있는 모델을 개발할 필요성이 있다.

Method: Python을 사용하여 변수 선택 및 다변량 분석을 결합한 관찰 연구를 수행하였다.

Result: RF 모델이 로지스틱 회귀보다 뛰어난 성능을 보였으며, 연구에 따르면 치매 유병률은 9.6%였다.

Conclusion: 결과는 치매의 다면적 특성과 취약한 개인을 식별하기 위한 접근 가능성 있는 요소의 중요성을 강화한다.

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [24] [Linear Convergence in Games with Delayed Feedback via Extra Prediction](https://arxiv.org/abs/2602.17486)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 이 논문에서는 지연된 피드백이 있는 환경에서 WOGDA의 선형 수렴 속도를 도출한다.


<details>
  <summary>Details</summary>
Motivation: 실제 멀티 에이전트 학습에서 피드백 지연은 피할 수 없으며, 성능 저하를 심각하게 초래하는 것으로 알려져 있다.

Method: WOGDA 알고리즘을 사용하여 비제한 이차 게임에서 피드백 지연 하의 성능을 분석하였다. 또한, 알고리즘을 고전적인 Proximal Point (PP)보다 더 먼 미래 보상을 기반으로 업데이트되는 Extra Proximal Point (EPP) 근사로 해석했다.

Result: 표준 낙관론은 다음 단계 보상을 예측해 $t$ 이터레이션 후 지연 $m$에 대해 $	ext{exp}(-Θ(t/m^{5}))$의 속도로 평형에 선형 수렴함을 보여준다. 추가 낙관론을 사용하면 더 큰 단계 크기를 허용하고 수렴 속도를 $	ext{exp}(-Θ(t/(m^{2}	ext{log} m)))$로 크게 가속화한다.

Conclusion: 추가 낙관론이 피드백 지연으로 인한 성능 저하에 대한 유망한 대응책임을 입증한다.

Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-Θ(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-Θ(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.

</details>


### [25] [Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners](https://arxiv.org/abs/2602.17103)
*Sajad Ashkezari,Shai Ben-David*

Main category: cs.LG

TL;DR: 이 논문은 학습 개선 모델을 통해 에이전트가 더 바람직한 레이블을 얻기 위해 특성 값을 소폭 조정할 수 있는 방식에 대해 연구합니다.


<details>
  <summary>Details</summary>
Motivation: 최근 도입된 개선 학습 모델을 탐구하여 에이전트가 더 바람직한 레이블을 얻기 위한 작은 변화의 가능성을 제시합니다.

Method: 온라인 학습 가능성을 특성화하는 조합적 차원을 제공하고, 다중 클래스 설정 및 밴딧 피드백 설정에서의 학습 가능성을 분석합니다.

Result: 이 논문에서는 에이전트의 개선 비용 모델링을 포함하여 이전에 발표된 결과들을 광범위하게 확장합니다.

Conclusion: 이 연구는 개선 학습 모델에 대한 이해를 심화시키고 다양한 설정에서의 학습 가능성을 분석함으로써 이 분야의 발전에 기여합니다.

Abstract: We investigate the recently introduced model of learning with improvements, where agents are allowed to make small changes to their feature values to be warranted a more desirable label. We extensively extend previously published results by providing combinatorial dimensions that characterize online learnability in this model, by analyzing the multiclass setup, learnability in a bandit feedback setup, modeling agents' cost for making improvements and more.

</details>


### [26] [CounterFlowNet: From Minimal Changes to Meaningful Counterfactual Explanations](https://arxiv.org/abs/2602.17244)
*Oleksii Furman,Patryk Marszałek,Jan Masłowski,Piotr Gaiński,Maciej Zięba,Marek Śmieja*

Main category: cs.LG

TL;DR: CounterFlowNet는 모델 예측을 위한 효과적인 반사실적 설명 생성을 위한 새로운 방법입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 반사실적 설명 기법은 사용자가 정의한 제약 조건을 유지하면서 고품질의 설명을 생성하는 데 어려움을 겪고 있습니다.

Method: 이 논문에서는 조건부 생성 흐름 네트워크를 사용하는 특징 수정의 순차적 형식으로 반사실적 설명 생성을 공식화하는 CounterFlowNet을 제안합니다.

Result: CounterFlowNet은 주어진 제약 조건을 완전히 만족시키면서 유효성, 희소성, 신뢰성 및 다양성 간의 우수한 균형을 달성합니다.

Conclusion: CounterFlowNet은 희소한 편집을 생성하며, 특징의 실시간 제약 조건을 적용할 수 있습니다.

Abstract: Counterfactual explanations (CFs) provide human-interpretable insights into model's predictions by identifying minimal changes to input features that would alter the model's output. However, existing methods struggle to generate multiple high-quality explanations that (1) affect only a small portion of the features, (2) can be applied to tabular data with heterogeneous features, and (3) are consistent with the user-defined constraints. We propose CounterFlowNet, a generative approach that formulates CF generation as sequential feature modification using conditional Generative Flow Networks (GFlowNet). CounterFlowNet is trained to sample CFs proportionally to a user-specified reward function that can encode key CF desiderata: validity, sparsity, proximity and plausibility, encouraging high-quality explanations. The sequential formulation yields highly sparse edits, while a unified action space seamlessly supports continuous and categorical features. Moreover, actionability constraints, such as immutability and monotonicity of features, can be enforced at inference time via action masking, without retraining. Experiments on eight datasets under two evaluation protocols demonstrate that CounterFlowNet achieves superior trade-offs between validity, sparsity, plausibility, and diversity with full satisfaction of the given constraints.

</details>


### [27] [Flickering Multi-Armed Bandits](https://arxiv.org/abs/2602.17315)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 이 논문에서는 변화하는 무장 선택 문제인 FMAB를 소개하며, 각 라운드마다 사용 가능한 선택지가 바뀔 수 있음을 모델링하고 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 변화하는 선택지에 따라 최적의 선택을 효율적으로 식별하는 필요성이 있다.

Method: 두 단계의 알고리즘을 제안하고 분석한다. 첫 번째 단계에서 느린 랜덤 워크를 사용하여 탐사를 수행하고, 두 번째 단계에서 탐색된 정보를 기반으로 선택을 고수하는 접근 방식을 취한다.

Result: 두 가지 그래프 설정에 대해 높은 확률과 예상되는 하위 선형 후회 경계를 확립하였다, 탐사의 비용은 정보 이론적 하한과 일치하여 근사 최적임을 입증했다.

Conclusion: 이론적 보장을 수치적 시뮬레이션으로 보완하였으며, 재난 지역을 조사하는 로봇 차량의 시나리오를 포함하였다.

Abstract: We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i.i.d. Erdős--Rényi (ER) process and an Edge-Markovian process. We propose and analyze a two-phase algorithm that employs a lazy random walk for exploration to efficiently identify the optimal arm, followed by a navigation and commitment phase for exploitation. We establish high-probability and expected sublinear regret bounds for both graph settings. We show that the exploration cost of our algorithm is near-optimal by establishing a matching information-theoretic lower bound for this problem class, highlighting the fundamental cost of exploration under local-move constraints. We complement our theoretical guarantees with numerical simulations, including a scenario of a robotic ground vehicle scouting a disaster-affected region.

</details>


### [28] [Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models](https://arxiv.org/abs/2602.17497)
*Wen-Tse Chen,Jiayu Chen,Fahim Tajwar,Hao Zhu,Xintong Duan,Ruslan Salakhutdinov,Jeff Schneider*

Main category: cs.LG

TL;DR: 이 논문은 희소한 환경 피드백을 해결하기 위해 대규모 언어 모델의 사전 학습 지식으로부터 학습하는 새로운 방법론을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 자기 진화 에이전트를 훈련하는 과정에서 자체 샘플링 데이터와 희소한 환경 피드백으로부터 학습하는 것은 여전히 중요한 도전 과제입니다.

Method: 희소한 보상을 Dense한 훈련 신호(즉, 이점 함수)로 변환하기 위해 대규모 언어 모델(LLM)의 사전 학습 지식을 활용하며, 이를 RICL(회고적 맥락 학습)이라는 방법을 통해 구현합니다. 또한, RICO(L)라는 온라인 학습 프레임워크를 제안하여 RICL의 신용 할당 결과를 바탕으로 정책을 반복적으로 개선합니다.

Result: RICL은 제한된 샘플로 이점 함수를 정확하게 추정하고 환경에서의 중요한 상태를 효과적으로 식별하는 데 유용함을 실험적으로 입증합니다. BabyAI 시나리오에서의 확장된 평가 결과, RICOL은 전통적인 온라인 RL 알고리즘과 비교하여 상당히 높은 샘플 효율성으로 유사한 수렴 성능을 달성합니다.

Conclusion: 우리의 연구 결과는 LLM을 활용한 시간적 신용 할당의 잠재력을 강조하며, 샘플 효율성이 높고 일반화 가능한 RL 패러다임의 길을 열어줍니다.

Abstract: Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.

</details>


### [29] [Continual uncertainty learning](https://arxiv.org/abs/2602.17174)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: cs.LG

TL;DR: 비선형 동적 시스템에서 불확실성을 동시에 처리하기 위한 새로운 커리큘럼 기반의 지속적 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 비선형 동적 시스템에서 여러 불확실성을 동시에 처리하는 것은 로봇 시스템의 강인한 제어 성능을 보장하는 데 필요하다.

Method: 여러 불확실성을 포함한 복잡한 제어 문제를 지속적 학습 작업의 시퀀스로 분해하고, 각 불확실성에 대한 처리 전략을 순차적으로 학습한다.

Result: 진행 중인 학습에 따라 동적 불확실성이 점진적으로 확대되고 다양화되는 유한한 식물 집합으로 원래 시스템을 확장하고, 각 불확실성을 위한 고유한 최적화가 이루어진다.

Conclusion: 산업 응용으로 자동차 파워트레인을 위한 능동 진동 제어기를 설계하는 데 성공하였고, 이는 구조적 비선형성과 동적 변동에 강인함을 보였다.

Abstract: Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.

</details>


### [30] [FAMOSE: A ReAct Approach to Automated Feature Discovery](https://arxiv.org/abs/2602.17641)
*Keith Burghardt,Jienan Liu,Sadman Sakib,Yuning Hao,Bo Li*

Main category: cs.LG

TL;DR: 본 논문에서는 FAMOSE라는 새로운 프레임워크를 소개하며, 이는 자동 기능 탐색 및 선택을 통해 기계 학습에서의 기능 공학 문제를 해결하는 데 도움을 줍니다.


<details>
  <summary>Details</summary>
Motivation: 기계 학습에서의 기능 공학은 최적의 특성을 식별하는 데 상당한 도메인 전문 지식이 필요한 병목 현상입니다.

Method: FAMOSE는 ReAct 패러다임을 활용하여 기능을 자율적으로 탐색, 생성 및 다듬으며, 에이전트 아키텍처 내에서 기능 선택 및 평가 도구를 통합합니다.

Result: FAMOSE는 분류 작업에서 최첨단 성능을 발휘하고, 회귀 작업에서는 RMSE를 평균 2.0% 줄이며, 오류에 대한 강건성을 유지합니다.

Conclusion: FAMOSE는 특성 공학 문제를 해결하는 데 있어 AI 에이전트가 뛰어난 효과를 발휘함을 보여줍니다.

Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [31] [Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data](https://arxiv.org/abs/2602.16726)
*Hua Yan,Heng Tan,Yu Yang*

Main category: cs.MA

TL;DR: M2LSimu는 대규모 인간 이동 시뮬레이션을 위한 새로운 프레임워크로, 개인 수준의 프롬프트를 조정하여 집단 행동을 효과적으로 캡처할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 인간 이동 시뮬레이션은 도시 과학, 전염병학, 교통 분석 등 여러 과학 분야에 중요한 역할을 한다.

Method: M2LSimu는 이동 측정값을 활용하여 공유 데이터에서 유도된 정보를 기반으로 개인 수준의 프롬프트를 조정하는 프레임워크이다.

Result: 실험 결과, M2LSimu는 두 개의 공개 데이터셋에서 최신의 LLM 기반 방법들보다 상당히 우수한 성능을 보였다.

Conclusion: M2LSimu는 제한된 예산 내에서 여러 인구 수준의 이동 목표를 충족하면서도 개별 수준의 정교한 적응을 가능하게 한다.

Abstract: Large-scale human mobility simulation is critical for many science domains such as urban science, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility trajectories by modeling individual-level cognitive processes. However, these approaches generate individual mobility trajectories independently, without any population-level coordination mechanism, and thus fail to capture the emergence of collective behaviors. To address this issue, we design M2LSimu, a mobility measures-guided multi-prompt adjustment framework that leverages mobility measures derived from shared data as guidance to refine individual-level prompts for realistic mobility generation. Our framework applies coarse-grained adjustment strategies guided by mobility measures, progressively enabling fine-grained individual-level adaptation while satisfying multiple population-level mobility objectives under a limited budget. Experiments show that M2LSimu significantly outperforms state-of-the-art LLM-based methods on two public datasets.

</details>


### [32] [AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence](https://arxiv.org/abs/2602.16873)
*Geunbin Yu*

Main category: cs.MA

TL;DR: AdaptOrch는 다중 에이전트 오케스트레이션을 위한 동적 프레임워크로, 네 가지 전형적인 토폴로지(병렬, 순차, 계층, 혼합)를 사용하여 작업 의존성과 도메인 특성에 따라 최적의 오케스트레이션을 선택하여 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 다양한 제공업체의 대규모 언어 모델들이 유사한 기준 성능으로 수렴하면서, 단일 최상의 모델을 선택하는 전통적인 패러다임은 점차 수익 감소를 겪고 있습니다.

Method: AdaptOrch는 작업 의존성 그래프와 경험적으로 도출된 도메인 특성에 기반하여 네 가지 전형적인 토폴지(병렬, 순차, 계층, 혼합) 중에서 동적으로 선택하는 작업 적응 다중 에이전트 오케스트레이션의 공식 프레임워크입니다.

Result: AdaptOrch는 코딩(SWE-bench), 추론(GPQA), 검색-증강 생성 작업에서 매우 유의미한 성능 개선을 보여주며, 정적 단일 토폴로지 기준보다 12-23% 향상된 결과를 도출합니다.

Conclusion: 우리의 결과는 모델 스케일링과 독립적으로 오케스트레이션 설계를 일급 최적화 대상으로 설정합니다.

Abstract: As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.

</details>


### [33] [Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form](https://arxiv.org/abs/2602.17078)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Husheng Li,Ahmed H. Qureshi*

Main category: cs.MA

TL;DR: 본 논문은 연속 시간 제약 MDP(CT-CMDP) 공식을 제안하고, 이를 통해 안전성을 고려한 다중 에이전트 강화 학습(MARL)을 위한 새로운 프레임워크를 개발했다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 강화 학습 알고리즘은 고정된 결정 간격이 있는 이산 시간 마르코프 결정 과정(MDP)에 의존하고 있지만, 이는 복잡한 다중 에이전트 동역학에 부적합하다. 특히 고주파 또는 불규칙한 시간 간격 설정에서 성능 저하가 발생한다.

Method: 이 논문에서는 이산 MDP를 에피그래프 기반 재구성을 통해 연속 시간 제약 MDP(CT-CMDP)로 변환하는 새로운 MARL 프레임워크와 물리정보 신경망(PINN) 기반의 액터-비평자 방법을 제안하여 안정적이고 효율적인 최적화를 가능하게 한다.

Result: 연속 시간 안전 다중 입자 환경(MPE)과 안전한 다중 에이전트 MuJoCo 벤치마크에서 접근 방식을 평가한 결과, 더 부드러운 가치 근사와 안정적인 훈련, 안전한 MARL 기준에 비해 개선된 성능을 보였다.

Conclusion: 제안된 방법의 효과성과 강인성을 검증하였다.

Abstract: Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Bellman (HJB) equations. However, they rarely account for safety constraints such as collision penalties, since these introduce discontinuities that make HJB-based learning difficult. To address this challenge, we propose a continuous-time constrained MDP (CT-CMDP) formulation and a novel MARL framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. We then solve this by proposing a novel physics-informed neural network (PINN)-based actor-critic method that enables stable and efficient optimization in continuous time. We evaluate our approach on continuous-time safe multi-particle environments (MPE) and safe multi-agent MuJoCo benchmarks. Results demonstrate smoother value approximations, more stable training, and improved performance over safe MARL baselines, validating the effectiveness and robustness of our method.

</details>


### [34] [AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation](https://arxiv.org/abs/2602.17100)
*Siyu Wang,Ruotian Lu,Zhihao Yang,Yuchao Wang,Yanzhou Zhang,Lei Xu,Qimin Xu,Guojun Yin,Cailian Chen,Xinping Guan*

Main category: cs.MA

TL;DR: 이 논문은 LLM 기반의 다중 에이전트 시스템인 AgentConductor를 제안하여 상호작용 토폴로지를 동적으로 생성하고 코드 생성 성능을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 기존의 다중 에이전트 시스템은 과제 난이도에 따라 토폴로지 밀도를 조정하지 않고, 실행 피드백을 활용하여 토폴로지를 반복적으로 개선하지 못해 중복 통신과 성능 병목 현상이 발생한다.

Method: AgentConductor는 LLM 기반의 오케스트레이터 에이전트를 중심으로 하는 강화 학습 최적화된 다중 에이전트 시스템으로, 쿼리에 따라 에이전트 역할과 작업 난이도를 유추하고, 작업에 적합한 밀도 인식 계층형 비순환 그래프(DAG) 토폴로지를 구성한다.

Result: AgentConductor는 3개의 경쟁 수준과 2개의 기초 코드 데이터 세트에서 최첨단 정확도를 달성하여 가장 강력한 기준선보다 pass@1 정확도에서 최대 14.6%, 밀도 감소에서 13%, 토큰 비용 감소에서 68% 향상되었다.

Conclusion: AgentConductor는 코드 생성 성능을 극적으로 향상시키고, 다중 에이전트 시스템의 상호작용을 동적으로 최적화 할 수 있다.

Abstract: Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction.

</details>


### [35] [Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation](https://arxiv.org/abs/2602.17203)
*Yuhong Luo,Daniel Schoepflin,Xintong Wang*

Main category: cs.MA

TL;DR: 알고리즘적 담합의 위험과 규제 개입의 필요성에 대한 논의가 진행되고 있으며, 본 논문에서는 테스트 시 제약 조건 하에서 알고리즘 행동을 분석하기 위한 메타 게임 설계를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 알고리즘적 담합의 위협 및 규제 개입 필요성에 대한 논의를 다루고, 기존 평가의 한계를 극복하기 위해 실험적 방법론을 제안한다.

Method: 에이전트를 훈련된 정책으로 모델링하고 메타 전략, 초기 정책과 게임 내 적응 규칙을 결합하여 담합 위험을 분석한다.

Result: 균형 혼합의 대항자와의 게임 통계 결과를 통해 알고리즘적 담합의 가능성을 평가하고 가격 전략의 효과를 다룬다.

Conclusion: 대칭 및 비대칭 비용 설정 하에서 반복 가격 게임을 평가함으로써 알고리즘적 담합의 실행 가능성과 가격 전략의 효용을 발견하였다.

Abstract: The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies with distinct strategic characteristics (e.g., competitive, naively cooperative, robustly collusive), and formulate the problem as selecting a meta-strategy that combines a pretrained, initial policy with an in-game adaptation rule. We seek to examine whether collusion can emerge under rational choices and how agents co-adapt toward cooperation or competition. To this end, we sample normal-form empirical games over meta-strategy profiles, % across random initial game states, compute relevant game statistics (e.g., payoffs against individuals and regret against an equilibrium mixture of opponents), and construct empirical best-response graphs to uncover strategic relationships. We evaluate both reinforcement-learning and LLM-based strategies in repeated pricing games under symmetric and asymmetric cost settings, and present findings on the feasibility of algorithmic collusion and the effectiveness of pricing strategies in practical ``test-time'' environments.
  The source code and the full paper with appendix are available at: https://github.com/chailab-rutgers/CollusionMetagame.

</details>
