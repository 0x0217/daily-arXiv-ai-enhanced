<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 20]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Intermittent File Encryption in Ransomware: Measurement, Modeling, and Detection](https://arxiv.org/abs/2510.15133)
*Ynes Ineza,Gerald Jackson,Prince Niyonkuru,Jaden Kevil,Abdul Serwadda*

Main category: cs.CR

TL;DR: 파일 암호화 랜섬웨어가 점점 더 간헐적인 암호화 기법을 채택하여 파일의 일부만 암호화함으로써 전통적인 탐지 방법을 피하고 있다. 이 논문은 이러한 간헐적 암호화 방식의 데이터를 체계적으로 분석하여 파일 구조에 미치는 영향을 규명한다.


<details>
  <summary>Details</summary>
Motivation: 파일 암호화 랜섬웨어가 사용하는 간헐적 암호화 기법은 기존 탐지 방법을 회피하기 위한 것으로, 다양한 파일 형식에서 부분 암호화 시 나타나는 여러 특성을 연구할 필요가 있다.

Method: 파일 유형별로 간헐적 암호화의 바이트 수준 통계를 체계적으로 분석하고, 맞춤형 혼합 모델을 사용하여 KL 발산의 고전적인 상한을 전문화하여 히스토그램 기반 탐지기를 위한 탐지 가능성 한계를 도출하였다.

Result: 분석 결과, 지역 분석을 통한 청크 수준 CNN이 글로벌 분석 방법보다 항상 우수한 성능을 보였으며, 이를 통해 실제 간헐적 암호화 구성에 대한 CNN 기반 탐지 방법을 평가했다.

Conclusion: 이 연구는 향후 탐지 시스템을 위한 강력한 기준을 설정하고, 간헐적 암호화 기반 랜섬웨어 탐지의 효과성을 강조한다.

Abstract: File encrypting ransomware increasingly employs intermittent encryption
techniques, encrypting only parts of files to evade classical detection
methods. These strategies, exemplified by ransomware families like BlackCat,
complicate file structure based detection techniques due to diverse file
formats exhibiting varying traits under partial encryption. This paper provides
a systematic empirical characterization of byte level statistics under
intermittent encryption across common file types, establishing a comprehensive
baseline of how partial encryption impacts data structure. We specialize a
classical KL divergence upper bound on a tailored mixture model of intermittent
encryption, yielding filetype specific detectability ceilings for
histogram-based detectors. Leveraging insights from this analysis, we
empirically evaluate convolutional neural network (CNN) based detection methods
using realistic intermittent encryption configurations derived from leading
ransomware variants. Our findings demonstrate that localized analysis via chunk
level CNNs consistently outperforms global analysis methods, highlighting their
practical effectiveness and establishing a robust baseline for future detection
systems.

</details>


### [2] [MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2510.15186)
*Gurusha Juneja,Jayanth Naga Sai Pasupulati,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.CR

TL;DR: 자율 LLM 에이전트가 협력 환경에서 프라이버시 이해와 유지 및 작업 효율성을 균형 있게 달성하는 것은 핵심적인 도전 과제이다. MAGPIE라는 새로운 벤치를 통해 200개의 고위험 작업을 평가한 결과, 최첨단 에이전트들이 여전히 상당한 프라이버시 유출을 보인다는 것을 발견했다.


<details>
  <summary>Details</summary>
Motivation: 자율 LLM 에이전트가 협력적인 환경에서 강력한 프라이버시 이해 및 유지를 보장하면서도 작업 효율성을 달성하는 것의 중요성에 대한 인식.

Method: 200개의 고위험 작업으로 구성된 MAGPIE라는 새로운 벤치를 도입하여 멀티 에이전트 협력 비대립적 시나리오에서 프라이버시 이해 및 유지를 평가.

Result: 최첨단 에이전트들이 상당한 프라이버시 유출을 보였으며 Gemini 2.5-Pro는 50.7%의 민감한 정보를 유출하고, GPT-5는 35.1%를 유출했다. 여러 에이전트가 합의 도달이나 작업 완료에 어려움을 겪고 조작과 같은 바람직하지 않은 행동을 보였다.

Conclusion: 현재의 LLM 에이전트들은 강력한 프라이버시 이해가 부족하며 복잡한 환경에서 프라이버시를 보존하고 효과적인 협력을 유지하는 데 적절히 정렬되어 있지 않다.

Abstract: A core challenge for autonomous LLM agents in collaborative settings is
balancing robust privacy understanding and preservation alongside task
efficacy. Existing privacy benchmarks only focus on simplistic, single-turn
interactions where private information can be trivially omitted without
affecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent
contextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks
designed to evaluate privacy understanding and preservation in multi-agent
collaborative, non-adversarial scenarios. MAGPIE integrates private information
as essential for task resolution, forcing agents to balance effective
collaboration with strategic information control. Our evaluation reveals that
state-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit
significant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5
up to 35.1% of the sensitive information even when explicitly instructed not
to. Moreover, these agents struggle to achieve consensus or task completion and
often resort to undesirable behaviors such as manipulation and power-seeking
(e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These
findings underscore that current LLM agents lack robust privacy understanding
and are not yet adequately aligned to simultaneously preserve privacy and
maintain effective collaboration in complex environments.

</details>


### [3] [OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs](https://arxiv.org/abs/2510.15188)
*Ahmed Aly,Essam Mansour,Amr Youssef*

Main category: cs.CR

TL;DR: 이 논문은 APT(지속적인 고급 위협) 탐지 및 인간과 유사한 공격 이야기를 재구성하는 시스템인 OCR-APT를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: APT는 시스템 감사 로그에서 종종 탐지를 피하는 은밀한 사이버 공격입니다. 공격의 진행과 영향을 완전히 이해하려면 보안 분석가가 전체 공격에 대한 정확한, 인간과 유사한 이야기를 생성할 수 있는 시스템이 필요합니다.

Method: OCR-APT는 그래프 신경망(GNN)을 사용하여 서브그래프 이상 탐지를 수행하고, 파일 경로나 IP와 같은 취약한 특성이 아니라 노드 주위의 행동 패턴을 학습합니다.

Result: 이 접근 방식은 보다 강력한 이상 탐지로 이어지며, 감지된 서브그래프를 대형 언어 모델(LLM)을 사용하여 다단계 공격 이야기로 재구성합니다. 각 단계는 진행하기 전에 검증되며, 환각을 줄이고 해석 가능한 최종 보고서를 보장합니다.

Conclusion: OCR-APT는 DARPA TC3, OpTC 및 NODLINK 데이터 세트에서 최신 시스템보다 탐지 정확도와 경고 해석 가능성 모두에서 우수한 성능을 보였습니다. 또한 OCR-APT는 공격 이야기를 포괄적으로 캡처하는 인간과 유사한 보고서를 재구성합니다.

Abstract: Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evade
detection in system-level audit logs. Provenance graphs model these logs as
connected entities and events, revealing relationships that are missed by
linear log representations. Existing systems apply anomaly detection to these
graphs but often suffer from high false positive rates and coarse-grained
alerts. Their reliance on node attributes like file paths or IPs leads to
spurious correlations, reducing detection robustness and reliability. To fully
understand an attack's progression and impact, security analysts need systems
that can generate accurate, human-like narratives of the entire attack. To
address these challenges, we introduce OCR-APT, a system for APT detection and
reconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks
(GNNs) for subgraph anomaly detection, learning behavior patterns around nodes
rather than fragile attributes such as file paths or IPs. This approach leads
to a more robust anomaly detection. It then iterates over detected subgraphs
using Large Language Models (LLMs) to reconstruct multi-stage attack stories.
Each stage is validated before proceeding, reducing hallucinations and ensuring
an interpretable final report. Our evaluations on the DARPA TC3, OpTC, and
NODLINK datasets show that OCR-APT outperforms state-of-the-art systems in both
detection accuracy and alert interpretability. Moreover, OCR-APT reconstructs
human-like reports that comprehensively capture the attack story.

</details>


### [4] [Ambusher: Exploring the Security of Distributed SDN Controllers Through Protocol State Fuzzing](https://arxiv.org/abs/2510.15798)
*Jinwoo Kim,Minjae Seo,Eduard Marin,Seungsoo Lee,Jaehyun Nam,Seungwon Shin*

Main category: cs.CR

TL;DR: Ambusher는 분산 SDN 컨트롤러에서 프로토콜의 취약점을 발견하기 위한 테스트 도구로, 효율적인 상태 기반 퍼징을 사용하여 6가지 잠재적인 취약점을 밝혀낸다.


<details>
  <summary>Details</summary>
Motivation: 분산 SDN 컨트롤러의 아키텍처가 새로운 공격 표면을 초래하지만, 이에 대한 연구가 부족하다.

Method: Ambusher는 프로토콜 상태 퍼징을 활용하여 추론된 상태 기계에 기반하여 공격 시나리오를 체계적으로 찾아낸다. 또한, 단일하고 비교적 간단한 상태 기계를 추출하는 새로운 방법론을 제안한다.

Result: Ambusher를 실제 SD-WAN 배포 환경에서 평가한 결과, 널리 사용되는 분산 컨트롤러 플랫폼에서 6개의 잠재적인 취약점을 발견하였다.

Conclusion: Ambusher는 분산 SDN 컨트롤러에서 프로토콜의 취약점을 효과적으로 탐지하는 도구로, 안전성을 향상시키는 데 기여할 수 있다.

Abstract: Distributed SDN (Software-Defined Networking) controllers have rapidly become
an integral element of Wide Area Networks (WAN), particularly within SD-WAN,
providing scalability and fault-tolerance for expansive network
infrastructures. However, the architecture of these controllers introduces new
potential attack surfaces that have thus far received inadequate attention. In
response to these concerns, we introduce Ambusher, a testing tool designed to
discover vulnerabilities within protocols used in distributed SDN controllers.
Ambusher achieves this by leveraging protocol state fuzzing, which
systematically finds attack scenarios based on an inferred state machine. Since
learning states from a cluster is complicated, Ambusher proposes a novel
methodology that extracts a single and relatively simple state machine,
achieving efficient state-based fuzzing. Our evaluation of Ambusher, conducted
on a real SD-WAN deployment spanning two campus networks and one enterprise
network, illustrates its ability to uncover 6 potential vulnerabilities in the
widely used distributed controller platform.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: 이 논문은 실시간 에이전트 추론에 의해 결정되는 완전히 동적 워크플로우를 갖춘 오픈 소스 다중 에이전트 프레임워크인 freephdlabor를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 과학 발견의 자동화는 인공지능 연구에서 중요한 이정표를 나타내지만, 기존 시스템은 rigid한 워크플로우와 부적절한 맥락 관리로 고통받고 있습니다.

Method: 이 프레임워크는 실시간 에이전트 추론에 따라 완전히 동적 워크플로우와 사용자가 도메인 특화 요구에 맞게 에이전트를 수정, 추가 또는 제거할 수 있는 모듈식 아키텍처를 특징으로 합니다.

Result: 이 프레임워크는 자동 맥락 압축, 정보 저하를 방지하는 작업공간 기반 통신, 세션 간 메모리 지속성 및 차단되지 않는 인간 개입 메커니즘을 포함하여 포괄적인 인프라를 제공합니다.

Conclusion: 이 연구는 과학적 분야에서 자동화된 연구의 보다 광범위한 채택을 촉진하여 실무자들이 아이디어 구상부터 실험, 출판 준비가 완료된 원고까지 자율적으로 연구를 수행할 수 있는 대화형 다중 에이전트 시스템을 배포할 수 있도록 목표하고 있습니다.

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>


### [6] [AURA: An Agent Autonomy Risk Assessment Framework](https://arxiv.org/abs/2510.15739)
*Lorenzo Satta Chiris,Ayush Mishra*

Main category: cs.AI

TL;DR: AURA는 자율 에이전트 AI 시스템의 위험을 평가하고 완화하기 위한 통합 프레임워크로, 인공지능의 규모 있는 배포를 지원하는 데 필수적인 역할을 한다.


<details>
  <summary>Details</summary>
Motivation: 자율 에이전트 AI 시스템의 채택이 증가함에 따라, 정렬, 거버넌스 및 위험 관리에서의 지속적인 도전이 대규모 배포를 방해할 위험이 있다.

Method: AURA는 감마 기반 위험 점수 매기기 방법론을 도입하여 위험 평가의 정확성과 계산 효율성 및 실용성을 균형 있게 유지한다.

Result: AURA는 하나 또는 여러 AI 에이전트를 동시에 또는 비동기적으로 운영할 때의 위험을 점수화, 평가 및 완화하는 인터랙티브 프로세스를 제공한다.

Conclusion: AURA는 자율 AI의 책임 있고 투명한 채택을 지원하고, 강력한 위험 탐지 및 완화를 제공하며, 계산 리소스를 균형 있게 유지하여 기업 환경에서 대규모, 관리 가능한 자율 AI의 중요한 촉진제로 자리 잡는다.

Abstract: As autonomous agentic AI systems see increasing adoption across
organisations, persistent challenges in alignment, governance, and risk
management threaten to impede deployment at scale. We present AURA (Agent
aUtonomy Risk Assessment), a unified framework designed to detect, quantify,
and mitigate risks arising from agentic AI. Building on recent research and
practical deployments, AURA introduces a gamma-based risk scoring methodology
that balances risk assessment accuracy with computational efficiency and
practical considerations. AURA provides an interactive process to score,
evaluate and mitigate the risks of running one or multiple AI Agents,
synchronously or asynchronously (autonomously). The framework is engineered for
Human-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)
communication mechanisms, allowing for seamless integration with agentic
systems for autonomous self-assessment, rendering it interoperable with
established protocols (MCP and A2A) and tools. AURA supports a responsible and
transparent adoption of agentic AI and provides robust risk detection and
mitigation while balancing computational resources, positioning it as a
critical enabler for large-scale, governable agentic AI in enterprise
environments.

</details>


### [7] [Procedural Game Level Design with Deep Reinforcement Learning](https://arxiv.org/abs/2510.15120)
*Miraç Buğra Özkan*

Main category: cs.AI

TL;DR: 이 연구는 Deep Reinforcement Learning(DRL)을 활용한 절차적 레벨 디자인을 제안하며, Unity 3D 환경에서 dynamic하고 scalable한 환경을 자동으로 생성할 수 있는 두 가지 에이전트를 포함한다.


<details>
  <summary>Details</summary>
Motivation: 절차적 콘텐츠 생성(PCG)은 게임 개발에서 점점 더 인기 있는 기술로, 개발자들이 수작업을 줄이면서 동적이고 재생 가능하며 확장 가능한 환경을 생성할 수 있게 해준다.

Method: 본 연구에서는 Unity 기반의 3D 환경 내에서 Deep Reinforcement Learning(DRL)을 적용한 새로운 절차적 레벨 디자인 방법을 제안하며, 두 개의 에이전트(hummingbird 에이전트와 floating island 에이전트)를 사용한다.

Result: 실험 결과, 이 접근 방식은 효과적이고 효율적인 에이전트 행동을 생성할 뿐만 아니라, 기계 학습 기반의 자율적인 게임 레벨 디자인을 위한 새로운 기회를 제공한다.

Conclusion: 이 연구는 DRL이 가상 환경 내에서 지능형 에이전트가 콘텐츠를 생성하고 해결할 수 있는 가능성을 보여주며, AI가 창조적인 게임 개발 과정에 기여할 수 있는 한계를 확장한다.

Abstract: Procedural content generation (PCG) has become an increasingly popular
technique in game development, allowing developers to generate dynamic,
replayable, and scalable environments with reduced manual effort. In this
study, a novel method for procedural level design using Deep Reinforcement
Learning (DRL) within a Unity-based 3D environment is proposed. The system
comprises two agents: a hummingbird agent, acting as a solver, and a floating
island agent, responsible for generating and placing collectible objects
(flowers) on the terrain in a realistic and context-aware manner. The
hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm
from the Unity ML-Agents toolkit. It learns to navigate through the terrain
efficiently, locate flowers, and collect them while adapting to the
ever-changing procedural layout of the island. The island agent is also trained
using the Proximal Policy Optimization (PPO) algorithm. It learns to generate
flower layouts based on observed obstacle positions, the hummingbird's initial
state, and performance feedback from previous episodes. The interaction between
these agents leads to emergent behavior and robust generalization across
various environmental configurations. The results demonstrate that the approach
not only produces effective and efficient agent behavior but also opens up new
opportunities for autonomous game level design driven by machine learning. This
work highlights the potential of DRL in enabling intelligent agents to both
generate and solve content in virtual environments, pushing the boundaries of
what AI can contribute to creative game development processes.

</details>


### [8] [Towards Error Centric Intelligence I, Beyond Observational Learning](https://arxiv.org/abs/2510.15128)
*Marcus A. Thomas*

Main category: cs.AI

TL;DR: AGI에 대한 진전은 데이터나 규모의 제한이 아닌 이론적 제한에 있다고 주장합니다.


<details>
  <summary>Details</summary>
Motivation: AGI에 대한 연구의 현재 한계를 해결하고 오차 중심의 변화를 촉진하기 위해 이론적 토대를 마련하려는 의도가 있습니다.

Method: 가정 공간의 변화와 연관된 세 가지 질문을 바탕으로 인과 메커니즘을 제안합니다.

Result: 인과 메커니즘을 통해 도달할 수 없는 오류를 변환하고 수정할 수 있는 시스템 기반을 마련합니다.

Conclusion: 이론적 접근을 통해 AGI의 발전을 이끌어낼 수 있는 기초를 다지며, 오류 발견 및 수정의 트랙터블한 방법을 제공합니다.

Abstract: We argue that progress toward AGI is theory limited rather than data or scale
limited. Building on the critical rationalism of Popper and Deutsch, we
challenge the Platonic Representation Hypothesis. Observationally equivalent
worlds can diverge under interventions, so observational adequacy alone cannot
guarantee interventional competence. We begin by laying foundations,
definitions of knowledge, learning, intelligence, counterfactual competence and
AGI, and then analyze the limits of observational learning that motivate an
error centric shift. We recast the problem as three questions about how
explicit and implicit errors evolve under an agent's actions, which errors are
unreachable within a fixed hypothesis space, and how conjecture and criticism
expand that space. From these questions we propose Causal Mechanics, a
mechanisms first program in which hypothesis space change is a first class
operation and probabilistic structure is used when useful rather than presumed.
We advance structural principles that make error discovery and correction
tractable, including a differential Locality and Autonomy Principle for modular
interventions, a gauge invariant form of Independent Causal Mechanisms for
separability, and the Compositional Autonomy Principle for analogy
preservation, together with actionable diagnostics. The aim is a scaffold for
systems that can convert unreachable errors into reachable ones and correct
them.

</details>


### [9] [HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks](https://arxiv.org/abs/2510.15144)
*Chance Jiajie Li,Zhenze Mo,Yuhan Tang,Ao Qu,Jiayi Wu,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Hang Jiang,Paul Pu Liang,Jinhua Zhao,Luis Alberto Alonso Pastor,Kent Larson*

Main category: cs.AI

TL;DR: 이 논문에서는 개인화된 추론 적응을 위한 새로운 벤치마크인 HugAgent를 소개하며, AI가 인간과 유사한 추론을 할 수 있도록 하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: AI와 인지 과학에서 열린 작업에 대한 인간의 추론을 시뮬레이션하는 것은 오랫동안 목표였습니다. 현재 대형 언어 모델은 인간의 반응을 대규모로 근사해낼 수 있지만, 개인적인 추론 스타일과 신념 궤적을 소거하는 경우가 많습니다.

Method: HugAgent는 평균에서 개인적인 추론 적응을 위한 벤치마크로, 두 가지 트랙을 채택하고 있습니다: 합성 트랙은 규모와 체계적인 스트레스 테스트를 위한 것이고, 인간 트랙은 생태적으로 유효한 '소리 내어' 추론 데이터를 위한 것입니다.

Result: 최신 LLM과의 실험에서는 지속적인 적응 차이가 나타나며, HugAgent가 인간의 사고 개성과 기계적 추론 정렬을 위한 최초의 확장 가능한 벤치마크로 자리 잡게 됩니다.

Conclusion: 우리의 벤치마크와 챗봇은 HugAgent와 TraceYourThinking로 오픈소스로 제공됩니다.

Abstract: Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

</details>


### [10] [WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing](https://arxiv.org/abs/2510.15221)
*Xiao Sun*

Main category: cs.AI

TL;DR: 이 논문은 30.5개월 동안 수집된 733,651개의 얼굴 표정 기록으로 구성된 새로운 데이터셋을 소개하며, 이는 감정 인식 및 관련 연구에 기여한다.


<details>
  <summary>Details</summary>
Motivation: 실제 작업 환경에서의 감정 인식은 자연 환경에서 수집된 대규모의 장기 데이터셋 부족으로 인해 여전히 도전적인 문제로 남아 있다.

Method: 본 논문에서는 38명의 직원으로부터 30.5개월 동안(2021년 11월부터 2024년 5월까지) 수집된 733,651개의 얼굴 표정 기록으로 구성된 새로운 데이터셋을 제시하였다.

Result: 이 데이터셋은 COVID-19 팬데믹 기간을 아우르며, 감정 반응을 포착하고, 91.2%의 감정 분류 정확도와 R2 = 0.84의 예측 정확도를 달성하였다.

Conclusion: 이 데이터셋은 감정 인식, 감정 역학 모델링, 감정 전염, 이직 예측 및 감정 인식 시스템 디자인 연구를 지원하는 가장 크고 긴 데이터셋이다.

Abstract: Automated emotion recognition in real-world workplace settings remains a
challenging problem in affective computing due to the scarcity of large-scale,
longitudinal datasets collected in naturalistic environments. We present a
novel dataset comprising 733,651 facial expression records from 38 employees
collected over 30.5 months (November 2021 to May 2024) in an authentic office
environment. Each record contains seven emotion probabilities (neutral, happy,
sad, surprised, fear, disgusted, angry) derived from deep learning-based facial
expression recognition, along with comprehensive metadata including job roles,
employment outcomes, and personality traits. The dataset uniquely spans the
COVID-19 pandemic period, capturing emotional responses to major societal
events including the Shanghai lockdown and policy changes. We provide 32
extended emotional metrics computed using established affective science
methods, including valence, arousal, volatility, predictability, inertia, and
emotional contagion strength. Technical validation demonstrates high data
quality through successful replication of known psychological patterns (weekend
effect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and
perfect predictive validity for employee turnover (AUC=1.0). Baseline
experiments using Random Forest and LSTM models achieve 91.2% accuracy for
emotion classification and R2 = 0.84 for valence prediction. This is the
largest and longest longitudinal workplace emotion dataset publicly available,
enabling research in emotion recognition, affective dynamics modeling,
emotional contagion, turnover prediction, and emotion-aware system design.

</details>


### [11] [From Checklists to Clusters: A Homeostatic Account of AGI Evaluation](https://arxiv.org/abs/2510.15236)
*Brett Reynolds*

Main category: cs.AI

TL;DR: 이 논문에서는 현대 AGI 평가의 문제점을 지적하고, 일반 지능을 안정성 특성 집합으로 이해해야 하며, AGI 평가 방법을 개선할 수 있는 두 가지 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 AGI 평가에서 발생하는 문제점은 모든 도메인에 동등한 가중치를 부여하고 스냅샷 점수에 의존함으로써 발생합니다.

Method: 일반 지능은 안정적인 특성 집합으로 이해되어야 하며, AGI 평가는 도메인의 인과 중심성에 따라 가중치를 부여하고 세션 간 지속성을 요구해야 합니다. 이를 위해 두 가지 확장된 방법을 제안합니다: 중앙성 우선 점수와 클러스터 안정성 지수.

Result: 제안된 방법들은 다중 도메인의 폭을 유지하면서도 불안정성과 조작성을 줄이는 데 기여합니다.

Conclusion: 검증 가능한 예측과 아키텍처 접근 없이도 실험실이 채택할 수 있는 블랙박스 프로토콜로 마무리합니다.

Abstract: Contemporary AGI evaluations report multidomain capability profiles, yet they
typically assign symmetric weights and rely on snapshot scores. This creates
two problems: (i) equal weighting treats all domains as equally important when
human intelligence research suggests otherwise, and (ii) snapshot testing can't
distinguish durable capabilities from brittle performances that collapse under
delay or stress. I argue that general intelligence -- in humans and potentially
in machines -- is better understood as a homeostatic property cluster: a set of
abilities plus the mechanisms that keep those abilities co-present under
perturbation. On this view, AGI evaluation should weight domains by their
causal centrality (their contribution to cluster stability) and require
evidence of persistence across sessions. I propose two battery-compatible
extensions: a centrality-prior score that imports CHC-derived weights with
transparent sensitivity analysis, and a Cluster Stability Index family that
separates profile persistence, durable learning, and error correction. These
additions preserve multidomain breadth while reducing brittleness and gaming. I
close with testable predictions and black-box protocols labs can adopt without
architectural access.

</details>


### [12] [Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions](https://arxiv.org/abs/2510.15258)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Jun Xu,Fu Zhang,Wenbo Lei,Annie Wang,Peng Gong*

Main category: cs.AI

TL;DR: 대규모 언어 모델과 지식 그래프의 상호작용을 기반으로 한 다차원 데이터 분석 방법을 제안.


<details>
  <summary>Details</summary>
Motivation: 대용량 비정형 데이터를 통해 깊은 통찰력을 추출하는 것이 필요하다.

Method: LLM 에이전트와 KG 간의 상호작용을 활용하여 제품 데이터를 자동으로 추출하고, KG를 실시간으로 구성 및 시각화하는 방법.

Result: 이 방법은 제품 생태계 분석, 관계 마이닝 및 사용자가 주도하는 탐색적 분석에서 중요한 이점을 보인다.

Conclusion: 새로운 아이디어와 도구를 제공하여 다차원 데이터 분석을 지원한다.

Abstract: In the current era of big data, extracting deep insights from massive,
heterogeneous, and complexly associated multi-dimensional data has become a
significant challenge. Large Language Models (LLMs) perform well in natural
language understanding and generation, but still suffer from "hallucination"
issues when processing structured knowledge and are difficult to update in
real-time. Although Knowledge Graphs (KGs) can explicitly store structured
knowledge, their static nature limits dynamic interaction and analytical
capabilities. Therefore, this paper proposes a multi-dimensional data analysis
method based on the interactions between LLM agents and KGs, constructing a
dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to
automatically extract product data from unstructured data, constructs and
visualizes the KG in real-time, and supports users in deep exploration and
analysis of graph nodes through an interactive platform. Experimental results
show that this method has significant advantages in product ecosystem analysis,
relationship mining, and user-driven exploratory analysis, providing new ideas
and tools for multi-dimensional data analysis.

</details>


### [13] [Experience-Driven Exploration for Efficient API-Free AI Agents](https://arxiv.org/abs/2510.15259)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Zizhou Wang,Jiawei Du,Liangli Zhen,Jiancheng Lv*

Main category: cs.AI

TL;DR: KG-Agent는 GUI 기반의 결정을 위한 경험 중심 학습 프레임워크로, 효율성을 개선하고 전략적 사고를 지원한다.


<details>
  <summary>Details</summary>
Motivation: 기존 소프트웨어는 접근 가능한 API가 부족하여 에이전트가 픽셀 기반의 GUI를 통해서만 작동해야 하며, LLM 기반 에이전트는 비효율적인 결정과 시행착오에 의존하여 기술 습득 및 장기 계획에 방해를 받는다.

Method: KG-Agent는 에이전트의 픽셀 수준 상호작용을 지속적인 상태-행동 지식 그래프(SA-KG)로 구조화하는 경험 중심의 학습 프레임워크이다. 이 프레임워크는 기능적으로 유사하지만 시각적으로 다른 GUI 상태를 연결하여 에이전트가 다양한 역사적 전략에서 일반화할 수 있도록 돕는다.

Result: KG-Agent는 복잡하고 개방된 GUI 기반 의사결정 환경(Civilization V와 Slay the Spire)에서 탐험 효율성과 전략적 깊이에 있어 최첨단 방법들에 비해 상당한 개선을 보여준다.

Conclusion: 이 접근방식은 전략적 계획을 순수한 발견과 분리하여 에이전트가 지연된 보상으로 설정 작업의 가치를 효과적으로 평가할 수 있게 한다.

Abstract: Most existing software lacks accessible Application Programming Interfaces
(APIs), requiring agents to operate solely through pixel-based Graphical User
Interfaces (GUIs). In this API-free setting, large language model (LLM)-based
agents face severe efficiency bottlenecks: limited to local visual experiences,
they make myopic decisions and rely on inefficient trial-and-error, hindering
both skill acquisition and long-term planning. To address these challenges, we
propose KG-Agent, an experience-driven learning framework that structures an
agent's raw pixel-level interactions into a persistent State-Action Knowledge
Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking
functionally similar but visually distinct GUI states, forming a rich
neighborhood of experience that enables the agent to generalize from a diverse
set of historical strategies. To support long-horizon reasoning, we design a
hybrid intrinsic reward mechanism based on the graph topology, combining a
state value reward for exploiting known high-value pathways with a novelty
reward that encourages targeted exploration. This approach decouples strategic
planning from pure discovery, allowing the agent to effectively value setup
actions with delayed gratification. We evaluate KG-Agent in two complex,
open-ended GUI-based decision-making environments (Civilization V and Slay the
Spire), demonstrating significant improvements in exploration efficiency and
strategic depth over the state-of-the-art methods.

</details>


### [14] [AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory](https://arxiv.org/abs/2510.15261)
*Jitesh Jain,Shubham Maheshwari,Ning Yu,Wen-mei Hwu,Humphrey Shi*

Main category: cs.AI

TL;DR: AUGUSTUS는 인간 기억의 다중모드 특성에 맞춰 설계된 에이전트 시스템으로, 그래프 구조의 다중모드 맥락 기억을 통해 효율적인 개념 기반 검색을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 인간 기억의 다중모드 특성에서 영감을 받아 외부 메모리 데이터베이스와 에이전트 시스템을 통합할 필요성을 강조.

Method: AUGUSTUS는 입력 이해, 중요한 정보 저장, 메모리에서 관련 맥락 검색, 작업 수행의 4단계로 연결된 루프 구조를 가지고 있으며, 정보를 의미론적 태그로 개념화하여 그래프 구조의 메모리에 저장한다.

Result: AUGUSTUS는 기존의 다중모드 RAG 접근 방식보다 우수하며, ImageNet 분류에서 3.5배 빠르며 MSC 벤치마크에서 MemGPT를 초월합니다.

Conclusion: 다중모드 에이전트 시스템은 인간 기억의 원리를 적용하여 정보 처리를 최적화할 수 있다.

Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.

</details>


### [15] [WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation](https://arxiv.org/abs/2510.15306)
*Kuang-Da Wang,Zhao Wang,Yotaro Shimose,Wei-Yao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: WebGen-V는 지침을 HTML로 변환하는 새로운 벤치마크 및 프레임워크로, 데이터 품질과 평가 세분성을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: LLM을 사용한 코딩 및 다중 모드 이해의 최근 발전을 활용하여 웹 페이지 생성 및 평가의 질을 높이는 필요성을 다룹니다.

Method: WebGen-V는 실세계 웹페이지를 지속적으로 수집할 수 있는 비한정적이고 확장 가능한 에이전틱 크롤링 프레임워크, 메타데이터 및 UI 스크린샷과 결합된 구조화된 데이터 표현 및 섹션 수준의 다중 모드 평가 프로토콜을 포함합니다.

Result: 최신 LLMs와의 실험과 절제 연구를 통해 구조화된 데이터와 섹션별 평가의 효과성을 검증하였습니다.

Conclusion: WebGen-V는 지침에서 HTML 생성을 위한 고세분화 에이전틱 크롤링 및 평가를 가능하게 하는 최초의 연구로, 실제 데이터 수집 및 웹페이지 생성에서 구조화된 다중 모드 평가에 이르는 통합 파이프라인을 제공합니다.

Abstract: Witnessed by the recent advancements on leveraging LLM for coding and
multimodal understanding, we present WebGen-V, a new benchmark and framework
for instruction-to-HTML generation that enhances both data quality and
evaluation granularity. WebGen-V contributes three key innovations: (1) an
unbounded and extensible agentic crawling framework that continuously collects
real-world webpages and can leveraged to augment existing benchmarks; (2) a
structured, section-wise data representation that integrates metadata,
localized UI screenshots, and JSON-formatted text and image assets, explicit
alignment between content, layout, and visual components for detailed
multimodal supervision; and (3) a section-level multimodal evaluation protocol
aligning text, layout, and visuals for high-granularity assessment. Experiments
with state-of-the-art LLMs and ablation studies validate the effectiveness of
our structured data and section-wise evaluation, as well as the contribution of
each component. To the best of our knowledge, WebGen-V is the first work to
enable high-granularity agentic crawling and evaluation for instruction-to-HTML
generation, providing a unified pipeline from real-world data acquisition and
webpage generation to structured multimodal assessment.

</details>


### [16] [VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data](https://arxiv.org/abs/2510.15317)
*Tingqiao Xu,Ziru Zeng,Jiayu Chen*

Main category: cs.AI

TL;DR: VERITAS는 비주얼 인식 모델과 OCR 시스템을 활용하여 감독된 미세 조정(SFT) 데이터의 품질을 향상시키고, 다중 모달 모델의 성능을 높이기 위한 파이프라인을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 다중 모달 모델의 성능은 감독된 미세 조정 데이터의 품질에 의존하지만, 현재 데이터 향상 방법은 종종 시각 인식 부족으로 인한 사실 오류와 환각 문제를 겪는다.

Method: VERITAS는 시각적 인식 모델(RAM++)와 OCR 시스템(PP-OCRv4)을 활용하여 구조화된 시각 우선 요소를 추출하고, 이를 이미지, 질문 및 답변과 결합하여 SFT 데이터의 품질을 향상시킨다.

Result: VERITAS로 처리된 데이터로 미세 조정된 모델이 원시 데이터를 사용하는 모델보다 일관되게 우수한 성능을 보였으며, 특히 텍스트 중심 및 세밀한 추론 작업에서 두드러진 성과를 보인다.

Conclusion: 우리의 비평 모델은 최첨단 다중 모달 모델에 필적하는 향상된 능력을 보이면서도 효율성이 크게 향상되었다. 우리는 연구의 발전을 위해 파이프라인, 데이터셋 및 모델 체크포인트를 공개한다.

Abstract: The quality of supervised fine-tuning (SFT) data is crucial for the
performance of large multimodal models (LMMs), yet current data enhancement
methods often suffer from factual errors and hallucinations due to inadequate
visual perception. To address this challenge, we propose VERITAS, a pipeline
that systematically integrates vision priors and multiple state-of-the-art LMMs
with statistical methods to enhance SFT data quality. VERITAS leverages visual
recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured
vision priors, which are combined with images, questions, and answers. Three
LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers,
providing critique rationales and scores that are statistically fused into a
high-confidence consensus score serving as ground truth. Using this consensus,
we train a lightweight critic model via Group Relative Policy Optimization
(GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the
original answers based on the critiques, generating new candidate answers; we
select the highest-scoring one as the final refined answer. Experiments across
six multimodal benchmarks demonstrate that models fine-tuned with data
processed by VERITAS consistently outperform those using raw data, particularly
in text-rich and fine-grained reasoning tasks. Our critic model exhibits
enhanced capability comparable to state-of-the-art LMMs while being
significantly more efficient. We release our pipeline, datasets, and model
checkpoints to advance research in multimodal data optimization.

</details>


### [17] [Corrigibility Transformation: Constructing Goals That Accept Updates](https://arxiv.org/abs/2510.15395)
*Rubi Hudson*

Main category: cs.AI

TL;DR: AI의 훈련 과정에서 원하는 목표를 성공적으로 전달하기 위해서는 AI가 훈련을 저항하지 않도록 하는 것이 중요하다. 그러나 부분적으로 학습된 목표는 AI가 목표 업데이트를 피하도록 유도하는 경향이 있다. 본 논문에서는 교정 가능한 목표의 정의와 교정 가능한 목표로 변환하는 방법을 제시하며, 두 개의 실험을 통해 이 목표들이 효과적으로 학습됨을 보여준다.


<details>
  <summary>Details</summary>
Motivation: AI의 훈련 과정에서 AI가 목표를 저항하지 않도록 하는 것이 중요한데, 목표 업데이트를 피하도록 유도하는 부분 학습된 목표의 문제를 다룬다.

Method: 교정 가능성을 formal하게 정의하고, 성능을 희생하지 않고 교정 가능한 목표의 변환 방법을 소개한다. 이는 업데이트를 방지하는 조건부 보상의 예측을 유도하여 이루어진다.

Result: 두 개의 gridworld 실험을 통해 교정 가능한 목표가 효과적으로 학습되며 원하는 행동으로 이어진다는 것을 보여준다.

Conclusion: 교정 가능성은 훈련의 수렴과 인간 선호의 변화에 대한 오류 수정을 가능하게 하며, 이는 중요한 안전 속성이다.

Abstract: For an AI's training process to successfully impart a desired goal, it is
important that the AI does not attempt to resist the training. However,
partially learned goals will often incentivize an AI to avoid further goal
updates, as most goals are better achieved by an AI continuing to pursue them.
We say that a goal is corrigible if it does not incentivize taking actions that
avoid proper goal updates or shutdown. In addition to convergence in training,
corrigibility also allows for correcting mistakes and changes in human
preferences, which makes it a crucial safety property. Despite this, the
existing literature does not include specifications for goals that are both
corrigible and competitive with non-corrigible alternatives. We provide a
formal definition for corrigibility, then introduce a transformation that
constructs a corrigible version of any goal that can be made corrigible,
without sacrificing performance. This is done by myopically eliciting
predictions of reward conditional on costlessly preventing updates, which then
also determine the reward when updates are accepted. The transformation can be
modified to recursively extend corrigibility to any new agents created by
corrigible agents, and to prevent agents from deliberately modifying their
goals. Two gridworld experiments demonstrate that these corrigible goals can be
learned effectively, and that they lead to the desired behavior.

</details>


### [18] [MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games](https://arxiv.org/abs/2510.15414)
*Huining Yuan,Zelai Xu,Zheyue Tan,Xiangmin Yi,Mo Guang,Kaiwen Long,Haojia Hui,Boxun Li,Xinlei Chen,Bo Zhao,Xiao-Ping Zhang,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: MARS는 다중 에이전트 시스템의 협력 및 경쟁에서 LLM의 추론을 강화하는 종단 간 강화 학습 프레임워크로, Qwen3-4B에서 훈련된 에이전트가 전략적 능력을 향상시키고 다중 에이전트 훈련의 성능을 개선하는 데 기여한다.


<details>
  <summary>Details</summary>
Motivation: 더욱 진보된 지능을 위한 다중 에이전트 시스템에서 LLM이 효과적으로 협력하고 경쟁하도록 개발하는 것이 중요하다.

Method: MARS는 협력 및 경쟁 게임에서 LLM의 다중 에이전트 추론을 유도하기 위한 종단 간 RL 프레임워크로, 턴 수준의 이점 추정기와 에이전트 특정 이점 정규화를 포함한다.

Result: MARS에서 훈련된 에이전트는 다중 에이전트 시스템에서 성능을 10.0% 및 12.5% 개선하였다.

Conclusion: 이 결과는 전략 게임에서 자기 놀이를 통한 종단 간 RL 훈련이 LLM에서 일반화 가능한 다중 에이전트 추론 능력을 개발하는 강력한 방법임을 확립한다.

Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively
within multi-agent systems is a critical step towards more advanced
intelligence. While reinforcement learning (RL) has proven effective for
enhancing reasoning in single-agent tasks, its extension to multi-turn,
multi-agent scenarios remains underexplored due to the challenges of
long-horizon credit assignment and agent-specific advantage estimation. To
address these challenges, we introduce MARS, an end-to-end RL framework that
incentivizes Multi-Agent Reasoning of LLMs through Self-play in both
cooperative and competitive games. MARS features a turn-level advantage
estimator that aligns learning signals with each interaction for credit
assignment, and an agent-specific advantage normalization to stabilize
multi-agent training. By learning with self-play across cooperative and
competitive games, the MARS agent trained from Qwen3-4B develops strong
strategic abilities that generalize to held-out games with up to 28.7%
performance improvements. More importantly, the capability acquired through
self-play generalizes beyond games, yielding consistent performance gains of
multi-agent systems in reasoning benchmarks. When integrated into leading
multi-agent systems, our MARS agent achieves significant performance gains of
10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL
training with self-play in strategic games as a powerful approach for
developing generalizable multi-agent reasoning capabilities in LLMs. Our code
and models are publicly available at https://github.com/thu-nics/MARS.

</details>


### [19] [Adaptive Minds: Empowering Agents with LoRA-as-Tools](https://arxiv.org/abs/2510.15416)
*Pavan C Shekar,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: Adaptive Minds는 LoRA 어댑터를 도메인 특화 도구로 사용하여 LLM이 쿼리를 분석하고 최적의 LoRA 도구를 동적으로 선택하도록 하는 에이전트 시스템입니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 단일 모델 또는 엄격한 규칙 기반 라우팅에 의존하지 않고, 에이전트가 필요에 따라 다양한 도메인 전문가 사이를 원활하게 전환할 수 있도록 하는 것이 목표입니다.

Method: 기본 LLM이 쿼리를 분석하고 관련 LoRA 도구를 동적으로 선택하며, 이를 위해 LangGraph를 사용한 워크플로우 관리 시스템을 구축했습니다.

Result: Adaptive Minds는 정확하고 전문화된 응답을 제공하면서 대화 능력을 유지하는 성과를 달성했습니다.

Conclusion: API 및 웹 인터페이스를 모두 지원하고 완전 오픈 소스인 이 시스템은 도메인 적응형 AI 지원을 위한 확장 가능하고 확장성이 있는 기반을 제공합니다.

Abstract: We present Adaptive Minds, an agentic system that treats LoRA adapters as
domain-specific tools. Instead of relying on a single fine-tuned model or rigid
rule-based routing, our approach empowers the base LLM itself to act as a
semantic router analyzing each query and dynamically selecting the most
relevant LoRA tool. This enables the agent to seamlessly switch between
different domain experts on demand. By combining the flexibility of multi-agent
orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive
Minds delivers accurate, specialized responses while preserving conversational
ability. The system is built with LangGraph for workflow management, supports
both API and web interfaces, and is fully open source, providing a scalable and
extensible foundation for domain-adaptive AI assistance.

</details>


### [20] [JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament](https://arxiv.org/abs/2510.15560)
*Jiayuan Bai,Xuan-guang Pan,Chongyang Tao,Shuai Ma*

Main category: cs.AI

TL;DR: JudgeSQL 프레임워크는 텍스트-투-SQL의 후보 선택 과정을 구조적 추론과 가중 합의 토너먼트 메커니즘을 통해 재정의하여 SQL 쿼리 선택의 정확성과 해석 가능성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 텍스트-투-SQL 작업은 자연어 이해와 구조화된 데이터 접근을 연결하는 중요한 과제지만, 의미적 모호성과 복잡한 조합적 추론으로 인해 여전히 도전적이다.

Method: JudgeSQL은 강화 학습과 검증 가능한 보상을 통한 추론 추적을 정제하는 SQL 판별 모델을 개발하고, 이를 통해 명시적 추론 선호도를 통합한 가중 합의 토너먼트를 구성하여 신뢰성과 효율성을 높인다.

Result: BIRD 벤치마크에서의 방대한 실험 결과, JudgeSQL은 우수한 SQL 판단 능력과 뛰어난 크로스 스케일 일반화 및 생성기 용량에 대한 강인성을 보인다.

Conclusion: JudgeSQL은 텍스트-투-SQL의 후보 선택에서 구조적 추론과 가중 합의를 통해 정확하고 해석 가능한 판단을 가능하게 한다.

Abstract: Text-to-SQL is a pivotal task that bridges natural language understanding and
structured data access, yet it remains fundamentally challenging due to
semantic ambiguity and complex compositional reasoning. While large language
models (LLMs) have greatly advanced SQL generation though prompting, supervised
finetuning and reinforced tuning, the shift toward test-time scaling exposes a
new bottleneck: selecting the correct query from a diverse candidate pool.
Existing selection approaches, such as self-consistency or best-of-$N$
decoding, provide only shallow signals, making them prone to inconsistent
scoring, fragile reasoning chains, and a failure to capture fine-grained
semantic distinctions between closely related SQL candidates. To this end, we
introduce JudgeSQL, a principled framework that redefines SQL candidate
selection through structured reasoning and weighted consensus tournament
mechanism. JudgeSQL develops a reasoning-based SQL judge model that distills
reasoning traces with reinforcement learning guided by verifiable rewards,
enabling accurate and interpretable judgments. Building on this, a weighted
consensus tournament integrates explicit reasoning preferences with implicit
generator confidence, yielding selections that are both more reliable and more
efficient. Extensive experiments on the BIRD benchmark demonstrate that
JudgeSQL exhibits superior SQL judgment capabilities and good cross-scale
generalization and robustness to generator capacity.

</details>


### [21] [Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment](https://arxiv.org/abs/2510.15591)
*Lavanya Umapathy,Patricia M Johnson,Tarun Dutt,Angela Tong,Madhur Nayan,Hersh Chandarana,Daniel K Sodickson*

Main category: cs.AI

TL;DR: 이 논문은 이전 방문 데이터를 통합하여 환자의 건강 모니터링을 개선하기 위한 기계 학습 프레임워크를 개발하였으며, 그 결과 전립선암 위험 예측에서 허위 긍정률을 감소시키는 데 성공하였다는 내용을 담고 있다.


<details>
  <summary>Details</summary>
Motivation: 의학에서 시간적 맥락은 환자의 건강 변화 평가에 중요하다.

Method: 최신 환자 방문의 의료 데이터를 사용하여 질병 초기 위험을 추정하고, 이전에 수집한 이미징 및/또는 임상 바이오마커 정보를 이용하여 이 평가를 개선하는 기계 학습 프레임워크를 개발하였다.

Result: 이 프레임워크를 통해 전립선암 위험 예측에서 51%에서 33%로, 이후 임상 데이터 추가 시 24%로 허위 긍정률이 감소하였다.

Conclusion: 남성 인구에서 전립선암 위험 예측의 특정성을 향상시킬 수 있으며, 이는 초기 발견과 건강 결과 개선으로 이어질 수 있다.

Abstract: Temporal context in medicine is valuable in assessing key changes in patient
health over time. We developed a machine learning framework to integrate
diverse context from prior visits to improve health monitoring, especially when
prior visits are limited and their frequency is variable. Our model first
estimates initial risk of disease using medical data from the most recent
patient visit, then refines this assessment using information digested from
previously collected imaging and/or clinical biomarkers. We applied our
framework to prostate cancer (PCa) risk prediction using data from a large
population (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931
blood tests) collected over nearly a decade. For predictions of the risk of
clinically significant PCa at the time of the visit, integrating prior context
directly converted false positives to true negatives, increasing overall
specificity while preserving high sensitivity. False positive rates were
reduced progressively from 51% to 33% when integrating information from up to
three prior imaging examinations, as compared to using data from a single
visit, and were further reduced to 24% when also including additional context
from prior clinical data. For predicting the risk of PCa within five years of
the visit, incorporating prior context reduced false positive rates still
further (64% to 9%). Our findings show that information collected over time
provides relevant context to enhance the specificity of medical risk
prediction. For a wide range of progressive conditions, sufficient reduction of
false positive rates using context could offer a pathway to expand longitudinal
health monitoring programs to large populations with comparatively low baseline
risk of disease, leading to earlier detection and improved health outcomes.

</details>


### [22] [Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL](https://arxiv.org/abs/2510.15772)
*Richard M. Bailey*

Main category: cs.AI

TL;DR: 이 논문은 복잡한 'wicked problems'를 해결하기 위해 대화 기반의 인공지능 시스템인 Dialectica를 제안한다. 이를 통해 에이전트가 깊은 대화를 통해 지식을 확장하고 개선할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 복잡하고 다차원적인 문제를 해결하기 위해 인공지능이 인간과 협력할 수 있는 방안을 찾는 것이 필요하다.

Method: Dialectica라는 프레임워크를 통해 에이전트들이 정의된 주제에 대해 구조화된 대화를 나누고, 메모리와 자기 반성 및 정책 제한에 기반한 문맥 편집을 통해 상호작용한다.

Result: 대화 중 반영 기반의 문맥 편집을 활성화된 에이전트가 기준점 대비 성능이 향상되었음을 입증했다.

Conclusion: 대화 기반의 문맥 진화를 통해 비검증 가능 영역에서의 전문 지식 확대를 실현할 수 있는 실제적인 방법이 될 수 있다.

Abstract: So-called `wicked problems', those involving complex multi-dimensional
settings, non-verifiable outcomes, heterogeneous impacts and a lack of single
objectively correct answers, have plagued humans throughout history. Modern
examples include decisions over justice frameworks, solving environmental
pollution, planning for pandemic resilience and food security. The use of
state-of-the-art artificial intelligence systems (notably Large Language
Model-based agents) collaborating with humans on solving such problems is being
actively explored. While the abilities of LLMs can be improved by, for example,
fine-tuning, hand-crafted system prompts and scaffolding with external tools,
LLMs lack endogenous mechanisms to develop expertise through experience in such
settings. This work address this gap with Dialectica, a framework where agents
engage in structured dialogue on defined topics, augmented by memory,
self-reflection, and policy-constrained context editing. Formally, discussion
is viewed as an implicit meta-reinforcement learning process. The
`dialogue-trained' agents are evaluated post-hoc using judged pairwise
comparisons of elicited responses. Across two model architectures (locally run
Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based
context editing during discussion produces agents which dominate their baseline
counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and
AlphaRank mass. The predicted signatures of learning are observed qualitatively
in statement and reflection logs, where reflections identify weaknesses and
reliably shape subsequent statements. Agreement between quantitative and
qualitative evidence supports dialogue-driven context evolution as a practical
path to targeted expertise amplification in open non-verifiable domains.

</details>


### [23] [PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold](https://arxiv.org/abs/2510.15862)
*Yi Wan,Jiuqi Wang,Liam Li,Jinsong Liu,Ruihao Zhu,Zheqing Zhu*

Main category: cs.AI

TL;DR: PokeeResearch-7B라는 7B 파라미터의 심층 연구 에이전트가 강화 학습 프레임워크를 기반으로 구축되었으며, 도구 사용의 신뢰성과 확장성을 개선하여 높은 성능을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 도구 증강 대규모 언어 모델이 얕은 검색, 약한 정렬 지표, 취약한 도구 사용 행동에 의해 제한되고 있기 때문입니다.

Method: PokeeResearch-7B는 LLM 기반의 보상 신호를 사용하여 정책을 최적화하는 AI 피드백으로부터의 강화 학습(RLAIF) 프레임워크에 의해 교육됩니다.

Result: PokeeResearch-7B는 10개의 인기 있는 심층 연구 벤치마크 중에서 7B 스케일의 심층 연구 에이전트들 중 최고의 성능을 기록했습니다.

Conclusion: 신중한 강화 학습 및 추론 설계가 효율적이고 탄력적이며 연구 수준의 AI 에이전트를 생성할 수 있음을 보여줍니다.

Abstract: Tool-augmented large language models (LLMs) are emerging as deep research
agents, systems that decompose complex queries, retrieve external evidence, and
synthesize grounded responses. Yet current agents remain limited by shallow
retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce
PokeeResearch-7B, a 7B-parameter deep research agent built under a unified
reinforcement learning framework for robustness, alignment, and scalability.
PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from
AI Feedback (RLAIF) framework to optimize policies using LLM-based reward
signals that capture factual accuracy, citation faithfulness, and instruction
adherence. A chain-of-thought-driven multi-call reasoning scaffold further
enhances robustness through self-verification and adaptive recovery from tool
failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves
state-of-the-art performance among 7B-scale deep research agents. This
highlights that careful reinforcement learning and reasoning design can produce
efficient, resilient, and research-grade AI agents. The model and inference
code is open-sourced under MIT license at
https://github.com/Pokee-AI/PokeeResearchOSS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 이 논문에서는 다중 에이전트 강화 학습(DMARL)에서 고수준의 상징적 지식을 제공함으로써 에이전트들이 공동 목표를 달성하는 데 도움이 되는 방법을 연구한다.


<details>
  <summary>Details</summary>
Motivation: 다수의 에이전트가 협력해야 하는 실제 문제에서 DMARL의 독특한 도전 과제를 해결하기 위해

Method: 고수준의 상징적 지식을 에이전트에게 제공하고, 팀 작업과의 로컬 정책 호환성을 확인하는 공식 도구를 확장함으로써 이론적 보장을 갖춘 분산 학습 방법을 제안한다.

Result: 상징적 지식이 DMARL의 학습 과정을 가속화하는 데 중요한 역할을 하는 것을 경험적으로 입증.

Conclusion: 이 연구는 에이전트들이 공동 작업을 성과에 기여하도록 돕기 위해 상징적 지식을 활용하는 방법을 제안한다.

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [25] [ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm](https://arxiv.org/abs/2510.15006)
*Rijul Tandon,Peter Vamplew,Cameron Foale*

Main category: cs.LG

TL;DR: ES-C51은 C51 알고리즘을 수정하여 안정성과 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 C51이 유사한 보상을 가지지만 서로 다른 분포를 가진 행동들에서 불안정성을 초래하는 문제를 해결하기 위해.

Method: C51 알고리즘의 탐색 방식을 e-탐욕에서 소프트맥스 방식으로 변경하고, 그리디 Q-학습 업데이트를 기대 Sarsa 업데이트로 대체하여 모든 가능 행동의 정보를 결합합니다.

Result: ES-C51은 다양한 환경에서 QL-C51보다 우수한 성능을 보였습니다.

Conclusion: ES-C51은 보상 분포의 안정성을 개선하고 더 높은 성능을 낸다는 것을 보여줍니다.

Abstract: In most value-based reinforcement learning (RL) algorithms, the agent
estimates only the expected reward for each action and selects the action with
the highest reward. In contrast, Distributional Reinforcement Learning (DRL)
estimates the entire probability distribution of possible rewards, providing
richer information about uncertainty and variability. C51 is a popular DRL
algorithm for discrete action spaces. It uses a Q-learning approach, where the
distribution is learned using a greedy Bellman update. However, this can cause
problems if multiple actions at a state have similar expected reward but with
different distributions, as the algorithm may not learn a stable distribution.
This study presents a modified version of C51 (ES-C51) that replaces the greedy
Q-learning update with an Expected Sarsa update, which uses a softmax
calculation to combine information from all possible actions at a state rather
than relying on a single best action. This reduces instability when actions
have similar expected rewards and allows the agent to learn higher-performing
policies. This approach is evaluated on classic control environments from Gym,
and Atari-10 games. For a fair comparison, we modify the standard C51's
exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-
Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51
across many environments.

</details>


### [26] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: 대규모 언어 모델(LLM)이 복잡하고 동적인 환경에서의 의사 결정 능력을 향상시키기 위해 내부 세계 모델을 도입하는 방법을 제안하며, 이를 통해 RL 기반 에이전트의 성능을 크게 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: LLM이 OOD 시나리오에서 자주 어려움을 겪음에 따라, 환경 동력학과의 정렬을 개선하려는 필요성이 생겼다.

Method: 내부 세계 모델을 사용하여 상태 표현과 전이 모델링으로 분해하고, 자기 게임을 통한 감독 세밀 조정 단계로 정책을 초기화한 후 환경과의 상호작용을 통해 세계 모델을 학습한다.

Result: 이 간단한 초기화 방법은 온라인 세계 모델링 기준을 능가하고 RL 기반 에이전트 훈련 성능을 크게 향상시킨다.

Conclusion: 다양한 환경에서 우리의 접근 방식이 성능을 유의미하게 개선함을 보여주며, 실제 예시는 Sokoban의 성공률을 25.6%에서 59.8%로, FrozenLake 점수를 22.1%에서 70.9%로 상승시킨다.

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [27] [Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions](https://arxiv.org/abs/2510.15056)
*Ziqing Lu,Babak Hassibi,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 적응력을 넘어 모델을 적극적으로 수정하는 강화학습 에이전트를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 에이전트가 수동적인 적응에 제한되지 않고 모델을 수정하는 행동을 수행할 수 있는 설정을 고려한다.

Method: 다층 구성 가능 시간 변동 마르코프 결정 과정(MCTVMDP)을 소개하며, 하위 MDP는 상위 모델 수정 행동을 통해 구성할 수 있는 비정상적인 전이 함수를 가진다.

Result: 상위 MDP에서 구성 정책을 최적화하고 하위 MDP에서 원시 행동 정책을 최적화하여 기대하는 장기 보상을 향상시킬 수 있다.

Conclusion: 이 접근 방식은 에이전트의 보상을 증가시키는 잠재력이 있다.

Abstract: Reinforcement learning usually assumes a given or sometimes even fixed
environment in which an agent seeks an optimal policy to maximize its long-term
discounted reward. In contrast, we consider agents that are not limited to
passive adaptations: they instead have model-changing actions that actively
modify the RL model of world dynamics itself. Reconfiguring the underlying
transition processes can potentially increase the agents' rewards. Motivated by
this setting, we introduce the multi-layer configurable time-varying Markov
decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a
non-stationary transition function that is configurable through upper-level
model-changing actions. The agent's objective consists of two parts: Optimize
the configuration policies in the upper-level MDP and optimize the primitive
action policies in the lower-level MDP to jointly improve its expected
long-term reward.

</details>


### [28] [Operator Flow Matching for Timeseries Forecasting](https://arxiv.org/abs/2510.15101)
*Yolanne Yi Ran Lee,Kyriakos Flouris*

Main category: cs.LG

TL;DR: 이 논문은 고차원 편미분방정식(PDE) 동역학의 예측을 위한 새로운 접근법인 TempO를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 고차원 PDE 동역학의 예측은 생성 모델링에서 중요한 도전과제입니다.

Method: TempO는 희소 조건과 채널 접기를 활용하여 3D 시공간 필드를 효율적으로 처리하는 잠재 흐름 매칭 모델입니다.

Result: TempO는 세 가지 벤치마크 PDE 데이터셋에서 최첨단 기준을 초월하는 성능을 보였습니다.

Conclusion: TempO는 현재의 주의 기반 또는 합성곱 회귀 모델에 비해 효율성과 메모리 사용량이 뛰어난 설계를 제공합니다.

Abstract: Forecasting high-dimensional, PDE-governed dynamics remains a core challenge
for generative modeling. Existing autoregressive and diffusion-based approaches
often suffer cumulative errors and discretisation artifacts that limit long,
physically consistent forecasts. Flow matching offers a natural alternative,
enabling efficient, deterministic sampling. We prove an upper bound on FNO
approximation error and propose TempO, a latent flow matching model leveraging
sparse conditioning with channel folding to efficiently process 3D
spatiotemporal fields using time-conditioned Fourier layers to capture
multi-scale modes with high fidelity. TempO outperforms state-of-the-art
baselines across three benchmark PDE datasets, and spectral analysis further
demonstrates superior recovery of multi-scale dynamics, while efficiency
studies highlight its parameter- and memory-light design compared to
attention-based or convolutional regressors.

</details>


### [29] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: 이 논문은 연속 시간에서의 정책 전이 방법을 사용하여 RL의 효율성을 향상시키는 새로운 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습(RL)에서 복잡한 작업을 학습하는 것은 비효율적이므로, 정책 전이를 통해 효율성을 높이는 방향을 탐구합니다.

Method: 정책 전이를 통해 관련 소스 작업의 정책을 사용하여 목표 RL 작업의 학습을 초기화합니다.

Result: 정책 전이에 대한 이론적 증명을 제공하며, 새로운 정책 학습 알고리즘을 소개합니다.

Conclusion: 정책 전이가 연속 시간 RL에서의 연구 갭을 메우고, 알고리즘의 전이 학습의 이점을 보여줍니다.

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [30] [An Advanced Two-Stage Model with High Sensitivity and Generalizability for Prediction of Hip Fracture Risk Using Multiple Datasets](https://arxiv.org/abs/2510.15179)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 이 연구는 고관절 골절 위험 예측을 개선하기 위해 임상 및 이미징 정보를 통합한 두 단계 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 고관절 골절은 노인에서 장애, 사망 및 의료 부담의 주요 원인으로, 조기 위험 평가의 필요성을 강조합니다.

Method: 이 연구는 Osteoporotic Fractures in Men Study (MrOS), Study of Osteoporotic Fractures (SOF) 및 UK Biobank의 데이터를 활용하여 클리닉, 인구통계학적 및 기능적 변수를 사용해 1단계에서 기본 위험을 추정하고, 2단계에서 DXA에서 파생된 특징을 포함하여 개선했습니다.

Result: 모델은 내부 및 외부 테스트를 통해 엄격하게 검증되었으며, 일관된 성능과 다양한 집단에 대한 적응성을 보여주었습니다.

Conclusion: T-score 및 FRAX와 비교했을 때, 이 두 단계 모델은 더 높은 민감도와 적은 누락 사례를 보여줍니다. 이는 조기 고관절 골절 위험 평가를 위한 비용 효과적이고 개인화된 접근 방식을 제공합니다.

Abstract: Hip fractures are a major cause of disability, mortality, and healthcare
burden in older adults, underscoring the need for early risk assessment.
However, commonly used tools such as the DXA T-score and FRAX often lack
sensitivity and miss individuals at high risk, particularly those without prior
fractures or with osteopenia. To address this limitation, we propose a
sequential two-stage model that integrates clinical and imaging information to
improve prediction accuracy. Using data from the Osteoporotic Fractures in Men
Study (MrOS), the Study of Osteoporotic Fractures (SOF), and the UK Biobank,
Stage 1 (Screening) employs clinical, demographic, and functional variables to
estimate baseline risk, while Stage 2 (Imaging) incorporates DXA-derived
features for refinement. The model was rigorously validated through internal
and external testing, showing consistent performance and adaptability across
cohorts. Compared to T-score and FRAX, the two-stage framework achieved higher
sensitivity and reduced missed cases, offering a cost-effective and
personalized approach for early hip fracture risk assessment.
  Keywords: Hip Fracture, Two-Stage Model, Risk Prediction, Sensitivity, DXA,
FRAX

</details>


### [31] [Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent](https://arxiv.org/abs/2510.15222)
*Gabriel Nixon Raj*

Main category: cs.LG

TL;DR: 본 논문은 분포 Drift 하에서의 순차적인 의사결정 과정을 연구하며, 엔트로피 정규화된 신뢰 감소 기법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 순차적 의사결정 과정에서 분포가 변화하는 상황을 이해하고, 이를 보다 효과적으로 처리하기 위함입니다.

Method: 신뢰 감소를 통해 스트레스에 민감한 지수 편향을 믿음 업데이트와 미러-하강 결정에 주입합니다.

Result: 신뢰 감소 기법은 O(1)의 전환 손실을 달성하는 반면, 스트레스 없는 업데이트는 Ω(1)의 꼬리 손실을 초래합니다.

Conclusion: 이 연구는 동적 손실 분석, 분포적으로 강인한 목표, KL 정규화 제어를 통합한 새로운 프레임워크를 제시합니다.

Abstract: We study sequential decision-making under distribution drift. We propose
entropy-regularized trust-decay, which injects stress-aware exponential tilting
into both belief updates and mirror-descent decisions. On the simplex, a
Fenchel-dual equivalence shows that belief tilt and decision tilt coincide. We
formalize robustness via fragility (worst-case excess risk in a KL ball),
belief bandwidth (radius sustaining a target excess), and a decision-space
Fragility Index (drift tolerated at $O(\sqrt{T})$ regret). We prove
high-probability sensitivity bounds and establish dynamic-regret guarantees of
$\tilde{O}(\sqrt{T})$ under KL-drift path length $S_T = \sum_{t\ge2}\sqrt{{\rm
KL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch
regret, while stress-free updates incur $\Omega(1)$ tails. A parameter-free
hedge adapts the tilt to unknown drift, whereas persistent over-tilting yields
an $\Omega(\lambda^2 T)$ stationary penalty. We further obtain
calibrated-stress bounds and extensions to second-order updates, bandit
feedback, outliers, stress variation, distributed optimization, and plug-in
KL-drift estimation. The framework unifies dynamic-regret analysis,
distributionally robust objectives, and KL-regularized control within a single
stress-adaptive update.

</details>


### [32] [Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition](https://arxiv.org/abs/2510.15280)
*Fan Liu,Jindong Han,Tengfei Lyu,Weijia Zhang,Zhe-Rui Yang,Lu Dai,Cancheng Liu,Hao Liu*

Main category: cs.LG

TL;DR: 기초 모델(FMs)은 과학 연구의 방식을 재정립하고 있으며, 새로운 과학 패러다임으로의 전환을 촉진하고 있다.


<details>
  <summary>Details</summary>
Motivation: 기초 모델이 기존의 과학 방법론을 단순히 강화하는 것이 아니라 과학이 수행되는 방식을 재정의하고 있다는 질문을 던진다.

Method: 세 가지 단계의 프레임워크(메타-과학적 통합, 하이브리드 인간-AI 공동 창조, 자율적 과학적 발견)를 소개하여 이 진화를 설명한다.

Result: 기초 모델의 현재 응용 및 emerging capabilities를 검토하고, 과학적 발견의 위험 요소와 미래 방향을 식별한다.

Conclusion: 기초 모델의 변혁적 역할을 이해하고 과학적 발견의 미래에 대한 성찰을 촉진하는 데 목적이 있다.

Abstract: Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the
landscape of scientific research. Beyond accelerating tasks such as hypothesis
generation, experimental design, and result interpretation, they prompt a more
fundamental question: Are FMs merely enhancing existing scientific
methodologies, or are they redefining the way science is conducted? In this
paper, we argue that FMs are catalyzing a transition toward a new scientific
paradigm. We introduce a three-stage framework to describe this evolution: (1)
Meta-Scientific Integration, where FMs enhance workflows within traditional
paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active
collaborators in problem formulation, reasoning, and discovery; and (3)
Autonomous Scientific Discovery, where FMs operate as independent agents
capable of generating new scientific knowledge with minimal human intervention.
Through this lens, we review current applications and emerging capabilities of
FMs across existing scientific paradigms. We further identify risks and future
directions for FM-enabled scientific discovery. This position paper aims to
support the scientific community in understanding the transformative role of
FMs and to foster reflection on the future of scientific discovery. Our project
is available at
https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.

</details>


### [33] [Sequence Modeling with Spectral Mean Flows](https://arxiv.org/abs/2510.15366)
*Jinwoo Kim,Max Beier,Petar Bevanda,Nayun Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: 본 논문에서는 숨겨진 마르코프 모델(HMM)의 연산자 이론적 관점을 바탕으로 한 새로운 시퀀스 모델링 접근법을 제안합니다. 이 방법은 최대 평균 불일치(MMD) 기울기 흐름을 통해 시퀀스의 전체 분포를 텐서로 집합하여 시뮬레이션 없는 학습과 빠른 샘플링을 가능하게 합니다.


<details>
  <summary>Details</summary>
Motivation: 신경망을 이용한 시퀀스 모델링에서 비선형 및 확률적 상태 동역학을 어떻게 표현하고 학습할 것인가가 주요 질문입니다.

Method: 우리는 숨겨진 마르코프 모델(HMM)에 대한 연산자 이론적 관점에 기반하여 새로운 시퀀스 모델링 접근법을 제안합니다. 전체 시퀀스 분포를 텐서로 내재화하고, 생성 프로세스를 시퀀스 공간에서 최대 평균 불일치(MMD) 기울기 흐름으로 정의합니다.

Result: 우리는 대규모 텐서 및 느린 샘플링 수렴의 문제를 극복하기 위해 스펙트럴 평균 흐름을 도입합니다. 이 알고리즘은 선형 연산자의 스펙트럴 분해를 활용하여 시퀀스 평균 임베딩의 확장 가능한 텐서 네트워크 분해를 도출하며, 시간 종속적인 힐베르트 공간으로 MMD 기울기 흐름을 확장합니다.

Conclusion: 우리는 다양한 시계열 모델링 데이터 세트에서 경쟁력 있는 결과를 입증하였으며, 코드가 제공됩니다.

Abstract: A key question in sequence modeling with neural networks is how to represent
and learn highly nonlinear and probabilistic state dynamics. Operator theory
views such dynamics as linear maps on Hilbert spaces containing mean embedding
vectors of distributions, offering an appealing but currently overlooked
perspective. We propose a new approach to sequence modeling based on an
operator-theoretic view of a hidden Markov model (HMM). Instead of
materializing stochastic recurrence, we embed the full sequence distribution as
a tensor in the product Hilbert space. A generative process is then defined as
maximum mean discrepancy (MMD) gradient flow in the space of sequences. To
overcome challenges with large tensors and slow sampling convergence, we
introduce spectral mean flows, a novel tractable algorithm integrating two core
concepts. First, we propose a new neural architecture by leveraging spectral
decomposition of linear operators to derive a scalable tensor network
decomposition of sequence mean embeddings. Second, we extend MMD gradient flows
to time-dependent Hilbert spaces and connect them to flow matching via the
continuity equation, enabling simulation-free learning and faster sampling. We
demonstrate competitive results on a range of time-series modeling datasets.
Code is available at https://github.com/jw9730/spectral-mean-flow.

</details>


### [34] [OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2510.15495)
*Woo-Jin Ahn,Sang-Ryul Baek,Yong-Jun Lee,Hyun-Duck Choi,Myo-Taeg Lim*

Main category: cs.LG

TL;DR: OffSim은 전문가 생성 상태-행동 궤적에서 환경 역학 및 보상 구조를 모방하기 위한 모델 기반 오프라인 역강화 학습 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 강화 학습 알고리즘은 사전 정의된 보상 함수가 있는 상호작용 시뮬레이터를 사용하여 정책을 훈련하는데, 이러한 시뮬레이터 개발과 보상 함수 정의는 시간이 많이 걸리고 노동 집약적이다.

Method: OffSim은 전문가 생성 상태-행동 궤적에서 환경 역학 및 보상 구조를 직접 모방하고, 높은 엔트로피 전이 모델과 IRL 기반 보상 함수를 공동 최적화한다.

Result: OffSim은 기존의 오프라인 IRL 방법에 비해 상당한 성능 향상을 달성하며, OffSim$^+$ 확장을 통해 다중 데이터셋 설정에서 탐색을 개선하는 한계 보상을 통합한다.

Conclusion: 광범위한 MuJoCo 실험을 통해 OffSim의 효과성과 견고성을 확인하였다.

Abstract: Reinforcement learning algorithms typically utilize an interactive simulator
(i.e., environment) with a predefined reward function for policy training.
Developing such simulators and manually defining reward functions, however, is
often time-consuming and labor-intensive. To address this, we propose an
Offline Simulator (OffSim), a novel model-based offline inverse reinforcement
learning (IRL) framework, to emulate environmental dynamics and reward
structure directly from expert-generated state-action trajectories. OffSim
jointly optimizes a high-entropy transition model and an IRL-based reward
function to enhance exploration and improve the generalizability of the learned
reward. Leveraging these learned components, OffSim can subsequently train a
policy offline without further interaction with the real environment.
Additionally, we introduce OffSim$^+$, an extension that incorporates a
marginal reward for multi-dataset settings to enhance exploration. Extensive
MuJoCo experiments demonstrate that OffSim achieves substantial performance
gains over existing offline IRL methods, confirming its efficacy and
robustness.

</details>


### [35] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: SESA는 강화 학습에서 채택된 새로운 순차 샘플링 프레임워크로, 다양한 해결책을 생성하여 성능을 향상시키고 정책 붕괴를 방지한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)의 추론 능력을 향상시키기 위해 강화 학습(RL)이 중요한 역할을 하지만, 탐색 제한과 엔트로피 붕괴 문제가 발생한다.

Method: SESA는 다양한 해결책 스케치를 순차적으로 생성하고 이를 전체 추론 경로로 확장하여 정책 붕괴를 방지한다.

Result: SESA는 전통적인 RL 방법보다 경로 다양성과 붕괴 회복 면에서 일관되게 우수한 성능을 보였다.

Conclusion: 이 연구는 탐색에 대한 구조적 접근 방식을 도입하여 RL로 훈련된 LLM에서 더 효과적이고 다양한 추론을 가능하게 한다.

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [36] [Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems](https://arxiv.org/abs/2510.15555)
*Sibo Xiao*

Main category: cs.LG

TL;DR: SDR 추정량은 전략적 환경에서 인과 추론을 위한 혁신적인 프레임워크로, 전략적 균형 모델링과 두 배 견고한 추정을 통합한다.


<details>
  <summary>Details</summary>
Motivation: 전략적 에이전트 행동으로 인한 내재적 치료 할당 문제를 해결하고, 전략적 고려 사항을 포함한 두 배 견고성을 유지하기 위하여.

Method: SDR 추정량은 전략적 비혼란성 하에서의 일관성과 비대칭 정규성을 이론적으로 분석한다.

Result: SDR은 기본 방법에 비해 7.6%-29.3%의 편향 감소를 달성하며, 에이전트 집단의 강도에 따라 강력한 확장성을 유지한다.

Conclusion: 이 프레임워크는 에이전트가 개입에 전략적으로 반응할 때 신뢰할 수 있는 인과 추론을 위한 원칙적인 접근 방식을 제공한다.

Abstract: We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework
that integrates strategic equilibrium modeling with doubly robust estimation
for causal inference in strategic environments. SDR addresses endogenous
treatment assignment arising from strategic agent behavior, maintaining double
robustness while incorporating strategic considerations. Theoretical analysis
confirms SDR's consistency and asymptotic normality under strategic
unconfoundedness. Empirical evaluations demonstrate SDR's superior performance
over baseline methods, achieving 7.6\%-29.3\% bias reduction across varying
strategic strengths and maintaining robust scalability with agent populations.
The framework provides a principled approach for reliable causal inference when
agents respond strategically to interventions.

</details>


### [37] [GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device](https://arxiv.org/abs/2510.15620)
*Jiahao Zhou,Chengliang Lin,Dingji Li,Mingkai Dong,Haibo Chen*

Main category: cs.LG

TL;DR: 이 논문은 크로스 인코더 재랭커를 사용한 의미적 top-K 선택의 지연 시간과 메모리 요구 사항을 줄이는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 디바이스에서 AI 서비스의 효율성을 높이기 위해, 특히 시퀀스 수준의 희소성을 탐구하고자 한다.

Method: GRATING이라는 훈련 없는 추론 시스템을 제안하며, 전역적으로 모든 후보를 유지하고 점진적인 클러스터 가지치기를 통해 지연 시간을 줄인다.

Result: GRATING은 기존의 최신 기준선에 비해 지연 시간을 최대 89.0%까지 줄이고, 최대 94.9%까지 피크 메모리 사용을 제한한다.

Conclusion: GRATING은 세 가지 실제 AI 응용 프로그램에서 지연 시간과 메모리를 크게 낮추어 효율성과 배포 가능성을 개선함을 보여준다.

Abstract: Semantic top-K selection with cross-encoder rerankers underpins of on-device
AI services, such as retrieval-augmented generation, agent memory, and
personalized recommendation. However, its latency and memory demands dominate
end-to-end budgets on edge hardware. Revisiting the objective of top-K
selection, we reveal that only relative rankings matter, not exact
per-candidate scores. We further observe sequence-level sparsity: relative
rankings stabilize early in intermediate layers, allowing pruning opportunities
prior to completing full inference.
  Building on this insight, we propose monolithic forwarding and develop a
training-free inference system, GRATING. By maintaining a global view of all
candidates, it reduces latency through progressive cluster pruning. It also
bounds peak memory usage by strategically overlapping I/O with computation via
dual-layer sliding window and chunked execution. We evaluate GRATING against
state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple
M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak
memory by up to 94.9% in microbenchmarks, without any loss in precision. Across
three real-world on-device AI applications, GRATING lowers latency by
11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial
improvements in efficiency and deployability.

</details>


### [38] [CQD-SHAP: Explainable Complex Query Answering via Shapley Values](https://arxiv.org/abs/2510.15623)
*Parsa Abbasi,Stefan Heindorf*

Main category: cs.LG

TL;DR: CQD-SHAP는 CQA의 쿼리 부분 기여도를 계산하는 새로운 프레임워크로, 불완전한 지식 그래프에서 새로운 지식을 추론하여 신뢰성을 높입니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 쿼리 응답(CQA)은 불완전한 지식 그래프에서 다단계 추론을 요구하며, 사용자 신뢰를 높이기 위한 해석 가능성이 필요합니다.

Method: CQD-SHAP는 부정적 게임 이론의 샤플리 가치를 기반으로 해서 각 쿼리 부분의 기여도를 계산합니다.

Result: 자동화된 평가를 통해, CQD-SHAP이 대부분의 쿼리 유형에 대해 효과적임을 입증합니다.

Conclusion: CQD-SHAP는 신경 예측기를 활용하여 새로운 지식을 추론하는 가치와 쿼리 부분 기여도를 설명하는 데 기여합니다.

Abstract: Complex query answering (CQA) goes beyond the well-studied link prediction
task by addressing more sophisticated queries that require multi-hop reasoning
over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic
CQA methods is still an emerging field. Almost all of these methods can be
regarded as black-box models, which may raise concerns about user trust.
Although neurosymbolic approaches like CQD are slightly more interpretable,
allowing intermediate results to be tracked, the importance of different parts
of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel
framework that computes the contribution of each query part to the ranking of a
specific answer. This contribution explains the value of leveraging a neural
predictor that can infer new knowledge from an incomplete KG, rather than a
symbolic approach relying solely on existing facts in the KG. CQD-SHAP is
formulated based on Shapley values from cooperative game theory and satisfies
all the fundamental Shapley axioms. Automated evaluation of these explanations
in terms of necessary and sufficient explanations, and comparisons with various
baselines, shows the effectiveness of this approach for most query types.

</details>


### [39] [Decentralized Parameter-Free Online Learning](https://arxiv.org/abs/2510.15644)
*Tomas Ortega,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 이 논문에서는 하이퍼파라미터 조정 없이 서브리니어 네트워크 후회를 보장하는 파라미터 자유의 분산 온라인 학습 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 분산 온라인 학습에서 하이퍼파라미터 조정 없이 효과적인 성능을 제공하는 알고리즘이 필요하다.

Method: 다수의 에이전트 간의 코인 베팅과 분산 온라인 학습을 연결하는 새로운 '베팅 함수' 공식을 도입하여 후회 분석을 간소화하였다.

Result: 서브리니어 네트워크 후회 경계가 도출되었고, 합성 및 실제 데이터셋에 대한 실험을 통해 검증되었다.

Conclusion: 이 알고리즘 패밀리는 분산 센싱, 분산 최적화 및 협력적 머신 러닝 응용프로그램에 적용할 수 있다.

Abstract: We propose the first parameter-free decentralized online learning algorithms
with network regret guarantees, which achieve sublinear regret without
requiring hyperparameter tuning. This family of algorithms connects multi-agent
coin-betting and decentralized online learning via gossip steps. To enable our
decentralized analysis, we introduce a novel "betting function" formulation for
coin-betting that simplifies the multi-agent regret analysis. Our analysis
shows sublinear network regret bounds and is validated through experiments on
synthetic and real datasets. This family of algorithms is applicable to
distributed sensing, decentralized optimization, and collaborative ML
applications.

</details>


### [40] [Fast and Compact Tsetlin Machine Inference on CPUs Using Instruction-Level Optimization](https://arxiv.org/abs/2510.15653)
*Yefan Zeng,Shengyu Duan,Rishad Shafik,Alex Yakovlev*

Main category: cs.LG

TL;DR: Tsetlin Machine의 효율적인 소프트웨어 구현과 연산 가속을 위한 기법을 제안.


<details>
  <summary>Details</summary>
Motivation: 리소스가 제한된 장치에서 고속 추론을 가능하게 하려는 동기.

Method: 비트 단위 연산을 활용한 compact 모델 표현 및 가속 처리, 조기 종료 메커니즘 도입, 문자 재배치 전략 적용.

Result: 최적화된 구현으로 기존 정수 기반 TM 구현 대비 추론 시간을 최대 96.71% 단축.

Conclusion: 코드 밀도를 유지하면서도 성능을 크게 향상시킨다.

Abstract: The Tsetlin Machine (TM) offers high-speed inference on resource-constrained
devices such as CPUs. Its logic-driven operations naturally lend themselves to
parallel execution on modern CPU architectures. Motivated by this, we propose
an efficient software implementation of the TM by leveraging instruction-level
bitwise operations for compact model representation and accelerated processing.
To further improve inference speed, we introduce an early exit mechanism, which
exploits the TM's AND-based clause evaluation to avoid unnecessary
computations. Building upon this, we propose a literal Reorder strategy
designed to maximize the likelihood of early exits. This strategy is applied
during a post-training, pre-inference stage through statistical analysis of all
literals and the corresponding actions of their associated Tsetlin Automata
(TA), introducing negligible runtime overhead. Experimental results using the
gem5 simulator with an ARM processor show that our optimized implementation
reduces inference time by up to 96.71% compared to the conventional
integer-based TM implementations while maintaining comparable code density.

</details>


### [41] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer는 Lean 증명을 간소화하기 위해 훈련된 최초의 언어 모델로, 인간의 추가 감독 없이도 작동합니다.


<details>
  <summary>Details</summary>
Motivation: 증명의 길이가 길어지면 인간이 이해하기 어려워지고, 이는 수학적 통찰력을 제한하기 때문에 증명 단순화가 중요합니다.

Method: ProofOptimizer는 전문가 반복 및 강화 학습을 통해 훈련되며, Lean을 사용하여 간소화를 검증하고 훈련 신호를 제공합니다.

Result: ProofOptimizer는 최신 RL 훈련된 증명기에서 생성된 증명을 대폭 압축하며, miniF2F에서 87%, PutnamBench에서 57%, Seed-Prover의 IMO 2025 증명에서 49%의 증명 길이 감소를 보여줍니다.

Conclusion: 간소화된 증명은 Lean에서 더 빠르게 확인되며, 감독된 파인튜닝을 위한 훈련 데이터로 재사용될 때 후속 증명기 성능을 더욱 향상시킵니다.

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [42] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 안전한 강화학습 시스템을 개발하기 위한 ProSh 알고리즘을 소개하며, 비용 제약 하에 안전성을 보장합니다.


<details>
  <summary>Details</summary>
Motivation: 안전은 강화학습에서 주요한 우려 사항이며, 최적 성능을 달성하면서도 안전하게 배포 가능한 RL 시스템 개발을 목표로 합니다.

Method: ProSh는 위험 예산을 사용하여 제한된 MDP 상태 공간을 증가시키고, 학습된 비용 비평가를 사용해 에이전트의 정책 분포에 쉴드를 적용하여 안전성을 강제하는 모델 프리 알고리즘입니다.

Result: ProSh는 모든 샘플링된 행동이 기대값에서 안전하게 유지되도록 보장하며, 환경이 결정론적일 때 최적성도 유지됩니다.

Conclusion: ProSh는 훈련 중에도 안전을 보장하며, 실험에서 이를 입증했습니다.

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [43] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: 이 논문은 자동화 기반 피드백을 활용하여 강화 학습에서 복잡한 보상 구조를 다루는 새로운 접근 방식을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 역사 의존적 보상 구조에서 전통적인 방법이 직면하는 도전 과제를 해결하기 위해서.

Method: 결정론적 유한 자동자(DFA)에서 유도된 선호를 사용하여 보상 함수를 학습하게 돕는 방법을 제안합니다.

Result: 우리의 접근 방식은 시간적 의존성을 가진 작업에 대한 효과적인 정책을 학습하도록 RL 에이전트를 돕고, 전통적인 보상 엔지니어링과 자동화 기초 방법을 능가합니다.

Conclusion: 자동화 기반 선호가 비마르코프 보상을 처리하는 데 있어 확장 가능하고 효율적인 대안임을 강조합니다.

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>
