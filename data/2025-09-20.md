<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 14]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity](https://arxiv.org/abs/2509.14276)
*Yuxiang Mai,Qiyue Yin,Wancheng Ni,Pei Xu,Kaiqi Huang*

Main category: cs.MA

TL;DR: CoDiCon은 다중 에이전트 강화 학습에서 정책 교환과 전략적 다양성을 촉진하기 위해 경쟁적 동기를 포함하는 새로운 접근 방식을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법은 개별 에이전트의 특성에 기반한 정책 설계에 주로 집중하고, 정책 형성 시 에이전트 간 상호 작용과 영향을 간과했습니다.

Method: CoDiCon은 협력적인 상황에 경쟁적 유인을 통합하여 정책 교환을 촉진하고 에이전트 간 전략적 다양성을 촉진하는 내재적 보상 메커니즘을 설계합니다.

Result: 실험 결과, CoDiCon은 SMAC 및 GRF 환경에서 최첨단 방법에 비해 우수한 성능을 달성했습니다.

Conclusion: 경쟁적 내재적 보상이 협력 에이전트 간의 다양한 적응 전략을 효과적으로 촉진하는 것으로 나타났습니다.

Abstract: In recent years, diversity has emerged as a useful mechanism to enhance the
efficiency of multi-agent reinforcement learning (MARL). However, existing
methods predominantly focus on designing policies based on individual agent
characteristics, often neglecting the interplay and mutual influence among
agents during policy formation. To address this gap, we propose Competitive
Diversity through Constructive Conflict (CoDiCon), a novel approach that
incorporates competitive incentives into cooperative scenarios to encourage
policy exchange and foster strategic diversity among agents. Drawing
inspiration from sociological research, which highlights the benefits of
moderate competition and constructive conflict in group decision-making, we
design an intrinsic reward mechanism using ranking features to introduce
competitive motivations. A centralized intrinsic reward module generates and
distributes varying reward values to agents, ensuring an effective balance
between competition and cooperation. By optimizing the parameterized
centralized reward module to maximize environmental rewards, we reformulate the
constrained bilevel optimization problem to align with the original task
objectives. We evaluate our algorithm against state-of-the-art methods in the
SMAC and GRF environments. Experimental results demonstrate that CoDiCon
achieves superior performance, with competitive intrinsic rewards effectively
promoting diverse and adaptive strategies among cooperative agents.

</details>


### [2] [LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.14680)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Dong Huang,Yuanye Zhao,Zheng Lin,Zihan Fang,Dianxin Luan,Heming Cui,Yong Cui*

Main category: cs.MA

TL;DR: LEED는 다중 에이전트 강화 학습을 위한 프레임워크로, 에이전트 수가 증가함에 따른 조정 및 확장성의 병목 현상을 해결한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 강화 학습(MARL)은 복잡한 환경에서의 지능형 의사결정을 위한 잠재력이 있지만, 에이전트 수가 증가하면 조정 및 확장성 문제에 직면한다.

Method: LEED는 시연 생성 모듈(DG)과 정책 최적화 모듈(PO)로 구성되며, DG 모듈은 대형 언어 모델을 활용하여 환경과 상호작용하기 위한 지침을 생성하여 고품질 시연을 만들어낸다. PO 모듈은 각 에이전트가 생성된 시연을 활용하여 전문가 정책 손실을 구성하고 이를 자신의 정책 손실과 통합하는 분산 훈련 패러다임을 채택한다.

Result: LEED는 최신 기준선에 비해 우수한 샘플 효율성, 시간 효율성 및 강력한 확장성을 달성한다.

Conclusion: LEED를 통해 각 에이전트는 전문가 지식과 개인 경험을 바탕으로 자신의 지역 정책을 효과적으로 개인화하고 최적화할 수 있다.

Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for
intelligent decision-making in complex environments. However, it suffers from a
coordination and scalability bottleneck as the number of agents increases. To
address these issues, we propose the LLM-empowered expert demonstrations
framework for multi-agent reinforcement learning (LEED). LEED consists of two
components: a demonstration generation (DG) module and a policy optimization
(PO) module. Specifically, the DG module leverages large language models to
generate instructions for interacting with the environment, thereby producing
high-quality demonstrations. The PO module adopts a decentralized training
paradigm, where each agent utilizes the generated demonstrations to construct
an expert policy loss, which is then integrated with its own policy loss. This
enables each agent to effectively personalize and optimize its local policy
based on both expert knowledge and individual experience. Experimental results
show that LEED achieves superior sample efficiency, time efficiency, and robust
scalability compared to state-of-the-art baselines.

</details>


### [3] [Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.15103)
*Simin Li,Zheng Yuwei,Zihao Mao,Linhao Wang,Ruixiao Xu,Chengdong Ma,Xin Yu,Yuqing Ma,Qi Dou,Xin Wang,Jie Luo,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: 이 논문은 대규모 다중 에이전트 강화 학습(MARL)에서 취약한 에이전트를 식별하는 문제를 다루며, 특히 계층적 적대적 분산 평균 장(field control) 접근 방식을 통해 해결책을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 시스템 규모가 커짐에 따라 부분 에이전트 실패가 불가피해져, 전체 성능을 가장 크게 저하시킬 수 있는 에이전트 집합을 식별하는 것이 중요합니다.

Method: 취약한 에이전트 식별(VAI) 문제를 계층적 적대적 분산 평균 장 제어(HAD-MFC)로 프레임화하고, Fenchel-Rockafellar 변환을 사용하여 계층적 과정을 분리하여 각 레벨에서 독립적으로 학습할 수 있도록 정규화된 평균 장 벨만 연산자를 도출합니다.

Result: 실험을 통해 우리의 방법이 대규모 MARL에서 더 취약한 에이전트를 효과적으로 식별하고, 규칙 기반 시스템을 혼란시켜 더 나쁜 실패를 유도하며, 각 에이전트의 취약성을 드러내는 가치 함수를 학습함을 보여줍니다.

Conclusion: 이 분해는 원래의 HAD-MFC의 최적 해를 보존합니다.

Abstract: Partial agent failure becomes inevitable when systems scale up, making it
crucial to identify the subset of agents whose compromise would most severely
degrade overall performance. In this paper, we study this Vulnerable Agent
Identification (VAI) problem in large-scale multi-agent reinforcement learning
(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field
Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task
of selecting the most vulnerable agents, and the lower level learns worst-case
adversarial policies for these agents using mean-field MARL. The two problems
are coupled together, making HAD-MFC difficult to solve. To solve this, we
first decouple the hierarchical process by Fenchel-Rockafellar transform,
resulting a regularized mean-field Bellman operator for upper level that
enables independent learning at each level, thus reducing computational
complexity. We then reformulate the upper-level combinatorial problem as a MDP
with dense rewards from our regularized mean-field Bellman operator, enabling
us to sequentially identify the most vulnerable agents by greedy and RL
algorithms. This decomposition provably preserves the optimal solution of the
original HAD-MFC. Experiments show our method effectively identifies more
vulnerable agents in large-scale MARL and the rule-based system, fooling system
into worse failures, and learns a value function that reveals the vulnerability
of each agent.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)을 통한 침투 테스트 자동화의 효율성과 신뢰성을 평가한 연구로, 다수의 LLM 기반 에이전트를 종합적으로 검토하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 침투 테스트에서 자동화 또는 보완의 수단으로 점점 더 많이 사용되고 있지만, 공격 단계 전반에 걸친 효과와 신뢰성이 불확실하여 이들 모델의 성능을 평가할 필요가 있다.

Method: 다양한 LLM 기반 에이전트를 단일 에이전트에서 모듈형 디자인까지 포괄적으로 평가하고, 현실적인 침투 테스트 시나리오에서 경험적 성능과 반복적으로 나타나는 실패 패턴을 측정하였다. 또한, 다섯 가지 핵심 기능인 GCM, IAM, CCI, AP 및 RTM의 영향을 겨냥한 증강을 통해 고립시켰다.

Result: 일부 아키텍처는 이러한 속성의 하위 집합을 본래적으로 나타내지만, 목표로 한 증강이 모듈형 에이전트 성능을 상당히 개선시킨다는 결과가 나타났다, 특히 복잡하고 다단계이며 실시간 침투 테스트 작업에서 더욱 그러하다.

Conclusion: 이 연구는 모듈형 에이전트의 성능이 주요 기능의 타겟 증강을 통해 향상될 수 있음을 보여준다.

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [5] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 본 연구는 웹 에이전트의 성능을 모듈화된 평가 프레임워크를 통해 분석하여 중간 오류를 식별하고 향후 개선점을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 현재의 평가가 전체 성공에만 초점을 맞추고 중간 오류를 간과함으로써 실패 모드에 대한 통찰력을 제한하고 체계적인 개선을 방해하는 문제를 해결할 필요성이 있다.

Method: 모듈화된 평가 프레임워크를 제안하여 에이전트 파이프라인을 해석 가능한 단계로 분해하고 자세한 오류 분석을 수행한다.

Result: SeeAct 프레임워크와 Mind2Web 데이터셋을 사례로 활용하여 이 접근 방식이 표준 메트릭스에서 놓친 취약점을 드러내는 방법을 보여준다.

Conclusion: 이 연구는 보다 강력하고 일반화 가능한 웹 에이전트를 위한 기틀을 마련한다.

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [6] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: VCBench는 벤처 캐피탈에서 창업자의 성공 예측을 위한 첫 번째 벤치마크로, AGI 발전을 위한 데이터셋 공유의 중요성을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 벤처 캐피탈 영역에서 신호가 희소하고 결과가 불확실한 가운데, 창업자의 성공 예측을 위한 필요성이 높아지고 있습니다.

Method: VCBench는 9,000개의 익명화된 창업자 프로필을 제공하며, 대조 테스트를 통해 재식별 위험을 90% 이상 줄였습니다. 또한 아홉 개의 최신 대형 언어 모델을 평가했습니다.

Result: DeepSeek-V3는 기준 정확도의 6배 이상 달성하고, GPT-4o는 최고 F0.5 점수를 기록하였으며, 대부분의 모델이 인간 기준을 초과했습니다.

Conclusion: VCBench는 벤처 예측의 AGI에 대한 재현 가능하고 개인정보를 보호하는 평가를 위한 커뮤니티 주도의 기준을 수립하며, 공공의 evolving 자원으로 제공됩니다.

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [7] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: OpenLens AI는 건강 정보학을 위한 완전 자동화된 프레임워크로, 문헌 검토, 데이터 분석 및 논문 준비를 위한 전문 에이전트를 통합하여 연구 파이프라인을 자동화합니다.


<details>
  <summary>Details</summary>
Motivation: 건강 정보학 연구는 다양한 데이터 양식과 빠른 지식 확장이 특징이며, 생물 의학 과학, 데이터 분석 및 임상 실습 간의 통합 통찰력을 필요로 합니다.

Method: OpenLens AI는 문헌 검토, 데이터 분석, 코드 생성 및 원고 준비를 위한 전문 에이전트를 통합하고, 의료 시각화를 위한 비전-언어 피드백과 재현성을 위한 품질 관리를 강화합니다.

Result: 이 프레임워크는 연구 파이프라인 전체를 자동화하여 투명하고 추적 가능한 워크플로우로 출판 가능 LaTeX 원고를 생성합니다.

Conclusion: OpenLens AI는 건강 정보학 연구를 발전시키기 위한 도메인 적합형 솔루션을 제공합니다.

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [8] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 인공지능(AGI) 분야는 인간의 성능 복제와 인간의 인지 과정 복제라는 본질적으로 다른 목표들로 인해 여전히 논쟁 중이다. 현재의 성능 기반 정의는 체계적인 연구의 로드맵을 제공하지 않으며, 진정한 지능의 질적 특성을 제대로 정의하지 못한다. 본 연구는 인간 뇌에서 영감을 받아 외부 모방에서 기초 인지 구조의 개발로 초점을 옮기는 새로운 패러다임을 제안한다. 진정한 지능(TI)은 여섯 가지 핵심 구성 요소로 정의되며, AGI를 다섯 가지 측정 가능한 구성 요소 수에 따라 분류하는 실용적인 세분화 체계를 제안한다. 이 연구는 AGI의 첫 번째 포괄적이고 체계적인 정의를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능(AGI)에 대한 논쟁이 두 가지 기본적으로 다른 목표(인간 같은 성능 복제 vs. 인간 같은 인지 과정 복제)로 인해 열려 있기 때문입니다.

Method: 인간 뇌에서 영감을 받아 외부 모방에서 기초 인지 구조 개발로 초점을 전환하는 새로운 패러다임을 제안합니다. TI는 여섯 가지 핵심 구성 요소로 특징지어지는 시스템으로 정의합니다.

Result: AGI를 다섯 가지 측정 가능한 구성 요소의 수에 따라 분류하는 실용적인, 다섯 단계의 세분화 체계를 제안합니다. 수준 5 AGI에 도달하면 TI와의 차이는 순전히 철학적 논쟁으로 남습니다.

Conclusion: AGI의 다섯 번째 수준은 TI와 기능적으로 그리고 실질적으로 동등하다고 결론지었습니다. 이 연구는 AGI를 위한 첫 포괄적이고 메커니즘 기반 정의를 제공합니다.

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [9] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 이 논문은 다중 에이전트 시스템(MAS)의 보안과 신뢰성을 향상시키기 위한 새로운 아키텍처 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템의 안전과 신뢰성을 높이기 위한 필요성.

Method: 분산 보안층으로서 기능하는 Sentinel Agents 네트워크와 정책 구현 및 에이전트 참여 관리를 담당하는 Coordinator Agent를 사용하는 이중 레이어 보안 접근 방식.

Result: 162개의 다양한 합성 공격을 포함한 시뮬레이션에서 Sentinel Agents가 효과적으로 공격 시도를 탐지하여 제안된 모니터링 접근 방식의 실용성을 입증하였다.

Conclusion: 제안된 프레임워크는 시스템 관찰 가능성을 향상시키고 규제 준수를 지원하며 시간에 따른 정책 진화를 가능하게 한다.

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [10] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 이 논문에서는 사회적 AI 평가를 위한 Melting Pot 대회에서 AI 시스템의 협력 능력을 평가하고, 베이지안 접근 방식인 Measurement Layouts를 활용하여 다중 에이전트 시스템의 능력 프로필을 유추하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 사회적 능력 개발 및 평가는 경쟁적이고 협력적인 행동이 자연스럽게 발생하는 복잡한 환경이 필요합니다.

Method: 이 논문에서는 Measurement Layouts라는 베이지안 접근 방식을 적용하여 Melting Pot 대회에서 다중 에이전트 시스템의 능력 프로필을 추론합니다.

Result: 이 능력 프로필은 Melting Pot 환경 내에서 미래 성능을 예측할 뿐만 아니라 에이전트의 기본적인 친사회적 능력을 보여줍니다.

Conclusion: 이 연구는 Measurement Layouts가 AI 시스템의 평가에 있어 강력한 예측 정확성과 실행 가능한 통찰을 제공함을 보여줍니다.

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [11] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)의 합리성을 평가하기 위한 첫 번째 벤치마크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 인공지능 일반 지능을 위한 잠재력을 지닌 LLM의 인간 행동 유사성에 대한 우려가 커지고 있다.

Method: LLM의 전반적인 합리성을 평가하기 위한 벤치마크를 제안한다.

Result: 벤치마크는 사용자 친화적인 툴킷과 실험 결과, 분석을 포함하여 LLM의 인간 합리성과의 수렴 및 발산 지점을 조명한다.

Conclusion: 이 벤치마크는 LLM의 개발자와 사용자 모두에게 기초적인 도구가 될 것으로 믿는다.

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [12] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 이 연구는 자동화된 작업 흐름 구성에 대한 동적 프레임워크를 제안하여, 역사적 경험을 활용하면서도 각 작업의 고유 특성에 반응할 수 있도록 한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLM)을 활용한 작업 효율성을 높이기 위해, 기존의 역사적 경험에만 의존한 접근의 한계를 극복할 필요가 있다.

Method: Q-table 학습을 활용하여 결정 공간을 최적화하고, 진행 중인 작업을 평가하여 다음 작업 실행 에이전트를 사전 결정하는 자동화된 작업 흐름 구성 프레임워크를 제안한다.

Result: 우리의 방법은 최신 기준 대비 평균 4.05%의 개선을 이루었으며, 작업 흐름 구축과 추론 비용을 기존 방법의 30.68%-48.31%로 줄였다.

Conclusion: 제안된 접근 방식은 다양한 벤치마크 데이터 세트에서 효과성과 실현 가능성을 입증하였다.

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [13] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass는 대형 언어 모델을 기반으로 한 다중 에이전트 워크플로우의 배포 후 모니터링 및 디버깅을 위한 최초의 평가 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 조직들이 복잡한 작업을 자동화하기 위해 LLM을 채택함에 따라, 기존 평가 방법으로는 포착할 수 없는 오류, 발생 행동, 시스템 실패 등이 증가하고 있다.

Method: 이 프레임워크는 전문가 디버거의 추론 과정을 모방하며 오류 식별 및 분류, 주제 클러스터링, 정량적 점수 매기기, 전략적 요약으로 구성된 다단계 분석 파이프라인을 사용한다.

Result: 실제 배포에서 프레임워크의 유용성을 입증하며, 공적으로 제공되는 TRAIL 벤치마크와 비교하여 효과성을 확인한다.

Conclusion: AgentCompass는 주요 지표에서 최첨단 결과를 달성하며, 인간 주석에서 놓치는 중요한 문제들을 발견하여 생산 중인 에이전트 시스템의 신뢰할 수 있는 모니터링 및 개선을 위한 강력하고 개발자 중심의 도구로서의 역할을 강조한다.

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [14] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: RationAnomaly는 체인-오브-생각(CoT) 미세 조정과 강화 학습을 결합하여 로그 이상 탐지를 개선하는 새로운 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: 소프트웨어 시스템의 운영 상태를 신호하는 증거로써 로그의 중요성과 현대 소프트웨어 시스템의 신뢰성을 확보하기 위해 자동화된 로그 이상 탐지의 필요성.

Method: RationAnomaly는 CoT에 기반한 전문가 수준의 추론 패턴 학습과 다면적 보상 함수를 사용하는 강화 학습 단계를 통해 로그 이상 탐지를 최적화합니다.

Result: 실험 결과, RationAnomaly는 최신 기법들을 능가하며 주요 벤치마크에서 우수한 F1 점수를 기록했습니다.

Conclusion: RationAnomaly는 투명하고 단계적인 분석 출력도 제공하며, 관련된 코드와 데이터 세트를 공개했습니다.

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [15] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: AC-RAG 프레임워크는 정보 검색 과정에서의 품질 문제를 해결하기 위해 개발되었으며, 다양한 도메인에서 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: RAG는 도메인 특화 LLM을 위한 일반적인 접근 방식이지만, 낮은 품질의 검색 문서에 대한 인식 부족으로 성능이 저하되는 문제인 '검색 환각'에 시달리고 있습니다.

Method: AC-RAG는 지식 격차를 식별하는 일반화된 탐지기와 정확한 솔루션을 제공하는 도메인 전문 해결기 두 가지 이질적 에이전트를 사용합니다. 이들은 중재자의 안내에 따라 적대적인 협력을 통해 문제를 분석합니다.

Result: AC-RAG는 검색 정확성을 크게 향상시키며 다양한 도메인에서 최첨단 RAG 방법들을 초월하는 성능을 보여줍니다.

Conclusion: AC-RAG 프레임워크는 정보 검색의 효율성을 높이고 도메인 특화 LLM의 성능을 개선하는 데 기여합니다.

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [16] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: KAMAC는 진화하는 진단 맥락에 따라 전문가 팀을 동적으로 형성하고 확장할 수 있는 지식 기반 적응형 다중 에이전트 협업 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 의료 의사 결정은 여러 임상 전문 분야의 지식을 통합해야 하며, 이 과정은 다학제 팀을 통해 이루어진다.

Method: KAMAC는 LLM 에이전트가 진단 맥락에 따라 지식 기반 논의를 통해 추가 전문가를 모집하여 지식 격차를 식별하고 보완하도록 지원한다.

Result: KAMAC는 복잡한 진료 상황에서 단일 에이전트 및 고급 다중 에이전트 방법보다 훨씬 우수한 성능을 보였다.

Conclusion: KAMAC는 동적인 교차 전문 지식이 필요한 복잡한 임상 시나리오에서 특히 뛰어난 성과를 나타냈다.

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [17] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: 이 연구는 미국의 한 공립대학 대학원 온라인 과정에서 동료 리뷰의 기계 생성 리뷰를 통해 형성적 평가를 지원하는 생성 AI의 사용을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 형성적 평가에서 동료 리뷰에 대한 기계 생성 피드백의 효과를 분석하고자 한다.

Method: 120개의 메타 리뷰를 분석하여 생성 AI 피드백이 의미를 구성하는 방식을 탐구하였다.

Result: 생성 AI는 효과적인 인간 피드백의 주요 수사적 및 관계적 특성을 근사할 수 있다는 점을 제시하였다.

Conclusion: AI 메타 피드백은 피드백 문해력을 지원하고 동료 리뷰에 대한 학습자의 참여를 향상시킬 가능성이 있다.

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [18] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: 언어 모델들이 일관된 추론을 하지 못하는 문제를 해결하기 위해, 다중 에이전트 합의 정렬(MACA)라는 강화 학습 프레임워크를 제안한다. 이 방법은 에이전트 간의 논의에서 발생하는 경로를 통해 더 나은 결정적이고 간결한 추론을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델들은 동일한 프롬프트에 대해 모순된 응답을 생성하는 경향이 있다. 이는 탐색 샘플링 하에서 일관된 결과로 이어지는 추론 경로를 안정적으로 선택하는 데 어려워하는 것을 반영한다.

Method: MACA는 다중 에이전트 토론에서 다수/소수 결과를 활용하여 내적 합의에 맞는 추론 경로를 선호하도록 모델을 후 훈련하는 강화 학습 프레임워크이다.

Result: MACA는 자기 일관성에서 +27.6%, 단일 에이전트 추론에서 +23.7%, 샘플링 기반 추론에서 +22.4%, 다중 에이전트 앙상블 의사결정에서 +42.7%의 성과 향상을 보였다.

Conclusion: 이 연구는 언어 모델의 잠재적인 추론 능력을 더 신뢰성 있게 열 수 있는 강한 자기 정렬을 보여준다.

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [19] [Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention](https://arxiv.org/abs/2506.11445)
*Xuan Duy Ta,Bang Giang Le,Thanh Ha Le,Viet Cuong Ta*

Main category: cs.AI

TL;DR: 이 논문은 혼합 교통 환경에서 자율 주행 차량들이 인간이 조종하는 차량과 비정상적인 주행 상황에 적응할 수 있도록 하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 혼합 교통 환경에서 자율 주행 차량의 적응성을 높이기 위해 효율적인 방법이 필요하다.

Method: 자기 주의 연산자를 이용한 지역 상태 주의 모듈을 제안한다.

Result: 제안된 접근법은 병합 효율성을 크게 개선하였고, 특히 고밀도 교통 상황에서 성과가 두드러졌다.

Conclusion: 이 접근법은 자율 차량의 교통 상황에서의 충돌 해결에 효과적이며, 병합 과정을 관리하는 데 중요한 역할을 한다.

Abstract: In mixed-traffic environments, autonomous vehicles must adapt to
human-controlled vehicles and other unusual driving situations. This setting
can be framed as a multi-agent reinforcement learning (MARL) environment with
full cooperative reward among the autonomous vehicles. While methods such as
Multi-agent Proximal Policy Optimization can be effective in training MARL
tasks, they often fail to resolve local conflict between agents and are unable
to generalize to stochastic events. In this paper, we propose a Local State
Attention module to assist the input state representation. By relying on the
self-attention operator, the module is expected to compress the essential
information of nearby agents to resolve the conflict in traffic situations.
Utilizing a simulated highway merging scenario with the priority vehicle as the
unexpected event, our approach is able to prioritize other vehicles'
information to manage the merging process. The results demonstrate significant
improvements in merging efficiency compared to popular baselines, especially in
high-density traffic settings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
*Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CR

TL;DR: 대형 언어 모델이 다중 에이전트 시스템에 필수적이게 됨에 따라 새로운 프라이버시 위험이 발생하고 있다. 특히 무해해 보이는 응답이 상호작용을 통해 민감한 정보를 회수할 수 있게 만드는 현상을 '구성적 프라이버시 유출'이라고 명명하고, 이를 체계적으로 연구하였다. 이를 해결하기 위해 두 가지 방어 전략을 제안하고 실험하여 균형 잡힌 프라이버시-유틸리티 거래를 분석하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 다중 에이전트 시스템에서 필수적이 되면서 나타나는 새로운 프라이버시 위험을 조사하고자 한다.

Method: 구성적 프라이버시 유출을 모델링하기 위한 프레임워크를 개발하고, 두 가지 방어 전략을 제안한다: ToM(마음의 이론) 방어와 CoDef(협력적 합의 방어) 전략이다.

Result: 임상 실험을 통해 ToM 방어가 민감한 쿼리 차단을 크게 개선하고(최대 97%), CoDef가 최상의 균형을 달성함을 발견하였다(79.8% Balanced Outcome).

Conclusion: 우리의 연구 결과는 협업 LLM 배포에서 새로운 위험 계층을 드러내고, 구성적인 맥락 주도 프라이버시 유출에 대한 안전 장치를 설계하는 데 유용한 통찰을 제공한다.

Abstract: As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.

</details>


### [21] [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)
*S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin*

Main category: cs.CR

TL;DR: 본 논문은 LLM 배포에서의 프롬프트 주입 공격에 대한 다중 에이전트 방어 프레임워크를 제안하며, 이를 통해 실시간으로 공격을 탐지하고 중화할 수 있다.


<details>
  <summary>Details</summary>
Motivation: 프롬프트 주입 공격은 대형 언어 모델 배포에서 주요한 취약점으로, 악의적인 지시가 시스템 프롬프트를 무시하고 예기치 않은 행동을 유도할 수 있다는 점에서 중요한 문제이다.

Method: 전문 LLM 에이전트를 사용하여 협조된 파이프라인 내에서 프롬프트 주입 공격을 탐지하고 중화하는 다중 에이전트 방어 프레임워크를 구현하였다.

Result: 55개의 독특한 프롬프트 주입 공격을 평가한 결과, 전통적인 방어 없이 ChatGLM에서 30%, Llama2에서 20%의 공격 성공률을 보였으나, 다중 에이전트 파이프라인을 적용했을 때 모든 테스트 시나리오에서 공격 성공률을 0%로 낮추어 100% 완화를 달성하였다.

Conclusion: 이 프레임워크는 시스템 기능을 유지하면서도 다양한 공격 카테고리에 대한 강건성을 보여준다.

Abstract: Prompt injection attacks represent a major vulnerability in Large Language
Model (LLM) deployments, where malicious instructions embedded in user inputs
can override system prompts and induce unintended behaviors. This paper
presents a novel multi-agent defense framework that employs specialized LLM
agents in coordinated pipelines to detect and neutralize prompt injection
attacks in real-time. We evaluate our approach using two distinct
architectures: a sequential chain-of-agents pipeline and a hierarchical
coordinator-based system. Our comprehensive evaluation on 55 unique prompt
injection attacks, grouped into 8 categories and totaling 400 attack instances
across two LLM platforms (ChatGLM and Llama2), demonstrates significant
security improvements. Without defense mechanisms, baseline Attack Success
Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent
pipeline achieved 100% mitigation, reducing ASR to 0% across all tested
scenarios. The framework demonstrates robustness across multiple attack
categories including direct overrides, code execution attempts, data
exfiltration, and obfuscation techniques, while maintaining system
functionality for legitimate queries.

</details>


### [22] [Enterprise AI Must Enforce Participant-Aware Access Control](https://arxiv.org/abs/2509.14608)
*Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao*

Main category: cs.CR

TL;DR: 본 논문은 대규모 언어 모델(LLM)의 보안 위험과 이를 방지하기 위한 정밀 접근 제어의 중요성을 다룬다.


<details>
  <summary>Details</summary>
Motivation: LLM이 기업의 내부 데이터로 훈련되고 여러 사용자와 상호작용하면서 발생할 수 있는 민감한 정보 유출 위험을 해결하고자 한다.

Method: LLM과 검색 증강 생성(RAG) 아키텍처를 결합하여, 적절한 접근 제어가 부재할 때 민감한 정보가 유출될 수 있는 공격 방법을 검토한다.

Result: 현재의 방어 메커니즘은 확률적이며 이러한 공격에 대해 강력한 보호를 제공하지 못한다는 것을 보여준다.

Conclusion: 정확하고 철저한 세분화된 접근 제어를 통해 민감한 데이터의 유출을 예방할 수 있음을 강조한다.

Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility](https://arxiv.org/abs/2509.14386)
*Arjun S. Nair,Kristina P. Sinaga*

Main category: cs.LG

TL;DR: 신경망은 이진 정답/오답 감독 아래에서 잘 보정된 신뢰도 추정치를 의미 있는 다양성과 함께 학습할 수 없다는 근본적인 불가능성 정리를 증명한다.


<details>
  <summary>Details</summary>
Motivation: 신경망이 신뢰도와 다양성을 동시에 학습할 수 없는 이유를 이해하고자 하며, 이는 정보 이론적 제약 사항임을 강조한다.

Method: 엄격한 수학적 분석과 부정적 보상 훈련, 대칭 손실 함수, 사후 보정 방법을 포함한 포괄적인 실험 평가를 수행했다.

Result: 부정적 보상은 극심한 신뢰도 부족을 초래하며(0.8 이상의 ECE), 신뢰도 다양성을 파괴하고, 대칭 손실은 이진 신호 평균을 피하지 못하며, 사후 보정 방법은 신뢰도 분포를 압축함으로써만 보정을 달성한다.

Conclusion: 이 결과는 신경망의 환각을 직접적으로 설명하며, 사후 보정이 수학적으로 필요하다는 것을 수립한다. 우리는 이러한 근본적인 한계를 극복할 수 있는 새로운 감독 패러다임을 제안한다.

Abstract: We prove a fundamental impossibility theorem: neural networks cannot
simultaneously learn well-calibrated confidence estimates with meaningful
diversity when trained using binary correct/incorrect supervision. Through
rigorous mathematical analysis and comprehensive empirical evaluation spanning
negative reward training, symmetric loss functions, and post-hoc calibration
methods, we demonstrate this is an information-theoretic constraint, not a
methodological failure. Our experiments reveal universal failure patterns:
negative rewards produce extreme underconfidence (ECE greater than 0.8) while
destroying confidence diversity (std less than 0.05), symmetric losses fail to
escape binary signal averaging, and post-hoc methods achieve calibration (ECE
less than 0.02) only by compressing the confidence distribution. We formalize
this as an underspecified mapping problem where binary signals cannot
distinguish between different confidence levels for correct predictions: a 60
percent confident correct answer receives identical supervision to a 90 percent
confident one. Crucially, our real-world validation shows 100 percent failure
rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while
post-hoc calibration's 33 percent success rate paradoxically confirms our
theorem by achieving calibration through transformation rather than learning.
This impossibility directly explains neural network hallucinations and
establishes why post-hoc calibration is mathematically necessary, not merely
convenient. We propose novel supervision paradigms using ensemble disagreement
and adaptive multi-agent learning that could overcome these fundamental
limitations without requiring human confidence annotations.

</details>


### [24] [Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models](https://arxiv.org/abs/2509.14427)
*Ilyass Moummad,Kawtar Zaher,Lukas Rauch,Alexis Joly*

Main category: cs.LG

TL;DR: 훈련이 필요 없는 Hashing-Baseline 방법을 통해 강력한 이진 해싱을 제안하며, 데이터 전처리에 있어 세계적 수준의 인코더를 활용하여 경쟁력 있는 검색 성능을 달성한다.


<details>
  <summary>Details</summary>
Motivation: 스케일러블한 빠른 검색 애플리케이션을 위해 해싱 방법은 필수적이지만, 기존의 최신 해싱 방법은 비싼 시나리오별 훈련을 요구한다.

Method: 훈련이 필요 없는 해싱 기법을 활용하여 풍부한 사전 훈련된 임베딩을 생성하는 강력한 해싱 방법인 Hashing-Baseline을 도입하고, 고전적인 해싱 기법인 주성분 분석, 무작위 직교 투영, 임계값 이진화를 재검토하였다.

Result: 아무런 추가 학습이나 미세 조정 없이 최첨단 비전 및 오디오 인코더의 고정 임베딩과 결합하여 경쟁력 있는 검색 성능을 제공한다.

Conclusion: 기술의 일반성과 효과성을 입증하기 위해 이 방법을 표준 이미지 검색 벤치마크 및 새로 도입된 오디오 해싱 벤치마크에서 평가하였다.

Abstract: Information retrieval with compact binary embeddings, also referred to as
hashing, is crucial for scalable fast search applications, yet state-of-the-art
hashing methods require expensive, scenario-specific training. In this work, we
introduce Hashing-Baseline, a strong training-free hashing method leveraging
powerful pretrained encoders that produce rich pretrained embeddings. We
revisit classical, training-free hashing techniques: principal component
analysis, random orthogonal projection, and threshold binarization, to produce
a strong baseline for hashing. Our approach combines these techniques with
frozen embeddings from state-of-the-art vision and audio encoders to yield
competitive retrieval performance without any additional learning or
fine-tuning. To demonstrate the generality and effectiveness of this approach,
we evaluate it on standard image retrieval benchmarks as well as a newly
introduced benchmark for audio hashing.

</details>


### [25] [Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition](https://arxiv.org/abs/2509.14577)
*Yang Xu,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: SPMD-LRT는 고차원 텐서 데이터를 효과적으로 분류하기 위해 구조를 보존하는 마진 분포 학습을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 LMDM은 벡터화된 입력에만 제한되어 있으며 고차원 텐서 데이터에서 문제를 겪고 있습니다.

Method: 텐서 표현에서 직접 작동하고 1차 및 2차 텐서 통계를 포함하여 목표를 설정하며, 저순위 텐서 분해 기법을 활용합니다.

Result: SPMD-LRT는 다양한 데이터셋에서 기존 SVM 및 텐서 기반 방법들보다 우수한 분류 정확도를 보였습니다.

Conclusion: SPMD-LRT는 고차원 텐서 데이터를 효과적으로 처리하고 마진 분포를 최적화하는 데 강력함을 입증했습니다.

Abstract: The Large Margin Distribution Machine (LMDM) is a recent advancement in
classifier design that optimizes not just the minimum margin (as in SVM) but
the entire margin distribution, thereby improving generalization. However,
existing LMDM formulations are limited to vectorized inputs and struggle with
high-dimensional tensor data due to the need for flattening, which destroys the
data's inherent multi-mode structure and increases computational burden. In
this paper, we propose a Structure-Preserving Margin Distribution Learning for
High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates
directly on tensor representations without vectorization. The SPMD-LRT
preserves multi-dimensional spatial structure by incorporating first-order and
second-order tensor statistics (margin mean and variance) into the objective,
and it leverages low-rank tensor decomposition techniques including rank-1(CP),
higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An
alternating optimization (double-gradient descent) algorithm is developed to
efficiently solve the SPMD-LRT, iteratively updating factor matrices and core
tensor. This approach enables SPMD-LRT to maintain the structural information
of high-order data while optimizing margin distribution for improved
classification. Extensive experiments on diverse datasets (including MNIST,
images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior
classification accuracy compared to conventional SVM, vector-based LMDM, and
prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker
Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest
accuracy, highlighting the benefit of structure preservation. These results
confirm the effectiveness and robustness of SPMD-LRT in handling
high-dimensional tensor data for classification.

</details>


### [26] [Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models](https://arxiv.org/abs/2509.14723)
*Sosuke Hosokawa,Toshiharu Kawakami,Satoshi Kodera,Masamichi Ito,Norihiko Takeda*

Main category: cs.LG

TL;DR: 본 논문에서는 단일세포 기초 모델(scFMs)의 결정 과정 해석 가능성을 개선하기 위해 트랜스코더를 이용하여 실 생물학적 메커니즘과 대응되는 의사 결정 회로를 추출하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 단일세포 기초 모델들은 다양한 작업에서 뛰어난 성능을 보여주지만, 이들의 의사 결정 과정이 전통적인 방법보다 해석하기 어려운 문제를 해결하고자 합니다.

Method: cell2sentence (C2S) 모델에 트랜스코더를 훈련시켜 의사 결정 회로를 추출하는 방법을 사용합니다.

Result: 발견된 회로들이 실제 생물학적 메커니즘에 해당함을 입증했습니다.

Conclusion: 트랜스코더는 복잡한 단일세포 모델 내에서 생물학적으로 신뢰할 수 있는 경로를 밝혀낼 가능성이 있음을 확인했습니다.

Abstract: Single-cell foundation models (scFMs) have demonstrated state-of-the-art
performance on various tasks, such as cell-type annotation and perturbation
response prediction, by learning gene regulatory networks from large-scale
transcriptome data. However, a significant challenge remains: the
decision-making processes of these models are less interpretable compared to
traditional methods like differential gene expression analysis. Recently,
transcoders have emerged as a promising approach for extracting interpretable
decision circuits from large language models (LLMs). In this work, we train a
transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By
leveraging the trained transcoder, we extract internal decision-making circuits
from the C2S model. We demonstrate that the discovered circuits correspond to
real-world biological mechanisms, confirming the potential of transcoders to
uncover biologically plausible pathways within complex single-cell models.

</details>


### [27] [STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models](https://arxiv.org/abs/2509.14801)
*Julian F. Schumann,Anna Mészáros,Jens Kober,Arkady Zgonnikov*

Main category: cs.LG

TL;DR: STEP 프레임워크는 자율주행 차량의 궤적 예측 모델을 평가하는 새로운 기준을 제시하여 다양한 데이터셋에 대한 통합 인터페이스를 제공하고, 일관된 훈련 및 평가 조건을 강제하며, 다양한 예측 모델을 지원합니다.


<details>
  <summary>Details</summary>
Motivation: 자율주행 차량의 안전하고 효과적인 경로 계획을 위해 궤적 예측이 중요하지만, 모델 평가를 위한 기준화된 관행이 부족합니다.

Method: STEP이라는 새로운 벤치마킹 프레임워크를 소개하며, 이는 여러 데이터셋에 대한 통합 인터페이스를 제공하고 일관된 훈련 및 평가 조건을 시행합니다.

Result: STEP을 통해 여러 실험을 통해 1) 일반적으로 사용되는 테스트 절차의 한계, 2) 상호작용 예측 향상을 위한 에이전트의 공동 모델링 중요성, 3) 현재의 최첨단 모델이 분포 변화와 적대적 공격에 취약하다는 것을 구체적으로 보여줍니다.

Conclusion: STEP을 통해 우리는 '리더보드' 접근 방식에서 모델의 행동과 복합적인 다중 에이전트 환경에서의 일반화에 대한 깊은 통찰로 초점을 전환하고자 합니다.

Abstract: While trajectory prediction plays a critical role in enabling safe and
effective path-planning in automated vehicles, standardized practices for
evaluating such models remain underdeveloped. Recent efforts have aimed to
unify dataset formats and model interfaces for easier comparisons, yet existing
frameworks often fall short in supporting heterogeneous traffic scenarios,
joint prediction models, or user documentation. In this work, we introduce STEP
-- a new benchmarking framework that addresses these limitations by providing a
unified interface for multiple datasets, enforcing consistent training and
evaluation conditions, and supporting a wide range of prediction models. We
demonstrate the capabilities of STEP in a number of experiments which reveal 1)
the limitations of widely-used testing procedures, 2) the importance of joint
modeling of agents for better predictions of interactions, and 3) the
vulnerability of current state-of-the-art models against both distribution
shifts and targeted attacks by adversarial agents. With STEP, we aim to shift
the focus from the ``leaderboard'' approach to deeper insights about model
behavior and generalization in complex multi-agent settings.

</details>


### [28] [Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics](https://arxiv.org/abs/2509.14894)
*Guillermo Hijano Mendizabal,Davide Lancierini,Alex Marshall,Andrea Mauri,Patrick Haworth Owen,Mitesh Patel,Konstantinos Petridis,Shah Rukh Qasim,Nicola Serra,William Sutcliffe,Hanae Tilquin*

Main category: cs.LG

TL;DR: 본 논문은 레인포스먼트 러닝을 활용하여 쿼크 매력에 대한 주요 배경을 체계적으로 판단하는 새로운 접근 방식을 제시하고, 이 과정에서 유전자 알고리즘과의 시너지를 활용하는 알고리즘을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 아름다움 하드론 붕괴의 실험 연구는 유사한 최종 상태를 가진 다양한 붕괴 채널로 인해 많은 배경으로 인한 심각한 도전에 직면해 있습니다.

Method: 본 연구에서는 레인포스먼트 러닝(RL)을 활용하여 주요 배경 프로세스를 체계적으로 결정하며, RL과 유전자 알고리즘(GA)의 시너지를 이용해 희소한 보상과 큰 궤적 공간을 가진 환경에 적합한 새로운 알고리즘을 제안합니다.

Result: 본 방법은 주요 배경을 효과적으로 탐색하고 성공적인 궤적을 식별하여 RL 에이전트의 훈련을 안내합니다.

Conclusion: 우리가 제안한 접근 방식은 입자 물리학 측정에 광범위하게 적용될 수 있으며, RL 에이전트가 붕괴를 나타내는 토큰 시퀀스를 처리할 수 있도록 transformer 구조도 포함됩니다.

Abstract: Experimental studies of beauty hadron decays face significant challenges due
to a wide range of backgrounds arising from the numerous possible decay
channels with similar final states. For a particular signal decay, the process
for ascertaining the most relevant background processes necessitates a detailed
analysis of final state particles, potential misidentifications, and kinematic
overlaps, which, due to computational limitations, is restricted to the
simulation of only the most relevant backgrounds. Moreover, this process
typically relies on the physicist's intuition and expertise, as no systematic
method exists.
  This paper has two primary goals. First, from a particle physics perspective,
we present a novel approach that utilises Reinforcement Learning (RL) to
overcome the aforementioned challenges by systematically determining the
critical backgrounds affecting beauty hadron decay measurements. While beauty
hadron physics serves as the case study in this work, the proposed strategy is
broadly adaptable to other types of particle physics measurements. Second, from
a Machine Learning perspective, we introduce a novel algorithm which exploits
the synergy between RL and Genetic Algorithms (GAs) for environments with
highly sparse rewards and a large trajectory space. This strategy leverages GAs
to efficiently explore the trajectory space and identify successful
trajectories, which are used to guide the RL agent's training. Our method also
incorporates a transformer architecture for the RL agent to handle token
sequences representing decays.

</details>


### [29] [Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models](https://arxiv.org/abs/2509.14945)
*Amsalu Tessema,Tizazu Bayih,Kassahun Azezew,Ayenew Kassie*

Main category: cs.LG

TL;DR: 에티오피아 임산부의 영양 부족 문제를 해결하기 위해 머신러닝 기반의 예측 모델을 개발하여 효과적인 영양 상태 분류를 실현하였다.


<details>
  <summary>Details</summary>
Motivation: 에티오피아의 임산부 영양 부족 문제는 공중 보건에 있어 중대한 도전 과제가 되고 있으며, 모성 및 신생아 결과에 부정적인 영향을 미칠 수 있다.

Method: 이 연구는 2005-2020년 에티오피아 인구 및 건강 조사 데이터를 기반으로 하여, 결측치 처리, 정규화, SMOTE를 활용한 데이터 균형화 및 주요 예측 변수 선별을 포함한 데이터 전처리를 수행하였다. XGBoost, Random Forest, CatBoost, AdaBoost와 같은 여러 감독형 앙상블 알고리즘을 사용하여 영양 상태를 분류하였다.

Result: Random Forest 모델이 97.87% 정확도, 97.88% 정밀도, 97.87% 재현율, 97.87% F1 점수 및 99.86% ROC AUC를 기록하며, 임산부를 정상, 적당한 영양 부족, 심각한 영양 부족 및 과영양의 네 가지 범주로 분류한 결과가 가장 우수했다.

Conclusion: 이 연구는 복잡한 데이터셋에서 숨겨진 패턴을 포착하는 앙상블 학습의 유효성을 입증하며, 임산부 영양 위험의 조기 발견을 위한 시기적절한 통찰을 제공한다. 그 결과는 에티오피아의 모성 영양 및 건강 결과를 개선하기 위한 데이터 기반 전략을 지원하는 데 실질적인 의미를 제공한다.

Abstract: Malnutrition among pregnant women is a major public health challenge in
Ethiopia, increasing the risk of adverse maternal and neonatal outcomes.
Traditional statistical approaches often fail to capture the complex and
multidimensional determinants of nutritional status. This study develops a
predictive model using ensemble machine learning techniques, leveraging data
from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108
records with 30 socio-demographic and health attributes. Data preprocessing
included handling missing values, normalization, and balancing with SMOTE,
followed by feature selection to identify key predictors. Several supervised
ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost
were applied to classify nutritional status. Among them, the Random Forest
model achieved the best performance, classifying women into four categories
(normal, moderate malnutrition, severe malnutrition, and overnutrition) with
97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86%
ROC AUC. These findings demonstrate the effectiveness of ensemble learning in
capturing hidden patterns from complex datasets and provide timely insights for
early detection of nutritional risks. The results offer practical implications
for healthcare providers, policymakers, and researchers, supporting data-driven
strategies to improve maternal nutrition and health outcomes in Ethiopia.

</details>


### [30] [Sample Efficient Experience Replay in Non-stationary Environments](https://arxiv.org/abs/2509.15032)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Yuanye Zhao,Zheng Lin,Zihan Fang,Yi Liu,Dianxin Luan,Dong Huang,Heming Cui,Yong Cui*

Main category: cs.LG

TL;DR: 비정상적인 환경에서의 강화 학습(RL)은 도전적이며, 기존 경험 재생 방법에 비해 DEER가 효율적인 학습을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 비정상적인 환경에서의 변화하는 역학과 보상으로 인해 과거 경험이 빠르게 구식이 되는 문제가 있다.

Method: 환경 역학의 불일치(DoE)라는 메트릭을 사용하여 가치 함수에 대한 환경 변화의 영향을 분리하고, 정책 업데이트와 환경 변화에 따라 전이 우선순위를 정하는 DEER라는 적응형 경험 재생 프레임워크를 도입한다.

Result: DEER는 환경 변화 감지를 위해 이진 분류기를 사용하고 각 변화 전후에 다른 우선순위 전략을 적용하여 샘플 효율적인 학습을 가능하게 한다.

Conclusion: 비정상적인 벤치마크에서 DEER는 최신 경험 재생 방법에 비해 오프 정책 알고리즘의 성능을 11.54% 개선시킨다.

Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as
changing dynamics and rewards quickly make past experiences outdated.
Traditional experience replay (ER) methods, especially those using TD-error
prioritization, struggle to distinguish between changes caused by the agent's
policy and those from the environment, resulting in inefficient learning under
dynamic conditions. To address this challenge, we propose the Discrepancy of
Environment Dynamics (DoE), a metric that isolates the effects of environment
shifts on value functions. Building on this, we introduce Discrepancy of
Environment Prioritized Experience Replay (DEER), an adaptive ER framework that
prioritizes transitions based on both policy updates and environmental changes.
DEER uses a binary classifier to detect environment changes and applies
distinct prioritization strategies before and after each shift, enabling more
sample-efficient learning. Experiments on four non-stationary benchmarks
demonstrate that DEER further improves the performance of off-policy algorithms
by 11.54 percent compared to the best-performing state-of-the-art ER methods.

</details>


### [31] [Reinforcement Learning Agent for a 2D Shooter Game](https://arxiv.org/abs/2509.15042)
*Thomas Ackermann,Moritz Spang,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 복잡한 게임 환경에서 강화 학습 에이전트는 희소 보상, 훈련 불안정성 및 낮은 샘플 효율성의 문제를 겪는다. 이 논문은 오프라인 모방 학습과 온라인 강화 학습을 결합한 하이브리드 훈련 방법을 제안한다. 이 방법은 규칙 기반 에이전트의 시연 데이터에서 시작해 행동 클로닝 후 강화 학습으로 전환한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 게임 환경에서 강화 학습 에이전트가 겪는 희소 보상, 훈련 불안정성 및 낮은 샘플 효율성 문제를 해결하고자 한다.

Method: 멀티 헤드 신경망을 구현하여 행동 클로닝과 Q-학습에 대해 별도의 출력을 생성하며, 주의 메커니즘이 적용된 공유 특성 추출 레이어로 통합한다.

Result: 하이브리드 접근 방식은 규칙 기반 상대에 대해 70% 이상의 지속적인 승률을 달성하였고, 순수한 강화 학습 방법보다 훨씬 우수한 성능을 보였다.

Conclusion: 시연 기반 초기화와 강화 학습 최적화를 결합함으로써 복잡한 다중 에이전트 환경에서 게임 AI 에이전트를 개발하는 데 효과적인 솔루션을 제공한다.

Abstract: Reinforcement learning agents in complex game environments often suffer from
sparse rewards, training instability, and poor sample efficiency. This paper
presents a hybrid training approach that combines offline imitation learning
with online reinforcement learning for a 2D shooter game agent. We implement a
multi-head neural network with separate outputs for behavioral cloning and
Q-learning, unified by shared feature extraction layers with attention
mechanisms. Initial experiments using pure deep Q-Networks exhibited
significant instability, with agents frequently reverting to poor policies
despite occasional good performance. To address this, we developed a hybrid
methodology that begins with behavioral cloning on demonstration data from
rule-based agents, then transitions to reinforcement learning. Our hybrid
approach achieves consistently above 70% win rate against rule-based opponents,
substantially outperforming pure reinforcement learning methods which showed
high variance and frequent performance degradation. The multi-head architecture
enables effective knowledge transfer between learning modes while maintaining
training stability. Results demonstrate that combining demonstration-based
initialization with reinforcement learning optimization provides a robust
solution for developing game AI agents in complex multi-agent environments
where pure exploration proves insufficient.

</details>


### [32] [Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits](https://arxiv.org/abs/2509.15073)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: 비정상 다중 무장밴딧에서 제약된 피드백 새로운 모델을 도입하여 동적 환경에서 거의 최적의 동적 후회를 달성하는 알고리즘을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 비정상 다중 무장밴딧의 기존 접근법은 모든 단계에서 보상 피드백이 있다고 가정하지만, 실제 상황에서는 피드백이 제한적입니다.

Method: 제약된 피드백 모델을 도입하고, 비사전적 알고리즘을 제안하여 비정상성의 정도에 대한 사전 지식 없이 동적 후회를 최소화합니다.

Result: 우리의 알고리즘은 비정상성 정도를 포착하는 변동 예산 $V_T$과 쿼리 예산 $B$에 따라 동적 후회 $	ilde{	ext{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$를 달성합니다.

Conclusion: 제안한 알고리즘은 제약된 피드백 환경에서도 효과적으로 작동하여 비정상 다중 무장밴딧 문제에 대한 새로운 접근법을 제공합니다.

Abstract: Non-stationary multi-armed bandits enable agents to adapt to changing
environments by incorporating mechanisms to detect and respond to shifts in
reward distributions, making them well-suited for dynamic settings. However,
existing approaches typically assume that reward feedback is available at every
round - an assumption that overlooks many real-world scenarios where feedback
is limited. In this paper, we take a significant step forward by introducing a
new model of constrained feedback in non-stationary multi-armed bandits, where
the availability of reward feedback is restricted. We propose the first
prior-free algorithm - that is, one that does not require prior knowledge of
the degree of non-stationarity - that achieves near-optimal dynamic regret in
this setting. Specifically, our algorithm attains a dynamic regret of
$\tilde{\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the
number of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$
is the variation budget capturing the degree of non-stationarity.

</details>


### [33] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang*

Main category: cs.LG

TL;DR: 본 논문은 AI 기반 에이전트를 제안하여 하늘 이미지를 통해 대기 오염 수준을 예측하고 오염 시나리오의 사실적인 시각화를 생성합니다.


<details>
  <summary>Details</summary>
Motivation: 대기 오염은 공공 건강 및 환경 지속 가능성에 심각한 위협이지만, 기존 모니터링 시스템은 공간적 범위와 접근성의 제한으로 인해 종종 제약을 받습니다.

Method: 이 연구는 오염 분류를 위해 통계적 텍스처 분석과 감독 학습을 결합하고, 비전-언어 모델(VLM)을 활용한 이미지 생성을 통해 대기 질 조건의 해석 가능한 표현을 생성합니다.

Result: 도시 하늘 이미지 데이터셋을 사용하여 우리 방법을 검증했으며, 오염 수준 추정과 의미적으로 일관된 시각적 종합에서 효과성을 입증했습니다.

Conclusion: 향후 발전에서는 에지 플랫폼에서 실시간 추론을 가능하게 하는 FPGA 기반의 점진적 학습이 적용된 친환경 CNN 아키텍처를 통합하여 확장 가능하고 에너지 효율적인 배포를 지원할 것입니다.

Abstract: Air pollution remains a critical threat to public health and environmental
sustainability, yet conventional monitoring systems are often constrained by
limited spatial coverage and accessibility. This paper proposes an AI-driven
agent that predicts ambient air pollution levels from sky images and
synthesizes realistic visualizations of pollution scenarios using generative
modeling. Our approach combines statistical texture analysis with supervised
learning for pollution classification, and leverages vision-language model
(VLM)-guided image generation to produce interpretable representations of air
quality conditions. The generated visuals simulate varying degrees of
pollution, offering a foundation for user-facing interfaces that improve
transparency and support informed environmental decision-making. These outputs
can be seamlessly integrated into intelligent applications aimed at enhancing
situational awareness and encouraging behavioral responses based on real-time
forecasts. We validate our method using a dataset of urban sky images and
demonstrate its effectiveness in both pollution level estimation and
semantically consistent visual synthesis. The system design further
incorporates human-centered user experience principles to ensure accessibility,
clarity, and public engagement in air quality forecasting. To support scalable
and energy-efficient deployment, future iterations will incorporate a green CNN
architecture enhanced with FPGA-based incremental learning, enabling real-time
inference on edge platforms.

</details>


### [34] [Emergent Alignment via Competition](https://arxiv.org/abs/2509.15090)
*Natalie Collina,Surbhi Goel,Aaron Roth,Emily Ryu,Mirah Shi*

Main category: cs.LG

TL;DR: AI 시스템을 인간 가치와 일치시키는 것은 근본적인 도전 과제이며, 완벽하게 일치하는 모델을 만들 수 없는 것이 alignment의 이점을 얻는 것을 방해하지는 않는가를 연구한다. 여러 개의 AI 에이전트와의 상호작용에서 사용자의 효용이 에이전트의 효용의 볼록 껍질 내에 있을 때, 전략적 경쟁이 완벽한 모델과 유사한 결과를 초래할 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템이 인간 가치와 일치하는 방법을 찾는 것은 여전히 큰 도전이다. 하지만 완벽하게 일치하는 모델을 만들 수 없다면, 우리는 alignment의 이점을 얻을 수 없는가?

Method: 사용자가 여러 가지 서로 다르게 잘못 정렬된 AI 에이전트와 상호작용하는 전략적 설정을 연구하였다. 이를 다리더 스택엘버그 게임으로 모델링하고, 정보가 다른 당사자 간의 다회 대화에 베이지안 설득을 확장하여 세 가지 결과를 증명하였다.

Result: 사용자의 효용이 에이전트의 효용의 볼록 껍질 내에 있을 때, 사용자는 모든 평형에서 그녀의 베이즈 최적 행동을 배우는 것이 가능하다. 또한, 비전략적 사용자가 양자 반응을 사용할 때에도 모든 평형에서 근사 최적 효용을 달성할 수 있다.

Conclusion: 사용자가 평가 기간 후에 최상의 단일 AI를 선택했을 때도 평형 보장이 여전히 근사 최적성을 유지한다는 것을 확립하였다. 이론을 두 가지 실험과 함께 보완한다.

Abstract: Aligning AI systems with human values remains a fundamental challenge, but
does our inability to create perfectly aligned models preclude obtaining the
benefits of alignment? We study a strategic setting where a human user
interacts with multiple differently misaligned AI agents, none of which are
individually well-aligned. Our key insight is that when the users utility lies
approximately within the convex hull of the agents utilities, a condition that
becomes easier to satisfy as model diversity increases, strategic competition
can yield outcomes comparable to interacting with a perfectly aligned model. We
model this as a multi-leader Stackelberg game, extending Bayesian persuasion to
multi-round conversations between differently informed parties, and prove three
results: (1) when perfect alignment would allow the user to learn her
Bayes-optimal action, she can also do so in all equilibria under the convex
hull condition (2) under weaker assumptions requiring only approximate utility
learning, a non-strategic user employing quantal response achieves near-optimal
utility in all equilibria and (3) when the user selects the best single AI
after an evaluation period, equilibrium guarantees remain near-optimal without
further distributional assumptions. We complement the theory with two sets of
experiments.

</details>


### [35] [Efficient Conformal Prediction for Regression Models under Label Noise](https://arxiv.org/abs/2509.15120)
*Yahav Cohen,Jacob Goldberger,Tom Tirer*

Main category: cs.LG

TL;DR: 본 논문은 잡음 레이블이 포함된 보정 집합에서 회귀 모델에 대한 정규화 예측을 적용하는 문제를 다룬다.


<details>
  <summary>Details</summary>
Motivation: 정확한 신뢰 구간을 제공하는 회귀 모델의 예측 결과는 의료 영상 애플리케이션과 같은 고위험 시나리오에서 매우 중요하다.

Method: 우리는 잡음이 없는 CP 임계값을 추정하기 위한 수학적으로 근거 있는 절차를 수립하고, 다음으로 회귀 문제의 연속적 특성으로 인한 문제를 해결하는 실용적인 알고리즘으로 변형한다.

Result: 우리 방법은 가우시안 레이블 노이즈가 있는 두 개의 의료 영상 회귀 데이터셋에서 평가되었으며, 기존 대안에 비해 성능이 크게 향상되어 깨끗한 레이블 설정에 근접한 성능을 달성했다.

Conclusion: 제안된 방법은 잡음 레이블이 포함된 회귀 모델에 CP를 효과적으로 적용할 수 있음을 보여준다.

Abstract: In high-stakes scenarios, such as medical imaging applications, it is
critical to equip the predictions of a regression model with reliable
confidence intervals. Recently, Conformal Prediction (CP) has emerged as a
powerful statistical framework that, based on a labeled calibration set,
generates intervals that include the true labels with a pre-specified
probability. In this paper, we address the problem of applying CP for
regression models when the calibration set contains noisy labels. We begin by
establishing a mathematically grounded procedure for estimating the noise-free
CP threshold. Then, we turn it into a practical algorithm that overcomes the
challenges arising from the continuous nature of the regression problem. We
evaluate the proposed method on two medical imaging regression datasets with
Gaussian label noise. Our method significantly outperforms the existing
alternative, achieving performance close to the clean-label setting.

</details>


### [36] [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](https://arxiv.org/abs/2509.15147)
*Viktor Kovalchuk,Nikita Kotelevskii,Maxim Panov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: 로지트 기반 연합 학습 방법은 대규모 모델에서의 통신 비용을 줄이기 위해 로그잇만 공유하는 방식을 연구하며, 이 논문은 세 가지 로그잇 집계 방법을 소개하고 비교한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 모델의 경우 모델 가중치나 그래디언트를 공유하는 것이 비용이 크기 때문에, 로지트 기반 연합 학습을 통해 이러한 비용을 줄일 필요가 있다.

Method: 논문에서는 세 가지 로그잇 집계 방법인 단순 평균, 불확실성 가중 평균, 학습된 메타 집계기를 소개하고 비교한다.

Result: MNIST와 CIFAR-10 데이터세트에서 평가된 바에 따르면, 이러한 방법들은 통신 부담을 줄이고 비IID 데이터에서의 강건성을 향상시키며 중앙 집중식 훈련과 경쟁할 만한 정확도를 달성한다.

Conclusion: 로그잇 기반 연합 학습은 대규모 모델의 통신 비용을 줄이는 효과적인 방법으로, 더욱 개선된 성능을 보인다.

Abstract: Federated learning (FL) usually shares model weights or gradients, which is
costly for large models. Logit-based FL reduces this cost by sharing only
logits computed on a public proxy dataset. However, aggregating information
from heterogeneous clients is still challenging. This paper studies this
problem, introduces and compares three logit aggregation methods: simple
averaging, uncertainty-weighted averaging, and a learned meta-aggregator.
Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead,
improve robustness under non-IID data, and achieve accuracy competitive with
centralized training.

</details>
