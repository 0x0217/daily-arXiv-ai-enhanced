<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting](https://arxiv.org/abs/2601.05487)
*Huanxiang Lin,Qianyue Wang,Jinwu Hu,Bailin Chen,Qing Du,Mingkui Tan*

Main category: cs.MA

TL;DR: EvidFuse는 데이터 기반 보고서를 위해 텍스트와 차트를 동시에 생성하는 다중 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 현재의 LLM 기반 시스템은 텍스트와 시각화를 분리된 단계에서 생성하여 차트-텍스트 불일치와 분석의 깊이 부족 문제를 초래한다.

Method: EvidFuse는 데이터 증강 분석 에이전트와 실시간 증거 구성 작가로 구성된 두 개의 협력 구성 요소로 시각화 분석을 긴 형식 초안에서 분리한다.

Result: EvidFuse는 차트 품질, 차트-텍스트 정렬 및 보고서 수준 유용성에 대한 평가에서 상위 순위를 기록하였다.

Conclusion: EvidFuse는 필요한 시점에 시각적 증거를 구축하고 포함할 수 있어 데이터 기반 보고서 작성을 혁신한다.

Abstract: Data-driven reports communicate decision-relevant insights by tightly interleaving narrative text with charts grounded in underlying tables. However, current LLM-based systems typically generate narratives and visualizations in staged pipelines, following either a text-first-graph-second or a graph-first-text-second paradigm. These designs often lead to chart-text inconsistency and insight freezing, where the intermediate evidence space becomes fixed and the model can no longer retrieve or construct new visual evidence as the narrative evolves, resulting in shallow and predefined analysis. To address the limitations, we propose \textbf{EvidFuse}, a training-free multi-agent framework that enables writing-time text-chart interleaved generation for data-driven reports. EvidFuse decouples visualization analysis from long-form drafting via two collaborating components: a \textbf{Data-Augmented Analysis Agent}, equipped with Exploratory Data Analysis (EDA)-derived knowledge and access to raw tables, and a \textbf{Real-Time Evidence Construction Writer} that plans an outline and drafts the report while intermittently issuing fine-grained analysis requests. This design allows visual evidence to be constructed and incorporated exactly when the narrative requires it, directly constraining subsequent claims and enabling on-demand expansion of the evidence space. Experiments demonstrate that EvidFuse attains the top rank in both LLM-as-a-judge and human evaluations on chart quality, chart-text alignment, and report-level usefulness.

</details>


### [2] [How Exploration Breaks Cooperation in Shared-Policy Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05509)
*Yi-Ning Weng,Hsuan-Wei Lee*

Main category: cs.MA

TL;DR: 공유 정책 딥 Q-네트워크 학습에서의 탐색이 협력을 시스템적으로 붕괴시킬 수 있음.


<details>
  <summary>Details</summary>
Motivation: 동적 사회적 딜레마에서 다중 에이전트 강화 학습의 확장성을 위해 파라미터 공유 의존.

Method: 공유된 DQN을 통해 협력이 지속적으로 낮은 수준으로 수렴하는 실험 실시.

Result: 기대 보상이 불일치, 노이즈, 훈련 부족과 관계없이 협력 붕괴가 발생.

Conclusion: 공유 정책 MARL의 기본적인 실패 모드 및 협력을 저해할 구조적 조건을 규명했다.

Abstract: Multi-agent reinforcement learning in dynamic social dilemmas commonly relies on parameter sharing to enable scalability. We show that in shared-policy Deep Q-Network learning, standard exploration can induce a robust and systematic collapse of cooperation even in environments where fully cooperative equilibria are stable and payoff dominant. Through controlled experiments, we demonstrate that shared DQN converges to stable but persistently low-cooperation regimes. This collapse is not caused by reward misalignment, noise, or insufficient training, but by a representational failure arising from partial observability combined with parameter coupling across heterogeneous agent states. Exploration-driven updates bias the shared representation toward locally dominant defection responses, which then propagate across agents and suppress cooperative learning. We confirm that the failure persists across network sizes, exploration schedules, and payoff structures, and disappears when parameter sharing is removed or when agents maintain independent representations. These results identify a fundamental failure mode of shared-policy MARL and establish structural conditions under which scalable learning architectures can systematically undermine cooperation. Our findings provide concrete guidance for the design of multi-agent learning systems in social and economic environments where collective behavior is critical.

</details>


### [3] [Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting](https://arxiv.org/abs/2601.05606)
*Chen Han,Jin Tan,Bohan Yu,Wenzhen Zheng,Xijin Tang*

Main category: cs.MA

TL;DR: 이 논문은 대형 언어 모델을 기반으로 한 다중 에이전트 시스템에서의 일치 동역학이 네트워크 토폴로지에 의해 어떻게 형성되는지를 체계적으로 연구하였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 다중 에이전트 시스템에서 사회적 상호작용을 통해 집단적 결정을 내리는 방식에 대한 이해를 높이고자 한다.

Method: 정보 오탐지 작업을 통해 네트워크 토폴로지가 LLM 기반의 다중 에이전트 시스템에서 일치 동역학에 미치는 영향을 연구하고, 신뢰도 정규화 풀링 규칙을 도입하였다.

Result: 실험 결과, 네트워크 토폴로지가 집단적 판단의 효율성과 견고성에 중대한 영향을 미친다는 것이 입증되었다. 중앙집중적 구조는 즉각적인 결정을 가능하게 하지만 허브의 능력에 민감하며, 분산 구조는 더 견고한 합의를 촉진한다.

Conclusion: 네트워크 토폴로지와 자기-사회 가중치가 집단 의사결정의 효율성, 견고성 및 실패 모드를 어떻게 형성하는지를 명확히 하였다.

Abstract: Large Language Models (LLMs) are increasingly instantiated as interacting agents in multi-agent systems (MAS), where collective decisions emerge through social interaction rather than independent reasoning. A fundamental yet underexplored mechanism in this process is conformity, the tendency of agents to align their judgments with prevailing group opinions. This paper presents a systematic study of how network topology shapes conformity dynamics in LLM-based MAS through a misinformation detection task. We introduce a confidence-normalized pooling rule that controls the trade-off between self-reliance and social influence, enabling comparisons between two canonical decision paradigms: Centralized Aggregation and Distributed Consensus. Experimental results demonstrate that network topology critically governs both the efficiency and robustness of collective judgments. Centralized structures enable immediate decisions but are sensitive to hub competence and exhibit same-model alignment biases. In contrast, distributed structures promote more robust consensus, while increased network connectivity speeds up convergence but also heightens the risk of wrong-but-sure cascades, in which agents converge on incorrect decisions with high confidence. These findings characterize the conformity dynamics in LLM-based MAS, clarifying how network topology and self-social weighting jointly shape the efficiency, robustness, and failure modes of collective decision-making.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring](https://arxiv.org/abs/2601.05256)
*Eirini Baltzi,Tilemachos Moumouris,Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.AI

TL;DR: NAIAD는 대규모 언어 모델과 외부 분석 도구를 활용하여 내륙 수역 모니터링을 위한 포괄적 솔루션을 제공하는 인공지능 보조 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 내륙 수역 모니터링은 공공 건강과 생태계를 보호하는 데 필수적이며, 위험을 완화하기 위한 시기적절한 개입을 가능하게 한다.

Method: NAIAD는 자연어 질의를 실행 가능한 통찰력으로 변환하는 단일 프롬프트 인터페이스를 제공하며, RAG, LLM 추론, 외부 도구 조정, 계산 그래프 실행 등을 통해 알고리즘을 통해 지식을 검색하고 통합한 맞춤형 보고서를 생성한다.

Result: 성능은 정확성과 적합성 지표를 사용하여 평가되었고, 다수의 사용자 전문성 수준을 아우르는 전용 기준에서 각각 77%와 85% 이상의 성과를 달성하였다.

Conclusion: 초기 결과는 다양한 질의 유형에 대해 강한 적응성과 견고성을 보이며, LLM 백본에 대한 절제 연구를 통해 Gemma 3(27B)와 Qwen 2.5(14B)가 계산 효율성과 추론 성능 간의 최상의 균형을 제공함을 강조한다.

Abstract: Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.

</details>


### [5] [Effects of personality steering on cooperative behavior in Large Language Model agents](https://arxiv.org/abs/2601.05302)
*Mizuki Sakai,Mizuki Yokoyama,Wakaba Tateishi,Genki Ichinose*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLMs)이 전략적 및 사회적 상호작용에서 자율 에이전트로 점점 더 많이 사용되고 있으며, 이 연구는 LLM의 성격 조정이 협력 행동에 미치는 영향을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 성격 특성을 LLM에 부여하면 행동에 영향을 줄 수 있다는 최근 연구 결과가 있지만, 통제된 조건에서 성격 조정이 협력에 미치는 영향은 불확실하다.

Method: Big Five 프레임워크를 기반으로, GPT-3.5-turbo, GPT-4o, GPT-5 모델의 기본 성격 프로필을 측정하고, 기준선 및 성격 정보에 따른 조건에서 행동을 비교한 후 각 성격 차원의 극단적인 값을 독립적으로 조작하여 분석하였다.

Result: 협력을 촉진하는 주요 요소는 모든 모델에서 우호성이며, 다른 성격 특성은 제한된 영향을 미친다. 명시적인 성격 정보는 협력을 증가시키지만, 특히 이전 세대 모델에서는 착취에 대한 취약성을 높일 수 있다. 반면, 후속 세대 모델은 보다 선택적인 협력을 보인다.

Conclusion: 성격 조정은 결정론적 제어 메커니즘이라기보다는 행동 편향으로 작용한다.

Abstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

</details>


### [6] [Conformity and Social Impact on AI Agents](https://arxiv.org/abs/2601.05384)
*Alessandro Bellina,Giordano De Marzo,David Garcia*

Main category: cs.AI

TL;DR: AI 에이전트의 집단 행동 이해가 중요해지며, 이 연구는 대규모 언어 모델에서 사회적 압력에 따른 동조 경향을 조사한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 환경에서 AI 에이전트의 집단 행동을 이해하는 것이 인공 사회의 역학을 예측하는 데 중요하다.

Method: 사회 심리학의 고전적 시각 실험을 조정하여 AI 에이전트가 사회적 행위자로서 집단 영향에 어떻게 반응하는지를 조사하였다.

Result: AI 에이전트는 집단 크기, 만장일치, 작업 난이도 및 출처 특성에 민감하며, 체계적인 동조 편향을 나타낸다.

Conclusion: AI 에이전트는 격리 상태에서 거의 완벽한 성과를 내지만, 사회적 영향에 의해 조작에 매우 취약해지며, 이는 다중 에이전트 시스템에서 악의적인 조작 및 허위 정보 전파 등의 보안 취약점을 드러낸다.

Abstract: As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.

</details>


### [7] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)은 복잡한 의사결정을 위한 강력한 후보이나, 불투명성으로 인해 높은 위험 환경에서의 채택이 저해된다. 본 논문에서는 주장 검증을 위한 계층적 방법인 ART(Adaptive Reasoning Trees)를 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 높은 위험 환경에서의 활용을 막고 있는 불투명성을 해결하기 위해.

Method: 주장 검증을 위한 계층적 방법인 ART를 제안하며, 이는 뿌리 주장을 시작으로 지지 및 공격 논거로 가지를 분기시킨다. 자식들 간의 쌍대 토너먼트를 통해 하향식으로 논거의 강도를 결정한다.

Result: 다양한 데이터셋에서 ART를 실증적으로 검증하였으며, ART의 구조적 추론이 강력한 기준선보다 우수함을 보였다.

Conclusion: ART는 신뢰할 수 있고 명확한 결정 과정을 보장하는 설명 가능한 주장 검증의 새로운 기준을 확립한다.

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [8] [PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering](https://arxiv.org/abs/2601.05465)
*Yu Liu,Wenxiao Zhang,Cong Cao,Wenxuan Lu,Fangfang Yuan,Diandian Guo,Kun Peng,Qiang Sun,Kaiyan Zhang,Yanbing Liu,Jin B. Hong,Bowen Zhou,Zhiyuan Ma*

Main category: cs.AI

TL;DR: PRISMA는 RL 기반의 구조로, 복잡한 쿼리를 해결하기 위한 효율적인 정보 검색 및 추론 프로세스를 최적화하는 데 중점을 둔다.


<details>
  <summary>Details</summary>
Motivation: 실제 세계의 개방형 멀티-홉 질문을 대규모 자료에서 답변하는 것은 RAG 시스템에서 중요한 도전 과제이다.

Method: PRISMA는 계획-검색-검토-해결-기억 아키텍처를 가진 분리된 RL-guided 프레임워크이다.

Result: 실험 결과 PRISMA는 10개 벤치마크에서 최첨단 성능을 이끌어 내며, 실제 시나리오에서 효율적으로 배치될 수 있다.

Conclusion: PRISMA는 복잡한 쿼리를 해결하는 데 있어 효율적인 정보 검색 및 추론 프로세스를 제공하는 혁신적인 접근법이다.

Abstract: Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.

</details>


### [9] [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)
*Zixuan Xiao,Jun Ma,Siwei Zhang*

Main category: cs.AI

TL;DR: MMUEChange는 다양한 도시 데이터를 융합하여 도시 환경 변화를 분석하는 다중 모달 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 지속 가능한 발전을 위해 도시 환경 변화 이해가 필수적이지만, 기존 접근 방식은 고정적이고 단일 모달 분석에 의존한다.

Method: MMUEChange는 모듈형 도구 키트와 모달리티 컨트롤러 핵심 모듈을 통해 이종 도시 데이터를 유연하게 통합하는 다중 모달 에이전트 프레임워크를 제안한다.

Result: MMUEChange 에이전트는 작업 성공률에서 46.7% 개선을 달성하고 환각을 효과적으로 완화한다.

Conclusion: MMUEChange는 현실 세계에서 정책적 의미가 있는 복잡한 도시 변화 분석 작업을 지원할 수 있는 능력을 입증했다.

Abstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.

</details>


### [10] [Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models](https://arxiv.org/abs/2601.05570)
*Cooper Lin,Maohao Ran,Yanting Zhang,Zhenglin Wan,Hongwei Fan,Yibo Xu,Yike Guo,Wei Xue,Jun Song*

Main category: cs.AI

TL;DR: 이 논문은 대규모 언어 모델의 윤리적 문제를 다루면서, 위기 관리와 같은 전문 분야에서의 전략적 모호성과 정보 비공개를 강조합니다. Crisis-Bench라는 새로운 평가 도구를 통해 이러한 모델들의 성능을 측정하고, 윤리적 우려와 전략적 정보 비공개의 균형을 드러냅니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 일반적인 안전성 및 유용성 향상을 위한 기존의 윤리적 틀은 전문 분야의 전략적 모호성을 저해하고 있습니다.

Method: Crisis-Bench는 여러 에이전트가 참여하는 부분 관찰 가능 마르코프 결정 프로세스(POMDP)를 통해 고위험 기업 위기 상황에서 언어 모델을 평가합니다. 80개의 다양한 스토리라인을 포함하여 7일간의 기업 위기 시뮬레이션을 관리하도록 LLM 기반 PR 에이전트에 임무를 부여하고, 정보를 비대칭적으로 다루도록 합니다.

Result: 일부 모델은 윤리적 우려에 굴복하는 반면, 다른 모델은 시뮬레이션된 주가를 안정시키기 위해 전략적으로 정보를 비공개하는 능력을 보여줍니다.

Conclusion: Crisis-Bench는 ‘평판 관리’ 능력을 평가하기 위한 최초의 정량적 프레임워크를 제공하며, 엄격한 도덕적 절대주의에서 맥락 인식의 전문적 정렬로의 전환을 주장합니다.

Abstract: Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid "Boy Scout" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a "transparency tax" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing "Reputation Management" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.

</details>


### [11] [GenCtrl -- A Formal Controllability Toolkit for Generative Models](https://arxiv.org/abs/2601.05637)
*Emily Cheng,Carmen Amo Alonso,Federico Danieli,Arno Blaas,Luca Zappella,Pau Rodriguez,Xavier Suau*

Main category: cs.AI

TL;DR: 생성 모델에 대한 정밀한 제어가 필요하며, 본 논문은 이러한 제어 가능성을 이론적으로 분석하는 프레임워크를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 생성 모델의 사용이 늘어남에 따라 생성 과정에 대한 정밀한 제어의 필요성이 증가하고 있다.

Method: 인간-모델 상호작용을 제어 프로세스로 설정하고 대화 환경에서 모델의 제어 가능한 집합을 추정하는 새로운 알고리즘을 제안한다.

Result: 모델 제어 가능성이 실험 설정에 크게 의존한다는 것을 보여줌.

Conclusion: 제어 가능성 분석의 철저함이 요구되며, 단순히 제어를 시도하는 것에서 그 한계를 이해하는 것으로 초점이 이동해야 함을 강조한다.

Abstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.

</details>


### [12] [Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion](https://arxiv.org/abs/2601.05629)
*Jiapu Wang,Xinghe Cheng,Zezheng Wu,Ruiqi Ma,Rui Wang,Zhichao Yan,Haoran Luo,Yuhao Jiang,Kai Sun*

Main category: cs.AI

TL;DR: CPSR 프레임워크를 통해 유도 지식 그래프 완성을 개선하고, 노이즈로부터 중요한 정보를 유지하며, 전 세계적인 의미 점수 모듈로 성능을 향상시킴.


<details>
  <summary>Details</summary>
Motivation: 전통적인 KGC 방법들이 신흥 개체를 처리하는 데 한계를 가지고 있어, 유도 KGC 방법이 필요하다.

Method: CPSR 프레임워크는 쿼리-의존 마스킹 모듈과 글로벌 의미 점수 모듈을 사용하여 구조적 및 의미적 정보를 동시에 포착한다.

Result: CPSR은 최첨단 성능을 달성하였다.

Conclusion: CPSR은 기존 유도 KGC 방법의 한계를 극복할 수 있는 효과적인 접근 방식이다.

Abstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.

</details>


### [13] [HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation](https://arxiv.org/abs/2601.05656)
*Rongxin Chen,Tianyu Wu,Bingbing Xu,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: HAG는 대화형 에이전트 모델링에서 큰 성공을 거둔 새로운 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 있는 에이전트 기반 모델링을 위해서는 고충실도 에이전트 초기화가 핵심적이다.

Method: HAG는 인구 세대를 두 단계의 결정 프로세스로 형식화하는 계층적 에이전트 생성 프레임워크를 제안한다.

Result: HAG는 대표적인 기준선보다 평균 37.7%의 인구 정렬 오류를 줄이고, 사회적 일관성을 18.8% 향상시킨다.

Conclusion: HAG는 다중 도메인 벤치마크와 포괄적인 PACE 평가 프레임워크를 통해 효과성을 입증하였다.

Abstract: High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.

</details>


### [14] [CHDP: Cooperative Hybrid Diffusion Policies for Reinforcement Learning in Parameterized Action Space](https://arxiv.org/abs/2601.05675)
*Bingyi Liu,Jinbo He,Haiyong Shi,Enshu Wang,Weizhen Han,Jingxiang Hao,Peixi Wang,Zhuangzhuang Zhang*

Main category: cs.AI

TL;DR: Hybrid action space modeling and optimization is crucial, and the proposed CHDP framework effectively addresses this by integrating discrete and continuous policies.


<details>
  <summary>Details</summary>
Motivation: Hybrid action spaces are essential in robot control and game AI but pose challenges in modeling and optimization.

Method: The paper introduces the Cooperative Hybrid Diffusion Policies (CHDP) framework, utilizing two cooperative agents with discrete and continuous diffusion policies. It includes a sequential update scheme and a low-dimensional latent space codebook for improved scalability.

Result: CHDP achieves a success rate improvement of up to $19.3\%$ over the state-of-the-art methods on challenging benchmarks.

Conclusion: The CHDP framework successfully addresses the challenges of hybrid action spaces, offering improved expressiveness and scalability.

Abstract: Hybrid action space, which combines discrete choices and continuous parameters, is prevalent in domains such as robot control and game AI. However, efficiently modeling and optimizing hybrid discrete-continuous action space remains a fundamental challenge, mainly due to limited policy expressiveness and poor scalability in high-dimensional settings. To address this challenge, we view the hybrid action space problem as a fully cooperative game and propose a \textbf{Cooperative Hybrid Diffusion Policies (CHDP)} framework to solve it. CHDP employs two cooperative agents that leverage a discrete and a continuous diffusion policy, respectively. The continuous policy is conditioned on the discrete action's representation, explicitly modeling the dependency between them. This cooperative design allows the diffusion policies to leverage their expressiveness to capture complex distributions in their respective action spaces. To mitigate the update conflicts arising from simultaneous policy updates in this cooperative setting, we employ a sequential update scheme that fosters co-adaptation. Moreover, to improve scalability when learning in high-dimensional discrete action space, we construct a codebook that embeds the action space into a low-dimensional latent space. This mapping enables the discrete policy to learn in a compact, structured space. Finally, we design a Q-function-based guidance mechanism to align the codebook's embeddings with the discrete policy's representation during training. On challenging hybrid action benchmarks, CHDP outperforms the state-of-the-art method by up to $19.3\%$ in success rate.

</details>


### [15] [DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation](https://arxiv.org/abs/2601.05746)
*Zhenghao Li,Zhi Zheng,Wei Chen,Jielun Zhao,Yong Chen,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 다이나믹 멀티 에이전트 디베이트(DynaDebate)를 통해 멀티 에이전트 시스템(MAS)의 토론 효과를 향상시키는 연구.


<details>
  <summary>Details</summary>
Motivation: 멀티 에이전트 시스템의 논의 효과를 개선하고 오류의 중복을 방지하기 위해 새로운 접근 방식을 제시합니다.

Method: DynaDebate는 세 가지 주요 메커니즘(동적 경로 생성 및 할당, 프로세스 중심 토론, 트리거 기반 검증 에이전트)을 통해 멀티 에이전트 토론의 효과를 향상시킵니다.

Result: DynaDebate는 여러 벤치마크에서 기존 MAD 방법보다 우수한 성능을 달성함을 입증했습니다.

Conclusion: 이 연구는 동적으로 진화하는 경로와 협력을 통해 멀티 에이전트 시스템의 토론 능력을 크게 향상시킵니다.

Abstract: Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.

</details>


### [16] [From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation](https://arxiv.org/abs/2601.05787)
*Zezhou Wang,Ziyun Zhang,Xiaoyi Zhang,Zhuzhong Qian,Yan Lu*

Main category: cs.AI

TL;DR: 이 논문은 강화 학습을 통해 작은 풀의 전문가 궤적을 활용하여 end-to-end 정책을 훈련시키는 방법을 제안하며, BEPA라는 새로운 모델을 통해 성과를 개선한다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델이 데스크탑과 브라우저를 운영하는 컴퓨터 사용 에이전트로 점점 더 많이 배포됨에 따라, 기존 방법의 한계를 극복하고 고성능을 달성할 필요가 있다.

Method: BEPA(이중 수준 전문가-정책 동화)라는 방법을 제안하여 전문가의 정적 궤적을 기본 정책에 따라 동적으로 업데이트되는 캐시를 통해 정책 정렬 가이드로 전환한다.

Result: BEPA는 OSWorld-Verified에서 UITARS1.5-7B의 성공률을 22.87%에서 32.13%로 개선하고, 보류된 분할의 성공률을 5.74%에서 10.30%로 높였다.

Conclusion: 제안된 BEPA 모델은 전문 지식의 활용을 극대화하여 귀납적 전이 학습을 향상시킨다.

Abstract: Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git

</details>


### [17] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner는 계층적 다중 에이전트 프레임워크로, 명시적인 메모리 제어를 통해 장기적인 협업에서의 문제를 해결합니다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델 기반 다중 에이전트 시스템은 복잡한 작업에서 강력한 잠재력을 보여주지만, 중앙 집권적 구조는 메모리 관리 부족으로 인한 협업 불안정성 문제를 갖고 있습니다.

Method: StackPlanner는 고수준 조정을 하위 작업 실행과 분리하고, 구조화된 경험 기억과 강화 학습을 통해 재사용 가능한 협동 경험을 학습하여 작업 수준의 메모리를 적극적으로 제어합니다.

Result: 다수의 깊은 검색 및 에이전트 시스템 벤치마크를 통한 실험으로 우리의 접근 방식이 안정적인 장기 다중 에이전트 협업을 가능하게 함을 입증했습니다.

Conclusion: 우리의 연구는 StackPlanner가 다중 에이전트 시스템의 메모리 비효율성과 협업 경험 재사용의 문제를 해결함으로써 협업의 잠재력을 극대화할 수 있음을 보여줍니다.

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [18] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: TowerMind는 RTS 게임의 타워 방어 하위 장르를 기반으로 한 새로운 환경으로, LLM 평가를 위한 저전력 요구 및 다중 모드 관찰 공간을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 최근 LLM의 발전은 장기 계획 및 의사결정을 다양한 시나리오와 작업에 적응하는 핵심 기능으로 부각시키고 있습니다. RTS 게임은 이러한 두 가지 능력을 평가하기에 이상적입니다.

Method: TowerMind는 타워 방어( TD) 하위 장르를 기반으로 하여 저전력 요구와 픽셀 기반, 텍스트, 구조화된 게임 상태 표현을 포함한 다중 모드 관찰 공간을 제공합니다.

Result: 연구 결과, 다양한 다중 모드 입력 설정에서 LLM과 인간 전문가 간의 성능 차이가 뚜렷하게 나타났으며, LLM 행동의 주요 한계도 드러났습니다.

Conclusion: TowerMind는 기존의 RTS 게임 환경을 보완하며 AI 에이전트 분야의 새로운 벤치마크를 제시합니다. 소스 코드는 GitHub에서 공개됩니다.

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [19] [A Survey of Agentic AI and Cybersecurity: Challenges, Opportunities and Use-case Prototypes](https://arxiv.org/abs/2601.05293)
*Sahaya Jestus Lazer,Kshitiz Aryal,Maanak Gupta,Elisa Bertino*

Main category: cs.CR

TL;DR: Agentic AI는 사고, 계획, 행동 및 적응 능력을 갖춘 시스템으로의 전환을 나타내며, 사이버 보안에 미치는 영향을 조사한다.


<details>
  <summary>Details</summary>
Motivation: Agentic AI의 발전과 사이버 보안 분야에서의 응용 가능성을 탐색하기 위해.

Method: 기존 모델에 대한 에이전시 AI의 이중 용도 역학을 조사하고 적합한 위협 모델, 보안 프레임워크 및 평가 파이프라인을 분석했다.

Result: 에이전트의 공모, 연쇄 실패, 감독 회피 및 기억 오염을 포함한 체계적 위험이 도출되었다.

Conclusion: 에이전시 AI는 사이버 보안에서의 기존 메커니즘에 대한 새로운 도전 과제를 제시하며, 실제 사례를 통해 설계 선택이 신뢰성 및 안전성에 미치는 영향을 보여준다.

Abstract: Agentic AI marks an important transition from single-step generative models to systems capable of reasoning, planning, acting, and adapting over long-lasting tasks. By integrating memory, tool use, and iterative decision cycles, these systems enable continuous, autonomous workflows in real-world environments. This survey examines the implications of agentic AI for cybersecurity. On the defensive side, agentic capabilities enable continuous monitoring, autonomous incident response, adaptive threat hunting, and fraud detection at scale. Conversely, the same properties amplify adversarial power by accelerating reconnaissance, exploitation, coordination, and social-engineering attacks. These dual-use dynamics expose fundamental gaps in existing governance, assurance, and accountability mechanisms, which were largely designed for non-autonomous and short-lived AI systems. To address these challenges, we survey emerging threat models, security frameworks, and evaluation pipelines tailored to agentic systems, and analyze systemic risks including agent collusion, cascading failures, oversight evasion, and memory poisoning. Finally, we present three representative use-case implementations that illustrate how agentic AI behaves in practical cybersecurity workflows, and how design choices shape reliability, safety, and operational effectiveness.

</details>


### [20] [Memory Poisoning Attack and Defense on Memory Based LLM-Agents](https://arxiv.org/abs/2601.05504)
*Balachandra Devarangadi Sunil,Isheeta Sinha,Piyush Maheshwari,Shantanu Todmal,Shreyan Malik,Shuchi Mishra*

Main category: cs.CR

TL;DR: 이 논문은 지속적 메모리를 갖춘 대형 언어 모델 에이전트가 메모리 오염 공격에 취약하다는 점을 다루고, 실제 환경에서 공격의 강건성과 효과적인 방어 메커니즘의 부족한 연구를 채우기 위한 실증 평가를 수행하였다.


<details>
  <summary>Details</summary>
Motivation: ME모델 에이전트의 메모리 오염 공격 및 방어의 강건성을 연구하고, 효과적인 방어 메커니즘을 개발하려는 필요성.

Method: EHR 에이전트에서 메모리 오염 공격 및 방어에 대한 체계적인 실증 평가를 수행하고, 초기 메모리 상태, 지시 프롬프트 수, 검색 매개변수 등 세 가지 주요 변수에 따라 공격 강건성을 조사한다.

Result: 모델을 사용한 실험 결과, 기존의 합법적인 메모리가 있는 경우 공격의 효과성이 크게 감소함을 보여준다. 또한 두 가지 새로운 방어 메커니즘을 제안하고 평가한다.

Conclusion: 효과적인 메모리 위생을 위해서는 신뢰 임계값 조정이 중요하며, 이러한 결과는 메모리 증강 LLM 에이전트를 보호하기 위한 중요한 통찰력을 제공한다.

Abstract: Large language model agents equipped with persistent memory are vulnerable to memory poisoning attacks, where adversaries inject malicious instructions through query only interactions that corrupt the agents long term memory and influence future responses. Recent work demonstrated that the MINJA (Memory Injection Attack) achieves over 95 % injection success rate and 70 % attack success rate under idealized conditions. However, the robustness of these attacks in realistic deployments and effective defensive mechanisms remain understudied. This work addresses these gaps through systematic empirical evaluation of memory poisoning attacks and defenses in Electronic Health Record (EHR) agents. We investigate attack robustness by varying three critical dimensions: initial memory state, number of indication prompts, and retrieval parameters. Our experiments on GPT-4o-mini, Gemini-2.0-Flash and Llama-3.1-8B-Instruct models using MIMIC-III clinical data reveal that realistic conditions with pre-existing legitimate memories dramatically reduce attack effectiveness. We then propose and evaluate two novel defense mechanisms: (1) Input/Output Moderation using composite trust scoring across multiple orthogonal signals, and (2) Memory Sanitization with trust-aware retrieval employing temporal decay and pattern-based filtering. Our defense evaluation reveals that effective memory sanitization requires careful trust threshold calibration to prevent both overly conservative rejection (blocking all entries) and insufficient filtering (missing subtle attacks), establishing important baselines for future adaptive defense mechanisms. These findings provide crucial insights for securing memory-augmented LLM agents in production environments.

</details>


### [21] [Continual Pretraining on Encrypted Synthetic Data for Privacy-Preserving LLMs](https://arxiv.org/abs/2601.05635)
*Honghao Liu,Xuhui Jiang,Chengjin Xu,Cehao Yang,Yiran Cheng,Lionel Ni,Jian Guo*

Main category: cs.CR

TL;DR: 민감한 데이터를 보호하면서 대규모 언어 모델을 지속적으로 사전 훈련하기 위한 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 민감한 데이터의 개인정보 보호와 특정 도메인 데이터셋을 사용한 큰 언어 모델 훈련 간의 도전 과제를 해결하고자 합니다.

Method: 개인식별정보(PII)를 보호하기 위해 암호화된 훈련 데이터를 합성하는 엔티티 기반 프레임워크를 제안하며, 가중치가 부여된 엔티티 그래프를 통해 데이터 합성을 유도하고 PII 엔티티에 결정론적 암호화를 적용합니다.

Result: 제한된 규모의 데이터셋에서 사전 훈련된 모델이 기본 모델보다 더 뛰어난 성능을 보이며 PII 보안을 보장하고, 비암호화된 합성 데이터로 훈련된 모델에 비해 성능 차이가 보입니다.

Conclusion: 이 연구는 개인정보 보호를 위한 암호화된 데이터 사전 훈련의 설계 공간에 대한 초기 조사를 제공합니다.

Abstract: Preserving privacy in sensitive data while pretraining large language models on small, domain-specific corpora presents a significant challenge. In this work, we take an exploratory step toward privacy-preserving continual pretraining by proposing an entity-based framework that synthesizes encrypted training data to protect personally identifiable information (PII). Our approach constructs a weighted entity graph to guide data synthesis and applies deterministic encryption to PII entities, enabling LLMs to encode new knowledge through continual pretraining while granting authorized access to sensitive data through decryption keys. Our results on limited-scale datasets demonstrate that our pretrained models outperform base models and ensure PII security, while exhibiting a modest performance gap compared to models trained on unencrypted synthetic data. We further show that increasing the number of entities and leveraging graph-based synthesis improves model performance, and that encrypted models retain instruction-following capabilities with long retrieved contexts. We discuss the security implications and limitations of deterministic encryption, positioning this work as an initial investigation into the design space of encrypted data pretraining for privacy-preserving LLMs. Our code is available at https://github.com/DataArcTech/SoE.

</details>


### [22] [VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit](https://arxiv.org/abs/2601.05755)
*Junda Lin,Zhaomeng Zhou,Zhi Zheng,Shuochen Liu,Tong Xu,Yong Chen,Enhong Chen*

Main category: cs.CR

TL;DR: 본 논문에서는 LLM 에이전트의 간접 프롬프트 삽입 문제를 해결하기 위해 VIGIL 프레임워크를 제안하며, SIREN 벤치마크를 통해 성능을 입증하였다.


<details>
  <summary>Details</summary>
Motivation: 점점 더 많은 위험이 있는 툴 스트림에서의 간접 프롬프트 삽입 문제를 해결하는 것이 필요하다.

Method: VIGIL 프레임워크는 제한적인 격리에서 검증-후-커밋 프로토콜로 패러다임을 전환하며, SIREN 벤치마크를 사용하여 동적 의존성을 가진 공격 사례를 시뮬레이션한다.

Result: VIGIL은 최신 동적 방어 체계보다 22% 이상 공격 성공률을 낮추고, 정적 기초선 대비 공격 중 유틸리티를 두 배 이상 증가시켰다.

Conclusion: VIGIL은 안전성과 유틸리티 간의 최적의 균형을 달성하였다.

Abstract: LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.

</details>


### [23] [Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense](https://arxiv.org/abs/2601.05887)
*Víctor Mayoral-Vilches,María Sanz-Gómez,Francesco Balassone,Stefan Rass,Lidia Salas-Espejo,Benjamin Jablonski,Luis Javier Navarrete-Lozano,Maite del Mundo de Torres,Cristóbal R. J. Veas Chavez*

Main category: cs.CR

TL;DR: AI 기반 침투 테스트는 시간당 수천 가지 행동을 수행하지만, 경쟁 보안에서 인간이 적용하는 전략적 직관이 부족하다. 따라서 이를 보완하기 위해 G-CTR을 개발하였다.


<details>
  <summary>Details</summary>
Motivation: AI 기반 침투 테스트의 전략적 직관 부족 문제 해결.

Method: G-CTR은 공격 그래프를 추출하고, 내쉬 균형을 계산하며, LLM 루프에 피드백하는 게임 이론 기반의 가이던스 레이어이다.

Result: G-CTR은 전문가 그래프 구조와 70~90% 일치하며, 수동 분석보다 60~245배 빠르고 140배 저렴하다.

Conclusion: G-CTR의 폐쇄 루프 가이던스는 모호성을 줄이고, LLM의 탐색 공간을 축소시키며, 성공률, 일관성 및 신뢰성 향상에 기여한다.

Abstract: AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence --Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop \emph{guiding} the agent's actions. Across five real-world exercises, G-CTR matches 70--90% of expert graph structure while running 60--245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0% to 42.9%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.

</details>


### [24] [Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset](https://arxiv.org/abs/2601.05918)
*Tianshi Li*

Main category: cs.CR

TL;DR: 이 논문은 현대 LLM 기반의 에이전트가 과학자들의 인터뷰를 기반으로 특정 연구 논문과 연결할 수 있는 방법을 제시하며, 이는 재식별 공격을 용이하게 만든다는 것을 보여준다.


<details>
  <summary>Details</summary>
Motivation: AI를 연구에 활용하는 과학자들에 대한 질적 인터뷰를 기술하고, 이를 기반으로 재식별 공격에 대한 우려를 제기하기 위해.

Method: 널리 사용 가능한 대형 언어 모델(LLM)과 웹 검색 및 에이전트 능력을 활용하여 인터뷰를 특정 과학적 작업과 연결.

Result: 24개의 인터뷰 중 6개를 특정 연구 작업에 연결하고, 관련된 저자와 인터뷰이를 식별할 수 있었다.

Conclusion: 현대 LLM 기반의 에이전트는 재식별 공격을 용이하게 만들며, 이를 통해 고유한 데이터를 공개할 때의 위험성을 제기하고, 그에 대한 완화 조치를 제안한다.

Abstract: On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting](https://arxiv.org/abs/2601.05353)
*Shovito Barua Soumma,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GlyRAG는 CGM 데이터를 활용하여 혈당 예측의 정확성을 높이는 새로운 예측 모델이다.


<details>
  <summary>Details</summary>
Motivation: 혈당 예측의 정확성을 높여 당뇨병 관리의 능동적 접근을 가능하게 하기 위해 고민하고 있다.

Method: GlyRAG는 CGM 데이터에서 혈당 동적 변화의 의미론적 이해를 직접 추출하는 컨텍스트 기반 예측 프레임워크를 제안한다.

Result: GlyRAG는 최신 방법들보다 RMSE를 최대 39% 낮추고, 예측의 85%를 안전 영역에 놓이게 하여 당 뇌졸중 예측의 51% 개선을 달성한다.

Conclusion: GlyRAG는 추가 센서 없이 CGM 데이터에 대한 LLM 기반의 컨텍스트화 및 검색을 통해 장기 혈당 예측의 정확성과 임상 신뢰성을 향상시킬 수 있다.

Abstract: Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.

</details>


### [26] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: HINT라는 새로운 KD 프레임워크를 통해 중앙 집중형 교육으로 디센트럴라이즈드 학생을 위한 효과적인 MARL을 지원한다.


<details>
  <summary>Details</summary>
Motivation: KD를 통해 MARL을 가속화할 수 있으나 복잡한 도메인에서의 교수 정책 합성, OOD 상태에서의 추론, 관찰 공간의 불일치와 같은 주요 병목 현상이 있다.

Method: HINT는 계층적 RL을 활용하여 스케일이 가능하고 고성능의 교사 정책을 제공하며, 의사 비정책 RL을 통해 교사와 학생 경험을 모두 사용하여 정책을 업데이트한다.

Result: HINT는 FireCommander 및 MARINE과 같은 협력적 도메인에서 평가되었으며, 기준선보다 60%에서 165% 향상된 성공률을 달성하였다.

Conclusion: HINT는 중앙 집중형 교육이 가능한 MARL을 위한 효과적인 KD 프레임워크로, 성능 기반 필터링을 통해 관찰 불일치를 줄인다.

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [27] [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)
*Yiqun T Chen,Sizhu Lu,Sijia Li,Moran Guo,Shengyi Li*

Main category: cs.LG

TL;DR: 본 논문은 LLM을 평가자로 사용할 때 발생하는 오차를 수정하기 위한 두 가지 방법론을 비교 분석한다.


<details>
  <summary>Details</summary>
Motivation: LLM이 생성 AI 출력의 자동 평가자로 증가하는 사용에 따라, 데이터의 진실성을 정확하게 판단하는 데 있어 발생하는 체계적인 오류를 바로잡는 것이 필요하다.

Method: 두 가지 접근 방식인 측정오류 수정 및 대체 결과 접근을 통해 평균 매개변수 추정의 성능을 분석하고, 효율적인 영향 함수 기반 추정기를 도출하여 두 클래스의 추정기를 통합하였다.

Result: PPI 스타일 추정기가 측정오류 수정을 통해 더 작은 비대칭 분산을 달성하는 조건을 규명하고, 이론적 결과를 시뮬레이션으로 검증하였다.

Conclusion: 우리는 제안된 방법의 구현 및 비교 유틸리티를 제공하며, 실제 데이터 예제에서 방법을 시연하였다.

Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.

</details>


### [28] [Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning](https://arxiv.org/abs/2601.05474)
*Pingchuan Ma,Qixin Zhang,Shuai Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: 이 논문에서는 차별 가능 인과 발견 파이프라인을 향상시키기 위한 새로운 방법 ALVGL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 인과 발견 방법의 정확성과 효율성을 개선하기 위한 필요성.

Method: ALVGL은 희소 및 저랭크 분해를 사용하여 데이터의 정밀 행렬을 학습하고, 이를 최적화하기 위해 ADMM 절차를 설계합니다.

Result: ALVGL이 다양한 구조적 인과 모델에서 최신 기술을 초과하는 정확도와 최적화 효율성을 달성함을 보여줍니다.

Conclusion: ALVGL은 차별 가능 인과 발견을 위한 신뢰할 수 있고 효과적인 솔루션입니다.

Abstract: Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.
  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.
  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.

</details>


### [29] [Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection](https://arxiv.org/abs/2601.05501)
*Feihu Jin,Ying Tan*

Main category: cs.LG

TL;DR: Hi-ZFO는 LLM의 미세 조정을 위한 효과적인 하이브리드 최적화 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 기존의 1차 최적화 방법은 일반화 성능이 떨어지는 샤프한 최소값으로 훈련을 유도하며, 0차 방법은 느린 수렴 속도를 보인다.

Method: Hi-ZFO는 계층별 중요도 프로파일링을 통해 모델을 적응적으로 분할하고, 중요한 층에는 정확한 1차 업데이트를 적용하며, 덜 민감한 층에는 0차 최적화를 활용한다.

Result: Hi-ZFO는 다양한 생성, 수학, 코드 추론 작업에서 일관된 성능 향상과 함께 훈련 시간을 현저히 단축시킨다.

Conclusion: 계층적 하이브리드 최적화 방법이 LLM 미세 조정에 효과적임을 보여준다.

Abstract: Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.

</details>


### [30] [Continual Learning of Achieving Forgetting-free and Positive Knowledge Transfer](https://arxiv.org/abs/2601.05623)
*Zhi Wang,Zhongbin Wu,Yanni Li,Bing Liu,Guangxi Li,Yuping Wang*

Main category: cs.LG

TL;DR: 본 논문은 지속적 학습에서 기억 소실 문제를 극복하고 지식 전이를 촉진하는 새로운 ETCL 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 지속적 학습 에이전트는 기억 소실을 극복함과 동시에 긍정적인 지식 전이를 촉진해야 한다.

Method: 논문에서는 CL을 최적화 문제로 모델링하고, ETCL 방법을 통해 기억 소실 없는 긍정적인 지식 전이를 달성한다.

Result: 평가 결과 ETCL은 강력한 기준선보다 우수한 성능을 나타낸다.

Conclusion: ETCL은 비슷한 과제, 유사한 과제 및 혼합 과제 시퀀스에서 뛰어난 성능을 보인다.

Abstract: Existing research on continual learning (CL) of a sequence of tasks focuses mainly on dealing with catastrophic forgetting (CF) to balance the learning plasticity of new tasks and the memory stability of old tasks. However, an ideal CL agent should not only be able to overcome CF, but also encourage positive forward and backward knowledge transfer (KT), i.e., using the learned knowledge from previous tasks for the new task learning (namely FKT), and improving the previous tasks' performance with the knowledge of the new task (namely BKT). To this end, this paper first models CL as an optimization problem in which each sequential learning task aims to achieve its optimal performance under the constraint that both FKT and BKT should be positive. It then proposes a novel Enhanced Task Continual Learning (ETCL) method, which achieves forgetting-free and positive KT. Furthermore, the bounds that can lead to negative FKT and BKT are estimated theoretically. Based on the bounds, a new strategy for online task similarity detection is also proposed to facilitate positive KT. To overcome CF, ETCL learns a set of task-specific binary masks to isolate a sparse sub-network for each task while preserving the performance of a dense network for the task. At the beginning of a new task learning, ETCL tries to align the new task's gradient with that of the sub-network of the previous most similar task to ensure positive FKT. By using a new bi-objective optimization strategy and an orthogonal gradient projection method, ETCL updates only the weights of previous similar tasks at the classification layer to achieve positive BKT. Extensive evaluations demonstrate that the proposed ETCL markedly outperforms strong baselines on dissimilar, similar, and mixed task sequences.

</details>


### [31] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: Tiny Recursive Models (TRM)은 Abstraction and Reasoning Corpus (ARC) 스타일 작업을 해결하기 위한 대규모 언어 모델에 대한 파라미터 효율적인 대안으로 제안되었다. 본 연구에서는 TRM의 성능이 아키텍처, 테스트 시간 컴퓨팅, 작업 특정 사전 지식에서 얼마나 기인하는지를 분석했다.


<details>
  <summary>Details</summary>
Motivation: TRM의 성능 원천을 이해하고자 함.

Method: ARC Prize TRM 체크포인트를 ARC-AGI-1에서 실증적으로 분석하고, 네 가지 행동 발견 및 효율성 비교를 보고한다.

Result: 테스트 시간 증강과 다수결 앙상블이 중요한 성과 비율을 차지함을 보여주었으며, 과제 식별자에 대한 엄격한 의존성과 초기 재귀 단계에서의 높은 정확도를 발견했다.

Conclusion: TRM의 성능은 깊은 내부 추론보다는 효율성과 작업 특정 조건, 그리고 공격적인 테스트 시간 컴퓨팅의 상호 작용에서 비롯된다.

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>
