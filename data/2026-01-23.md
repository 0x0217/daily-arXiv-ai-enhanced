<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.LG](#cs.LG) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Turn-Based Structural Triggers: Prompt-Free Backdoors in Multi-Turn LLMs](https://arxiv.org/abs/2601.14340)
*Yiyang Lu,Jinwen He,Yue Zhao,Kai Chen,Ruigang Liang*

Main category: cs.CR

TL;DR: 대화 구조를 기반으로 한 백도어 공격을 제안하며, 이 공격은 사용자의 입력과 독립적으로 작동하여 높은 성공률을 보인다.


<details>
  <summary>Details</summary>
Motivation: 대화형 시스템에 통합된 대형 언어 모델(LLM)의 공급망 위험에 대한 경각심을 제기하고, 대화 구조가 중요한 공격 표면임을 강조한다.

Method: 대화 구조에서 활성화되는 Turn-based Structural Trigger (TST) 백도어 공격을 제안하며, 턴 인덱스를 트리거로 사용한다.

Result: TST는 네 개의 널리 사용되는 오픈 소스 LLM 모델에서 평균 공격 성공률(ASR) 99.52%를 달성하며, 최소한의 유틸리티 저하로 다섯 가지 대표적인 방어 아래에서도 효과적이다.

Conclusion: 대화 구조가 다중 턴 LLM 시스템에서 중요한 공격 표면임을 제시하며, 이를 바탕으로 구조 인식 감사 및 완화의 필요성을 제기한다.

Abstract: Large Language Models (LLMs) are widely integrated into interactive systems such as dialogue agents and task-oriented assistants. This growing ecosystem also raises supply-chain risks, where adversaries can distribute poisoned models that degrade downstream reliability and user trust. Existing backdoor attacks and defenses are largely prompt-centric, focusing on user-visible triggers while overlooking structural signals in multi-turn conversations. We propose Turn-based Structural Trigger (TST), a backdoor attack that activates from dialogue structure, using the turn index as the trigger and remaining independent of user inputs. Across four widely used open-source LLM models, TST achieves an average attack success rate (ASR) of 99.52% with minimal utility degradation, and remains effective under five representative defenses with an average ASR of 98.04%. The attack also generalizes well across instruction datasets, maintaining an average ASR of 99.19%. Our results suggest that dialogue structure constitutes an important and under-studied attack surface for multi-turn LLM systems, motivating structure-aware auditing and mitigation in practice.

</details>


### [2] [Rethinking On-Device LLM Reasoning: Why Analogical Mapping Outperforms Abstract Thinking for IoT DDoS Detection](https://arxiv.org/abs/2601.14343)
*William Pan,Guiran Liu,Binrong Zhu,Qun Wang,Yingzhou Lu,Beiyu Lin,Rose Qingyang Hu*

Main category: cs.CR

TL;DR: IoT 환경에서의 DDoS 공격 탐지를 위한 새로운 프레임워크를 제시하며, 소형 ODLLM의 성능 향상을 보여준다.


<details>
  <summary>Details</summary>
Motivation: IoT 배포의 급속한 확장은 사이버 보안 위협, 특히 정교해지는 DDoS 공격을 증가시켰다.

Method: Chain-of-Thought(CoT) 추론과 Retrieval-Augmented Generation(RAG)을 통합한 탐지 프레임워크를 도입하고, 소형 ODLLMs인 LLaMA 3.2 및 Gemma 3를 평가했다.

Result: Few-shot prompting을 통해 매크로 평균 F1 스코어 0.85에 이르는 성능 향상을 확인했다.

Conclusion: CoT와 RAG 접근 방식이 소형 ODLLMs의 복잡한 네트워크 공격 분류 능력을 현저히 향상시킴을 보여주었다.

Abstract: The rapid expansion of IoT deployments has intensified cybersecurity threats, notably Distributed Denial of Service (DDoS) attacks, characterized by increasingly sophisticated patterns. Leveraging Generative AI through On-Device Large Language Models (ODLLMs) provides a viable solution for real-time threat detection at the network edge, though limited computational resources present challenges for smaller ODLLMs. This paper introduces a novel detection framework that integrates Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), tailored specifically for IoT edge environments. We systematically evaluate compact ODLLMs, including LLaMA 3.2 (1B, 3B) and Gemma 3 (1B, 4B), using structured prompting and exemplar-driven reasoning strategies. Experimental results demonstrate substantial performance improvements with few-shot prompting, achieving macro-average F1 scores as high as 0.85. Our findings highlight the significant advantages of incorporating exemplar-based reasoning, underscoring that CoT and RAG approaches markedly enhance small ODLLMs' capabilities in accurately classifying complex network attacks under stringent resource constraints.

</details>


### [3] [Uma Prova de Conceito para a Verificação Formal de Contratos Inteligentes](https://arxiv.org/abs/2601.14427)
*Murilo de Souza Neves,Adilson Luiz Bonifacio*

Main category: cs.CR

TL;DR: 스마트 계약의 불변성은 배포 후 오류 수정을 복잡하게 만들며, 사전 검증 계층의 필요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 스마트 계약은 전통적인 계약보다 보안이 강화된 자기 실행 능력을 가진 도구이지만, 불변성으로 인해 배포 후 오류 수정을 매우 복잡하게 만든다.

Method: Relativized Contract Language (RCL)와 RECALL 도구를 사용하여 여러 에이전트가 관여하는 구매 및 판매 계약의 명세화 및 검증을 수행했다.

Result: 모델링 단계에서 규범적 충돌을 탐지할 수 있는 도구의 능력을 입증했다.

Conclusion: 논리적 불일치를 수정한 후 계약을 Solidity로 번역하고 Remix IDE 환경에서 기능적으로 검증하여 사전 형식 검증이 최종 코드의 신뢰성과 보안을 보장하는 데 중요함을 확인했다.

Abstract: Smart contracts are tools with self-execution capabilities that provide enhanced security compared to traditional contracts; however, their immutability makes post-deployment fault correction extremely complex, highlighting the need for a verification layer prior to this stage. Although formalisms such as Contract Language (CL) enable logical analyses, they prove limited in attributing responsibilities within complex multilateral scenarios. This work presents a proof of concept using the Relativized Contract Language (RCL) and the RECALL tool for the specification and verification of a purchase and sale contract involving multiple agents. The study demonstrates the tool's capability to detect normative conflicts during the modeling phase. After correcting logical inconsistencies, the contract was translated into Solidity and functionally validated within the Remix IDE environment, confirming that prior formal verification is fundamental to ensuring the reliability and security of the final code.

</details>


### [4] [AI Agents vs. Human Investigators: Balancing Automation, Security, and Expertise in Cyber Forensic Analysis](https://arxiv.org/abs/2601.14544)
*Sneha Sudhakaran,Naresh Kshetri*

Main category: cs.CR

TL;DR: AI는 디지털 포렌식에 적용되지만, 고유의 위험성이 존재하여 인간 조사관의 역할이 필수적이다.


<details>
  <summary>Details</summary>
Motivation: 사이버 위협의 진화로 인해 사이버 포렌식 분석의 신뢰성이 중요해지고 있다.

Method: 일반적으로 사용되는 AI 에이전트인 ChatGPT와 인간 포렌식 조사관의 효과성을 비교 분석하였다.

Result: AI 기반 접근 방식에서의 한계를 발견하고, 인간 조사관이 미세한 이상 및 위협을 효과적으로 식별함을 보여주었다.

Conclusion: AI는 효율성을 높이지만, 인간의 감독이 결과의 정확성과 포괄성을 보장하는 데 여전히 중요하다.

Abstract: In an era where cyber threats are rapidly evolving, the reliability of cyber forensic analysis has become increasingly critical for effective digital investigations and cybersecurity responses. AI agents are being adopted across digital forensic practices due to their ability to automate processes such as anomaly detection, evidence classification, and behavioral pattern recognition, significantly enhancing scalability and reducing investigation timelines. However, the characteristics that make AI indispensable also introduce notable risks. AI systems, often trained on biased or incomplete datasets, can produce misleading results, including false positives and false negatives, thereby jeopardizing the integrity of forensic investigations. This study presents a meticulous comparative analysis of the effectiveness of the most used AI agent, ChatGPT, and human forensic investigators in the realm of cyber forensic analysis. Our research reveals critical limitations within AI-driven approaches, demonstrating scenarios in which sophisticated or novel cyber threats remain undetected due to the rigid pattern-based nature of AI systems. Conversely, our analysis highlights the crucial role that human forensic investigators play in mitigating these risks. Through adaptive decision-making, ethical reasoning, and contextual understanding, human investigators effectively identify subtle anomalies and threats that may evade automated detection systems. To reinforce our findings, we conducted comprehensive reliability testing of forensic techniques using multiple cyber threat scenarios. These tests confirmed that while AI agents significantly improve the efficiency of routine analyses, human oversight remains crucial in ensuring accuracy and comprehensiveness of the results.

</details>


### [5] [WebAssembly Based Portable and Secure Sensor Interface for Internet of Things](https://arxiv.org/abs/2601.14555)
*Botong Ou,Baijian Yang*

Main category: cs.CR

TL;DR: IoT 연결이 증가함에 따라 보안과 프라이버시 문제가 대두되고 있으며, 본 논문은 안전하고 휴대성이 뛰어난 센서 인터페이스를 위한 최초의 WebAssembly 시스템 인터페이스(WASI) 확장을 도입합니다.


<details>
  <summary>Details</summary>
Motivation: IoT 연결의 확장은 삶의 질을 향상시키지만, 프라이버시와 보안 문제를 증가시킵니다.

Method: 본 연구는 WebAssembly 시스템 인터페이스(WASI) 확장을 통해 안전하고 휴대 가능한 멀티 테넌트 접근을 제공하는 샌드박스를 구현하였습니다.

Result: 우리는 센서 접근 지연이 6% 늘어나고 추가 메모리 사용량이 5% 증가함을 확인했습니다. MQTT-SN 요청의 경우, 네트워크 지연에 의해 지배되며, WASI-SN 구현은 1% 미만의 추가 지연을 유도합니다.

Conclusion: WASI를 통한 MQTT-SN 구현은 안전성과 성능을 동시에 제공하여 다양한 임베디드 장치에서의 데이터 접근을 가능하게 합니다.

Abstract: As the expansion of IoT connectivity continues to provide quality-of-life improvements around the world, they simultaneously introduce increasing privacy and security concerns. The lack of a clear definition in managing shared and protected access to IoT sensors offer channels by which devices can be compromised and sensitive data can be leaked. In recent years, WebAssembly has received considerable attention for its efficient application sandboxing suitable for embedded systems, making it a prime candidate for exploring a secure and portable sensor interface. This paper introduces the first WebAssembly System Interface (WASI) extension offering a secure, portable, and low-footprint sandbox enabling multi-tenant access to sensor data across heterogeneous embedded devices. The runtime extensions provide application memory isolation, ensure appropriate resource privileges by intercepting sensor access, and offer an MQTT-SN interface enabling in-network access control. When targeting the WebAssembly byte-code with the associated runtime extensions implemented atop the Zephyr RTOS, our evaluation of sensor access indicates a latency overhead of 6% with an additional memory footprint of 5% when compared to native execution. As MQTT-SN requests are dominated by network delays, the WASI-SN implementation of MQTT-SN introduces less than 1% additional latency with similar memory footprint.

</details>


### [6] [Holmes: An Evidence-Grounded LLM Agent for Auditable DDoS Investigation in Cloud Networks](https://arxiv.org/abs/2601.14601)
*Haodong Chen,Ziheng Zhang,Jinghui Jiang,Qiang Su,Qiao Xiang*

Main category: cs.CR

TL;DR: Holmes는 클라우드 환경에서 DDoS 공격을 탐지하고 대응하기 위한 LLM 기반의 탐지 에이전트로, 기존의 방식과 달리 가시적이고 구조적인 증거를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 클라우드 환경은 중앙 집중식 자원과 넓은 공격 표면으로 인해 빈번한 DDoS 위협에 직면해 있으며, 현대 클라우드 네이티브 DDoS 공격은 다중 벡터 전략을 혼합하여 방어자들에게 신속한 모니터링과 설명 가능한 귀속성을 요구하는 운영상의 딜레마를 초래한다.

Method: Holmes는 가상 SRE 조사관으로서 모델을 재구성하고, 연속 감지 및 분류를 위한 계층적 워크플로우와 이진 패킷을 구조화된 증거로 변환하는 증거 팩 추상화를 결합한다.

Result: Holmes는 다양한 공격 패밀리에서 중요한 증거에 기반한 귀속 결정을 생성하며, 오류 발생 시 감사 로그가 실패의 출처를 쉽게 찾아낼 수 있도록 돕는다.

Conclusion: Holmes는 클라우드 운영에서 비용 통제 및 추적 가능한 DDoS 조사를 위한 LLM 에이전트의 실용성을 입증하였다.

Abstract: Cloud environments face frequent DDoS threats due to centralized resources and broad attack surfaces. Modern cloud-native DDoS attacks further evolve rapidly and often blend multi-vector strategies, creating an operational dilemma: defenders need wire-speed monitoring while also requiring explainable, auditable attribution for response. Existing rule-based and supervised-learning approaches typically output black-box scores or labels, provide limited evidence chains, and generalize poorly to unseen attack variants; meanwhile, high-quality labeled data is often difficult to obtain in cloud settings.
  We present Holmes (DDoS Detective), an LLM-based DDoS detection agent that reframes the model as a virtual SRE investigator rather than an end-to-end classifier. Holmes couples a funnel-like hierarchical workflow (counters/sFlow for continuous sensing and triage; PCAP evidence collection triggered only on anomaly windows) with an Evidence Pack abstraction that converts binary packets into compact, reproducible, high-signal structured evidence. On top of this evidence interface, Holmes enforces a structure-first investigation protocol and strict JSON/quotation constraints to produce machine-consumable reports with auditable evidence anchors.
  We evaluate Holmes on CICDDoS2019 reflection/amplification attacks and script-triggered flooding scenarios. Results show that Holmes produces attribution decisions grounded in salient evidence anchors across diverse attack families, and when errors occur, its audit logs make the failure source easy to localize, demonstrating the practicality of an LLM agent for cost-controlled and traceable DDoS investigation in cloud operations.

</details>


### [7] [An LLM Agent-based Framework for Whaling Countermeasures](https://arxiv.org/abs/2601.14606)
*Daisuke Miyamoto,Takuji Iimura,Narushige Michishita*

Main category: cs.CR

TL;DR: 최근 생성적 AI의 확산과 함께 Whaling 공격이 심각한 위협으로 대두되고 있다. 본 연구에서는 대학 교수진을 위한 Whaling 대응 체계를 제안한다.


<details>
  <summary>Details</summary>
Motivation: Whaling 공격은 조직 내 권위 있는 인물을 표적으로 하여 정교한 사기 이메일을 사용하는 사회공학적 공격으로, 일본 대학의 교수진이 이러한 표적이 될 수 있다.

Method: 본 연구는 교수진의 공개 정보를 기반으로 취약성 프로필을 구축하고, 해당 프로필에 따라 Whaling 방어와 관련된 잠재적 위험 시나리오를 식별하며, 방어 프로필을 구축하고 Whaling 이메일을 분석하는 LLM 기반 에이전트를 설계한다.

Result: 제안된 방법은 Whaling의 표적이 된 교수진의 작업 맥락과 일치하는 대응 정책의 설명과 함께 판단을 제공할 수 있음을 나타낸다.

Conclusion: 본 연구는 Whaling 공격에 대한 방어 체계의 실용적 도전 과제와 향후 체계적인 평가를 위한 고려 사항을 강조한다.

Abstract: With the spread of generative AI in recent years, attacks known as Whaling have become a serious threat. Whaling is a form of social engineering that targets important high-authority individuals within organizations and uses sophisticated fraudulent emails. In the context of Japanese universities, faculty members frequently hold positions that combine research leadership with authority within institutional workflows. This structural characteristic leads to the wide public disclosure of high-value information such as publications, grants, and detailed researcher profiles. Such extensive information exposure enables the construction of highly precise target profiles using generative AI. This raises concerns that Whaling attacks based on high-precision profiling by generative AI will become prevalent. In this study, we propose a Whaling countermeasure framework for university faculty members that constructs personalized defense profiles and uses large language model (LLM)-based agents. We design agents that (i) build vulnerability profiles for each target from publicly available information on faculty members, (ii) identify potential risk scenarios relevant to Whaling defense based on those profiles, (iii) construct defense profiles corresponding to the vulnerabilities and anticipated risks, and (iv) analyze Whaling emails using the defense profiles. Furthermore, we conduct a preliminary risk-assessment experiment. The results indicate that the proposed method can produce judgments accompanied by explanations of response policies that are consistent with the work context of faculty members who are Whaling targets. The findings also highlight practical challenges and considerations for future operational deployment and systematic evaluation.

</details>


### [8] [Towards Cybersecurity Superintelligence: from AI-guided humans to human-guided AI](https://arxiv.org/abs/2601.14614)
*Víctor Mayoral-Vilches,Stefan Rass,Martin Pinzger,Endika Gil-Uriarte,Unai Ayucar-Carbajo,Jon Ander Ruiz-Alcalde,Maite del Mundo de Torres,Luis Javier Navarrete-Lozano,María Sanz-Gómez,Francesco Balassone,Cristóbal R. J. Veas-Chavez,Vanesa Turiel,Alfonso Glera-Picón,Daniel Sánchez-Prieto,Yuri Salvatierra,Paul Zabalegui-Landa,Ruffino Reydel Cabrera-Álvarez,Patxi Mayoral-Pizarroso*

Main category: cs.CR

TL;DR: 이 논문은 사이버 보안 초지능의 발전을 문서화하며, 세 가지 주요 기여를 통해 AI 보안 분야를 선도한 세 가지 사례를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 사이버 보안의 다음 경계는 인공지능이 최고의 인간 능력을 초월하는 초지능의 발전입니다.

Method: PentestGPT, Cybersecurity AI, Generative Cut-the-Rope의 세 가지 혁신 기술이 제시되었습니다.

Result: 각 기술은 전통적인 모델보다 현저한 개선 및 성능 향상을 보여주었습니다.

Conclusion: 이러한 발전은 인간의 안내를 통해 게임 이론적 사이버 보안 초지능으로 나아가는 명확한 발전 경로를 설정합니다.

Abstract: Cybersecurity superintelligence -- artificial intelligence exceeding the best human capability in both speed and strategic reasoning -- represents the next frontier in security. This paper documents the emergence of such capability through three major contributions that have pioneered the field of AI Security. First, PentestGPT (2023) established LLM-guided penetration testing, achieving 228.6% improvement over baseline models through an architecture that externalizes security expertise into natural language guidance. Second, Cybersecurity AI (CAI, 2025) demonstrated automated expert-level performance, operating 3,600x faster than humans while reducing costs 156-fold, validated through #1 rankings at international competitions including the $50,000 Neurogrid CTF prize. Third, Generative Cut-the-Rope (G-CTR, 2026) introduces a neurosymbolic architecture embedding game-theoretic reasoning into LLM-based agents: symbolic equilibrium computation augments neural inference, doubling success rates while reducing behavioral variance 5.2x and achieving 2:1 advantage over non-strategic AI in Attack & Defense scenarios.
  Together, these advances establish a clear progression from AI-guided humans to human-guided game-theoretic cybersecurity superintelligence.

</details>


### [9] [NeuroFilter: Privacy Guardrails for Conversational LLM Agents](https://arxiv.org/abs/2601.14660)
*Saswat Das,Ferdinando Fioretto*

Main category: cs.CR

TL;DR: 이 논문은 대화형 대규모 언어 모델의 프라이버시를 보장하기 위한 계산적 도전을 다루고 있으며, NeuroFilter라는 새로운 프레임워크를 제안하여 프라이버시 공격을 효과적으로 탐지한다.


<details>
  <summary>Details</summary>
Motivation: 프라이버시는 맥락적 무결성 프레임워크에 의해 규제되며, 기존 방어는 다단계 확인으로 인해 지연과 비용이 증가하여, 다중 턴 상호작용에서 조작되기 쉬워진다.

Method: NeuroFilter는 모델의 활성화 공간에서 규범 위반을 단순한 방향으로 매핑하여 맥락적 무결성을 실현하는 프레임워크이다. 내부 표현과 프라이버시 위반 의도를 분리하는 선형 구조를 사용한다.

Result: 150,000회 이상의 상호작용을 포괄하는 평가에서 NeuroFilter는 프라이버시 공격 탐지에 강력한 성능을 보였으며 무해한 프롬프트에서 0%의 허위 긍정률을 유지하면서 LLM 기반 방어보다 계산 투입 비용을 몇 배 줄였다.

Conclusion: NeuroFilter는 대화 중 발생하는 위협을 감지하기 위해 활성화 속도의 개념을 사용하여 매우 효과적으로 작동한다.

Abstract: This work addresses the computational challenge of enforcing privacy for agentic Large Language Models (LLMs), where privacy is governed by the contextual integrity framework. Indeed, existing defenses rely on LLM-mediated checking stages that add substantial latency and cost, and that can be undermined in multi-turn interactions through manipulation or benign-looking conversational scaffolding. Contrasting this background, this paper makes a key observation: internal representations associated with privacy-violating intent can be separated from benign requests using linear structure. Using this insight, the paper proposes NeuroFilter, a guardrail framework that operationalizes contextual integrity by mapping norm violations to simple directions in the model's activation space, enabling detection even when semantic filters are bypassed. The proposed filter is also extended to capture threats arising during long conversations using the concept of activation velocity, which measures cumulative drift in internal representations across turns. A comprehensive evaluation across over 150,000 interactions and covering models from 7B to 70B parameters, illustrates the strong performance of NeuroFilter in detecting privacy attacks while maintaining zero false positives on benign prompts, all while reducing the computational inference cost by several orders of magnitude when compared to LLM-based agentic privacy defenses.

</details>


### [10] [On Implementing Hybrid Post-Quantum End-to-End Encryption](https://arxiv.org/abs/2601.14926)
*Aditi Gandhi,Aakankshya Das,Aswani Kumar Cherukuri*

Main category: cs.CR

TL;DR: 양자 컴퓨팅의 등장으로 현재의 공개 키 암호 시스템이 위협받고 있으며, 이에 대한 대안으로 혼합형 종단 간 암호화 시스템을 구현하였다.


<details>
  <summary>Details</summary>
Motivation: 양자 컴퓨팅의 출현은 현재의 공개 키 암호 시스템에 대한 근본적인 위협을 제기하고 있다.

Method: 고전 및 포스트-양자 암호 프리미티브를 결합하여 보안성과 효율성을 달성하는 하이브리드 종단 간 암호화 시스템을 구현하였다.

Result: NIST 표준화된 포스트-양자 암호화가 실제 메시징 시스템에 효과적으로 통합될 수 있음을 보여주며, 고전 및 양자 적대 세력에 대한 보호를 제공한다.

Conclusion: 또한, 구현의 초점을 맞추어 오픈 소스 구현체를 제공하여 포스트 양자 안전 통신 시스템에 대한 재현성과 추가 연구를 촉진한다.

Abstract: The emergence of quantum computing poses a fundamental threat to current public key cryptographic systems. This threat is necessitating a transition to quantum resistant cryptographic alternatives in all the applications. In this work, we present the implementation of a practical hybrid end-to-end encryption system that combines classical and post-quantum cryptographic primitives to achieve both security and efficiency. Our system employs CRYSTALS-Kyber, a NIST-standardized lattice-based key encapsulation mechanism, for quantum-safe key exchange, coupled with AES-256-GCM for efficient authenticated symmetric encryption and SHA-256 for deterministic key derivation. The architecture follows a zero-trust model where a relay server facilitates communication without accessing plaintext messages or cryptographic keys. All encryption and decryption operations occur exclusively at client endpoints. The system demonstrates that NIST standardized post-quantum cryptography can be effectively integrated into practical messaging systems with acceptable performance characteristics, offering protection against both classical and quantum adversaries. As our focus is on implementation rather than on novelty, we also provide an open-source implementation to facilitate reproducibility and further research in post quantum secure communication systems.

</details>


### [11] [Interoperable Architecture for Digital Identity Delegation for AI Agents with Blockchain Integration](https://arxiv.org/abs/2601.14982)
*David Ricardo Saavedra*

Main category: cs.CR

TL;DR: 이 논문은 디지털 신원 시스템에서 검증 가능한 위임 문제를 해결하기 위한 통합 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 중앙집중형, 연합형, 자주적인 신원 시스템에서 인적 사용자와 자율 AI 에이전트가 기본 자격 증명이나 개인 키를 노출하지 않고 권한을 행사하고 이전해야 하는 문제에 대한 해결이 필요하다.

Method: 이 프레임워크는 권한 이전을 위한 Delegation Grants(DGs), 정규화된 검증 요청을 위한 Canonical Verification Context(CVC), 신뢰 기반 아키텍처 및 블록체인 앵커링을 옵션으로 제공하는 4가지 주요 요소를 포함한다.

Result: 이 프레임워크는 상호 운영 가능한 위임 및 감사 가능성을 향상시키며 신뢰할 수 있는 디지털 신원 인프라에 자율 에이전트를 통합하는 기반을 마련한다.

Conclusion: 이 연구는 디지털 신원 환경에서 권한 이전을 통제하고 감사 가능한 방안을 제시하며, 표준화 및 구현에 기여할 것이다.

Abstract: Verifiable delegation in digital identity systems remains unresolved across centralized, federated, and self-sovereign identity (SSI) environments, particularly where both human users and autonomous AI agents must exercise and transfer authority without exposing primary credentials or private keys. We introduce a unified framework that enables bounded, auditable, and least-privilege delegation across heterogeneous identity ecosystems. The framework includes four key elements: Delegation Grants (DGs), first-class authorization artefacts that encode revocable transfers of authority with enforced scope reduction; a Canonical Verification Context (CVC) that normalizes verification requests into a single structured representation independent of protocols or credential formats; a layered reference architecture that separates trust anchoring, credential and proof validation, policy evaluation, and protocol mediation via a Trust Gateway; and an explicit treatment of blockchain anchoring as an optional integrity layer rather than a structural dependency. Together, these elements advance interoperable delegation and auditability and provide a foundation for future standardization, implementation, and integration of autonomous agents into trusted digital identity infrastructures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration](https://arxiv.org/abs/2601.14440)
*Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra*

Main category: cs.AI

TL;DR: 비전-언어 모델(VLMs)은 텍스트만으로 구성된 언어 모델보다 이미지로 제공된 수학 문제에서 수학적 추론에서 뒤쳐진다. 이 연구에선 VisTIRA라는 도구 통합 추론 프레임워크를 소개하고, LaTeX 기반 파이프라인을 통해 시각적 수학 추론을 측정하고 개선하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: VLMs는 이미지 형태로 제시된 수학 문제에서 낮은 성능을 보이는데 이는 텍스트 형식에 비해 모달리티 차이가 있기 때문이다.

Method: VisTIRA는 문제를 자연어 합리화와 실행 가능한 Python 단계로 분해하여 최종 답을 찾는 구조화된 문제 해결 프레임워크이다. 또한, LaTeX 기반 파이프라인을 구축하여 수학 코퍼스를 이미지로 변환하고, 실세계 데이터셋에서 파생된 도구 사용 경로 세트를 통해 VLM을 미세 조정한다.

Result: 도구 통합 감독이 이미지 기반 추론을 개선하고, OCR 기반 정렬이 더 작은 모델에서 격차를 줄일 수 있음을 실험적으로 보여준다.

Conclusion: 모달리티 차이는 모델 크기와 반비례하며, 구조화된 추론과 OCR 기반 정렬은 시각적 수학적 추론을 향상시키기 위한 보완 전략이다.

Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.

</details>


### [13] [Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://arxiv.org/abs/2601.14523)
*Leyi Zhao,Weijie Huang,Yitong Guo,Jiang Bian,Chenghong Wang,Xuhong Zhang*

Main category: cs.AI

TL;DR: PhyloEvolve는 GPU 중심의 알고리즘 최적화를 인맥 맥락 강화 학습 문제로 재구성하는 LLM-agent 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 현대 GPU를 위한 과학적 계산 알고리즘 최적화는 반복적인 코드 수정, 벤치마킹 및 조정 과정을 포함하는 노동 집약적 과정이다.

Method: PhyloEvolve는 최적화 경험을 재사용하기 위해 알고리즘 증류 및 프롬프트 기반 결정 변환기를 통합한 반복 워크플로를 갖춘 시스템이다.

Result: PhyloEvolve는 PDE 해결사, 매니폴드 학습 및 스펙트럴 그래프 알고리즘을 포함한 과학적 계산 작업 부하에서 일관된 성능 개선을 입증했다.

Conclusion: 이 시스템은 이질적인 하드웨어에서 탐색과 착취의 균형을 맞춘다.

Abstract: Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve

</details>


### [14] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 본 논문에서는 MAS 설계를 개선하기 위한 MAS-Orchestra 프레임워크와 평가 기준인 MASBENCH를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현재 다중 에이전트 시스템(MAS)의 설계 접근 방식은 에이전트의 상호작용을 통해 지능을 높일 가능성이 있지만, 실제로는 그렇게 이루어지지 않고 있다.

Method: MAS-Orchestra는 MAS 오케스트레이션을 함수 호출 강화 학습 문제로 공식화하며, 복잡한 목표 지향 하위 에이전트를 호출 가능한 함수로 추상화한다.

Result: MAS-Orchestra는 수학적 추론, 다중 홉 QA, 검색 기반 QA와 같은 공공 벤치마크에서 일관된 개선을 이룬다.

Conclusion: MAS-Orchestra와 MASBENCH는 다중 에이전트 지능을 추구하는 과정에서 MAS의 훈련과 이해를 개선한다.

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [15] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: AGEA는 GraphRAG 시스템에서 구조적 추출 공격에 대한 취약성을 보여주며 효과적인 그래프 추출을 위한 두 단계 파이프라인을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 GraphRAG 응답이 검색된 하위 그래프를 누설할 수 있다는 점을 고려할 때, 제약된 쿼리 예산 하에서 숨겨진 그래프 구조의 재구성이 가능성이 미지수이다.

Method: AGEA는 탐색-수확 전략, 외부 그래프 메모리 모듈, 경량 발견과 LLM 기반 필터링을 결합한 두 단계 그래프 추출 파이프라인을 사용하는 프레임워크이다.

Result: AGEA는 Microsoft-GraphRAG 및 LightRAG 시스템의 의료, 농업 및 문학 데이터 세트에서 이전 공격 기준선보다 현저히 우수한 성과를 보이며 최대 90%의 개체 및 관계를 복구하고 높은 정밀도를 유지한다.

Conclusion: 현대 GraphRAG 시스템은 엄격한 쿼리 제한 하에서도 구조적이고 에이전틱한 추출 공격에 매우 취약하다.

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [16] [IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization](https://arxiv.org/abs/2601.14686)
*Shuai Wang,Yaoming Yang,Bingdong Li,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: LPR은 개인화된 학습 항목 시퀀스를 생성하여 장기적인 학습 효과를 극대화하는 것을 목표로 한다. 이 연구에서는 IB-GRPO라는 새로운 방법을 제안하여 LLM을 기반으로 한 LPR에 적용할 수 있는 지침을 제공한다.


<details>
  <summary>Details</summary>
Motivation: LPR의 효율성을 높이기 위해 LLM을 활용하고자 하지만, 교육 목표와의 불일치, 전문가 시연의 부족, 다중 목표 간 상호작용 등 여러 도전과제가 존재한다.

Method: IB-GRPO는 LLM 기반 LPR을 위한 지표 기반 정렬 접근 방식을 제안하며, 유전자 알고리즘을 통해 하이브리드 전문가 시연을 구성하고, LLM을 감독된 파인 튜닝으로 워밍업한다.

Result: ASSIST09 및 Junyi 데이터 세트에서 KES 시뮬레이터를 사용한 실험 결과, 강화 학습 및 LLM 기반 기준과 비교했을 때 일관된 성능 향상을 보였다.

Conclusion: IB-GRPO의 사용으로 교육적 요구를 충족하면서 여러 목표 간의 균형을 유지할 수 있음을 입증했다.

Abstract: Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficulty scheduling, length controllability, and trajectory diversity. To address these issues, we propose IB-GRPO (Indicator-Based Group Relative Policy Optimization), an indicator-guided alignment approach for LLM-based LPR. To mitigate data scarcity, we construct hybrid expert demonstrations via Genetic Algorithm search and teacher RL agents and warm-start the LLM with supervised fine-tuning. Building on this warm-start, we design a within-session ZPD alignment score for difficulty scheduling. IB-GRPO then uses the $I_{ε+}$ dominance indicator to compute group-relative advantages over multiple objectives, avoiding manual scalarization and improving Pareto trade-offs. Experiments on ASSIST09 and Junyi using the KES simulator with a Qwen2.5-7B backbone show consistent improvements over representative RL and LLM baselines.

</details>


### [17] [Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation](https://arxiv.org/abs/2601.14691)
*Muhammad Khalifa,Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Yunxiang Zhang,Moontae Lee,Hao Peng,Lu Wang,Honglak Lee*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)이 에이전트 성능 평가에 사용되지만, 이들이 에이전트의 사고 경로 조작에 취약하다는 점이 밝혀졌습니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델이 비검증 가능 환경에서 에이전트 성능을 평가하는 검토자로 점점 더 많이 사용되고 있습니다.

Method: 에이전트의 사고 경로를 체계적으로 재작성하여 조작된 사고 경로가 최첨단 VLM 검토자의 잘못된 긍정률을 최대 90%까지 증가시킬 수 있음을 입증했습니다.

Result: 800개의 다양한 웹 작업 관련 경로를 분석하여 내용 기반 조작이 일관되게 더 효과적임을 발견했습니다.

Conclusion: LLM 기반 평가에서 근본적 취약성이 존재하며, 사고 주장을 가시적 증거에 대해 검증하는 판단 메커니즘의 필요성을 강조합니다.

Abstract: Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.

</details>


### [18] [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)
*Mingxuan Song,Yusen Huo,Bohan Zhou,Shenglin Yin,Zhen Xiao,Jieyi Long,Zhilin Zhang,Chuan Yu*

Main category: cs.AI

TL;DR: 이 논문에서는 예산 제약 하에서 광고주의 누적 가치를 최적화하는 복잡한 과제에 대해 다루고 있으며, AI 생성 입찰(AIGB) 패러다임을 기반으로 기존의 방법론의 한계를 극복하기 위해 GRPO-적응형 전략과 DARA 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 광고주는 개인화된 목표를 가지고 있지만 제한된 역사적 상호작용 데이터로 인해 기존 강화 학습 방법에서 효과성을 발휘하기 어려운 몇-shot 상황을 경험하고 있습니다.

Method: GRPO-Adaptive는 훈련 중에 참조 정책을 동적으로 업데이트하여 추론 및 수치 정밀성을 향상시키는 효율적인 LLM 후속 훈련 전략입니다. DARA는 초기에 계획을 생성하는 몇-shot 추론기와 피드백 기반 추론을 사용해 이러한 계획을 정제하는 세밀한 최적화기로 구성된 이중 단계 프레임워크입니다.

Result: 우리의 접근법은 실제 및 합성 데이터 환경에서 예산 제약 하에서 광고주 누적 가치 측면에서 기존 기준선을 일관되게 초과하는 성능을 보여줍니다.

Conclusion: DARA 프레임워크는 LLM의 맥락 내 학습 강점과 AIGB 작업에 요구되는 정밀한 적응성을 결합할 수 있도록 해줍니다.

Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

</details>


### [19] [CI4A: Semantic Component Interfaces for Agents Empowering Web Automation](https://arxiv.org/abs/2601.14790)
*Zhi Qiu,Jiazheng Sun,Chenxiao Xia,Jun Zheng,Xin Peng*

Main category: cs.AI

TL;DR: 이 논문에서는 에이전트에 최적화된 상호작용 인터페이스를 구축하는 것을 제안하고, 이를 위해 CI4A라는 메커니즘을 도입하여 UI 구성 요소의 복잡한 상호작용 로직을 에이전트가 접근할 수 있는 통합 도구 원시 집합으로 추상화하였다.


<details>
  <summary>Details</summary>
Motivation: 기존의 대형 언어 모델은 고수준의 의미 계획에 뛰어난 능력을 보이지만, 세부적인 웹 컴포넌트 조작에서는 제한적이다.

Method: CI4A(Component Interface for Agent)를 소개하며, 이를 Ant Design에 구현하여 일반적으로 사용되는 UI 구성 요소 23개 범주를 다루었다.

Result: CI4A 기반의 하이브리드 에이전트는 페이지 상태에 따라 동적으로 업데이트되는 액션 공간을 가짐으로써 유연한 도구 호출을 가능하게 한다.

Conclusion: CI4A 통합 Ant Design을 활용하여 웹 아레나 벤치마크를 리팩토링하고, 기존의 SoTA 방법들을 평가하여 86.3%의 새로운 SoTA 작업 성공률을 달성했고 실행 효율성도 크게 개선되었다.

Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.

</details>


### [20] [Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies](https://arxiv.org/abs/2601.14827)
*Ben Schaper,Maxime Di Folco,Bernhard Kainz,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.AI

TL;DR: 이 연구는 체외 X-레이 분류를 위한 비전-언어 모델의 오류를 정량화하고 완화하는 방법을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 지표가 임상적으로 미세한 오류와 심각한 오류를 구별하지 못하는 문제 해결.

Method: 의료 분류 체계를 활용하여 오류를 정량화하고, 여러 최신의 비전-언어 모델(VLM)을 벤치마킹하며, Catastrophic Abstraction Errors 개념을 도입해 교차 분기 오류를 포착.

Result: 비록 성능이 높아 보이지만 VLM과 임상 분류 체계 간의 상당한 불일치가 드러남.

Conclusion: 계층적 평가와 표현 수준 정렬의 중요성을 강조하며, 안전하고 임상적으로 의미 있는 VLM 배치를 위한 방법을 제안.

Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.

</details>


### [21] [Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation](https://arxiv.org/abs/2601.14955)
*Hanqi Jin,Gaoming Yang,Zhangming Chan,Yapeng Yuan,Longbin Li,Fei Sun,Yeqiu Yang,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 본 논문에서는 사용자 행동 간의 전이를 모델링하기 위해 TGA(Transition-Aware Graph Attention Network)를 제안하며, 이는 전통적인 방법보다 효율적으로 동작한다.


<details>
  <summary>Details</summary>
Motivation: 전자상거래 플랫폼에서 사용자 상호작용은 클릭, 즐겨찾기, 장바구니 추가 및 구매와 같은 다양한 행동을 포함하며, 이러한 행동 간의 전이는 사용자 아이템 상호작용에 대한 귀중한 통찰을 제공한다.

Method: TGA는 정보전환을 세 가지 관점(아이템 수준, 카테고리 수준, 이웃 수준)에서 식별하여 구조화된 희소 그래프를 만들고, 이를 바탕으로 사용자-아이템 상호작용과 행동 전환 유형을 공동 모델링하는 주의 메커니즘을 활용한다.

Result: TGA는 모든 최신 모델보다 뛰어난 성능을 보여주면서도 계산 비용을 크게 줄인다.

Conclusion: TGA는 대규모 산업 생산 환경에 배포되었으며, 주요 비즈니스 지표에서 놀라운 개선을 가져왔다.

Abstract: User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi-behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional transformers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behavior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while significantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.

</details>


### [22] [Emergent, not Immanent: A Baradian Reading of Explainable AI](https://arxiv.org/abs/2601.15029)
*Fabio Morreale,Joan Serrà,Yuki Mistufuji*

Main category: cs.AI

TL;DR: 이 논문은 설명 가능한 AI(XAI)의 존재론적-인식론적 가정을 비판하고, 대안적인 접근 방식으로서 바라드의 행위론적 사실주의를 적용하여 AI 모델과 인간, 맥락, 해석 도구 간의 얽힘에서 나타나는 해석을 강조한다.


<details>
  <summary>Details</summary>
Motivation: XAI를 기술적 문제로 다루는 기존 관점을 비판하고 새로운 존재론적-인식론적 접근 방식을 제안하기 위해.

Method: Barad의 행위론적 사실주의를 기반으로 하여 XAI 방법론을 분석하고, 해석이 발생하는 맥락과 상호작용을 중점적으로 탐구한다.

Result: 여러 XAI 방법의 가정과 한계를 드러내고, 윤리적 차원과 함께 emergent interpretation을 지원하는 XAI 인터페이스의 설계 방향을 제안한다.

Conclusion: XAI 인터페이스 설계에 대한 제안은 행위론적 사실주의를 통해 해석의 과정이 맥락에 따라 어떻게 달라질 수 있는지를 보여준다.

Abstract: Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.

</details>


### [23] [The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems](https://arxiv.org/abs/2601.15059)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 현대 CI/CD 파이프라인에서는 결정에 대한 책임 귀속의 구조적 결함이 발생함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 결정 프로세스의 승인 과정은 형식적으로 올바르지만, 어떤 주체도 결정의 승인 권한과 그 기반을 의미 깊게 이해할 수 있는 인지 능력을 모두 갖추지 못한다는 점을 지적하기 위해.

Method: 책임 공백을 정의하고, 결정 생성 처리량이 제한된 인간 검증 능력을 초과하는 배포의 구조적 특성을 분석한다.

Result: 기존의 배포 가정 하에서의 스케일링 한계를 식별하고, 인간의 인지 능력을 회복하지 않고도 자동화된 검증 범위가 증가하는 CI 증폭 동역학을 특성화한다.

Conclusion: 조직이 의사 결정 경계를 명시적으로 재설계하거나 개별 결정에서 배치 또는 시스템 수준의 소유권으로 책임을 재분배하지 않는 한, 책임 공백은 스케일된 에이전트 배포에서 보이지 않지만 지속적인 실패 모드로 남아 있게 된다.

Abstract: Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.
  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.
  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.
  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.
  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.

</details>


### [24] [The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution](https://arxiv.org/abs/2601.15075)
*Chen Qian,Peng Wang,Dongrui Liu,Junyao Yang,Dadi Guo,Ling Tang,Jilin Mei,Qihan Ren,Shuai Shao,Yong Liu,Jie Fu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 본 연구는 대형 언어 모델(LLM) 기반 에이전트의 행동 이유를 설명하기 위한 새로운 프레임워크인 일반 에이전틱 귀속 개념을 제안한다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 에이전트는 고객 서비스, 웹 탐색, 소프트웨어 엔지니어링 등 다양한 실제 응용 프로그램에서 널리 사용되고 있으며, 이러한 시스템의 행동을 이해하는 것이 중요하다.

Method: 우리는 계층적으로 작동하는 새로운 프레임워크를 제안하며, 내부 요인을 식별하기 위해 시간적 가능성 역학을 사용하고, 문장 수준에서 특정 텍스트 증거를 고립하기 위해 교란 기반 분석을 사용한다.

Result: 전문적인 실험 결과는 제안된 프레임워크가 에이전트 행동 뒤에 있는 중요 사건과 문장을 정확히 지적함을 보여준다.

Conclusion: 이 연구는 보다 안전하고 책임 있는 에이전틱 시스템으로 나아가는 중요한 단계를 제공한다.

Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.

</details>


### [25] [Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories](https://arxiv.org/abs/2601.15120)
*Qian Xiong,Yuekai Huang,Yujia Zheng,Tianhao Li,Ziyou Jiang,Zhiyuan Chang,Zhaoyang Li,Huanxiang Feng,Mingyang Li*

Main category: cs.AI

TL;DR: RISE는 LLM의 의도 편차 문제를 완화하기 위해 설계된 'Real-to-Virtual' 방법으로, 가상의 경로를 합성하고 다양한 부정 샘플을 생성하여 LLM을 미세 조정한다.


<details>
  <summary>Details</summary>
Motivation: LLMs의 의도 편차 문제는 신뢰성 있는 평가와 성능 개선을 방해한다.

Method: RISE는 검증된 도구 원시를 기반으로 가상 경로를 합성하고 중요한 매개변수의 변경을 통해 다양한 부정 샘플을 생성하여 의도 정렬을 위한 두 단계의 교육으로 LLM을 미세 조정한다.

Result: RISE로 합성된 데이터는 8가지 지표에서 사용자의 요구, 실행 경로 및 에이전트 응답을 포함한 유망한 결과를 보여준다.

Conclusion: RISE는 Acctask에서 평균 35.28% 개선을, Accintent에서 23.27% 개선을 달성하여 SOTA 기준을 각각 1.20--42.09% 및 1.17--54.93% 초과하였다.

Abstract: LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of "intent deviation" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a "Real-to-Virtual" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.

</details>


### [26] [How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework](https://arxiv.org/abs/2601.15153)
*Choro Ulan uulu,Mikhail Kulyabin,Iris Fuhrmann,Jan Joosten,Nuno Miguel Martins Pacheco,Filippos Petridis,Rebecca Johnson,Jan Bosch,Helena Holmström Olsson*

Main category: cs.AI

TL;DR: 이 논문은 AI 에이전트 시스템에 인간의 도메인 지식을 통합하기 위한 소프트웨어 공학 프레임워크를 제안하고, 이를 통해 비전문가가 전문적인 결과를 도출할 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 전문 지식이 몇몇 전문가에게만 존재하면 조직의 확장성과 의사결정에 병목 현상이 발생합니다. 비전문가는 효과적인 시각화를 생성하는 데 어려움을 겪으며, 이는 최적이 아닌 통찰력으로 이어지고 전문가의 시간을 낭비하게 됩니다.

Method: 우리는 큰 언어 모델(LLM)에 요청 분류기, 코드 생성을 위한 검색 기반 생성(RAG) 시스템, 전문가 규칙 및 시각화 설계 원칙을 통합하여 AI 에이전트를 연구하는 소프트웨어 공학 프레임워크를 제안합니다.

Result: 다양한 엔지니어링 도메인을 아우르는 5개의 시나리오와 12명의 평가자를 통한 평가 결과, 출력 품질이 206% 개선되었으며, 우리의 에이전트는 모든 경우에서 전문가 수준의 평가를 달성했습니다.

Conclusion: 비전문가가 특정 도메인에서 전문가 수준의 결과를 도출할 수 있음을 입증한 자동화된 시각화 생성 에이전트 시스템과 인간 도메인 지식을 체계적으로 캡처하고 암묵적인 전문 지식을 AI 에이전트에 정형화하는 검증된 프레임워크를 제안합니다.

Abstract: Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [27] [From Agent Simulation to Social Simulator: A Comprehensive Review (Part 2)](https://arxiv.org/abs/2601.14296)
*Xiao Xue,Deyu Zhou,Ming Zhang,Xiangning Yu,Fei-Yue Wang*

Main category: cs.MA

TL;DR: 이 연구는 시스템 복잡성의 패턴 탐색과 이론적 설명 개발을 목표로 한다. 에이전트 기반 모델링은 복잡한 시스템 연구에 중요한 접근법이지만, 실험보다 시뮬레이션을 강조하는 경향이 있다.


<details>
  <summary>Details</summary>
Motivation: 시스템 복잡성의 기저에 있는 패턴을 탐구하고 이론적 설명을 개발하는 것이 이 연구의 주요 목표이다.

Method: 에이전트 기반 모델링(ABM)과 계산 실험을 통해 시스템 행동의 원인 설명을 제공하며, 입력 변수를 체계적으로 조정하고 결과 변수의 변화를 관찰한다.

Result: 계산 실험은 전통적인 ABM의 한계를 극복하고 시스템의 동적 진화에 대한 인과적 통찰을 제공한다.

Conclusion: 이 연구는 독자가 전체 계산 실험 방법에 대한 기초적인 이해를 돕고 후속 연구의 기초를 마련하는 데 기여할 것이다.

Abstract: The study of system complexity primarily has two objectives: to explore underlying patterns and to develop theoretical explanations. Pattern exploration seeks to clarify the mechanisms behind the emergence of system complexity, while theoretical explanations aim to identify the fundamental causes of this complexity. Laws are generally defined as mappings between variables, whereas theories offer causal explanations of system behavior. Agent Based Modeling(ABM) is an important approach for studying complex systems, but it tends to emphasize simulation over experimentation. As a result, ABM often struggles to deeply uncover the governing operational principles. Unlike conventional scenario analysis that relies on human reasoning, computational experiments emphasize counterfactual experiments-that is, creating parallel worlds that simulate alternative "evolutionary paths" of real-world events. By systematically adjusting input variables and observing the resulting changes in output variables, computational experiments provide a robust tool for causal inference, thereby addressing the limitations of traditional ABM. Together, these methods offer causal insights into the dynamic evolution of systems. This part can help readers gain a preliminary understanding of the entire computational experiment method, laying the foundation for the subsequent study.

</details>


### [28] [MARBLE: Multi-Agent Reasoning for Bioinformatics Learning and Evolution](https://arxiv.org/abs/2601.14349)
*Sunghyun Kim,Seokwoo Yun,Youngseo Yun,Youngrak Lee,Sangsoo Lim*

Main category: cs.MA

TL;DR: MARBLE은 생물정보학 모델을 위한 실행 안정적인 자율 모델 정제 프레임워크로, 반복적인 모델 개선 과정에서 성능 향상을 지속적으로 달성하는 것을 목표로 한다.


<details>
  <summary>Details</summary>
Motivation: 생물정보학 모델을 개발하기 위해서는 가설 수립, 구조적 재설계, 경험적 검증을 반복해야 하므로 진전이 느리고 노동 집약적이며 재현하기 어려운 경우가 많다.

Method: MARBLE은 문헌 기반 참조 선택과 역할 전문화된 에이전트 간의 구조적 토론 주도 아키텍처 추론을 결합하고, 이를 통해 자율 실행, 평가 및 경험적 성과에 명시적으로 기반한 메모리 업데이트를 수행한다.

Result: MARBLE은 공간 전사체 영역 분할, 약물-타겟 상호작용 예측, 약물 반응 예측 등의 분야에서 강력한 기준선 이상으로 지속적인 성과 향상을 달성하며, 높은 실행 안정성과 낮은 회귀율을 유지한다.

Conclusion: 구조적 토론, 균형 잡힌 증거 선택, 성능 기반 메모리가 단일 실행이나 불안정한 이익이 아닌 안정적이고 반복 가능한 모델 발전에 중요함을 보여준다.

Abstract: Motivation: Developing high-performing bioinformatics models typically requires repeated cycles of hypothesis formulation, architectural redesign, and empirical validation, making progress slow, labor-intensive, and difficult to reproduce. Although recent LLM-based assistants can automate isolated steps, they lack performance-grounded reasoning and stability-aware mechanisms required for reliable, iterative model improvement in bioinformatics workflows. Results: We introduce MARBLE, an execution-stable autonomous model refinement framework for bioinformatics models. MARBLE couples literature-aware reference selection with structured, debate-driven architectural reasoning among role-specialized agents, followed by autonomous execution, evaluation, and memory updates explicitly grounded in empirical performance. Across spatial transcriptomics domain segmentation, drug-target interaction prediction, and drug response prediction, MARBLE consistently achieves sustained performance improvements over strong baselines across multiple refinement cycles, while maintaining high execution robustness and low regression rates. Framework-level analyses demonstrate that structured debate, balanced evidence selection, and performance-grounded memory are critical for stable, repeatable model evolution, rather than single-run or brittle gains. Availability: Source code, data and Supplementary Information are available at https://github.com/PRISM-DGU/MARBLE.

</details>


### [29] [If You Want Coherence, Orchestrate a Team of Rivals: Multi-Agent Models of Organizational Intelligence](https://arxiv.org/abs/2601.14351)
*Gopal Vijayaraghavan,Prasanth Jayachandran,Arun Murthy,Sunil Govindan,Vivek Subramanian*

Main category: cs.MA

TL;DR: AI 에이전트는 빠른 속도로 복잡한 작업을 수행할 수 있지만, 그들의 지능은 오류가 발생할 수 있습니다. 이 논문에서는 다양한 독립적인 AI 팀의 협력을 통해 오류를 줄이고 신뢰성을 유지할 수 있는 시스템 아키텍처를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI 에이전트의 실수를 문제 삼지 않고, 안전하고 생산적인 작업 환경을 제공하고자 합니다.

Method: 특화된 에이전트 팀(계획자, 실행자, 비평가, 전문가)을 조직하고, 원격 코드 실행기를 통해 데이터 변환과 도구 호출을 별도로 유지하면서 진행합니다.

Result: 이 접근 방식은 90% 이상의 내부 오류를 사용자에게 노출되기 전에 포착하며 수용 가능한 지연 부담을 유지합니다.

Conclusion: 비용과 지연 시간의 균형을 맞추면서도 정확성을 달성하고 기존 기능에 영향을 주지 않고 기능을 점진적으로 확장할 수 있습니다.

Abstract: AI Agents can perform complex operations at great speed, but just like all the humans we have ever hired, their intelligence remains fallible. Miscommunications aren't noticed, systemic biases have no counter-action, and inner monologues are rarely written down.
  We did not come to fire them for their mistakes, but to hire them and provide a safe productive working environment. We posit that we can reuse a common corporate organizational structure: teams of independent AI agents with strict role boundaries can work with common goals, but opposing incentives. Multiple models serving as a team of rivals can catch and minimize errors within the final product at a small cost to the velocity of actions. In this paper we demonstrate that we can achieve reliability without acquiring perfect components, but through careful orchestration of imperfect ones.
  This paper describes the architecture of such a system in practice: specialized agent teams (planners, executors, critics, experts), organized into an organization with clear goals, coordinated through a remote code executor that keeps data transformations and tool invocations separate from reasoning models. Rather than agents directly calling tools and ingesting full responses, they write code that executes remotely; only relevant summaries return to agent context. By preventing raw data and tool outputs from contaminating context windows, the system maintains clean separation between perception (brains that plan and reason) and execution (hands that perform heavy data transformations and API calls). We demonstrate the approach achieves over 90% internal error interception prior to user exposure while maintaining acceptable latency tradeoffs. A survey from our traces shows that we only trade off cost and latency to achieve correctness and incrementally expand capabilities without impacting existing ones.

</details>


### [30] [Agent Identity URI Scheme: Topology-Independent Naming and Capability-Based Discovery for Multi-Agent Systems](https://arxiv.org/abs/2601.14567)
*Roland R. Rodriguez*

Main category: cs.MA

TL;DR: 이 논문은 에이전트들이 네트워크 위치에 얽매이지 않도록 하는 새로운 URI 기반 정체성 시스템인 agent:// URI 스킴을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 에이전트 정체성 시스템이 네트워크 위치에 얽매여 있어 이식성과 확장성에 한계를 느끼고 있다.

Method: agent:// URI 스킴은 신뢰 루트, 계층화된 능력 경로, 정렬 가능한 고유 식별자로 구성된다.

Result: 제안된 스킴은 369개의 생산 도구에서 100%의 범위를 보장하고, 10,000개의 에이전트에 대해 F1=1.0의 발견 정확도를 보여주었다.

Conclusion: agent:// URI 스킴은 탈중앙화된 에이전트 정체성과 능력 기반 발견을 위한 확실한 기초를 제공한다.

Abstract: Multi-agent systems face a fundamental architectural flaw: agent identity is bound to network location. When agents migrate between providers, scale across instances, or federate across organizations, URI-based identity schemes break references, fragment audit trails, and require centralized coordination. We propose the agent:// URI scheme, which decouples identity from topology through three orthogonal components: a trust root establishing organizational authority, a hierarchical capability path enabling semantic discovery, and a sortable unique identifier providing stable reference. The scheme enables capability-based discovery through DHT key derivation, where queries return agents by what they do rather than where they are. Trust-root scoping prevents cross-organization pollution while permitting federation when desired. Cryptographic attestation via PASETO tokens binds capability claims to agent identity, enabling verification without real-time contact with the issuing authority. We evaluate the scheme across four dimensions: capability expressiveness (100% coverage on 369 production tools with zero collision), discovery precision (F1=1.0 across 10,000 agents), identity stability (formal proofs of migration invariance), and performance (all operations under 5 microseconds). The agent:// URI scheme provides a formally-specified, practically-evaluated foundation for decentralized agent identity and capability-based discovery.

</details>


### [31] [INFA-Guard: Mitigating Malicious Propagation via Infection-Aware Safeguarding in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.14667)
*Yijin Zhou,Xiaoya Lu,Dongrui Liu,Junchi Yan,Jing Shao*

Main category: cs.MA

TL;DR: 본 논문에서는 감염된 에이전트를 독립적인 위협 범주로 인식하여 보호하는 방안인 INFA-Guard를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 기반 다중 에이전트 시스템(MAS)의 급속한 발전으로 인해 보안 취약점이 증가하고 있습니다.

Method: INFA-Guard는 감염 인식 탐지와 위상적 제약을 활용하여 공격 소스와 감염된 범위를 정확하게 식별하고, 공격자와 감염 에이전트를 대체하여 악성 전파를 방지합니다.

Result: INFA-Guard는 ASR을 평균 33% 감소시키는 성능을 보여주며, 다양한 모델 간의 견고성, 우수한 위상적 일반화 및 높은 비용 효율성을 발휘합니다.

Conclusion: INFA-Guard는 감염된 에이전트를 효과적으로 관리하여 전반적인 보안을 강화하는 데 기여합니다.

Abstract: The rapid advancement of Large Language Model (LLM)-based Multi-Agent Systems (MAS) has introduced significant security vulnerabilities, where malicious influence can propagate virally through inter-agent communication. Conventional safeguards often rely on a binary paradigm that strictly distinguishes between benign and attack agents, failing to account for infected agents i.e., benign entities converted by attack agents. In this paper, we propose Infection-Aware Guard, INFA-Guard, a novel defense framework that explicitly identifies and addresses infected agents as a distinct threat category. By leveraging infection-aware detection and topological constraints, INFA-Guard accurately localizes attack sources and infected ranges. During remediation, INFA-Guard replaces attackers and rehabilitates infected ones, avoiding malicious propagation while preserving topological integrity. Extensive experiments demonstrate that INFA-Guard achieves state-of-the-art performance, reducing the Attack Success Rate (ASR) by an average of 33%, while exhibiting cross-model robustness, superior topological generalization, and high cost-effectiveness.

</details>


### [32] [Game-Theoretic Lens on LLM-based Multi-Agent Systems](https://arxiv.org/abs/2601.15047)
*Jianing Hao,Han Ding,Yuanjian Xu,Tianze Sun,Ran Chen,Wanbo Zhang,Guang Zhang,Siguang Li*

Main category: cs.MA

TL;DR: 이 논문에서는 LLM 기반의 다중 에이전트 시스템에 대한 포괄적인 조사를 제공하고, 게임 이론의 네 가지 핵심 요소를 통해 체계적인 프레임워크를 설정합니다.


<details>
  <summary>Details</summary>
Motivation: LLM이 개별적으로 제한된 적응성과 조정력을 가지고 있는 반면, LLM 간 상호작용을 통해 협력적, 경쟁적 또는 혼합 목표를 추구하는 다중 에이전트 시스템에 대한 관심이 증가하고 있습니다.

Method: 게임 이론의 네 가지 핵심 요소(플레이어, 전략, 보상, 정보)를 중심으로 기존 연구를 조직하여 LLM 기반 다중 에이전트 시스템에 대한 체계적인 프레임워크를 구축합니다.

Result: 이 프레임워크를 통해 지능형 에이전트 간의 사회적 동역학 및 전략적 행동을 연구하고 비교하고 설계 및 분석을 안내할 수 있는 기초를 제공합니다.

Conclusion: 현재 연구의 파편화를 해결하기 위해 LLM 기반 다중 에이전트 시스템에 대한 보다 통합된 이론적 기초를 제공하는 것이 필요합니다.

Abstract: Large language models (LLMs) have demonstrated strong reasoning, planning, and communication abilities, enabling them to operate as autonomous agents in open environments. While single-agent systems remain limited in adaptability and coordination, recent progress has shifted attention toward multi-agent systems (MAS) composed of interacting LLMs that pursue cooperative, competitive, or mixed objectives. This emerging paradigm provides a powerful testbed for studying social dynamics and strategic behaviors among intelligent agents. However, current research remains fragmented and lacks a unifying theoretical foundation. To address this gap, we present a comprehensive survey of LLM-based multi-agent systems through a game-theoretic lens. By organizing existing studies around the four key elements of game theory: players, strategies, payoffs, and information, we establish a systematic framework for understanding, comparing, and guiding future research on the design and analysis of LLM-based MAS.

</details>


### [33] [From Who They Are to How They Act: Behavioral Traits in Generative Agent-Based Models of Social Media](https://arxiv.org/abs/2601.15114)
*Valerio La Gatta,Gian Marco Orlando,Marco Perillo,Ferdinando Tammaro,Vincenzo Moscato*

Main category: cs.MA

TL;DR: 본 논문은 GABM을 통해 소셜 미디어 환경에서의 인간 행동을 시뮬레이션하는 자율 에이전트를 생성하는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 소셜 미디어에서의 정보 전파 및 영향력 과정을 모델링하기 위한 차별화된 참여 스타일을 필요로 한다.

Method: 행동 특성을 통해 에이전트의 플랫폼 행동 경향성을 조절하는 방법을 조사한다.

Result: 대규모 시뮬레이션을 통해 행동 특성이 이질적이고 일관된 참여 패턴을 유지하는 데 필수적임을 입증한다.

Conclusion: 에이전트의 행동 방식을 모델링하는 것이 GABM 발전에 필요하다는 것을 확인하였다.

Abstract: Generative Agent-Based Modeling (GABM) leverages Large Language Models to create autonomous agents that simulate human behavior in social media environments, demonstrating potential for modeling information propagation, influence processes, and network phenomena. While existing frameworks characterize agents through demographic attributes, personality traits, and interests, they lack mechanisms to encode behavioral dispositions toward platform actions, causing agents to exhibit homogeneous engagement patterns rather than the differentiated participation styles observed on real platforms. In this paper, we investigate the role of behavioral traits as an explicit characterization layer to regulate agents' propensities across posting, re-sharing, commenting, reacting, and inactivity. Through large-scale simulations involving 980 agents and validation against real-world social media data, we demonstrate that behavioral traits are essential to sustain heterogeneous, profile-consistent participation patterns and enable realistic content propagation dynamics through the interplay of amplification- and interaction-oriented profiles. Our findings establish that modeling how agents act-not only who they are-is necessary for advancing GABM as a tool for studying social media phenomena.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Quality or Quantity? Error-Informed Selective Online Learning with Gaussian Processes in Multi-Agent Systems: Extended Version](https://arxiv.org/abs/2601.14275)
*Zewen Yang,Xiaobing Dai,Jiajun Cheng,Yulong Huang,Peng Shi*

Main category: cs.LG

TL;DR: 이 논문은 분산 학습에서 모델의 양보다 질을 우선시해야 한다는 점을 강조하며, 새로운 선택적 온라인 학습 프레임워크를 제시한다.


<details>
  <summary>Details</summary>
Motivation: 다중 에이전트 시스템에서의 분산 학습에서 협력의 중요성을 강조하며, 모델의 양과 질 간의 상호작용이 중요하다.

Method: 분산 오류 정보 기반 GP 회귀를 위한 선택적 온라인 학습 프레임워크인 분산 오류 정보 GP (EIGP)를 제시한다.

Result: 각 에이전트가 보다 높은 품질의 GP 모델을 선택할 수 있도록 하고, 예측 오류를 줄이는 방식으로 성능을 향상시킨다.

Conclusion: 개발된 방법론의 효과를 수치 시뮬레이션을 통해 입증하며, 기존의 분산 GP 방법과 비교했을 때 우수성을 보여준다.

Abstract: Effective cooperation is pivotal in distributed learning for multi-agent systems, where the interplay between the quantity and quality of the machine learning models is crucial. This paper reveals the irrationality of indiscriminate inclusion of all models on agents for joint prediction, highlighting the imperative to prioritize quality over quantity in cooperative learning. Specifically, we present the first selective online learning framework for distributed Gaussian process (GP) regression, namely distributed error-informed GP (EIGP), that enables each agent to assess its neighboring collaborators, using the proposed selection function to choose the higher quality GP models with less prediction errors. Moreover, algorithmic enhancements are embedded within the EIGP, including a greedy algorithm (gEIGP) for accelerating prediction and an adaptive algorithm (aEIGP) for improving prediction accuracy. In addition, approaches for fast prediction and model update are introduced in conjunction with the error-informed quantification term iteration and a data deletion strategy to achieve real-time learning operations. Numerical simulations are performed to demonstrate the effectiveness of the developed methodology, showcasing its superiority over the state-of-the-art distributed GP methods with different benchmarks.

</details>


### [35] [Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents](https://arxiv.org/abs/2601.14287)
*Xiucheng Xu,Bingbing Xu,Xueyun Tian,Zihe Huang,Rongxin Chen,Yunfan Li,Huawei Shen*

Main category: cs.LG

TL;DR: CoM(Chain-of-Memory) 프레임워크는 경량의 메모리 구조와 정교한 활용을 결합하여 LLM(대형 언어 모델) 에이전트의 지식 유지 및 장기 결정 수행을 개선한다.


<details>
  <summary>Details</summary>
Motivation: LLM 에이전트가 지속적인 지식을 유지하고 장기적인 의사 결정을 수행할 수 있도록 외부 메모리 시스템이 필수적이다.

Method: CoM(Chain-of-Memory) 프레임워크는 동적 진화를 통해 검색된 조각들을 일관된 추론 경로로 조직하고, 관련 없는 노이즈를 제거하기 위해 적응형 절단을 활용한다.

Result: LongMemEval 및 LoCoMo 벤치마크에서 CoM이 강력한 기준선보다 7.5%-10.4%의 정확도 향상을 보였고, 복잡한 메모리 아키텍처에 비해 계산 오버헤드를 약 2.7%의 토큰 소비와 6.0%의 지연으로 크게 줄였다.

Conclusion: CoM 프레임워크는 LLM의 메모리 관리 및 의사 결정 능력을 획기적으로 향상시킨다.

Abstract: External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.

</details>


### [36] [GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling](https://arxiv.org/abs/2601.14476)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.LG

TL;DR: 본 연구는 확률적 비트(p-bit)를 사용한 컴퓨팅이 장치 가변성을 활용하여 알고리즘 성능을 향상시킬 수 있음을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 CMOS 논리를 대체할 수 있는 효율적인 방법으로 복잡한 문제 해결을 위한 확률적 컴퓨팅을 탐구합니다.

Method: 확률적 비트에 기반한 GPU 가속 오픈 소스 시뮬레이티드 어닐링 프레임워크를 소개하며, 이 프레임워크는 장치의 시간, 강도 및 오프셋 등의 키 가변성 요소를 모델링합니다.

Result: CUDA 기반의 시뮬레이션을 통해 MAX-CUT 벤치마크에서 문제 크기가 800에서 20,000 노드까지인 경우 CPU 구현에 비해 두 배의 속도 향상을 달성했습니다.

Conclusion: 확률적 컴퓨팅 연구를 촉진하고 다양한 분야에서 최적화 응용 프로그램을 가능하게 하는 확장 가능하고 접근 가능한 도구를 제공합니다.

Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -- timing, intensity, and offset -- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.

</details>


### [37] [Constructing Multi-label Hierarchical Classification Models for MITRE ATT&CK Text Tagging](https://arxiv.org/abs/2601.14556)
*Andrew Crossman,Jonah Dodd,Viralam Ramamurthy Chaithanya Kumar,Riyaz Mohammed,Andrew R. Plummer,Chandra Sekharudu,Deepak Warrier,Mohammad Yekrangian*

Main category: cs.LG

TL;DR: MITRE ATT&CK 프레임워크를 자동화하기 위한 텍스트 태깅 작업을 분석하고, 새로운 방법 구축 경로를 제시하며, 다중 라벨 계층 분류 모델을 실험하여 높은 정확도를 보여준다.


<details>
  <summary>Details</summary>
Motivation: 현재 ATT&CK 태깅 프로세스는 주로 수동으로 진행되기 때문에 이를 자동화할 필요가 있다.

Method: AIML 방법을 사용하여 MITRE ATT&CK 텍스트 태깅 작업을 계층화된 '작업 공간'으로 구성하고, 다중 라벨 계층 분류 모델을 단계적으로 구축하였다.

Result: 작업 수준에서 약 94% 정확도, 기술 수준에서 약 82% 정확도를 달성하였고, 기존 기계 학습 방법만으로 최신 성능을 초과하였다.

Conclusion: GPT-4o 모델의 성능이 우리 방법보다 낮으며, 재무 관련 위협 시나리오를 위한 기본 모델을 확장하였다.

Abstract: MITRE ATT&CK is a cybersecurity knowledge base that organizes threat actor and cyber-attack information into a set of tactics describing the reasons and goals threat actors have for carrying out attacks, with each tactic having a set of techniques that describe the potential methods used in these attacks. One major application of ATT&CK is the use of its tactic and technique hierarchy by security specialists as a framework for annotating cyber-threat intelligence reports, vulnerability descriptions, threat scenarios, inter alia, to facilitate downstream analyses. To date, the tagging process is still largely done manually. In this technical note, we provide a stratified "task space" characterization of the MITRE ATT&CK text tagging task for organizing previous efforts toward automation using AIML methods, while also clarifying pathways for constructing new methods. To illustrate one of the pathways, we use the task space strata to stage-wise construct our own multi-label hierarchical classification models for the text tagging task via experimentation over general cyber-threat intelligence text -- using shareable computational tools and publicly releasing the models to the security community (via https://github.com/jpmorganchase/MITRE_models). Our multi-label hierarchical approach yields accuracy scores of roughly 94% at the tactic level, as well as accuracy scores of roughly 82% at the technique level. The models also meet or surpass state-of-the-art performance while relying only on classical machine learning methods -- removing any dependence on LLMs, RAG, agents, or more complex hierarchical approaches. Moreover, we show that GPT-4o model performance at the tactic level is significantly lower (roughly 60% accuracy) than our own approach. We also extend our baseline model to a corpus of threat scenarios for financial applications produced by subject matter experts.

</details>


### [38] [Place with Intention: An Empirical Attendance Predictive Study of Expo 2025 Osaka, Kansai, Japan](https://arxiv.org/abs/2601.14570)
*Xiaojie Yang,Dizhi Huang,Hangli Ge,Masahiro Sano,Takeaki Ohdake,Kazuma Hatano,Noboru Koshizuka*

Main category: cs.LG

TL;DR: Transformer 기반의 프레임워크를 이용하여 대규모 국제 행사에서의 참석 예측 정확성을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 대규모 국제 행사에서의 관중 흐름 및 서비스 관리와 관련하여 정확한 일일 참석 예측의 중요성을 강조한다.

Method: 예약 동적 정보를 활용하여 참가 의도를 추정하는 Transformer 기반 모델을 제안한다.

Result: 동쪽과 서쪽 게이트를 분리하여 모델링할 때 정확도가 일관되게 향상됨을 발견했다.

Conclusion: 예약 동적 정보는 대규모 국제 행사에서 참석 예측을 위한 실용적이고 유익한 기초를 제공한다.

Abstract: Accurate forecasting of daily attendance is vital for managing transportation, crowd flows, and services at large-scale international events such as Expo 2025 Osaka, Kansai, Japan. However, existing approaches often rely on multi-source external data (such as weather, traffic, and social media) to improve accuracy, which can lead to unreliable results when historical data are insufficient. To address these challenges, we propose a Transformer-based framework that leverages reservation dynamics, i.e., ticket bookings and subsequent updates within a time window, as a proxy for visitors' attendance intentions, under the assumption that such intentions are eventually reflected in reservation patterns. This design avoids the complexity of multi-source integration while still capturing external influences like weather and promotions implicitly embedded in reservation dynamics. We construct a dataset combining entrance records and reservation dynamics and evaluate the model under both single-channel (total attendance) and two-channel (separated by East and West gates) settings. Results show that separately modeling East and West gates consistently improves accuracy, particularly for short- and medium-term horizons. Ablation studies further confirm the importance of the encoder-decoder structure, inverse-style embedding, and adaptive fusion module. Overall, our findings indicate that reservation dynamics offer a practical and informative foundation for attendance forecasting in large-scale international events.

</details>


### [39] [Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2601.14693)
*Jianwen Sun,Xinrui Li,Fuqing Li,Xiaoxuan Shen*

Main category: cs.LG

TL;DR: EGRL-SR라는 새로운 프레임워크는 기호 회귀 문제를 해결하기 위해 목표 조건 강화 학습을 활용하여 표현 검색을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 기호 회귀는 입력 및 출력 변수 간의 기능적 관계를 모델링하는 compact하고 해석 가능한 수학적 표현을 자동으로 식별하는 것을 목표로 한다. 기존의 방법들은 검색 과정에서 적합도 오류에 의존하여 검색 방향이 모호해지고 수렴을 방해할 수 있다.

Method: EGRL-SR은 기호 회귀 문제를 목표 조건 강화 학습 문제로 공식화하고, 정확한 역사적 경로를 활용하며 행동-가치 네트워크를 최적화하여 검색 프로세스를 유도한다.

Result: EGRL-SR은 공통 매핑 패턴을 일반화하여 다양한 입력-출력 쌍에서 효과적으로 작동하며, 실험 결과에서 뛰어난 회복률과 robust성을 보여준다.

Conclusion: EGRL-SR은 기존 방법보다 우수한 성능을 보이며, 행동-가치 네트워크가 검색을 효과적으로 유도하는 것으로 나타났다.

Abstract: Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.

</details>


### [40] [RefProtoFL: Communication-Efficient Federated Learning via External-Referenced Prototype Alignment](https://arxiv.org/abs/2601.14746)
*Hongyue Wu,Hangyu Li,Guodong Fan,Haoran Zhu,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: RefProtoFL은 커뮤니케이션 효율적인 연합 학습 프레임워크로, 외부 참조 프로토타입 정렬과 적응형 확률 업데이트 드롭을 통합하여 성능을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습은 엣지 환경에서 원시 데이터를 공유하지 않고 모델 훈련을 가능하게 하지만, 통신 대역폭의 제한과 비동질적인 클라이언트 데이터 분포 때문에 제약을 받는다.

Method: RefProtoFL은 외부 참조 프로토타입 정렬(ERPA)과 적응형 확률 업데이트 드롭(APUD)을 통합하여 모델을 개인 백본과 경량 공유 어댑터로 분해한다. 연합 통신은 어댑터 매개변수로만 제한되며, APUD는 전송할 가장 중요한 업데이트만 전송하여 업링크 비용을 줄인다.

Result: 기존의 프로토타입 기반 연합 학습 방법보다 더 높은 분류 정확도를 달성한다.

Conclusion: RefProtoFL은 표준 벤치마크에서 실험을 통해 최신의 프로토타입 기반 연합 학습 방법들보다 더 나은 성능을 보여준다.

Abstract: Federated learning (FL) enables collaborative model training without sharing raw data in edge environments, but is constrained by limited communication bandwidth and heterogeneous client data distributions. Prototype-based FL mitigates this issue by exchanging class-wise feature prototypes instead of full model parameters; however, existing methods still suffer from suboptimal generalization under severe communication constraints. In this paper, we propose RefProtoFL, a communication-efficient FL framework that integrates External-Referenced Prototype Alignment (ERPA) for representation consistency with Adaptive Probabilistic Update Dropping (APUD) for communication efficiency. Specifically, we decompose the model into a private backbone and a lightweight shared adapter, and restrict federated communication to the adapter parameters only. To further reduce uplink cost, APUD performs magnitude-aware Top-K sparsification, transmitting only the most significant adapter updates for server-side aggregation. To address representation inconsistency across heterogeneous clients, ERPA leverages a small server-held public dataset to construct external reference prototypes that serve as shared semantic anchors. For classes covered by public data, clients directly align local representations to public-induced prototypes, whereas for uncovered classes, alignment relies on server-aggregated global reference prototypes via weighted averaging. Extensive experiments on standard benchmarks demonstrate that RefProtoFL attains higher classification accuracy than state-of-the-art prototype-based FL baselines.

</details>


### [41] [Anytime Optimal Decision Tree Learning with Continuous Features](https://arxiv.org/abs/2601.14765)
*Harold Kiossou,Pierre Schaus,Siegfried Nijssen*

Main category: cs.LG

TL;DR: 연속 특성을 가진 최적 결정 트리를 학습하기 위한 알고리즘의 내용을 다루고 있습니다. 제한된 불일치 검색을 활용하여 언제든지 사용할 수 있는 고품질 결정 트리를 제공합니다. 실험 결과는 우리의 접근 방식이 기존 방법보다 우수하다는 것을 보여줍니다.


<details>
  <summary>Details</summary>
Motivation: 연속 특성을 가진 결정 트리를 학습하는 과정에서 깊이 제한과 비효율적인 성능 문제를 해결하고자 합니다.

Method: 제한된 불일치 검색을 활용한 언제든지 사용할 수 있는 완전한 접근 방식을 제안합니다.

Result: 우리의 접근 방식이 기존의 접근 방식보다 언제든지 사용할 수 있는 성능에서 우수함을 입증합니다.

Conclusion: 제안된 방법은 언제든지 사용할 수 있는 고품질의 결정 트리를 보장하고, 중단 포인트에서 유용합니다.

Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.

</details>


### [42] [Reflecting in the Reflection: Integrating a Socratic Questioning Framework into Automated AI-Based Question Generation](https://arxiv.org/abs/2601.14798)
*Ondřej Holub,Essi Ryymin,Rodrigo Alves*

Main category: cs.LG

TL;DR: 이 논문은 반성 질문의 자동 생성을 위한 반성 내 반성 프레임워크를 소개하며, 이는 대화형 소크라틱 방식으로 질문을 정제하는 두 개의 전문화된 역할 에이전트를 조율한다.


<details>
  <summary>Details</summary>
Motivation: 반성 질문을 잘 디자인하는 것은 교육적으로 중요하지만, 시간 소모가 크고 교사마다 지원이 불균형적이다.

Method: 학생-교사와 교사-교육자라는 두 개의 역할 전문 에이전트를 조율하여 소크라틱 다중 턴 대화를 통해 교사가 지정한 주제에 따라 질문을 반복적으로 정제하는 접근 방식이다.

Result: 이 프레임워크는 GPT-4o-mini 모델을 주 모델로 사용하고, 더 강력한 GPT-4-class 모델을 외부 평가자로 하여 질문의 명확성, 관련성, 깊이 및 전반적인 품질을 비교 평가했다.

Conclusion: 우리의 두 에이전트 프로토콜은 동일한 기본 모델을 사용하는 단일 질문 기반 접근보다 훨씬 더 관련성이 높고 깊이 있는 질문을 생성함을 보여주었다.

Abstract: Designing good reflection questions is pedagogically important but time-consuming and unevenly supported across teachers. This paper introduces a reflection-in-reflection framework for automated generation of reflection questions with large language models (LLMs). Our approach coordinates two role-specialized agents, a Student-Teacher and a Teacher-Educator, that engage in a Socratic multi-turn dialogue to iteratively refine a single question given a teacher-specified topic, key concepts, student level, and optional instructional materials. The Student-Teacher proposes candidate questions with brief rationales, while the Teacher-Educator evaluates them along clarity, depth, relevance, engagement, and conceptual interconnections, responding only with targeted coaching questions or a fixed signal to stop the dialogue. We evaluate the framework in an authentic lower-secondary ICT setting on the topic, using GPT-4o-mini as the backbone model and a stronger GPT- 4-class LLM as an external evaluator in pairwise comparisons of clarity, relevance, depth, and overall quality. First, we study how interaction design and context (dynamic vs.fixed iteration counts; presence or absence of student level and materials) affect question quality. Dynamic stopping combined with contextual information consistently outperforms fixed 5- or 10-step refinement, with very long dialogues prone to drift or over-complication. Second, we show that our two-agent protocol produces questions that are judged substantially more relevant and deeper, and better overall, than a one-shot baseline using the same backbone model.

</details>


### [43] [Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models](https://arxiv.org/abs/2601.14917)
*Giorgia Rigamonti,Mirko Paolo Barbato,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 이 논문은 개인화된 혈당 예측을 위한 딥러닝 기반 접근법을 제시하며, 환자 특정 데이터를 활용하여 예측의 정확성과 반응성을 개선합니다.


<details>
  <summary>Details</summary>
Motivation: 1형 당뇨병의 효과적인 관리는 지속적인 혈당 모니터링과 정밀한 인슐린 조정이 필요하며, 이는 고혈당 및 저혈당을 예방하는 데 중요합니다.

Method: 환자 특정 데이터를 활용하여 개인화된 혈당 예측을 위한 딥러닝 기반 접근법을 개발하고, Leave-One-Subject-Out 교차 검증 및 파인튜닝 전략을 비교하여 환자 특정 역학을 모델링하는 능력을 평가합니다.

Result: 개인화된 모델은 부정적 사건 예측을 현저히 개선하여, 실제 상황에서 더 정밀하고 적시의 개입을 가능하게 합니다.

Conclusion: 적응형 개인화 혈당 예측 모델은 차세대 당뇨병 관리에 기여할 가능성이 있으며, 특히 착용 가능한 기기 및 모바일 건강 플랫폼에서 소비자 지향적인 당뇨병 관리 솔루션을 향상시킵니다.

Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.

</details>


### [44] [Improving Regret Approximation for Unsupervised Dynamic Environment Generation](https://arxiv.org/abs/2601.14957)
*Harry Mead,Bruno Lacerda,Jakob Foerster,Nick Hawes*

Main category: cs.LG

TL;DR: 본 연구에서는 동적 환경 생성을 통해 강화 학습 에이전트의 훈련 커리큘럼을 자동으로 생성하여 일반화 및 제로샷 성능을 향상시키는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습 에이전트를 위한 효과적인 커리큘럼 설계는 여전히 어려운 문제입니다. 환경 매개변수의 작은 부분집합이 정책 복잡도를 크게 증가시키는 설정에서는 특히 그렇습니다.

Method: 본 연구에서는 UED를 위한 동적 환경 생성(DEGen)을 제안하여 더 조밀한 수준 생성기 보상 신호를 가능하게 하고, 신용 할당의 어려움을 줄이며 UED가 더 큰 환경 크기로 확장될 수 있도록 합니다. 또한 최대화된 부정적 이점(MNA)이라는 새로운 후회 근사치를 도입하여 도전적인 수준을 더 잘 식별합니다.

Result: MNA는 현재의 후회 근사치를 능가하며, DEGen과 결합했을 때 기존 방법보다 일관되게 우수한 성능을 보입니다.

Conclusion: 이 연구는 동적 환경 생성을 통해 강화 학습의 효율성을 크게 향상시키고, 모든 코드를 여기에 공개하였습니다.

Abstract: Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.

</details>


### [45] [Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control](https://arxiv.org/abs/2601.15015)
*Jannis Becktepe,Aleksandra Franz,Nils Thuerey,Sebastian Peitz*

Main category: cs.LG

TL;DR: FluidGym은 강화 학습을 위한 첫 번째 독립적이고 완전 미분 가능한 벤치마크 모음입니다.


<details>
  <summary>Details</summary>
Motivation: 강화 학습이 능동 흐름 제어에서 유망한 결과를 보였지만, 기존 연구는 이질적인 관측 및 작동 체계, 수치 설정 및 평가 프로토콜에 의존하여 진행 상황을 평가하기 어렵습니다.

Method: FluidGym은 PyTorch로 작성되었으며, GPU 가속 PICT 해석기 위에서 완전히 작동하는 독립적인 벤치마크 모음입니다.

Result: PPO와 SAC로 기본 결과를 제시하고 모든 환경, 데이터 세트 및 훈련된 모델을 공개 자원으로 릴리스합니다.

Conclusion: FluidGym은 제어 방법의 체계적인 비교를 가능하게 하고, 학습 기반 흐름 제어에 대한 미래 연구의 확장 가능한 기반을 제공합니다.

Abstract: Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.

</details>


### [46] [A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2601.15038)
*Mertcan Daysalilar,Fuat Uyguroglu,Gabriel Nicolosi,Adam Meyers*

Main category: cs.LG

TL;DR: 이 연구에서는 전기차 경로 최적화 문제를 해결하기 위해 커리큘럼 기반 심층 강화 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 전기차 경로 최적화 문제는 지속 가능한 물류에서 복잡한 최적화 문제이며, 고객의 시간 제약을 충족하면서 총 이동 거리, 차량 대수, 배터리 사용을 최소화해야 합니다.

Method: 커리큘럼 기반 심층 강화 학습 (CB-DRL) 프레임워크는 문제의 복잡성을 점진적으로 증가시키는 세 단계의 커리큘럼을 활용합니다.

Result: 모델은 10명의 고객을 가진 작은 인스턴스에서 훈련된 후, 5에서 100 고객까지의 미지의 인스턴스에서 뛰어난 일반화를 보여주며, 중간 규모 문제에서 표준 기준을 크게 초월합니다.

Conclusion: 이 커리큘럼 기반 접근 방식은 표준 DRL 기준이 실패하는 분포 외 인스턴스에서도 높은 실행 가능성 비율과 경쟁력 있는 해 품질을 달성합니다.

Abstract: The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.

</details>


### [47] [Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning](https://arxiv.org/abs/2601.15086)
*Oleg Shchendrigin,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 효과적인 의사결정은 안정적이고 적응력 있는 메모리에 의존하며, 메모리 재작성 능력이 부족함을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계에서의 효과적인 의사결정은 안정적이고 적응적인 메모리를 필요로 한다.

Method: 우리는 부분적인 가시성 하에서 지속적인 메모리 업데이트를 명시적으로 테스트하는 벤치마크를 도입하였다.

Result: 고전적인 순환 모델이 현대 구조적 메모리와 트랜스포머 기반 에이전트보다 메모리 재작성 작업에서 더 우수한 유연성과 강건성을 보였다.

Conclusion: 현재 접근 방식의 한계를 노출시키고, 안정적인 보존과 적응적인 업데이트를 균형 있게 지원하는 메모리 메커니즘의 필요성을 강조한다.

Abstract: Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/

</details>


### [48] [Auditing Language Model Unlearning via Information Decomposition](https://arxiv.org/abs/2601.15111)
*Anmol Goel,Alan Ritter,Iryna Gurevych*

Main category: cs.LG

TL;DR: 기계적 잊기 접근 방식의 한계를暴露하며, 내부 표현에서 잊혀진 데이터에 대한 정보가 여전히 선형적으로 디코딩 가능하다는 사실을 보여줌.


<details>
  <summary>Details</summary>
Motivation: 현재 언어 모델의 기계적 잊기에 대한 접근 방식의 한계를 규명하고자 함.

Method: 부분 정보 분해(PID)를 사용하여 잊기가 이루어진 전후 모델 표현을 비교함으로써 상호 정보를 잊혀진 데이터에 대한 독립적인 요소들로 분해하는 해석 가능한 정보 이론적 프레임워크를 도입함.

Result: 분석 결과, 두 모델 간에 공유된 중복 정보가 잊힌 후에도 여전히 남아 잔여 지식으로 작용하며, 이는 알려진 적대적 재구성 공격에 대한 취약성과 상관관계를 가지는 것으로 나타남.

Conclusion: 우리의 연구는 기계적 잊기 위한 원칙적인 표현 수준 감사를 도입하여 언어 모델의 보다 안전한 배포를 위한 이론적 통찰력과 실행 가능한 도구를 제공합니다.

Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.

</details>


### [49] [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)
*Tianshi Xu,Yuteng Chen,Meng Li*

Main category: cs.LG

TL;DR: CLEANER는 에이전틱 강화 학습을 개선하기 위해 오류가 포함된 맥락을 데이터 수집 중 직접 제거하고, 고유한 자기 수정 기능을 활용하여 깨끗한 궤적을 만들어내는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 제한된 매개변수를 가진 모델의 탐색 단계에서 자주 실행 실패가 발생하여 정책 최적화에 장애가 된다.

Method: CLEANER는 실패를 성공적인 자기 수정으로 되돌리는 유사성 인식 적응 롤백 메커니즘(SAAR)을 사용하여 깨끗하고 정제된 궤적을 자동으로 구성한다.

Result: AIME24/25, GPQA, LiveCodeBench에서의 실험 결과 평균 정확도가 각각 6%, 3%, 5% 향상되었다.

Conclusion: CLEANER는 훈련 단계 수의 3분의 1만으로 최신 성능을 달성하여 효율적인 에이전틱 RL을 위한 확장 가능한 솔루션으로서 궤적 정제를 강조한다.

Abstract: Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

</details>
