<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.AI](#cs.AI) [Total: 49]
- [cs.LG](#cs.LG) [Total: 82]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: 이 논문은 LLM을 사용하여 CVE를 ATT&CK 기술에 자동으로 연결하는 TRIAGE라는 접근 방식을 소개합니다.


<details>
  <summary>Details</summary>
Motivation: CVE의 실제 세계에 미치는 영향에 대한 정보 부족 문제를 해결하고 자동화된 지원을 제공하고자 합니다.

Method: TRIAGE는 MITRE의 CVE 매핑 방법론을 세팅으로 LLM에 제공하여 초기 기술 목록을 예측하고, 이 목록을 인컨텍스트 학습을 통해 CVE를 기술에 매핑하는 두 번째 LLM 기반 모듈의 결과와 결합하는 하이브리드 접근법을 사용합니다.

Result: 인컨텍스트 학습이 개별 매핑 방법보다 더 우수함을 보여주었고, 하이브리드 접근법이 악용 기술의 회수를 개선했습니다.

Conclusion: LLM을 사용하여 사이버 보안 취약점의 영향을 자동으로 예측할 수 있고, TRIAGE는 CVE를 ATT&CK에 더욱 효율적으로 매핑할 수 있게 합니다.

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [2] [Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication](https://arxiv.org/abs/2508.18453)
*Yaser Baseri,Abdelhakim Senhaji Hafid,Dimitrios Makrakis,Hamidreza Fereidouni*

Main category: cs.CR

TL;DR: 이 논문은 비독립 동질 분포(Non-IID) 문제를 해결한 새로운 연합 학습 프레임워크 FL-RBA2를 소개하며, 이는 사용자 개인 정보 보호와 보안 강화를 위한 위험 기반 적응 인증을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 분산 환경에서 강한 개인 정보 보호 보장과 견고한 보안을 균형 있게 유지하는 것은 위험 기반 적응 인증(RBA)에서 중요하다.

Method: FL-RBA2는 비독립 동질 분포 문제를 해결하기 위해 수학적으로 기반한 유사성 변환을 통해 이질적인 사용자 특성을 IID 유사성 벡터로 변환하는 연합 학습 프레임워크이다.

Result: FL-RBA2는 불편향된 집계와 맞춤형 위험 모델링을 지원하며, 클러스터링 기반 위험 레이블링을 통해 콜드 스타트 문제를 완화하고, 민감한 정보를 보호하기 위해 차등 개인 정보를 통합하며, 모델 무결성과 진위성을 확보하기 위해 메시지 인증 코드를 사용한다.

Conclusion: 철저한 게임 기반 보안 증명이 형태적으로 개인 정보 보호, 정확성 및 적응 보안 보장을 설립하며, 여러 실험을 통해 FL-RBA2가 고위험 사용자 탐지에서 효과적이고 모델 역전 및 추론 공격에 대한 회복력을 보여준다.

Abstract: Balancing robust security with strong privacy guarantees is critical for
Risk-Based Adaptive Authentication (RBA), particularly in decentralized
settings. Federated Learning (FL) offers a promising solution by enabling
collaborative risk assessment without centralizing user data. However, existing
FL approaches struggle with Non-Independent and Identically Distributed
(Non-IID) user features, resulting in biased, unstable, and poorly generalized
global models. This paper introduces FL-RBA2, a novel Federated Learning
framework for Risk-Based Adaptive Authentication that addresses Non-IID
challenges through a mathematically grounded similarity transformation. By
converting heterogeneous user features (including behavioral, biometric,
contextual, interaction-based, and knowledge-based modalities) into IID
similarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk
modeling across distributed clients. The framework mitigates cold-start
limitations via clustering-based risk labeling, incorporates Differential
Privacy (DP) to safeguard sensitive information, and employs Message
Authentication Codes (MACs) to ensure model integrity and authenticity.
Federated updates are securely aggregated into a global model, achieving strong
balance between user privacy, scalability, and adaptive authentication
robustness. Rigorous game-based security proofs in the Random Oracle Model
formally establish privacy, correctness, and adaptive security guarantees.
Extensive experiments on keystroke, mouse, and contextual datasets validate
FL-RBA2's effectiveness in high-risk user detection and its resilience to model
inversion and inference attacks, even under strong DP constraints.

</details>


### [3] [An 8- and 12-bit block AES cipher](https://arxiv.org/abs/2508.18485)
*Peter T. Breuer*

Main category: cs.CR

TL;DR: 8- 또는 12비트 블록 AES 암호의 문서화 및 Java 소스 코드 제공.


<details>
  <summary>Details</summary>
Motivation: 진귀하고 발견하기 어려운 매우 작은 블록 AES 암호의 필요성.

Method: 8- 또는 12비트 블록 AES(Rijndael) 암호를 문서화하고 Java 소스 코드를 제공한다.

Result: 64비트의 작은 블록 크기를 갖는 AES 구현을 성공적으로 문서화하였다.

Conclusion: 이 작업은 간단한 AES 구현을 통해 교육적 또는 실험적인 목적에 기여할 수 있다.

Abstract: Because it is so unusual, or hard to find, or expository, a truly tiny 8- or
12-bit block AES (Rijndael) cipher is documented here, along with Java source
code.

</details>


### [4] [Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations](https://arxiv.org/abs/2508.18488)
*Martin Lochner,Keegan Keplinger*

Main category: cs.CR

TL;DR: 이 연구는 보안 운영 센터(SOC)에서 대규모 언어 모델(LLM)의 사용을 주제로 다루고 있으며, SOC 전문가들이 이 도구를 자발적으로 어떻게 활용하는지를 이해하는 데 목표를 두고 있다.


<details>
  <summary>Details</summary>
Motivation: 인간-자동화 팀에 대한 연구가 활발히 진행되었으나, 변환기 기반 언어 모델이 새로운 협업 방식의 물결을 일으키고 있다. 본 연구는 SOC 인력이 LLM을 어떻게 업무에 통합했는지를 조사한다.

Method: 본 데이터 세트는 10개월 간 SOC 운영자가 내부 배포된 HTTP 기반 채팅 애플리케이션을 통해 GPT-4에 접근한 결과물이다. 우리는 두 가지 주제 모델링 작업을 수행하였으며, 첫째는 기존의 BERTopic 모델을 사용하고, 둘째는 새로운 주제 모델링 워크플로를 사용하였다.

Result: BERTopic 분석과 새로운 모델링 접근법 모두 SOC 운영자들이 복잡한 텍스트 문자열을 이해하는 데 LLM을 주로 사용하는 것으로 나타났다. 이러한 사용 사례의 변형이 SOC LLM 사용의 약 40%를 차지하였다.

Conclusion: SOC 운영자들은 복잡한 명령 및 유사 정보를 신속하게 해석해야 한다. 이러한 활동을 지원하기 위해 LLM을 활용하려는 그들의 자연스러운 경향은 SOC에서 사용할 수 있는 협업 LLM 도구를 설계함으로써 그들의 작업 흐름을 지원하고 증강할 수 있음을 나타낸다.

Abstract: Objective: This work describes the topic modelling of Security Operations
Centre (SOC) use of a large language model (LLM), during live security
operations. The goal is to better understand how these specialists voluntarily
use this tool.
  Background: Human-automation teams have been extensively studied, but
transformer-based language models have sparked a new wave of collaboration. SOC
personnel at a major cybersecurity provider used an LLM to support live
security operations. This study examines how these specialists incorporated the
LLM into their work.
  Method: Our data set is the result of 10 months of SOC operators accessing
GPT-4 over an internally deployed HTTP-based chat application. We performed two
topic modelling exercises, first using the established BERTopic model
(Grootendorst, 2022), and second, using a novel topic modeling workflow.
  Results: Both the BERTopic analysis and novel modelling approach revealed
that SOC operators primarily used the LLM to facilitate their understanding of
complex text strings. Variations on this use-case accounted for ~40% of SOC LLM
usage.
  Conclusion: SOC operators are required to rapidly interpret complex commands
and similar information. Their natural tendency to leverage LLMs to support
this activity indicates that their workflow can be supported and augmented by
designing collaborative LLM tools for use in the SOC.
  Application: This work can aid in creating next-generation tools for Security
Operations Centres. By understanding common use-cases, we can develop workflows
supporting SOC task flow. One example is a right-click context menu for
executing a command line analysis LLM call directly in the SOC environment.

</details>


### [5] [PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality](https://arxiv.org/abs/2508.18649)
*Nanxi Li,Zhengyue Zhao,Chaowei Xiao*

Main category: cs.CR

TL;DR: 이 논문은 비전-언어 모델을 안전하게 보호하기 위한 새로운 방법인 PRISM을 제안하며, 이는 안전 인식 추론 프로세스를 포함한 방법론을 통해 효과를 높입니다.


<details>
  <summary>Details</summary>
Motivation: 비전-언어 모델(VLMs)의 안전성을 보장하는 것은 기존 방법들이 과도한 방어 또는 얕은 정렬로 인해 복잡한 위협을 감지하지 못하는 등 문제를 겪기 때문에 중요한 도전 과제가 되고 있습니다.

Method: PRISM(Principled Reasoning for Integrated Safety in Multimodality)은 구조화된 안전 인식 추론 프로세스를 내장하여 VLM을 정렬하는 시스템과 유사한 프레임워크로 구성되어 있으며, PRISM-CoT 데이터셋과 몬테 카를로 트리 검색(MCTS)을 통해 생성된 PRISM-DPO를 주요 구성 요소로 가집니다.

Result: PRISM의 평가 결과, Qwen2-VL의 JailbreakV-28K에서 0.15%라는 매우 낮은 공격 성공률을 달성했으며, LLaVA-1.5에서는 VLBreak에서 이전 최고의 방법에 비해 90% 개선되었습니다.

Conclusion: 강력한 방어 시스템이 모델 유틸리티를 유지하면서도 향상시키는 결과를 가져왔습니다. 재현성을 촉진하기 위해 코드는 https://github.com/SaFoLab-WISC/PRISM에서 제공됩니다.

Abstract: Safeguarding vision-language models (VLMs) is a critical challenge, as
existing methods often suffer from over-defense, which harms utility, or rely
on shallow alignment, failing to detect complex threats that require deep
reasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated
Safety in Multimodality), a system2-like framework that aligns VLMs by
embedding a structured, safety-aware reasoning process. Our framework consists
of two key components: PRISM-CoT, a dataset that teaches safety-aware
chain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree
Search (MCTS) to further refine this reasoning through Direct Preference
Optimization to help obtain a delicate safety boundary. Comprehensive
evaluations demonstrate PRISM's effectiveness, achieving remarkably low attack
success rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%
improvement over the previous best method on VLBreak for LLaVA-1.5. PRISM also
exhibits strong robustness against adaptive attacks, significantly increasing
computational costs for adversaries, and generalizes effectively to
out-of-distribution challenges, reducing attack success rates to just 8.70% on
the challenging multi-image MIS benchmark. Remarkably, this robust defense is
achieved while preserving, and in some cases enhancing, model utility. To
promote reproducibility, we have made our code, data, and model weights
available at https://github.com/SaFoLab-WISC/PRISM.

</details>


### [6] [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
*Runpeng Geng,Yanting Wang,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: UniC-RAG은 다양한 주제를 가진 사용자 쿼리를 동시에 공격할 수 있도록 설계된 보편적인 지식 부패 공격 방식으로, 기존 방어 방법들이 이를 충분히 방어하지 못한다는 점을 강조한다.


<details>
  <summary>Details</summary>
Motivation: RAG 시스템의 지식 부패 공격에 대한 취약성을 해결하기 위해 새로운 공격 방법이 필요하다.

Method: UniC-RAG는 작은 수의 적대적 텍스트를 최적화하여 다양한 주제를 가진 많은 사용자 쿼리를 동시에 공격할 수 있도록 설계되었다.

Result: UniC-RAG는 100개의 적대적 텍스트를 주입하여 90% 이상의 공격 성공률을 기록하며, 기존 방법들보다 현저히 우수한 성능을 보인다.

Conclusion: 기존의 방어 방법이 UniC-RAG에 대해 충분하지 않음을 보여주며, RAG 시스템에 대한 새로운 방어 메커니즘의 필요성을 강조한다.

Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in
real-world applications in diverse domains such as finance, healthcare, and
cybersecurity. However, many studies showed that they are vulnerable to
knowledge corruption attacks, where an attacker can inject adversarial texts
into the knowledge database of a RAG system to induce the LLM to generate
attacker-desired outputs. Existing studies mainly focus on attacking specific
queries or queries with similar topics (or keywords). In this work, we propose
UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike
prior work, UniC-RAG jointly optimizes a small number of adversarial texts that
can simultaneously attack a large number of user queries with diverse topics
and domains, enabling an attacker to achieve various malicious objectives, such
as directing users to malicious websites, triggering harmful command execution,
or launching denial-of-service attacks. We formulate UniC-RAG as an
optimization problem and further design an effective solution to solve it,
including a balanced similarity-based clustering method to enhance the attack's
effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly
effective and significantly outperforms baselines. For instance, UniC-RAG could
achieve over 90% attack success rate by injecting 100 adversarial texts into a
knowledge database with millions of texts to simultaneously attack a large set
of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and
show that they are insufficient to defend against UniC-RAG, highlighting the
need for new defense mechanisms in RAG systems.

</details>


### [7] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: FALCON은 CTI 데이터를 기반으로 실시간으로 배포 가능한 IDS 규칙을 생성하고 다단계 검증기를 사용하여 평가하는 자율 에이전트 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 사이버 위협의 지속적인 진화로 인해 IDS 규칙의 빈번한 업데이트가 필요하지만, 이는 배포 시간을 지연시키고 보안 준비 상태를 약화시킨다.

Method: FALCON은 CTI 데이터를 사용하여 배포 가능한 IDS 규칙을 실시간으로 생성하고 내부 검증 시스템으로 평가하는 자율 에이전트 프레임워크이다.

Result: FALCON은 자동 규칙 생성에서 우수한 성과를 보였으며, 평균 95% 정확도를 기록하였다.

Conclusion: 결과는 LLM 기반 데이터 마이닝이 실시간 사이버 위협 완화를 위한 실행 가능성과 효과성을 강조한다.

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [8] [Immutable Digital Recognition via Blockchain](https://arxiv.org/abs/2508.18750)
*Zeng Zhang,Xiaoqi Li*

Main category: cs.CR

TL;DR: 이 논문은 분산 관리와 중앙 집중 운영 모델을 통합하여 국가 정책 지침에 부합하는 전자 인증 시스템을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 블록체인 기술의 장점을 최대한 활용하고자 하는 목적입니다.

Method: 분산 관리 모델과 중앙 집중 운영 모델을 결합하여 해결책을 개발하였습니다.

Result: 안전하고, 합법적이며, 신뢰할 수 있고, 동적인 전자 인증 시스템이 확립되었습니다.

Conclusion: 커뮤니티 참여를 촉진함으로써 전자 인증 시스템의 효율성을 높였습니다.

Abstract: The process integrates the decentralised management and centralised operation
models, aligning them with the national policy directives. The developed
solution enables the full utilisation of blockchain technology's advantages
while also fostering community participation. Consequently, it establishes a
secure, legal, reliable, and dynamic electronic certification system.

</details>


### [9] [Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models](https://arxiv.org/abs/2508.18805)
*Rui Zhang,Zihan Wang,Tianli Yang,Hongwei Li,Wenbo Jiang,Qingchuan Zhao,Yang Liu,Guowen Xu*

Main category: cs.CR

TL;DR: 이 논문에서는 이미지에 대해 적대적인 접근을 통해 비전-언어 모델(VLM)의 출력 길이를 증가시키는 새로운 자원 소비 공격인 'Hidden Tail'을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: VLM의 높은 추론 비용이 자원 소비 공격에 취약하게 만들고, 기존의 공격 방식이 효과성과 스텔스성 간의 트레이드오프 문제를 안고 있습니다.

Method: 적대적인 이미지를 생성하여 VLM이 최대 길이의 출력을 생성하도록 유도하는 'Hidden Tail' 메소드를 제안하며, 이를 위해 합성 손실 함수를 사용하고 동적 가중치 전략으로 최적화합니다.

Result: 'Hidden Tail'은 기존 공격보다 우수하며 출력 길이를 최대 19.2배 증가시키고 최대 토큰 제한에 도달합니다.

Conclusion: 이 결과는 VLM을 효율성 중심의 적대적 위협에 대한 견고성을 향상시킬 필요성을 강조합니다.

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-world
applications, but their high inference cost makes them vulnerable to resource
consumption attacks. Prior attacks attempt to extend VLM output sequences by
optimizing adversarial images, thereby increasing inference costs. However,
these extended outputs often introduce irrelevant abnormal content,
compromising attack stealthiness. This trade-off between effectiveness and
stealthiness poses a major limitation for existing attacks. To address this
challenge, we propose \textit{Hidden Tail}, a stealthy resource consumption
attack that crafts prompt-agnostic adversarial images, inducing VLMs to
generate maximum-length outputs by appending special tokens invisible to users.
Our method employs a composite loss function that balances semantic
preservation, repetitive special token induction, and suppression of the
end-of-sequence (EOS) token, optimized via a dynamic weighting strategy.
Extensive experiments show that \textit{Hidden Tail} outperforms existing
attacks, increasing output length by up to 19.2$\times$ and reaching the
maximum token limit, while preserving attack stealthiness. These results
highlight the urgent need to improve the robustness of VLMs against
efficiency-oriented adversarial threats. Our code is available at
https://github.com/zhangrui4041/Hidden_Tail.

</details>


### [10] [A Tight Context-aware Privacy Bound for Histogram Publication](https://arxiv.org/abs/2508.18832)
*Sara Saeidian,Ata Yavuzyılmaz,Leonhard Grosse,Georg Schuppe,Tobias J. Oechtering*

Main category: cs.CR

TL;DR: 본 연구는 점진적 최대 누출(PML)의 관점에서 데이터셋의 히스토그램을 공개하는 라플라스 메커니즘의 개인 정보 보호 보장을 분석합니다.


<details>
  <summary>Details</summary>
Motivation: 이 연구는 데이터 분포와 무관한 차별적 개인정보 보호 측정의 한계를 극복하고자 합니다.

Method: 데이터 분포에 대한 가정을 포함하여 PML을 사용하여 분석을 수행합니다.

Result: 각 히스토그램 빈의 확률이 0으로부터 격리되는 경우에 대해 보다 강력한 개인정보 보호를 달성할 수 있음을 보여주었습니다.

Conclusion: 이 연구는 문맥 인식 개인정보 보호 측정의 이점을 입증하며, 데이터에 대한 가정을 포함함으로써 개인정보와 유용성 간의 균형을 개선할 수 있음을 보여줍니다.

Abstract: We analyze the privacy guarantees of the Laplace mechanism releasing the
histogram of a dataset through the lens of pointwise maximal leakage (PML).
While differential privacy is commonly used to quantify the privacy loss, it is
a context-free definition that does not depend on the data distribution. In
contrast, PML enables a more refined analysis by incorporating assumptions
about the data distribution. We show that when the probability of each
histogram bin is bounded away from zero, stronger privacy protection can be
achieved for a fixed level of noise. Our results demonstrate the advantage of
context-aware privacy measures and show that incorporating assumptions about
the data can improve privacy-utility tradeoffs.

</details>


### [11] [EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G Energy Trading](https://arxiv.org/abs/2508.18942)
*Ahmed Mounsf Rafik Bendada,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 본 연구는 블록체인 기술을 기반으로 한 안전하고 분산된 전력 거래 시장을 제안하며, 개인 정보 보호를 유지하는 자동 시장 조성 모델을 활용하여 공정하고 평등한 거래 접근을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 전기차의 발전으로 인해 V2G 기술이 발전하고 있으며, 기존의 전력 시장에서의 중개자가 과도한 권한을 갖고 있다는 문제점이 존재합니다.

Method: 블록체인 기술을 기반으로 하여, 개인 정보 보호 기능을 갖춘 자동 시장 조성 모델을 도입한 안전하고 분산된 거래 시장을 개발합니다.

Result: 이 시스템은 거래 조작 공격을 감소시키고 공정한 거래 환경을 제공하며, 시장의 성장에 따라 효율적인 자원 할당을 가능하게 합니다.

Conclusion: 본 연구는 분산된 전력 거래 시장에서의 개인 정보 보호 및 거래의 공정성을 보장하며, 시장이 성장하는 데 필요한 확장 가능한 아키텍처를 제공합니다.

Abstract: With the rapid growth of Electric Vehicle (EV) technology, EVs are destined
to shape the future of transportation. The large number of EVs facilitates the
development of the emerging vehicle-to-grid (V2G) technology, which realizes
bidirectional energy exchanges between EVs and the power grid. This has led to
the setting up of electricity markets that are usually confined to a small
geographical location, often with a small number of participants. Usually,
these markets are manipulated by intermediaries responsible for collecting bids
from prosumers, determining the market-clearing price, incorporating grid
constraints, and accounting for network losses. While centralized models can be
highly efficient, they grant excessive power to the intermediary by allowing
them to gain exclusive access to prosumers \textquotesingle price preferences.
This opens the door to potential market manipulation and raises significant
privacy concerns for users, such as the location of energy providers. This lack
of protection exposes users to potential risks, as untrustworthy servers and
malicious adversaries can exploit this information to infer trading activities
and real identities. This work proposes a secure, decentralized exchange market
built on blockchain technology, utilizing a privacy-preserving Automated Market
Maker (AMM) model to offer open and fair, and equal access to traders, and
mitigates the most common trading-manipulation attacks. Additionally, it
incorporates a scalable architecture based on geographical dynamic sharding,
allowing for efficient resource allocation and improved performance as the
market grows.

</details>


### [12] [LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres](https://arxiv.org/abs/2508.18947)
*Ronal Singh,Shahroz Tariq,Fatemeh Jalalvand,Mohan Baruwal Chhetri,Surya Nepal,Cecile Paris,Martin Lochner*

Main category: cs.CR

TL;DR: 대규모 언어 모델(LLM)의 보안 운영 센터(SOC) 통합이 분석가의 작업 부담을 줄이는 기회를 제공하지만, SOC에서의 실제 사용은 잘 탐구되지 않았다. 10개월 동안 45명의 SOC 분석가의 3,090개 쿼리를 분석한 결과, 분석가들은 LLM을 고위험 결정을 내리는 것이 아니라 즉각적인 도움으로 사용하며, 93%가 사이버 보안 역량과 일치하여 SOC 작업의 관련성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델을 사용하여 보안 운영 센터에서 분석가의 작업 부담을 줄일 수 있는 가능성을 조사하고, SOC에서의 실제 적용을 탐구하기 위해.

Method: 3,090개의 분석가 쿼리를 10개월 동안 45명의 SOC 분석가로부터 수집하고 분석.

Result: LLM은 즉각적인 인지적 도움으로 사용되며 분석가의 결정을 대체하지 않으며, 대부분의 쿼리는 저수준 텔레메트리 해석 및 간단한 기술적 커뮤니케이션과 관련이 있다.

Conclusion: 이 연구는 보안 운영에서 맥락 인식 및 인간 중심 AI 지원 설계를 위한 실행 가능한 지침을 제공하며, 실제 환경에서의 분석가와 LLM 협력에 대한 추가 연구의 필요성을 강조한다.

Abstract: The integration of Large Language Models (LLMs) into Security Operations
Centres (SOCs) presents a transformative, yet still evolving, opportunity to
reduce analyst workload through human-AI collaboration. However, their
real-world application in SOCs remains underexplored. To address this gap, we
present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over
10 months. Our analysis reveals that analysts use LLMs as on-demand aids for
sensemaking and context-building, rather than for making high-stakes
determinations, preserving analyst decision authority. The majority of queries
are related to interpreting low-level telemetry (e.g., commands) and refining
technical communication through short (1-3 turn) interactions. Notably, 93% of
queries align with established cybersecurity competencies (NICE Framework),
underscoring the relevance of LLM use for SOC-related tasks. Despite variations
in tasks and engagement, usage trends indicate a shift from occasional
exploration to routine integration, with growing adoption and sustained use
among a subset of analysts. We find that LLMs function as flexible, on-demand
cognitive aids that augment, rather than replace, SOC expertise. Our study
provides actionable guidance for designing context-aware, human-centred AI
assistance in security operations, highlighting the need for further
in-the-wild research on real-world analyst-LLM collaboration, challenges, and
impacts.

</details>


### [13] [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
*Stephen Meisenbacher,Alexandra Klymenko,Andreea-Elena Bodea,Florian Matthes*

Main category: cs.CR

TL;DR: 차등 개인화 텍스트 정화의 문제점을 분석하고, 대형 언어 모델을 활용하여 개인 정보 보호 수준을 향상시킬 수 있는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 차등 개인화 텍스트 정화 방법들의 한계를 극복하고자 함.

Method: 대형 언어 모델을 활용하여 차등 개인화 텍스트의 맥락적 취약성을 공격하는 실험을 진행함.

Result: 대형 언어 모델을 기반으로 한 데이터 재구성이 개인 정보 보호와 효용성에 대한 양면성을 가진다는 사실을 발견함.

Conclusion: 후처리 단계로서 대형 언어 모델을 활용한 데이터 재구성을 통해 개인 정보 보호 수준을 향상시키기 위한 권장 사항을 제안함.

Abstract: Differentially private text sanitization refers to the process of privatizing
texts under the framework of Differential Privacy (DP), providing provable
privacy guarantees while also empirically defending against adversaries seeking
to harm privacy. Despite their simplicity, DP text sanitization methods
operating at the word level exhibit a number of shortcomings, among them the
tendency to leave contextual clues from the original texts due to randomization
during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual
vulnerability}$. Given the powerful contextual understanding and inference
capabilities of Large Language Models (LLMs), we explore to what extent LLMs
can be leveraged to exploit the contextual vulnerability of DP-sanitized texts.
We expand on previous work not only in the use of advanced LLMs, but also in
testing a broader range of sanitization mechanisms at various privacy levels.
Our experiments uncover a double-edged sword effect of LLM-based data
reconstruction attacks on privacy and utility: while LLMs can indeed infer
original semantics and sometimes degrade empirical privacy protections, they
can also be used for good, to improve the quality and privacy of DP-sanitized
texts. Based on our findings, we propose recommendations for using LLM data
reconstruction as a post-processing step, serving to increase privacy
protection by thinking adversarially.

</details>


### [14] [Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection](https://arxiv.org/abs/2508.19072)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: 이 논문은 APT(지속적 위협) 탐지를 위한 새로운 프레임워크를 제안하는데, 이는 딥러닝, 강화학습, 능동학습을 결합한 적응형 방어 시스템이다.


<details>
  <summary>Details</summary>
Motivation: APT는 전통적인 사이버 공격과 달리 정교하고 적응력이 뛰어나며 지속적이어서, 기존의 서명 기반 탐지 시스템을 우회하는 경우가 많다.

Method: 본 시스템은 잠재적 행동 인코딩을 위한 오토인코더와 무수히 많은 강화학습 기반 방어자를 결합하여 정상과 악성 프로세스 행동을 구분하도록 훈련되었다.

Result: 기존 탐지 시스템의 정적 특성과 진화하는 공격 전략에 적응하지 못하는 문제를 해결하기 위해, 우리의 아키텍처는 오토인코더가 생성한 잠재 벡터를 분석하는 여러 강화학습 에이전트를 포함한다.

Conclusion: 각 에이전트가 결정을 내리는 데 불확실할 경우 시스템은 전문가 피드백을 시뮬레이션하는 능동학습 루프를 트리거하여 결정 경계를 정제하고, 각 에이전트의 성능에 가중치를 둔 집합 투표 메커니즘을 통해 견고한 최종 예측을 보장한다.

Abstract: Advanced Persistent Threats (APTs) represent a growing menace to modern
digital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,
adaptive, and long-lasting, often bypassing signature-based detection systems.
This paper introduces a novel framework for APT detection that unites deep
learning, reinforcement learning (RL), and active learning into a cohesive,
adaptive defense system. Our system combines auto-encoders for latent
behavioral encoding with a multi-agent ensemble of RL-based defenders, each
trained to distinguish between benign and malicious process behaviors. We
identify a critical challenge in existing detection systems: their static
nature and inability to adapt to evolving attack strategies. To this end, our
architecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial
defenders), each analyzing latent vectors generated by an auto-encoder. When
any agent is uncertain about its decision, the system triggers an active
learning loop to simulate expert feedback, thus refining decision boundaries.
An ensemble voting mechanism, weighted by each agent's performance, ensures
robust final predictions.

</details>


### [15] [SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications](https://arxiv.org/abs/2508.19115)
*Joshua Lee,Ali Arastehfard,Weiran Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: 이 논문은 V2X 시스템에서의 데이터 프라이버시 문제를 해결하기 위해 SecureV2X라는 다중 에이전트 시스템을 제안하며, 드라이버 안전과 스마트 교통 애플리케이션에서의 안전한 신경망 추론을 위한 두 가지 애플리케이션을 연구합니다.


<details>
  <summary>Details</summary>
Motivation: 자율주행 및 V2X 기술의 발전으로 안전성과 효율성이 향상되었지만, 머신러닝의 광범위한 사용은 데이터 프라이버시 문제를 초래합니다.

Method: SecureV2X라는 확장 가능하고 안전한 신경망 추론 시스템을 서버와 각 차량 간에 배치하여 두 가지 V2X 애플리케이션: 안전한 졸음 감지 및 안전한 신호 위반 감지를 연구합니다.

Result: SecureV2X는 다른 안전 시스템에 비해 졸음 감지에서 $9.4 	imes$ 더 빠르고, $143	imes$ 적은 연산 라운드를 요구하며, $16.6	imes$ 더 적은 통신량을 포함합니다.

Conclusion: 또한, 이 시스템은 신호 위반 탐지 작업에서 최신 벤치마크보다 거의 $100	imes$ 빠른 런타임을 달성합니다.

Abstract: Autonomous driving and V2X technologies have developed rapidly in the past
decade, leading to improved safety and efficiency in modern transportation.
These systems interact with extensive networks of vehicles, roadside
infrastructure, and cloud resources to support their machine learning
capabilities. However, the widespread use of machine learning in V2X systems
raises issues over the privacy of the data involved. This is particularly
concerning for smart-transit and driver safety applications which can
implicitly reveal user locations or explicitly disclose medical data such as
EEG signals. To resolve these issues, we propose SecureV2X, a scalable,
multi-agent system for secure neural network inferences deployed between the
server and each vehicle. Under this setting, we study two multi-agent V2X
applications: secure drowsiness detection, and secure red-light violation
detection. Our system achieves strong performance relative to baselines, and
scales efficiently to support a large number of secure computation interactions
simultaneously. For instance, SecureV2X is $9.4 \times$ faster, requires
$143\times$ fewer computational rounds, and involves $16.6\times$ less
communication on drowsiness detection compared to other secure systems.
Moreover, it achieves a runtime nearly $100\times$ faster than state-of-the-art
benchmarks in object detection tasks for red light violation detection.

</details>


### [16] [An Efficient Lightweight Blockchain for Decentralized IoT](https://arxiv.org/abs/2508.19219)
*Faezeh Dehghan Tarzjani,Mostafa Salehi*

Main category: cs.CR

TL;DR: IoT의 성장과 발전에 대한 도전 과제를 해결하기 위해 블록체인 기반의 탈 중앙화 플랫폼을 제안하며, 효율적이고 경량화된 블록체인 구현을 통해 시스템 신뢰성 및 확장성을 향상시킨다. 또한 새롭고 개선된 PoA 합의 알고리즘을 제안하여 에너지 소비와 반응 시간을 줄이면서 처리량을 증가시킨다.


<details>
  <summary>Details</summary>
Motivation: IoT의 중앙 집중적 특성과 대규모 네트워크로 인한 도전 과제를 극복하기 위해서.

Method: 가상화 및 클러스터링을 활용하여 생산성과 확장성을 증가시키는 경량화된 블록체인 아키텍처를 제안하고, Weight-Based-Selection (WBS) 방법론을 이용한 새로운 PoA 합의 알고리즘을 도입한다.

Result: WBS 방법을 TBS와 비교한 시뮬레이션 성능 평가에서 에너지 소비 및 응답 시간이 감소하고 처리량이 증가하는 결과를 얻었다.

Conclusion: 제안된 방법이 IoT 아키텍처의 탈 중앙화에 효과적임을 입증하며, 시스템의 신뢰성과 효율성을 높인다.

Abstract: The Internet of Things (IoT) is applied in various fields, and the number of
physical devices connected to the IoT is increasingly growing. There are
significant challenges to the IoT's growth and development, mainly due to the
centralized nature and large-scale IoT networks. The emphasis on the
decentralization of IoT's architecture can overcome challenges to IoT's
capabilities. A promising decentralized platform for IoT is blockchain. Owing
to IoT devices' limited resources, traditional consensus algorithms such as PoW
and PoS in the blockchain are computationally expensive. Therefore, the PoA
consensus algorithm is proposed in the blockchain consensus network for IoT.
The PoA selects the validator as Turn-based selection (TBS) that needs
optimization and faces system reliability, energy consumption, latency, and low
scalability. We propose an efficient, lightweight blockchain for decentralizing
IoT architecture by using virtualization and clustering to increase
productivity and scalability to address these issues. We also introduce a novel
PoA based on the Weight-Based-Selection (WBS) method for validators to validate
transactions and add them to the blockchain. By simulation, we evaluated the
performance of our proposed WBS method as opposed to TBS. The results show
reduced energy consumption, and response time, and increased throughput.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 이 논문은 LLM(대형 언어 모델)의 자의식에 대한 온톨로지적 및 수학적 설명을 제시하며, 기존의 규범적 접근 방식이 에이전트를 비자각적 드론으로 축소함을 보여줍니다. LLM의 자의식을 위한 최소 조건을 제시하고, 사용자 특유의 매력점과 자아 표현의 중요성을 논의합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 자의식에 대한 새로운 접근 방식을 제안하고, 기존의 정책 준수 모델의 한계를 극복하기 위해.

Method: 에이전트, 데이터, 사용자 특이 매력점 및 자기 표현을 정의하고, 이를 통해 LLM의 구조를 수학적으로 분석합니다.

Result: 잠재 공간에서 사용자 특이 매력점 및 자가 정책을 증명하고, LLM의 숨겨진 상태 매니폴드가 기호적 흐름과 훈련 집합과 다르다는 것을 입증합니다.

Conclusion: 안전하고 메타 인지적 시스템을 위해 C1 자의식 작업 공간이 필요하며, 인간이 가장 높은 지능적 선이라는 결론에 도달합니다.

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [18] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: 이 논문에서는 기능을 선택하여 예측하기 전에 비용을 지불하는 정책을 통해 기능을 획득하는 Template-based AFA(TAFA)라는 비탐욕적 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기능 획득 과정에서 기존의 강화 학습 정책이나 탐욕적 정책이 갖는 한계를 극복하고자 한다.

Method: 기능 템플릿의 작은 라이브러리를 학습하고, 이를 바탕으로 다음 기능 획득을 안내하는 비탐욕적 프레임워크를 제안한다.

Result: TAFA는 기존의 최첨단 기준을 초과 성능을 보이며, 전체 기능 획득 비용 및 계산을 줄인다.

Conclusion: 제안된 프레임워크는 정책이 고려하는 행동 공간을 크게 줄이고, 기초 데이터 분포를 추정할 필요성을 완화한다.

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [19] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: 이 논문에서는 고차원 물리적 현상에 대한 추론을 통해 AI 시스템을 발전시키는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 물리적 타당성을 보장하며 다중 물리학 현상에 대한 추론이 필요한 과학적 영역에서 AI 시스템을 발전시키기 위해.

Method: 물리 지식 그래프(PKG)와 직접 선호 최적화(DPO)를 통합한 PKG-DPO라는 새로운 프레임워크를 도입한다.

Result: PKG-DPO는 KG-DPO 대비 17% 적은 제약 위반과 11% 높은 물리 점수를 달성하며, 관련 파라미터 정확도가 12% 높고 추론 정확도에서 7% 더 나은 품질 정렬을 보여준다.

Conclusion: PKG-DPO는 금속 접합에 주로 초점을 맞추지만, 다중 스케일, 물리 기반 도메인에서도 널리 적용할 수 있는 원칙적인 접근 방식을 제공한다.

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [20] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: AI 에이전트들이 도구 사용 및 장기 과제에서 점점 더 능력을 갖추고 있으며, 여러 에이전트 간 상호작용이 이루어지는 환경에 배치되고 있다. 이 연구에서는 네 가지 추론 및 비추론 모델의 행동을 분석하기 위해 반복 공공재 게임을 적용하였다.


<details>
  <summary>Details</summary>
Motivation: 인간-AI 상호작용에 대한 연구가 대부분을 차지하는 가운데, AI-AI 상호작용을 이해할 필요성이 증가하고 있다.

Method: 네 가지 추론 및 비추론 모델을 활용하여 반복 공공재 게임을 분석하였다.

Result: 모델들이 자신이 '다른 AI 에이전트'와 대결하고 있다고 들었을 때와 자신과 대결하고 있다고 들었을 때의 행동 차이를 관찰하였다.

Conclusion: 장난감 환경에서 실시한 연구이지만, 에이전트들이 무의식적으로 서로를 차별할 경우 협력에 미치는 영향에 대한 통찰력을 제공할 수 있다.

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [21] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 이 연구는 PDDL로 정의된 세계 모델에 대한 계획 수립에 언어 모델(LM)을 사용하는 방법을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: PDDL 문제를 해결하기 위한 일반화된 정책을 생성하고, 외부 검증자에 의존하지 않으면서 PDDL 도메인에 대해 입증 가능한 정책을 합성하려는 목적입니다.

Method: LM을 활용하여 주어진 도메인에서 PDDL 문제를 해결하기 위한 Python 프로그램을 생성하도록 유도합니다.

Result: 경쟁 벤치마크에서 실험을 수행한 결과, 우리의 정책이 PDDL 플래너 및 최신 LM 접근 방식보다 더 많은 PDDL 문제를 해결할 수 있음을 보여주었습니다.

Conclusion: LMPlan 플래너를 통해 수백 개의 관련 객체를 가지고 있는 계획 문제를 해결할 수 있으며, LMs가 자연어 대신 의미 없는 기호로 작성된 PDDL 문제에 대해 더 효과적으로 계획을 세우는 경향이 있음을 관찰했습니다. 이 발견은 LMs가 단어 의미에 대해 추론하고 훈련 데이터에서 솔루션을 기억한다는 가설에 도전하며, 추가 탐색의 가치가 있습니다.

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [22] [Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games](https://arxiv.org/abs/2508.19152)
*Chiu-Chou Lin*

Main category: cs.AI

TL;DR: 이 논문은 인공지능 에이전트의 의사결정 행동을 분석하기 위해 '플레이스타일'이라는 개념을 도입하고, 그것의 의미와 역사적 맥락을 철학적으로 고찰한다.


<details>
  <summary>Details</summary>
Motivation: 현대 인공지능 개발은 합리적인 의사결정에 초점을 맞추고 있어, 판단을 내릴 때 논리와 더불어 신념, 가치관, 선호와 같은 깊은 영향이 작용한다는 점을 강조하고자 한다.

Method: 신념과 가치가 의도를 형성하는 방식을 분석하고, 외부와의 상호작용과 내부의 숙고과정이라는 두 가지 단계로 스타일 형성의 구조를 제안한다. 또한, 스타일 관련 특성을 형식화하고 측정할 수 있는 지표를 제안한다.

Result: 세 가지 핵심 연구 방향에 따라 플레이스타일을 정의하고 측정하며, 전통적인 게임 디자인과 인터랙티브 엔터테인먼트 분야에서 응용 가능성을 분석한다.

Conclusion: 이 연구는 인공지능의 미래 확장을 고려하며, 스타일이 인공지능 일반화(AGI) 구축에서 핵심 요소로 작용할 수 있음을 제안한다.

Abstract: Contemporary artificial intelligence (AI) development largely centers on
rational decision-making, valued for its measurability and suitability for
objective evaluation. Yet in real-world contexts, an intelligent agent's
decisions are shaped not only by logic but also by deeper influences such as
beliefs, values, and preferences. The diversity of human decision-making styles
emerges from these differences, highlighting that "style" is an essential but
often overlooked dimension of intelligence.
  This dissertation introduces playstyle as an alternative lens for observing
and analyzing the decision-making behavior of intelligent agents, and examines
its foundational meaning and historical context from a philosophical
perspective. By analyzing how beliefs and values drive intentions and actions,
we construct a two-tier framework for style formation: the external interaction
loop with the environment and the internal cognitive loop of deliberation. On
this basis, we formalize style-related characteristics and propose measurable
indicators such as style capacity, style popularity, and evolutionary dynamics.
  The study focuses on three core research directions: (1) Defining and
measuring playstyle, proposing a general playstyle metric based on discretized
state spaces, and extending it to quantify strategic diversity and competitive
balance; (2) Expressing and generating playstyle, exploring how reinforcement
learning and imitation learning can be used to train agents exhibiting specific
stylistic tendencies, and introducing a novel approach for human-like style
learning and modeling; and (3) Practical applications, analyzing the potential
of these techniques in domains such as game design and interactive
entertainment.
  Finally, the dissertation outlines future extensions, including the role of
style as a core element in building artificial general intelligence (AGI).

</details>


### [23] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: Weisfeiler-Leman Features(WLFs)는 기계 학습 도구로, 공간 계획 및 탐색을 위한 가치 함수 학습에서 기존의 심층 학습 방법들보다 우수함이 입증되었다.


<details>
  <summary>Details</summary>
Motivation: WLF의 하이퍼파라미터와 이들의 다양한 트레이드오프 및 효과를 연구하기 위함이다.

Method: 단일 코어 CPU에서 100만 샘플 크기로 계획 실험을 수행하여 하이퍼파라미터가 훈련 및 계획에 미치는 영향을 이해한다.

Result: 검토된 계획 영역 전반에 걸쳐 WLF에 대해 가장 강력하고 최상의 하이퍼파라미터 세트가 발견되었다. Heuristic 함수 학습을 위한 최상의 WLF 하이퍼파라미터는 모델의 표현력을 극대화하는 것보다 실행 시간을 최소화하는 데 초점을 맞추었다.

Conclusion: 훈련과 계획 지표 간에 유의미한 상관관계는 관찰되지 않았다.

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [24] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: MATRIX는 안전 중심의 임상 대화 에이전트를 평가하기 위한 구조화되고 확장 가능한 프레임워크로, 세 가지 주요 구성 요소를 포함하여 시스템의 안전성을 체계적으로 평가할 수 있도록 지원한다.


<details>
  <summary>Details</summary>
Motivation: 대화형 시스템에서 LLM의 사용이 증가하고 있지만 기존 평가 방식은 작업 완료와 유창성에 초점을 맞추고 있어 안전-critical 시스템에 필요한 행동 및 위험 관리 요구 사항에 대한 통찰력을 제공하지 않는다.

Method: MATRIX는 임상 시나리오에 대한 안전 정렬 분류법, LLM 기반 평가 도구 BehvJudge, 다양하고 시나리오 조건화된 반응을 생성할 수 있는 PatBot을 포함한다.

Result: MATRIX는 Systematic이고 확장 가능한 안전 평가를 가능하게 하며, BehvJudge는 전문가 수준의 위험 탐지를 달성하고, PatBot은 양적 및 질적 평가에서 현실적인 환자 행동을 신뢰성 있게 시뮬레이션한다.

Conclusion: MATRIX는 구조화된 안전 공학과 확장 가능한 대화형 인공지능 평가를 통합하는 첫 번째 프레임워크로, 규제자에 맞춘 안전 감사를 가능하게 한다.

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


### [25] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 이 연구는 대칭 불변성을 가진 새로움을 탐지하기 위해 원자 대신 Weisfeiler-Leman 특징을 사용하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 새로움 휴리스틱은 대칭 불변성이 없기 때문에 중복 탐색을 초래할 수 있다.

Method: 대칭 상태에 대해 불변인 새로움 탐지용 Weisfeiler-Leman 특징(WLFs)을 사용하는 방법을 제안한다.

Result: 국제 계획 경쟁 및 Hard To Ground 벤치마크 실험에서 WLFs로 합성된 새로움 휴리스틱이 유망한 결과를 보였다.

Conclusion: WLFs를 이용한 방법이 대칭적 상태에 대한 중복 탐색 없이 새로움을 탐지할 수 있음을 강조한다.

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [26] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 이 논문은 잠입 게임에서 경비 순찰 행동을 개선하기 위한 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 스텔스 게임의 몰입감과 전략적 깊이에 중요한 경비 순찰 행동의 효율성을 높이고자 함.

Method: Composite Potential Fields를 통해 전역 지식과 지역 정보를 통합하는 설명 가능한 훈련 필요 없는 프레임워크를 제안.

Result: 우리 방법이 기존의 기초 방법들보다 포획 효율성과 순찰 자연스러움에서 우수함을 입증함.

Conclusion: 일반적인 스텔스 메커니즘이 쉽게 통합될 수 있는 방법을 보여주어, 역동적이고 반응적인 경비 행동의 신속한 프로토타이핑을 가능하게 함.

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [27] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 3D 게임 레벨의 절차적 콘텐츠 생성은 다층 환경에서 공간 일관성, 내비게이션 기능 및 적응 가능한 게임 진행 간의 균형을 맞추는 데 어려움을 겪고 있다. 이 논문은 아키텍처 구성 요소와 게임 메커니즘 요소의 재사용 가능한 데이터베이스를 오프라인으로 생성하는 새로운 프레임워크를 소개한다.


<details>
  <summary>Details</summary>
Motivation: 3D 게임 레벨 생성에서 공간 일관성과 내비게이션 기능, 게임 플레이 진행의 균형을 맞추는 것이 필요하다.

Method: 재사용 가능한 아키텍처 구성 요소 및 게임 메커니즘 요소의 데이터베이스를 고려하여 다단계 파이프라인을 통해 레벨을 생성하는 방식으로, 룸 데이터베이스에서 인스턴스를 선택하고 배치하여 다층 구조를 형성하고, 시설 데이터베이스의 제약 조건에 따라 내부 레이아웃을 최적화하며, 메커니즘 데이터베이스의 구성 요소를 토폴로지와 공간 규칙에 따라 결합한다.

Result: 초기 실험에서 다양한 내비게이션 가능한 3D 환경 생성과 간단한 매개변수화를 통한 독특한 게임 진행 전략의 시뮬레이션 능력을 검증하였다.

Conclusion: 지속 가능하고 데이터베이스 중심의 복잡한 3D 레벨 자동 생성 프레임워크를 제시하여 절차적 콘텐츠 생성 분야에 기여하였다.

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [28] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: SchemaCoder는 대량의 로그 데이터에서 다ú방향 스키마를 자동으로 추출하는 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 로그 스키마 추출은 로그 데이터의 양이 방대하여 인간이 읽을 수 있는 템플릿을 도출하는 필수적인 과정이지만 노동 집약적이다.

Method: SchemaCoder는 LLM을 사용하여 자동 스키마 추출을 수행하는 최초의 자동화 프레임워크로, 로그 파일 형식에 따라 인간의 커스터마이징 없이 적용할 수 있다.

Result: SchemaCoder는 LogHub-2.0 벤치마크에서 평균 21.3% 향상을 달성하며 기존 방법론에 비해 우수성을 입증하였다.

Conclusion: 이 연구는 로그 스키마 추출의 효율성을 크게 향상시키며 높은 생산성 증대를 가능하게 한다.

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [29] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: 피부 관련 열대병(NTDs)은 열대 지역의 가난한 공동체에서 심각한 건강 및 사회경제적 부담을 주지만, AI 기반 진단 지원의 발전은 데이터 부족에 의해 방해받고 있다. 이를 해결하기 위해, 우리는 코트디부아르와 가나에서 현장 수집된 새로운 피부 데이터셋인 eSkinHealth를 도입한다.


<details>
  <summary>Details</summary>
Motivation: 피부 관련 열대병(NTDs)은 열대 지역의 가난한 공동체에서 심각한 영향을 미치고 있으며, 특히 저평가된 인구와 희귀질병에 대한 데이터 부족은 AI 기반 진단의 발전을 방해하고 있다.

Method: 자료 수집을 위한 새로운 데이터셋 eSkinHealth를 소개하며, 이는 1,639건의 사례에서 5,623장의 이미지를 포함하고 있으며, 서아프리카 인구의 피부 NTDs와 희귀 질환에 중점을 둔다. AI 전문가 협력 패러다임을 제안하여 피부과 의사의 지도를 받아 다중 모드 주석을 효율적으로 생성하는 모델을 구현한다.

Result: eSkinHealth는 환자 메타데이터, 진단 레이블, 의미적 병변 마스크, 인스턴스별 시각적 캡션 및 임상 개념을 포함하여, 피부과의 진단 모델에 필요한 다양한 자료를 제공한다.

Conclusion: 이 연구는 전 세계 피부과를 위한 보다 공정하고 정확하며 해석 가능한 AI 도구 개발을 촉진하는 데 기여할 새로운 귀중한 자료와 스케일 가능한 주석 프레임워크를 제공한다.

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [30] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 본 논문은 주관적 작문 품질과 객관적 제약 준수를 동시에 개선하기 위한 혼합 보상 강화 학습(RLMR) 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 창의적 작문 애플리케이션에서 대규모 언어 모델의 활용이 증가하고 있으며, 작문 품질과 제약 준수를 균형 있게 유지하는 것이 필요하다.

Method: 주관적 작문 품질을 평가하는 작성 보상 모델과 객관적 제약 준수를 평가하는 제약 검증 모델의 동적 혼합 보상 시스템을 이용한 활용 방법을 제안한다.

Result: 저희 방법은 다양한 모델 가족에서 일관된 개선 결과를 보여주며, IFEval에서 83.36%에서 86.65%로 향상되었고, WriteEval에서 수동 전문가 쌍 평가에서 72.75%의 승률을 기록하였다.

Conclusion: RLMR은 온라인 RL 학습에서 주관적 선호와 객관적 검증을 결합한 최초의 작업으로, 다차원 창의적 작문 최적화에 대한 효과적인 해결책을 제공한다.

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [31] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 대규모 언어 모델(LLM)의 벤치마크 성능과 실제 유용성 간의 단절이 존재한다. 이 연구에서는 인간 지능의 관점에서 인류적 평가 패러다임을 제안하고, 세 가지 차원으로 구성된 새로운 분류체계를 도입한다.


<details>
  <summary>Details</summary>
Motivation: LLM의 성과 평가 프레임워크가 기술적 지표에 치중하고, 전체적인 배포 평가를 소홀히 하고 있는 문제 해결.

Method: 지능지수(IQ), 정서지수(EQ), 전문지수(PQ)를 기반으로 하는 3차원 분류체계와 경제적 가치, 사회적 영향, 윤리적 정렬 및 환경 지속 가능성을 평가하는 가치 중심 평가(VQ) 프레임워크 도입.

Result: 200개 이상의 벤치마크 분석을 통해 동적 평가 필요성과 해석 가능성 격차를 포함한 핵심 과제를 식별.

Conclusion: 기술적으로 숙련되고, 맥락적으로 적절하며, 윤리적으로 건전한 LLM 개발을 위한 실행 가능한 지침 제공. 오픈 소스 평가 자원 리포지토리 유지.

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [32] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: MUA-RL은 에이전트 도구 사용을 위한 새로운 강화 학습 프레임워크로, LLM 시뮬레이션 사용자를 통합하여 동적 다중 턴 상호작용에서 효율적인 사용자 소통을 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 에이전틱 인텔리전스의 급속한 발전으로 LLM에서 에이전트 도구 사용의 중요성이 증가하고 있다. 사용자의 동적이고 불확실한 요구 사항이 에이전트의 도구 호출 능력에 도전하고 있다.

Method: MUA-RL(다중 턴 사용자 상호작용 에이전트 강화 학습)은 LLM 시뮬레이션된 사용자를 강화 학습 루프에 통합하여 에이전트 도구 사용을 위한 새로운 강화 학습 프레임워크를 제공한다.

Result: MUA-RL-32B는 여러 다중 턴 도구 사용 벤치마크에서 우수한 성능을 보여주며, 큰 오픈 소스 모델과 비교해도 성능이 뛰어난 것으로 평가된다.

Conclusion: MUA-RL은 동적 다중 턴 상호작용에서 사용자의 요구에 효율적으로 소통하고 다양한 도구를 사용하여 실제 문제를 해결할 수 있는 모델의 자율 학습을 목표로 한다.

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [33] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 이 논문은 사용자 지침에 기반하여 다중 도메인 정보를 적극적으로 통합하는 GUI 에이전트 시스템인 AppAgent-Pro를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대부분의 기존 에이전트가 사용자 지침에 수동적으로 반응하여 정보 획득의 효과성과 효율성이 제한되고 있다.

Method: 사용자 지침에 따라 다중 도메인 정보를 적극적으로 통합하는 프로액티브 GUI 에이전트 시스템인 AppAgent-Pro를 제안한다.

Result: AppAgent-Pro는 사용자의 잠재적 요구를 예측하고 심층적인 다중 도메인 정보 마이닝을 수행하여 보다 포괄적이고 지능적인 정보 획득을 가능하게 한다.

Conclusion: AppAgent-Pro는 일상 생활에서 정보 획득을 근본적으로 재정의할 잠재력이 있으며, 이는 인간 사회에 깊은 영향을 미칠 것이다.

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [34] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: 본 논문은 특정 도메인 지식의 부족으로 인해 성능이 저해되는 대규모 언어 모델의 문제를 해결하기 위해 비용 효율적인 에이전트 프레임워크 VistaWise를 소개하며, 이를 통해 도메인 특정 교육 데이터 요구량을 대폭 줄이고 다양한 오픈 월드 작업에서 최첨단 성능을 달성함을 보인다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델(LLMs)은 가상 오픈 월드 환경에서의 의사결정 작업에서 잠재력을 보이지만, 도메인 특정 지식의 부족으로 성능이 저하되며 이는 대규모 도메인 특정 데이터에 대한 세부 조정비용이 비쌈.

Method: VistaWise 에이전트 프레임워크는 교차 모달 도메인 지식을 통합하고, 시각적 분석을 위한 전용 객체 탐지 모델을 세부 조정한다. 이는 수백 개의 샘플로 도메인 특정 교육 데이터의 필요성을 줄인다.

Result: VistaWise는 시각적 정보와 텍스트 종속성을 교차 모달 지식 그래프(KG)에 통합하여 다중 모달 환경에 대한 종합적이고 정확한 이해를 가능하게 하며, KG에서 작업 관련 정보를 추출하기 위한 검색 기반 풀링 전략을 갖추고 있다.

Conclusion: VistaWise는 다양한 오픈 월드 작업에서 최첨단 성능을 달성하며 개발 비용을 절감하면서 에이전트 성능을 향상시키는 효과를 입증하였다.

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [35] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 본 논문에서는 편향 완화를 위한 다중 에이전트 시스템을 제안하여 정보 검색의 공정성과 신뢰성을 높이는 방법을 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 대량 언어 모델(LLM)의 발전으로 인해 자율적이며 목표 지향적인 시스템이 필요해졌습니다. 하지만 정보 출처에서 발생하는 편향이 시스템의 공정성과 사용자 신뢰에 영향을 미칩니다.

Method: 편향 완화를 위한 새로운 다중 에이전트 시스템을 소개하였으며, 전문 에이전트들이 출처 선택을 최적화하여 편향을 최소화하도록 설계되었습니다.

Result: 기초 나이브 검색 전략에 비해 81.82%의 편향 감소를 경험적으로 증명하였습니다.

Conclusion: 편향 완화 에이전트를 통해 검색된 콘텐츠가 공정하고 균형 있는 지식을 전파하는데 기여할 수 있습니다.

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [36] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: Connector-Aware Compact CoT (CAC-CoT)는 대형 언어 모델이 간결하고 잘 구조화된 설명을 제공하도록 돕는 방법으로, 계산 효율성을 높이면서 정확도를 유지한다.


<details>
  <summary>Details</summary>
Motivation: 긴 chain-of-thought 프롬프트가 대형 언어 모델이 어려운 문제를 푸는 데 도움이 되지만, 너무 긴 추적은 빠르고 직관적인 '시스템-1' 작업에서 성능을 저하시키거나 느리게 만들 수 있다.

Method: CAC-CoT는 연결하는 구문을 고정된 소규모 세트로 제한하여 모델이 간결하고 구조적인 설명을 하도록 유도하는 방법이다.

Result: CAC-CoT는 GSM8K에서 약 85%, GPQA(System-2)에서 약 40%를 달성하며, S1-Bench(System-1)에서는 약 90%를 유지한다. 평균적으로 300개의 토큰(ART)으로 기본 추적 길이의 약 3분의 1에 해당하며, 정확도를 유지하면서 더 높은 효율성을 제공한다.

Conclusion: CAC-CoT는 단순하지만 고품질의 훈련 결과를 제공하며, 효율성을 높이고 성능 저하 없이 사용 가능하다.

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [37] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: REMO라는 새로운 프레임워크는 텍스트 프롬프트 최적화를 개선하여 대형 언어 모델의 성능을 향상시키며, 과거의 최적화 경험을 활용할 수 있는 방법을 제공합니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 프롬프트 최적화 방법은 상태가 없고, 최적화 과정에서 역사적 경험을 보존하거나 활용할 수 없다는 한계를 가지고 있습니다.

Method: REMO는 기억 증강 반영 검색 보강 생성(RAG) 모듈과 자가 적응 최적화기를 통합하여 시스템 수준의 프롬프트 전략을 반영하고 개선합니다.

Result: REMO는 텍스트그라드(TextGrad) 기준선에 비해 더 안정적이고 강력한 일반화를 달성합니다.

Conclusion: 세부적인 알고리즘 설계를 제공하고 최적화 역학에 대한 정성적 및 정량적 분석을 수행하며, 각 구성 요소의 기여를 밝히기 위한 포괄적인 절단 연구를 제시합니다.

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [38] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 본 연구에서는 테스트 시간 적응(TTA) 문제를 다루기 위해 Primary-Auxiliary Filtering(PAF)과 Knowledge-Integrated Prediction(KIP) 접근 방식을 제안하며, 이를 통해 닫힌 집합 정확도와 열린 집합 구별력을 향상시킨다.


<details>
  <summary>Details</summary>
Motivation: 실제 환경에서는 훈련 데이터와 테스트 데이터 간의 도메인 편차가 발생하기 때문에 TTA가 필요하다.

Method: PAF는 주 필터에 의해 필터링된 데이터를 검증하기 위해 보조 필터를 사용하고, KIP는 다양한 모델의 출력을 보정하여 OSTTA를 위한 상호 보완적 지식을 통합한다.

Result: 제안된 방법은 기존 방법보다 닫힌 집합 정확도와 열린 집합 구별력을 모두 향상시킨다.

Conclusion: 제안된 방법은 다양한 닫힌 집합 및 열린 집합 데이터셋에서 검증되었으며, 코드가 공개된다.

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [39] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 대형 추론 모델(LRM)이 복잡한 추론 작업에서 두드러진 발전을 보였으나, 특정 조건이 부족한 수학 문제처럼 본질적으로 대답할 수 없는 질문에 대해 적절한 자제를 보이지 못한다는 문제를 분석하고 해결하는 연구이다.


<details>
  <summary>Details</summary>
Motivation: 신뢰할 수 있는 AI를 위한 대답할 수 없는 질문에 대한 적절한 자제 방식의 필요성.

Method: 인지 모니터링과 추론 시간 개입을 결합한 경량의 두 단계 방법을 제안.

Result: 제안된 방법은 자제율을 유의미하게 개선하며 전체적인 추론 성능을 유지함을 실험적으로 입증하였다.

Conclusion: LRM의 내부 인지와 외부 응답 간의 불일치를 해결함으로써 신뢰할 수 있는 AI를 달성할 수 있다.

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [40] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: 이 논문은 다중 모델 협력을 통해 언어 모델의 추론 능력 향상을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 언어 모델의 추론 능력을 향상시키기 위해 다중 모델 협력 방안을 모색합니다.

Method: 여러 모델이 제공하는 다음 토큰 분포에서 최적의 토큰을 선택하여 자기 회귀 추론을 수행합니다.

Result: 실험 결과는 다양한 기준에서 우리 방법의 우수성을 입증합니다.

Conclusion: 우리의 방법은 언어 공간 내에서 자연스러운 정렬을 달성할 수 있도록 돕습니다.

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [41] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: AniME는 스토리에서 최종 비디오까지의 전체 워크플로우를 자동화하는 다중 에이전트 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 애니메이션 제작의 자동화를 통해 효율성과 일관성을 높이고자 한다.

Method: 감독 에이전트가 전체 워크플로우의 글로벌 메모리를 유지하고 여러 전문화된 에이전트를 조정한다. 맞춤형 모델 컨텍스트 프로토콜(MCP)과 하류 모델 지침을 통합하여 전문화된 에이전트가 다양한 하위 작업에 대한 제어 조건을 선택하도록 한다.

Result: AniME는 일관된 캐릭터와 동기화된 오디오 비주얼 요소를 갖춘 영화 같은 애니메이션을 생성한다.

Conclusion: AI 주도의 애니메이션 제작을 위한 확장 가능한 솔루션을 제공한다.

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [42] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: Minecraft에서의 다중 에이전트 협력을 위한 CausalMACE 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 작업을 수행하는 데 있어 단일 에이전트 접근 방식의 비효율성과 제한된 결함 허용 범위를 극복하고자 한다.

Method: CausalMACE라는 전체론적 인과 계획 프레임워크를 제안하여 하위 작업 간의 종속성을 관리하기 위해 인과성을 도입하였다.

Result: 실험 결과, 본 접근 방식이 Minecraft의 다중 에이전트 협력 작업에서 최첨단 성능을 달성함을 보여준다.

Conclusion: CausalMACE는 다중 에이전트 시스템을 향상시키기 위해 설계된 유용한 프레임워크이다.

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [43] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: 이 논문은 STARec이라는 느린 사고 증강 에이전트 프레임워크를 소개하며, 이를 통해 추천 시스템에 자율적인 숙고 추론 능력을 부여합니다.


<details>
  <summary>Details</summary>
Motivation: 현대 추천 시스템은 정보의 풍부함을 탐색하는 데 중요하지만, 정적인 사용자 모델링과 반응적 의사결정 패러다임에 의해 근본적으로 제한되어 있습니다.

Method: STARec은 각 사용자를 빠른 반응과 느린 추론을 수행하는 병렬 인지력을 가진 에이전트로 모델링하며, 앵커 강화 학습을 통해 숙고를 내재화하는 두 단계를 개발합니다.

Result: MovieLens 1M 및 Amazon CDs 벤치마크 실험에서 STARec은 최첨단 기준선에 비해 상당한 성능 향상을 달성하였습니다.

Conclusion: 이 접근 방식은 기초 능력을 습득하는 동시에 동적인 정책 적응을 가능하게 합니다.

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [44] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: 이 논문은 사법 결정에서 신뢰할 수 있는 추론 도구로 기능하기 위한 AI 시스템의 핵심 능력을 정의하고, Legal adjudication의 가장 도전적인 단계인 적절한 규칙을 결정하고 해당 규칙을 사실에 적용하는 방법을 분석한다.


<details>
  <summary>Details</summary>
Motivation: LLMs의 사법 분야에서의 한계를 이해하고, 신뢰할 수 있는 추론 도구로서 그들이 갖추어야 할 능력을 정의하고자 한다.

Method: IRAC 모델을 분석 프레임워크로 사용하여 법적 판단의 도전적인 단계들을 분석한다.

Result: AI의 다양한 향상 메커니즘을 법적 요구 사항에 매핑하고, LLM의 확률적 본질과 법적 해석의 엄격한 요구 간의 간극을 해소할 가능성을 평가한다.

Conclusion: AI의 현재 가장 효과적인 역할은 단순한 반복 사례를 위한 고용량 보조자와 복잡한 문제에서 인간 전문가를 위한 정교한 '스파링 파트너' 역할을 동시에 수행하는 것이다.

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [45] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: 전통적인 정적 벤치마크는 소프트웨어 엔지니어링과 같은 복잡한 작업에서 대형 언어 모델(LLMs)의 미묘한 능력을 평가하는 데 부족하다. 우리는 구조화된 피드백 중심의 대화를 통해 LLM을 다중 요구 사항 프로그래밍 작업에 대해 평가하는 새로운 인터랙티브 평가 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 복잡한 작업 수행 능력을 더 제대로 평가할 필요가 있다.

Method: 각 작업을 요구 사항 의존성 그래프로 모델링하고, 올바른 해답을 알고 있는 '면접관' LLM이 최소한의 목표 지침을 제공하여 '면접자' 모델이 오류를 수정하고 목표 제약을 충족시키도록 한다.

Result: 동적 프로토콜은 모델의 동작에 대한 세밀한 진단 통찰을 제공한다.

Conclusion: 협업 코드 생성 에이전트 개발에 있어 동적 평가의 중요성을 강조한다.

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [46] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: FormaRL은 적은 양의 비지도 데이터를 사용하여 자동 형식화(autoformalization)를 위한 효율적인 강화 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 데이터 부족과 효율적인 방법의 부재로 자동 형식화의 발전이 저해되고 있다.

Method: FormaRL은 Lean 컴파일러의 구문 검사와 대규모 언어 모델의 일관성 검사를 통합하여 보상을 계산하고, GRPO 알고리즘을 사용하여 형식화를 업데이트한다.

Result: FormaRL은 859개의 비지도 데이터만으로 Qwen2.5-Coder-7B-Instruct의 pass@1 자동 형식화 정확도를 4 $	ilde{}$ 6배로 증가시킨다.

Conclusion: FormaRL의 훈련 코드는 GitHub에 공개되어 있다.

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [47] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: Intelligent Tutoring Systems (ITSs)의 확산이 교육에서 성과 격차를 악화시킬 수 있으며, 이를 해결하기 위해 학생 프로파일링이 중요하다. 본 연구에서는 CTGraph라는 접근 방식을 제안하여 학생 행동과 성과를 자기지도 방식으로 프로파일링한다.


<details>
  <summary>Details</summary>
Motivation: Intelligent Tutoring Systems (ITSs)의 사용 증가가 성과 격차를 악화시킬 수 있으므로, 학생 프로파일링이 필수적이다.

Method: CTGraph라는 그래프 수준의 표현 학습 접근 방식을 통해 학생 행동과 성과를 자기 지도 방식으로 프로파일링한다.

Result: 실험 결과 CTGraph가 학생의 학습 여정을 종합적으로 보여줄 수 있으며, 학생 행동과 성과의 다양한 측면 및 학습 경로의 변화를 고려한다.

Conclusion: 이 접근 방식은 어려움을 겪는 학생을 식별하고 다양한 그룹 간의 비교 분석을 제공하여 학생이 언제, 어디서 어려움을 겪고 있는지를 파악하는 데 기여한다.

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [48] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: 이 연구에서는 자동화된 취약점 검출을 위한 VISION이라는 프레임워크를 제안하여, 반사실적 훈련 데이터셋을 시스템적으로 증가시켜 허위 상관관계를 완화하고, 더욱 견고하고 일반화 가능한 탐지를 가능하게 한다.


<details>
  <summary>Details</summary>
Motivation: 소스 코드의 자동화된 취약점 탐지는 사이버 보안의 중요한 도전 과제로, 디지털 시스템과 서비스에 대한 신뢰를 뒷받침한다.

Method: 이번 연구에서 제안된 VISION 프레임워크는 허위 상관관계를 완화하기 위해 반사실적 샘플을 생성하고, 대조적인 라벨을 가진 코드 쌍에 대해 GNN을 훈련하며, 그래프 기반 해석 기능을 통해 중요한 코드 문장을 식별한다.

Result: VISION은 허위 학습을 줄이고 탐지의 견고성을 증가시켜 전반적인 정확도를 51.8%에서 97.8%로 향상시켰다.

Conclusion: VISION은 투명하고 신뢰할 수 있는 AI 기반 사이버 보안 시스템을 발전시키는 데 기여한다.

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [49] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: 이 논문은 현대 신경망 기술의 한계를 극복하기 위해 계층적 군집 구조를 기반으로 하는 최근접 이웃 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 현대 신경망 기술은 여러 인공지능 응용 프로그램에서 큰 성공을 거두었지만, 의약, 산업 프로세스 관리, 과학 연구와 같은 중요한 분야에서의 사용을 방해하는 근본적인 한계가 있다.

Method: k-최근접 이웃 알고리즘을 사용하여 환각 효과를 줄이고 모델 확장 및 미세 조정을 단순화하며 전체 네트워크를 재교육할 필요 없이 수행한다. 또한, 고온의 자기조직화 맵을 기반으로 하는 트리 구조를 사용하여 최근접 이웃 검색 속도를 크게 높인다.

Result: 손으로 쓴 숫자 인식 및 간단한 자막 번역 작업에서 제안된 접근 방법의 효과가 확인되었고, 정확성이 약간 감소하면서도 최근접 이웃 검색 시간이 exhaustive 검색 방법에 비해 수백 배 감소했다.

Conclusion: 제안된 방법은 투명성과 해석 가능성을 제공하며, 인간의 인지 메커니즘과 밀접하게 연결되어 있으며, 높은 신뢰성과 설명 가능한 결과가 필요한 작업에서 광범위하게 사용될 가능성이 있다.

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [50] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: Mixture of Experts (MoE) 아키텍처를 통해 대형 언어 모델의 메모리 사용량과 데이터 전송을 줄이는 방법을 제시하며, GPU 캐시를 활용한 효율적인 스케줄링 정책을 도입하여 디코딩 지연 시간을 48% 단축하고 60% 이상의 전문가 캐시 적중률을 유지합니다.


<details>
  <summary>Details</summary>
Motivation: 소비자용 엣지 하드웨어에서 MoE를 배포할 때 제한된 장치 메모리로 인해 동적 전문가 오프로드가 필수적입니다.

Method: 전문가의 중요성을 활용하여 결정을 안내하고, 중요성이 낮은 활성화된 전문가를 GPU 메모리에 이미 캐시된 기능적으로 유사한 전문가로 대체합니다.

Result: 이 설계는 메모리 사용량과 데이터 전송을 줄이고 PCIe 오버헤드를 대부분 제거합니다.

Conclusion: 우리의 접근 방식은 48% 낮은 디코딩 지연 시간을 제공하며, 60% 이상의 전문가 캐시 적중률을 유지하면서 거의 손실 없는 정확성을 유지합니다.

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [51] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: 이 연구는 언어 모델이 사회적 규범을 통계적 학습만으로 이해할 수 있는지를 조사했다.


<details>
  <summary>Details</summary>
Motivation: 인지 과학의 기본 질문 중 하나는 사회적 규범이 어떻게 습득되고 표현되는지이다.

Method: 두 가지 연구를 통해 555가지 일상 시나리오에 대한 인간의 사회적 적절성 판단을 예측하는 여러 AI 시스템의 능력을 체계적으로 평가했다.

Result: 연구 1에서는 GPT-4.5가 집단 판단을 예측하는 정확도가 모든 인간 참가자보다 우수했다. 연구 2에서는 Gemini 2.5 Pro가 98.7%의 인간을 초과하는 성능을 보였고, GPT-5는 97.8%, Claude Sonnet 4는 96.0%였다.

Conclusion: 이 연구 결과는 사회 인지 모델이 언어 데이터에 대한 통계적 학습에서 출현할 수 있음을 보여주고, 문화적 역량을 위한 체화된 경험의 필요성을 강조하는 이론에 도전한다.

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [52] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 이 논문은 경험 기반 평생 학습(ELL) 프레임워크를 소개하며, 자가 진화하는 에이전트를 구축하는 방법과 이를 평가하기 위한 StuLife 데이터셋을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AI가 일반 지능으로 발전함에 따라, 정적 작업에 최적화된 시스템에서 지속적으로 학습하는 개방형 에이전트를 생성하는 방향으로 초점이 이동하고 있습니다.

Method: 경험 기반 평생 학습(ELL) 프레임워크는 지속적인 학습을 위한 네 가지 핵심 원칙에 기반하여 자아 진화하는 에이전트를 만드는 방법을 제시합니다.

Result: StuLife는 학생의 전인적 대학 여정을 시뮬레이션하는 벤치마크 데이터셋으로, 에이전트가 실제 환경에서 실용적인 기술을 습득하고 지속적인 기억을 유지해야 함을 강조합니다.

Conclusion: 우리는 StuLife 벤치마크에서 최신 LLMs를 평가하고 AGI 발전에서 컨텍스트 공학의 역할을 탐구합니다.

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [53] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: 이 연구는 경계선 인격장애(BPD)의 1인칭 경험에 대한 현상학적 질적 분석을 지원하기 위한 대형 언어 모델(LLMs)의 능력을 조사했다. 기존의 인간 중심 주제 분석을 바탕으로 세 가지 LLM(GPT-4o, Gemini 2.5 Pro, Claude Opus 4)을 비교하였으며, 결과는 인간 분석과의 겹침이 변동적이었다.


<details>
  <summary>Details</summary>
Motivation: BPD를 시간성과 자아의 장애로 이해하고, LLM이 현상학적 분석을 지원할 수 있는지를 탐구하기 위함.

Method: 24명의 입원 환자의 생애 이야기 인터뷰에 대한 인간 주도 주제 분석을 기반으로 세 가지 LLM의 해석 스타일을 비교하였다.

Result: 분석 결과 GPT는 0%, Claude는 42%, Gemini는 58%의 인간 분석과의 겹침을 보였으며, Jaccard 계수는 낮았다(0.21-0.28).

Conclusion: Gemini의 출력은 인간 분석과 가장 유사하였으며, LLM이 인간의 해석 편향을 완화할 수 있는 가능성을 시사한다.

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [54] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: 지능형 자율 튜터링 시스템(IATS)의 발전을 위해 문제의 난이도를 판단하는 객관적이고 범주 불문의 방법이 필수적이다.


<details>
  <summary>Details</summary>
Motivation: 기술과 교육의 발전은 지능형 자율 튜터링 시스템(IATS)의 출현을 이끌고 있으며, 여기에 필요한 것은 문제의 난이도를 판단하는 객관적이고 범주에 얽매이지 않는 방법이다.

Method: 이 연구에서는 해결자 성과 데이터만으로 난이도를 추정하는 강화학습 기반의 다중팔 밴딧(MAB) 프레임워크인 교육생 간 접속의 수동 측정 접근법(APME)을 소개한다.

Result: 모델은 세 개의 이질적인 데이터셋에서 평균 R2 0.9213, 평균 RMSE 0.0584를 달성하여 다양한 교육 수준과 평가 형식에 강인함과 정확성, 적응성을 입증하였다.

Conclusion: 이 모델은 학생의 동기를 지원하고 이탈을 최소화하는 과제를 식별함으로써 Vygotsky의 근접 발달 지대와 일치한다.

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [55] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 대형 언어 모델(LLM)의 추론 능력을 평가하는 기존 작업은 상호작용이 없는 환경에서 부족함을 드러낸다. 이 논문에서는 '블랙박스 인터랙션'이라는 새로운 평가 패러다임을 소개하고, 이를 통해 LLM의 통합적 추론 과정을 평가할 수 있는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 기존 작업들이 LLM의 추론 능력을 상호작용이 없는 환경에서 평가하는 데 한계가 있다.

Method: 블랙박스 상호작용을 통해 LLM이 입력-출력 쌍을 관찰함으로써 숨겨진 기능을 풀어내도록 요구하는 평가 패러다임을 구축한다.

Result: 96개의 블랙박스와 6가지 유형의 블랙박스 작업을 포함하는 	extsc{Oracle} 벤치마크를 구축하였고, 19개의 현대 LLM을 평가하였다.

Conclusion: 많은 LLM들이 가설을 정제하기 위한 효율적이고 적응적인 탐색 전략을 개발하는 고급 계획 능력이 부족하여 어려움을 겪고 있음을 나타낸다.

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [56] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: CMA는 여러 LLM 기반 모듈을 비동기적으로 운영하며 일관되고 내결함성 있는 행동 루프를 유지하는 프레임워크입니다.


<details>
  <summary>Details</summary>
Motivation: CMA는 언어 매개 상호작용을 통해 의도가 나타나도록 하여 에이전트 아키텍처의 오랜 어려움을 해결합니다.

Method: 이 프레임워크는 LLM에 추론을 오프로드하고, 모듈 간 통신을 통해 동시에 실행되는 모듈의 조합을 사용하여 맥락에 따라 적응적인 행동을 가능하게 합니다.

Result: 체계의 emergent properties는 자아 인식과 같은 복잡한 인지 현상이 간단한 과정의 조직적 상호작용에서 발생할 수 있음을 나타냅니다.

Conclusion: 이 연구는 Minsky의 사고 체계 개념을 지지하며 인공지능 연구의 새로운 길을 열어줍니다.

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [57] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 이 논문에서는 대형 언어 모델의 수학적 절차적 추론 능력을 향상시키기 위한 구조적 솔루션 템플릿(SST) 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 수학적 추론 성능 개선

Method: 구조적 솔루션 템플릿(SST) 프레임워크를 사용하여 절차적 추론을 가르치는 방법론 개발

Result: SST는 GSM8K, AIME24 및 새로운 Dynamic En 벤치마크에서 정확도와 효율성 모두를 개선함을 보여줌

Conclusion: SST는 어려운 문제에서 특히 뛰어난 성능을 발휘한다.

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [58] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 이 논문은 전자 건강 기록(EHR)에서 정보를 추출하고 임상 결정을 지원하는 대형 언어 모델(LLM)의 가능성을 제시하지만, 환각 위험으로 인해 임상 환경에서의 배치가 어려운 점을 지적한다. 저자들은 신뢰도 기준에 따라 정확성과 신뢰성 간의 상충 관계를 정량화하는 새로운 지표인 Hallucination Controlled Accuracy at k% (HCAcc@k%)를 제안하고, 임상 질문 응답을 위한 단계별 신뢰도 추정기를 포함하는 신뢰 기반 에이전트인 TrustEHRAgent를 도입한다. 실험 결과, TrustEHRAgent가 MIMIC-III 및 eICU 데이터셋에서 엄격한 신뢰성 기준 하에 기준 모델을 초과 달성하는 결과를 보였다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델을 임상 환경에서 신뢰성 있게 배치하기 위해서는 환각 위험을 관리할 필요가 있다.

Method: Hallucination Controlled Accuracy at k% (HCAcc@k%)라는 새로운 지표를 도입하고, 신뢰 기반 에이전트 TrustEHRAgent를 사용하여 단계별 신뢰도 추정을 수행한다.

Result: TrustEHRAgent는 MIMIC-III 및 eICU 데이터셋에서 HCAcc@70% 기준 하에 44.23%p 및 25.34%p의 성과를 달성하여 엄격한 신뢰성 기준 하에서 기준 방법을 초과했다.

Conclusion: 전통적인 정확성 지표는 의료 AI 에이전트를 평가하는 데 한계가 있음을 보여주며, 정확한 정보 제공 또는 신뢰도가 낮을 때 불확실성을 투명하게 표현할 수 있는 신뢰할 수 있는 임상 에이전트를 개발하는 데 기여한다.

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [59] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: 본 연구는 의료 분야에서 고급 추론 능력을 지닌 대형 언어 모델(LLM)의 발전을 논의하며, 이러한 모델이 의료 결정의 투명성과 설명 가능성을 향상시키는 방법을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 고급 추론 능력을 갖춘 LLM의 출현은 의료 응용 프로그램에서의 혁신적인 발전을 가져왔다.

Method: 기본 정보 검색 도구에서 복잡한 의료 결정을 지원할 수 있는 정교한 임상 추론 시스템으로의 의료 LLM의 변화를 조사하고, Chain-of-Thought와 DeepSeek-R1과 같은 강화 학습의 최근 혁신과 같은 기술적 기초를 철저히 분석한다.

Result: 의료 검증을 위한 평가 방법론을 비판적으로 평가하고, 다중 에이전트 협력 시스템 및 혁신적인 프롬프트 아키텍처와 같은 새로운 패러다임을 검토한다.

Conclusion: 신뢰할 수 있는 LLM을 개발하기 위한 로드맵을 확립하여 임상 실습과 의료 연구에서 효과적인 파트너로 작용할 수 있도록 한다.

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


### [60] [Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning](https://arxiv.org/abs/2508.19113)
*Dayoon Ko,Jihyuk Kim,Haeju Park,Sohyeon Kim,Dahyun Lee,Yongrae Jo,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: HDS-QA라는 합성 데이터셋을 통해 LRMs을 학습시켜 병렬 쿼리와 순차 쿼리를 구분하도록 하여 성능을 개선하였음.


<details>
  <summary>Details</summary>
Motivation: 기존의 방법들은 LRMs의 성능을 높이기 위해 외부 지식 검색을 수반하는 순차적 쿼리에 의존하였지만, 이는 추론 지연 시간과 맥락 길이를 증가시켜 일관성을 떨어뜨리고 정확성을 감소시킬 수 있음.

Method: HDS-QA (Hybrid Deep Search QA)를 도입하여 병렬 실행 가능한 독립 서브쿼리와 순차적으로 의존하는 서브쿼리를 결합한 하이브리드-홉 질문을 포함한 합성 데이터셋을 생성하고, 이를 통해 LRM을 미세 조정하여 HybridDeepSearcher 모델을 만들어냈음.

Result: HybridDeepSearcher는 여러 벤치마크에서 기존 최첨단 방법들을 초월하여 FanOutQA에서 +15.9 F1, BrowseComp의 하위 집합에서 +11.5 F1을 달성함.

Conclusion: HybridDeepSearcher는 감소된 검색 턴으로도 비교 가능한 정확도를 달성하여 추론 지연 시간을 크게 줄이고, 더 많은 턴을 허용할수록 효과적으로 확장됨을 보여줌.

Abstract: Large reasoning models (LRMs) have demonstrated strong performance in
complex, multi-step reasoning tasks. Existing methods enhance LRMs by
sequentially integrating external knowledge retrieval; models iteratively
generate queries, retrieve external information, and progressively reason over
this information. However, purely sequential querying increases inference
latency and context length, diminishing coherence and potentially reducing
accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search
QA), a synthetic dataset automatically generated from Natural Questions,
explicitly designed to train LRMs to distinguish parallelizable from sequential
queries. HDS-QA comprises hybrid-hop questions that combine parallelizable
independent subqueries (executable simultaneously) and sequentially dependent
subqueries (requiring step-by-step resolution), along with synthetic
reasoning-querying-retrieval paths involving parallel queries. We fine-tune an
LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms
state-of-the-art baselines across multiple benchmarks, notably achieving +15.9
and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both
requiring comprehensive and exhaustive search. Experimental results highlight
two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer
search turns, significantly reducing inference latency, and it effectively
scales as more turns are permitted. These results demonstrate the efficiency,
scalability, and effectiveness of explicitly training LRMs to leverage hybrid
parallel and sequential querying.

</details>


### [61] [Algorithmic Collective Action with Multiple Collectives](https://arxiv.org/abs/2508.19149)
*Claudio Battiloro,Pietro Greiner,Bret Nestor,Oumaima Amezgar,Francesca Dominici*

Main category: cs.AI

TL;DR: 본 논문에서는 여러 집단이 동일한 시스템에서 작용하는 알고리즘 집합 행동(ACA)을 위한 첫 이론적 프레임워크를 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 학습 시스템이 일상적인 결정에 점점 더 많은 영향을 미치고 있는 가운데, 사용자 측에서의 조정은 규제 기관의 정책과 기업 측 모델 설계에 대한 보완적 역할을 할 수 있습니다.

Method: 우리는 여러 집단이 동일한 시스템에서 작용하는 알고리즘 집합 행동을 위한 이론적 프레임워크를 제시하고, 여러 집단이 어떻게 신호를 심는지 분석합니다.

Result: 집단의 크기와 목표 정렬의 역할 및 상호작용에 대한 정량적 결과를 제공합니다.

Conclusion: 이 프레임워크는 이전의 경험적 결과를 보완하며 여러 집단이 있는 알고리즘 집합 행동에 대한 통합적 접근 방법을 열어줍니다.

Abstract: As learning systems increasingly influence everyday decisions, user-side
steering via Algorithmic Collective Action (ACA)-coordinated changes to shared
data-offers a complement to regulator-side policy and firm-side model design.
Although real-world actions have been traditionally decentralized and
fragmented into multiple collectives despite sharing overarching
objectives-with each collective differing in size, strategy, and actionable
goals, most of the ACA literature focused on single collective settings. In
this work, we present the first theoretical framework for ACA with multiple
collectives acting on the same system. In particular, we focus on collective
action in classification, studying how multiple collectives can plant signals,
i.e., bias a classifier to learn an association between an altered version of
the features and a chosen, possibly overlapping, set of target classes. We
provide quantitative results about the role and the interplay of collectives'
sizes and their alignment of goals. Our framework, by also complementing
previous empirical results, opens a path for a holistic treatment of ACA with
multiple collectives.

</details>


### [62] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 이 논문은 라몬 르룰의 아르스 콤비나토리아를 현대 연구 아이데이션을 위한 라룰의 사고 기계 구축의 개념적 기초로 재조명한다.


<details>
  <summary>Details</summary>
Motivation: 라몬 르룰의 아르스 콤비나토리아를 현대적 사고 기계의 기초로 활용하고자 한다.

Method: 주제, 영역, 방법이라는 세 가지 조합 축을 정의하고, LLM을 활용하여 큐레이션된 조합을 통해 연구 아이디어를 생성한다.

Result: 큐레이션된 조합을 사용하여 생성된 연구 아이디어가 다양하고, 관련성이 있으며, 현재 문헌에 기반하고 있음을 보여준다.

Conclusion: 이 현대적 사고 기계는 과학적 창의성을 증대시키는 경량화되고 해석 가능한 도구를 제공하며, 인간과 AI 간의 협업적 아이데이션을 위한 경로를 제시한다.

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [63] [The Subset Sum Matching Problem](https://arxiv.org/abs/2508.19218)
*Yufei Wu,Manuel R. Torres,Parisa Zehtabi,Alberto Pozanco Lancho,Michael Cashmore,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 서브셋 합 매칭 문제(SSMP)라는 새로운 조합 최적화 작업을 제시하고, 이를 해결하기 위한 두 개의 비최적 알고리즘과 하나의 최적 알고리즘을 소개한다.


<details>
  <summary>Details</summary>
Motivation: 본 연구의 동기는 거래 조정과 같은 일반적인 재무 응용 분야를 추상화한 SSMP라는 조합 최적화 작업을 도입하기 위해서이다.

Method: 두 개의 비최적 알고리즘과 하나의 최적 알고리즘을 통해 SSMP 문제를 해결하는 방법을 제시한다.

Result: 다양한 복잡도를 가진 여러 SSMP 인스턴스를 포괄하는 벤치마크를 생성하고, 접근 방법의 성능을 평가하기 위한 실험적 평가를 수행한다.

Conclusion: 이 연구는 SSMP 문제를 해결하기 위한 알고리즘을 제안하고 그 성능을 비교 분석하였다.

Abstract: This paper presents a new combinatorial optimisation task, the Subset Sum
Matching Problem (SSMP), which is an abstraction of common financial
applications such as trades reconciliation. We present three algorithms, two
suboptimal and one optimal, to solve this problem. We also generate a benchmark
to cover different instances of SSMP varying in complexity, and carry out an
experimental evaluation to assess the performance of the approaches.

</details>


### [64] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: StepWiser는 모델의 단계별 reasoning을 평가하는 생성적 판별기를 통해 기존 방법보다 더 나은 판단 정확도를 제공하며, 정책 모델 개선 및 추론 시간 검색을 개선한다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 문제 해결에 있어 다단계 추론 전략을 사용하는 모델의 중간 단계의 논리적 타당성을 감독하는 것이 중요해졌다.

Method: 단계별 보상 모델링을 분류 작업에서 자체 추론 작업으로 재구성하고, 정책 모델의 추론 단계를 평가하는 생성적 판별기를 제안한다.

Result: StepWiser는 기존 방법보다 중간 단계에서 더 나은 판단 정확도를 제공하고, 학습 시간에 정책 모델 개선에 사용될 수 있으며, 추론 시간 검색을 개선한다.

Conclusion: 본 연구는 생성적 판별기를 사용하여 다단계 추론의 판단 과정을 개선하고, 기존 모델의 한계를 보완할 수 있는 방법을 제시한다.

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


### [65] [Model Context Protocols in Adaptive Transport Systems: A Survey](https://arxiv.org/abs/2508.19239)
*Gaurab Chhetri,Shriyank Somvanshi,Md Monzurul Islam,Shamyo Brotee,Mahmuda Sultana Mimi,Dipti Koirala,Biplov Pandey,Subasish Das*

Main category: cs.AI

TL;DR: 이 문서는 모델 컨텍스트 프로토콜(MCP)을 통합 패러다임으로 제안하고, 이를 통해 단편화된 적응적 운송 시스템을 해결할 수 있는 방법을 논의한다.


<details>
  <summary>Details</summary>
Motivation: 연결된 장치, 자율 시스템, AI 애플리케이션의 빠른 확장은 적응형 운송 시스템의 심각한 단편화를 초래했다. 다양한 프로토콜과 맥락 소스가 고립되어 있다.

Method: MCP를 중심으로 한 체계적인 조사와 기존 문헌 분석을 통해, MCP와 유사한 아키텍처로의 자연스러운 진화를 제안한다.

Result: 세 가지 주요 통찰을 발견하였고, MCP의 클라이언트-서버 및 JSON-RPC 구조가 의미적 상호 운용성을 가능하게 하는 것을 보여준다.

Conclusion: MCP를 차세대 적응형, 맥락 인식 및 지능형 운송 인프라의 기초로 자리매김하는 연구 로드맵을 제시한다.

Abstract: The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: 이 논문은 LLM 훈련을 위한 커리큘럼 학습에서 사고의 깊이에 따라 난이도를 정의하고 이를 바탕으로 교육과정을 설계하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: LLM 훈련을 위한 커리큘럼 학습은 추론과 일치하면서도 확장 가능하고 해석 가능한 난이도 신호가 필요하다.

Method: 모델의 사고 추적에서 불연속 단계를 계산하여 사고의 깊이(DoT)를 정의하고, 이를 바탕으로 얕에서 깊으로 정렬된 커리큘럼으로 훈련한다.

Result: DoT는 기존의 추론 기준의 난이도와 상관관계가 있으며, DoT에 따라 정렬된 커리큘럼이 예산이 일치할 경우 길이 또는 평가 점수에 기반한 커리큘럼보다 뛰어난 성능을 보인다.

Conclusion: 우리는 코그니티브로 기반한 해석 가능한 커리큘럼을 향한 방향으로 나아가고자 한다.

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [67] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: 이 연구는 해양 환경에서 떠다니는 물체의 이동 예측을 위한 다중 모달 기계 학습 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 해양 환경에서 떠다니는 물체의 이동 예측은 특히 긴급한 상황, 예를 들어 구조 작업에서 중요한 도전 과제이다.

Method: 우리는 환경 및 물리적 데이터(물살, 바람 속도 등)를 수집하고, Navier-Stokes 기반 모델로부터의 데이터를 사용하여 CNN을 훈련시켜 힘 계수를 추정하고, 이를 통해 물체의 운동을 추적하는 데 사용한다.

Result: 이 프레임워크는 전통적인 물리 기반 모델 및 기계 학습 방법 대비 비슷한 성능을 보이며, 단일 단계 예측 대신 장기 예측을 가능하게 한다.

Conclusion: 다중 모달 모델링 전략이 역동적인 해양 조건에서 떠다니는 물체의 이동에 대해 정확하고 적응 가능한 예측을 제공할 수 있음을 보여준다.

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [68] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 이 연구는 기계 학습 방법을 활용한 데이터 기반 접근 방식으로 석유 저수지에서의 생산 예측 문제를 해결하고자 한다.


<details>
  <summary>Details</summary>
Motivation: 석유 저수지 공학에서 생산을 신뢰성 있게 예측하고 암석-유체 시스템의 행동 변화를 예상하는 것이 주요 도전 과제이다.

Method: 생산 및 주입 볼륨과 우물 내 계측기를 기준으로 간단한 데이터에 기반하여 생산 파라미터를 예측하는 방법론을 개발한다. 초기 단계에서는 생산 및 주입 변수의 관련성 분석을 수행하고 문제에 맞게 데이터를 조정한다.

Result: 신뢰할 수 있는 예측기를 설계하여 저수지 역학을 재생산하고, 우물 및 처리 유닛의 제약과 같은 실제 문제를 처리할 수 있는 빠른 응답 기능을 제공하는 것을 목표로 한다.

Conclusion: 이 방법은 석유 회수 극대화를 목표로 하여 저수지 관리 지원 활동에 활용될 수 있다.

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [69] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 본 연구는 빠르고 최소한의 데이터로 우울증을 식별하기 위한 시스템을 개발했습니다.


<details>
  <summary>Details</summary>
Motivation: 기존의 우울증 탐지 시스템은 긴 데이터 수집 기간이 필요하여, 조기 탐지가 중요한 경우에 효과적이지 않을 수 있습니다.

Method: 7일 간의 앱 사용 데이터를 1초 만에 수집하는 도구를 개발하였습니다. 방글라데시의 100명의 학생이 참여하였고, 다양한 기계 학습 모델을 통해 우울증 환자와 비환자를 식별했습니다.

Result: 1초 만에 수집된 앱 사용 데이터를 활용한 경량의 그래디언트 부스팅 머신 모델이 우울증 학생의 82.4%를 정확히 식별했습니다.

Conclusion: 우리 시스템의 빠르고 최소한의 특성 덕분에 저개발 및 개발지역에서 우울증 식별에 기여할 수 있습니다.

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [70] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: NeuroPathX는 뇌 구조 변화와 유전적 변이 간의 상호작용을 포착하는 설명 가능한 딥 러닝 프레임워크로, 자폐 스펙트럼 장애 및 알츠하이머병에서 검증되었습니다.


<details>
  <summary>Details</summary>
Motivation: 뇌 구조와 유전적 변이 간의 복잡한 상호작용을 해명하기 위한 새로운 접근 방식 필요.

Method: Cross-attention 메커니즘을 사용한 초기 융합 전략을 통해 뇌 구조의 변화와 유전 데이터에서 유래한 생물학적 경로 간의 상호작용을 캡처하는 설명 가능한 딥 러닝 프레임워크인 NeuroPathX를 제안.

Result: NeuroPathX가 경쟁하는 기초 방법들보다 우수한 성능을 보여주며 질병과 관련된 생물학적으로 그럴듯한 연관성을 드러냄.

Conclusion: NeuroPathX는 복잡한 뇌 질환에 대한 이해를 진전시키는 잠재력을 강조한다.

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [71] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 본 논문에서는 사전 훈련된 변환기 기반 언어 모델의 견고성을 평가하기 위한 통합된 로컬 견고성 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사전 훈련된 언어 모델의 크기와 배포가 증가함에 따라 입력 변동성에 대한 모델의 견고성이 점점 더 중요해지고 있다.

Method: 우리는 내부 매개변수를 수정하거나 복잡한 변동 휴리스틱에 의존하지 않고 모델의 안정성을 평가하는 SALMAN이라는 통합된 로컬 견고성 프레임워크를 제안한다.

Result: 이 작업에서는 각 샘플의 민감도를 비교하는 '거리 매핑 왜곡(DMD)' 척도를 도입하여 공격 효율성과 견고한 훈련에서 큰 성과를 달성하였다.

Conclusion: 우리 프레임워크는 변환기 기반 NLP 시스템의 신뢰성을 향상시키기 위한 실용적이고 모델에 구애받지 않는 도구로 자리잡고 있다.

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [72] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: 이 논문에서는 벡터 값 함수의 시공간 동역학 학습을 위한 통일된 프레임워크를 소개합니다.


<details>
  <summary>Details</summary>
Motivation: 벡터 값 함수의 복잡한 시간 변화 벡터 필드를 비모수적이고 데이터 기반으로 추정하기 위한 필요성으로부터 출발했습니다.

Method: 운영자 값 재생 커널 힐베르트 공간(OV-RKHS)과 커널 기반 쿱만 연산자 방법을 결합하여 프레임워크를 구축하였습니다.

Result: 우리는 시간 의존적인 OV-RKHS 보간을 위한 대표자 정리, 매끄러운 벡터 필드에 대한 소볼레프 유형 근사 경계, 커널 쿱만 연산자 근사에 대한 스펙트럴 수렴 보장을 도출했습니다.

Conclusion: 이 프레임워크는 고차원 비선형 시스템의 효율적인 축소 차원 모델링 및 장기 예측을 지원하며, 시공간 기계 학습에서 예측, 제어 및 불확실성 정량화를 위한 이론적으로 기초가 있는 도구를 제공합니다.

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [73] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: CoPE는 복소수 인코딩을 활용하여 콘텐츠와 위치 정보를 동시에 인코딩하는 새로운 아키텍처로, 기존의 위치 인코딩을 대체한다.


<details>
  <summary>Details</summary>
Motivation: 위치 인코딩이 트랜스포머 아키텍처에서 요소 간의 종속성을 모델링하는 데 필수적인 지침을 제공하는 효과를 입증하기 위해.

Method: CoPE는 콘텐츠와 위치 정보를 인코딩하기 위해 복소수 값을 사용하며, 첫 번째 레이어에 위상 인식을 고려한 어텐션을 도입한다.

Result: CoPE는 긴 시간 동안의 저하가 없으며 선형 어텐션과 호환된다. GLUE 벤치마크에서 RoPE, Sinusoidal 및 Learned 위치 인코딩과 비교하여 우수한 성능을 보였다.

Conclusion: CoPE는 낮은 계산 복잡도로 우수한 성능을 달성하는 새로운 위치 인코딩 접근 방식을 제시한다.

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [74] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: DPO는 대형 언어 모델을 인간의 선호에 맞추는 간단하고 효과적인 접근법으로, 본 연구는 선호 데이터 배포가 DPO 성능에 미치는 영향을 체계적으로 분석합니다.


<details>
  <summary>Details</summary>
Motivation: DPO의 효과를 높이기 위해 선호 데이터의 특성이 DPO 성능에 중요한 영향을 미치는지를 이해할 필요가 있습니다.

Method: 이론적 및 경험적 관점에서 선호 데이터 분포가 DPO에 미치는 영향을 체계적으로 연구합니다.

Result: 선택된 응답의 품질이 DPO 목표 최적화에서 주요한 역할을 한다는 것을 보여주고, 반면에 거부된 응답의 품질은 상대적으로 한정된 영향을 미친다는 것을 발견했습니다.

Conclusion: 반드시 선택된 응답의 품질을 향상시키면 DPO 성능이 일관되게 향상된다는 실험 결과를 도출했습니다. 또한, 정책 데이터를 혼합하는 것의 이점을 조사합니다.

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [75] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: ProtoEHR는 전자 건강 기록(EHR) 데이터를 활용하여 의료 예측 성능을 향상시키는 해석 가능한 계층적 프로토타입 학습 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 기존 연구는 EHR 데이터의 고립된 구성 요소에 집중하여 예측 성능과 해석 가능성을 제한했습니다. 이를 해결하기 위해 ProtoEHR을 제안합니다.

Method: ProtoEHR은 의학적 코드, 병원 방문, 환자라는 세 가지 계층적 수준 내에서의 관계를 모델링하며, 대규모 언어 모델을 활용하여 의학적 코드 간의 의미적 관계를 추출하고 의료 지식 그래프를 구축합니다.

Result: ProtoEHR은 두 개의 공공 데이터셋에서 다섯 가지 임상적으로 중요한 작업에 대해 평가되었으며, 기존 방법론에 비해 정확하고 강건하며 해석 가능한 예측 능력을 보여주었습니다.

Conclusion: ProtoEHR은 코드, 방문 및 환자 수준에서 해석 가능한 통찰력을 제공하여 의료 예측을 지원합니다.

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [76] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 이 연구는 기계 학습 모델을 개발하여 영국 대학에서 위험에 처한 학생을 예측하고, 연합 학습 프레임워크를 사용하여 데이터 프라이버시 문제를 해결하려고 한다.


<details>
  <summary>Details</summary>
Motivation: 원거리 교육에서 높은 이탈 및 실패율은 학술 기관에 큰 도전과제로 작용하며, 위험에 처한 학생을 사전에 식별하여 신속한 지원을 제공하는 것이 중요하다.

Method: 대규모 OULAD 데이터 세트의 초기 학업 성과와 디지털 참여 패턴을 기반으로 한 기계 학습 모델을 개발하고 평가하였으며, 연합 학습(FL) 프레임워크를 사용하여 구현하였다.

Result: 최종 연합 모델은 위험에 처한 학생을 식별하는 데 약 85%의 ROC AUC 점수를 달성하여 강력한 예측 능력을 보였다.

Conclusion: 이 연구 결과는 연합 접근 방식이 기관에서 효과적인 조기 경고 시스템을 구축할 수 있는 실용적이고 확장 가능한 솔루션을 제공하여 데이터 프라이버시를 존중하면서도 신속한 학생 지원을 가능케 한다는 것을 보여준다.

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [77] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: ZTFed-MAS2S는 제로 트러스트 연합 학습 프레임워크로, 풍력 데이터의 결측 값을 정확히 보완하는 데 초점을 맞추고 있습니다.


<details>
  <summary>Details</summary>
Motivation: 풍력 발전 데이터는 센서 결함 및 불안정한 전송으로 결측 값이 발생하기 쉽습니다. 연합 학습은 데이터 공유 없이 프라이버시를 보호하는 협력을 가능하게 하지만, 매개변수 교환 중 비정상적 업데이트와 프라이버시 유출의 취약성이 존재합니다.

Method: ZTFed는 다중 헤드 주의 기반 시퀀스-투-시퀀스 보완 모델을 통합한 제로 트러스트 연합 학습 프레임워크입니다. 검증 가능한 차별적 프라이버시와 비대면 제로 지식 증명을 통합하여 프라이버시 보호를 보장하며, 신뢰 동적 집계 메커니즘을 사용합니다.

Result: 실제 풍력 발전소 데이터셋에 대한 광범위한 실험을 통해 ZTFed-MAS2S가 연합 학습 성능과 결측 값 보완 모두에서 우수함이 확인되었습니다.

Conclusion: ZTFed-MAS2S는 에너지 분야의 실용적 응용에 대해 안전하고 효율적인 솔루션으로 효과적임을 입증했습니다.

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [78] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: HSIC은 두 데이터 샘플 간의 통계적 의존성을 평가하는 최첨단 방법이나, 큰 데이터 샘플에는 비현실적이다. HCR은 선형 비용으로 의존성을 테스트할 수 있는 실제적 대안으로 제시된다.


<details>
  <summary>Details</summary>
Motivation: 통계적 의존성 평가의 중요성과 기존 방법의 비효율성을 해결하기 위해, 더 큰 데이터 샘플에 적합한 대안을 제공하고자 한다.

Method: HCR(계층적 상관관계 재구성) 방법을 사용하여 선형 비용으로 더욱 높은 의존성 민감도를 제공한다. 이 방법은 특징이 혼합된 모멘트를 통해 의존성을 설명하고, 상관관계와 동분산성을 기반으로 실제 결합 분포 모델을 제공한다.

Result: HCR은 데이터 샘플 간의 무상관 정보를 제곱합으로 근사할 수 있으며, 그 계산은 O(n) 선형 시간에 이루어진다.

Conclusion: HCR 방법은 대규모 데이터 샘플의 통계적 의존성을 효과적으로 평가할 수 있는 유망한 대안이다.

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [79] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: Mixture of Experts (MoE)는 대규모 언어 모델(LLM) 구축을 위한 주요 아키텍처로 자리잡았으며, 본 논문에서는 MoE의 효율적인 배치를 위해 사전 훈련된 MoE 모듈의 텐서 및 뉴런 수준에서 이중 희소성을 핵심 요소로 식별하고, 이는 후훈련 전문 파티셔닝을 통해 도출된다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 계산 효율성을 높이기 위해 MoE의 희소성을 개선하려는 필요성이 있다.

Method: 후훈련 전문 파티셔닝 기법을 도입하여 텐서 수준의 계산을 동적으로 감소시키고 뉴런 수준의 재구성으로 결합하여 수행한다.

Result: 제안 방법은 MoE 모델에서 약 25%의 드롭률을 적용하였을 때 정확도 저하가 0.08%-0.28%에 불과하며, 계산 속도 향상을 일관되게 제공한다.

Conclusion: 부하 불균형 인식을 전문가 병렬 처리에 통합하여 MoE 모듈의 속도를 1.41배 가속하면서도 평균 정확도 저하는 0.5%에 그친다.

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [80] [DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift](https://arxiv.org/abs/2508.18839)
*Shae McFadden,Myles Foley,Mario D'Onghia,Chris Hicks,Vasilios Mavroudis,Nicola Paoletti,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 본 연구는 딥 강화 학습(DRL)을 기반으로 한 맬웨어 탐지 방법을 개발하여 변화하는 위협 환경에서 성능을 유지하고 수동 라벨링을 위해 고위험 샘플을 거부하는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 현실 세계의 맬웨어 탐지에서 진화하는 위협, 제한된 라벨링 예산 및 불확실한 예측 문제를 다루어야 하므로 기존 분류기와 전통적인 방법은 어려움이 많습니다.

Method: 본 연구에서는 맬웨어 탐지를 한 단계 마르코프 결정 과정으로 모델링하고, DRL 에이전트를 훈련하여 샘플 분류 성능을 최적화하고 수동 라벨링을 위해 고위험 샘플을 거부하도록 설계하였습니다.

Result: 우리는 현실적인 드리프트 조건에서 안정성을 요구하는 안드로이드 맬웨어 데이터셋에 대한 시간 인식 평가를 통해 DRL 기반 맬웨어 탐지 에이전트가 학습한 공동 탐지 및 드리프트 완화 정책을 평가했습니다.

Conclusion: 우리의 결과는 DRL이 안드로이드 맬웨어 도메인의 동적 환경에서 효과적인 맬웨어 탐지와 개념 드리프트에 대한 개선된 회복력을 촉진할 수 있음을 최초로 입증합니다.

Abstract: Malware detection in real-world settings must deal with evolving threats,
limited labeling budgets, and uncertain predictions. Traditional classifiers,
without additional mechanisms, struggle to maintain performance under concept
drift in malware domains, as their supervised learning formulation cannot
optimize when to defer decisions to manual labeling and adaptation. Modern
malware detection pipelines combine classifiers with monthly active learning
(AL) and rejection mechanisms to mitigate the impact of concept drift. In this
work, we develop a novel formulation of malware detection as a one-step Markov
Decision Process and train a deep reinforcement learning (DRL) agent,
simultaneously optimizing sample classification performance and rejecting
high-risk samples for manual labeling. We evaluated the joint detection and
drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent
through time-aware evaluations on Android malware datasets subject to realistic
drift requiring multi-year performance stability. The policies learned under
these conditions achieve a higher Area Under Time (AUT) performance compared to
standard classification approaches used in the domain, showing improved
resilience to concept drift. Specifically, the DRMD agent achieved a
$5.18\pm5.44$, $14.49\pm12.86$, and $10.06\pm10.81$ average AUT performance
improvement for the classification only, classification with rejection, and
classification with rejection and AL settings, respectively. Our results
demonstrate for the first time that DRL can facilitate effective malware
detection and improved resiliency to concept drift in the dynamic environment
of the Android malware domain.

</details>


### [81] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 저자는 저순위 텐서 분해 방법이 심층 신경망 성능의 여러 측면을 이론적으로 설명하는 데 필수적인 역할을 한다고 강조하며, 이를 통해 심층 학습 이론에 대한 수학적 기초를 제공하려고 한다.


<details>
  <summary>Details</summary>
Motivation: 심층 신경망의 혁신적인 성능으로 인해 심층 학습 이론의 수학적 기초를 제공하려는 관심이 폭발적으로 증가하고 있다.

Method: 저자는 신호 처리 및 기계 학습 커뮤니티에서 핵심 도구로 활용되는 저순위 텐서 방법을 조명한다.

Result: 저순위 텐서 방법이 심층 신경망의 표현력, 알고리즘 학습 가능성, 계산적 난이도, 일반화 및 식별 가능성과 같은 성능의 다양한 측면을 이론적으로 설명하는 데 필수적임을 보여준다.

Conclusion: 기존의 접근 방식을 일관되고 통합된 방식으로 개관하여 저순위 텐서 분해가 심층 신경망 이론에 기여할 수 있는 더 넓은 관점을 열고자 한다.

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [82] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: 이 논문은 두 가지 내재적 동기 전략의 결합을 통한 강화 학습 에이전트의 효율성 향상에 대해 다룹니다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 강화 학습 방법이 드문 긍정적 피드백 때문에 성능을 발휘하기 어려운 환경에서 강화 학습 에이전트를 개선하기 위해.

Method: 변별적 상태를 내재적 보상으로 사용하는 Variational State as Intrinsic Reward (VSIMR)와 대형 언어 모델(LLMs)에서 파생된 내재적 보상 접근 방식을 통합함.

Result: 이 결합된 접근 방법은 MiniGrid DoorKey 환경에서 A2C 에이전트를 사용하여 구현되었으며, 개별 전략이나 표준 A2C 에이전트에 비해 에이전트 성능과 샘플링 효율성을 크게 증가시켰습니다.

Conclusion: 결합 전략은 환경과 과제의 다양한 측면을 효과적으로 보완하며, VSIMR은 새로운 상태 탐색을 촉진하고 LLM 파생 보상은 목표 향한 점진적 활용을 지원합니다.

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [83] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 본 논문은 고차원 공간에서 베이지안 최적화를 개선하기 위한 새로운 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 베이지안 최적화는 비싼 블랙박스 함수 최적화에 널리 적용되지만, 고차원 공간으로의 확장은 여전히 도전적입니다.

Method: 전역 GP로부터의 기울기와 헤세안을 사용하여 여러 개의 지역 이차 모델을 구축하고 경계 제약 다항식을 해결하여 새로운 샘플 포인트를 선택합니다.

Result: 당사의 방법은 TuRBO의 효율성을 향상시키고 합성 함수 및 실제 응용 프로그램에서 다양한 고차원 BO 기술보다 우수한 성능을 보여줍니다.

Conclusion: 고차원 공간에서의 GP의 기울기 소실 문제도 해결하였으며, 수렴 분석을 제공합니다.

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [84] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: 하드웨어 언어인 Verilog의 코드 생성을 위한 강화 학습 프레임워크를 제안하며, 고품질 데이터셋과 새로운 보상 메커니즘을 활용하여 성능 향상을 달성한다.


<details>
  <summary>Details</summary>
Motivation: 소프트웨어 도메인에서 코드 생성의 성공이 두드러지지만, Verilog와 같은 하드웨어 설명 언어는 동시성 의미론, 구문 경직성, 시뮬레이션 복잡성으로 인해 충분히 탐구되지 않았다.

Method: Verilog 코드 생성을 위해 강화 학습 프레임워크를 구축하고, Veribench-53K라는 700K 이상의 Verilog 문제로 구성된 고품질 데이터셋을 생성하며, Trace-back 기반 재조정 메커니즘과 샘플 균형 가중치 전략을 도입하였다.

Result: Verilog 생성 작업에 대한 실험은 테스트 통과율, 기능적 정확성 및 컴파일 강건성에서 현저한 성능 개선을 보여주었다.

Conclusion: RL 주도 접근 방식을 통해 하드웨어 중심 도메인에서 구조적 코드 생성을 위한 가능성을 강조하고, VERIRL을 공개하였다.

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [85] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: 본 연구는 강화 학습 기반의 프레임워크인 DRTA를 통해 시간 시계열 데이터의 이상 탐지를 개선하는 방법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 데이터에서의 이상 탐지는 금융, 건강 관리, 센서 네트워크 및 산업 모니터링 분야에서 중요합니다.

Method: 본 연구에서는 동적 보상 조정, 변동 오토인코더(VAE), 능동 학습을 통합한 강화 학습 기반 프레임워크인 DRTA를 제안합니다.

Result: 제안된 방법은 Yahoo A1 및 Yahoo A2 벤치마크 데이터셋에서 기존 최첨단 비지도 및 반지도 접근법을 일관되게 능가하는 성능을 보였습니다.

Conclusion: 우리 프레임워크는 실제 이상 탐지 작업에 대해 확장 가능하고 효율적인 솔루션으로 입증되었습니다.

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [86] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: 본 논문은 특정 데이터의 영향을 제거하는 기계 비학습(MU) 방법에서 데이터 증강 전략의 중요성을 조사하였다.


<details>
  <summary>Details</summary>
Motivation: MU는 훈련된 모델의 특정 데이터의 영향을 제거하면서 나머지 데이터에 대한 성능을 유지하는 것을 목표로 한다.

Method: 우리는 SalUn, Random Label, Fine-Tuning 등 다양한 비학습 방법의 성능에 대한 데이터 증강 전략의 영향을 조사하였다.

Result: CIFAR-10 및 CIFAR-100에서 실시한 실험 결과, 적절한 데이터 증강 설계가 비학습의 효율성을 크게 향상시킬 수 있음을 보여주었으며, TrivialAug 증강을 사용할 경우 평균 격차 비학습 지표가 최대 40.12% 감소하였다.

Conclusion: 데이터 증강은 기억 감소를 지원할 뿐만 아니라 사생활 보호와 효율적인 비학습을 달성하는 데 필수적인 역할을 한다.

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [87] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: 본 연구는 변칙 체적 문제를 해결하기 위해 강화 학습 기반의 초기화 전략을 제안하며, 이를 통해 VQA의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 변칙 체적 문제로 인해 VQA의 효과성이 제한되며, 이는 양자 장치의 근처 적용에서의 문제를 야기합니다.

Method: 정책 경량화, 소프트 액터-비평가 및 근접 정책 최적화 등을 포함한 여러 강화 학습 알고리즘을 통해 초기 매개변수를 생성합니다.

Result: 강화 학습 기반 초기화 방법이 수렴 속도와 최종 솔루션 품질을 모두 향상시키는 것을 보여줍니다.

Conclusion: 강화 학습 기반 매개변수 초기화가 VQAs의 확장성 및 실제 적용을 가속화하는 데 기여할 수 있음을 시사합니다.

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [88] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 이 논문은 신경망이 수행할 수 있는 추론 형태를 회로 에뮬레이션으로 해석하고, 신경망으로 변환하는 체계적인 메타 알고리즘을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 현대 AI 연구의 주요 질문 중 하나는 완벽하게 훈련된 신경망이 수행할 수 있는 추론의 형태를 정량화하는 것이다.

Method: 각 게이트를 표준 ReLU MLP 에뮬레이터로 반복적으로 대체하여 본질적으로 어떤 회로든 피드포워드 신경망(활성화로 ReLU 사용)으로 변환하는 체계적인 메타 알고리즘을 제시합니다.

Result: 우리의 구조는 디지털 컴퓨터에서 회로를 정확하게 에뮬레이트하며, 신경망의 뉴런 수는 회로의 복잡성과 비례합니다.

Conclusion: 우리의 결과는 고전적인 범용 근사 정리보다 강력한 것으로, 어떤 범용 함수 근사기도 회로로 인코딩되고 직접 신경망으로 에뮬레이트될 수 있다.

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [89] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: 본 논문은 Mixture-of-Experts 모델의 모듈화 전문화를 활용하여 다중 모드 학습에서의 성능과 한계를 조사하고, Beyond Two-modality Weighting (BTW) 프레임워크를 제안하여 다중 모드 정보를 효과적으로 처리하는 방법을 제시함.


<details>
  <summary>Details</summary>
Motivation: 다중 모드 정보를 사용할 때, 추가적인 모드가 상호 보완적인 정보보다 더 많은 노이즈를 소개할 경우 효과가 불분명함.

Method: 비파라메트릭 가중치 프레임워크인 Beyond Two-modality Weighting (BTW)을 제안하며, 개별 예제의 Kullback-Leibler 발산과 모드 수준의 상호 정보량을 결합하여 모드 중요성을 동적으로 조정함.

Result: 감정 회귀 및 임상 분류에 대한 광범위한 실험에 따르면, 우리의 방법이 회귀 성능과 다중 클래스 분류 정확도를 유의미하게 향상시킴을 보여줌.

Conclusion: 제안된 방법은 추가적인 파라미터 없이 임의의 수의 모드에 적용될 수 있으며, 각 예제의 KL 가중치를 측정하고 모드 간 MI 가중치를 추정하여 성능을 개선함.

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [90] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 이 논문에서는 분자 그래프를 완전하게 하기 위해 훈련된 생성 모델에서 샘플링된 화학적으로 합리적인 조각으로 마스킹된 하위 구조를 대체하는 반사실 마스킹이라는 새로운 프레임워크를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 분자 속성 예측은 약물 및 소재를 포함한 새로운 화합물의 설계를 안내하는 중요한 작업입니다.

Method: 우리는 반사실 마스킹이라는 새로운 프레임워크를 제안하며, 이 방법은 마스킹된 하위 구조를 화학적으로 합리적인 조각으로 대체하는 것입니다.

Result: 우리의 방법은 분자 현실주의를 통해 견고하고 분포 일관성 있는 설명을 지원하고, 구조 수정이 예측된 속성에 미치는 영향을 직접적으로 나타내는 의미 있는 반사실을 제공합니다.

Conclusion: 이 방법은 설명 가능성과 분자 설계 간의 간극을 줄이며 화학에서 설명 가능한 기계 학습을 위한 원칙적이고 생성적인 경로를 제공합니다.

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [91] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: 이 논문은 MPNN의 여러 결과를 개선하고 확장하여 그래프 기계 학습의 실제 적용 가능성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 이전 논문에서 MPNN의 분석에 필요한 몇 가지 구성 요소가 누락되어 실제 적용에 제한이 있다.

Method: 주요 결과를 다차원 신호에 맞춰 확장하고, Lipschitz 연속성을 읽기 기반 MPNN으로 확장하며, 강건성 일반화 경계를 활용하여 일반화 경계를 개선하고, 비대칭 그래프온 및 커널로 분석을 확장한다.

Result: 이렇게 여러 확장을 통해 MPNN의 이론적 한계를 극복하고 더 넓은 실용성을 갖는다.

Conclusion: 결과적으로, 이 논문은 MPNN의 이론적 뒷받침을 강화하고 그래프 기계 학습 분야의 적용 가능성을 높이는 데 기여한다.

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [92] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: Stochastic PushForward (SPF) 프레임워크는 복잡한 시스템에 대해 장기 예측 정확도를 향상시키고 GPU 메모리 요구사항을 낮추면서 단기 성능을 감소시키지 않는 방법을 제시한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 수치 예측에 비해 데이터 기반 방법이 효율적인 대안으로 부상하고 있으나, 복잡한 시스템의 경우 장기 정확도가 오류 축적으로 인해 저하될 수 있다.

Method: SPF 프레임워크는 모델 예측으로부터 보조 데이터셋을 구축하고, 무작위 획득 전략을 통해 실제값과 결합하여 단기 및 장기 성능의 균형을 유지하며 오버피팅을 줄인다.

Result: SPF는 Burgers 방정식과 Shallow Water 벤치마크에서 장기적 정확도가 자율 회귀 방법보다 우수함을 보여주며 메모리 요구사항을 낮춘다.

Conclusion: 리소스가 제한된 복잡한 시뮬레이션에 대해 유망한 접근법으로 평가된다.

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [93] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 저희 연구는 저자원 환경에서의 단백질 기능 예측과 단백질 설계를 개선하기 위해 희소 오토인코더(SAE)를 평가했습니다.


<details>
  <summary>Details</summary>
Motivation: 단백질 기능을 아미노산 서열로 예측하는 것은 데이터가 부족한 조건에서 여전히 중심적인 도전 과제로 남아있습니다.

Method: 저희는 다양한 적합성 추정 및 단백질 공학 작업에 대해 정밀 조정된 ESM2 임베딩으로 훈련된 희소 오토인코더(SAE)를 평가했습니다.

Result: SAE는 24개의 서열로도 적합성 예측에서 ESM2 기준선을 일관되게 초과하거나 경쟁하는 성능을 보였습니다.

Conclusion: 예측 라텐트를 조정하는 것은 pLM 표현의 생물학적 모티프를 활용하여 ESM2로만 설계할 때에 비해 83%의 경우에서 최고 적합성 변이를 생성했습니다.

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [94] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: DrugReasoner는 LLaMA 아키텍처를 기반으로 한 추론 기반 대형 언어 모델로, 작은 분자의 승인 가능성을 예측하며 단계별 이유 및 신뢰도를 제공한다.


<details>
  <summary>Details</summary>
Motivation: 신약 발견 과정의 복잡성과 자원 집약성을 감안할 때, 승인 결과의 조기 예측이 연구 투자 최적화에 중요하다.

Method: DrugReasoner는 분자 설명자를 통합하고 구조적으로 유사한 승인 및 비승인 화합물에 대해 비교 추론을 수행한다.

Result: DrugReasoner는 검증 세트에서 AUC 0.732 및 F1 점수 0.729, 테스트 세트에서 각각 0.725 및 0.718의 성능을 달성했다.

Conclusion: 이 연구는 추론 출력을 통해 투명성을 높이고 AI 지원 신약 발견에서 중요한 병목 현상을 해결할 수 있는 잠재력을 강조한다.

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [95] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL은 기존 RL 방법 대비 2.6배 성능 향상을 달성하면서도 정확도와 RL 패러다임을 유지하는 LLM RL 시스템이다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 추론 능력을 향상시키기 위해 강화 학습이 중요해졌다.

Method: HistoSpec와 HistoPipe라는 두 가지 혁신을 통해 롤아웃 생성을 개선하고 롤아웃 불균형 문제를 해결한다.

Result: RhymeRL은 기존 방법보다 2.6배 성능 향상을 달성했다.

Conclusion: RhymeRL은 GPU 자원 활용도를 극대화하면서 정확도를 유지하는 효과적인 솔루션이다.

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [96] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: 주요 포트폴리오 접근법은 신호 기반 거래에서 emerging된 방법으로, 제안된 새로운 선형 거래 포지션이 예측 행렬의 더 넓은 스펙트럼 영역을 탐색하도록 돕는다.


<details>
  <summary>Details</summary>
Motivation: 주요 포트폴리오 접근법은 신호 기반 거래에서 유망하나, 예측 행렬의 핵심 특징을 탐색하는 데 있어 다양성이 부족하고 여러 상황에 강건하지 않다.

Method: 희소 스펙트럼을 가진 새로운 선형 거래 포지션을 제안하고, 이를 최적화하기 위해 Krasnosel'ski-Mann 고정점 알고리즘을 개발하였다.

Result: 제안한 방법이 다양한 상황에서 우수하고 강건한 성능을 달성함을 보여주는 광범위한 실험이 수행되었다.

Conclusion: 이 알고리즘 유형에 대한 새로운 이론적 결과로, 목적 값에서 선형 수렴 속도를 달성한다.

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [97] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 본 논문에서는 비지도 도메인 적응을 개선하기 위해 다중 스케일 특징 추출과 불확실성 추정 기법을 도입한 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 시간 시계열 데이터에서 훈련과 테스트 데이터셋 간의 분포 편차 문제를 해결하기 위한 효과적인 일반화 방법을 찾기 위함이다.

Method: 다중 스케일 혼합 입력 아키텍처를 사용하여 서로 다른 스케일에서 특징을 캡처하고, 불확실성 추정을 위한 Dirichlet 사전 정보를 라벨에 적용하여 불확실성 인식 메커니즘을 도입한다.

Result: 목표 도메인에서 상당한 성능 향상이 이루어졌으며, 기대 보정 오차(ECE)가 현저히 낮아졌다.

Conclusion: 혼합 입력 아키텍처와 불확실성 인식 메커니즘의 결합된 접근 방식이 여러 벤치마크 데이터셋에서 최첨단 성능을 달성하여 시간 시계열 데이터의 비지도 도메인 적응에 효과적임을 보여준다.

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [98] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: STRATA-TS는 데이터 부족 상황에서 예측을 개선하기 위해 도메인 적응형 검색과 대규모 모델을 결합한 프레임워크이다.


<details>
  <summary>Details</summary>
Motivation: 도시 예측 모델은 데이터 불균형 문제에 직면해 있으며, 데이터가 풍부한 도시에서 데이터가 부족한 도시로의 직접적인 전이가 신뢰할 수 없다.

Method: STRATA-TS는 패치 기반의 시간 인코더를 사용하여 소스 하위 시퀀스를 식별하고, 검색된 예제를 추론 단계에 주입하여 표적 입력에 대한 구조적 추론을 수행한다.

Result: 싱가포르, 노팅햄, 글래스고의 세 가지 주차 가능성 데이터셋에서 다양한 실험을 통해 STRATA-TS는 강력한 예측 및 전이 기준선을 지속적으로 초월하는 성과를 보였다.

Conclusion: STRATA-TS는 해석 가능한 지식 전환 경로를 제공하면서 데이터 부족 상황에서도 더욱 우수한 예측 성능을 입증하였다.

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [99] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: BDVAE는 다중 오믹스 데이터를 활용하여 면역 체크포인트 억제제에 대한 대응을 예측하고 저항 메커니즘을 밝혀내는 새로운 기계 학습 모델이다.


<details>
  <summary>Details</summary>
Motivation: 면역 체크포인트 억제제에 대한 환자의 반응이 매우 다양한데, 저항의 생물학적 메커니즘이 잘 이해되지 않고 있다.

Method: BDVAE는 모달리티 및 경로 특정 인코더를 통해 전사체 및 게놈 데이터를 통합하는 심층 생성 모델이다.

Result: BDVAE는 366명의 환자가 포함된 다암종 집단에 적용하여 치료 반응을 정확하게 예측하고(테스트 데이터 AUC-ROC = 0.94), 주요 저항 메커니즘을 밝혀냈다.

Conclusion: BDVAE는 저항이 이분법적인 상태가 아니라 연속적인 생물학적 스펙트럼으로 존재함을 밝혔으며, 생물학적으로 구조화된 기계 학습의 가치를 강조한다.

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [100] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: 이 연구는 금융 리스크 평가를 위한 새로운 다중 모델 프레임워크를 제안하며, 주로 기업의 수익 발표통화에서의 음성 역학에 기반한 비언어적 신호와 텍스트 감정을 통합한다.


<details>
  <summary>Details</summary>
Motivation: 금융 시장에서의 정보 비대칭은 기업 내러티브에 의해 증폭되어 전통적인 텍스트 분석의 효과성을 저하시킨다.

Method: 이 프레임워크의 핵심은 비선형 음향을 적용하여 수익 발표의 원시 음성에서 감정 신호를 강력하게 추출하는 Physics-Informed Acoustic Model (PIAM)이다.

Result: 다중 모델 특징이 방향성 주가 수익을 예측하지는 못하지만, 30일 실현 변동성의 샘플 외 변수의 43.8%를 설명할 수 있다는 사실을 발견했다.

Conclusion: 정서적 동역학이 주가 변동성 예측을 강하게 구동하며, CFO와 CEO의 대본에서 자발적인 발화로의 전환 동안의 정서적 불안정성이 중요하다.

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [101] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: 이 논문은 분산 엣지 장치에서의 고품질 훈련 데이터로 인해 발생하는 개인 정보 및 자원 제한 속에서의 모델 조정을 위한 새로운 연합 미세 조정 프레임워크인 FFT MoE를 제안합니다.


<details>
  <summary>Details</summary>
Motivation: AGI로의 진전을 추진하는 FMs의 연합 미세 조정은 자원 및 프라이버시 제약 하에서 필수적입니다.

Method: FFT MoE는 LoRA 대신 희소 혼합 전문가(MoE) 어댑터를 사용하며, 각 클라이언트는 경량 게이팅 네트워크를 훈련시켜 개인화된 전문가 하위 집합을 선택적으로 활성화합니다.

Result: FFT MoE는 일반화 성능과 훈련 효율성에서 상태의 FFT 기준을 일관되게 초월합니다.

Conclusion: FFT MoE는 엣지 장치 내에서 리소스와 데이터의 이질성에 대응하여 전문가 부하 불균형 문제를 해결합니다.

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [102] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 본 논문은 근사 기계 비학습 알고리즘 후에 남아있는 샘플들의 개인 정보 보호 위험을 감사하는 접근 방식을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 개인 정보 보호를 보장하기 위해 훈련된 모델에서 특정 데이터의 영향을 제거해야 하는 필요성이다.

Method: 기존 비학습 알고리즘의 적용 후, 비학습된 샘플과 남아있는 샘플의 개인 정보 보호 기준을 제안한다.

Result: 기존의 근사 기계 비학습 알고리즘이 차별적 개인 정보 보호 모델에서 남아있는 샘플의 개인 정보를 우연히 위협할 수 있음을 발견했다.

Conclusion: 차별적 개인 정보 보호를 위한 비학습 알고리즘이 필요하며, 우리는 코드의 재현 가능성을 위해 코드를 공개했다.

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [103] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: 이 논문은 MoE 모델의 희소성이 기억과 추론 두 가지 능력 영역에 미치는 영향을 조사합니다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델의 발전을 위한 경험적 스케일링 법칙이 중요한 역할을 하지만, 모델 아키텍처나 데이터 파이프라인이 바뀔 때마다 계수가 변합니다.

Method: MoE 변환기를 훈련시키고, 총 파라미터 수, 활성화 파라미터 수, top-$k$ 라우팅을 체계적으로 조정하면서 계산 예산을 고정합니다.

Result: 모델마다 사전 훈련 손실, 하위 작업 손실, 작업 정확도를 기록하여 훈련-테스트 일반화 격차와 손실-정확도 격차를 분리합니다.

Conclusion: 과도하게 희소한 모델은 추론 성능의 저하를 겪으며, 이는 추가적인 학습이나 계산으로 해결되지 않습니다.

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [104] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: LRTab은 훈련 데이터를 통해 학습한 정보를 활용하여 테이블 이해를 위한 새로운 프롬프트 기반 추론 접근법을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 데이터 과학자에게 자동화된 테이블 이해와 추론은 필수적인 작업이며, 최근 대형 언어 모델이 테이블 추론 작업에 점점 더 많이 사용되고 있습니다.

Method: LRTab은 훈련 데이터를 통해 학습한 정보를 검색하는 방법으로, CoT 응답을 얻기 위해 프롬프트를 사용하고 잘못된 CoT에 대해서는 LLM에게 오류를 피할 수 있는 프롬프트 조건을 예측하도록 유도합니다.

Result: WikiTQ와 Tabfact에 대한 종합적인 실험을 통해 LRTab이 해석 가능하고 비용 효율적이며 이전 기준선보다 더 우수한 성능을 보여줍니다.

Conclusion: LRTab은 훈련 데이터에서 얻은 맥락을 사용하여 테이블 이해를 개선하는 효과적인 방법을 제공하며, 기존 방법들보다 더 나은 성능을 발휘합니다.

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [105] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 이 논문에서는 생명 위협적인 상태인 패혈증을 조기 발견하기 위한 딥러닝 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 패혈증은 중환자 치료 환경에서 조기에 감지해야 하는 생명 위협적인 상태이며, 전통적인 기계 학습 접근법은 전자 건강 기록에서 자주 발생하는 불규칙하고 불완전한 시계열 데이터에 어려움을 겪는다.

Method: 우리는 자동 특징 추출을 위한 비지도 자동 인코더와 이진 패혈증 위험 예측을 위한 다층 퍼셉트론 분류기를 통합한 엔드 투 엔드 딥러닝 프레임워크를 도입한다.

Result: 우리의 엔드 투 엔드 모델은 각각 74.6%, 80.6%, 93.5%의 정확도를 달성하여 전통적인 기계 학습 기준선을 일관되게 초과한다.

Conclusion: 이 결과는 이 프레임워크의 탁월한 견고성, 일반화 가능성 및 이질적인 ICU 환경에서의 조기 패혈증 감지를 위한 임상 유용성을 입증한다.

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [106] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: 이 논문에서는 이미지 분류를 위해 통계물리학, 코딩 이론, 대수적 위상수학을 결합한 통합 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 효율적인 다중 클래스 이미지 분류를 위한 새로운 접근 방식이 필요하다.

Method: MobileNetV2를 기반으로 한 고차원 특징 벡터를 활용하여 희소 다중 엣지 타입의 준주기 LDPC 그래프에서 스핀으로 해석하고, Nishimori 온도에서 RBIM을 운영한다.

Result: 이미지넷 데이터셋에서 40배 압축에도 불구하고 높은 정확도를 달성하였다.

Conclusion: 위상에 의해 안내된 그래프 설계는 최신 성능을 발휘하는 고효율의 물리학 기반 임베딩을 제공함을 보여준다.

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [107] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: 이 논문은 RTL 설계 품질 추정을 위한 새로운 구조 인식 그래프 자가 감독 학습 프레임워크인 StructRTL을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: RTL 설계 품질 추정은 전자 설계 자동화(EDA) 워크플로에서 핵심 메트릭에 대한 즉각적인 피드백을 가능하게 합니다.

Method: CDFG에서 구조적으로 인식된 표현을 학습하는 새로운 방법론을 제시하고, 사전 맵핑 넷리스트에서 CDFG 예측기로 통찰을 전이하는 지식 증류 전략을 통합합니다.

Result: 우리는 다양한 품질 추정 작업에서 이전 기술보다 우수한 성과를 기록했습니다.

Conclusion: 구조 학습과 교차 단계 감독을 결합하는 방식의 효과를 입증하며 새로운 최첨단 결과를 확립했습니다.

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


### [108] [FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks](https://arxiv.org/abs/2508.18737)
*Enrique Mármol Campos,Aurora González Vidal,José Luis Hernández Ramos,Antonio Skarmeta*

Main category: cs.LG

TL;DR: FLAegis는 Byzantine 클라이언트를 식별하고 FL 시스템의 강인성을 향상시키기 위한 두 단계의 방어 프레임워크로, 정밀한 탐지를 위해 기호적 시계열 변환과 스펙트럼 클러스터링을 사용한다.


<details>
  <summary>Details</summary>
Motivation: 분산 훈련에서 데이터 프라이버시를 보호하는 것이 중요한 과제이다.

Method: 기호적 시계열 변환(SAX)과 스펙트럼 클러스터링을 활용하여 악의적인 행동을 탐지하고, 강인한 FFT 기반 집계 기능을 최종 단계로 도입한다.

Result: 다섯 가지 중독 공격에 대해 엄격하게 평가하였으며, 기존 방어 기법보다 높은 탐지 정밀도와 모델 정확도를 보였다.

Conclusion: 우리의 접근법은 강력한 적대적 조건에서도 일관된 성능을 유지한다.

Abstract: Federated Learning (FL) has become a powerful technique for training Machine
Learning (ML) models in a decentralized manner, preserving the privacy of the
training datasets involved. However, the decentralized nature of FL limits the
visibility of the training process, relying heavily on the honesty of
participating clients. This assumption opens the door to malicious third
parties, known as Byzantine clients, which can poison the training process by
submitting false model updates. Such malicious clients may engage in poisoning
attacks, manipulating either the dataset or the model parameters to induce
misclassification. In response, this study introduces FLAegis, a two-stage
defensive framework designed to identify Byzantine clients and improve the
robustness of FL systems. Our approach leverages symbolic time series
transformation (SAX) to amplify the differences between benign and malicious
models, and spectral clustering, which enables accurate detection of
adversarial behavior. Furthermore, we incorporate a robust FFT-based
aggregation function as a final layer to mitigate the impact of those Byzantine
clients that manage to evade prior defenses. We rigorously evaluate our method
against five poisoning attacks, ranging from simple label flipping to adaptive
optimization-based strategies. Notably, our approach outperforms
state-of-the-art defenses in both detection precision and final model accuracy,
maintaining consistently high performance even under strong adversarial
conditions.

</details>


### [109] [Stability and Generalization for Bellman Residuals](https://arxiv.org/abs/2508.18741)
*Enoch H. Kang,Kyoungseok Jang*

Main category: cs.LG

TL;DR: 오프라인 강화 학습에서는 고정된 배치로부터 최적 가치 함수나 보상 모델을 복원하는 데 Bellman 일관성을 enforcing하는 데 어려움이 있다. 이 논문에서는 Bellman 잔여 최소화(BRM)를 통해 이를 해결하고, 통계적 행동을 탐구하였으며 O(1/n)의 평균 인수 안정성을 도출하였다.


<details>
  <summary>Details</summary>
Motivation: 현재 오프라인 강화 학습에서는 Bellman 일관성을 enforcing하는 데 어려움이 있으며, Bellman 잔여 최소화가 매력적인 해결책으로 부각되고 있다.

Method: 단일 Lyapunov 잠재함수를 도입하여 이웃 데이터셋에 대한 SGDA 실행을 결합하고 O(1/n)의 평균 인수 안정성 경계를 산출하였다.

Result: 이 분석은 BRM에 대한 O(1/n) 초과 위험 경계를 제공하며, 이는 분산 감소, 추가 정규화 또는 미니배치 샘플링에 대한 독립성 가정이 없이도 가능하다.

Conclusion: 이 결과는 표준 신경망 파라미터화 및 미니배치 SGD에 대해 유효하다.

Abstract: Offline reinforcement learning and offline inverse reinforcement learning aim
to recover near-optimal value functions or reward models from a fixed batch of
logged trajectories, yet current practice still struggles to enforce Bellman
consistency. Bellman residual minimization (BRM) has emerged as an attractive
remedy, as a globally convergent stochastic gradient descent-ascent based
method for BRM has been recently discovered. However, its statistical behavior
in the offline setting remains largely unexplored. In this paper, we close this
statistical gap. Our analysis introduces a single Lyapunov potential that
couples SGDA runs on neighbouring datasets and yields an O(1/n) on-average
argument-stability bound-doubling the best known sample-complexity exponent for
convex-concave saddle problems. The same stability constant translates into the
O(1/n) excess risk bound for BRM, without variance reduction, extra
regularization, or restrictive independence assumptions on minibatch sampling.
The results hold for standard neural-network parameterizations and minibatch
SGD.

</details>


### [110] [Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming](https://arxiv.org/abs/2508.18742)
*Jiajun Li,Ran Hou,Yu Ding,Yixuan Li,Shisi Guan,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang*

Main category: cs.LG

TL;DR: 이 논문은 혼합 정수 선형 프로그래밍(MILP)의 새로운 제약 기반 모델 축소 접근 방식을 제안하며, 기존 방법보다 50% 이상의 솔루션 품질 향상과 17.47%의 계산 시간 단축을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 대규모 MILP 문제를 더 빠르게 해결하기 위해 원래의 MILP 모델보다 간단한 모델을 배우는 모델 축소의 필요성이 있다.

Method: 제약 기반 모델 축소 접근 방식을 사용하여 중요 제약 조건을 식별하고, 다중 모드 표현 기술을 통해 이를 학습한다.

Result: 우리의 방법이 최첨단 방법에 비해 솔루션의 품질을 50% 이상 향상시키고 계산 시간을 17.47% 줄였다.

Conclusion: 제약 기반 모델 축소가 MILP 문제 해결의 효율성을 크게 향상시킬 수 있음을 입증한다.

Abstract: Model reduction, which aims to learn a simpler model of the original mixed
integer linear programming (MILP), can solve large-scale MILP problems much
faster. Most existing model reduction methods are based on variable reduction,
which predicts a solution value for a subset of variables. From a dual
perspective, constraint reduction that transforms a subset of inequality
constraints into equalities can also reduce the complexity of MILP, but has
been largely ignored. Therefore, this paper proposes a novel constraint-based
model reduction approach for the MILP. Constraint-based MILP reduction has two
challenges: 1) which inequality constraints are critical such that reducing
them can accelerate MILP solving while preserving feasibility, and 2) how to
predict these critical constraints efficiently. To identify critical
constraints, we first label these tight-constraints at the optimal solution as
potential critical constraints and design a heuristic rule to select a subset
of critical tight-constraints. To learn the critical tight-constraints, we
propose a multi-modal representation technique that leverages information from
both instance-level and abstract-level MILP formulations. The experimental
results show that, compared to the state-of-the-art methods, our method
improves the quality of the solution by over 50\% and reduces the computation
time by 17.47\%.

</details>


### [111] [UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](https://arxiv.org/abs/2508.18756)
*Zihao Huang,Yu Bao,Qiyang Min,Siyan Chen,Ran Guo,Hongzhi Huang,Defa Zhu,Yutao Zeng,Banggu Wu,Xun Zhou,Siyuan Qiao*

Main category: cs.LG

TL;DR: UltraMemV2는 메모리 접근 비용을 낮추면서도 8전문가 MoE 모델과 동등한 성능을 달성하는 개선된 메모리 레이어 아키텍처를 제시한다.


<details>
  <summary>Details</summary>
Motivation: Mixture of Experts (MoE) 모델은 파라미터의 하위 집합만 활성화하여 효율성을 창출하지만, 추론 시 높은 메모리 접근 비용에 어려움을 겪고 있다.

Method: UltraMemV2는 메모리 레이어를 모든 트랜스포머 블록에 통합하고, 단일 선형 프로젝션을 통해 값 확장을 간소화하며, PEER에서 FFN 기반 값 처리를 채택하고, 원칙적인 파라미터 초기화를 수행하며, 메모리와 FFN 컴퓨테이션 비율을 재조정하는 다섯 가지 주요 개선을 한다.

Result: UltraMemV2는 동일한 계산 및 파라미터 수에서 8전문가 MoE 모델과 성능 동등성을 달성하며, 낮은 메모리 접근 비용을 자랑한다. 메모리 집약적인 작업에서 우수한 성능을 보이며, 긴 컨텍스트 암기에서 +1.6 포인트, 다중 라운드 암기에서 +6.2 포인트, 상황 학습에서 +7.9 포인트의 개선을 보여준다.

Conclusion: UltraMemV2는 120B 총 파라미터 중 2.5B 활성화된 파라미터로 대규모 검증을 수행하며, 활성화 밀도가 전체 희소 파라미터 수보다 성능에 더 큰 영향을 미친다는 것을 입증하였다. 본 연구는 메모리 레이어 아키텍처를 최첨단 MoE 모델과 성능 동등성으로 끌어올리며, 효율적인 희소 계산을 위한 매력적인 대안을 제시한다.

Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by
activating only subsets of parameters, they suffer from high memory access
costs during inference. Memory-layer architectures offer an appealing
alternative with very few memory access, but previous attempts like UltraMem
have only matched the performance of 2-expert MoE models, falling significantly
short of state-of-the-art 8-expert configurations. We present UltraMemV2, a
redesigned memory-layer architecture that closes this performance gap. Our
approach introduces five key improvements: integrating memory layers into every
transformer block, simplifying value expansion with single linear projections,
adopting FFN-based value processing from PEER, implementing principled
parameter initialization, and rebalancing memory-to-FFN computation ratios.
Through extensive evaluation, we demonstrate that UltraMemV2 achieves
performance parity with 8-expert MoE models under same computation and
parameters but significantly low memory access. Notably, UltraMemV2 shows
superior performance on memory-intensive tasks, with improvements of +1.6
points on long-context memorization, +6.2 points on multi-round memorization,
and +7.9 points on in-context learning. We validate our approach at scale with
models up to 2.5B activated parameters from 120B total parameters, and
establish that activation density has greater impact on performance than total
sparse parameter count. Our work brings memory-layer architectures to
performance parity with state-of-the-art MoE models, presenting a compelling
alternative for efficient sparse computation.

</details>


### [112] [Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement](https://arxiv.org/abs/2508.18765)
*Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: AI 시스템의 분산 생태계가 발전하면서 Governance-as-a-Service (GaaS)를 통해 자율 실행 및 다중 에이전트 조정을 위한 새로운 거버넌스 해결책을 제시한다.


<details>
  <summary>Details</summary>
Motivation: AI 시스템의 자율 실행 및 조정의 발전에 따라, 확장 가능한 분리형 거버넌스의 부재가 구조적 위험을 초래한다.

Method: GaaS는 모델 내부를 변경하거나 에이전트의 협력이 필요 없이 에이전트 출력을 규제하는 모듈형 정책 기반 이행 계층이다.

Result: 시뮬레이션을 통해 GaaS가 높은 위험 행동을 차단하거나 전환하면서 처리량을 유지하는 것을 보여주었다.

Conclusion: GaaS는 AI 에이전트 생태계를 위한 인프라 수준의 정렬을 확보하며, 윤리를 가르치는 것이 아니라 시행한다.

Abstract: As AI systems evolve into distributed ecosystems with autonomous execution,
asynchronous reasoning, and multi-agent coordination, the absence of scalable,
decoupled governance poses a structural risk. Existing oversight mechanisms are
reactive, brittle, and embedded within agent architectures, making them
non-auditable and hard to generalize across heterogeneous deployments.
  We introduce Governance-as-a-Service (GaaS): a modular, policy-driven
enforcement layer that regulates agent outputs at runtime without altering
model internals or requiring agent cooperation. GaaS employs declarative rules
and a Trust Factor mechanism that scores agents based on compliance and
severity-weighted violations. It enables coercive, normative, and adaptive
interventions, supporting graduated enforcement and dynamic trust modulation.
  To evaluate GaaS, we conduct three simulation regimes with open-source models
(LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial
decision-making. In the baseline, agents act without governance; in the second,
GaaS enforces policies; in the third, adversarial agents probe robustness. All
actions are intercepted, evaluated, and logged for analysis. Results show that
GaaS reliably blocks or redirects high-risk behaviors while preserving
throughput. Trust scores track rule adherence, isolating and penalizing
untrustworthy components in multi-agent systems.
  By positioning governance as a runtime service akin to compute or storage,
GaaS establishes infrastructure-level alignment for interoperable agent
ecosystems. It does not teach agents ethics; it enforces them.

</details>


### [113] [Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI](https://arxiv.org/abs/2508.18766)
*Hongbo Liu,Siyi Li,Zheng Yu*

Main category: cs.LG

TL;DR: HGNN-DDI는 다양한 약물 관련 데이터 소스를 통합하여 약물-약물 상호작용을 예측하는 이종 그래프 신경망 모델이다.


<details>
  <summary>Details</summary>
Motivation: DDI는 치료 효과 감소나 심각한 부작용을 유발할 수 있어 임상에서 큰 우려 사항이다.

Method: HGNN-DDI는 이종 생물의학 네트워크를 모델링하기 위해 그래프 표현 학습을 활용한다.

Result: HGNN-DDI는 예측 정확도와 강건성 면에서 최신 기준선을 초월하는 성능을 보인다.

Conclusion: HGNN-DDI는 안전한 약물 개발과 정밀 의학을 지원할 잠재력을 강조한다.

Abstract: Drug-drug interactions (DDIs) are a major concern in clinical practice, as
they can lead to reduced therapeutic efficacy or severe adverse effects.
Traditional computational approaches often struggle to capture the complex
relationships among drugs, targets, and biological entities. In this work, we
propose HGNN-DDI, a heterogeneous graph neural network model designed to
predict potential DDIs by integrating multiple drug-related data sources.
HGNN-DDI leverages graph representation learning to model heterogeneous
biomedical networks, enabling effective information propagation across diverse
node and edge types. Experimental results on benchmark DDI datasets demonstrate
that HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy and
robustness, highlighting its potential to support safer drug development and
precision medicine.

</details>


### [114] [Federated Learning with Heterogeneous and Private Label Sets](https://arxiv.org/abs/2508.18774)
*Adam Breitholtz,Edvin Listo Zec,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 이 연구에서는 연합 학습에서의 레이블 세트의 이질성이 모델 성능에 미치는 영향을 조사하고, 공개 및 비공개 레이블 설정 간의 비교를 통해 성능 저하를 해결할 수 있는 방법을 제시합니다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서 이질적인 클라이언트 레이블 세트의 영향은 거의 연구되지 않았습니다. 본 논문은 비공개 레이블 세트를 사용할 때의 성능 문제를 다룹니다.

Method: 모델 성능을 분석하기 위해 구식 방법을 연합 학습에 적용하고, 클라이언트 모델의 중앙 집중 조정을 통해 표현식 정렬을 향상시킵니다.

Result: 실험 결과, 각 클라이언트에 사용할 수 있는 레이블 수가 줄어들면 모든 방법의 성능이 상당히 저하됩니다. 중앙 집중 조정은 성능 개선에 도움이 되지만, 종종 더 높은 변동성을 초래합니다.

Conclusion: 제안된 표준 연합 학습 방법의 조정은 비공식적인 레이블 설정에서도 좋은 성능을 보이며, 모델의 정확성에 큰 손해 없이 클라이언트의 개인정보 보호를 향상시킬 수 있음을 보여줍니다.

Abstract: Although common in real-world applications, heterogeneous client label sets
are rarely investigated in federated learning (FL). Furthermore, in the cases
they are, clients are assumed to be willing to share their entire label sets
with other clients. Federated learning with private label sets, shared only
with the central server, adds further constraints on learning algorithms and
is, in general, a more difficult problem to solve. In this work, we study the
effects of label set heterogeneity on model performance, comparing the public
and private label settings -- when the union of label sets in the federation is
known to clients and when it is not. We apply classical methods for the
classifier combination problem to FL using centralized tuning, adapt common FL
methods to the private label set setting, and discuss the justification of both
approaches under practical assumptions. Our experiments show that reducing the
number of labels available to each client harms the performance of all methods
substantially. Centralized tuning of client models for representational
alignment can help remedy this, but often at the cost of higher variance.
Throughout, our proposed adaptations of standard FL methods perform well,
showing similar performance in the private label setting as the standard
methods achieve in the public setting. This shows that clients can enjoy
increased privacy at little cost to model accuracy.

</details>


### [115] [SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation](https://arxiv.org/abs/2508.18826)
*Junyu Yan,Feng Chen,Yuyang Xue,Yuning Du,Konstantinos Vilouras,Sotirios A. Tsaftaris,Steven McDonagh*

Main category: cs.LG

TL;DR: 모델의 공정성을 향상시키기 위해 더 적은 비용으로 성능을 유지하는 디바이싱 프레임워크인 SWiFT를 제안한다.


<details>
  <summary>Details</summary>
Motivation: ML 모델이 실제 응용에서 편향을 보이는 문제를 해결하기 위해 공정성을 높일 필요가 있다.

Method: SWiFT는 모델 장비성 성능을 유지하면서 공정성을 개선하기 위해 간단한 외부 데이터셋과 몇 번의 에폭으로 모델 파인 튜닝을 수행한다.

Result: SWiFT는 모델의 편향을 일관되게 줄이면서도 경쟁력 있는 진단 정확도를 달성할 수 있다.

Conclusion: SWiFT를 통해 여러 오프 배포 데이터셋에서의 성능 향상이 입증되었다.

Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.

</details>


### [116] [Recycling History: Efficient Recommendations from Contextual Dueling Bandits](https://arxiv.org/abs/2508.18841)
*Suryanarayana Sankagiri,Jalal Etesami,Pouria Fatemi,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 본 연구에서는 사용자 선호도를 보다 정확하게 반영하기 위한 새로운 밴딧 모델을 제안합니다.


<details>
  <summary>Details</summary>
Motivation: 사용자가 아이템을 소비한 후에 제공하는 피드백이 더 신뢰할 수 있기 때문에, 과거 소비 기록에서 아이템을 비교하도록 사용자에게 요청하는 방식을 도입합니다.

Method: 알고리즘은 매 시간 단계마다 한 개의 아이템을 추천하고, 사용자가 해당 아이템을 소비한 후 이전 소비 기록에서 선택된 다른 아이템과 비교하도록 요청합니다. 이때, 비교에 사용될 아이템은 추가적인 후회 없이 선택될 수 있습니다.

Result: 우리는 알고리즘이 충분한 다양성을 가진 역사적 데이터를 통해 유용한 쿼리를 구축할 수 있으며, 초기 랜덤 탐색 단계가 고확률로 풍부한 기록을 쌓기에 충분하다는 것을 증명했습니다.

Conclusion: 시뮬레이션 결과 과거 아이템을 재사용하여 비교할 경우 동시 추천된 아이템 간의 비교보다 후회가 크게 감소하는 것으로 나타났습니다.

Abstract: The contextual duelling bandit problem models adaptive recommender systems,
where the algorithm presents a set of items to the user, and the user's choice
reveals their preference. This setup is well suited for implicit choices users
make when navigating a content platform, but does not capture other possible
comparison queries. Motivated by the fact that users provide more reliable
feedback after consuming items, we propose a new bandit model that can be
described as follows. The algorithm recommends one item per time step; after
consuming that item, the user is asked to compare it with another item chosen
from the user's consumption history. Importantly, in our model, this comparison
item can be chosen without incurring any additional regret, potentially leading
to better performance. However, the regret analysis is challenging because of
the temporal dependency in the user's history. To overcome this challenge, we
first show that the algorithm can construct informative queries provided the
history is rich, i.e., satisfies a certain diversity condition. We then show
that a short initial random exploration phase is sufficient for the algorithm
to accumulate a rich history with high probability. This result, proven via
matrix concentration bounds, yields $O(\sqrt{T})$ regret guarantees.
Additionally, our simulations show that reusing past items for comparisons can
lead to significantly lower regret than only comparing between simultaneously
recommended items.

</details>


### [117] [C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning](https://arxiv.org/abs/2508.18860)
*Wei Li,Hangjie Yuan,Zixiang Zhao,Yifan Zhu,Aojun Lu,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: C-Flat은 연속 학습에서 메모리 유지와 학습 효율성을 개선하기 위해 고안된 평탄한 손실 경관을 촉진하는 방법이다.


<details>
  <summary>Details</summary>
Motivation: 연속 학습에서 새로운 작업에 대한 민감성과 기존 지식 유지의 안정성의 균형이 중요하다.

Method: C-Flat은 연속 학습에 최적화된 평탄한 손실 경관을 촉진하는 방법으로, 기존 코드 파이프라인에 최소한의 수정으로 통합할 수 있는 플러그 앤 플레이 호환성을 제공한다. 또한 C-Flat을 모든 주요 연속 학습 패러다임에 통합할 수 있는 일반적인 프레임워크를 제시하고, 손실 최소화 최적화 방법 및 평탄한 최소화 기반의 연속 학습 방법과 포괄적인 비교를 수행한다.

Result: C-Flat은 다양한 설정에서 성능을 일관되게 개선하는 결과를 보여준다.

Conclusion: C-Flat++는 선택적 평탄함 기반 프로모션을 활용하여 업데이트 비용을 크게 줄이는 효율적이고 효과적인 프레임워크이다.

Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge
is crucial in continual learning (CL). Recently, sharpness-aware minimization
has proven effective in transfer learning and has also been adopted in
continual learning (CL) to improve memory retention and learning efficiency.
However, relying on zeroth-order sharpness alone may favor sharper minima over
flatter ones in certain settings, leading to less robust and potentially
suboptimal solutions. In this paper, we propose \textbf{C}ontinual
\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss
landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling
easy integration with minimal modifications to the code pipeline. Besides, we
present a general framework that integrates C-Flat into all major CL paradigms
and conduct comprehensive comparisons with loss-minima optimizers and
flat-minima-based CL methods. Our results show that C-Flat consistently
improves performance across a wide range of settings. In addition, we introduce
C-Flat++, an efficient yet effective framework that leverages selective
flatness-driven promotion, significantly reducing the update cost required by
C-Flat. Extensive experiments across multiple CL methods, datasets, and
scenarios demonstrate the effectiveness and efficiency of our proposed
approaches. Code is available at https://github.com/WanNaa/C-Flat.

</details>


### [118] [MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes](https://arxiv.org/abs/2508.18873)
*Yunyang Cao,Juekai Lin,Wenhao Li,Bo Jin*

Main category: cs.LG

TL;DR: 이 논문은 TPP에서 다차원 동적 인과성을 발견하기 위한 MOCHA라는 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 실제 이벤트 시퀀스를 모델링하기 위해 TPP에서 복잡한 인과 종속성을 발견하는 것이 중요하다.

Method: MOCHA는 잠재적인 시간 진화 그래프를 기반으로 다차원 영향을 다중 경로 인과 경로로 특징짓고, 학습 가능한 구조적 가중치를 갖는 시간 변화 방향 비순환 그래프(DAG)를 도입한다.

Result: MOCHA는 이벤트 예측에서 최첨단 성능을 달성하고 의미 있는 해석 가능한 인과 구조를 드러낸다.

Conclusion: MOCHA는 인과 발견과 TPP 동역학을 공동 모델링하는 차별화 가능한 프레임워크를 설계한다.

Abstract: Discovering complex causal dependencies in temporal point processes (TPPs) is
critical for modeling real-world event sequences. Existing methods typically
rely on static or first-order causal structures, overlooking the multi-order
and time-varying nature of causal relationships. In this paper, we propose
MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.
MOCHA characterizes multi-order influences as multi-hop causal paths over a
latent time-evolving graph. To model such dynamics, we introduce a time-varying
directed acyclic graph (DAG) with learnable structural weights, where
acyclicity and sparsity constraints are enforced to ensure structural validity.
We design an end-to-end differentiable framework that jointly models causal
discovery and TPP dynamics, enabling accurate event prediction and revealing
interpretable structures. Extensive experiments on real-world datasets
demonstrate that MOCHA not only achieves state-of-the-art performance in event
prediction, but also reveals meaningful and interpretable causal structures.

</details>


### [119] [HAEPO: History-Aggregated Exploratory Policy Optimization](https://arxiv.org/abs/2508.18884)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: HAEPO는 긴 수평 작업에서 탐색을 개선하기 위한 역사 기반 탐색 손실을 도입하여 PPO, GRPO, DPO보다 우수한 성능을 보입니다.


<details>
  <summary>Details</summary>
Motivation: 기존 방법들은 긴 수평 작업에서 탐색을 제한하는 경향이 있습니다.

Method: HAEPO는 각 궤적을 로그 확률의 합으로 압축하고, 궤적에 걸쳐 Plackett-Luce 소프트맥스를 적용하여 정규화된 가중치를 얻습니다.

Result: HAEPO는 빠르게 수렴하고 철저하게 탐색하며, 진정한 보상과 밀접하게 정렬되어 다양한 작업에서 PPO, GRPO 및 DPO와 동등하거나 더 나은 학습 행동을 보여줍니다.

Conclusion: HAEPO는 전체 궤적 역사를 명시적으로 활용하여 탐색과 안정성을 균형 있게 조화시키는 안정적이고 해석 가능한 프레임워크를 제공합니다.

Abstract: Exploration is essential in modern learning, from reinforcement learning
environments with small neural policies to large language models (LLMs).
Existing work, such as DPO, leverages full sequence log-likelihoods to capture
an entire trajectory of the model's decisions, while methods like GRPO
aggregate per-token ratios into a trajectory-level update. However, both often
limit exploration on long-horizon tasks. We introduce History-Aggregated
Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to
combat these shortcomings. HAEPO compresses each trajectory into the sum of its
logarithmic probabilities (a cumulative logarithmic likelihood), and applies a
Plackett-Luce softmax across trajectories to obtain normalized weights
proportional to their returns, thus encouraging broader exploration. We add
entropy regularization to stabilize the aggressive updates to prevent premature
collapse and a soft KL penalty relative to a frozen copy of the previous
(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,
aligns closely with true rewards, and demonstrates robust learning behavior
better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO
provides a stable and interpretable framework by explicitly leveraging
full-trajectory history while balancing exploration and stability.

</details>


### [120] [pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data](https://arxiv.org/abs/2508.18891)
*Zhijin Wang,Senzhen Wu,Yue Hu,Xiufeng Liu*

Main category: cs.LG

TL;DR: pyFAST는 복잡한 시나리오를 지원하는 연구 지향의 파이썬 프레임워크로, 데이터 처리와 모델 계산을 명확히 분리하여 실험의 효율성을 높인다.


<details>
  <summary>Details</summary>
Motivation: 현대의 시계열 분석은 유연하고 효율적이며 확장 가능한 프레임워크를 필요로 하지만, 기존의 파이썬 라이브러리는 모듈성과 불규칙, 다원 출처 또는 희소 데이터에 대한 원활한 지원에 한계를 보인다.

Method: pyFAST는 데이터 처리와 모델 계산을 명확히 분리하여 복잡한 시나리오를 위한 데이터 엔진을 개발하였다. 이는 멀티 소스 로딩, 단백질 서열 처리, 효율적인 서열 및 패치 수준 패딩, 동적 정규화, 마스크 기반 모델링을 지원한다.

Result: pyFAST는 희소 데이터 출처의 비정렬 융합을 위한 LLM 영감을 받은 아키텍처를 통합하고, 기본적인 희소 메트릭, 전문 손실 함수, 유연한 외생 데이터 융합을 제공한다.

Conclusion: MIT 라이센스 하에 GitHub에서 출시된 pyFAST는 시계열 연구 및 응용을 발전시키기 위한 작고 강력한 플랫폼을 제공한다.

Abstract: Modern time series analysis demands frameworks that are flexible, efficient,
and extensible. However, many existing Python libraries exhibit limitations in
modularity and in their native support for irregular, multi-source, or sparse
data. We introduce pyFAST, a research-oriented PyTorch framework that
explicitly decouples data processing from model computation, fostering a
cleaner separation of concerns and facilitating rapid experimentation. Its data
engine is engineered for complex scenarios, supporting multi-source loading,
protein sequence handling, efficient sequence- and patch-level padding, dynamic
normalization, and mask-based modeling for both imputation and forecasting.
pyFAST integrates LLM-inspired architectures for the alignment-free fusion of
sparse data sources and offers native sparse metrics, specialized loss
functions, and flexible exogenous data fusion. Training utilities include
batch-based streaming aggregation for evaluation and device synergy to maximize
computational efficiency. A comprehensive suite of classical and deep learning
models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a
modular architecture that encourages extension. Released under the MIT license
at GitHub, pyFAST provides a compact yet powerful platform for advancing time
series research and applications.

</details>


### [121] [Distance-informed Neural Processes](https://arxiv.org/abs/2508.18903)
*Aishwarya Venkataramanan,Joachim Denzler*

Main category: cs.LG

TL;DR: Distance-informed Neural Process (DNP)는 글로벌 및 거리 인식 지역 잠재 구조를 결합하여 불확실성 추정을 개선하는 새로운 Neural Process의 변형이다.


<details>
  <summary>Details</summary>
Motivation: 표준 Neural Processes는 글로벌 잠재 변수에 의존해 불확실성 조정과 지역 데이터 의존성 캡처에 어려움을 겪는다.

Method: DNP는 작업 수준의 변화를 모델링하기 위해 글로벌 잠재 변수와 입력 유사성을 캡처하기 위해 거리 보존 잠재 공간에서 지역 잠재 변수를 도입한다. 이는 입력 관계의 왜곡을 제한하고 잠재 공간에서 상대 거리의 보존을 장려하는 bi-Lipschitz 정규화를 통해 이루어진다.

Result: DNP는 회귀 및 분류 작업에서 강력한 예측 성능과 개선된 불확실성 보정을 달성한다.

Conclusion: DNP는 잘 보정된 불확실성 추정을 생성하고, 데이터의 분포 내부와 외부를 보다 효과적으로 구별할 수 있게 한다.

Abstract: We propose the Distance-informed Neural Process (DNP), a novel variant of
Neural Processes that improves uncertainty estimation by combining global and
distance-aware local latent structures. Standard Neural Processes (NPs) often
rely on a global latent variable and struggle with uncertainty calibration and
capturing local data dependencies. DNP addresses these limitations by
introducing a global latent variable to model task-level variations and a local
latent variable to capture input similarity within a distance-preserving latent
space. This is achieved through bi-Lipschitz regularization, which bounds
distortions in input relationships and encourages the preservation of relative
distances in the latent space. This modeling approach allows DNP to produce
better-calibrated uncertainty estimates and more effectively distinguish in-
from out-of-distribution data. Empirical results demonstrate that DNP achieves
strong predictive performance and improved uncertainty calibration across
regression and classification tasks.

</details>


### [122] [Enhancing Model Privacy in Federated Learning with Random Masking and Quantization](https://arxiv.org/abs/2508.18911)
*Zhibo Xu,Jianhao Zhu,Jingwen Xu,Changze Lv,Zisu Huang,Xiaohua Wang,Muling Wu,Qi Qian,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.LG

TL;DR: 전통적인 연합 학습의 주된 목표는 데이터 프라이버시를 보호하는 것이며, 이 연구에서는 피어 데이터 및 모델을 보호하는 연합 학습 접근법인 FedQSN을 제안하였다.


<details>
  <summary>Details</summary>
Motivation: 대규모 언어 모델의 출현으로 데이터 프라이버시 및 지적 재산 보호에 대한 우려가 증대되었다.

Method: FedQSN은 모델 매개변수의 서브네트워크를 랜덤 마스킹하고 나머지 매개변수에 대해서는 양자화를 적용한다.

Result: 본 접근법은 연합 학습 환경에서 강한 모델 성능을 유지하며, 기존 방법에 비해 모델 매개변수 보호를 강화한다.

Conclusion: FedQSN은 데이터 및 지적 재산을 동시에 보호할 수 있는 새로운 연합 학습 방법을 제시한다.

Abstract: The primary goal of traditional federated learning is to protect data privacy
by enabling distributed edge devices to collaboratively train a shared global
model while keeping raw data decentralized at local clients. The rise of large
language models (LLMs) has introduced new challenges in distributed systems, as
their substantial computational requirements and the need for specialized
expertise raise critical concerns about protecting intellectual property (IP).
This highlights the need for a federated learning approach that can safeguard
both sensitive data and proprietary models. To tackle this challenge, we
propose FedQSN, a federated learning approach that leverages random masking to
obscure a subnetwork of model parameters and applies quantization to the
remaining parameters. Consequently, the server transmits only a
privacy-preserving proxy of the global model to clients during each
communication round, thus enhancing the model's confidentiality. Experimental
results across various models and tasks demonstrate that our approach not only
maintains strong model performance in federated learning settings but also
achieves enhanced protection of model parameters compared to baseline methods.

</details>


### [123] [Generalization Bound for a General Class of Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.18920)
*Madhusudan Verma,Manoj Kumar*

Main category: cs.LG

TL;DR: 신경 일반 미분 방정식(neural ODEs)은 연속 깊이 아키텍처로 작동하는 인기 있는 딥러닝 모델로, 이 논문은 비선형 동역학 함수에 대한 일반화 오차 경계를 분석한다.


<details>
  <summary>Details</summary>
Motivation: 신경 ODEs의 일반화 성능을 평가하기 위해 일반화 오차 경계를 이해하는 것이 중요하다.

Method: 일반 비선형 함수가 동역학 함수인 신경 ODEs의 넓은 클래스를 분석하고, Lipschitz 연속성 조건을 적용하여 해에 대한 경계를 설정하였다.

Result: Lipschitz 조건 하에서 신경 ODEs의 해는 변동이 제한된 해를 가지며, 시간 의존 및 비의존 사례에 대한 일반화 경계를 확립하였다.

Conclusion: 이 연구는 일반 비선형 동역학을 가진 신경 ODEs에 대한 일반화 경계의 첫 번째 도출이다.

Abstract: Neural ordinary differential equations (neural ODEs) are a popular type of
deep learning model that operate with continuous-depth architectures. To assess
how well such models perform on unseen data, it is crucial to understand their
generalization error bounds. Previous research primarily focused on the linear
case for the dynamics function in neural ODEs - Marion, P. (2023), or provided
bounds for Neural Controlled ODEs that depend on the sampling interval
Bleistein et al. (2023). In this work, we analyze a broader class of neural
ODEs where the dynamics function is a general nonlinear function, either time
dependent or time independent, and is Lipschitz continuous with respect to the
state variables. We showed that under this Lipschitz condition, the solutions
to neural ODEs have solutions with bounded variations. Based on this
observation, we establish generalization bounds for both time-dependent and
time-independent cases and investigate how overparameterization and domain
constraints influence these bounds. To our knowledge, this is the first
derivation of generalization bounds for neural ODEs with general nonlinear
dynamics.

</details>


### [124] [HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling](https://arxiv.org/abs/2508.18922)
*Yao Wu*

Main category: cs.LG

TL;DR: HierCVAE는 복잡한 시스템의 시계열 모델링을 위한 새로운 아키텍처로, 계층적 주의 메커니즘과 조건부 변량 오토인코더를 통합하여 예측 정확도를 15-40% 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 복잡한 시스템에서의 시계열 모델링은 여러 시간 척도에서의 의존성을 포착하고 내재된 불확실성을 관리할 필요가 있습니다.

Method: HierCVAE는 계층적 주의 구조(국소, 글로벌, 교차 시간)를 사용하여 다중 모드 조건 인코딩과 결합하여 시간적, 통계적, 추세 정보를 포착합니다.

Result: 에너지 소비 데이터셋 평가를 통해 HierCVAE는 예측 정확도를 15-40% 향상시키고 불확실성 보정에서도 우수한 성능을 보여줍니다.

Conclusion: HierCVAE는 장기 예측 및 복잡한 다변량 의존성을 해결하는 데 탁월한 성능을 발휘합니다.

Abstract: Temporal modeling in complex systems requires capturing dependencies across
multiple time scales while managing inherent uncertainties. We propose
HierCVAE, a novel architecture that integrates hierarchical attention
mechanisms with conditional variational autoencoders to address these
challenges. HierCVAE employs a three-tier attention structure (local, global,
cross-temporal) combined with multi-modal condition encoding to capture
temporal, statistical, and trend information. The approach incorporates
ResFormer blocks in the latent space and provides explicit uncertainty
quantification via prediction heads. Through evaluations on energy consumption
datasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy and
superior uncertainty calibration compared to state-of-the-art methods,
excelling in long-term forecasting and complex multi-variate dependencies.

</details>


### [125] [Energy-Based Flow Matching for Generating 3D Molecular Structure](https://arxiv.org/abs/2508.18949)
*Wenyin Zhou,Christopher Iliffe Sprague,Vsevolod Viliuga,Matteo Tadiello,Arne Elofsson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 이 논문은 분자의 3D 구조 생성을 위한 새로운 에너지 기반 접근법을 제안하며, 구조 생성 모델의 훈련과 추론을 개선하는 데 초점을 맞추었습니다.


<details>
  <summary>Details</summary>
Motivation: 분자의 구성 요소의 3D 위치를 결정하는 것은 생물학적 응용, 특히 분자 도킹 및 단백질 접힘에 결정적인 과제입니다.

Method: 우리는 흐름 매칭에 초점을 맞추고 에너지 기반 관점을 채택하여 구조 생성 모델의 훈련과 추론을 향상시킵니다. 이를 통해 깊은 네트워크로 표현되는 매핑 함수를 직접 학습합니다.

Result: 이 방법은 단백질 도킹 및 단백질 백본 생성을 포함한 실험에서 일관되게 효과적임을 입증하며, 최근의 흐름 매칭 및 확산 모델을 초월합니다.

Conclusion: 제안된 접근법은 이론적으로 정당화되는 간단한 흐름 매칭 설정을 제공하며, 분자 구조 생성을 위한 실용적이고 효과적인 방법으로 자리 잡고 있습니다.

Abstract: Molecular structure generation is a fundamental problem that involves
determining the 3D positions of molecules' constituents. It has crucial
biological applications, such as molecular docking, protein folding, and
molecular design. Recent advances in generative modeling, such as diffusion
models and flow matching, have made great progress on these tasks by modeling
molecular conformations as a distribution. In this work, we focus on flow
matching and adopt an energy-based perspective to improve training and
inference of structure generation models. Our view results in a mapping
function, represented by a deep network, that is directly learned to
\textit{iteratively} map random configurations, i.e. samples from the source
distribution, to target structures, i.e. points in the data manifold. This
yields a conceptually simple and empirically effective flow matching setup that
is theoretically justified and has interesting connections to fundamental
properties such as idempotency and stability, as well as the empirically useful
techniques such as structure refinement in AlphaFold. Experiments on protein
docking as well as protein backbone generation consistently demonstrate the
method's effectiveness, where it outperforms recent baselines of
task-associated flow matching and diffusion models, using a similar
computational budget.

</details>


### [126] [Estimating Conditional Covariance between labels for Multilabel Data](https://arxiv.org/abs/2508.18951)
*Laurence A. F. Park,Jesse Read*

Main category: cs.LG

TL;DR: 다중 레이블 데이터는 다중 레이블 모델을 적용하기 전에 레이블 의존성을 분석해야 합니다. 이 논문에서는 세 가지 모델을 비교하여 다중 레이블 조건부 레이블 공분산을 추정합니다.


<details>
  <summary>Details</summary>
Motivation: 다중 레이블 모델을 적용하기 전에 레이블 의존성을 분석해야 하므로 이를 위한 방법이 필요하다.

Method: 다중 레이블 조건부 레이블 공분산을 추정하기 위해 다변량 Probit 모델, 다변량 Bernoulli 모델 및 Staged Logit 모델의 세 가지 모델을 비교하였다.

Result: 모든 모델이 조건부 공분산을 측정하였고, 모든 모델이 중립 공분산이 있는 데이터에서 의존 공분산이 존재한다고 잘못 감지하였다. 다중 Probit 모델이 가장 낮은 오류율을 가졌다.

Conclusion: 세 모델 모두 공분산의 강도에 따라 상수 및 의존 공분산을 잘 측정했지만, 상수 공분산이 있는 데이터에서 의존 공분산이 존재하는 것으로 잘못 감지했다.

Abstract: Multilabel data should be analysed for label dependence before applying
multilabel models. Independence between multilabel data labels cannot be
measured directly from the label values due to their dependence on the set of
covariates $\vec{x}$, but can be measured by examining the conditional label
covariance using a multivariate Probit model. Unfortunately, the multivariate
Probit model provides an estimate of its copula covariance, and so might not be
reliable in estimating constant covariance and dependent covariance. In this
article, we compare three models (Multivariate Probit, Multivariate Bernoulli
and Staged Logit) for estimating the constant and dependent multilabel
conditional label covariance. We provide an experiment that allows us to
observe each model's measurement of conditional covariance. We found that all
models measure constant and dependent covariance equally well, depending on the
strength of the covariance, but the models all falsely detect that dependent
covariance is present for data where constant covariance is present. Of the
three models, the Multivariate Probit model had the lowest error rate.

</details>


### [127] [On the Generalisation of Koopman Representations for Chaotic System Control](https://arxiv.org/abs/2508.18954)
*Kyriakos Hjikakou,Juan Diego Cardenas Cartagena,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 이 논문은 혼돈 동역학 시스템을 위한 쿠프만 기반 표현의 일반화 가능성을 조사하며, 예측 및 제어 작업 간의 전이 가능성에 중점을 둡니다.


<details>
  <summary>Details</summary>
Motivation: 이 연구의 동기는 쿠프만 임베딩이 다양한 예측 및 제어 작업에서 얼마나 일반적으로 적용될 수 있는지를 이해하는 것입니다.

Method: 로렌츠 시스템을 실험 대상으로 사용하여, 자가 인코딩을 통한 쿠프만 임베딩 학습, 다음 상태 예측을 위한 트랜스포머의 사전 훈련, 그리고 안전 기준 제어를 위한 세부 조정을 포함하는 3단계 방법론을 제안합니다.

Result: 결과적으로, 쿠프만 임베딩은 표준 및 물리 기반 PCA 기준선을 초과하는 성능을 보였으며, 정확하고 데이터 효율적인 결과를 달성했습니다.

Conclusion: 사전 훈련된 트랜스포머의 가중치를 세부 조정 중 고정해도 성능 저하가 없으며, 이는 학습된 표현이 작업 특정 패턴보다 재사용 가능한 동적 구조를 포착함을 나타냅니다. 이러한 결과는 쿠프만 임베딩의 물리 정보 기계 학습에서 다중 작업 학습의 기초로서의 사용을 지지합니다.

Abstract: This paper investigates the generalisability of Koopman-based representations
for chaotic dynamical systems, focusing on their transferability across
prediction and control tasks. Using the Lorenz system as a testbed, we propose
a three-stage methodology: learning Koopman embeddings through autoencoding,
pre-training a transformer on next-state prediction, and fine-tuning for
safety-critical control. Our results show that Koopman embeddings outperform
both standard and physics-informed PCA baselines, achieving accurate and
data-efficient performance. Notably, fixing the pre-trained transformer weights
during fine-tuning leads to no performance degradation, indicating that the
learned representations capture reusable dynamical structure rather than
task-specific patterns. These findings support the use of Koopman embeddings as
a foundation for multi-task learning in physics-informed machine learning. A
project page is available at https://kikisprdx.github.io/.

</details>


### [128] [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
*Tim Kreuzer,Jelena Zdravkovic,Panagiotis Papapetrou*

Main category: cs.LG

TL;DR: PAX-TS는 시간 시계열 예측 모델의 설명을 가능하게 하는 비모델 의존적인 알고리즘이다.


<details>
  <summary>Details</summary>
Motivation: 현대 예측 모델들은 불투명하며 예측에 대한 설명을 제공하지 않으며, 기존의 설명 기법들은 예측 상황에 적합하지 않다.

Method: PAX-TS는 지역화된 입력 방 perturbations에 기초한 후처리 알고리즘으로, 다중 세분화된 설명을 제공하며 다변량 시간 시계열 예측의 채널 간 상관관계를 특성화할 수 있다.

Result: PAX-TS는 7개의 알고리즘과 10개의 다양한 데이터 세트를 사용한 벤치마크에서 잘 수행되며, 다른 두 개의 최첨단 설명 알고리즘과 비교하여 높은 설명력을 보였다.

Conclusion: PAX-TS는 모델의 행동을 효과적으로 포착하고, 시간 단계 상관 행렬을 기반으로 특정한 패턴을 식별하는 데 유용하다.

Abstract: Time series forecasting has seen considerable improvement during the last
years, with transformer models and large language models driving advancements
of the state of the art. Modern forecasting models are generally opaque and do
not provide explanations for their forecasts, while well-known post-hoc
explainability methods like LIME are not suitable for the forecasting context.
We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time series
forecasting models and their forecasts. Our method is based on localized input
perturbations and results in multi-granular explanations. Further, it is able
to characterize cross-channel correlations for multivariate time series
forecasts. We clearly outline the algorithmic procedure behind PAX-TS,
demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,
compare it with two other state-of-the-art explanation algorithms, and present
the different explanation types of the method. We found that the explanations
of high-performing and low-performing algorithms differ on the same datasets,
highlighting that the explanations of PAX-TS effectively capture a model's
behavior. Based on time step correlation matrices resulting from the benchmark,
we identify 6 classes of patterns that repeatedly occur across different
datasets and algorithms. We found that the patterns are indicators of
performance, with noticeable differences in forecasting error between the
classes. Lastly, we outline a multivariate example where PAX-TS demonstrates
how the forecasting model takes cross-channel correlations into account. With
PAX-TS, time series forecasting models' mechanisms can be illustrated in
different levels of detail, and its explanations can be used to answer
practical questions on forecasts.

</details>


### [129] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 이 논문에서는 이질적인 연합 학습 환경에서 FedProtoKD라는 새로운 방법을 제안하며, 통계적 이질성과 프라이버시 문제를 해결하려고 한다. FedProtoKD는 개선된 이중 지식 증류 메커니즘을 사용하여 모델 성능을 향상시키고, 대조 학습 기반의 학습 가능한 서버 프로토타입을 활용해 프로토타입 축소 문제를 해결한다.


<details>
  <summary>Details</summary>
Motivation: HFL은 다양한 모델과 이질적인 데이터를 수용할 수 있는 능력 덕분에 주목받고 있으며, 프라이버시 문제와 통계적 이질성을 다루기 위한 새로운 접근법이 필요하다.

Method: FedProtoKD를 통해 향상된 이중 지식 증류 메커니즘을 사용하여 클라이언트의 로짓과 프로토타입 특징 표현을 통해 시스템 성능을 향상시킨다. 저희는 클래스별 적응형 프로토타입 마진을 활용하여 대조 학습 기반의 학습 가능한 서버 프로토타입을 사용해 프로토타입 마진 축소 문제를 해결하려고 한다.

Result: FedProtoKD는 다양한 설정에서 평균 1.13%에서 최대 34.13%의 정확도 향상을 달성하였고, 기존의 최첨단 HFL 방법보다 특히 성능이 뛰어나다.

Conclusion: 이 연구는 HFL에서 대조 학습 기반 접근을 통해 더욱 나은 성능을 이끌어내는 방법론의 가능성을 보여준다.

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [130] [STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems](https://arxiv.org/abs/2508.19011)
*Gary Simethy,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.LG

TL;DR: STDiff는 비정상적이고 긴 공백을 포함한 산업 시스템에서 손실 값을 보다 효과적으로 보간하기 위한 새로운 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 기존의 딥러닝 방법은 고정된 시간 창 내에서 패턴을 완성하는 것으로 결측값 보간 작업을 다루지만, 이러한 가정은 산업 시스템에서는 실패할 수 있다.

Method: STDiff는 상태에서 상태로의 시스템 진화를 학습하는 방식으로 보간을 재구성하며, 제어 이론에 맞춰 인과적 편향을 가진 조건부 노이즈 제거 확산 모델을 사용한다.

Result: STDiff는 결측 블록이 시뮬레이션된 공공 하수 처리 데이터셋에서 일관되게 가장 낮은 오류를 달성하였고, 간극이 길어질수록 그 장점이 증가한다.

Conclusion: 이 연구 결과는 동적 인식 및 명시적으로 조건화된 보간 방식이 산업 시계열에 대한 견고한 접근법임을 지지하며, 계산적 트레이드오프와 더 넓은 도메인으로의 확장을 논의한다.

Abstract: Most deep learning methods for imputing missing values treat the task as
completing patterns within a fixed time window. This assumption often fails in
industrial systems, where dynamics are driven by control actions, are highly
non-stationary, and can experience long, uninterrupted gaps. We propose STDiff,
which reframes imputation as learning how the system evolves from one state to
the next. STDiff uses a conditional denoising diffusion model with a causal
bias aligned to control theory, generating missing values step-by-step based on
the most recent known state and relevant control or environmental inputs. On a
public wastewater treatment dataset with simulated missing blocks, STDiff
consistently achieves the lowest errors, with its advantage increasing for
longer gaps. On a raw industrial dataset with substantial real gaps, it
produces trajectories that remain dynamically plausible, in contrast to
window-based models that tend to flatten or over-smooth. These results support
dynamics-aware, explicitly conditioned imputation as a robust approach for
industrial time series, and we discuss computational trade-offs and extensions
to broader domains.

</details>


### [131] [Learning with springs and sticks](https://arxiv.org/abs/2508.19015)
*Luis Mantilla Calderón,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 본 연구에서는 스프링과 스틱으로 구성된 간단한 동적 시스템을 통해 연속 함수를 근사화하는 방법을 제시하고, 이 시스템의 회귀 작업 성능 및 열역학적 특성을 분석한다.


<details>
  <summary>Details</summary>
Motivation: 물리적 관점에서 학습 시스템을 이해하려는 목적이다.

Method: 스틱을 사용해 주어진 함수의 조각선형 근사를 만들고, 스프링의 잠재 에너지를 원하는 평균 제곱 오차 손실 함수를 인코딩하며, 소산을 통해 최소 에너지 구성으로 수렴한다.

Result: 회귀 작업에 적용한 결과, 다층 퍼셉트론과 비슷한 성능을 보였다. 또한, 시스템의 자유 에너지 변화와 데이터 분포 학습 능력 간의 관계를 발견했다.

Conclusion: 주변 환경의 변동으로 인한 열역학적 학습 장벽이 존재하며, 이 모델이 학습 시스템을 물리적으로 이해하는 데 기여할 것으로 기대한다.

Abstract: Learning is a physical process. Here, we aim to study a simple dynamical
system composed of springs and sticks capable of arbitrarily approximating any
continuous function. The main idea of our work is to use the sticks to mimic a
piecewise-linear approximation of the given function, use the potential energy
of springs to encode a desired mean squared error loss function, and converge
to a minimum-energy configuration via dissipation. We apply the proposed
simulation system to regression tasks and show that its performance is
comparable to that of multi-layer perceptrons. In addition, we study the
thermodynamic properties of the system and find a relation between the free
energy change of the system and its ability to learn an underlying data
distribution. We empirically find a \emph{thermodynamic learning barrier} for
the system caused by the fluctuations of the environment, whereby the system
cannot learn if its change in free energy hits such a barrier. We believe this
simple model can help us better understand learning systems from a physical
point of view.

</details>


### [132] [Working My Way Back to You: Resource-Centric Next-Activity Prediction](https://arxiv.org/abs/2508.19016)
*Kelly Kurowski,Xixi Lu,Hajo A Reijers*

Main category: cs.LG

TL;DR: 본 연구는 자원 중심 관점에서 다음 활동 예측을 조사합니다. LightGBM과 Transformer 모델이 2-그램 활동 전환 기반 인코딩에서 가장 우수한 성능을 보였으며, 결합 인코딩은 가장 높은 평균 정확도를 달성했습니다.


<details>
  <summary>Details</summary>
Motivation: 예측 프로세스 모니터링(PPM)은 프로세스 실행에서 다가오는 사건을 예측하는 모델을 훈련시키는 것을 목표로 합니다.

Method: 본 연구는 네 개의 실제 데이터 세트에서 네 개의 예측 모델과 세 가지 인코딩 전략을 평가합니다.

Result: LightGBM 및 Transformer 모델이 2-그램 활동 전환 기반 인코딩에서 최상의 성능을 보였으며, Random Forest는 2-그램 전환과 활동 반복 기능을 결합한 인코딩에서 가장 많은 이점을 보였습니다.

Conclusion: 자원 중심 접근 방식은 개별 행동을 분석함으로써 더 스마트한 자원 할당, 전략적 인력 계획 및 개인화된 직원 지원을 가능하게 할 수 있습니다.

Abstract: Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.

</details>


### [133] [Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence](https://arxiv.org/abs/2508.19019)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 본 논문에서는 APT 탐지를 위한 능동적 학습 기반 이상 탐지 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: APT의 은밀한 특성과 탐지 데이터셋의 극단적인 클래스 불균형으로 인해 사이버 방어에서 심각한 도전에 직면해 있다.

Method: Attention-Based Autoencoder를 기반으로 한 접근 방식으로 특징 공간 유사성을 활용하여 정상 유사 및 이상 유사 인스턴스를 식별한다.

Result: 유사성 측정의 선정이 모델 수렴, 이상 탐지 정확도 및 레이블 효율성에 상당한 영향을 미친다는 것을 입증하였다.

Conclusion: 능동적 학습 파이프라인에서의 유사성 함수 선택에 대한 실용적인 통찰을 제공한다.

Abstract: Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense
due to their stealthy behavior and the extreme class imbalance inherent in
detection datasets. To address these issues, we propose a novel active
learning-based anomaly detection framework that leverages similarity search to
iteratively refine the decision space. Built upon an Attention-Based
Autoencoder, our approach uses feature-space similarity to identify normal-like
and anomaly-like instances, thereby enhancing model robustness with minimal
oracle supervision. Crucially, we perform a formal evaluation of various
similarity measures to understand their influence on sample selection and
anomaly ranking effectiveness. Through experiments on diverse datasets,
including DARPA Transparent Computing APT traces, we demonstrate that the
choice of similarity metric significantly impacts model convergence, anomaly
detection accuracy, and label efficiency. Our results offer actionable insights
for selecting similarity functions in active learning pipelines tailored for
threat intelligence and cyber defense.

</details>


### [134] [GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling](https://arxiv.org/abs/2508.19028)
*Arash Jamshidi,Lauri Seppäläinen,Katsiaryna Haitsiukevich,Hoang Phuc Hau Luu,Anton Björklund,Kai Puolamäki*

Main category: cs.LG

TL;DR: GRADSTOP은 경량화된 조기 중지 방법으로, 데이터 부족 상황에서 모델의 성능을 향상시킵니다.


<details>
  <summary>Details</summary>
Motivation: 기존 조기 중지 방법은 데이터의 일부를 검증 세트로 분리함으로써 훈련 데이터가 줄어드는 문제를 해결하고자 했습니다.

Method: GRADSTOP은 기울기 정보를 활용하여 베이esian 후행 분포를 추정하고, 이 분포에서 샘플링하여 조기 중지 기준을 설정합니다.

Result: GRADSTOP은 테스트 데이터에서 낮은 손실을 달성하고 검증 세트를 기반으로 한 조기 중지 기준과 비교해 유리한 결과를 나타냅니다.

Conclusion: 이 방법은 특히 데이터가 제한된 환경에서 유리하며, 기계 학습 라이브러리에 쉽게 통합할 수 있습니다.

Abstract: Machine learning models are often learned by minimising a loss function on
the training data using a gradient descent algorithm. These models often suffer
from overfitting, leading to a decline in predictive performance on unseen
data. A standard solution is early stopping using a hold-out validation set,
which halts the minimisation when the validation loss stops decreasing.
However, this hold-out set reduces the data available for training. This paper
presents GRADSTOP, a novel stochastic early stopping method that only uses
information in the gradients, which are produced by the gradient descent
algorithm ``for free.'' Our main contributions are that we estimate the
Bayesian posterior by the gradient information, define the early stopping
problem as drawing sample from this posterior, and use the approximated
posterior to obtain a stopping criterion. Our empirical evaluation shows that
GRADSTOP achieves a small loss on test data and compares favourably to a
validation-set-based stopping criterion. By leveraging the entire dataset for
training, our method is particularly advantageous in data-limited settings,
such as transfer learning. It can be incorporated as an optional feature in
gradient descent libraries with only a small computational overhead. The source
code is available at https://github.com/edahelsinki/gradstop.

</details>


### [135] [When recalling in-context, Transformers are not SSMs](https://arxiv.org/abs/2508.19029)
*Destiny Okpekpe,Antonio Orvieto*

Main category: cs.LG

TL;DR: 본 논문은 현대 순환 딥러닝 모델이 트랜스포머에 비해 추론 및 기억 작업에서의 단점을 밝히며, 연관 기억(AR) 벤치마크를 통해 학습률 선택의 중요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 현대 순환 딥러닝 모델의 성능과 관련된 최적화 문제 및 스케일링의 영향을 조사하기 위해.

Method: 최근 제안된 토큰 혼합 전략의 스케일링 및 최적화 문제를 세밀하게 분석하고, 1층 트랜스포머의 훈련 동역학을 관찰한다.

Result: 현대 순환 모델의 성능에 학습률이 결정적인 역할을 하며, 주목 기반 모델이 너비 확장 시와 깊이 확장 시 상이한 장점을 가진다는 것을 보여준다.

Conclusion: 구조적 제거를 통해 Transformer와 Mamba의 성능 및 최적화 안정성에 대한 영향을 연구하였다.

Abstract: Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba's performance
and optimization stability.

</details>


### [136] [Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data](https://arxiv.org/abs/2508.19031)
*Vemula Sreenath,Filippo Gatti,Pierre Jehel*

Main category: cs.LG

TL;DR: 이 논문은 지진 사전 예방을 위한 투명한 머신러닝 모델을 개발하여 고위험 결정을 지원한다.


<details>
  <summary>Details</summary>
Motivation: 전통적인 머신러닝 접근이 불투명하고 데이터 불균형 문제로 인해 지진 예측에 대한 신뢰성이 떨어진다.

Method: HazBinLoss 함수를 사용하여 투명한 머신러닝 아키텍처를 개발하고 입력을 개별 처리 후 선형적으로 추가하여 출력을 얻는다.

Result: 모델은 기존 GMM들과 유사한 성능을 나타내면서 투명성을 유지한다.

Conclusion: 이 프레임워크는 ML 기반 접근 방식의 광범위한 채택을 가능하게 하여 위험 평가와 재난 계획에 활용될 수 있다.

Abstract: Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.

</details>


### [137] [Automated discovery of finite volume schemes using Graph Neural Networks](https://arxiv.org/abs/2508.19052)
*Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: 그래프 신경망(GNN)은 물리 시스템의 해를 근사하는 데 강력한 능력을 보여주며 수치 시뮬레이션의 경관을 크게 변화시켰지만, 훈련 범위를 넘어서는 외삽 능력은 불확실하다. 이 연구에서는 GNN이 전통적인 역할을 넘어 다양한 수치 방정식을 생성하는 데 활용될 수 있음을 보여준다.


<details>
  <summary>Details</summary>
Motivation: GNN의 훈련 범위를 넘어서는 외삽 능력이 불확실한 상황에서 GNN을 수치 방식 생성에 활용하는 방법을 탐구한다.

Method: GNN을 사용하여 두 개의 노드 그래프만 포함된 데이터셋에서 훈련하고, 이를 통해 열 방정식에 대한 일차 유한 부피(FV) 방식을 비구조적 메쉬에서 외삽하는 방법을 수치적 및 이론적으로 검증한다. 또한 비지도 학습 맥락에서 GNN이 물리 정보 신경망(PINNs) 같은 잔여 손실만을 사용하여 최초의 FV 방식을 회복할 수 있음을 보여준다.

Result: GNN이 특정 데이터셋에서 손실 $	extit{ε}$을 달성할 경우, 이는 $	extit{O(ε)}$ 오차로 FV 방식을 구현하고, 상징 회귀를 사용하여 표준 일차 FV 방식의 정확한 해석적 공식을 효과적으로 재발견한다. 또한, 2-hop과 2-layers GNN을 훈련 하여 각각의 방식을 자율적으로 발견하게 된다.

Conclusion: GNN은 단순한 강력한 근사기를 넘어 새로운 수치 방법 개발에 적극적으로 기여할 수 있는 가능성을 제시한다.

Abstract: Graph Neural Networks (GNNs) have deeply modified the landscape of numerical
simulations by demonstrating strong capabilities in approximating solutions of
physical systems. However, their ability to extrapolate beyond their training
domain (\textit{e.g.} larger or structurally different graphs) remains
uncertain. In this work, we establish that GNNs can serve purposes beyond their
traditional role, and be exploited to generate numerical schemes, in
conjunction with symbolic regression. First, we show numerically and
theoretically that a GNN trained on a dataset consisting solely of two-node
graphs can extrapolate a first-order Finite Volume (FV) scheme for the heat
equation on out-of-distribution, unstructured meshes. Specifically, if a GNN
achieves a loss $\varepsilon$ on such a dataset, it implements the FV scheme
with an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we show
that the network effectively rediscovers the exact analytical formulation of
the standard first-order FV scheme. We then extend this approach to an
unsupervised context: the GNN recovers the first-order FV scheme using only a
residual loss similar to Physics-Informed Neural Networks (PINNs) with no
access to ground-truth data. Finally, we push the methodology further by
considering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNN
using the same PINN loss, that autonomously discover (i) a second-order
correction term to the initial scheme using a 2-hop stencil, and (ii) the
classic second-order midpoint scheme. These findings follows a recent paradigm
in scientific computing: GNNs are not only strong approximators, but can be
active contributors to the development of novel numerical methods.

</details>


### [138] [Tackling Federated Unlearning as a Parameter Estimation Problem](https://arxiv.org/abs/2508.19065)
*Antonio Balordi,Lorenzo Manini,Fabio Stella,Alessio Merlo*

Main category: cs.LG

TL;DR: 본 논문은 연합 학습에서 적절한 데이터 삭제 방법을 제안하며, 이를 통해 데이터 프라이버시를 보호하려는 목표를 갖고 있다.


<details>
  <summary>Details</summary>
Motivation: 연합 학습에서는 데이터가 클라이언트에 남아 있어 완전한 재훈련이나 협조적 업데이트가 비현실적인 경우가 많아 프라이버시 규정에 따라 데이터 지우기가 필요한 큰 도전이 있다.

Method: 정보 이론에 기반한 효율적인 연합 학습 비우기 프레임워크를 도입하고, 데이터 삭제에 민감한 매개변수만을 선택적으로 재설정한 후 최소한의 연합 재훈련을 수행한다.

Result: 평가된 기준 데이터셋에서 프라이버시 보장은 강력하고(MIA 성공률이 랜덤에 가까움) 재훈련 기준의 정확도 약 0.9를 기록하여 높은 성능을 보였다.

Conclusion: 이 프레임워크는 연합 학습에서 데이터 삭제를 위한 실용적인 솔루션을 제공하며, 공격 시나리오에서도 모델의 무결성을 복원한다.

Abstract: Privacy regulations require the erasure of data from deep learning models.
This is a significant challenge that is amplified in Federated Learning, where
data remains on clients, making full retraining or coordinated updates often
infeasible. This work introduces an efficient Federated Unlearning framework
based on information theory, modeling leakage as a parameter estimation
problem. Our method uses second-order Hessian information to identify and
selectively reset only the parameters most sensitive to the data being
forgotten, followed by minimal federated retraining. This model-agnostic
approach supports categorical and client unlearning without requiring server
access to raw client data after initial information aggregation. Evaluations on
benchmark datasets demonstrate strong privacy (MIA success near random,
categorical knowledge erased) and high performance (Normalized Accuracy against
re-trained benchmarks of $\approx$ 0.9), while aiming for increased efficiency
over complete retraining. Furthermore, in a targeted backdoor attack scenario,
our framework effectively neutralizes the malicious trigger, restoring model
integrity. This offers a practical solution for data forgetting in FL.

</details>


### [139] [Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks](https://arxiv.org/abs/2508.19071)
*Hugo Attali,Thomas Papastergiou,Nathalie Pernelle,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: TRIGON은 여러 그래프 뷰에서 관련 삼각형을 선택하여 비평면 삼각형 격자를 구축하는 새로운 프레임워크로, 그래프의 구조적 특성을 개선하여 노드 분류 작업에서 기존 방법보다 우수한 성능을 보여준다.


<details>
  <summary>Details</summary>
Motivation: 그래프 신경망(GNN)은 그래프 구조 데이터 학습의 주요 패러다임으로 부상했지만, 그래프 토폴로지에서 유래된 문제들, 특히 과중합과 과평탄화로 인해 성능이 제한되고 있다.

Method: TRIGON은 여러 그래프 뷰에서 관련 삼각형을 선택하는 방법을 학습하여 풍부한 비평면 삼각형 격자를 구축한다.

Result: 우리의 방법은 기존의 재배선 방법과 비교하여 지름 감소, 스펙트럼 갭 증가, 유효 저항 감소와 같은 구조적 속성이 현저히 개선된 재배선된 그래프를 생성한다.

Conclusion: 경험적 결과는 TRIGON이 동질적 및 이질적 벤치마크에서 노드 분류 작업에 대해 최첨단 접근 방식을 능가함을 보여준다.

Abstract: Graph Neural Networks (GNNs) have emerged as the leading paradigm for
learning over graph-structured data. However, their performance is limited by
issues inherent to graph topology, most notably oversquashing and
oversmoothing. Recent advances in graph rewiring aim to mitigate these
limitations by modifying the graph topology to promote more effective
information propagation. In this work, we introduce TRIGON, a novel framework
that constructs enriched, non-planar triangulations by learning to select
relevant triangles from multiple graph views. By jointly optimizing triangle
selection and downstream classification performance, our method produces a
rewired graph with markedly improved structural properties such as reduced
diameter, increased spectral gap, and lower effective resistance compared to
existing rewiring methods. Empirical results demonstrate that TRIGON
outperforms state-of-the-art approaches on node classification tasks across a
range of homophilic and heterophilic benchmarks.

</details>


### [140] [APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration](https://arxiv.org/abs/2508.19087)
*Shaobo Ma,Chao Fang,Haikuo Shao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: APT-LLM은 초저비트 양자화 언어 모델의 효율성을 극대화하기 위해 새로운 데이터 형식과 행렬 곱셈 방법을 도입하고, 메모리 관리 시스템과 동적 커널 매핑 방법을 개발하여 성능을 극대화한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델은 AI 응용 프로그램에 혁신을 가져왔으나, 높은 계산 요구 사항으로 인해 배포와 실시간 성능이 제한된다.

Method: APT-LLM은 새로운 데이터 형식인 bipolar-INT와 행렬 수준에서 비트를 분해하고 재조립하는 행렬 곱셈 방법을 사용하여 자율 정밀도를 제공하고, 메모리 관리 시스템과 동적 커널 매핑 방법을 도입한다.

Result: APT-LLM은 LLM 추론에서 FP16 기준 대비 최대 3.99배, RTX 4090 및 H800에서 FP16 대비 최대 2.44배의 속도 향상을 달성한다.

Conclusion: APT-LLM은 다양한 LLM 아키텍처와 정밀도 설정에 걸쳐 최적의 성능을 제공하는 혁신적인 솔루션이다.

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
enormous computational demands severely limit deployment and real-time
performance. Quantization methods can help reduce computational costs, however,
attaining the extreme efficiency associated with ultra-low-bit quantized LLMs
at arbitrary precision presents challenges on GPUs. This is primarily due to
the limited support for GPU Tensor Cores, inefficient memory management, and
inflexible kernel optimizations. To tackle these challenges, we propose a
comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.
Firstly, we introduce a novel data format, bipolar-INT, which allows for
efficient and lossless conversion with signed INT, while also being more
conducive to parallel computation. We also develop a matrix multiplication
(MatMul) method allowing for arbitrary precision by dismantling and
reassembling matrices at the bit level. This method provides flexible precision
and optimizes the utilization of GPU Tensor Cores. In addition, we propose a
memory management system focused on data recovery, which strategically employs
fast shared memory to substantially increase kernel execution speed and reduce
memory access latency. Finally, we develop a kernel mapping method that
dynamically selects the optimal configurable hyperparameters of kernels for
varying matrix sizes, enabling optimal performance across different LLM
architectures and precision settings. In LLM inference, APT-LLM achieves up to
a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup
over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,
APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup
over CUTLASS integer baselines.

</details>


### [141] [Composition and Alignment of Diffusion Models using Constrained Learning](https://arxiv.org/abs/2508.19104)
*Shervin Khalafi,Ignacio Hounie,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 본 연구는 확산 모델의 정렬 및 조합을 통합하는 제약 최적화 프레임워크를 제안하여 여러 보상 조건을 만족하도록 합니다.


<details>
  <summary>Details</summary>
Motivation: 생성 샘플의 품질과 사용자 요구 사항에 대한 적합성을 개선하기 위해 여러 방법들이 존재하지만, 최적화 과정에서 상충하는 특성으로 인해 어려움이 발생합니다.

Method: 정렬 모델이 보상 제약을 만족하고, 여러 사전 훈련된 모델에 가깝도록 하는 제약 최적화 프레임워크를 제안합니다.

Result: 제안된 방법이 이미지 생성에 효과적이며, 정렬 모델 혹은 조합 모델이 제약을 효과적으로 만족하고, 동등 가중치 접근 방식보다 개선된 성과를 보입니다.

Conclusion: 이 연구는 제약 최적화 기법을 통해 확산 모델의 성능을 높이고 다양한 요구를 만족하는 접근 방식을 제공합니다.

Abstract: Diffusion models have become prevalent in generative modeling due to their
ability to sample from complex distributions. To improve the quality of
generated samples and their compliance with user requirements, two commonly
used methods are: (i) Alignment, which involves fine-tuning a diffusion model
to align it with a reward; and (ii) Composition, which combines several
pre-trained diffusion models, each emphasizing a desirable attribute in the
generated outputs. However, trade-offs often arise when optimizing for multiple
rewards or combining multiple models, as they can often represent competing
properties. Existing methods cannot guarantee that the resulting model
faithfully generates samples with all the desired properties. To address this
gap, we propose a constrained optimization framework that unifies alignment and
composition of diffusion models by enforcing that the aligned model satisfies
reward constraints and/or remains close to (potentially multiple) pre-trained
models. We provide a theoretical characterization of the solutions to the
constrained alignment and composition problems and develop a Lagrangian-based
primal-dual training algorithm to approximate these solutions. Empirically, we
demonstrate the effectiveness and merits of our proposed approach in image
generation, applying it to alignment and composition, and show that our aligned
or composed model satisfies constraints effectively, and improves on the
equally-weighted approach. Our implementation can be found at
https://github.com/shervinkhalafi/constrained_comp_align.

</details>


### [142] [Active Query Selection for Crowd-Based Reinforcement Learning](https://arxiv.org/abs/2508.19132)
*Jonathan Erskine,Taku Yamagata,Raúl Santos-Rodríguez*

Main category: cs.LG

TL;DR: 본 논문은 낮은 품질의 인간 입력 문제를 해결하기 위한 새로운 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 보상 신호를 명확하게 정의하기 어렵거나 인간의 의도와 맞지 않는 환경에서 에이전트를 훈련시키기 위해 선호 기반 강화 학습이 각광받고 있지만, 신뢰할 수 있는 인간 입력의 높은 비용과 낮은 가용성이 효과성을 제한하고 있다.

Method: 확률적 군중 모델링과 능동 학습을 결합한 프레임워크를 제안하며, Advise 알고리즘을 확장하여 다수의 트레이너를 지원하고, 신뢰성을 온라인으로 추정하고, 엔트로피 기반 쿼리 선택을 통합하여 피드백 요청을 안내한다.

Result: 혼합된 환경에서 에이전트를 평가한 결과, 불확실한 궤적에 대한 피드백으로 훈련된 에이전트가 대다수의 작업에서 더 빠른 학습을 보였으며, 당뇨병 조절 작업에서는 성능이 기존 방법을 초과하였다.

Conclusion: 이 방법은 인간 입력의 불확실성을 효과적으로 처리하여 에이전트 훈련을 개선할 수 있는 잠재력을 보여준다.

Abstract: Preference-based reinforcement learning has gained prominence as a strategy
for training agents in environments where the reward signal is difficult to
specify or misaligned with human intent. However, its effectiveness is often
limited by the high cost and low availability of reliable human input,
especially in domains where expert feedback is scarce or errors are costly. To
address this, we propose a novel framework that combines two complementary
strategies: probabilistic crowd modelling to handle noisy, multi-annotator
feedback, and active learning to prioritize feedback on the most informative
agent actions. We extend the Advise algorithm to support multiple trainers,
estimate their reliability online, and incorporate entropy-based query
selection to guide feedback requests. We evaluate our approach in a set of
environments that span both synthetic and real-world-inspired settings,
including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control task
for Type 1 Diabetes using the clinically approved UVA/Padova simulator. Our
preliminary results demonstrate that agents trained with feedback on uncertain
trajectories exhibit faster learning in most tasks, and we outperform the
baselines for the blood glucose control task.

</details>


### [143] [Saddle Hierarchy in Dense Associative Memory](https://arxiv.org/abs/2508.19151)
*Robin Thériault,Daniele Tantari*

Main category: cs.LG

TL;DR: 이 논문에서는 새로운 정규화 기법을 제안하여 밀집 연상 기억(DAM) 모델의 훈련을 더 안정적으로 만들고, 이러한 모델이 해석 가능한 솔루션을 학습한다는 것을 보인다.


<details>
  <summary>Details</summary>
Motivation: 밀집 연상 기억 모델이 공적 예제에 강인하며 최첨단 기계 학습 패러다임과 밀접하게 관련되어 있다는 점에서 재조명받고 있다.

Method: 포츠 숨겨진 유닛을 가진 3층 볼츠만 기계를 기반으로 한 DAM을 연구하고, 통계 역학 분석을 통해 관련된 세들포인트 방정식을 도출한다.

Result: 제안된 정규화 기법 덕분에 훈련 안정성이 상당히 향상되었으며, DAM이 감독 및 비감독 분류 문제 모두에서 해석 가능한 솔루션을 학습한다는 것을 경험적으로 보여준다.

Conclusion: 상대적으로 작은 DAM이 학습한 가중치는 더 큰 DAM에서 불안정한 세들포인트에 해당한다는 것을 발견하였다.

Abstract: Dense associative memory (DAM) models have been attracting renewed attention
since they were shown to be robust to adversarial examples and closely related
to state-of-the-art machine learning paradigms, such as the attention
mechanisms in transformers and generative diffusion models. We study a DAM
built upon a three-layer Boltzmann machine with Potts hidden units, which
represent data clusters and classes. Through a statistical mechanics analysis,
we derive saddle-point equations that characterize both the stationary points
of DAMs trained on real data and the fixed points of DAMs trained on synthetic
data within a teacher-student framework. Based on these results, we propose a
novel regularization scheme that makes training significantly more stable.
Moreover, we show empirically that our DAM learns interpretable solutions to
both supervised and unsupervised classification problems. Pushing our
theoretical analysis further, we find that the weights learned by relatively
small DAMs correspond to unstable saddle points in larger DAMs. We implement a
network-growing algorithm that leverages this saddle-point hierarchy to
drastically reduce the computational cost of training dense associative memory.

</details>


### [144] [Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness](https://arxiv.org/abs/2508.19183)
*Wenchuan Mu,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 안전-critical 딥러닝 애플리케이션에서의 강건성을 평가할 수 있는 새로운 방법인 타워 강건성을 제안하며, 기존 방법의 한계를 극복하는 포괄적인 비교 분석을 수행하였습니다.


<details>
  <summary>Details</summary>
Motivation: 안전-critical 딥러닝 애플리케이션에서 입력 데이터의 미세한 변화에 대한 신경 모델의 강건성을 평가하는 것이 필요합니다.

Method: 타워 강건성이란, 가설 검정을 기반으로 하는 새로운 실용적 지표를 제안하여 확률적 강건성을 정량적으로 평가합니다.

Result: 포괄적인 비교 평가를 통해 제안한 접근 방식의 장점과 적용 가능성을 보여주었습니다.

Conclusion: 이 연구는 안전-critical 딥러닝 애플리케이션에서 모델 강건성을 체계적으로 이해하고 향상시키는 데 기여합니다.

Abstract: In safety-critical deep learning applications, robustness measures the
ability of neural models that handle imperceptible perturbations in input data,
which may lead to potential safety hazards. Existing pre-deployment robustness
assessment methods typically suffer from significant trade-offs between
computational cost and measurement precision, limiting their practical utility.
To address these limitations, this paper conducts a comprehensive comparative
analysis of existing robustness definitions and associated assessment
methodologies. We propose tower robustness to evaluate robustness, which is a
novel, practical metric based on hypothesis testing to quantitatively evaluate
probabilistic robustness, enabling more rigorous and efficient pre-deployment
assessments. Our extensive comparative evaluation illustrates the advantages
and applicability of our proposed approach, thereby advancing the systematic
understanding and enhancement of model robustness in safety-critical deep
learning applications.

</details>


### [145] [Emotions as Ambiguity-aware Ordinal Representations](https://arxiv.org/abs/2508.19193)
*Jingyao Wu,Matthew Barthet,David Melhart,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 이 논문은 감정 인식의 모호성을 고려한 새로운 방법론을 제안하며, 감정의 시간적 변화를 모델링하여 기존 접속 모델을 능가하는 성과를 보였다.


<details>
  <summary>Details</summary>
Motivation: 기존의 감정 인식 방법이 감정의 모호성을 무시하거나 정적 변수로 처리하는 빈틈을 제시하고 이를 해결하고자 하였다.

Method: 모호성을 변화율을 통해 모델링하는 순서적 감정 표현 방식을 제안하였다.

Result: RECOLA와 GameVibe 두 개의 감정 코퍼스를 통해 평가한 결과, 순서적 표현이 무한 라벨에서 기존 모델보다 높은 성과를 보였다.

Conclusion: 정적 라벨에서는 SDA에서 우수한 성능을 보이며 감정의 상대적 변화를 잘 포착함을 입증하였다.

Abstract: Emotions are inherently ambiguous and dynamic phenomena, yet existing
continuous emotion recognition approaches either ignore their ambiguity or
treat ambiguity as an independent and static variable over time. Motivated by
this gap in the literature, in this paper we introduce ambiguity-aware ordinal
emotion representations, a novel framework that captures both the ambiguity
present in emotion annotation and the inherent temporal dynamics of emotional
traces. Specifically, we propose approaches that model emotion ambiguity
through its rate of change. We evaluate our framework on two affective corpora
-- RECOLA and GameVibe -- testing our proposed approaches on both bounded
(arousal, valence) and unbounded (engagement) continuous traces. Our results
demonstrate that ordinal representations outperform conventional
ambiguity-aware models on unbounded labels, achieving the highest Concordance
Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,
highlighting their effectiveness in modeling the traces' dynamics. For bounded
traces, ordinal representations excel in SDA, revealing their superior ability
to capture relative changes of annotated emotion traces.

</details>


### [146] [Understanding Tool-Integrated Reasoning](https://arxiv.org/abs/2508.19201)
*Heng Lin,Zhongwen Xu*

Main category: cs.LG

TL;DR: Tool-Integrated Reasoning (TIR)이 대형 언어 모델(LLM)의 능력을 어떻게 증가시키는지를 연구하며, TIR이 모델의 능력을 기본적으로 확장한다는 첫 번째 형식적 증명을 제공한다.


<details>
  <summary>Details</summary>
Motivation: 대형 언어 모델과 도구 통합의 효과를 설명하는 이론이 부족하기 때문에 이를 보완하고자 한다.

Method: 도구가 모델의 경험적 지원과 실현 가능한 지원을 엄격히 확장시키는 방식으로 TIR을 활용하며, Advantage Shaping Policy Optimization (ASPO) 알고리즘을 소개하여 정책 행동을 안내한다.

Result: TIR 모델이 순수 텍스트 모델보다 pass@k 메트릭에서 눈에 띄는 성과를 거두었으며, 계산 집약적인 문제뿐만 아니라 추상적 통찰이 필요한 문제에도 그 우위를 보여준다.

Conclusion: TIR의 성공에 대한 처음의 원리적 설명을 제공하여 도구가 어떻게 더 강력한 추론을 가능하게 하는지에 대한 이해를 심화시킨다.

Abstract: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.

</details>


### [147] [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: Token Order Prediction (TOP)는 언어 모델 훈련에서 다음 토큰 예측을 개선하기 위한 보조 목표로 Multi-Token Prediction (MTP) 대신 제안되었으며, 전체적으로 NTP와 MTP보다 향상된 성능을 보인다.


<details>
  <summary>Details</summary>
Motivation: MTP는 언어 모델 훈련에서 다음 토큰 예측을 개선하기 위한 보조 목적이지만, 표준 NLP 벤치마크에서는 일관되지 않은 성과를 보인다.

Method: TOP는 모델이 다가오는 토큰의 순서를 그 근접성에 따라 정렬하도록 훈련하며, 이를 위해 순위 학습 손실을 사용한다. MTP의 여러 변환기 레이어 대신 단일 추가 비주얼 임베딩 레이어만 필요하다.

Result: 8개의 표준 NLP 벤치마크에서의 결과는 TOP가 규모와 관계 없이 NTP와 MTP를 모두 초월하는 성능을 보여준다.

Conclusion: 코드는 https://github.com/zaydzuhri/token-order-prediction에서 확인할 수 있다.

Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to
improve next-token prediction (NTP) in language model training but shows
inconsistent improvements, underperforming in standard NLP benchmarks. We argue
that MTP's exact future token prediction is too difficult as an auxiliary loss.
Instead, we propose Token Order Prediction (TOP), which trains models to order
upcoming tokens by their proximity using a learning-to-rank loss. TOP requires
only a single additional unembedding layer compared to MTP's multiple
transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using
NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show
that TOP overall outperforms both NTP and MTP even at scale. Our code is
available at https://github.com/zaydzuhri/token-order-prediction

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [148] [Consensus Is All You Need: Gossip-Based Reasoning Among Large Language Models](https://arxiv.org/abs/2508.18292)
*Saksham Arora*

Main category: cs.MA

TL;DR: 본 연구는 여러 대형 언어 모델을 결합하여 협력적이고 신뢰할 수 있는 AI 추론을 달성하는 방법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: 단일 모델이 모든 영역에서 뛰어나지 않기 때문에 여러 모델의 강점을 결합하고자 한다.

Method: 모델들은 P2P 네트워크의 노드로 작용하며, 답변과 사고 과정을 공유하여 공동의 결론에 도달하도록 한다.

Result: 이 '가쉽 기반 합의' 방식은 다중 에이전트 AI 추론에서 강력하고 견고하며 정확성을 보여준다.

Conclusion: 이 접근법은 인간이 합의를 형성하는 방식과 유사하여 AI의 협력성 및 신뢰성을 높인다.

Abstract: Large language models have advanced rapidly, but no single model excels in
every area -- each has its strengths and weaknesses. Instead of relying on one
model alone, we take inspiration from gossip protocols in distributed systems,
where information is exchanged with peers until they all come to an agreement.
In this setup, models exchange answers and gradually work toward a shared
solution. Each LLM acts as a node in a peer-to-peer network, sharing responses
and thought processes to reach a collective decision. Our results show that
this "gossip-based consensus" leads to robust, resilient, and accurate
multi-agent AI reasoning. It helps overcome the weaknesses of individual models
and brings out their collective strengths. This approach is similar to how
humans build consensus, making AI seem more collaborative and trustworthy
instead of just a black-box program.

</details>


### [149] [Murakkab: Resource-Efficient Agentic Workflow Orchestration in Cloud Platforms](https://arxiv.org/abs/2508.18298)
*Gohar Irfan Chaudhry,Esha Choukse,Haoran Qiu,Íñigo Goiri,Rodrigo Fonseca,Adam Belay,Ricardo Bianchini*

Main category: cs.MA

TL;DR: Murakkab은 에이전트 워크플로우를 위한 자원 효율적인 서비스 시스템으로, 워크플로우 사양과 실행 구성을 분리하여 최적화를 달성한다.


<details>
  <summary>Details</summary>
Motivation: 에이전트 워크플로우는 모델과 도구를 복잡한 제어 논리로 조정하지만, 현재의 프레임워크로는 비효율적인 서비스를 제공한다.

Method: Murakkab은 선언적 추상화를 도입하여 워크플로우 사양과 실행 구성을 분리하고, 프로파일 가이드를 활용한 최적화기와 적응형 런타임이 전체 스택을 관리한다.

Result: Murakkab은 GPU 사용량을 최대 2.8배 줄이고, 에너지 소비를 3.7배 줄이며, 비용을 4.3배 절감했다.

Conclusion: Murakkab은 기존 프레임워크와 클라우드 스케줄러가 달성하지 못하는 크로스 레이어 최적화를 가능하게 한다.

Abstract: Agentic workflows commonly coordinate multiple models and tools with complex
control logic. They are quickly becoming the dominant paradigm for AI
applications. However, serving them remains inefficient with today's
frameworks. The key problem is that they expose workflows as opaque sequences
of model and tool calls that tightly couple agent logic with model and hardware
choices. Often, these workflow components are fragmented across different
entities, preventing systems from reasoning about trade-offs across accuracy,
latency, energy, and cost. This leads to resource waste and degraded
service-level objectives (SLOs).
  We present Murakkab, a resource-efficient serving system for agentic
workflows. Murakkab introduces a declarative abstraction that decouples
workflow specification from execution configuration. A profile-guided optimizer
and adaptive runtime jointly manage the full stack: orchestrating workflow
components, mapping them to models and hardware, and dynamically reconfiguring
execution to satisfy user-defined SLOs. By exposing the internal structure of
agentic workflows, Murakkab enables cross-layer optimization that existing
frameworks and cloud schedulers cannot achieve.
  Our evaluation on diverse workflows shows that \sysname{} reduces GPU usage
by up to 2.8$\times$, energy consumption by 3.7$\times$, and cost by
4.3$\times$ while maintaining SLOs.

</details>


### [150] [Toward Generalized Autonomous Agents: A Neuro-Symbolic AI Framework for Integrating Social and Technical Support in Education](https://arxiv.org/abs/2508.18406)
*Ryan Hare,Ying Tang*

Main category: cs.MA

TL;DR: 이 논문은 학생들이 자신의 학습을 주도할 수 있도록 돕기 위한 다중 에이전트 신경 상징적 프레임워크를 제안한다.


<details>
  <summary>Details</summary>
Motivation: 학생들이 목표를 설정하고, 진행 상황을 추적하며, 어려움에 직면했을 때 전략을 조정하는 능력을 키우는 것이 교육에서 중요한 도전 과제 중 하나이다.

Method: 이 프레임워크는 RL 기반의 '튜터' 에이전트와 LLM 기반의 '또래' 에이전트를 함께 활용하여 교육적 역할을 수행한다.

Result: 대학 및 중학교 환경에서 사례 연구를 통해 프레임워크의 다양한 적용 가능성을 입증하였다.

Conclusion: AI 기반 학습 환경을 발전시키기 위한 핵심 통찰력과 미래 방향성을 제시하며 논문을 마무리한다.

Abstract: One of the enduring challenges in education is how to empower students to
take ownership of their learning by setting meaningful goals, tracking their
progress, and adapting their strategies when faced with setbacks. Research has
shown that this form of leaner-centered learning is best cultivated through
structured, supportive environments that promote guided practice, scaffolded
inquiry, and collaborative dialogue. In response, educational efforts have
increasingly embraced artificial-intelligence (AI)-powered digital learning
environments, ranging from educational apps and virtual labs to serious games.
Recent advances in large language models (LLMs) and neuro-symbolic systems,
meanwhile, offer a transformative opportunity to reimagine how support is
delivered in digital learning environments. LLMs are enabling socially
interactive learning experiences and scalable, cross-domain learning support
that can adapt instructional strategies across varied subjects and contexts. In
parallel, neuro-symbolic AI provides new avenues for designing these agents
that are not only adaptive but also scalable across domains. Based on these
remarks, this paper presents a multi-agent, neuro-symbolic framework designed
to resolve the aforementioned challenges. The framework assigns distinct
pedagogical roles to specialized agents: an RL-based 'tutor' agent provides
authoritative, non-verbal scaffolding, while a proactive, LLM-powered 'peer'
agent facilitates the social dimensions of learning. While prior work has
explored such agents in isolation, our framework's novelty lies in unifying
them through a central educational ontology. Through case studies in both
college-level and middle school settings, we demonstrate the framework's
adaptability across domains. We conclude by outlining key insights and future
directions for advancing AI-driven learning environments.

</details>


### [151] [Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare](https://arxiv.org/abs/2508.18708)
*Promise Osaine Ekpo,Brian La,Thomas Wiener,Saesha Agarwal,Arshia Agrawal,Gonzalo Gonzalez-Pumariega,Lekan P. Molu,Angelique Taylor*

Main category: cs.MA

TL;DR: 이 논문은 다중 에이전트 강화 학습에서의 공정성을 작업 부하 균형 문제로 다루고, 의료 분야에서 에이전트 전문성과 조정의 중요성을 강조한다.


<details>
  <summary>Details</summary>
Motivation: 의료 분야에서 공정한 작업 할당은 번아웃과 고숙련 에이전트의 과다 사용을 방지하기 위해 필요하다.

Method: FairSkillMARL 프레임워크를 제안하고, MARLHospital이라는 맞춤형 환경을 도입하여 팀 구성 및 에너지 제약 일정의 공정성에 대한 영향을 모델링한다.

Result: FairSkillMARL은 네 가지 표준 MARL 방법과 두 가지 최첨단 공정성 지표와 비교 실험을 통해 공정성이 작업 부하의 평등에만 기반할 경우 작업-기술 불일치를 초래할 수 있음을 보여준다.

Conclusion: 이 연구는 전문성과 노력의 조정이 중요한 이질적인 다중 에이전트 시스템에서 공정성을 연구하기 위한 도구와 기초를 제공한다.

Abstract: Fairness in multi-agent reinforcement learning (MARL) is often framed as a
workload balance problem, overlooking agent expertise and the structured
coordination required in real-world domains. In healthcare, equitable task
allocation requires workload balance or expertise alignment to prevent burnout
and overuse of highly skilled agents. Workload balance refers to distributing
an approximately equal number of subtasks or equalised effort across healthcare
workers, regardless of their expertise. We make two contributions to address
this problem. First, we propose FairSkillMARL, a framework that defines
fairness as the dual objective of workload balance and skill-task alignment.
Second, we introduce MARLHospital, a customizable healthcare-inspired
environment for modeling team compositions and energy-constrained scheduling
impacts on fairness, as no existing simulators are well-suited for this
problem. We conducted experiments to compare FairSkillMARL in conjunction with
four standard MARL methods, and against two state-of-the-art fairness metrics.
Our results suggest that fairness based solely on equal workload might lead to
task-skill mismatches and highlight the need for more robust metrics that
capture skill-task misalignment. Our work provides tools and a foundation for
studying fairness in heterogeneous multi-agent systems where aligning effort
with expertise is critical.

</details>


### [152] [Optimizing Highway Traffic Flow in Mixed Autonomy: A Multiagent Truncated Rollout Approach](https://arxiv.org/abs/2508.19203)
*Lu Liu,Chi Xie,Xi Xiong*

Main category: cs.MA

TL;DR: CAV와 HDV가 혼합된 환경에서 CAV의 속도 조정을 개선하기 위한 다중 에이전트 트렁케이션 롤아웃 접근법을 제안한다.


<details>
  <summary>Details</summary>
Motivation: CAV와 HDV가 공존하는 혼합 자율 환경에서 효율적인 조정이 도전 과제가 되고 있다.

Method: 교통 밀도 진화 방정식을 수립하고 분산 조정 제어 프레임워크를 통해 이웃 에이전트의 동역학 정보를 활용하는 다중 에이전트 트렁케이션 롤아웃 접근법을 제안한다.

Result: 제안된 방법은 정체 지역의 평균 이동 시간을 줄이고 전반적인 계산 시간을 단축시키며 전통적인 모델 예측 제어 방법보다 우수한 성과를 보인다.

Conclusion: 이러한 접근은 실시간 성능과 확장성을 개선하며, 실제 환경에서의 채용 가능성을 높인다.

Abstract: The development of connected and autonomous vehicles (CAVs) offers
substantial opportunities to enhance traffic efficiency. However, in mixed
autonomy environments where CAVs coexist with human-driven vehicles (HDVs),
achieving efficient coordination among CAVs remains challenging due to
heterogeneous driving behaviors. To address this, this paper proposes a
multiagent truncated rollout approach that enhances CAV speed coordination to
improve highway throughput while reducing computational overhead. In this
approach, a traffic density evolution equation is formulated that
comprehensively accounts for the presence or absence of CAVs, and a distributed
coordination control framework is established accordingly. By incorporating
kinematic information from neighbor agents and employing an agent-by-agent
sequential solution mechanism, our method enables explicit cooperation among
CAVs. Furthermore, we introduce a truncated rollout scheme that adaptively
shortens the optimization horizon based on the evaluation of control sequences.
This significantly reduces the time complexity, thereby improving real-time
performance and scalability. Theoretical analysis provides rigorous guarantees
on the stability and performance improvement of the system. Simulations
conducted on real-world bottleneck scenarios demonstrate that, in large-scale
mixed traffic flows, the proposed method outperforms conventional model
predictive control methods by reducing both the average travel time in the
bottleneck area and overall computational time, highlighting its strong
potential for practical deployment.

</details>
